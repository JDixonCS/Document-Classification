,sentence,label,data
0,Short Resource Papers,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Experiments with Convolutional Neural Network Models for Answer Selection,null,null
3,"Jinfeng Rao, Hua He",null,null
4,Department of Computer Science University of Maryland,null,null
5,"jinfeng@cs.umd.edu,huah@umd.edu",null,null
6,Jimmy Lin,null,null
7,David R. Cheriton School of Computer Science University of Waterloo jimmylin@uwaterloo.ca,null,null
8,ABSTRACT,null,null
9,"In recent years, neural networks have been applied to many text processing problems. One example is learning a similarity function between pairs of text, which has applications to paraphrase extraction, plagiarism detection, question answering, and ad hoc retrieval. Within the information retrieval community, the convolutional neural network model proposed by Severyn and Moschi i in a SIGIR 2015 paper has gained prominence. is paper focuses on the problem of answer selection for question answering: we a empt to replicate the results of Severyn and Moschi i using their open-source code as well as to reproduce their results via a de novo (i.e., from scratch) implementation using a completely di erent deep learning toolkit. Our de novo implementation is instructive in ascertaining whether reported results generalize across toolkits, each of which have their idiosyncrasies. We were able to successfully replicate and reproduce the reported results of Severyn and Moschi i, albeit with minor di erences in e ectiveness, but a rming the overall design of their model. Additional ablation experiments break down the components of the model to show their contributions to overall e ectiveness. Interestingly, we nd that removing one component actually increases e ectiveness and that a simpli ed model with only four word overlap features performs surprisingly well, even be er than convolution feature maps alone.",null,null
10,1 INTRODUCTION,null,null
11,"e problem of learning a similarity function between pairs of texts has broad applicability to a variety of natural language processing tasks, including paraphrase extraction, plagiarism detection, question answering, and ad hoc retrieval. e natural language processing community has made substantial strides in tackling this problem with approaches based on neural networks. In contrast to previous work that relies heavily on hand-cra ed features, models based on neural networks are not only more e ective, but also obviate the need for feature engineering.",null,null
12,"Recently, Severyn and Moschi i [14] (henceforth, SM for short) proposed a convolutional neural network model for learning similarities between pairs of short texts, which was applied to answer selection for question answering and to tweet reranking in an ad",null,null
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080648",null,null
14,"hoc retrieval task. Because this work has gained prominence in the information retrieval community (garnering, for example, a large number of citations in a short amount of time), in this paper we a empted to replicate and reproduce the authors' results.",null,null
15,"Our e orts proceeded in two steps: First, SM released their source code on GitHub,1 which uses the eano deep learning toolkit,2 and from this we a empted to replicate the results in their paper--by running their code directly. Second, we set aside their code and a empted to reproduce their model de novo (i.e., from scratch) using the Torch deep learning toolkit.3 Both e orts are instructive for di erent reasons: Replicating the original results using the authors' code veri es that the code does indeed correspond to the models described in the paper, and that there are no ""hidden dependencies"" missing in the code. Reproducing the model from scratch, using a completely di erent toolkit, represents a higher standard of veri ability--that the authors have adequately described their model, and that their results are robust with respect to idiosyncrasies in the underlying deep learning toolkits.",null,null
16,Contributions. We view our work as having three contributions:,null,null
17,"· We were able to successfully replicate and reproduce the results of Severyn and Moschi i, contributing to the community's growing interests in reproducibility and related issues [2, 10, 13].",null,null
18,"· Our ablation studies reveal new insights about the inner workings of the SM model in terms of understanding the contribution of di erent components: convolution feature maps, a similarity modeling component, and ""extra features"" that derive from traditional retrieval measures. Interestingly, we nd that removing the similarity modeling component actually increases e ectiveness in answer selection and that a simpli ed model with only four word overlap features performs surprisingly well, even be er than the complete SM model without the extra features.",null,null
19,"· We make our source code publicly available4 so that others can build on our work. Having publicly-available implementations of the same model in two di erent deep learning toolkits is instructive for developers trying to understand the intricacies of neural network models and the primitive ""building blocks"" o ered by competing toolkits.",null,null
20,2 MODEL OVERVIEW,null,null
21,2.1 Network Architecture,null,null
22,"As this paper primarily focuses on the reproducibility of results, we introduce the model architecture brie y and refer the reader to the",null,null
23,1h ps://github.com/aseveryn/deep-qa 2h p://deeplearning.net/so ware/theano/ 3h p://torch.ch/ 4h ps://github.com/castorini/SM-CNN-Torch,null,null
24,1217,null,null
25,Short Resource Papers,null,null
26,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
27,sentence matrix,null,null
28,convolution feature maps !,null,null
29,pooled representation!,null,null
30,similarity matching!,null,null
31,join ! layer!,null,null
32,hidden ! layer!,null,null
33,softmax!,null,null
34,document!,null,null
35,Fd,null,null
36,The! cat! sat! on! the! mat!,null,null
37,xd,null,null
38,xsim M,null,null
39,quer !y,null,null
40,Fq,null,null
41,xq,null,null
42,Where! was! the! cat! ?!,null,null
43,additional ! features xfeat,null,null
44,"Figure 1: e SM convolutional neural network model, comprised of three main components: convolution feature maps with pooling, a similarity matrix, and extra features.",null,null
45,"original paper for more details. e overall architecture of the SM model is shown in Figure 1, taken from the original paper for clarity.",null,null
46,"e model has a general ""Siamese"" structure [3] with two subnetworks processing the ""query"" and the ""document"" in parallel. is general architecture is fairly common and used in a variety of other models as well [6­8]. Implicit in this architecture is the assumption that both inputs are relatively short (i.e., sentences), since they are ultimately converted into xed-length vector representations.",null,null
47,"e input to each ""arm"" in the neural network is a sequence of words [w1, w2, ...w |S |], each of which is translated into its corresponding distributional vector (i.e., from a word embedding), yielding a sentence matrix. Convolution feature maps are applied to this sentence matrix, followed by ReLU activation and simple max-pooling, to arrive at a representation vector xq for the query and xd for the document.",null,null
48,"At the join layer (see Figure 1), all intermediate representations are concatenated into a single vector:",null,null
49,"xjoin , [xTq ; xsim; xTd ; xTfeat]",null,null
50,(1),null,null
51,where xsim de nes the bilinear similarity between xq and xd as,null,null
52,follows:,null,null
53,"sim(xq , xd ) , xTq Mxd",null,null
54,(2),null,null
55,is similarity matrix M is a parameter of the network that is,null,null
56,optimized during training.,null,null
57,e bilinear similarity matrix can be viewed as an adaptation,null,null
58,"of the noisy-channel model from machine translation, which has",null,null
59,"previously been used for question answering [4]. Basically, it cap-",null,null
60,"tures a transformation from the source embedding xd to the target embedding xd , Mxd where similarity to the input query xq is maximized. e parameters are initialized randomly and optimized",null,null
61,"through back-propagation during training. Finally, xfeat represents additional features that are task speci c--these are signi cant, as",null,null
62,we discuss later.,null,null
63,e original SM paper examined two speci c tasks: answer se-,null,null
64,lection for question answering and tweet reranking in an ad hoc,null,null
65,"retrieval task. In this paper, we only focus on the rst task for a",null,null
66,"number of reasons. Primarily, there are aspects of the SM model",null,null
67,that make its setup somewhat unrealistic for tweet reranking (as,null,null
68,"described in their paper): speci cally, the model uses as one of the",null,null
69,extra features the normalized rank across all systems that partici-,null,null
70,pated in the TREC Microblog evaluations. is setup is closer to an,null,null
71,Set,null,null
72,#,null,null
73,TRAIN TRAIN-ALL,null,null
74,Dev Test,null,null
75,estion,null,null
76,"94 1,229",null,null
77,84 100,null,null
78,# Answers,null,null
79,"4,718 53,417 1,148 1,517",null,null
80,% Correct,null,null
81,7.4% 12.0% 19.3% 18.7%,null,null
82,"Table 1: Statistics of the TrecQA dataset for answer selection, which contains two distinct training sets.",null,null
83,"oracle run fusion task than an actual tweet reranking task, since a system would not have access to scores from all other systems that participated in the evaluation. We have begun to examine the SM model on the tweet reranking task, but the page limitations of a short paper preclude us from diving into the full details.",null,null
84,2.2 Answer Selection,null,null
85,"Answer selection is an important component of an overall question answering system: given a question q and a candidate set of sentences {c1, c2, . . . cn }, the task is to identify sentences that contain the answer. In a standard pipeline architecture [15], answer selection is applied to the output of a module that performs passage retrieval based on some form of term matching. Selected sentences can then be directly presented to users or serve as input to subsequent stages that identify ""exact"" answers [16]. e experiments in this paper evaluate the answer selection task for question answering. Although nominally a classi cation task, system output is usually evaluated in terms of ranked retrieval metrics.",null,null
86,"For answer selection, the SM model uses ""extra features"" xfeat comprised of four word overlap measures between each sentence pair: word overlap and IDF-weighted word overlap computed between all words and only non-stopwords. e use of these external features aims to mitigate two issues associated with word embeddings. First, about 15% of words in the vocabulary are not found in the word embeddings; those word vectors are randomly initialized during training, which could result in suboptimal similarity learning. Second, even for those words found in the word embeddings, distributional representations sometimes can not capture the relatedness of numbers and proper nouns. However, such information is important, especially for factoid questions where many answers are either numbers or proper nouns.",null,null
87,3 EXPERIMENTS,null,null
88,"e SM paper evaluates their convolutional neural network on the popular TrecQA dataset, rst introduced by Wang et al. [19] and further elaborated by Yao et al. [20]. Basic statistics are shown in Table 1. e dataset contains a number of factoid questions, each of which is associated with candidate sentences that either contain or do not contain the answer (i.e., positive and negative examples).",null,null
89,"e questions are taken from the estion Answering Tracks from TREC 8­13, and the candidate answers derive from the output of track participants. Note that there are two distinct training sets, known as TRAIN and TRAIN-ALL. e TRAIN-ALL set is much larger but also contains more noise. Following previous work, the task is evaluated in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",null,null
90,We rst a empted to replicate the results of SM using the authors' open-source eano code. Results are reported in Table 2: e,null,null
91,1218,null,null
92,Short Resource Papers,null,null
93,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
94,Condition,null,null
95,TRAIN (full model) TRAIN (-xfeat) TRAIN-ALL (full model) TRAIN-ALL (-xfeat),null,null
96,MAP Reported,null,null
97,0.7329 0.6258,null,null
98,0.7459 0.6709,null,null
99,eano,null,null
100,0.7325 0.6271,null,null
101,0.7538 0.6817,null,null
102,Torch,null,null
103,0.7318 0.6408,null,null
104,0.7428 0.6841,null,null
105,MRR Reported,null,null
106,0.7962 0.6591,null,null
107,0.8078 0.7280,null,null
108,eano,null,null
109,0.8018 0.6570,null,null
110,0.8078 0.7249,null,null
111,Torch,null,null
112,0.7950 0.6780,null,null
113,0.8079 0.7398,null,null
114,"Table 2: Attempts to replicate and reproduce SM results. ""Reported"" columns list results in the original paper. "" eano"" columns list results from running the eano code provided by SM. ""Torch"" columns list results from our de novo Torch implementation of the model. Table also presents results of removing the ""extra features"" from the joined representation.",null,null
115,"columns marked ""Reported"" contain results copied directly from their SIGIR paper. e columns marked "" eano"" contain results from our replication e orts, following the original experimental conditions. e original paper conducted an ablation analysis removing the extra features, which we also a empt to replicate (the -xfeat condition in the table). Although we do not obtain exactly the same numbers for the di erent conditions, the results are quite close. Overall, we consider this replication a empt successful.",null,null
116,"Next, we a empted to reproduce the SM model from scratch using the Torch toolkit, based as close as possible to the description in the SIGIR paper. In our e orts to reproduce the results, we a empted to match the se ings described in the paper as closely as possible: choice of word embeddings, the width of the convolution",null,null
117,"lters, the number of feature maps, the optimization objective, the training regime, etc. ese results are also reported in Table 2 under the columns marked ""Torch"". Following the original paper and our replication study, we also performed an experimental run removing the extra features (the -xfeat condition in the table). In all cases, our numbers are quite close to both the eano replication results and the original reported numbers in the paper--despite a completely di erent codebase and the use of a di erent deep learning toolkit. We consider this reproduction e ort successful and can conclude that the model is robust to di erences in the underlying deep learning toolkits.",null,null
118,"Comparing the two implementations, we believe that our Torch code is more succinct and readable than the original eano implementation. Our Torch implementation consists of about 50 lines of model code and about 200 lines of training code, while the eano implementation has around 1000 lines of code for constructing the model and another 500 lines for training the model. is is because the Torch framework provides lots of modular pieces (e.g., convolution feature maps and a bilinear similarity modeling module) that are easy to combine, while the eano version implements all these components from scratch. Although eano o ers the nice abstraction of a computation graph that supports automatic gradient computation, writing the forward computations of complex modules remains burdensome. In terms of performance, both the Torch and eano implementations are very e cient, usually taking no more than 2­3 minutes per epoch when running on ve CPU threads on a standard commodity server. We typically reach convergence in 10­20 epochs.",null,null
119,"In Tables 3a and 3b, we report a number of results on the same experimental conditions from the literature: these are taken from an ACL wiki page that nicely summarizes the state of the art in this answer selection task [1]. We see that, without the extra features,",null,null
120,Reference,null,null
121,MAP,null,null
122,Yih et al. [21] (2013) 0.709,null,null
123,He et al. [6] (2015) 0.717,null,null
124,SM (full model),null,null
125,0.732,null,null
126,(a) TRAIN,null,null
127,MRR,null,null
128,0.770 0.800 0.795,null,null
129,Reference,null,null
130,MAP,null,null
131,Wang et al. [17] (2015) Miao et al. [11] (2015) He et al. [6] (2015) He and Lin [7] (2016) Rao et al. [12] (2016) SM (full model),null,null
132,0.713 0.734 0.762 0.755 0.780 0.743,null,null
133,(b) TRAIN-ALL,null,null
134,MRR,null,null
135,0.791 0.812 0.830 0.825 0.834 0.808,null,null
136,"Table 3: E ectiveness of the SM model on the TrecQA dataset under the TRAIN and TRAIN-ALL conditions, compared to other results from the literature.",null,null
137,"the SM model performs well below the state of the art. With the complete model, we can see that its e ectiveness is reasonably competitive but still below the best system today.",null,null
138,3.1 Ablation Study,null,null
139,"At a high-level, the SM model consists of three distinct components: the output of the convolution feature maps a er pooling xq and xd, the xsim similarity matrix, and the ""extra features"" xfeat. From a philosophical perspective, it can be argued that the ""extra"" word overlap features go against the ""spirit"" of neural network modeling, in that one of the major advantages of neural networks is the avoidance of manual feature engineering (which is what these extra features represent). Instead, one hopes that the network would discover correlates of these features automatically. As a point of comparison, the work of He et al. [6] requires no additional input features other than the original sentences.",null,null
140,"To be er understand the contributions of the various components to e ectiveness, we build on the ablation experiments in the previous section, removing each of the components in turn and in combination. For these experiments, we used our Torch implementation. Results are shown in Table 4.",null,null
141,"e rst three columns are the three categories of features used in the fully-connected layer, with a ""-"" and ""+"" entry denoting the removal or inclusion of each type of feature. e remaining columns show MAP and MRR scores on the TRAIN and TRAIN-ALL conditions. Note that in addition to the bilinear similarity modeling",null,null
142,1219,null,null
143,Short Resource Papers,null,null
144,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
145,TRAIN,null,null
146,TRAIN-ALL,null,null
147,"xq, xd xfeat xsim MAP MRR MAP MRR",null,null
148,- 0.638 0.669 0.631 0.690,null,null
149,+,null,null
150,-,null,null
151,bilinear 0.641 0.678 0.684 0.740 cosine 0.661 0.720 0.644 0.727,null,null
152,dot 0.636 0.681 0.655 0.707,null,null
153,-+,null,null
154,- 0.648 0.716 0.649 0.709,null,null
155,- 0.747 0.812 0.762 0.815,null,null
156,+,null,null
157,+,null,null
158,bilinear 0.732 0.795 0.743 0.808 cosine 0.735 0.774 0.742 0.804,null,null
159,dot 0.684 0.740 0.735 0.794,null,null
160,Table 4: Results of an ablation analysis removing di erent components of the SM model.,null,null
161,"(xsim ,"" xTq Mxd ) used in the original SM model, we also replaced it with a simple cosine similarity and dot product between the query and document feature maps. ese alternative formulations of xsim are also shown in the third column.""",null,null
162,"Looking at the rst block of rows where the extra features xfeat are discarded, we see that adding the similarity feature xsim consistently improves e ectiveness. is is no surprise as answer selection is basically a matching task, and thus the joint representation of xsim contributes additional signal beyond the query and document features. Comparing the TRAIN and TRAIN-ALL conditions, we observe a larger improvement in the data-rich condition.",null,null
163,"In the second block of rows, we show results with only the four word overlap features. is condition essentially uses the fullyconnected layer as a learning-to-rank module, which reduces the number of model parameters from 100K to 1200. Surprisingly, this simple feature engineering baseline works well, and actually be er than the SM model without the extra features (Row ""+ - bilinear""). Note that this simple model still contains two layers with non-linear activation between them, so it is learning useful nonlinear transformations of the input features for the task. is nding suggests that the lexical matching signals captured in word overlaps are more e ective than the convolution feature maps alone (given the amount of training data we have), which seems consistent with the ndings of Guo et al. [5]. Note that this simple baseline is more e ective than all approaches prior to around 2013 (e.g., [9, 18, 19]), according to the ACL Wiki [1].",null,null
164,"e third block of rows shows the results of varying the xsim component while retaining the other two. An interesting nding is that the model achieves the best e ectiveness in both the TRAIN and TRAIN-ALL conditions when the similarity feature is removed. In other words, removing xsim actually makes the model be er than the full model described by SM! Furthermore, looking at the rst and third blocks of rows, we see that cosine similarity is generally comparable to bilinear similarity for the xsim component. Cosine similarity, however, is far simpler in requiring no parameter tuning. Dot product similarity is consistently less e ective due to its lack of normalization.",null,null
165,"e above nding, combined with the ""+ - -"" and ""- + -"" conditions, suggests that the e ectiveness of the lexical word overlap features and the convolution feature maps are additive--these two components together explain the e ectiveness of the SM model.",null,null
166,4 CONCLUSIONS,null,null
167,ere is no doubt that deep learning applied to information retrieval,null,null
168,"is a ""hot"" emerging area, which makes it even more important to",null,null
169,verify published results and to ensure that we as a community are,null,null
170,"building on a solid foundation. In this work, we have successfully",null,null
171,replicated and reproduced a recent SIGIR paper that has gained,null,null
172,"prominence: for answer selection in question answering, we have",null,null
173,"not only validated the design of the SM model, but also deepened",null,null
174,our understanding of how its various components interact and,null,null
175,contribute to overall e ectiveness.,null,null
176,REFERENCES,null,null
177,"[1] ACL. 2017. estion Answering (State of the art). h p://www.aclweb.org/ aclwiki/index.php?title, estion Answering (State of the art). Accessed: 201705-01.",null,null
178,"[2] Jaime Arguello, Ma Crane, Fernando Diaz, Jimmy Lin, and Andrew Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR). SIGIR Forum 49, 2 (2015), 107­116.",null,null
179,"[3] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sa¨ckinger, and Roopak Shah. 1993. Signature Veri cation Using a ""Siamese"" Time Delay Neural Network. In NIPS. 737­744.",null,null
180,[4] Abdessamad Echihabi and Daniel Marcu. 2003. A Noisy-Channel Approach to estion Answering. In ACL. 16­23.,null,null
181,"[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM. 55­64.",null,null
182,"[6] Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks. In EMNLP. 1576­1586.",null,null
183,[7] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement. In NAACL. 937­948.,null,null
184,"[8] Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, and Jimmy Lin. 2016. UMDTTIC-UW at SemEval-2016 Task 1: A ention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement. In SemEval. 662­667.",null,null
185,"[9] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to estions. In HLT-NAACL. 1011­1019.",null,null
186,"[10] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",null,null
187,"[11] Yishu Miao, Lei Yu, and Phil Blunsom. 2015. Neural Variational Inference for Text Processing. arXiv:1511.06038.",null,null
188,"[12] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In CIKM. 1913­1916.",null,null
189,"[13] Jinfeng Rao, Jimmy Lin, and Miles Efron. 2015. Reproducible Experiments on Lexical and Temporal Feedback for Tweet Search. In ECIR. 755­767.",null,null
190,[14] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. 373­382.,null,null
191,"[15] Stefanie Tellex, Boris Katz, Jimmy Lin, Gregory Marton, and Aaron Fernandes. 2003. antitative Evaluation of Passage Retrieval Algorithms for estion Answering. In SIGIR. 41­47.",null,null
192,[16] Ellen M. Voorhees. 2002. Overview of the TREC 2002 estion Answering Track. In TREC.,null,null
193,[17] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In ACL. 707­712.,null,null
194,[18] Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and estion Answering. In COLING. 1164­1172.,null,null
195,"[19] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32.",null,null
196,"[20] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In HLT-NAACL. 858­867.",null,null
197,"[21] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. estion Answering Using Enhanced Lexical Semantic Models. In ACL. 1744­",null,null
198,1753.,null,null
199,Acknowledgments. is research was supported by the Natural Sciences,null,null
200,"and Engineering Research Council (NSERC) of Canada, with additional con-",null,null
201,tributions from the U.S. National Science Foundation under CNS-1405688.,null,null
202,We'd like to thank Aliaksei Severyn for sharing additional details about the,null,null
203,SM model and for commenting on an earlier dra of this paper.,null,null
204,1220,null,null
205,,null,null

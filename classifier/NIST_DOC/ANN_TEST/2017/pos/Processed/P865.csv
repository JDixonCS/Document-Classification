,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,An Extended Relevance Model for Session Search,null,null
3,Nir Levine,null,null
4,Technion - Israel Institute of Technology,null,null
5,"Haifa, Israel 32000 levin.nir1@gmail.com",null,null
6,Haggai Roitman,null,null
7,"IBM Research - Haifa Haifa, Israel 31905 haggai@il.ibm.com",null,null
8,Doron Cohen,null,null
9,"IBM Research - Haifa Haifa, Israel 31905 doronc@il.ibm.com",null,null
10,ABSTRACT,null,null
11,"e session search task aims at best serving the user's information need given her previous search behavior during the session. We propose an extended relevance model that captures the user's dynamic information need in the session. Our relevance modelling approach is directly driven by the user's query reformulation (change) decisions and the estimate of how much the user's search behavior a ects such decisions. Overall, we demonstrate that, the proposed approach signi cantly boosts session search performance.",null,null
12,1 INTRODUCTION,null,null
13,"We propose an extended relevance model for session search. Relevance models aim at identifying terms (words, concepts, etc) that are relevant to a given (user's) information need [5]. Within a session, user's information need, expressed as a sequence of one or more queries [1], may evolve over time. User's search behavior during the session may be utilized as an additional relevance feedback source by the underlying search system [1]. Given user's session history (i.e., previous queries, result impressions and clicks), the goal of the session search task is to best serve the user's newly submi ed query in the session [1].",null,null
14,"We derive a relevance model that aims at ""tracking"" the user's dynamic information need by observing the user's search behavior so far during the session. To this end, the proposed relevance model is driven by the user's query reformulation decisions. Our relevance modelling approach relies on previous studies that suggest that user query change decisions may (at least partially) be explained by the previous user search behavior in the session [4, 9, 12]. We utilize the derived relevance model for re-ranking the search results that are retrieved for the current user information need in the session. Overall, we demonstrate that, our relevance modeling approach can signi cantly boost session search performance compared to many other alternatives that also utilize session data.",null,null
15,2 RELATED WORK,null,null
16,"Few previous works have also utilized the session context (i.e., previous queries, retrieved results and clicks) as an implicit feedback",null,null
17,Work was done during a summer internship in IBM Research - Haifa.,null,null
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080664",null,null
19,"source for re ning the user's query [3, 8, 10, 11]. To this end, the query language model was either combined with the language models of previous queries [11] or retrieved (clicked) results [8, 10]. In addition, di erent query score aggregation strategies for session search were explored [3]. Yet, none of these previous works have actually considered the user's query change process itself as a possible implicit feedback source.",null,null
20,"Several recent works have studied various query reformulation (change) behaviors during search sessions [4, 9, 12]. Among the various features that were studied, word-level features were found to best explain the changes in user queries during search sessions [4, 9]. A notable feature was found to be the occurrence of query (changed) words in the contents of results that the user previously viewed or clicked [4, 9, 12].",null,null
21,"Few previous works have also utilized query change for the session search task (e.g., [6, 12]). Common to such works is the modeling of user queries and their change as states and actions within various Reinforcement Learning inspired query weighting and aggregation schemes [7]; In this work we take a rather more ""traditional"" approach, inspired by the relevance model framework [5].",null,null
22,3 APPROACH,null,null
23,3.1 Session model,null,null
24,"Session search is a multi-step process, where at each step t, the user may submit a new query qt . e search system then retrieves the top-k documents Dq[kt] from a given corpus D that best match the user's query1. e user may then examine the results list; each result usually includes a link to the actual content and is accompanied with a summary snippet. e user may also decide to click on one or more of the results in the list in order to examine their actual content. Let Cqt denote the corresponding set of clicked results in Dq[kt]. In case the user decides to continue and submits a subsequent query, step t ends and a new step t + 1 begins. Let Sn-1 represent the session history (i.e., user queries, retrieved result documents, and clicked results) that was ""recorded"" prior to the current (latest) submi ed user query qn . On each step 1  t  n - 1, the session history is represented by a tuple St ,"" Qt , Dt , Ct . Qt "","" (q1, q2, . . . , qt ) is the sequence of queries submi ed by the user up to step t . Dt "","" (Dq[k1], Dq[k2], . . . , Dq[kt]) is the corresponding sequence of (top-k) retrieved result lists. Ct "","" (Cq1 , Cq2 , . . . , Cqt ) further represents the corresponding sequence of user clicks.""",null,null
25,"1With k , 10 in the TREC session track benchmarks [1] that we use later on in our evaluation.",null,null
26,865,null,null
27,Short Research Paper,null,null
28,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
29,3.2 Information need dynamics,null,null
30,e session search task is to best answer the current user's query,null,null
31,qn while considering Sn-1 [1]. Let I denote the user's (hidden) information need in the session. e goal of our relevance mod-,null,null
32,"elling approach is, therefore, to be er capture the user's informa-",null,null
33,tion need I which may evolve during the session. In order to cap-,null,null
34,"ture such dynamics, let It further represent the user's information",null,null
35,"need at step t. We now assume that, It depends both on the previ-",null,null
36,ous (dynamic) information need It-1 prior to query qt submission,null,null
37,and the possible change in such need It,null,null
38,def,null,null
39,",",null,null
40,It -1  It ; It,null,null
41,is,null,null
42,assumed be to implied by the change the user has made to her pre-,null,null
43,vious query qt -1 to obtain query qt .,null,null
44,3.3 ery change as relevance feedback,null,null
45,"We utilize the user's query reformulation (change) process during the session as an implicit relevance feedback for estimating the change in the user's information need It . As been suggested by previous works [4, 9, 12], user's query changed terms may actually occur in the contents of previously viewed (clicked) search results in St-1. is, therefore, may (partially) explain how the user decided to reformulate her query from qt-1 to qt [4, 9, 12]. Our proposed relevance model aims at exploiting such query changed term occurrences within the contents of previously viewed (clicked) results so as to discover those terms w (over some vocabulary V ) that are the most relevant to the current user's information need In . As a consequence, such terms may be used for query expansion aiming to be er serve the current user's information need In .",null,null
46,"Given query qt , compared to the previous query qt-1, there can be three main query change types, namely term retention, addition and removal [4, 9, 12]. User term retention, given by the set of terms that appear in both query qt and qt-1 and denoted qt, usually represent the (general) thematic aspects of the user's information need [4, 9, 12]. Added terms (denoted qt+) are those terms that the user added to query qt-1 to obtain query qt . A user may add new related terms that were encountered in previous results so as to improve the chance of nding relevant content [4]. On the other hand, a user may remove terms from a previous query qt-1 (further denoted qt-) in order to terminate a subtask or trying to improve bad performing queries [4].",null,null
47,3.4 Relevance model derivation,null,null
48,"Similar to previous works on relevance models [5], our goal is to discover those terms w ( V ) that are the most relevant to the user's information need In ; To this end, given the user's current query qn and session history Sn-1, let Sn denote our estimate of the relevance (language) model. On each step 1  t  n, such estimation is given by the following rst-order autoregressive model:",null,null
49,p(w,null,null
50,|,null,null
51,St,null,null
52,),null,null
53,def,null,null
54,",",null,null
55,"t p(w | St-1 ) + (1 - t )p(w | Ft ),",null,null
56,(1),null,null
57,"where  Ft now denotes the feedback model which depends on the user's (reformulated) query qt . While St-1 estimates the dynamic information need prior to step t (i.e., It-1),  Ft captures the relative change in such need at step t (i.e., It ).",null,null
58,t further controls the relative importance we assign to model,null,null
59,"exploitation (i.e., St-1 ) versus model exploration (i.e.,  Ft ). t parameter is dynamically determined based on the relevance model's",null,null
60,self-clarity at step t [2]. Self-clarity estimates how much the prior,null,null
61,"model St-1 already ""covers"" the feedback model  Ft ; formally:",null,null
62,t,null,null
63,def,null,null
64,",",null,null
65,· exp-DK L (Ft,null,null
66,"St -1 ),",null,null
67,(2),null,null
68,"where   [0, 1] and DK L( Ft St-1 ) is the Kullback-Leibler divergence between the two (un-smoothed) language models [13].",null,null
69,"Finally, given qn , the current user's query in the session, we de-",null,null
70,rive the relevance model Sn by inductively applying Eq. 1 (with,null,null
71, S0,null,null
72,def,null,null
73,",",null,null
74,0). We next derive the feedback model  Ft .,null,null
75,3.5 Feedback model derivation,null,null
76,"Our estimate of  Ft aims at discovering those terms (in qt , qt -1 or others in V ) that are most relevant to the change in user's dynamic",null,null
77,"information need from It-1 to It (i.e., It ). Given queries qt and qt-1, we rst classify their occurring terms w  according to their role in the query change. Let qt further denote the set of terms w  that are classi ed to the same type of query change (i.e., qt  {qt , qt +, qt -}).",null,null
78,Our relevance model now relies on the fact that query changed,null,null
79,terms may also occur within the contents of results that were pre-,null,null
80,"viously viewed (or clicked) by the user [4, 9, 12]. erefore, on",null,null
81,"each step t, let Ft denote the set of results that are used for (implicit) relevance feedback. We determine the set of results to be",null,null
82,"included in Ft as follows. If up to step t < n there is at least one clicked result, then we assign Ft ,"" 1j t Cqj . Otherwise, we rst de ne a pseudo information need Qt. Qt represents a (crude) estimate of the user's (dynamic) information need up to step t and""",null,null
83,"is obtained by concatenating the text of all observed queries in Qt (with each query having the same importance, following [11]). We",null,null
84,then de ne Ft as the set of top-m results in 1j t Dqj with the,null,null
85,highest query-likelihood given Qt (representing pseudo-clicks). Let,null,null
86,p[µ,null,null
87,](w,null,null
88,|x,null,null
89,),null,null
90,def,null,null
91,",",null,null
92,t,null,null
93,f,null,null
94,"(w,",null,null
95,x,null,null
96,)+µ,null,null
97,t,null,null
98,f,null,null
99,"(w , D) |D|",null,null
100,|x |+µ,null,null
101,now denote the Dirichlet smoothed,null,null
102,language model of text x with parameter µ [13]. Inspired by the,null,null
103,"RM1 relevance model [5], we estimate  Ft as follows:",null,null
104,def,null,null
105,"p(w | Ft ) ,",null,null
106,p[0](w |d ) ·,null,null
107,"p(d |qt )p(qt ) , (3)",null,null
108,d Ft,null,null
109,qt,null,null
110,"where p(qt ) denotes the (prior) likelihood that, while reformulating query qt-1 into qt , the user will choose to either add (i.e., qt +), remove (i.e., qt -) or retain (i.e., qt ) terms. Such likelihood can be pre-estimated [9] (i.e., parameterized); e.g., similarly",null,null
111,"to the QCM approach [12]. Yet, for simplicity, in this work we as-",null,null
112,"sume that every user query change action has the same odds (i.e.,",null,null
113,p(qt ),null,null
114,",",null,null
115,1 3,null,null
116,).,null,null
117,"Please note that, the main di",null,null
118,erence between our,null,null
119,estimate of  Ft and the RM1 model is in the way the later scores documents in Ft . Such score in RM1 is based on a given query,null,null
120,"qt [5], with no further distinction between the role that each query",null,null
121,term plays or the fact that some of the terms are actually removed,null,null
122,"terms that appeared in the previous query qt-1. Similar to RM1,",null,null
123,we,null,null
124,further,null,null
125,estimate p(d |qt ),null,null
126,d,null,null
127,p(qt |d ) p(qt |d,null,null
128,Ft,null,null
129,),null,null
130,.,null,null
131,866,null,null
132,Short Research Paper,null,null
133,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
134,User added or retained terms are those terms that are preferred,null,null
135,"to be included in the feedback documents Ft . On the other hand, removed terms are those terms that should not appear in Ft [4]. In accordance, we de ne:",null,null
136,p(qt,null,null
137,|d,null,null
138,),null,null
139,def,null,null
140,",",null,null
141,"p[µ](w |d ),",null,null
142,w qt,null,null
143,"1 - w qt- p[0](w |d ),",null,null
144,"qt  {qt, qt +} qt , qt - (4)",null,null
145,"In order to avoid query dri , on each step t, we further anchor",null,null
146,the feedback model  Ft to the query model qt [5] as follows:,null,null
147,p(w,null,null
148,|,null,null
149, Ft,null,null
150,),null,null
151,def,null,null
152,",",null,null
153,"(1 - t )p[0](w |qt ) + t p(w | Ft ),",null,null
154,(5),null,null
155,where t,null,null
156,def,null,null
157,",",null,null
158," · sim(qt , qn ) is a dynamic query anchoring",null,null
159,"parameter,   [0, 1] and sim(qt , qn ) is calculated using the (idf-",null,null
160,boosted) Generalized-Jaccard similarity measure; i.e.:,null,null
161,"min (t f (w, qt ), t f (w, qn )) · id f (w)",null,null
162,s i m (qt,null,null
163,",",null,null
164,qn,null,null
165,),null,null
166,def,null,null
167,",",null,null
168,w qt qn,null,null
169,"max (t f (w, qt ), t f (w, qn )) · id f (w)",null,null
170,(6),null,null
171,w qt qn,null,null
172,"According to t de nition, the similar query qt is to the current query qn , the more relevant is the query change in user's information need It (modelled by  Ft ) is assumed to be to the current user's information need In; erefore, less query anchoring e ect is assumed to be needed using query qt .",null,null
173,4 EVALUATION 4.1 Datasets,null,null
174,2011,null,null
175,2012,null,null
176,2013,null,null
177,(train),null,null
178,(test),null,null
179,(test),null,null
180,Sessions,null,null
181,Sessions,null,null
182,76,null,null
183,98,null,null
184,87,null,null
185,eries/session,null,null
186,3.7±1.8,null,null
187,3.0±1.6,null,null
188,5.1±3.6,null,null
189,Topics,null,null
190,Sessions/topic,null,null
191,1.2±0.5,null,null
192,2.0±1.0,null,null
193,2.2±1.0,null,null
194,Judged docs/topic 313±115,null,null
195,372±163,null,null
196,268±117,null,null
197,Collection,null,null
198,Name,null,null
199,ClueWeb09B ClueWeb09B ClueWeb12B,null,null
200,#documents,null,null
201,"28,810,564 28,810,564 15,700,650",null,null
202,Table 1: TREC session track benchmarks,null,null
203,"Our evaluation is based on the TREC 2011-2013 session tracks [1] (see benchmarks details in Table 1). e Category B subsets of the ClueWeb09 (2011-2012 tracks) and ClueWeb12 (2013 track) collections were used. Each collection has nearly 50M documents. Documents with spam score below 70 were ltered out. Documents were indexed and searched using the Apache Solr2 search engine. Documents and queries were processed using Solr's English text analysis (i.e., tokenization, Poter stemming, stopwords, etc).",null,null
204,2h p://lucene.apache.org/solr/,null,null
205,4.2 Baselines,null,null
206,We compared our proposed relevance modelling approach (hereina er denoted SRM3) with several di erent types of baselines.,null,null
207,"is includes state-of-the-art language modeling methods that utilize session context data (i.e., previous queries, viewed or clicked results); namely FixedInt [8] (with  ,"" 0.1,  "", 1.0 following [8]) and its Bayesian extension BayesInt [8] (with µ ,"" 0.2,  "","" 5.0, following [8]) ­ both methods combine the query qn model with the history queries Qn-1 and clicks Cn-1 centroid models; BatchUp [8] (with µ "","" 2.0,  "","" 15.0, following [8]) which iteratively interpolates the language model of clicks that occur up to each step t using a batched approach; and the Expectation Maximization (EM) based approach [10] (hereina er denoted LongTEM with q "","" 0, C "", 20 and N C ,"" 1, following [10]), which rst interpolates each query qt model with its corresponding session history model (based on both clicked (C) and non-clicked (NC) results in Dq[kt]); the (locally) interpolated query models are then combined based on their relevant session history using the EM-algorithm [10].""",null,null
208,"Next, we implemented two versions of the Relevance Model [5]. e rst is the basic RM3 model, denoted RM3(qn ), learned using the last query qn and the top-m retrieved documents as pseudo relevance feedback. e second, denoted RM3(Qn ), uses the pseudo information need Qn (see Section 3.5) instead of qn . We also implemented two query aggregation methods, namely: QA(uniform) which is equivalent to submi ing Qn as the query [11]; the second, denoted QA(decay), further applies an exponential decay approach to prefer recent queries to earlier ones (with decay parameter  ,"" 0.92, following [3, 12]). We further implemented three versions of the ery Change Model (QCM) ­ an MDP-inspired query weighting and aggregation approach [12]. Following [12] recommendation, QCM's parameters were set as follows  "","" 2.2,  "","" 1.8,  "","" 0.07,  "", 0.4 and  ,"" 0.92. e three QCM versions are the basic QCM approach [12]; QCM(SAT) which utilizes only """"satis ed"""" clicks (i.e., clicks whose dwell-time is at least 30 seconds [12]); and QCM(DUP) which ignores duplicate session queries [12]. Finally, in order to evaluate the relative e ect of the query-change driven feedback model (i.e.,  Ft ), we implemented a variant of SRM by replacing the query-change driven score of Eq. 3 with the RM1 document score (i.e., p(d |qn )). Let SRM(QC) and SRM(RM1) further denote the query-change and """"RM1- avoured"""" variants of SRM, respectively. It is important to note that, SRM(RM1) still relies on the dynamic relevance model updating formula (see Eq. 1) and the dynamic coe cients t and t ­ both further depend on the session dynamics (captured by St-1 and  Ft ).""",null,null
209,4.3 Setup,null,null
210,"Our evaluation is equivalent to the TREC 2011-2012 RL4 and TREC 2013 RL2 sub-tasks [1]. To this end, given each session's (last) query qn, we rst retrieved the top-2000 documents with the highest query likelihood (QL) score4 to qn . Documents were then reranked using the various baselines by multiplying their (initial)",null,null
211,"3Stands for ""Session-Relevance Model"". 4For this we used Solr's LMSimilarity with Dirchlet smoothing parameter µ , 2500 which is similar to Indri's default parameter.",null,null
212,867,null,null
213,Short Research Paper,null,null
214,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
215,TREC 2012,null,null
216,TREC 2013,null,null
217,Method Initial retrieval FixInt [8] BayesInt [8] BatchUp [8] LongTEM [10] RM3(qn ) [5] RM3(Qn ) QA(uniform) [11] QA(decay) [3] QCM [12] QCM(SAT) [12] QCM(DUP) [12] SRM(RM1) SRM(QC),null,null
218,nDCG@10 0.249r q 0.333r q 0.334r q 0.320r q 0.332r q 0.311r q 0.305r q 0.301r q 0.303r q 0.329r q 0.298r q 0.299r q 0.348q 0.356r,null,null
219,nDCG 0.256r q 0.296r q 0.297r q 0.288r q 0.295r q 0.284r q 0.284r q 0.282r q 0.284r q 0.262r q 0.281r q 0.281r q,null,null
220,0.300,null,null
221,0.304,null,null
222,nERR@10 0.302r q 0.380r q 0.382r q 0.368r q 0.389r q 0.369r q 0.354r q 0.352r q 0.353r q 0.306r q 0.347r q 0.350r q 0.395q 0.405r,null,null
223,MRR 0.594r q 0.679r q 0.674r q 0.664r q 0.667r q 0.654r q 0.647r q 0.646r q 0.645r q 0.574r q 0.635r q 0.631r q 0.699q 0.716r,null,null
224,nDCG@10 0.113r q 0.165r q 0.171r q 0.181r q 0.167r q 0.134r q 0.153r q 0.160r q 0.163r q 0.158r q 0.158r q 0.160r q 0.188q 0.193r,null,null
225,nDCG 0.105r q 0.132r q 0.131r q,null,null
226,0.134 0.131r q 0.122r q 0.129r q 0.130r q 0.131r q 0.129r q 0.129r q 0.130r q,null,null
227,0.137,null,null
228,0.138,null,null
229,nERR@10 0.140r q 0.209r q 0.208r q 0.233r q 0.205r q 0.161r q 0.203r q 0.204r q 0.207r q 0.201r q 0.202r q 0.208r q 0.240q 0.248r,null,null
230,MRR 0.390r q 0.544r q 0.527r q 0.581r q 0.530r q 0.422r q 0.553r q 0.546r q 0.550r q 0.535r q 0.545r q 0.559r q 0.601q 0.612r,null,null
231,"Table 2: Evaluation results. e r and q superscripts denote signi cant di erence with SRM(RM1) and SRM(QC), respectively (p < 0.05).",null,null
232,"QL score with the score determined by each method. e document scores of the various language model baselines (i.e., FixInt, BayesInt, BatchUp, LongTEM and the variants of RM3 and SRM ) were further determined using the KL-divergence score [13]; where each baseline's learned model was clipped using a xed cuto of 100 terms [13]. e TREC session track trec eval tool5 was used for measuring retrieval performance. Using this tool, we measured the nDCG@10, nDCG (@2000), nERR@10 and MRR of each baseline. Finally, we tuned the RM3 and SRM's free parameters6 using the TREC 2011 track as a train set. e parameters were optimized so as to maximize MAP. e TREC 2012-2013 tracks were used as the test sets.",null,null
233,4.4 Results,null,null
234,"e evaluation results are summarized in Table 2. e rst row reports the quality of the initial retrieval. Overall, compared to the various alternative baselines, the two SRM variants provided signi cantly be er performance; with at least +6.6%, +2.4%, +4.1% and +5.3% be er performance in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. e results clearly demonstrate the dominance of the session-context sensitive language modeling approaches (and the two SRM variants among them) over the other alternatives we evaluated. Furthermore, SRM's consideration of the user's query-change process as an additional relevance feedback source results in a more accurate estimate of the user's information need.",null,null
235,"Next, compared to the RM3 variants, it is clear from the results that a dynamic relevance modeling approach that is driven by query-change (such as SRM) is a be er choice for the session search task. Moving from an ad-hoc relevance modelling approach (i.e., one that only focuses on the last query in the session) to a session-context sensitive approach provides signi cant boost in performance; with at least +14%, +7.0%, +9.8% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks.",null,null
236,"5h p://trec.nist.gov/data/session/12/session eval main.py 6  {0.1, 0.2, . . . , 0.9},   {0.1, 0.2, . . . , 0.9}, m  {5, 10, . . . , 100}",null,null
237,"We further observe that, compared to the baseline methods that",null,null
238,"implement various query aggregation and scoring schemes (i.e.,",null,null
239,"QA and QCM variants), a query-expansion strategy based on the",null,null
240,user's dynamic information need (such as the one implemented by,null,null
241,SRM variants) provides a much be er alternative; with at least,null,null
242,"+18.5%, +6.1%, +15.1% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks.",null,null
243,"Finally, comparing the two SRM variants side-by-side, it be-",null,null
244,"comes even more clear that, using the query-change as an addi-",null,null
245,tional relevance feedback source is the be er choice; with at least,null,null
246,"+2.3%, +1.0%, +2.5% and +1.8% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. Please",null,null
247,"recall that, SRM(QC) was trained with a xed and equal-valued",null,null
248,"priors p(qt ). Hence, a further improvement may be obtained by be er tuning of these priors.",null,null
249,REFERENCES,null,null
250,"[1] Ben Cartere e, Paul Clough, Mark Hall, Evangelos Kanoulas, and Mark Sanderson. Evaluating retrieval over sessions: e trec session track 2011-2014. In Proceedings of SIGIR'2016.",null,null
251,[2] Steve Cronen-Townsend and W. Bruce Cro . antifying query ambiguity. In Proceedings of HLT'2002.,null,null
252,[3] Dongyi Guan and Hui Yang. ery aggregation in session search. In Proceedings of DUBMOD'2014.,null,null
253,[4] Jiepu Jiang and Chaoqun Ni. What a ects word changes in query reformulation during a task-based search session? In Proceedings of CHIIR'2016.,null,null
254,[5] Victor Lavrenko and W. Bruce Cro . Relevance based language models. In Proceedings of SIGIR'2001.,null,null
255,"[6] Jiyun Luo, Xuchu Dong, and Hui Yang. Session search by direct policy learning. In Proceedings of ICTIR'2015.",null,null
256,"[7] Jiyun Luo, Sicong Zhang, Xuchu Dong, and Hui Yang. Designing states, actions, and rewards for using pomdp in session search. In Proceedings of ECIR'2005, pages 526­537. Springer, 2015.",null,null
257,"[8] Xuehua Shen, Bin Tan, and ChengXiang Zhai. Context-sensitive information retrieval using implicit feedback. In Proceedings of SIGIR'2005.",null,null
258,"[9] Marc Sloan, Hui Yang, and Jun Wang. A term-based methodology for query reformulation understanding. Inf. Retr., 18(2):145­165, April 2015.",null,null
259,"[10] Bin Tan, Xuehua Shen, and ChengXiang Zhai. Mining long-term search history to improve search accuracy. In Proceedings of KDD'2006.",null,null
260,"[11] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. Lexical query modeling in session search. In Proceedings of ICTIR'2016.",null,null
261,"[12] Hui Yang, Dongyi Guan, and Sicong Zhang. e query change model: Modeling session search as a markov decision process. ACM Trans. Inf. Syst., 33(4):20:1­ 20:33, May 2015.",null,null
262,"[13] Chengxiang Zhai and John La erty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), April 2004.",null,null
263,868,null,null
264,,null,null

,sentence,label,data
0,Session 4A: Evaluation 2,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Comparing In Situ and Multidimensional Relevance Judgments,null,null
3,Jiepu Jiang,null,null
4,"Center for Intelligent Information Retrieval, University of Massachuse s",null,null
5,Amherst jpjiang@cs.umass.edu,null,null
6,Daqing He,null,null
7,"School of Computing and Information, University of Pi sburgh",null,null
8,dah44@pi .edu,null,null
9,James Allan,null,null
10,"Center for Intelligent Information Retrieval, University of Massachuse s",null,null
11,Amherst allan@cs.umass.edu,null,null
12,ABSTRACT,null,null
13,"To address concerns of TREC-style relevance judgments, we explore two improvements. e rst one seeks to make relevance judgments contextual, collecting in situ feedback of users in an interactive search session and embracing usefulness as the primary judgment criterion. e second one collects multidimensional assessments to complement relevance or usefulness judgments, with four distinct alternative aspects examined in this paper--novelty, understandability, reliability, and e ort.",null,null
14,"We evaluate di erent types of judgments by correlating them with six user experience measures collected from a lab user study. Results show that switching from TREC-style relevance criteria to usefulness is fruitful, but in situ judgments do not exhibit clear bene ts over the judgments collected without context. In contrast, combining relevance or usefulness with the four alternative judgments consistently improves the correlation with user experience measures, suggesting future IR systems should adopt multi-aspect search result judgments in development and evaluation.",null,null
15,"We further examine implicit feedback techniques for predicting these judgments. We nd that click dwell time, a popular indicator of search result quality, is able to predict some but not all dimensions of the judgments. We enrich the current implicit feedback methods using post-click user interaction in a search session and achieve be er prediction for all six dimensions of judgments.",null,null
16,KEYWORDS,null,null
17,Relevance judgment; search experience; implicit feedback.,null,null
18,1 INTRODUCTION,null,null
19,"Test collection-based IR evaluation relies on human assessments of search result quality. e most popular method is the Cran eldstyle relevance judgments [9], such as the approach used in TREC [10], where assessors (usually trained experts) judge a preassigned set of search results one a er another using criteria that focus on topical relevance. is method had achieved great success but also a racted criticism such as focusing solely on topical relevance and ignoring real users' perceptions of the usefulness of results in a particular search context. We examine two directions to improve this status quo.",null,null
20,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080840",null,null
21,"One direction is to incorporate context into assessments. at is, the value of a search result depends on the scenario and context of accessing the result. Belkin et al. [5] proposed to evaluate interactive search systems by the usefulness of each interaction for accomplishing a search task. We can apply this model to search result judgments--to assess the usefulness of a click (the perceived usefulness of a clicked result). is intrinsically requires us to switch from relevance to usefulness as the primary judgment criteria, and to collect in situ judgments to take into account the particular time and context of accessing a search result.",null,null
22,"Two recent e orts [25, 36] examined this direction. Kim et al. [25] collected users' in situ feedback of clicked results a er they had",null,null
23,"nished examining the results. However, they restricted the in situ feedback to ""thumbs-up"" or ""thumbs-down"". Mao et al. [36] asked users to assess the usefulness of the clicked results a er a search session without considering the particular context. Both studies reported improved correlations with search experience measures comparing to TREC-style relevance judgments by external assessors. However, neither study excluded the in uence of the di erence between searchers and external assessors on relevance judgments.",null,null
24,"e other direction is to use a combination of multiple aspects of judgments. Many previous studies tried to complement relevance with seemingly reasonable dimensions, such as novelty [6, 55], understandability [41, 56], credibility [39, 46, 51, 53], readability [42, 49], e ort [20, 50, 54], freshness [11], etc. Multidimensional judgments are also popular approaches used in user-centric evaluation models [19, 27, 52]. However, most previous IR studies had only examined one particular alternative dimension to relevance, and they had not veri ed the value of multidimensional judgments by correlating with user experience measures.",null,null
25,"We evaluate and compare these two directions. We collected users' search result judgments from six dimensions (relevance, usefulness, novelty, understandability, reliability, and e ort) in two se ings--an in situ one that happened right a er users had nished examining a clicked search result (called in situ judgments), and a context-independent one collected a er a search session (called post-session judgments). We evaluate the two types of judgments on six dimensions by correlating with six search experience measures collected from a laboratory user study. We also examined implicit feedback methods for predicting these judgments.",null,null
26,We examine the following questions in the rest of this article:,null,null
27,· Do in situ judgments be er correlate with search experience measures than context-independent (post-session) ones? Do multiple dimensions of judgments help relevance/usefulness judgments be er correlate with search experience measures? Which dimensions of judgments should we collect to improve a particular user experience measure? Section 3 seeks answers to these questions.,null,null
28,405,null,null
29,Session 4A: Evaluation 2,null,null
30,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
31,Figure 1: A screenshot of the search interface and the in situ judgments interface.,null,null
32,· Can we e ectively predict di erent search result judgments using implicit feedback signals? Section 4 and Section 5 examine techniques for addressing this challenge.,null,null
33,2 USER STUDY,null,null
34,We designed a user study to collect search result judgments. e user study asked participants to work on di erent tasks in an experimental search system. We recorded users' search behavior and collected their in situ and post-session search result judgments.,null,null
35,2.1 Experiment Design,null,null
36,e user study employed a 2×2 within-subject design to balance di erent types of search tasks. e tasks come from the TREC session tracks [7] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classi cation framework [28]. e targeted task product is either factual (to locate facts) or intellectual (to enhance the user's understanding of a topic). e goal of a task is either speci c (clear and fully developed) or amorphous (an ill-de ned or unclear goal that may evolve along with the user's exploration).,null,null
37,We divided participants into groups of four. Participants in the same group worked on the same four tasks (one task for each type) but using a di erent sequence (rotated using a Latin square). We assigned di erent tasks to di erent groups to increase task diversity.,null,null
38,"For each task, the participants went through two stages: · Search stage (10 minutes). e participants performed an in-",null,null
39,"teractive search session to address the task. ey could submit and reformulate any queries and click on any search results. After clicking on a result's link, the participants switched to the result webpage in a new browser tab. When they had nished examining the result and turned back to the SERP, the participant needed to provide in situ judgments on the clicked results before they could resume the search session. Figure 1 shows the screenshots of the search interface and the in situ judgments. · Judgment stage (about 10 minutes). e participants rated their search experience in the session and nished post-session judgments on each result they visited in the session. Section 2.2 introduces details of the in situ and post-session judgments. As Figure 1 shows, the interface of the experimental system is similar to popular web search engines. e system redirected users' queries to Google and returned ltered Google search results. e system only showed the ""10-blue links"", vertical search results",null,null
40,"(except image verticals), and related queries. Other SERP elements were removed to simplify the user study. e system displayed results in the same way they would appear on Google. e main di erence between our system and Google in SERP design was that our system showed task description on the top of a SERP (to help participants recall task requirements) and we showed related searches on the right side of a SERP.",null,null
41,"e participants spent about 100 minutes to nish an experiment. First, they worked on a training task (including all the steps) for 10 minutes. en, they worked on four formal tasks, spending about 20 minutes on each task. We required the participants to take a 5-minute break a er two formal tasks to reduce fatigue.",null,null
42,2.2 Collecting Search Result Judgments,null,null
43,We collected search result judgments in two di erent scenarios: · In situ judgments ­ participants assessed a clicked result when,null,null
44,they had nished examining it and turned back to the SERP. · Post-session judgments ­ the judgments collected a er a search,null,null
45,session (in the judgment stage). e in situ judgments measure the participants' perceptions of,null,null
46,"the clicked result at (roughly) the same time and contexts they visit the result. e approach is similar to Kim et al. [25], except that we adopted di erent measures to assess search results. In the search stage, we instructed the participants to examine results as they would normally do when using a search engine in their daily lives. For example, they did not need to fully read a result and they could abandon examining. Particularly, they were instructed that during the in situ judgments, they should not revisit the result for the purpose of answering the judgment questions (and we did not o er a link for revisiting in the in situ judgment interface). is is to ensure that the in situ judgments only measure participants' perceptions of the latest click activity.",null,null
47,"e post-session judgments resemble the TREC-style relevance judgments, where the assessors judge results without a particular search context and in a random order--they are asked to judge a set of results one a er another in detail. In our post-session judgments, the assessors are real searchers. We asked them to judge the set of results they visited in the session. We instructed them to examine the results in a be er detail in the post-session judgments. e system also required participants to revisit each clicked result and spend at least 30 seconds to judge a result.",null,null
48,406,null,null
49,Session 4A: Evaluation 2,null,null
50,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
51,Table 1: estions for collecting search result judgments and users' search experience.,null,null
52,Search Result Judgments,null,null
53,Topical Relevance (TRel),null,null
54,Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),null,null
55,"estion & Options How relevant is this webpage? · Key (3): this page or site is dedicated to the topic; authoritative and comprehensive; it is worthy of being a top result. · Highly Relevant (2): the content of this page provides substantial information on the topic. · Relevant (1): the content of this page provides some information on the topic, which may be minimal. · Not Relevant or Spam (0). In Situ: How much useful information did you get from this web page? From 1 (none) to 7 (a lot of). Post-session: How much useful information did this web page provide for the task? From 1 (none) to 7 (a lot of). How much new information did you get from this web page? From 1 (none) to 7 (a lot of). How much e ort did you spend on this web page? From 1 (none) to 7 (a lot of). How di cult was it for you to follow the content of this web page? From 1 (very di cult) to 7 (very easy). How trustworthy is the information in this web page? From 1 (not at all trustworthy) to 7 (very trustworthy).",null,null
56,Search Experience Measures Satisfaction (Sat) Frustration (Frus) System Helpfulness (Help) Goal Success (Succ) Session E ort (S.Eff) Di culty (Diff),null,null
57,estion & Options How satis ed were you with your search experience? From 1 (very unsatis ed) to 7 (very satis ed). How frustrated were you with this task? From 1 (not frustrated) to 7 (very frustrated). How well did the system help you in this task? From 1 (very badly) to 7 (very well). How well did you ful ll the goal of this task? From 1 (very badly) to 7 (very well). How much e ort did this task take? From 1 (minimum) to 7 (a lot of). How di cult was this task? From 1 (very easy) to 7 (very di cult).,null,null
58,We collected users' in situ and post-session judgments of six different measures. Table 1 shows the detailed questions and options.,null,null
59,"· TREC relevance (TRel) ­ the de facto standard of relevance judgments due to the popularity of TREC test collections. We collected TRel using the criteria of the latest TREC web track [10]. As Table 1 shows, the criteria focus on topical relevance. We excluded the relevance level Nav (the correct homepage of a navigational query) from the original TREC criteria because our search tasks do not include navigational search.",null,null
60,"· Usefulness (Usef) ­ Following Belkin et al.'s model [5] and Mao et al.'s study [36], we collected users' perceptions regarding the usefulness of the clicked results.",null,null
61,"· Novelty (Nov) ­ Novelty was o en assessed algorithmically in previous studies based on sub-topic or ""nugget"" level relevance judgments [8, 40, 43, 55]. In contrast, we collect users' explicit novelty judgments.",null,null
62,· Understandability (Under) ­ the easiness of understanding the content of the result. Recent studies incorporated understandability into search result ranking [41] and evaluation [56].,null,null
63,"· Reliability (Relia) ­ the reliability, credibility, and trustworthy of the information presented in the result [39, 46, 51] (here we do not distinguish the three constructs).",null,null
64,· E ort ­ Yilmaz et al. [54] and Verma et al. [50] examined e ort as a dimension of evaluating search result.,null,null
65,"e following table summarizes the measures collected in in situ and post-session judgments. We only collected TRel in post-session judgments because the TREC criteria do not consider context. We only collected Nov and Effort during in situ judgments because the participants of a pilot study reported confusions assessing the two measures twice. In the rest of this paper, we will use .i and .p su xes to denote in situ and post-session judgments, respectively. For example, Usef.i denotes users' in situ usefulness judgments.",null,null
66,"Except for TRel, we collected judgments using a 7-point Likert scale, because a previous study [48] showed that assessors approximate the optimal level of con dence when using a 7-point scale for relevance judgments. TRel used a di erent scale so that it is",null,null
67,TREC relevance (TRel) Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),null,null
68,In Situ (.i),null,null
69,Post-session (.p),null,null
70,consistent with the TREC web track (as a representative example of the state-of-the-art relevance judgment methods).,null,null
71,2.3 Search Experience Measures,null,null
72,"In the judgment stage, participants rated their search experience in a session. We collected six representative user experience measures used in previous studies of information retrieval and recommender systems--satisfaction (Sat) [17, 21, 26, 35, 36, 45], goal success (Succ) [1, 18], frustration (Frus) [12, 13], task di culty (Diff) [4, 15, 29, 31, 32], the helpfulness of the system (Help) [19] and the total e ort spent (S.Eff) [27]. Table 1 includes the questions.",null,null
73,2.4 Rationale of Experiment Design,null,null
74,"e way we balance di erent types of tasks is similar to previous studies [22, 24, 30, 33, 36]. However, we acknowledge that the selected tasks cannot cover all varieties. It is also worth noting that the TREC session track tasks [7] are more complex than regular web search requests such as navigational search.",null,null
75,"Our study aims to collect both in situ judgments and user behaviors related to the clicked results. is poses challenges to the experiment design. On the one hand, we hope to collect accurate in situ judgments, which o en requires multi-item measurements [27, 52]. On the other hand, interrupting participants for in situ judgments breaks the ow of search session and can a ect their subsequent search behaviors. To balance between the two purposes, we made a few compromises in experiment design, e.g., we only collected six popular dimensions of judgments, and we simply used one question to measure each dimension.",null,null
76,407,null,null
77,Session 4A: Evaluation 2,null,null
78,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
79,Table 2: Spearman's correlation () matrix of di erent judgments for the 727 unique clicks.,null,null
80,In Situ Judgments,null,null
81,Post-session Judgments,null,null
82,Usef.i Novelty Effort Under.i Relia.i TRel Usef.p Under.p,null,null
83,Novelty,null,null
84,0.67,null,null
85,-,null,null
86,-,null,null
87,-,null,null
88,-,null,null
89,-,null,null
90,-,null,null
91,-,null,null
92,In Situ,null,null
93,E ort,null,null
94,0.22,null,null
95,Understandability,null,null
96,0.20,null,null
97,0.24,null,null
98,-,null,null
99,0.14 -0.45,null,null
100,-,null,null
101,-,null,null
102,-,null,null
103,-,null,null
104,-,null,null
105,-,null,null
106,-,null,null
107,Reliability,null,null
108,0.42,null,null
109,0.37,null,null
110,0.05,null,null
111,0.26,null,null
112,-,null,null
113,-,null,null
114,-,null,null
115,-,null,null
116,Topical Relevance,null,null
117,0.63,null,null
118,0.46,null,null
119,0.16,null,null
120,0.14,null,null
121,0.42,null,null
122,-,null,null
123,-,null,null
124,-,null,null
125,Post-session,null,null
126,Usefulness Understandability,null,null
127,0.72 0.20,null,null
128,0.52,null,null
129,0.16,null,null
130,0.18 -0.36,null,null
131,0.18 0.68,null,null
132,0.43 0.83 0.29 0.18,null,null
133,0.24,null,null
134,-,null,null
135,Reliability,null,null
136,0.43,null,null
137,0.38,null,null
138,0.04,null,null
139,0.22,null,null
140,0.82 0.48,null,null
141,0.51,null,null
142,0.31,null,null
143,e reported values are estimated from 1000 bootstrap samples (we used strati ed sampling to balance user and task dependency).,null,null
144,"While examining search behaviors, we excluded the time spent on answering in situ judgments from dwell time. On average the participants spent 57.1 seconds on a clicked result and 12.1 seconds to answer the ve in situ judgment questions.",null,null
145,2.5 Collected Data,null,null
146,We recruited 28 participants (16 are female) through iers posted on the campuses of two universities in the United States. We required participants to be English native speakers to exclude the in uence of language uency on relevance judgments [16]. All the participants were undergraduate or graduate students studying di erent elds.,null,null
147,"ey were reimbursed $15 per hour. We collected 112 sessions by 28 participants on 28 tasks. Each participant worked on four unique tasks and each task was performed by four unique users. In total, we collected 537 queries (4.8 per session) and 736 clicks (6.6 per session) on 727 unique sessionURL pairs (9 cases of revisiting). We exclude the 9 cases of revisiting from the analysis (about 1% of the data) to simply the analysis.",null,null
148,3 IN SITU VS. POST-SESSION JUDGMENTS,null,null
149,3.1 Correlation of Di erent Judgments,null,null
150,"Table 2 reports the correlation of di erent judgments, which are generally consistent with previous studies. For example, relevance and usefulness positively correlate with novelty and reliability [52], understandability negatively correlates with e ort [50], etc. We examined the relationship of the judgments in another article [23].",null,null
151,"Note that Mao et al. [36] reported a weak correlation (0.332) of searchers' post-session usefulness judgments and external assessors' relevance judgments. However, Table 2 shows that TRel and Usef.p are strongly correlated ( ,"" 0.83) when both of them are assessed by searchers. is suggests that the low correlation reported by Mao et al. [36] may be mostly due to the disparity between searchers and external assessors, rather than the di erence between using relevance or usefulness as the judgment criteria.""",null,null
152,3.2 Correlating with User Experience,null,null
153,"We evaluate di erent search result judgments by correlating with (regressing) users' search experience measures in a session. is is based on the assumption that the ""quality"" of the clicked results in a session can in uence users' search experience in that session--thus, a reasonable search result judgment (assumed to indicate certain ""quality""), or a reasonable set of judgments, should also correlate with users' search experience in a session.",null,null
154,3.2.1 Regression Analysis. We use multilevel regression analysis,null,null
155,to examine the relationship between the judgments of the clicked,null,null
156,results and users' search experience in a session. e dependent,null,null
157,variables (DVs) are each of the six search experience measures. e,null,null
158,independent variables (IVs) include the statistics of judgments re-,null,null
159,"garding the clicked results in a session (such as the mean, maximum,",null,null
160,"and minimum ratings). For TRel, Usef.i, and Usef.p, we include",null,null
161,"the mean, maximum, and minimum ratings of the clicked results in",null,null
162,a session as IVs in the regression analysis. For other search result,null,null
163,"judgments, we only include the maximum and minimum ratings of",null,null
164,the clicked results as IVs. is is because the mean ratings of the,null,null
165,"other measures o en highly correlate with those of TRel and Usef,",null,null
166,causing multicollinearity issues for regression analysis.,null,null
167,"For each user experience measure (the DV), we examine six",null,null
168,di erent models that include di erent judgments as IVs.,null,null
169,· Unidimensional & Context-independent ­ Model 1 and 2,null,null
170,only include context-independent search result judgments from,null,null
171,a single dimension--Model 1 includes the statistics of TRel and,null,null
172,Model 2 includes those of Usef.p.,null,null
173,· Unidimensional & In Situ ­ Model 3 includes in situ judg-,null,null
174,ments from a single dimension (the statistics of Usef.i) as IVs.,null,null
175,· Multidimensional & Context-independent ­ Model 4 and,null,null
176,5 extend Model 1 and 2 to include other dimensions of judg-,null,null
177,"ments (the statistics of Under.p, Relia.p, Nov, and Effort).",null,null
178,Note that Model 4 and 5 include two in situ judgments (Nov,null,null
179,and Effort) because we did not collect post-session judgments,null,null
180,on these two dimensions (as discussed in § 2.2).,null,null
181,· Multidimensional & In Situ ­ Model 6 extends Model 3 to,null,null
182,"include other dimensions of judgments (the statistics of Under.i,",null,null
183,"Relia.i, Nov, and Effort).",null,null
184,Contextindependent,null,null
185,In Situ,null,null
186,Unidimensional,null,null
187,1 TRel only 2 Usef.p only,null,null
188,3 Usef.i only,null,null
189,Multidimensional,null,null
190,4 TRel + others 5 Usef.p + others,null,null
191,6 Usef + others,null,null
192,"All six models also include the same set of control variables, including: gender (Male or Female), age (four levels; 0 for 18­24, 1 for 25­30, 2 for 31­40, and 3 for Over 40), highest degree obtained or expected (Undergraduate or Graduate), the expertise of using web search engines (SE Expertise) rated using a Likert scale from 1 (very badly) to 5 (very well), task product and goal, user's familiarity with the topic of the task (Topic Familiarity) rated using a Likert scale from 1 (very unfamiliar) to 7 (very familiar), and the number of clicks (# clicks) and queries (# queries) in the session.",null,null
193,408,null,null
194,Session 4A: Evaluation 2,null,null
195,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
196,Table 3: e adjusted R2 of di erent regression models.,null,null
197,Models,null,null
198,Sat Frus Succ S.Eff Help,null,null
199,Base (control only) 0.12 0.06 0.11 0.06 0.11,null,null
200,1 TRel,null,null
201,0.23 0.09 0.18 0.10 0.16,null,null
202,2 Usef.p,null,null
203,0.25 0.15 0.36 0.17 0.18,null,null
204,3 Usef.i,null,null
205,0.29 0.14 0.35 0.16 0.22,null,null
206,4 TRel + others 0.31 0.25 0.33 0.33 0.31,null,null
207,4 vs. 1,null,null
208,**,null,null
209,**,null,null
210,**,null,null
211,**,null,null
212,**,null,null
213,5 Usef.p + others 0.30 0.26 0.42 0.37 0.31,null,null
214,5 vs. 2,null,null
215,**,null,null
216,**,null,null
217,**,null,null
218,**,null,null
219,**,null,null
220,6 Usef.i + others 0.30 0.27 0.34 0.37 0.27,null,null
221,6 vs. 3,null,null
222,**,null,null
223,**,null,null
224,*,null,null
225,* and ** indicate p < 0.05 and p < 0.01 by F-test.,null,null
226,Diff,null,null
227,0.03 0.09 0.18 0.22 0.21,null,null
228,** 0.26,null,null
229,** 0.33,null,null
230,**,null,null
231,We examine multicollinearity between variables using variance,null,null
232,"in ation factor (VIF). e IVs of all models satisfy VIF < 4, the com-",null,null
233,monly suggested threshold (4­10) for concerns of multicollinearity issues [37]. Table 3 reports the adjusted R2 of the six models for,null,null
234,regressing the six dimensions of search experience.,null,null
235,"3.2.2 TREC Relevance vs. Usefulness. We rst compare TREC relevance criteria (TRel) and post-session usefulness judgments (Usef.p). is is a revisit of Mao et al.'s study [36], which compared searchers' usefulness judgments and external assessors' relevance judgments. Here we collected both judgments from real searchers, removing the in uence caused by the di erence between searchers and external annotators in relevance judgments. e regression analysis suggest that switching from TREC relevance to usefulness is fruitful, consistently enhancing the ability of the regression models to correlate with user experience (by adjusted R2).",null,null
236,"Models 1 and 2 include the mean, maximum, and minimum TRel or Usef.p ratings of the clicked results. Model 2 consistently explains the six search experience measures be er than Model 1 (by adjusted R2). We note that usefulness (Usef.p) seems to be particularly be er than TREC relevance (TRel) in terms of correlating with goal success (Succ), with adjusted R2 , 0.36 vs 0.18.",null,null
237,"Models 4 and 5 further include other dimensions of judgments as IVs. is helps compare TRel and Usef.p judgments with other search result judgments as controls. Still, we consistently observe that Model 5 explains the six search experience measures be er than or as well as model 4 . ese results verify that usefulness is indeed a be er criteria of relevance judgments than TREC-style relevance (in terms of correlating with users' search experience).",null,null
238,"3.2.3 In Situ vs. Context-independent (Post-session) Judgments. We further compare in situ and post-session judgments in both unidimensional and multidimensional se ings. Results suggest in situ usefulness judgments have be er correlations with a few (but not all) user experience measures than post-session usefulness judgments. However, a er combining search result judgments from di erent dimensions, in situ judgments show limited advantages over post-session ones.",null,null
239,"Models 3 and 2 include the mean, maximum, and minimum Usef.i or Usef.p ratings of the clicked results as IVs. Results show Model 3 explains satisfaction (Sat), helpfulness (Help), and task di culty (Di ) slightly be er than Model 2 , with about 0.04 di erence in adjusted R2.",null,null
240,"We further compare in situ and post-session judgments in a multidimensional se ing, using a combination of Usef.p/Usef.i",null,null
241,"and other four judgments as IVs (Models 5 and 6 ). Results show that the post-session multidimensional model ( 5 ) be er correlates with search success (Succ) than the in situ one (adjusted R2 0.42 vs 0.34), but the la er also be er correlates with task di culty (adjusted R2 0.26 vs. 0.21). Overall, no evidence suggests either model is consistently be er than another in terms of correlating with users' search experience measures.",null,null
242,"Even though Model 3 (Usef.i only) performs slightly be er than Model 2 (Usef.p only), results suggest limited advantages of in situ judgments over post-session ones in terms of correlating with search experience measures. We suspect a possible reason is that a 10-minute session is not long enough to trigger su cient di erences between in situ and post-session judgments. Although we expect to observe a greater di erence between in situ and post-session judgments in longer sessions, we believe a substantial proportion of web search sessions are no longer than 10 minutes, which may not bene t much from in situ judgments. In addition, it also requires a more complex experiment design to collect in situ judgments.",null,null
243,"3.2.4 Unidimensional vs. Multidimensional Judgments. We further compare models using a combination of multiple aspects of judgments (Models 4 , 5 , and 6 ) with those using a single dimension (Models 1 , 2 , and 3 ). Results suggest that it is almost always helpful (enhancing the correlation with most of the six search experience measures signi cantly) to complement either relevance or usefulness with the alternative dimensions.",null,null
244,"Models 4 and 5 explain all six dimensions of search experience measures signi cantly be er than Models 1 and 2 , suggesting that multidimensional judgments are almost always helpful for TREC-style relevance judgments (TRel) and post-session usefulness judgments (Usef.p). We also note that in situ usefulness judgments (Usef.i) worked particularly well for correlating with users' satisfaction (Sat) and goal success (Succ), such that combining with more dimensions of judgments adds li le to the model.",null,null
245,"Results demonstrate that multidimensional search result judgments are helpful, complementing unidimensional judgments and yielding be er correlation with search experience measures. is also suggests the advantages of multidimensional search result judgments over the in situ one--the former can consistently improve relevance/usefulness to be er correlate with almost all user experience measures, while the la er shows limited advantages.",null,null
246,3.3 Which Dimensions To Judge?,null,null
247,"A crucial issue of information retrieval is deciding which criteria to use to rank search results. We come to initial answers by looking into the standardized coe cients () of Model 5 (Table 4) as an example due to its superiority over other models. e standardized coe cient  stands for the magnitude of change in the DV (relative to its standard deviation) caused by one-unit change in the IV (relative to the IV's standard deviation) while other variables being equal. e coe cients of the model indicate how changes in the ""quality"" of the clicked results will (theoretically) a ect users' search experience in a session. Table 4 suggests that: · To enhance user satisfaction, a search system should present",null,null
248,useful and novel results--both Usef.p (mean) and Nov (max) show signi cant positive e ects on Sat in Model 5 .,null,null
249,409,null,null
250,Session 4A: Evaluation 2,null,null
251,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
252,Table 4: Multilevel regression: standardized coe cients () of independent variables for Model 5 ­ Usef.p + others.,null,null
253,Independent,null,null
254,DV: session-level search experience,null,null
255,Variables,null,null
256,Sat Frus Succ S.Eff Help Diff,null,null
257,Gender: Male Age Degree: Graduate SE Expertise,null,null
258,0.10 -0.05 -0.03,null,null
259,0.12,null,null
260,0.19 0.09 0.06 0.00 0.16 0.00 -0.03 -0.02 -0.11 0.00 0.05 0.01 0.13 -0.08 0.23 0.04 0.08 0.01 0.12 -0.00,null,null
261,Product: Factual Goal: Speci c Topic Familiarity,null,null
262,0.02 -0.02 -0.06 -0.09 -0.00 0.02 0.02 -0.07 0.04 0.05 0.07 0.01 0.10 -0.23 0.17 -0.20 0.19 -0.24,null,null
263,# clicks,null,null
264,0.20 -0.12 0.17 -0.07 0.18 -0.01,null,null
265,# queries,null,null
266,-0.36 0.17 -0.25 0.16 -0.35 -0.02,null,null
267, Usef.p (mean) 0.23 -0.38 0.36 -0.36 0.08 -0.43,null,null
268, Usef.p (max)  Usef.p (min),null,null
269,0.16 0.09 0.22 -0.07 0.11 -0.08 0.01 0.19 -0.04 0.19 0.01 0.18,null,null
270, Nov (max)  Nov (min),null,null
271,0.24 -0.10 0.18 -0.09 -0.01 -0.20 -0.08 -0.06,null,null
272,0.25 -0.11 0.07 -0.00,null,null
273, Under.p (max)  Under.p (min),null,null
274,0.09 -0.27 0.16 -0.08,null,null
275,0.30 -0.15 0.14 -0.26,null,null
276,0.14 -0.22 0.29 -0.27,null,null
277, Relia.p (max) -0.13 -0.08 0.01 0.05 -0.03 0.06,null,null
278, Relia.p (min) 0.06 0.01 -0.05 0.08 -0.07 0.04,null,null
279, Effort (max) -0.12 0.16 0.08 0.28 -0.13 0.02,null,null
280, Effort (min) Adjusted R2,null,null
281,0.21 0.04 0.12 0.01 0.25 0.02 0.30 0.26 0.42 0.37 0.31 0.26,null,null
282,"Light and dark shadings indicate p < 0.05 and 0.01, respectively.",null,null
283,"· To reduce user frustration, a search system should o er results that are useful and easy-to-understand--both Usef.p (mean) and Under.p (max) show signi cant negative e ects on Frus.",null,null
284,"· To help users successfully reach the goal (Succ), a search system should retrieve useful, novel, and easy-to-understand results-- Usef.p (mean), Nov (max), and Under.p (max) show signi cant positive e ects on Succ.",null,null
285,"· To reduce the total e ort of a search session, the system should retrieve easy-to-understand results and avoid those requiring too much e ort--Under.p (min) shows a signi cant negative e ect on S.Eff and Effort (max) shows a positive one.",null,null
286,"· To be er help users in a session (enhance the helpfulness of the system), a system should retrieve novel and easy-to-understand results--both Nov (max) and Under.p (max) show signi cant positive e ects on Help.",null,null
287,"· To reduce the perceived task di culty, we need to retrieve useful and easy-to-understand results--both Usef.p (mean) and Under.p (min) show signi cant negative e ects on Diff. e coe cients suggest that the mean usefulness of the clicked",null,null
288,"results is helpful for explaining all six search experience measures (has statistically signi cant coe cients). In addition, novelty, understandability, and e ort also signi cantly relate to many di erent search experience measures, suggesting they are useful complements to usefulness in search result judgments. In contrast, reliability shows no signi cant e ect on any of the six user experience measures in Model 5 . However, we suspect this is because the top-ranked results returned by Google are mostly reliable ones, which makes reliability a less important judgment measure among the clicked results.",null,null
289,Table 5: Statistics of the absolute di erence of two users' ratings on the same results (||).,null,null
290,Usef.i Effort Nov Relia.i Under.i TRel Usef.p Relia.p Under.p,null,null
291,| | mean (SD) 1.55 (1.45) 1.52 (1.25) 1.60 (1.47) 1.23 (1.21) 1.18 (1.23) 0.63 (0.68) 1.53 (1.54) 1.38 (1.35) 1.08 (1.31),null,null
292,"|| , 0 25.9% 22.9% 26.4% 31.3% 34.8% 48.3% 29.9% 30.8% 38.8%",null,null
293,||  1 58.2% 57.7% 54.2% 67.2% 68.2% 89.1% 60.7% 62.2% 76.6%,null,null
294,||  2 79.1% 76.1% 78.1% 86.1% 88.1% 100.0% 77.6% 81.1% 90.5%,null,null
295,3.4 Variability of Judgments,null,null
296,"We further examine the variability of judgments among di erent searchers, because in many practical scenarios we may have to train and evaluate retrieval systems based on relevance judgments made by external assessors. We suspect di erent users may have a greater degree of inconsistencies in their in situ judgments than their post-session ones (due to the contextual nature of the former). However, results do not support this conjecture well.",null,null
297,"We examine the absolute di erence of two users' ratings on the same result. Table 5 reports the mean absolute di erence and the distribution. e mean absolute di erence for in situ and postsession usefulness judgments (Usef.i and Usef.p) are very close (1.55 vs. 1.53). e mean absolute di erence of post-session reliability judgments (Relia.p) is slightly higher than that for in situ ones (Relia.i) (1.38 vs. 1.23), but that for post-session understandability judgments (Under.p) is also slightly lower than the in situ ones (Under.i, 1.08 vs. 1.18). Overall, no evidence suggests that either in situ or post-session judgments is more or less consistent than the other across di erent users.",null,null
298,"Further, we note that di erent users' reliability and understandability judgments seem more consistent than those for usefulness, e ort, and novelty judgments, regardless of performed in an in situ se ing or a post-session one. is suggests that usefulness, e ort, and novelty judgments may su er from inter-rate consistency by a greater extent, while inter-rate agreement is less likely a concern for understandability and reliability judgments. However, since users judged TRel by a di erent scale, it remains unclear how do the other ve judgments compare with standard TREC relevance judgments in terms of inter-rate consistency.",null,null
299,3.5 Summary,null,null
300,"To sum up, this section discloses both opportunities and challenges for future search result judgments. · Opportunity ­ Since a combination of multidimensional judg-",null,null
301,"ments explains user experience measures be er than using relevance or usefulness alone, we expect that an appropriate ranking of search results by multiple criteria may potentially yield be er user experience as well. e results in Table 4 also help select ranking criteria according to a targeted user experience measure. · Challenge ­ Extending current judgments from a single dimension to multiple aspects largely increases the cost of judgments.",null,null
302,is is a crucial issue for the scalability of multidimensional judgments. e following sections address this concern by predicting multidimensional judgments using implicit feedback techniques.,null,null
303,410,null,null
304,Session 4A: Evaluation 2,null,null
305,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
306,Table 6: Implicit feedback features and their correlation with di erent search result quality measures.,null,null
307,Pearson's r with search result judgments,null,null
308,Click Dwell Time Features,null,null
309,Note,null,null
310,TRel Usef.p Nov Effort Under.p Relia.p,null,null
311,T1 Click dwell time (log).,null,null
312,0.38,null,null
313,0.43 0.41,null,null
314,0.36,null,null
315,0.12,null,null
316,0.34,null,null
317,T2 T3 T4 T5,null,null
318,(t - µ )/ . t is the result's dwell time; µ is average click dwell time;  is the standard deviation of click dwell time. T3-5 are based on personalized versions of µ and  .,null,null
319,all clicks by user by task by length,null,null
320,0.31,null,null
321,0.34 0.30,null,null
322,0.32,null,null
323,0.31,null,null
324,0.36 0.38,null,null
325,0.32,null,null
326,0.31,null,null
327,0.35 0.30,null,null
328,0.32,null,null
329,0.29,null,null
330,0.33 0.29,null,null
331,0.31,null,null
332,0.06 0.09 0.06 0.06,null,null
333,0.24 0.24 0.24 0.24,null,null
334,Follow-up ery Features,null,null
335,Q1,null,null
336,Q2,null,null
337,e number of terms in the next query found in the URL/title/body,null,null
338,Q3 of the result.,null,null
339,URL title body,null,null
340,TRel,null,null
341,-0.04 -0.03 -0.03,null,null
342,Usef.p,null,null
343,0.03 -0.00 -0.03,null,null
344,Nov,null,null
345,-0.01 -0.00,null,null
346,0.10,null,null
347,Effort,null,null
348,-0.02 0.01 0.06,null,null
349,Under.p,null,null
350,-0.02 -0.00,null,null
351,0.03,null,null
352,Relia.p,null,null
353,0.03 -0.02 -0.02,null,null
354,Q4,null,null
355,e percentage of terms in the next query found in the,null,null
356,Q5 URL/title/body of the result.,null,null
357,Q6,null,null
358,URL title body,null,null
359,0.02 0.09 0.02 -0.04,null,null
360,0.01,null,null
361,0.08,null,null
362,0.07 0.10 0.06 -0.00,null,null
363,0.05,null,null
364,0.07,null,null
365,0.17,null,null
366,0.18 0.21,null,null
367,0.04,null,null
368,0.13,null,null
369,0.18,null,null
370,Q7,null,null
371,e number of newly added query terms in the next query refor-,null,null
372,Q8 mulation found in the URL/title/body of the result.,null,null
373,Q9,null,null
374,URL title body,null,null
375,0.03 -0.01 -0.03 -0.06 0.07 0.04 0.00 -0.07 0.07 0.07 0.13 -0.04,null,null
376,0.02,null,null
377,0.02,null,null
378,0.01 -0.00,null,null
379,0.04 -0.02,null,null
380,Q10,null,null
381,e number of removed query terms in the next query reformula-,null,null
382,Q11 tion found in the URL/title/body of the result.,null,null
383,Q12,null,null
384,URL title body,null,null
385,0.01 0.01 -0.00 -0.07 -0.04 -0.12,null,null
386,0.06 0.09 0.06 -0.09,null,null
387,0.03 -0.06,null,null
388,0.08,null,null
389,0.07 0.06,null,null
390,0.01,null,null
391,-0.09,null,null
392,-0.04,null,null
393,Q13,null,null
394,e mean/max/min log likelihood scores between the full content,null,null
395,Q14 of the result and follow-up queries.,null,null
396,Q15,null,null
397,mean max min,null,null
398,0.22 0.23 0.22 -0.03,null,null
399,0.19,null,null
400,0.23,null,null
401,0.22 0.23 0.21 -0.03,null,null
402,0.17,null,null
403,0.18,null,null
404,0.15 0.19 0.19 -0.01,null,null
405,0.17,null,null
406,0.19,null,null
407,Follow-up Click Features,null,null
408,TRel Usef.p Nov Effort Under.p Relia.p,null,null
409,C1,null,null
410,e mean/max/min similarity between the title of the result and,null,null
411,C2 the titles of clicked results in follow-up searches.,null,null
412,C3,null,null
413,mean max min,null,null
414,0.04,null,null
415,0.05 0.12,null,null
416,0.02,null,null
417,0.04,null,null
418,0.01,null,null
419,0.05,null,null
420,0.04 0.09,null,null
421,0.00,null,null
422,0.06,null,null
423,0.00,null,null
424,0.06,null,null
425,0.08 0.10,null,null
426,0.01,null,null
427,-0.01,null,null
428,0.03,null,null
429,C4,null,null
430,e mean/max/min similarity between the snippet of the result,null,null
431,C5 and the snippets of clicked results in follow-up searches.,null,null
432,C6,null,null
433,mean max min,null,null
434,-0.00 -0.04,null,null
435,0.08,null,null
436,0.01 -0.06,null,null
437,0.11,null,null
438,0.01 -0.06,null,null
439,0.10,null,null
440,0.04 -0.05,null,null
441,0.07,null,null
442,0.02,null,null
443,0.03,null,null
444,0.01 -0.04,null,null
445,0.06,null,null
446,0.09,null,null
447,C7,null,null
448,e mean/max/min similarity between the full content of the result,null,null
449,C8 and the full contents of SAT clicks (dwell time > 30s) in follow-up,null,null
450,C9 searches.,null,null
451,C10,null,null
452,e mean/max/min similarity between the title of the result and,null,null
453,C11 the titles of skipped results in follow-up searches.,null,null
454,C12,null,null
455,mean max min mean max min,null,null
456,0.19,null,null
457,0.23 0.09,null,null
458,0.01,null,null
459,-0.02,null,null
460,0.10,null,null
461,0.20 0.20 0.05 -0.00,null,null
462,0.00,null,null
463,0.08,null,null
464,0.12 0.17 0.07 -0.00 -0.01,null,null
465,0.07,null,null
466,0.09,null,null
467,0.11 0.11,null,null
468,0.02,null,null
469,0.05,null,null
470,0.05,null,null
471,0.11 0.09 0.06 -0.01,null,null
472,0.05,null,null
473,0.02,null,null
474,0.09 0.13 0.13 -0.05,null,null
475,0.00,null,null
476,0.05,null,null
477,C13,null,null
478,e mean/max/min similarity between the snippet of the result,null,null
479,C14 and the snippets of skipped results in follow-up searches.,null,null
480,C15,null,null
481,mean max min,null,null
482,0.01,null,null
483,0.03 0.03,null,null
484,0.11,null,null
485,-0.05,null,null
486,0.03,null,null
487,0.02 0.01 -0.01 -0.00,null,null
488,0.02 -0.02,null,null
489,0.10,null,null
490,0.12 0.12,null,null
491,0.13,null,null
492,-0.05,null,null
493,0.11,null,null
494,"Light and dark shadings indicate the correlation is signi cant at 0.05 and 0.01 levels, respectively.",null,null
495,4 PREDICTION,null,null
496,"is section introduces our techniques for predicting multidimensional judgments of clicked results from search logs. We model the prediction task as a regression problem--the input is features related to a target click, the output is the predicted judgment score of the clicked result. We use gradient boosted regression trees (GBRT) for prediction. Table 6 lists the prediction features. Due to the limited space, we only report results for predicting TRel and the judgments included in Model 5 --Usef.p, Nov, Effort, Under.p, and Relia.p. However, the described approach can also e ectively predict other search result judgments as well.",null,null
497,4.1 Click Dwell Time Features,null,null
498,"Click dwell time (T1) is one of the most widely used implicit feedback measure. As Table 6 shows, T1 does not correlate much with understandability, but it still has 0.3­0.4 correlations (signi cant at 0.01 level) with other measures.",null,null
499,T2­T5 measure the deviation of a click's dwell time from the mean dwell time (µ) of a group of clicks (normalized by the standard deviation  ). T2 computes µ and  based on all clicks in the training sets. T3 is based on clicks by the same user. T4 is based on clicks in sessions with the same task type. T5 is based on clicks on documents with similar length (we divide the clicked results into ten bins by length and compute µ and  of a click based on its bin).,null,null
500,4.2 Follow-up ery Features,null,null
501,Follow-up query features are based on the intuition that a clicked result may in uence follow-up query reformulation in a session.,null,null
502,"us, we can infer the quality of a click from queries issued a er the clicked result in the same session.",null,null
503,Q1­Q6 match the terms in the immediate follow-up query with the target click. Q7­Q12 match the newly added and removed terms in the immediate follow-up query reformulation with the target click. Q13­Q15 match the target click with all follow-up queries.,null,null
504,411,null,null
505,Session 4A: Evaluation 2,null,null
506,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
507,"Many of the follow-up query features (such as Q6 and Q13­Q15) have signi cant correlations with the search result quality measures, con rming that the intuition is reasonable. We also note that Q6 and Q13­Q15 have stronger correlations with understandability than click dwell time features.",null,null
508,4.3 Follow-up Click Features,null,null
509,"Similar to follow-up query features, we may also infer the quality of a target click based on follow-up clicks in a session.",null,null
510,"C1­C6 measure the similarity between the target click and followup clicks. C7­C9 measure the similarity with follow-up satisfactory (SAT) clicks. C10­C15 measure the similarity with follow-up skipped results (unclicked results ranked higher than a clicked result). Some features have signi cant correlations with the search result quality measures, suggesting they may be useful predictors.",null,null
511,4.4 Prior-to-click Features (Baseline),null,null
512,"Prior-to-click features include the existing techniques that predict search result quality measures using information available before users clicking on the result. In this paper, they serve as the baseline for the implicit feedback features. We include a full list of prior-toclick features in an online appendix1.",null,null
513,"We incorporate di erent prior-to-click features for predicting di erent measures. e shared features for all six measures include the rank of the result by Google search, ad hoc search models (QL, BM25, DFR [3], and SDM [38]), and session search models [14, 47].",null,null
514,e unique features for predicting each measure are: · TRel ­ a subset of LETOR features [34]. · Usef.p ­ a subset of LETOR features [34] and a subset of the,null,null
515,"usefulness features by Mao et al. [36] that do not rely on postclick information. · Nov ­ the similarity of the click with previous clicks and higher ranked results in the same SERP (motivated by previous work on novelty-based search result diversi cation [6, 43, 44, 55]). · Effort ­ Yilmaz et al. [54] and Verma et al. [50]. · Under.p ­ Palo i et al. [41, 42]. · Relia.p ­ Olteanu et al. [39] and Wawer et al. [51]. Our prior-to-click features are representatives of the state-of-theart techniques for predicting each dimension of judgments without using implicit feedback. However, we did not include features that we do not have the resource to calculate, which include link structure based features and social media popularity features such as Twi er mention. Note this may reduce the e ectiveness of predicting reliability since the excluded features take about 1/3 of the features by Olteanu et al. [39] and Wawer et al. [51].",null,null
516,5 EVALUATION,null,null
517,5.1 Experiment Settings,null,null
518,"We evaluate prediction (regression) by the Pearson's correlation between the predicted values and actual judgments (prediction correlation) and the root mean square error (RMSE) of the predicted values. Note that the RMSE for predicting di erent measures is not comparable-- rst, TREC relevance ranges from 0­3 while others from 1­7; second, their distributions vary a lot. Here we only report",null,null
519,1 h p://ciir.cs.umass.edu/downloads/mdrel/,null,null
520,prediction correlation for its easy interpretability. e results of RMSE is highly consistent with that using prediction correlation.,null,null
521,"e dataset for evaluation includes multidimensional judgments on the 727 unique clicked results. We use 10-fold cross validation for evaluation (using eight folds for training, one for validation, and one for testing). We randomly shu e the dataset 10 times and apply 10-fold cross-validation for each random shu ing of the whole dataset--this generates prediction results on 10 × 10 ,"" 100 test folds in total (note that we are not using a 100-fold cross validation). We report the mean and standard deviation (SD) of prediction correlation on the test folds. We note that the prediction correlation reported in this section is di erent from and cannot be compared with the correlation in Table 6, which are computed for the whole dataset without cross validation.""",null,null
522,5.2 Click Dwell Time Features,null,null
523,"Current techniques for inferring search result quality from logs rely on click dwell time. Results ( 1 in Table 7) suggest the click dwell time features work reasonably well for predicting usefulness, novelty, and e ort, but they have di culties inferring the understandability and reliability of results.",null,null
524,"e click dwell time features ( 1 ) are e ective predictors for usefulness, novelty, and e ort. For these three measures, the predicted values have about 0.3­0.4 mean Pearson's correlation with the actual judgments, which is comparable to that for predicting TREC relevance (mean r ,"" 0.35). However, the click dwell time features perform much worse for predicting understandability and reliability. On average the predicted and actual judgments have only 0.10 and 0.22 correlation, suggesting it is necessary to incorporate new implicit feedback signals.""",null,null
525,5.3 Follow-up ery and Click Features,null,null
526,We extend click dwell time to include signals from follow-up search activities. Results suggest the new features are helpful.,null,null
527,"e follow-up query ( 2 ) and click features ( 3 ) alone have limited prediction capability. However, combining them with the click dwell time features ( 4 ) consistently produces be er prediction than using click dwell time features alone ( 1 ): except for e ort, the prediction correlation for the other ve measures using feature set 4 is signi cantly be er than that for click dwell time features ( 1 ). is indicates that follow-up queries and clicks indeed provide useful implicit feedback that are complementary to click dwell time.",null,null
528,"e follow-up query and click features are particularly helpful for predicting reliability. Combining them with the click dwell time features improves the mean correlation of prediction from 0.22 to 0.36. e new features are also helpful for predicting TREC relevance and usefulness as well. is partly con rms our intuition--the quality of a clicked result may in uence follow-up search activities, making it possible to infer the quality of a clicked result based on what happened a erward in the session.",null,null
529,"e new features also improved the mean prediction correlation for understandability from 0.10 to 0.20. However, we note the combination of all implicit feedback features still does not work well for predicting understandability (mean r ,"" 0.20). is suggests that, compared with other judgments, it is more challenging to predict understandability based on the implicit feedback information.""",null,null
530,412,null,null
531,Session 4A: Evaluation 2,null,null
532,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
533,Table 7: e e ectiveness of di erent features for predicting multidimensional search result judgments.,null,null
534,Mean (SD) Pearson's r between true and predicted judgments over the test folds,null,null
535,Features,null,null
536,TRel,null,null
537,Usef.p,null,null
538,Nov,null,null
539,Effort,null,null
540,Under.p,null,null
541,Relia.p,null,null
542,1 Click Dwell Time,null,null
543,0.35 (0.11),null,null
544,0.40 (0.11),null,null
545,0.42 (0.11),null,null
546,0.31 (0.10),null,null
547,0.10 (0.14),null,null
548,0.22 (0.13),null,null
549,2 Follow-up ery,null,null
550,0.19 (0.11),null,null
551,0.17 (0.14),null,null
552,0.13 (0.13),null,null
553,0.12 (0.11),null,null
554,0.14 (0.12),null,null
555,0.19 (0.12),null,null
556,3 Follow-up Click,null,null
557,0.15 (0.12),null,null
558,0.20 (0.11),null,null
559,0.11 (0.12),null,null
560,0.14 (0.11),null,null
561,0.11 (0.12),null,null
562,0.17 (0.12),null,null
563,4 All ( 1 + 2 + 3 ),null,null
564,0.39 (0.09),null,null
565,0.46 (0.08),null,null
566,0.45 (0.09),null,null
567,0.33 (0.11),null,null
568,0.20 (0.13),null,null
569,0.36 (0.12),null,null
570,1 vs. 4,null,null
571,**,null,null
572,**,null,null
573,*,null,null
574,**,null,null
575,**,null,null
576,5 Prior-to-click,null,null
577,0.36 (0.10),null,null
578,0.29 (0.10),null,null
579,0.28 (0.11),null,null
580,0.13 (0.12),null,null
581,0.20 (0.14),null,null
582,0.18 (0.13),null,null
583,4 vs. 5,null,null
584,**,null,null
585,**,null,null
586,**,null,null
587,**,null,null
588,**,null,null
589,6 All+Prior-to-click,null,null
590,0.45 (0.08),null,null
591,0.49 (0.09),null,null
592,0.47 (0.09),null,null
593,0.39 (0.09),null,null
594,0.26 (0.12),null,null
595,0.40 (0.11),null,null
596,5 vs. 6,null,null
597,**,null,null
598,**,null,null
599,**,null,null
600,**,null,null
601,**,null,null
602,**,null,null
603,* and ** indicate the di erence is statistically signi cant at 0.05 and 0.01 levels by two-tail paired t -test.,null,null
604,5.4 Comparing to Prior-to-click Features,null,null
605,An important application of implicit feedback techniques is to infer relevance labels from search logs. Aggregating inferred relevance labels or implicit feedback signals from past search logs may help rank search results in the future [2]. We examine whether or not implicit feedback techniques can serve a similar purpose for multidimensional judgments.,null,null
606,"e combination of the implicit feedback features and the priorto-click features ( 6 ) generated signi cantly be er prediction results on all the six judgments than using the prior-to-click features alone ( 5 ). is suggests that the implicit feedback features are indeed helpful and complementary to the prior-to-click features for predicting these judgments. We also note that the improvements in mean prediction correlation can be as large as over 0.2 (such as for predicting reliability and e ort). However, even combining the two sets of features still cannot adequately predict understandability (mean r , 0.26).",null,null
607,6 DISCUSSION AND CONCLUSION,null,null
608,"A crucial issue of information retrieval is deciding which criteria to use to rank search results. We compared two seemingly reasonable directions for improving current TREC-style relevance judgments. One direction is to collect in situ search result judgments. e other one is to complement a single dimension of judgments (such as relevance or usefulness) by combining with other aspects. We found that the la er direction seems more e ective and versatile-- using a combination of di erent dimensions of judgments, we can almost always improve correlation with user experience measures.",null,null
609,"We envision future search engines should rank results by multiple aspects. We also o ered initial suggestions on which criteria to adopt and when to adopt them. We further examined and improved implicit feedback techniques for predicting multiple judgments, addressing the scalability concern of applying multidimensional judgments in real web search applications.",null,null
610,Our study makes the following contributions: · We evaluated and compared in situ usefulness judgments with,null,null
611,"regular relevance/usefulness judgments by searchers. We show that using usefulness as the judgment criteria is fruitful, but in situ judgments do not show clear bene ts over regular ones. · We evaluate multidimensional search result judgments considering four alternative aspects other than relevance/usefulness. We show that multidimensional judgments be er correlate with user",null,null
612,"experience measures than using relevance/usefulness judgments alone. We also note that multidimensional judgments is a be er direction for improving TREC-style relevance judgments. · Our study also discloses the connections between di erent user experience measures and various dimensions of search result judgments. is o ers practical suggestions for system design, such as the appropriate dimensions to judge search results for the purpose of improving a particular user experience measure. · We successfully generalize implicit feedback signals to include follow-up searches and clicks in a search session to help click dwell time be er predict multidimensional judgments. To the best of our knowledge, we are also the rst to examine the e ectiveness of implicit feedback approaches for predicting novelty, understandability, reliability, and e ort.",null,null
613,Our work also sheds lights on a few critical areas for exploration in the future:,null,null
614,"An important line of future work is to provide more accurate criteria for search result ranking and evaluation. Based on a regression analysis, we have already o ered initial suggestions on what criteria to use and when to use them, as discussed in Section 3.3. We note that, with a su ciently large dataset, one can possibly learn a prediction model for search experience measures by taking multidimensional judgments of results as input. Such a model can further address issues such as what are the proper weights to put on di erent aspects when ranking search results. It may also solve the discrepancy between o ine evaluation measures and user experience measures, and ultimately serve as a be er objective function for training ranking models.",null,null
615,"Another important application is to perform multidimensional ranking of search results based on implicit feedback signals and other information. We have already demonstrated that implicit feedback approaches can infer judgments of usefulness, novelty, e ort, and reliability with reasonable accuracy comparing to those for relevance labels. Aggregating such inferred judgments from past search logs may serve as useful features for performing multidimensional search result ranking in the future. However, we also note that our current technique needs to be improved to be er infer understandability of results from search logs.",null,null
616,"We do admit certain limitations in our current study. First, our analysis and experiments are solely based on data collected from one laboratory user study, which is limited in both scale and representativeness. We suggest that further studies employ larger",null,null
617,413,null,null
618,Session 4A: Evaluation 2,null,null
619,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
620,"datasets to verify our ndings. Second, it is worth noting that our way of collecting in situ judgments in uenced users' natural search behaviors. We observed in our log that users spent on average 12.1 seconds to nish the in situ judgments. us some particular user behavior pa erns may vary when applied to another scenario (without interrupting users for in situ judgments). ird, we also note that we only collected search result judgments for the clicked results, while it remains unclear to which extent the ndings can be generalized to the unclicked ones. Last but not least, the collected post-session judgments are more or less in uenced by the search session and the in situ judgments (although we meant to collect context-independent judgments such as to compare with contextual ones). It is also worth noting that our post-session judgments are not fully representative of the existing TREC-style approach.",null,null
621,7 ACKNOWLEDGMENT,null,null
622,"is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.",null,null
623,REFERENCES,null,null
624,"[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: A game for modeling di erent types of web search success using interaction data. In SIGIR '11, pages 345­354, 2011.",null,null
625,"[2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR '06, pages 19­26, 2006.",null,null
626,"[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.",null,null
627,"[4] J. Arguello. Predicting search task di culty. In ECIR '14, pages 88­99, 2014. [5] N. J. Belkin, M. J. Cole, and J. Liu. A model for evaluation of interactive infor-",null,null
628,"mation retrieval. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [6] J. Carbonell and J. Goldstein. e use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR '98, pages 335­336, 1998. [7] B. Cartere e, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: e TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016. [8] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ cher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, pages 659­666, 2008. [9] C. W. Cleverdon. e evaluation of systems used in information retrieval. In Proceedings of the International Conference on Scienti c Information, pages 687­ 698, 1959. [10] K. Collins- ompson, C. Macdonald, P. Benne , F. Diaz, and E. Voorhees. TREC 2014 web track overview. In TREC 2014, 2014. [11] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR '11, pages 95­104, 2011. [12] H. A. Feild and J. Allan. Modeling searcher frustration. In HCIR '09, pages 5­8, 2009. [13] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR '10, pages 34­41, 2010. [14] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13, pages 453­462, 2013. [15] J. Gwizdka. Revisiting search task di culty: Behavioral and individual di erence measures. In ASIS&T '08, 2008. [16] P. Hansen and J. Karlgren. E ects of foreign language and task scenario on relevance assessment. J. Doc., 61(5):623­639, 2005. [17] A. Hassan. A semi-supervised approach to modeling web search satisfaction. In SIGIR '12, pages 275­284, 2012. [18] A. Hassan, R. Jones, and K. L. Klinkner. Beyond DCG: User behavior as a predictor of a successful search. In WSDM '10, pages 221­230, 2010. [19] R. Hu and P. Pu. A study on user perception of personality-based recommender systems. In UMAP '10, pages 291­302, 2010. [20] J. Jiang and J. Allan. Adaptive e ort for search evaluation metrics. In ECIR '16, pages 187­199, 2016.",null,null
629,"[21] J. Jiang, A. Hassan Awadallah, X. Shi, and R. W. White. Understanding and predicting graded search satisfaction. In WSDM '15, pages 57­66, 2015.",null,null
630,"[22] J. Jiang, D. He, and J. Allan. Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. In SIGIR '14, pages 607­616, 2014.",null,null
631,"[23] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017.",null,null
632,"[24] D. Kelly, J. Arguello, A. Edwards, and W.-c. Wu. Development and evaluation of search tasks for IIR experiments using a cognitive complexity framework. In ICTIR '15, pages 101­110, 2015.",null,null
633,"[25] J. Y. Kim, J. Teevan, and N. Craswell. Explicit in situ user feedback for web search results. In SIGIR '16, pages 829­832, 2016.",null,null
634,"[26] J. Kiseleva, E. Crestan, R. Brigo, and R. Di el. Modelling and detecting changes in user satisfaction. In CIKM '14, pages 1449­1458, 2014.",null,null
635,"[27] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, H. Soncu, and C. Newell. Explaining the user experience of recommender systems. User Modeling and User-Adapted Interaction, 22(4-5):441­504, 2012.",null,null
636,"[28] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, 2008.",null,null
637,"[29] C. Liu, J. Liu, and N. J. Belkin. Predicting search task di culty at di erent search stages. In CIKM '14, pages 569­578, 2014.",null,null
638,"[30] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task di culty for di erent task types. In ASIS&T '10, 2010.",null,null
639,"[31] J. Liu, C. Liu, M. Cole, N. J. Belkin, and X. Zhang. Exploring and predicting search task di culty. In CIKM '12, pages 1313­1322, 2012.",null,null
640,"[32] J. Liu, C. Liu, J. Gwizdka, and N. J. Belkin. Can search systems detect users' task di culty?: Some behavioral signals. In SIGIR '10, pages 845­846, 2010.",null,null
641,"[33] J. Liu, C. Liu, X. Yuan, and N. J. Belkin. Understanding searchers' perception of task di culty: Relationships with task type. In ASIS&T '11, 2011.",null,null
642,"[34] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 workshop on learning to rank for information retrieval, pages 3­10, 2007.",null,null
643,"[35] Y. Liu, Y. Chen, J. Tang, J. Sun, M. Zhang, S. Ma, and X. Zhu. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In SIGIR '15, pages 493­502, 2015.",null,null
644,"[36] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016.",null,null
645,"[37] S. Menard. Applied Logistic Regression Analysis. Sage, 1997. [38] D. Metzler and W. B. Cro . A markov random eld model for term dependencies.",null,null
646,"In SIGIR '05, pages 472­479, 2005. [39] A. Olteanu, S. Peshterliev, X. Liu, and K. Aberer. Web credibility: Features",null,null
647,"exploration and credibility prediction. In ECIR '13, pages 557­568, 2013. [40] P. Over. e TREC interactive track: An annotated bibliography. Inf. Process.",null,null
648,"Manage., 37(3):369­381, 2001. [41] J. Palo i, L. Goeuriot, G. Zuccon, and A. Hanbury. Ranking health web pages",null,null
649,"with relevance and understandability. In SIGIR '16, pages 965­968, 2016. [42] J. Palo i, G. Zuccon, and A. Hanbury. e in uence of pre-processing on the",null,null
650,"estimation of readability of web documents. In CIKM '15, pages 1763­1766, 2015. [43] D. Ra ei, K. Bharat, and A. Shukla. Diversifying web search results. In WWW",null,null
651,"'10, pages 781­790, 2010. [44] R. L. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result",null,null
652,"diversi cation. Inf. Retr., 15(5):478­502, 2012. [45] A. Schuth, K. Hofmann, and F. Radlinski. Predicting search satisfaction metrics",null,null
653,"with interleaved comparisons. In SIGIR '15, pages 463­472, 2015. [46] J. Schwarz and M. Morris. Augmenting web pages and search results to support",null,null
654,"credibility assessment. In CHI '11, pages 1245­1254, 2011. [47] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using",null,null
655,"implicit feedback. In SIGIR '05, pages 43­50, 2005. [48] R. Tang, W. M. Shaw, Jr., and J. L. Vevea. Towards the identi cation of the optimal",null,null
656,"number of relevance categories. J. Am. Soc. Inf. Sci., 50(3):254­264, 1999. [49] J. van Doorn, D. Odijk, D. M. Roijers, and M. de Rijke. Balancing relevance",null,null
657,"criteria through multi-objective optimization. In SIGIR '16, pages 769­772, 2016. [50] M. Verma, E. Yilmaz, and N. Craswell. On obtaining e ort based judgements for",null,null
658,"information retrieval. In WSDM '16, pages 277­286, 2016. [51] A. Wawer, R. Nielek, and A. Wierzbicki. Predicting webpage credibility using",null,null
659,"linguistic features. In WWW '14 Companion, pages 1135­1140, 2014. [52] Y. Xu and Z. Chen. Relevance judgment: What do information users consider",null,null
660,"beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57(7):961­973, 2006. [53] Y. Yamamoto and K. Tanaka. Enhancing credibility judgment of web search",null,null
661,"results. In CHI '11, pages 1235­1244, 2011. [54] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and",null,null
662,"e ort: An analysis of document utility. In CIKM '14, pages 91­100, 2014. [55] C. X. Zhai, W. W. Cohen, and J. La erty. Beyond independent relevance: Methods",null,null
663,"and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10­17, 2003. [56] G. Zuccon. Understandability biased evaluation for information retrieval. In",null,null
664,"ECIR '16, pages 280­292, 2016.",null,null
665,414,null,null
666,,null,null

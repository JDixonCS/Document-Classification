,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge,null,null
3,Arman Cohan,null,null
4,"Information Retrieval Lab, Dept. of Computer Science Georgetown University",null,null
5,arman@ir.cs.georgetown.edu,null,null
6,ABSTRACT,null,null
7,"Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles.",null,null
8,KEYWORDS,null,null
9,"Text Summarization, Scienti c Text, Information Retrieval",null,null
10,1 INTRODUCTION,null,null
11,"In scienti c literature, related work is often referenced along with a short textual description regarding that work which we call citation text. Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16]).",null,null
12,"While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18]. For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17]. This problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives.",null,null
13,"We present an approach for addressing such concerns by adding the appropriate context from the reference article to the citation texts. Enriching the citation texts with relevant context from the reference paper helps the reader to better understand the context for the ideas, methods or ndings stated in the citation text.",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080740",null,null
15,Nazli Goharian,null,null
16,"Information Retrieval Lab, Dept. of Computer Science Georgetown University",null,null
17,nazli@ir.cs.georgetown.edu,null,null
18,"A challenge in citation contextualization is the discourse and terminology variations between the citing and the referenced authors. Hence, traditional IR models that rely on term matching for",null,null
19,nding the relevant information are ine ective. We propose to address this challenge by a model that utilizes,null,null
20,"word embeddings and domain speci c knowledge. Speci cally, our approach is a retrieval model for nding the appropriate context of citations, aimed at capturing terminology variations and paraphrasing between the citation text and its relevant reference context.",null,null
21,"We perform two sets of experiments to evaluate the performance of our system. First, we evaluate the relevance of extracted contexts intrinsically. Then we evaluate the e ect of citation contextualization on the application of scienti c summarization. Experimental results on TAC 2014 benchmark show that our approach signi cantly outperforms several strong baselines in extracting the relevant contexts. We furthermore, demonstrate that our contextualization models can enhance summarizing scienti c articles.",null,null
22,2 CONTEXTUALIZING CITATIONS,null,null
23,"Given a citation text, our goal is to extract the most relevant context",null,null
24,to it in the reference article. These contexts are essentially certain,null,null
25,"textual spans within the reference article. Throughout, colloquially,",null,null
26,we refer to the citation text as query and reference spans in the,null,null
27,reference article as documents. Our approach extends Language,null,null
28,Models for IR (LM) by incorporating word embeddings and domain,null,null
29,ontology to address shortcomings of LM for this research purpose.,null,null
30,"The goal in LM is to rank a document d according to the conditional probability p(d |q)  p(q|d ) , qi q p(qi |d ) where qi shows the tokens in the query q. Estimating p(qi |d ) is often achieved by max-",null,null
31,imum likelihood estimate from term frequencies with some sort of,null,null
32,"smoothing. Using Dirichlet smoothing [21], we have:",null,null
33,p(qi |d ),null,null
34,",",null,null
35,"f (qi , d ) + µ p(qi |C) w V f (w, d ) + µ",null,null
36,(1),null,null
37,"where f (qi , d ) shows the frequency of term qi in document d, C",null,null
38,"is the entire collection, V is the vocabulary and µ the Dirichlet",null,null
39,"smoothing parameter. In the citation contextualization problem, (i)",null,null
40,the target reference sentences are short documents and (ii) there,null,null
41,exist terminology variations between the citing author and the,null,null
42,"referenced author. Hence, the citation terms usually do not appear",null,null
43,"in the documents and relying only on the frequencies of citation terms in the documents (f (qi , d )) for estimating p(qi |d ) yields an almost uniform smoothed distribution that is unable to decisively",null,null
44,distinguish between the documents.,null,null
45,"Embeddings. Distributed representation (embedding) of a word w in a eld F is a mapping w  Fn where words semantically similar to w will be ideally located close to it. Given a query q, we rank the documents d according to the following scoring function",null,null
46,1133,null,null
47,Short Research Paper,null,null
48,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
49,normalized dot product,null,null
50,normalized dot product,null,null
51,1.0,null,null
52,normalized logit,null,null
53,1.0,null,null
54,normalized logit,null,null
55,0.8,null,null
56,0.8,null,null
57,0.6,null,null
58,0.6,null,null
59,0.4,null,null
60,0.4,null,null
61,0.2,null,null
62,0.0 0,null,null
63,500,null,null
64,1000,null,null
65,1500,null,null
66,2000,null,null
67,0.2,null,null
68,0.0 0,null,null
69,200 400 600 800 1000,null,null
70,Figure 1: Dot product of embeddings and its logit for a sample word and its top most similar words (top 2000 and 1000).,null,null
71,which leverages this property:,null,null
72,p(qi |d ),null,null
73,",",null,null
74,"fsem (qi , d ) + µ p(qi |C ) w V fsem (w, d ) + µ",null,null
75,(2),null,null
76,where fsem is a function that measures semantic relatedness of the,null,null
77,"query term qi to the document d and is de ned as: fsem (qi , d ) ,",null,null
78,"dj d s (qi , dj ); where dj 's are document terms and s (qi , dj ) is the",null,null
79,relatedness between the the query term and document term which,null,null
80,is calculated by applying a similarity function to the distributed,null,null
81,representations of qi and dj . We use a transformation () of dot products between the unit vectors e (qi ) and e (dj ) corresponding to the embeddings of the terms qi and dj for the similarity function:,null,null
82,"s (qi , dj ) ,  (e (qi ).e (dj )); if e (qi ).e (dj ) > ",null,null
83,0;,null,null
84,otherwise,null,null
85,We rst explain the role of  and then the reason for considering,null,null
86,the function  instead of raw dot product.  is a parameter that,null,null
87,controls the noise introduced by less similar words. Many unrelated,null,null
88,word vectors have non-zero similarity scores and adding them up introduces noise to the model and reduces the performance.  's function is to set the similarity between unrelated words to zero,null,null
89,"instead of a positive number. To identify an appropriate value for  , we select a random set of words from the embedding model and calculate the average and standard deviation of pointwise absolute",null,null
90,value of similarities between terms from these two samples. We then select the threshold  to be two standard deviations larger than the average to only consider very high similarity values (this,null,null
91,choice was empirically justi ed).,null,null
92,Examining term similarity values between words shows that,null,null
93,there are many terms with high similarities associated with each,null,null
94,"term and these values are not highly discriminative. We apply a transfer function  to the dot product e (qi ).e (dj ) to dampen the e ect of less similar words. In other words, we only want highly",null,null
95,related words to have high similarity values and similarity should,null,null
96,quickly drop as we move to less related words. We use the logit,null,null
97,function for  to achieve this dampening e ect:,null,null
98, (x,null,null
99,),null,null
100,",",null,null
101,log(,null,null
102,1,null,null
103,x -,null,null
104,x,null,null
105,),null,null
106,Figure 1 shows this e ect. The purple line is the normalized dot,null,null
107,product of a sample word with the most similar words in the model.,null,null
108,"As illustrated, the similarity score di erences among top words",null,null
109,"is not very discriminative. However, applying the logit function",null,null
110,(green line) causes the less similar words to have lower similarity,null,null
111,values to the target word. Domain knowledge. Successful word embedding methods have,null,null
112,previously shown to be e ective in capturing syntactic and semantic,null,null
113,relatedness between terms. These co-occurrence based models are,null,null
114,"data driven. On the other hand, domain ontologies and lexicons",null,null
115,that are built by experts include some information that might not,null,null
116,"be captured by embedding methods [8]. Therefore, using domain",null,null
117,knowledge can further help the embedding based retrieval model;,null,null
118,we incorporate it in our model in the following ways: 1) Retro tting: Faruqui et al. [6] proposed a model that uses the,null,null
119,constraints on WordNet lexicon to modify the word vectors and,null,null
120,pull synonymous words closer to each other. To inject the domain,null,null
121,"knowledge in the embeddings, we apply this model on two domain speci c ontologies, namely, M and Protein Ontologies (PO)1.",null,null
122,We chose these two biomedical domain ontologies because they,null,null
123,are in the same domain as the articles in the TAC dataset. M,null,null
124,is a broad ontology that consists of biomedical terms and PO is a,null,null
125,more focused ontology related to biology of proteins and genes. 2) Interpolating in the LM: We also directly incorporate the do-,null,null
126,main knowledge in the retrieval model; we modify the LM into the,null,null
127,following interpolated LM with parameter :,null,null
128,"p(qi |d ) , p1 (qi |d ) + (1 - )p2 (qi |d ) where p1 is estimated using Eq. 2 and p2 is similar to p1 except that",null,null
129,we replace fsem with the function font which considers domain,null,null
130,ontology in calculating similarities:,null,null
131,"font (qi , d ),",null,null
132,"1,",null,null
133,"s2 (qi , dj ); s2 (qi , dj ),"" ,""",null,null
134,"if qi ,dj if qi dj",null,null
135,dj d,null,null
136,"0, o.w. ",null,null
137,"where   [0, 1] is a parameter and qi  dj shows that there is",null,null
138,an is-synonym relation in ontology between qi and dj 2.,null,null
139,3 EXPERIMENTS,null,null
140,Data. We use the TAC 2014 Biomedical Summarization benchmark3.,null,null
141,This dataset contains 220 scienti c biomedical journal articles and,null,null
142,313 total citation texts where the relevant contexts for each citation,null,null
143,"text are annotated by 4 experts. Baselines. To our knowledge, the only published results on TAC",null,null
144,"2014 is [4], where the authors utilized query reformulation (QR)",null,null
145,"based on UMLS ontology. In addition to [4], we also implement sev-",null,null
146,eral other strong baselines to better evaluate the e ectiveness of our,null,null
147,model: 1) BM25; 2) VSM: Vector Space Model that was used in [4]; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling,null,null
148,with LDA smoothing which is a recent extension of the LMD to,null,null
149,also account for the latent topics [10]. All the baseline parameters,null,null
150,"are tuned for the best performance, and the same preprocessing is",null,null
151,applied to all the baselines and our methods. Our methods. We rst report results based on training the em-,null,null
152,"beddings on Wikipedia (WEWiki). Since TAC dataset is in biomedical domain, many of the biomedical terms might be either out-",null,null
153,of-vocabulary or not captured in the correct context using gen-,null,null
154,"eral embeddings, therefore we also train biomedical embeddings (WEBio)4. In addition, we report results for biomedical embeddings with retro tting (WEBio+rtrft), as well as interpolating domain knowledge (WEBio+dmn)",null,null
155,3.1 Intrinsic Evaluation,null,null
156,"First, we analyze the e ectiveness of our proposed approach for contextualization intrinsically. That is, we evaluate the quality of the",null,null
157,1https://www.nlm.nih.gov/mesh/; http://pir.georgetown.edu/pro/ 2The values of the parameters  and  were selected empirically by grid search,null,null
158,3,null,null
159,http://www.nist.gov/tac/2014/BiomedSumm/ 4We train biomedical embeddings on TREC Genomics 2004 and 2006 collections (both,null,null
160,Wikipedia and Genomics embeddings were trained using gensim implementation of,null,null
161,"Word2Vec, negative sampling, window size of 5 and 300 dimensions.",null,null
162,1134,null,null
163,Short Research Paper,null,null
164,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
165,"Table 1: Results on TAC 2014 dataset. c-P, c-R, c-F: character o set Precision, Recall and F-1 scores; R : R ; cP@K: character o set precision at K. shows statistical signi cant improvement over the best baseline performance (two-tailed t-test, p<0.05). Values are percentages.",null,null
166,Method,null,null
167,c-P c-R c-F nDCG R -1 R -2 R -3 c-P@1 c-P@5,null,null
168,VSM [4],null,null
169,20.5 24.7 21.2 48.1 49.5 26.4 20.0 31.9 26.1,null,null
170,BM25,null,null
171,19.5 18.6 17.8 38.1 43.6 23.2 16.3 25.5 24.2,null,null
172,DESM [12],null,null
173,20.3 23.8 22.3 45.6 50.3 26.2 20.6 32.5 26.5,null,null
174,LMD-LDA [10] 22.6 24.8 22.3 46.0 48.3 26.4 20.1 31.4 27.7,null,null
175,QR [4],null,null
176,22.2 29.4 23.8 49.8 50.6 27.2 21.8 37.7 28.1,null,null
177,WEWiki WEBio WEBio+rtrft WEBio+dmn,null,null
178,21.8 28.5 23.2 52.8 50.0 26.9 20.9 36.5 29.9,null,null
179,23.9 31.2 25.5 57.1 51.9 29.2 23.1 46.2 34.1 24.8 33.6 26.4 58.3 52.4 30.7 24.0 55.5 34.9 25.4 33.0 27.0 59.8 53.0 30.6 24.4 56.1 37.1,null,null
180,"Table 2: Top relevant words to the word ""expression"" according to embeddings trained on Wikipedia vs. Genomics.",null,null
181,General (Wikipedia),null,null
182,interpretation sense emotion function intension manifestation expressive,null,null
183,Biomedical (Genomics),null,null
184,upregulation mrna,null,null
185,induction protein,null,null
186,abundance gene,null,null
187,downregulation,null,null
188,extracted citation contexts using our contextualization methods in terms of how accurate they are with respect to human annotations.,null,null
189,"Evaluation. We consider the following evaluation metrics for assessing the quality of the retrieved contexts for each citation from multiple aspects: (i) Character o set overlaps of the retrieved contexts with human annotations in terms of precision (c-P), recall (c-R) and F-score (c-F). These are the recommended metrics for the task per TAC5. (ii) nDCG: we treat any partial overlaps with the gold standard as a correct context and then calculate the nDCG scores. (iii) R -N scores: To also consider the content similarity of the retrieved contexts with the gold standard, we calculate the R scores between them. (iv) Character precision at K (c-P@K): Since we are usually interested in the top retrieved spans, we consider character o set precision only for the top K spans and we denote it with ""c-P@K"".",null,null
190,Results. The results of intrinsic evaluation of contextualization are presented in Table 1. Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics. This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by [4] which improves over other baselines.,null,null
191,"We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WEwiki and QR in the Table). However, using the domain speci c embeddings (WEBio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word ""expression"" gives better intuition why is that the case. As shown, using general embeddings (left column in the table), the most similar words to ""expression"" are those related to",null,null
192,5 https://tac.nist.gov/2014/BiomedSumm/guidelines.html,null,null
193,Table 3: Breakdown of our best model's character F-score (cF) by quartiles of human performance measured by c-P.,null,null
194,Quartiles (c-P),null,null
195,Q1 Q2 Q3 Q4,null,null
196,c-F of our model 16.14 25.41 33.72 37.50 (mean ± stdev.) ±20.20 ±7.78 ±5.81 ±5.93,null,null
197,"the general meaning of it. However, many of these related words are not relevant in the biomedical context. In the biomedical context, ""expression"" refers to ""the appearance in a phenotype attributed to a particular gene"". As shown on the right column, the domain speci c embeddings (Bio) trained on genomics data are able to capture this meaning. This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13]. Last two rows in Table 1 show incorporating the domain knowledge in the model which results in signi cant improvement over the best baseline in terms of most metrics (e.g. 14% and 16% c-F improvements). This shows that domain ontologies provide additional information that the domain trained embeddings may not contain. While both our methods of incorporating domain ontologies prove to be e ective, interpolating domain knowledge directly (WEBio +dmn) has the edge over retro tting (WEBio +rtrft). This is likely due to the direct e ect of ontology on the interpolated language model, whereas in retro tting, the ontology rst a ects the embeddings and then the context extraction model.",null,null
198,"To analyze the performance of our system more closely, we took the context identi ed by 1 annotator as the candidate and the other 3 as gold standard and evaluated the precision to obtain an estimate of human performance on each citation. We then divided the citations based on human performance to 4 groups by quartiles. Table 3 shows our system's performance on each of these groups. We observe that, when human precision is higher (upper quartiles in the table), our system also performs better and with more con dence (lower std). Therefore, the system errors correlate well with human disagreement on the correct context for the citations. Averaged over the 4 annotators for each citation, the mean precision was 56.7% (note that this translates to our c-P@1 metric). In Table 1, we observe that our best method (c-P@1 of 56.1%) is comparable with average human precision score (c-P@1 of 56.7%) which further demonstrates the e ectiveness of our model.",null,null
199,3.2 External evaluation,null,null
200,"Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15]. However; as argued in section 1, citation texts do not always accurately re ect the original paper. We show how adding context from the original paper can address this concern, while keeping the bene ts of citation-based summarization. Speci cally, we compare how using no contextualization, versus various proposed contextualization approaches a ect the quality of summarization. We apply the following well-known summarization algorithms on the set of citation texts, and the retrieved citation-contexts: LexRank, LSAbased, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details). We then compare the e ect of our proposed contextualization methods using the standard R -N summarization evaluation metrics.",null,null
201,"Results. The results of external evaluation are illustrated in Table 4. The rst row (""No context"") shows the performance of each",null,null
202,1135,null,null
203,Short Research Paper,null,null
204,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
205,Table 4: E ect of contextualization on summarization.,null,null
206,Columns are summarization algorithms and rows show ci-,null,null
207,tation contextualization approaches. No Context uses only,null,null
208,citations without any contextualization. Evaluation metrics,null,null
209,are R,null,null
210,(R ) scores. () shows statistically signi cant im-,null,null
211,provement over the best baseline performance (p<0.05).,null,null
212,KLSUM LexRank,null,null
213,LSA,null,null
214,SumBasic,null,null
215,Method,null,null
216,R1 R2 R1 R2 R1 R2 R1 R2,null,null
217,No Context,null,null
218,36.0 8.3 41.3 10.8 34.7 6.5 38.7 8.7,null,null
219,VSM [4],null,null
220,35.3 7.9 40.0 9.9 33.5 6.2 39.5 9.4,null,null
221,BM25,null,null
222,35.5 8.0 39.8 9.9 33.7 6.0 38.9 8.6,null,null
223,DESM [12],null,null
224,36.3 8.7 40.2 10.4 32.6 6.5 38.3 7.9,null,null
225,LMD-LDA [10] 38.4 9.1 43.1 11.0 37.8 7.6 40.1 8.9,null,null
226,QR [4],null,null
227,39.9 10.2 43.8 11.7 38.9 8.0 40.1 8.6,null,null
228,WEWiki WEBio,null,null
229,39.7 10.2 42.7 11.8 38.0 8.0 40.2 9.2 41.7 11.7 45.6 13.8 40.3 9.1 42.4 12.6,null,null
230,WEBio+rtrft 42.9 12.2 46.2 11.6 40.0 8.9 41.3 9.7 WEBio+dmn 44.0 13.4 47.3 13.6 42.3 10.4 44.0 11.7,null,null
231,"summarization approach solely on the citations without any contextualization. The next 5 rows show the baselines and last 4 rows are our proposed contextualization methods. As shown, e ective contextualization positively impacts the generated summaries. For example, our best method is ""WEBio + dmn"" which signi cantly improves the quality of generated summaries in terms of R over the ones without any context. We observe that two low-performing baseline methods for contextualization according to Table 1 (""VSM"" and ""BM25"") also do not result in any improvements for summarization. Therefore, the intrinsic quality of citation contextualization has direct impact on the quality of generated summaries. These results further demonstrate that e ective contextualization is helpful for scienti c citation-based summarization.",null,null
232,4 RELATED WORK,null,null
233,"Related work has mostly focused on extracting the citation text in the citing article (e.g. [1]). In this work, given the citation texts, we focus on extracting its relevant context from the reference paper. Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20]. Our proposed model utilizes word embeddings and the domain knowledge. Embeddings have been recently used in general information retrieval models. Vuli and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation. Mitra et al. [12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations.",null,null
234,5 CONCLUSIONS,null,null
235,Citation texts are textual spans in a citing article that explain certain contributions of a reference paper. We presented an e ective model for contextualizing citation texts (associating them with the,null,null
236,appropriate context from the reference paper). We obtained statisti-,null,null
237,cally signi cant improvements in multiple evaluation metrics over,null,null
238,"several strong baseline, and we matched the human annotators",null,null
239,precision. We showed that incorporating embeddings and domain,null,null
240,knowledge in the language modeling based retrieval is e ective for,null,null
241,situations where there are high terminology variations between,null,null
242,the source and the target (such as citations and their reference,null,null
243,context). Citation contextualization not only can help the readers,null,null
244,"to better understand the citation texts but also as we demonstrated,",null,null
245,they can improve other downstream applications such as scienti c,null,null
246,"document summarization. Overall, our results show that citation",null,null
247,contextualization enables us to take advantage of the bene ts of,null,null
248,"citation texts, while ensuring accurate dissemination of the claims,",null,null
249,ideas and ndings of the original referenced paper.,null,null
250,ACKNOWLEDGEMENTS,null,null
251,We thank the three anonymous reviewers for their helpful com-,null,null
252,ments and suggestions. This work was partially supported by Na-,null,null
253,tional Science Foundation (NSF) through grant CNS-1204347.,null,null
254,REFERENCES,null,null
255,"[1] Amjad Abu-Jbara and Dragomir Radev. 2012. Reference scope identi cation in citing sentences. In NAACL-HLT. ACL, 80­90.",null,null
256,[2] Arman Cohan and Nazli Goharian. 2015. Scienti c Article Summarization Using Citation-Context and Article's Discourse Structure. In EMNLP. 390­400.,null,null
257,[3] Arman Cohan and Nazli Goharian. 2017. Scienti c document summarization via citation contextualization and scienti c discourse. International Journal on Digital Libraries (2017).,null,null
258,"[4] Arman Cohan, Luca Soldaini, and Nazli Goharian. 2015. Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach. In NAACL-HLT. 1042­1048.",null,null
259,"[5] Anita de Waard and Henk Pander Maat. 2012. Epistemic modality and knowledge attribution in scienti c discourse: A taxonomy of types and overview of features. In Workshop on Detecting Structure in Scholarly Discourse. ACL, 47­55.",null,null
260,"[6] Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar, Chris Dyer, Eduard Hovy, and A. Noah Smith. 2015. Retro tting Word Vectors to Semantic Lexicons. In NAACL-HLT. Association for Computational Linguistics, 1606­1615.",null,null
261,"[7] Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth J.F. Jones. 2015. Word Embedding Based Generalized Language Model for Information Retrieval. In SIGIR. ACM, 795­798.",null,null
262,"[8] Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with similarity estimation. Computational Linguistics (2015).",null,null
263,"[9] Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, and Min-Yen Kan. 2016. Overview of the CL-SciSumm 2016 Shared Task.. In BIRNDL@ JCDL.",null,null
264,"[10] Fanghong Jian, Jimmy Xiangji Huang, Jiashu Zhao, Tingting He, and Po Hu. 2016. A simple enhancement for ad-hoc information retrieval via topic modelling. In SIGIR. ACM, 733­736.",null,null
265,"[11] Qiaozhu Mei and ChengXiang Zhai. 2008. Generating Impact-Based Summaries for Scienti c Literature.. In ACL, Vol. 8. 816­824.",null,null
266,"[12] Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. 2016. A dual embedding space model for document ranking. CoRR arXiv:1602.01137 (2016).",null,null
267,"[13] Ramesh Nallapati, Bowen Zhou, and Mingbo Ma. 2016. Classify or Select: Neural Architectures for Extractive Document Summarization. arXiv:1611.04244 (2016).",null,null
268,"[14] Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Mining text data. Springer, 43­76.",null,null
269,[15] Vahed Qazvinian and Dragomir R. Radev. 2008. Scienti c Paper Summarization Using Citation Summary Networks (COLING '08). 689­696.,null,null
270,"[16] Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing Citation Contexts for Information Retrieval. In CIKM. ACM, 213­222.",null,null
271,"[17] Ágnes Sándor and Anita De Waard. 2012. Identifying claimed knowledge updates in biomedical research articles. In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse. ACL, 10­17.",null,null
272,[18] Simone Teufel and Marc Moens. 2002. Summarizing scienti c articles: experiments with relevance and rhetorical status. Computational linguistics 28 (2002).,null,null
273,[19] Ivan Vuli and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In SIGIR.,null,null
274,"[20] Stephen Wan, Cécile Paris, and Robert Dale. 2009. Whetting the appetite of scientists: Producing summaries tailored to the citation context. In JCDL. 59­68.",null,null
275,"[21] Chengxiang Zhai and John La erty. 2004. A study of smoothing methods for language models applied to information retrieval. TOIS 22, 2 (2004), 179­214.",null,null
276,1136,null,null
277,,null,null

,sentence,label,data
0,Session 6B: Conversations and Question Answering,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Learning to Rank estion Answer Pairs with Holographic Dual LSTM Architecture,null,null
3,Yi Tay,null,null
4,Nanyang Technological University ytay017@e.ntu.edu.sg,null,null
5,Minh C. Phan,null,null
6,Nanyang Technological University phan0005@e.ntu.edu.sg,null,null
7,Luu Anh Tuan,null,null
8,Institute for Infocomm Research at.luu@i2r.a-star.edu.sg,null,null
9,Siu Cheung Hui,null,null
10,Nanyang Technological University asschui@ntu.edu.sg,null,null
11,ABSTRACT,null,null
12,"We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory (LSTM) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the bene ts of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual LSTM (HDLSTM), a uni ed architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the LSTM are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive experiments, we show that HD-LSTM outperforms many other neural architectures on two popular benchmark QA datasets. Empirical studies con rm the e ectiveness of holographic composition over the neural tensor layer.",null,null
13,KEYWORDS,null,null
14,"Deep Learning, Long Short-Term Memory, Learning to Rank, estion Answering",null,null
15,1 INTRODUCTION,null,null
16,"Learning to rank techniques are central to many web-based question answering (QA) systems such as factoid-based QA or communitybased QA (CQA). In these applications, questions are matched against an extensive database to nd the most relevant answer. Essentially, this is highly related to many search and information retrieval tasks such as traditional document retrieval and text matching. However, a key di erence is that questions and answers are o en much shorter compared to full- edged documents whereby the problem of lexical chasm [1, 8, 32] becomes more prevalent. As such, this makes the already di cult task of designing features for questions and answers even harder. Furthermore, traditional",null,null
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080790",null,null
18,"approaches o en involve extensive handcra ed features and domain expertise which can be laborious and expensive. In addition, constructing features from textual clues [24, 27, 33, 36] such as lexical and syntactic features is di cult and provide limited bene ts. Overall, the challenges of learn-to-rank QA systems are two-fold. First, feature representations of questions and answers have to be learned or designed. Second, a similarity function has to be de ned to match questions to answers.",null,null
19,"Recently, deep learning architectures have been an extremely popular choice for learning distributed representations of words, sentences or documents [11, 14]. Generally, this is known as representational learning whereby low dimensional vectors are learned for words or documents via neural networks such as the convolutional neural networks (CNN), recurrent neural network (RNN) or standard feed-forward multi-layer perceptron (MLP). is has widespread applications in the eld of NLP and IR such as semantic text matching [20, 25], relation detection [10], language modeling [14] and question answering [18, 20]. Essentially, the a ractiveness of these models stem from the fact that features are learned in deep learning architectures in an end-to-end fashion and o en require li le or no human involvement. Furthermore, the performance of these models are o en spectacular.",null,null
20,"Additionally and recently, it has also been fashionable to model the relationship between vectors via tensor layers [18, 25, 30]. A recent work, the convolutional neural tensor network (CNTN) [18] demonstrates impressive results on community-based question answering. In their work, a convolutional neural network is used to learn representations for questions and answers while a tensor layer is used to model the relationship between the representations using an additional tensor parameter. is is powerful because the tensor layer models multiple views of dyadic interactions between question and answer pairs which enables rich representational learning. Overall, the CNTN is a uni ed architecture that learns representations and performs matching in an end-to-end fashion.",null,null
21,"However, the use of a tensor layer may not be implication free. Firstly, adding a tensor layer severely increases the number of parameters which naturally and inevitably increases the risk of over ing. Secondly, this signi cantly increases computational and memory cost of the overall network. irdly, the inclusion of a tensor layer also indirectly restricts the expressiveness of the QA representations since increasing the parameters of the QA representations would easily incur memory and computational costs of quadratic scale at the tensor layer.",null,null
22,695,null,null
23,Session 6B: Conversations and Question Answering,null,null
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
25,"In lieu of the above mentioned weaknesses, we propose an alternative to the tensor layer. For the rst time, we adopt holographic composition to model the relationship between question and answer embeddings. Our approach is largely based on holographic models of associative memory and employs circular correlation to learn the relationship between QA pairs. e prime contributions of our paper can be summarized as follows:",null,null
26,"· For the rst time, we adopt holographic composition for modeling the interaction between representations of QA pairs. Unlike the tensor layer, our compositional approach is essentially parameterless, memory e cient and scalable. Furthermore, our approach also enables rich representational learning by employing circular correlation.",null,null
27,"· As a whole, we present a novel deep learning architecture, HD-LSTM (Holographic Dual LSTM) for learning to rank QA pairs. Our model is a uni ed architecture similar to [18]. However, instead of the CNN, we use multi-layered long short-term memory neural networks to learn representations for questions and answers. Similar to other deep learning models, our approach does not require extensive feature engineering or domain knowledge.",null,null
28,· We provide extensive experimental evidence of the e ectiveness of our model on both factoid question answering and community-based question answering. Our proposed approach outperforms many other neural architectures on TREC QA task and on the Yahoo CQA dataset.,null,null
29,2 RELATED WORK,null,null
30,"Our work is concerned with ranking question and answer pairs to select the most suitable answer for each question. Across the rich history of IR research, techniques for doing so have been primarily focused on lexical and syntactic feature-dependent approaches.",null,null
31,"ese techniques include the Tree Edit Distance (TED) model [5], Support Vector Machines (SVMs) with tree kernels [21] and linear chain Conditional Random Fields (CRFs) [33] with features from the TED model. However, apart from relying heavily on handcra ed features such as cumbersome parse trees, these approaches have limited performance and have been shown to be outclassed by modern deep learning approaches such as convolutional neural networks [20, 35].",null,null
32,"e key intuition behind deep learning architectures is to learn low-dimensional representations of words, documents or sentences which can be used as input features. For example, Yu et al. [35] employed a convolutional neural network for feature learning of QA pairs and subsequently applied logistic regression for prediction. Despite its simplicity, the performance of Yu et al. has already surpassed all traditional approaches [5, 21, 27­29]. Another a ractive quality of deep learning architectures is that features can be learned in an end-to-end fashion. Severyn et al. [20] demonstrated a uni ed architecture that trains a convolutional neural network together with a multi-layer perceptron. In short, features are learned while the parameters of the network are optimized for the task at hand.",null,null
33,"In the architectures of Severyn et al. [20] and Yu et al. [35], representations of questions and answers are learned separately and concatenated for prediction at the end. Qiu et al. [18] introduced a tensor layer to model the relationship between question and answer",null,null
34,"representations. e tensor layer can be seen as a compositional technique to learn the relationship between two vectors and was adapted from the neural tensor network (NTN) by Socher et al. [22, 23]. e NTN was originally incepted in the eld of NLP for semantic parsing and used as a compositional operator in recursive neural tensor networks (RNTN) [23] and also relational learning on knowledge bases [22]. It has also recently seen adoption for modeling document novelty in [30]. e tensor layer models multiple dyadic interactions between two vectors via an additional tensor parameter. is suggests rich representational learning that is useful for matching text pairs.",null,null
35,"Additionally, recurrent neural networks such as the long shortterm memory (LSTM) networks are also widely popular for learning sentence representations and has seen wide adoption in a variety of NLP tasks. Without an exception, LSTM networks are also widely adopted in QA [12, 25, 26]. e usage of grid-wise similarity matrices within neural architectures are also recently very fashionable and have seen wide adoption1 in QA tasks [4, 15, 25] to model the interactions between QA pairs. For example, in the MV-LSTM [25], all positional hidden states from both LSTMs are being matched grid-wise using a variety of similarity scoring functions followed by a max-pooling layer. On the other hand, the works of [31] are concerned with learning grid-wise a entions. On a side note, it is good to note that, grid-wise matching, though highly competitive, naturally incurs a prohibitive computational cost of quadratic scale.",null,null
36,"As seen in many recent works, the tensor layer is highly popular to model relationship between two vectors [18, 25]. However, a tensor layer adds a signi cant amount of parameters to the network causing implications in terms of runtime, memory, risk of over tting as well as an inevitable restriction of exibility in designing representations for questions and answers. Speci cally, increasing the dimensionality of the LSTM or CNN output by x would incur a parameter cost of x2 in the tensor layer which can be non-ideal especially in terms of scalability. As an alternative to the tensor layer, our novel deep learning architecture adopts the circular correlation of vectors to model the interactions between question and answer representations. e circular correlation of vectors, along with circular convolution, are typically used in Holography to store and retrieve information [3, 17] and are also known as correlationconvolution (holographic-like) memories. Due to its connections with holographic models of associative memories [17], we refer to our model as Holographic Dual LSTM. It is good to note that a similar but fundamentally di erent work [16] also used holography inspired operations within recurrent neural networks. However, our work is the rst work to incorporate holographic representational learning for QA embeddings.",null,null
37,"In addition, holographic composition [17] can also be interpreted as compressed tensor product which also enables rich representational learning without severely increasing the number of parameters of the network. In this case, the parameters of the network are learned in a way that best explains the correlation between questions and answers. In the same domain where the neural tensor network was incepted, holographic embeddings of knowledge graphs [13], demonstrates the e ectiveness of holographic composition in the task of relational learning on knowledge bases. As",null,null
38,"1Notably, our proposed holographic composition can also be used for grid-wise matching.",null,null
39,696,null,null
40,Session 6B: Conversations and Question Answering,null,null
41,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
42,"a whole, we propose a novel deep learning architecture based on long short-term memory neural networks while using holographic composition to model the interactions between QA embeddings, this enables rich representational learning with improved exibility and scalability. e outcome is a highly performant end-to-end deep learning architecture for learning to rank for QA applications.",null,null
43,3 PRELIMINARIES,null,null
44,"In this section, we introduce the background for the remainder of the paper. Namely, we formally give the problem de nition and introduce fundamental deep learning models required to understand the remainder of the paper.",null,null
45,3.1 Problem Statement and Approach,null,null
46,"e task of supervised learning to rank can be typically regarded as a binary classi cation problem. Given a set of questions qi  Q, the task is to rank a list of candidate answers ai  A. Speci cally, we try to learn a function f (q, a) that outputs a relevancy score f (q, a)  [0, 1] for each question answer pair. is score is then used to rank a list of possible candidates. Typically, there are three di erent ways for supervised text ranking, namely, pairwise, pointwise and listwise. Pairwise considers maximizing the margin between positive and negative question-answer pairs with an objective function such as the hinge loss. Pointwise considers each pair, positive or negative, individually. On the other hand, listwise considers a question and all candidate answers as a training instance. Naturally, pairwise and listwise are much harder to train, implement and take a longer time due to having to process more instances. erefore, in this work, we mainly consider a pointwise approach when designing our deep learning model.",null,null
47,3.2 Long short-term Memory (LSTM),null,null
48,"First, we introduce the Long Short-Term Memory (LSTM) [6]. LSTMs are a type of recurrent neural network that are capable of learning long term dependencies across sequences. Given an input sentence S ,"" (xo, x1..., xn ), the LSTM returns a sentence embedding ht for position t with the following equations:""",null,null
49,"it ,  (Wi xt + Uiht -1 + bi ) ft ,  (Wf xt + Uf ht -1 + bf ) ct , ft ct -1 + it tanh(Wc xt + Ucht -1 + bc ) ot ,  (Woxt + Uoht -1 + bo ) ht , ot tanh(ct )",null,null
50,"where xt and ht are the input vectors at time t. W, b, U are the parameters of the LSTM network and  ,"" {o, i, f , u, c}.  is the sigmoid function and ct is the cell state. For the sake of brevity, we omit the technical details of LSTM which can be found in many related works. e output of this layer is a sequence of hidden vectors H  RL×d where L is the maximum sequence length and d is the dimensional size of LSTM. It is also possible to stack layers of LSTMs on top of one another which form multi-layered LSTMs which we will adopt in our approach.""",null,null
51,3.3 Neural Tensor Network,null,null
52,"e Neural Tensor Network [22, 23] is a parameterized composition technique that learns the relationships between two vectors. e scoring function between two vectors are de ned as follows:",null,null
53,"st (q, a) ,"" uT f (q T M[1:r ]a + V [q, a] + b)""",null,null
54,(1),null,null
55,"where f is a non-linear function such as tanh applied element wise. M[1:r ]  Rn×n×r is a tensor (3d matrix). For each slice of the tensor M, each bilinear tensor product q T Mr a returns a scalar value to form a r dimensional vector. e other parameters are the standard",null,null
56,form of a neural network. We can clearly see that the NTN enables,null,null
57,rich representational learning of embedding pairs by using a large,null,null
58,number of parameters.,null,null
59,4 OUR DEEP LEARNING MODEL,null,null
60,"In this section, we introduce Holographic Dual LSTM for representational learning and ranking of short text pairs. In our model, we use a pair of multi-layered LSTMs denoted Q-LSTM and A-LSTM. First, the LSTMs learn sentence representations of question and answer pairs and subsequently holographic composition is employed to model the similarity between the outputs of Q-LSTM and A-LSTM. Finally, we pass the network through a fully connected hidden layer and perform binary classi cation. is is all done in an end-to-end fashion. Figure 1 shows the overall architecture.",null,null
61,4.1 Learning QA Representations,null,null
62,"Our model accepts two sequences of indices (one for question and the other for answer) and a one-hot encoded ground truth for training. ese sequence of indices are rst passed through a look-up layer. At this layer, each index is converted into a low-dimensional vector representation. e parameters of this layer are W  R|V |×n where V is the size of the vocabulary and n is the dimensionality of the word embeddings. Even though these word embeddings can be learned from the training process of our model, i.e., end-to-end, we do not do so since learning word embeddings o en require an extremely large corpus like Wikipedia. erefore, we initialize W with pretrained SkipGram [11] embeddings which is aligned with the works of [20, 35]. Next, these embeddings from question and answer sequences are fed into Q-LSTM and A-LSTM respectively. Subsequently, the last hidden output from Q-LSTM and A-LSTM are taken to be the nal representation for question and answer respectively.",null,null
63,4.2 Holographic Matching of QA pairs,null,null
64,"e QA embeddings learned from LSTM are then passed into what we call the holographic layer. In this section, we introduce our novel compositional deep learning model for modeling the relationship between q and a. We denote qa as a compositional operator applied to vectors q and a. We employ the circular correlation of vectors to learn relationships between question and answer embeddings.",null,null
65,"qa ,q a",null,null
66,(2),null,null
67,697,null,null
68,Session 6B: Conversations and Question Answering,null,null
69,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
70,Figure 1: Holographic Dual LSTM Deep Learning Model for Ranking of QA Pairs,null,null
71,where : Rd × Rd  Rd denotes the circular correlation operator2.,null,null
72,d -1,null,null
73,"[q a]k , qi a(k+i ) mod d",null,null
74,(3),null,null
75,"i ,0",null,null
76,Circular correlation can be computed as follows:,null,null
77,"q a , F -1 (F (q) F (a))",null,null
78,(4),null,null
79,where F (.) and F -1 (.) are the fast Fourier transform (FFT) and,null,null
80,inverse fast Fourier transform. F (q) denotes the complex conjugate,null,null
81,"of F (q). is element-wise (or Hadamard) product. Additionally,",null,null
82,circular correlation can be viewed as a compressed tensor product,null,null
83,"[13]. In the tensor product [q  a]ij , qiaj a separate element is used to store each pairwise multiplication or interaction between q",null,null
84,"and a. In circular correlation, each element of the composed vector",null,null
85,is a sum of multiplicative interactions over a xed summation,null,null
86,pa ern between q and a. Figure 2 describes this process where the,null,null
87,circular arrows depict the summation process in which vector c is,null,null
88,the result of composing q and a with circular correlation.,null,null
89,One key advantage of this composition method is that there,null,null
90,are no increase in parameters. e fact that the composed vector,null,null
91,remains at the same length of its constituent vectors is an extremely,null,null
92,a ractive quality of our proposed model. In the case where question,null,null
93,"and answer representations are of di erent dimensions, we can",null,null
94,simply zero-pad the vectors to make them the same length. As,null,null
95,"circular correlation uses summation pa erns, it is still possible to",null,null
96,"compose them without much implications. However, for this paper",null,null
97,we consider that q and a to have the same dimensions.,null,null
98,4.3 Holographic Hidden Layer,null,null
99,"Subsequently, a fully connected hidden layer follows our compositional operator which forms the holographic layer.",null,null
100,"hout ,  (Wh . [q a] + bh )",null,null
101,(5),null,null
102,"2For Holographic Composition, we use zero-indexed vectors for notational convenience.",null,null
103,"Figure 2: Circular Correlation as Compressed Tensor Product, Circular Arrows denote Summation Process, Adapted from Plate (1995) [17].",null,null
104,"where wh and bh are parameters of the hidden layer and  is a non-linear activation function like tanh. Traditionally, most models",null,null
105,"[7, 20] use the composition operator of concatenation, denoted  to combine the vectors of questions and answers.  : Rd1 × Rd2  Rd1+d2 simply appends one vector a er another to form a vector of",null,null
106,"their lengths combined. Obviously, concatenation does not consider",null,null
107,"the relationship between latent features of QA embeddings. us,",null,null
108,the relationship has to be learned from the parameters of the deep,null,null
109,"learning model, i.e., the subsequent hidden layer. In summary, the",null,null
110,fully connected dense layer that maps [q a] to hout forms the holographic hidden layer of our network.,null,null
111,"Incorporating Additional Features Following the works of [2, 20], it is also possible (though optional) to incorporate additional features. First, we include an additional similarity measure in our model between QA embeddings. Namely, this similarity is known as the bilinear similarity which can be de ned as:",null,null
112,"sim(q, a) , q T Ma",null,null
113,(6),null,null
114,where M  Rn×n is a similarity matrix between vectors q  Rn and a  Rn . e bilinear similarity is a parameterized approach,null,null
115,698,null,null
116,Session 6B: Conversations and Question Answering,null,null
117,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
118,"where M is an additional parameter of the network. e output of sim(q, a) is a scalar value that is concatenated with [q a]. We experimented with concatenation at hout but empirically found it to perform worse. It is also possible to include other manual features. For example, in [20], word overlap features Xf eat were included before the hidden layers at the join layer. e rationale for doing so is as follows: First, it is di cult to encode features like word overlap into deep learning architectures [20, 35]. Secondly, word overlap features are relatively easy and trivial to implement and incorporate. As such, we are able to do the same with our model.",null,null
119,"us, when using external features, the inputs to the holographic hidden layer becomes a vector [[q a], sim(q, a), Xf eat ].",null,null
120,4.4 So max Layer,null,null
121,e output from the holographic hidden layer is then passed into a fully connected so max layer which introduces another two parameters Wf and bf .,null,null
122,"p , so f tmax (Wf . hout + bf )",null,null
123,(7),null,null
124,where k is the weight vector of the kth class and x is the nal vector representation of question and answer a er passing through,null,null
125,all the layers in the network.,null,null
126,4.5 Training and Optimization,null,null
127,"Finally, we describe the optimization and training procedure of our network. Our network minimizes the cross-entropy loss function as follows:",null,null
128,N,null,null
129,"L,-",null,null
130,[ i log ai + (1 -,null,null
131,i ) log(1 - ai )] +  ,null,null
132,2 2,null,null
133,(8),null,null
134,"i ,1",null,null
135,where a is the output of the so max layer.  contains all the pa-,null,null
136,rameters of the network and ,null,null
137,2 2,null,null
138,is,null,null
139,the,null,null
140,L2,null,null
141,regularization.,null,null
142,e,null,null
143,parameters of the network can be updated by Stochastic Gradient,null,null
144,"Descent (SGD). In our network, we mainly employ the Adam [9]",null,null
145,optimizer.,null,null
146,5 DISCUSSION,null,null
147,"In this section, we discuss and highlight some of the interesting and advantageous properties of our proposed approach.",null,null
148,5.1 Complexity Analysis,null,null
149,"To be er understand the merits of our proposed approach, we study the computational and memory complexity of our model with respect to the alternatives like the tensor layer.",null,null
150,Operator Tensor Product  Concatenation  Circular Correlation,null,null
151,#Parameters d2,null,null
152,2d,null,null
153,d,null,null
154,Complexity O (d 2 ),null,null
155,O(d ),null,null
156,O(d log d ),null,null
157,Table 1: Complexity Comparison between Compositional,null,null
158,Operators,null,null
159,"First, Table 1 shows the parameter cost and complexity for the",null,null
160,"three di erent compositional operators. is assumes a simple example where q, a  Rd and a vector of the same dimensionality as",null,null
161,Network #Parameters,null,null
162,d/h/k On TREC,null,null
163,NTN d2k + 2dk + 2k 640 / 0 / 5 2.1M,null,null
164,Ours,null,null
165,2dh + 4h 640 / 64 / 0 41.2K,null,null
166,Table 2: Memory Complexity Comparison between Tensor,null,null
167,Layer and Holographic Layer,null,null
168,Network,null,null
169,Complexity,null,null
170,NTN,null,null
171,O(d2k + 2dk + 2k ),null,null
172,Ours O(2dh + 4h + d log d ),null,null
173,Table 3: Complexity Comparison between Tensor Layer and,null,null
174,Holographic Layer,null,null
175,"q a is used to map the composed vector into a scalar value. Clearly, the cost of a simple tensor layer is of quadratic scale which makes it di cult to scale with large d. Concatenation, on the other hand, doubles the length of composed vectors and increases the memory cost by a factor of two. Finally, we see that the parameter cost of circular correlation that we employ is only d. As such, there can be signi cantly less parameters in the subsequent layers. Finally, the computational complexity of circular correlation is also relatively low at d log d. Next, we compare the complexity of our network and the NTN. To enable direct comparison, we exclude any additional features at the holographic hidden layer of our network and include the subsequent so max layer. Finally, the overall network and similarity between q and a can be modeled as follows:",null,null
176,"sh (q, a) , so f tmax (WfT f (WhT [q a] + bh ) + bf )",null,null
177,(9),null,null
178,"where Wh  Rd×r is parameters at the holographic hidden layer following the composition operation, bh is the scalar bias at the hidden layer, Wf  Rr ×2 converts the output at the hidden layer to a 2-class classi cation problem and q, a  Rd where d is the",null,null
179,dimension size of the LSTM. f (.) is a non-linear activation function.,null,null
180,"Note that since we consider use a so max layer at the output, our",null,null
181,"nal output s (q, a) is a vector of 2 dimensions. Similarly, in the",null,null
182,"traditional tensor layer described in Equation (1), we are able to simply adapt the vector u to become a weight matrix of u  Rk×2",null,null
183,where k is the number of slices of tensors.,null,null
184,"In Table 2 and Table 3, we compare the di erences between the",null,null
185,tensor layer and our holographic layer with respect to the number,null,null
186,of parameters and computational complexity respectively. Note,null,null
187,"that d is the dimensionality of QA embeddings, h is the size of",null,null
188,the hidden layer and k is the number of tensor slices. To facilitate,null,null
189,"easier comparison, we do not include complexity from learning QA",null,null
190,"representations, computing bias and activation functions but only",null,null
191,"matrix and vector operations. From Table 2, it is clear and evident",null,null
192,that our approach does not require as much parameters as the,null,null
193,NTN. We also report the optimal dimensions of the QA embeddings,null,null
194,and hidden layer size on TREC QA. We see that our model only requires 41.2k parameters3 as opposed to 2.1M parameters with,null,null
195,"the NTN. As such, we see that when optimal parameters required",null,null
196,"for sentence modeling is high, the cost on the subsequent matching",null,null
197,3We exclude word embedding and LSTM parameters in this comparison,null,null
198,699,null,null
199,Session 6B: Conversations and Question Answering,null,null
200,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
201,"layer becomes impractical. e problem of quadratic scale is also re ected in computational complexity. us, from a theoretical point of view, the holographic composition can be seen as a memory e cient and faster alternative to the neural tensor network layer.",null,null
202,5.2 Associative Holographic Memories,null,null
203,"Holographic models of associative memories employ a series of convolutions and correlations to store and retrieve item pairs. is is sometimes referred to as convolution-correlation (holographiclike) memories [17]. At this point, it is apt to introduce circular convolution:  : Rd × Rd  Rd which is closely related to circular correlation.",null,null
204,d -1,null,null
205,"[q  a]k , qi a(k-i ) mod d",null,null
206,(10),null,null
207,"i ,0",null,null
208,"In holographic associative memory models, association of vector",null,null
209,pairs can be encoded via correlation and then decoded with circular convolution. e relationship between correlation and convolution is as follows:,null,null
210,"q a , q~  a",null,null
211,(11),null,null
212,"where q~ is the approximate inverse of q such that qi ,"" q(-i mod d ). Typically, the encoding-decoding4 process is done via Hebbian learning in associative memory models. However, in our case, our model is holographic in the sense that correlation-convolution memories are learned implicitly via back-propagation. For example, let h be the input to the hidden layer hout , the gradients at a can be represented as:""",null,null
213,E ai,null,null
214,",",null,null
215,k,null,null
216,E hj,null,null
217,q (k -j,null,null
218,mod d ),null,null
219,(12),null,null
220,e gradient at ai according to Equation (12) [16] is equivalent to correlating h with the approximate inverse of q. Recall that,null,null
221,correlating with the inverse is equivalent to circular convolution.,null,null
222,"As such, this establishes the relation of our model to holographic",null,null
223,"memories. Since our main point is to illustrate these connections,",null,null
224,we omit the entire back-propagation derivation due to the lack of,null,null
225,"space. Finally, we describe and summarize the overall advantages of",null,null
226,employing a holographic layer in our deep architecture for learning,null,null
227,to rank question answer pairs.,null,null
228,"· Unlike circular convolution, circular correlation is noncommutative, i.e., q a a q. is is useful as most applications of text matching are non-symmetric. For example, questions to answers or queries to documents are not symmetric in nature. As such, we utilize correlation as the encoding operation and allow the network to decode via circular convolution while learning parameters.",null,null
229,· e rst index of the circular correlation composed vector [q a]0 is the dot product of q and a. is is extremely helpful since question answer matching requires a measure of similarity.,null,null
230,· e computational complexity of FFT is O(d lo d ) which makes it an e cient composition.,null,null
231,"4Ideally, that the euclidean norm of q and a should be  1. However, our preliminary experiments showed that adding a normalization layer did not improve the performance.",null,null
232,"· Our composition does not increase the dimensionality of its constituent vectors, i.e, the composition of q a preserves its dimensionality. On the other hand, concatenation doubles the parameter cost at the subsequent hidden layers. Furthermore, the relationship between question and answer embeddings have to be learned by the hidden layer.",null,null
233,"· e association of two vectors, namely question and answer vectors are modeled end-to-end in the network. Via back-propagation, the network learns parameters that best explains this correlation via gradient descent.",null,null
234,"Here it is good to note that the original holographic reduced representations [17] used convolution to encode and correlation to decode. However, this can be done vice versa as well [13].",null,null
235,6 EMPIRICAL EVALUATION ON TREC QA,null,null
236,We evaluate our model on the TREC QA task of answer sentence selection on factoid based QA.,null,null
237,6.1 Experiment Setup,null,null
238,"In this section, we introduce the dataset, evaluation procedure, metrics and compared baselines used in this experiment.",null,null
239,"6.1.1 Dataset. In this task, we use the benchmark dataset provided by Wang et al. [29]. is dataset was collected from TREC QA tracks 8-13. In this task of factoid QA, questions are generally factual based questions such as ""What is the monetary value of the Nobel Peace Prize in 1989?"""". In this dataset, we are provided with two training sets, namely, TRAIN and TRAIN-ALL. TRAIN consists of QA pairs that have been manually judged and annotated. TRAINALL is a automatically judged dataset of QA pairs and contains a larger number of QA pairs. TRAIN-ALL, being a larger dataset, also contains more noise. Nevertheless, both datasets enable the comparison of all models with respect to availability and volume of training samples. Additionally, we are also provided with a development set for parameter tuning. e results of both training sets, development set and testing set are reported in Table 4. Finally, it is good to note that the maximum number of tokens for questions and answers are 11 and 38 respectively and the length of the vocabulary |V | , 16468.",null,null
240,Data TRAIN-ALL,null,null
241,TRAIN DEV TEST,null,null
242,# estions 1229 94 82 100,null,null
243,# QA pairs 53417 4718 1148 1517,null,null
244,% Correct 12 7.4 9.3 18.7,null,null
245,Table 4: Dataset Statistics for TREC QA Dataset,null,null
246,"6.1.2 Evaluation Procedure and Metrics. Following the experimental procedure in [20], we report the results of all models in two se ings. In the rst se ing, we measure the representational learning ability of all deep learning models without the aid of external features. Conversely, in the second se ing, we include an additional feature vector Xf eat  R4 containing the count of word overlaps (ordinary and idf weighted) between question-answer pairs by considering inclusion and dis-inclusion of stop-words. Finally, the o cial evaluation metrics of MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are used as our evaluation metrics.",null,null
247,700,null,null
248,Session 6B: Conversations and Question Answering,null,null
249,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
250,MRR is de,null,null
251,ned as,null,null
252,1 |q |,null,null
253,"|Q | q,1",null,null
254,1 r ank (q),null,null
255,where rank(q),null,null
256,is,null,null
257,the rank of,null,null
258,the,null,null
259,rst correct answer. MAP is de,null,null
260,ned,null,null
261,as,null,null
262,1 Q,null,null
263,"Q q,1",null,null
264,A,null,null
265,P (q).,null,null
266,e MAP,null,null
267,"is the average precision across all queries qi  Q. For evaluation,",null,null
268,we use the o cial trec eval script.,null,null
269,"6.1.3 Algorithms Compared. Aside from comparison with all published state-of-the-art methods, we also evaluate our model against other deep learning architectures. Since the deep learning models compared in [20] are based on Convolutional Neural Networks, we additionally compare our model with MV-LSTM [25] and a simple LSTM baseline in our experimental evaluation. e following lists the major and popular deep learning based models for direct comparison with our model. Model names with  indicate that we implemented the model ourselves.",null,null
270,"· CNN + Logistic Regression (Yu et al.) is is the model introduced in [35]. Representations of questions and answers are learned by a convolutional neural network. Subsequently, logistic regression over the learned features is used to score QA pairs. We report two se ings, namely Unigram and Bigram which are also reported in their work.",null,null
271,"· CNN We implemented a CNN model following the architecture and hyperparameters of [20]. is model includes a bilinear similarity feature while concatenating two CNN encoded sentence representations. Unlike Yu et al.'s model, this work ranks QA pairs using an end-to-end architecture.",null,null
272,"· CNTN We implemented a neural tensor network layer to performing matching of question and answer representations encoded by convolutional neural networks. is is similar to [18] but adopts the CNN architecture and hyperparameters of Severyn et al. [20]. For the tensor layer, we use k , 5 where k is the number of slices of the tensor.",null,null
273,"· LSTM We consider both single layer and multi-layered LSTMs as our baselines. ese baseline models do not specially model the relationships between questions and answers. Instead, a concatenation operation is used to combine the QA embeddings in which the relationships between the two vectors are modeled by the hidden layer.",null,null
274,"· MV-LSTM (Wan et al.) is model, introduced in [25], considers matching of multiple positional embeddings and subsequently applying max-pooling of top-k interactions. For scalability reasons, we only consider the bilinear similarity se ing for this model. We consider this su cient for three reasons. First, it is reported in [25] that the performance bene ts of tensor over bilinear is minimal. Second, it is extremely expensive computationally even when considering a bilinear similarity let alone the tensor similarity. ird, the comparison with this model mainly aims to investigate the e ectiveness of multiple positional embeddings.",null,null
275,"· NTN-LSTM We consider a Neural Tensor Network + LSTM architecture instead of the CNTN to enable fairer comparison with our LSTM based model. In this model, we replace the holographic layer in HD-LSTM with a NTN layer which forms the major comparison in this paper. For the tensor layer, similar to the CNTN, we use k , 5 where k is the number of slices of the tensor.",null,null
276,"· HD-LSTM (Ours) Holographic Dual LSTMS is the model architecture introduced in this paper. In our HD-LSTM model, the QA representations are merged with Holographic Composition.",null,null
277,"6.1.4 Implementation Details and Hyperparameters. We implemented all deep learning architectures ourselves with the exception of Yu et al. [35] which we directly report the results. To facilitate fair comparison, we implement the exact architecture of the CNN model from Severyn et al. [20] ourselves using the same evaluation procedure and optimizer. All hyperparameters were tuned on the development set using extensive grid search. We trained all models using the Adam [9] optimizer with an initial learning rate of 10-5 for LSTM models and 10-2 for CNN models5 and minimized the same cross entropy loss in a point-wise fashion. We applied gradient clipping of 1.0 of the norm for all LSTM models.",null,null
278,"With the exception of the single-layered LSTM and MV-LSTM, all LSTM-based models use a single-direction and multi-layered se ing.",null,null
279,"e input sequences are all padded with zero vectors to the max length for questions and answers separately. e dimensionality of the LSTM models are tuned in multiples of 128 in the range of [128, 640] for TRAIN and amongst {512, 1024} for TRAIN-ALL in lieu of the larger dataset. Here it is good to note that the nal feature vector of the model in Severyn et al is  1000. e number of LSTM layers are tuned from a range of 2 - 4 and batch size is",null,null
280,"xed to 256 for all LSTM based models. e hidden layer size for all LSTM models are amongst {32, 64, 128, 256, 512}. For regularization and preventing over- ing, we apply a dropout of d , 0.5 and set the regularization hyperparameter  ,"" 0.00001. For MV-LSTM, we followed the con guration se ing as stated in [25]. We used the pretrained word embeddings [20] of 50 dimensions trained on Wikipedia and AQUAINT corpus. e word embedding layer is set to non-trainable in lieu of the small dataset. We trained all models for a maximum of 30 epochs with early stopping, i.e., if the MAP score does not increase a er 5 epochs. We take MAP scores on the development set at every epoch and save the parameters of the network for the top three models on the development set. We report the best test score from the saved models. All experiments were conducted on a Linux machine with a single Nvidia GTX1070 GPU (8GB RAM).""",null,null
281,6.2 Experimental Results,null,null
282,"is section shows the experimental results on the TREC QA answer sentence selection task. Table 5 shows the result of all deep learning architectures in four di erent con gurations, i.e., di erent training sets (TRAIN vs TRAIN-ALL) and di erent feature se ings (with and without additional features). Overall, we see that HD-LSTM outperforms all other deep learning models. e relative ranking of each deep learning architecture is in concert with our intuitions. Using a tensor layer for matching improves performance over their base models which is aligned with the results of [18]. However, we see that HD-LSTM outperforms NTN-LSTM by a signi cant margin across all datasets and se ings despite being more e cient. is shows the e ectiveness of holographic composition for rich representational learning of QA pairs despite having less parameters. Additionally, the average increase over the baseline multi-layered",null,null
283,5 is learning rate works best for CNN models with Adam,null,null
284,701,null,null
285,Session 6B: Conversations and Question Answering,null,null
286,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
287,Se ing 1 (raw),null,null
288,TRAIN,null,null
289,TRAIN-ALL,null,null
290,Se ing 2 (with extra features),null,null
291,TRAIN,null,null
292,TRAIN-ALL,null,null
293,All Average,null,null
294,Model,null,null
295,MAP MRR MAP MRR MAP MRR MAP MRR MAP MRR,null,null
296,CNN + LR (unigram) 0.5387 0.6284 0.5470 0.6329 0.6889 0.7727 0.6934 0.7677 0.6170 0.6982,null,null
297,CNN + LR (bigram) 0.5476 0.6437 0.5693 0.6613 0.7058 0.7846 0.7113 0.7846 0.6335 0.7186,null,null
298,LSTM (1 layer) LSTM,null,null
299,0.5731 0.6056 0.6204 0.6685 0.6406 0.7494 0.6782 0.7604 0.6280 0.6960 0.6093 0.6821 0.5975 0.6533 0.7007 0.7777 0.7350 0.8064 0.6606 0.7299,null,null
300,CNN CNTN,null,null
301,0.5994 0.6584 0.6691 0.6880 0.7000 0.7469 0.7216 0.7899 0.6725 0.7208 0.6154 0.6701 0.6580 0.6978 0.7045 0.7562 0.7278 0.7831 0.6764 0.7268,null,null
302,MV-LSTM NTN-LSTM,null,null
303,0.6307 0.6675 0.6488 0.6824 0.7327 0.7940 0.7077 0.7821 0.6800 0.7315 0.6274 0.6831 0.6340 0.6772 0.7225 0.7904 0.7364 0.8009 0.6800 0.7379,null,null
304,HD-LSTM,null,null
305,0.6404 0.7123 0.6744 0.7511 0.7520 0.8146 0.7499 0.8153 0.7042 0.7733,null,null
306,Table 5: Experimental Results of all Deep Learning Architectures on TREC QA Dataset. Best result is in boldface.,null,null
307,"LSTM are 4% and 5% in terms of MAP and MRR respectively which can be considered signi cant. We also note that there is quite significant improvement with using multi-layered LSTMs over a single layered LSTM. e performance of MV-LSTM is competitively similar to NTN-LSTM in this task. However, it is good to note that MV-LSTM takes  30s per epoch at the bilinear se ing as opposed to our model's  0.1s epoch with the same LSTM con gurations and se ings and on the same machine and GPU. On the other hand, we see that the baseline LSTM models perform worse than CNN based models whereby a single layer LSTM performs poorly and does almost as poor as CNN with logistic regression from Yu et al. [35]. However, the NTN-LSTM and MV-LSTM perform be er than the CNTN. It is good to note that our CNN model implementation achieves slightly worst results as compared to [20] because model parameters are saved at the batch level in their work while we evaluate at an epoch level instead. Nevertheless, the performance is quite similar.",null,null
308,"Finally, Table 6 shows the results of all published models including non deep learning systems. Evidently, deep learning has signi cantly outperformed traditional methods in this task. It is also good to note that HD-LSTM outperforms all models (both deep learning and non-deep learning) even with the smaller TRAIN dataset. We nd this result remarkable.",null,null
309,Model,null,null
310,MAP MRR,null,null
311,Wang et al. (2007) [29] Heilman and Smith (2010) [5] Wang and Manning (2010) [28] Yao (2013) [33] Severyn & Moschi i (2013) [21] Yih et al. (2013) [34] Yu et al. (2014) [35] Severyn et al. (2015) [20] HD-LSTM TRAIN HD-LSTM TRAIN-ALL,null,null
312,0.6029 0.6091 0.5951 0.6307 0.6781 0.7092 0.7113 0.7459,null,null
313,0.7520 0.7499,null,null
314,0.6852 0.6917 0.6951 0.7477 0.7358 0.7700 0.7846 0.8078 0.8146,null,null
315,0.8153,null,null
316,Table 6: Performance Comparison of all Published Models on TREC QA Dataset.,null,null
317,7 EMPIRICAL EVALUATION ON,null,null
318,COMMUNITY-BASED QA,null,null
319,"In this experiment, we consider the task of community-based question answering (CQA). We use the Yahoo QA Dataset6 for this purpose. e objectives of this experiment are two-fold. First, we provide more experimental evidence of the QA ranking capabilities of our model. Second, we test all models on the Yahoo QA dataset which can be considered as a large web-scale dataset with a diverse range of topics which additionally includes informal social language.",null,null
320,7.1 Experimental Setup,null,null
321,"We describe the dataset used, evaluation metrics and implementation details",null,null
322,# QA Pairs # Correct,null,null
323,TRAIN 253440,null,null
324,50688,null,null
325,DEV,null,null
326,31680,null,null
327,6336,null,null
328,TEST,null,null
329,31680,null,null
330,6336,null,null
331,Table 7: Dataset Statistics of Yahoo QA Dataset.,null,null
332,"7.1.1 Dataset. e dataset we use is the Yahoo QA Dataset containing 142,627 questions and answers. We select QA pairs containing questions and best answers of length 5-50 tokens a er",null,null
333,"ltering away non-alphanumeric characters. As such, we obtain 63, 360 QA pairs in the end. e total vocabulary size |V | of this dataset is 116,900. We construct negative samples for each question by sampling 4 samples from the top 1000 hits obtained via Lucene7 search. e overall statistics of the constructed dataset is shown in Table 7. In general, we can consider Yahoo to be a much larger dataset over TREC QA. Furthermore, in CQA, the questions and answers are generally of longer length.",null,null
334,"7.1.2 Baselines and Implementation Details. For this experiment, our comparison against competitors are similar to the rst experiment. Speci cally, we compare our model with LSTM (baseline), MV-LSTM (Bilinear) and NTN-LSTM for LSTM-based deep learning models along with CNN and CNTN. In addition, we include the popular Okapi BM25 benchmark [19] as an indicator of the",null,null
335,"6h p://webscope.sandbox.yahoo.com/catalog.php?datatype,l&did,10 7h p://lucene.apache.org/core/",null,null
336,702,null,null
337,Session 6B: Conversations and Question Answering,null,null
338,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
339,"di culty of the test set. Note that our experimental results would be naturally di erent from [25] due to di erent train/test/dev splits and variations in the negative sampling process. e implementation details for LSTM based deep learning models are the same as Section 6.1.4. However, due to scalability reasons and the requirement of processing signi cantly much more QA pairs, we limit the dimensions of the LSTM and hidden layer to be 50. e number of layers of the LSTM is also set to 1. For all models, we only consider a single direction LSTM. e other hyperparameters, including the choice of pretrained word embeddings, dropout and regularization are the same unless stated otherwise.",null,null
340,7.1.3 Evaluation Metrics. For this experiment we use the same,null,null
341,"metrics as [25], namely the Precision@1 and MRR. P@1 can be",null,null
342,danednAe+diassthN1e,null,null
343,"N i ,1",null,null
344,ground,null,null
345,"(r (A+) , 1) where  truth. For the sake of",null,null
346,"is the indicator function brevity, we do not restate",null,null
347,MRR as it is already de ned in Section 6.1.2. Note that we only,null,null
348,consider the ranking of the ground truth amongst all the negative,null,null
349,samples for a given question.,null,null
350,7.2 Experimental Results,null,null
351,"Table 8 shows the results of the experiments on Yahoo QA Dataset. We show that HD-LSTM achieves state-of-the-art performance on the Yahoo QA Dataset. First, we notice that the performance of Okapi BM25 model is only marginal compared to random guessing.",null,null
352,is signi es that the testing set is indeed a di cult one.,null,null
353,Model,null,null
354,P@1 MRR,null,null
355,Random Guess Okapi BM-25 CNN CNTN LSTM NTN-LSTM HD-LSTM,null,null
356,0.2000 0.2250 0.4125 0.4654 0.4875 0.5448,null,null
357,0.5569,null,null
358,0.4570 0.4927 0.6323 0.6687 0.6829 0.7309,null,null
359,0.7347,null,null
360,Table 8: Experimental Results on Yahoo QA Dataset.,null,null
361,"Unfortunately, we were not able to obtain any results with MVLSTM due to computational restrictions. Speci cally, each training instance involves 5000 matching computations to be made. Hence, each epoch takes easily  3 hours even with GPUs. Hence, from the perspective of practical applications, we can safely eliminate the MV-LSTM as an alternative. Once again, we see the trend that tensor layer improves results over their base models similar to the earlier evaluation on the TREC QA task. However, unlike the TREC datasets, the NTN-LSTM performs signi cantly be er than the baseline LSTM probably due to the larger dataset. On the other hand, we also observe that the LSTM performs be er compared to CNN on this dataset similar to the results reported in [25]. Finally, our HD-LSTM performs the best and outperforms the NTN-LSTM despite having less parameters and being more e cient as discussed in our complexity analysis section earlier.",null,null
362,8 ANALYSIS OF HYPERPARAMETERS,null,null
363,"In this section, we discuss some important observations in our experiments. such as hidden layer size on our model. In particular, we investigate the HD-LSTM, NTN-LSTM and the baseline LSTM.",null,null
364,"Due to the lack of space, we only report the hyperparameter tuning process of the TREC QA task speci cally with respect to the MAP metric.",null,null
365,8.1 E ect of Embedding Dimension,null,null
366,Figure 3: E ect of QA Embedding Size (LSTM Dimensions).,null,null
367,"Figure 3 shows the in uence of the size of the QA embedding on MAP performance on TREC TRAIN dataset. We investigate the NTN-LSTM at three di erent k levels (the number of tensor slices). We see that NTN-LSTM outperforms LSTM and HD-LSTM when the dimensionality of QA embeddings are small. is is because the introduction of extra parameters of quadratic scale at the tensor layer helps the NTN-LSTM t the data. However, when increasing the dimensions of the sentence embedding, HD-LSTM starts to steadily outperform the NTN-LSTM. Furthermore, we note that at higher LSTM dimensions, i.e., 640, there is a steep decline in the performance of the NTN-LSTM probably due to over ing. Overall, the performance of the baseline LSTM cannot be compared to both the HD-LSTM and NTN-LSTM. Evidently, the method used to model the relationship between the embedding of text pairs is crucial and has implications on the entire network. We see that the holographic composition allows more representational freedom in the LSTM by allowing it to have larger dimensions of text representations without possible implications.",null,null
368,8.2 E ect of Hidden Layer Size,null,null
369,Figure 4: E ect of the Size of Hidden Layer on MAP.,null,null
370,"e number of nodes at the hidden layer is an important hyperparameter in our experiments due to its close proximity and direct interaction with the composition between question and answer embeddings. Note that the hidden layer size is directly related to the number of parameters of the model. In this section, we aim",null,null
371,703,null,null
372,Session 6B: Conversations and Question Answering,null,null
373,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
374,"to study the in uence of the size of the hidden layer with respect to the LSTM and HD-LSTM. Note that we are unable to directly compare with the NTN-LSTM as the tensor M in the NTN layer acts like a hidden layer. Figure 4 shows the e ect of the number of hidden nodes. Evidently, we see that a smaller hidden layer size bene ts the HD-LSTM. On the other hand, the performance of LSTM is only decent above a certain threshold of hidden layer size.",null,null
375,"is is in concert with our understandings of the interactions of the parameters with the composition layer. Our model requires less parameters to model the relationship between text pairs because the correlation between question and answer embeddings is modeled via holographic composition. We see that our model achieves good results even with a smaller hidden layer, i.e., 64. Contrarily, LSTM requires more parameters to model the relationship between the text embeddings. We see that HD-LSTM with a small hidden layer produces the best results.",null,null
376,9 CONCLUSION,null,null
377,"We proposed a novel deep learning architecture based on holographic associative memories for learning to rank QA pairs. e circular correlation of vectors has a ractive qualities such as memory e ciency and rich representational learning. Additionally, we overcome the problem of scaling QA representations while keeping the compositional parameters low which is prevalent in models that adopt a tensor layer. We also outperform many variants of deep learning architectures including the NTN-LSTM and CNTN in the task of learning to rank for question answering applications.",null,null
378,REFERENCES,null,null
379,"[1] Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu O. Mi al. 2000. Bridging the lexical chasm: statistical approaches to answer- nding. In SIGIR. 192­199. DOI: h p://dx.doi.org/10.1145/345508.345576",null,null
380,"[2] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open estion Answering with Weakly Supervised Embedding Models. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I. 165­180. DOI:h p://dx.doi.org/10.1007/978-3-662-44848-9 11",null,null
381,"[3] D. Gabor. 1969. Associative Holographic Memories. IBM J. Res. Dev. 13, 2 (March 1969), 156­ 159. DOI:h p://dx.doi.org/10.1147/rd.132.0156",null,null
382,"[4] Hua He, Kevin Gimpel, and Jimmy J Lin. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks.",null,null
383,"[5] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to estions. In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 2-4, 2010, Los Angeles, California, USA. 1011­1019.",null,null
384,"[6] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735­1780.",null,null
385,"[7] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional Neural Network Architectures for Matching Natural Language Sentences. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, ebec, Canada. 2042­2050.",null,null
386,"[8] Jiwoon Jeon, W. Bruce Cro , and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 2005 ACM CIKM International Conference on Information and Knowledge Management, Bremen, Germany, October 31 - November 5, 2005. 84­90. DOI:h p://dx.doi.org/10.1145/1099554.1099572",null,null
387,[9] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980 (2014).,null,null
388,"[10] Anh Tuan Luu, Yi Tay, Siu Cheung Hui, and See-Kiong Ng. 2016. Learning Term Embeddings for Taxonomic Relation Identi cation Using Dynamic Weighting Neural Network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016. 403­413.",null,null
389,"[11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Je rey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. 3111­3119.",null,null
390,"[12] Jonas Mueller and Aditya yagarajan. 2016. Siamese Recurrent Architectures for Learning Sentence Similarity. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 2786­2792.",null,null
391,"[13] Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. 2016. Holographic Embeddings of Knowledge Graphs. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 1955­1961.",null,null
392,"[14] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab K. Ward. 2016. Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval. IEEE/ACM Trans. Audio, Speech & Language Processing 24, 4 (2016), 694­707. DOI:h p://dx.doi.org/10.1109/TASLP. 2016.2520371",null,null
393,"[15] Ankur P. Parikh, Oscar Ta¨ckstro¨m, Dipanjan Das, and Jakob Uszkoreit. 2016. A Decomposable A ention Model for Natural Language Inference. In Proceedings of the 2016 Conference on",null,null
394,"Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016. 2249­2255. [16] Tony Plate. 1992. Holographic Recurrent Networks. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992]. 34­41. [17] Tony A. Plate. 1995. Holographic reduced representations. IEEE Trans. Neural Networks 6, 3 (1995), 623­641. DOI:h p://dx.doi.org/10.1109/72.377968 [18] Xipeng Qiu and Xuanjing Huang. 2015. Convolutional Neural Tensor Network Architecture for Community-Based estion Answering. In Proceedings of the Twenty-Fourth International Joint Conference on Arti cial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015. 1305­1311. [19] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of e ird Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994. 109­126. [20] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015. 373­382. DOI:h p://dx.doi.org/10.1145/2766462.2767738 [21] Aliaksei Severyn, Alessandro Moschi i, Manos Tsagkias, Richard Berendsen, and Maarten de Rijke. 2014. A syntax-aware re-ranker for microblog retrieval. In e 37th International",null,null
395,"ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014. 1067­1070. DOI:h p://dx.doi.org/10.1145/2600428. 2609511 [22] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In Advances in Neural",null,null
396,Information Processing Systems 26: 27th Annual Conference on Neural Information Processing,null,null
397,"Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. 926­934. [23] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Po s. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. Citeseer. [24] Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid estions from Web Collections. Computational Linguistics 37, 2 (2011), 351­383. DOI:h p://dx.doi.org/10.1162/COLI a 00051 [25] Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng. 2016. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 2835­2841. [26] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In Proceedings of the 53rd Annual Meeting of the Association",null,null
398,for Computational Linguistics and the 7th International Joint Conference on Natural Language,null,null
399,"Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 2: Short Papers. 707­712. [27] Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009. A syntactic tree matching approach to",null,null
400,nding similar questions in community-based qa services. In Proceedings of the 32nd Annual,null,null
401,"International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009. 187­194. DOI:h p://dx.doi.org/10.1145/1571941. 1571975 [28] Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and estion Answering. In COLING 2010, 23rd",null,null
402,"International Conference on Computational Linguistics, Proceedings of the Conference, 23-27 August 2010, Beijing, China. 1164­1172. [29] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint",null,null
403,"Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic. 22­32. [30] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation. In Proceedings of the",null,null
404,"39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016. 395­404. DOI:h p://dx.doi.org/10.1145/2911451. 2911498 [31] Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic Coa ention Networks For",null,null
405,"estion Answering. CoRR abs/1611.01604 (2016). [32] Xiaobing Xue, Jiwoon Jeon, and W. Bruce Cro . 2008. Retrieval models for question and",null,null
406,"answer archives. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008, Singapore, July 20-24, 2008. 475­482. DOI:h p://dx.doi.org/10.1145/1390334.1390416 [33] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Human Language Technologies:",null,null
407,"Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA. 858­867. [34] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. estion Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meet-",null,null
408,"ing of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, So a, Bulgaria, Volume 1: Long Papers. 1744­1753. [35] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. CoRR abs/1412.1632 (2014). [36] Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011. Phrase-Based Translation Model for",null,null
409,estion Retrieval in Community estion Answer Archives. In e 49th Annual Meeting of,null,null
410,"the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA. 653­662.",null,null
411,704,null,null
412,,null,null

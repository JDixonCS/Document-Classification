,sentence,label,data
0,Session 4B: Retrieval Models and Ranking 2,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Eicient Cost-Aware Cascade Ranking in Multi-Stage Retrieval,null,null
3,Ruey-Cheng Chen,null,null
4,"RMIT University Melbourne, Australia",null,null
5,Roi Blanco,null,null
6,"RMIT University Melbourne, Australia",null,null
7,ABSTRACT,null,null
8,"Complex machine learning models are now an integral part of modern, large-scale retrieval systems. However, collection size growth continues to outpace advances in eciency improvements in the learning models which achieve the highest eectiveness. In this paper, we re-examine the importance of tightly integrating feature costs into multi-stage learning-to-rank (LTR) IR systems. We present a novel approach to optimizing cascaded ranking models which can directly leverage a variety of dierent state-of-the-art LTR rankers such as LambdaMART and Gradient Boosted Decision Trees. Using our cascade model, we conclusively show that feature costs and the number of documents being re-ranked in each stage of the cascade can be balanced to maximize both eciency and eectiveness. Finally, we also demonstrate that our cascade model can easily be deployed on commonly used collections to achieve state-of-the-art eectiveness results while only using a subset of the features required by the full model.",null,null
9,1 INTRODUCTION,null,null
10,"Learning-to-Rank (LTR) systems are now commonly deployed by major search engine companies and they have been repeatedly shown to be highly eective for a variety of search related problems [6, 15, 26, 30]. ere has been a growing body of recent work which focuses on improving the eciency of multi-stage LTR systems using several dierent techniques: improving tree traversal [19], cascaded ranking [36], tree pruning [18, 38, 39], and minimizing sample sizes in stages [11, 22].",null,null
11,"In this paper we revisit the idea of cascaded ranking in order to provide more control over eciency and eectiveness tradeos in large scale search systems. A cascade ranking model [36] is a sequence of learning-to-rank models (called stages) chained together to collectively rank a set of documents for a query.e main assumption behind cascaded ranking is that full inspection of the content, which would presumably require generating expensive features is not required for every incoming document as only a small fraction of all documents will be relevant. erefore, LTR",null,null
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: hp://dx.doi.org/10.1145/3077136.3080819",null,null
13,Luke Gallagher,null,null
14,"RMIT University Melbourne, Australia",null,null
15,J. Shane Culpepper,null,null
16,"RMIT University Melbourne, Australia",null,null
17,"models in a cascade can be deployed in an ascending order of model complexity, and only a fraction of documents in each stage will advance to the next stage. Generally, early-stage rankers are cheaper to run, and usually focus on executing an early-exit strategy, such as ltering out non-relevant documents as quickly as possible. Ranking models in later stages are usually more accurate but require more resources.",null,null
18,"When discussing system performance, it is important to consider both ranking eectiveness and system throughput within the same framework. Wang et al. [36] used a modied AdaRank algorithm to incorporate the costs of individual rankers, in terms of execution time for each single-feature weak learner used in the training procedure. is cascade model, however, cannot be used with gradient-boosted tree models, which are now widely believed to be state-of-the art for web search ranking algorithms [25, 30].",null,null
19,"Conceptually, the making of a tree-based cascade model can be reasonably separated into two steps, which are cascade construction and model deployment. In the rst step, a learning algorithm takes into account the eectiveness of features and the cost of feature extraction, makes the best tradeos by following the direction from cascade designer, and automatically trains a cascade of ranking models. e learned cascade can then be deployed in the second step, focusing on optimizing low-level system performance. In this paper, we develop a new approach to constructing a cost-aware cascade. A considerable amount of recent research eort has been invested in the space of optimizing the run-time performance of gradient-boosted tree models [3, 14, 19, 20], which can be directly leveraged by our new cascading approach.",null,null
20,"Research Goals. In this work, we revisit the problem of integrating feature costs into learning-to-rank models. In particular, we focus on how best to balance feature importance and feature costs in multi-stage cascade ranking models. Our overarching goal is to devise a generic framework which can be used with any state-ofthe-art LTR algorithm, and allows more control over balancing eciency and eectiveness in the entire re-ranking process. In order to achieve these goals, we focus on two related research problems:",null,null
21,"Research estion (RQ1): When designing multi-stage retrieval systems, what approaches provide the best balance between extraction/runtime costs and feature importance when using cascaded LTR algorithms?",null,null
22,"Research estion (RQ2): Can we build multi-stage ranking models that require substantially less costs than a full cost-insensitive model, and still achieve overall eectiveness close to the full model?",null,null
23,445,null,null
24,Session 4B: Retrieval Models and Ranking 2,null,null
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
26,relevance,null,null
27,Inverted Index,null,null
28,Dynamic Features,null,null
29,d1,null,null
30,d2,null,null
31,BOW Run,null,null
32,Reorder d3,null,null
33,( ) create initial sample,null,null
34,d4 d5,null,null
35,dk,null,null
36,Learning to Rank,null,null
37,Pre-Computed Features,null,null
38,"Figure 1: A typical learning-to-rank system conguration is composed of an inverted index which is used to generate an initial candidate set (sample) of s documents. is set of documents is then re-ordered using one or more rounds of machine learning algorithms. e number of documents can be pruned in each round, or iteratively smaller subsets of the highest ranking documents in the initial sample s are re-ordered. A nal top-k set of documents are then returned from the system in relevance order.",null,null
39,2 BACKGROUND AND RELATED WORK,null,null
40,"Learning to rank. A signicant body of prior work exists in the area of learning-to-rank (LTR) [15]. e majority of research advances in LTR have focused on ways to improve the eectiveness of the systems, with several document collections released to test their performance. A recent study by Tax et al. [30] compare 87 learning to rank methods using 20 dierent test collections.",null,null
41,"However, one common problem with these test collections is that the features used by the models are oen not fully dened, making it very dicult to implement them using commonly used IR test collections. is in turn prevents easily transferring the advances made into working end-to-end search systems. While many dierent publicly available search engines [32] are commonly used by researchers and practitioners, only Terrier 4.x [23] currently supports end-to-end multi-stage retrieval on commonly used IR document collections with lile or no manual intervention. So the chasm between academic research and large search engine companies on provably good system architectures remains relatively wide. Figure 1 shows the architecture of a complete LTR system consisting of at least two stages. Every aspect of this architecture should be considered when building eective and ecient search systems. Macdonald et al. [22] were among the rst to consider all of the dierent angles when building an LTR system for adhoc search.",null,null
42,Improving Eciency in LTR Algorithms. A critical aspect of LTR must be considered when translating these powerful models,null,null
43,"into working search engines which must index internet-scale document collections ­ eciency. Eciency concerns may be strictly algorithmic [3, 7, 14, 19, 20], they may explicitly focus on feature costs [1, 6, 23, 35­37, 39], or they may perform post-learning optimizations to reduce the size of the tree ensembles [18, 38, 39].",null,null
44,"Another related line of research is to focus on the importance of balancing eciency and eectiveness in LTR systems, which is directly aligned with our current work. Perhaps the most comprehensive study on this problem is the recent work of Capannini et al. [7]. A less obvious trade-o concern is how to construct the ""sample"" of documents that are used for training and for scoring at runtime for new queries coming into the system [10, 22].is issue can have an important impact on both training and runtime scoring in multi-stage systems, and a problem that we revisit in the context of cascaded ranking. Finally, the cost of model training can also be an important problem [2, 22], but is not explored further in this work.",null,null
45,"Cascade Ranking. Raykar et al. [28] described an approach to jointly train a cascade of classiers to support clinical decision making, with the expected cost of feature acquisition taken into account. is approach does not aempt to address the issues of cascade design, such as the number of cascade stages and the design of cutos. e closest work to our own are the cascade models previously explored by Wang et al. [36] and Xu et al. [39]. Wang et al. proposed a cascade learning algorithm based on an additive ranking model AdaRank. e algorithm produces a cascade by incrementally incorporating weaker rankers in the ascending order of cost eciency. In each stage only one weak ranker is incorporated. e document scores are accumulative, so conceptually all the previously selected features are involved in the scoring. However, more recent improvements in GBRT-based LTR algorithms has made this approach less competitive than state-of-the-art learning models.",null,null
46,"Xu et al. [39] proposed an algorithm that takes a trained GBDT model and produces a cascade by reweighting the trees in the full model. e eectiveness of the cascade is roughly the same as a full GBDT model. ey use a monolithic cost function which accounts for several variables such as: model loss, tree evaluation costs, and feature cost. However, optimization of this loss function is quite complex, and their approach do not address the design issues pointed out in this paper, such as the eect of cascade structures on the nal retrieval eectiveness of the cascade model.",null,null
47,3 APPROACH,null,null
48,"Stage-wise cascades are exible models that allow for a number of architectural decisions, such as: the number of stages used, the number of documents forwarded to the next stage, and so on.e choice of features involved in each stage is a critical factor in balancing eciency and eectiveness in the end-to-end system.is trade-o is further elucidated by the following observations:",null,null
49,"· A cascade may choose to defer the use of expensive features to later cascade stages as feature extraction on fewer documents is necessary, and will be more cost ecient.",null,null
50,"· A cascade may choose to include useful features early on, since features extracted in earlier stages can be re-used in all remaining stages without incurring additional costs.e reusability of key features can make the cascade more eective.",null,null
51,446,null,null
52,Session 4B: Retrieval Models and Ranking 2,null,null
53,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
54,d1,null,null
55,"C,K,3",null,null
56,d2,null,null
57,d3,null,null
58,"C,K,1",null,null
59,dN3,null,null
60,d5,null,null
61,d6,null,null
62,dN2 d8,null,null
63,d9,null,null
64,"C,K,2",null,null
65,d10,null,null
66,d11,null,null
67,d12,null,null
68,dN1,null,null
69,Figure 2: A three level cascade which initially takes ds documents,null,null
70,"as the sample input. In Round 1, C, K ,"" 1 reorders all dN1 documents. In Round 2, a subset of the dN2 documents are reordered by C, K "","" 2. In the nal round, dN3 documents are reordered. Up to dN documents total can be returned from the nal level.""",null,null
71,"Our general approach to cascade construction is to rst assign feature sets to dierent stages using a set of predened heuristics (c.f. Sec. 4), and then perform automatic feature selection for every stage of the cascade, while jointly optimizing ranking eectiveness and eciency. Ideally, the procedure should maintain performance comparable to a complete feature set model while at the same time accounting for feature costs. We now describe a theoretical framework for model regularization that reuses well-known solutions in the machine learning eld in order to achieve both objectives. Even though the goal of regularization is to minimize the eect of overing, in this paper we show how it can also be used to produce compact models that are feature extraction cost aware.",null,null
72,3.1 Cost-Aware Feature Selection,null,null
73,"Regularization. Supervised machine learning algorithms are exposed to a training set of pairs {(xi, i )}n with the goal ofnding an approximation to the function h, mapping to x that minimizes the expected value of a predened loss function L( , h(x)) over",null,null
74,"the joint distribution of all (x, ) values:",null,null
75,"h ,"" argmin E ,x[L( , h(x))] "","" argmin Ex[E [L( , h(x))]|x]. (1)""",null,null
76,h 2H,null,null
77,h 2H,null,null
78,"e choice of loss function depends on the type of problems being learned (classication, regression, pairwise, listwise). It is common practice to incorporate a regularization term R(h) in the loss function to prevent overing. Regularization usually leads to improved eectiveness because sparsity is enforced in model training and, as a result, the learned model is less likely to overt the training set. e most common type of regularizers apply a penalty on the complexity, shrinking the value of the parameters in order to reduce overing. For instance, if h(x) ,"" wT x is a linear model with its parameters represented by a weight vector w 2 Rd , a widely-used regularizer is the L2 norm of the weight vector. Given a training set of n instances, the model would minimize the following""",null,null
79,expression:,null,null
80,Xn,null,null
81,"h ,"" argmin L( , hw (x)) + kwk22 ,""",null,null
82,(2),null,null
83,"h 2H i,1",null,null
84,"where in this case h functionally depends on w. For instance h(x) ,"" wT (x), where is a kernel feature mapping.""",null,null
85,"Cost-Aware L1 regularization. One problem with Equation 2 is that the learning algorithm is agnostic to feature costs. In order to minimize costs, one would like to reduce the number of features (covariates) that are used by the model, weighted by their cost, and at the same time maximize the performance. is problem is closely related to feature selection, and has a close connection with regularization. In fact, Equation 2 tries to bring down the contribution (weight) of each feature as much as possible while also minimizing the loss. In our case, however, having a non-zero weight for a particular feature implies that we have to pay the whole cost of extracting it, no maer how small it is.",null,null
86,"Let c 2 Rd be the feature-cost vector, in which each entry represents the normalized cost for extracting the feature. In the case of a linear model, we want to minimize cT I>0 (w), where I>0 is the component-wise indicator function, which is 1 if the weight is over zero, and 0 otherwise. is penalty factor would be included in the formulation of Equation 2. In practice, this means we need a procedure for controlling the amount of covariates included in the nal model automatically. To do this, we allow the learner to perform automatic feature selection by adding a L1 penalty to the loss function (Eq. 2). is penalty is the L1 norm of the weight vector weighted by the feature costs c.",null,null
87,"Conventionally, L1 regularized regression models with a least square loss function are also known as LASSO (least absolute shrinkage and selection operator) and were originally designed to perform covariate selection, and help to make the model more interpretable. Lasso is able to achieve this by forcing the sum of the absolute value of the regression coecients to be less than a xed value, which in practice forces certain coecients to be set to zero, eectively choosing a simpler model that does not include those coecients [31]. In our case, we will exploit this property to generate less expensive models in terms of feature extraction time.",null,null
88,"To sum up, the ranker would minimize the expression:",null,null
89,"Xn h ,"" argmin L( , hw (x)) + kwk22+ kc wk , (3)""",null,null
90,"h 2H i,1",null,null
91,where and are parameters that control the trade-o between the,null,null
92,"loss and regularization penalty, and is the component-wise prod-",null,null
93,"uct. erefore, the main idea is to learn a model using Equation 3, and then select the features that have a wi > 0, either directly in a linear model (we would use hw for ranking), or as an input to other LTR methods (which would learn a model using only the subset of",null,null
94,parameters selected). ere are several options for learning the parameters w of such,null,null
95,"a model. An eective method is to use stochastic gradient descent and update w one example at a time; in this case the training update for a sample (xj, j ) is as follows:",null,null
96,"wt +1 , wt",null,null
97,"t @ *L( @w ,",null,null
98,X,null,null
99,",hw(x)) + n",null,null
100,i,null,null
101,ci |wi |+ -,null,null
102,",",null,null
103,(4),null,null
104,447,null,null
105,Session 4B: Retrieval Models and Ranking 2,null,null
106,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
107,"where t is the learning rate, which may depend on t, the number",null,null
108,of iterations so far. Note that the L2 regularizer in Eq (3) is omied here for clarity.,null,null
109,"Strictly speaking, the L1 norm is not dierentiable (at w ,"" 0). However, methods that rely on subgradients can be used to solve""",null,null
110,"minimization problems that involve L1 regularized objective functions, which in this particular case, boils down to replacing the",null,null
111,"partial derivative of the regularizer with its sign, which for each",null,null
112,feature i results in:,null,null
113,"wit +1 , wit",null,null
114,"t @L( ,hw(x)) @wi",null,null
115,t n sign(wki ),null,null
116,(5),null,null
117,One drawback of this formulation is that it may not produce a,null,null
118,"compact model, because the weight of a feature does not become",null,null
119,"zero unless it happens to be exactly zero, which is rare in prac-",null,null
120,"tice. To overcome this limitation, we use a variant of a proximal",null,null
121,method proposed by Tsuruoka et al. [33] which works well with,null,null
122,"the Stochastic Gradient Descent (SGD) optimization procedure, and",null,null
123,has been shown empirically to produce very compact models.e,null,null
124,main motivation is to smooth out the uctuation of the gradients,null,null
125,"through multiple iterations, which can be high when using SGD as",null,null
126,"it approximates the true gradient, and is computed using the whole",null,null
127,"sample, one example at a time. e original method, named SGD-",null,null
128,"cumulative, approximates the loss gradient using the following",null,null
129,update rules:,null,null
130,"w^it +1 , wit",null,null
131,"t @L( ,hw(x))",null,null
132,",",null,null
133,@wi,null,null
134,"w,wt",null,null
135,(6),null,null
136,Xt,null,null
137,"ut ,",null,null
138,"j,",null,null
139,(7),null,null
140,"n j,1",null,null
141,qit,null,null
142,",",null,null
143,Xt,null,null
144, wi,null,null
145,j,null,null
146,+1,null,null
147,"w^ij+1 ,",null,null
148,(8),null,null
149,"j ,1",null,null
150,andnally,null,null
151,"( wit +1 ,",null,null
152,"max(0, w^i t +1 (ut + qt min(0, w^i t +1 + (ut qt",null,null
153,"1)), 1)),",null,null
154,w^i t +1 > 0 w^i t +1  0,null,null
155,(9),null,null
156,In order to introduce the per-feature,null,null
157,variable per feature as uit,null,null
158,",",null,null
159,ci,null,null
160,n,null,null
161,Pt,null,null
162,"j ,1",null,null
163,"cost ci , we j which is",null,null
164,create one then used,null,null
165,uit to,null,null
166,update wit+1. It is important to note that the method is able to select,null,null
167,a subset of features that can be used to further retrain any arbitrary,null,null
168,"model, and thus it can be used in combination with state of the art",null,null
169,non-linear rankers such as the ones commonly used in production,null,null
170,"systems (GBRT or LambdaMART for example). Henceforth, we will",null,null
171,"use a least squares loss function, and simple linear regression for h:",null,null
172,L(,null,null
173,",hw(x)) ,",null,null
174,1 2,null,null
175,wT,null,null
176, x,null,null
177,2,null,null
178,(10),null,null
179,"is proved to be empirically eective in our setup, while also",null,null
180,converging quickly in fewer epochs.,null,null
181,"ere are several alternative feature selection methods, that in",null,null
182,general are based on an optimality criteria metrics such as Bayesian,null,null
183,"information criterion, or Minimum Description Length. In this work, we also make further use of GBRT's feature importance [12],1",null,null
184,as it intrinsically captures interdependencies between covariates.,null,null
185,"In short, the process learns a set of decision trees, where each node",null,null
186,1An equivalent process exists for the case of multi-class classication.,null,null
187,"splits the data using one feature. With each split, the tree outputs are modied, and the training squared loss varies. en, once an ensemble is learned, the non-terminal nodes of the trees can be iterated through to compute the reduction of squared loss for every feature, and the results aggregated for dierent feature splits. Lastly, the nal importance is computed as the average over all of the trees.",null,null
188,3.2 Cascade Construction,null,null
189,"Constructing a cascade model involves seing a number of parameters, including the number of cascade stages K, the cuto thresholds hc1, c2, . . . , cK i, and the features sets used in each stage hF1, . . . FK i. As one might expect, the design space of a cascade model is humongous. e complexity of exploring the entire space of all possible parameter combinations and feature allocations is prohibitively large, and interdependencies between features can aect both the eectiveness and computational costs signicantly.",null,null
190,"Randomized Search. To tackle this problem, randomized search [4] is performed in this study to select the cascade conguration. is is done by randomly sampling a large number of cascade congurations from this space, followed by a seletion step that maximizes the cascade eectiveness on validation data. Ideally, this approach can explore any search space fairly eciently within a relatively small number of rounds, but when feature allocation is involved many feature combinations it explores will not be eective. Randomized search does not work well when good congurations are dicult to reach.",null,null
191,"e cost-aware L1 regularization algorithm, as described in Sec. 3.1, was developed to mitigate this issue and simplify the search. It turns the search problem in a combinatorial space (that covers all possible ways of feature allocation) into a simple line search, making it possible to ""si through"" the feature allocation space eciently by tweaking . In our formulation, the coecient controls the desired level of eectiveness-eciency tradeo, so when a dierent tradeo is given a dierent subset of features that reects this change will be selected. Practically speaking, a small leads to a gently reduced feature set with slightly decreased eectiveness compared to the full model; a large , on the other hand, will prune the feature set fairly aggressively and result in a compact model that uses only couples of features, which is ideal as an early stage model. Using this algorithm, a cascade can then be constructed by feeding in a sequence of decreasing values (from early to late) to generate cascade stages.",null,null
192,"More details about the use of randomized search will be described in later sections. In the rst two experiments, we use a set of predened cascade congurations to simplify the experimental setup and serve as the experimental control. Further investigations on ne-tuning cascade congurations is carried out using GOV2 with the best-performing cascade methods discussed in Sec. 4.3.",null,null
193,"Feature Availability. We also experimented with a number of feature availability seings, and assume that the availability of a feature may change across cascade stages. In a production search system, some features might arrive much later than the others for various reasons, such as that they are expensive to run or their generation being deferred due to the design of the feature extraction procedure. To simulate this eect, our approach is to have",null,null
194,448,null,null
195,Session 4B: Retrieval Models and Ranking 2,null,null
196,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
197,"certain models subdivide the full feature set into K equal-sized partitions and assign feature partitions to the respective cascade stages. e rationale behind this approach is that, by presenting a limited choice of features, which is 1/K of the full set, a feature extraction pipeline can be simulated to work in parallel with the ranking models, serving features in an order based on a pre-dened criteria. Each cascade stage has access to all features extracted in the previous stages without incurring additional costs. In this paper, we explored three dierent feature availability seings:",null,null
198,(1) Cost-biased allocation (C): Features are rst sorted in ascending order of unit cost and partitioned into K stages.is seing is a close approximation to the scenario where cheap features are available to the ranking model earlier than expensive ones.,null,null
199,"(2) Cost-eciency-biased allocation (E): e features arerst sorted in descending order of cost eciency and partitioned into K stages. e cost eciency of a feature is dened as the importance score divided by its unit cost, where importance is computed from a ground-truth tree model as described in Sec. 3.1. is seing simulates having a dedicated extraction pipeline for more cost-ecient features.",null,null
200,"(3) Full allocation (F): All features are accessible from individual cascade stages. is seing represents the scenario where the choice of extracted feature is unrestricted, providing the greatest exibility to the underlying cost-aware feature selection algorithm.",null,null
201,"Aer applying one of these seings, cost-aware L1 regularization is performed to each cascade stage separately with a sequence of decreasing values. e algorithm (Sec. 3.1) will reach the desired level of feature size in 10­20 epochs. Running this procedure for more iterations does not change the results. We also set a constant decaying learning rate , 0.1 across the board.",null,null
202,4 EXPERIMENTS,null,null
203,"We now evaluate the impact of our approaches on reducing costs in cascade learners in two dierent seings, one large but shallow LTR dataset, and a standard TREC benchmark with 150 queries but a large number of documents to be ranked per query.",null,null
204,Experimental Setup. All experiments were executed on a 24-,null,null
205,"core Intel Xeon E5-2630 with 256 GB of RAM hosting RedHat RHEL v7.2, and baselines generated using Indri2, Krovetz stemming,",null,null
206,and dependency models generated using Metzler's MRF congu-,null,null
207,ration3. All LTR algorithms were implemented in Python using,null,null
208,4,null,null
209,scikit-learn,null,null
210,0.18.1,null,null
211,and,null,null
212,xgboost,null,null
213,5,null,null
214,0.6a2.,null,null
215,Source,null,null
216,"code,",null,null
217,con-,null,null
218,"guration les, and detailed explanations for all experiments can",null,null
219,be found in the GitHub repository for this paper6.,null,null
220,Two dierent test collections were used for the experiments.e,null,null
221,rst collection is the C14 Webscope Yahoo Learning To Rank dataset 7 [9]. e dataset contains two subsets designed for dierent pur-,null,null
222,poses and used dierent feature sets. We use only Set 1 (Y!S1) which,null,null
223,"2 hp://www.lemurproject.org/indri.php 3 hp://ciir.cs.umass.edu/metzler/dm.pl 4 hp://scikit- learn.org/stable/ 5 hps://github.com/dmlc/xgboost 6hps://github.com/rmit-ir/LTR Cascade 7 hps://webscope.sandbox.yahoo.com/catalog.php?datatype,c",null,null
224,Table 1: Summary of the key properties of the two benchmark collections used in this study.,null,null
225,# eries # Total Docs # Features,null,null
226,Y!S1,null,null
227,"6,983",null,null
228,"165,660",null,null
229,519,null,null
230,GOV2,null,null
231,"150 1,500,000",null,null
232,425,null,null
233,"contains 519 features (out of 700 in total) with an associated feature cost, and has 19,944 training queries, 1,266 validation queries, and 3,798 test queries. e original cost estimates included with the data were used without modication in our experiments. All features used in Set 1 have extraction costs between 1 and 200. Our second collection is the TREC GOV2 test collection (GOV2) using queries 701­850 in a 5-fold cross validated conguration. We created 425 features for this collection as described next in Section 4.2. For all queries, we created the initial sample by running BM25 with k1 , 0.9 and b ,"" 0.4 to an initial depth of 5,000. A summary of the two benchmark collections are shown in Table 1.""",null,null
234,"Learning Algorithms. Table 2 summarizes all of the baselines, as well as all of the new cascade model congurations tested on the two collections. We used a broad range of dierent learning algorithms in our experiments. ese ranking models can be divided into the following three categories:",null,null
235,"(1) Ground Truth Models: We compare with ranking models executed in a non-cascade seing, where the full set of features is used in training and prediction. ree ranking models are employed: Gradient-Boosted Decision Trees (GBDT) [12], Gradient-Boosted Regression Trees (GBRT) [12], and LambdaMART [5]. Our implementations of these ranking models are based on xgboost.",null,null
236,"(2) Baselines: We use several baseline methods, such as QL [40], BM25 [29], and SDM [24], on GOV2 to verify the gain in eectiveness relative to a standard retrieval seing. ese baseline methods are however not available for Y!S1.",null,null
237,"(3) Cascade Baselines: We implemented the cascade ranking algorithm described in Wang et al. [36], using the suggested seing , 0.1. Note that seing a smaller does not improve its eectiveness. We also implemented early stopping on training eectiveness to avoid explicitly seing the number of cascade iterations.",null,null
238,"Model hyperparameters (number of trees, depth, learning rate) were trained with the provided independent validation set for Y!S1, and using 5-fold cross-validation on GOV2. For ease of experimentation, some cascade parameters, such as the number of stages K, and cuto thresholds hc1, c2, . . . , cK i, were xed in the rst two experiments. Other parameters, such as , were tuned on the validation data using randomized search. Tree cascade parameters were tuned dierently on the two datasets, as previous parameters for the ground truth models did not always generalize well on the learned cascades. We also empirically found that the linear cascades work beer with the L2 regularization turned o (i.e., ,"" 0), making the SGD optimizations more stable. Further experimental details are described in Sections 4.1 and 4.2.""",null,null
239,449,null,null
240,Session 4B: Retrieval Models and Ranking 2,null,null
241,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
242,Table 2: Summary of the baselines and new cascading methods used.,null,null
243,"Method Name GBDT-BL GBRT-BL LambdaMART-BL QL-BL, BM25-BL, SDM-BL",null,null
244,WLM-BL,null,null
245,LM-C3-X,null,null
246,GBDT-C3-X GBRT-C3-X LambdaMART-C3-X,null,null
247,"Parameters Y!S1/GOV2: 1,000/525 trees, 16/16 nodes Y!S1/GOV2: 1,000/525 trees, 16/16 nodes Y!S1/GOV2: 1,000/525 trees, 16/32 nodes Default Indri seings",null,null
248,"Y!S1: ,"" 0.1, GOV2: "", 0.1",null,null
249,"Y!S1: 1,000 trees, 16 nodes GOV2: adaptive (Sec. 4.2) Y!S1: 1,000 trees, 16 nodes GOV2: adaptive (Sec. 4.2) Y!S1: 1,000 trees, 16 nodes GOV2: adaptive (Sec. 4.2)",null,null
250,Description,null,null
251,GBDT [12] (,null,null
252,"), ,"" 0.05, subsample rate 0.8.""",null,null
253,xgboost,null,null
254,GBRT[12] (,null,null
255,"), ,"" 0.05, subsample rate 0.8.""",null,null
256,xgboost,null,null
257,LambdaMART [5] (,null,null
258,"), ,"" 0.05, subsample rate 0.8.""",null,null
259,xgboost,null,null
260,"Commonly used single-pass retrieval runs to depth 1,000 using ery likeli-",null,null
261,"hood with Dirichlet priors smoothing, BM25, and a Sequential Dependency",null,null
262,"Model (SDM). Note that while SDM is a strong eectiveness baseline, it has",null,null
263,well-known eciency limitations when used on large document collection [17].,null,null
264,"Reimplementation of the linear cascade model by Wang et al. [36], with early stopping on training NDCG.",null,null
265,"ree level cascade using linear model under the elected feature availability seing X, trained using Stochastic Gradient Descent (SGD) with batch size set to 50 and ,"" 0.1. e seing X could be C/E/F. ree level cascade using GBDT under the elected feature availability seing X, using the same SGD conguration as LM-C3-X. ree level cascade using GBRT under the elected feature availability seing X, using the same SGD conguration as LM-C3-X. ree level cascade using LambdaMART under the elected feature availability seing X, using the same SGD conguration as LM-C3-X.""",null,null
266,"Evaluation Metrics. For retrieval eectiveness, we used standard",null,null
267,"early precision evaluation metrics: Expected Reciprocal Rank (ERR),",null,null
268,"Normalized Discounted Cumulative Gain (NDCG), and Precision (P),",null,null
269,"with three cutos (5, 10, and 20). We use",null,null
270,8 to compute ERR,null,null
271,gdeval,null,null
272,"and NDCG, and",null,null
273,9 to compute the precision to ensure,null,null
274,trec eval,null,null
275,that reported numbers are easily reproducible.,null,null
276,"In this work, we focus on early precision improvements only,",null,null
277,"but if deeper metrics are desirable, our cascade approach can be",null,null
278,tuned to support it. e cost of a cascade is given by the following,null,null
279,formula:,null,null
280,1 XK X,null,null
281,N,null,null
282,"Ni C(f ),",null,null
283,"i,1 f 2Fi",null,null
284,"where C ( f ) denotes the unit cost of feature f , Ni denotes the number of documents that enter cascade level i, and N denotes the",null,null
285,total number of documents that enter the cascade.,null,null
286,4.1 Experiments on the Y!S1 Collection,null,null
287,"In the rst experiment, we tested the eectiveness of the proposed cascade ranking algorithm on the Y!S1 collection. As a signicant number of queries in this data have less than 40 retrieved documents, there is relatively lile exibility in the design of cascade stages and cutos. In our initial investigation, we chose to utilize a xed conguration to simplify the experimental design. e cascade is congured to contain only 3 stages, with xed cutos h20, 10i between stages.",null,null
288,"e values for the linear cascade models were derived using randomized search and NDCG on the validation data (cf. Table 3). For simplicity, all of the tree cascades in this experiment were trained with the same parameter seing as their ground truth counterparts. Note that tuning the number of trees/nodes in the tree",null,null
289,8 hp://trec.nist.gov/data/web/10/gdeval.pl 9hp://trec.nist.gov/trec eval/,null,null
290,"cascades can further improve the performance, and this approach is explored further in Sec. 4.2.",null,null
291,"Main Results. e main results for the Y!S1 experiments are presented in Table 3. In the table, the results are divided into three sections. From top to boom they are: ground truth models, the cascade baseline, and the proposed cascade ranking models. Ground truth models, such as GBDT-BL or GBRT-BL, provide the best effectiveness in general, but the feature extraction costs are also signicantly higher. Interestingly, these models already perform their own kind of feature selection as some of the input features are never used in the nal trees, and therefore incur dierent costs.",null,null
292,"When comparing cascading models, the cascade baseline WLMBL spends far less (0.62% of the cost incurred by GBDT-BL) on feature extraction than full models, at the cost of degraded eectiveness. Cascade models LM-C3-C, LM-C3-E, and LM-C3-F performed relatively poorly in terms of ERR@k and NDCG@k with respect to ground truth models, but in general their eectiveness is beer than the WLM-BL baseline. e tree-based cascade models are more competitive than their linear model counterparts. Among all cascading models, LambdaMART-based cascades appear to provide the best tradeo. e best-performing cascade LambdaMART-C3-F signicantly outperformed WLM-BL on all 9 tested metrics, but is still less ecient that all three ground truth models, leaving a noticeable gap of 0.01­0.02 in ERR@k, 0.04­0.05 in NDCG@k, and 0.01­0.03 in P@k.",null,null
293,4.2 Experiments on the GOV2 Collection,null,null
294,"In the second experiment, we investigate the use of the cascade ranking models on a commonly used web test collection, GOV2, where documents and features are to be processed and extracted by ourselves. To prepare the data for the cascade ranking experiment, for each query we retrieved 5,000 documents using BM25, and for each retrieved document 425 query or non-query features were",null,null
295,450,null,null
296,Session 4B: Retrieval Models and Ranking 2,null,null
297,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
298,"Table 3: Main results on Yahoo! Learning-to-Rank Challenge data. For the proposed cascade models, signicant improvements over WLM-BL are indicated by * for p < 0.05 and ** for p < 0.01 in a paired t-test.",null,null
299,ERR@k,null,null
300,System,null,null
301,@5 @10 @20,null,null
302,Ground Truth Models,null,null
303,GBDT-BL GBRT-BL LambdaMART-BL,null,null
304,0.4605 0.4751 0.4598 0.4744 0.4526 0.4674,null,null
305,Cascade Models (including Baseline) a,null,null
306,0.4789 0.4782 0.4712,null,null
307,WLM-BL,null,null
308,0.3679 0.3876 0.3933,null,null
309,LM-C3-C LM-C3-E LM-C3-F GBDT-C3-C GBDT-C3-E GBDT-C3-F GBRT-C3-C GBRT-C3-E GBRT-C3-F LambdaMART-C3-C LambdaMART-C3-E LambdaMART-C3-F,null,null
310,0.3950 0.3871 0.3876 0.4191 0.4264 0.4178 0.4025 0.4100 0.4158 0.4163 0.4183 0.4353,null,null
311,0.4127 0.4039 0.4047 0.4357 0.4419 0.4350 0.4203 0.4260 0.4332 0.4332 0.4346 0.4513,null,null
312,0.4175 0.4089 0.4093 0.4405 0.4466 0.4395 0.4254 0.4313 0.4378 0.4379 0.4394 0.4557,null,null
313,NDCG@k @5 @10 @20,null,null
314,0.7448 0.7872 0.8279 0.7420 0.7852 0.8264 0.7314 0.7768 0.8203,null,null
315,0.5886 0.6506 0.7088,null,null
316,0.6461 0.6503 0.6541 0.6535 0.6721 0.6554 0.6304 0.6380 0.6479 0.6577 0.6629 0.6847,null,null
317,0.7067 0.7033 0.7113 0.7100 0.7180 0.7163 0.6931 0.6867 0.7094 0.7145 0.7133 0.7354,null,null
318,0.7638 0.7618 0.7666 0.7631 0.7703 0.7672 0.7488 0.7431 0.7612 0.7673 0.7671 0.7851,null,null
319,@5,null,null
320,0.8323 0.8322 0.8330,null,null
321,0.7832 0.8086 0.8192 0.8226 0.7878 0.7942 0.7866 0.7743 0.7697 0.7862 0.7994 0.7968 0.8060,null,null
322,P@k @10,null,null
323,0.7577 0.7562 0.7564,null,null
324,0.7171 0.7364 0.7413 0.7483 0.7245 0.7241 0.7310 0.7168 0.7009 0.7294 0.7328 0.7268 0.7379,null,null
325,@20,null,null
326,0.5967 0.5962 0.5964,null,null
327,0.5673 0.5856 0.5885 0.5915 0.5781 0.5778 0.5819 0.5737 0.5637 0.5802 0.5820 0.5786 0.5847,null,null
328,Cost,null,null
329,15988 15876 15856,null,null
330,99,null,null
331,1871 1580 5278 1760 1535 4953 1760 1535 4949 1760 1535 4929,null,null
332,"aAll -C models set values h100000, 30000, 500i, -E models use h8000, 8000, 3000i, and -F models use h5000, 800, 300i.",null,null
333,"extracted. All 425 of the features implemented depend on either: the query; the query and term statistics from the indexed postings; the query, document and bigram statistics from ephemeral postings; the query and the document; or, the document. Table 5 shows a summary of these features. e majority of these features were derived from prior work within the LTR literature [15, 22, 23].",null,null
334,"LTR Features. For all experiments, with GOV2, a total of 425 features were used. For each feature, several timing experiments were ran to compute the relative feature costs. We then normalized the costs based on the cheapest and most expensive features used in the experiments. Table 5 shows the complete feature breakdown based on the two main categories of features used.",null,null
335,"e rst set of features are a large collection of pre-retrieval features commonly used for predicting query diculty [8], and more recently within LTR [21, 34] were gathered. ese features draw on statistical information contained within the query alone or on simple scoring methods that require postings list access. As such they are reasonably ecient to compute on-the-y at query time. e most important point about these features is that they are query specic, but must only be computed once using pre-computed unigram scores. is makes it relatively dicult to properly account for their true costs as LTR systems use SVM formaed inputles, which implicitly have a per document feature cost in the model. erefore, we divide these one-o pre-retrieval feature costs by the number of documents produced in the initial retrieval stage, resulting in an amortized unit cost of 1. All other costs are computed relative to this cost.",null,null
336,"e second set of features are per document costs. While the cost of a single Document Prior lookup is very fast in practice, it must be done for every document in the current stage, and therefore more expensive than the one-o cost of the aggregate pre-retrieval feature scores. Likewise, all models incorporating bigrams are more expensive than their unigram counterparts. e bigram costs include a one-o cost to generate an ephemeral posting for the bigram [16], that can be reused to compute all of the bigram preretrieval features, and also used on the y for per document bigram scoring. is amortized cost is reected in the nal unit costs used for our experiments. Alternative indexing approaches [13, 27] have been proposed to improve the eciency of n-gram scoring in recent years, but feature-specic performance enhancements are beyond the scope of this work.",null,null
337,"Due to space constraints, we cannot describe all of the features or costs. A detailed description for all of the features as well as how costs (both estimated and real) can be found in the GitHub repository for the paper. e main point we want to make is that Table 5 provides realistic relative costs for both one-o and perdocument features, and takes into account the relative complexity of each.",null,null
338,"Main Results. In our initial investigation, the cascade is congured to 3 stages with cutos h1000, 100i. In this basic conguration, the cuto thresholds are selected from widely used cutovalues in adhoc retrieval experiments. e experiment is conducted in a 5-fold cross-validated seing, so xing the cascade conguration can considerably speed up the search, with the caveat of achieving",null,null
339,451,null,null
340,Session 4B: Retrieval Models and Ranking 2,null,null
341,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
342,"Table 4: Main results on the GOV2 collection using 5-fold cross validation. For the proposed cascade models, signicant improvements over QL-BL/WLM-BL are indicated by */ for p < 0.05 (**/ for p < 0.01) in a paired t-test.",null,null
343,ERR@k,null,null
344,System,null,null
345,@5,null,null
346,@10,null,null
347,@20,null,null
348,Baseline Bag-of-Words and Term Dependency Models,null,null
349,QL-BL BM25-BL SDM-BL,null,null
350,0.3937 0.3781 0.4453,null,null
351,0.4131 0.3980 0.4632,null,null
352,0.4218 0.4062 0.4702,null,null
353,Ground Truth LTR Models,null,null
354,GBDT-BL GBRT-BL LambdaMART-BL,null,null
355,0.4361 0.4501 0.4590,null,null
356,0.4590 0.4678 0.4802,null,null
357,0.4652 0.4745 0.4849,null,null
358,Cascade Models (cost  5000),null,null
359,WLM-BL,null,null
360,LM-C3-C LM-C3-E LM-C3-F,null,null
361,0.4221,null,null
362,0.4297 0.4298 0.4366,null,null
363,0.4422,null,null
364,0.4454 0.4465 0.4537,null,null
365,0.4485,null,null
366,0.4537 0.4545 0.4608,null,null
367,Cascade Models (cost  1/2 full model cost),null,null
368,LM-C3-F,null,null
369,0.4332 0.4508,null,null
370,LambdaMART-C3-F a 0.4396 0.4578,null,null
371,"LM-C3-F, adaptive b 0.4295 0.4469",null,null
372,0.4566 0.4647 0.4530,null,null
373,@5,null,null
374,0.3839 0.3796 0.4396,null,null
375,0.4441 0.4546 0.4684,null,null
376,0.4204 0.4453 0.4418 0.4435,null,null
377,0.4419 0.4373 0.4435,null,null
378,NDCG@k,null,null
379,@10,null,null
380,@20,null,null
381,0.3826 0.3806 0.4346,null,null
382,0.3950 0.3814 0.4345,null,null
383,0.4473 0.4446 0.4692,null,null
384,0.4411 0.4345 0.4593,null,null
385,0.4177,null,null
386,0.4328 0.4315 0.4440,null,null
387,0.4132,null,null
388,0.4314 0.4294 0.4509,null,null
389,0.4452 0.4333 0.4492,null,null
390,0.4442 0.4208,null,null
391,0.4501,null,null
392,@5,null,null
393,0.5275 0.5114 0.6013,null,null
394,0.6255 0.6161 0.6470,null,null
395,0.5919 0.5933 0.5946 0.6161,null,null
396,0.6174 0.6094 0.6242,null,null
397,P@k @10,null,null
398,0.5007 0.4893 0.5711,null,null
399,0.6027 0.5805 0.6215,null,null
400,0.5664 0.5624 0.5624 0.5779,null,null
401,0.5872 0.5732 0.5926,null,null
402,@20,null,null
403,0.5000 0.4705 0.5443,null,null
404,0.5487 0.5305 0.5641,null,null
405,0.5242 0.5312 0.5285 0.5601,null,null
406,0.5517 0.5181 0.5611,null,null
407,Cost,null,null
408,­ ­ (High),null,null
409,213683 211640 213482,null,null
410,1249 4013,null,null
411,11 4717,null,null
412,145693 129529 110473,null,null
413,"a With 650 trees and 32 nodes b With values h800, 0.1, 0.05i and cutos h2500, 700i",null,null
414,Table 5: Summary of all features used in this work.,null,null
415,Description,null,null
416,Unit Cost # Features,null,null
417,Pre-Retrieval Features,null,null
418,ery Dependent (Unigram),null,null
419,1,null,null
420,159,null,null
421,ery Dependent (Bigram),null,null
422,100,null,null
423,147,null,null
424,Document Dependent Features,null,null
425,Stage 0 Score Static Document Priors Score (Unigram) Score (Bigram),null,null
426,1,null,null
427,1,null,null
428,500,null,null
429,9,null,null
430,"2,000",null,null
431,107,null,null
432,"8,000",null,null
433,2,null,null
434,Total,null,null
435,425,null,null
436,"limited improvement on retrieval eectiveness. is issue is briey investigated in this experiment by including a run that also optimizes the cuto thresholds. In Sec 4.3, we explore various cascade congurations and investigate the eect of cascade parameters on retrieval eectiveness.",null,null
437,"Using a randomized search-based approach, the cascade models LM-C3-C, LM-C3-E, and LM-C3-F are selected by maximizing the unbounded NDCG score on the validation folds. In a 5-fold cross validated seing, this metric is averaged across 5 folds on the respective validation sets. e unbounded NDCG is not specic to any cuto threshold, so essentially it can be used to optimize any",null,null
438,cascade stage. Similarly behaved recall-oriented metrics (such as Mean Average Precision) could also be used.,null,null
439,"e main results for GOV2 are presented in Table 4. In contrast to the Y!S1 collection in the previous experiment, more than 72% of the features used on GOV2 are query dependent pre-retrieval features. e presence of query specic features poses a serious challenge to all cascade models. ery features are usually cheaper to compute, and more likely to be selected (by cost-biased strategy, for example) in early cascade stages. GBDT and LambdaMART can eectively use these query features, but for other ranking models the query features are not as useful. As a result, cascading models that do not eectively utilize query features oen see reduced eectiveness in the early cascade stages.",null,null
440,"Ground truth models, as expected, give the best eectiveness among all baselines but also incurred the most feature extraction cost, around 210, 000­220, 000 unit cost. When compared with GBDT-BL and GBRT-BL, LambdaMART-BL achieves the best eectiveness. e cascade baseline WLM-BL spends far less on feature extraction, requiring only 0.58% of the full model cost, but at the cost of eectiveness.",null,null
441,"A range of cascade models that spend less than 1/20 of the full model cost are rst selected using the LM-C3-C, LM-C3-E, and LMC3-F approaches. All three linear cascades outperform the WLM-BL baseline in nearly all metrics with the exception of P@10. Compared to WLM-BL, LM-C3-F signicantly improves NDCG@20 by 0.037, and P@20 by 0.035, spending three times more on feature extraction. All three selected models behave dierently than in the",null,null
442,452,null,null
443,Session 4B: Retrieval Models and Ranking 2,null,null
444,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
445,ERR@20 NDCG@20,null,null
446,P@20,null,null
447,0.47,null,null
448,0.43,null,null
449,0.39,null,null
450,SDM,null,null
451,BM25,null,null
452,1K,null,null
453,10K,null,null
454,Unit Cost,null,null
455,2 stage,null,null
456, 3 stage,null,null
457,4 stage,null,null
458,5 stage,null,null
459,100K,null,null
460,(a) ERR@20,null,null
461,0.46,null,null
462,0.43,null,null
463,0.40,null,null
464,0.37,null,null
465,SDM ,null,null
466,2 stage ,null,null
467,BM25,null,null
468,  3 stage,null,null
469,4 stage,null,null
470,5 stage,null,null
471,1K,null,null
472,10K,null,null
473,100K,null,null
474,Unit Cost,null,null
475,(b) NDCG@20,null,null
476,0.55,null,null
477,0.50,null,null
478,0.45,null,null
479,SDM ,null,null
480,2 stage BM25,null,null
481,3 stage   ,null,null
482,4 stage,null,null
483,5 stage,null,null
484,1K,null,null
485,10K,null,null
486,100K,null,null
487,Unit Cost,null,null
488,(c) P@20,null,null
489,"Figure 3: Eectiveness versus Cascade Cost in the GOV2 collection using the LM-C*-F models. e solid line at the boom represents the eectiveness of a BM25 BOW run, the doed line is a Sequential Dependency Model run which represents a competitive baseline on the collection, and the dots represent dierent LTR congurations and their respective trade-os. Based on the validation data, the highlighted dots in black signify the most eective runs overall, while the highlighted dots in blue are the best cost-eective runs.",null,null
490,"previous experiment. Both LM-C3-F and LM-C3-C are of comparable costs roughly in the range of 4, 000­4, 800. LM-C3-E tends to select extremely compact feature sets and results in a greatly reduced cascade model that uses only the cheapest features. In general, the order of the three models in terms of eectiveness (in descending order) is LM-C3-F, LM-C3-E, and LM-C3-C, despite the fact that LM-C3-E actually costs much less than LM-C3-C. Other cascading models which require 1/2 of the full model cost are also shown in the table. None of the congurations from LM-C3-C and LM-C3E fall into this range. e best-scoring LM-C3-F model (in terms of validation set NDCG) achieves comparable performance to the same model selected in the previous group, but requires much more feature extraction resources. A LambdaMART-C3-F model trained by ing the selected feature sets in LM-C3-F does slightly better on ERR@k but sees degraded performance on NDCG@k and P@k. Note that, unlike the Y!S1 experiment, the parameters used in training LambdaMART-BL do not generalize over LambdaMARTC3-F. Another round of randomized search is needed to nd the conguration that maximizes the tradeo.",null,null
491,"Finally, a LM-C3-F run that simultaneously optimizes the values and cuto thresholds is also presented. is model is generally the most eective cascade model in terms of NDCG@k and P@k. It signicantly outperforms the WLM-BL model on P@10 by 0.03, on P@20 by 0.035, and on NDCG@20 by 0.035. is result suggests that jointly optimizing feature allocation and cascade conguration can lead to further improvements. is issue is investigated further in the next experiment.",null,null
492,4.3 Eect of Cascade Conguration,null,null
493,"In the third experiment, we investigate the eect of cascade congurations on retrieval eectiveness and cascade cost. We relax two variables that were held xed in the previous experiments ­ the number of cascade stages, K, and the cutos hc1, c2, . . . , cK i ­ and jointly optimize these parameters together with values in a combined random search-based framework. As these congurations are more expensive to tune, the exploration was deferred until the inuence of other variables was beer understood. is experiment",null,null
494,was carried out using the GOV2 collection. Our exploration starts by executing a full-range randomized search over the entire cascade design space.,null,null
495,"We used predened grids of each variable to ensure that the explored data points were not too densely packed 10. We then iterated from K ,"" 2 to 5, which indicates the number of cascade stages, and for each seing of K, sampled a set of feasible cutos and values from the aforementioned seings. In the experiment, each seing of K produced more than 200 congurations.""",null,null
496,"e eectiveness versus cascade cost for each explored combination were then ploed and shown in Figure 3, in which points from dierent seings of K are ploed in dierent colors and shapes. For each K seing, the best conguration (with cost < 1/2 full model cost) found by using NDCG validation is ploed as a black dot. ese ""best"" congurations are summarized in Table 6.",null,null
497,"Figure 3 shows that a wide range of low cost but eective models can be found regardless of the choice of K. For ERR@20, two stage cascades are oen quite eective, but can also be among the most expensive. For NDCG@20 and P@20, three stage cascades consistently provided the most eective congurations. Among all seings of K, three level cascades consistently provided the best trade-o between eectiveness and eciency. We intend to investigate these trade-os further in future work.",null,null
498,5 CONCLUSION,null,null
499,"In this work, we have presented a new approach to cascaded ranking which can be used with any commonly used LTR algorithms. We make direct comparisons to several state-of-the-art approaches, and conclusively show that our approach can consistently achieve beer trade-os than other cascade ranking systems such as WLMBL. In the experiments, we have presented several eective feature allocation strategies that have not previously been explored, and are the rst to directly explore the relationship between the number",null,null
500,"10e range of searched was {0.01, 0.03, 0.05, 0.08, 0.1, 0.3, 0.5, 0.8, 1, 3, 5, 8, 10, 30, 50, 80, 100, 300, 500, 800 }; the range of the cuto threshold is the union of the three sets: {20, 30, . . . , 100}, {100, 200, . . . , 1000}, and {2000, 2500, 3000, . . . , 5000}.",null,null
501,453,null,null
502,Session 4B: Retrieval Models and Ranking 2,null,null
503,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
504,"Table 6: e best conguration for the K-stage LM-C3-F cascade (for K ,"" 2, 3, 4, 5) found by maximizing the unbounded NDCG on the validation data. Signicant improvements over WLM-BL are indicated by */** for p < 0.05/p < 0.01 in a paired t-test.""",null,null
505,System ( values; cutos),null,null
506,"WLM-BL h800, 0.01i; h400i h800, 0.1, 0.05i; h2500, 700i h500, 10, 0.03, 0.01i; h3000, 2000, 700i h800, 0.5, 0.1, 0.08, 0.05i; h2000, 800, 500, 80i",null,null
507,@5,null,null
508,0.4204,null,null
509,0.4529 0.4435 0.4446 0.4343,null,null
510,NDCG@k,null,null
511,@10,null,null
512,@20,null,null
513,0.4177,null,null
514,0.4511 0.4492 0.4446 0.4340,null,null
515,0.4132 0.4476 0.4501 0.4435 0.4408,null,null
516,@5,null,null
517,0.5919,null,null
518,0.6094 0.6242 0.6161 0.6040,null,null
519,P@k,null,null
520,@10,null,null
521,0.5664,null,null
522,0.5866 0.5926 0.5913 0.5691,null,null
523,@20,null,null
524,0.5242 0.5517 0.5611 0.5530 0.5466,null,null
525,Cost,null,null
526,1249 18297 110472 106465 80612,null,null
527,of cascades stages and document sample sizes on performance trade-os.,null,null
528,"In future work we wish to more closely explore the relationship between feature costs and feature importance weighting at dierent levels of the cascade. Our current approach to parameter selection is largely empirical, and is quite costly when using hundreds of features and large scale document collections, resulting in several strong Linear Models, which are currently needed before generalizing to gradient boosted tree models. erefore, an appealing next step in this work is to nd more principled approaches to dynamically select the best cascade conguration on a per-query basis, and to further explore the best congurations for a wider variety of LTR ranking algorithms",null,null
529,Funding Statement. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP140103256 and DP170102231).,null,null
530,REFERENCES,null,null
531,"[1] N. Asadi and J. Lin. 2013. Document Vector Representations for Feature Extraction in Multi-Stage Document Ranking. Inf. Retr. 16, 6 (2013), 747­768.",null,null
532,[2] N. Asadi and J. Lin. 2013. Training ecient tree-based models for document ranking. In Proc. ECIR. 146­157.,null,null
533,"[3] N. Asadi, J. Lin, and A. P. De Vries. 2014. Runtime optimizations for tree-based machine learning models. Trans. on Know. and Data Eng. 26, 9 (2014), 2281­2292.",null,null
534,"[4] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research 13, Feb (2012), 281­305.",null,null
535,"[5] C. Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.",null,null
536,"[6] B. B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. 2010. Early Exit Optimizations for Additive Machine Learned Ranking Systems.. In Proc. WSDM. 411­420.",null,null
537,"[7] G. Capannini, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, and N. Tonelloo. 2016. ality versus eciency in document scoring with learning-to-rank models. Inf. Proc. & Man. 52, 6 (2016), 1161­1177.",null,null
538,[8] D. Carmel and E. Yom-Tov. 2010. Estimating the ery Diculty for Information Retrieval. Morgan & Claypool.,null,null
539,"[9] O. Chapelle and Y. Chang. 2011. Yahoo! Learning to Rank Challenge Overview. 14 (2011), 1­24.",null,null
540,"[10] C. L. A. Clarke, J. S. Culpepper, and A. Moat. 2016. Assessing eciency­ eectiveness tradeos in multi-stage retrieval systems without using relevance judgments. Inf. Retr. 19, 4 (2016), 351­377.",null,null
541,"[11] J. S. Culpepper, C. L. A. Clarke, and J. Lin. 2016. Dynamic Cuto Prediction in Multi-Stage Retrieval Systems. In Proc. ADCS. 17­24.",null,null
542,"[12] J. Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001), 1189­1232.",null,null
543,"[13] S. Huston, J. S. Culpepper, and W. B. Cro. 2014. Indexing Word-Sequences for Ranked Retrieval. ACM Trans. Information Systems 32, 1 (2014), 3.1­3.26.",null,null
544,"[14] X. Jin, T. Yang, and X. Tang. 2016. A Comparison of Cache Blocking Methods for Fast Execution of Ensemble-based Score Computation. In Proc. SIGIR. 629­638.",null,null
545,"[15] T.-Y. Liu. 2009. Learning to Rank for Information Retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009), 225­331.",null,null
546,"[16] X. Lu, A. Moat, and J. S. Culpepper. 2015. On the Cost of Extracting Proximity Features for Term-Dependency Models. In Proc. CIKM. 293­302.",null,null
547,"[17] X. Lu, A. Moat, and J. S. Culpepper. 2016. Ecient and Eective Higher Order Proximity Modeling. In Proc. ICTIR. 21­30.",null,null
548,"[18] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, F. Silvestri, and S. Trani. 2016. Post-learning optimization of tree ensembles for ecient ranking. In Proc. SIGIR. 949­952.",null,null
549,"[19] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonelloo, and R. Ven-",null,null
550,"turini. 2015. ickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees. In Proc. SIGIR. 73­82. [20] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonelloo, and R. Venturini.",null,null
551,"2016. Exploiting CPU SIMD extensions to speed-up document scoring with tree ensembles. In Proc. SIGIR. 833­836. [21] C. Macdonald, R. L. T. Santos, and I. Ounis. 2012. On the Usefulness of ery Features for Learning to Rank. In Proc. CIKM. 2559­2562. [22] C. Macdonald, R. L. T. Santos, and I. Ounis. 2013. e whens and hows of learning to rank for web search. Inf. Retr. 16, 5 (2013), 584­628. [23] C. Macdonald, R. L. T. Santos, I. Ounis, and B. He. 2013. About learning models with multiple query-dependent features. ACM Trans. Information Systems 31, 3 (2013), 11:1­11:39.",null,null
552,[24] D. Metzler and W. B. Cro. 2005. A Markov random eld model for term dependencies.. In Proc. SIGIR. 472­479.,null,null
553,"[25] A. Mohan, Z. Chen, and K. Q. Weinberger. 2011. Web-Search Ranking with Initialized Gradient Boosted Regression Trees. Journal of Machine Learning Research 14 (2011), 77­89.",null,null
554,"[26] J. Pedersen. 2010. ery understanding at Bing. Invited talk, SIGIR (2010). [27] M. Petri, A. Moat, and J. S. Culpepper. 2014. Score-safe term dependency",null,null
555,"processing with hybrid indexes. In Proc. SIGIR. 899­902. [28] V. C. Raykar, B. Krishnapuram, and S. Yu. 2010. Designing ecient cascaded",null,null
556,"classiers: tradeo between accuracy and cost. In Proc. KDD. 853­860. [29] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994.",null,null
557,"Okapi at TREC-3.. In Proc. TREC-3. [30] N. Tax, S. Bockting, and D. Hiemstra. 2015. A cross-benchmark comparison of",null,null
558,"87 learning to rank methods. Inf. Proc. & Man. 51, 6 (2015), 757­772. [31] R. Tibshirani. 1994. Regression Shrinkage and Selection Via the Lasso. Journal",null,null
559,"of the Royal Statistical Society, Series B 58 (1994), 267­288. [32] A. Trotman, C. L. A. Clarke, I. Ounis, J. S. Culpepper, M.-A. Cartright, and S. Geva.",null,null
560,"2012. Open source information retrieval: a report on the SIGIR 2012 workshop. SIGIR Forum 46, 2 (2012), 95­101. [33] Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty. In Proc. ACL. 477­485. [34] S. Tyree, K. Q. Weinberger, K. Agrawal, and J. Paykin. 2011. Parallel Boosted Regression Trees for Web Search Ranking. In Proc. WWW. 387­396. [35] L. Wang, J. Lin, and D. Metzler. 2010. Learning to eciently rank. In Proc. SIGIR. 138­145.",null,null
561,"[36] L. Wang, J. Lin, and D. Metzler. 2011. A Cascade Ranking Model for Ecient Ranked Retrieval. In Proc. SIGIR. 105­114.",null,null
562,"[37] L. Wang, J. Lin, D. Metzler, and J. Han. 2014. Learning to eciently rank on big data. In Proc. WWW (Companion Volume). 209­210.",null,null
563,"[38] Z. Xu, M. J. Kusner, K. Q. Weinberger, and M. Chen. 2013. Cost-Sensitive Tree of Classiers.. In Proc. ICML. 133­141.",null,null
564,"[39] Z. Xu, M. J. Kusner, K. Q. Weinberger, M. Chen, and O. Chapelle. 2014. Classier Cascades and Trees for Minimizing Feature Evaluation Cost. Journal of Machine Learning Research 15 (2014), 2113­2144.",null,null
565,"[40] C. Zhai and J. Laerty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Information Systems 22, 2 (April 2004), 179­214.",null,null
566,454,null,null
567,,null,null

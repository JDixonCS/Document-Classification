,sentence,label,data
0,Short Resource Papers,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,A Cross-Platform Collection for Contextual Suggestion,null,null
3,"Mohammad Aliannejadi, Ida Mele, and Fabio Crestani",null,null
4,"Faculty of Informatics, Universita` della Svizzera italiana (USI) Lugano, Switzerland",null,null
5,"{mohammad.alian.nejadi,ida.mele,fabio.crestani}@usi.ch",null,null
6,ABSTRACT,null,null
7,"Suggesting personalized venues helps users to nd interesting places on location-based social networks (LBSNs). Although there are many LBSNs online, none of them is known to have thorough information about all venues. e Contextual Suggestion track at TREC aimed at providing a collection consisting of places as well as user context to enable researchers to examine and compare di erent approaches, under the same evaluation se ing. However, the o cially released collection of the track did not meet many participants' needs related to venue content, online reviews, and user context. at is why almost all successful systems chose to crawl information from di erent LBSNs. For example, one of the best proposed systems in the TREC 2016 Contextual Suggestion track crawled data from multiple LBSNs and enriched it with venuecontext appropriateness ratings, collected using a crowdsourcing platform. Such collection enabled the system to be er predict a venue's appropriateness to a given user's context. In this paper, we release both collections that were used by the system above. We believe that these datasets give other researchers the opportunity to compare their approaches with the top systems in the track. Also, it provides the opportunity to explore di erent methods to predicting contextually appropriate venues.",null,null
8,CCS CONCEPTS,null,null
9,·Information systems Test collections; Recommender systems;,null,null
10,KEYWORDS,null,null
11,Collection; Venue Suggestion; Context-Awareness,null,null
12,1 INTRODUCTION,null,null
13,"Location-based social networks (LBSNs), such as Yelp, TripAdvisor, and Foursquare, allow users to share check-in data using their mobile devices. Such platforms also collect valuable information about users' mobility records such as check-in data and users' feedback (e.g., ratings, tags, and reviews). One important related task consists in suggesting personalized venues to a user who is exploring a new venue or visiting a new city [3]. e Contextual Suggestion track in the Text REtrieval Conference (TREC) aimed at providing a standard evaluation setup for researchers in which they can compare",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11,2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080752",null,null
15,"their proposed approaches [5]. For each user, the participants had to produce a ranked list of venues to recommend to a user visiting a new city. ey were given the users' history of preferences expressed in one or two previously visited cities (30-60 venues per user) and had to consider both preferences and context of the users for making their suggestions.",null,null
16,"In the last two years of the track, the organizers a empted to create reusable datasets. Consequently, the track consisted of two phases. Phase 11: participants were free to suggest any venue in the target cities as long as they existed in a reference venue collection (see Table 1). Phase 22: participants had to rerank a given list of venues in the target city for each user. erefore, the coordinators were able to release the ground truth for Phase 2, that is an essential step toward making a collection reusable.",null,null
17,"In spite of such a empts, there are still some drawbacks with the current collection that can limit its reusability. First, even though the organizers released a crawl of the collection in 2016, it is unstructured and does not introduce a homogeneous set of data (see Table 1). Hence most top-ranked systems ignored it and crawled their collections. Second, the collection has a set of contextual data such as type and duration of the trip. However, the contextual data was neglected by most of the participants. In particular, many of them just ignored the contextual information or used it with handcra ed rule-based methods. It could be due to the current structure of the collection which does not give the researchers many options concerning context-aware recommendation.",null,null
18,"In an a empt to address these limitations, we present a collection that was crawled very carefully to be homogeneous, cross platform, and context aware. More speci cally, we release the collection that we used for our participation at the TREC Contextual Suggestion track performing best in both phases of the track. e collection was crawled from two major LBSNs: Foursquare3 and Yelp4. We searched for the venues present in the TREC dataset on the LBSNs to nd their corresponding pro les and veri ed the retrieved data very carefully to prevent adding any noise to the dataset. It is worth noting that more than half of the submi ed systems to TREC 2016 had crawled data from either Yelp or Foursquare or both. More speci cally, we observed that for 12 tasks, namely, the last 2 years with 2 phases each and taking into account the top 3 systems, 11/12 (,92%) of the systems had crawled data from one or both sources and 7/12 (,""58%) of the systems crawled data from more than one LBSN. Hence, it is clear that there is the need for a uni ed, comprehensive dataset of this information which is available publicly. As for the contextual information, we created a secondary dataset""",null,null
19,1Phase 1 is the 2016 equivalent of Live Experiment in 2015. For simplicity we refer to both of them as Phase 1. 2Phase 2 is the 2016 equivalent of Batch Experiment in 2015. For simplicity we refer to both of them as Phase 2. 3h p://www.foursquare.com 4h p://www.yelp.com,null,null
20,1269,null,null
21,Short Resource Papers,null,null
22,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
23,Table 1: Sample rows from the collection of venues used in TREC Contextual Suggestion 2015 and 2016.,null,null
24,ID,null,null
25,TRECCS-00001768-394 TRECCS-00001776-394 TRECCS-00001777-394 TRECCS-00001927-182 TRECCS-00001934-182,null,null
26,Context,null,null
27,394 394 394 182 182,null,null
28,URL,null,null
29,h p://shop.hobbylobby.com h p://dominos.com h p://www.arbys.com h p://www.milwaukeepublicmarket.org h p://www.botanasrestaurant.com,null,null
30,Title,null,null
31,Hobby Lobby Dominos Pizza Arbys Milwaukee Public Market Botanas Restaurant,null,null
32,of contextual information that enables researchers to investigate di erent approaches to predict the contextual appropriateness of venues and study the in uence of context on user behavior.,null,null
33,e released collection5 comprises:,null,null
34,· more than 300K crawled venues from Foursquare providing enough information for venue recommendation research;,null,null
35,· more than 15K crawled venues from Foursquare and Yelp providing information for cross-platform recommendation;,null,null
36,· a human-annotated contextual appropriateness dataset containing human-tailored features for almost 2K context example features.,null,null
37,e release of this new collection will provide researchers with a unique opportunity to develop context-aware venue recommender systems under the same se ing and data as the one of the bestsubmi ed systems in the TREC 2016. is will enable them to compare their work with state-of-the-art approaches and explore the brand new venue-context appropriateness dataset.,null,null
38,"e remainder of the paper is organized as follows: Section 2 brie y describes the TREC Contextual Suggestion track. Section 3 describes the method we used for the dataset construction. In Section 4 we provide more details on the dataset. Finally, Section 5 concludes the paper.",null,null
39,2 TREC CONTEXTUAL SUGGESTION TRACK,null,null
40,"In 2012 TREC introduced the task of Contextual Suggestion6 which provided a common evaluation framework for participants who wanted to deal with the challenging problem of improving contextual suggestions. More in details, given a set of example venues as user's preferences (pro le) and some contextual information, such as geographical, temporal, and personal contexts, the task consisted in returning a ranked list of candidate venues ing the user's pro le and context.",null,null
41,"e crawled dataset of this paper covers the places that were part of the last two years of the track, that is 2015 and 2016. For 2015 the dataset includes the venues for Phase 2, while for 2016 the dataset covers the venues for both Phase 1 and Phase 2. e additional contextual appropriateness dataset can also be used for 2015 and 2016 collections. In these tasks, there is a number of users, and for each user there is a list of 30 to 60 venues that a particular user has previously rated. Additionally, there is a set of contextual information for each user. Given such information, the task is to return a ranked list of candidate suggestions of venues based on their relevance to the user's needs and context. e collection has also a ground-truth made of relevance assessments which indicate",null,null
42,5Available at h p://inf.usi.ch/phd/aliannejadi/data.html 6h ps://sites.google.com/site/treccontext/,null,null
43,"whether a candidate suggestion is relevant to the user or not (i.e., whether the user likes the candidate suggestion or not).",null,null
44,"For each venue the pro le includes a rating in the following range 4: very interested, 3: interested, 2: neutral, 1: uninterested, 0: very uninterested, and -1: not rated. Venues may also have tags that indicate why the user liked the particular venue. Contextual information is represented by: location, time, type of trip (Business, Holiday, or Other), duration of trip (Night out, Day trip, Weekend trip, or Longer), group of people involved (Alone, Friends, Family, or Other), and season (Winter, Summer, Autumn, or Spring). Moreover, user's age and gender are optionally included. is information can be exploited to have a be er understanding of user's context in order to recommend appropriate venues. For example, you know that a user liked or disliked some venues in ""New York City"", then she goes to ""Boston"" for a weekend trip, she is alone, and it is winter. Given such information, the recommendation system should be able to rank the candidate suggestions so that the top-ranked places are the most appropriate venues for the user to visit in ""Boston"".",null,null
45,3 METHODOLOGY,null,null
46,"We chose Foursquare and Yelp not only because they are two of the most popular LBSNs where many users leave their check-in data, but also because the type of information provided by Yelp is a perfect complement for the type of information on Foursquare. Moreover, as we will show in the statistics of the dataset, there is a considerable overlap of venues that have a pro le on both LBSNs. However, there are places in the TREC dataset that appear only on one of the two crawled LBSNs, hence crawling data from both of them allows making the data gathering more complete.",null,null
47,"Deveaud et al. [4] showed that venue-centric features which were extracted from Foursquare play a key role in venue recommendation. On the other hand, Chen et al. [3] argued that user reviews on venues provide a wealth of information that can be leveraged to address the data-sparsity and the cold-start problems for venue recommendation. Also, Yang et al. [6] showed that the accuracy of a recommender system can be signi cantly improved by extracting opinions from user reviews in Yelp. Also, almost all of the best performing systems in the TREC Contextual Suggestion track 2015 and 2016 [5] crawled data from these LBSNs. In particular, our previous work which were among the best runs in both 2015 [2] and 2016 [1] bene ted from a comprehensive crawled dataset from Yelp and Foursquare. We also showed that a system can bene t from multiple LBSNs, and systems using reviews from Yelp and venue tags from Foursquare had the best performance.",null,null
48,1270,null,null
49,Short Resource Papers,null,null
50,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
51,"Table 2: Sample rows from the collection on venues contextual appropriateness. e numbers in parentheses show the degree of agreement between di erent assessors and ranges from -1 (full agreement on no) to +1 (full agreement on yes). Hence, 0 means that there is no agreement between the assessors and so the task is subjective.",null,null
52,Category,null,null
53,Candy Store Library Pharmacy BBQ Joint Sandwich Place Lounge,null,null
54,Trip Type,null,null
55,Features Trip Duration,null,null
56,Business (-0.60) Holiday (-1.00) Holiday (-0.60) Holiday (+1.00) Business (-0.60) Holiday (+0.58),null,null
57,Day Trip (+0.60) Weekend Trip (-0.58) Night out (-0.60) Night out (-0.18) Longer trip (+0.20) Weekend trip (+1.00),null,null
58,Group Type,null,null
59,Friends (+1.00) Family (+0.62) Family (-0.18) Friends (+1.00) Alone (+0.60) Friends (+1.00),null,null
60,Output Appropriateness,null,null
61,No (-1.00) No (-1.00) No (-1.00) Yes (+1.00) Yes (+0.68) Yes (+1.00),null,null
62,3.1 Data Crawling,null,null
63,"For our collection, we crawled data from two LBSNs: Foursquare and Yelp. Foursquare provides an easy-to-access API7 which makes crawling quite easy. We used Foursquare's API to crawl a large number of venues and scraped a very smaller fraction of venues for additional information on the website. Yelp's API8, on the other hand, has more restrictions and therefore we were able to crawl much fewer venues on Yelp. For TREC 2016 Phase 1, we only crawled data using the Foursquare's API since there were virtually 630K venues to crawl in a very limited time and thus the only option was using the Foursquare's API. For Phase 2 of TREC 2015 and 2016, there was more time and much fewer venues to crawl so that we could crawl data from both LBSNs.",null,null
64,"e data was crawled in two time periods: July - August 2015 and July - August 2016. To nd the corresponding pro les of venues on LBSNs, we used two search engines: 1) Foursquare venue search engine and 2) Google Custom Search. For TREC 2015 there were 8,794 venues from which we crawled 6,427 places from Yelp and 5,639 from Foursquare, with a considerable overlap between data crawled from Yelp and Foursquare. For TREC 2016 there were 18,808 venues from which we crawled 13,868 places from Yelp and 13,417 from Foursquare, again emerging a big overlap between the two sources. As all the venues are in the US cities, we expected that most of the users who reviewed the venues were English speakers.",null,null
65,"ery Structure. For each venue in the collection, we created a query to search on the LBSNs. e query consisted of a venue's name and its location. We cleaned the venues' names in the TREC collection since many contained unrelated terms such as the host service (e.g., Facebook, Wikipedia). Finally, the query we used to search for venues was in the form:",null,null
66,"query , venue's name + venue's city + venue's state .",null,null
67,"Search Result Validation. Since we could not trust the results of search and in order to minimize the noise, we validated the returned results from the search engine following these steps:",null,null
68,(1) We rst validated the city and state of the returned venue. (2) We then measured the similarity between the name of the,null,null
69,venue and the name of the returned place using Levenshtein distance.,null,null
70,7h ps://developer.foursquare.com/docs/ 8h ps://www.yelp.com/developers/documentation/v2/overview,null,null
71,"(3) If the similarity between the two names (calculated in Step 2) was more than a threshold (70%), we considered that result as a match. If not, we continued steps 1-3 for other returned results up to the 5th result.",null,null
72,Note that the high similarity threshold (70%) was set to prevent adding possible noise to the collection.,null,null
73,3.2 Crowdsourcing,null,null
74,"We used the CrowdFlower9 crowdsourcing platform to collect judgments of contextual appropriateness of venues and create the additional contextual-appropriateness dataset. We asked a number of crowdworkers to judge if a venue category is appropriate for a trip description. For instance, if a trip was described in the collection as trip type: business, trip duration: one day, and group type family, for a venue with category Pizza Place then we asked crowdworkers to judge if the venue was appropriate to the trip. In particular, we asked them: ""Is a Pizza Place appropriate for a business trip?"", ""Is it appropriate to go to a Pizza Place on a one-day trip?"", ""Is it appropriate to go to a Pizza Place with family?"" While assessing such tasks could seem trivial and objective, in fact it is subjective in many cases (e.g., going to a pharmacy with family). erefore, we asked at least 5 crowdworkers to provide their judgment to each row. If we found no agreement among the assessors, we considered the task as subjective. We considered the answer ""appropriate"" as a +1 score and ""inappropriate"" as a -1 score. us, the assessment agreement is the average of assessment scores. We asked workers to judge the context/category pairs for almost all possible pairs regardless of their existence in the TREC collections. is makes this dataset general enough to be used for other purposes.",null,null
75,"We made sure to explain the task clearly to the workers and asked them to assess such appropriateness regardless of their personal preferences over categories. Also, we performed a training step and allowed only top-quality workers to do the task.",null,null
76,4 COLLECTION,null,null
77,"e released collection contains more than 330K venues from Foursquare for TREC 2016 Phase 1 and 15,765 venues from both Foursquare and Yelp for TREC 2016 Phase 2. As we can see in Table 4 there were many broken or unrelated links in the the TREC collection (300K out of 600K), however, there were much fewer unrelated links",null,null
78,9h p://www.crowd ower.com,null,null
79,1271,null,null
80,Short Resource Papers,null,null
81,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
82,Table 3: Statistics on the crawled collection,null,null
83,Phase 1 TREC'16,null,null
84,# venues in TREC collection # venues crawled: Yelp # venues crawled: Foursquare # Yelp and Foursquare overlap avg. reviews per venue avg. categories per venue avg. tags per venue avg. user tags per user # distinct user tags,null,null
85,"633,009 336,080 1.35 3.61 150",null,null
86,Phase 2 TREC'15 TREC'16,null,null
87,"8,794 6,427 5,639 4,844 117.34 1.63 8.73 1.46 186",null,null
88,"18,808 13,868 13,417 11,520 66.82 1.57 7.89 3.61 150",null,null
89,Table 4: Statistics on the crowdsourced contextual appropriateness collection,null,null
90,Number of categories Number of category-context pairs Number of assessments Average assessments per pair Average assessment agreement,null,null
91,"179 1969 11,487 5.83 85%",null,null
92,Number of full travel annotations 760,null,null
93,"for Phase 2 (3K out of 18K). For each venue we release all available information: venue name, address, category, tags, ratings, reviews, check-in count, menu, opening hours, parking availability, etc.",null,null
94,"e contextual-appropriateness collection consists of 1,969 pairs of trip descriptors and venue categories as features. In order to enable researchers to train their models using the contextual appropriateness of venues, we created another collection providing ground truth assessments for the contextual appropriateness of the venue categories. It completes the contextual information (i.e., trip type, group type, trip duration) for 10% of the whole TREC collection.",null,null
95,"is collection contains 760 rows including the features we already created using crowdsourcing and the context-appropriateness labels for venues. e 10% of labeled data allows to model the venues' contextual appropriateness given the users' context and to make prediction for the remaining 90% of the data, as we did in [1].",null,null
96,"In Table 2 we report some rows of the collection, and Table 4 lists some statistics of the crawled collection. Figure 1 shows the histogram of venue-appropriateness features assessed by the workers. We divided the assessments in three groups based on appropriateness scores: [-1.00, -0.40): not appropriate (objective), [-0.40, +0.40]: no agreement (subjective), (+0.40, +1.00]: appropriate (objective). To categorize the tasks as subjective and objective, we assumed that those tasks for which there was a high agreement between the assessors could be considered objective since everybody agreed on their (in)appropriateness. While we assumed that those tasks with relatively lower agreement between the assessors could be considered subjective.",null,null
97,Figure 1: Histogram of venue-context appropriateness score ranges. We partition the histogram into 3 parts based on the scores range. Scores below -0.4 represent inappropriateness and score higher than +0.4 represent appropriateness. Scores between -0.4 and +0.4 do not provide much information and show no agreement among assessors (subjective task).,null,null
98,5 CONCLUSIONS,null,null
99,"In this paper we present the dataset we used for our participation to the TREC Contextual Suggestion tracks. We crawled information of venues used in the TREC dataset from two popular LBSNs. Also we collected, using crowdsourcing, ratings on the venue-context appropriateness which allows to make predictions on the appropriateness of a recommended venue to a given user's context. Such collection can be helpful for other researchers interested in comparing their venue-recommendation techniques against state-of-the-art approaches. It could also foster further research on contextual suggestions of venues.",null,null
100,ACKNOWLEDGMENTS,null,null
101,is research was partially funded by the RelMobIR project of the Swiss National Science Foundation (SNSF).,null,null
102,REFERENCES,null,null
103,[1] Mohammad Aliannejadi and Fabio Crestani. 2017. Venue Appropriateness Prediction for Personalized Context-Aware Venue Suggestion. In SIGIR 2017. ACM.,null,null
104,"[2] Mohammad Aliannejadi, Ida Mele, and Fabio Crestani. 2016. User Model Enrichment for Venue Recommendation. In AIRS 2016. Springer, 212­223.",null,null
105,"[3] Li Chen, Guanliang Chen, and Feng Wang. 2015. Recommender systems based on user reviews: the state of the art. UMUAI 25, 2 (2015), 99­154.",null,null
106,"[4] Romain Deveaud, M-Dyaa Albakour, Craig Macdonald, and Iadh Ounis. 2015. Experiments with a Venue-Centric Model for Personalised and Time-Aware Venue Suggestion. In CIKM 2015. ACM, 53­62.",null,null
107,"[5] Seyyed Hadi Hashemi, Charles L. A. Clarke, Jaap Kamps, Julia Kiseleva, and Ellen M. Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track. In TREC 2016. NIST.",null,null
108,"[6] Peilin Yang, Hongning Wang, Hui Fang, and Deng Cai. 2015. Opinions ma er: a general approach to user pro le modeling for contextual suggestion. Information Retrieval Journal 18, 6 (2015), 586­610.",null,null
109,1272,null,null
110,,null,null

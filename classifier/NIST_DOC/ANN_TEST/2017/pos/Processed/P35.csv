,sentence,label,data
0,Session 1A: Evaluation 1,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Can Deep E ectiveness Metrics Be Evaluated Using Shallow Judgment Pools?,null,null
3,Xiaolu Lu,null,null
4,"RMIT University Melbourne, Australia",null,null
5,Alistair Mo at,null,null
6,"e University of Melbourne Melbourne, Australia",null,null
7,J. Shane Culpepper,null,null
8,"RMIT University Melbourne, Australia",null,null
9,ABSTRACT,null,null
10,"Increasing test collection sizes and limited judgment budgets create measurement challenges for IR batch evaluations, challenges that are greater when using deep e ectiveness metrics than when using shallow metrics, because of the increased likelihood that unjudged documents will be encountered. Here we study the problem of metric score adjustment, with the goal of accurately estimating system performance when using deep metrics and limited judgment sets, assuming that dynamic score adjustment is required per topic due to the variability in the number of relevant documents. We seek to induce system orderings that are as close as is possible to the orderings that would arise if full judgments were available.",null,null
11,"Starting with depth-based pooling, and no prior knowledge of sampling probabilities, the rst phase of our two-stage process computes a background gain for each document based on rank-level statistics. e second stage then accounts for the distributional variance of relevant documents. We also exploit the frequency statistics of pooled relevant documents in order to determine a threshold for dynamically determining the set of topics to be adjusted. Taken together, our results show that: (i) be er score estimates can be achieved when compared to previous work; (ii) by se ing a global threshold, we are able to adapt our methods to di erent collections; and (iii) the proposed estimation methods reliably approximate the system orderings achieved when many more relevance judgments are available. We also consider pools generated by a two-strata sampling approach.",null,null
12,KEYWORDS,null,null
13,Test collection; relevance assessment; pooling; shallow judgments.,null,null
14,1 INTRODUCTION,null,null
15,"Batch evaluations are performed by calculating a metric score based on a set of judged documents. Despite ve decades of success, this ""Cran eld/TREC"" paradigm also faces challenges. One of the key issues is that realistic collection sizes now greatly exceed the budget available to perform human judgments. ""Pooling-to-depth-d"" is one widely-used approach [25], in which documents in the union of the top-d lists returned from a set of contributing systems are judged, but other documents are not. e pooling depth d is ideally",null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080793",null,null
17,"determined by the needs of the e ectiveness metric to be used, but in reality is also constrained by the experimental budget. Although pooling has identi ed the majority of relevant documents in earlier collections [30], there is growing evidence that this is not true for the web collections that are now the norm [2, 11].",null,null
18,"e uncertainty in e ectiveness measurement in large collections is the key emphasis of our work here, focusing on how to estimate evaluation scores when reduced judgment sets are used.",null,null
19,"is is not a new problem, and a range of prediction mechanisms have been proposed [1, 22, 23, 27, 28], mainly focusing on predicting system orderings. We focus on prevailing pool-based test collection construction methods, as these best match our methodology, and on deep evaluation metrics, noting that pool depth has a lesser impact on shallow evaluation metrics such as ERR [6]. Alternative approaches using direct sampling exploit prior knowledge of the probability of each document being judged, and are applied during pool construction, on the assumption that all systems requiring measurement have been identi ed. But that process makes it di cult to infer scores for any new systems that get added later. On the other hand, pooling selects documents based on the assumption that top-ranked documents are both more likely to be relevant, and hence more in uential in computing e ectiveness scores. In this more general se ing there is no a priori knowledge of the system scores, and while that means that regression cannot be applied, new systems can be considered. We also argue that the decision to apply score adjustment should be done on a per topic basis. Robertson [17] notes that topics vary in terms of the number of potential relevant documents, and that this can have a signi cant impact on evaluation scores. Dynamically identifying when to perform score adjustment is thus a second challenge that must be considered.",null,null
20,"e end objective of an evaluation goes beyond the metric scores, of course; in the end we wish to be able to compare and choose between systems, meaning that it is also important that the score estimations are concordant with the system orderings that would arise if full knowledge were available. Since the la er is measured according to a reference point which may not be known, there is no clear optimization goal, another complication that we address.",null,null
21,"ese various considerations lead to two questions: Research estion 1: For each topic, how can we estimate the evaluation score of a system using a shallow pooling depth? Research estion 2: Can stable system rankings be achieved using the adjusted scores? In considering these two questions, we perform experiments using several di erent ad-hoc test collections and a range of modeled pool depths. Our results show that: (i) a two-stage optimization framework generates more accurate score estimations than previous approaches; (ii) topic-based adjustment thresholds identi ed using early TREC collections allow additional improvements in",null,null
22,35,null,null
23,Session 1A: Evaluation 1,null,null
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
25,"estimation accuracy; and (iii) the adjusted evaluation scores yield be er approximations of the ""true"" system rankings than do the unadjusted scores. In addition to standard pooling methods, we also consider two-strata sampling [24].",null,null
26,2 RELATED WORK,null,null
27,"Incomplete Judgments and Evaluation Bias. Two types of bias arise in batch evaluations: pooling depth bias [19] and system bias [20]. e rst is caused by the use of shallow pools, and the second by performance underestimation for systems that did not contribute to the pool. Both are a result of documents appearing in the ranking for which judgments are not available. e simplest response to unjudged documents is to stipulate that anything not examined in the pooling process is not relevant. Zobel [30] challenged this notion using a series of leave-one-out experiments, and showed for several early TREC collections that while it was likely there were indeed further relevant documents that had not been identi ed, system bias was nevertheless within acceptable levels. However, on more recent web collections, there is growing evidence that this situation may not be assumed [2, 11].",null,null
28,"Other responses to the issue of unjudged documents have been proposed. Buckley and Voorhees [3] describe BPref, which balances the rank positions of documents judged as non-relevant and relevant, and ignores unjudged documents. In a related approach, Sakai [18] considers condensed lists, which compute scores using a",null,null
29,"ltered ranking containing only judged documents, and nds that standard metrics give higher discriminative ratios than achieved by BPref. However, the condensed list methodology has not been shown to be stable when comparing relative system orderings using Kendall's  or discrimination ratios [19, 20]. Score Estimation / Collection Construction. Documents without judgments are not distributed randomly in ranked result lists.",null,null
30,"erefore, sample-based collection construction approaches have been suggested to support statistical inference [1, 22, 23, 27, 28]. Yilmaz and Aslam [27] present an inferred Average Precision (AP) metric that uses an expectation model, and can be coupled with a sampling process to select documents to be judged. eir InfAP metric uses uniform random sampling during collection construction. When compared with standard TREC-style pooling, the results produced by InfAP were strongly correlated with AP. However, this sampling process is random, and retrieval systems return documents in rank order, meaning that relevant documents are more likely to be returned at the top of the list if the system is e ective.",null,null
31,"e use of non-random sampling has also been explored. Yilmaz et al. [28] extended their previous work, proposing metrics XInfAP and XInfNDCG, based on a strati ed sampling process. In contrast, Aslam et al. [1] consider the use of importance sampling for the same task, proposing statAP, which estimates the expectation of AP. e key di erence between InfAP and statAP is that statAP is designed to generate the optimal distribution estimates using all of the contributing systems. Voorhees [24] further examines the e ect of sampling methods on inferred metrics.",null,null
32,"A recent study by Schnabel et al. [23] also used importance sampling, this time in conjunction with Discounted Cumulative Gain (DCG). e key idea in their approach was to use the probability of",null,null
33,"relevance with respect to rank information when determining the sample distribution. ey provide an analysis on how to derive the optimal sampling distributions under di erent system comparison se ings [22]. Using the proposed framework, any metric can be reformulated in the form of expectations and be estimated directly from the sampling process. Mo at et al. [14] had earlier examined targeted pooling and document judgment order in conjunction with the Rank-Biased Precision (RBP) metric.",null,null
34,"Score Estimation Based on Pooling Methods. Estimation in traditional pooling techniques has also received considerable a ention [4, 7, 9, 10, 16, 26]. Most existing techniques focus on adjusting the bias which exists between pooled and unpooled systems. Webber and Park [26] proposed two methods to perform score adjustment.",null,null
35,"e rst uses an adjustment factor, which is computed from the contributing systems. Each contributing system has an error value assigned when it is le out of the training process, and the mean of those values is applied to any new system to be measured. e second approach requires a set of common topics with ""complete judgments"". A similar calculation is performed in order to obtain the adjustment factor, but restricted to the subset of common topics. To obtain additional adjustment accuracy, Webber and Park introduced randomization to build an unbiased estimator.",null,null
36,"Recent work by Lipani et al. [9] using a precision metric outperformed the rst method of Webber and Park. eir ""anti-Precision"" measurement is similar in spirit to the residual computed by RBP [15]. Lipani et al. [9] compute adjustment factors using the leaveone-run out methodology, and then improve their previous approach by computing an average distribution [10].",null,null
37,"e closest work to our current approach is that of Ravana and Mo at [16]. ey focus on pooling depth bias, proposing three methods to estimate the e ect of unjudged documents, using the residual that can be computed for weighted-precision metrics [15].",null,null
38,"eir rst method uses a background estimation based on a static scaling factor; the second assumes that the percentage of relevant but unjudged documents can be derived directly from the known score component; and the third uses a parametric combination of the rst two. Lu et al. [12] subsequently de ne the same problem in terms of the anticipated e ectiveness gain as a function of ranking depth. Based on di erent assumptions derived from the underlying gain distributions, they propose several alternatives, and compare the estimates achieved. ey empirically show that relatively simple models can be used to estimate gain values for unjudged documents.",null,null
39,"An approach due to Bu¨ cher et al. [4] directly predicts the relevance of unjudged documents, using two types of classi ers trained with the existing pool to predict the relevance of unjudged document in a new system. Although the e ectiveness of the classi er is low, their results show that classi cation does help maintain similar system orderings when measured via Kendall's  . Jayasinghe et al. [7] take a similar approach, and show that reliably predicting document relevance is o en di cult.",null,null
40,3 PRELIMINARIES AND BASELINES,null,null
41,"Pools. Figure 1 shows the construction of a pool for one topic, with sj,i (on the le ) corresponding to the j th document in the run for system Si , and with the corresponding documents (on the",null,null
42,36,null,null
43,Session 1A: Evaluation 1,null,null
44,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
45,T,null,null
46,S1 s11,null,null
47,S2 s12,null,null
48,s21 s31 s41 s51 ... sd1 ...,null,null
49,s22 s32 s42 s52 ... sd2 ...,null,null
50,sk1 sk2,null,null
51,S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,null,null
52,...,null,null
53,... ... ... ... ... ... ... ... ...,null,null
54,Sn s1n s2n s3n s4n s5n ... sdn ... skn,null,null
55,Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,null,null
56,... sdn+1,null,null
57,... skn+1,null,null
58,Rank,null,null
59,1 2 3 4 .5. . .d. .,null,null
60,k,null,null
61,S1,null,null
62,D1 D3 D2 D7 D6 ... D10 ... D49,null,null
63,Complete Set J,null,null
64,J,null,null
65,S2 S3 . . . Sn Sn+1 ,null,null
66,D2 D1 D6 D5 D3 ... D6 ...,null,null
67,D3 D7 D2 D8 D5 ... D1 ...,null,null
68,... ... ... ... ... ... ... ...,null,null
69,D3 D4 D7 D2 D1 ... D5 ...,null,null
70,D4 D2 D8 D1 D9 ... D3 ...,null,null
71,D50 D30 . . . D18 D6,null,null
72,System Matrix: S,null,null
73,M@k: M1 M2 M3 . . . Mn Mn+1,null,null
74,Figure 1: Pooling process for a topic T . e le matrix is a rankbased representation; the right one shows the equivalent document,null,null
75,"identi ers. e two boxes indicate two possible sets of pooled documents, the larger to depth d, and the smaller to some depth d < d. e metric M is evaluated at some depth k, where k may or may not be less than or equal to d or d .",null,null
76,"right), each potentially retrieved by multiple systems at di erent rank positions. Hence, a document D can also be represented by its rank-position information, D, (pD,1, pD,2, . . . , pD,n ) , in which pD,i is the rank returned for D by contributing system Si . Metric evaluation to depth d for systems S1 to Sn requires that the documents in the set , {D | minni,""1 pD,i  d } be judged. at is, both matrices can be further mapped to a matrix of relevance Rd×n in which rj,i is a relevance, or gain, value.""",null,null
77,"If there is insu cient judgment volume available, a shallower pool might be formed, with documents D for which minni,""1 pD,i > d not judged, and elements in Rd×n le without values. Unknown relevance labels may also arise for a new system Sn+1, regardless of the pooling and evaluation depths. In this framework, the rst of""",null,null
78,the two research questions proposed in Section 1 can be split into,null,null
79,"two aspects: (1a) for each topic, how do we estimate the scores of",null,null
80,a system using a set of shallow pooled judgments; and (1b) which,null,null
81,topics may assume that unjudged documents are not relevant and,null,null
82,which ones should not.,null,null
83,One method for dealing with missing data is to compute expected,null,null
84,"gains as a function of retrieval rank [12]. However, modeling rele-",null,null
85,vance as a function of rank only considers the LHS representation,null,null
86,"in Figure 1, and ignores that documents can have multiple ranks.",null,null
87,Addressing that limitation is a key part of our work here.,null,null
88,"Metric Residuals. Suppose that for some topic T , a set of docu-",null,null
89,ments results from pooling to depth d (Figure 1). Consider the,null,null
90,"ranked list returned by some system Si ,"" (s1,i , s2,i , . . . , sk,i ) and let rj,i represent the gain of the document at rank j, normally (but not necessarily) a value in [0, 1]. e e ectiveness Mi of Si when computed to depth k by a weighted-precision metric M is:""",null,null
91,k,null,null
92,"Mi ,"" M@k (Si , ) "",",null,null
93,"rj,i · WM (j) ,",null,null
94,(1),null,null
95,"j ,1",null,null
96,"sj,i ",null,null
97,"where WM (j) is the weight assigned by the metric at depth j, with",null,null
98," j ,1",null,null
99,WM,null,null
100,(j,null,null
101,),null,null
102,",",null,null
103,"1 [13, 15]; and where the restriction sj,i",null,null
104,is,null,null
105,"required to ensure that only de ned values of rj,i are included.",null,null
106,"A corresponding residual i can then be computed, quantifying the",null,null
107,metric weighting associated with the unjudged documents [15]:,null,null
108,k,null,null
109,"i ,",null,null
110,rmax · WM (j ) +,null,null
111,"rmax · WM (j ) ,",null,null
112,(2),null,null
113,"j ,""1 sj,i""",null,null
114,"j,k +1",null,null
115,"where rmax is the maximum possible gain. Either term might be zero, depending on whether Si contributed to the pool, on the relationship between the evaluation depth k and the pooling depth d, and on whether WM (j) ,"" 0 when j > k, as occurs with truncated""",null,null
116,metrics.,null,null
117,ere is a three-way tension between metric depth (quanti ed as,null,null
118,the expected point reached in the ranking in the corresponding user,null,null
119,"model [13]); accuracy of measurement, captured by the residual; and the cost | | of performing the judgments. For example, in RBP",null,null
120,"the tail residual (the second component in Equation 2) is given by pk , and if p ,"" 0.5, k  10 is su cient. Similar calculations apply""",null,null
121,"for ERR [6]. But in either case, the rst term of Equation 2 might be",null,null
122,"non-zero for new runs. Furthermore, even the tail residuals might become large for deeper metrics, for example, RBP with p , 0.95.",null,null
123,"Truncated (that is, non-in nite) metrics such as Scaled DCG at",null,null
124,"depth 100, SDCG@100, also require deep pools if the residual is to be moderately bounded. e same requirement must apply by",null,null
125,implication to other deep metrics such as Average Precision.,null,null
126,"Problem De nition. Consider a set of n contributing systems {S1, S2, . . . , Sn }. For one topic T , let d be a pooling depth at which",null,null
127,it is believed that a majority of the relevant documents occurring,null,null
128,in the runs of those systems have been identi ed. We refer to this,null,null
129,"set of judgments as the ""complete set"". Let d < d be a shallower pooling depth, with judgments forming an incomplete set  . Given a weighted precision metric M, the e ectiveness score of Si evaluated using M and to depth d is denoted as Mi ,"" M@d (Si , ), with a residual of i . Similarly, an estimated metric score based on judgments to depth d < d, is denoted as M^ i "","" Ed (M@d (Si , )) where Ed (·) is an estimation function for the same metric at depth d. Following Lu et al. [12], the estimation error i is then de ned as:""",null,null
130,i,null,null
131,",",null,null
132,Mi 0,null,null
133,-,null,null
134,M^ i,null,null
135,"if M^ i < Mi ,",null,null
136,"if Mi  M^ i  Mi + i ,",null,null
137,(3),null,null
138,M^ i - (Mi + i ) if M^ i > Mi + i .,null,null
139,"is de nition respects the residual range, and only gives non-zero",null,null
140,"values if the estimated e ectiveness falls outside the score range arising from the use of at depth d. e challenge is to develop a method Ed (·) that estimates the depth-d e ectiveness score of a contributing system based on a subset of the judgments, and minimizes the average value of i .",null,null
141,"In the experiments in Section 6 we report the RMS aggregate of the i values computed, across systems and topics; and, as a ""percentage accurate"", the fraction of those values that are zero.",null,null
142,"Lower-Bound Estimation. A simple approach is to take Ed (x ) ,"" x, that is M^ i "","" Mi , where Mi is the score for system Si when evaluated using , and assert that documents outside do not""",null,null
143,alter the score. Taking unjudged documents to be not relevant is the,null,null
144,"normal default in batch evaluation, and is a valid estimator. But the",null,null
145,"estimation quality depends on the breadth of the pool, and whether",null,null
146,a majority of relevant documents have been identi ed. When there,null,null
147,37,null,null
148,Session 1A: Evaluation 1,null,null
149,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
150,J,null,null
151,Fitted Gain,null,null
152,Rank ,null,null
153,1,null,null
154,S1 s11,null,null
155,2 3 4 .5. . .d. .,null,null
156,s21 s31 s41 s51 ... sd1 ...,null,null
157,k,null,null
158,sk1,null,null
159,S2 s12 s22 s32 s42 s52 ... sd2 ... sk2,null,null
160,S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,null,null
161,...,null,null
162,... ... ... ... ... ... ... ... ...,null,null
163,Sn s1n s2n s3n s4n s5n ... sdn ... skn,null,null
164,Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,null,null
165,... sdn+1,null,null
166,... skn+1,null,null
167,Gain Vector:,null,null
168,"ni,1 r1i/n ni,1 r2i/n ni,1 r3i/n ni,1 r4i/n",null,null
169,"ni,1 r5i/n",null,null
170,g,null,null
171, Fitting,null,null
172,G1(k),null,null
173,g011 g021 g031 g041 g051 ... g0d1 ...,null,null
174,g0k1,null,null
175,G2(k),null,null
176,g012 g022 g032 g042 g052 ... g0d2 ... g0k2,null,null
177,G3(k),null,null
178,g013 g023 g033 g043 g053 ... g0d3 ... g0k3,null,null
179,...,null,null
180,... ... ... ... ... ... ...,null,null
181,...,null,null
182,Figure 2: Overview of rank-based estimation for a single topic. e judgments are used to infer an observed gain vector g; each,null,null
183,of a set of m functions G (k ) is then ed to g.,null,null
184,"are still many unjudged relevant documents, this estimator results in underestimation of system performance. Reasonably good rank correlation between the estimates and the true score over a set of systems can be obtained, but there is no guarantee that the performance of each of the systems has been accurately measured.",null,null
185,"Interpolative Estimation. A second baseline is provided by the RM interpolative estimator proposed by Ravana and Mo at [16], who scale the metric score across the residual, assuming that unjudged documents are relevant at the same rate as judged ones:",null,null
186,"M^ i , Mi /(1 - i ) .",null,null
187,(4),null,null
188,"A collection-based background probability is used when i , 1. is estimator assumes that gain is accrued at the same rate across",null,null
189,"all of the documents retrieved by the system, both judged and unjudged. Although more robust than the LB estimator, it does not allow the likelihood of relevance to decrease as the pool is extended from d to d.",null,null
190,"Rank-Based Estimators. Lu et al. [12] introduce rank-based estimation, illustrated in Figure 2. e judgments are used to estimate expected gain as a function of rank on a per-topic basis. ose rank-based fractional gain predictions are then used for unjudged documents ­ interpolated at depths up to d , and extrapolated from d to the metric evaluation depth k. Lu et al. explore alternative estimation functions, measuring the prediction error using the mechanism described in Equation 3, and nd that while improvements are possible, no single estimator works consistently well across all collections and topics. Rank-based estimation also has the drawback of ignoring the fact that a document can appear at di erent ranks for di erent systems; and hence potentially assigns di erent gain estimates to the same document in di erent runs, a representational issue that usually leads to a biased estimation [29]. As a further drawback, an entire row in the system matrix S (see Figure 1) must be judged in order to compute the expected gain, limiting construction methods to pooling or sampling by rank, and possibly excluding strati ed sampling processes.",null,null
191,Sampling-Based Estimation. Other sampling approaches can also be used when forming the judgment set. Voorhees [24] de-,null,null
192,"scribes a two-strata sampling method, which consists of shallow pooled judgments to some depth d , and then 10% random sampling to depth d in a second set s . ese judgments allow computation of inferred recall-based metrics, and also inferred versions",null,null
193,"of weighted-precision metrics, with M^ i for system i calculated as:",null,null
194,k,null,null
195,k,null,null
196,"M^ i ,",null,null
197,"rj,i · WM (j) +  ·",null,null
198,"rj,i · WM (j) ,",null,null
199,(5),null,null
200,"j ,""1 sj,i """,null,null
201,"j ,""1 sj,i  s""",null,null
202,where,null,null
203,-1,null,null
204,k,null,null
205,k,null,null
206,",",null,null
207,WM (j) ·,null,null
208,WM (j),null,null
209,"j ,""1 sj,i""",null,null
210,"j ,""1 sj,i  s""",null,null
211,and where the second term in Equation 5 estimates the total gain,null,null
212,"associated with documents contained in the second stratum. Here,  is the interpolation estimator. Note that Equation 5 only adapts the RM method for sample based judgments.",null,null
213,4 TWO-STAGE ESTIMATION,null,null
214,"Overview of the Framework. To compute score estimates, we propose a two-stage framework, guided by a uni ed optimization goal, and built on a set of m  1 per-topic rank-level estimators.",null,null
215,e overall structure of this mechanism is described in Algorithm 1.,null,null
216,"We omit the process of obtaining rank-level estimations, discussed",null,null
217,"brie y in the previous section, and in detail by Lu et al. [12]. at is, we assume as our starting point here that m di erent rank-based estimators have been generated, each derived from the judged documents D  , and that values for a set of gain functions have been computed, with 0j, the gain associated with an unjudged document that appears in the j th position of any of the n system",null,null
218,Algorithm 1 Estimation Framework,null,null
219,Input: System matrix Sk×n ; partial relevance judgments with 2[D] the gain associated with document D for D  and,null,null
220,unde ned otherwise; and a set of m rank-level background,null,null
221,"gain estimates, 0j, for 1  j  k and 1   m, with 0,  0j, | 1  j  k and 1[D]  1 [D] | 1   m .",null,null
222,"Output: Values 2[D], gain estimates for the documents D  ",null,null
223,1: for D  \ do 2[D]  0,null,null
224,2:   C,null,null
225,"CV( , S) // compute coe cient of variance",null,null
226,3: if  >  then,null,null
227,// adjust only if  exceeds threshold,null,null
228,4: for  1 to m do,null,null
229,5:,null,null
230,for D  do 1 [D]  0,null,null
231,6:,null,null
232,"w1opt  arg min L h1 ( 0, , w1) | D ",null,null
233,"w1 [0, 1]n",null,null
234,7:,null,null
235,for D  do,null,null
236,8:,null,null
237,"1 [D]  h1 0, , w1opt",null,null
238,9:,null,null
239,end for,null,null
240,10: end for,null,null
241,"11: w2opt  arg min L h2 ( 1[D], w2) | D  w2 [0, 1]m",null,null
242,12: // get nal per-document estimation,null,null
243,13: for D  \ do,null,null
244,14:,null,null
245,"2[D]  h2 1[D], w2opt",null,null
246,15: end for,null,null
247,16: end if,null,null
248,17: return 2,null,null
249,38,null,null
250,Session 1A: Evaluation 1,null,null
251,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
252,"rankings, as predicted by the th of the m di erent estimators.",null,null
253,"Prior to forming the new combined estimates, we rst compute the coe cient of covariance  from the judgment set [5], in order to",null,null
254,"determine whether to use a background ""unjudged are not relevant"" predictor. Estimation is computed by steps 4 to 15, with h1 (·) and h2 (·) two parametric combining functions, in which the parameters are obtained by minimizing a loss function L(·). We discuss the details of Algorithm 1, including the rationale behind the use of  ,",null,null
255,in the next few paragraphs.,null,null
256,"First Stage. As noted already, one problem with rank-based estimators is the potential inconsistency across runs of the gain a ached",null,null
257,"to any particular document. As always, we assume that one topic is being addressed; the goal in the rst stage is to aggregate the m × n per-document estimates across the m estimators and n systems into a smaller set of m estimates per document. at is, the m rank-level estimators are treated separately at rst, in the loop at step 4, to obtain a consistent background gain for each document D for each model, denoted 1 [D]. is is done via a combining function h1 (·) that maps a vector to a single value. Several options for h1 (·) are available, with the choice between them depending on assumptions",null,null
258,about system quality and the degree to which the systems are cor-,null,null
259,"related. For simplicity, we assume that the systems are independent and that they vary in quality. erefore, for each document D, a natural combining function is to compute a weighted average, with h1 (step 6) parameterized by an n-element weighting vector w1 that is speci c to the th estimator:",null,null
260,n,null,null
261,"D  , h1 ( 0, , w1 | D) ,",null,null
262,"0pD,i, · w1i",null,null
263,"i ,1",null,null
264,n,null,null
265,(6),null,null
266,"with w1i ,"" 1 and w1i  [0, 1] ,""",null,null
267,"i ,1",null,null
268,"and where 0pD,i, applies the th estimator to the rank at which document D appears in the i th of the n runs. One practical issue is that a document may not be retrieved by all systems in their top-k ranked lists, where k is the maximum depth of lists returned. In such cases the rank-based background gain of that document for that system is set to the modeled gain at depth k.",null,null
269,"To compute a value for w1, we consider the aggregation process as an optimization problem, where the goal is to minimize the",null,null
270,estimation error. e estimation error has two granularities: (i) the,null,null
271,total error of system e ectiveness score calculated using ; and,null,null
272,(ii) the total error of estimating the background gain of the labeled,null,null
273,"documents. From either perspective, we can formalize an objective function L and use it at step 6 of Algorithm 1. Consider the rst case, with the system matrix as shown in Figure 1. We de ne L as:",null,null
274,2,null,null
275,nk,null,null
276,"La (·) ,",null,null
277,"WM (j) · (h1 (·, w1 | sj,i ) - rj,i ) , (7)",null,null
278,"i,1 j,""1 sj,i """,null,null
279,"where WM (j) · h1 (·, w1 | sj,i ) is the estimated background gain for document sj,i  , and rj,i is the known relevance value of that same document. As noted, La minimizes the overall estimation",null,null
280,error of the evaluation scores for the set of systems.,null,null
281,"e second alternative uses the document-position representation (pD,1, pD,2, . . . , pD,n ):",null,null
282,"Lb (·) ,",null,null
283,D,null,null
284,n 2,null,null
285,"WM (pD,i ) · (h1 (·, w1 | D) - rD ) , (8)",null,null
286,"i ,1",null,null
287,"in which rD is the relevance value of document D and is included only once per document, rather than once per document-rank.",null,null
288,"When compared to Equation 7, which considers estimation er-",null,null
289,"rors at the system level, this loss function is focused at the per-",null,null
290,"document level, seeking to minimize the overall estimation error",null,null
291,for the weighted gain of each document. Either Equation 7 or Equa-,null,null
292,"tion 8 can be used at step 6 of Algorithm 1, with the combination function h1 (·) and constraints de ned in Equation 6. e result is the computation of a sequence of w1opt vectors, one for each of the m di erent rank-level estimators.",null,null
293,Second Stage. Multiple ing models have been proposed because di erent assumptions about the underlying relevance distributions,null,null
294,"across all systems are plausible, with a risk that no single model",null,null
295,"covers the true hypothesis space. Indeed, the limited non-random",null,null
296,training data means that we may su er from a high variance if only,null,null
297,"one model is considered. erefore, a ""meta"" optimizer is also used,",null,null
298,"combining results from the rst stage, as described by steps 11",null,null
299,"to 15. A weighted average is used in this role too, considering each document D, together with the estimated background gains generated by the m previous computations, 1[D]. at combiner, h2 (·) (step 11), is de ned via the m-vector w2 as:",null,null
300,m,null,null
301,"D  , h2 1[D], w2 ,"" 1 [D] · w2 ,""",null,null
302,",1",null,null
303,m,null,null
304,(9),null,null
305,"with w2 ,"" 1 and w2  [0, 1] .""",null,null
306,",1",null,null
307,"Both La and Lb can be used in step 11, but may not necessarily be the same. Note that the m-vector w2opt, computed at step 11 as the minimizing value for Equation 9, provides an indication of",null,null
308,the importance of individual optimizers from the previous stage.,null,null
309,Previous work has shown that the expected error of combining loss,null,null
310,functions is smaller than the average error on results output by,null,null
311,each optimizer in isolation from the rst stage [29].,null,null
312,"Computing the Coe cient of Variance. e score adjustment and estimation process has been presented on a per-topic basis, with an underlying assumption that a shallow judgment pool cannot identify a majority of the relevant documents. However, some topics may have only a small number of relevant documents, and a shallow depth may be su cient to identify most of them, with adjustment unnecessary. Only if deeper pooling would identify further relevant documents can score adjustment have an e ect on system e ectiveness scores. Hence a coe cient of variance [5] is computed for the relevant documents in the shallow pool and used as an indicator, as described in step 2.",null,null
313,"Pooling is treated as a sampling with replacement process, with an unknown probability of a relevant document being sampled. Although the nal judgment process considers only the documents in the pool, a document returned by multiple systems has a selection",null,null
314,39,null,null
315,Session 1A: Evaluation 1,null,null
316,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
317,frequency. e intuition behind  is to make use of that frequency,null,null
318,information to describe the sample coverage of relevant documents.,null,null
319,"Consider the system matrix S in Figure 1 and a pooling depth d . Each document sj,i (1  j  d , 1  i  n) has a multiplicity in Sd ×n ; we then group them by that frequency count. Let fi be the number of relevant documents appearing i times in Sd ×n , R ,"" i fi the number of relevant documents, and C "", i i · fi be",null,null
320,"the total occurrence count of relevant documents. For example, if",null,null
321,"only D8 and D1 in Figure 1 are identi ed as relevant documents, then we have f1 ,"" 1, f3 "","" 1, and R "", 2 and C ,"" 4. Based on these elements, the coe cient of variance,  , is estimated via [5]:""",null,null
322,2,null,null
323,",",null,null
324,max,null,null
325,|R | 1-f1 /C,null,null
326,C,null,null
327,i i · (i - 1) · · (C - 1),null,null
328,fi,null,null
329,-,null,null
330,"1,",null,null
331, 0,null,null
332,.,null,null
333,(10),null,null
334,"When  ,"" 0, the probability of sampling a relevant document follows a uniform distribution; and when  is high, the distribution""",null,null
335,"is skewed, and it is likely that more relevant documents exist due to",null,null
336,"the low sampling coverage. Based on this, we have two hypotheses:",null,null
337,Hypothesis 1:  tends to decrease as pooling depth increases.,null,null
338,"Hypothesis 2: ere is a threshold  , where if  <  , then the existence of unjudged documents will only negligibly a ect the estimate of the system performance, and they can be ignored.",null,null
339,"e rst hypothesis is easy to understand, because increasing the pooling depth increases the sample size, and increases the sampling coverage. e second hypothesis assumes that the score can be dynamically adjusted based on a threshold. If this is correct, then a point at which the total estimation error is minimal can be observed. Otherwise, we must conclude that a shallow pool is not su cient for nding relevant documents, and adjustment must be applied to all topics in all evaluations.",null,null
340,"Discussion. We have described two possible realizations of loss functions, and one option for the combining functions h1 (·) and h2 (·). More sophisticated mechanisms are also possible. For example, the relationship between systems might be leveraged to derive a be er h1 (·) and its constraint.",null,null
341,Note also that although our process targets the problem of es-,null,null
342,"timating the e ectiveness of runs that contribute to the pool, it is",null,null
343,possible to apply the same process to estimate the score of a new,null,null
344,"system, and is demonstrated empirically in Section 6. Section 6",null,null
345,also shows that the framework can be applied to the judgments,null,null
346,"constructed using two-strata sampling [24], incorporating the addi-",null,null
347,tional information provided in the second stratum.,null,null
348,5 COMPARING SYSTEM RANKINGS,null,null
349,"Section 3 already de ned i , a score-based evaluation criterion. But we are also interested in comparing system orderings as a measure",null,null
350,of usefulness of an estimation regime.,null,null
351,"Kendall's Distance. is distance metric is widely used to measure the similarity between ranked lists, and counts the number of inverted pairs between two n-item orderings. Let i, j represent the pairwise relationship between the e ectiveness metric means S¯i and S¯j of systems Si and Sj over a set of topics according to one measurement regime, with i, j  {-1, 0, 1} indicating that S¯i < S¯j ,",null,null
352,"that S¯i ,"" S¯j , and that S¯i > S¯j , respectively; and let i, j be the corresponding values for a second measurement regime and the system""",null,null
353,"means that it induces, for example, using pooling to a di erent depth. en Kendall's normalized  distance is the number of pairs 1  i < j  n in which i, j · i, j < 0, divided by n(n - 1)/2 to bring it into the range 0    1, with 0 meaning ""identical"".",null,null
354,Statistical Weighting. Paired t-tests are o en used to quantify the,null,null
355,"strength of the relationship between two systems, and the values i, j and i, j might be thought of as being continuous rather than ternary. Kumar and Vassilvitskii [8] describe a weighted  distance",null,null
356,"that counts the strength of each discordant pair, focusing solely on cases where i, j · i, j < 0. In practice we are not only interested in the discordant pairs, but also in pairs that are deemed to be",null,null
357,signi cantly di erent according to one of the measurement regimes,null,null
358,"but not the other, even if their overall relationship is concordant. Suppose that S¯i > S¯j according to the rst measurement, and",null,null
359,"that a paired one-tail statistical test across topics yields pi, j . Values of pi, j near zero indicate a signi cant superiority of Si over Sj ; values close to 0.5 indicate that it is by chance. If we de ne",null,null
360,"i, j ,"" 00..05 - pi, j""",null,null
361,"pj,i - 0.5 ",null,null
362,"if S¯i > S¯j if S¯i ,"" S¯j if S¯i < S¯j ,""",null,null
363,"then -0.5  i, j  0.5 is a real-valued quantity that captures",null,null
364,both the direction and strength of the relationship between the two,null,null
365,systems according to the rst measurement regime. We compute,null,null
366,"i, j similarly using a second measurement approach, and then, to compare the alternative rankings of n systems induced by the two",null,null
367,"measurement techniques, calculate",null,null
368,"dist ,",null,null
369," · |i, j - i, j | ,",null,null
370,(11),null,null
371,1i <j n,null,null
372,"where   0 is an additional scaling factor. For example, if  ,"" |i, j | then the strength of the relationship between Si and Sj according to the rst measurement regime also in uences the measured distance. Overall, if dist  0, then the two measurement regimes agree in terms of both the direction of each pairwise relationship Si versus Sj , and also its strength. If dist is substantially greater then zero, then the two measurement regimes give rise to many system""",null,null
373,pairs for which there are non-trivial disagreements (including in,null,null
374,"both discords and in concords) over the strength of the measured relationships. Compared with Kendall's  distance, Equation 11 operates over continuous values, which makes it both resistant",null,null
375,"to inconclusive changes in rank position, and also sensitive to di erences in which the direction of the relationship between Si and Sj stays the same, but the statistical strength varies markedly.",null,null
376,6 EXPERIMENTS,null,null
377,"e experiments described in this section include: (i) a post-hoc analysis for testing two hypotheses proposed in Section 4, and se ing the threshold  ; (ii) evaluating prediction accuracy using RMSE and Acc% as de ned by Lu et al. [12]; (iii) system ordering stability evaluation using the distance metric de ned in Equation 11 with  ,"" 1, and using normalized  distance; and (iv) a case study covering the ClueWeb 2010 (CW10) task.""",null,null
378,40,null,null
379,Session 1A: Evaluation 1,null,null
380,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
381,Dataset d |S |,null,null
382,Judgments per topic,null,null
383,2-strata,null,null
384,"d , 10 d , 20 d , 30 d , d",null,null
385,TREC5 100 76 272 (13) 512 (10) 747 (8) 2298 (4) ­,null,null
386,TREC9 100 59 174 (11) 322 (8) 462 (7) 1382 (4) 294 (7),null,null
387,TREC10 100 54 182 (13) 335 (10) 480 (9) 1402 (5) 303 (9),null,null
388,Rob04 100 42 75 (25) 139 (18) 206 (15) 710 (7) 134 (15),null,null
389,TB04 80 33 164 (31) 313 (27) 453 (25) 1121 (19) 270 (25),null,null
390,TB05 100 34 111 (41) 202 (36) 291 (33) 878 (25) 187 (33),null,null
391,TB06 50 39 141 (31) 270 (26) 394 (23) 633 (19) ­,null,null
392,CW10 20 21 98 (30) ­,null,null
393,­ 187 (28) ­,null,null
394,Table 1: Datasets used: d is the original pooling depth and provides the reference point for metric scores; d is a notional pooling depth used our experimentation; and |S | is the number of contributing,null,null
395,runs. Only Adhoc Task runs are used. e middle four column pairs,null,null
396,"show the number of judgments averaged across topics at each pooling depth d , and the percentage of relevant documents. e last",null,null
397,"column shows the statistics when using two-strata sampling [24],",null,null
398,averaged over topics and over ten random iterations.,null,null
399,"Experimental Setup. e collections and con guration parameters used in our experiments are shown in Table 1. We also measured a range of behavior using the TREC7 and TREC8 collections, but do not include them here because those two collections were used as part of the post-hoc analysis and parameter se ing. Scripts are available to reproduce all of the various results given here1.",null,null
400,"Pooling to di erent depths is simulated using the identi ed contributing systems, and the average number of judgments required per topic at di erent pool depths is also shown in Table 1, together with the corresponding percentage of documents identi ed as being relevant. In the experiments measuring rank stability, we also examine the two-strata sampling method described by Voorhees, and averages over ten runs for this randomized approach are included in the table. For the Robust04 task the last 49 topics are used, and judged to a depth of 100; for other tasks, we use all of the original topic set and judgments. Our goal in collection selection was to capture as much variety as possible. TREC5 and Rob04 use the NewsWire document collection, TREC9 and TREC10 use WT10G, a small web collection, and TB04/05/06 use the GOV2 web collection.",null,null
401,"e ClueWeb 2010 task (CW10) uses the largest web collection but also has fewer contributing systems and a shallow pool depth. It is representative of newer collections, which are large, and have more uncertainty associated with the judgment coverage ­ the core issue which motivated our investigation. We show results for this dataset as a practical application of our work, noting that a pooling depth of d , 20 cannot provide a ground truth for a deep metric [11].",null,null
402,"We use RBP with p ,"" 0.95 for training and for all testing, as a representative weighted-precision metric. RBP supports graded relevance (needed to make use of the estimated background gains we generate); allows residuals to be computed; and with p "","" 0.95 gives similar system orderings to AP and NDCG [15]. e estimated background gain of each document generated via training using RBP0.95 can also be used to compute other weighted-precision measures, such as the truncated metric SDCG@k when k > d.""",null,null
403,1h ps://github.com/xiaolul/opt est,null,null
404,"We consider ve methods for predicting e ectiveness scores,",null,null
405,"three of which are baselines. e rst baseline is the lower bound, LB, which assumes unjudged documents are not relevant; the second is the interpolative estimator of Ravana and Mo at [16] (Equation 4), denoted RM; and the third is the linear model Lin. that is the best of the rank-based approaches described by Lu et al. [12]. ey",null,null
406,"are compared to the loss functions de ned in Equations 7 and 8, denoted La and Lb respectively, with the same loss function used in both stages, and aggregation via Equations 6 and 9.",null,null
407,"We use Linear, Zipf and Discrete Weibull models as initial rankbased estimators [12], and hence have m ,"" 3. Two experiments explore rank stability, categorized by how the judgment set is con-""",null,null
408,structed: (i) pooling based judgments; and (ii) two-strata sampling,null,null
409,based judgments. Rank stability is measured using the approaches,null,null
410,discussed in Section 5. e same baselines are used in the rst rank,null,null
411,"stability evaluation. However, for the sample-based judgments, we consider the metrics InfRBP (p ,"" 0.95) de ned by Equation 5, and Yilmaz and Aslam's InfAP [27] as baselines. roughout the ex-""",null,null
412,"periments, the system scores (plus residuals) and system orderings",null,null
413,"computed using the same metric, but evaluated at the full pool depth (that is, at k ,"" d), are taken as the """"gold standard"""". e truncated metric AP@d is computed as described by Sakai [21]. Setting  . We rst test the two hypotheses in Section 4, with  in Equation 10 normalized by the number of systems. Average (over topics)  values are plo ed against pool depth in the le -hand plot in Figure 3, showing that  decreases as the pooling depth increases.""",null,null
414,"is is as expected, since the increasing pooling depth results in",null,null
415,"a more complete judgment set. Among the plo ed datasets the TB06 collection has the largest average  , and corresponds to a high relevance rate (Table 1). TREC5 is a relatively complete test collection, and hence has the lowest  among the datasets plo ed.",null,null
416,"e center pane in Figure 3 shows the distribution of  across topics for d ,"" 10. Although  is usually low for TREC5, there are still some topics that have high values. e same pa ern is""",null,null
417,"also observable for Rob04 and TREC10. Based on our hypothesis,",null,null
418,"this observation indicates that, on earlier TREC collections, not all topics necessarily require score adjustment even at d , 10.",null,null
419,"To set  we use the earlier datasets TREC5, TREC7 and TREC8 and perform a post-hoc analysis, noting that the majority of relevant",null,null
420,"documents have been identi ed in these collections, and hence",null,null
421,that the computed RMSE should be close to the true error. e,null,null
422,"right-hand pane in Figure 3 shows TREC5 outcomes, with three rank-based models plo ed. Weibull (Wei.) may be an overestimate due to the shaping parameter, and Linear (Lin.) tends to provide low estimates due to the monotonically decreasing nature of the",null,null
423,"model [12]. At rst, neither of the score estimation methods works be er than the lower bound LB, but as  increases, fewer topics need to be estimated, and when  ,"" 0.018, both estimation methods outperform LB. Similar cross-overs occur for TREC7 and TREC8.""",null,null
424,"Prediction Accuracy. We then employed  ,"" 0.018 for the other datasets, obtaining the results shown in Table 2. When  "","" 0 the Lb method outperforms all three baselines (LB, RM and Lin.) in terms of RMSE and Acc%, while the La approach has a higher RMSE than Lb and on earlier datasets (TREC9, TREC10) is slightly worse than the LB and Lin. baselines. at is, the loss function La provides poorer coverage of the true hypothesis space than does Lb . e RM""",null,null
425,41,null,null
426,Session 1A: Evaluation 1,null,null
427,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
428,Mean  (10-3) Percentage (%) RMSE (10-3),null,null
429,30,null,null
430,Dataset,null,null
431,TREC5 Rob04,null,null
432,TREC10 TB06,null,null
433,25,null,null
434,20,null,null
435,15,null,null
436,10 10 20 30 40 50 60 70 80 90 100,null,null
437,Pooling Depth,null,null
438,50 Datasets,null,null
439,Rob04 TREC10,null,null
440,40,null,null
441,TB06 TREC5,null,null
442,30,null,null
443,20,null,null
444,10,null,null
445,0,null,null
446,0 4 8 12 16 20 24 28 32,null,null
447, (10-3),null,null
448,50,null,null
449,Method,null,null
450,LB Lin Wei,null,null
451,45,null,null
452,40,null,null
453,0,null,null
454,4,null,null
455,8 12 16 20 24,null,null
456, (10-3),null,null
457,"Figure 3: Le :  relative to pooling depth d . Middle: distribution of  per topic when d ,"" 10. Right: impact of the threshold on the training set TREC5, with d "", 10.",null,null
458,Dataset d,null,null
459,LB,null,null
460,Lin. RM,null,null
461,La,null,null
462,Lb,null,null
463," , 0  , 0.018",null,null
464," , 0  , 0.018",null,null
465," , 0  , 0.018",null,null
466,10 TREC9 20,null,null
467,30,null,null
468,0.031 (45) 0.012 (59) 0.006 (67),null,null
469,0.056 (22) 0.025 (32) 0.012 (45),null,null
470,0.037 (31) 0.031 (44) 0.013 (51) 0.012 (60) 0.006 (66) 0.006 (68),null,null
471,0.038 (26) 0.030 (44) 0.013 (42) 0.012 (58) 0.005 (63) 0.006 (68),null,null
472,0.031 (41) 0.031 (46) 0.010 (62) 0.012 (61) 0.005 (71) 0.006 (68),null,null
473,10 TREC10 20,null,null
474,30,null,null
475,0.038 (39) 0.016 (53) 0.007 (64),null,null
476,0.064 (16) 0.030 (25) 0.015 (37),null,null
477,0.034 (25) 0.033 (31) 0.015 (45) 0.014 (51) 0.007 (61) 0.007 (63),null,null
478,0.036 (13) 0.031 (27) 0.016 (36) 0.014 (50) 0.007 (55) 0.007 (62),null,null
479,0.027 (34) 0.028 (38) 0.012 (53) 0.012 (55) 0.006 (66) 0.006 (66),null,null
480,10 Rob04 20,null,null
481,30,null,null
482,0.046 (21) 0.020 (34) 0.008 (49),null,null
483,0.088 (5) 0.040 (9) 0.020 (14),null,null
484,0.043 (21) 0.039 (20) 0.015 (33) 0.016 (34) 0.007 (49) 0.007 (52),null,null
485,0.045 (9) 0.039 (17) 0.016 (26) 0.015 (32) 0.007 (47) 0.006 (53),null,null
486,0.039 (20) 0.035 (20) 0.013 (38) 0.016 (35) 0.005 (59) 0.006 (55),null,null
487,10 0.117 (14) 0.082 (14) 0.082 (15) 0.087 (14) 0.072 (15) 0.077 (15) 0.073 (16) 0.077 (16) TB04 20 0.053 (21) 0.039 (23) 0.039 (25) 0.041 (25) 0.035 (28) 0.037 (28) 0.033 (32) 0.036 (31),null,null
488,30 0.026 (26) 0.020 (39) 0.018 (39) 0.019 (38) 0.015 (45) 0.016 (44) 0.015 (44) 0.016 (43),null,null
489,10 0.125 (6),null,null
490,0.080 (5),null,null
491,0.085 (7) 0.085 (7),null,null
492,0.070 (6) 0.070 (8),null,null
493,0.067 (7) 0.067 (9),null,null
494,TB05 20 0.056 (10) 0.041 (10) 0.039 (13) 0.039 (13) 0.034 (14) 0.034 (14) 0.033 (18) 0.033 (19),null,null
495,30 0.028 (16) 0.022 (18) 0.021 (24) 0.021 (24) 0.018 (24) 0.018 (24) 0.017 (29) 0.017 (29),null,null
496,10 0.089 (24) 0.065 (43) 0.059 (43) 0.059 (43) 0.047 (55) 0.047 (55) 0.053 (51) 0.053 (50) TB06 20 0.033 (40) 0.023 (68) 0.021 (66) 0.021 (66) 0.013 (81) 0.013 (81) 0.017 (73) 0.017 (73),null,null
497,30 0.013 (58) 0.007 (87) 0.006 (85) 0.006 (85) 0.003 (94) 0.003 (94) 0.005 (89) 0.005 (89),null,null
498,"Table 2: RMSE and Acc% scores for RBP0.95 for all estimation methods, with d the depth of the reduced pool, and the reference depth d of",null,null
499,each dataset as listed in Table 1. Bold numbers are the lowest RMSE and highest Acc% for that collection at that depth.,null,null
500,"approach performs poorly on all of the earlier datasets, for which the assumption that unjudged documents are equivalent to judged ones is inappropriate. On the larger collections such as TB04/05/06, the gain decreases at a slower rate, making the assumptions in RM more appropriate. e LB approach has similar issues, seen in the TB04/05/06 collections. However, for TB06, smaller RMSE (and larger Acc%) values are achieved when compared to the other collections. is is because the reference depth d ,"" 50 is smaller, resulting in larger residuals. As shown in Figure 3, some of the topics may not necessarily require a score adjustment process, especially in the earlier test collections. is explains why the LB estimator works well on those collections. As expected, applying a threshold  improves the estimation for both La and for the Lin. model, on TREC9, TREC10 and Rob04 test collections. Unsurprisingly, on TB04/05/06, only minor score changes are observed when  "","" 0.018 is used, because the computed  values are larger than""",null,null
501,"the threshold, indicating low coverage of the relevant documents",null,null
502,identi ed. e only unexpected observation occurs on the TB04,null,null
503,"test collection, where the threshold falsely identi es Topic 734 as",null,null
504,"having a ""su cient"" sampling of relevant documents, but around",null,null
505,"48% in the nal judged set are relevant, which increases the RMSE",null,null
506,"value. Table 3 shows the results for a leave-one-group-out experiment at d , 10 (with  ,"" 0), demonstrating the applicability of the framework in adjusting for both system and pooling depth bias.""",null,null
507,"System Ordering Stability on Pooling-Based Judgments. e system orderings derived from the score estimates when compared against the orderings at the reference depth of k ,"" d are shown in Figure 4. Kendall's  correlation was also computed, but the closely-related  distance is used here since it has a strictly positive value. In the rst row, when normalized  distance is measured, the estimation framework gives orderings close to the reference""",null,null
508,42,null,null
509,Session 1A: Evaluation 1,null,null
510,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
511,12,null,null
512,Method,null,null
513,12,null,null
514,Method,null,null
515,12,null,null
516,Method,null,null
517,LB RM Lb,null,null
518,LB RM Lb,null,null
519,LB RM Lb,null,null
520,Lin. La,null,null
521,Lin. La,null,null
522,Lin. La,null,null
523,8,null,null
524,8,null,null
525,8,null,null
526, Distance (10-2),null,null
527, Distance (10-2),null,null
528, Distance (10-2),null,null
529,4,null,null
530,4,null,null
531,4,null,null
532,0,null,null
533,10,null,null
534,20,null,null
535,30,null,null
536,40,null,null
537,50,null,null
538,60,null,null
539,Pooling Depth,null,null
540,0,null,null
541,10,null,null
542,20,null,null
543,30,null,null
544,40,null,null
545,50,null,null
546,60,null,null
547,Pooling Depth,null,null
548,0,null,null
549,10,null,null
550,20,null,null
551,30,null,null
552,40,null,null
553,50,null,null
554,60,null,null
555,Pooling Depth,null,null
556,Distance,null,null
557,20 16 12,null,null
558,8 4 0,null,null
559,10,null,null
560,Method,null,null
561,LB RM Lb Lin. La,null,null
562,20,null,null
563,30,null,null
564,40,null,null
565,50,null,null
566,60,null,null
567,Pooling Depth,null,null
568,Distance,null,null
569,20 16 12,null,null
570,8 4 0,null,null
571,10,null,null
572,Method,null,null
573,LB RM Lb Lin. La,null,null
574,20,null,null
575,30,null,null
576,40,null,null
577,50,null,null
578,60,null,null
579,Pooling Depth,null,null
580,Distance,null,null
581,20 16 12,null,null
582,8 4 0,null,null
583,10,null,null
584,Method,null,null
585,LB RM Lb Lin. La,null,null
586,20,null,null
587,30,null,null
588,40,null,null
589,50,null,null
590,60,null,null
591,Pooling Depth,null,null
592,"Figure 4: System ordering comparisons (RBP0.95) for ve estimators. e rst row uses normalized  distance; the second row uses dist (Equation 11). e columns (from le ) show Rob04, TB04, and TB05, with reference lists using LB at d ,"" 100, d "", 80 and d ,"" 100, respectively.""",null,null
593,Dataset LB,null,null
594,RM,null,null
595,Lin.,null,null
596,La,null,null
597,Lb,null,null
598,Rob04 TB04 TB05 TB06,null,null
599,0.060 (19) 0.126 (4) 0.068 (11) 0.060 (9) 0.050 (19) 0.181 (11) 0.202 (11) 0.131 (9) 0.117 (11) 0.119 (11) 0.170 (6) 0.141 (5) 0.125 (4) 0.110 (4) 0.110 (5) 0.125 (22) 0.185 (32) 0.112 (35) 0.090 (48) 0.086 (46),null,null
600,"Table 3: RMSE and Acc% for leave-out-one-group experiments with d ,"" 10 throughout, averages across groups assuming that""",null,null
601,each group in turn is omi ed from pool construction (RBP0.95).,null,null
602,"ordering across a range of nominal pool depths d . e RM approach performs well on TB04/05, agreeing with the results in Table 2. However, as noted above,  is sensitive to swaps that might be inconclusive. e bo om row of Figure 4 shows the dist measure of Equation 11. Overall, there are situations in which LB performs poorly, and situations in which RM performs poorly. e Lin., La , and Lb methods consistently provide the highest agreements.",null,null
603,"We also carried out paired t-tests and calculated the discrimination ratio for a signi cance level p ,"" 0.05, and compared against the original discrimination ratios. e Lin., La , and Lb estimation methods used with all have only a small e ect on discrimination""",null,null
604,ratio when compared to the use of LB and .,null,null
605,System Ordering Stability on Sample-Based Judgments. We,null,null
606,also show the applicability of our methods on the judgment set,null,null
607,"constructed using a two-strata sampling method [24], which has",null,null
608,been empirically shown to assist when computing inferred metrics.,null,null
609,"On this set of judgments we compute InfAP using trec eval, and InfRBP as de ned in Equation 5. Figure 6 shows that La , Lb and InfRBP give rise to stable system orderings, with normalized ",null,null
610,Estimation Methods,null,null
611,"d',20 CW10",null,null
612,"d',20 TB05",null,null
613,"d',60 TB05",null,null
614,UB,null,null
615,RM,null,null
616,dist,null,null
617,0.125,null,null
618,Lb,null,null
619,0.100,null,null
620,0.075,null,null
621,La,null,null
622,0.050,null,null
623,0.025,null,null
624,Lin,null,null
625,0.000,null,null
626,LB,null,null
627,LB Lin La Lb RM UB LB Lin La Lb RM UB LB Lin La Lb RM UB,null,null
628,Estimation Methods,null,null
629,Figure 5: Normalized  distance between system orderings gen-,null,null
630,erated by di erent estimation methods based on a pool of depth,null,null
631,"d ,"" 20, and on TB05 based on pool depths of d "", 20 and d , 60.",null,null
632,"distance scores below 0.05 across all collections. When dist is measured, La outperforms InfRBP on all collections but TREC9, while Lb outperforms InfRBP except on TB05. e slightly worse outcome for La on TREC9 is a consequence of the increase in the number of signi cantly di erent system pairs. Note the more",null,null
633,variable outcomes generated when InfAP is used as the metric,null,null
634,driving the system orderings.,null,null
635,"Predictions in ClueWeb. As a nal test of our approach, we examine the CW10 collection. It has a shallow pool depth (d ,"" 20), meaning that validation is not possible, as there is no deep-pool reference ordering. Instead, we compute the normalized  distance between each pair of estimation methods, and simply record how""",null,null
636,"much the rankings di er, as shown in Figure 5. e UB estimator assumes that all unjudged documents are relevant. As a reference",null,null
637,43,null,null
638,Session 1A: Evaluation 1,null,null
639,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
640,Method,null,null
641,100,null,null
642,10,null,null
643,InfAP La,null,null
644,InfRBP Lb,null,null
645, Distance (10-2) Distance,null,null
646,40 4,null,null
647,2,null,null
648,1,null,null
649,TREC9,null,null
650,TREC10,null,null
651,Rob04,null,null
652,Collection,null,null
653,TB04,null,null
654,TB05,null,null
655,20,null,null
656,Method,null,null
657,10,null,null
658,InfAP La,null,null
659,InfRBP Lb,null,null
660,TREC9,null,null
661,TREC10,null,null
662,Rob04,null,null
663,Collection,null,null
664,TB04,null,null
665,TB05,null,null
666,"Figure 6: System ordering comparisons on a two-strata sampled judgment set, repeated ten times. Judgments are to depth d ,"" 10, plus a 10% random sample of remaining documents to depth 100 to form the second stratum. Note the logarithmic vertical scales.""",null,null
667,"point, we also compute the same values for TB05, at two depths, d , 20 and d ,"" 60. At the la er depth all estimation approaches tend to agree with each other. On TB05, all of the estimation results, including UB, tend to agree on the system ordering. However, on CW10, there is clear uncertainty, con rming that d "", 60 is a more robust pool depth for TB05 than is d , 20 on either TB05 or CW10 when seeking to apply RBP0.95 as an evaluation metric. Great caution should be exercised when the d , 20 CW10 judgments are used for anything other than shallow metrics.",null,null
668,7 CONCLUSIONS,null,null
669,"We have presented new methods to improve system comparisons in batch IR evaluation, with the key idea being to predict a gain value for each unjudged document. We show that estimation is a viable technique to predict scores for deep evaluation metrics when limited judgments are available, including the case when the judgments are obtained using strati ed sampling rather than pooling. One important aspect of our approach is to make decisions on when to adjust topics, instead of treating all topics equally.",null,null
670,"A secondary contribution is the development of a new technique to more precisely compare system orderings. By focusing on swaps that are conclusive, our weighted rank correlation coe cient dist can be used to measure the stability of a variety of estimation techniques. Using dist, we show that estimation improves our ability to score and compare systems using limited judgments.",null,null
671,"It must be noted, however, that the estimation is built on the m rank-based ed models, each of which requires that when constructing the judgment set, documents up to some rank d be fully judged. is means that for some sampling-based judgment approaches, the proposed method is not applicable. Second, while we show that our estimation methods can also account for system bias to some extent, outcomes might be further improved by introducing more randomization into the optimization framework. Hence, in answer to the question posed in the title, our answer remains a somewhat cautious ""be er than before"", rather than a ""yes"".",null,null
672,Funding. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP140103256 and DP170102231).,null,null
673,REFERENCES,null,null
674,"[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. 2006. A statistical method for system evaluation using incomplete judgments. In Proc. SIGIR. 541­548.",null,null
675,"[2] C. Buckley, D. Dimmick, I. Soboro , and E. M. Voorhees. 2007. Bias and the limits of pooling for large collections. Inf. Retr. 10, 6 (2007), 491­508.",null,null
676,[3] C. Buckley and E. M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proc. SIGIR. 25­32.,null,null
677,"[4] S. Bu¨ cher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboro . 2007. Reliable information retrieval evaluation with incomplete and biased judgements. In Proc. SIGIR. 63­70.",null,null
678,"[5] A. Chao and S. Lee. 1992. Estimating the number of classes via sample coverage. J. American Statistical Association 87, 417 (1992), 210­217.",null,null
679,"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proc. CIKM. 621­630.",null,null
680,"[7] J. K. Jayasinghe, W. Webber, M. Sanderson, and J. S. Culpepper. 2014. Improving test collection pools with machine learning. In Proc. Aust. Doc. Comp. Symp. 2­9.",null,null
681,[8] R. Kumar and S. Vassilvitskii. 2010. Generalized distances between rankings. In Proc. WWW. 571­580.,null,null
682,"[9] A. Lipani, M. Lupu, and A. Hanbury. 2015. Spli ing water: Precision and antiprecision to reduce pool bias. In Proc. SIGIR. 103­112.",null,null
683,"[10] A. Lipani, M. Lupu, E. Kanoulas, and A. Hanbury. 2016. e solitude of relevant documents in the pool. In Proc. CIKM. 1989­1992.",null,null
684,"[11] X. Lu, A. Mo at, and J. S. Culpepper. 2016. e e ect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445.",null,null
685,"[12] X. Lu, A. Mo at, and J. S. Culpepper. 2016. Modeling relevance as a function of retrieval rank. In Proc. AIRS. 3­15.",null,null
686,"[13] A. Mo at, P. omas, and F. Scholer. 2013. Users versus models: What observation tells us about e ectiveness metrics. In Proc. CIKM. 659­668.",null,null
687,"[14] A. Mo at, W. Webber, and J. Zobel. 2007. Strategic system comparisons via targeted relevance judgments. In Proc. SIGIR. 375­382.",null,null
688,"[15] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Information Systems 27, 1 (2008), 2:1­2:27.",null,null
689,"[16] S. D. Ravana and A. Mo at. 2010. Score estimation, incomplete judgments, and signi cance testing in IR evaluation. In Proc. AIRS. 97­109.",null,null
690,[17] S. E. Robertson. 2007. On document populations and measures of IR e ectiveness. In Proc. ICTIR. 9­22.,null,null
691,[18] T. Sakai. 2007. Alternatives to BPref. In Proc. SIGIR. 71­78. [19] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,null,null
692,pool depth bias. In Proc. SIGIR. 691­692. [20] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,null,null
693,"system bias. In Proc. CIKM. 581­590. [21] T. Sakai. 2014. Metrics, statistics, tests. In Bridging Between Information Retrieval",null,null
694,"and Databases, N. Ferro (Ed.). Springer, 116­163. [22] T. Schnabel, A. Swaminathan, P. I. Frazier, and T. Joachims. 2016. Unbiased",null,null
695,"comparative evaluation of ranking functions. In Proc. ICTIR. 109­118. [23] T. Schnabel, A. Swaminathan, and T. Joachims. 2015. Unbiased ranking evaluation",null,null
696,on a budget. In Proc. WWW. 935­937. [24] E. M. Voorhees. 2014. e e ect of sampling strategy on inferred measures. In,null,null
697,Proc. SIGIR. 1119­1122. [25] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiment and Evaluation in,null,null
698,Information Retrieval. e MIT Press. [26] W. Webber and L. A. F. Park. 2009. Score adjustment for correction of pooling,null,null
699,bias. In Proc. SIGIR. 444­451. [27] E. Yilmaz and J. A. Aslam. 2008. Estimating average precision when judgments,null,null
700,"are incomplete. Knowledge and Information Systems 16, 2 (2008), 173­211. [28] E. Yilmaz, E. Kanoulas, and J. A. Aslam. 2008. A simple and e cient sampling",null,null
701,method for estimating AP and NDCG. In Proc. SIGIR. 603­610. [29] Z. Zhou. 2012. Ensemble Methods: Foundations and Algorithms. CRC press. [30] J. Zobel. 1998. How reliable are the results of large-scale information retrieval,null,null
702,experiments?. In Proc. SIGIR. 307­314.,null,null
703,44,null,null
704,,null,null

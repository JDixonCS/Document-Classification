,sentence,label,data
0,Session 4A: Evaluation 2,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Retrieval Consistency in the Presence of ery Variations,null,null
3,Peter Bailey,null,null
4,"Microso Canberra, Australia",null,null
5,Falk Scholer,null,null
6,"RMIT University Melbourne, Australia",null,null
7,ABSTRACT,null,null
8,"A search engine that can return the ideal results for a person's information need, independent of the speci c query that is used to express that need, would be preferable to one that is overly swayed by the individual terms used; search engines should be consistent in the presence of syntactic query variations responding to the same information need. In this paper we examine the retrieval consistency of a set of ve systems responding to syntactic query variations over one hundred topics, working with the UQV100 test collection, and using Rank-Biased Overlap (RBO) relative to a centroid ranking over the query variations per topic as a measure of consistency. We also introduce a new data fusion algorithm, Rank-Biased Centroid (RBC), for constructing a centroid ranking over a set of rankings from query variations for a topic. RBC is compared with alternative data fusion algorithms.",null,null
9,"Our results indicate that consistency is positively correlated to a moderate degree with ""deep"" relevance measures. However, it is only weakly correlated with ""shallow"" relevance measures, as well as measures of topic complexity and variety in query expression.",null,null
10,ese ndings support the notion that consistency is an independent property of a search engine's retrieval e ectiveness.,null,null
11,CCS CONCEPTS,null,null
12,·Information systems Retrieval e ectiveness; Test collections;,null,null
13,KEYWORDS,null,null
14,"Test collections, retrieval consistency, semantic e ectiveness",null,null
15,1 INTRODUCTION,null,null
16,"Evaluating search e ectiveness has several aspects. One aspect that has been especially popular is calculating average scores according to some relevance measure (such as NDCG or AP) over a set of topics and associated queries for some common corpus of information. In the batch evaluation methodology, di erent systems are compared using the same measure, and statistical tests are applied",null,null
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080839",null,null
18,Alistair Mo at,null,null
19,"e University of Melbourne Melbourne, Australia",null,null
20,Paul omas,null,null
21,"Microso Canberra, Australia",null,null
22,"to determine whether the di erence in performance is likely due to factors other than chance. Alternative methods for determining relevance include user studies, online interleaving, and A/B testing. What is important to note is that retrieval e ectiveness encompasses more than just relevance.",null,null
23,"Batch evaluation has typically used only a single query per topic, although a number of researchers working on early test collections advocated and explored the e ect of using multiple queries per topic (see work by Spa¨rck Jones and van Rijsbergen [35], Belkin et al. [5] and Buckley and Walz [7] among others). Recent work by Bailey et al. [3] and Koopman and Zuccon [22] has returned to this theme, resulting in new test collections with large numbers of query variations responding to each topic's information need.",null,null
24,"e availability of such test collections allows us to consider a new dimension in assessing the retrieval e ectiveness of search engines ­ namely, how consistent they are when returning results in response to query variations that address the same information need. e importance of consistency can be understood when we consider simple examples like mis-spellings. For example ""facebook"", ""facebok"", and ""faecbook"" pre y clearly all want to nd the Facebook home page. Consistency also applies to more complex examples involving synonyms (for example, ""health bene ts of vitamin c"" and ""health bene ts of ascorbic acid"") or entirely rephrased needs (for example, ""how much does a raspberry pi cost"" and ""price of raspberry pi computer""). In each of these cases, we can contemplate that there exists an ideal ranked set of relevant results drawn from the corpus. Test collections are premised on this principle, where the ideal set for a topic is discovered through judging a document pool formed from di erent rankings. An ideal search engine would return (only) this set of results given a query for a topic, and the di erence in relevance from what is actually returned and this ideal ranking is captured by a relevance measure. Equally, given a set of syntactic query variations, an ideal search engine would return this ideal ranking of results, independent of the query variation.",null,null
25,"Indeed, much research in information retrieval seeks to tackle exactly this problem of nding an ideal set of results without relying solely on the original query's syntactic expression. For instance, query re-writing techniques such as spelling correction [11], term stemming [24], query expansion [33], and query substitutions [19] are used to manipulate the user-entered query and thereby extract a be er set of documents from the index. Stemming and stopword removal [23, 25] may also be used in indexing processes or within the matching algorithms at query execution time.",null,null
26,"Two strands of research have investigated how to combine rankings to improve relevance e ectiveness: data fusion (for example,",null,null
27,395,null,null
28,Session 4A: Evaluation 2,null,null
29,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
30,"Belkin et al. [6]), which merges rankings from di erent query representations; and distributed information retrieval or meta-search (for example, Callan [8]), which merges rankings from di erent underlying search engines or indexes. ese techniques have been assessed principally from the standpoint of improving relevance overall, as measured by the relevance score of the resulting ranking.",null,null
31,"People use many di erent expressions to describe the same information need (see, for example, Furnas et al. [15]). Even when re nding a single information resource, the same person may use di erent queries [37]. Bailey et al. [2] give evidence that the e ect of query variation on relevance scores dwarfs that of system and topic e ects. We believe that these ndings make it important to consider new approaches to characterizing e ectiveness, including ones that address query variation for a single information need.",null,null
32,"We propose that the consistency in rankings of a system when faced with many di erent query variations for a single topic can be one such measure. e Rank-Biased Overlap measure developed by Webber et al. [38] is adopted to characterize consistency, and used with the UQV100 test collection [3] to investigate consistency across a set of ve systems. Due to the scale of variations within UQV100 (19­101 unique query variations per topic, for 100 topics), each individual topic has a similar number of queries as might be found in an entire query-only processed test collection like the TREC 2014 Web track [10].",null,null
33,"To determine relative consistency for a single system and topic combination, RBO requires us to declare some reference ranking against which the individual rankings for each query variation can be compared. We develop a new fusion algorithm, the RankBiased Centroid (RBC), drawing inspiration from both RBO and Rank-Biased Precision [27], to determine this reference ranking.",null,null
34,We consider these research questions in regard to RBC fusion:,null,null
35,RQ-F1 Do RBC rankings outperform the initial query rankings for a system?,null,null
36,RQ-F2 How does RBC compare to other data fusion algorithms in relevance e ectiveness?,null,null
37,RQ-F3 Does combining both query variations and systems for RBC outperform query variations-only RBC?,null,null
38,"en, in connection with consistency, we ask:",null,null
39,RQ-C1 Do topics vary in consistency? RQ-C2 Does consistency vary with the number of query variations,null,null
40,or with changes in topic complexity? RQ-C3 Do systems vary in consistency? RQ-C4 Are increases in per-topic consistency for a system inde-,null,null
41,pendent of increases in relevance for a system?,null,null
42,2 RELATED WORK,null,null
43,2.1 Data fusion,null,null
44,"Data fusion ­ combining evidence from di erent sources ­ is a widely-studied problem. In IR fusion is typically applied when evidence from multiple ranked answer lists needs to be combined into a single ranked list, for example in meta-search, where results from multiple independent search systems are combined into a single ranking [1], and in multi-lingual retrieval, where results from searches across collections in di erent languages are combined into a single answer list [16].",null,null
45,"Data fusion approaches can be broadly grouped into those that use the ranker's score (that is, the value assigned by a ranking function) of each document in a results list, and those that make use only of the rank position of each document in the answer list. Perhaps the most well-known approaches in the former category are by Fox and Shaw [13], including CombMAX, where the nal score of a document is the maximum of the ranker scores that it received in any input ranked list; CombSUM, where the nal document score is the total of the ranker's scores that it received in the input lists; and CombMNZ, where the nal document score is calculated as in CombSUM but further multiplied by the number of input lists in which the document appears, thereby promoting those documents that were retrieved in multiple lists. e document ranker scores in the input ranked lists may also be normalized in di erent ways, including linear re-scaling into a chosen range [39], or by controlling an upper bound based on the sum or variance of the input scores [28].",null,null
46,"For fusion based only on rank information, techniques from social choice theory such as the Borda count and Condorcet criterion have been applied. In the Borda count [1], each candidate (document) receives a score determined by how many other candidates were ranked lower, with these scores summed across all ballots (input lists). e Condorcet criterion instead determines an outcome based on which candidate achieves the highest number of wins based on pairwise comparisons with all other candidates [12, 29]",null,null
47,"e impact of data fusion techniques on e ectiveness can vary from case to case, leading Wu and McClean [40] to investigate approaches for predicting the performance impact of applying fusion.",null,null
48,"eir results showed that the selective application of fusion, based on features such as the number of component result lists and the overlap of items in these lists, can further enhance the positive impact on nal retrieval e ectiveness.",null,null
49,"Prior work that has speci cally considered data fusion in the context of multiple queries for the same underlying information need was carried out by Belkin et al. [5], who investigated the e ects of combining ve independent Boolean query formulations for ten TREC topics, and demonstrated that fusing results can substantially boost performance. Subsequent work using the TREC-2 collection further demonstrated that good methods for fusing the results of multiple queries can lead to results that are be er than those of the best single query [6]. Pickens et al. [30] also con rmed that combining multiple queries for the same intent boosts e ectiveness.",null,null
50,2.2 Measuring consistency,null,null
51,"In IR, as in many other domains, it can be important to compare the similarity, or consistency, of groups of things. ese groups may be conjoint (consisting of the same items) or disjoint (one group may include items that do not occur in the other group), and may be setbased (where there is no known or inferred ordering of the items) or ordered (where the sequence in which the items occur ma ers). Typical examples where one might wish to compare groups include measuring the similarity of the answer lists returned by two search engines in response to the same query; or the similarity of the e ectiveness ranking of a set of several di erent retrieval systems, when evaluated over two di erent test collections. A wide range of list similarity measures have been proposed and applied.",null,null
52,396,null,null
53,Session 4A: Evaluation 2,null,null
54,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
55,"One of the similarity measures most commonly used in IR is Kendall's  , a rank correlation coe cient that calculates the normalized number of concordant pairs (items that are ordered the same in two rankings) minus discordant pairs (items that are ordered di erently in the two rankings), resulting in a score between 1 (perfect agreement between the two rankings) and -1 (perfect disagreement) [9]. Kendall's  is a measure that assumes conjoint ranked lists, that is, the lists are permutations of the same set of items; it is also an unweighted measure, where each pair contributes equally to the outcome, wherever it occurs in the ranking. However, when comparing ranked answer lists, items that are higher in the ranking are more important (users pay more a ention to top-ranked results); similarly, when comparing rankings of system e ectiveness scores, di erences between the top systems are generally of greater interest than di erences between systems that perform less well. TauAP [41] addresses a number of these weaknesses, while Rank-Biased Overlap (RBO) [38] addresses even more. RBO is a generalized measure of similarity between rankings based on a probabilistic user model, and readily handles non-conjoint lists. It applies a geometric sequence of weights to items in the lists; with the emphasis of the weighting adjustable via a user persistence parameter , which also determines the probability that the user will reach a certain rank. Inspired by RBO, Tan and Clarke [36] de ne a family of Maximized E ectiveness Di erence (MED) measures, each based on an IR e ectiveness metric (and hence a di erent underlying user model).",null,null
56,"Jiang et al. [18] examine ranking consistency in web search from the basis of nding similar classes of queries (based on sharing common entity types in a knowledge base), and preserving the relative ordering of URL domains in the rankings for queries belonging to the same class, for example, people who are professional basketballers but also appear in movies. Large scale web log click data is used to derive their models of class similarity, based on URL pa erns. Jiang et al. develop a consistency measure based on Kendall's  across all pairs of queries belonging to the same class.",null,null
57,"Finally, Zuccon et al. [43] present an evaluation framework using mean variance analysis over retrieval e ectiveness, for both intertopic and intra-topic sources of variation. Systems are preferred, all other things being equal, when one system is more stable than another in the presence of such variation.",null,null
58,3 RANK-BIASED CENTROIDS,null,null
59,"As noted in the previous section, a range of methods have been proposed for constructing fused rankings, given an initial set of same-basis source rankings. In this section we introduce a further approach: the rank-biased centroid, or RBC.",null,null
60,3.1 User model for Borda fusion,null,null
61,"To motivate the discussion, consider the four alternative rankings R1, R2, R3, and R4 shown in the le side of Figure 1, with each of the elements denoted by a le er of the alphabet. One run has ordered all of the seven di erent elements, while the other three are truncated and omit one or more of the items ­ a typical situation. Moreover, note that even if they are all of the same length, the runs might contain di erent subsets of a larger group of elements ­ they need not be permutations of each other. Finally, note that in the",null,null
62,"Rank R1 R2 R3 R4  , 0.6  , 0.8  , 0.9",null,null
63,1 A B A G A (0.89) D (0.61) D (0.35) 2 D D B D D (0.86) A (0.50) C (0.28) 3 B E D E B (0.78) B (0.49) A (0.27) 4 C C C A G (0.50) C (0.37) B (0.27) 5 G ­ G F E (0.31) G (0.37) G (0.23) 6 F ­ F C C (0.29) E (0.31) E (0.22) 7 ­ ­ E ­ F (0.11) F (0.21) F (0.18),null,null
64,"Figure 1: Example of RBC fusion: four example rankings (le ); and three di erent fused orderings (right). Note that the RBC weights are shown to two decimal places only, and there are no score ties.",null,null
65,"most general scenario, there are situations in which the provided rankings are pre xes of longer lists, themselves of unknown (and perhaps even in nite) length.",null,null
66,"e Borda scoring process assigns a weight to item A of 7+7+4 ,"" 18 (note that A does not appear in ranking R2), tying it with B, and placing it behind item D, which gets 6 + 6 + 6 + 5 "","" 23 points. e overall Borda ordering is D, A"",""B, C, G, E, F. In the Borda regime, swapping the two adjacent items at any pair of consecutive ranks gives one of the items a +1 score change, and the other item a -1 change. is occurs regardless of whether the swap takes place at rank 1, at rank 10, or at rank 100. at is, all binary item preferences as expressed in the visible input rankings are regarded as being of equal merit; and any preferences that may not have been surfaced (in the case that the provided rankings are pre xes) are ignored.""",null,null
67,"To create a user model that captures this behavior we can imagine a universe of agents, each of whom acts independently of the others, but follows the same simple rule: they randomly pick a depth d according to some probability distribution, they examine all of the input rankings to depth d (at most ­ but less if the rankings are shorter), and they sort the pool of items according to decreasing order of the number of times they saw each item in their set of length-d pre xes. e nal fused ranking is then a probabilistic expectation over all agents of the orderings that were constructed.",null,null
68,"Given this overall probabilistic structure, the Borda ordering is derived when the probability distribution used by the agents is taken to be P(d , x) ,"" 1/n; that is, each agent is equally likely to select a pre x of any length between 1 and n, where n is the number of items. e Borda score for an item is then proportional to the expected value of the total number of times it was observed in the individual top-d sets of the probabilistic universe of agents.""",null,null
69,3.2 An alternative weighting regime,null,null
70,"Because there are many situations in which the supplied rankings are assumed to be pre xes of arbitrary-length ones, we contend that swaps near the heads of each of the rankings are somehow more indicative of preference than swaps deeper in the rankings. In the example shown in the le side of Figure 1, swapping A and D in ranking R1 has the same net e ect on A's Borda score as does swapping A and F in ranking R4, but the la er swap might seem to be somewhat less damaging to A, since in R4 it has already been deprecated by the person or system that generated that ordering.",null,null
71,397,null,null
72,Session 4A: Evaluation 2,null,null
73,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
74,"Instead of assigning a Borda weight of (n - x + 1)/n (in a normalized sense) to each item at rank 1  x  n when the rankings are over n items, we suggest that a geometrically decaying weight function be employed, with the distribution of d over depths x given not by 1/n, but instead by (1 - )x-1 for some value 0    1 determined by considering the purpose for which the fused ranking is being constructed. e parameter  is the persistence, or patience of the imagined universe of probabilistic agents; and use of a geometric sequence models the same behavior as is embedded in the e ectiveness metric RBP [27] and in the rank correlation coe cient RBO [38] ­ namely, that the person examining the rankings always examines the rst item in each, and therea er proceeds from the i th to the i + 1 st with conditional probability , and ends their search at the i th with conditional probability (1 - ). In an implementation the fused ranking is determined by assigning a weight of (1 - )i-1 to each item at depth i in any of the rankings, and then summing over items and sorting by total weight.",null,null
75,ere are a number of bene ts of this proposed approach:,null,null
76,"· as already motivated, greater emphasis is placed on the earlier preferences than on deeper ones in each ranking;",null,null
77,"· an upper bound on the lengths of the rankings is not required, nor are the rankings required to be the same length (the Borda method shares this exibility, albeit somewhat awkwardly);",null,null
78,"· if further items are added at the tail of any of the rankings, the resultant item scores converge smoothly.",null,null
79,"As extreme values, consider  , 0 and  , 1. When  ,"" 0, the agents only ever examine the rst item in each of the input rankings, and the fused output is by decreasing score of rst preference; this is somewhat akin to a rst-past-the-post election regime. When  "","" 1, each agent examines the whole of every list, and the fused ordering is determined by the number of lists that contain each item ­ a kind of """"popularity count"""" of each item across the input sets. In between these extremes, the expected depth reached by the agents viewing the rankings is given by 1/(1 - ). For example, when  "","" 0.9, on average the rst 10 items in each ranking are being used to contribute to the fused ordering; of course, in aggregate, across the whole universe of agents, all of the items in every ranking contribute to the overall outcome.""",null,null
80,"In practice, what this means is that di erent values of  between 0 and 1 give rise to di erent fused orderings, balancing topweightedness and exhaustivity. e right side of Figure 1 shows the orderings generated for the rankings R1, R2, R3, and R4, discussed earlier, for three di erent values of . e total weight associated with each item (to two decimals) is also shown. Note how item A is top-ranked when the ranking agents are relatively impatient, and (on average) abandon the ranking early ( ,"" 0.6), but that if the fused ranking is assembled on a more patient basis ( "", 0.8 and  ,"" 0.9), items D and C become preferred, and A is demoted.""",null,null
81,"As with Borda fusion, unanimous preferences are respected: in the example, because C is below D in all of the four input rankings, it must also fall below D in the fused ranking, regardless of the value of . e di erences that arise as  varies are limited only to the elements where there is disagreement in the input rankings as to their respective ordering. ese are, arguably, exactly the elements that we might be interested in focusing on.",null,null
82,3.3 Discussion,null,null
83,"We have de ned RBC in terms of a one-state user model [27]. Another way of looking at it is as an estimation of the normalized document scores used in CombMNZ and CombSUM. By assigning decreasingly small weights to documents further down the ranking, RBC can be viewed as seeking to approximate the long tail of document-query similarity scores generated by disjunctive ranked retrieval systems. Functions other than the geometric sequence might also be suitable for use, for example, Zip an weightings.",null,null
84,4 FUSION OVER QUERY VARIATIONS,null,null
85,"is section explores the practical bene t of fusing over query variations, and also shows that fusion over systems retains some of its power even a er query variations have been incorporated.",null,null
86,4.1 e UQV100 collection,null,null
87,"e UQV100 test collection is made up of 100 topics and associated information need statements, with approximately 100 individual query variations per topic; 10,835 in total [3]. When spelling correction and normalization are applied there are between 19 and 101 unique query variations per topic; 5,765 in total. ere are also 55,587 relevance judgments available in regard to those 100 topics, covering ClueWeb12-CatB documents pooled from ve systems, spanning three separate search engine code bases, and ve di erent ranking algorithms [26]. We again employ the runs for those ve contributing systems, anonymized here as Systems 1, 2, 3, 4, and 5. Due to some processing anomalies we observed in the run data, the overlapping set of unique query variations processed by all ve systems contains 5,736 queries. Each system run contains a ranking of length 200 for each of those distinct queries. For de niteness we ordered the set of queries for each topic by decreasing frequency according to crowd-based process used to originally collect them [2], with ties broken randomly.",null,null
88,4.2 Fusion over query variations,null,null
89,"Table 1 provides a detailed evaluation of approaches for fusion as applied to query variations. Each pane of the table gives results for one e ectiveness metric, and within each pane the columns represent increasing numbers of query variations (note that for",null,null
90," 20, the legend , x indicates that as many as x query variations were used ­ some topics had fewer than the listed number of variations). Note that , 1 makes use of the most frequentlysuggested query for each of the UQV100 topics; ,"" 2 adds the second most frequently suggested one; and so on. Stepping across each row thus involves more and more input runs being used to form each output run, and as can be seen, e ectiveness scores (with a few exceptions) increase. Many of the fusion methods give very similar e ectiveness. Even so, there are some notable pa erns:""",null,null
91,"· using as few as , 2 query variations gives improved e ectiveness (relative to the , 1 baseline) for all metrics and all fusion methods;",null,null
92,"· for all of the metrics, RBC with high values of p provides good fused outcomes, comparable with or be er than those achieved by CombMNZ and Borda;",null,null
93,"· for the two recall-based metrics, RBC-based fusion provides markedly be er outcomes than Borda and CombMNZ;",null,null
94,398,null,null
95,Session 4A: Evaluation 2,null,null
96,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
97,Fusion,null,null
98,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",null,null
99,"RBC,  ,"" 0.9 0.491 0.490 RBC,  "","" 0.95 0.497 0.504 RBC,  "","" 0.98 0.505 0.511 RBC,  "", 0.99 0.511 0.520",null,null
100,0.510 0.516 0.522 0.525,null,null
101,0.516 0.526 0.535 0.534,null,null
102,0.524 0.529 0.533 0.533,null,null
103,Borda,null,null
104,0.511 0.522 0.527 0.534 0.532,null,null
105,CombMNZ 0.513 0.521 0.527 0.534 0.532,null,null
106,"(a) RBP0.85, common baseline 0.474",null,null
107,0.522 0.526 0.532 0.534,null,null
108,0.535,null,null
109,0.531,null,null
110,Fusion,null,null
111,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",null,null
112,"RBC,  ,"" 0.9 0.490 0.490 RBC,  "","" 0.95 0.496 0.504 RBC,  "","" 0.98 0.503 0.510 RBC,  "", 0.99 0.508 0.519",null,null
113,0.506 0.514 0.519 0.523,null,null
114,0.516 0.526 0.533 0.532,null,null
115,0.523 0.528 0.533 0.531,null,null
116,Borda,null,null
117,0.507 0.520 0.525 0.530 0.528,null,null
118,CombMNZ 0.509 0.517 0.524 0.531 0.528,null,null
119,"(b) INST, common baseline 0.471",null,null
120,0.522 0.527 0.532 0.532,null,null
121,0.532,null,null
122,0.528,null,null
123,Fusion,null,null
124,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",null,null
125,Fusion,null,null
126,"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",null,null
127,"RBC,  ,"" 0.9 0.222 0.230 0.248 0.265 0.288 0.299 RBC,  "","" 0.95 0.223 0.234 0.256 0.280 0.297 0.303 RBC,  "","" 0.98 0.225 0.239 0.260 0.275 0.281 0.284 RBC,  "", 0.99 0.226 0.241 0.254 0.264 0.266 0.270",null,null
128,"RBC,  ,"" 0.9 0.437 0.454 RBC,  "","" 0.95 0.438 0.458 RBC,  "","" 0.98 0.440 0.462 RBC,  "", 0.99 0.442 0.464",null,null
129,0.484 0.505 0.539 0.553 0.489 0.519 0.545 0.554 0.492 0.510 0.521 0.525 0.481 0.494 0.499 0.505,null,null
130,Borda,null,null
131,0.226 0.239 0.251 0.260 0.262 0.267 Borda,null,null
132,0.442 0.464 0.478 0.489 0.493 0.502,null,null
133,CombMNZ 0.227 0.240 0.252 0.260 0.273 0.266 CombMNZ 0.442 0.463 0.479 0.490 0.494 0.500,null,null
134,"(c) AP, common baseline 0.204",null,null
135,"(d) NDCG, common baseline 0.409",null,null
136,"Table 1: Fusion over query variations, average e ectiveness across 100 UQV topics for runs generated from di erent numbers of query variations and according to di erent fusion approaches for System 1: (a) RBP0.85 scores; (b) INST scores; (c) AP scores; (d) NDCG scores.",null,null
137,"ery variations are sorted in decreasing order of occurrence frequency in the UQV100 collection. e baseline scores for ,"" 1 (that is, executing the single most popular query variation) are shown under each table. So that pa erns of behavior can be seen, the two largest""",null,null
138,"values in each column are highlighted in bold. Daggers indicate arrangements in which the RBC-based system was signi cantly be er than the corresponding Borda run (one-sided paired t-tests with p < 0.05). No signi cant di erences were detected for CombMNZ fusion, or using RBP0.85 or INST. Similar behavior was observed for other combinations of system and metric (not shown here).",null,null
139,Fusion,null,null
140,RBP0.85,null,null
141,Metric,null,null
142,INST,null,null
143,AP,null,null
144,NDCG,null,null
145,"RBC,  ,"" 0.9 RBC,  "","" 0.95 RBC,  "","" 0.98 RBC,  "", 0.99",null,null
146,0.503 0.508 0.506 0.505,null,null
147,0.501 0.506 0.504 0.502,null,null
148,0.217 0.219 0.220 0.217,null,null
149,0.441 0.442 0.443 0.440,null,null
150,Borda,null,null
151,0.503 0.500 0.215 0.440,null,null
152,CombMNZ,null,null
153,0.506 0.505 0.219 0.442,null,null
154,"Table 2: Fusion over ve di erent retrieval systems, based on one query variation ( , 1). All numbers are average e ectiveness scores over the 100 topics in the UQV100 collection. Single-system scores for the four metrics are shown in the rst two columns of Table 3. e largest two entries in each column are shown in bold. Daggers represent signi cance relative to Borda fusion (p < 0.05).",null,null
155,"· moreover, the greater the number of query variations being fused, the smaller the value of  needed to obtain those outcomes.",null,null
156,We also explored round-robin fusion and CombSUM fusion; the former was never competitive (and e ectiveness decreased as query variations were added); and CombSUM typically gave performance slightly inferior to CombMNZ.,null,null
157,"Metric Initial, , 1 mean max",null,null
158,"Fused, , all mean max",null,null
159,"Fused2, s , 5 mean gain",null,null
160,RBP0.85 INST AP NDCG,null,null
161,0.474 0.470 0.190 0.400,null,null
162,0.487 0.481 0.204 0.411,null,null
163,0.530 0.532 0.268 0.517,null,null
164,0.557 0.558 0.303 0.554,null,null
165,0.559 0.563 0.303 0.561,null,null
166,+18% +20% +59% +41%,null,null
167,"Table 3: Summary of e ectiveness gains achieved by fusing rst over query variations, and then second over systems. e rst four data columns are mean and maximum average scores over ve systems; the ""gain"" is relative to the initial system average in the",null,null
168,rst column. All fusion is carried out using RBC0.95.,null,null
169,4.3 Fusion over systems,null,null
170,"Table 2 shows the outcome of applying fusion across the ve systems used in our experiments. A single query is used in each input run ( ,"" 1), and fusion applied to the ve rankings for each topic. In this se ing, all methods give comparable improvements in e ectiveness, with Borda fusion marginally the worst of them.""",null,null
171,4.4 Double fusion,null,null
172,"Table 3 provides an overall summary of the e ectiveness gains that can be achieved by fusing over query variations and then over systems, with RBC0.95 used at all fusing steps. Starting on the le ,",null,null
173,"ve systems each execute one query variation ( , 1) for each of the",null,null
174,399,null,null
175,Session 4A: Evaluation 2,null,null
176,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
177,Description,null,null
178,RBP0.85 INST AP NDCG,null,null
179,"All queries ( , all) First queries ( , 1) Best query per topic",null,null
180,0.405 0.474 0.712,null,null
181,0.394 0.151 0.336 0.471 0.204 0.409 0.718 0.271 0.503,null,null
182,"Table 4: Average metric scores for all queries per topic; for the most popular query per topic; and for per-metric per-topic ""omniscient"" query selections. System 1 is used throughout. ese scores can be directly compared with those shown in the four panes of Table 1.",null,null
183,UQV100 topics; this can be regarded as being the starting baseline condition (no fusion performed) for both this table and Table 2. e,null,null
184,"ve-way mean ""average over 100 topics"" and ve-way maximum ""average over 100 topics"" values show typical behavior for ve good retrieval systems when measured using a single query per topic. If each of those ve systems is given more query variations, and generates a single fused run for each of the 100 topics as its output, the values in the middle pair of columns arise. Substantial performance improvements can be observed, and the means of the",null,null
185,"ve ""fusion over query variations"" systems handsomely exceeds the best average score of the ve original systems.",null,null
186,"e third pair of columns in Table 3 then shows the outcome of fusing the ve system runs generated a er the query variations have been folded in. e mean scores shown (now with just a single ranking for each of the 100 topics) exceed the previous maximum scores in three of four cases, and exceed the middle-column mean scores in all four cases. e nal column shows the end-to-end gain in e ectiveness that has been achieved by the compound fusing (""initial mean"" to ""fused2 mean""). at is, fusing rst over query variations, and then over systems, gives rise to average e ectiveness gains of 18% and higher. In terms of statistical signi cance, and looking at the various relativities summarized in Table 3:",null,null
187,"· across the ve systems and four metrics (twenty paired runs in total), the largest p-value computed by a two-tailed paired Student's t-test comparing the corresponding , 1 and , all conditions was less than 0.005;",null,null
188,"· when the ve ,"" all fused runs are compared with the nal """"fused2"""" run, each metric yields one relatively large p-value, arising when the system that happens to be the """"max"""" is compared with the nal fused run (p  0.8, 0.7, 0.9, and 0.3 respectively across the four metrics, with two di erent systems represented twice each as the """"max"""" one), and a range of other smaller p-values, the largest of which was 0.059 (INST, comparing System 2 with "","" all against the nal fused run), and the remainder of which were 0.01 or smaller.""",null,null
189,"at is, we are highly con dent that fusion over queries helps retrieval e ectiveness regardless of system and regardless of metric; and also con dent that additional fusion across systems is also bene cial, helping ensure that the outcomes are as good as, or be er than, what would have been a ained if by chance we were already working with the best system for that metric.",null,null
190,4.5 An unrealistic target?,null,null
191,"Hindsight is a wonderful guide, a fact that is also true in IR. Table 4 shows the result of a post-hoc evaluation of the runs generated",null,null
192,"for System 1. e rst row shows the (unweighted) average metric scores across all of the UQV100 topics, and for each topic across the (approximately, on average) 55 distinct query variations. e second row shows the baseline e ectiveness scores used in Table 1, arrived at by selecting the most popular of the query variations for each topic. e third row then shows results for four ""oracle"" query subsets, one for each metric, each incorporating (based post hoc on the relevance judgments and the computed metric scores) the ""best"" query variation for each of the UQV100 topics.",null,null
193,"Comparing the rst and second rows, the most frequently posed query generated by the crowd-workers for each topic obtains notably be er e ectiveness than the average of the variations. at di erence is why we ordered the query variations as we did (Section 4.1). Comparing the second and third rows reveals a substantial further gap ­ for each of the topics and each of the metrics there are highly e ective queries available within the sets created by the crowd-workers. For two of the metrics the single-query oracle runs are outperformed by the best of the fused approaches (Table 1), but for two metrics they are considerably be er. Also worth noting is that the oracle runs have non-trivial di erences, with di erent best queries arising for di erent metrics in many cases. Across the four metrics, a total of 193 best queries were identi ed.",null,null
194,5 CONSISTENCY DEFINED,null,null
195,5.1 De nition,null,null
196,"As discussed in Section 2, Rank-Biased Overlap (RBO) [38] measures the top-weighted rank similarity between two non-conjoint inde nite rankings. As the rankings increase in similarity, especially towards the start of the rankings, the value of RBO trends towards 1.0. Due to the geometric sum of weights, governed by parameter , the total overlap score is bounded, ranging from 0.0 (no overlap) to 1.0 (total overlap).",null,null
197,"To use RBO to measure consistency across query variations for a topic, we rst select a common reference or objective ranking (the centroid) for each system-topic pair, making use of the RBC algorithm described in the previous sections. e persistence factor  for RBC is set to 0.90, to mirror a user whose expected depth of examination into a ranked list is 10, a reasonably deep level of examination relative to standard web search; also, the UQV100 pooling ensured at least depth-10 judging for each query from each system. Di erent persistence factors could be selected, which would emphasize shallower or deeper probabilities of inspection of the lists forming the centroid or the depth of overlap. e centroid for all-systems might also have been considered; however we sought to measure a system's self-consistency, rather than with respect to a centroid that requires knowledge of other systems.",null,null
198,"Given a centroid, we compute the RBO score for every query variation, using the same value of  ,"" 0.90. Computation of RBO provides both a point-estimate, and a minimum, residual, and maximum value. Since system runs typically report 200 or more documents, when  "","" 0.90 the residual is less than 10-10, and the point value is essentially equivalent to the minimum and maximum.""",null,null
199,"Formally, given a set of query variations Vi ,"" { i .1, i .2, . . . , i .k } for a topic ti  T "","" {t1, t2, ..., tn }, given a system S and its rankings for this set of variations Di "","" {d i .1 , d i .2 , ..., d i .k }, and given the corresponding RBC ranking for S and topic ti , denoted di .rbc, we""",null,null
200,400,null,null
201,Session 4A: Evaluation 2,null,null
202,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
203,measure topic consistency Cti with respect to S as,null,null
204,"Cti ,",null,null
205,"k q,1",null,null
206,RBO(d,null,null
207,"i .q , di .rbc) ,",null,null
208,k,null,null
209,(1),null,null
210,"and the collection consistency CT as the average topic consistency over the set of topics T , again with respect to S, as",null,null
211,"CT ,",null,null
212,"n i ,1",null,null
213,Cti,null,null
214,.,null,null
215,n,null,null
216,(2),null,null
217,"at is, consistency is the average RBO score relative to the pertopic centroids generated for the system, expressed either as a set of per-topic scores, or aggregated over topics for a per-collection score, but always with regard to a system S. We can also speak of a system's consistency, which is simply topic (or collection) consistency for a particular system. Note that the near-zero RBO residual means that issues of averaging over RBO scores with di erent residuals can be ignored. We choose the average of averages for CT because the number of variations per topic may vary, and topics with large numbers of unique variations should not unduly bias nal scores.",null,null
218,"Unlike for RBC, where we explored the consequence of adding more variations and thus needed an ordering, for consistency we use all unique query expressions without repeats (unweighted). In UQV100, each topic typically has a small number of commonly chosen variations which occur multiple times, and a large number of variations that occur only once. We wished to avoid biasing the consistency measure unduly by counting the contribution of the more popular variations multiple times. e choice of unweighted query variations also reinforces the decision to compute average of averages for CT ; in UQV100 the number of unique query variations per topic ranges from 19 to 101, so double averaging helps avoid undue in uence from the topics with more diverse variations.",null,null
219,5.2 Why this de nition of consistency?,null,null
220,"Test collection-based evaluation reduces many sources of variance that occur in information seeking in the wild to a level that is tractable from the standpoint of statistical analysis. Our de nition of consistency is predicated on having test collections that embody some plausible set of query variations per information need. We do not claim that it can address all possible sources of, or needs for, desirable consistency (or inconsistency) in information seeking.",null,null
221,"Our de nition of consistency might be brought into question by queries that exhibit extrinsic diversity [31] or intrinsic diversity [32] or searching as learning [14]. Extrinsic diversity occurs when a query has many di erent information needs that might be associated with it, while intrinsic diversity addresses cases where there are multiple sub-tasks associated in satisfying the information need. Searching as learning involves evolving query expression throughout a session. For cases involving extrinsic diversity, test collections without query variations typically declare one information need and judge relevance with respect to that information need; other plausible information needs are ignored. From the standpoint of assessing consistency, our approach is the same and has the same aw of ignoring other information needs. For cases involving intrinsic diversity and searching as learning, approaches have been developed that target aspects of such complex evaluation situations, including TREC's Web track's Diversity task [10] and",null,null
222,"the Session [20] and Tasks [42] tracks. For assessing just consistency, we suggest that test collections should involve more narrow information needs, with an emphasis on developing speci c unambiguous topic statements. Despite this, we believe that consistency is also important for systems that are able to accurately detect and respond to intrinsic diversity queries, and just as with narrower information needs, there will be a wide range of query variations for an information need that is intrinsically diverse.",null,null
223,"One more question the reader might have is why measure consistency at all, and why not just go straight to average relevance. Two issues arise: rst, relevance judgments are a substantially more expensive resource to accumulate, particularly when dealing with test collections with thousands of query variations. Second, although two rankings may have identical relevance scores, they may be completely di erent. Consider an information need such as ""You are worried about the prevalence of fake news, and decide to nd authoritative newspapers to read instead, just like people did last century."" Now consider two rankings in response to two query variations, one of which lists [theguardian.com, zeit.de, .com, washingtonpost.com] and one of which lists [wsj.com, lemonde.fr, nytimes.com, theglobeandmail.com]. From a relevance standpoint, these rankings are e ectively identical, but from a consistency perspective, they share nothing. If we accept that a searcher cares about re-",null,null
224,"nding the same information for an information need, even if they forget the precise query variation they used previously [37], then it is clear that being able to quantify consistency in rankings is not captured by relevance equivalence alone.",null,null
225,6 ANALYSIS OF CONSISTENCY,null,null
226,6.1 Consistency and topics,null,null
227,"We address RQ-C1 by assessing Cti over the ve contributed runs described in Section 4.1. RQ-C1 asks whether topics vary in consistency, and to do this we plot the average and standard deviation of Cti against all 100 topics of UQV100, sorted by increasing Cti . We characterize this in two ways: Figure 2 shows the results for System 1 (where standard deviation is of the RBO scores per variation for the topic), while Figure 3 shows the results when aggregating over the Cti scores obtained from the ve systems. Even with the large standard deviations that can be observed in both plots (while being clearly smaller in the second), we can conclude that di erent topics have di erent consistency. For persistence  ,"" 0.9, approximately 25% of topics have consistency under 0.25, while around 12% have consistency greater than 0.5. We also observe that some topics have great variation in their consistency scores, and others much less; and, overall, that consistency does indeed vary across topics.""",null,null
228,6.2 Consistency and topic attributes,null,null
229,"In the rst part of RQ-C2 we ask whether there is a relationship between the number of query variations per topic ti and corresponding consistency scores Cti . Intuitively, we might expect that the more unique query variations there are for a topic, the lower the consistency scores. We address this aspect through a correlation analysis, using Spearman's , a non-parametric rank correlation statistic. Unlike Pearson's product-moment coe cient statistic, this does not rely on the data having equal variance or having few to no outliers. A sca er plot, not shown, demonstrated that these",null,null
230,401,null,null
231,Session 4A: Evaluation 2,null,null
232,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
233,1.00,null,null
234,Avg. RBO,null,null
235,0.75 0.50 0.25 0.00,null,null
236,0,null,null
237,25,null,null
238,50,null,null
239,75,null,null
240,100,null,null
241,Topics,null,null
242,"Figure 2: Consistency scores Cti for System 1, ordered by score, for 100 topics. Bars are ±1 s.d. of the underlying RBO values.",null,null
243,1.00,null,null
244,System,null,null
245,(a) Num. variations,null,null
246,(b) Est. docs,null,null
247,(c) Est. queries,null,null
248,1,null,null
249,-0.35,null,null
250,-0.30,null,null
251,-0.43,null,null
252,2,null,null
253,-0.43,null,null
254,-0.27,null,null
255,-0.43,null,null
256,3,null,null
257,-0.42,null,null
258,-0.35,null,null
259,-0.45,null,null
260,4,null,null
261,-0.36,null,null
262,-0.25,null,null
263,-0.39,null,null
264,5,null,null
265,-0.37,null,null
266,-0.36,null,null
267,-0.43,null,null
268,"Table 5: Correlation measured using Spearman's  between Cti and: (a) number of unique query variations per topic; (b) average estimated useful documents per topic; and (c) average estimated useful queries per topic, for each (system, topic) pair. In all cases, p < 0.01().",null,null
269,1.00,null,null
270,0.75,null,null
271,Consistency,null,null
272,Avg. average RBO,null,null
273,0.75,null,null
274,0.50 0.25,null,null
275,0.00 0,null,null
276,25,null,null
277,50,null,null
278,75,null,null
279,100,null,null
280,Topics,null,null
281,"Figure 3: Average consistency scores Cti from ve systems, ordered by increasing score, for 100 topics. Bars are ±1 s.d. Note that",null,null
282,the topics may not be in the same order as in Figure 2.,null,null
283,"requirements might not hold. In Table 5(a), we show the results for all systems. As expected, the direction of the association is negative (Cti goes down when the number of query variations goes up). However, the correlations are relatively weak weak magnitude (0.3 <  < 0.5) so although there is an association, it is not something we can reliably anticipate. e corresponding sca er plot of values is not shown for space reasons, but con rms that there is a broad range of consistency scores as the number of query variations per topic changes. ese outcomes are a li le surprising, suggesting that the causes of increased consistency are complex.",null,null
284,In the second part of RQ-C2 we ask whether there is a relationship between the estimated topic complexity and corresponding consistency scores. Estimated topic complexity is available as the average of the estimates of the number of useful documents (and the number of queries) expected by each person providing a query,null,null
285,0.50,null,null
286,0.25,null,null
287,0.00,null,null
288,1,null,null
289,2,null,null
290,3,null,null
291,4,null,null
292,5,null,null
293,Systems,null,null
294,"Figure 4: Consistency scores Cti for ve systems and 100 topics. e diamond marks the median, and the horizontal line marks CT .",null,null
295,"variation for a topic description in UQV100. More complex topics have higher values for these two averaged estimates. Again, intuitively we might expect that the more complex a topic is, the lower the consistency score. As above, we assess the relationship using Spearman's , for both the average estimated documents and average estimated queries per topic ti and corresponding consistency scores Cti . Results are shown in Table 5(b) and (c), and just as before indicate a negative association, as surmised. However, once again the correlations are weak at best, at best of weak magnitude, meaning that increases in estimated topic complexity are only loosely associated with decreases in consistency. Interestingly, the estimates of the required number of queries all have a stronger correlation than the corresponding estimates of the required number of useful documents. is outcome might in part arise because the complexity estimate providers may be be er at estimating changes in small numbers than in larger ones, but there are other possible explanations.",null,null
296,402,null,null
297,Session 4A: Evaluation 2,null,null
298,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
299,System,null,null
300,2,null,null
301,3,null,null
302,4,null,null
303,5,null,null
304,1,null,null
305,-9.756 -10.497 -14.362 -1.030,null,null
306,2,null,null
307,-0.930 -6.821 9.107,null,null
308,3,null,null
309,-8.035 8.490,null,null
310,4,null,null
311,12.330,null,null
312,Table 6: System di erences measured using a paired Student's,null,null
313,"t-test between all pairs of systems over the corresponding Cti per topic. Values reported are the t statistic, df ,"" 99 in all cases, and""",null,null
314,signi cant di erences at p < 0.05 () and p < 0.01 ().,null,null
315,6.3 Consistency and systems,null,null
316,"If consistency was identical across di erent retrieval systems, then it would be uninteresting when selecting an e ective system. In RQC3, we examine the relationship between collection consistency CT and the ve systems which contributed to UQV100. In Figure 4 we report on the consistency scores for each system, using boxplots to show the spread of values. From this we can observe that Systems 1 and 5 are very similar to each other (CT 0.32); Systems 2 and 3 are very similar to each other and more consistent (CT 0.40), and System 4 is di erent and yet more consistent (CT ,"" 0.44). Using a two-tailed paired Student's t-test, we also assessed each pair of systems; Table 6 con rms our observations.""",null,null
317,"Another way of understanding the relationship between consistency and systems is to treat each topic as an ""assessor"" and each system as the ""subject"" being assessed with regards to its degree of consistency. Since topics in UQV100 contain a similar number of query variations (average of 55 per topic) as many existing test collections have queries, and past practice has been to examine rank order correlation of systems by comparing sets of systems across two or more collections, we will adopt a similar method here. We assess how similar the relative ordering of systems is using one-way Intraclass Correlation [4], Kendall's Coe cient of Concordance [21] (commonly wri en as Kendall's W ), and Krippendor 's  [17]. All of these measures address inter-assessor agreement, for three or more assessors, and can accommodate ordinal data (and for ICC and , interval or ratio data). In Table 7, we report the results for these measures of rank agreement, where the score is the topic consistency Cti . We use these measures rather than pair-wise comparisons of topics using Kendall's  , since we have 100 topic ""assessors"" involved, and the measures allow us to consider multiple ""assessors"" with a single test statistic, while Kendall's  only supports two ""assessors"". Due to the di erences between ICC and Kendall's W , it is expected that ICC scores may be lower than Kendall's W scores over the same data, since it considers not just relative rank order but also the magnitude of di erences, as discussed by Sheskin [34]. In all three measures, 1 indicates perfect agreement among the assessors, and 0 indicates no agreement beyond what would be expected by chance. Both ICC and  can report small negative values, which also signify no agreement.",null,null
318,"e values from ICC and Krippendor 's  both indicate there is a very low degree of inter-assessor agreement in rank ordering the systems by consistency; and although Kendall's W is 0.491 for Cti , this is still a relatively low degree of agreement.",null,null
319,"From these two analyses, we conclude that although these systems do have di erent overall collection consistency CT , they are",null,null
320,Agreement,null,null
321,(a) ICC,null,null
322,(b),null,null
323,(c),null,null
324,Kendall's W Krippendor 's ,null,null
325,Cti,null,null
326,NDCG INST,null,null
327,0.111,null,null
328,-0.002 -0.005,null,null
329,0.491,null,null
330,0.169 0.033,null,null
331,0.090,null,null
332,-0.003 -0.006,null,null
333,"Table 7: Rank agreement over all ve systems measured using (a) Intraclass Correlation; (b) Kendall'sW ; and (c) Krippendor 's . For ICC and W , signi cance is reported as p < 0.05(), and p < 0.01(); for  it is not reported. e top row uses Cti as the ranking score for each system; the bo om two rows use NDCG and INST (averaged by topic, as for Cti ).",null,null
334,System AP NDCG Q RBP INST,null,null
335,1,null,null
336,0.61 0.68 0.56 0.32 0.35,null,null
337,2,null,null
338,0.62 0.70 0.58 0.33 0.37,null,null
339,3,null,null
340,0.55 0.62 0.53 0.32 0.36,null,null
341,4,null,null
342,0.59 0.65 0.55 0.39 0.40,null,null
343,5,null,null
344,0.59 0.65 0.53 0.25 0.30,null,null
345,"Table 8: Correlation measured using Spearman's  between topic consistency Cti and corresponding relevance measures (AP, NDCG, Q measure, RBP0.85, and INST), for each (system, topic) pair. In all cases, there is a signi cant correlation, with p < 0.01 (), except for System 5 and RBP, signi cant only at p < 0.05 ().",null,null
346,"not systematically ordered on the basis of topic consistency Cti . at is, for one topic a particular system may have high consistency,",null,null
347,"while for another topic a completely di erent system may have high consistency, and the earlier system may have low consistency.",null,null
348,6.4 Consistency and relevance,null,null
349,"Our last investigation, addressing RQ-C4, concerns the relationship between consistency and relevance. e construction of UQV100 guaranteed relevance judgments to at least depth 10 for all query variations for the ve systems being analyzed. us we are able to explore the degree of correlation between consistency and relevance, across a range of relevance measures, including AP, NDCG, Q measure, RBP0.85, and INST. e results, calculated using Spearman's , are displayed in Table 8. While there is moderate correlation for the ""deep"" relevance measures (AP, NDCG, and Q), there is only weak correlation for the ""shallow"" relevance measures (RBP0.85 and INST). Sca er plots of the data, not shown, indicate considerable spread of scores for all measures as Cti increases.",null,null
350,"As a comparison with consistency, we repeat the topics-as""assessors"" inter-assessor agreement analysis, with results reported for average-by-topic NDCG and INST scores in rows two and three of Table 7. While any degree of agreement was only just observable for consistency, with these relevance measures, any agreement on the ordering of systems by relevance is e ectively random.",null,null
351,7 CONCLUSIONS,null,null
352,Consistency ­ the ability to give similar results for a topic even when presented with di erent queries ­ is desirable for search engines in a variety of circumstances. We have de ned a consistency,null,null
353,403,null,null
354,Session 4A: Evaluation 2,null,null
355,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
356,"measure and explored it across a set of 5,736 query variations across 100 topics, using a novel relevance-based centroid algorithm.",null,null
357,"e RBC algorithm for fusing rankings has the bene t of incorporating a persistence parameter that allows modeling of di erent depths of a ention into rankings, and adds another strand to the ""RB-"" family. RBC is competitive or be er than existing algorithms, and like Borda count has no reliance on system scores. We con-",null,null
358,"rmed previous ndings that data fusion over queries and over systems is bene cial, and fusion over both is even be er. With the oracle runs we have also demonstrated that substantially be er e ectiveness performance is possible, at least hypothetically.",null,null
359,"Based on the various analyses, we can also state that the consistency measure informs us about something di erent to existing measures. Consistency varies by topic and by system, tends to decrease as topic complexity and the number of query variations increases, and has weak-to-moderate correlations with several relevance measures. However, in no circumstance is consistency strongly correlated with any of these existing test collection dimensions, con rming that it measures a di erent property altogether. Neither the measures of consistency nor relevance reliably order the",null,null
360,"ve systems over the UQV100 topics, indicating there are no clear system winners or losers for this test collection on these dimensions of e ectiveness when examined topic by topic.",null,null
361,"More investigation is required into the nature of consistency and its e ect on perceptions of the retrieval e ectiveness of search systems. Such work might include user studies, low-level analysis of the root causes of variable ranking within one or more systems, and broadening the current analysis to similar test collections with multiple query variations per topic.",null,null
362,"Acknowledgment is work was supported by the Australian Research Council's Discovery Projects Scheme (projects DP110101934 and DP140102655). Ma Crane, Xiaolu Lu, David Maxwell, and Andrew Trotman assisted greatly, providing the system runs that were analyzed. e UQV100 judgments were generated using resources provided by Microso .",null,null
363,REFERENCES,null,null
364,"[1] J. A. Aslam and M. Montague. 2001. Models for metasearch. In Proc. SIGIR. ACM, 276­284.",null,null
365,"[2] P. Bailey, A. Mo at, F. Scholer, and P. omas. 2015. User variability and IR system evaluation. In Proc. SIGIR. 625­634.",null,null
366,"[3] P. Bailey, A. Mo at, F. Scholer, and P. omas. 2016. UQV100: A test collection with query variability. In Proc. SIGIR. 725­728. Public data: h p://dx.doi.org/10. 4225/49/5726E597B8376.",null,null
367,"[4] J. J. Bartko. 1966. e intraclass correlation coe cient as a measure of reliability. Psychological Reports 19, 1 (1966), 3­11.",null,null
368,"[5] N. J. Belkin, C. Cool, W. B. Cro , and J. P. Callan. 1993. e e ect of multiple query representations on information retrieval system performance. In Proc. SIGIR. 339­346.",null,null
369,"[6] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. 1995. Combining the evidence of multiple query representations for information retrieval. Inf. Proc. & Man. 31, 3 (1995), 431­448.",null,null
370,[7] C. Buckley and J. Walz. 1999. e TREC-8 query track. In Proc. TREC.,null,null
371,"[8] J. Callan. 2002. Distributed information retrieval. In Advances in Information Retrieval. Springer, 127­150.",null,null
372,[9] B. Cartere e. 2009. On rank correlation and the distance between rankings. In Proc. SIGIR. 436­443.,null,null
373,"[10] K. Collins- ompson, C. Macdonald, P. N. Benne , F. Diaz, and E. M. Voorhees. 2014. TREC 2014 web track overview. In Proc. TREC.",null,null
374,[11] S. Cucerzan and E. Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proc. EMNLP. 293­300.,null,null
375,"[12] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. 2001. Rank aggregation methods for the web. In Proc. WWW. 613­622.",null,null
376,[13] E. A. Fox and J. Shaw. 1993. Combination of multiple searches. In Proc. TREC. 243­252.,null,null
377,"[14] L. Freund, H. O'Brien, and R. Kopak. 2014. Ge ing the big picture: Supporting comprehension and learning in search. In Proc. Searching As Learning (SAL) Workshop.",null,null
378,"[15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. 1987. e vocabulary problem in human-system communication. Comm. ACM 30, 11 (1987), 964­971.",null,null
379,"[16] F. C. Gey, N. Kando, and C. Peters. 2005. Cross-language information retrieval: The way ahead. Inf. Proc. & Man. 41, 3 (2005), 415­431.",null,null
380,"[17] A. F. Hayes and K. Krippendor . 2007. Answering the call for a standard reliability measure for coding data. Commun. Methods and Measures 1, 1 (2007), 77­89.",null,null
381,"[18] J.-Y. Jiang, J. Liu, C.-Y. Lin, and P.-J. Cheng. 2015. Improving ranking consistency for web search by leveraging a knowledge base and search logs. In Proc. CIKM. 1441­1450.",null,null
382,"[19] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proc. WWW. 387­396.",null,null
383,"[20] E. Kanoulas, B. Cartere e, M. Hall, P. Clough, and M. Sanderson. 2011. Overview of the TREC 2011 session track. In Proc. TREC.",null,null
384,"[21] M. G. Kendall and B. B. Smith. 1939. e problem of m rankings. Annals of Mathematical Statistics 10, 3 (1939), 275­287.",null,null
385,[22] B. Koopman and G. Zuccon. 2016. A test collection for matching patients to clinical trials. In Proc. SIGIR. 669­672.,null,null
386,"[23] R. T.-W. Lo, B. He, and I. Ounis. 2005. Automatically building a stopword list for an information retrieval system. J. Dig. Inf. Man. 3, 1 (2005), 3­8.",null,null
387,"[24] J. B. Lovins. 1968. Development of a stemming algorithm. MIT Information Processing Group, Electronic Systems Laboratory Cambridge.",null,null
388,"[25] H. P. Luhn. 1957. A statistical approach to mechanized encoding and searching of literary information. IBM J. Res. Dev. 1, 4 (1957), 309­317.",null,null
389,[26] A. Mo at. 2016. Judgment pool e ects caused by query variations. In Proc. Aust. Doc. Comp. Symp. 65­68.,null,null
390,"[27] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Inf. Sys. 27, 1 (2008), 2.1­2.27.",null,null
391,[28] M. Montague and J. A. Aslam. 2001. Relevance score normalization for metasearch. In Proc. CIKM. 427­433.,null,null
392,[29] M. Montague and J. A. Aslam. 2002. Condorcet fusion for improved retrieval. In Proc. CIKM. 538­548.,null,null
393,"[30] J. Pickens, G. Golovchinsky, C. Shah, P. Qvarfordt, and M. Back. 2008. Algorithmic mediation for collaborative exploratory search. In Proc. SIGIR. 315­322.",null,null
394,"[31] F. Radlinski, P. N. Benne , B. Cartere e, and T. Joachims. 2009. Redundancy, diversity and interdependent document relevance. SIGIR Forum 43, 2 (2009), 46­52.",null,null
395,"[32] K. Raman, P. N. Benne , and K. Collins- ompson. 2013. Toward whole-session relevance: Exploring intrinsic diversity in web search. In Proc. SIGIR. 463­472.",null,null
396,"[33] S. E. Robertson. 1990. On term selection for query expansion. J. Documentation 46, 4 (1990), 359­364.",null,null
397,[34] D. J. Sheskin. 2003. Handbook of Parametric and Nonparametric Statistical Procedures. CRC Press.,null,null
398,"[35] K. Spa¨rck Jones and C. J. van Rijsbergen. 1975. Report on the need for and the provision of an ""ideal"" information retrieval test collection. Technical Report 5266. Computer Laboratory, University of Cambridge. British Library Research and Development Report.",null,null
399,"[36] L. Tan and C. L. A. Clarke. 2015. A family of rank similarity measures based on maximized e ectiveness di erence. IEEE Trans. Know. Data Eng. 27, 11 (2015), 2865­2877.",null,null
400,"[37] J. Teevan, E. Adar, R. Jones, and M. A. S. Po s. 2007. Information re-retrieval: Repeat queries in Yahoo's logs. In Proc. SIGIR. 151­158.",null,null
401,"[38] W. Webber, A. Mo at, and J. Zobel. 2010. A similarity measure for inde nite rankings. ACM Trans. Inf. Sys. 28, 4 (2010), 20.1­20.38.",null,null
402,"[39] M. Wu, D. Hawking, A. Turpin, and F. Scholer. 2012. Using anchor text for homepage and topic distillation search tasks. JASIST 63, 6 (2012), 1235­1255.",null,null
403,"[40] S. Wu and S. McClean. 2006. Performance prediction of data fusion for information retrieval. Inf. Proc. & Man. 42 (2006), 899­915.",null,null
404,"[41] E. Yilmaz, J. A. Aslam, and S. Robertson. 2008. A new rank correlation coe cient for information retrieval. In Proc. SIGIR. 587­594.",null,null
405,"[42] E. Yilmaz, M. Verma, R. Mehrotra, E. Kanoulas, B. Cartere e, and N. Craswell. 2015. Overview of the TREC 2015 tasks track. In Proc. TREC.",null,null
406,"[43] G. Zuccon, J. Palo i, and A. Hanbury. 2016. ery variations and their e ect on comparing information retrieval systems. In Proc. CIKM. 691­700.",null,null
407,404,null,null
408,,null,null

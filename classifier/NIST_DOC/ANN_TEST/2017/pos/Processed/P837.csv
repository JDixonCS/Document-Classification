,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Learning To Rank Resources,null,null
3,Zhuyun Dai,null,null
4,Carnegie Mellon University zhuyund@cs.cmu.edu,null,null
5,Yubin Kim,null,null
6,Carnegie Mellon University yubink@cs.cmu.edu,null,null
7,Jamie Callan,null,null
8,Carnegie Mellon University callan@cs.cmu.edu,null,null
9,ABSTRACT,null,null
10,"We present a learning-to-rank approach for resource selection. We develop features for resource ranking and present a training approach that does not require human judgments. Our method is well-suited to environments with a large number of resources such as selective search, is an improvement over the state-of-the-art in resource selection for selective search, and is statistically equivalent to exhaustive search even for recall-oriented metrics such as MAP@1000, an area in which selective search was lacking.",null,null
11,KEYWORDS,null,null
12,"selective search, resource selection, federated search",null,null
13,"ACM Reference format: Zhuyun Dai, Yubin Kim, and Jamie Callan. 2017. Learning To Rank Resources. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: 10.1145/3077136.3080657",null,null
14,2 RELATED WORK,null,null
15,"ere are three main classes of resource selection algorithms: termbased, sample-based, and supervised approaches. Term-based algorithms models the language distribution of each shard. At query time, they determine the relevance of a shard by comparing the query to the stored language model [1, 12]. Sample-based algorithms estimate the relevance of a shard by querying a small sample index of the collection, known as the centralized sample index (CSI) [9, 11, 13, 14]. Supervised methods use training data to learn models to evaluate shard relevance, with most methods training a classi er per shard [2, 4]. However, training a classi er for every shard is expensive in selective search, where shards number in hundreds.",null,null
16,"us, supervised methods have not been used for selective search. Techniques that train a single classi er would be more suitable for selective search. Balog [3] trained a learning-to-rank algorithm for a TREC task and Hong et al. [6] learned a joint probabilistic classi er. e la er is used as a baseline in this work.",null,null
17,1 INTRODUCTION,null,null
18,"Selective search is a federated search architecture where a collection is clustered into topical shards. At query time, a resource selection algorithm is used to select a small subset of shards to search.",null,null
19,"Recent work showed that while selective search is equivalent to exhaustive search for shallow metrics (e.g. P@10), it performs worse for recall-oriented metrics (e.g. MAP) [5]. is is a problem because modern retrieval systems apply re-ranking operations to a base retrieval, which can require deep result lists [10].",null,null
20,"In this paper, we present learning to rank resources, a resource selection method based on learning-to-rank. While learning-to-rank has been widely studied for ranking documents, its application to ranking resources has not been studied in depth. We take advantage of characteristics of the resource ranking problem that are distinct from document ranking; we present new features; and we propose a training approach that uses exhaustive search results as the gold standard and show that human judgments are not necessary.",null,null
21,"Our approach is suitable for e ciently ranking the hundreds of shards produced by selective search and is an improvement over the state-of-the-art in resource selection for selective search. In addition, our approach is statistically equivalent to exhaustive search in MAP@1000, a deep recall-oriented metric.",null,null
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080657",null,null
23,3 MODEL,null,null
24,"Let q denote a query and (q, si ) denote features extracted from the ith shard for the query. e goal of learning-to-rank is to",null,null
25,"nd a shard scoring function f ((q, s)) that can minimize the loss",null,null
26,function de,null,null
27,ned as:,null,null
28,L(f ),null,null
29,",",null,null
30,q,null,null
31,Q,null,null
32,l,null,null
33,"(q,",null,null
34,f,null,null
35,)dP,null,null
36,(q).,null,null
37,"We use l(q, f )",null,null
38,",",null,null
39,"si >qsj 1{ f ((q, si )) < f ((q, sj ))} , where si >q sj denotes shard",null,null
40,pairs for which si is ranked higher than sj in the gold standard,null,null
41,shard ranking w.r.t query q.,null,null
42,"We used SV Mr ank [7], which optimizes pair-wise loss. List-wise",null,null
43,"algorithms such as ListMLE [16] produced similar results, thus we",null,null
44,only report results with SV Mr ank .,null,null
45,e training process requires a gold standard shard ranking for,null,null
46,"each training query. We propose two de nitions of the ground truth,",null,null
47,"relevance-based and overlap-based. In the relevance-based approach,",null,null
48,the optimal shard ranking is determined by the number of relevant,null,null
49,"documents a shard contains. us, the training data require queries",null,null
50,"with relevance judgments, which can be expensive to obtain. e",null,null
51,overlap-based approach assumes that the goal of selective search,null,null
52,is to reproduce the document ranking of exhaustive search. e,null,null
53,optimal shard ranking is determined by the number of documents,null,null
54,in a shard that were ranked highly by exhaustive search. is does,null,null
55,not require manual relevance judgments.,null,null
56,4 FEATURES,null,null
57,4.1 ery-Independent Information,null,null
58,Shard Popularity: Indicates how o en the shard had relevant (relevance-based) or top-ranked (overlap-based) documents for training queries. It is query-independent and acts as a shard prior.,null,null
59,837,null,null
60,Short Research Paper,null,null
61,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
62,4.2 Term-Based Statistics,null,null
63,"Term-based features can be easily precomputed, thus are e cient. Taily Features: One feature is the Taily [1] score calculated for query q and shard s. However, Taily scores can vary greatly across shards and queries. For robustness, we add two additional features. If shard s is ranked rs for query q, the inverse rank is 1/rs , which directly describes the importance of s relative to other shards. e binned rank is ceilin (rs /b), where b is a bin-size. We use b ,"" 10, meaning that every 10 consecutive shards are considered equally relevant. is feature helps the model to ignore small di erences between shards with similar rankings. Champion List Features: For each query term, the top-k best documents were found. e number of documents each shard contributes to the top-k was stored for each shard-term pair. For multi-term queries, the feature values of each query term were summed. We use two values of k "","" {10, 100}, generating 2 features.""",null,null
64,"ery Likelihood Features: e log-likelihood of a query with respect to the unigram language model of each shard is: L(q|s) ,",null,null
65,"t q log p(t |s), where p(t |s) is the shard language model, the average of all document language models p(t |d) in the shard. Document language model p(t |d) is estimated using MLE with Jelinek-Mercer smoothing. ery likelihood, inverse query likelihood, and binned query likelihood features are created for body, title, and inlink representations, yielding a total of 9 features.",null,null
66,"ery Term Statistics: e maximum and minimum shard term frequency across query terms, e.g. st fmax (q, s) ,"" maxt q st f (t, s), where st f (t, s) is the frequency of term t in shard s. We include the maximum and minimum of st f · id f where id f is the inverse document frequency over the collection. ese 4 features are created for body, title, and inlink representations, yielding 12 features. Bigram Log Frequency: e frequency of each bigram of the query in a shard is b fq (s) "","" b q log b fb (s), where b fb (s) is the frequency of bigram b in shard s. is feature can estimate term correlation. To save storage, we only store bigrams that appear more than 50 times in the collection.""",null,null
67,4.3 Sample-Document (CSI-Based) Features,null,null
68,"ese features are based on retrieval from the centralized sample index (CSI), which may provide term co-occurrence information. CSI retrieval is expensive, and thus is slower to calculate. Rank-S and ReDDE Features: Similar to Taily features, the shard scores given by Rank-S [9] and ReDDE [13], as well as the inverse rank and binned rank features for a total of 6 features. Average Distance to Shard Centroid: e distance between the top-k documents retrieved from the CSI to their respective shards' centroids. Intuitively, if the retrieved documents are close to the centroid, the shard is more likely to contain other similar, highlyscoring documents. For multiple documents from the same shard, the distances are averaged. We use two distance metrics: KL divergence and cosine similarity Note that because KL divergence measures distance rather than similarity, we use the inverse of the averaged KL divergence as the metric. We generated features for k ,"" {10, 100} and also a feature measuring the distance between the shard's centroid to its single highest scoring document in the top 100 of the CSI results, for a total of 6 features.""",null,null
69,5 EXPERIMENTAL METHODOLOGY,null,null
70,"Datasets: Experiments were conducted with ClueWeb09-B and Gov2. ClueWeb09-B (CW09-B) consists of 50 million pages from the ClueWeb09 dataset. Gov2 is 25 million web pages from the US government web domains. For relevance-based models, 200 queries from the TREC 09-12 Web Track topics were used for CW09-B, and 150 queries from the TREC 04-06 Terabyte Track topics were used for Gov2. Models were trained by 10-fold cross-validation. For overlap-based models, training queries were sampled from the AOL and Million ery Track query logs. Models were tested with the TREC queries. Optimal shard ranking for the overlap method was de ned by the number of documents each shard contains that were within the top N ,"" 2K retrieved from exhaustive search. We found N  [1K, 3K] produced stable results. Proposed methods and baselines: We used three sources of training data: relevance-based training data (L2R-TREC), and overlapbased training data (L2R-AOL and L2R-MQT). We used linear SV Mr ank , where C was chosen by cross validation. Our method was compared against state-of-the-art unsupervised models (Taily [1], ReDDE [13], and Rank-S [9]); and a supervised model Jnt [6]. Jnt was trained and tested using TREC queries with 10-fold cross-validation. Evaluation Metrics: Search accuracy was measured by P@10, NDCG@30 and MAP@1000. To test the proposed methods' superiority to baselines, a query-level permutation test with p < 0.05 was used. To test the equivalence to exhaustive search, a non-inferiority test [15] was used to assert that results of the more e cient selective search were at least as accurate as exhaustive search. e equivalence is established by rejecting the null hypothesis that selective search is at least 5% worse than exhaustive search with a 95% con dence interval. Selective Search Setup: We used 123 shards for CW09-B and 199 shards for Gov2 [5]. A 1% central sample index (CSI) was created for ReDDE and Rank-S baselines and CSI based features. Jnt followed the original implementation and used a 3% CSI. Search Engine Setup: Retrieval was performed with Indri, using default parameters. eries were issued using the sequential dependency model (SDM) with parameters (0.8, 0.1, 0.1). For CW09-B, documents with a Waterloo spam score below 50 were removed 1.""",null,null
71,6 EXPERIMENTS,null,null
72,6.1 Overall Comparison,null,null
73,Our method was compared to four baselines and exhaustive search. We tested shard rank cuto s from 1­8% of total shards; 10 for CW09-B and 16 for Gov2. e automatic cuto s of Rank-S and Taily performed similarly to xed cuto s and are not shown. Shard rankings by L2R enabled more accurate search than all baselines in both datasets (Figure 1). e search accuracy of L2R models is higher than the baselines at nearly every shard rank cuto .,null,null
74,"Table 1 compares L2R models and the baselines at two shard rank cuto s. e rst cuto is the point where the shallow metrics (P@10 and NDCG@30) stablize: 4 for CW09-B and 6 for Gov2. e second cuto is where MAP@1000 become stable: 8 for CW09-B and 12 for Gov2. L2R models improve over the baselines at both shard cuto s. For shallow metrics, L2R reaches exhaustive search",null,null
75,1h ps://plg.uwaterloo.ca/ gvcormac/,null,null
76,838,null,null
77,Short Research Paper,null,null
78,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
79,"at the rst cuto . Furthermore, searching the rst 8 out of 12 shards ranked by L2R is statistically non-inferior to searching all shards exhaustively, even for the recall-oriented MAP@1000. All the baselines have a 10% gap from exhaustive search in MAP@1000.",null,null
80,6.2 E ect of Training Data,null,null
81,"One might expect the relevance-based model (L2R-TREC) to be better than overlap-based models (L2R-AOL and AOL-MQT), because it uses manual relevance judgments. A model trained with overlap data might favor shards that contain false-positive documents. However, there is li le di erence between the two training methods. L2R-TREC was statistically be er than TREC or AOL for MAP@1000 in Gov2, but the relative gain is only 2%; in all other cases, there is no statistically signi cant di erences among the three models. Furthermore, models trained with relevance and overlap data agreed on which features are important (not shown due to space constraints).",null,null
82,"is analysis indicates that unlike learning to rank document models, we can train a learning to rank resource selector on a new dataset before we have relevance judgments.",null,null
83,6.3 ery Length,null,null
84,"We compare L2R-MQT to the baselines using MAP@1000 for queries with di erent lengths on CW09-B, shown in Figure 1. Gov2 and other training data produced similar results and are not shown. For single-term queries, existing methods are already equivalent to or be er than exhaustive search, and L2R-MQT retains this good performance. e advantage of L2R-MQT comes from multi-term queries, where the best baseline Jnt still has a 10% gap from exhaustive search. For these queries, the improvement of L2R-MQT over the Taily is expected, because Taily does not model term co-occurrence. However, L2R-MQT also out-performs ReDDE and Rank-S, which account for term co-occurrence by retrieving documents from the CSI, but are limited by only having a sample view of the collection. L2R draws evidence from both the sample and the whole collection. Jnt also fuses sample- and term-based features, but most of its features are derived from ReDDE or Taily-like methods and do not carry new information. L2R improved over Jnt by using novel features that encode new evidence.",null,null
85,"Figure 1: MAP@1000 for queries on CW09-B, grouped by query length. Parentheses on the X axis present the number of queries in each group. T is the shard rank cuto .",null,null
86,6.4 Feature Analysis,null,null
87,"e L2R approach uses three classes of features: query-independent, term-based, and sample-document (CSI). ese three feature classes have substantially di erent computational costs and contributions.",null,null
88,"Fast vs. Slow features: Sample-document (CSI-based) features have a high computational cost, because they search a sample (typically 1-2%) of the entire corpus. Term-based features have a low computational cost, because they lookup just a few statistics per query term per shard. Costs for query-independent features are lower still. e third experiment compares a slow model that uses all features (ALL) to a fast version that does not use sample-document features (FAST).",null,null
89,"We estimate the resource selection cost by the amount of data retrieved from storage. For CSI-based features, the cost is the size of postings of every query term in the CSI. For term-based features, the cost is the amount of su cient statistics required to derive all term-based features. e query-independent feature only looks up the shard popularity, so the cost is one statistic per shard.",null,null
90,"Table 2 compares FAST with ALL and baselines by their accuracy and average resource selection cost per query. ReDDE results were similar to Rank-S and are not shown. Taily has been the state-ofthe-art term-based (`faster') resource selection algorithm. However, FAST is substantially more accurate. FAST also outperformed Jnt with over 100× speed up. Compared to ALL, FAST is 67 times faster on CW09-B and 34 times faster on Gov2. Although FAST has slightly lower search accuracy than ALL, the gap is not large and is not statistically signi cant, indicating that the information from the CSI features can be covered by the more e cient features.",null,null
91,"We conclude that a resource ranker composed of only queryindependent and term-based features is as accurate as exhaustive search and a ranker that includes CSI features. CSI features improve accuracy slightly, but at a signi cant additional computational cost.",null,null
92,"Importance of Feature Types: We investigate the contribution of other types of features: query-independent features and term-based features, where the term-based features were sub-divided into unigram and bigram features. Table 3 presents the results for the leave-one-out analysis conducted on FAST. On CW09-B, removing any feature set from FAST led to lower performance. is indicates that each set of features covers di erent types of information, and all are necessary for accurate shard ranking. Among these features, unigram features were most important because CW09-B has many single-term queries. On Gov2, the only substantial di erence is observed when bigram features are excluded.",null,null
93,7 CONCLUSION,null,null
94,"is paper investigates a learning-to-rank approach to resource selection for selective search. Much a ention has been devoted to learning-to-rank documents, but there has been li le study of learning-to-rank resources such as index shards. Our research shows that training data for this task can be generated automatically using a slower system that searches all index shards for each query. is approach assumes that the goal of selective search is to mimic the accuracy of an exhaustive search system, but with lower computational cost. is assumption is not entirely true--we would like selective search to also be more accurate--but it is convenient and e ective.",null,null
95,839,null,null
96,Short Research Paper,null,null
97,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
98,"Table 1: Search accuracy comparison between 3 L2R models and baselines at two rank cuto s for two datasets. : statistically signi cant improvement compared to Jnt, the best resource selection baseline. : non-inferiority to exhaustive search .",null,null
99,Method,null,null
100,Redde Rank-S Taily Jnt L2R-TREC L2R-AOL L2R-MQT Exh,null,null
101,P @10,null,null
102,0.355 0.350 0.346 0.370 0.374 0.374 0.382,null,null
103,0.372,null,null
104,"T,4 NDCG @30",null,null
105,0.262 0.259 0.260,null,null
106,0.269 0.281 0.281  0.285 ,null,null
107,0.288,null,null
108,CW09-B,null,null
109,MAP @1000 0.176 0.175 0.172 0.178 0.192 0.191 0.193 0.208,null,null
110,P@10,null,null
111,0.363 0.360 0.346 0.367 0.377 0.375 0.375 0.372,null,null
112,"T,8",null,null
113,NDCG,null,null
114,@30 0.275,null,null
115,0.268,null,null
116,0.260 0.277 0.286  0.287  0.286 ,null,null
117,0.288,null,null
118,MAP @1000,null,null
119,0.187 0.183 0.175,null,null
120,0.192 0.202  0.202  0.202 ,null,null
121,0.208,null,null
122,P,null,null
123,@10 0.580,null,null
124,0.570,null,null
125,0.518 0.582 0.593 0.593 0.586,null,null
126,0.585,null,null
127,"T,6 NDCG @30",null,null
128,0.445 0.440 0.403,null,null
129,0.459 0.469 0.470  0.465,null,null
130,0.479,null,null
131,Gov2,null,null
132,MAP @1000 0.267 0.263 0.235 0.278 0.299 0.291 0.292 0.315,null,null
133,P,null,null
134,@10 0.587 0.585,null,null
135,0.530 0.588 0.591 0.587 0.593,null,null
136,0.585,null,null
137,"T,12",null,null
138,NDCG,null,null
139,@30 0.4600 0.461,null,null
140,0.418 0.465 0.475  0.470 0.474 ,null,null
141,0.479,null,null
142,MAP @1000,null,null
143,0.289 0.286 0.256,null,null
144,0.292 0.313  0.307  0.309 ,null,null
145,0.315,null,null
146,Table 2: E ectiveness and e ciency of FAST features. ALL uses all features. FAST does not use sample-document features. T: shard rank cuto . : non-inferiority to exhaustive.,null,null
147,Cw09 -B,null,null
148,"(T,8) Gov2",null,null
149,"(T,12)",null,null
150,Method,null,null
151,Redde Taily Jnt ALL FAST Redde Taily Jnt ALL FAST,null,null
152,P,null,null
153,@10 0.363,null,null
154,0.346 0.367 0.375 0.373 0.579,null,null
155,0.518 0.588 0.593 0.587,null,null
156,NDCG,null,null
157,@30 0.275,null,null
158,0.260 0.277 0.286 0.285 0.445,null,null
159,0.403 0.465 0.474 0.471,null,null
160,MAP @1000 0.187 0.175 0.192 0.202 0.201 0.289 0.256 0.292 0.309 0.310,null,null
161,Average Cost,null,null
162,"156,180 470",null,null
163,"468,710 158,529",null,null
164,"2,349 105,080",null,null
165,"758 315,875 108,306",null,null
166,"3,226",null,null
167,Table 3: Performance of L2R-MQT using feature sets constructed with leave-one-out. `- X' means the feature was excluded from FAST. Text in bold indicates the lowest value in the column.,null,null
168,CW09 -B,null,null
169,"(T,8)",null,null
170,Gov2,null,null
171,"(T,12)",null,null
172,Feature Set,null,null
173,FAST - Unigram - Bigram - Independent,null,null
174,FAST - Unigram - Bigram - Independent,null,null
175,P@10 0.373,null,null
176,0.303 0.364 0.368 0.592 0.592,null,null
177,0.582 0.591,null,null
178,NDCG@30 0.285,null,null
179,0.226 0.275 0.282 0.471 0.468,null,null
180,0.462 0.471,null,null
181,MAP@1000 0.201,null,null
182,0.138 0.187 0.199 0.310 0.301,null,null
183,0.296 0.303,null,null
184,"We show that the learned resource selection algorithm produces search accuracy comparable to exhaustive search down to rank 1,000. is paper is the rst that we know of to demonstrate results that are statistically signi cantly equivalent to exhaustive search for MAP@1000 on an index that does not have badly skewed shard sizes. Accuracy this deep in the rankings opens up the possibility of using a learned reranker on results returned by a selective search system, which was not practical in the past.",null,null
185,"Most prior research found that sample-document algorithms such as ReDDE and Rank-S are a li le more accurate than term-based algorithms such as Taily for selective search resource selection; however, sample-document resource selection algorithms have far",null,null
186,higher computational costs that increases query latency in some con gurations [8]. is work suggests that sample-document features provide only a small gain when combined with other types of features. It may no longer be necessary to choose between accuracy and query latency when using a learned resource ranker.,null,null
187,8 ACKNOWLEDGMENTS,null,null
188,"is research was supported by National Science Foundation (NSF) grant IIS-1302206. Yubin Kim is the recipient of the Natural Sciences and Engineering Research Council of Canada PGS-D3 (438411). Any opinions, ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.",null,null
189,REFERENCES,null,null
190,"[1] R. Aly, D. Hiemstra, and T. Demeester. Taily: shard selection using the tail of score distributions. pages 673­682, 2013.",null,null
191,"[2] J. Arguello, F. Diaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. pages 315­322, 2009.",null,null
192,"[3] K. Balog. Learning to combine collection-centric and document-centric models for resource selection. In TREC, 2014.",null,null
193,"[4] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. pages 1867­1870, 2009.",null,null
194,"[5] Z. Dai, X. Chenyan, and J. Callan. ery-biased partitioning for selective search. pages 1119­1128, 2016.",null,null
195,"[6] D. Hong, L. Si, P. Bracke, M. Wi , and T. Juchcinski. A joint probabilistic classi cation model for resource selection. pages 98­105. ACM, 2010.",null,null
196,"[7] T. Joachims. Training linear SVMs in linear time. In Proc. SIGKDD, pages 217­226, 2006.",null,null
197,"[8] Y. Kim, J. Callan, J. S. Culpepper, and A. Mo at. Load-balancing in distributed selective search. pages 905­908, 2016.",null,null
198,"[9] A. Kulkarni, A. S. Tigelaar, D. Hiemstra, and J. Callan. Shard ranking and cuto estimation for topically partitioned collections. pages 555­564, 2012.",null,null
199,"[10] C. Macdonald, R. L. T. Santos, and I. Ounis. e whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.",null,null
200,"[11] I. Markov and F. Crestani. eoretical, qualitative, and quantitative analyses of small-document approaches to resource selection. 32(2):9, 2014.",null,null
201,"[12] H. No elmann and N. Fuhr. Evaluating di erent methods of estimating retrieval quality for resource selection. pages 290­297, 2003.",null,null
202,"[13] L. Si and J. P. Callan. Relevant document distribution estimation method for resource selection. pages 298­305, 2003.",null,null
203,"[14] P. omas and M. Shokouhi. SUSHI: scoring scaled samples for server selection. pages 419­426, 2009.",null,null
204,"[15] E. Walker and A. S. Nowacki. Understanding equivalence and noninferiority testing. Journal of General Internal Medicine, 26(2):192­196, 2011.",null,null
205,"[16] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank: theory and algorithm. In Proc. ICML, pages 1192­1199, 2008.",null,null
206,840,null,null
207,,null,null

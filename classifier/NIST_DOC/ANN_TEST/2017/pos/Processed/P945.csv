,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Reinforcement Learning to Rank with Markov Decision Process,null,null
3,"Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng",null,null
4,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences zengwei@so ware.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn",null,null
5,ABSTRACT,null,null
6,"One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures such as normalized discounted cumulative gain (NDCG). Existing methods usually focus on optimizing a speci c evaluation measure calculated at a xed position, e.g., NDCG calculated at a xed position K. In information retrieval the evaluation measures, including the widely used NDCG and P@K, are usually designed to evaluate the document ranking at all of the ranking positions, which provide much richer information than only measuring the document ranking at a single position. us, it is interesting to ask if we can devise an algorithm that has the ability of leveraging the measures calculated at all of the ranking postilions, for learning a be er ranking model. In this paper, we propose a novel learning to rank model on the basis of Markov decision process (MDP), referred to as MDPRank. In the learning phase of MDPRank, the construction of a document ranking is considered as a sequential decision making, each corresponds to an action of selecting a document for the corresponding position. e policy gradient algorithm of REINFORCE is adopted to train the model parameters. e evaluation measures calculated at every ranking positions are utilized as the immediate rewards to the corresponding actions, which guide the learning algorithm to adjust the model parameters so that the measure is optimized. Experimental results on LETOR benchmark datasets showed that MDPRank can outperform the state-of-the-art baselines.",null,null
7,KEYWORDS,null,null
8,learning to rank; Markov decision process,null,null
9,"ACM Reference format: Zeng Wei, Jun Xu , Yanyan Lan, Jiafeng Guo, Xueqi Cheng. 2017. Reinforcement Learning to Rank with Markov Decision Process. In Proceedings of SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080685",null,null
10,1 INTRODUCTION,null,null
11,"Learning to rank has been widely used in information retrieval and recommender systems. Among the learning to rank methods,",null,null
12,* Corresponding author: Jun Xu,null,null
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080685",null,null
14,"directly optimizing the ranking evaluation measures is a representative approach and has been proved to be e ective. Following the idea of directly optimizing evaluation measures, a number of methods have been proposed, including SVMMAP [17] , Adarank [14] , and PermuRank [15] etc. In training, these methods rst construct a loss function on the basis of a prede ned ranking evaluation measure. en, approximations of the loss function or convex upper bounds upon the loss function are constructed and then optimized, leading to di erent directly optimizing learning to rank algorithms. In ranking, each document is assigned a relevance score based on the learned ranking model.",null,null
15,"Several learning to rank models that directly optimize evaluation measure have been proposed and applied to variant ranking tasks. Di erent loss functions and optimization techniques are adopted in these methods. For example, the loss function of SVMMAP is constructed on the basis of MAP. e upper bound of the hinge loss is de ned and is optimized with structure SVM. AdaRank, another directly optimizing method, construct its loss function on the basis of any evaluation measure whose values are between 0 and 1. Exponential upper bound is constructed and Boosting is adopted for conduct the optimization.",null,null
16,"In general, all the methods that directly optimize evaluation measures perform well in many ranking tasks, especially in terms of the speci c evaluation measure used in the training phase. We also note that some widely used evaluation measures are designed to measure the goodness of a document ranking at all of the ranking positions. For example, the evaluation measure of NDCG can be calculated at all of the ranking positions, each re ects the goodness of the document ranking from the beginning to the corresponding position. Existing methods, however, can only directly optimize the evaluation measure calculated at a prede ned ranking position. For example, the AdaRank algorithm will focus on optimizing NDCG at rank K, if its loss function is con gured based on the evaluation measure NDCG@K. e information carried by the documents a er the rank K are ignored, because they have no contribution to NDCG@K. us, it is natural to ask is it possible to devise a learning to rank algorithm that directly optimizes evaluation measures and has the ability to leverage the measures at all of the ranks?",null,null
17,"To answer the above question, we propose a new learning to rank model on the basis of Markov decision process (MDP) [10], called MDPRank. e training phase of MDPRank considers the construction of a document ranking as a process of sequential decision making and learns the the model parameters through maximizing the cumulated rewards to all of the decisions. Speci cally, the ranking of M documents is considered as a sequence of M discrete time steps where each time step corresponds to a ranking position.",null,null
18,"e ranking of documents, thus, is formalized as a sequence of M decisions and each action corresponds to selecting one document. At each time step, the agent receives the environment's state and",null,null
19,945,null,null
20,Short Research Paper,null,null
21,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
22,"chooses an action on the basis of the state. One time step later, as a consequence of the action the system transit to a new state. At each time step, the chosen of the action depends on a policy, which is a function maps from the current state to a probability distribution of selecting each possible action.",null,null
23,"Reinforcement learning is employed to train the model parameters. Given a set of labeled queries, at each time step, the agent can receive a numerical action-dependent reward which is de ned upon the evaluation measure calculated at that position. e policy gradient algorithm of REINFORCE [10] is adopted to adjust the model parameters so that expected long-term discounted rewards in terms of the evaluation measure is maximized. us, MDPRank can directly optimize the evaluation measure and leverages the performances at all of the ranks as rewards.",null,null
24,"Compared with existing methods that directly optimize IR measure, MDPRank enjoys the following advantages: 1) the ability of utilizing the IR measures calculated at all of the ranking positions as supervision in training; 2) directly optimizes the IR measure on the training data without approximation or upper bounding.",null,null
25,"To evaluate the e ectiveness of MDPRank, we conducted experiments on the basis of LETOR benchmark datasets. e experimental results showed that MDPRank can outperform the state-of-the-art learning to rank models including the methods that directly optimize evaluation measures such as SVM-MAP and AdaRank.",null,null
26,2 RELATED WORK,null,null
27,"In learning to rank for information retrieval, one of the most straightforward way to learn the ranking model is directly optimizing the measure used for evaluating the ranking performance. A number of methods have been developed in recent years. Some methods try to construct a continuous and di erentiable approximation of the measure-based ranking error. For example, So Rank [11] makes use of the expectation of NDCG over all possible rankings as an approximation of the original evaluation measure NDCG. SmoothRank [2] smooth the evaluation measure by approximating the rank position. Some other methods try to optimize a continuous and di erentiable upper bound of the measure-based ranking error. For example, SVMMAP [17] , SVMNDCG [1] optimize the relaxation of IR evaluation measures of MAP and NDCG, respectively. Structure SVM is adopted for conducting the optimization in both of these two methods. AdaRank [14] directly optimizes the exponential upper bound of the ranking error with a Boosting procedure. [15] summarized the framework of directly optimizing evaluation measure and new algorithms can be derived and analyzed under the framework. Recently, the idea is applied to search result diversi cation. For example, in Xu et al. [16] and Xia et al. [13], structure Perceptron is utilized to directly optimizes the diversity evaluation measures.",null,null
28,"In this paper we propose to directly optimize ranking evaluation measure with MDP [10], which has been widely used in variant IR applications. For example, in [6], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [18], the log-based document re-ranking is also modeled as a POMDP to improve the re-ranking performances. MDP is also used for building recommender systems.",null,null
29,"For example, [9] designed an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. e multibandit also widely used for ranking [4, 8] and recommendation [5].",null,null
30,3 MDP FORMULATION OF LEARNING TO RANK,null,null
31,"In supervised learning se ings, we are given N labeled training",null,null
32,queries,null,null
33,"(q(n), X (n), Y (n)) N , where X (n) ,",null,null
34,"n,1",null,null
35,"x(1n), · · · , x(Mnn)",null,null
36,and,null,null
37,"Y (n) ,",null,null
38,"1(n), · · · ,",null,null
39,(n) Mn,null,null
40,are the query-document feature set and,null,null
41,"the relevance label 1 set for documents retrieved by query q(n),",null,null
42,"respectively; and Mn is the number of the candidate documents {d1, · · · , dMn } retrieved by query q(n).",null,null
43,3.1 Ranking as MDP,null,null
44,"e process of document ranking can be formalized as an MDP, in which the construction of a document ranking can be considered as a sequential decision making where each time step corresponds to a ranking position and each action selects a document for the corresponding position. e model, referred to as MDPRank, can be is represented by a tuple S, A, T , R,  composed by states, actions, transition, reward, and policy, which are respectively de ned as follows:",null,null
45,"States S is a set of states which describe the environment. In ranking, the agent should know the ranking position as well as the the candidate document set that the agent can choose from. us, at time step t, the state st is de ned as a pair [t, Xt ] where Xt is the remaining documents for ranking.",null,null
46,"Actions A is a discrete set of actions that an agent can take. e set of possible actions depends on the state st , denoted as A(st ). At the time step t, at  A(st ) selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at .",null,null
47,"Transition T (S, A) is a function T : S × A  S which maps a state st into a new state st+1 in response to the selected action at . Choosing an action at means removing the document xm(at ) out of the candidate set, as shown in the following equation:",null,null
48,"st +1 ,"" T ([t, Xt ], at ) "","" [t + 1, Xt \ {xm(at )}],""",null,null
49,"Reward R(S, A) is the immediate reward, also known as reinforcement. In ranking, the reward can be considered as an evaluation of the quality of the selected document. It is natural to de ne the reward function on the basis of the IR evaluation measures. In this paper, we de ne the reward received in responds to choosing action at as the promotion of the DCG [3]:",null,null
50,"2 m(at ) - 1 t , 0",null,null
51,"RDCG(st , at ) ,",null,null
52,2 m(at ) -1 log2(t +1),null,null
53,"t>0 ,",null,null
54,(1),null,null
55,where m(at ) is the relevance label of the selected document dm(at ). Note that the calculation of each reward corresponds the DCG at,null,null
56,"each ranking position, which enables the learning of MDPRank to",null,null
57,fully utilize the DCG values calculated at all ranking positions.,null,null
58,"Policy  (a|s) : A × S  [0, 1] describes the behaviors of the",null,null
59,"agent, which is a probabilistic distribution over the possible actions.",null,null
60,"1 e relevance labels are a set of ranks {r1, · · · , r } and there exists a total order between the ranks: r r -1 · · · r1 where ` ' denotes a preference relationship.",null,null
61,946,null,null
62,Short Research Paper,null,null
63,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
64,Agent,null,null
65,"state st ,"" [t, Xt]""",null,null
66,"reward rt ,"" RDCG(st 1, at 1)""",null,null
67,rt+1,null,null
68,action at  (at|st; w),null,null
69,Environment,null,null
70,st+1,null,null
71,Figure 1: e agent-environment interaction in MDP.,null,null
72,"Speci cally, the policy of MDPRank calculates the probabilities of selecting each of the documents for the current ranking position:",null,null
73," (at |st ; w) ,",null,null
74,exp wT xm(at ) a A(st ) exp wT xm(a),null,null
75,",",null,null
76,(2),null,null
77,where w  RK is the model parameters whose dimension is same,null,null
78,with the ranking feature.,null,null
79,Construction of a document ranking given a training query can,null,null
80,"be formalized as follows. Given a user query q, the set of M retrieved",null,null
81,"documents X , and the corresponding human labels Y , the system",null,null
82,"state is initialized as s0 ,"" [0, X ]. At each of the time steps t "",",null,null
83,"0, · · · , M - 1, the agent receives the state st ,"" [t, Xt ], chooses an""",null,null
84,"action at which selects the document xm(at ) from the document set and places it to the rank t. Moving to the next step t + 1, the",null,null
85,"state becomes st+1 ,"" [t + 1, Xt+1]. In the same time, on the basis""",null,null
86,"of the human labels m(at ) for the selected documents, the agent receives immediate reward rt+1 ,"" R(st , at ). e process is repeated""",null,null
87,until all of the M documents are selected. Figure 1 illustrates the,null,null
88,agent-environment the interaction in MDPRank.,null,null
89,"In the online ranking/testing phase, there is no reward available",null,null
90,because there exists no labels. e model fully trust the learned,null,null
91,policy  and choose the action with maximal probability at each,null,null
92,"time step. us, the online ranking is equivalent to assigning scores f (x; w) , wT x to all of the retrieved documents and sorting the",null,null
93,documents in descending order.,null,null
94,3.2 Learning with policy gradient,null,null
95,"MDPRank has parameters w to determine. In this paper, we propose to learn the parameters with the REINFORCE [10, 12], a widely used policy gradient algorithm in reinforcement learning. e goal of the learning algorithm is to maximize the expected long term return from the beginning:",null,null
96,"(w) , EZw [G(Z)]",null,null
97,"where Z ,"" {xm(a0), xm(a1) · · · , xm(aM-1)} is the ranking list sampled from the MDPRank model and G(Z) is the long-term return""",null,null
98,"of the sampled ranking list, which is de ned as the discounted sum",null,null
99,of the rewards:,null,null
100,M,null,null
101,"G(Z) ,  k-1rk .",null,null
102,"k ,1",null,null
103,Note that the de nition of G is identical to the IR evaluation measure,null,null
104,"DCG if  ,"" 1. us, the learning of MDPRank is actually directly""",null,null
105,optimizing the evaluation measure.,null,null
106,"According to REINFORCE algorithm, the gradient w (w) can be calculated as",null,null
107,"w (w) ,""  t Gt w log w(at |st ; w),""",null,null
108,Algorithm 1 MDPRank learning,null,null
109,"Input: Labeled training set D ,"" {(q(n), X (n), Y (n))}nN"",""1, learning rate , discount factor  , and reward function R""",null,null
110,Output: w,null,null
111,1: Initialize w  random values,null,null
112,2: repeat,null,null
113,"3: w , 0",null,null
114,"4: for all (q, X, Y )  D do",null,null
115,5:,null,null
116,"(s0, a0, r1, · · · , sM-1, aM-1, rM )  SampleAnEpisode(w, q, X, Y , R)",null,null
117,"{Algorithm (2), and M , |X |}",null,null
118,6:,null,null
119,"for t , 0 to M - 1 do",null,null
120,7:,null,null
121,Gt ,null,null
122,"M -t k ,1",null,null
123, k -1rt +k,null,null
124,{Equation,null,null
125,(3)},null,null
126,8:,null,null
127,w  w +  t Gt w log  (at |st ; w) {Equation (4)},null,null
128,9:,null,null
129,end for,null,null
130,10: end for,null,null
131,11: w  w + w,null,null
132,12: until converge,null,null
133,13: return w,null,null
134,Algorithm 2 SampleAnEpisode,null,null
135,"Input: Parameters w, q, X , Y , and R Output: An episode",null,null
136,"1: Initialize s0  [0, X ], M  |X |, and episode E   2: for t ,"" 0 to M - 1 do 3: Sample an action at  A(st )   (at |st ; w) {Equation (2)} 4: rt +1  R(st , at ){Equation (1), calculation on the basis of Y } 5: Append (st , at , rt +1) at the end of E 6: State transition st +1  t + 1, X \ {xm(at ) } 7: end for 8: return E "","" (s0, a0, r1, · · · , sM-1, aM-1, rM )""",null,null
137,"where is further be estimated with Monte-Carlo sampling and shown in Algorithm 1. Algorithm 2 shows the procedure of sampling an episode for Algorithm 1. Speci cally, Algorithm 1 updates the parameters via Monte-Carlo stochastic gradient ascent. At each iteration, an episode (consisting a sequence of M states, actions, and rewards) is sampled according to current policy. en, at each time step t of the sampled episode, the model parameters are adjusted according to the gradients of the parameters w log  (at |st ; w), scaled by the step size , the discount rate  t , and the long-term return of the sampled episode starting from t, denoted as Gt :",null,null
138,M -t,null,null
139,"Gt ,",null,null
140, k-1rt +k .,null,null
141,(3),null,null
142,"k ,1",null,null
143,"e gradient of w at time step t is w log  (at |st ; w), which the direction that most increase the probability of repeating the action",null,null
144,"at on future visits to state st , and is de ned as",null,null
145,w log  (at |st ; w),null,null
146,",",null,null
147,w (at |st ; w)  (at |st ; w),null,null
148,", xm(at ) -",null,null
149,a At xm(a) exp wT xm(a) a At exp wT xm(a),null,null
150,.,null,null
151,(4),null,null
152,"Intuitively, the se ing of Gt let the parameters move most in the directions that favor actions that yield the highest return. Note",null,null
153,"that if  ,"" 1, G0 is exactly the evaluation measure calculated at the nal rank of the document list, i.e., DCG@M.""",null,null
154,947,null,null
155,Short Research Paper,null,null
156,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
157,4 EXPERIMENTS,null,null
158,4.1 Experimental settings,null,null
159,We conducted experiments to test the performances of MDPRank using two LETOR benchmark datasets [7]: OHSUMED and Million,null,null
160,"ery track of TREC2007 (MQ2007). Each dataset consists of queries, corresponding retrieved documents and human judged labels.",null,null
161,"e possible relevance labels are relevant, partially relevant, and not relevance. Following the LETOR con guration, we conducted 5-fold cross-validation experiments on these two datasets. e results reported were the average over the ve folds. In all of the experiments, we used LETOR standard features and set  ,"" 1 for making the algorithm to directly optimize DCG. NDCG at position of 1, 3, 5 and 10 were used for evaluation. We compared the proposed MDPRank with several state-of- the-art baselines in LETOR, including pairwise methods of RankSVM, listwise methods of ListNet, and methods that directly optimizing evaluation measures: AdaRank-MAP, AdaRank-NDCG [14], and SVMMAP [17] .""",null,null
162,4.2 Experimental results,null,null
163,"Table 1 and Table 2 report the performances of MDPRank and all of the baseline methods on OSHUMED and MQ2007, respectively, in terms of NDCG at the positions of 1, 3, 5, and 10. Boldface indicates the highest score among all runs. From the results, we can see that MDPRank outperformed all of the baselines, except on MQ2007 in terms of NDCG@10. We conducted signi cant testing (t-test) on the improvements of MDPRank over the best baseline. e experimental results indicated that the improvements on OHSUMED are signi cant. e results show that MDPRank is e ective algorithm to directly optimize IR evaluation measures.",null,null
164,"One advantage of MDPRank is that it has the ability of utilizing evaluation measure calculated at all of the ranking positions (the rewards) as the supervision in training. To test the e ectiveness of this, we modi ed algorithm MDPRank so that the algorithm only utilizes the long term return of the whole episode for training, denoted as MDPRank(ReturnOnly). e modi cation actually controls the Algorithm 1 to execute line 7 and 8 only when t ,"" 0. From the results shown in Table 1 and Table 2, we can see that MDPRank(ReturnOnly) underperformed the original MDPRank, indicating that fully utilizing the evaluation measures calculated at all of the ranking positions is e ective for improving the ranking performances.""",null,null
165,5 CONCLUSION,null,null
166,"In this paper we have proposed to formalize learning to rank as an MDP and training with policy gradient, referred to as MDPRank. By de ning the MDP rewards on the basis of IR evaluation measure and adopting the REINFORCE algorithm for the optimization, MDPRank actually directly optimizes IR measures with Monte-Carlo stochastic gradient assent. Compared with existing learning to rank algorithms, MDPRank enjoys the advantage of fully utilizing the IR evaluation measures calculated at all of the ranking positions in the training phase. Experimental results based on LETOR benchmarks show that MDPRank cam outperform the state-of-the-art baselines. e experimental results also showed that utilizing the IR evaluation measures calculated at all of the ranking positions do help to improve the performances.",null,null
167,Table 1: Ranking accuracies on OHSUMED dataset.,null,null
168,Method,null,null
169,RankSVM ListNet,null,null
170,AdaRank-MAP AdaRank-NDCG,null,null
171,SVMMAP,null,null
172,MDPRank MDPRank(ReturnOnly),null,null
173,NDCG@1 0.4958 0.5326 0.5388 0.5330 0.5229,null,null
174,0.5925 0.5363,null,null
175,NDCG@3 0.4207 0.4732 0.4682 0.4790 0.4663,null,null
176,0.4992 0.4885,null,null
177,NDCG@5 0.4164 0.4432 0.4613 0.4673 0.4516,null,null
178,0.4909 0.46949,null,null
179,NDCG@10,null,null
180,0.4140 0.4410 0.4429 0.4496 0.4319,null,null
181,0.4587 0.4591,null,null
182,Table 2: Ranking accuracies on MQ2007 dataset.,null,null
183,Method,null,null
184,RankSVM ListNet,null,null
185,AdaRank-MAP AdaRank-NDCG,null,null
186,SVMMAP,null,null
187,MDPRank MDPRank(ReturnOnly),null,null
188,NDCG@1 0.4045 0.4002 0.3821 0.3876 0.3853,null,null
189,0.4061 0.4033,null,null
190,NDCG@3 0.4019 0.4091 0.3984 0.4044 0.3899,null,null
191,0.4101 0.4059,null,null
192,NDCG@5 0.4072 0.4170 0.407 0.4102 0.3983,null,null
193,0.4171 0.4113,null,null
194,NDCG@10 0.4383,null,null
195,0.4440 0.4335 0.4369 0.4187 0.4416 0.4350,null,null
196,6 ACKNOWLEDGMENTS,null,null
197,"e work was funded by the 973 Program of China under Grant No. 2014CB340401, National Key R&D Program of China under Grant No. 2016QY02D0405, the National Natural Science Foundation of China (NSFC) under Grants No. 61232010, 61472401, 61433014, 61425016, and 61203298, the Key Research Program of the CAS under Grant No. KGZD-EW-T03-2, and the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102.",null,null
198,REFERENCES,null,null
199,"[1] Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and Chiru Bha acharyya. 2008. Structured Learning for Non-smooth Ranking Losses. In Proceedings of SIGKDD. 88­96.",null,null
200,"[2] Olivier Chapelle and Mingrui Wu. 2010. Gradient descent optimization of smoothed information retrieval metrics. Information Retrieval 13, 3 (2010), 216­235.",null,null
201,"[3] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ cher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of SIGIR. 659­666.",null,null
202,"[4] Nathan Korda, Balazs Szorenyi, and Shuai Li. 2016. Distributed Clustering of Linear Bandits in Peer to Peer Networks. In Proceedings of ICML, 1301­1309.",null,null
203,"[5] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. 2016. Collaborative Filtering Bandits. In Proceedings of SIGIR. 539­548.",null,null
204,"[6] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings SIGIR. 587­596.",null,null
205,"[7] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2009. LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval. Information Retrieval (2009).",null,null
206,"[8] Filip Radlinski, Robert Kleinberg, and orsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of ICML. 784­791.",null,null
207,"[9] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. Machine Learning Research 6 (2005), 1265­1295.",null,null
208,[10] Richard S. Su on and Andrew G. Barto. 2016. Reinforcement Learning: An Introduction (2nd ed.). MIT Press.,null,null
209,"[11] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. So Rank: Optimizing Non-smooth Rank Metrics. In Proceedings of WSDM. 77­86.",null,null
210,"[12] Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 3 (1992), 229­256.",null,null
211,"[13] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of SIGIR. 113­122.",null,null
212,[14] Jun Xu and Hang Li. 2007. AdaRank: A Boosting Algorithm for Information Retrieval. In Proceedings SIGIR. 391­398.,null,null
213,"[15] Jun Xu, Tie-Yan Liu, Min Lu, Hang Li, and Wei-Ying Ma. 2008. Directly Optimizing Evaluation Measures in Learning to Rank. In Proceedings of SIGIR. 107­114.",null,null
214,"[16] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM TIST 8, 3 (2017), 41:1­41:26.",null,null
215,"[17] Yisong Yue, omas Finley, Filip Radlinski, and orsten Joachims. 2007. A Support Vector Method for Optimizing Average Precision. In Proceedings of SIGIR. 271­278.",null,null
216,"[18] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Content-free Document Re-ranking. In Proceedings of SIGIR. 1139­1142.",null,null
217,948,null,null
218,,null,null

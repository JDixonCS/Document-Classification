,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Generating ery Suggestions to Support Task-Based Search,null,null
3,Dar´io Gariglio i,null,null
4,University of Stavanger dario.gariglio i@uis.no,null,null
5,ABSTRACT,null,null
6,"We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the rst place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.",null,null
7,CCS CONCEPTS,null,null
8,·Information systems  ery suggestion;,null,null
9,KEYWORDS,null,null
10,"ery suggestions, task-based search, supporting search tasks",null,null
11,"ACM Reference format: Dar´io Gariglio i and Krisztian Balog. 2017. Generating ery Suggestions to Support Task-Based Search. In Proceedings of SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080745",null,null
12,1 INTRODUCTION,null,null
13,"Search is o en performed in the context of some larger underlying task [11]. ere is a growing stream of research aimed at making search engines more task-aware (i.e., recognizing what task the user is trying to accomplish) and customizing the search experience accordingly (see §2). In this paper, we focus our a ention on one particular tool for supporting task-based search: query suggestions. ery suggestions are an integral part of modern search engines [16]. We envisage an user interface where these suggestions are presented once the user has issued an initial query; see Figure 1. Note that this is di erent from query autocompletion, which tries to recommend various possible completions while the user is still typing the query. e task-aware query suggestions we propose are intended for exploring various aspects (subtasks) of the given task a er inspecting the initial search results. Selecting them would allow the user to narrow down the scope of the search.",null,null
14,"e Tasks track at the Text REtrieval Conference (TREC) has introduced an evaluation platform for this very problem, referred to as task understanding [20]. Speci cally, given an initial query,",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080745",null,null
16,Krisztian Balog,null,null
17,University of Stavanger krisztian.balog@uis.no,null,null
18,choose bathroom,null,null
19,choose bathroom cabinets lightning choose bathroom decoration style bathroom get ideas renew floor bathroom changing furniture bathroom,null,null
20,Figure 1: ery suggestions to support task-based search.,null,null
21,"the system should return a ranked list of keyphrases ""that represent the set of all tasks a user who submi ed the query may be looking for"" [20]. e goal is to provide a complete coverage of subtasks for an initial query, while avoiding redundancy. We use these keyphrases as query suggestions.",null,null
22,Our aim is to generate such suggestions in a se ing where past usage data and query logs are not available or cannot be utilized.,null,null
23,"is would be typical for systems that have a smaller user base (e.g., in the enterprise domain) or when a search engine has been newly deployed [4]. One possibility is to use query suggestion APIs, which are o ered by all major web search engines. ese are indeed one main source type we consider. Additionally, we use the initial query to search for relevant documents, using web search engines, and extract keyphrases from search snippets and from full text documents. Finally, given the task-based focus of our work, we lend special treatment to the WikiHow site,1 which is an extensive database of how-to guides.",null,null
24,"e main contribution of this paper is twofold. First, we propose a transparent architecture, using generative probabilistic modeling, for extracting keyphrases from a variety of sources and generating query suggestions from them. Second, we provide a detailed analysis of the components of our framework using di erent estimation methods. Many systems that participated in the TREC Tasks track have relied on strategic combinations of di erent sources to produce query suggestions, see, e.g., [7­9]. However, no systematic comparison of the di erent source types has been performed yet--we ll this gap. Additional components include estimating a document's importance within a given source, extracting keyphrases from documents, and forming query suggestions from these keyphrases. Finally, we check whether our ndings are consistent across the 2015 and 2016 editions of the TREC Tasks track.",null,null
25,2 RELATED WORK,null,null
26,"ere is a large body of work on understanding and supporting users in carrying out complex search tasks. Log-based studies have been one main area of focus, including the identi cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query",null,null
27,1h p://www.wikihow.com/,null,null
28,1153,null,null
29,Short Research Paper,null,null
30,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
31,q0,null,null
32,QS,null,null
33,WS,null,null
34,WD,null,null
35,WH,null,null
36,Keyphrases,null,null
37,Query suggestions,null,null
38,Figure 2: High-level overview of our approach.,null,null
39,"reformulations [10] or search trails [19]. Another theme is supporting exploratory search, where users pursue an information goal to learn or discover more about a given topic. Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17]. Our main interest is in query suggestions, as a distinguished support mechanism. Most of the related work utilizes large-scale query logs. For example, Craswell and Szummer [6] perform a random walk on a query-click graph. Boldi et al. [5] model the query ow in user search sessions via chains of queries. Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus. However, their focus is on query autocompletion, representing the completed and partial terms in a query. Kelly et al. [12] have shown that users prefer query suggestions, rather than term suggestions. We undertake the task of suggesting queries to users, related to the task they are performing, as we shall explain in the next section.",null,null
40,3 PROBLEM STATEMENT,null,null
41,"We adhere to the problem de nition of the task understanding task of the TREC Tasks track. Given an initial query q0, the goal is to return a ranked list of query suggestions q1, . . . qn that cover all the possible subtasks related to the task the user is trying to achieve. In addition to the initial query string, the entities mentioned in it are also made available (identi ed by their Freebase IDs).",null,null
42,"For example, for the query ""low wedding budget,"" subtasks include (but are not limited to) ""buy a used wedding gown,"" ""cheap wedding cake,"" and ""make your own invitations."" ese subtasks have been manually identi ed by the track organizers based on information extracted from the logs of a commercial search engine.",null,null
43,"e suggested keyphrases are judged with respect to each subtask on a three point scale (non-relevant, relevant, and highly relevant). Note that subtasks are only used in the evaluation, these are not available when generating the keyphrases.",null,null
44,4 APPROACH,null,null
45,"We now present our approach for generating query suggestions. As Figure 2 illustrates, we obtain keyphrases from a variety of sources, and then construct a ranked list of query suggestions from these.",null,null
46,4.1 Generative Modeling Framework,null,null
47,"We introduce a generative probabilistic model for scoring the candidate query suggestions according to P (q|q0), i.e., the probability that a query suggestion q was generated by the initial query q0.",null,null
48,Formally:,null,null
49,"P (q|q0) ,"" P (q|q0, s)P (s |q0)""",null,null
50,s,null,null
51,",",null,null
52,"P (q|q0, s, d )P (d |q0, s) P (s |q0)",null,null
53,sd,null,null
54,",",null,null
55,"P (q|q0, s, k )P (k |s, d ) P (d |q0, s) P (s |q0) .",null,null
56,sdk,null,null
57,"is model has four components: (i) P (s |q0) expresses the importance of a particular information source s for the initial query q0; (ii) P (d |q0, s) represents the importance of a document d originating from source s, with respect to the initial query; (iii) P (k |d, s) is",null,null
58,the relevance of a keyphrase k extracted from a document d from,null,null
59,"source s; and (iv) P (q|q0, s, k ) is the probability of generating query suggestion q, given keyphrase k, source s, and the initial query q0. Below, we detail the estimation of each of these components.",null,null
60,4.2 Source Importance,null,null
61,"We collect relevant information from four kinds of sources: query suggestions (QS), web search snippets (WS), web search documents (WD), and WikiHow (WH). For the rst three source types, we use three di erent web search engines (Google, Bing, and DuckDuckGo), thereby having a total of 10 individual sources. In this work, we assume conditional independence between a source s and the initial query q0, i.e., set P (s |q0) , P (s).",null,null
62,4.3 Document Importance,null,null
63,"From each source s, we obtain the top-K (K ,"" 10) documents for the query q0. We propose two ways of modeling the importance of a document d originating from s: (i) uniform and (ii) inversely proportional to the rank of d among the top-K documents, that is:""",null,null
64,"P (d |q0, s) ,",null,null
65,K -r +1,null,null
66,"K i ,1",null,null
67,K,null,null
68,-,null,null
69,i,null,null
70,+,null,null
71,1,null,null
72,",",null,null
73,K -r +1 K (K + 1)/2,null,null
74,",",null,null
75,where r is the rank position of d (r  [1..K]).,null,null
76,4.4 Keyphrase Relevance,null,null
77,"We obtain keyphrases from each document, using an automatic keyphrase extraction algorithm. Speci cally, we use the RAKE keyword extraction system [15]. For each keyphrase k, extracted from document d, the associated con dence score is denoted by c (k, d ). Upon a manual inspection of the extraction output, we introduce some data cleansing steps. We only retain keyphrases that: (i) have an extraction con dence above an empirically set threshold of 2; (ii) are at most 5 terms long; (iii) each of the terms has a length between 4 and 15 characters, and is either a meaningful number (i.e., max. 4 digits) or a term (excluding noisy substrings and reserved keywords from mark-up languages). Finally, we set the relevance of k as P (k |d, s) ,"" c (k, d )/ k c (k , d ).""",null,null
78,"In case s is of type QS, each returned document actually corresponds to a query suggestion. us, we treat each of these documents d as a single keyphrase k, for which we set P (k |d, s) , 1.",null,null
79,1154,null,null
80,Short Research Paper,null,null
81,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
82,Table 1: Comparison of query suggestion generators across the di erent types of sources. Statistical signi cance is tested against the corresponding line in the top block.,null,null
83,"P (q|q0, s, k ) S",null,null
84,2015,null,null
85,2016,null,null
86,ERR-IA -NDCG ERR-IA -NDCG,null,null
87,Using raw keyphrases,null,null
88,QS 0.0755 WS 0.2011 WD 0.1716 WH 0.0744,null,null
89,0.1186 0.2426 0.2154 0.1044,null,null
90,0.4114 0.3492 0.2339 0.1377,null,null
91,0.5289 0.4038 0.2886 0.1723,null,null
92,Using expanded keyphrases,null,null
93,QS 0.0751 WS 0.1901 WD 0.1551 WH 0.0849,null,null
94,0.1182 0.2274 0.2097 0.1090,null,null
95,0.4046 0.2927 0.1045 0.0789,null,null
96,0.5233 0.3467 0.1667 0.0932,null,null
97,Table 2: Comparison of document importance estimators across the di erent types of sources. Statistical signi cance is tested against the corresponding line in the top block.,null,null
98,"P (d |q0, s) S",null,null
99,2015,null,null
100,2016,null,null
101,ERR-IA -NDCG ERR-IA -NDCG,null,null
102,Uniform,null,null
103,QS 0.0755 WS 0.2011 WD 0.1716 WH 0.0849,null,null
104,0.1186 0.2426 0.2154 0.1090,null,null
105,0.4114 0.3492 0.2339 0.1377,null,null
106,0.5289 0.4038 0.2886 0.1723,null,null
107,Rank-based QS 0.0891 0.1307,null,null
108,decay,null,null
109,WS 0.1906 0.2315,null,null
110,WD 0.1688 0.2119,null,null
111,WH 0.0935 0.1225,null,null
112,0.4288 0.3386 0.1964 0.1195,null,null
113,0.5455 0.4011 0.2608 0.1495,null,null
114,4.5 ery Suggestions,null,null
115,"As a nal step, we need to generate query suggestions from the extracted keyphrases. As a baseline option, we take each raw keyphrase k as-is, i.e., with q ,"" k we set P (q|q0, s, k ) "", 1.",null,null
116,"Alternatively, we can form query suggestions by expanding keyphrases. Here, k is combined with the initial query q0 using a set of expansion rules proposed in [7]: (i) adding k as a suf-",null,null
117,"x to q0; (ii) adding k as a su x to an entity mentioned in q0; and (iii) using k as-is. Rules (i) and (ii) further involve a custom string concatenation operator; we refer to [7] for details. Each query suggestion q, that is generated from keyword k, has an associated con dence score c (q, q0, s, k ). We then set P (q|q0, s, k ) ,"" c (q, q0, s, k )/ q c (q , q0, s, k ). By conditioning the suggestion probability on s, it is possible to apply a di erent approach for each source. Like in the previous subsection, we treat sources of type QS distinctly, by simply taking q "","" k and se ing P (q|q0, s, k ) "", 1.",null,null
118,"We note that it is possible that multiple query suggestions have the same nal probability P (q|q0). We resolve ties using a deterministic algorithm, which orders query suggestions by length (favoring short queries) and then sorts them alphabetically.",null,null
119,5 RESULTS,null,null
120,In this section we present our experimental setup and results.,null,null
121,5.1 Experimental Setup,null,null
122,"We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20]. ese contain 34 and 50 queries with relevance judgments, respectively. We report on the o cial evaluation metrics used at the TREC Tasks track, which are ERR-IA@20 and -NDCG@20. In accordance with the track's se ings, we use ERR-IA@20 as our primary metric. (For simplicity, we omit mentioning the cut-o rank of 20 in all the table headers.) We noticed that in the ground truth the initial query itself has been judged as a highly relevant suggestion in numerous cases. We removed these cases, as they make li le sense for the envisioned scenario; we note that this leads to a drop in absolute terms of performance. We report on statistical signi cance using a two-tailed paired t-test at p < 0.05 and p < 0.001, denoted by  and , respectively.",null,null
123,"In a series of experiments, we evaluate each component of our approach, in a bo om-up fashion. For each query set, we pick the",null,null
124,"con guration that performed best on that query set, which is an idealized scenario. Note that our focus is not on absolute performance",null,null
125,"gures, but on answering the following research questions:",null,null
126,RQ1 What are the most useful information sources? RQ2 What are e ective ways of (i) estimating the importance,null,null
127,of documents and (ii) generating query suggestions from keyphrases? RQ3 Are our ndings consistent across the two query sets?,null,null
128,5.2 ery Suggestion Generation,null,null
129,"We start our experiments by focusing on the generation of query suggestions and compare the two methods described in §4.5. e document importance is set to be uniform. We report performance separately for each of the four source types S (that is, we set P (s) uniformly among sources s  S and set P (s) ,"" 0 for s S). Table 1 presents the results. It is clear that, with a single exception (2015 WH), it is be er to use the raw keyphrases, without any expansion. e di erences are signi cant on the 2016 query set for all source types but QS. Regarding the comparison of di erent source types, we nd that QS > WS > WD > WH on the 2016 query set, meanwhile for 2015, the order is WS > WD > QS, WH.""",null,null
130,5.3 Document Importance,null,null
131,"Next, we compare the two document importance estimator methods, uniform and rank-based decay (cf. §4.3), for each source type. Table 2 reports the results. We nd that rank-based document importance is bene cial for the query suggestion (QS) source types, for both years, and for WikiHow (WH) on the 2015 topics. However, the di erences are only signi cant for QS 2015. For all other source types, the uniform se ing performs be er.",null,null
132,"We also compare performance across the 10 individual sources. Figure 3 shows the results, in terms of ERR-IA@20, using the uniform estimator. We observe a very similar pa ern using the rankbased estimator (which is not included due to space constraints). On the 2016 query set, the individual sources follow the exact same pa erns as their respective types (i.e., QS > WS > WD > WH), with one exception. e Bing API returned an empty set of search suggestions for many queries, hence the low performance of QSBin . We can observe a similar pa ern on the 2015 topics, with the exception of sources of type QS, which are the least e ective here.",null,null
133,1155,null,null
134,Short Research Paper,null,null
135,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
136,ERR-IA@20,null,null
137,0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00,null,null
138,2015 Query set 2016 Query set,null,null
139,QS,null,null
140,Google,null,null
141,QS,null,null
142,DDG,null,null
143,WS,null,null
144,Bing,null,null
145,WS,null,null
146,Google,null,null
147,WS,null,null
148,DDG,null,null
149,WD,null,null
150,Bing,null,null
151,WD,null,null
152,Google,null,null
153,WD,null,null
154,DDG,null,null
155,WH QS,null,null
156,Bing,null,null
157,"Figure 3: Performance of individual sources, sorted by performance on the 2016 query set.",null,null
158,5.4 Source Importance,null,null
159,"Finally, we combine query suggestions across di erent sources; for that, we need to set the importance of each source. We consider three di erent strategies for se ing P (s): (i) uniformly; (ii) proportional to the importance of the corresponding source type (QS, WS, WD, and WH) from the previous step (cf. Table 2); (iii) proportional to the importance of the individual source (cf. Figure 3). e results are presented in Table 3. Firstly, we observe that the combination of sources performs be er than any individual source type on its own. As for se ing source importance, on the 2015 query set we",null,null
160,"nd that (iii) delivers the best results, which is in line with our expectations. On the 2016 query set, only minor di erences are observed between the three methods, none of which are signi cant.",null,null
161,5.5 Summary of Findings,null,null
162,"(RQ1) ery suggestions provided by major web search engines are unequivocally the most useful information source on the 2016 queries. We presume that these search engine suggestions are already diversi ed, which we can directly bene t from for our task.",null,null
163,"ese are followed, in order, by keyphrases extracted from (i) web search snippets, (ii) web search results, i.e., full documents, and (iii) WikiHow articles. On the 2015 query set, query suggestions proved much less e ective; see RQ3 below. (RQ2) With a single exception, using the raw keyphrases, as-is, performs be er than expanding them by taking the original query into account. For web query suggestions it is bene cial to consider the rank order of suggestions, while for web search snippets and documents the uniform se ing performs be er. For WikiHow, it varies across query sets. (RQ3) Our main observations are consistent across the 2015 and 2016 query sets, regarding documents importance estimation and suggestions generation methods. It is worth noting that some of our methods were o cially submi ed to TREC 2016 [7] and were included in the assessment pools. is is not the case for 2015, where many of our query suggestions are missing relevance assessments (and, thus, are considered irrelevant). is might explain the low performance of QS sources on the 2015 queries.",null,null
164,6 CONCLUSIONS,null,null
165,"In this paper, we have addressed the task of generating query suggestions that can assist users in completing their tasks. We have proposed a probabilistic generative framework with four components:",null,null
166,Table 3: Combination of all sources using di erent source importance estimators. Signi cance is tested against the uniform setting (line 1).,null,null
167,P (s),null,null
168,Uniform Source-type Individual,null,null
169,2015 ERR-IA -NDCG,null,null
170,0.2219 0.2835 0.2381 0.2905 0.2518 0.3064,null,null
171,2016 ERR-IA -NDCG,null,null
172,0.4561 0.4570 0.4562,null,null
173,0.5793 0.5832 0.5832,null,null
174,"source importance, document importance, keyphrase relevance,",null,null
175,and query suggestions. We have proposed and experimentally,null,null
176,compared various alternatives for these components.,null,null
177,"One important element, missing from our current model, is the",null,null
178,"representation of speci c subtasks. As a next step, we plan to cluster",null,null
179,query suggestions together that belong to the same subtask. is,null,null
180,would naturally enable us to provide diversi ed query suggestions.,null,null
181,REFERENCES,null,null
182,"[1] Salvatore Andolina, Khalil Klouche, Jaakko Peltonen, Mohammad E. Hoque, Tuukka Ruotsalo, Diogo Cabral, Arto Klami, Dorota Glowacka, Patrik Flore´en, and Giulio Jacucci. 2015. IntentStreams: Smart Parallel Search Streams for Branching Exploratory Search. In Proc. of IUI. 300­305.",null,null
183,"[2] Ahmed H. Awadallah, Ryen W. White, Patrick Pantel, Susan T. Dumais, and YiMin Wang. 2014. Supporting Complex Search Tasks. In Proc. of CIKM. 829­838.",null,null
184,[3] Krisztian Balog. 2015. Task-completion Engines: A Vision with a Plan. In Proc. of the 1st International Workshop on Supporting Complex Search Tasks.,null,null
185,"[4] Sumit Bhatia, Debapriyo Majumdar, and Prasenjit Mitra. 2011. ery Suggestions in the Absence of ery Logs. In Proc. of SIGIR. 795­804.",null,null
186,"[5] Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. e ery- ow Graph: Model and Applications. In Proc. of CIKM. 609­618.",null,null
187,[6] Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In Proc. of SIGIR. 239­246.,null,null
188,[7] Dar´io Gariglio i and Krisztian Balog. 2016. e University of Stavanger at the TREC 2016 Tasks Track. In Proc. of TREC.,null,null
189,"[8] Ma hias Hagen, Steve Go¨ring, Magdalena Keil, Olaoluwa Anifowose, Amir Othman, and Benno Stein. 2015. Webis at TREC 2015: Tasks and Total Recall Tracks. In Proc. of TREC.",null,null
190,"[9] Ma hias Hagen, Johannes Kiesel, Payam Adineh, Masoud Alahyari, Ehsan Fatehifar, Arafeh Bahrami, Pia Fichtl, and Benno Stein. 2016. Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks. In Proc. of TREC.",null,null
191,"[10] Jiepu Jiang, Daqing He, Shuguang Han, Zhen Yue, and Chaoqun Ni. 2012. Contextual Evaluation of ery Reformulations in a Search Session by User Simulation. In Proc. of CIKM. 2635­2638.",null,null
192,"[11] Diane Kelly, Jaime Arguello, and Robert Capra. 2013. NSF Workshop on Taskbased Information Search Systems. SIGIR Forum 47, 2 (2013), 116­127.",null,null
193,"[12] Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009. A Comparison of ery and Term Suggestion Features for Interactive Searching. In Proc. of SIGIR. 371­378.",null,null
194,"[13] Udo Kruschwitz, Deirdre Lungley, M-Dyaa Albakour, and Dawei Song. 2013. Deriving query suggestions for site search. JASIST 64, 10 (2013), 1975­1994.",null,null
195,"[14] Claudio Lucchese, Salvatore Orlando, Ra aele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2013. Discovering Tasks from Search Engine ery Logs. ACM Trans. Inf. Syst. 31, 3, Article 14 (2013), 43 pages.",null,null
196,[15] Alyona Medelyan. 2015. Modi ed RAKE algorithm. h ps://github.com/zelandiya/ RAKE-tutorial. (2015). Accessed: 2017-01-23.,null,null
197,"[16] Umut Ozertem, Olivier Chapelle, Pinar Donmez, and Emre Velipasaoglu. 2012. Learning to Suggest: A Machine Learning Framework for Ranking ery Suggestions. In Proc. of SIGIR. 25­34.",null,null
198,"[17] Tuan A. Tran, Sven Schwarz, Claudia Niedere´e, Heiko Maus, and Na iya Kanhabua. 2016. e Forgo en Needle in My Collections: Task-Aware Ranking of Documents in Semantic Information Space. In Proc. of CHIIR. 13­22.",null,null
199,"[18] Manisha Verma, Evangelos Kanoulas, Emine Yilmaz, Rishabh Mehrotra, Ben Cartere e, Nick Craswell, and Peter Bailey. 2016. Overview of the TREC Tasks Track 2016. In Proc. of TREC.",null,null
200,[19] Ryen W. White and Je Huang. 2010. Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs. In Proc. of SIGIR. 587­594.,null,null
201,"[20] Emine Yilmaz, Manisha Verma, Rishabh Mehrotra, Evangelos Kanoulas, Ben Cartere e, and Nick Craswell. 2015. Overview of the TREC 2015 Tasks Track. In Proc. of TREC.",null,null
202,1156,null,null
203,,null,null

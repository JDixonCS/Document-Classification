,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,The Impact of Linkage Methods in Hierarchical Clustering for Active Learning to Rank,null,null
3,Ziming Li,null,null
4,"University of Amsterdam Amsterdam, e Netherlands",null,null
5,z.li@uva.nl,null,null
6,ABSTRACT,null,null
7,"Document ranking is a central problem in many areas, including information retrieval and recommendation. e goal of learning to rank is to automatically create ranking models from training data. e performance of ranking models is strongly a ected by the quality and quantity of training data. Collecting large scale training samples with relevance labels involves human labor which is timeconsuming and expensive. Selective sampling and active learning techniques have been developed and proven e ective in addressing this problem. However, most active methods do not scale well and need to rebuild the model a er selected samples are added to the previous training set. We propose a sampling method which selects a set of instances and labels the full set only once before training the ranking model. Our method is based on hierarchical agglomerative clustering (average linkage) and we also report the performance of other linkage criteria that measure the distance between two clusters of query-document pairs. Another di erence from previous hierarchical clustering is that we cluster the instances belonging to the same query, which usually outperforms the baselines.",null,null
8,CCS CONCEPTS,null,null
9,·Information systems Learning to rank;,null,null
10,1 INTRODUCTION,null,null
11,"Document ranking is an essential feature in many information retrieval applications. How to sort the returned results according to their degree of relevance has given birth to the area of learning to rank [6], which aims to automatically create ranking models from a training dataset; the learned models are then used to rank the results of new queries. Many learning to rank algorithms have been proposed; they can be categorized into three types of approach: pointwise [2], pairwise [5] and listwise [1].",null,null
12,"Like other supervised machine learning methods, the performance of a ranking model highly depends on the quantity and quality of training datasets. To create training datasets, experts are hired to manually provide relevance labels for training data, which is expensive and time-consuming. Human labeling, whether by",null,null
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080684",null,null
14,Maarten de Rijke,null,null
15,"University of Amsterdam Amsterdam, e Netherlands",null,null
16,derijke@uva.nl,null,null
17,"experts or crowds, can be noisy and biased [11]. Active learning is a paradigm to reduce the labeling e ort [12]; it has mostly been studied in the context of classi cation tasks [9, 10, 14].",null,null
18,"In this paper, we present an active learning method to select the most informative query-document pairs to be labeled for learning to rank. Our method relies on hierarchical clustering. Unlike traditional active learning methods, our method is unsupervised and the selected training sets can be used to train di erent learning to rank models. We build on the hypothesis that the information contained in an instance is highly correlated to the instance position in the feature space. Hierarchical clustering has the ability to group instances with similar information into the same cluster and each cluster can be represented by its centroid. While most active learning methods need to rebuild the training models each time new labeled documents are added to the training set, our method labels the instances only once before the training process. We rst evaluate our method on three datasets from Letor 3.0 and nd that the performance of our method is similar or superior to the baselines while we can achieve full training performance with fewer instances. We also analyze the limitations of our method and nd that the e ectiveness of our sampling method is closely related to the features and structures each dataset has.",null,null
19,2 RELATED WORK,null,null
20,"In order to address the lack of labeled data, active learning has proven to be a promising direction that aims to achieve high accuracy using as few labeled instances as possible [12]. A number of active learning methods have been proposed for classi cation. Most methods start with only a small set of labeled instances and sequentially select the most informative instances to be labeled by an oracle. e trained model will be updated when new labeled instances are added to the training set. Di erent strategies are proposed to choose the most informative instances that can maximize the information value to the current model [9, 10, 14].",null,null
21,"It is not straightforward to extend these methods to ranking problems. On the one hand, these methods try to minimize the classi cation error and do not take into account the rank order while position-based measures are usually non-continuous and non-di erentiable. On the other, each instance in most supervised classi cation tasks can be treated as independent of each other while the instances in learning to rank are conditionally independent. Compared with traditional classi cation problems, there is limited work about active learning in ranking. Long et al. [8] adopt a two-stage active learning strategy schema to integrate query level and document level data selection. ey select samples minimizing the expected loss as the most informative ones and achieves good results in the case of web search ranking. But this method requires",null,null
22,941,null,null
23,Short Research Paper,null,null
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
25,"a relative big seed set and the ranking models are restricted to pointwise models. Donmez and Carbonell [3] select the documents that woluld impart the greatest change to the current model. While all these methods sequentially select instances to be labeled, Silva et al. [13] adopt hierarchical clustering to ""compress"" the original training set, which is the state-of-the-art selection method in overall performance and e ciency. e method we propose can be viewed as an extension of [13]. e di erence are that we cluster the query-document pairs belonging to the same query separately and average linkage is used, which achieves be er performance.",null,null
26,3 METHOD,null,null
27,3.1 Hierarchical agglomerative clustering,null,null
28,"In hierarchical agglomerative clustering, each instance is regarded as a singleton cluster and then merged based on the distance or similarity between clusters until there is only one cluster that contains all instances. e structure of the nal cluster is a tree or dendrogram and each level of the resulting tree is a segmentation of the data. For two given clusters, C1 and C2, and a non-negative real value , if distance f (C1, C2) < , then C1 and C2 will be merged.",null,null
29,"According to the merging rule, the number of clusters is associated with the value of , which is the indistinguishability threshold [13]. Di erent linkage criterions have been proposed to measure the distance or dissimilarity between two clusters. We use average as our linkage criterion and we also report the performance of minimum linkage, maximum linkage and ward linkage.",null,null
30,"3.1.1 Minimum linkage clustering. In minimum linkage clustering (also called single linkage clustering), one of the simplest agglomerative hierarchical clustering methods, the value of the shortest link from any member of one cluster to the member of another cluster denotes the distance of these two clusters. For two clusters C1 and C2, the distance between C1 and C2 is:",null,null
31,f,null,null
32,"(C1, C2)",null,null
33,",",null,null
34,u,null,null
35,min,null,null
36,"C1, C2",null,null
37,"d (u,",null,null
38,"),",null,null
39,"In this paper, d is the Euclidean distance. is method is also known as the nearest technique and used in [13]. In this case, minimum linkage clustering can group the instances in ""stringy"" clusters and be converted to nd the Minimum Spanning Tree (MST) of querydocument pairs [4]. By deleting all edges longer than a speci ed indistinguishability threshold in the MST, the remaining connected instances form a hierarchical cluster.",null,null
40,3.1.2 Average linkage clustering. Average linkage clustering uses,null,null
41,"the average of distances between all pairs of instances, where each",null,null
42,"pair consists of two points from two di erent clusters, as the dis-",null,null
43,tance between two clusters:,null,null
44,f,null,null
45,"(C1, C2)",null,null
46,",",null,null
47,N1,null,null
48,1  N2,null,null
49,"u C1,",null,null
50,"d (u,",null,null
51,C2,null,null
52,"),",null,null
53,"where N1 and N2 are the sizes of clusters C1 and C2, respectively. We use average linkage as our linkage criterion to measure the",null,null
54,cluster distance and we perform clustering on each subset which,null,null
55,contains all the query-document pairs belonging to the same query.,null,null
56,"3.1.3 Maximum linkage clustering. Di erent from single linkage clustering, the distance of two clusters in maximum linkage clustering is the value of the largest link from one cluster to another",null,null
57,"cluster. For two clusters C1 and C2, the distance is computed as follows:",null,null
58,f,null,null
59,"(C1, C2)",null,null
60,",",null,null
61,u,null,null
62,max,null,null
63,"C1, C2",null,null
64,"d (u,",null,null
65,"),",null,null
66,where d is the Euclidean distance.,null,null
67,"3.1.4 Ward linkage clustering. In Ward's linkage method, the",null,null
68,distance between two clusters is the sum of the squares of the,null,null
69,distances between all objects in the cluster and the centroid of the,null,null
70,"cluster. For two clusters C1 and C2, the distance is computed as",null,null
71,follows:,null,null
72,"f (C1, C2) ,",null,null
73,"d (x, µC1C2 )2,",null,null
74,x C1C2,null,null
75,where µ is the centroid of the new cluster merged from C1 and C2.,null,null
76,3.2 Hierarchical clustering for learning to rank,null,null
77,"In this paper, we apply di erent linkage criteria to hierarchical clustering. As shown in Algorithm 1, we rst cut all the querydocuments pairs into di erent subsets which have di erent query ids and then adopt hierarchical clustering on each subset. A er the last two steps, we can get the clusters on each subset. In our sampling strategy, we use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster. In fact, the clustering distribution shows that there is only one single point in most clusters. e nal dataset to be labeled is made up of all the selected instances.",null,null
78,Algorithm 1 Hierarchical Clustering on Each ery (HCEQ),null,null
79,"Require: Unlabeled dataset D with m queries, desired sampling",null,null
80,"size n, linkage criterion linka e",null,null
81,Ensure: e subset S to be labeled,null,null
82,"1: D1, D2, . . . , Di , . . . , Dm  Di idin Dataset (D). 2: S  ",null,null
83,"3: for all i  {1, 2, . . . , m} do",null,null
84,4:,null,null
85,ni,null,null
86,n,null,null
87,|Di | |D |,null,null
88,"5: C1, C2, . . . , Cj , . . . , Cni  A Clusterin (Di , ni , linka e )",null,null
89,"6: for all j  {1, 2, . . . , ni } do",null,null
90,7:,null,null
91,j  GeometricCentroid (Cj ),null,null
92,8:,null,null
93,"pj  N earest N ei hbour (Cj , j )",null,null
94,9:,null,null
95,S  S  {pj },null,null
96,10: end for,null,null
97,11: end for,null,null
98,12: return S,null,null
99,4 EXPERIMENTAL SETUP,null,null
100,4.1 Data Sets and Evaluation Measure,null,null
101,"To compare the performance of di erent linkage criteria, we apply hierarchical clustering to a well-known L2R benchmarking collection, Letor 3.0 [7]. In Letor 3.0, there are 7 datasets, based on two document collections: Gov and OHSUMED. We focus on topic distillation (TD2003, TD2004) from GOV and OHSUMED from OHSUMED. In the sets from GOV, each instance is represented by 64 features extracted from the corresponding query-document pairs and has a label indicating its relevance level. e datasets from GOV have binary relevance labels (relevant, not relevant) while",null,null
102,942,null,null
103,Short Research Paper,null,null
104,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
105,"OHSUMED has 3 levels (de nitely relevant, possibly relevant, not relevant) and each instance is represented by a 45-dimensional feature vector. Di erent datasets have di erent numbers of instances; there are 50, 75 and 106 queries, each with 50K, 74K and 16K instances in TD2003, TD2004 and OHSUMED, respectively.",null,null
106,e metric we use is Normalized Discounted Cumulative Gain (NDCG). We run 5-fold cross-validation on all datasets which are query-level normalized and report the average NDCG@10 on 5 folds as nal performance.,null,null
107,4.2 Baselines,null,null
108,"We compare the results obtained using our methods with the approach in [13], which is also based on hierarchical clustering; the results of random sampling and the full training set are also reported for reference:",null,null
109,"Cover. e method proposed in [13] is an unsupervised and compression-based selection mechanism that tries to create a small and highly informative set to represent the original training dataset. Hierarchical clustering (single linkage) is employed to group querydocument pairs into di erent clusters with required number of clusters. e authors also use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster and form the nal training set to be labeled. Unlike our method, they perform clustering globally and instances belonging to di erent queries could be grouped into the same cluster.",null,null
110,"Random. e instances to be labeled are selected randomly and no active learning methods are used here. For the same dataset, we run random sampling 10 times and report the average performance.",null,null
111,Full Training. We use the labeled original training datasets to train the learning to rank models.,null,null
112,"To be able to show the di erence between our methods and [13], we select SVMRank and Random Forests as our ranking models which are also used in [13]. e parameters of SVMRank and Random Forests are tuned using a small validation set.",null,null
113,We run all sampling methods until the fraction of selected instances reaches 50% of the original set.,null,null
114,5 RESULTS AND ANALYSIS,null,null
115,5.1 Experimental Results,null,null
116,"Fig. 1 shows the NDCG@10 of di erent linkage criteria and baselines (denoted by Average, Max, Min, Ward, Cover, Random and Full respectively) on the TD2003, TD2004 and OHSUMED datasets. As we use the instance closest to the centroid to represent the corresponding cluster, the instance selected before may not be selected in later sampling rounds; accuracy is not monotonically increasing.",null,null
117,"On TD2003, in terms of SVMRank (Fig. 1(a)), the accuracy of Average rst exceeds the accuracy of Full at size 2%. Before 24% of the original training set have been selected, all the curves uctuate around 0.35 except Random and Ward. When more and more instances are added to the training set, the performance of Cover goes down and becomes worse than Full. For Random Forests (Fig. 1(d)), Average achieves the highest accuracy before 4% of the original training set has been selected and is the rst one to reach the same accuracy as Full. e accuracies of Ward, Max and Cover start from relatively low points. All methods achieve the performance of Full",null,null
118,"at around 12% except Random. A er 18%, Cover stays below Full (close to 0.36) and uctuates around 0.35.",null,null
119,"Fig. 1(b) and 1(e) describe the performance of di erent methods on the TD2004 dataset. In Fig. 1(b) we see that Average has the highest starting point when 2% of the original training set have been selected. However, a er one more percent has been added to the training set, all methods have a very similar performance at around 0.295. Average and Cover reach the performance of Full with about 5% and continue to rise before they reach their peaks with 13% and 8%, respectively. Except Random, all methods reach the performance of Full with about 11% selected; their accuracy stays higher than that of Full. In terms of Random Forests (Fig. 1(e)), Average still has the highest starting accuracy and a er the uctuations before 11% of the training set has been selected, Average always performs be er than Full and peaks around 19% of the training set, which is also the highest accuracy in all methods. Cover has a relatively high accuracy when the selected size is 7­ 11%.",null,null
120,"e OHSUMED dataset is based on another document collection, di erent from TD2003 and TD2004. e performance of SVMRank and Random Forests on OHSUMED are shown in Fig. 1(c) and 1(f), respectively. In terms of SVMRank, an interesting thing is that Random has similar performance as hierarchical clustering, which means that hierarchical clustering plays a small role when selecting informative instances in this case. With respect to Random Forests, Average, Min, Ward and Max have very similar performance a er 16% of the training data has been selected and they reach the accuracy of Full at around 30%. Although Cover is the rst method to achieve full training set performance with around 7% and reaches the peak at the same time, its accuracy is not stable and dramatically decreases until 17% of the training set has been selected. A er 30% of the original training set has been selected, all methods achieve the same performance as Full.",null,null
121,5.2 Analysis,null,null
122,"As we can see from the results presented above, most of the time, Average outperforms other methods on the TD2003 and TD2004 datasets. Although Min and Cover both use the shortest link to measure the distance between two clusters, they have di erent performance and Min performs be er. One possible reason is that our selection mechanism can guarantee that the proportions of selected instances from each query are same and every query contributes to the nal performance. Another reason might be Cover clusters query-document pairs globally, which causes that query-document pairs from di erent queries will be represented by the instances from one speci c query. And this will limit the number of applicable instance pairs for pairwise ranking models.",null,null
123,"On OHSUMED, Average, Min and Max have similar and stable performance while Cover uctuates dramatically when relatively few instances are selected, especially with the Random Forests learner. e di erence between the proposed methods and Random is not signi cant. How datasets are constructed and what structures datasets have will in uence the performance of clusteringbased active learning methods. For example, an instance from the OHSUMED dataset is represented by 45 features which is fewer than for TD2003 and 2004, and some speci c features could have greater impact on the clustering results. In addition, a query has",null,null
124,943,null,null
125,Short Research Paper,null,null
126,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
127,NDCG@10,null,null
128,0.37 0.35,null,null
129,0.34 0.32,null,null
130,0.45 0.43 0.41,null,null
131,0.33 0.31,null,null
132,0.30,null,null
133,0.39 0.37 0.35,null,null
134,NDCG@10,null,null
135,NDCG@10,null,null
136,0.29,null,null
137,Average,null,null
138,0.27,null,null
139,Cover Max,null,null
140,Min,null,null
141,0.25,null,null
142,Ward Random,null,null
143,Full,null,null
144,0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,null,null
145,(a) SVMRank on TD2003,null,null
146,% selected,null,null
147,0.28,null,null
148,Average,null,null
149,Cover,null,null
150,Max,null,null
151,0.26,null,null
152,Min,null,null
153,Ward,null,null
154,Random,null,null
155,Full,null,null
156,0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,null,null
157,(b) SVMRank on TD2004,null,null
158,% selected,null,null
159,0.33,null,null
160,0.31,null,null
161,Average,null,null
162,0.29,null,null
163,Cover Max,null,null
164,0.27,null,null
165,Min Ward,null,null
166,0.25,null,null
167,Random Full,null,null
168,0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,null,null
169,(c) SVMRank on OHSUMED,null,null
170,% selected,null,null
171,NDCG@10,null,null
172,0.38,null,null
173,0.36,null,null
174,0.34,null,null
175,0.32,null,null
176,0.30,null,null
177,Average,null,null
178,0.28,null,null
179,Cover Max,null,null
180,Min,null,null
181,0.26,null,null
182,Ward Random,null,null
183,Full,null,null
184,0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,null,null
185,%selected,null,null
186,(d) Random Forests on TD2003,null,null
187,NDCG@10,null,null
188,0.35,null,null
189,0.33,null,null
190,0.31,null,null
191,0.29,null,null
192,Average Cover,null,null
193,Max,null,null
194,0.27,null,null
195,Min Ward,null,null
196,Random,null,null
197,Full,null,null
198,0.25 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,null,null
199,%selected,null,null
200,(e) Random Forests on TD2004,null,null
201,NDCG@10,null,null
202,0.45,null,null
203,0.43,null,null
204,0.41,null,null
205,0.39,null,null
206,0.37,null,null
207,0.35,null,null
208,0.33,null,null
209,0.31,null,null
210,0.29,null,null
211,Average Cover,null,null
212,0.27 0.25,null,null
213,Max Min Ward,null,null
214,0.23,null,null
215,Random Full,null,null
216,0.21 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,null,null
217,%selected,null,null
218,(f) Random Forests on OHSUMED,null,null
219,"Figure 1: Performance on TD2003, TD2004, and OHSUMED. e x-axis displays the percentage of selected instances.",null,null
220,"only about 152 associated documents in OHSUMED. When we cluster query-document pairs with respect to each query, the number of selected instances from each query is small and every individual query will have very few associated documents which can in uence the performance of ranking models.",null,null
221,6 CONCLUSIONS AND FUTURE WORK,null,null
222,"In this paper, we adopt hierarchical clustering to select the most informative instances for learning to rank and report the performance of di erent linkage criteria and baselines. On the Letor 3.0 dataset, the performance of average linkage is similar or superior to the baselines while fewer instances are needed. In the future, we will investigate how to make our method more stronger and robust on di erent datasets. One possible direction is to detect correlations between speci c features and clustering performance. How to choose an optimal fraction of instances for each query while the total number of selected instances is xed is another future direction worth exploring.",null,null
223,"Acknowledgments. is research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Microso Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scienti c Research (NWO) under project nrs 612.001.116, HOR-11-10, CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.",null,null
224,REFERENCES,null,null
225,"[1] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML '07, pages 129­136, 2007.",null,null
226,"[2] K. Crammer and Y. Singer. Pranking with ranking. In NIPS '01, pages 641­647, 2001.",null,null
227,"[3] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In ICML '08, pages 248­255, 2008.",null,null
228,"[4] J. C. Gower and G. Ross. Minimum spanning trees and single linkage cluster analysis. Applied statistics, pages 54­64, 1969.",null,null
229,"[5] R. Herbrich, T. Graepel, and K. Obermayer. Support vector learning for ordinal regression. In ICANN '99, pages 97­102, 1999.",null,null
230,"[6] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.",null,null
231,"[7] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pages 3­10, 2007.",null,null
232,"[8] B. Long, J. Bian, O. Chapelle, Y. Zhang, Y. Inagaki, and Y. Chang. Active learning for ranking through expected loss optimization. IEEE Transactions on Knowledge and Data Engineering, 27(5):1180­1191, 2015.",null,null
233,"[9] A. K. McCallumzy and K. Nigamy. Employing em and pool-based active learning for text classi cation. In ICML '98, pages 359­367, 1998.",null,null
234,"[10] H. T. Nguyen and A. Smeulders. Active learning using pre-clustering. In ICML '04, page 79, 2004.",null,null
235,"[11] F. Radlinski and T. Joachims. Active exploration for learning rankings from clickthrough data. In KDD '07, pages 570­579, 2007.",null,null
236,"[12] B. Se les. Active learning literature survey. Technical report, University of Wisconsin­Madison, 2009.",null,null
237,"[13] R. M. Silva, G. Gomes, M. S. Alvim, and M. A. Gon¸calves. Compression-based selective sampling for learning to rank. In CIKM '16, pages 247­256, 2016.",null,null
238,"[14] S. Tong and D. Koller. Support vector machine active learning with applications to text classi cation. Journal of Machine Learning Research, 2(Nov):45­66, 2001.",null,null
239,944,null,null
240,,null,null

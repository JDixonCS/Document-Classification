,sentence,label,data
0,Session 4A: Evaluation 2,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Online In-Situ Interleaved Evaluation of Real-Time Push Notification Systems,null,null
3,"Adam Roegiest, Luchen Tan, and Jimmy Lin",null,null
4,"David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada",null,null
5,"{aroegies,luchen.tan,jimmylin}@uwaterloo.ca",null,null
6,ABSTRACT,null,null
7,"Real-time push noti cation systems monitor continuous document streams such as social media posts and alert users to relevant content directly on their mobile devices. We describe a user study of such systems in the context of the TREC 2016 Real-Time Summarization Track, where system updates are immediately delivered as push noti cations to the mobile devices of a cohort of users. Our study represents, to our knowledge, the rst deployment of an interleaved evaluation framework for prospective information needs, and also provides an opportunity to examine user behavior in a realistic se ing. Results of our online in-situ evaluation are correlated against the results a more traditional post-hoc batch evaluation. We observe substantial correlations between many online and batch evaluation metrics, especially for those that share the same basic design (e.g., are utility-based). For some metrics, we observe li le correlation, but are able to identify the volume of messages that a system pushes as one major source of di erences.",null,null
8,1 INTRODUCTION,null,null
9,"ere is growing interest in systems that address prospective information needs against continuous document streams, exempli ed by social media services such as Twi er. We might imagine a user having some number of ""interest pro les"" representing prospective information needs, and the system's task is to automatically monitor the stream of documents to keep the user up to date on topics of interest. For example, a journalist might be interested in collisions involving autonomous vehicles and wishes to receive updates whenever such an event occurs. Although there are a number of ways such updates can be delivered, we consider the case where they are immediately pushed to the user's mobile device as noti cations. At a high level, these push noti cations must be relevant, novel, and timely.",null,null
10,"To date, there have been two formal evaluations of the push noti cation problem, at the TREC 2015 Microblog Track [15] and the TREC 2016 Real-Time Summarization (RTS) Track [16]. Despite the obvious real-time nature of this problem, systems have been assessed with a post-hoc batch evaluation methodology. It seems",null,null
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080808",null,null
12,obvious that the push noti cation task should be evaluated in an online manner that be er matches how content is actually delivered in operational se ings.,null,null
13,"We describe a user study of real-time push noti cation systems in the context of the TREC 2016 RTS Track, in which systems' noti cations are delivered to users' mobile devices as soon as they are generated. is evaluation is online, in contrast to post-hoc batch evaluations, and in-situ, in that the users are going about their daily activities and are interrupted by the systems' output. Since the RTS Track deployed both this online in-situ methodology and a more traditional batch methodology, the setup provided us with an opportunity to compare the results of both.",null,null
14,"Contributions. We view our work as having two main contributions: First, we describe, to our knowledge, the rst user study and actual deployment of an interleaved evaluation for prospective noti-",null,null
15,cations. Our work is based on a previously-proposed interleaving framework [18] that has only been examined in simulation. We present an analysis of user behavior in such an evaluation methodology and demonstrate that it is workable in practice.,null,null
16,"Second, we compare results of our online methodology to a more traditional batch methodology in the same evaluation. A number of metrics for assessing push noti cation systems have been proposed: we observe substantial correlations between many online and batch metrics, particularly those that share the same basic design (e.g., are utility-based). is is a non-obvious nding, since all judgments in our online methodology are sparse and made locally, with respect to one tweet at a time, whereas the batch evaluation methodology takes into account all relevant tweets via a global clustering process.",null,null
17,ere are two interpretations of this nding:,null,null
18,"· If one believes in the primacy of user-centered evaluations, our results suggest that established batch evaluation metrics are able to capture user preferences.",null,null
19,"· On the other hand, our online evaluation methodology is less mature than the batch evaluation methodology, which has been extensively examined over the past several years; its core ideas date back at least a decade. If one takes this perspective and believes in the primacy of the established approach, then our results suggest a cheaper way to conduct evaluation of push noti cations systems that yield similar conclusions.",null,null
20,"Despite substantial correlations between many online and batch metrics, there are some metrics that exhibit no meaningful correlation. We observe that systems vary widely in the volume of messages they push, and that this is the biggest source of metric disagreement. We do not believe that the proper role of message volume in evaluating push noti cation systems is fully understood, but this paper elucidates key issues as an important rst step.",null,null
21,415,null,null
22,Session 4A: Evaluation 2,null,null
23,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
24,2 BACKGROUND AND RELATED WORK,null,null
25,"Work on prospective information needs against document streams dates back at least a few decades and is closely related to ad hoc document retrieval [6]. Major initiatives in the 1990s include the TREC Filtering Tracks, which ran from 1995 [13] to 2002 [21], and the research program commonly known as topic detection and tracking (TDT) [2]. e TREC Filtering Tracks are best understood as binary classi cation on every document in the collection with respect to standing queries, and TDT is similarly concerned with identifying all documents related to a particular event--with an intelligence analyst in mind. In contrast, we are focused on identifying a small set of the most relevant updates to deliver to users--any more than a handful of noti cations per day would surely be annoying. Furthermore, in both TREC Filtering and TDT, systems must make online decisions as soon as documents arrive. In the case of push noti cations, systems can choose to push older content, thus giving rise to the possibility of algorithms operating on bounded bu ers. Latency is one aspect of the evaluation, allowing systems to trade o output quality with timeliness.",null,null
26,"More recently, Guo et al. [8] introduced the temporal summarization task, whose goal is to generate concise update summaries from news sources about unexpected events as they develop. is has been operationalized in the TREC Temporal Summarization (TS) Tracks from 2013 to 2015 [4]. e task is closely related to the push noti cation problem that we study, and in fact the TREC Real-Time Summarization Track, which provides the context for our work, represents a merger of the TS and Microblog Tracks. However, nearly all previous evaluations, including TDT, TREC Filtering, and Temporal Summarization, merely simulated the streaming nature of the document collection, whereas in RTS the participants were required to build working systems that operated on tweets posted in real time (more details in Section 3).",null,null
27,"Our online in-situ evaluation framework builds on growing interest in so-called Living Labs [22, 24] and related Evaluation-as-aService (EaaS) [9] approaches that a empt to be er align evaluation methodologies with user task models and real-world constraints to increase the delity of research experiments. In this respect, our comparison between user-oriented and batch evaluations ties into a long history of research that examines the correlation between e ectiveness metrics from system-oriented evaluations and metrics from user-oriented evaluations [1, 3, 10, 23, 26, 29­31]. ere is, however, one important di erence: all of these cited papers, with one exception [31], focus on ad hoc retrieval, which has received much a ention over the years. Although there have been previous user studies on push noti cations from the HCI perspective (e.g., [17]), there is relatively li le empirical work on prospective information needs that we can draw from.",null,null
28,"e nal thread of relevant work concerns interleaved evaluations [7, 11, 19, 20, 25], which have emerged as the preferred approach to evaluating web search engines over traditional A/B testing [12]. Our work departs from this large body of literature because these papers all focus on web search ranking, whereas we tackle the push noti cation problem: in our task, systems must take into account temporality and redundancy, both of which are less important in web search. e length of system output (i.e., volume of pushed messages) is another major di erence between",null,null
29,"our task and web ranking. ese issues were explored in a recent paper by Qian et al. [18], who extended the interleaved evaluation methodology to retrospective and prospective information needs on document streams. However, their proposed approach was only validated in simulation. We take the next step by deploying an adapted version of their proposed technique in a live user study.",null,null
30,3 EVALUATION METHODOLOGY,null,null
31,"Although the push noti cation problem is applicable to document streams in general, we focus on social media posts: the public nature of Twi er makes tweets the ideal source for shared evaluations. In particular, Twi er provides a streaming API through which clients can obtain a sample (approximately 1%) of public tweets--this level of access is available to anyone who signs up for an account. In order to evaluate push noti cation systems in a realistic se ing, the TREC 2016 RTS Track de ned an o cial evaluation period during which all participants ""listened"" to the tweet sample stream to identify relevant and novel tweets with respect to users' interest pro les in a timely manner. e evaluation period began Tuesday, August 2, 2016 00:00:00 UTC and lasted until ursday, August 11, 2016 23:59:59 UTC.",null,null
32,"Interest pro les, which represent users' information needs, followed the standard TREC ad hoc topic format of ""title"", ""description"", and ""narrative"". ese were made available to all participants a few weeks prior to the beginning of the evaluation period. Given the prospective nature of the pro les, it is di cult to anticipate what topics will be discussed during the evaluation period and what events will be ""interesting"". Instead, the organizers adopted the strategy of ""overgenerate and cull"": in total, 203 interest pro les were provided to the participants, more than there were resources available for assessment, with the anticipation of le ing users decide what pro les should be assessed (more details below).",null,null
33,3.1 Online Evaluation Setup,null,null
34,"e TREC 2016 RTS Track contained two separate tasks: push noti cations (so-called ""Scenario A"") and email digests (so-called ""Scenario B""). In this paper we are only concerned with push noti cations, but for more details we refer the reader to the track overview [16].",null,null
35,"e overall evaluation framework is shown in Figure 1. Before the evaluation period, participants ""registered"" their systems with the evaluation broker to request unique tokens (via a REST API), which are used in subsequent requests to associate submi ed tweets with speci c systems.1 During the evaluation period, whenever a system identi ed a relevant tweet with respect to an interest pro le, the system submi ed the tweet id to the evaluation broker (also via a REST API), which recorded the submission time. Each system was allowed to push at most ten tweets per interest pro le per day; this limit represents an a empt to model user fatigue.",null,null
36,"Once the evaluation broker recorded a system's submission, the tweet was immediately delivered to the mobile devices of a group of users, where it was rendered as a push noti cation containing both the text of the tweet and the corresponding interest pro le.",null,null
37,"1As is standard in TREC, each participant was permi ed to submit multiple ""runs"" (usually system variants), but for the purposes of this discussion we refer to them as di erent systems.",null,null
38,416,null,null
39,Session 4A: Evaluation 2,null,null
40,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
41,Stream of Tweets,null,null
42,Participating TREC RTS Systems evaluation broker,null,null
43,Twitter API,null,null
44,Assessors,null,null
45,Figure 1: Evaluation setup for push noti cations: systems,null,null
46,listen to the live Twitter sample stream and send results,null,null
47,"to the evaluation broker, which then delivers push noti ca-",null,null
48,tions to users.,null,null
49,"e user may choose to a end to the tweet immediately, or if it arrived at an inopportune time, to ignore it. Either way, the tweet is added to a queue in a custom app on the user's mobile device, which she can access at any time to examine the queue of accumulated tweets. For each tweet, the user can make one of three judgments with respect to the associated interest pro le: relevant, if the tweet contains relevant and novel information; redundant, if the tweet contains relevant information, but is substantively similar to another tweet that the user had already seen; not relevant, if the tweet does not contain relevant information. As the user provides judgments, results are relayed back to the evaluation broker and recorded. Users have the option of logging out of the app, at which point they will cease to receive noti cations completely (until they log back in).",null,null
50,"Our setup has two distinct characteristics: First, judgments happen online as systems generate output, as opposed to traditional batch post-hoc evaluation methodologies, which consider the documents some time (typically, weeks) a er they have been generated by the systems. Note that although the push noti cations are delivered in real-time, it is not necessarily the case that judgments are provided in real time since users can ignore the noti cations and come back to them later. Second, our judgments are in situ, in the sense that the users are going about their daily activities (and are thus interrupted by the noti cations). is aspect of the design accurately mirrors the intended use of push noti cation systems. Furthermore, from the evaluation perspective, we believe that this setup yields more situationally-accurate assessments, particularly for rapidly developing events. With post-hoc batch evaluations, there is always a bit of disconnect as the assessor needs to ""imagine"" herself at the time the update was pushed. With our evaluation framework, we remove this disconnect.",null,null
51,"Our entire evaluation was framed as a user study (with appropriate ethics review and approval). A few weeks prior to the beginning of the evaluation period, we recruited users from the undergraduate and graduate student population at the University of Waterloo, via posts on various email lists as well as paper yers on bulletin boards.",null,null
52,e users were compensated $5 to install the mobile assessment app and then $1 per 20 judgments provided.,null,null
53,"As part of the training process, users installed the custom app described above on their mobile devices. In addition, they subscribed, using an online form, to receive noti cations for interest pro les they were interested in, selecting from the complete list of",null,null
54,"203 interest pro les provided to all systems. To encourage diversity, we did not allow more than three users to select the same pro le (on a rst come, rst served basis).",null,null
55,"e evaluation broker followed the temporal interleaving strategy proposed by Qian et al. [18], which meant that tweets were pushed to users as soon as the broker received the submi ed tweets from the systems. Although Qian et al. only discussed interleaving the output of two systems, it is straightforward to extend their strategy to multiple systems. e broker made sure that each tweet was only pushed once (per pro le), in the case where the same tweet was submi ed by multiple systems at di erent times. Although one can imagine di erent ""routing"" algorithms for pushing tweets to di erent users that have subscribed to a pro le, we implemented the simplest possible algorithm where the tweet was pushed to all users that had subscribed to the pro le. is meant that the broker might receive more than one judgment per tweet.",null,null
56,3.2 Online Metrics,null,null
57,"e output of our online in-situ evaluation is a sequence of judgments, which need further aggregation before we can use the results to compare the e ectiveness of di erent systems. Note that this aggregation is more complicated than a similar process in interleaved evaluations for web search because systems can vary widely in tweet volume (i.e., how many tweets they push). In standard interleaving techniques for evaluating web search, both variant algorithms being tested contribute to the nal ranking for all queries-- thus, it usually su ces to count the number (or fraction) of clicks to determine the winner (e.g., [7, 24]). However, in our case, there isn't a query that lends itself to a natural paired comparison. Some systems are quite pro igate in dispatching noti cations, while other systems are very quiet.",null,null
58,"Another implication of our interleaved evaluation setup is that a user will encounter tweets from di erent systems, which makes the proper interpretation of ""redundant"" judgments more complex. A tweet might only be redundant because the same information was contained in a tweet pushed earlier by another system (and thus not the ""fault"" of the particular system that pushed the tweet). In other words, the interleaving itself was directly responsible for introducing the redundancy. is observation was made by Qian et al. [18], who proposed a heuristic for more accurate credit assignment when interleaving two systems. However, we decided to adopt a much simpler approach (explained below), which is justi ed by our experimental results (more details later).",null,null
59,"Recognizing the issues discussed above, we computed two aggregate metrics based on user judgments:",null,null
60,"Online Precision. A simple and intuitive metric is to measure precision, or the fraction of relevant judgments:",null,null
61,relevant relevant + redundant + not relevant,null,null
62,(1),null,null
63,"We term this ""strict"" precision because systems don't get credit for redundant judgments. As an alternative, we could compute ""lenient"" precision, where the numerator includes redundant judgments. Extending this further, redundant judgments could in principle be assigned fractional credit, but as we discuss later, such schemes do not appear to have any impact on our overall ndings.",null,null
64,417,null,null
65,Session 4A: Evaluation 2,null,null
66,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
67,"Two minor details are worth mentioning for the proper interpretation of this metric: First, tweets may be judged multiple times since a tweet is pushed to all users who had subscribed to the pro-",null,null
68,"le. For simplicity, all judgments are included in our calculation. Second, our precision computation represents a micro-average (and not an average across per-pro le precision). is choice was made due to the sparsity of judgments: macro-averaging would magnify the e ects of pro les with few judgments.",null,null
69,"Online Utility. As an alternative to online precision, we could take a utility-based perspective and measure the total gain received by the user. e simplest method would be to compute the following:",null,null
70,relevant - redundant - not relevant,null,null
71,(2),null,null
72,"which we refer to as the ""strict"" variant of online utility. Paralleling the precision variants above, we de ne a ""lenient"" version of the metric as follows:",null,null
73,(relevant + redundant) - not relevant,null,null
74,(3),null,null
75,"Of course, we could further generalize with weights for each type of judgment. However, we lack the empirical basis for se ing the weights. Furthermore, experimental analyses show that our",null,null
76,ndings are insensitive to weight se ings.,null,null
77,"To summarize: from user judgments, we compute two aggregate metrics--online precision and online utility. Note that there is no good way to compute a recall-oriented metric since we have no control over when and how frequently user judgments are provided.",null,null
78,is is a fundamental limitation of this type of user study.,null,null
79,3.3 Batch Evaluation Setup,null,null
80,"In order to mitigate the risk inherent in any new evaluation methodology, the TREC 2016 RTS Track also deployed a more traditional post-hoc batch evaluation methodology--speci cally, the approach developed for the Tweet Timeline Generation (TTG) task at the TREC 2014 Microblog Track [14], which was also used in 2015 [15].",null,null
81,"e methodology has been externally validated [31] and can be considered mature due to its deployment in multiple formal evaluations. e assessment work ow proceeded in two major stages: relevance assessment and semantic clustering. Here we provide only a brief overview, referring the reader to the cited papers above for additional details.",null,null
82,"Tweets returned by participating systems were judged for relevance by NIST assessors via pooling. Note that this occurred a er the live evaluation period ended, so it was possible to gather all tweets pushed by all participating systems. NIST assessors began a few days a er the end of the evaluation period to minimize the ""staleness"" of tweets. Each tweet was assigned one of three judgments: not relevant, relevant, or highly-relevant. A er the relevance assessment process, the NIST assessors proceeded to perform semantic clustering on only the relevant and highly-relevant tweets. Using a custom interface, they grouped tweets into clusters in which tweets share substantively similar content, or more colloquially, ""say the same thing"". e interpretation of what this means operationally was le to the discretion of the assessor. In particular, they were not given a particular target number of clusters to form; rather, they were asked to use their judgment, considering both the interest pro le and the actual tweets. e output of the cluster annotation",null,null
83,process is a list of tweet clusters; each cluster contains tweets that are assumed to convey the same information.,null,null
84,3.4 Batch Evaluation Metrics,null,null
85,"As previously discussed, push noti cations should be relevant, nonredundant, and timely. One challenge, however, is that there is li le empirical work on how users perceive timeliness. erefore, instead of devising a single-point metric that tries to combine all three characteristics, the organizers decided to separately capture output quality (relevance and redundancy) and timeliness (latency). In this paper, we only focus on output quality metrics. In short, RTS batch evaluation metrics a empt to capture precision, recall, and overall utility. We elaborate below:",null,null
86,Expected Gain (EG) for an interest pro le on a particular day is,null,null
87,de,null,null
88,ned as,null,null
89,1 N,null,null
90,"G(t ), where N is the number of tweets returned and",null,null
91,G(t ) is the gain of each tweet: not relevant tweets receive a gain of 0;,null,null
92,relevant tweets receive a gain of 0.5; highly-relevant tweets receive,null,null
93,"a gain of 1.0. Once a tweet from a cluster is retrieved, all other tweets",null,null
94,from the same cluster automatically become not relevant. is,null,null
95,penalizes systems for returning redundant information. Expected,null,null
96,gain can be interpreted as a precision metric.,null,null
97,Normalized Cumulative Gain (nCG) for an interest pro le on,null,null
98,a particular day is de,null,null
99,ned as,null,null
100,1 Z,null,null
101,"G(t ), where Z is the maximum",null,null
102,possible gain (given the ten tweet per day limit). e gain of each,null,null
103,individual tweet is computed in the same way as above. Note that,null,null
104,gain is not discounted (as in nDCG) because the notion of document,null,null
105,ranks is not meaningful in this context. We can interpret nCG as a,null,null
106,recall-like metric.,null,null
107,"e score for a run is the average over scores for each day over all interest pro les. An interesting question is how scores should be computed for days in which there are no relevant tweets: for rhetorical convenience, we call days in which there are no relevant tweets for a particular interest pro le (in the pool) ""silent days"", in contrast to ""eventful days"" (when there are relevant tweets). In the EG-1 and nCG-1 variants of the metrics, on a silent day, the system receives a score of one (i.e., a perfect score) if it does not push any tweets, or a score of zero otherwise. In the EG-0 and nCG-0 variants of the metrics, for a silent day, all systems receive a gain of zero no ma er what they do. For more details about this distinction, see Tan et al. [28].",null,null
108,"erefore, under EG-1 and nCG-1, systems are rewarded for recognizing that there are no relevant tweets for an interest pro le on a particular day and remaining silent (i.e., the system does not push any tweets). e EG-0 and nCG-0 variants of the metrics do not reward recognizing silent days: that is, it never hurts to push tweets. We show later in our analyses that EG-0 and nCG-0 are poorly-formulated metrics.",null,null
109,"Gain Minus Pain (GMP) is de ned as  · G - (1 -  ) · P, where G (gain) is computed in the same manner as above, pain P is the number of non-relevant tweets that the system pushed, and  controls the balance of weights between the two. We investigated three  se ings: 0.33, 0.50, and 0.66. Note that this metric is the same as the linear utility metrics used in the TREC Filtering [13, 21] and Microblog [27] Tracks, although our formulation takes a slightly di erent mathematical form.",null,null
110,418,null,null
111,Session 4A: Evaluation 2,null,null
112,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
113,,null,null
114,,null,null
115,,null,null
116, Day   ,null,null
117,,null,null
118, ,null,null
119, ,null,null
120, ,null,null
121, ,null,null
122,"Figure 2: Distribution of response times over the rst minute (14.3%), rst ten minutes (44.9%), rst hour (71.3%), and across the entire evaluation period. Percentages in parentheses show how many judgments were received in the corresponding period.",null,null
123,User Judgments Pro les Messages Response,null,null
124,1,null,null
125,53,null,null
126,4,null,null
127,1619,null,null
128,3.27%,null,null
129,2,null,null
130,3305,null,null
131,10,null,null
132,7141 46.28%,null,null
133,3,null,null
134,136,null,null
135,10,null,null
136,5860,null,null
137,2.32%,null,null
138,4,null,null
139,327,null,null
140,8,null,null
141,3795,null,null
142,8.62%,null,null
143,5,null,null
144,949,null,null
145,12,null,null
146,6330 14.99%,null,null
147,6,null,null
148,28,null,null
149,12,null,null
150,7211,null,null
151,0.39%,null,null
152,7,null,null
153,281,null,null
154,10,null,null
155,4162,null,null
156,6.75%,null,null
157,8,null,null
158,1908,null,null
159,15,null,null
160,7754 24.61%,null,null
161,9,null,null
162,3791,null,null
163,33,null,null
164,16654 22.76%,null,null
165,10,null,null
166,680,null,null
167,16,null,null
168,7257,null,null
169,9.37%,null,null
170,11,null,null
171,107,null,null
172,43,null,null
173,22676,null,null
174,0.47%,null,null
175,12,null,null
176,324,null,null
177,2,null,null
178,938 34.54%,null,null
179,13,null,null
180,226,null,null
181,12,null,null
182,7058,null,null
183,3.20%,null,null
184,"Table 1: User statistics. For each user, columns show the number of judgments provided, the number of interest pro-",null,null
185,"les subscribed to, the maximum number of push noti cations received, and the response rate.",null,null
186,"To summarize: we have multiple batch metrics for evaluating push noti cation systems: EG-1 and EG-0 (both of which measure precision), nCG-1 and nCG-0 (both of which measure recall), and GMP with  ,"" {0.33, 0.50, 0.66} (which capture utility).""",null,null
187,4 USER BEHAVIOR,null,null
188,"e evaluation methodology for push noti cations detailed above was deployed in the TREC 2016 Real-Time Summarization Track. In total, 18 groups from around the world participated, submi ing a total of 41 systems (runs). Over the evaluation period, these runs pushed a total of 161,726 tweets, or 95,113 unique tweets a er de-duplicating within pro les.",null,null
189,"To simplify app development, we only targeted users of Android devices. For our evaluation, we recruited a total of 18 users, 13 of whom ultimately provided judgments. Of these, 11 were either graduates or undergraduate students at the University of Waterloo. In total, we received 12,115 judgments over the assessment period, with a minimum of 28 and a maximum of 3,791 by an individual user. Overall, 122 interest pro les received at least one judgment; 93 received at least 10 judgments; 67 received at least 50 judgments; 44 received at least 100 judgments.",null,null
190,We begin with descriptive characterizations of user behavior: a breakdown is shown in Table 1. e second column lists the number of judgments each user provided and the third column shows,null,null
191,Distribution of Notifications and Judgments,null,null
192,8/02,null,null
193,8/03,null,null
194,8/04,null,null
195,8/05,null,null
196,8/06,null,null
197,8/07,null,null
198,8/08,null,null
199,8/09,null,null
200,8/10,null,null
201,8/11 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Hour,null,null
202,"Figure 3: Heatmap showing the volume of push noti cations, overlaid with circles whose areas are proportional to the number of received judgments.",null,null
203,"the number of pro les that each user subscribed to. e fourth column shows the sum of all push noti cations for the pro les that each user subscribed to: this count captures the maximum number of push noti cations that the user could have received during the evaluation period. Note that we do not have the actual number of noti cations each user received because the user could have logged out during some periods of time or otherwise adjusted the local device se ings (e.g., to disable noti cations). e nal column shows the response rate, computed as the fraction between the second and fourth columns (which is a lower-bound estimate). From this table, we see that some users are quite diligent in providing judgments, while others provide judgments more sporadically.",null,null
204,"How quickly do users provide judgments? e plots in Figure 2 answer this question, showing the distribution of response times over the rst minute, rst ten minutes, rst hour, and across the entire evaluation period. e bars show bucketed counts, while the line graph shows cumulative counts. Normalizing, we nd that 14.3% of judgments arrive within the rst minute a er the push noti cation has been delivered, 44.9% of judgments arrive in the",null,null
205,"rst ten minutes, and 71.3% of judgments arrive in the rst hour. We nd that users are quite responsive to interruptions!",null,null
206,"Finally, Figure 3 provides an overview of the entire evaluation period. In the heatmap, each box represents one hour across the ten-day evaluation period: the color re ects the total number of pushed tweets by all systems across all pro les that at least one user subscribed to. A deeper red indicates more tweets pushed. e overlaid circles represent judgments received from all users, where",null,null
207,419,null,null
208,Session 4A: Evaluation 2,null,null
209,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
210,Figure 4: Analyses of online metrics. e le and middle plots show tweet volume vs. online precision and online utility. e right plot shows almost no correlation between online precision and online utility because systems with roughly the same online precision can vary widely in push volume.,null,null
211,"the area is proportional to the number of judgments. Note that time is given in UTC; for reference, 00:00:00 UTC translates into 20:00:00 in the local time zone of the users.",null,null
212,"A few interesting observations follow: we nd that relatively more tweets are pushed by systems in the rst and nal hours of each day. We believe that this is mostly an evaluation artifact: recall that each system receives a quota of ten tweets per day per pro le. At the beginning of each day (hour 00), the quota resets-- thus allowing systems that have used up their quota the previous day to start pushing noti cations again. At the end of each day (hour 23), we believe that the rise in tweets corresponds to systems ""using up"" the remainder of their quota.",null,null
213,"Looking at the circles, which represent the volume of judgments, we see that they mostly line up with the push volume. at is, darker red cells generally have larger circles--the more tweets systems push, the more judgments we receive. However, there are some deviations, which represent delayed judgments--for example, a burst of tweets that wasn't examined until some time later. It is also interesting to note that with the exception of night time when users are asleep, there does not appear to be a consistent diurnal cycle across our population of users. e users are exposed to a pre y constant stream of push noti cations throughout the day (and indeed during sleeping hours also), but there doesn't appear to be a time of the day when we consistently receive more judgments.",null,null
214,5 ANALYSIS,null,null
215,"By design, the TREC 2016 RTS Track employed both the online in-situ interleaved evaluation methodology as well as the more traditional post-hoc batch evaluation methodology. is means that for the same systems and interest pro les, we have independentlyderived metrics from two very di erent approaches. For the batch metrics, NIST assessors fully judged 56 interest pro les (relevance judgments and clusters). Section 3.3 and Section 3.4 provide an overview, but since this is not the focus of our work, we refer the reader to details provided in the track overview [16].",null,null
216,"We begin by presenting separate analyses of online and batch metrics, and then describe results of correlation analyses between them. In particular, comparing online and batch metrics allows us to explore two questions: From the perspective of the user, do user preferences correlate with batch metrics? From the perspective of system-centered evaluations, can unreliable online judgments replace high-quality NIST assessors?",null,null
217,"In considering the online metrics, there is a question regarding which metric to use--the ""strict"" or ""lenient"" variant of online precision and online utility (see Section 3.2). We performed analyses with both: All plots look very similar, except for systematic shi s due to the metric variants; for example, all the absolute precision values increase from ""strict"" to ""lenient"" precision, but the overall relationships between the points remain largely unchanged. erefore, we only report the ""strict"" variants here for brevity. is also suggests that the credit assignment heuristic of Qian et al. [18], which lies somewhere between the strict and lenient variants, is also unlikely to alter our ndings.",null,null
218,5.1 Online Metrics,null,null
219,ree di erent analyses of the online metrics are shown in Figure 4. We organize our ndings around two themes:,null,null
220,"Precision is an intrinsic metric of push noti cation quality, while utility is a convenient composite metric. Online precision computes the fraction of relevant user judgments, but does not factor in the volume of tweets that a system pushes. Online utility implies a particular precision target with volume as a scaling factor, and thus serves as a convenient composite metric. To see why this is so, consider a system that achieves a precision of 0.5: se ing aside relevance grades for now, the expected utility per tweet is zero (for  ,"" 0.5) and the overall expected utility is also zero, regardless of how many tweets the system pushes. A system with lower precision has a negative expected utility per tweet, and the total expected utility is simply that value multiplied by the volume of tweets. Since the precision of most systems in the evaluation falls below 0.5, we observe a strong negative correlation between tweet volume and utility: this can be clearly seen in the middle plot in Figure 4, which shows tweet volume against online utility. Here, volume is measured as the number of tweets pushed by the system for all interest pro les that received at least one judgment.""",null,null
221,"Our argument can be generalized to other ways of computing utility. Of course, one could assign di erent weights to non-relevant tweets, but for every weighting scheme, there is an implied precision at which the expected utility per tweet is zero. Only systems that have higher precision can provide positive utility; otherwise, negative utility is directly proportional to push volume. e same idea can be straightforwardly extended to relevance grades: all utility-based metrics encode (at least implicitly) a breakeven point between ""good"" results and ""bad"" results.",null,null
222,420,null,null
223,Session 4A: Evaluation 2,null,null
224,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
225,"Figure 5: Analyses of batch metrics. e le and middle plots show tweet volume vs. EG-0 and nCG-0, illustrating the dominant e ect of tweet volume, which is a major aw in those metrics. e right plot shows a strong correlation between EG-1 and nCG-1 (with the exception of a few outliers).",null,null
226,"Tweet volume is an independent and important measure of system output. Building on the previous observation, the independence of tweet volume and precision can be clearly seen in the right plot of Figure 4, where we observe almost no relationship between online precision and online utility. is is further reinforced in the le plot, which shows tweet volume vs. online precision. Although we observe a negative correlation overall, the e ect is primarily due to outliers. If we focus only on systems with tweet volume under 2000, there is li le correlation between online precision and volume. In particular, in the band from 0.3 to 0.4 precision, systems vary widely in volume. is leads to the wide spread of precision values for systems that have similar utility in the right plot. us, from the user perspective, we believe that online precision and tweet volume are the two fundamental inputs to metrics for measuring system e ectiveness.",null,null
227,5.2 Batch Metrics,null,null
228,Analyses of batch metrics are shown in Figure 5. Our two main ndings are as follows:,null,null
229,"EG-0 and nCG-0 are awed metrics. Recall that these variants do not reward systems for recognizing that there is no relevant information and staying silent, and thus it never hurts to push noti cations. As a result, these two metrics reward systems that push a large volume of tweets without necessarily di erentiating the quality of those tweets. is is most evident in the middle plot in Figure 5, which shows tweet volume against nCG-0. Tweet volume here is measured as the total number of tweets pushed across the interest pro les that were evaluated by NIST assessors. Due to the much more involved batch evaluation methodology, the NIST assessors considered a smaller set of interest pro les than users in the online evaluation, and thus the plots report smaller tweet volumes. While it is possible to push a large number of non-relevant tweets (bo om right corner of the middle plot), in general, the more tweets a system pushes, the higher its nCG-0 score.",null,null
230,"We note a similar e ect for EG-0, although less pronounced, from the le plot in Figure 5, which shows tweet volume against EG-0. Once again, disregarding the outliers in the bo om right corner, higher tweet volumes correlate with higher EG-0 scores. Since under EG-0 all systems receive EG scores of zero for silent days when there are no relevant tweets, it never hurts to ""guess"" by pushing tweets. us, we believe that EG-0 and nCG-0 are awed metrics since it is unlikely that users desire high-volume systems",null,null
231,"that push tweets of questionable quality. We advocate that these metrics be dropped in future evaluations, and we remove EG-0 and nCG-0 from subsequent analyses in this paper.",null,null
232,"EG-1 and nCG-1 are highly correlated. is correlation can be seen in the right plot in Figure 5. In contrast to EG-0 and nCG-0, these metrics reward systems for remaining silent on days when there is no relevant content. e plot shows that systems with higher gain (utility) also tend to achieve higher precision.",null,null
233,"It is interesting to observe that such a strong correlation exists between EG-1 and nCG-1, since EG is quite similar to precision and nCG is recall-like: in principle, systems could make tradeo s along these two dimensions independently. However, this might simply be a statement about the current state of push noti cation techniques. Nevertheless, we do observe some outliers: the group of runs around 0.06 in EG-1 and around 0.2 in nCG-1 are those that push a high volume of tweets. What they lack in the overall quality of individual tweets, they make up in volume, leading to higher nCG-1 than their EG-1 scores would otherwise suggest (i.e., the points lie above the trend).",null,null
234,5.3 Online vs. Batch Metrics,null,null
235,"Sca er plots correlating various batch metrics against online utility and online precision are shown in Figure 6. In the top row we show correlations between EG-1, nCG-1, and GMP ( ,"" 0.50) against online utility; in the bo om row, the same metrics against online precision. Note that we removed EG-0 and nCG-0 from consideration given the discussion above. For GMP, the choice of  does not change the shape of the plots and does not a ect our conclusions, so for brevity we omit GMP with  "","" {0.33, 0.66}.""",null,null
236,"When performing correlational studies on retrieval experiments, outlier runs may have a disproportional in uence on the results. For example, poor performing systems are easy to distinguish, and most metrics can easily identify poor systems. erefore, including such systems tends to increase correlations in ways that are not particularly helpful in discriminating systems that are not outliers.",null,null
237,"e outliers in our case are systems that push a large volume of tweets and those that push very few tweets. From Figure 4 and Figure 5 we can identify the outliers as those runs that push more than 4000 tweets in the online evaluation and more than 1500 tweets in the batch evaluation. ere are eight such systems and both criteria identify exactly the same systems. On the whole, these are systems that perform poorly. In the plots, we identify",null,null
238,421,null,null
239,Session 4A: Evaluation 2,null,null
240,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
241,      ,null,null
242,      ,null,null
243,      ,null,null
244,"       !""",null,null
245,       ,null,null
246,       !,null,null
247, ,null,null
248, ,null,null
249,,null,null
250, ,null,null
251, ,null,null
252,,null,null
253, ,null,null
254, ,null,null
255,,null,null
256,             ,null,null
257, ,null,null
258,          !    ,null,null
259, ,null,null
260,               ,null,null
261, ,null,null
262,"Figure 6: Scatter plots comparing EG-1, nCG-1, and GMP ( , 0.50) to online utility (top row) and to online precision (bottom row). High-volume systems are represented by empty diamonds and low-volume systems are represented by empty circles. Solid lines denote best t lines with all points; dotted lines denote best t lines discarding high- and low-volume systems.",null,null
263,"these runs separately as empty diamonds. At the other end of the spectrum, we have runs that push very few tweets. We arbitrarily set this threshold to be less than 100 tweets pushed based on the batch evaluation. Since the batch evaluation considered 56 interest pro les spanning 10 days, this translates into less than two tweets per interest pro le (over the entire span), which is close to a system that basically does nothing. ere are ve such runs, identi ed as empty circles in the plots. is leaves us with 28 systems (runs) whose tweet volumes fall somewhere in the middle, identi ed by the solid squares in the plots.",null,null
264,"For each analysis, we considered two separate conditions: First, with all runs. e results of linear regressions are shown as solid lines. Second, we discarded high- and low-volume systems (as described above); the results of linear regressions in these cases are shown as do ed lines. e second condition a empts to remove the in uence of these outliers: in some cases, it a ects the ndings, but in other cases, not. For both conditions, alongside the coe cient of determination for the linear regression shown in the legend, we also report rank correlation in terms of Kendall's  , the standard metric of rank stability in information retrieval experiments.",null,null
265,"ere is a lot to unpack in our results, and so we organize our ndings around several major themes:",null,null
266,"Online utility is highly correlated with GMP. Our strongest nding is a high correlation between online utility and GMP (see Figure 6, top row, right plot). e correlation weakens slightly if we discard the high-volume and low-volume systems, but is still substantial. Because there are many points packed in the top right corner of the plot, the Kendall's  we observe is a bit lower compared to the coe cient of determination, but still solidly in the range that would be considered good agreement for retrieval experiments.",null,null
267,"At rst glance, this might seem like an obvious nding since online utility and GMP are both utility-based metrics, but this is a non-obvious result for several reasons: GMP is computed over",null,null
268,"cluster judgments from pooled tweets and therefore represents a global view over tweets from all systems. In particular, tweets are grouped into clusters and systems do not get credit for pushing tweets that say the same thing. In contrast, online utility captures only a local view of content--users are making decisions about each tweet pushed to them, and the redundant judgments are subjected to the fallacies of human memory (i.e., users may have forgo en having seen similar tweets).",null,null
269,"In addition, GMP is computed using ""dense"" judgments over a smaller set of pro les gathered by pooling, whereas online utility is computed from sparse judgments over uncontrolled samples, since we have no control over when and how frequently users provide judgments. It is surprising that sporadic, unpredictable, in-situ judgments from a multitude of users yield results that are highly-correlated with the careful deliberations of professional NIST assessors.",null,null
270,"Online precision exhibits moderate correlations with EG-1 and nCG-1. is is shown in Figure 6, bo om row, le and center plots. Since",null,null
271,"the de nition of EG-1 shares similarities with online precision, one might expect this, and since EG-1 and nCG-1 are correlated (right plot, Figure 5), it is no surprise to nd that online precision also correlates with nCG-1. As with GMP above, we emphasize that online precision is computed from tweets evaluated in isolation, whereas EG-1 and nCG-1 are based on cluster annotations, which take into account the global cluster structure of tweets relevant to the interest pro le.",null,null
272,"In both cases, the correlation strengthens if we discard high- and low-volume systems (although for nCG-1, Kendall's  is essentially unchanged). An empty run (i.e., a system that does nothing) would receive a score of 0.2339 for EG-1 and nCG-1, which is simply the fraction of silent days when there are no relevant tweets. erefore, low-volume systems receive scores that are close to the score of an empty run, and this throws o the correlation.",null,null
273,422,null,null
274,Session 4A: Evaluation 2,null,null
275,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
276,"Online utility exhibits at best a weak correlation with EG-1 and nCG-1. is is shown in Figure 6, top row, le and center plots. In both",null,null
277,"these cases, the correlation weakens substantially if we remove the high- and low-volume systems. erefore, outliers are giving the impression of a stronger correlation than one that actually exists. In a sense, it is not surprising that we observe li le correlation between a utility metric vs. precision-oriented and recall-like metrics.",null,null
278,"Online precision exhibits almost no correlation with GMP. is is shown in Figure 6, bo om row, right plot. is nding is consistent with what we see in Figure 4. Precision doesn't capture tweet volume, whereas tweet volume has a substantial impact on utility, as previously discussed.",null,null
279,6 DISCUSSION AND LIMITATIONS,null,null
280,"One potential objection to our evaluation methodology is our reliance on explicit user judgments. is, of course, stands in contrast to web ranking, which bene ts from a tremendous amount of implicit judgments that are collected as a byproduct of users searching. However, we argue that explicit judgments are an important component of push noti cations since they, by de nition, interrupt the user. Given that the interruption has already occurred (if the user has chosen to a end to the noti cation), allowing the user an opportunity to provide feedback seems like good design. us, we argue that our evaluation setup is realistic, mirroring how a production push noti cation system might be deployed. For example, in some implementations today, noti cations already appear with a ""dismiss"" option for users to take explicit action; adding options for quality feedback would incur minimal extra cost.",null,null
281,"At a high-level, our work supports three ndings: First, that online in-situ interleaved evaluations of push noti cations systems are workable in practice. It is indeed possible to recruit users and they are willing to provide su cient judgments (quite diligently, in fact) to meaningfully evaluate systems. is seems consistent with our arguments above regarding the role of explicit judgments in push noti cation systems. Second, we observe substantial correlations between online and batch evaluation metrics that share the same design (e.g., are precision-oriented or utility-based). ird, the volume of messages that a system pushes is an important aspect of system evaluation, but its role is not fully understood.",null,null
282,"With respect to the second nding--the substantial correlation between online and batch metrics--this is by no means obvious, given the large body of literature that has shown divergences between user- and system-oriented metrics (see Section 2). We have touched on some of the main di erences between the online and batch methodologies, but they bear additional emphasis:",null,null
283,"· e batch evaluation considered all tweets that all systems pushed during the evaluation period. at is, all push noti cations were included in the pool and so the judgments are exhaustive from the perspective of the participants. is stands in contrast to the online judgments, which are best characterized as a small convenience sample by users (see response rates in Table 1). We have no control over when and how many judgments are provided-- and whether there are any systematic biases, for example, a user who only marks relevant tweets but ignores non-relevant tweets (i.e., a bias against explicit negative judgments).",null,null
284,"· e batch metrics all operate at the level of semantic clusters, taking into account redundancy. ese clusters are formed from the pool and therefore contain a ""global"" view of tweets pushed by all systems. Accordingly, systems are penalized for retrieving multiple tweets that say that same thing. In contrast, user judgments occur tweet-by-tweet and represent a ""local"" view-- our users assess only the tweets in front of them. Furthermore, redundant judgments are made with respect only to tweets the users had previously assessed, and are subject to the e ects of imperfect memory. Another consequence of this setup is that from batch judgments we can characterize silent days (at least within the limitations of pooling), whereas with online judgments there is no way for the user to obtain this information. us, it is not possible for an online metric to reward systems for ""staying quiet"". Finally, high-volume systems (which tend to have lower precision) are disproportionately represented.",null,null
285,"· e batch evaluation used professional NIST assessors, many with decades of experience. ey have become the gold standard against which human judgments are compared [5]. Contrast this with our users: since they are simply going about their daily lives (which is indeed the point), we have no idea in what context they are assessing tweets--were they alone in a quiet se ing considering tweets with care or hurriedly skimming tweets while multi-tasking? We assume our users were acting in good faith and judging the tweets to the best of their ability (and we have no reason to suspect otherwise), but the overall delity and quality of judgments are likely to be lower than the NIST assessors who operated in a carefully-controlled environment.",null,null
286,"ese di erences considered, we nd the correlations between online and batch metrics non-obvious and interesting. ere are two di erent interpretations to these results:",null,null
287,"If one believes in the primacy of user-centered evaluations, our ndings suggest that established batch evaluation metrics are able to capture user preferences. at is, the batch metrics are capturing aspects of what users care about in useful systems. is result nicely complements the ndings of Wang et al. [31], who validated batch metrics for the related task of retrospective timeline summarization over tweet streams.",null,null
288,"On the other hand, one might put more faith in a mature batch evaluation methodology that has been through the gauntlet of multiple deployments, and whose core ideas date back at least a decade. If one takes this perspective and believes in the primacy of the established approach, then our results suggest a cheaper way to conduct evaluations of push noti cation systems that yield similar conclusions. Of course, these two perspectives are not necessarily con icting. Instead, they point to more work that is necessary to fully align user- and system-oriented perspectives to assessing push noti cation systems.",null,null
289,"It makes sense to discuss some of the limitations of this work. Our users are compensated for their participation in the study and thus can be assumed to operate under certain social norms. One might argue that their behavior would be di erent had they ""organically"" discovered an app for push noti cations. While this is certainly a legitimate criticism, it could be leveled against any user study that involves compensation--potential di erences between paid subjects and ""real users"" are beyond the scope of this study.",null,null
290,423,null,null
291,Session 4A: Evaluation 2,null,null
292,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
293,"A closely-related issue is the fact that our users subscribed to interest pro les that were not ""their own"", i.e., they did not come up with the information needs themselves. is concern, however, is mitigated by having users select from a broad range of pro les (a couple hundred) to match their interests. erefore, our evaluation is less likely to have su ered from user indi erence.",null,null
294,"Another limitation of our study is that it captures only a snapshot of current technology. is, of course, is an implicit quali cation of any evaluation, not just our work. For example, consider a empts to control the volume of push noti cations and to recognize when there is no relevant content: such techniques are nascent at best, since the community is just beginning to understand the nuances of systems ""learning when to shut up"". We have found that these issues are confounding variables when trying to correlate online and batch metrics, but as the technology evolves and matures, the nature of the confound might change. As another example, we empirically observe that EG-1 and nCG-1 are correlated, even though in principle systems can operate in a tradeo space in which the measures are not correlated. Nevertheless, we are unable to speculate on future developments that have yet to happen--we can only draw conclusions based on the data at hand. e only way to address this limitation is a follow-up study that considers systems once push noti cation techniques have substantially progressed.",null,null
295,7 CONCLUSIONS,null,null
296,"is paper describes a formal user study of push noti cation systems with two distinct characteristics: tweets are assessed online and in-situ. As the infrastructure for conducting our evaluation can be reused (all so ware deployed in this study is open source), future iterations will take less e ort. erefore, we hope to see more of these evaluations as the methodology becomes ""just another hammer"" in the toolbox of information retrieval researchers and practitioners.",null,null
297,"As an outstanding issue, we believe that the proper role of notication volume in evaluating systems is not yet fully understood. As we have empirically observed, systems with the same precision can vary widely in the volume of noti cations they push. However, the question remains: how much content should a system actually push? Even assuming that systems can achieve high precision--let's say, 90% or greater--are more noti cations really be er? Intuitively, one would expect that, at some point, user fatigue sets in, even for a stream of high-quality tweets. We might imagine the user having access to a ""volume dial"" to provide feedback: ""yes, these are all good tweets, but too many!"" As we have shown, tweet volume is not directly captured in existing metrics, but the problem lies deeper: our understanding of how users perceive noti cations in response to prospective information needs remains quite poor, especially when factoring in the cost of interruptions. More work on fundamental issues along these lines is needed.",null,null
298,8 ACKNOWLEDGMENTS,null,null
299,"is work was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada, with additional contributions from the U.S. National Science Foundation under IIS-1218043 and CNS-1405688. Any ndings, conclusions, or recommendations expressed do not necessarily re ect the views of the sponsors.",null,null
300,REFERENCES,null,null
301,"[1] Azzah Al-Maskari, Mark Sanderson, Paul Clough, and Eija Airio. 2008. e Good and the Bad System: Does the Test Collection Predict Users' E ectiveness? In SIGIR. 59­66.",null,null
302,"[2] James Allan. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publishers, Dordrecht, e Netherlands.",null,null
303,"[3] James Allan, Ben Cartere e, and Joshua Lewis. 2005. When Will Information Retrieval Be ""Good Enough""? User E ectiveness as a Function of Retrieval Accuracy. In SIGIR. 433­440.",null,null
304,"[4] Javed Aslam, Fernando Diaz, Ma hew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Tetsuya Sakai. 2015. TREC 2015 Temporal Summarization Track Overview. In TREC.",null,null
305,"[5] Peter Bailey, Nick Craswell, Ian Soboro , Paul omas, Arjen P. de Vries, and Emine Yilmaz. 2008. Relevance Assessment: Are Judges Exchangeable and Does it Ma er? In SIGIR. 667­674.",null,null
306,"[6] Nicholas J. Belkin and W. Bruce Cro . 1992. Information Filtering and Information Retrieval: Two Sides of the Same Coin? CACM 35, 12 (1992), 29­38.",null,null
307,"[7] Olivier Chapelle, orsten Joachims, Filip Radlinski, and Yisong Yue. 2012. LargeScale Validation and Analysis of Interleaved Search Evaluation. ACM TOIS 30, 1 (2012), Article 6.",null,null
308,"[8] Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013. Updating Users about Time Critical Events. In ECIR. 483­494.",null,null
309,"[9] Allan Hanbury, Henning Mu¨ller, Krisztian Balog, Torben Brodt, Gordon V. Cormack, Ivan Eggel, Tim Gollub, Frank Hopfgartner, Jayashree Kalpathy-Cramer, Noriko Kando, Anastasia Krithara, Jimmy Lin, Simon Mercer, and Martin Potthast. 2015. Evaluation-as-a-Service: Overview and Outlook. arXiv:1512.07454.",null,null
310,"[10] William Hersh, Andrew Turpin, Susan Price, Benjamin Chan, Dale Kramer, Lyne a Sacherek, and Daniel Olson. 2000. Do Batch and User Evaluations Give the Same Results? In SIGIR. 17­24.",null,null
311,"[11] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2011. A Probabilistic Method for Inferring Preferences from Clicks. In CIKM. 249­258.",null,null
312,"[12] Ron Kohavi, Randal M. Henne, and Dan Sommer eld. 2007. Practical Guide to Controlled Experiments on the Web: Listen to Your Customers not to the HiPPO. In KDD. 959­967.",null,null
313,"[13] David D. Lewis. 1995. e TREC-4 Filtering Track. In TREC. 165­180. [14] Jimmy Lin, Miles Efron, Yulu Wang, and Garrick Sherman. 2014. Overview of",null,null
314,"the TREC-2014 Microblog Track. In TREC. [15] Jimmy Lin, Miles Efron, Yulu Wang, Garrick Sherman, and Ellen Voorhees. 2015.",null,null
315,"Overview of the TREC-2015 Microblog Track. In TREC. [16] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees,",null,null
316,"and Fernando Diaz. 2016. Overview of the TREC 2016 Real-Time Summarization Track. In TREC. [17] Abhinav Mehrotra, Veljko Pejovic, Jo Vermeulen, Robert Hendley, and Mirco Musolesi. 2016. My Phone and Me: Understanding People's Receptivity to Mobile Noti cations. In CHI. 1021­1032. [18] Xin Qian, Jimmy Lin, and Adam Roegiest. 2016. Interleaved Evaluation for Retrospective Summarization and Prospective Noti cation on Document Streams. In SIGIR. 175­184. [19] Filip Radlinski and Nick Craswell. 2010. Comparing the Sensitivity of Information Retrieval Metrics. In SIGIR. 667­674. [20] Filip Radlinski and Nick Craswell. 2013. Optimized Interleaving for Online Retrieval Evaluation. In WSDM. 245­254. [21] Stephen Robertson and Ian Soboro . 2002. e TREC 2002 Filtering Track Report. In TREC. [22] Alan Said, Jimmy Lin, Alejandro Bellog´in, and Arjen P. de Vries. 2013. A Month in the Life of a Production News Recommender System. In CIKM Workshop on Living Labs for Information Retrieval Evaluation. 7­10. [23] Mark Sanderson, Monica Paramita, Paul Clough, and Evangelos Kanoulas. 2010. Do User Preferences and Evaluation Measures Line Up? In SIGIR. 555­562. [24] Anne Schuth, Krisztian Balog, and Liadh Kelly. 2015. Overview of the Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab 2015. In CLEF. [25] Anne Schuth, Katja Hofmann, and Filip Radlinski. 2015. Predicting Search Satisfaction Metrics with Interleaved Comparisons. In SIGIR. 463­472. [26] Mark Smucker and Chandra Jethani. 2010. Human Performance and Retrieval Precision Revisited. In SIGIR. 595­602. [27] Ian Soboro , Iadh Ounis, Craig Macdonald, and Jimmy Lin. 2012. Overview of the TREC-2012 Microblog Track. In TREC. [28] Luchen Tan, Adam Roegiest, Jimmy Lin, and Charles L. A. Clarke. 2016. An Exploration of Evaluation Metrics for Mobile Push Noti cations. In SIGIR. 741­ 744. [29] Andrew Turpin and William R. Hersh. 2001. Why Batch and User Evaluations Do Not Give the Same Results. In SIGIR. 225­231. [30] Andrew Turpin and Falk Scholer. 2006. User Performance versus Precision Measures for Simple Search Tasks. In SIGIR. 11­18. [31] Yulu Wang, Garrick Sherman, Jimmy Lin, and Miles Efron. 2015. Assessor Di erences and User Preferences in Tweet Timeline Generation. In SIGIR. 615­ 624.",null,null
317,424,null,null
318,,null,null

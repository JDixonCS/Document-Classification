,sentence,label,data
0,Short Resource Papers,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,A Large-Scale ery Spelling Correction Corpus,null,null
3,Ma hias Hagen,null,null
4,Martin Po hast Marcel Gohsen Anja Rathgeber,null,null
5,"Bauhaus-Universita¨t Weimar 99421 Weimar, Germany",null,null
6,rstname . lastname @uni-weimar.de,null,null
7,Benno Stein,null,null
8,ABSTRACT,null,null
9,"We present a new large-scale collection of 54,772 queries with manually annotated spelling corrections. For 9,170 of the queries (16.74%), spelling variants that are di erent to the original query are proposed. With its size, our new corpus is an order of magnitude larger than other publicly available query spelling corpora. In addition to releasing the new large-scale corpus, we also provide an implementation of the winner of the Microso Speller Challenge from 2011 and compare it on the di erent publicly available corpora to spelling corrections mined from Google and Bing. is way, we also shed some light on the spelling correction performance of state-of-the-art commercial search systems.",null,null
10,1 INTRODUCTION,null,null
11,"ery spelling correction is an important step of the query understanding process at search engine side. When a query is submi ed, it is usually rst tokenized and ""normalized"" (e.g., lowercasing), directly followed by a spelling correction. A er that, the query might be lemmatized/stemmed, entities might be detected, etc. However, these subsequent steps of understanding a user's query heavily rely on good spelling (e.g., entities with wrong spelling can be very di cult to accurately detect). us, spelling correction for queries a racted a lot of a ention, both within the Microso Speller Challenge 2011 [22] and in subsequent publications on participating approaches as well as improved versions thereof.",null,null
12,"Today, commercial search engines typically o er corrections even while the user is typing [5], and they correct misspelled queries very reliably, asking ""Did you mean [alternative spelling],"" or even directly ""Showing results for"" their best guess. However, not too many details about the underlying systems are published. Instead, academic research on improved spelling correction algorithms still has to rely on only two publicly available corpora with about 6,000 annotated queries each (16­19% with spelling variants di erent to the original query), one being a training set of the mentioned Microso Speller Challenge 2011 [22], the other being published by the third-placed team who used it as an additional training set for the challenge [7]. To o er an alternative, largescale resource, we release a corpus of 54,772 web search queries, out of which 16.74% come with spelling variants di erent to the",null,null
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080749",null,null
14,"original query.1 Along the corpus, we also release the code2 of a re-implementation of the best-performing approach from the Microso Speller Challenge [14] and compare it on our new corpus and the two other publicly available ones against spelling corrections mined from Google's and Bing's search engines.",null,null
15,"e analysis of the results shows that our new corpus is a li le harder for the spelling correctors, with Precision@1 scores dropping by about 5­10%. Only the Google spelling correction performs be er than a baseline that does not change the input query at all.",null,null
16,"us, our new corpus o ers a challenging alternative to the two existing corpora. Our re-implementation of Lueck's approach [14], who achieved the best performance within the Microso Speller Challenge, also struggles to beat the baseline. is indicates that the version that participated in the challenge probably heavily relied on not fully documented optimizations against the challenge's evaluation metrics that might not help in real-world situations.",null,null
17,2 RELATED WORK,null,null
18,"ery spelling correction has been a lively research topic since the mid 2000's, especially in the NLP community [1, 4, 11]. Back in that time an (in)famous slide from some Google presentation presented literally hundreds of misspellings of the then-celebrity Britney Spears (or Bri any Spiers ).",null,null
19,"Most systems for spelling correction from that time (and still today) are based on language models for the a priori probabilities of words and an error model (e.g., noisy channel) to estimate probabilities of misspellings [16]. Especially due to the error models trained on the input of billions of users, today's commercial search engines can provide a spelling performance that seems to ""magically"" second guess the intended query for most misspellings. In particular, today's search engines go as far as to suggest corrections even while the user is still typing [5], or they try to avoid user misspellings at all by sensible query auto-completions without errors [2], which is also still an ongoing research problem [10].",null,null
20,"e problem of query spelling correction a racted a lot of attention around 2010, with the Microso Speller Challenge organized in the year 2011 [22] having more than 300 teams participating. With this challenge, a large public set of 5,892 spellcorrected queries sampled from the TREC Million ery track was released for training. e best-performing approach of Gord Lueck [14], based on combining Hunspell3 suggestions, was followed by Cloudspeller [12] and qSpell [7]. Also the ideas of the other top-performing participants [15, 17, 20] in uenced approaches published later [3, 6, 9, 13, 19, 21], indicating that query spelling is not ""solved"" yet.",null,null
21,1h ps://www.uni-weimar.de/medien/webis/corpora/ 2h ps://github.com/webis-de/SIGIR-17 3h p://hunspell.github.io/,null,null
22,1261,null,null
23,Short Resource Papers,null,null
24,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
25,"Still, there are only two publicly available larger query corpora with annotations of potential alternative spellings. First, the aforementioned set of 5,892 queries published by the Microso Speller Challenge [22], and a set of 6,000 queries which the qSpell team released as their additional training set [7]. To further add to the publicly available corpora, we publish a set of 54,772 queries and possible alternative spellings for 9,170 of them.",null,null
26,3 OUR NEW CORPUS,null,null
27,"We detail the sampling and annotation process of our corpus and compare it to the mentioned other two publicly available ones with regard to error types, error frequencies, etc.",null,null
28,3.1 ery Sampling,null,null
29,"For the creation of a previous query segmentation corpus [8], we had sampled 55,555 queries with 3 up to 10 words (i.e., with 2 up to 9 whitespaces) from the AOL query log [18] in three steps: (1) the raw query log was ltered in order to remove ill-formed queries, (2) from the remainder, queries were sampled at random respecting the query length distribution, and (3) the sampled queries were manually checked for anonymity-breaking words, languages other than English, containing child porn intents, etc.",null,null
30,"In the rst step ( ltering), queries were discarded according to the following exclusion criteria:",null,null
31,"· eries comprising remnants of URLs (e.g., .com or h p) or URL character encodings (to exclude strictly ""navigational"" queries caused by confusing the search box with the address bar).",null,null
32,"· eries from searchers having more than 10,000 queries in the logged 3-month period (to exclude some query bots).",null,null
33,· eries from searchers whose average time between consecutive queries is less than one second (to further exclude query bots).,null,null
34,· eries from searchers whose median number of le ers per query is more than 100 (probably also bots).,null,null
35,· eries that contain non-alphanumeric characters except for dashes and apostrophes in-between characters.,null,null
36,· eries from searchers that duplicate preceding queries of themselves (to exclude result page interaction from the query frequency calculation).,null,null
37,· eries with less than three or more than ten words.,null,null
38,"We had a corpus size of more than 50,000 queries in mind and anticipated that the necessary manual cleansing (third step) could reduce the size of any query sample--thus, initially 55,555 queries were drawn to account for up to a potential 10% reduction.",null,null
39,"To accomplish the query length distribution sampling (second step), the ltered log was divided into query length classes, where the i-th class contains all queries with i words (i.e., i-1 whitespaces), keeping duplicate queries from di erent searchers. en, the query length distribution was computed and the amount of each length class to be expected in a 55,555 query sample was determined. Based on these expectations, for each length class, queries were sampled without replacement until the expected amount of distinct queries was reached. Hence, our sample represents the query length distribution of the ltered log. And since each length class in the ltered log contained duplicate entries of queries according to their frequency, our sample also represents the query frequency distribution in the ltered query log. One might argue that our",null,null
40,"sampling may miss many rare spelling errors but on the other hand, one might also argue that we just favor the more frequent errors whose correction could help many users. Either way, our later analyses of the amount of errors will show that they are similar to the previous corpora.",null,null
41,"In the nal manual cleansing (third step), we had one annotator go through all the 55,555 queries, labeling those that are non-English (the target language of our corpus), containing child porn intents (to be excluded from our corpus), or containing any potentially anonymity-breaking information (e.g., social security numbers, etc.). A er the cleansing, 54,772 queries remained such that our goal of sampling more than 50,000 queries was easily reached. ese 54,772 queries then went into manual spelling variant annotation.",null,null
42,"Parenthesis: A Word on Anonymity. e AOL query log has been released without proper anonymization (only replacing the searchers' IP addresses with numerical IDs) [18]. is raised a lot of concerns among researchers as well as in the media, since some AOL users could be personally identi ed by analyzing their queries. We address this problem in our corpus by removing searcher IDs entirely and only publishing query strings without submission times or surrounding interactions. is way, queries from our sample could only be reliably mapped back to some original searcher if they contain user-identifying information or if they were submi ed by only one user in the AOL log. With our cleansing step described above, we try to avoid the former potential anonymity breach, while, against the la er, someone would have to actually trace a query back in the AOL log and then be able to de-anonymize the respective user(s).",null,null
43,3.2 ery Spelling Correction,null,null
44,"As for the spelling correction, 2 independent annotators went through all the 54,772 queries; allowed to use any tool they wanted to support their work (e.g., Hunspell, aspell, search engines, dictionaries, Wikipedia). For each query, potential alternative spellings (also possibly more than one) had to be annotated. A er two months of working on the spelling corrections (not necessarily full-time), both annotators discussed the cases where they disagreed. is typically resulted in di erent reasonable spelling variants being fed into the nal corpus. A er this step, three annotators each independently checked one third of the queries that contained alternative spellings from the rst iteration and could further add or remove variants if need be--also using tools of their choice. Finally, for 9,170 queries (16.74%) some variant di erent to the original spelling was annotated in the process.",null,null
45,"Of course, this annotation process is not perfect and some spelling errors might have been missed or even been introduced. Hence, correcting the queries will remain an ongoing task with potential future corpus updates. For instance, a er the corpus release, the community working with the corpus may submit further spelling variants that will then be included and also made publicly available.",null,null
46,3.3 Corpora Analysis and Comparison,null,null
47,Table 1 contains the characteristics of the two previously available spelling correction corpora and our new corpus. e typical spelling error types reported in the table are deletion (entertaner,null,null
48,1262,null,null
49,Short Resource Papers,null,null
50,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
51,"Table 1: Corpora characteristics (MS ,"" Microso Speller Challenge, JDB "","" qSpell corpus, Ours "", our new corpus).",null,null
52,MS,null,null
53,JDB,null,null
54,Ours,null,null
55,Corpus size,null,null
56,eries,null,null
57,"5,892",null,null
58,"6,000",null,null
59,"54,772",null,null
60,"w/ alternative spellings 1,121 (19.04%) 983 (16.38%) 9,170 (16.74%)",null,null
61,Error type frequency (percentage of all queries with alternative spellings),null,null
62,Deletion Insertion Space Special character Substitution Transposition,null,null
63,308 (27.45%) 163 (14.53%) 625 (55.70%),null,null
64,0 ( 0.00%) 135 (12.03%) 31 ( 2.76%),null,null
65,226 (22.99%) 235 (23.91%) 497 (50.56%),null,null
66,0 ( 0.00%) 118 (12.00%) 27 ( 2.75%),null,null
67,"3,054 (33.30%) 1,688 (18.41%) 2,821 (30.76%) 3,229 (35.21%) 1,751 (19.09%)",null,null
68,386 ( 4.21%),null,null
69," entertainer), insertion (baseballl  baseball), missing or added spaces (e.g., sponge bob  spongebob), missing or wrong special characters (e.g., noahs ark  noah's ark), substitution (canfederate  confederate), and transposition (chevorlet  chevrolet). Note that the numbers per error type do not necessarily add up to the number of queries with alternative spellings since some queries might contain more than one error type (the percentages indicate the ratio of queries with spelling variants that have a particular error in some variant).",null,null
70,"As can be seen, the overall ratio of queries with alternative spellings is similar in all corpora. However, per error type, it is obvious that our annotators were the only ones who also annotated special characters as possible spelling variants; although we did not instruct them to do so. Since spelling correction o en takes place a er query normalization (i.e., a er removal of special characters), we added respective variants without special characters in a postprocessing. is ensures compatibility of our corpus with any ordering of the query understanding pipeline (i.e., normalization before or a er spelling correction). On average, the number of spelling variants per query is around 1.05­2.36 for di erent query classes in the corpora (i.e., most corrected queries have just one or two spelling variants) while the average Levenshtein distance from the original query to its closest variant is around 0.3­1.5 for queries with alternative spellings (especially in the Microso Speller Challenge corpus, the original spelling o en is among the alternative spellings).",null,null
71,"Altogether, our new corpus has similar error characteristics as the smaller previous corpora with the potential additional bonus of also including corrections with special characters.",null,null
72,4 EVALUATION,null,null
73,"To compare the di erent corpora not just based on the annotated errors but also with respect to how hard it is for state-of-the-art query spelling correction to handle the ones contained, we conduct a pilot experiment on all three corpora. As a baseline, we choose the approach that does nothing, which turns out to be a rather strong competitor due to the large number of queries not containing any error (>80%) or having the original spelling as one variant. is baseline is contrasted with a re-implementation of Gord Lueck's approach that won the Microso Speller Challenge;",null,null
74,Table 2: ery spelling correction performance.,null,null
75,Prec@1,null,null
76,Microso Corpus,null,null
77,Google Bing Lueck Baseline,null,null
78,0.962 0.948 0.650 0.947,null,null
79,JDB Corpus,null,null
80,Google Bing Lueck Baseline,null,null
81,0.947 0.929 0.619 0.906,null,null
82,Our Corpus,null,null
83,Google Bing Lueck Baseline,null,null
84,0.912 0.851 0.541 0.851,null,null
85,EF1,null,null
86,0.892 0.865 0.854 0.873,null,null
87,0.914 0.888 0.877 0.870,null,null
88,0.904 0.833 0.836 0.842,null,null
89,EP,null,null
90,0.961 0.928 0.887 0.947,null,null
91,0.941 0.918 0.900 0.906,null,null
92,0.905 0.833 0.812 0.851,null,null
93,ER,null,null
94,0.833 0.810 0.823 0.810,null,null
95,0.888 0.860 0.855 0.836,null,null
96,0.903 0.833 0.863 0.833,null,null
97,basing the re-implementation solely on Lueck's publication for the,null,null
98,challenge to also conduct a small-scale reproducibility study. To,null,null
99,"also include current search systems, we submi ed all the queries",null,null
100,from the three corpora to the Google and Bing search engines and,null,null
101,"checked whether they suggested corrections (""Showing results for,""",null,null
102,"Containing results for, ""Did you mean,"" etc.). Table 2 contains the",null,null
103,"results of the three aforementioned approaches, and the baseline",null,null
104,that does nothing.,null,null
105,"As evaluation measures, we employ the ones established in the",null,null
106,"Microso Speller Challenge (EF1, EP, ER), and additionally Precision@1 to check how good an approach's candidate with the",null,null
107,"highest con dence actually is. For the Microso Speller Challenge,",null,null
108,the spell correction approaches could submit a set C of potential,null,null
109,correction candidates for each query q from the query set Q of the,null,null
110,corpus that also contains the gold standard corrections G (q) for ev-,null,null
111,ery query. A correction candidate c from the derived correction set,null,null
112,C (q) of query q has to come with a likelihood or con dence P (c |q),null,null
113,that c actually is a valid spelling for q; the P (c |q) values have to sum,null,null
114,"up to 1 for each query. e ""expected precision"" EP and ""expected",null,null
115,"recall"" ER of a spelling correction approach then are de ned as",null,null
116,follows:,null,null
117,"EP , 1",null,null
118,"P (c |q), and",null,null
119,|Q | q Q c C (q)G (q),null,null
120,"ER , 1",null,null
121,|C (q)  G (q)| .,null,null
122,|Q | q Q |G (q)|,null,null
123,"e combined EF1 score is de ned as 0.5 · (1/EP + 1/ER). Note that with the above de nitions, a successful strategy can be to include many potential corrections with low con dence scores in order to increase ER without harming EP too much. To somewhat counter this possibility, we also report Precision@1, which is simply the average over all queries of the precision at the rst rank given the con dence scores (i.e., ""simulating"" the real-world scenario that a search system has to actually decide whether to correct a query or not, whereas giving tens of possible candidates is not supporting a user). In case of ties at the rst rank (i.e., same con dence scores),",null,null
124,1263,null,null
125,Short Resource Papers,null,null
126,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
127,"we checked whether one of these top-ranked corrections is in the gold standard (i.e., in doubt, favor the approach). To compute the con dence scores in case of Google and Bing, we resorted to a simple heuristic: (a) if just results for an alternative spelling are shown, this variant gets a con dence of 1 (""Showing results for""), (b) if results for an alternative and the original spelling are shown, the alternative gets 0.55 and the original 0.45 (""Containing results for""), (c) if only results for the original spelling are shown but an alternative is suggested, the original gets a con dence of 0.75 and the alternative of 0.25 (""Did you mean"").",null,null
128,"As can be seen from Table 2, only Google reliably outperforms the do-nothing baseline. It is also particularly striking how low the Prec@1 scores of Lueck's approach are. In fact, we also could not really reproduce the performance of EF1 > 0.9 that was reported for Lueck's approach on the Microso Corpus [6]. We tried to follow Lueck's description of his approach [14] as closely as possible but some parts of the scoring scheme might not have been described and also some ""optimizations"" targeting the Speller Challenge's evaluation measures might not have been reported. Still, even the Bing system struggles to improve upon the baseline.",null,null
129,"To further analyze the problems of Bing and Lueck's approach, we take a closer look on the error classes and on queries without spelling problems. While an in-depth analysis is beyond the scope of this paper, we summarize some particularly interesting insights with a focus on Prec@1 since the top rank would probably be the basis for retrieving search results. On queries with no errors, only Google and Bing achieve Prec@1 close to 0.99 while Lueck's approach for about every second or third such query suggests a topvariant that is not in the ground truth. e only approach achieving Prec@1 above 0.5 for most classes (error types and without error) is the Google system (except space and special character). Bing and Lueck's approach for many error classes like insertion or deletion only perform around 0.1­0.2 for Prec@1 (only at most one to two out of ten rank 1 suggestions actually are in the gold standard). On such cases, Lueck's approach rather achieves be er EF1 scores than Bing on our corpus. is is probably due to the many reported possible candidates (our scraping of the Bing suggestions can just report one or two candidates). However, on the Microso and the JDB corpora, the EF1 scores of Bing on error classes are about twice as large as the ones from Lueck; both still being below 0.4, though.",null,null
130,Our brief experimental study shows that Google actually seems to have the most useful spelling corrections (high Prec@1 for almost all classes and also highest EF1 scores) while Bing is somewhat behind and the many suggestions produced by Lueck's approach do not help in the practically important Prec@1 category.,null,null
131,5 CONCLUSION AND OUTLOOK,null,null
132,"Our new freely available corpus of query spelling corrections is about an order of magnitude larger than the two previously available corpora. As future work, we plan to include entity linking and maybe related queries to provide a large-scale corpus that supports research on several components of the query understanding pipeline. In fact, as a rst step, we will link the spelling corrections to our previously collected query segmentations [8].",null,null
133,"e portion of queries with alternative spellings in our new corpus is similar to the previous corpora (16.74%). However, our",null,null
134,corpus is the only one containing spelling variants with special,null,null
135,characters--providing a testbed for query spelling before or a er,null,null
136,"normalization (i.e., before or a er treating special characters).",null,null
137,"In a rst study, we have compared the spelling corrections",null,null
138,from the commercial search engines Google and Bing to a re-,null,null
139,implementation of the best performing approach from the Microso,null,null
140,Speller Challenge 2011. Our results on all corpora indicate that only,null,null
141,Google is able to substantially improve upon a simple do-nothing,null,null
142,"baseline, while the other two approaches o en perform worse. But",null,null
143,even the Google system is not able to always correct a typo and,null,null
144,for some of the queries without errors suggests di erent spellings.,null,null
145,"Hence, query spelling correction is still not a ""solved"" problem.",null,null
146,REFERENCES,null,null
147,[1] Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of HLT/EMNLP 2005.,null,null
148,"[2] Fei Cai and Maarten de Rijke. 2016. A survey of query auto completion in information retrieval. Foundations and Trends in Information Retrieval 10 (2016), 273­363.",null,null
149,"[3] Ishan Cha opadhyaya, Kannappan Sirchabesan, and Krishanu Seal. 2013. A fast generative spell corrector based on edit distance. In Proceedings of ECIR 2013, 404­410.",null,null
150,"[4] Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of EMNLP 2004, 293­300.",null,null
151,"[5] Huizhong Duan and Bo-June Paul Hsu. 2011. Online spelling correction for query completion. In Proceedings of WWW 2011, 117­126.",null,null
152,"[6] Huizhong Duan, Yanen Li, ChengXiang Zhai, and Dan Roth. 2012. A discriminative model for query spelling correction with latent structural SVM. In Proceedings of EMNLP-CoNLL 2012, 1511­1521.",null,null
153,"[7] Yasser Ganjisa ar, Andrea Zilio, Sara Javanmardi, Inci Cetindil, Manik Sikka, Sandeep Paul Katumalla, Narges Khatib-Astaneh, Chen Li, and Cristina Lopes. 2011. qSpell: Spelling correction of web search queries using ranking models and iterative correction. In Spelling Alteration for Web Search Workshop 2011.",null,null
154,"[8] Ma hias Hagen, Martin Po hast, Benno Stein, and Christof Bra¨utigam. 2011. ery segmentation revisited. In Proceedings of WWW 2011, 97­106.",null,null
155,"[9] Sasa Hasan, Carmen Heger, and Saab Mansour. 2015. Spelling correction of user search queries through statistical machine translation. In Proceedings of EMNLP 2015, 451­460.",null,null
156,"[10] Liangda Li, Hongbo Deng, Jianhui Chen, and Yi Chang. 2017. Learning parametric models for context-aware query auto-completion via Hawkes processes. In Proceedings of WSDM 2017, 131­139.",null,null
157,"[11] Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of ACL 2006.",null,null
158,"[12] Yanen Li, Huizhong Duan, and ChengXiang Zhai. 2012. CloudSpeller: ery spelling correction by using a uni ed hidden Markov model with web-scale resources. In Proceedings of WWW 2012, 561­562.",null,null
159,"[13] Yanen Li, Huizhong Duan, and ChengXiang Zhai. 2012. A generalized hidden Markov model with discriminative training for query spelling correction. In Proceedings of SIGIR 2012, 611­620.",null,null
160,[14] Gord Lueck. 2011. A data-driven approach for correcting search queries. In Spelling Alteration for Web Search Workshop 2011.,null,null
161,"[15] Peter Nalyvyko. 2011. A REST-based online English spelling checker ""Pythia"". In Spelling Alteration for Web Search Workshop 2011.",null,null
162,[16] Peter Norvig. 2007. How to write a spelling corrector. h p://norvig.com/spell-correct.html. (2007).,null,null
163,[17] Yoh Okuno. 2011. Spelling generation based on edit distance. In Spelling Alteration for Web Search Workshop 2011.,null,null
164,"[18] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A picture of search. In Proceedings of Infoscale 2006, 1.",null,null
165,"[19] Jason J. Soo. 2013. A non-learning approach to spelling correction in web queries. In Proceedings of WWW 2013, 101­102.",null,null
166,"[20] Dan Stefanescu, Radu Ion, and Tiberiu Boros. 2011. TiradeAI: An ensemble of spellcheckers. In Spelling Alteration for Web Search Workshop 2011.",null,null
167,"[21] Xu Sun, Anshumali Shrivastava, and Ping Li. 2012. Fast multi-task learning for query spelling correction. In Proceedings of CIKM 2012, 285­294.",null,null
168,"[22] Kuansan Wang and Jan Pedersen. 2011. Review of MSR-Bing web scale speller challenge. In Proceedings of SIGIR 2011, 1339­1340.",null,null
169,1264,null,null
170,,null,null

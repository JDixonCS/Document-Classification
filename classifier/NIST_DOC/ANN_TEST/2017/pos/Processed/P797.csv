,sentence,label,data
0,Short Research Paper,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Automatically Extracting High- ality Negative Examples for Answer Selection in estion Answering,null,null
3,"Haotian Zhang1, Jinfeng Rao2, Jimmy Lin1, and Mark D. Smucker3",null,null
4,"1 David R. Cheriton School of Computer Science, University of Waterloo 2 Department of Computer Science, University of Maryland",null,null
5,"3 Department of Management Sciences, University of Waterloo",null,null
6,"{haotian.zhang,jimmylin,mark.smucker}@uwaterloo.ca,jinfeng@cs.umd.edu",null,null
7,ABSTRACT,null,null
8,"We propose a heuristic called ""one answer per document"" for automatically extracting high-quality negative examples for answer selection in question answering. Starting with a collection of question­answer pairs from the popular TrecQA dataset, we identify the original documents from which the answers were drawn. Sentences from these source documents that contain query terms (aside from the answers) are selected as negative examples. Training on the original data plus these negative examples yields improvements in e ectiveness by a margin that is comparable to successive recent publications on this dataset. Our technique is completely unsupervised, which means that the gains come essentially for free. We con rm that the improvements can be directly a ributed to our heuristic, as other approaches to extracting comparable amounts of training data are not e ective. Beyond the empirical validation of this heuristic, we also share our improved TrecQA dataset with the community to support further work in answer selection.",null,null
9,1 INTRODUCTION,null,null
10,"ere are three key components to solving problems with machine learning: the training data, the model, and the optimization technique. To improve e ectiveness, data is o en the easiest path since in some applications it is easy to collect a large amount of data, such as user behavior logs in the web context. In contrast, improving models and optimization techniques o en require inspiration.",null,null
11,"In this paper, we focus on the data dimension of improving answer selection for question answering. We propose a heuristic that we call ""one answer per document"", which yields a simple technique for extracting high-quality negative examples. Starting with question­answer pairs from the popular TrecQA dataset, one of the most widely-used collections for evaluating answer selection in question answering, we identify the original documents from which the answers are drawn. e best-matching sentences from these source documents that contain query terms (other than the answer sentences) are selected as negative examples. Training on the original data plus these negative examples yields improved",null,null
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080645",null,null
13,"e ectiveness. Our intuition is that the answer to a question is only likely to occur once in a document, and thus other sentences in the document with query terms can serve as high-quality negative examples. We argue that these examples are particularly valuable because they lie near the decision boundary (by virtue of containing query terms). is intuition is con rmed by contrastive experiments that show alternative techniques for acquiring comparable amounts of training data are not e ective.",null,null
14,"e contributions of this work are two-fold: First, we propose and empirically validate the e ectiveness of the ""one answer per document"" heuristic. Our approach is completely unsupervised, which means that gains in e ectiveness come with minimal e ort. Examining the history of improvements on this task, the gain we achieve is around the same level of e ectiveness as reported in successive recent publications on this dataset, nearly all of which come from improved modeling using neural networks. Second, our technique yields an improved and augmented version of the widely-used TrecQA dataset that we share with the community to foster further work on answer selection.",null,null
15,2 BACKGROUND AND RELATED WORK,null,null
16,"Answer selection is an important component of an overall question answering system: given a question q and a candidate set of sentences {s1, s2, . . . sn }, the task is to identify sentences that contain the answer. In a standard pipeline architecture [12], answer selection is applied to the output of a module that performs passage retrieval, typically using lightweight term-based matching. Selected sentences can then be directly presented to users or serve as input to subsequent stages that identify exact answers [13].",null,null
17,"In recent years, researchers have had substantial success in tackling the answer selection problem with neural networks, e.g., [6, 7, 10, 11, 17]. e continuous representations that deep-learning approaches provide are e ective in combating data sparsity, a perpetual challenge in natural language processing tasks. Solutions based on neural networks represent an advance over previous approaches driven by feature engineering. Although our work is primarily about techniques for acquiring training data, we assume a deep-learning framework for evaluation purposes.",null,null
18,"It is a well-known fact that the amount of training data drives e ectiveness in a broad range of tasks. Colloquially referred to as the ""unreasonable e ectiveness of data"" [5], researchers have been empirically examining the impact of training data for machine learning since at least the early 2000s. e seminal work of Banko and Brill [2] in examining the e ects of training data size on natural language disambiguation tasks contained a slightly subversive message, that the e ort of researchers might be be er spent gathering",null,null
19,797,null,null
20,Short Research Paper,null,null
21,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
22,Dataset TREC8,null,null
23,TREC9 TREC10,null,null
24,TREC11 TREC12 TREC13,null,null
25,Document Collections TREC disks 4&5 minus Congressional Record,null,null
26,AP newswire (Disks 1-3) Wall Street Journal (Disks 1-2) San Jose Mercury News (Disk 3),null,null
27,Financial Times (Disk 4) Los Angeles Times (Disk 5) Foreign Broadcast Information Service (FBIS) (Disk 5),null,null
28,AQUAINT disks,null,null
29,Table 1: Source document collections for TrecQA.,null,null
30,"training data as opposed to building more sophisticated models. Speci cally in the realm of question answering, researchers have long known that, all things being equal, larger collections yield higher e ectiveness due to data redundancy [3, 4].",null,null
31,"In this context, our work focuses on gathering training data for answer selection in order to improve machine-learned models. Speci cally, our ""one answer per document"" heuristic is a nod to the ""one sense per discourse"" heuristic Yarowsky [16] applied to word-sense disambiguation, which dates back to the 1990s. is work is an early example of a clever technique for acquiring (noisy) labeled data for free, much like our work. e intuition behind the heuristic is that polysemous words occurring close together are unlikely to have di erent senses. For example, if the word ""bank"" occurs in nearby sentences, it is unlikely that one refers to a nancial institution and the other to the side of a river. is is an artifact of how authors naturally communicate when writing. From this heuristic Yarowsky described an approach to bootstrap a word-sense disambiguation algorithm. In the same way, our ""one answer per document"" heuristic re ects how authors write.",null,null
32,"ere can, of course, be violations of this heuristic, but the point is that su cient signal can be extracted with this heuristic to aid in training machine-learned models. Our technique is closely related to what researchers today would call distant supervision, but our focus is speci cally on data acquisition.",null,null
33,3 METHODS,null,null
34,"In order to operationalize our ""one answer per document"" heuristic, we build on the TrecQA dataset that is broadly used as a benchmark for answer selection. Note that although this paper focuses on a speci c dataset--since one of our contributions is a resource we share with the community--the assumptions we make about the general technique are fairly minimal: simply that answer sentences are drawn from documents within some collection.",null,null
35,"e TrecQA dataset was rst introduced by Wang et al. [14] and further elaborated by Yao et al. [15]. e dataset contains a set of factoid questions, each of which is associated with a number of candidate sentences that either contain or do not contain the answer (i.e., positive and negative examples). e questions are from the estion Answering Tracks from TREC 8­13, and the candidate answers are derived from the output of track participants, ultimately drawn from the collections listed in Table 1.",null,null
36,"e TrecQA dataset comes pre-split into train, development, and test sets, with statistics shown in Table 2. estions from TREC",null,null
37,Set #,null,null
38,Train Dev Test,null,null
39,All,null,null
40,estion,null,null
41,"1,229 84 100",null,null
42,"1,411",null,null
43,# Pos Answers,null,null
44,"6,403 222 284",null,null
45,"6,909",null,null
46,# Neg Answers,null,null
47,"47,014 926",null,null
48,"1,233",null,null
49,"49,173",null,null
50,Table 2: Statistics for various splits of TrecQA.,null,null
51,"8­12 are used for training (1229 questions), while questions from TREC 13 are used for development (84 questions) and testing (100 questions). To generate the candidate answers for the development and test splits, sentences were selected from each question's evaluation pool that contained one or more non-stopwords from the question [14]. For generating the training candidates, in addition to the sentences that contain non-stopwords from the question, sentences that match the correct answer pa erns (from an automatic evaluation script) were also added. Data from all of TREC 13 (development and test splits) and the rst 100 questions from TREC 8­12 (training split) were manually assessed. e motivation behind the manual annotation e ort is that answer pa erns in the automatic evaluation script may yield false positives--i.e., sentences that match the pa ern may not actually contain correct answers.",null,null
52,"Although the TrecQA dataset was ultimately constructed from TREC evaluations, the provenance information connecting answer candidates to their source documents does not exist. erefore, to operationalize our ""one answer per document"" heuristic, we needed to ""backproject"" each answer candidate to recover its source document. Note that due to tokenization, case folding, and other sentence processing di erences, nding the answer sentence is more complex than just an exact string match.",null,null
53,"Answer backprojection was accomplished by rst indexing all the collections in Table 1 with Anserini,1 our information retrieval toolkit built on Lucene. We then issued each question as a query and retrieved the top 1000 hits using BM25. For each answer a, we used the shingle matching method [8, 9] to select the most likely candidate document d that contains the answer a. For an answer a, let s be the minimum span of words in a candidate document d that contains the most words from a in any order. A span s matches a well if s contains many words from a within a small window. We used the algorithm presented by Krenzel [8] to nd the shortest span s of shingle words within a document in linear time:",null,null
54,Scores d,null,null
55,",",null,null
56,max,null,null
57,|s  a|2 |s | · |a|,null,null
58,(1),null,null
59,"A er we nd the best matching document d for an answer a, we split the sentences in d using the NLTK Punkt sentence tokenizer.2 Equation (1) is used again to score all sentences in d; we take as the matching answer the highest scoring sentence above a threshold of 0.1. If no sentence scores above this threshold, we drop the answer from consideration. Based on spot-checking, this se ing is able to nd the source sentence with nearly perfect precision. Once we have found the source sentence, all other non-zero scoring sentences in the document provide negative examples for the answer selection task, based on our ""one answer per document"" heuristic.",null,null
60,1h p://anserini.io/ 2h p://www.nltk.org/api/nltk.tokenize.html,null,null
61,798,null,null
62,Short Research Paper,null,null
63,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
64,estion Answer,null,null
65,Ranking Score Matching 0.681,null,null
66,1,null,null
67,0.244,null,null
68,2,null,null
69,0.208,null,null
70,3,null,null
71,0.203,null,null
72,4,null,null
73,0.195,null,null
74,5,null,null
75,0.133,null,null
76,6,null,null
77,0.130,null,null
78,7,null,null
79,0.125,null,null
80,"Who is the author of the book , "" e Iron Lady : A Biography of Margaret atcher "" ? the iron lady ; a biography of margaret thatcher by hugo young -lrb- farrar , straus & giroux -rrb-",null,null
81,Sentence,null,null
82,"THE IRON LADY: A BIOGRAPHY OF MARGARET THATCHER BY HUGO YOUNG (FARRAR, STRAUS &amp; GIROUX: $25; 570 PP. In "" e Iron Lady,"" Young traces the winding staircase of fortune that transformed the younger daughter of a provincial English grocer into the greatest woman political leader since Catherine the Great. It is without question the best of a bevy of new",null,null
83,"atcher biographies that set out the o en surprising, always dramatic story of the British political revolution of the 1980s. In this same revisionist mold, Hugo Young, the distinguished British journalist, has performed a brilliant dissection of the notion of atcher as a conservative icon.",null,null
84,"e implied paradox has been nicely captured by a recent British assessment of the last six years titled "" e Free Economy and the Strong State: e Politics of atcherism"" by Andrew Gamble. It sees atcher as the new Me ernich (the 19thCentury master of the diplomatic nesse), as a power-driven politician and as a militant Puritan. Young observes that "" ere was a genuine clash of cultures, between an almost Cromwellian impatience with the status quo (on the part of the",null,null
85,"atcherites) and the mandarin world of Whitehall, in which skepticism and rumination were more highly rated habits of mind than zeal or blind conviction.""",null,null
86,"e only company nominated by atcher's team for denationalization was the National Freight Corp., and this from the people who much later made ""privatization"" one of the household words of the age.",null,null
87,Table 3: Example backprojection of an answer to recover the source document (LA111289-0002) and the source sentence. Non-matching sentences serve as negative examples.,null,null
88,"A complete example of this backprojection process is shown in Table 3. At the top we show the question and the answer we are trying to nd. First, we identi ed document LA111289-0002 as the source document. In this document, in addition to the topscoring sentence (a correct match), we also show the non-matching sentences in decreasing score order. is example illustrates why",null,null
89,"nding the source answer requires more than an exact string match. e non-matching sentences show the intuition behind our ""one",null,null
90,"answer per document"" heuristic--indeed, none of the sentences answer the question. Note that although sentence 3 contains the author ""Hugo Young"", it doesn't provide any contextual justi cation, i.e., there is no way for the reader to infer the answer in isolation. Accordingly, it should be considered a negative example.",null,null
91,4 EVALUATION AND RESULTS,null,null
92,We applied the procedure described above to backproject answer sentences from the TrecQA dataset to reconstruct their sources.,null,null
93,"Operationalizing the ""one answer per document"" heuristic, nonmatching sentences from the source document containing the answer serve as negative examples we can use to augment the training data. We considered cases where m  {1, 3, 5, 7} of these top non-matching sentences are added to the training set as negative examples (see Table 3). We tokenized these sentences using the standard Penn Treebank format3 to match the original dataset.",null,null
94,"How e ective is the ""one answer per document"" heuristic? To nd out, we trained an answer selection model using our augmented training data and compared the results with training on the original data. For this task, we used the convolutional neural network model of Severyn and Moschi i [11] (SM for short). eir model achieves competitive accuracy and the authors provide an open-source implementation.4 Following previous work, we evaluated the task in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). In order to train the model more e ciently, for negative sentences selected by our technique, we truncated their lengths to 60 tokens. All parameters and the training procedure remained the same as in the original model. We emphasize that in all our experiments, the only di erence is an augmented training set: the development set and test set remain exactly the same, thus supporting a fair comparison of results. Any di erence in e ectiveness can be directly a ributed to the training data.",null,null
95,"Results of this experiment are shown in Table 4. e row labeled ""Baseline SM Model"" is the result of our replication using the implementation provided by Severyn and Moschi i and in fact we obtain slightly higher e ectiveness than what they reported. e next four rows in the table show the e ects of adding di erent numbers of negative examples per each backprojected answer. For example, with m ,"" 5, we would add up to 6,403 × 5 "","" 32,015 negative examples. We see that the m "","" 1 condition reduces e ectiveness slightly, likely due to the noise introduced. Adding more sentences helps, peaking at m "","" 5, and then e ectiveness drops again.""",null,null
96,"e intuition behind our ""one answer per document"" heuristic is that our data acquisition algorithm yields high-quality negative examples that are valuable because they lie near the decision boundary. Experimental results support this claim, but to further validate our heuristic, there are two alternative explanations to rule out: First, that these sentences might be even more useful as positive examples, and second, that the gains aren't derived from simply having more training data.",null,null
97,"To explore the rst alternative explanation, we repeated the same experiment as above, augmenting the training set with m  {1, 3, 5, 7} of the top non-matching sentences, but as positive examples. Results are also shown in Table 4. We clearly see that such a treatment hurts e ectiveness for all examined values of m. is",null,null
98,"nding is consistent with the assumption that the answer will only appear once in each document, thus supporting our heuristic.",null,null
99,"To explore the second alternative explanation, we experimented with two di erent approaches to augmenting the training set: in the rst case, we selected ve random sentences from the answer document to serve as negative examples (and thus, they may or may not contain terms from the question), and in the second case, we randomly selected ve sentences from all documents to serve as",null,null
100,3h p://www.nltk.org/ modules/nltk/tokenize/treebank.html 4h ps://github.com/aseveryn/deep-qa,null,null
101,799,null,null
102,Short Research Paper,null,null
103,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
104,Strategy,null,null
105,Baseline SM Model,null,null
106,Add top 1 sent as neg Add top 3 sent as neg Add top 5 sent as neg Add top 7 sent as neg,null,null
107,Add top 1 sent as pos Add top 3 sent as pos Add top 5 sent as pos Add top 7 sent as pos,null,null
108,Add random 5 neg sent from correct documents Add random 5 neg sent,null,null
109,from all documents,null,null
110,MAP,null,null
111,0.7538,null,null
112,0.7468 0.7588 0.7612 0.7493,null,null
113,0.7409 0.7193 0.7117 0.7016,null,null
114,"0.7548 [0.7499, 0.7597]",null,null
115,"0.7526 [0.7496, 0.7556]",null,null
116,MRR,null,null
117,0.8078,null,null
118,0.7953 0.8012 0.8088 0.7993,null,null
119,0.7974 0.7670 0.7456 0.7639,null,null
120,"0.8075 [0.8038, 0.8112]",null,null
121,"0.7969 [0.7883, 0.8054]",null,null
122,Multi-Perspective CNN Add top 3 sent as neg Add top 5 sent as neg,null,null
123,0.762 0.7864 0.7788,null,null
124,0.830 0.8325 0.8316,null,null
125,Table 4: Results of comparing di erent strategies.,null,null
126,"negative examples. We conducted ve trials of each experimental condition so that we can compute the mean and 95% con dence intervals for both MAP and MRR. ese results are also shown in Table 4. We see that both sampling approaches have minimal e ect on e ectiveness (to be expected). Note that in these cases the neural network is trained with the same amount of data as in the negative sampling case. Combined with the above experiments, these results con rm that e ectiveness gains do not come from simply having more data, but having high-quality negative examples, thus supporting our ""one answer per document"" heuristic.",null,null
127,"Let us tackle the next possible criticism: that we are improving on a low baseline. As a point of reference, we can consult an ACL wiki page that nicely summarizes the state of the art in this answer selection task [1]. We clearly see that while the SM model isn't the top-performing model on this task, its e ectiveness remains competitive. To show the robustness of the e ectiveness gains that we observe, we also experimented with the multi-perspective convolutional neural network (MPCNN) architecture of He et al. [6], which also has open-source code available.5 Since this model is more complex than the SM model and hence takes longer to train, we only repeated the condition of adding m  {3, 5} negative examples. Results show gains in both conditions, with m , 3 appearing to be the be er se ing.",null,null
128,"Finally, let us try to contextualize the magnitude of gains that derive from our technique. e ACL wiki page [1] provides the history of e ectiveness improvements over time. Of course, in the beginning right a er this dataset was published, researchers made great strides in improving e ectiveness. However, the magnitude of advances has dramatically shrunk: in recent years, publications are reporting small gains in the second decimal point. All of these improvements are from increasingly-sophisticated neural network models. e magnitude of our observed improvements is comparable to di erences in successive recent publications on this particular dataset: for example, the improvement from He et al. [6] (published",null,null
129,5h ps://github.com/castorini/MP-CNN-Torch,null,null
130,"in 2015) to Rao et al. [10] (published in 2016) is less than 0.02 in terms of absolute MAP. e magnitude of our gains is at least as large, and in fact, our best condition appears to be the highest reported result on TrecQA (as of this writing). Since our technique is completely unsupervised, these gains basically come for free.",null,null
131,"As a resource for the community, we release all data from this paper, including the source document mappings and the negative examples to augment the original TrecQA dataset.6",null,null
132,5 CONCLUSIONS,null,null
133,"Data, model, and optimization represent three di erent approaches to increasing the e ectiveness of machine learning solutions. is paper adopts the data approach to tackling answer selection: We begin with an intuition, the ""one answer per document"" heuristic, that we then operationalize into a data acquisition algorithm. Augmented training data improves the e ectiveness of existing models, and contrastive experiments rule out alternative explanations for our ndings, thus validating our approach. As applied to a speci c dataset, the widely-used TrecQA benchmark, our work yields an improved data resource that we share with the community. However, we believe that this heuristic is equally applicable to other tasks and datasets, a future direction that we are currently pursuing.",null,null
134,REFERENCES,null,null
135,"[1] ACL. 2017. estion Answering (State of the art). h p://www.aclweb.org/ aclwiki/index.php?title, estion Answering (State of the art). (2017). Accessed: 2017-05-01.",null,null
136,[2] Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation. In ACL. 26­33.,null,null
137,"[3] Charles L. A. Clarke, Gordon Cormack, and omas Lynam. 2001. Exploiting Redundancy in estion Answering. In SIGIR. 375­383.",null,null
138,"[4] Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, and Andrew Ng. 2002. Web estion Answering: Is More Always Be er? In SIGIR. 291­298.",null,null
139,"[5] Alon Halevy, Peter Norvig, and Fernando Pereira. 2009. e Unreasonable E ectiveness of Data. IEEE Intelligent Systems 24, 2 (2009), 8­12.",null,null
140,"[6] Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks. In EMNLP. 1576­1586.",null,null
141,[7] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Neural Networks for Semantic Similarity Measurement. In NAACL-HLT. 937­948.,null,null
142,[8] Steve Krenzel. 2010. Finding blurbs. h p://www.stevekrenzel.com/articles/ blurbs.,null,null
143,"[9] Virgil Pavlu, Shahzad Rajput, Peter B. Golbus, and Javed A. Aslam. 2012. IR System Evaluation using Nugget-based Test Collections. In WSDM. 393­402.",null,null
144,"[10] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In CIKM. 1913­1916.",null,null
145,[11] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. 373­382.,null,null
146,"[12] Stefanie Tellex, Boris Katz, Jimmy Lin, Gregory Marton, and Aaron Fernandes. 2003. antitative Evaluation of Passage Retrieval Algorithms for estion Answering. In SIGIR. 41­47.",null,null
147,[13] Ellen M. Voorhees. 2002. Overview of the TREC 2002 estion Answering Track. In TREC.,null,null
148,"[14] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32.",null,null
149,"[15] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In HLT-NAACL. 858­867.",null,null
150,[16] David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In ACL. 189­196.,null,null
151,"[17] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. In NIPS Deep Learning Workshop.",null,null
152,"Acknowledgments. is research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada and by a Google Founders Grant, with additional contributions from the U.S. National Science Foundation under CNS-1405688.",null,null
153,6h ps://github.com/castorini/TrecQA-NegEx,null,null
154,800,null,null
155,,null,null

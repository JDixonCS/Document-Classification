,sentence,label,data
0,Session 7B: Entities,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Word-Entity Duet Representations for Document Ranking,null,null
3,Chenyan Xiong,null,null
4,"Language Technologies Institute Carnegie Mellon University Pi sburgh, PA 15213, USA cx@cs.cmu.edu",null,null
5,Jamie Callan,null,null
6,"Language Technologies Institute Carnegie Mellon University Pi sburgh, PA 15213, USA callan@cs.cmu.edu",null,null
7,Tie-Yan Liu,null,null
8,"Microso Research Beijing 100080, P.R. China tie-yan.liu@microso .com",null,null
9,ABSTRACT,null,null
10,"is paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entitybased representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the a ention mechanism successfully steers the model away from noisy entities, and together they signi cantly outperform both word-based and entity-based learning to rank systems.",null,null
11,KEYWORDS,null,null
12,"Word-Entity Duet, Entity-based Search, Explicit Semantics, Text Representation, Document Ranking.",null,null
13,1 INTRODUCTION,null,null
14,"Utilizing knowledge bases in text-centric search is a recent breakthrough in information retrieval [5]. e rapid growth of information extraction techniques and community e orts have generated large scale general domain knowledge bases, such as DBpedia and Freebase. ese knowledge bases store rich semantics in semistructured formats and have great potential in improving text understanding and search accuracy.",null,null
15,"ere are many possible ways to utilize knowledge bases' semantics in di erent components of a search system. ery representation can be improved by introducing related entities and their texts to expand the query [4, 20]. Document representation can be enriched by adding the annotated entities into the document's vector space model [17, 21, 23]. e ranking model can also be improved by utilizing the entities and their a ributes to build additional connections between query and documents [14, 19]. e",null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080768",null,null
17,rich choices of available information and techniques raise a new challenge of how to use all of them together and fully explore the potential of knowledge graphs in search engines.,null,null
18,"is work proposes a new framework for utilizing knowledge bases in information retrieval. Instead of centering around words and using the knowledge graph as an additional resource, this work treats entities equally with words, and represents the query and documents using both word-based and entity-based representations.",null,null
19,"us the interaction of query and document is no longer a `solo' of their words, but a `duet' of their words and entities. Working together, the word-based and entity-based representations form a four-way interaction: query words to document words (Qw-Dw), query entities to document words (Qe-Dw), query words to document entities (Qw-De), and query entities to document entities (Qe-De). is leads to a general methodology for incorporating knowledge graphs into text-centric search systems.",null,null
20,"e rich and novel ranking evidence from the word-entity duet does come with a cost. Because it is created automatically, the entity-based representation also introduces uncertainties. For example, an entity can be mistakenly annotated to a query, and may mislead the search system. is paper develops an a ention-based ranking model, AttR-Duet, that employs a simple a ention mechanism to handle the noise in the entity representation. e matching component of AttR-Duet focuses on ranking with the word-entity duet, while its a ention component focuses on steering the model away from noisy entities. Trained directly from relevance judgments, AttR-Duet learns how to demote noisy entities and how to rank documents with the word-entity duet simultaneously.",null,null
21,"e e ectiveness of AttR-Duet is demonstrated on ClueWeb Category B corpora and TREC Web Track queries. On both ClueWeb09B and ClueWeb12-B13 , AttR-Duet outperforms previous wordbased and entity-based ranking systems by at least 14%. We demonstrate that the entities bring additional exact match and so match ranking signals from the knowledge graph; all entity-based rankings perform similar or be er compared to solely word-based rankings. We also nd that, when the automatically-constructed entity representations are not as clean, the a ention mechanism is necessary for the ranking model to utilize the ranking signals from the knowledge graph. Jointly learned, the a ention mechanism is able to demote noisy entities and distill the ranking signals, while without such puri cation, ranking models become vulnerable to noisy entity representations, and the mixed evidence from the knowledge graph may be more of a distraction than an asset.",null,null
22,"In the rest of this paper, Section 2 discusses related work; Section 3 presents the word-entity duet framework for utilizing knowledge graphs in ranking; Section 4 is about the a ention-based ranking model; Experimental se ings and evaluations are described in Section 5 and 6; e conclusions and future work are in Section 7.",null,null
23,763,null,null
24,Session 7B: Entities,null,null
25,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
26,2 RELATED WORK,null,null
27,"ere is a long history of research on utilizing semantic resources to improve information retrieval. Controlled vocabulary based information retrieval uses a set of expert-created index terms (mostly organized in ontologies) to represent query and documents. e retrieval is then performed by query and document's overlaps in the controlled vocabulary space, and the human knowledge in the controlled vocabulary is included. Controlled vocabulary is almost a necessity in some special domains. For example, in medical search where queries are o en about diseases, treatments, and genes, the search intent may not be covered by the query words, so external information about medical terms is needed. esauruses and lexical resources such as WordNet have also been used to address this issue. Synonyms and related concepts stored in these resources can be added to queries and documents to reduce language varieties and may improve the recall of search results [12].",null,null
28,"Recently, large general domain knowledge bases, such as DBpedia [11] and Freebase [1], have emerged. Knowledge bases contain human knowledge about real-world entities, such as descriptions, a ributes, types, and relationships, usually in the form of knowledge graphs. ey share the same spirit with controlled vocabularies but are usually created by community e orts or information extraction systems, thus are o en at a larger scale. ese knowledge bases provide a new opportunity for search engines to be er `understand' queries and documents. Many new techniques have been developed to explore their potential in various components of ad-hoc retrieval.",null,null
29,"An intuitive way is to use the texts associated with related entities to form be er query representations. Wikipedia contains well-wri en entity descriptions and can be used as an external and cleaner pseudo relevance feedback corpus to obtain be er expansion terms [24]. e descriptions of related Freebase entities have been utilized to provide be er expansion terms; the related entities are retrieved by entity search [3], or selected from top retrieved documents' annotations [8]. e text elds of related entities can also be used to provide expanded learning to rank features: Entity",null,null
30,"ery Feature Expansion (EQFE) expands the query using the texts from related entities' a ributes, and these expanded texts generate rich ranking features [4].",null,null
31,"Knowledge bases also provide additional connections between query and documents through related entities. Latent Entity Space (LES) builds an unsupervised retrieval model that ranks documents based on their textual similarities to latent entities' descriptions [14]. EsdRank models the connections between query to entities, and entities to documents using various information retrieval features.",null,null
32,"ese connections are utilized by a latent space learning to rank model, which signi cantly improved state-of-the-art learning to rank methods [19].",null,null
33,"A recent progress is to build entity-based representations for texts. Bag-of-entities representations built from entity annotations have been used in unsupervised retrieval models, including vector space models [21] and language models [17]. e entity-based so match was studied by Semantics-Enabled Language Model (SELM); it connects query and documents in the entity space using their entities' relatedness calculated from an entity linking system [6].",null,null
34,ese unsupervised entity-based retrieval models perform be er,null,null
35,"or can be e ectively combined with word-based retrieval models. Recently, Explicit Semantic Ranking (ESR) performs learning to rank with query and documents' entity representations in scholar search [23]. ESR rst trains entity embeddings using a knowledge graph, and then converts the distances in the embedding space to exact match and so match ranking features, which signi cantly improved the ranking accuracy of semanticscholar.org.",null,null
36,3 WORD-ENTITY DUET FRAMEWORK,null,null
37,"is section presents our word-entity duet framework for utilizing knowledge bases in search. Given a query q and a set of candidate documents D ,"" {d1, ..., d |D | }, our framework aims to provide a systematic approach to be er rank D for q, with the help of a knowledge graph (knowledge base) G. In the framework, query and documents are represented by two representations, one wordbased and one entity-based (Section 3.1). e two representations' interactions create the word-entity duet and provide four matching components (Section 3.2).""",null,null
38,3.1 Word and Entity Based Representations,null,null
39,"Word-based representations of query and document are standard bag-of-words: Qw(w) ,"" tf(w, q), and Dw(w) "","" tf(w, d). Each dimension in the bag-of-words Qw and Dw corresponds to a word w. Its weight is the word's frequency (tf) in the query or document.""",null,null
40,"A standard approach is to use multiple elds of a document, for example, title and body. Each document eld is usually represented by a separate bag-of-words, for example, Dwtitle and Dwbody, and the ranking scores from di erent elds are combined by ranking models. In this work, we assume that a document may have multiple",null,null
41,"elds. However, to make notation simpler, the eld notation is omi ed in the rest of this paper unless necessary.",null,null
42,"Entity-based representations are bag-of-entities constructed from entity annotations [21]: Qe(e) ,"" tf(e, q) and De(e) "","" tf(e, d), where e is an entity linked to the query or the document. We use automatic entity annotations from an entity linking system to construct the bag-of-entities [21].""",null,null
43,"An entity linking system nds the entity mentions (surface forms) in a text, and links each surface form to a corresponding entity. For example, the entity `Barack Obama' can be linked to the query `Obama Family Tree'. `Obama' is the surface form.",null,null
44,Entity linking systems usually contain two main steps [7]:,null,null
45,"(1) Spo ing: To nd surface forms in the text, for example, to identify the phrase `Obama'.",null,null
46,"(2) Disambiguation: To link the most probable entity from the candidates of each surface form, for example, choosing `Barack Obama' from all possible Obama-related entities.",null,null
47,"A commonly used information in spo ing is the linked probability (lp), the probability of a surface form being annotated in a training corpus, such as Wikipedia. A higher lp means the surface form is more likely to be linked. For example, `Obama' should have a higher lp than `table'. e disambiguation usually considers two factors.",null,null
48,"e rst is commonness (CMNS), the universal probability of the surface form being linked to the entity. e second is the context in the text, which provides additional evidence for disambiguation. A con dence score is usually assigned to each annotated entity by",null,null
49,764,null,null
50,Session 7B: Entities,null,null
51,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
52,Table 1: Ranking features from query words to document words (title and body) (Qw-Dw).,null,null
53,Feature Description BM25 TF-IDF Boolean OR Boolean And Coordinate Match Language Model (Lm) Lm with JM smoothing Lm with Dirichlet smoothing Lm with two-way smoothing Total,null,null
54,Dimension 2 2 2 2 2 2 2 2 2 18,null,null
55,Table 2: Ranking features from query entities (name and description) to document words (title and body) (Qe-Dw).,null,null
56,Feature Description BM25 TF-IDF Boolean Or Boolean And Coordinate Match Lm with Dirichlet Smoothing Total,null,null
57,Dimension 4 4 4 4 4 4 24,null,null
58,"the entity linking system, based on spo ing and disambiguation scores.",null,null
59,"e bag-of-entities is not the set of surface forms that appear in the text (otherwise it is not much di erent from phrase-based representation). Instead, the entities are associated with rich semantics from the knowledge graph. For example, in Freebase, the information associated with each entity includes (but is not limited to) its name, alias, type, description, and relations with other entities. e entity-based representation makes these semantics available when matching query and documents.",null,null
60,3.2 Matching with the Word-Entity Duet,null,null
61,"By adding the entity based representation into the search system, the ranking is no longer a solo match between words, but a wordentity duet that includes four di erent ways a query can interact with a document: query words to document words (Qw-Dw); query entities to document words (Qe-Dw); query words to document entities (Qw-De); and query entities to document entities (Qe-De). Each of them is a matching component and generates unique ranking features to be used in our ranking model.",null,null
62,"ery Words to Document Words (Qw-Dw): is interaction has been widely studied in information retrieval. e matches of Qw and Dw generate term-level statistics such as term frequency and inverse document frequency. ese statistics are combined in various ways by standard retrieval models, for example, BM25, language model (Lm), and vector space model. is work applies these standard retrieval models on document title and body elds to extract the ranking features Qw-Dw in Table 1.",null,null
63,Table 3: Ranking features from query words to document entities (name and description) (Qw-De).,null,null
64,Feature Description Top 3 Coordinate Match on Title Entities Top 5 Coordinate Match on Body Entities Top 3 TF-IDF on Title Entities Top 5 TF-IDF on Body Entities Top 3 Lm-Dirichlet on Title Entities Top 5 Lm-Dirichlet on Body Entities Total,null,null
65,Dimension 6 10 6 10 6 10 48,null,null
66,Table 4: Ranking features from query entities to document's title and body entities (Qe-De).,null,null
67,Feature Description,null,null
68,Dimension,null,null
69,"Binned translation scores, 1 exact match bin, 5 so match Bins in the range [0, 1).",null,null
70,12,null,null
71,"ery Entities to Document Words (Qe-Dw): Knowledge bases contain textual a ributes about entities, such as names and descriptions. ese textual a ributes make it possible to build cross-space interactions between query entities and document words. Specifically, given a query entity e, we use its name and description as pseudo queries, and calculate their retrieval scores on a document's title and body bag-of-words, using standard retrieval models. e retrieval scores from query entities (name or description) to document's words (title or body) are used as ranking features Qe-Dw.",null,null
72,"e detailed feature list is in Table 2. ery Words to Document Entities (Qw-De): Intuitively, the",null,null
73,"texts from document entities should help the understanding of the document. For example, when reading a Wikipedia article, the description of a linked entity in the article is helpful for a reader who does not have the background knowledge about the entity.",null,null
74,"e retrieval scores from the query words to document entities' name and descriptions are used to model this interaction. Di erent from Qe-Dw, in Qw-De, not all document entities are related to the query. To exclude unnecessary information, only the highest retrieval scores from each retrieval model are included as features:",null,null
75,"Qw-De  max-k({score(q, e)|e  De}).",null,null
76,"score(q, e) is the score of q and document entity e from a retrieval model. max-k() takes the k biggest scores from the set. Applying retrieval models on title and body entities' names and descriptions, the ranking features Qw-De in Table 3 are extracted. We choose a smaller k for title entities as titles are short and rarely have more than three entities.",null,null
77,"ery Entities to Document Entities (Qe-De): ere are two ways the interactions in the entity space can be useful. e exact match signal addresses the vocabulary mismatch of surface forms [17, 21]. For example, two di erent surface forms, `Obama' and `US President', are linked to the same entity `Barack Obama' and thus are matched. e so match in the entity space is also useful. For example, a document that frequently mentions `the white house' and `executive order' may be relevant to the query `US President'.",null,null
78,765,null,null
79,Session 7B: Entities,null,null
80,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
81,"We choose a recent technique, Explicit Semantic Ranking (ESR), to model the exact match and so match in the entity space [23]. ESR rst calculates an entity translation matrix of the query and document using entity embeddings. en it gathers ranking features from the matrix by histogram pooling. ESR was originally applied in scholar search and its entity embeddings were trained using domain speci c information like author and venue.",null,null
82,"In the general domain, there is much research that aims to learn entity embeddings from the knowledge graph [2, 13]. We choose the popular TransE model which is e ective and e cient to be applied on large knowledge graphs [2].",null,null
83,"Given the triples (edges) from the knowledge graph (eh, p, et ), including eh and et the head entity and the tail entity, and p the edge type (predicate), TransE learns entity and relationship embeddings (eì and pì) by optimizing the following pairwise loss:",null,null
84,"[1 + ||eìh + pì - eìt ||1 - ||eìh + pì - eìt ||1]+,",null,null
85,"(eh,p,et )G (eh,p,et )G",null,null
86,"where [·]+ is the hinge loss, G is the set of existing edges in the knowledge graph, and G is the randomly sampled negative instances. e loss function ensures that entities in similar graph structures are mapped closely in the embedding space, using the compositional assumption along the edge: eìh + pì , eìt .",null,null
87,"e distance between two entity embeddings describes their similarity in the knowledge graph [2]. Using L1 similarity, a translation matrix can be calculated:",null,null
88,"T (ei , ej ) , 1 - ||eìi - eìj ||1.",null,null
89,(1),null,null
90,T is the |Qe| × |De| translation matrix. ei and ej are the entities in the query and the document respectively.,null,null
91,"en the histogram pooling technique is used to gather querydocument matching signals from T [9, 23]:",null,null
92,"Sì(De) ,"" max T (e, De)""",null,null
93,e Qe,null,null
94,"Bk (Sì(De)) , log I (stk  Sìj (De) < edk ).",null,null
95,j,null,null
96,"Sì(d) is the max-pooled |De| dimensional vector, whose jth dimension is the maximum similarity of the jth document entity to any query entities. Bk () is the kth bin that counts the number of translation scores in its range [stk , edk ).",null,null
97,"We use the same six bins as in the ESR paper: [1, 1], [0.8, 1),",null,null
98,"[0.6, 0.8), [0.4, 0.6), [0, 2, 0.4), [0, 0, 2). e rst bin is the exact",null,null
99,match bin and is equivalent to the entity frequency model [21].,null,null
100,e other bin scores capture the so match signal between query,null,null
101,and documents at di erent levels. ese bin scores generated the,null,null
102,ranking features Qe-De in Table 4.,null,null
103,3.3 Summary,null,null
104,"e word-entity duet incorporates various semantics from the knowledge graph: e textual a ributes of entities are used to model the cross-space interactions (Qe-Dw and Qw-De); the relations in the knowledge graphs are used to model the interactions in the entity space (Qe-De), through the knowledge graph embedding.",null,null
105,e word-based retrieval models are also included (Qw-Dw).,null,null
106,Table 5: Attention features for query entities.,null,null
107,Feature Description Entropy of the Surface Form Is the Most Popular Candidate Entity Margin to the Next Candidate Entity Embedding Similarity with ery Total,null,null
108,Dimension 1 1 1 1 4,null,null
109,"Many prior methods are included in the duet framework. For example, the query expansion methods using Wikipedia or Freebase represent the query using related entities, and then use these entities' texts to build additional connections with the document's text [4, 20, 24]; the latent entity space techniques rst nd a set of highly related query entities, and then rank documents using their connections with these entities [14, 19]; the entity based ranking methods model the interactions between query and documents in the entity space using exact match [17, 21] and so match [23].",null,null
110,"Each of the four interactions generates a set of ranking signals. A straightforward way is to use them as features in learning to rank models. However, the entity representations may include noise and generate misleading ranking signals, which motivates our AttR-Duet ranking model in the next section.",null,null
111,4 ATTENTION BASED RANKING MODEL,null,null
112,"Unlike bag-of-words, entity-based representations are constructed using automatic entity linking systems. It is inevitable that some entities are mistakenly annotated, especially in short queries where there is less context for disambiguation. If an unrelated entity is annotated to the query, it will introduce misleading ranking features; documents that match the unrelated entity might be promoted. Without additional information, ranking models have li le leverage to distinguish the useful signals brought in by correct entities from those by the noisy ones, and their accuracies might be limited.",null,null
113,"We address this problem with an a ention based ranking model AttR-Duet. It rst extracts a ention features to describe the quality of query entities. en AttR-Duet builds a simple a ention mechanism using these features to demote noisy entities. e a ention and the matching of query-documents are trained together using back-propagation, enabling the model to learn simultaneously how to weight entities of varying quality and how to rank with the wordentity duet. e a ention features are described in Section 4.1. e details of the ranking model are discussed in Section 4.2.",null,null
114,4.1 Attention Features,null,null
115,Two groups of a ention features are extracted for each query entity to model its annotation ambiguity and its closeness to the query.,null,null
116,"Annotation Ambiguity features describe the risk of an entity annotation. ere is a risk that the linker may fail to disambiguate the surface form to the correct entity, especially when the surface form is too ambiguous. For example, `Apple' in a short query can be the fruit or the brand. It is risky to put high a ention on it. ere are three ambiguity features used in AttR-Duet.",null,null
117,"e rst feature is the entropy of the surface form. Given a training corpus, for example, Wikipedia, we gather the probability of a surface form being linked to di erent candidate entities, and",null,null
118,766,null,null
119,Session 7B: Entities,null,null
120,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
121,Qw-Dw Qw-De 1D-CNN,null,null
122,"AttR-Duet: ,",null,null
123,Obama Family,null,null
124,Tree Qe-Dw Qe-De 1D-CNN,null,null
125,Concatenation,null,null
126,`Barack Obama',null,null
127,`Family Tree',null,null
128,Matching Feature Matching Model,null,null
129,Dot Product,null,null
130,1D-CNN,null,null
131,Concatenation 1D-CNN,null,null
132,Obama Family Tree,null,null
133,`Barack Obama' `Family Tree',null,null
134,Attention Model Attention Feature,null,null
135,Figure 1: e Architecture of the Attention based Ranking Model for Word-Entity Duet (AttR-Duet). e le side models the query-document matching in the word-entity duet. e right side models the importances of query entities using attention features. ey together produce the nal ranking score.,null,null
136,"calculate the entropy of this probability. e higher the entropy is, the more ambiguous the surface form is, and the less a ention the model should put on the corresponding query entity. e second feature is whether the annotated entity is the most popular candidate of the surface form, i.e. has the highest commonness score (CMNS). e third feature is the di erence between the linked entity's CMNS to the next candidate entity's.",null,null
137,"A closeness a ention feature is extracted using the distance between the query entity and the query words in an embedding space. An entity and word joint embedding model are trained on a corpus including the original documents and the documents with surface forms replaced by linked entities. e cosine similarity between the entity embedding to the query embedding (the average of its words' embeddings) is used as the feature. Intuitively, a higher similarity score should lead to more a ention.",null,null
138,"e full list of entity a ention features, A (e), is listed in Table 5.",null,null
139,4.2 Model,null,null
140,"e architecture of AttR-Duet is illustrated in Figure 1. It produces a ranking function f (q, d) that re-ranks candidate documents D for the query q, with the ranking features in Table 1-4 and a ention features in Table 5. f (q, d) is expected to weight query elements more properly and rank document more accurately.",null,null
141,"Model inputs: Suppose the query contains words {w1, ..., wn } and entities {e1, ... , em }, there are four input feature matrices: Rw , Re , Aw , and Ae . Rw and Re are the ranking feature matrices for query words and entities in the document. Aw and Ae are the a ention feature matrices for words and entities. ese matrices' rows are feature vectors previously described:",null,null
142,"Rw (wi , ·) , Qw-Dw(wi ) Qw-De(wi )",null,null
143,(2),null,null
144,"Re (ej , ·) , Qe-Dw(ej ) Qe-De(ej )",null,null
145,(3),null,null
146,"Aw (wi , ·) , 1",null,null
147,(4),null,null
148,"Ae (ej , ·) , A (ej ).",null,null
149,(5),null,null
150,"Qw-Dw, Qw-De, Qe-Dw, and Qe-De are the ranking features from the word-entity duet, as described in Section 3. concatenates the two",null,null
151,"feature vectors of a query element. A (ej ) is the a ention features for entity ej (Table 5). In this work, we use uniform word a ention (Aw ,"" 1), because the main goal of the a ention mechanism is to handle the uncertainty in the entity representations.""",null,null
152,"e matching part contains two Convolutional Neural Networks (CNN's). One matches query words to d (Rw ); the other one matches query entities to d (Re ). e convolution is applied on the query element (word/entity) dimension, assuming that the ranking evidence from di erent query words or entities should be treated the same. e simplest setup with one 1d CNN layer, 1 lter, and linear activation function can be considered as a linear model applied `convolutionally' on each word or entity:",null,null
153,"Fw (wi ) ,"" Wwm · Rw (wi , ·) + bwm""",null,null
154,(6),null,null
155,"Fe (ej ) ,"" Wem · Re (ej , ·) + bem .""",null,null
156,(7),null,null
157,"Fw (wi ) and Fe (ej ) are the matching scores from query word wi and query entity ej , respectively. e matching scores from all query words form an n dimensional vector Fw , and those from entities form an m dimensional vector Fe . Wwm,Wem, bwm , and bem are the matching parameters to learn.",null,null
158,e attention part also contains two CNN's. One weights query,null,null
159,words with Aw and the other one weights query entities with Ae . e same convolution idea is applied as the a ention features on,null,null
160,each query word/entity should be treated the same.,null,null
161,e simplest set-ups with one CNN layer are:,null,null
162,"w (wi ) ,"" ReLU(Wwa · Aw (wi , :) + bwa )""",null,null
163,(8),null,null
164,"e (ej ) ,"" ReLU(Wea · Ae (ej , :) + bea ).""",null,null
165,(9),null,null
166,"w (wi ) and e (ej ) are the a ention weights on word wi and entity ej . {Wwa ,Wea, bwa , bea } are the a ention parameters to learn. ReLU activation is used to ensure non-negative a ention weights.",null,null
167,e matching scores can be negative because only the di erences between documents' matching scores ma er.,null,null
168,e nal ranking score combines the matching scores using the a ention scores:,null,null
169,"f (q, d) , Fw · w + Fe · e .",null,null
170,(10),null,null
171,e training is done by optimizing the pairwise hinge loss:,null,null
172,"l(q, D) ,",null,null
173,"[1 - f (q, d) + f (q, d )]+.",null,null
174,(11),null,null
175,d D+ d D-,null,null
176,D+ and D- are the set of relevant documents and the set of irrel-,null,null
177,"evant documents. [·]+ is the hinge loss. e loss function can be optimized using back-propagation in the neural network, and the",null,null
178,matching part and the a ention part are learned simultaneously.,null,null
179,767,null,null
180,Session 7B: Entities,null,null
181,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
182,5 EXPERIMENTAL METHODOLOGY,null,null
183,"is section describes the experiment methodology, including dataset, baselines, and the implementation details of our methods.",null,null
184,"Dataset: Ranking performances were evaluated on the TREC Web Track ad-hoc task, the standard benchmark for web search. TREC 2009-2012 provided 200 queries for ClueWeb09, and TREC 2013-2014 provided 100 queries for ClueWeb12. e Category B of both corpora (ClueWeb09-B and ClueWeb12-B13) and corresponding TREC relevance judgments were used.",null,null
185,"On ClueWeb09-B, the SDM runs provided by EQFE [4] are used as the base retrieval. It is a well-tuned Galago-based implementation and performs be er than Indri's SDM. All their se ings are inherited, including spam ltering using waterloo spam score (with a threshold of 60), INQUERY plus web-speci c stopwords removal, and KStemming. On ClueWeb12-B13, not all queries' rankings are available from prior work, and Indri's SDM performs similarly to language model. For simplicity, the base retrieval on ClueWeb12B13 used is Indri's default language model with KStemming, INQUERY stopword removal, and no spam ltering. All our methods and learning to rank baselines re-ranked the rst 100 documents from the base retrieval.",null,null
186,"e ClueWeb web pages were parsed using Boilerpipe1. e `KeepEverythingExtractor' was used to keep as much text from the web page as possible, to minimize the parser's in uence. e documents were parsed to two elds: title and body. All the baselines and methods implemented by ourselves were built upon the same parsed results for fair comparisons.",null,null
187,e Knowledge Graph used in this work is Freebase [1]. e query and document entities were both annotated by TagMe [7]. No,null,null
188,"lter was applied on TagMe's results; all annotation were kept. is is the most widely used se ing of entity-based ranking methods on ClueWeb [17, 19, 21].",null,null
189,"Baselines included standard word-based baselines: Indri's language model (Lm), sequential dependency model (SDM), and two state-of-the-art learning to rank methods: RankSVM2 [10] and coordinate ascent (Coor-Ascent3) [15]. RankSVM was trained and evaluated using a 10-fold cross-validation on each corpus. Each fold was split to train (80%), develop (10%), and test (10%). e develop part was used to tune the hyper-parameter c of the linear SVM from the set {1e - 05, 0.0001, 0.001, 0.01, 0.03, 0.05, 0.07, 0.1, 0.5, 1}. Coor-Ascent was trained using RankLib's recommended se ings, which worked well in our experiments. ey used the same word based ranking features as in Table 1.",null,null
190,"Entity-based ranking baselines were also compared. EQFE [4], EsdRank [19], and BOE-TagMe [21] runs are provided on their authors' websites. e comparisons with EQFE and EsdRank were mainly done on ClueWeb09 as the full ranking results on ClueWeb12 are not publicly available. BOE-TagMe is the best TagMe based runs, which is TagMe-EF on ClueWeb09-B and TagMe-COOR on ClueWeb12-B13 [21]. Explicit Semantic Ranking (ESR) was implemented by ourselves as originally it was only applied on scholar search [23].",null,null
191,1h ps://github.com/kohlschu er/boilerpipe 2h ps://www.cs.cornell.edu/people/tj/svm light/svm rank.html 3h ps://sourceforge.net/p/lemur/wiki/RankLib/,null,null
192,"ere are also other unsupervised entity-based systems [14, 17, 20]; it is unfair to compare them with supervised methods.",null,null
193,"Evaluation was done by NDCG@20 and ERR@20, the o cial TREC Web Track ad-hoc task evaluation metrics. Statistical signi cances were tested by permutation test with p< 0.05.",null,null
194,"Feature Details: All parameters in the unsupervised retrieval model features were kept default. All texts were reduced to lower case, punctuation was discarded, and standard INQUERY stopwords were removed. Document elds included title and body, both parsed by Boilerpipe. Entity textual elds included name and description. When extracting Qw-De features, if a document did not have enough entities (3 in title or 5 in body), the feature values were set to -20.",null,null
195,"e TransE embeddings were trained using Fast-TransX library4. e embedding dimension used is 50. When extracting the a ention features in Table 5, the word and entity joint embeddings were obtained by training a skip-gram model on the candidate documents using Google's word2vec [16] with 300 dimensions; the surface form's statistics were calculated from Google's FACC1 annotation [8]. Model Details: AttR-Duet was evaluated using 10-fold cross validation with the same partitions as RankSVM. Deeper neural networks were explored but did not provide much improvement so the simplest CNN se ing was used: 1 layer, 1 lter, linear activation for the ranking part, and ReLU activation for the a ention part. All CNN's weights were L2-regularized. Regularization weights were selected from the set {0, 0.001, 0.01, 0.1} using the develop fold in the cross validation. Training loss was optimized using the Nadam algorithm [18]. Our implementation was based on Keras. To facilitate implementation, input feature matrices of query elements were padded to the maximum length with zeros. Batch training was used, given the small size of training data. Using a common CPU, the training took 4-8 hours to converge on ClueWeb09-B and 2-4 hours on ClueWeb12-B13. e testing is e cient as the neural network is shallow. e document annotations, TransE embeddings, and surface form information can be obtained o line. ery entity linking is e cient given the short query length. If the embedding results and entities' texts are maintained in memory, the feature extraction is of the same complexity as typical learning to rank features.",null,null
196,"e rankings, evaluation results, and the data used in our experiments are available online at h p://boston.lti.cs.cmu.edu/appendices/ SIGIR2017 word entity duet/.",null,null
197,6 EVALUATION RESULTS,null,null
198,is section rst evaluates the overall ranking performances of the word-entity duet with a ention based learning to rank. en it analyzes the two parts of AttR-Duet: Matching with the wordentity duet and the a ention mechanism.,null,null
199,6.1 Overall Performance,null,null
200,"e overall accuracies of AttR-Duet and baselines are shown in Table 6. Relative performances over RankSVM are shown in percentages. Win/Tie/Loss are the number of queries a method improves, does not change, and hurts, compared with RankSVM on NDCG@20.",null,null
201,4h ps://github.com/thunlp/Fast-TransX,null,null
202,768,null,null
203,Session 7B: Entities,null,null
204,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
205,"Table 6: Overall accuracies of AttR-Duet and baselines. (U) and (S) indicate unsupervised or supervised method. (E) indicates that information from entities is used. Relative performances compared with RankSVM are shown in percentages. Win/Tie/Loss are the number of queries a method improves, does not change, or hurts, compared with RankSVM on NDCG@20. Best results in each metric are marked bold. § marks statistically signi cant improvements (p< 0.05) over all baselines.",null,null
206,Method,null,null
207,Lm,null,null
208,(U),null,null
209,SDM,null,null
210,(U),null,null
211,RankSVM,null,null
212,(S),null,null
213,Coor-Ascent (S),null,null
214,BOE-TagMe (UE),null,null
215,ESR,null,null
216,(SE),null,null
217,EQFE5,null,null
218,(SE),null,null
219,EsdRank5,null,null
220,(SE),null,null
221,AttR-Duet (SE),null,null
222,ClueWeb09-B,null,null
223,NDCG@20,null,null
224,ERR@20,null,null
225,0.1757 -33.33% 0.1195 -22.63%,null,null
226,0.2496 -5.26% 0.1387 -10.20%,null,null
227,0.2635,null,null
228,­ 0.1544,null,null
229,­,null,null
230,0.2681 +1.75% 0.1617 +4.72%,null,null
231,0.2294 -12.94% 0.1488 -3.63%,null,null
232,0.2695 +2.30% 0.1607 +4.06%,null,null
233,0.2448 -7.10% 0.1419 -8.10%,null,null
234,0.2644 +0.33% 0.1756 +13.69% 0.3197§ +21.32% 0.2026§ +31.21%,null,null
235,W/T/L 47/28/125 62/38/100,null,null
236,­/­/­ 71/47/82 74/25/101 80/39/81 77/33/90 88/28/84,null,null
237,101/37/62,null,null
238,ClueWeb12-B13,null,null
239,NDCG@20,null,null
240,ERR@20,null,null
241,0.1060 -12.02% 0.0863 -6.67%,null,null
242,0.1083 -10.14% 0.0905 -2.08%,null,null
243,0.1205,null,null
244,­ 0.0924,null,null
245,­,null,null
246,0.1206 +0.08% 0.0947 +2.42%,null,null
247,0.1173 -2.64% 0.0950 +2.83%,null,null
248,0.1166 -3.22% 0.0898 -2.81%,null,null
249,,null,null
250,­ n/a,null,null
251,­,null,null
252,,null,null
253,­ n/a,null,null
254,­,null,null
255,0.1376§ +14.22% 0.1154§ +24.92%,null,null
256,W/T/L 35/22/43 27/25/48,null,null
257,­/­/­ 36/32/32 44/19/37 30/23/47,null,null
258,­/­/­ ­/­/­,null,null
259,45/24/31,null,null
260,"Table 7: Ranking accuracy with each group of matching feature from the word-entity duet. Base Retrieval is SDM on ClueWeb09 and Lm on ClueWeb12. LeToR-Qw-Dw uses the query and document's BOW (Table 1). LeToR-Qe-Dw uses the query's BOE and document's BOW (Table 2), LeToR-Qw-De is the query BOW + document BOE (Table 3), and LeToR-Qe-De uses the query and document's BOE (Table 4). LeToR-All uses all groups. Relative performances in percentages, Win/Tie/Loss on NDCG@20, and statistically signi cant improvements () are all compared with Base Retrieval.",null,null
261,Method Base Retrieval LeToR-Qw-Dw LeToR-Qe-Dw LeToR-Qw-De LeToR-Qe-De LeToR-All,null,null
262,ClueWeb09-B,null,null
263,NDCG@20,null,null
264,ERR@20,null,null
265,0.2496 0.2635,null,null
266,0.2729 0.2867 0.2695 0.3099,null,null
267,--,null,null
268,+5.55% +9.33% +14.83% +7.97% +24.13%,null,null
269,0.1387 0.1544 0.1824 0.1651 0.1607 0.1955,null,null
270,--,null,null
271,+11.36% +31.51% +19.07% +15.88% +40.97%,null,null
272,W/T/L ­/­/­ 100/38/62 82/34/84 91/39/70 99/40/61 103/38/59,null,null
273,ClueWeb12-B13,null,null
274,NDCG@20,null,null
275,ERR@20,null,null
276,0.1060,null,null
277,-- 0.0863,null,null
278,--,null,null
279,0.1205 +13.67% 0.0924 +7.14%,null,null
280,0.1110 +4.66% 0.0928 +7.63%,null,null
281,0.1146 +8.09% 0.0880 +1.96%,null,null
282,0.1166 +10.01% 0.0898 +4.13%,null,null
283,0.1205 +13.69% 0.1000 +15.93%,null,null
284,W/T/L ­/­/­ 43/22/35 40/20/40 42/20/38 38/20/42 47/19/34,null,null
285,Best results in each metric are marked Bold. § indicates statistically signi cant improvements over all available baselines.,null,null
286,"AttR-Duet outperformed all baselines signi cantly by large margins. On ClueWeb09-B, a widely studied benchmark for web search, AttR-Duet improved RankSVM, a strong learning to rank baseline, by more than 20% at NDCG@20, and more than 30% at ERR@20, showing the advantage of the word-entity duet over bag-of-words. ESR, EQFE and EsdRank, previous state-of-the-art entity-based ranking methods, were outperformed by at least 15%. It is not surprising because the word-entity duet framework was designed to include all of their e ects, as discussed at Section 3.3. ClueWeb12-B13 has been considered a hard dataset due to its noisy corpus and harder queries. e size of its training data is also smaller, which limits the strength of our neural network. However, AttR-Duet still signi cantly outperformed all available baselines by at least 14%. e information from entities is e ective and also di erent with those from words: AttR-Duet in uences more than three-quarters of the queries, and improves the majority of them.",null,null
287,"5Ranking results are obtained from the authors' websites. ClueWeb09-B scores are higher than in original papers [4, 19] as we evaluate them using TREC's Category B qrels. e original papers used Category A's qrels although they ranked Category B documents. EQFE and EsdRank's ClueWeb12 results are not available as they were only evaluated on the rst 50 queries of the 100.",null,null
288,6.2 Matching with Word-Entity Duet,null,null
289,"In a sophisticated system like AttR-Duet, it is hard to tell the contributions of di erent components. is experiment studies how each of the four-way interactions in the word-entity duet contributes to the ranking performance individually. For each group of the matching features in Table 1- 4, we train a RankSVM individually, which resulted in four ranking models: LeToR-Qw-Dw, LeToR-Qe-Dw, LeToR-Qw-De, and LeToR-Qe-De. LeToR-All which uses all ranking features is also evaluated. In LeToR-Qw-De and LeToR-Qe-De, the score of the base retrieval model is included as a feature, so that there is a feature to indicate the strength of the word-based match for the whole document. All these methods were trained and tested in the same se ing as RankSVM. As a result, LeToR-Qw-Dw is equivalent to the RankSVM baseline, and LeToR-Qe-De is equivalent to the ESR baseline.",null,null
290,"eir evaluation results are listed in Table 7. Relative performances (percentages), Win/Tie/Loss, and statistically signi cant improvements () are all compared with Base Retrieval (SDM on ClueWeb09 and Lm on ClueWeb12). All four groups of matching features were able to improve the ranking accuracy of Base Retrieval when used individually as ranking features, demonstrating the usefulness of all matching components in the duet. On ClueWeb09-B, all three entity related components, LeToR-Qe-Dw,",null,null
291,769,null,null
292,Session 7B: Entities,null,null
293,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
294,"Table 8: Examples of entities used in Qw-De and Qe-De. e rst half are examples of matched entities in relevant and irrelevant documents, which are used to extract Qw-De features. e second half are examples of entities falls into the exact match bin and the closest so match bins, used to extract Qe-De features.",null,null
295,ery Uss Yorktown Charleston SC Flushing,null,null
296,Examples of Most Similar Entities to the ery,null,null
297,Top Entities in Relevant Documents Top Entities in Irrelevant Documents,null,null
298,`USS Yorktown (CV-10)',null,null
299,"`Charles Cornwallis', `USS Yorktown (CV-5)'",null,null
300,"`Roosevelt Avenue', `Flushing, eens'",null,null
301,"`Flushing (physiology)', `Flush (cards)'",null,null
302,Examples of Neighbors in Knowledge Graph Embedding,null,null
303,ery,null,null
304,Entities in Exact Match Bin,null,null
305,Entities in So Match Bins,null,null
306,"Uss Yorktown Charleston SC `USS Yorktown (CV-10)', `Charleston, SC'",null,null
307,"`Empire of Japan', `World War II'",null,null
308,Flushing,null,null
309,"`Flushing, eens'",null,null
310,"`Brooklyn', `Manha an', `New York'",null,null
311,Relative NDCG@20 Relative NDCG@20,null,null
312,20%,null,null
313,12%,null,null
314,15%,null,null
315,10%,null,null
316,5%,null,null
317,0%,null,null
318,-5%,null,null
319,ClueWeb09,null,null
320,Top 1 Scores,null,null
321,Top 2 Scores,null,null
322,Top 4 Scores,null,null
323,Top 5 Scores,null,null
324,ClueWeb12,null,null
325,Top 3 Scores,null,null
326,9%,null,null
327,6%,null,null
328,3%,null,null
329,0% ClueWeb09,null,null
330,Exact Bins,null,null
331,First 2 Bins,null,null
332,First 4 Bins,null,null
333,First 5 Bins,null,null
334,ClueWeb12,null,null
335,First 3 Bins All Bins,null,null
336,(a) Features from ery Words to Document Entities (Qw-De) (b) Features from ery Entities to Document Entities (Qe-De),null,null
337,"Figure 2: Incremental ranking feature analysis. e y-axis is the relative NDCG@20 improvement over the base retrieval. e x-axis refers to the features from only top k (1-5) entity match scores (2a), and the features from only rst k (1-6) bins in the ESR model (2b), both ordered incrementally from le to right.",null,null
338,"LeToR-Qw-De, and LeToR-Qe-De, provided similar or be er performances than the word-based RankSVM. When all features were used together, LeToR-All signi cantly improved RankSVM by 17% and 26% on NDCG@20 and ERR@20, showing that the ranking evidence from di erent parts of the duet can reinforce each other.",null,null
339,"On ClueWeb12-B13, entity-based matching was less e ective. LeToR-All's NDCG@20 was the same as RankSVM's, despite additional matching features. e di erence is that the annotation quality of TagMe on ClueWeb12 queries is lower (Table 9) [21]. e noisy entity representation may mislead the ranking model, and prevent the e ective usage of entities. To deal with this uncertainty is the motivation of the a ention based ranking model, which is studied in Section 6.4.",null,null
340,6.3 Matching Feature Analysis,null,null
341,"e features from the word space (Qw-Dw) are well understood, and the feature from the query entities to document words (Qe-Dw) have been studied in prior research [4, 14, 19]. is experiment analyzes the features from the two new components (Qw-De and Qe-De).",null,null
342,"Qw-De features match the query words with the document entities. For each document, the query words are matched with the textual elds of document entities using retrieval models, and the highest scores are Qw-De features.",null,null
343,"We performed an incremental feature analysis of LeToR-Qw-De. Starting with the highest scored entities from each group in Table 3, we incrementally added the next highest ones to the model and evaluated the ranking performance. e results are shown in Figure 2a. e y-axis is the relative NDCG@20 improvements over the base retrieval model. e x-axis is the used features. For example, `Top 3 Scores' uses the top 3 entities' retrieval scores in each row of Table 3.",null,null
344,"e highest scores were very useful. Simply combining them with the base retrieval provided nearly 10% gain on ClueWeb09-B and about 7% on ClueWeb12-B13. Adding the following scores was not that stable, perhaps because the corresponding entities were rather noisy, given the simple retrieval models used to match query words with them. Nevertheless, the top 5 scores together further improve the ranking accuracy.",null,null
345,"e rst half of Table 8 shows examples of entities with highest matching scores. We found that such `top' entities from relevant documents are frequently related to the query, for example, `Roosevelt Avenue' is an avenue across Flushing, NY. In comparison, entities from irrelevant documents are much noisier. Qw-De features make use of this information and generate useful ranking evidence.",null,null
346,"Qe-De features are extracted using the Explicit Semantic Ranking (ESR) method [23]. ESR is built upon the translation model. It operates in the entity space, and extracts the ranking features using histogram pooling. ESR was originally applied on scholar search.",null,null
347,770,null,null
348,Session 7B: Entities,null,null
349,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
350,"Table 9: ery annotation accuracy and the gain of attention mechanism. TagMe Accuracy includes the precision and recall of TagMe on ClueWeb queries, evaluated in prior research [21]. Attention Gains are the relative improvements of AttR-Duet compared with LeToR-All. Statistical signi cant gains are marked by .",null,null
351,ClueWeb09 ClueWeb12,null,null
352,TagMe Accuracy Precision Recall,null,null
353,0.581 0.597 0.460 0.555,null,null
354,Attention Gain,null,null
355,NDCG@20 ERR@20,null,null
356,+3.16%,null,null
357,+3.65%,null,null
358,+14.20% +15.45%,null,null
359,Number of Queries Attention Relative Gain,null,null
360,Number of Queries Attention Relative Gain,null,null
361,NDCG ERR,null,null
362,150,null,null
363,8%,null,null
364,6% 100,null,null
365,4% 50,null,null
366,2%,null,null
367,0,null,null
368,0%,null,null
369,1,null,null
370,2,null,null
371,3,null,null
372,(a) ClueWeb09-B,null,null
373,NDCG ERR 50,null,null
374,40,null,null
375,40%,null,null
376,30,null,null
377,20,null,null
378,20%,null,null
379,10,null,null
380,0,null,null
381,0%,null,null
382,1,null,null
383,2,null,null
384,3,null,null
385,(b) ClueWeb12-B13,null,null
386,"Figure 3: Attention mechanism's gain on queries that contain di erent number of entities. e x-axis is the number of entities in the queries. e y-axis is the number of queries in each group (histogram), and the gain from attention (plots).",null,null
387,is work introduces ESR into web search and uses TransE model to train general domain embeddings from the knowledge graph.,null,null
388,"To study the e ectiveness of ESR in our se ing, we also performed an incremental feature analysis of LeToR-Qe-De. Starting from the rst bin (exact match), the following bins (so matches) were incrementally added into RankSVM, and their rankings were evaluated. e results are shown in Figure 2b. e y-axis is the relative NDCG@20 over the base retrieval model they re-rank. e x-axis is the features used. For example, First 3 Bins refers to using the rst three bins: [1, 1], [0.8, 1), and [0.6, 0.8).",null,null
389,"e observation on scholar search [23] holds on ClueWeb: Both exact match and so match with entities are useful. e exact match bin provided a 7% improvement on ClueWeb09-B, while only 2% on ClueWeb12-B13. Similar exact match results were also observed in a prior study [21]. It is another re ection of the entity annotation quality di erences on the two datasets. Adding the later bins almost always improves the ranking accuracy, especially on ClueWeb12.",null,null
390,"e second half of Table 8 shows some examples of entities in the exact match bin and the nearest so match bins. e exact match bin includes the query entities and is expected to help. e rst so match bin usually contains related entities. For example, the neighbors of `USS Yorktown (CV-10)' include `World War II' which is when the ship was built. e further bins are mostly background noise because they are too far away. e improvements are mostly from the rst 3 bins.",null,null
391,Table 10: Examples of learned attention. e entities in bold blue draw more attention; those in gray draw less attention.,null,null
392,ery Balding Cure,null,null
393,Nicolas Cage Movies Hawaiian Volcano Observatories Magnesium Rich Foods Kids Earth Day Activities,null,null
394,Entity Attention,null,null
395,`Cure' `Clare Balding',null,null
396,`Nicolas Cage' `Pokemon (Anime)' `Volcano'; `Observatory' `Hawaiian Narrative'; `Magnesium'; `Food' `First World',null,null
397,`Earth Day' `Youth Organizations in the USA',null,null
398,6.4 Attention Mechanism Analysis,null,null
399,"e last experiment studies the e ect of the a ention mechanism by comparing AttR-Duet with LeToR-All. If enforcing at a ention weights on all query words and entities, AttR-Duet is equivalent to LeToR-All: e matching features, model function, and loss function are all the same. e a ention part is their only di erence, whose e ect is re ected in this comparison.",null,null
400,"e gains from the a ention mechanism are shown in Table 9. To be er understand the a ention mechanism's e ectiveness in demoting noisy query entities, the query annotation's quality evaluated in a prior work [21] is also listed. e percentages in the A ention Gain columns are relative improvements of AttR-Duet compared with LeToR-All.  marks statistical signi cance. Figure 3 breaks down the relative gains to queries with di erent numbers of query entities. e x-axis is the number of query entities. e histograms are the number of queries in each group, marked by the le y-axis.",null,null
401,"e plots are the relative gains, marked by the right y-axis. e a ention mechanism is essential to ClueWeb12-B13. With-",null,null
402,"out the a ention model, LeToR-All was confused by the noisy query entities and could not provide signi cant improvements over word-based models, as discussed in the last experiment. With the a ention mechanism, AttR-Duet improved LeToR-All by about 15%, outperforming all baselines. On ClueWeb09 where TagMe's accuracy is be er [21], the ranking evidence from the word-entity duet was clean enough for LeToR-All to improve ranking, so the a ention mechanism's e ect was smaller. Also, in general, the a ention mechanism is more e ective when there are more query entities, while if there is only one entity there is not much to tweak.",null,null
403,"e motivation for using a ention is to handle the uncertainties in the query entities, a crucial challenge in utilizing knowledge bases in search. ese results demonstrated its ability to do so. We also found many intuitive examples in the learned a ention weights, some listed in Table 10. e bold blue entities on the rst line of each block gain more a ention (> 0.6 a ention score). ose in gray on the second line draw less a ention (< 0.4 score). e a ention mechanism steers the model away from those mistakenly linked query entities, which makes it possible to utilize the correct entities' ranking evidence from a noisy representation.",null,null
404,771,null,null
405,Session 7B: Entities,null,null
406,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
407,7 CONCLUSIONS AND FUTURE WORK,null,null
408,"is work presents a word-entity duet framework for utilizing knowledge bases in document ranking. In this paper, the query and documents are represented by both word-based and entitybased representations. e four-way interactions between the two representation spaces form a word-entity duet that can systematically incorporate various semantics from the knowledge graph. From query words to document words (Qw-Dw), word-based ranking features are included. From query entities to document entities (Qe-De), entity-based exact match and so match evidence from the knowledge graph structure are included. e entities' textual elds are used in the cross-space interactions Qe-Dw, which expands the query, and Qw-De, which enriches the document.",null,null
409,"To handle the uncertainty introduced from the automatic-thusnoisy entity representations, a new ranking model AttR-Duet is developed. It employs a simple a ention mechanism to demote the ambiguous or o -topic query entities, and learns simultaneously how to weight entities of varying quality and how to rank documents with the word-entity duet.",null,null
410,"Experimental results on the TREC Web Track ad-hoc task demonstrate the e ectiveness of proposed methods. AttR-Duet signi cantly outperformed all word-based and entity-based ranking baselines on both ClueWeb corpora and all evaluation metrics. Further experiments reveal that the strength of the method comes from both the advanced matching evidence from the word-entity duet, and the a ention mechanism that successfully `puri es' them. On ClueWeb09 where the query entities are cleaner, all the entity related matching components from the duet provide similar or be er improvements compared with word-based features. On ClueWeb12 where the query entities are noisier, the a ention mechanism steers the ranking model away from noisy entities and is necessary for stable improvements.",null,null
411,"Our method provides a uni ed representation framework to utilize knowledge graphs in information retrieval. As the rst step, this work kept its components as simple as possible. It is easy to imagine further developments in various places. For example, the recent approaches in neural ranking with word embeddings can be incorporated [9]; be er knowledge graph embeddings can be used [13]; be er entity search methods can be applied when extracting word to entity features [3]; the a ention mechanism can be extended to document's entity-based representations. More sophisticated neural ranking models [22] can also be applied with the word-entity duet, especially when more training data are available.",null,null
412,8 ACKNOWLEDGMENTS,null,null
413,"is research was supported by National Science Foundation (NSF) grant IIS-1422676, a Google Faculty Research Award, and a fellowship from the Allen Institute for Arti cial Intelligence. We thank Xu Han for helping us train the TransE embedding. Any opinions,",null,null
414,"ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.",null,null
415,REFERENCES,null,null
416,"[1] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data (SIGMOD 2008). ACM, 1247­1250.",null,null
417,"[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NIPS 2013). 2787­ 2795.",null,null
418,"[3] Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An empirical study of learning to rank for entity search. In Proceedings of the 39th Annual International ACM",null,null
419,"SIGIR Conference on Research and Development in Information Retrieval,(SIGIR 2016). ACM, 737­740. [4] Je rey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th Annual International ACM",null,null
420,"SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2014). ACM, 365­374. [5] Laura Dietz, Alexander Kotov, and Edgar Meij. 2017. Utilizing knowledge graphs in text-centric information retrieval. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 815­816. [6] Faezeh Ensan and Ebrahim Bagheri. 2017. Document retrieval model through semantic linking. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 181­190. [7] Paolo Ferragina and Ugo Scaiella. 2010. Fast and accurate annotation of short texts with Wikipedia pages. arXiv preprint arXiv:1006.3498 (2010). [8] Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 201306-26, Format version 1, Correction level 0). (June 2013). [9] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W.Bruce Cro . 2016. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (CIKM 2016). ACM, 55­64. [10] orsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2002). ACM, 133­142. [11] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, So¨ren Auer, and Christian Bizer. 2014. DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic Web Journal (2014). [12] Hang Li and Jun Xu. 2014. Semantic matching in search. Foundations and Trends in Information Retrieval 8 (2014), 89. [13] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference on Arti cial Intelligence (AAAI 2015). 2181­ 2187. [14] Xitong Liu and Hui Fang. 2015. Latent entity space: A novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18, 6 (2015), 473­503. [15] Donald Metzler and W Bruce Cro . 2007. Linear feature-based models for information retrieval. Information Retrieval 10, 3 (2007), 257­274. [16] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed representations of words and phrases and their compositionality. In",null,null
421,"Proceedings of the 2 h Advances in Neural Information Processing Systems 2013 (NIPS 2013). 3111­3119. [17] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document retrieval using entity-based language models. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016). ACM, 65­74. [18] Ilya Sutskever, James Martens, George E Dahl, and Geo rey E Hinton. 2013. On the importance of initialization and momentum in deep learning.. In Proceedings of the 29th International Conference on Machine Learning (ICML 2013). 1139­1147. [19] Chenyan Xiong and Jamie Callan. 2015. EsdRank: Connecting query and documents through external semi-structured data. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management (CIKM 2015). ACM, 951­960. [20] Chenyan Xiong and Jamie Callan. 2015. ery expansion with Freebase. In",null,null
422,"Proceedings of the h ACM International Conference on the eory of Information Retrieval (ICTIR 2015). ACM, 111­120. [21] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2016. Bag-of-Entities representation for ranking. In Proceedings of the sixth ACM International Conference on the",null,null
423,"eory of Information Retrieval (ICTIR 2016). ACM, 181­184. [22] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.",null,null
424,2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of,null,null
425,"the 40th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2017). ACM, To Appear. [23] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 25th International Conference on World Wide Web (WWW 2017). ACM, 1271­1279. [24] Yang Xu, Gareth JF Jones, and Bin Wang. 2009. ery dependent pseudorelevance feedback based on Wikipedia. In Proceedings of the 32nd Annual In-",null,null
426,"ternational ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2009). ACM, 59­66.",null,null
427,772,null,null
428,,null,null

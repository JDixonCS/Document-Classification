,sentence,label,data
0,Short Resource Papers,null,null
1,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
2,Anserini: Enabling the Use of Lucene for Information Retrieval Research,null,null
3,"Peilin Yang, Hui Fang",null,null
4,Department of Electrical and Computer Engineering University of Delaware,null,null
5,"{franklyn,hfang}@udel.edu",null,null
6,ABSTRACT,null,null
7,"So ware toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrieval models over standard test collections. E orts are generally directed toward be er ranking and less a ention is usually given to scalability and other operational considerations. On the other hand, Lucene has become the de facto platform in industry for building search applications (outside a small number of companies that deploy custom infrastructure). Compared to academic IR toolkits, Lucene can handle heterogeneous web collections at scale, but lacks systematic support for evaluation over standard test collections. is paper introduces Anserini, a new information retrieval toolkit that aims to provide the best of both worlds, to be er align information retrieval practice and research. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial e orts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web-scale collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for many TREC test collections, providing a convenient way to replicate competitive baselines right out of the box. Experiments verify that our system is both e cient and e ective, providing a solid foundation to support future research.",null,null
8,1 INTRODUCTION,null,null
9,"Information retrieval researchers have a long history of developing, sharing, and using so ware toolkits to support their work. Over the past several decades, various IR toolkits have been built to aid in the development of new retrieval models, to test hypotheses about information seeking, and to validate new evaluation methodologies. As the eld moves forward, IR toolkits are expected to keep up with emerging requirements such as the ability to handle large web collections and new data formats. e growing complexity of modern so ware ecosystems and the resource constraints most academic research groups operate under make maintaining opensource toolkits a constant struggle.",null,null
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080721",null,null
11,Jimmy Lin,null,null
12,David R. Cheriton School of Computer Science University of Waterloo jimmylin@uwaterloo.ca,null,null
13,"Most IR toolkits developed by academics, such as Indri,1 Galago,2 and Terrier3 were primarily designed to facilitate evaluation over standard test collections from evaluation forums such as TREC, CLEF, NTCIR, etc. In many cases, scalability took a back seat to e orts around improving retrieval models, and thus these systems o en struggle to scale to modern web collection. As an example, the ClueWeb12 collection4 contains 733 million web pages, totaling 5.54 TB compressed (or 27.3 TB uncompressed). e standard practice for working with this collection, as exempli ed by the infrastructure built for the TREC 2014 Session Track [4], is to separately index partitions of the collection and then build a distributed broker architecture that integrates results from each partition. In general, working with web-scale collections using existing academic IR toolkits is time- and resource-intensive, even for basic tasks.",null,null
14,"With the exception of a small number of companies (e.g., commercial web search engines), the open-source Lucene system5 and its derivatives such as Solr and Elasticsearch (for convenience, we simply refer to as ""Lucene"" collectively in this paper) have become the de facto platform for deploying search applications in industry. Examples include LinkedIn, Twi er, Bloomberg, as well as a number of online retailers and many large companies in the nancial services space. Despite its undeniable operational success, a large user base, and a vibrant community of contributors, Lucene is not well suited to information retrieval research. For many reasons, including poor documentation of system internals and a number of unintuitive abstractions, Lucene is not as widely used for research as academic toolkits such as Indri or Terrier.",null,null
15,"In this paper, we describe our e orts in developing a new opensource information retrieval toolkit called Anserini that builds on Lucene.6 We aim to bridge the gap described above that separates information retrieval research from the practice of building real-world search applications. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial e orts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for standard TREC test collections, providing a convenient way to replicate competitive baselines ""right out of the box"", supporting the community's aspirations toward reproducible research [1, 7, 8, 10, 16, 18].",null,null
16,1h p://www.lemurproject.org/indri/ 2h p://www.lemurproject.org/galago.php 3h p://terrier.org/ 4h p://www.lemurproject.org/clueweb12/ 5h ps://lucene.apache.org/ 6h p://anserini.io/,null,null
17,1253,null,null
18,Short Resource Papers,null,null
19,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
20,"We experimentally evaluate the e ciency and e ectiveness of Anserini on a number of standard test collections. In terms of indexing performance, it is able to handle the largest research web collection available today with ease on a single modern server. We observe be er indexing performance compared to Indri, a popular choice among researchers today. In terms of retrieval, we also nd that Anserini is not only faster than Indri, but returns rankings that are comparable in quality. In other words, Anserini is faster and just as good. We present the case that Anserini should be adopted as the toolkit of choice for information retrieval researchers.",null,null
21,2 ANSERINI OVERVIEW,null,null
22,2.1 Motivation,null,null
23,"Despite its popularity in industry and broad adoption for operational search deployments, Lucene remains under-utilized in information retrieval research. We begin with some high-level discussions of why we believe this might be so to motivate our e orts in building Anserini.",null,null
24,"From the very beginning, Lucene was wri en for ""real world"" search applications, not with researchers in mind. For the most part, its developers targeted an audience that mostly used search engines as black boxes, as opposed to researchers that required access to ranking internals such as scoring models, mechanisms for postings traversal, etc. Because of the target user population, documentation for Lucene internals has always been quite poor, especially in keeping up with the relatively rapid pace at which the developer community has been releasing improved versions of the so ware. Access to these internals is exactly what information retrieval researchers need for their studies, and therefore poor documentation has been a barrier to entry.",null,null
25,"To further compound this issue, the internal APIs in Lucene are not organized in a way that would be intuitive to most IR researchers, with class names that are not indicative of functionality and many levels of indirection. is is not an issue for ""black box"" users of Lucene, but presents a hurdle for information retrieval researchers who desire access to system internals. As an example, the code to open up a Lucene index and to traverse postings programmatically (without invoking the scoring function) is unnecessarily complex and involves dispatching to several intermediate classes along the way. Some researchers have the impression that Lucene is di cult to use, and indeed there is some truth to this, especially with respect to low-level abstractions.",null,null
26,"Another side e ect of Lucene's focus on ""black box"" search is that it has severely lagged behind in the implementation of modern ranking functions. For the longest time, the default scoring model was an ad hoc variant of tf-idf. Okapi BM25 was not added to Lucene until 2011,7 more than a decade a er it gained widespread adoption in the research community as being more e ective than tf-idf variants. is lag in adopting ""research best practices"" has contributed to the perception that Lucene is not e ective and illsuited for information retrieval research. However, this perception is no longer accurate today. Lucene comes with implementations of modern baseline retrieval models, and we show that the e ectiveness of Lucene's implementations is at least as good as those o ered by academic IR toolkits (see Section 3).",null,null
27,7h ps://issues.apache.org/jira/browse/LUCENE-2959,null,null
28,"Finally, because Lucene is wri en in Java, there is sometimes the perception that it is slow and ine cient, particularly when scaling up to modern web collections. Developers o en point to the managed memory environment of the Java Virtual Machine (JVM) as not being conducive to e cient low-level implementations of search engine internals. We experimentally show that this is de nitely not true (see Section 3). e open-source community has devoted substantial e ort to optimizing the performance of Lucene and taking advantage of today's multi-core processors. It is capable of handling large web collections on a single server with ease.",null,null
29,"e goal of Anserini is to align the practice of building search applications with research in information retrieval. Colloquially speaking, our toolkit aims to smooth the ""rough edges"" around Lucene for the purposes of information retrieval research. It is not our goal to replace or to reimplement Lucene, but rather to facilitate its use for research by presenting as gentle a learning curve as possible to newcomers.",null,null
30,2.2 Main Components,null,null
31,"Anserini components fall into two categories: wrappers and extensions. Wrappers provide APIs that leverage core Lucene library components to accomplish speci c tasks. ey are tightly integrated with ""core"" Lucene and in some cases, represent custom implementations of existing Lucene APIs. Extensions, on the other hand, are components that are distinct from Lucene and more loosely coupled: these may represent our own implementations or connectors to third-party libraries.",null,null
32,"Multi-threaded indexing (wrapper). Inverted indexing is one of the most fundamental tasks in information retrieval and the starting point of many research studies. In working with large web collections, it is imperative that indexing operations are e cient and scalable. While academic researchers have a empted to address this issue via MapReduce and related frameworks [5, 11], these solutions impose the burden of requiring clusters and additional so ware infrastructure.",null,null
33,"Lucene supports multi-threaded indexing, and as we experimentally show (Section 3), it is able to scale up to large web collections on a single commodity server. e biggest issue, however, is that Lucene itself only provides access to a collection of indexing components that researchers need to assemble together to build an end-to-end indexer. For example, the developer would need to write from scratch custom document processing pipelines, code for managing individual indexing threads, and implementations of load balancing and synchronization procedures.",null,null
34,"We address these issues in Anserini by providing abstractions for document collections that an IR researcher would be comfortable with, as well as the implementation of an e cient, high-throughput, multi-threaded indexer that takes advantage of these abstractions. Anserini models collections as comprised of individual segments (for example, the ClueWeb12 collection is comprised of a number of compressed WARC les) and provides implementations for common document formats--for parsing TREC-style XML documents, web pages stored in WARCs, tweets in JSON format, etc. In fact, Anserini ships with the ability to index many TREC collections ""right out of the box"". is greatly reduces the learning curve for researchers to get started with Lucene.",null,null
35,1254,null,null
36,Short Resource Papers,null,null
37,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
38,Table 1: Indexing performance of Anserini and Indri on smaller collections using 16 threads on a modest commodity server.,null,null
39,Collection Disk12 Disk45 AQUAINT WT2G WT10G Gov2,null,null
40,docs 742k 528k 1.03m 246k 1.69m 25.2m,null,null
41,terms 219m 175m 318m 182m 752m 17.3b,null,null
42,Anserini (count) time size,null,null
43,00:01:24 199MB 00:01:13 166MB 00:01:53 305MB 00:02:21 143MB 00:04:55 708MB 01:16:32 11GB,null,null
44,Anserini (pos) time size,null,null
45,00:01:44 512MB 00:01:33 423MB 00:02:10 734MB 00:02:55 437MB 00:05:05 2.9GB 02:32:43 38GB,null,null
46,Anserini (doc) time size,null,null
47,00:03:09 2.5GB 00:02:51 2.1GB 00:04:32 3.8GB 00:04:24 2.3GB 00:09:51 12GB 06:52:35 331GB,null,null
48,Indri time size 00:12:28 2.5GB 00:06:55 1.9GB 00:17:36 3.9GB 00:07:25 2.2GB 00:42:51 9.6GB 14:51:12 215GB,null,null
49,Table 2: Indexing performance of Anserini on web collections using 88 threads on a high-end server.,null,null
50,Collection CW09b CW09 CW12b13 CW12,null,null
51,docs 50m 504m 52m 733m,null,null
52,terms 31b 268b 31b 429b,null,null
53,Anserini (count),null,null
54,time,null,null
55,size,null,null
56,00:42 28GB,null,null
57,07:32 254GB,null,null
58,00:57 29GB,null,null
59,17:01 376GB,null,null
60,Anserini (pos) time size 01:13 75GB 12:18 649GB 01:25 76GB 22:21 1.1TB,null,null
61,"Streamlined IR evaluation (extension). Test collections play an important role in information retrieval research, and a substantial amount of research activity in improving ranking models is focused around ad hoc retrieval runs. A research toolkit should make this ""inner loop"" of IR research as easy as possible. Since Lucene was not originally designed for researchers, support for running experiments on standard test collections is largely missing. Anserini lls this gap by implementing missing features: parsers for di erent query formats, a uni ed driver program for ad hoc experiments that outputs standard trec eval format, etc. For convenience, existing TREC topics and qrels are included directly in our code repository--once again, reducing the learning curve for researchers to get started with Lucene.",null,null
62,"ere are two main uses for this feature in Anserini: First, our toolkit provides an easy way for researchers to replicate baselines of standard retrieval models such as BM25 and query likelihood. Armstrong et al. [2] previously identi ed the prevalent problem of weak baselines in experimental IR papers. Lin et al. [10] further observed that authors are o en vague about the baseline parameter se ings and the implementations they use. For example, Mu¨hleisen et al. [13] reported large di erences in e ectiveness across four systems that all purport to implement BM25. Trotman et al. [15] pointed out that BM25 and query likelihood with Dirichlet priors can actually refer to at least half a dozen variants, and in some cases, di erences in e ectiveness are statistically signi cant. ere is substantial community interest in engaging with reproducibilityrelated issues [1, 8], and Anserini contributes to this discussion. Our proposed solution is to have widely-available baselines that are both competitive in e ectiveness and easy to replicate. It is our hope that Anserini can ll this role.",null,null
63,"Second, an easy-to-use baseline retrieval component in Anserini provides the starting point for additional ranking extensions. In particular, we advocate a multi-stage ranking architecture [3, 6, 14, 17] so that researchers will not need to directly work with native Lucene scoring APIs. at is, researchers should take advantage of Anserini APIs that generate an initial document ranking and hooks",null,null
64,"for feature extraction to build subsequent reranking stages. is, in fact, is the common architecture used in commercial web search engines today to support learning to rank [14].",null,null
65,"Relevance feedback (extension). Relevance feedback techniques provide robust solutions to the vocabulary mismatch problem between expressions of user information needs and relevant documents. Anserini provides a reference implementation of the RM3 variant of relevance models [9], built as a reranking module in the multi-stage architecture described above. us, our implementation is useful not only as a baseline for comparing query expansion techniques, but provides an example of how reranking extensions can be implemented in Anserini.",null,null
66,3 EVALUATION,null,null
67,"We describe experiments to support three claims about Anserini and the use of Lucene for information retrieval research. First, that Anserini is highly scalable and able to e ciently index large web collections. Second, that Anserini is similarly e cient in searching these collections and ranking documents using standard baseline models. Finally, Anserini is able to achieve scalable indexing and e cient retrieval without compromising ranking e ectiveness.",null,null
68,"e indexing performance of Anserini on a number of smaller and older collections is shown in Table 1. ese experiments were conducted on a server with dual AMD Opteron 6128 processors (2.0GHz, 8 cores) with 40GB RAM running CentOS 6.8. is machine can be characterized as an old, modest commodity server. All experiments were run on an otherwise idle machine. With Anserini, we used 16 threads for indexing and we report results from three di erent index con gurations: count indexes where only term frequency information is stored (count), positional indexes that also store term positions (pos), and positional indexes that also store the raw documents and parsed document vectors (doc). For each condition, we report the indexing time in HH:MM:SS (averaged over two trials) as well as the index size. e size of each collection is also shown for reference. As a comparison condition, we indexed the same collections using Indri 5.9 on the same machine.",null,null
69,"In Table 2, we report indexing performance for larger web collections on a server with dual Intel Xeon E5-2699 v4 processors (2.2GHz, 22 cores) and 1 TB RAM running Ubuntu 16.04. e table rows indicate di erent collections: CW09b refers to the ClueWeb09 (category B) web crawl, CW09 refers to all English pages in the ClueWeb09 web crawl, CW12b13 refers to the smaller ClueWeb12B13 web crawl, and CW12 refers to the complete ClueWeb12 web crawl. Due to the size of the collections, we only report the count and positional index con gurations. For these experiments, we used",null,null
70,1255,null,null
71,Short Resource Papers,null,null
72,"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",null,null
73,"Table 3: Retrieval e ciency for Terabyte 06 e ciency queries on Gov2, using a single thread.",null,null
74,Latency (ms),null,null
75,Indri,null,null
76,2403,null,null
77,Anserini,null,null
78,382,null,null
79,roughput (qps) 0.42 2.61,null,null
80,Table 4: E ectiveness comparisons between Anserini and Indri on standard TREC test collections.,null,null
81,Collection eries,null,null
82,BM25 (I) BM25 (A) LM (I) LM (A),null,null
83,Disk12 51-200,null,null
84,0.2040 0.2267 0.2269 0.2232,null,null
85,Disk45 301-450 601-700,null,null
86,0.2478 0.2500 0.2516 0.2465,null,null
87,WT2G 401-450,null,null
88,0.3152 0.3015 0.3116 0.2922,null,null
89,WT10G 451-550,null,null
90,0.1955 0.1981 0.1915 0.2015,null,null
91,Gov2 701-850,null,null
92,0.2970 0.3030 0.2995 0.2951,null,null
93,"88 threads on an otherwise idle machine; indexing time is reported in HH:MM (averaged over two trials). On this server, we are able to index all of ClueWeb12, one of the largest collections available to researchers today, in less than a day! As seen from Table 1, even on an older server, the indexing performance of Lucene is impressive. Compared to academic toolkits, Lucene does not appear to have any trouble scaling to large modern web collections.",null,null
94,"Our next set of experiments were conducted on the Gov2 collection with Terabyte 06 e ciency queries. We issued all 100,000 queries sequentially against both the Anserini and Indri indexes on the slower AMD Opteron server. Results are shown in Table 3, which reports latency (ms) and throughput (queries per second, or qps). In this experiment, we used only a single query thread, and therefore do not take advantage of Lucene's ability to execute queries in parallel on multiple threads (so in our case, throughput is simply the inverse of latency). We see from these experiments that Lucene is roughly six times faster than Indri.",null,null
95,"Finally, we compared the retrieval e ectiveness of Anserini and Indri. For Indri we refer to the RISE work of Yang and Fang [18], as they ne-tuned model parameters to achieve optimal e ectiveness. We considered two baseline ranking models: Okapi BM25 (BM25) and query likelihood with Dirichlet priors (LM). For Anserini, we removed stopwords (the default) and tuned parameters as follows: for BM25, k ,"" 0.9 and b  [0, 1] in increments of 0.1; for LM, µ  [0, 5000] in increments of 500. Results on standard TREC collections and queries are shown in Table 4, where (I) refers to Indri and (A) refers to Anserini. We see that e ectiveness results are comparable between the two systems.""",null,null
96,"In summary, our experiments show that Anserini is at least as good as Indri in terms of e ectiveness, and much faster in both indexing and retrieval. ese results are consistent with ndings from the recent Open-Source IR Reproducibility Challenge [10]. Together, empirical evidence presents a compelling case for adopting Lucene for information retrieval research.",null,null
97,4 CONCLUSIONS AND FUTURE WORK,null,null
98,"Our message to the information retrieval community is that Lucene is e cient and scalable without compromising e ectiveness. Furthermore, Lucene has the bene t of a large user community and",null,null
99,"broad adoption in industry. Anserini smooths over the ""rough",null,null
100,"edges"" of using Lucene for information retrieval research by pro-",null,null
101,viding wrappers and extensions that simplify common tasks such,null,null
102,as indexing large research web collections and performing standard,null,null
103,ad hoc retrieval runs. We hope that our toolkit will help to be er,null,null
104,align the research and practice of information retrieval.,null,null
105,"Broadly characterized, Anserini provides the foundation for an",null,null
106,"IR research toolkit, but currently lacks features that one would",null,null
107,associate with cu ing-edge research. Ongoing work is focused on,null,null
108,"addressing this issue, as we are actively exploring retrieval models",null,null
109,based on deep learning [12]. E orts include a empts to replicate,null,null
110,existing neural retrieval models within our framework. Given the,null,null
111,"existence of many deep learning toolkits (Torch, TensorFlow, etc.),",null,null
112,"it does not make sense to reinvent the wheel. In this spirit, we",null,null
113,have been building connectors between Lucene and the PyTorch,null,null
114,"deep learning toolkit. Moving forward, we anticipate substantial",null,null
115,continued interest at the intersection of deep learning and informa-,null,null
116,"tion retrieval, and the multi-stage ranking architecture of Anserini",null,null
117,provides a natural integration point for future explorations.,null,null
118,REFERENCES,null,null
119,"[1] Jaime Arguello, Ma Crane, Fernando Diaz, Jimmy Lin, and Andrew Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR). SIGIR Forum 49, 2 (2015), 107­116.",null,null
120,"[2] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don't Add Up: Ad-Hoc Retrieval Results Since 1998. In CIKM. 601­610.",null,null
121,[3] Nima Asadi and Jimmy Lin. 2013. E ectiveness/E ciency Tradeo s for Candidate Generation in Multi-Stage Retrieval Architectures. In SIGIR. 997­1000.,null,null
122,"[4] Ben Cartere e, Evangelos Kanoulas, Mark Hall, and Paul Clough. 2014. Overview of the TREC 2014 Session Track. In TREC.",null,null
123,"[5] Marc-Allen Cartright, Samuel Huston, and Henry Feild. 2012. Galago: A Modular Distributed Processing and Retrieval System. In SIGIR 2012 Workshop on Open Source Information Retrieval. 25­31.",null,null
124,"[6] Charles L. A. Clarke, J. Shane Culpepper, and Alistair Mo at. 2016. Assessing E ciency--E ectiveness Tradeo s in Multi-stage Retrieval Systems Without Using Relevance Judgments. IRJ 19, 4 (2016), 351­377.",null,null
125,"[7] Hui Fang, Hao Wu, Peilin Yang, and ChengXiang Zhai. 2014. VIRLab: A Webbased Virtual Lab for Learning and Studying Information Retrieval Models. In SIGIR. 1249­1250.",null,null
126,"[8] Nicola Ferro, Norbert Fuhr, Kalervo Ja¨rvelin, Noriko Kando, Ma hias Lippold, and Justin Zobel. 2016. Increasing Reproducibility in IR: Findings from the Dagstuhl Seminar on ""Reproducibility of Data-Oriented Experiments in e-Science"". SIGIR Forum 50, 1 (2016), 68­82.",null,null
127,[9] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR. 120­127.,null,null
128,"[10] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",null,null
129,"[11] Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan Wang. 2009. Of Ivory and Smurfs: Loxodontan MapReduce Experiments for Web Search. In TREC.",null,null
130,[12] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv:1705.01509.,null,null
131,"[13] Hannes Mu¨hleisen, aer Samar, Jimmy Lin, and Arjen de Vries. 2014. Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping. In SIGIR. 863­866.",null,null
132,"[14] Jan Pedersen. 2010. ery Understanding at Bing. Invited Talk at SIGIR. [15] Andrew Trotman, An i Puurula, and Blake Burgess. 2014. Improvements to",null,null
133,"BM25 and Language Models Examined. In ADCS. 58­65. [16] Ellen M. Voorhees, Shahzad Rajput, and Ian Soboro . 2016. Promoting Repeata-",null,null
134,"bility rough Open Runs. In EVIA. 17­20. [17] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A Cascade Ranking Model",null,null
135,for E cient Ranked Retrieval. In SIGIR. 105­114. [18] Peilin Yang and Hui Fang. 2016. A Reproducibility Study of Information Retrieval,null,null
136,Models. In ICITR. 77­86.,null,null
137,Acknowledgments. is research was supported by the Natural Sciences,null,null
138,and Engineering Research Council (NSERC) of Canada and the U.S. National,null,null
139,Science Foundation under IIS-1423002 and CNS-1405688.,null,null
140,1256,null,null
141,,null,null

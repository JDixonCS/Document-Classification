,sentence,label,data
0,Relevance-aware Filtering of Tuples Sorted by an Attribute Value via Direct Optimization of Search Quality Metrics,null,null
1,"Nikita Spirin1, Mikhail Kuznetsov2, Julia Kiseleva3, Yaroslav Spirin4, Pavel Izhutov5",null,null
2,"1UIUC, Urbana IL, USA; 2MIPT, Dolgoprudny, Russia; 3Eindhoven University of Technology, Eindhoven,",null,null
3,"Netherlands; 4Datastars, Moscow, Russia; 5Stanford University, Palo Alto CA, USA",null,null
4,"spirin2@illinois.edu1, mikhail.kuznecov@phystech.edu2, j.kiseleva@tue.nl3, izhutov@stanford.edu5",null,null
5,ABSTRACT,null,null
6,"Sorting tuples by an attribute value is a common search scenario and many search engines support such capabilities, e.g. price-based sorting in e-commerce, time-based sorting on a job or social media website. However, sorting purely by the attribute value might lead to poor user experience because the relevance is not taken into account. Hence, at the top of the list the users might see irrelevant results. In this paper we choose a different approach. Rather than just returning the entire list of results sorted by the attribute value, additionally we suggest doing the relevance-aware search results (post-)filtering. Following this approach, we develop a new algorithm based on the dynamic programming that directly optimizes a given search quality metric. It can be seamlessly integrated as the final step of a query processing pipeline and provides a theoretical guarantee on optimality. We conduct a comprehensive evaluation of our algorithm on synthetic data and real learning to rank data sets. Based on the experimental results, we conclude that the proposed algorithm is superior to typically used heuristics and has a clear practical value for the search and related applications.",null,null
7,Keywords,null,null
8,Search Metric; Attribute; Filtering; Dynamic Programming,null,null
9,Categories and Subject Descriptors,null,null
10,"H.3.3 [Information Search and Retrieval]: Information filtering, Retrieval models, Search process, Selection process",null,null
11,1. INTRODUCTION,null,null
12,"Many search engines support sorting of the search results by an attribute value, e.g. sort items by price in e-commerce or sort resumes by the update time on the job websites. A similar scenario exists in the social domain when the goal is to construct a chronologically sorted social feed, e.g. Twitter, Facebook. However, sorting purely by the attribute value might not be the best approach since at the top of the",null,null
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",null,null
14,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,null,null
15,DOI: http://dx.doi.org/10.1145/2766462.2767822.,null,null
16,"list users might find irrelevant results. For example, see the screenshots of the search user interfaces for Indeed.com and Amazon.com on Figure 1. In both cases the results sorted by the attribute values are hardly relevant for the queries.",null,null
17,"To evaluate how such search scenarios are supported today, we conducted the ad hoc evaluation of ten popular search engines from the e-commerce and job industries1. For each search engine we submitted 25 queries (different queries for different industries), applied the sorting based on one of the attributes (relevance, date, price), and judged the quality of results2. The ranking by relevance is of very high quality. The average Precision@10 is 0.86. On the other hand, we found that the search results are far from relevant when the attribute-based sorting is done. For instance, across the sites the average Precision@1 is 0.44, Precision@5 is 0.45, and 61% of queries have the Precision@10 below 0.5. We think that it is mainly due to the relevance not being taken into account when the attribute-based sorting is requested. Therefore, our research questions are: (RQ1) Can the quality of results sorted by the attribute value be improved by incorporating the relevance into the ranking process? (RQ2) What is the best way to accomplish it?",null,null
18,In this paper we propose a new principled approach to perform relevance-aware search results (post-)filtering via direct optimization of a given search quality metric. Our algorithm uses the ideas from dynamic programming and is guaranteed to deliver the optimal solution. The algorithm is presented in Section 3. The experiments on synthetic and real learning to rank (L2R) data sets are described in Section 4.,null,null
19,2. RELATED WORK,null,null
20,"This work is related to the research on search user behavior analysis, search metrics, and learning to rank. The proposed algorithm is based on the dynamic programming [1].",null,null
21,"Researchers studied the way people interact with search engines by analyzing mouse movements, eye-tracking and click logs. Joachims et al. [9] discovered the position bias phenomenon, i.e. the results at the first two positions receive most attention, and then it quickly drops. Plus, on average users tend to read the results in a linear order from top to bottom. Craswell et al. [4] explored how the position bias might arise and proposed four hypotheses and the corresponding probabilistic click models. They found that the ""cascade"" model, where users view results from top to bottom and leave as soon as they see a worthwhile document, is",null,null
22,"1Amazon, Walmart, Target, Etsy, BestBuy, NewEgg for products and Indeed, LinkedIn, SuperJob, Monster for jobs. 2we don't describe the exact setup due to the page limit.",null,null
23,979,null,null
24,"Figure 1: (A) Indeed.com resume search results for the query ""product manager"" sorted by ""date"" and (B) Amazon.com search results for the query ""bicycle"" sorted by ""price"". While sorting by relevance is accurate, the results sorted by the attribute value are hardly relevant for the query, which leads to poor user experience.",null,null
25,the best explanation for position bias in early ranks. Dupret et al. [5] generalized this model by allowing for the possibility that a user skips a document without examining it.,null,null
26,"Complementary to the work on search models, a lot of attention has been devoted to the design and analysis of search metrics. Thus, in addition to the traditional metrics, like the Precision and the Recall, J빠rvelin and Kek빠l빠inen proposed the (Normalized) Discounted Cumulative Gain (DCG) [8], Chapelle et al. -- the Expected Reciprocal Rank (ERR) [2], to name just a few. Recently, Chuklin et al. [3] developed a unified framework to convert any click model into the evaluation metric. Essentially, all search metrics model the position bias and penalize the top ranked irrelevant results.",null,null
27,"Numerous ranking algorithms have been developed to accurately predict the relevance of documents. Typically, these algorithms are based on machine learning and find the optimal parameters by optimizing the ""surrogate"" objective function. However, the solution to the approximation is not always optimal for the original ranking problem. Therefore, recently several approaches have been proposed that directly optimize a given search metric. For instance, Xu et al. [12] focus on the algorithms that optimize the objectives upperbounding the original non-smooth search metrics. Tan et al. [11] proposed DirectRank, which is based on the iterative coordinate ascent with the smart line search procedure.",null,null
28,"Attribute-based ranking, however, has been handled very differently. Rather than taking the relevance into account, search engines return the list of results sorted by the attribute value or suboptimal heuristics are used (Section 4.1). Inspired by the recent advancements in L2R, in this work we bridge the gap between the relevance-based ranking and the attribute-based ranking by proposing to do relevanceaware search results filtering, which directly optimizes a given search metric, when the sorting by the attribute value is requested. It is worth highlighting the difference between the proposed algorithm and a famous TA algorithm by Fagin et al. [6]. While TA algorithm finds the top-k most relevant tuples by scoring them individually, we return the tuples, which cumulatively optimize a given search quality metric. The ordering of the tuples is as crucial as their relevance.",null,null
29,3. OUR APPROACH,null,null
30,"We consider the scenario when a user requests the sorting of the search results by the attribute value, e.g. by date (Fig-",null,null
31,"ure 1, A) or by price (Figure 1, B). Our goal is to produce the final ranking that both satisfies the strict ordering constraints and optimizes a given search quality metric (in turn it minimizes the user's effort on finding relevant results). We only focus on the results filtering and assume that the relevance scores are already predicted by the ranking algorithm. Therefore, the formalization of our problem looks as follows.",null,null
32,"Input: a list of tuples {(ti, ri)}li,""1, where ti is the attribute value and ri  R+ is the relevance score predicted by the ranking algorithm; a search quality metric Q.""",null,null
33,"Output: a (sub)list of indices J delivering the maximum to the metric Q and totally ordered based on the attribute value, i.e. J ,"" arg max Q(rji |ji  J ), s.t. tj1 < ... < tjl .""",null,null
34,"Throughout the paper, we consider the DCG as the search quality metric (although ERR or other metric can be used), date as the attribute, and the input sorted chronologically. It is worth mentioning that the formalization above covers the post-filtering scenarios as well, i.e. the input might consist of tuples that passed some other filtering algorithm.",null,null
35,"Currently, this problem is solved heuristically. Mainly there are two approaches built around the same idea of thresholding. We can take only the results that have the relevance score above the threshold. We can also sort the results by the relevance score, take the top-k elements, and finally re-sort the list by date. While these approaches are easy to implement, they have two major drawbacks. First, it is not clear how to set the threshold. Second, the described approaches are the approximate solutions of our problem. Even the result set constructed from the top-k tuples sorted by relevance, being re-sorted by the attribute value, gets ordered randomly if we look at the relevance component.",null,null
36,"The solution that guarantees optimality is to enumerate all possible subsequences, compute the metric for each one, and take the best one. However, this approach is not tractable as the number of subsequences is exponential. We propose a polynomial algorithm based on the dynamic programming [1]. There are three key observations behind our algorithm: (1) natural enumeration order for subsequences; (2) additivity of the metric; (3) optimality of subproblems.",null,null
37,"First, all subsequences can be partitioned into the factor classes based on their length, i.e. in each factor class there will be the subsequences of the same length. To enumerate all subsequences, we can iterate over the factor classes and",null,null
38,980,null,null
39,Algorithm 1 (A1) Relevance-aware filtering of totally ordered set via direct optimization of a search quality metric,null,null
40,"Input: DCG and {(ti, ri)}li,""1, s.t. ti < ... < tl and ri  R+ Output: J "","" arg max DCG(rji |ji  J ), s.t. tj1 < ... < tjl 1: M  M atrix(l + 1, l + 1); M (:, 0)  0; M (0, :)  0;""",null,null
41,"2: P ath  M atrix(l + 1, l + 1); # to recover max sequence",null,null
42,"3: for i in 1, . . . , l",null,null
43,"4: for j in 1, . . . , i",null,null
44,5:,null,null
45,gain,null,null
46,2ri -1 log(j+1),null,null
47,;,null,null
48,"6: if M (i - 1, j - 1) + gain > M (i - 1, j)",null,null
49,7:,null,null
50,"M (i, j)  M (i - 1, j - 1) + gain;",null,null
51,8:,null,null
52,"P ath(i, j)  (i - 1, j - 1);",null,null
53,9: else,null,null
54,10:,null,null
55,"M (i, j)  M (i - 1, j);",null,null
56,11:,null,null
57,"P ath(i, j)  (i - 1, j);",null,null
58,"12: (i, j)  arg max M (l, :); # last element of solution",null,null
59,13: J  List(); J.append(j);,null,null
60,14: while i > 1 and j > 1,null,null
61,"15: if P (i, j).last < j",null,null
62,"16: J.append(P (i, j).last);",null,null
63,"17: (i, j)  P (i, j); # jump to shorter subsequence",null,null
64,18: return J.reverse(),null,null
65,"within each factor class enumerate all subsequences. Second, the search metrics are additive and can be computed in linear time from the beginning of the list to the end [2]. It means that having a partial metric value for the prefix, we can compute the new metric value by simply adding the gain/utility provided by the current element. Third, the optimal subsequence for the prefix of length k is one of the optimal subsequences from each of the factor classes for the prefix of length k - 1 with or without the current element appended (proof by induction for the prefix length).",null,null
66,"Combining the observations above, we present our algorithm and its analysis. It starts by initializing the memoization matrix to store the optimal DCG values for subproblems and the transition matrix to reconstruct the optimal subsequence. Then, it iterates over the prefixes of the input sequence in the outer loop and over the factor classes in the inner loop. The cell (i, j) is for the optimal subsequence of length j for the prefix of length i. At each step we decide whether we should append the current element of the input sequence i to the optimal subsequence of length j - 1 for the prefix of length i - 1 (the recursion on lines 6-11). If we append the current element, we go diagonal. If we don't ap-",null,null
67,"Figure 2: Dependencies in the memoization matrix, a legal evaluation order, and the optimal path.",null,null
68,"pend, we keep the existing optimal subsequence of length j and stay on the same column. A legal evaluation order and the dependencies between the cells are shown in Figure 2, A. Finally, to reconstruct the optimal subsequence, we find the maximum in the last row (the last element is always ""in"" since the elements are non-negative) and go backwards in the P ath matrix. If the line is diagonal, we take the element in the next cell. Otherwise, we skip. The P ath matrix is depicted in Figure 2, B. The complexity (both time and space) of the algorithm is O(l2) because we have two nested loops, costing us O(1) time at each iteration, and the square memoization matrices. It is guaranteed to deliver the optimum because we ""virtually"" enumerate all subsequences within the dynamic programming framework. For a toy example problem {(0, 0), (1, 3), (2, 1), (3, 2), (4, 1), (5, 3))} the optimal solution is {1, 3, 4, 5} with the DCG equal to 12.40.",null,null
69,4. EXPERIMENTS AND RESULTS,null,null
70,"In this section we study how our approach contributes to the ranking quality using two real LETOR [10] (MQ2007, MSLR-WEB10K) and synthetically generated data sets.",null,null
71,4.1 Learning to Rank Data Sets,null,null
72,"To answer our research questions, we do the simulations using the real learning to rank data sets. We extend MQ2007 and MSLR-WEB10K data sets by assigning a random timestamp to each document to model the sorting by the attribute value. Scikit-learn3 implementation of the Gradient Boosted Regression Trees (GBRT ) [7] is used to predict the relevance scores. The optimal parameters for the final GBRT model are picked using cross validation for each data set. We use the 5-fold cross validation partitioning from LETOR [10].",null,null
73,"Three popular baselines are considered, which are typically used to perform the filtering of the search results: Baseline 1 (B1): sort by the attribute value, no filtering; Baseline 2 (B2): sort by the attribute value, keep the results with the predicted relevance scores above the threshold (we normalized the scores to [0,1] and set the threshold,""0.5); Baseline 3 (B3): sort results by the predicted relevance score, take the top-k (where k is the cutoff point for the metric value calculation), and re-sort by the attribute value.""",null,null
74,"The evaluation procedure works as follows. First, we train the GBRT on the training folds. Second, we predict the relevance scores using the trained GBRT model for the documents in the testing fold. Third, we apply a baseline filtering algorithm to the documents in the testing fold by working with the relevance scores from the step two and the randomly generated timestamps. Fourth, we apply our filtering algorithm to the tuples that passed the baseline filtering. Finally, knowing the true relevance labels, we calculate the N DCG@k for the filtered result list sorted by the timestamp. To make sure that the conclusions are not due to randomness, we average the results from 1000 runs.",null,null
75,"The results of the experiment are presented in Table 1 and 2. We can see that the output (post-)filtered with our algorithm is regularly better than the baselines. We applied the binomial test and found that almost all differences in the N DCG values are statistically significant (marked in bold), p-value is below 0.001. One average the increase in the metric value is around 2-4%. Moreover, since the data sets used have very different characteristics (e.g. the average query",null,null
76,3http://scikit-learn.org/stable/index.html,null,null
77,981,null,null
78,NDCG B1 only A1  B1 B2 only A1  B2 B3 only A1  B3,null,null
79,@1 0.226 0.299 0.289 0.315 0.433 0.433,null,null
80,@5 0.245 0.287 0.318 0.326 0.417 0.417,null,null
81,@10 0.273 0.304 0.357 0.364 0.418 0.420,null,null
82,@20 0.336 0.363 0.448 0.453 0.451 0.455,null,null
83,@40 0.496 0.511 0.450 0.454 0.498 0.512,null,null
84,Table 1: The demonstration of effectiveness of the proposed approach on MQ2007 data set.,null,null
85,"length for MQ2007 is 40 and for MSLR-WEB10K -- 120),",null,null
86,the experiment suggests that the algorithm achieves good,null,null
87,"performance for a wide range of input problems. Yet, one",null,null
88,should note that the increase in the ranking quality comes,null,null
89,with the extra computational cost because the complexity of,null,null
90,our,null,null
91,algorithm,null,null
92,is,null,null
93,O(,null,null
94,l log,null,null
95,l,null,null
96,),null,null
97,times,null,null
98,larger,null,null
99,than,null,null
100,for,null,null
101,the,null,null
102,baselines.,null,null
103,4.2 Synthetic Data Sets,null,null
104,In this section we focus on the filtering only (both rele-,null,null
105,vance labels and timestamps are generated) and study how,null,null
106,the algorithm behavior changes for different input sizes and,null,null
107,relevance label distributions. We consider the following four,null,null
108,label distributions modeling the real situations: (a) uniform,null,null
109,"integer in the range [0, 5]; (b) uniform real in the range [0, 5];",null,null
110,"(c) power law,",null,null
111,the slope ,null,null
112,",",null,null
113,2.0;,null,null
114,(d),null,null
115,3x2 125,null,null
116,with,null,null
117,the,null,null
118,support,null,null
119,in,null,null
120,"the range [0, 5]. We generate the input lists for the filtering",null,null
121,algorithm by sampling from the corresponding distribution.,null,null
122,"Similarly to the previous experiment, we simulate each com-",null,null
123,bination of conditions 1000 times and average the runs. Only,null,null
124,the Baseline 1 is used in this experiment for simplicity. The,null,null
125,data from the simulation is presented in Figure 3.,null,null
126,There are several observations that could be made with,null,null
127,"the help of this figure. First, the output size is linearly pro-",null,null
128,"portional to the input size (Figure 3, C). DCG also grows",null,null
129,"linearly with the growth of the input size (Figure 3, A). Sec-",null,null
130,"ond, the proposed algorithm always outperforms the Base-",null,null
131,"line 1 (Figure 3, B), which is expected because we do the",null,null
132,filtering directly optimizing a given search quality metric.,null,null
133,"Third, both the graph for the ratio of the DCG values and",null,null
134,the graph for the ratio of the output sequence lengths for the,null,null
135,proposed algorithm and the baseline monotonically converge,null,null
136,"for the longer input lists (Figure 3, B and D). This means",null,null
137,Figure 3: The behavior of the algorithm (A1) for different input sizes and relevance label distributions.,null,null
138,NDCG B1 only A1  B1 B2 only A1  B2 B3 only A1  B3,null,null
139,@1 0.131 0.173 0.170 0.192 0.390 0.390,null,null
140,@5 0.161 0.183 0.208 0.215 0.362 0.362,null,null
141,@10 0.190 0.208 0.244 0.250 0.365 0.366,null,null
142,@20 0.236 0.250 0.300 0.304 0.380 0.382,null,null
143,@40 0.309 0.321 0.379 0.383 0.418 0.421,null,null
144,Table 2: The demonstration of effectiveness of the proposed approach on MSLR-WEB10K data set.,null,null
145,"that our algorithm works better when the original hit list is shorter. Fourth, higher gains in DCG over the baseline are characteristic for the relevance label distributions, where relevant results are more probable (Figure 3, B). The observations above are valid for non-degenerate cases, e.g. not all labels are the same or sorted in a special order.",null,null
146,5. CONCLUSIONS AND FUTURE WORK,null,null
147,"In this paper we addressed the important problem in search, that is, how to increase the relevance of the search results sorted by an attribute value. Our solution is based on the idea to perform relevance-aware search results filtering by directly optimizing a given search quality metric. We developed a simple, yet effective algorithm based on the dynamic programming, which consistently outperforms typically used heuristic approaches and is guaranteed to deliver the optimal solution. In the future, we plan to integrate the proposed algorithm in a real search engine and instrument an A/B test to see how such a modification will affect the user engagement and satisfaction with the search results.",null,null
148,6. ACKNOWLEDGEMENTS,null,null
149,"We thank Karrie Karahalios, ChengXiang Zhai, and anonymous reviewers for their valuable comments and suggestions.",null,null
150,7. REFERENCES,null,null
151,"[1] R. Bellman. Dynamic Programming. Dover Books on Computer Science, USA, 2003.",null,null
152,"[2] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. CIKM'09.",null,null
153,"[3] A. Chuklin, P. Serdyukov, and M. de Rijke. Click model-based information retrieval metrics. SIGIR'13.",null,null
154,"[4] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proceedings of Web Search and Data Mining 2008.",null,null
155,[5] G. E. Dupret and B. Piwowarski. A user browsing model to predict search engine click data from past observations. In Proceedings of ACM SIGIR 2008.,null,null
156,"[6] R. Fagin, A. Lotem, and M. Naor. Optimal aggregation algorithms for middleware. In Proceedings of PODS '01.",null,null
157,"[7] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 2001.",null,null
158,"[8] K. J빠rvelin and J. Kek빠l빠inen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4).",null,null
159,"[9] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately interpreting clickthrough data as implicit feedback. SIGIR'05.",null,null
160,"[10] T. Qin, T.-Y. Liu, J. Xu, and H. Li. LETOR: A benchmark collection for research on learning to rank for information retrieval. Inf. Retr., 13(4).",null,null
161,"[11] M. Tan, T. Xia, L. Guo, and S. Wang. Direct optimization of ranking measures for learning to rank models. KDD'13.",null,null
162,"[12] J. Xu, T.-Y. Liu, M. Lu, H. Li, and W.-Y. Ma. Directly optimizing evaluation measures in L2R. SIGIR'08.",null,null
163,982,null,null
164,,null,null

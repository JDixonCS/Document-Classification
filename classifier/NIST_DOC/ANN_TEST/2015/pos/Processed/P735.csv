,sentence,label,data
0,Document Comprehensiveness and User Preferences,null,null
1,in Novelty Search Tasks,null,null
2,Ashraf Bah,null,null
3,"University of Delaware Newark, Delaware, USA",null,null
4,ashraf@udel.edu,null,null
5,Praveen Chandar,null,null
6,"University of Delaware Newark, Delaware, USA",null,null
7,pcr@del.edu,null,null
8,Ben Carterette,null,null
9,"University of Delaware Newark, Delaware, USA",null,null
10,carteret@udel.edu,null,null
11,ABSTRACT,null,null
12,"Different users may be attempting to satisfy different information needs while providing the same query to a search engine. Addressing that issue is addressing Novelty and Diversity in information retrieval. Novelty and Diversity search models the retrieval task wherein users are interested in seeing documents that are not only relevant, but also cover more aspects (or subtopics) related to the topic of interest. This is in contrast with the traditional IR task in which topical relevance is the only factor in evaluating search results. In this paper, we conduct a user study where users are asked to give a preference between one of two documents B and C given a query and also given that they have already seen a document A. We then test a total of ten hypotheses pertaining to the relationship between the ""comprehensiveness"" of documents (i.e. the number of subtopics a document is relevant to) and real users' preference judgments. Our results show that users are inclined to prefer documents with higher comprehensiveness, even when the prior document A already covers more aspects than the two documents being compared, and even when the less preferred document has a higher relevance grade. In fact, users are inclined to prefer documents with higher overall aspect-coverage even in cases where B and C are relevant to the same number of novel subtopics.",null,null
13,Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval],null,null
14,Keywords: diversity; preference judgment; user study,null,null
15,1. INTRODUCTION,null,null
16,"In the recent past, more and more researchers in information retrieval (IR) evaluation have directed their attention to evaluation measures that account for both redundancy and ambiguity in search. Novelty aims at dealing with redundancy in search results, while diversity aims at handling ambiguity in queries. Research in that direction has seen such an interest that two different IR evaluation conferences have organized diversity retrieval tasks. The Text Retrieval Conference (TREC) included a diversity retrieval task as part of the Web track between 2009 and 2012 [8]. Unlike its ad-hoc counterpart in the same track, the diversity task judged documents based on subtopics as well as to the topic as a whole. In the same manner, the NTCIR11-IMine task incorporates a subtask focused on diversified document retrieval for which different user intents must be taken into account [11].",null,null
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09 - 13, 2015, Santiago, Chile Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767820",null,null
18,Popular evaluation metrics such as precision and recall do not account for intents and subtopics. Both assume binary relevance.,null,null
19,"Newer metrics go beyond simple binary relevance, and use graded relevance instead. The shortcoming of binary relevance addressed by graded relevance is that some documents are simply more useful to users than others. Two of the most widely used metrics that make use of graded judgments are nDCG [10] and ERR [7]. What these two metrics do not account for, however, are different user intents and subtopics.",null,null
20,"The simplest metric that accounts for different subtopics is subtopic recall [12]. Perhaps the more widely used measures in that category are -nDCG [9] and ERR-IA [6]. These measures produce specific hypotheses about user preferences, and their recapitulative idea is that a user is always interested in seeing more novel subtopics, with decreasing interest in seeing redundant subtopics. Although this idea is sound, we argue that there are other factors that impact users' preferences. Specifically, we adopt an experimental design described by Chandar and Carterette [4] to show that users are interested in seeing more subtopics, but also that users are biased towards more ""comprehensive"" documents-- that is, they prefer documents that cover the most subtopics regardless of how novel the subtopics are.",null,null
21,2. PREFERENCE JUDGMENTS,null,null
22,"Absolute judgments in IR such as Boolean relevance and graded relevance have been used widely in the literature. An alternative is pairwise preference judgments, in which an assessor is presented with two documents and gives a preference to one document over the other. Early work on preference judgments in IR involved inferring preferences from those absolute judgments [2]. However more recently, pairwise (i.e. binary) preference judgments have been adopted as an alternative that may offer advantages in terms of alleviating the burden on assessors by reducing the complexity of the assessment, rendering the assessment task easier than assigning grades and reducing assessor disagreement [3].",null,null
23,"The preference judgment scheme we adopt here, however, is a bit different and was first proposed by Chandar et al. [4]. This scheme is based on the so-called triplet framework, wherein given a query and a document A that the assessor is to pretend contains everything they know about the topics, the assessor chooses the next document they would like to see between two documents.",null,null
24,2.1 Data,null,null
25,"For our experiments, we use 10 queries from the TREC 2012 Web track dataset [8]. The 10 queries selected are a sample of broad queries (i.e. queries good for intrinsic diversity tasks). Documents were selected for preference assessment from those judged for",null,null
26,735,null,null
27,Figure 1. Screenshot of the HIT Layout,null,null
28,"the Web track. All documents are therefore from the ClueWeb09 collection of millions of web pages. We use the publicly available subtopic relevance judgments produced by experienced NIST assessors and based on graded relevance. We use the publicly available subtopic relevance judgments produced by experienced NIST assessors and based on graded relevance. Since documents were not judged for subtopic relevance, we consider the maximum subtopic relevance grade of a document to be its topical relevance. For each one of the 10 queries, we obtained triplets from several users. In the next section, we describe the experimental design that yielded the triplets.",null,null
29,2.2 Experimental Design,null,null
30,"The framework we adopt in our study for preference judgment is based on the work of Chandar and Carterette [4]. In the framework, an assessor (i.e. user) is shown three documents, one appearing at the top of the page, one appearing at the bottom left, and the third appearing at the bottom right. We will refer to the top document as DT, and the bottom ones as D1 and D2, in concordance with naming conventions of Chandar and Carterette. Given DT and a query, the assessor is asked to choose between D1 and D2. Essentially the assessor would need to indicate their preference for the second document they would like to see in a ranking by selecting either D1 or D2.",null,null
31,"Since we have graded topical and subtopic relevance judgments for the documents, we can use that information to determine what subtopics each document is relevant to, as well as the corresponding relevance grades. A document can thus be represented as the set of subtopics it has been judged relevant to. For instance, Di ,"" {Sj, Sk} means document i is relevant to subtopics j and k.""",null,null
32,"We used Amazon Mechanical Turk (AMT) [1]; an online labor marketplace to collect user judgments. AMT works as follow: a requestor creates a group of Human Intelligence Task (HITs) with various constraints and workers from the marketplace work to complete the tasks. Workers were instructed to assume that everything they know about the topic is in the top document, and they are now trying to find a document that would be most useful for learning more about the topic. No mentions of subtopics, novelty, or redundancy were given to them except as examples of properties assessors might take into account in their preferences (along with recency, ease of reading, and relevance). Each preference triplet consists of three documents, all of which were relevant to the topic; the documents were picked randomly from the data described in Section 2.1. One document appeared at the top followed by two documents below it. The HITs layout",null,null
33,"design, the quality control decisions and the HIT properties were the same as described by Carterette and Chandar [5]. Figure 1 shows an example of a triplet as it appeared in a HIT.",null,null
34,3. HYPOTHESES AND RESULTS,null,null
35,"In this section we enumerate some specific hypotheses about relationships between document ""comprehensiveness"", ad-hoc relevance, and user preferences. Our goal is to test the degree to which comprehensiveness (in the sense of covering more aspects in relevance judgments) is more important to users than relevance; in general, we hypothesize that it is the more important of the two factors in their preferences.",null,null
36,3.1 Comprehensiveness Hypotheses,null,null
37,"In the following, D1 > D2 means document D1 is preferred to document D2. R1 > R2 means document D1 was judged by NIST assessors to have a higher ad-hoc relevance grade than document D2. S1 > S2 means document D1 contains more subtopics than document D2. S1new > S2new means document D1 contains more novel subtopics (with respect to the subtopics already seen in DT) than document D2. Regardless of whether a document was placed on the left or right of a triplet, we will refer to the more comprehensive one as D1.",null,null
38,3.1.1 Hypotheses Set 1 (H1 through H3).,null,null
39,"This first set of hypotheses posits that a document D1 with higher aspect coverage, in general, tends to be preferred by users ­ regardless of whether D1 covers more novel aspects than the document it is being compared to. The three hypotheses are:",null,null
40,H1: If S1 > S2 and R1 > R2 then D1 > D2. This means users prefer a document with higher aspect coverage and higher ad-hoc relevance grade than a document with lower aspect coverage and lower ad-hoc relevance grade.,null,null
41,H2: If S1 > S2 and R1 < R2 then D1 > D2. This means users prefer a document with higher aspect coverage but lower ad-hoc relevance grade than a document with lower aspect coverage but higher ad-hoc relevance grade.,null,null
42,"H3: If S1 > S2 and R1 ,"" R2 then D1 > D2. This means that for documents with equal ad-hoc relevance grade, users prefer a document with higher aspect coverage than a document with lower aspect coverage.""",null,null
43,"We expected H1 to be largely true, and H2 and H3 to be more mitigated (or possibly inconclusive for H2) due to the fact that relevance grades are an important factor as well. The results in",null,null
44,736,null,null
45,"Table 1. Results for all the hypotheses, all results in the last column are significant (++) at p<0.01, except for H5 and H9",null,null
46,Q 152 Q 157 Q 158 Q 167 Q171 Q 173 Q178 Q 184 Q 196 Q199 All Q,null,null
47,H1 true/false 196/ 22 87/ 13 148/19 97/ 28 181/31 155/30 119/51 157/19 108/53 136/22 1384/288 (82.78% true)++,null,null
48,H2 true/false 0/0,null,null
49,0/0,null,null
50,4/1,null,null
51,10/2 5/0,null,null
52,0/0,null,null
53,1/1,null,null
54,4/4,null,null
55,0/0,null,null
56,6/1,null,null
57,30/9 (76.92% true)++,null,null
58,H3 true/false 10/15 0/0,null,null
59,33/13 18/6 12/1,null,null
60,6/11,null,null
61,14/11 21/14 18/15 12/14 144/90 (61.54% true)++,null,null
62,H4 true/false 40/3,null,null
63,20/2 49/14 31/6 42/15 82/21 37/17 56/8,null,null
64,59/32 41/10 457/128 (78.12% true)++,null,null
65,H5 true/false 0/0,null,null
66,0/0,null,null
67,0/0,null,null
68,0/0,null,null
69,0/0,null,null
70,0/0,null,null
71,1/1,null,null
72,2/2,null,null
73,0/0,null,null
74,6/1,null,null
75,9/4 (69.23% true),null,null
76,H6 true/false 4/1,null,null
77,0/0,null,null
78,21/4,null,null
79,0/0,null,null
80,0/0,null,null
81,2/7,null,null
82,6/1,null,null
83,1/6,null,null
84,5/2,null,null
85,8/6,null,null
86,47/27 (63.51% true)++,null,null
87,H7 true/false 181/21 67/11 97/5,null,null
88,79/24 149/18 73/9,null,null
89,105/50 115/11 49/21 91/11 1006/181 (84.75% true)++,null,null
90,H8 true/false 0/0,null,null
91,0/0,null,null
92,4/2,null,null
93,10/10 5/0,null,null
94,0/0,null,null
95,H9 true/false 10/5,null,null
96,0/0,null,null
97,9/24,null,null
98,18/6 12/1,null,null
99,4/4,null,null
100,1/1,null,null
101,2/2,null,null
102,13/17 16/8,null,null
103,0/0,null,null
104,0/0,null,null
105,13/13 4/8,null,null
106,22/7 (75.86% true)++ 99/86 (53.51% true),null,null
107,H10 true/false 15/1,null,null
108,20/2 63/18 20/5 32/13 84/28 18/5,null,null
109,49/16 64/34 59/18 424/140 (75.18% true)++,null,null
110,All H's T/F,null,null
111,293/68 194/28 400/100 283/87 306/79 406/110 315/155 423/90 316/170 363/91 3622/960++,null,null
112,"Table 1 support our hypotheses that, a document D1 with higher aspect coverage, in general, tend to be preferred by users ­ regardless of whether D1 covers more novel aspects than the document D2 it is being compared to. In fact, even H2 and H3 are true far more often than we expected them to be. According to the results, when D1 covers more aspects than D2 and D1 also has a higher ad-hoc relevance grade than D2, D1 was by far preferred by users. In our experiment this happened 1384 times (82.78%), and failed to happen 288 times (17.22%).",null,null
113,"The results also confirm H3 which posits that, for documents with equal ad-hoc relevance grade, users prefer the document with higher aspect coverage. And this happened 144 times (61.54%), and failed to happen 90 times (38.46%). H2 is also true more often than not, i.e. 30 times (76.92%) against 9 times (23.08%). The results, while proving H2 and H3 to be true, also suggest that when the least-comprehensive document has a higher relevance grade than the most-comprehensive document, the bias against the least-comprehensive document is reduced.",null,null
114,3.1.2 Hypotheses Set 2 (H4 through H6).,null,null
115,"The second set of hypotheses zooms into special cases where the prior document DT (i.e. document shown at the top) has higher aspect coverage than each of D1 and D2 and posits that, even then, the document with higher aspect coverage is preferred by users. The three hypotheses are:",null,null
116,"H4: If S1 > S2 | (ST > S1 and ST > S2) and R1 > R2 then D1 > D2. This means, given the prior document has higher aspect coverage than each of D1 and D2, users still prefer a document with higher aspect coverage and higher ad-hoc relevance grade than a document with lower aspect coverage and lower ad-hoc relevance grade.",null,null
117,"H5: If S1 > S2 | (ST > S1 and ST > S2) and R1 < R2 then D1 > D2. This means, given the prior document has higher aspect coverage than each of D1 and D2, users prefer a document with higher aspect coverage but lower ad-hoc relevance grade than a document with lower aspect coverage but higher ad-hoc relevance grade.",null,null
118,"H6: If S1 > S2 | (ST > S1 and ST > S2) and R1 ,"" R2 then D1 > D2. This means given the prior document has higher aspect coverage than each of D1 and D2, for documents with equal ad-hoc relevance grade, users prefer a document with higher aspect coverage than a document with lower aspect coverage.""",null,null
119,"The results in Table 1 support the claims made by Hypotheses H4 through H6. That is, even when the prior document DT has higher aspect coverage than each of D1 and D2, the document D1 with higher aspect coverage is preferred by users. And here again, whether documents D1 and D2 have the same relevance grade or not, it is the one with the highest aspect coverage that gets selected by users as most preferred. Although, as we expected, H5 and H6 are more mitigated than H4. H4 is very often true (in 78.12% of qualifying user preferences): given DT with higher aspect coverage than D1 and D2, the document D1 with higher aspect coverage and higher ad-hoc relevance grade is preferred. Also, H5 is often true (69.23% of qualifying user preferences, but this is not significant): given DT with higher aspect coverage than D1 and D2, the document D1 with higher aspect coverage but lower ad-hoc relevance grade is preferred. And finally, H6 is also often true (63.51% true and 36.49% false): given DT with higher aspect coverage than D1 and D2, the document D1 with higher aspect coverage but same ad-hoc relevance grade as the other document, is preferred. However the proportions in which H5 and H6 are true are not as strong as that of H4, which suggests that the bias against the leastcomprehensive document is reduced in the cases of H5 and H6.",null,null
120,3.1.3 Hypotheses Set 3 (H7 through H9).,null,null
121,"This second set of hypotheses focuses on ""novelcomprehensiveness"". Given a prior document DT, a document D1 is more ""novel-comprehensive"" than D2 if D1 covers more novel subtopics (with respect to the subtopics already seen in DT). We posit that a document D1 with higher novel aspect coverage, in general, tends to be preferred by users. The three hypotheses are:",null,null
122,H7: If S1new > S2new and R1 > R2 then D1 > D2. This means users prefer a document with higher novel-aspect coverage and higher ad-hoc relevance grade than a document with lower novel-aspect coverage and lower ad-hoc relevance grade.,null,null
123,H8: If S1new > S2new and R1 < R2 then D1 > D2. This means users prefer a document with higher novel-aspect coverage but lower ad-hoc relevance grade than a document with lower novel-aspect coverage but higher ad-hoc relevance grade.,null,null
124,"H9: If S1new > S2new and R1 ,"" R2 then D1 > D2. This means that for documents with equal ad-hoc relevance grade, users prefer a document with higher novel-aspect coverage than a document with lower novel-aspect coverage.""",null,null
125,737,null,null
126,"The results shown in Table 1 support hypotheses H7 through H9. In fact, H7 is true in 1006 cases (84.75%), and fails 181 times (15.25%). This means when D1 covers more novel aspects than D2 and D1 also has a higher ad-hoc relevance grade than D2, D1 was by far preferred by users. H9 is true in 99 cases (53.51%), and fails 86 times (46.49%). This means when D1 covers more novel aspects than D2 but has same ad-hoc relevance grade as D2, D1 was still preferred by users, but not significantly. Also, H8 is true only slightly more often than it is false. It is true 22 times (75.86%), and false 7 times (24.14%). These results",null,null
127,suggest that when the least novel-comprehensive document has,null,null
128,less than or equal ad-hoc relevance grade as the most novel-,null,null
129,"comprehensive document, the bias towards the most novel-",null,null
130,comprehensive document is reduced.,null,null
131,3.1.4 Hypothesis H10.,null,null
132,"This final hypothesis puts an emphasis on cases where the two documents being compared are equally ""novel-comprehensive"" ­ i.e. cover the same number of new subtopics ­ and posits that even in that case, users are more likely to prefer the one that covers the most number of subtopics.",null,null
133,"H10: If S1 > S2 | (S1new ,"" S2new) then D1 > D2. This means users are more likely to prefer a document that covers the most number of subtopics, even when both documents contain the same number of novel-aspects. In other words, users are biased towards more comprehensive documents, even in cases where both documents have the same number of novel-aspects.""",null,null
134,"The result for this hypothesis is perhaps the most interesting one. It shows that, even when the two documents being compared are relevant to an equal number of novel aspects, users are more inclined to choose the one with the highest overall subtopic coverage. And this happens in 75.18% of cases.",null,null
135,It should be noted that there are very few cases (17 cases) where the preferred document covers more aspects but fewer novelaspects; and even fewer cases (2 cases) where it contains more novel-aspects but fewer aspects.,null,null
136,100% 80%,null,null
137,60% 40% 20%,null,null
138,0%,null,null
139,H False H true,null,null
140,"Figure 2. Comparison of proportions in which hypotheses are true/false for three cases (considering all-pref, left-pref",null,null
141,only and right-pref only).,null,null
142,"It is important to note that, in most cases, the document with higher aspect coverage (i.e. more comprehensive) is either more relevant or equally relevant to the other document. There were not many cases where one of the document being compared is more comprehensive but with lower ad-hoc relevance. So those cases are underrepresented, possibly due to the fact that documents with high coverage tend to be very relevant.",null,null
143,"But is it the case that users prefer left docs to right docs (or vice versa) even when the preferred document has lower aspect coverage? That is, does the position of the document have an effect on it being preferred by a user? The results in Figure 2 suggest that is not the case. In fact, the proportions in which the hypotheses are true/false in both situations are relatively close. The triplets were indeed placed randomly in either left or right, that is, they are not placed according to any factor.",null,null
144,4. CONCLUSION AND FUTURE WORK,null,null
145,"In this paper, we have used the triplet framework to empirically show that users tend to prefer in large proportions documents with high aspect coverage, regardless of the topical relevance grade. We asked users to choose, given a prior document DT, between two documents D1 and D2 the one that is most useful for learning more about the topic. According to the results, users overwhelmingly prefer documents that are relevant to the largest number of aspects (i.e. highest aspect coverage), even when the prior document DT already covers more subtopics than each of D1 and D2. In fact, even in cases where D1 and D2 are relevant to the same number of novel subtopics, the one that is relevant to the largest overall subtopics tends to be preferred.",null,null
146,ACKNOWLEDGMENTS,null,null
147,"This material is based upon work supported by the National Science Foundation under Grant No. IIS-1350799. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",null,null
148,REFERENCES,null,null
149,[1] Amazon mechanical turk. http://www.mturk.com.,null,null
150,"[2] Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds, M., Hamilton, N., Hullen-der, G.: Learning to rank using gradient descent. In ICML. (2005)",null,null
151,"[3] Carterette, B., Bennett, P. N., Chickering, D. M., & Dumais, S. T.: Here or there. In ECIR. (2008).",null,null
152,"[4] Chandar, P. & Carterette, B.: What Qualities Do Users Prefer in Diversity Rankings? In Proc. WSDM Workshop on Diversity in Document Retrieval (2012)",null,null
153,"[5] Chandar, P., & Carterette, B.: Using preference judgments for novel document retrieval. In SIGIR. (2012)",null,null
154,"[6] Chapelle, O., Ji, S., Liao, C., Velipasaoglu, E., Lai, L., Wu, S. L.: Intent-based diversification of web search results: metrics and algorithms. IR14(6) (2011) 572-592",null,null
155,"[7] Chapelle, O., Metlzer, D., Zhang, Y., ...: Expected reciprocal rank for graded relevance. In CIKM. (2009)",null,null
156,"[8] Clarke, C. L., Craswell, N., & Soboroff, I.: Overview of the trec 2012 web track. In TREC (2012).",null,null
157,"[9] Clarke, C. L., Kolla, M., Cormack, G. V., Vechtomova, O., Ashkan, A., Büttcher, S., ...: Novelty and diversity in information retrieval evaluation. In SIGIR. (2008)",null,null
158,"[10] Järvelin, K., & Kekäläinen, J.: Cumulated gain-based evaluation of IR techniques. TOIS 20(4) (2002) 422-446",null,null
159,"[11] Liu, Y., Song, R., Zhang, M., Dou, Z., Yamamoto, T., ....: Overview of the ntcir-11 imine task. In NTCIR. (2014).",null,null
160,"[12] Zhai, C. X., Cohen, W. W., & Lafferty, J. (2003, July). Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR (2003)",null,null
161,738,null,null
162,,null,null

,sentence,label,data
0,A Word Embedding based Generalized Language Model for Information Retrieval,null,null
1,Debasis Ganguly,null,null
2,"ADAPT Centre, School of Computing Dublin City University Dublin, Ireland",null,null
3,dganguly@computing.dcu.ie,null,null
4,Mandar Mitra,null,null
5,CVPR Unit Indian Statistical Institute,null,null
6,"Kolkata, India",null,null
7,mandar@isical.ac.in,null,null
8,ABSTRACT,null,null
9,"Word2vec, a word embedding technique, has gained significant interest among researchers in natural language processing (NLP) in recent years. The embedding of the word vectors helps to identify a list of words that are used in similar contexts with respect to a given word. In this paper, we focus on the use of word embeddings for enhancing retrieval effectiveness. In particular, we construct a generalized language model, where the mutual independence between a pair of words (say t and t ) no longer holds. Instead, we make use of the vector embeddings of the words to derive the transformation probabilities between words. Specifically, the event of observing a term t in the query from a document d is modeled by two distinct events, that of generating a different term t , either from the document itself or from the collection, respectively, and then eventually transforming it to the observed query term t. The first event of generating an intermediate term from the document intends to capture how well a term fits contextually within a document, whereas the second one of generating it from the collection aims to address the vocabulary mismatch problem by taking into account other related terms in the collection. Our experiments, conducted on the standard TREC 6-8 ad hoc and Robust tasks, show that our proposed method yields significant improvements over language model (LM) and LDA-smoothed LM baselines.",null,null
10,Categories and Subject Descriptors,null,null
11,"H.3.3 [INFORMATION STORAGE AND RETRIEVAL]: Information Search and Retrieval--Retrieval models, Relevance Feedback, Query formulation",null,null
12,General Terms,null,null
13,"Theory, Experimentation",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'15, Aug 9­13, 2015, Santiago, Chile. Copyright 2015 ACM ISBN 978-1-4503-3621-5/15/08 ...$15.00. http://dx.doi.org/10.1145/2766462.2767780 .",null,null
15,Dwaipayan Roy,null,null
16,CVPR Unit Indian Statistical Institute,null,null
17,"Kolkata, India",null,null
18,dwaipayan_r@isical.ac.in,null,null
19,Gareth J.F. Jones,null,null
20,"ADAPT Centre School of Computing Dublin City University Dublin, Ireland",null,null
21,gjones@computing.dcu.ie,null,null
22,Keywords,null,null
23,"Generalized Language model, Word Embedding, Word2Vec",null,null
24,1. INTRODUCTION,null,null
25,"Word embedding as technique for representing the meaning of a word in terms other words, as exemplified by the Word2vec approach [7]. The embedding of the word vectors enables the identification of words that are used in similar contexts to a specufic word. a list of words that are used in similar contexts with respect to a given word. While word Embedding has gained significant interest among researchers in natural language processing (NLP) in recent years, there has to date been little exploration of the potential for use of these methods in information retrieval (IR).",null,null
26,This paper explores the use of word embeddings of enhance IR effectiveness. We begin with a brief introduction to word embedding techniques and then motivate how can these be applied in IR.,null,null
27,"A brief introduction to word embedding. Word embedding techniques seek to embed representations of words. For example, two vectors t and t , corresponding to the words t and t , are close in an abstract space of N dimensions if they have similar contexts and vice-versa, (i.e. the contexts in turn having similar words) [4]. Use of a cosine similarity measure on this abstract vector space of embedded words can be used to identify a list of words that are used in similar contexts with respect to a given word. These semantically related words may be used for various natural language processing (NLP) tasks. The general idea is to train moving windows with vector embeddings for the words (rather than training with the more conventional word count vectors), and classify the individual windows [2]. This finds application for examples in applications such as POS tagging, semantic role labeling, named-entity recognition and other tasks. The state-of-the-art word embedding approaches involve training deep neural networks with the help of negative sampling [7]. It is reported that this process of negative sampling (commonly known as word2vec1) produces reliable word embeddings in a very efficient manner [7].",null,null
28,"Potential use in IR. We now discuss how word embeddings can potentially be helpful in improving retrieval quality. In the context of IR, vocabulary mismatch, i.e. the inherent characteristic of using different but semantically similar terms across documents about",null,null
29,1The name word2vec comes from the name of the software tool released by Micholov et. al. (https://code.google.com/ p/word2vec/,null,null
30,795,null,null
31,"the same topic or between a query and its relevant documents, is a difficult problem to solve.",null,null
32,"However, the working principle of most standard retrieval models in IR involves an underlying assumption of term independence, e.g. the vector space model (VSM) assumes that the documents are embedded in a mutually orthogonal term space, while probabilistic models, such as the BM25 or the language model (LM) assume that the terms are sampled independently from documents. Standard approaches in IR take into account term association in two ways, one which involves a global analysis over the whole collection of documents (i.e. independent of the queries), while the other takes into account local co-occurrence information of terms in the top ranked documents retrieved in response to a query. The latter approach corresponds to the relevance feedback step in IR which we do not investigate in this paper. Existing global analysis methods such as the latent semantic indexing (LSA) [3] or latent Dirichlet allocation (LDA) [1] only take into account the co-occurrences between terms at the level of documents instead of considering the context of a term. Since the word embedding techniques that we introduced in the beginning of this section, leverage the information around the local context of each word to derive the embeddings (two words have close vector representations if and only if they are used in similar contexts), we believe that such an approach can potentially improve the global analysis technique of IR leading to better retrieval effectiveness.",null,null
33,"The rest of the paper is organized as follows. Section 2 discusses related work. In Section 3, we propose the generalized LM, which is evalaued in Section 4. Finally, Section 5 concludes the paper.",null,null
34,2. RELATED WORK,null,null
35,"Latent semantic analysis (LSA) [3] is a global analysis technique in which documents are represented in a term space of reduced dimensionality so as to take into account inter-term dependencies. More recent techniques such as the latent Dirichlet allocation (LDA) represent term dependencies by assuming that each term is generated from a set of latent variables called the topics [1]. A major problem of these approaches is that they only consider word co-occurrences at the level of documents to model term associations, which may not always be reliable. In contrast, the word embeddings take into account the local (window-based) context around the terms [7], and thus may lead to better modeling of the term dependencies.",null,null
36,"Moreover, most of these global analysis approaches, e.g. LDA, have been applied in IR in an ad-hoc way for re-assigning term weights without explicitly representing the term dependencies as an inherent part of an IR model. For example, an LDA document model (term sampling probabilities marginalized over a set of latent topic variables) is linearly added as a smoothing parameter to the standard LM probability [9], as a result of which the term dependencies are not clearly visible from the model definition. Contrastingly, in this paper, we intend to directly model the term dependencies as a part of an IR model.",null,null
37,3. A GENERALIZED LANGUAGE MODEL,null,null
38,"In this section, we propose the generalized language model (GLM) that models term dependencies using the vector embeddings of terms.",null,null
39,3.1 Language Modelling,null,null
40,"In LM, for a given query q, documents are returned as a ranked list sorted in decreasing order by the posterior probabilities P (d|q). These posterior probabilities are estimated for each document d during indexing time with the help of the prior probability (P (q|d))",null,null
41,"according to the Bayes rule [8, 6, 10].",null,null
42,P (q|d).P (d),null,null
43,"P (d|q) ,",null,null
44," P (q|d).P (d) , P (d). P (t|d)",null,null
45,d C P (q|d ).(d ),null,null
46,tq,null,null
47,", P^(t|d) + (1 - )P^(t|C) ,""  tf (t, d) + (1 - ) cf (t)""",null,null
48,|d|,null,null
49,cs,null,null
50,tq,null,null
51,tq,null,null
52,(1),null,null
53,"In Equation 1, the set C represents a universe of documents (commonly known as the collection), P^(t|d) and P^(t|C) denote the maximum likelihood estimated probabilities of generating a query term t from the document d and the collection respectively, using frequency statistics. The probabilities of these two (mutually exclusive) events are denoted by  and 1 -  respectively. The notations tf (t, d), |d|, cf (t) and cs denote the term frequency of term t in document d, the length of d, collection frequency of the term t and the total collection size respectively.",null,null
54,3.2 Term Transformation Events,null,null
55,"As per Equation 1, terms in a query are generated by sampling them independently from either the document or the collection. We propose the following generalization to the model. Instead of assuming that terms are mutually independent during the sampling process, we propose a generative process in which a noisy channel may transform (mutate) a term t into a term t. More concretely, if a term t is observed in the query corresponding to a document d, according to our model it may have occurred in three possible ways, shown as follows.",null,null
56,"· Direct term sampling: Standard LM term sampling, i.e. sampling a term t (without transformation) either from the document d or from the collection.",null,null
57,"· Transformation via Document Sampling: Sampling a term t (t , t) from d which is then transformed to t by a noisy channel.",null,null
58,· Transformation via Collection Sampling: Sampling the term t from the collection which is then transformed to t by the noisy channel.,null,null
59,"Transformation via Document Sampling. Let P (t, t |d) denote the probability of generating a term t from a document d and then transforming this term to t in the query.",null,null
60,"P (t, t |d) ,"" P (t|t , d)P (t |d)""",null,null
61,(2),null,null
62,"In Equation 2, P (t |d) can be estimated by maximum likelihood with the help of the standard term sampling method as shown in Equation 1. For the other part, i.e. transforming t to t, we make use of the cosine similarities between the two embedded vectors corresponding to t and t respectively. More precisely, this probability of selecting a term t, given the sampled term t , is proportional to the similarity of t with t . Note that this similarity is independent of the document d. This is shown in Equation 3, where sim(t, t ) is the cosine similarity between the vector representations of t and t and (d) is the sum of the similarity values between all term pairs occurring in document d, which being the normalization constant, can be pre-computed for each document d.",null,null
63,"sim(t, t )",null,null
64,"sim(t, t )",null,null
65,"P (t|t , d) ,",null,null
66,",",null,null
67,(3),null,null
68,"t d sim(t, t )",null,null
69,(d),null,null
70,"Consequently, we can write Equation 2 as",null,null
71,"sim(t , t) tf (t , d)",null,null
72,"P (t, t |d) , (d) |d|",null,null
73,(4),null,null
74,"Equation 4 favours those terms t s that are not only tend to co-occur with the query term t within d, but are also semantically related to",null,null
75,796,null,null
76,C d,null,null
77,1---,null,null
78,t,null,null
79,Noisy Channel,null,null
80,t,null,null
81,"Figure 1: Schematics of generating a query term t in our proposed Generalized Language Model (GLM). GLM degenerates to LM when  ,  , 0.",null,null
82,"it. Thus, words that are used in similar contexts with respect to the query term t over the collection, as predicted by the vector embeddings, are more likely to contribute to the term score of t. In other words, Equation 4 takes into account how well an observed query term t contextually fits into a document d. A term contextually fits well within a document if it co-occurs with other semantically similar terms. Terms, score high by Equation 4, potentially indicate a more relevant match for the query as compared to other terms with low values for this score.",null,null
83,"Transformation via Collection Sampling. Let the complementary event of transforming a term t , sampled from the collection instead of a particular document, to the observed query term t be denoted by P (t, t |C). This can be estimated as follows.",null,null
84,cf (t ),null,null
85,"P (t, t |C) ,"" P (t|t , C).P (t |C) "","" P (t|t , C). cs""",null,null
86,(5),null,null
87,"Now P (t|t , C) can be estimated in a way similar to computing P (t|t , d), as shown in Equation 3. However, instead of considering all (t, t ) pairs in the vocabulary for computation, it is reasonable to restrict the computation to a small neighbourhood of terms around the query term t, say Nt because taking too large a neighbourhood may lead to noisy term associations. This is shown in Equation 6.",null,null
88,"sim(t, t )",null,null
89,"sim(t, t )",null,null
90,"P (t|t , C) ,",null,null
91,",",null,null
92,(6),null,null
93,"t Nt sim(t, t )",null,null
94,(Nt),null,null
95,"While P (t, t |d) measures the contextual fitness of a term t in a document d with respect to its neighbouring (in the vector space of embedded terms) terms t in d, P (t, t |C), on the other hand, aims to alleviate the vocabulary mismatch between documents and queries in the sense that for each term t in d it expands the document with other related terms t s. From an implementation perspective, P (t, t |d) reweights existing document terms based on their contextual fit, whereas P (t, t |C) expands the document with additional terms with appropriate weights.",null,null
96,"Combining the Events. Finally, for putting all the events together in the LM generation model, let us assume that the probability of observing a query term t without the transformation process (as in standard LM) be . Let us denote the probability of sampling the query term t via a transformation through a term t sampled from the document d with , and let and the complementary probability of sampling t from the collection be then , as shown schematically in Figure 1. The LM term generation probability in this case can thus be written as shown in Equation 7. This is a generalized version of the standard LM, which we now henceforth refer to as generalized language model (GLM), that takes into account term relatedness with the help of the noisy channel transformation model, which in turn uses the word embeddings to derive the likelihood of term transformations. Note that the GLM degenerates to standard LM by setting  and  to zero, i.e. not using the",null,null
97,Table 1: Dataset Overview,null,null
98,TREC Qry Fields Qry Avg. qry Avg. #,null,null
99,disks set,null,null
100,Ids length rel. docs,null,null
101,TREC 6 title 301-350 2.48,null,null
102,4&5,null,null
103,TREC 7 TREC 8,null,null
104,title title,null,null
105,351-400 401-450,null,null
106,2.42 2.38,null,null
107,Robust title 601-700 2.88,null,null
108,92.22 93.48 94.56 37.20,null,null
109,transformation model in the term generation process.,null,null
110,"P (t|d) ,"" P (t|d) +  P (t, t |d)P (t )+""",null,null
111,t d,null,null
112,"(7)  P (t, t |C)P (t ) + (1 -  -  - )P (t|C)",null,null
113,t Nt,null,null
114,3.3 Implementation Outline,null,null
115,"An efficient approach to get the neighbours of a given term is to store a pre-computed list of nearest neighbours in memory for every word in the vocabulary. After this step, for each document d in the collection, we iterate over term pairs (t, t ) and assign a new term-weight to the term t representing the document sampling transformation according to Equation 4. Then we iterate again over every term t in d and use the pre-computed nearest neighbours of t (Nt) to compute a score for the collection sampling transformation, as shown in Equation 6. To account for the fact that these transformation probabilities are symmetrical, we add the term t to d. Note that it is not required to add the term t in case of the document sampling transformation event because t is already present in d.",null,null
116,4. EVALUATION,null,null
117,"Experimental Setup. Our experiments were conducted on the standard TREC ad hoc tasks from TREC 6, 7, 8 and the Robust track. Information about the document and the query sets is outlined in Table 1. We implemented GLM using the Lucene2 IR framework. As one of our baseline retrieval models, we used standard LM with Jelinek Mercer smoothing [6, 10], which is distributed as a part of Lucene. Additionally, we also used LM with LDA smoothing [9] as our second baseline to compare against. In contrast to [9], which reports retrieval results with LDA smoothed LM (LDA-LM) on individual document subsets (and their corresponding relevance judgements) from the TREC collection as categorized by their sources, i.e. the ""LA Times"" and the ""Financial Times"", we instead executed LDA on the whole TREC collection. The rationale for using LDA as a baseline is that analogous to our model, LDA also attempts to model term dependencies by taking into account latent variables (called the topics). This baseline was also implemented in Lucene.",null,null
118,"Parameters. The parameter  of the LM baseline was empirically set to 0.2 (after varying it within a range of [0.1, 0.9]). This value of  for the TREC collection agrees with the observations reported in [6]. According to the findings of [9], the number of topics in LDA, i.e. K, was set to 800. As prescribed in [5], we set the LDA hyper-parameters  and  (note that these are different from the GLM parameters) to 50/K and 0.01 respectively. Obtaining effective word embedding is an integral part of the GLM. The word embeddings for the experiments reported in this section were obtained on the TREC document collection with the parameter settings as prescribed in [7], i.e., we embedded the word vector in a 200 dimensional space, and used continuous bag-of-words",null,null
119,2http://lucene.apache.org/core/,null,null
120,797,null,null
121,MA P,null,null
122,0.2288,null,null
123,0.1955,null,null
124,0.2284 0.228,null,null
125,0.1945,null,null
126,MAP,null,null
127,0.2276,null,null
128,0.1935,null,null
129,0.2272,null,null
130,",0.1",null,null
131,",0.2",null,null
132,",0.1",null,null
133,",0.2",null,null
134,",0.3",null,null
135,",0.4",null,null
136,",0.3",null,null
137,",0.4",null,null
138,0.2268,null,null
139,0.1925,null,null
140,0.1,null,null
141,0.2,null,null
142,0.3,null,null
143,0.4,null,null
144,0.1,null,null
145,0.2,null,null
146,0.3,null,null
147,0.4,null,null
148,(a) TREC-6,null,null
149,(b) TREC-7,null,null
150,MAP,null,null
151,0.2505,null,null
152,0.2495,null,null
153,",0.1",null,null
154,",0.2",null,null
155,0.2485,null,null
156,",0.3",null,null
157,",0.4",null,null
158,0.1,null,null
159,0.2,null,null
160,0.3,null,null
161,0.4,null,null
162,(c) TREC-8,null,null
163,MAP,null,null
164,0.2865 0.2855 0.2845 0.2835,null,null
165,",0.1 ,0.3",null,null
166,0.1,null,null
167,0.2,null,null
168,0.3,null,null
169,(d) Robust,null,null
170,",0.2 ,0.7",null,null
171,0.4,null,null
172,Figure 2: Effect of varying the GLM parameters  and  on the MAP values for the TREC query sets.,null,null
173,"with negative sampling. The neighbourhood Nt of the GLM (see Equation 7) was set to 3, i.e., for each given term in a document, we consider adding at most 3 related terms from the collection.",null,null
174,"Results. First, we varied the GLM parameters, namely  and  within the range [0.1, 0.4] so as to ensure that  +  +  < 1 ( being set to 0.2) for all the query sets used in our experiments. The results are shown in Figure 2. It can be seen that the optimal values of  and  depend on the query set, e.g. for the TREC 8 query set (Figure 2c, the optimal results are obtained for (, ) ,"" (0.3, 0.2), whereas this combination does not produce the optimal results for the other query sets. It can be observed that a reasonable choice for these parameters is in the range [0.2, 0.3], which means imparting more or less uniform weights to all the term generation events, namely ,  and . In Table 2, we show the optimal results obtained with GLM for each individual query set and compare the results with the baselines, i.e. the LM and the LDA-LM. It can be observed that for each query set, GLM significantly3 outperforms the baselines. It turns out that the LDA-LM (almost) consistently outperforms the standard LM. However, the results (as measured by the percentage gains in comparison to standard LM) do not seem to be as high as reported in [9] (about 3% as compared to about 8%). We believe that the reason for this is due to the diversity in the LDA topics caused by the news articles from different sources.""",null,null
175,"From Table 2, we observe that GLM consistently and significantly outperforms both LM and LDA-LM for all the query sets. Not only does it increase the recall values in comparison to LM, but it also increases precision at top ranks by always outperforming LDA in terms of MAP. Although LDA achieves higher recall than GLM in two cases (TREC-6 and Robust), the higher recall in the case of LDA does not significantly increase the MAP, which is indicative of the fact that the precision at top ranks does not improve. For GLM however, an increase in the recall value is always associated with a significant increase in MAP as well, which indicates that precision at top ranks remains relatively stable in comparison to LDA.",null,null
176,5. CONCLUSIONS AND FUTURE WORK,null,null
177,We proposed a generalized version of the language model for IR. Our model considers two possible cases of generating a term from,null,null
178,3Measured by Wilcoxon statistical significance test with 95% confidence.,null,null
179,"Table 2: Comparative performance of LM, LDA and GLM on the TREC query sets.",null,null
180,Metrics,null,null
181,Topic Set Method,null,null
182,MAP GMAP Recall,null,null
183,TREC-6,null,null
184,LM LDA-LM GLM,null,null
185,0.2148 0.2192 0.2287,null,null
186,0.0761 0.0790 0.0956,null,null
187,0.4778 0.5333 0.5020,null,null
188,TREC-7,null,null
189,LM LDA-LM GLM,null,null
190,0.1771 0.1631 0.1958,null,null
191,0.0706 0.0693 0.0867,null,null
192,0.4867 0.4854 0.5021,null,null
193,TREC-8,null,null
194,LM LDA-LM GLM,null,null
195,0.2357 0.2428 0.2503,null,null
196,0.1316 0.1471 0.1492,null,null
197,0.5895 0.5833 0.6246,null,null
198,Robust,null,null
199,LM LDA-LM GLM,null,null
200,0.2555 0.2623 0.2864,null,null
201,0.1290 0.1712 0.1656,null,null
202,0.7715 0.8005 0.7967,null,null
203,"either a document or the collection and then changing it to another term after passing it through a noisy channel. The term transformation probabilities of the noisy channel, in turn, are computed by making use of the distances between the word vectors embedded in an abstract space. We argue that this model has two fold advantage, firstly it is able to estimate how well a term fits in the context of a document, and secondly it is able to decrease the vocabulary gap by adding other useful terms to a document. Empirical evaluation shows that our method significantly outperforms the standard LM and LDA-LM. Possible future work will be to investigate compositionality of terms from the vector embeddings of words.",null,null
204,"Acknowledgement. This research is supported by SFI through the CNGL Programme (Grant No: 12/CE/I2267) in the ADAPT Centre (www.adaptcentre.ie) at Dublin City University, and by a grant under the SFI ISCA India consortium.",null,null
205,6. REFERENCES,null,null
206,"[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993­1022, March 2003.",null,null
207,"[2] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493­2537, Nov. 2011.",null,null
208,"[3] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. JASIS, 41(6):391­407, 1990.",null,null
209,"[4] Y. Goldberg and O. Levy. word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method. CoRR, abs/1402.3722, 2014.",null,null
210,"[5] T. L. Griffiths and M. Steyvers. Finding scientific topics. Proceedings of the National Academy of Sciences (PNAS), 101(suppl. 1):5228­5235, 2004.",null,null
211,"[6] D. Hiemstra. Using Language Models for Information Retrieval. PhD thesis, Center of Telematics and Information Technology, AE Enschede, 2000.",null,null
212,"[7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Proc. of NIPS '13, pages 3111­3119, 2013.",null,null
213,"[8] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, pages 275­281. ACM, 1998.",null,null
214,"[9] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR '06, pages 178­185, 2006.",null,null
215,"[10] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.",null,null
216,798,null,null
217,,null,null

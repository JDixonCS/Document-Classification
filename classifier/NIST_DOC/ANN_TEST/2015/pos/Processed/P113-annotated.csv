,sentence,label,data
,,,
0,Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures,null,null
,,,
1,Long Xia Jun Xu Yanyan Lan Jiafeng Guo Xueqi Cheng,null,null
,,,
2,"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences",null,null
,,,
3,"xialong@software.ict.ac.cn, {junxu, lanyanyan, guojiafeng, cxq}@ict.ac.cn",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"In this paper we address the issue of learning a ranking model for search result diversification. In the task, a model concerns with both query-document relevance and document diversity is automatically created with training data. Ideally a diverse ranking model would be designed to meet the criterion of maximal marginal relevance, for selecting documents that have the least similarity to previously selected documents. Also, an ideal learning algorithm for diverse ranking would train a ranking model that could directly optimize the diversity evaluation measures with respect to the training data. Existing methods, however, either fail to model the marginal relevance, or train ranking models by minimizing loss functions that loosely related to the evaluation measures. To deal with the problem, we propose a novel learning algorithm under the framework of Perceptron, which adopts the ranking model that maximizes marginal relevance at ranking and can optimize any diversity evaluation measure in training. The algorithm, referred to as PAMM (Perceptron Algorithm using Measures as Margins), first constructs positive and negative diverse rankings for each training query, and then repeatedly adjusts the model parameters so that the margins between the positive and negative rankings are maximized. Experimental results on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods.",null,null
,,,
6,Categories and Subject Descriptors,null,null
,,,
7,H.3.3 [Information Search and Retrieval]: Information Search and Retrieval ­ Retrieval Models,null,null
,,,
8,General Terms,null,null
,,,
9,Algorithms,null,null
,,,
10,Keywords,null,null
,,,
11,search result diversification; maximal marginal relevance; directly optimizing evaluation measures,null,null
,,,
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",null,null
,,,
13,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,null,null
,,,
14,DOI: http://dx.doi.org/10.1145/2766462.2767710.,null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"It has been widely observed that users' information needs, described by keyword based queries, are often ambiguous or multi-faceted. It is important for commercial search engines to provide search results which balance query-document relevance and document diversity, called search result diversification [1, 30]. One of the key problems in search result diversification is ranking, specifically, how to develop a ranking model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents.",null,null
,,,
17,"Methods for search result diversification can be categorized into heuristic approaches and learning approaches. The heuristic approaches construct diverse rankings with handcrafted ranking rules. As a representative method in the category, Carbonell and Goldstein [2] propose the maximal marginal relevance (MMR) criterion for guiding the construction ranking models. In MMR, constructing of a diverse ranking is formulated as a process of sequential document selection. At each iteration, the document with the highest marginal relevance is selected. The marginal relevance can be defined as, for example, a linear combination of the querydocument relevance and the maximum distance of the document to the selected document set. A number of approaches have been proposed [8, 23, 24, 25] on the basis of the criterion and promising results have been achieved. User studies also shows that the user browsing behavior matches very well with the maximal marginal relevance criterion: usually users browse the web search results in a top-down manner, and perceive diverse information from each individual document based on what they have obtained in the preceding results [5]. Therefore, in a certain sense, we can say that maximal marginal relevance has been widely accepted as a criterion for guiding the construction of diverse ranking models.",null,null
,,,
18,"Recently, machine learning approaches have been proposed for the task of search result diversification [14, 20, 22, 29, 31], especially the methods that can directly optimize evaluation measures on training data [16, 28]. Yue and Joachims [28] propose SVM-DIV which formulates the task as a problem of structured output prediction. In the model, the measure of subtopic diversity is directly optimized under the structural SVM framework. Liang et al. [16] propose to conduct personalized search result diversification via directly optimizing the measure of -NDCG, also under the structural SVM framework. All of these methods try to resolve the mismatch between the objective function used in training and the final evaluation measure used in testing. Experimen-",null,null
,,,
19,113,null,null
,,,
20,"tal results also showed that directly optimizing the diversity evaluation measures can indeed improve the diverse ranking performances [16, 28]. One problem with the direct optimization approaches is that it is hard, if not impossible, to define a ranking model that can meet the maximal marginal relevance criterion under the direct optimization framework.",null,null
,,,
21,"In this paper, we aim to develop a new learning algorithm that utilizes the maximal marginal relevance model for ranking as well as can directly optimize any diversity evaluation measure in training. Inspired by the work of R-LTR [31] and Perceptron variations [7, 15], we propose a new algorithm for search result diversification, referred to as PAMM (Perceptron Algorithm using Measures as Margins). PAMM utilizes a sequential document selection process as its ranking model. In learning, it first generates positive rankings (ground truth rankings) and negative rankings for the training queries. It then repeats the process of estimating the probabilities for the rankings, calculating the margins between the positive rankings and negative rankings in terms of the ranking probabilities, and updating the model parameters so that the margins are maximized. We show that PAMM algorithm minimizes an upper bound of the loss function that directly defined over the diversity evaluation measures.",null,null
,,,
22,PAMM offers several advantages: 1) adopting the ranking model that meets the maximal marginal relevance criterion; 2) ability to directly optimize any diversity evaluation measure in training; 3) ability to use both positive rankings and negative rankings in training.,null,null
,,,
23,"To evaluate the effectiveness of PAMM, we conducted extensive experiments on three public TREC benchmark datasets. The experimental results showed that our methods significantly outperform the state-of-the-art diverse ranking approaches including MMR, SVM-DIV, and R-LTR. We analyzed the results and showed that PAMM makes a good balance between the relevance and diversity via maximizing marginal relevance in ranking. We also showed that by directly optimizing a measure in training, PAMM can indeed enhance the ranking performances in terms of the measure.",null,null
,,,
24,"The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the general framework of learning maximal marginal relevance model in Section 3. In Section 4 we discuss the proposed PAMM algorithm. Experimental results and discussions are given in Section 5. Section 6 concludes this paper and gives future work.",null,null
,,,
25,2. RELATED WORK,null,null
,,,
26,Methods of search result diversification can be categorized into heuristic approaches and learning approaches.,null,null
,,,
27,2.1 Heuristic approaches,null,null
,,,
28,"It is a common practice to use heuristic rules to construct a diverse ranking list in search. Usually, the rules are created based on the observation that in diverse ranking a document's novelty depends on not only the document itself but also the documents ranked in previous positions. Carbonell and Goldstein [2] propose the maximal marginal relevance criterion to guide the design of diverse ranking models. The criterion is implemented with a process of iteratively selecting the documents from the candidate document set. At each iteration, the document with the highest marginal relevance score is selected, where the score is a linear combination of the query-document relevance and the maximum",null,null
,,,
29,"distance of the document to the documents in current result set. The marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. More methods have been developed under the criterion. PM-2 [8] treats the problem of finding a diverse search result as finding a proportional representation for the document ranking. xQuAD [25] directly models different aspects underlying the original query in the form of sub-queries, and estimates the relevance of the retrieved documents to each identified sub-query. See also [3, 9, 10, 11, 21]",null,null
,,,
30,"Heuristic approaches rely on the utility functions that can only use a limited number of ranking signals. Also, the parameter tuning cost is high, especially in complex search settings. In this paper, we propose a learning approach to construct diverse ranking models that can meet the maximal marginal relevance criterion.",null,null
,,,
31,2.2 Learning approaches,null,null
,,,
32,"Methods of machine learning have been applied to search result diversification. In the approaches, rich features can be utilized and the parameters are automatically estimated from the training data. Some promising results have been obtained. For example, Zhu et al. [31] proposed the relational learning to rank model (R-LTR) in which the diverse ranking is constructed with a process of sequential document selection. The training of R-LTR amounts to optimizing the likelihood of ground truth rankings. More work please refer to [14, 20, 22, 29]. All these methods, however, formulate the learning problem as optimizing loss function that loosely related to diversity evaluation measures.",null,null
,,,
33,"Recently methods that can directly optimize evaluation measures have been proposed and applied to search result diversification. Yue and Joachims [28] formulate the task of constructing a diverse ranking as a problem of predicting diverse subsets. Structural SVM framework is adopted to perform the training. Liang et al. [16] propose to conduct personalized search result diversification, also under the structural SVM framework. In the model, the loss function is defined based on the diversity evaluation measure of -NDCG. Thus, the algorithm can be considered as directly optimizing -NDCG in training. One issue with the approach is that it is hard to learn a maximal marginal relevance model under the structural SVM framework.",null,null
,,,
34,"In this paper, we propose a Perceptron algorithm that can learn a maximal marginal relevance model, at the same time directly optimizing diversity evaluation measures.",null,null
,,,
35,3. LEARNING MAXIMAL MARGINAL RELEVANCE MODEL,null,null
,,,
36,We first describe the general framework of learning maximal marginal relevance model for search result diversification.,null,null
,,,
37,3.1 Maximal marginal relevance model,null,null
,,,
38,"Suppose that we are given a query q, which is associated with a set of retrieved documents X ,"" {x1, · · · , xM }, where each document xi is represented as a D-dimensional relevance feature vector. Let R "","" RM×M×K denotes a 3-way tensor representing relationship among the M documents, where Rijk stands for the k-th relationship feature of document xi and document xj.""",null,null
,,,
39,114,null,null
,,,
40,Algorithm 1 Ranking via maximizing marginal relevance,null,null
,,,
41,"Input: documents X, document relation R, and ranking model parameters r and d",null,null
,,,
42,"Output: ranking y 1: S0   2: for r ,"" 1, · · · , M do 3: y(r)  arg maxj:xj X\Sr-1 fSr-1 (xj , Rj ) 4: Sr  Sr-1  {xy(r)} 5: end for 6: return y""",null,null
,,,
43,"The maximal marginal relevance model creates a diverse ranking over X with a process of sequential document selection. At each step, the document with the highest marginal relevance is selected and added to the tail of the list [31]. Specifically, let S  X be the set of documents have been selected for query q at one of the document selection step. Given S, the marginal relevance score of each document xi  X\S at current step is defined as a linear combination of the query-document relevance and diversity of the document to the documents in S:",null,null
,,,
44,"fS (xi, Ri) ,"" rT xi + dT hS (Ri),""",null,null
,,,
45,-1,null,null
,,,
46,"where xi denotes the relevance feature vector of the document, Ri  RM×K is the matrix representation of the relationship between document xi and the other documents (note that Rij  RK denotes the relationship feature vector of document pair (xi, xj)), and r and d are the weights for the relevance features and diversity features, respectively.",null,null
,,,
47,The first term in Equation (1) represents the relevance of,null,null
,,,
48,"document xi to the query and the second term represents the diversity of xi w.r.t. documents in S. Following the practice in [31], the relational function hS(Ri) is defined as the minimal distance:",null,null
,,,
49,"hS (Ri) ,"" min Rij1, · · · , min RijK .""",null,null
,,,
50,xj S,null,null
,,,
51,xj S,null,null
,,,
52,"According to the maximal marginal relevance criterion, sequential document selection process can be used to create a diverse ranking, as shown in Algorithm 1. Specifically, given a query q, the retrieved documents X, and document relationship R, the algorithm initializes S0 as an empty set. It then iteratively selects the documents from the candidate set. At iteration r (r ,"" 1, · · · , M ), the document with the maximal marginal relevance score fSr-1 is selected and ranked at position r. At the same time, the selected document is inserted to Sr-1.""",null,null
,,,
53,3.2 Learning the ranking model,null,null
,,,
54,Machine learning approaches can be used to learn the,null,null
,,,
55,"maximal marginal relevance model. Suppose we are given N labeled training queries {(X(n), R(n), J (n))}Nn,""1, where J (n) denotes the human labels on the documents, in the form of a binary matrix. J(n)(i, s) "","" 1 if document x(in) contains the s-th subtopic of qn and 0 otherwise1. The learning process, thus, amounts to minimize the loss over all of the training""",null,null
,,,
56,queries:,null,null
,,,
57,N,null,null
,,,
58,min,null,null
,,,
59,"L y^(n), J (n) ,",null,null
,,,
60,-2,null,null
,,,
61,"r ,d n,1",null,null
,,,
62,"1Some datasets also use graded judgements. In this paper,",null,null
,,,
63,we assume that all labels are binary.,null,null
,,,
64,Table 1: Summary of notations.,null,null
,,,
65,Notations,null,null
,,,
66,Explanations,null,null
,,,
67,q,null,null
,,,
68,"X ,"" {x1, · · · , xM } xi  RD R  RM×M×K""",null,null
,,,
69,Y,null,null
,,,
70,yY,null,null
,,,
71,"y(t)  {1, · · · , M }",null,null
,,,
72,"Sr  X fS (xi, Ri) hS (Ri) d r J",null,null
,,,
73,"E(X, y, J)  [0, 1]",null,null
,,,
74,query list of documents for q,null,null
,,,
75,document relevant feature vector,null,null
,,,
76,relationship tensor among M documents set of rankings over documents the ranking of documents index of the document ranked at t selected documents before iteration r the scoring function at each step the relational function on Ri weights for relevance features weights for diversity features human labels on document subtopics diversity evaluation measure,null,null
,,,
77,"where y^(n) is the ranking constructed by the maximal marginal relevance model (Algorithm 1) for documents X(n), and L(y^(n), J(n)) is the function for judging the `loss' of the predicted ranking y(n) compared with the human labels J(n).",null,null
,,,
78,3.3 Diversity evaluation measures,null,null
,,,
79,"In search result diversification, query level evaluation measures are used to evaluate the `goodness' of a ranking model. These measures include -NDCG [5], ERR-IA [4], and NRBP [6] etc. We utilize a general function E(X, y, J)  [0, 1] to represent the evaluation measures. The first argument of E is the set of candidate documents, the second argument is a ranking y over documents in X, and the third argument is the human judgements. E measures the agreement between y and J.",null,null
,,,
80,"As an example of diversity evaluation measures, -NDCG [5] is a variation of NDCG [13] in which the newly found subtopics are rewarded and redundant subtopics are penalized. The -NDCG score at rank k can be defined by replacing the raw gain values in standard NDCG@k with novelty-baised gains:",null,null
,,,
81,"-NDCG@k ,",null,null
,,,
82,"k r,1",null,null
,,,
83,N,null,null
,,,
84,G(r)/,null,null
,,,
85,log(r,null,null
,,,
86,+,null,null
,,,
87,1),null,null
,,,
88,"k r,1",null,null
,,,
89,N,null,null
,,,
90,G,null,null
,,,
91,(r)/,null,null
,,,
92,log(r,null,null
,,,
93,+,null,null
,,,
94,1),null,null
,,,
95,",",null,null
,,,
96,-3,null,null
,,,
97,"where N G(r) ,"" s J (y(r), s)(1 - )Cs(r-1) is the novelty-""",null,null
,,,
98,"biased gain at rank r in ranking y, Cs(r-1) ,",null,null
,,,
99,"r-1 k,1",null,null
,,,
100,J,null,null
,,,
101,"(y(k),",null,null
,,,
102,s),null,null
,,,
103,denotes the number of documents observed within top r - 1,null,null
,,,
104,"that contain the s-th subtopic, N G(r) is the novelty-biased",null,null
,,,
105,"gain at rank r in a positive ranking, and y(k) denotes the",null,null
,,,
106,index of the document ranked at k. Usually the parameter,null,null
,,,
107, is set to 0.5.,null,null
,,,
108,ERR-IA [4] is another popular used diversity evaluation,null,null
,,,
109,measure. Given a query with several different subtopics,null,null
,,,
110,"s, the probability of each intent Pr(s|q) can be estimated,",null,null
,,,
111,"where s Pr(s|q) , 1. The intent-aware ERR at rank k can be computed as:",null,null
,,,
112,"ERR-IA@k ,"" Pr(s|q)ERR@k(s),""",null,null
,,,
113,-4,null,null
,,,
114,s,null,null
,,,
115,where ERR@k(s) is the expected reciprocal rank score at k in terms of subtopic s.,null,null
,,,
116,Table 1 gives a summary of the notations described above.,null,null
,,,
117,115,null,null
,,,
118,4. OUR APPROACH: PAMM,null,null
,,,
119,4.1 Evaluation measure as loss function,null,null
,,,
120,"We aim to maximize the diverse ranking accuracy in terms of a diversity evaluation measure on the training data. Thus, the loss function in Equation (2) becomes",null,null
,,,
121,N,null,null
,,,
122,"1 - E(X(n), y^(n), J (n)) .",null,null
,,,
123,-5,null,null
,,,
124,"n,1",null,null
,,,
125,It is difficult to directly optimize the loss as E is a nonconvex function.,null,null
,,,
126,"We resort to optimize the upper bound of the loss function under the framework of structured output prediction. According to Theorem (2) in [27], we know that the loss function defined in Equation (5) can be upper bounded by the function defined over the ranking pairs:",null,null
,,,
127,N,null,null
,,,
128,"max E(X(n),y+,J (n))-E(X(n),y-,J (n)) ·",null,null
,,,
129,"n,1",null,null
,,,
130,y+ Y+(n) ; y- Y-(n),null,null
,,,
131,"F (y+, X(n), R(n))  F (y-, X(n), R(n)) , (6)",null,null
,,,
132,"where Y+(n) is the set of all possible `positive' rankings (rankings whose -NDCG/ERR-IA equals to one) for the n-th query, Y-(n) is the set of all possible `negative' rankings (rankings whose -NDCG/ERR-IA is less than one) for the n-the query, · is one if the condition is satisfied otherwise zero, and F (X, R, y) is the query level ranking model. F takes the document set X, document relationship R, and ranking over the document y as inputs. The output of F is the confidence score of the ranking y. The predicted y^(n) in Equation (5) can be considered as the ranking that maximizes F :",null,null
,,,
133,"y^(n) ,"" arg max F (X(n), R(n), y),""",null,null
,,,
134,-7,null,null
,,,
135,yY (n),null,null
,,,
136,"where Y(n) is the set of all possible rankings over X(n). Here F is defined as the probability of generating the ranking list y with a process of iteratively selecting the top ranked documents from the remaining documents, and using the marginal relevance function fS in Equation (1) as the selection criterion:",null,null
,,,
137,"F (X, R, y) ,""Pr(y|X, R)""",null,null
,,,
138,",""Pr(xy(1) · · · xy(M)|X, R)""",null,null
,,,
139,M -1,null,null
,,,
140,",",null,null
,,,
141,"Pr(xy(r)|X, Sr-1, R)",null,null
,,,
142,-8,null,null
,,,
143,"r,1",null,null
,,,
144,M -1,null,null
,,,
145,",",null,null
,,,
146,"r,1",null,null
,,,
147,"exp{fSr-1 (xi, Ry(r))}",null,null
,,,
148,"M k,r",null,null
,,,
149,exp{fSr-1,null,null
,,,
150,"(xi,",null,null
,,,
151,Ry(k))},null,null
,,,
152,"where y(r) denotes the index of the document ranked at the r-th position in y, Sr-1 , {xy(k)}rk-,""11 is the documents ranked at the top r - 1 positions in y, fSr-1 (xi, Ri) is the marginal relevance score of document xi w.r.t. the selected documents in Sr-1, and S0 "",""  is an empty set. With the definition of F , it is obvious that the maximal marginal""",null,null
,,,
153,relevance process of Algorithm 1 actually greedily searches,null,null
,,,
154,the solution for optimizing the problem of Equation (7).,null,null
,,,
155,To conduct the optimization under the Perceptron frame-,null,null
,,,
156,"work, the upper bound of Equation (6) is further relaxed,",null,null
,,,
157,by replacing the max with sum and moving the term,null,null
,,,
158,"(E(X(n), y+, J (n)) - E(X(n), y-, J (n))) into · as margin. The upper bound of Equation (6) becomes:",null,null
,,,
159,N,null,null
,,,
160,"F (X(n), R(n), y+) - F (X(n), R(n), y-) ",null,null
,,,
161,"n,1y+ ;y-",null,null
,,,
162,"E(X(n),y+,J (n))-E(X(n),y-,J (n)) . (9)",null,null
,,,
163,"This is because i xi  maxi xi if xi  0 holds for all i, and x - y  z  z · x  y holds if z  [0, 1]. Please note that we assume E(X, y+, J)  [0, 1] and thus we have (E(X(n), y+, J (n)) - E(X(n), y-, J (n)))  [0, 1] because E(X(n), y+, J (n)) > E(X(n), y-, J (n)).",null,null
,,,
164,4.2 Direct optimization with Perceptron,null,null
,,,
165,"The loss function in Equation (9) can be optimized under the framework of Perceptron. In this paper, inspired by the work of structured Perceptron [7] and Perceptron algorithm with uneven margins [15], we have developed a novel learning algorithm to optimize the loss function in Equation (9). The algorithm is referred to as PAMM and is shown in Algorithm 2.",null,null
,,,
166,"PAMM takes a training set {(X(n), R(n), J (n))}Nn,""1 as input and takes the diversity evaluation measure E, learning rate , number of positive rankings per query  +, and number of negative rankings per query  - as parameters. For each query qn, PAMM first generates  + positive rankings P R(n) and  - negative rankings N R(n) (line (2) and line (3)). P R(n) and N R(n) play as the random samples of Y+(n) and Y-(n), respectively. PAMM then optimizes the model parameters r and d iteratively in a stochastic manner over the ranking pairs: at each round, for each pair between a positive ranking and a negative ranking (y+, y-), the gap of these two rankings in terms of the query level ranking model F "","" F (X, R, y+) - F (X, R, y-) is calculated based on current parameters r and d (line (9)). If F is smaller than the margin in terms of evaluation measure E "","" E(X, y+, J)-E(X, y-, J) (line (10)), the model parameters will be updated so that F will be enlarged (line (11) and line (12)). The iteration continues until convergence. Finally, PAMM outputs the optimized model parameters (r, d).""",null,null
,,,
167,"Next, we will explain the key steps of PAMM in detail.",null,null
,,,
168,4.2.1 Generating positive and negative rankings,null,null
,,,
169,"In PAMM, it is hard to directly conduct the optimization over the sets of positive rankings Y+(n) and negative rankings Y-(n), because in total these two sets have M ! rankings if the candidate set contains M documents. Thus, PAMM samples the rankings to reduce the training time.",null,null
,,,
170,"For each training query, PAMM first samples a set of positive rankings. Algorithm 3 illustrates the procedure. Similar to the online ranking algorithm shown in Algorithm 1, the positive rankings are generated with a sequential document selection process and the selection criteria is the diversity evaluation measure E. After generating the first positive ranking y(1), the algorithm constructs other positive rankings based on y(1), by randomly swapping the positions of two documents whose subtopic coverage are identical.",null,null
,,,
171,"For each training query, PAMM also samples a set of negative rankings. Algorithm 4 shows the procedure. The algorithm simply generates random rankings iteratively. If the generated ranking is not a positive ranking and satisfies the",null,null
,,,
172,116,null,null
,,,
173,Algorithm 2 The PAMM Algorithm,null,null
,,,
174,"Input: training data {(X(n), R(n), J (n))}Nn,""1, learning rate""",null,null
,,,
175,", diversity evaluation measure E, number of positive rankings per query  +, number of negative rankings per query  -.",null,null
,,,
176,"Output: model parameters (r, d)",null,null
,,,
177,"1: for n ,"" 1 to N do 2: P R(n) PositiveRankings(X(n), J (n), E,  +) {Algo-""",null,null
,,,
178,rithm 3},null,null
,,,
179,"3: N R(n) NegativeRankings(X(n), J (n), E,  -) {Algo-",null,null
,,,
180,rithm 4},null,null
,,,
181,4: end for,null,null
,,,
182,"5: initialize {r, d}  random values in [0, 1]",null,null
,,,
183,6: repeat,null,null
,,,
184,"7: for n , 1 to N do",null,null
,,,
185,8:00,null,null
,,,
186,"for all {y+, y-}  P R(n) × N R(n) do",null,null
,,,
187,9:00,null,null
,,,
188,"F  F (X(n), R(n), y+) - F (X(n), R(n), y-)",null,null
,,,
189,"{F (X, R, y) is defined in Equation (8)}",null,null
,,,
190,10:00,null,null
,,,
191,"if F  E(X(n), y+, J (n)) - E(X(n), y-, J (n))",null,null
,,,
192,then,null,null
,,,
193,11:00,null,null
,,,
194,calculate r(n) and d(n) {Equation (10),null,null
,,,
195,and Equation (11)},null,null
,,,
196,12:00,null,null
,,,
197,"(r, d)  (r, d) +  × (r(n), d(n))",null,null
,,,
198,13:00,null,null
,,,
199,end if,null,null
,,,
200,14: end for,null,null
,,,
201,15: end for,null,null
,,,
202,16: until convergence,null,null
,,,
203,"17: return (r, d)",null,null
,,,
204,"user predefined constraints (e.g, -NDCG@20  0.8), the ranking will be added into the ranking set N R.",null,null
,,,
205,"Please note that in some extreme cases Algorithm 3 and Algorithm 4 cannot create enough rankings. In our implementations, the algorithms are forced to return after running enough iterations.",null,null
,,,
206,4.2.2 Updating r and d,null,null
,,,
207,"Given a ranking pair (y+, y-)  P R(n) × N R(n), PAMM updates r and d as",null,null
,,,
208,"r  r +  × r and d  d +  × d,",null,null
,,,
209,"if F (X, R, y+) - F (X, R, y-)  E(X, y+, J) - E(X, y-, J). The goal of the update is to enlarge the margin between y+ and y- in terms of query level model: F ,"" F (X, R, y+) - F (X, R, y-). For convenience of calculation, we resort to the problem of""",null,null
,,,
210,"F (X, R, y+)",null,null
,,,
211,max log,null,null
,,,
212,",",null,null
,,,
213,"r,d F (X, R, y-)",null,null
,,,
214,"because F (X, R, y) > 0 and log(·) is a monotonous increasing function. Thus, r can be calculated as the gradient:",null,null
,,,
215,r,null,null
,,,
216," ,",null,null
,,,
217,log,null,null
,,,
218,"F (X,R,y+) F (X,R,y-)",null,null
,,,
219,r,null,null
,,,
220," log F (X, R, y+)  log F (X, R, y-)",null,null
,,,
221,",",null,null
,,,
222,-,null,null
,,,
223,",",null,null
,,,
224,r,null,null
,,,
225,r,null,null
,,,
226,-10,null,null
,,,
227,Algorithm 3 PositiveRankings,null,null
,,,
228,"Input: documents X, diversity labels J, evaluation measure E, and the number of positive rankings  +",null,null
,,,
229,"Output: positive rankings P R 1: for r , 1 to |X| do 2: y(1)(r)  arg maxj:xj X\Sr-1",null,null
,,,
230,"E Sr-1  {xj }, y(1)(1), · · · , y(1)(r - 1), j , J",null,null
,,,
231,"3: Sr  Sr-1  {xy(1)(r)} 4: end for 5: P R  {y(1)} 6: while |P R| <  + do 7: y  y(1) 8: (k, l)  randomly choose two documents whose hu-",null,null
,,,
232,"man labels are identical, i.e., J(y(k)) , J(y(1)(l)) 9: y(k)  y(l) {swap documents at rank k and l} 10: if y / P R then 11: P R  P R  {y} 12: end if 13: end while 14: return P R",null,null
,,,
233,Algorithm 4 NegativeRankings,null,null
,,,
234,"Input: documents X, diversity labels J, evaluation measure E, and number of negative rankings  -",null,null
,,,
235,"Output: N R 1: N R ,""  2: while |N R| <  - do 3: y  random shuffle (1, · · · , |X|) 4: if y / N R and E(X, y, J) is as expected then 5: N R  N R  {y} 6: end if 7: end while 8: return N R""",null,null
,,,
236,where,null,null
,,,
237," log F (X, R, y)  ,",null,null
,,,
238,"|X |-1 j,1",null,null
,,,
239,log,null,null
,,,
240,Pr(xy(j),null,null
,,,
241,|X,null,null
,,,
242,\Sj-1,null,null
,,,
243,",",null,null
,,,
244,R),null,null
,,,
245,r,null,null
,,,
246,r,null,null
,,,
247,|X |-1,null,null
,,,
248,",",null,null
,,,
249,"j,1",null,null
,,,
250,xy(j) -,null,null
,,,
251,"|X | k,j",null,null
,,,
252,xy(k),null,null
,,,
253,"exp{fSj-1 (xy(k),",null,null
,,,
254,Ry(k))},null,null
,,,
255,.,null,null
,,,
256,"|X | k,j",null,null
,,,
257,"exp{fSj-1 (xy(k),",null,null
,,,
258,Ry(k))},null,null
,,,
259,"Similarly, d can be calculated as",null,null
,,,
260," log F (X, R, y+)  log F (X, R, y-)",null,null
,,,
261,"d ,",null,null
,,,
262,d,null,null
,,,
263,-,null,null
,,,
264,", d",null,null
,,,
265,-11,null,null
,,,
266,where,null,null
,,,
267,|X |-1,null,null
,,,
268," log F (X, R, y)",null,null
,,,
269,",",null,null
,,,
270,d,null,null
,,,
271,"j,1",null,null
,,,
272,hSj-1 (Ry(j))-,null,null
,,,
273,"|X | k,j",null,null
,,,
274,hSj-1 (Ry(k)),null,null
,,,
275,"exp{fSj-1 (xy(k),",null,null
,,,
276,Ry(k))},null,null
,,,
277,.,null,null
,,,
278,"|X | k,j",null,null
,,,
279,"exp{fSj-1 (xy(k),",null,null
,,,
280,Ry(k))},null,null
,,,
281,"Intuitively, the gradients r and d are calculated so that the line 12 of Algorithm 2 will increase F (X, R, y+)",null,null
,,,
282,"and decrease F (X, R, y-).",null,null
,,,
283,4.3 Analysis,null,null
,,,
284,We analyzed time complexity of PAMM. The learning process of PAMM (Algorithm 2) is of order O(T · N ·  + ·  - ·,null,null
,,,
285,117,null,null
,,,
286,"Table 2: Statistics on WT2009, WT2010 and",Y,null
,,,
287,WT2011. Dataset #queries #labeled docs #subtopics per query,Y,null
,,,
288,WT2009,Y,null
,,,
289,50,null,null
,,,
290,WT2010,Y,null
,,,
291,48,null,null
,,,
292,WT2011,Y,null
,,,
293,50,null,null
,,,
294,5149 6554 5000,null,null
,,,
295,38 37 26,null,null
,,,
296,"M 2 · (D + K)), where T denotes the number of iterations, N the number of queries in training data,  + the number of positive rankings per query,  - the number of negative rankings per query, M the maximum number of documents for queries in training data, D the number of relevance features, and K the number of diversity features. The time complexity of online ranking prediction (Algorithm 1) is of order O(M 2(D + K)).",null,null
,,,
297,"PAMM is a simple yet powerful learning algorithm for search result diversification. It has several advantages compared with the existing learning methods such as R-LTR [31], SVM-DIV [28], and structural SVM [26].",null,null
,,,
298,"First, PAMM employs a more reasonable ranking model. The model follows the maximal marginal relevance criterion and can be implemented with a process of sequential document selection. In contrast, structural SVM approaches [26] calculate all of the ranking scores within a single step, as that of in relevance ranking. The marginal relevance of each document cannot be taken into consideration at ranking time.",null,null
,,,
299,"Second, PAMM can incorporate any diversity evaluation measure in training, which makes the algorithm focus on the specified measure when updating the model parameters. In contrast, R-LTR only minimizes loss function that is loosely related to diversity evaluation measures and SVM-DIV is trained to optimize the subtopic coverage.",null,null
,,,
300,"Third, PAMM utilizes the pairs between the positive rankings and the negative rankings in training, which makes it possible to leverage more information in training. Specifically, it enables PAMM algorithm to enlarge the margins between the positive rankings and negative rankings when updating the parameters. In contrast, R-LTR only uses the information in the positive rankings and the training is aimed to maximizing the likelihood.",null,null
,,,
301,5. EXPERIMENTAL RESULTS,null,null
,,,
302,5.1 Experiment setting,null,null
,,,
303,"We conducted experiments to test the performances of PAMM using three TREC benchmark datasets for diversity tasks: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), and TREC 2011 Web Track (WT2011). Each dataset consists of queries, corresponding retrieved documents, and human judged labels. Each query includes several subtopics identified by TREC assessors. The document relevance labels were made at the subtopic level and the labels are binary2. Statistics on the datasets are given in Table 2.",Y,null
,,,
304,"All the experiments were carried out on the ClueWeb09 Category B data collection3, which comprises of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied",Y,null
,,,
305,2WT2011 has graded judgements. In this paper we treat them as binary. 3http://boston.lti.cs.cmu.edu/data/clueweb09,Y,null
,,,
306,"to the documents as preprocessing. We conducted 5-fold cross-validation experiments on the three datasets. For each dataset, we randomly split the queries into five even subsets. At each fold three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the five trials.",null,null
,,,
307,"As for evaluation measures, -NDCG@k (Equation (3)) with  , 0.5 and k , 20 is used. We also used ERR-IA@k (Equation (4)) with k , 20 to evaluate the performances.",null,null
,,,
308,We compared PAMM with several types of baselines. The baselines include the conventional relevance ranking models in which document diversity is not taken into consideration. Query likelihood (QL) [18] language models for informa-,null,null
,,,
309,tion retrieval. ListMLE [17] a representative learning-to-rank model for,null,null
,,,
310,information retrieval. We also compared PAMM with three heuristic approaches to search result diversification in the experiments. MMR [2] a heuristic approach to search result diversifica-,null,null
,,,
311,"tion in which the document ranking is constructed via iteratively selecting the document with the maximal marginal relevance. xQuAD [25] a representative heuristic approach to search result diversification. PM-2 [8] another widely used heuristic approach to search result diversification. Please note that these three baselines require a prior relevance function to implement their diversification steps. In our experiments, ListMLE was chosen as the relevance function. Learning approaches to search result diversification are also used as baselines in the experiments. SVM-DIV [28] a representative learning approach to search result diversification. It utilizes structural SVMs to optimize the subtopic coverage. SVM-DIV does not consider relevance. For fair performance comparison, in the baseline, we first apply ListMLE to capture relevance, and then apply SVM-DIV to re-rank the top-K retrieved documents. Structural SVM [26] Structural SVM can be configured to directly optimize diversity evaluation measures, as shown in [16]. In the paper, we used structural SVM to optimize -NDCG@20 and ERR-IA@20, denoted as StructSVM(-NDCG) and StructSVM(ERR-IA), respectively. R-LTR [31] a state-of-the-art learning approach to search result diversification. The ranking function is a linear combination of the relevance score and diversity score between the current document and those previously selected. Following the practice in [31], in our experiments we used the results of R-LTRmin which defines the relation function hS(R) as the minimal distance.",null,null
,,,
312,5.2 Features,null,null
,,,
313,"As for features, we adopted the features used in the work of R-LTR [31]. There are two types of features: the relevance features which capture the relevance information of a query with respect to a document, and the diversity features which represent the relation information among documents. Table 3 and Table 4 list the relevance features and diversity features used in the experiments, respectively.",null,null
,,,
314,4http://www.dmoz.org,null,null
,,,
315,118,null,null
,,,
316,Table 3: Relevance features used in the experiments.,null,null
,,,
317,"The first 4 lines are query-document matching features, each applied to the fields of body, anchor, title, URL, and the whole documents. The latter 3",null,null
,,,
318,lines are document quality features. [31],null,null
,,,
319,Name Description,null,null
,,,
320,# Features,null,null
,,,
321,TF-IDF The tf-idf model,null,null
,,,
322,5,null,null
,,,
323,BM25 BM25 with default parameters,null,null
,,,
324,5,null,null
,,,
325,LMIR LMIR with Dirichlet smoothing,null,null
,,,
326,5,null,null
,,,
327,MRF[19] MRF with ordered/unordered phrase,null,null
,,,
328,10,null,null
,,,
329,PageRank PageRank score,null,null
,,,
330,1,null,null
,,,
331,#inlinks number of inlinks,null,null
,,,
332,1,null,null
,,,
333,#outlinks number of outlinks,null,null
,,,
334,1,null,null
,,,
335,Table 4: The seven diversity features used in the experiments. Each feature is extracted over two documents. [31],null,null
,,,
336,Name,null,null
,,,
337,Description,null,null
,,,
338,Subtopic Diversity Text Diversity Title Diversity Anchor Text Diversity ODP-Based Diversity Link-Based Diversity URL-Based Diversity,null,null
,,,
339,Euclidean distance based on PLSA[12] Cosine-based distance on term vectors Text diversity on title Text diversity on anchor ODP4 taxonomy-based distance Link similarity of document pair URL similarity of document pair,null,null
,,,
340,5.3 Experiments with TREC datasets,null,null
,,,
341,"In the experiments, we made use of the benchmark datasets of WT2009, WT2010, and WT2011 from the TREC Web Track, to test the performances of PAMM.",null,null
,,,
342,"PAMM has to tune some parameters. The learning rate parameter  was tuned based on the validation set during each experiment. In all of the experiments in this subsection, we set the number of positive rankings per query  + ,"" 5, and number of negative rankings per query  - "","" 20. As for the parameter E of PAMM, -NDCG@20 and ERR-IA@20 were utilized. The results for PAMM using -NDCG@20 in training are denoted as PAMM(-NDCG). The PAMM results using ERR-IA@20 as measures are denoted as PAMM(ERR-IA).""",null,null
,,,
343,"The experimental results on WT2009, WT2010, and WT2011 are reported in Table 5, Table 6, and Table 7, respectively. Numbers in parentheses are the relative improvements compared with the baseline method of query likelihood (QL). Boldface indicates the highest score among all runs. From the results, we can see that PAMM(-NDCG) and PAMM(ERRIA) outperform all of the baselines on all of the three datasets in terms of both -NDCG@20 and ERR-IA@20. We conducted significant testing (t-test) on the improvements of PAMM(-NDCG) over the baselines in terms of -NDCG@20 and ERR-IA@20. The results indicate that all of the improvements are statistically significant (p-value < 0.05). We also conducted t-test on the improvements of PAMM(ERRIA) over the baselines in terms of -NDCG@20 and ERRIA@20. The improvements are also statistically significant. All of the results show that PAMM is effective for the task of search result diversification.",null,null
,,,
344,"We observed that on all of the three datasets, PAMM(NDCG) trained with -NDCG@20 performed best in terms of -NDCG@20 while PAMM(ERR-IA) trained with ERRIA@20 performed best in terms of ERR-IA@20. The results indicate that PAMM can enhance diverse ranking perfor-",null,null
,,,
345,mances in terms of a measure by using the measure in training. We will further discuss the phenomenon in next section.,null,null
,,,
346,Table 5: Performance comparison of all methods in,null,null
,,,
347,official TREC diversity measures for WT2009.,null,null
,,,
348,Method,null,null
,,,
349,ERR-IA@20,null,null
,,,
350,-NDCG@20,null,null
,,,
351,QL ListMLE,null,null
,,,
352,MMR xQuAD,null,null
,,,
353,PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA),null,null
,,,
354,R-LTR PAMM(-NDCG) PAMM(ERR-IA),null,null
,,,
355,0.164 0.191(+16.46%) 0.202(+23.17%) 0.232(+41.46%) 0.229(+39.63%) 0.241(+46.95%) 0.260(+58.54%) 0.261(+59.15%) 0.271(+65.24%) 0.284(+73.17%) 0.294(+79.26%),null,null
,,,
356,0.269 0.307(+14.13%) 0.308(+14.50%) 0.344(+27.88%) 0.337(+25.28%) 0.353(+31.23%) 0.377(+40.15%) 0.373(+38.66%) 0.396(+47.21%) 0.427(+58.74%) 0.422(+56.88%),null,null
,,,
357,Table 6: Performance comparison of all methods in,null,null
,,,
358,official TREC diversity measures for WT2010.,Y,null
,,,
359,Method,null,null
,,,
360,ERR-IA@20,null,null
,,,
361,-NDCG@20,null,null
,,,
362,QL ListMLE,null,null
,,,
363,MMR xQuAD,null,null
,,,
364,PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA),null,null
,,,
365,R-LTR PAMM(-NDCG) PAMM(ERR-IA),null,null
,,,
366,0.198 0.244(+23.23%) 0.274(+38.38%) 0.328(+65.66%) 0.330(+66.67%) 0.333(+68.18%) 0.352(+77.78%) 0.355(+79.29%) 0.365(+84.34%) 0.380(+91.92%) 0.387(+95.45%),null,null
,,,
367,0.302 0.376(+24.50%) 0.404(+33.77%) 0.445(+47.35%) 0.448(+48.34%) 0.459(+51.99%) 0.476(+57.62%) 0.472(+56.29%) 0.492(+62.91%) 0.524(+73.51%) 0.511(+69.21%),null,null
,,,
368,Table 7: Performance comparison of all methods in,null,null
,,,
369,official TREC diversity measures for WT2011.,null,null
,,,
370,Method,null,null
,,,
371,ERR-IA@20,null,null
,,,
372,-NDCG@20,null,null
,,,
373,QL ListMLE,null,null
,,,
374,MMR xQuAD,null,null
,,,
375,PM-2 SVM-DIV StructSVM(-NDCG) StructSVM(ERR-IA),null,null
,,,
376,R-LTR PAMM(-NDCG) PAMM(ERR-IA),null,null
,,,
377,0.352 0.417(+18.47%) 0.428(+21.59%) 0.475(+34.94%) 0.487(+38.35%) 0.490(+39.20%) 0.512(+45.45%) 0.513(+45.74%) 0.539(+53.13%) 0.541(+53.70%) 0.548(+55.68%),null,null
,,,
378,0.453 0.517(+14.13%) 0.530(+17.00%) 0.565(+24.72%) 0.579(+27.81%) 0.591(+30.46%) 0.617(+36.20%) 0.613(+35.32%) 0.630(+39.07%) 0.643(+41.94%) 0.637(+40.62%),null,null
,,,
379,5.4 Discussions,null,null
,,,
380,"We conducted experiments to show the reasons that PAMM outperforms the baselines, using the results of the WT2009 dataset as examples.",null,null
,,,
381,5.4.1 Effect of maximizing marginal relevance,null,null
,,,
382,"We found that PAMM makes a good tradeoff between the query-document relevance and document diversity via maximizing marginal relevance. Here we use the result with regard to query number 24 (""diversity"" which contains 4 subtopics), to illustrate why our method is superior to the baseline method of Structural SVM trained with -NDCG@20 (denoted as StructSVM(-NDCG)). Note that structural",null,null
,,,
383,119,null,null
,,,
384,PAMM intermediate rankings,null,null
,,,
385,1,null,null
,,,
386,"StructSVM 2, 4",null,null
,,,
387,"2, 4",null,null
,,,
388,"2, 4",null,null
,,,
389,"2, 4",null,null
,,,
390,"2, 4",null,null
,,,
391,ranking positions,null,null
,,,
392,2,null,null
,,,
393,3,null,null
,,,
394,4,null,null
,,,
395,"1, 4",null,null
,,,
396,2,null,null
,,,
397,"1, 3",null,null
,,,
398,2,null,null
,,,
399,4,null,null
,,,
400,"1, 3",null,null
,,,
401,"1, 3",null,null
,,,
402,2,null,null
,,,
403,4,null,null
,,,
404,"1, 3",null,null
,,,
405,"1, 4",null,null
,,,
406,4,null,null
,,,
407,"1, 3",null,null
,,,
408,"1, 4",null,null
,,,
409,2,null,null
,,,
410,"5 -NDCG@5 4 0.788 1, 4 0.744 1, 4 0.803 2 0.812 4 0.815",null,null
,,,
411,Figure 1: Example rankings from WT2009. Each shaded block represents a document and the number(s) in the block represent the subtopic(s) covered by the document.,null,null
,,,
412,"SVM cannot leverage the marginal relevance in its ranking model. Figure 1 shows the top five ranked documents by StructSVM(-NDCG), as well as four intermediate rankings generated by PAMM(-NDCG) (denoted as fS0 , fS1 , fS2 , and fS3 ). The ranking denoted as fSr is generated as: first sequentially selecting the documents for ranking positions of 1, 2, · · · , r - 1 with models fS0 , fS1 , · · · , fSr-2 , respectively; then ranking the remaining documents with fSr-1 . For example, the intermediate ranking denoted as fS2 is generated as: selecting one document with fS0 and setting it to rank 1, then selecting one document with fS1 and set it to rank 2, and finally ranking the remaining documents with fS2 and putting them to the tail of the list. Each of the shaded block indicates a document and the number(s) in the block indicates the subtopic(s) assigned to the document by the human annotators. The performances in terms of -NDCG@5 are also shown in the last column. Here we used -NDCG@5 because only the top 5 documents are shown.",null,null
,,,
413,"The results in Figure 1 indicate the effectiveness of the maximal marginal relevance criterion. We can see that the -NDCG@5 increases steadily with the increasing rounds of document selection iterations. In the first iteration, fS0 selects the most relevant document and puts it to the first position, without considering the diversity. Thus, the NDCG@5 of the ranking generated by fS0 is lower than that of by StructSVM(-NDCG). In the second iteration, the ranking function fS1 selects the document associated with subtopics 1 and 3 and ranks it to the second position, according to the maximal marginal relevance criterion. From the view point of diverse ranking, this is obviously a better choice than StructSVM(-NDCG) made, which selects the document with subtopics 1 and 4. (Note that both Structural SVM and PAMM select the document with subtopics 2 and 4 for the first position.) In the following steps, fS2 and fS3 select documents for ranking positions of 3 and 4, also following the maximal marginal relevance criterion. As a result, fS1 , fS2 , and fS3 outperforms StructSVM(-NDCG).",null,null
,,,
414,5.4.2 Ability to improve the evaluation measures,null,null
,,,
415,"We conducted experiments to see whether PAMM has the ability to improve the diverse ranking quality in terms of a measure by using the measure in training. Specifically, we trained models using -NDCG@20 and ERR-IA@20 and",null,null
,,,
416,ERR-IA@20,null,null
,,,
417,-NDCG@20,null,null
,,,
418,0.43,null,null
,,,
419,0.42,null,null
,,,
420,0.41,null,null
,,,
421,0.4,null,null
,,,
422,0.39,null,null
,,,
423,fold1 fold2 fold3 fold4 fold5 PAMM(-NDCG) PAMM(ERR-IA),null,null
,,,
424,Figure 2: Performance in terms of -NDCG@20 when model is trained with -NDCG@20 or ERRIA@20.,null,null
,,,
425,0.3,null,null
,,,
426,0.28,null,null
,,,
427,0.26,null,null
,,,
428,0.24,null,null
,,,
429,fold1 fold2 fold3 fold4 fold5 PAMM(-NDCG) PAMM(ERR-IA),null,null
,,,
430,Figure 3: Performance in terms of ERR-IA@20 when model is trained with -NDCG@20 or ERRIA@20.,null,null
,,,
431,"evaluated their accuracies on the test dataset in terms of both -NDCG@20 and ERR-IA@20. The experiments were conducted for each fold of the cross validation and performances on each fold are reported. Figure 2 and Figure 3 show the results in terms of -NDCG@20 and ERR-IA@20, respectively. From Figure 2, we can see that on all of the 5 folds (except fold 1), PAMM(-NDCG) trained with NDCG@20 performs better in terms of -NDCG@20. Similarly, from Figure 3, we can see that on all of the 5 folds (except fold 4), PAMM(ERR-IA) trained with ERR-IA@20 performs better in terms of ERR-IA@20. Similar results have also been observed in experiments on other datasets (see the results in Table 5, Table 6, and Table 7). All of the results indicate that PAMM can indeed enhance the diverse ranking quality in terms of a measure by using the measure in training.",null,null
,,,
432,5.4.3 Effects of positive and negative rankings,null,null
,,,
433,"We examined the effects of the number of positive rankings generated per query (parameter  +). Specifically, we compared the performances of PAMM(-NDCG) w.r.t. different  + values. Figure 4 shows the performance curve in terms of -NDCG@20. The performance of R-LTR base-",null,null
,,,
434,120,null,null
,,,
435,-NDCG@20,null,null
,,,
436,0.44 12,null,null
,,,
437,10 0.42,null,null
,,,
438,8,null,null
,,,
439,6,null,null
,,,
440,0.4,null,null
,,,
441,4,null,null
,,,
442,PAMM(-NDCG),null,null
,,,
443,PAMMR-(LT-NRDCG) 2,null,null
,,,
444,0.38,null,null
,,,
445,traiRn-inLTg Rtime,null,null
,,,
446,0 1 2 3 4 5 6 7 8 9 10,null,null
,,,
447,parameter  +,null,null
,,,
448,Figure 4: Ranking accuracies and training time w.r.t.  +.,null,null
,,,
449,0.44 8,null,null
,,,
450,0.42,null,null
,,,
451,6,null,null
,,,
452,-NDCG@20,null,null
,,,
453,0.4,null,null
,,,
454,0.38 5,null,null
,,,
455,4,null,null
,,,
456,PAMM(-NDCG) 2 PAMMR-(LT-NRDCG),null,null
,,,
457,traiRn-inLTg Rtime 0,null,null
,,,
458,10 15 20 25 30 parameter  -,null,null
,,,
459,Figure 5: Ranking accuracies and training time w.r.t -.,null,null
,,,
460,time (hours) performance,null,null
,,,
461,time (hours) -NDCG@20,null,null
,,,
462,0.43,null,null
,,,
463,0.42,null,null
,,,
464,0.41,null,null
,,,
465,0.4,null,null
,,,
466,0.39,null,null
,,,
467,0.38 0.5,null,null
,,,
468,PAMM(-NDCG) R-LTR,null,null
,,,
469,0.6,null,null
,,,
470,0.7,null,null
,,,
471,0.8,null,null
,,,
472,0.9,null,null
,,,
473,-NDCG@20,null,null
,,,
474,Figure 6: Ranking accuracies w.r.t. different NDCG@20 values of the negative rankings.,null,null
,,,
475,0.4,null,null
,,,
476,0.3,null,null
,,,
477,0.2 -NDCG@20 ERR-IA@20,null,null
,,,
478,0,null,null
,,,
479,10 20 30 40 50 60,null,null
,,,
480,number of rounds,null,null
,,,
481,Figure 7: Learning curve of PAMM(-NDCG).,null,null
,,,
482,"line is also shown for reference. From the result, we can see that the curve does not change much with different  + val-",null,null
,,,
483,"ues, which indicates the robustness of PAMM. Figure 4 also shows training time (in hours) w.r.t. different  + values. The training time increased dramatically with large  +, be-",null,null
,,,
484,cause more ranking pairs are generated for training. In our experiments  + was set to 5.,null,null
,,,
485,"We further examined the effect of the number of negative rankings per query (parameter  -). Specifically, we",null,null
,,,
486,compared the performances of PAMM(-NDCG) w.r.t. different  - and the results are shown in Figure 5. From the,null,null
,,,
487,"results, we can see that the performance of PAMM increasing steadily with the increasing  - values until  - ,"" 20,""",null,null
,,,
488,which indicates that PAMM can achieve better ranking per-,null,null
,,,
489,formance with more information from the negative rankings.,null,null
,,,
490,"As the cost, the training time increased dramatically, be-",null,null
,,,
491,"cause more training instances are involved in training. In our experiments,  - was set to 20.",null,null
,,,
492,We also conducted experiments to show the effect of sam-,null,null
,,,
493,pling the negative rankings with different -NDCG values.,null,null
,,,
494,"Specifically, in each of the experiment, we configured the Al-",null,null
,,,
495,gorithm 4 to choose the negative rankings whose -NDCG@20,null,null
,,,
496,"values are 0.5, 0.6, 0.7, 0.8, and 0.9, respectively. Figure 6",null,null
,,,
497,shows the performances of PAMM(-NDCG) w.r.t. different,null,null
,,,
498,-NDCG@20 values of the sampled negative rankings. From,null,null
,,,
499,"the results, we can see that PAMM performs best when the",null,null
,,,
500,-NDCG@20 of the sampled negative rankings ranges from 0.6 to 0.9. The results also indicate that PAMM is robust and not very sensitive to different methods of sampling the negative rankings.,null,null
,,,
501,5.4.4 Convergence,null,null
,,,
502,"Finally we conducted experiments to show whether PAMM can converge in terms of the diversity evaluation measures. Specifically, we showed the learning curve of PAMM(-NDCG) in terms of -NDCG@20 and ERR-IA@20 during the training phase. At each training iteration the model parameters are outputted and evaluated on the test data. Figure 7 shows the performance curves w.r.t. the number of training iterations. From the results, we can see that the ranking accuracy of that PAMM(-NDCG) steadily improves in terms of both -NDCG@20 and ERR-IA@20, as the training goes on. PAMM converges and returns after running about 60 iterations. We also observed that in all of our experiments, PAMM usually converges and returns after running 50100 iterations. Similar phenomenon was also observed from the learning curve of PAMM(ERR-IA). The results indicates that PAMM converges fast and conducts the training efficiently.",null,null
,,,
503,121,null,null
,,,
504,6. CONCLUSION AND FUTURE WORK,null,null
,,,
505,"In this paper we have proposed a novel algorithm for learning ranking models in search result diversification, referred to as PAMM. PAMM makes use of the maximal marginal relevance model for constructing the diverse rankings. In training, PAMM directly optimizes the diversity evaluation measures on training queries under the framework of Perceptron. PAMM offers several advantages: employs a ranking model that follows the maximal marginal relevance criterion, ability to directly optimize any diversity evaluation measure, and ability to utilize both positive rankings and negative rankings in training. Experimental results based on three benchmark datasets show that PAMM significantly outperforms the state-of-the-art baseline methods including SVM-DIV, structural SVM, and R-LTR.",null,null
,,,
506,"Future work includes theoretical analysis on the convergence, generalization error, and other properties of the PAMM algorithm, and improving the efficiency of PAMM in both offline training and online prediction.",null,null
,,,
507,7. ACKNOWLEDGMENTS,null,null
,,,
508,"This research work was funded by the 973 Program of China under Grants No. 2014CB340401, No. 2012CB316303, the 863 Program of China under Grants No. 2014AA015204, the National Natural Science Foundation of China under Grant No. 61232010, No. 61425016, No. 61173064, No. 61472401, No. 61203298, and No. 61202214.",null,null
,,,
509,We would like to express our gratitude to Prof. Chengxiang Zhai who has offered us valuable suggestions in the academic studies.,null,null
,,,
510,8. REFERENCES,null,null
,,,
511,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proceedings of the 2th ACM WSDM, pages 5­14, 2009.",null,null
,,,
512,"[2] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st ACM SIGIR, pages 335­336, 1998.",null,null
,,,
513,"[3] B. Carterette and P. Chandar. Probabilistic models of ranking novel documents for faceted topic retrieval. In Proceedings of the 18th ACM CIKM, pages 1287­1296, 2009.",null,null
,,,
514,"[4] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM CIKM, pages 621­630, 2009.",null,null
,,,
515,"[5] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st ACM SIGIR, pages 659­666, 2008.",null,null
,,,
516,"[6] C. L. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. In Proceedings of the 2nd ICTIR, pages 188­199, 2009.",null,null
,,,
517,"[7] M. Collins. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In EMNLP '02, pages 1­8, 2002.",null,null
,,,
518,"[8] V. Dang and W. B. Croft. Diversity by proportionality: an election-based approach to search result diversification. In Proceedings of the 35th ACM SIGIR, pages 65­74, 2012.",null,null
,,,
519,"[9] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In Proceedings of the 18th WWW, pages 381­390, 2009.",null,null
,,,
520,"[10] S. Guo and S. Sanner. Probabilistic latent maximal marginal relevance. In Proceedings of the 33rd ACM SIGIR, pages 833­834, 2010.",null,null
,,,
521,"[11] J. He, V. Hollink, and A. de Vries. Combining implicit and explicit topic representations for result diversification. In Proceedings of the 35th ACM SIGIR, pages 851­860, 2012.",null,null
,,,
522,"[12] T. Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd ACM SIGIR, pages 50­57, 1999.",null,null
,,,
523,"[13] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.",null,null
,,,
524,"[14] L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th WWW, pages 71­80, 2009.",null,null
,,,
525,"[15] Y. Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, and J. Kandola. The perceptron algorithm with uneven margins. In Proceedings of the 19th ICML, pages 379­386, 2002.",null,null
,,,
526,"[16] S.-S. Liang, Z.-C. Ren, and M. de Rijke. Personalized search result diversification via structured learning. In Proceedings of the 20th ACM SIGKDD, pages 751­760, 2014.",null,null
,,,
527,"[17] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",null,null
,,,
528,"[18] C. D. Manning, P. Raghavan, and H. SchU¨ tze. An Introduction to Information Retrieval. Cambridge University Press, 2009.",null,null
,,,
529,"[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proceedings of the 28th ACM SIGIR, pages 472­479, 2005.",null,null
,,,
530,"[20] L. Mihalkova and R. Mooney. Learning to disambiguate search queries from short sessions. In Machine Learning and Knowledge Discovery in Databases, volume 5782, pages 111­127, 2009.",null,null
,,,
531,"[21] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proceedings of the 29th ACM SIGIR, pages 691­692, 2006.",null,null
,,,
532,"[22] F. Radlinski, R. Kleinberg, and T. Joachims. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th ICML, pages 784­791, 2008.",null,null
,,,
533,"[23] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of the 19th WWW, pages 781­790, 2010.",null,null
,,,
534,"[24] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD, pages 705­713, 2012.",null,null
,,,
535,"[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proceedings of the 19th WWW, pages 881­890, 2010.",null,null
,,,
536,"[26] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun. Large margin methods for structured and interdependent output variables. J. Mach. Learn. Res., 6:1453­1484, Dec. 2005.",null,null
,,,
537,"[27] J. Xu, T.-Y. Liu, M. Lu, H. Li, and M. Wei-Ying. Directly optimizing evaluation measures in learning to rank. In Proceedings of the 31th ACM SIGIR, pages 107­114, 2008.",null,null
,,,
538,"[28] Y. Yue and T. Joachims. Predicting diverse subsets using structural svms. In Proceedings of the 25th ICML, pages 1224­1231, 2008.",null,null
,,,
539,"[29] Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th ICML, pages 1201­1208, 2009.",null,null
,,,
540,"[30] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of the 26th ACM SIGIR, pages 10­17, 2003.",null,null
,,,
541,"[31] Y. Zhu, Y. Lan, J. Guo, X. Cheng, and S. Niu. Learning for search result diversification. In Proceedings of the 37th ACM SIGIR, pages 293­302, 2014.",null,null
,,,
542,122,null,null
,,,
543,,null,null

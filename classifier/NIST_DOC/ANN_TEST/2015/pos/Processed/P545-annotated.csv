,sentence,label,data
,,,
0,Representative & Informative Query Selection for Learning to Rank using Submodular Functions,null,null
,,,
1,Rishabh Mehrotra,null,null
,,,
2,"Dept of Computer Science University College London, UK",null,null
,,,
3,r.mehrotra@cs.ucl.ac.uk,null,null
,,,
4,Emine Yilmaz,null,null
,,,
5,"Dept of Computer Science University College London, UK",null,null
,,,
6,emine.yilmaz@ucl.ac.uk,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"The performance of Learning to Rank algorithms strongly depend on the number of labelled queries in the training set, while the cost incurred in annotating a large number of queries with relevance judgements is prohibitively high. As a result, constructing such a training dataset involves selecting a set of candidate queries for labelling. In this work, we investigate query selection strategies for learning to rank aimed at actively selecting unlabelled queries to be labelled so as to minimize the data annotation cost. In particular, we characterize query selection based on two aspects of informativeness and representativeness and propose two novel query selection strategies (i) Permutation Probability based query selection and (ii) Topic Model based query selection which capture the two aspects, respectively. We further argue that an ideal query selection strategy should take into account both these aspects and as our final contribution, we present a submodular objective that couples both these aspects while selecting query subsets. We evaluate the quality of the proposed strategies on three real world learning to rank datasets and show that the proposed query selection methods results in significant performance gains compared to the existing state-of-the-art approaches.",null,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,H.3.3 [Information Storage And Retrieval]: Information Search and Retrieval--Learning to Rank,null,null
,,,
11,Keywords,null,null
,,,
12,"Learning to Rank, Query Selection, Active Learning, Submodularity",null,null
,,,
13,1. INTRODUCTION,null,null
,,,
14,"Most modern search technologies are based on machine learning algorithms that learn to rank documents given a query, an approach that is commonly referred to as ""learning to rank"". Learning to Rank algorithms aim to learn ranking functions that achieve good ranking objectives on test data.",null,null
,,,
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",null,null
,,,
16,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,null,null
,,,
17,DOI: http://dx.doi.org/10.1145/2766462.2767753.,null,null
,,,
18,"Such learning methods require labelled data for training. As is the case with many supervised learning algorithms, the performance of Learning to Rank algorithms are often highly correlated with the amount of labelled training data available[1][17][7].",null,null
,,,
19,"Constructing such labelled training data for learning-torank tasks incurs prohibitive costs since it requires selecting candidate queries, extracting features from query-document pairs and annotating documents in terms of their relevance to these queries (annotations are used as labels for training). The major bottleneck in constructing learning-to-rank collections is annotating documents with query specific relevance grades. It is essential therefore, both for the efficiency of the construction methodology and for the efficiency of the training algorithm, that only a small subset of queries be selected. The query selection, though, should be done in a way that does not harm the effectiveness of learning.",null,null
,,,
20,"Active Learning algorithms help reduce the annotation costs by selecting a subset of informative instances to be labelled. Unlike traditional algorithms, active learning strategies for ranking algorithms are more complex because of the inherent query-document pair structure embodied in ranking datasets, non-smooth cost functions, etc., hence these cannot be applied directly in ranking setting.",null,null
,,,
21,"Existing approaches for active learning for ranking have focused on selecting documents [1], selecting queries [17] or balancing number of queries with depth of documents judged using random query selection [27].",null,null
,,,
22,"In this work, we focus on selecting subset of queries to be labelled so as to minimize the data annotation cost. Prior work on selecting queries made use of expected loss optimization [17] to estimate which queries should be selected but their approach is limited to rankers that predict absolute graded relevance which is not the case with modern Learning to Rank algorithms since many of them induce a ranking and not absolute labels [4]. Apart from the learning to rank setting, query selection has also received significant attention for evaluation setting [13] wherein the goal was to find a subset of queries that most closely approximates the system evaluation results that would be obtained if instead documents for the full set of queries was judged instead. However, it was shown by Aslam et a.l [1] that learning to rank and evaluation of retrieval systems are quite different from each other and that datasets constructed for evaluating quality of retrieval systems are not necessarily good for training and vice versa. Therefore, query selection strategies that are directly devised for learning to rank purposes are needed.",null,null
,,,
23,545,null,null
,,,
24,"Intuitively, an optimal subset of queries constructed for learning to rank should have two characteristics: (i) informativeness, which measures the ability of an instance (query) in reducing the uncertainty of a statistical model (ranking model) and (ii) representativeness, which measures if an instance (query) well represents the possible input patterns of unlabelled data (unlabelled queries) [22]. Most existing active learning for ranking algorithms solely focus on the informativeness aspect of queries without considering the representativeness aspect which can lead to possible selection of noisy queries, not quite representative of the whole population of queries; thus, significantly limiting the performance of query selection.",null,null
,,,
25,"In this work, we focus on query selection strategies for learning to rank and propose novel query selection algorithms aimed at finding an optimal subset of queries to be labelled. Since problems associated with subset selection are generally NP-Hard or NP-Complete[12], we approximate the solution by an iterative query selection process so as to minimize the data annotation cost without severely degrading the performance of the ranking model.",null,null
,,,
26,"We describe two paradigms of query selection strategies based on the aspects of informativeness and representativeness described above and propose novel query selection techniques: Permutation Probability based query selection and query selection based on topic models which capture these two aspects, respectively. We further present a new algorithm based on defining a submodular objective that combines the powers of the two paradigms. Submodular functions have the characteristic of diminishing returns [19], which is an important attribute of any query-subset selection technique since the value-addition from individual queries should ideally decrease as more and more queries are selected. Thus, not only are submodular functions natural for query subset selection, they can also be optimized efficiently and scalably such that the result has mathematical performance guarantees.",null,null
,,,
27,We show that our proposed algorithms result in significant improvements compared to state-of-the-art query selection algorithms thereby helping in reducing data annotation costs.,null,null
,,,
28,2. RELATED WORK,null,null
,,,
29,"Active Learning for Labelling Cost Reduction: A number of active learning strategies have been proposed for the traditional supervised learning setting, a common one being uncertainty sampling which selects the unlabelled example about which the model is most uncertain how to label. Some of the others adopt the idea of reducing the generalization error and select the unlabelled example that has the highest effect on the test error, i.e. points in the maximally uncertain and highly dense regions of the underlying data distribution[9]. A comprehensive active learning survey can be found in [22].",null,null
,,,
30,"Reducing judgment effort for learning to rank has received significant amount of attention from the research community. Learning to rank methods are quite different than approaches used for classification as they require optimizing nonsmooth cost functions such as NDCG and AP [24]. Moreover, owing to the unique query-document structure which inherent to the learning to rank setting, it is not straightforward to extend the models devised for traditional supervised learning settings to ranking problems. In recent",null,null
,,,
31,"years, active learning has been actively extended to rank learning and can be classified into two classes of approaches: document level and query level active learning.",null,null
,,,
32,"Document Selection for Learning to Rank: Based on uncertainty sampling, Yu et al [28] selected the most ambiguous document pairs, in which two documents received close scores predicted by the current model, as informative examples. Donmez et al.[8] chose those document pairs, which if labelled could change the current model parameters significantly. Silva et al [23] proposed a novel document level active sampling algorithm based on association rules, which does not rely on any initial training seed.",null,null
,,,
33,"Query Selection for Learning to Rank: For query level active learning, Yilmaz et al. [27] empirically showed that having more queries but shallow documents performed better than having less queries but deep documents. They balance number of queries with depth of documents judged using random query selection. Cai et al. [5] propose the use of Query-By-Committee (QBC) based method to select queries for ranking adaptation but omit the evaluation of the query selection part and focussed on the ranking adaptation results instead. Long et al. [17] introduced an expected loss optimization (ELO) framework for ranking, where the selection of query and documents were integrated in a principled 2 staged active learning framework and most informative queries selected by optimizing the expected DCG loss but the proposed approach is limited to rankers that predict absolute graded relevance and hence not generalizable to all rankers. Authors in [2] adapt ELO to work with any ranker by introducing a calibration phase where a classification model is trained over in the validation data. Moreover, they show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.",null,null
,,,
34,"Thus, QBC attempts to capture the informativeness aspect of queries by selecting queries which minimize the disagreement among a committee of rankers while the Expected loss optimization based approach formulates informativeness in terms of expected DCG loss; both these approaches fail to capture the representativeness aspect of queries which we show outperforms both these approaches.",null,null
,,,
35,"Submodular Maximization: Submodularity is a property of set functions with deep theoretical and practical consequences. Submodular maximization generalizes to many well-known problems, e.g., maximum weighted matching, max coverage, and finds numerous applications in machine learning and social networks. In Information Retrieval, submodular objectives have been majorly employed for diversified retrieval[29] & learning from implicit feedback[21]. A seminal result of Nemhauser et al. [19] states that a simple greedy algorithm, based on a submodular objective, produces solutions competitive with the optimal (intractable) solution. In fact, if assuming nothing but submodularity, no efficient algorithm produces better solutions in general [10].",null,null
,,,
36,3. QUERY SELECTION STRATEGIES,null,null
,,,
37,"Our aim is to actively select the optimal subset of unlabelled queries for obtaining relevance judgements so as to reduce data annotation costs. Intuitively, the selected queries",null,null
,,,
38,546,null,null
,,,
39,should have two major properties: informativeness & representativeness. We describe both these properties below and provide intuitions motivating each.,null,null
,,,
40,3.1 Informativeness,null,null
,,,
41,"Informativeness measures the ability of an instance in reducing the uncertainty of a statistical model[22]. Ideally, the selected queries should be maximally informative to the ranking model. In learning to rank setting, Informativeness based query selection focusses on greedily selecting queries which are most informative to the current version of the ranking model.",null,null
,,,
42,Different notions of informativeness can be encapsulated by different techniques depending on how query-level informativeness is quantified. Two possible measures of capturing a query's informativeness include: (i) Uncertainty based informativeness & (ii) Disagreement based informativeness.,null,null
,,,
43,"Uncertainty based informativeness quantifies the querylevel information as the uncertainty associated with the optimal document ranking order for that query. Query selection strategies focusing on uncertainty reduction would greedily select the query instance about which the current ranking model is most uncertain about, thereby trying to reduce the overall uncertainty associated with the ranking model.",null,null
,,,
44,"Disagreement based informativeness, on the other hand, quantifies the query-level informativeness as the disagreement in this query's document rankings among a committee of ranking models. The key idea here is that the maximally informative query is one about whose document rankings, the committee of ranking models maximally disagree; hence obtaining relevance labels for such a query would provide the maximum information. Among the existing approaches for query selection for ranking models, the Queryby-Committee [5] attempts to capture the Informativeness aspect of queries based on a disagreement measure.",null,null
,,,
45,3.2 Representativeness,null,null
,,,
46,"Representativeness measures if an instance well represents the overall input patterns of unlabelled data [22]. Web search queries can span a multitude of topics and information needs, with even a small dataset containing a broad set of queries ranging from simple navigational queries to very specific domain-dependent queries. In learning to rank settings, this implies that selected queries should have strong correlation with the remaining queries, as without this correlation there is no generalizability and predictive capability. Different notions of representativeness can be defined covering different characteristics of individual queries. Improving the representativeness of the selected query subset improves the coverage aspect of the query collection - the more representative selected queries are, the more they cover the entire query collection.",null,null
,,,
47,3.3 Informativeness vs Representativeness,null,null
,,,
48,"Selecting queries solely based on their informativeness aspects could possibly lead to selection of noisy queries. In line with the Meta-Search Hypothesis [14][15], rankers tend to agree on relevant documents and disagree about nonrelevant docs. Hence, the queries that a ranker is unsure about or there is big disagreements across rankers are likely to be the ones that contain a lot of nonrelevant documents. Such noisy, outlier queries which majorly have non-relevant documents would lead to maximal disagreement and uncertainty",null,null
,,,
49,"among ranking models, and thus would be wrongly labelled maximally informative. Also, the set of informative queries might not necessarily represent the set of all possible queries, which lead to less coverage of the unlabelled query set.",null,null
,,,
50,"On the other hand, selecting queries based on representativeness aspects could lead to the selection of a query that is very similar to the a query already in the labelled set and hence, does not provide much information to the ranking model. Despite being representative, such queries possibly offer redundant information to the ranking models. Ideally, a query selection algorithm should take into account both these aspects while selecting queries. Existing work has majorly looked into selecting queries by considering informativeness based on disagreement among rankers (Query-byCommittee) or informativeness in terms of expected DCG loss (Expected loss optimization). Both these approaches fail to capture the representativeness aspect of queries. In addition to a novel informativeness approach based on uncertainty reduction, we present a representativeness based approach and finally couple both these aspects for query selection via a joint submodular objective which jointly incorporates informativeness & representativeness.",null,null
,,,
51,"As our first contribution, we present a novel informativeness based query selection scheme ( 4) based on permutation probabilities of document rankings which tries to reduce uncertainty among rankers. While no existing query selection scheme for learning to rank incorporates the representativeness aspect of queries, we propose a LDA topic model based query selection scheme ( 5) which captures the representative aspect of queries while constructing the query subset. An ideal query subset would have both informative & representative queries. As our third contribution, we combine the two paradigms of representativeness & informativeness by proposing a coupled model based on submodular functions( 6).",null,null
,,,
52,4. CAPTURING INFORMATIVENESS VIA PERMUTATION PROBABILITIES,null,null
,,,
53,"Our first novel query selection scheme is aimed at capturing the informative-aspect of queries. We maintain a committee of ranking models C ,"" {1, 2, ..., C } which are trained on a randomly selected subset from the current labelled set, and thus contain different aspects of the training data depending on the queries in their subset. It is to be noted that these ranking models could be generated using any learning to rank algorithm. Given the set of currently labelled query instances, our goal is to pick the next query (q) from the set of unlabelled queries by selecting the maximally informative query instance. The query-level informativeness is defined in terms of the uncertainty associated with the optimal document ranking orders among the |C| ranking models. We follow a similar approach as outlined by Cai et al.[5] to maintain a committee of rankers. However, unlike Query-By-Committee [5] which encapsulates informativeness via ranker disagreements, our approach presents an alternate view of informativeness based on uncertainty reduction wherein a ranking model's uncertainty for the query's document ranking order is defined based on the concept of permutation probabilities.""",null,null
,,,
54,"More specifically, each committee ranking model is allowed to score the documents associated with each query following which a permutation probability is calculated on the",null,null
,,,
55,547,null,null
,,,
56,"ranking obtained on sorting these document scores. Thus, each query gets a permutation probability score by each committee member. The most informative query is considered to be the query instance which minimizes the maximum permutation probability of document scores given by each ranking model committee member.",null,null
,,,
57,"We postulate that a query which has the minimum permutation probability score from among the maximum scores assigned between the different ranking models is maximally informative in the sense that even the best ranker among the committee is highly uncertain about its document rankings and hence this query obtained the least permutation probability score among the set of unlabelled candidate queries. We select a query for which the probability with respect to the most certain (maximum permutation probability) model is minimal, i.e., a query for which even the most certain committee member has minimum confidence.",null,null
,,,
58,"To define permutation probabilities, we make use of the Plackett-Luce model [20]. The Plackett-Luce (P-L) model is a distribution over rankings of items (documents) which is described in terms of the associated ordering of these items (documents). We define P (|) as the probability of obtaining the ranking order () based on the score (k) assigned to each document (k) by the ranking model learnt thus far. For each query, we rank the documents based on the scores assigned by model learnt so far and calculate the probability of the ranking order obtained () using the permutation probability defined as follows:",null,null
,,,
59,"P (|) ,",null,null
,,,
60,i,null,null
,,,
61,"i,""1,...,K i +    + K""",null,null
,,,
62,-1,null,null
,,,
63,"where each ranking  has an associated ordering of document scores  ,"" (1,    , K ) and an ordering is defined as a permutation the K document indices with i representing the score assigned to document i (at rank i) by the ranking model. We make use of a committee of ranking models and select the maximally informative query based on a greedy min-max algorithm described next.""",null,null
,,,
64,4.1 Min-Max PL Probability Algorithm,null,null
,,,
65,Building a Min-Max PL Probability based selection system involves two components: (i) building a committee of ranking models that are well diversified and compatible with the currently labelled data and (ii) computing permutation probabilities by each committee member for each query in the unlabelled set of queries & selecting maximally informative query as per the min-max score.,null,null
,,,
66,"Committee Construction: Following the work of [5], we use query-by-bagging approach to construct the members. Given the set of currently labelled instances, bagging generates C partitions of sub samples by sampling uniformly with replacement, and then the committee can be constructed by training each of its members on one portion of the sub-sample partitions. We randomly initialize the initial set of labelled queries with a small base set of queries and their labelled documents. We sample with replacement for C times in the set of labelled queries and train a ranking model on each subset of queries. Such a sampling procedure allows us to create various different training datasets that each represent a subset of the data possibly having very different characteristics than each other. These C models represents our C committee mem-",null,null
,,,
67,"bers. We set the size of each subset to be 50 % of the current labelled subset size at each step. The maximally informative query q is selected for annotation which obtains the lowest min-max score, the calculation of which is described below.",null,null
,,,
68,"Calculating min-max score: For each query q in the candidate set of unlabelled queries, the C committee members return C ranked lists. Following the construction of |C| ranking models, for each ranking model per query, we sort the documents based on the scores given by the ranking model and compute the permutation probability of obtaining this ranking order.",null,null
,,,
69,"Thus, each query has |C| permutation probability scores. In order to minimize the overall uncertainty associated with the ranking models, we select the maximally informative query q, i.e., the query that has the minimum value of the permutation probability assigned by its most certain committee member, i.e., the committee member that has the highest permutation probability score associated with the query's document ranking order. Thus,",null,null
,,,
70,"q , argminqDu maxcC P (qc|qc)",null,null
,,,
71,"c k k,""1,...,K c k +    + c K""",null,null
,,,
72,-2,null,null
,,,
73,"where each ranking qc has an associated ordering  ,"" (1c,    , Kc ) and an ordering is defined as a permutation the K document indices with c k representing the score assigned to document k by the ranking model c.""",null,null
,,,
74,5. CAPTURING REPRESENTATIVENESS VIA LDA TOPICS,null,null
,,,
75,"A major drawback associated with pure-Informativeness based models is that often they tend to select outlier queries. As is confirmed by the Meta-Search Hypothesis [14][15], rankers tend to agree on relevant documents but disagree on non-relevant documents. In such a scenario, an outlier query which majorly has non-relevant documents would lead to maximal disagreement and uncertainty in the ranking model, and thus will be wrongly labelled maximally informative. This motivates the need for considering the representativeness aspect of queries.",null,null
,,,
76,"The information-seeking behaviour of users tend to vary based on the search task at hand [25] which suggests that the importance of feature weights for queries belonging to different tasks or topics are likely to be very different. The relative importance of different features are likely to be very different for different tasks. For example, queries belonging to a topic such as news would warrant high authority websites to be ranked higher (i.e., larger weight on the pagerank score) while queries belonging to (say) educational informational content would prefer the documents better matched with their query terms be ranked higher (i.e., larger weight on the relevance features such as BM25). To capture these diverse variations in the feature weights, the training set should ideally be composed of representative queries from different tasks. This makes it necessary that the labelled set of queries have representative queries spanning the entire",null,null
,,,
77,548,null,null
,,,
78,array of different topics. We propose a Latent Dirichlet Allocation (LDA) [3] topic model based query selection scheme which tries to capture this insight by selecting representative queries which are most topically similar to the set of unlabelled queries.,null,null
,,,
79,"Based on this intuition, we conjecture that representative queries would be those that are most similar to the set of unlabelled queries in terms of their topical distribution. To capture the heterogeneity among all queries in the search logs, we make use of the concept of latent topics. We learn these latent topics from the collection of queries and represent each query as a probability distribution over these latent topics. We train an LDA model, a generative model which posits that each document (query in our case) is a mixture of a small number of topics and that each word's (query term's) creation is attributable to one of the document's (query's) topics. Each query is represented as a feature vector corresponding to its distribution over the LDA topics. To find representative queries, we select the query with the maximum average similarity from among the unlabelled set of queries, i.e.,",null,null
,,,
80,q,null,null
,,,
81,",",null,null
,,,
82,argmaxq,null,null
,,,
83,1 |Du|,null,null
,,,
84,qi Du,null,null
,,,
85,"sim(Tq ,",null,null
,,,
86,Tqi ),null,null
,,,
87,-3,null,null
,,,
88,"where |Du| represents the number of queries in the unlabelled set Du; Tq represents the query q's feature vector in the LDA topic space and sim(Tq, Tqi ) can be any similarity score between queries; we use the cosine similarity between",null,null
,,,
89,the topic-space representations of queries q and qi.,null,null
,,,
90,6. COMBINING REPRESENTATIVENESS & INFORMATIVENESS,null,null
,,,
91,"The approaches discussed so far have looked at either the informativeness of queries and selected queries which are most informative in terms of their ability reduce the uncertainty of the ranking model or they have focussed on representativeness of queries and selected representative queries spanning the entire array of different topics. As we discussed earlier in subsection 3.3, optimizing for only one of the two criteria for query selection could significantly limit the performance of query selection by selecting suboptimal query subsets. In this section we present a way of combining the two objectives by means of submodular functions and propose a submodular objective which jointly captures the notions of representativeness and informativeness.",null,null
,,,
92,6.1 Submodular Functions,null,null
,,,
93,"Submodular functions are discrete functions that model laws of diminishing returns and can be defined as follows:[19]: Given a finite set of objects (samples) Q ,"" {q1, ..., qn} and a function f : 2S  + that returns a real value for any subset S  Q, f is submodular if given S  S , and q / S""",null,null
,,,
94,f (S + q) - f (S)  f (S + q) - f (S ),null,null
,,,
95,-4,null,null
,,,
96,"That is, the incremental ""value"" of q decreases when the set in which q is considered grows from S to S . A function is monotone submodular if S  S , f (S)  f (S ). Powerful guarantees exist for such subtypes of monotone submodular function maximization. Though NP-hard, the problem of maximizing a monotone submodular function subject to a cardinality constraint can be approximately solved by a simple greedy algorithm [19] with a worst-case approximation",null,null
,,,
97,"factor (1 - e-1). This is also the best solution obtainable in polynomial time unless P,NP [10].",null,null
,,,
98,6.2 Problem Formulation,null,null
,,,
99,"Submodularity is a natural model for query subset selection in Learning to Rank setting. Indeed, an important characteristic of any query-subset selection technique would be to decrease the value-addition of a query q  Q based on how much of that query has in common with the subset of queries already selected (S). The value f (q|S) of a query in the context of previously selected subset of queries S further diminishes as the subset grows S  S. In our setting, each q  Q is a distinct query, Q corresponds to the entire collection of queries and S corresponds to the subset of queries already selected from Q.",null,null
,,,
100,"Mathematically, the query subset selection problem can be formulated as selecting the subset of queries S which maximizes the value of f (S) where f (S) captures both the representativeness aspect as well as the informativeness aspects of queries. We next describe in detail the construction of such a monotone submodular function and later present a greedy algorithm to approximately solve the problem of query subset selection.",null,null
,,,
101,6.3 Submodular Query Selection,null,null
,,,
102,"We model the quality of the query subset in terms of both the representativeness & informativeness. To capture both these traits, we model the quality of the query subset as:",null,null
,,,
103,"F (S) , (S) + (1 - )(S)",null,null
,,,
104,-5,null,null
,,,
105,"where (S) captures the representativeness aspect of the query subset (S) with respect to the entire query set Q while (S) rewards selecting informative queries. The parameter  controls the trade-off between the importance of representativeness & informativeness while selecting queries. A single weighting scheme would not be suitable for all problems since depending on the constituent queries, size of the overall dataset and the size of the subset that needs to be selected, different weighting schemes would produce different results. The function F (S) will be monotone submodular if each of (S) and (S) are individually monotone submodular. We defer an in-depth analysis of the trade-off between representativeness & informativeness aspects to subsection 8.1 and next describe the details of both these functions.",null,null
,,,
106,6.3.1 Representativeness: (S),null,null
,,,
107,"(S) can be interpreted either as a set function that measures the similarity of query subset S to the overall query set Q, or as a function representing some form of ""representation"" of Q by S. Most naturally, (S) should be monotone, as representativeness improves with a larger subset. (S) should also be submodular: consider adding a new query to two query subsets, one a subset of the other. Intuitively, the increment when adding a new query to the small subset should be larger than the increment when adding it to the larger subset, as the information carried by the new query might have already been covered by those queries that are in the larger subset but not in the smaller subset. Indeed, this is the property of diminishing returns.",null,null
,,,
108,"We employ the same functional form of (S) as was adopted by Lin et al.[16]. Specifically, a saturated coverage function",null,null
,,,
109,549,null,null
,,,
110,is defined as follows:,null,null
,,,
111,"(S) ,"" min {Cq(S), Cq(Q)}""",null,null
,,,
112,-6,null,null
,,,
113,qQ,null,null
,,,
114,"where Cq(S) is a set based function defined as Cq(S) : 2S  and 0    1 is a threshold co-efficient. Intuitively,",null,null
,,,
115,"Cq(S) measures how topically similar S is to query q or how much of the query q is covered by the subset S. Building on top of the earlier proposed LDA topic model based query selection, we define the coverage function Cq(S) in terms of the topical coverage of queries. More specifically,",null,null
,,,
116,"Cq(S) ,",null,null
,,,
117,"wq,q",null,null
,,,
118,-7,null,null
,,,
119,q S,null,null
,,,
120,"where wq,q  0 measures the topical similarity between queries q and q . Since Cq(S) measures how topically similar S is to query q, summing Cq(S) q  Q would measure how similar the current subset S is to the overall set of queries Q. It is important to note that Cq(Q) is just the largest value Cq(S) can ever obtain because Q is the set of all the queries we have and it maximally represents all the information we have. We call a query q saturated by the subset of queries S when min {Cq(S), Cq(Q)} ,"" Cq(Q). When q is saturated in this way, any new query cannot further improve the coverage even if it is very similar to the query q. Thus, this gives other queries which are not yet saturated a higher chance of being better covered and hence the resulting subset tends to better cover the entire set of queries Q.""",null,null
,,,
121,6.3.2 Informativeness: (S),null,null
,,,
122,"The (S) function described above intuitively captures the notion of coverage or representativeness by selecting subset of queries S which are topically most representative of the entire set of queries Q. While representativeness is an important trait, we also wish to capture the informativeness aspect of queries and select queries which are most informative to the current version of the ranking model. We formulate the functional form of (S) based on top of the earlier proposed ways of encapsulating query-level informativeness in terms of either ranker disagreements or model uncertainity, or both. As a precursor, it is worth mentioning that to define the function (S) we make use of LDA topic model which gives us k-topics and we associate each query to one of these k-topics. Formally, we define the (S) function as follows:",null,null
,,,
123,K,null,null
,,,
124,"(S) ,",null,null
,,,
125,q,null,null
,,,
126,-8,null,null
,,,
127,"i,1 qPiS",null,null
,,,
128,"where Pi, i ,"" 1, ..., K is the topical-partition of the set of queries Q into K-topics and q captures the informativeness carried by the query q based on the current ranking model. The function (S) rewards topical-diversity along with valuing informativeness since there is usually more benefit to selecting a query from a topic not yet having one of its query already chosen. As soon as a query is selected from a topic, other queries from the same topic starthaving diminishing gain owing to the square root function ( 2+ 1 > 3+ 0). It is easy to show that (S) is submodular by the composition rule. The square root is non-decreasing concave function. Inside each square root lies a modular function with non-negative weights (and thus is monotone). Applying the square root to such a monotone submodular function yields""",null,null
,,,
129,"a submodular function, and summing them all together retains submodularity.",null,null
,,,
130,"The informativeness of a query q can be defined based on the metrics proposed earlier. To incorporate the informativeness aspects of queries, we experiment with various different formulations of the singleton-query rewards (q) include the following::",null,null
,,,
131, Disagreement Score for a query - this allows us to capture information about the disagreement about the document rankings for a query among a committee of ranking models [5],null,null
,,,
132, Uncertainty associated with the query - this allows us to capture the ranking model's uncertainty about the query's document rankings 4,null,null
,,,
133, Combination of uncertainty & disagreement.,null,null
,,,
134,"Based on empirical analysis, we find that the disagreement based reward functions perform better than the rest of the formulations across all datasets, so we skip the performance comparisons among these.",null,null
,,,
135,6.4 Greedy Optimization,null,null
,,,
136,"Having defined the individual functions based on the different paradigms, we formulate the overall query subset selection problem as the selection of the subset S of queries which maximizes the following function:",null,null
,,,
137,"F (S) ,  min",null,null
,,,
138,"wq,q , ",null,null
,,,
139," wq,q",null,null
,,,
140,qQ,null,null
,,,
141,q S,null,null
,,,
142,q Q K,null,null
,,,
143,-9,null,null
,,,
144,+ (1 - ),null,null
,,,
145,q,null,null
,,,
146,"i,1 qPiS",null,null
,,,
147,"Modelling the query selection problem in such an objective provides many advantages. Firstly, the submodular formulation provides a natural way of coupling the different aspects of query selection. Secondly, the above formulation can be optimized efficiently and scalably given the monotone submodular form of the function F (S). Assuming we wish to select a subset of N queries from the total unlabelled set of Q queries, the problem reduces to solving the following optimization problem:",null,null
,,,
148,"S , argmax F (S)",null,null
,,,
149,"SQ,|S|N",null,null
,,,
150,-10,null,null
,,,
151,"While solving this problem exactly is NP-complete [10], tech-",null,null
,,,
152,niques like ILP [18] can be used but scaling it to bigger,null,null
,,,
153,datasets becomes prohibitive. Since the function F (S) is,null,null
,,,
154,"submodular, it can be shown that a simple greedy algorithm",null,null
,,,
155,will have a worst-case guarantee of f (S)  0.63F (Sopt) where Sopt is the optimal and,null,null
,,,
156,(S1-is1e,null,null
,,,
157,)F (Sopt)  the greedy,null,null
,,,
158,solution [10]. This constant factor guarantee has practical,null,null
,,,
159,"importance. First, a constant factor guarantee stays the",null,null
,,,
160,"same as N grows, so the relative worst-case quality of the",null,null
,,,
161,solution is the same for small and for big problem instances.,null,null
,,,
162,"Second, the worst-case result is achieved only by very con-",null,null
,,,
163,trived and unrealistic function instances - the typical case is,null,null
,,,
164,almost always much better. The greedy solution works by,null,null
,,,
165,starting with an empty set and repeatedly augmenting the,null,null
,,,
166,set as,null,null
,,,
167,S  S  argmax F (q|S),null,null
,,,
168,-11,null,null
,,,
169,qQ\S,null,null
,,,
170,550,null,null
,,,
171,nQueries 30 50 100 150 250 350 400 500,null,null
,,,
172,SF 0.496& 0.502&,null,null
,,,
173,0.509 0.518& 0.528&,null,null
,,,
174,0.527 0.531& 0.535&,null,null
,,,
175,MQ2007 Dataset,Y,null
,,,
176,LDA,null,null
,,,
177,PL ELO,null,null
,,,
178,0.495,null,null
,,,
179,0.493 0.493,null,null
,,,
180,0.501 0.504,null,null
,,,
181,0.496 0.494 0.510& 0.506,null,null
,,,
182,0.517,null,null
,,,
183,0.510 0.511,null,null
,,,
184,0.527 0.528 0.531&,null,null
,,,
185,0.519 0.523 0.526,null,null
,,,
186,0.517 0.520 0.523,null,null
,,,
187,0.531,null,null
,,,
188,0.530 0.528,null,null
,,,
189,QBC 0.493 0.490 0.500 0.506 0.513 0.525 0.526 0.527,null,null
,,,
190,RDM 0.482 0.485 0.501 0.507 0.516 0.523 0.524 0.526,null,null
,,,
191,nQueries 30 50 100 150 250 350 400,null,null
,,,
192,SF 0.730 0.735 0.741& 0.743,null,null
,,,
193,0.745 0.751& 0.753&,null,null
,,,
194,MQ2008 Dataset,Y,null
,,,
195,LDA PL ELO 0.728 0.722 0.716 0.731 0.731 0.720 0.740 0.739 0.724 0.742 0.740 0.729 0.745 0.748& 0.735 0.749 0.745 0.749 0.750 0.748 0.746,null,null
,,,
196,QBC 0.728 0.734 0.735 0.742 0.746 0.747 0.747,null,null
,,,
197,RDM 0.714 0.721 0.733 0.734 0.740 0.744 0.745,null,null
,,,
198,nQueries 25 30 35 40 45 50,null,null
,,,
199,SF 0.466,null,null
,,,
200,0.464,null,null
,,,
201,0.463 0.478& 0.481& 0.484&,null,null
,,,
202,OHSUMED Dataset,Y,null
,,,
203,LDA,null,null
,,,
204,PL ELO QBC,null,null
,,,
205,0.459,null,null
,,,
206,0.466 0.478&,null,null
,,,
207,0.463 0.473&,null,null
,,,
208,0.476,null,null
,,,
209,0.463 0.454 0.467,null,null
,,,
210,0.465 0.455 0.458,null,null
,,,
211,0.46,null,null
,,,
212,0.468 0.455 0.469,null,null
,,,
213,0.473,null,null
,,,
214,0.456 0.455 0.464,null,null
,,,
215,0.466,null,null
,,,
216,0.467 0.467 0.472,null,null
,,,
217,RDM 0.432 0.462 0.463 0.460 0.473 0.464,null,null
,,,
218,Figure 1: Performance evaluation based on NDCG@10 scores for,null,null
,,,
219,"the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: Permutation Probability Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. nQueries is the number of queries in the labelled set ,"" base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.""",null,null
,,,
220,until we select the N number of queries in the subset we intended.,null,null
,,,
221,"Overall, we select query subsets based on the aforementioned formulations; we next describe in detail the experimental evaluation performed to compare the performances of the three proposed approaches against state-of-the-art baselines.",null,null
,,,
222,7. EXPERIMENTAL EVALUATION,null,null
,,,
223,"We evaluate the proposed query selection strategies on web search ranking and show that the proposed techniques can result in good performance with much fewer labelled queries. We next describe our experimental settings along with the baselines, dataset and evaluation metrics used.",null,null
,,,
224,7.1 Compared Approaches,null,null
,,,
225,We compare the performance of the proposed query selection strategies against existing state-of-the-art approaches. The compared approaches include:,null,null
,,,
226, Query-By-Committee (QBC): The Query-ByCommittee (QBC) approach involves maintaining a committee of models wherein each member is then allowed to vote on the labellings of query candidates. The most informative query is considered to be the instance about which the committee members most disagree. QBC based query selection strategy was used in [5] for ranking adaptation.,null,null
,,,
227," Expected Loss Optimization (ELO): Based on the ELO framework described by Long et al [17], we im-",null,null
,,,
228,"plemented the query-selection phase of the originally proposed 2-phase active learning framework to select queries wherein the most informative queries are selected by optimizing the expected DCG loss. As is mentioned in the original paper, we use score-range normalization to calculate the gain function. For details, please refer to [17].",null,null
,,,
229, Random Query Selection (RDM): Queries are selected randomly for labelling from among the set of unlabelled queries. It is to be noted that random query selection is the primary method used in most settings [6].,null,null
,,,
230, Permutation Probability Model (PL): Our first proposed approach ( 4) based on capturing informativeness of queries via the uncertainty reduction principle.,null,null
,,,
231, Topic Model (LDA): Our second proposed approach ( 5) based on selecting representative queries which are most topically similar to the set of unlabelled queries.,null,null
,,,
232, Submodular Model (SF): Our final proposed approach ( 6) based on the coupled submodular objective which incorporates both the aspects of query informativeness & representativeness.,null,null
,,,
233,7.2 Dataset,null,null
,,,
234,"We use three commonlyused real-world learning to rank datasets: (i) MQ2007; (ii) MQ2008 from LETOR 4.0 which uses query sets from Million Query track of TREC 2007, TREC 2008 and (iii) the OHSUMED test collection, a subset of the MEDLINE database, which is a bibliographic database of important, peer-reviewed medical literature maintained by the National Library of Medicine. It is worth mentioning that the proposed approaches make use of query term information which is not available in many other ranking datasets, hence we restrict our evaluation to these three datasets having query term information. There are 1700 queries in MQ2007, 800 queries in MQ2008 and 100 queries in the OHSUMED dataset. The MQ2007 & MQ2008 datasets are of notable size and query selection indeed makes sense in the such datasets; the OHSUMED dataset, on the other hand, has too few queries to select from which isn't ideal for a query selection scenario. Nevertheless, we compare performances across all datasets.",Y,null
,,,
235,"We adopt a 5-fold cross validation scheme with each fold divided into three parts, one each for training, validation and testing in the ratio 3:1:1. Each query-document pair is represented using 46 features [45 in case of the OHSUMED dataset) along with the relevance score from among {0,1,2}. The test set is used to evaluate the different query selection strategies while active learning is performed on queries from the training set.",Y,null
,,,
236,7.3 Experimental Setting,null,null
,,,
237,"We start with a base set of 40 labelled queries randomly sampled from the entire query set; the rest of the queries form the candidate set. We make use of C ,"" 4 committee members (where applicable) each of which is constructed based on the procedure described earlier (subsection 4.1). To learn the initial ranking models for each of the committee members, we randomly select a sample of 20 queries from the base set of 40 queries and build a ranking model based""",null,null
,,,
238,551,null,null
,,,
239,% Queries 5% 10% 25% 50%,null,null
,,,
240,SF,null,null
,,,
241,0.726 0.735 0.738& 0.745&,null,null
,,,
242,MQ2008 Dataset,Y,null
,,,
243,LDA PL ELO 0.731 0.721 0.729 0.737& 0.733 0.734 0.732 0.733 0.731 0.731 0.734 0.735,null,null
,,,
244,QBC 0.730 0.734 0.730 0.734,null,null
,,,
245,RDM 0.728 0.726 0.727 0.728,null,null
,,,
246,SF,null,null
,,,
247,0.514& 0.508 0.513& 0.516&,null,null
,,,
248,MQ2007 Dataset LDA PL ELO QBC 0.505 0.486 0.501 0.498 0.498 0.501 0.503 0.507 0.509 0.511 0.507 0.511 0.510 0.505 0.509 0.514,null,null
,,,
249,RDM 0.498 0.496 0.504 0.505,null,null
,,,
250,Table 1: Generalizability across different Learning to Rank algorithm: NDCG performance based on ADARANK algorithm. Performance,null,null
,,,
251,"evaluation based on NDCG@10 scores for the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: min-max Plackett-Luce Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. % Queries is the % of queries in the labelled set ,"" base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.""",null,null
,,,
252,"on these queries as training data. We first focus on LambdaMART [11], (a state-of-the-art learning to rank algorithm that was the winner of the Yahoo! Learning to Rank challenge [4]) to build ranking models used in the initial part of our experiments. We later show (subsection 8.2) that the queries selected by this method could also be used by other Learning to Rank algorithms.",null,null
,,,
253,"The entire experiment is repeated multiple times over the 5 folds on each dataset. We perform batch mode Active Learning for queries by selecting a batch of top 10 queries from the candidate set of queries based on the query selection criterion at each round and iteratively add them to our base set. Queries having no relevant documents were ignored while calculating the different metrics. Based on empirical estimation, the threshold parameter in equation 6 was initialized as  ,"" 0.8. For our initial results, we evaluate the performance of the proposed query selection strategies based on their NDCG@10 values in the test set. We later analyse the generalizability of our approach on a different metric (MAP).""",null,null
,,,
254,8. RESULTS,null,null
,,,
255,"We compare the NDCG@10 performance of the test set against the number of queries in training set (base queries plus actively selected) in Figure 1 for the different datasets and compare the performance of the proposed query selection schemes against the QBC, ELO and Random baselines (statistically significant results are highlighted in the respective tables). For all the methods, the NDCG@10 values tends to increase with the number of iterations which is in line with the intuition that the quality of the ranking model is positively correlated with the number of examples in the training set.",null,null
,,,
256,"While min-max PL based query selection stems from the same class of approaches (informativeness based) like the two baselines ELO & QBC, it performs better than both these baselines in most cases; this is in line with our initial claim of capturing informative queries from an alternate view of informativeness based on uncertainty reduction. We observe that LDA Topic Model based query selection performs better than existing baselines as well as the PL model which suggests that the quality of the queries selected by this scheme is better than those selected by other strategies which are mostly based on the informativeness aspect. Perhaps selecting queries based on the informativeness results in some noisy outlier queries getting selected, a case which LDA topic model based query selection avoids by selecting representative queries. The minor fluctuations and occasional dip in the NDCG values on adding more queries",null,null
,,,
257,0.54,null,null
,,,
258,0.535,null,null
,,,
259,0.53,null,null
,,,
260,0.525,null,null
,,,
261,NDCG 10,null,null
,,,
262,0.52,null,null
,,,
263,0.515,null,null
,,,
264,0.51,null,null
,,,
265,0.505,null,null
,,,
266,0.5,null,null
,,,
267,0.495,null,null
,,,
268,0.49 0,null,null
,,,
269,",0.1 ,0.2 ,0.3 ,0.4 ,0.5",null,null
,,,
270,50,null,null
,,,
271,100,null,null
,,,
272,150,null,null
,,,
273,200,null,null
,,,
274,250,null,null
,,,
275,300,null,null
,,,
276,350,null,null
,,,
277,400,null,null
,,,
278,450,null,null
,,,
279,500,null,null
,,,
280,no of queries,null,null
,,,
281,Figure 2: Tradeoff analysis between Informativeness & Representativeness for the MQ2007 datasets. The  coefficient in equation 5 controls the relative importance of the two aspects.,null,null
,,,
282,"to the labelled set could be explained by the fact that some queries are indeed noisy and selecting such queries induces noise in the ranking models, which results in a slightly worse ranker performance.",null,null
,,,
283,"Finally, we observe from the results that the submodular objective (SF) outperforms the baselines as well as (in most cases) our own proposed purely informativeness & purely representativeness based query selection schemes across the different datasets. While purely informativeness based methods tend to select noisy queries, purely representativeness based methods might possibly select queries which are representative but add redundant information. Hence, selecting queries based on the coupled aspects selects queries which are not only representative of other unselected queries, but are also informative to the ranking model.",null,null
,,,
284,8.1 Trade-Off between Informativeness & Representativeness,null,null
,,,
285,"Our main motivation behind introducing the submodular objective was to couple the notions of informativeness and representativeness in a joint coherent manner. Indeed, an ideal subset of queries would be a fine blend of queries which convey the maximal amount of information to the ranking model while at the same time, be characteristic of the unselected set of queries. In Figure 2, we present a example analysis on one of the datasets of the relative importance of the two aspects and how they contribute to the overall ranking performance. As can be seen in the figure, a relative weight-",null,null
,,,
286,552,null,null
,,,
287,% Queries 5% 10% 25% 50%,null,null
,,,
288,SF 0.354& 0.371&,null,null
,,,
289,0.362 0.360,null,null
,,,
290,MQ2008 Dataset,Y,null
,,,
291,LDA PL ELO,null,null
,,,
292,0.354& 0.344 0.341,null,null
,,,
293,0.328 0.369& 0.371&,null,null
,,,
294,0.354 0.357 0.367,null,null
,,,
295,0.362 0.357 0.369,null,null
,,,
296,QBC 0.340 0.361 0.362 0.346,null,null
,,,
297,RDM 0.342 0.352 0.361 0.365,null,null
,,,
298,SF,null,null
,,,
299,0.164 0.164& 0.165& 0.166&,null,null
,,,
300,MQ2007 Dataset LDA PL ELO,null,null
,,,
301,0.147 0.147 0.154 0.146 0.155 0.148 0.158 0.161 0.157 0.166& 0.160 0.153,null,null
,,,
302,QBC 0.163 0.160 0.161 0.135,null,null
,,,
303,RDM 0.149 0.159 0.160 0.159,null,null
,,,
304,Table 2: Generalizability across different Learning to Rank algorithm: AP performance based on Adarank algorithm. Performance evaluation,null,null
,,,
305,"based on NDCG@10 scores for the different algorithms; SF: Submodular function based query selection, LDA: LDA Topic Model based query selection, PL: min-max Plackett-Luce Based Query Selection, ELO: expected loss minimization baseline, QBC: Query-By-Committee baseline, RDM: Random query selection baseline. % Queries is the % of queries in the labelled set ,"" base set + actively labelled queries. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.""",null,null
,,,
306,"ing scheme of  ,"" 0.3 (which weighs representativenessvs-informativeness in 3:7 proportions) works best for query selection. This highlights that while representativeness is important, selecting informative queries from the different topics indeed helps. Also, it must be noted that the informativeness term in Equation 9 not only contains contributions from query's singleton informativeness reward, but also has contributions from the topical segregation of queries into partitions. Overall, we chose the coefficient  "","" 0.3 to weigh the contributions from the two aspects while reporting results. It is to be noted that domain knowledge about the dataset in consideration can be used to vary  accordingly, depending on the desired proportion of representativeness & informativeness.""",null,null
,,,
307,"For a milder sized dataset (MQ2008), putting more weight on informativeness helps initially while the relative contributions tend toequal out once a certain threshold of queries have been selected. Overall, the general weighting factor or  , 0.3 works well consistently across different datasets.",Y,null
,,,
308,8.2 Generalizability Across Learning Algorithms & Metrics,null,null
,,,
309,"For initial results shown before, the query selection method uses LambdaMART as the learning to rank algorithms optimized for the NDCG metric. Since the labelled learning to rank dataset generated as a result of the query selection process could potentially be used in any future ranking systems, the selected queries should ideally be usable by any learning to rank algorithm, optimized for any metric. We analyze such generalization performance in these sets of experiments. While the initial set of results presented above were NDCG values based on LambdaMART ranking algorithm optimizing for NDCG metric, we divert from our original setting and present results on a different ranker: AdaRank [26] in table 1. Similar results for the OHSUMED dataset can be seen in Fig 3. Additionally, we demonstrate the performance of the proposed query selection strategies on a different metric (MAP) and report results in Table 2. Overall, we can see that the proposed query selection methodologies consistently perform better than the baselines across different ranking algorithms and metrics.",Y,null
,,,
310,8.3 Labelling Cost Reduction,null,null
,,,
311,We next analyse the reduction in labelling cost achieved as compared to the case where the entire set of unlabelled queries were labelled. The performance of the ranking function trained with the whole labelled data set is referred to as the optimal performance. When the performance of the active learning model obtained with the proposed algorithms,null,null
,,,
312,Algorithm SF LDA PL ELO,null,null
,,,
313,QBC RDM,null,null
,,,
314,MQ2007,null,null
,,,
315,SS LCR  370 63%  390 61%  490 51%  560 44%  620 39%  720 29%,null,null
,,,
316,MQ2008,Y,null
,,,
317,SS LCR  330 57%  400 48%  510 35%  520 34%  540 31%  570 27%,null,null
,,,
318,OHSUMED,Y,null
,,,
319,SS LCR  45 29%  55 14%  55 14%  60 6%  60 6%  60 6%,null,null
,,,
320,Table 3: The performance in terms of the Labelling Cost Reduction (LCR) and the Saturated Size (SS) for the various compared approaches.,null,null
,,,
321,nQueries 30 40 50,null,null
,,,
322,SF 0.473 0.478 0.478,null,null
,,,
323,OHSUMED Dataset,Y,null
,,,
324,LDA PL ELO QBC 0.469 0.478 0.477 0.478 0.478 0.475 0.472 0.477 0.466 0.469 0.477 0.477,null,null
,,,
325,RDM 0.466 0.467 0.473,null,null
,,,
326,Figure 3: Results on the OHSUMED dataset with LamdaMART,Y,null
,,,
327,"Learning to Rank algorithm. * and & indicates a statistically significant result (t-test, p0.05) when compared to ELO & QBC respectively.",null,null
,,,
328,"is comparable to the optimal performance, we call the size of training data as the saturated size (SS). Table 3 highlights the approximate labelling cost reduction (LCR) results obtained via the proposed query selection techniques. The %ages were calculated based on the average number of queries in the training set. The corresponding values were calculated using the LambdaMART implementation with NDCG metric. Experimental evaluation shows the proposed query selection algorithms indeed require less number of queries to be labelled than baseline methods to achieve comparable ranking performance. It is worth mentioning that at some point, adding more queries to the labelled training set doesn't help improve ranking performance, as can be seen by the results of the RDM algorithm in the table: with about 720 labelled queries out of 1015 queries, the algorithm is able to demonstrate comparable ranking performance.",null,null
,,,
329,9. CONCLUSION & FUTURE WORK,null,null
,,,
330,"We formulated approaches to the query selection problem into two classes: informativeness based and representativeness based strategies and proposed two novel query selection strategies, one from each class respectively: permutation probability based and LDA Topic Model based query selection. Additionally, we argued that an ideal query selection scheme should incorporate insights from both the aspects and presented a principled way of coupling information from the two aspects. Based on rigorous experiments",null,null
,,,
331,553,null,null
,,,
332,we demonstrated the efficacy of the proposed query selection schemes. A possible line of future work could look at enriching the representativeness aspect by adding document level information to the topic model.,null,null
,,,
333,10. REFERENCES,null,null
,,,
334,"[1] J. A. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 468475. ACM, 2009.",null,null
,,,
335,"[2] M. Bilgic and P. N. Bennett. Active query selection for learning rankers. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 10331034. ACM, 2012.",null,null
,,,
336,"[3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. the Journal of machine Learning research, 3:9931022, 2003.",null,null
,,,
337,"[4] C. J. Burges, K. M. Svore, P. N. Bennett, A. Pastusiak, and Q. Wu. Learning to rank using an ensemble of lambda-gradient models. In Yahoo! Learning to Rank Challenge, 2011.",null,null
,,,
338,"[5] P. Cai, W. Gao, A. Zhou, and K.-F. Wong. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 115124. ACM, 2011.",null,null
,,,
339,"[6] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. In Yahoo! Learning to Rank Challenge, 2011.",null,null
,,,
340,"[7] O. Chapelle, Y. Chang, and T.-Y. Liu. Future directions in learning to rank. In Yahoo! Learning to Rank Challenge, 2011.",null,null
,,,
341,"[8] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In Proceedings of the 25th international conference on Machine learning, pages 248255. ACM, 2008.",null,null
,,,
342,"[9] P. Donmez, J. G. Carbonell, and P. N. Bennett. Dual strategy active learning. In Machine Learning: ECML 2007, pages 116127. Springer, 2007.",null,null
,,,
343,"[10] U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM, 1998.",null,null
,,,
344,"[11] Y. Ganjisaffar, R. Caruana, and C. V. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 8594. ACM, 2011.",null,null
,,,
345,"[12] Y. Hamo and S. Markovitch. The compset algorithm for subset selection. In IJCAI, pages 728733, 2005.",null,null
,,,
346,"[13] M. Hosseini, I. J. Cox, N. Milic-Frayling, M. Shokouhi, and E. Yilmaz. An uncertainty-aware query selection model for evaluation of ir systems. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 901910. ACM, 2012.",null,null
,,,
347,[14] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In Proceedings of the 18th annual international ACM SIGIR conference on,null,null
,,,
348,"Research and development in information retrieval, pages 180188. ACM, 1995.",null,null
,,,
349,"[15] J. H. Lee. Analyses of multiple evidence combination. In ACM SIGIR Forum. ACM, 1997.",null,null
,,,
350,"[16] H. Lin and J. Bilmes. Multi-document summarization via budgeted maximization of submodular functions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.",null,null
,,,
351,"[17] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, and B. Tseng. Active learning for ranking through expected loss optimization. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 267274. ACM, 2010.",null,null
,,,
352,"[18] G. L. Nemhauser and L. A. Wolsey. Integer and combinatorial optimization, volume 18. Wiley New York, 1988.",null,null
,,,
353,"[19] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An analysis of approximations for maximizing submodular set functions-i. Mathematical Programming, 1978.",null,null
,,,
354,"[20] R. L. Plackett. The analysis of permutations. Applied Statistics, pages 193202, 1975.",null,null
,,,
355,"[21] K. Raman, P. Shivaswamy, and T. Joachims. Online learning to diversify from implicit feedback. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2012.",null,null
,,,
356,"[22] B. Settles. Active learning literature survey. University of Wisconsin, Madison, 52:5566, 2010.",null,null
,,,
357,"[23] R. Silva, M. A. Gonc퇫lves, and A. Veloso. Rule-based active sampling for learning to rank. In Machine Learning and Knowledge Discovery in Databases, pages 240255. Springer, 2011.",null,null
,,,
358,"[24] M. Taylor, J. Guiver, S. Robertson, and T. Minka. Softrank: optimizing non-smooth rank metrics. In Proceedings of the 2008 International Conference on Web Search and Data Mining. ACM, 2008.",null,null
,,,
359,"[25] R. W. White and D. Kelly. A study on the effects of personalization and task information on implicit feedback performance. In Proceedings of the 15th ACM international conference on Information and knowledge management, pages 297306. ACM, 2006.",null,null
,,,
360,"[26] J. Xu and H. Li. Adarank: a boosting algorithm for information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007.",null,null
,,,
361,"[27] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 662663. ACM, 2009.",null,null
,,,
362,"[28] H. Yu. Svm selective sampling for ranking with application to data retrieval. In Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining, pages 354363. ACM, 2005.",null,null
,,,
363,"[29] Y. Yue and C. Guestrin. Linear submodular bandits and their application to diversified retrieval. In Advances in Neural Information Processing Systems, 2011.",null,null
,,,
364,554,null,null
,,,
365,,null,null

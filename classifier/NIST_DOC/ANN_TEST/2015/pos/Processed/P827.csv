,sentence,label,data
0,On the Reusability of Open Test Collections,null,null
1,Seyyed Hadi Hashemi1 Charles L.A. Clarke2 Adriel Dean-Hall2 Jaap Kamps1 Julia Kiseleva3,null,null
2,"1University of Amsterdam, Amsterdam, The Netherlands 2University of Waterloo, Waterloo, Canada",null,null
3,"3Eindhoven University of Technology, Eindhoven, The Netherlands",null,null
4,ABSTRACT,null,null
5,"Creating test collections for modern search tasks is increasingly more challenging due to the growing scale and dynamic nature of content, and need for richer contextualization of the statements of request. To address these issues, the TREC Contextual Suggestion Track explored an open test collection, where participants were allowed to submit any web page as a result for a personalized venue recommendation task. This prompts the question on the reusability of the resulting test collection: How does the open nature affect the pooling process? Can participants reliably evaluate variant runs with the resulting qrels? Can other teams evaluate new runs reliably? In short, does the set of pooled and judged documents effectively produce a post hoc test collection? Our main findings are the following: First, while there is a strongly significant rank correlation, the effect of pooling is notable and results in underestimation of performance, implying the evaluation of non-pooled systems should be done with great care. Second, we extensively analyze impacts of open corpus on the fraction of judged documents, explaining how low recall affects the reusability, and how the personalization and low pooling depth aggravate that problem. Third, we outline a potential solution by deriving a fixed corpus from open web submissions.",null,null
6,"Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval-- Query formulation, Search process, Selection process",null,null
7,"General Terms: Algorithms, Measurement, Experimentation",null,null
8,1. INTRODUCTION,null,null
9,"Controlled test collections remain crucial for evaluation and tuning of retrieval systems, both for offline testing in industry and for public benchmarks in academia. The TREC Contextual Suggestion Track experimented with an open test collection, where participants were allowed to submit any web page result for a personalized venue recommendation task. This option proved exceedingly popular amongst participants of the track, e.g., in 2014 the track received 25",null,null
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00.",null,null
11,DOI: http://dx.doi.org/10.1145/2766462.2767788.,null,null
12,"open web submissions against 6 runs based on ClueWeb12. We focus here exclusively on the open web submissions, and investigate the reusability of the resulting open web test collection. There are at least three factors that may impede the reusability of the resulting test collection. First, the open nature may result in little to no overlap between the submissions, frustrating the pooling effect and limiting its evaluation power. Second, the track includes personalization of results to a specific user profile, hence a ""topic"" consists of the main statement of request (in this case a North American city) and a profile of the requester. Third, the resulting pooling depth over submissions per topic (i.e., a unique context and profile pair) are limited to rank 5. It is well known that low pool depth affects reusability [7]. The key factor in case of sparse judgments is the presence or absence of pooling bias [1].",null,null
13,"In this paper, our main aim is to study the question: How reusable are open test collections? Specifically, we answer the following research questions:",null,null
14,1. How does the open nature affect the evaluation of nonpooled systems?,null,null
15,(a) What is the effect of leave out uniques on the score and ranking over all systems?,null,null
16,(b) What is the effect of leave out uniques on the score and ranking over top ranked systems?,null,null
17,2. How does the open nature affect the fraction of judged documents?,null,null
18,(a) What is the fraction of judged documents over ranks?,null,null
19,(b) What is the effect of personalization on the fraction of judged documents?,null,null
20,2. EXPERIMENTAL DATA,null,null
21,"The TREC Contextual Suggestion Track asks participants to submit venue recommendations (in the form of a valid URL). We give some statistics of the open web submissions in 2014. There were a total of 25 submissions by 14 teams (with 11 teams submitting 2 runs). A topic consists of a pair of both a context (a North American city) and a profile of the requester (consisting of likes and dislikes of venues in another city). For example, to recommend venues to visit in the unknown city of Buffalo, NY, based on a profile with ratings of attractions in Chicago, IL. Runs were pooled at depth 5 and in total 299 context-profile pairs were judged, with an average of 28.2 unique judged venues per pair, hence 8,441 judgments in total. Details of the run and their P@5 scores are shown later in Table 2.",null,null
22,827,null,null
23,LORO bpref,null,null
24,LORO MAP,null,null
25,LORO P@5,null,null
26,1,null,null
27,"Kendall  , 0.62 ap corr , 0.44",null,null
28,"avg diff. , 0.42",null,null
29,0.5,null,null
30,0.2,null,null
31,"Kendall  , 0.66 ap corr , 0.58 avg diff. , 0.36",null,null
32,0.1,null,null
33,0.2,null,null
34,"Kendall  , 0.70 ap corr , 0.52 avg diff. , 0.17",null,null
35,0.1,null,null
36,0,null,null
37,0,null,null
38,0.5,null,null
39,1,null,null
40,Actual P@5,null,null
41,0 0,null,null
42,0.1,null,null
43,0.2,null,null
44,Actual MAP,null,null
45,0 0,null,null
46,0.1,null,null
47,0.2,null,null
48,Actual bpref,null,null
49,"Figure 1: Difference in P@5, MAP, and bpref based on the leave one run out (LORO) test.",null,null
50,1,null,null
51,"Kendall  , 0.39 ap corr , 0.34",null,null
52,"avg diff. , 0.78",null,null
53,0.5,null,null
54,0.2,null,null
55,"Kendall  , 0.48 ap corr , 0.48 avg diff. , 0.73",null,null
56,0.1,null,null
57,0.2,null,null
58,"Kendall  , 0.64 ap corr , 0.56 avg diff. , 0.44",null,null
59,0.1,null,null
60,LOTO bpref,null,null
61,LOTO MAP,null,null
62,LOTO P@5,null,null
63,0,null,null
64,0,null,null
65,0.5,null,null
66,1,null,null
67,Actual P@5,null,null
68,0 0,null,null
69,0.1,null,null
70,0.2,null,null
71,Actual MAP,null,null
72,0 0,null,null
73,0.1,null,null
74,0.2,null,null
75,Actual bpref,null,null
76,"Figure 2: Difference in P@5, MAP, and bpref based on the leave one team out (LOTO) test.",null,null
77,3. IMPACT ON REUSABILITY,null,null
78,"This section studies the reusability of the test collection, aiming to answer our first research question: How does the open nature affect the evaluation of non-pooled systems?",null,null
79,3.1 Leave Out Uniques Analysis,null,null
80,We first look at the question: What is the effect of leave,null,null
81,out uniques on the score and ranking over all systems? Specif-,null,null
82,"ically, we perform both the leave-one-run-out [7] and leave-",null,null
83,one-team-out [1] experiments to see what would have hap-,null,null
84,pened if a run had not contributed to the pool of judged,null,null
85,documents. We also measure the effect on the runs' scores,null,null
86,as well as their system ranking--as the main goal of a test,null,null
87,collection is to determine the system ranking rather than,null,null
88,absolute scores. The standard system rank correlation mea-,null,null
89,sure,null,null
90,in,null,null
91,IR,null,null
92,research,null,null
93,is,null,null
94,Kendall's,null,null
95,(i.e.,null,null
96,",",null,null
97,N,null,null
98,C-D (N -1)/2,null,null
99,"),",null,null
100,where,null,null
101,"C is the number of concordant pairs, D is the number of dis-",null,null
102,"cordant pairs, and N is the number of systems in the given",null,null
103,"two rankings [6]. However, there are a number of researches",null,null
104,studied that the Kendall's  is not promising in some con-,null,null
105,"ditions [2, 3, 6]. In order to more precisely measure the test",null,null
106,"collection reusability, we also use AP Correlation Coefficient",null,null
107,n,null,null
108,"(i.e.,",null,null
109,AP,null,null
110,",",null,null
111,2 N -1,null,null
112,·,null,null
113,(,null,null
114,C(i) i-1,null,null
115,),null,null
116,-,null,null
117,"1),",null,null
118,where,null,null
119,C (i),null,null
120,is,null,null
121,the,null,null
122,number,null,null
123,"i,2",null,null
124,of systems above rank i and correctly ranked [6].,null,null
125,Leave One Run Out.,null,null
126,"In a leave-one-run-out (LORO) experiment, we exclude a pooled run's unique judgments from the test collection, and",null,null
127,"evaluate the run based on the new test collection in terms of P@5, MAP, or bpref metrics. This test is done for all of the pooled runs--hence for each run we obtain the score as if it had not been pooled and judged. Then, the ranking correlation of the official ranking of runs with the new one is estimated. In Figure 1, reusability of the test collection is evaluated based on the mentioned metrics. The Kendall's  of this experiment based on P@5, MAP and bpref metrics are much lower than 0.9 that is the threshold usually considered as the correlation of two effectively equivalent rankings [4].",null,null
128,"Moreover, difference of actual P@5, MAP and bpref and the ones based on LORO test is shown in Figure 1. As it is shown in this figure, average difference of MAP is 0.36 which is much higher than the ones reported for reusable test collections (e.g., from 0.5 to 2.2 [1, 5]). Figure 1 shows that bpref is a more reliable metric in comparison to (mean average) precision.",null,null
129,Leave One Team Out.,null,null
130,"The LORO experiment can be biased in case teams' submit closely related runs containing many mutual venues. In reality, a non-pooled system might use completely different collection than the ones used by the pooled runs. Hence, we also conduct a leave-one-team-out (LOTO) experiment. Figure 2 demonstrates the same pattern as observed above for the LORO experiment, with somewhat lower rank correlations, and larger differences in scores. Again, bpref remains the most stable of the three measures.",null,null
131,828,null,null
132,Table 1: Reusability in top of the ranking,null,null
133,Metric,null,null
134,Depth P@5 MAP bpref 5 All,null,null
135,Kendall  Kendall sig Bias,null,null
136,0.800 0.800 0.000 1.000 0.777 1.000 0.000 0.111 0.000,null,null
137,Kendall  Kendall sig Bias,null,null
138,0.393 0.480 0.646 0.418 0.572 0.691 0.290 0.213 0.154,null,null
139,3.2 Top Ranked Systems,null,null
140,"The leave out uniques experiments give a clear call to caution on the reuse of the open web judgments, but we observe in the scatter plots that the top ranked runs seem to fare slightly better. Hence, we look at the question: What is the effect of leave out uniques on the score and ranking over top ranked systems? We look both at Kendall's  and the sig, which only consider significant inversions [3]. We also look at bias, which is the fraction of all significant pairs that are significant inversions [3]. We use a paired Student's t-test with  ,"" 0.05 is used to find significant inversions (i.e., p < ). Table 1 reports the more critical LOTO test. Over all runs, we see that sig is somewhat better than  but still low enough to be very careful with using the resulting test collection for evaluating non-pooled runs. Over the top ranked systems (based on P@5 as reported in Table 2), the bias,  and sig correlations are substantially better.""",null,null
141,"In this section we looked at the leave out uniques analysis for the open test collection in both leave run and leave team out experiments. The outcome is mixed at best, while there is a strongly significant rank correlation, the effect of pooling is notable, and results in underestimation of score and hence affects the ranking. Although we observe a somewhat more reliable evaluation of the better scoring systems, this means that the judgments should be used with caution, and evaluating non-pooled systems requires great care.",null,null
142,4. IMPACT ON JUDGED DOCUMENTS,null,null
143,"This section studies in more detail the factors contributing to the observed low reusability, trying to answer our second research question: How does the open nature affect the fraction of judged documents?",null,null
144,4.1 Fraction of Judged Documents,null,null
145,We first look at the question: What is the fraction of judged documents over ranks? We define Overlap@N as the fraction of the top - N suggestions that is judged for the given set of topics:,null,null
146,Overlap@N (,null,null
147,"C, P",null,null
148,"),",null,null
149,"1 | C,P",null,null
150,|,null,null
151,#Judged@N ( N,null,null
152,"c,p",null,null
153,"),",null,null
154,"c,p  C,P",null,null
155,"where #Judged@N ( c, p ) corresponds to the count of judged suggestions for the given context and profile pair c, p in the top-N suggestions, and C, P is a set of judged context and profile pair. Table 2 shows the overlap@N of runs submitted to the contextual suggestion track in 2014. We see a significant drop after the pooling cut-off at rank 5, signaling that the recall base may be incomplete and the overlap between",null,null
156,Table 2: Overlap@N and P@5 of each pooled open web run based on the official TREC judgments,null,null
157,Run,null,null
158,Overlap@N (%),null,null
159,P@5 (%),null,null
160,"N,5 N,10 N,25 N,50",null,null
161,BJUTa,null,null
162,100.00 61.43 32.38 20.65,null,null
163,BJUTb,null,null
164,100.00 60.26 32.10 20.23,null,null
165,BUPT PRIS 01 44.88 23.24 09.36 04.68,null,null
166,BUPT PRIS 02 47.02 25.21 10.27 05.13,null,null
167,cat,null,null
168,99.93 59.06 31.90 19.83,null,null
169,choqrun,null,null
170,97.85 57.52 31.47 16.19,null,null
171,dixlticmu,null,null
172,100.00 59.49 32.33 21.46,null,null
173,gw1,null,null
174,97.99 51.97 24.98 14.21,null,null
175,lda,null,null
176,100.00 53.57 24.73 14.31,null,null
177,RAMARUN2 100.00 57.99 27.78 15.53,null,null
178,run DwD,null,null
179,99.53 61.00 35.30 24.68,null,null
180,run FDwD,null,null
181,99.59 79.79 37.61 23.68,null,null
182,RUN1,null,null
183,99.93 58.56 28.58 16.00,null,null
184,simpleScore,null,null
185,100.00 58.82 28.60 16.25,null,null
186,simpleScoreImp 100.00 59.43 28.86 16.34,null,null
187,tueNet,null,null
188,99.86 52.64 23.90 14.33,null,null
189,tueRforest,null,null
190,99.86 52.64 23.90 14.33,null,null
191,UDInfoCS2014 1 100.00 57.45 28.64 17.35,null,null
192,UDInfoCS2014 2 100.00 59.36 31.41 19.83,null,null
193,uogTrBunSumF 100.00 55.75 22.38 11.20,null,null
194,uogTrCsLtrF,null,null
195,100.00 55.75 27.30 16.40,null,null
196,waterlooA,null,null
197,99.79 64.28 31.10 19.67,null,null
198,waterlooB,null,null
199,99.79 59.53 30.50 19.36,null,null
200,webis 1,null,null
201,98.59 56.75 27.50 15.16,null,null
202,webis 2,null,null
203,98.59 56.75 27.50 15.16,null,null
204,50.57 50.37 14.45 14.25 20.94 22.47 39.13 10.99 08.43 49.97 31.44 42.41 49.36 44.88 45.22 22.81 22.81 40.74 55.72 48.63 39.26 42.21 43.08 45.69 45.69,null,null
205,Average,null,null
206,95.33 55.93 27.62 16.48 36.06,null,null
207,0.6,null,null
208,0.6,null,null
209,0.4,null,null
210,0.4,null,null
211,P@5 P@5,null,null
212,0.2,null,null
213,0.2,null,null
214,0.1 0.2 0.3 0.4,null,null
215,0.1 0.2,null,null
216,Overlap@25,null,null
217,Overlap@50,null,null
218,Figure 3: Overlap@N versus P@5 for open web runs.,null,null
219,the different runs is relatively low. Clearly the lack of a fixed collection will have contributed to this.,null,null
220,"In order to investigate the relation of the fraction of judged pages with the pooled runs' effectiveness, we plot Overlap@N vs. P@5 (i.e. the main official metric in this track) in Figure 3. Points in the graph represent pooled runs. Arguably, evaluating the best runs reliably is more important than separating the blatant failures. As it is shown in Figure 3, runs having higher P@5 usually have higher Overlap@N. This explains why for the evaluation is more reliable for the better performing runs. This figure also shows two runs that are outliers in terms of low fractions of judged documents. These two runs did usually provide fewer than 5 venues for the given topics.",null,null
221,829,null,null
222,1 0.8,null,null
223,Overlap@N OverlapL @N,null,null
224,0.6,null,null
225,0.4,null,null
226,0.2,null,null
227,0 0,null,null
228,10,null,null
229,20,null,null
230,30,null,null
231,40,null,null
232,50,null,null
233,N Figure 4: Effect of lenient judgments on Overlap@N.,null,null
234,1 0.8,null,null
235,Overlap@[m - n] OverlapL @[m - n],null,null
236,0.6,null,null
237,0.4,null,null
238,0.2,null,null
239,0 1-5 6-10 11-15 16-20 21-25 26-30 31-35 36-40 41-45 46-50 m-n,null,null
240,Figure 5: Overlap@N over rank intervals.,null,null
241,4.2 Impact of Personalization,null,null
242,"We now look at the question: What is the effect of personalization on the fraction of judged documents? Specifically, we exploit the fact that contexts (i.e., cities) are judged for multiple profiles of the same (and other) submissions: in the case that the relevance of a venue to the given context is not judged for the given profile, judgments made for other profiles will be used. We define Lenient Overlap (i.e., OverlapL@N) that is an instance of Overlap@N, in which #Judged@N is calculated by ignoring profile assumption. The results are shown in Figure 4, which shows that ignoring the exact profile substantially improves the fraction of judged pages.",null,null
243,"To highlight the number of judged pages after the pooling depth, we show the same data in an interval level analysis in Figure 5. Obviously, for pooled runs, the Overlap@5 is guaranteed to be 1, making Overlap@10 guaranteed to be at least 0.5, etc. This shows the drop in fraction of judged paged for the personalized runs in an even more dramatic way. The lenient profile-ignorant overlap measure however remains more stable over the intervals. This signals that the relatively low fractions of judged pages can be attributed for some part to the low pool depth and personalization, rather than the open nature of the test collection.",null,null
244,"This section looked at the fraction of judged pages in the open web submissions. The outcome clearly show the low recall: after the pooling depth the fraction plummets down, explaining the relatively low reusability of the open web judgments. We looked in the relative contribution of the open nature of the collection and the personalization and pool depth, which suggested that the latter play a major role in explaining the low fraction of overlap.",null,null
245,5. CONCLUSIONS,null,null
246,"We have studied reusability of the TREC 2014 Contextual Suggestion open test collection in terms of the reusability of the judgments to evaluate non-pooled runs and in terms of fraction of judged venues. We analyzed the effectiveness of the pool for building a reusable test collection. Experimental results of leave out uniques (i.e., run or a team) tests based on various metrics, including Kendall's  , AP correlation and average difference, showed that the test collection should be used with extreme care: non-pooled systems tend to be underestimated. However, for the high quality runs (i.e., top-5 of the ranking), the test collection performs somewhat better and had the highest correlation with the official ranking in terms of the  based on significant inversions. Our empirical investigation has also shown that using an open collection tends to produce a diverse pool and consequently less fraction of judged venues at ranks deeper than the pool cut-off (e.g., only 16% overlap at ranks between 6 and 10). In addition, we looked at the role of personalization and low pooling depth, and showed that the lenient profileignorant fractions of judged page leads to considerable larger fractions of judged documents.",null,null
247,"Our general observation is that the open collection leads to significantly lower recall, and low fraction of judged results, over individual runs. There are several ways in which this could be addressed. First, it is still an open question on whether we can derive a post hoc corpus and test collection from the open web submissions, by constructing a corpus based on the combined retrieved pages, and use this to evaluate runs over the combined set. We have done an initial analysis of this approach showing promising results. Second, the organizers of the TREC 2015 contextual suggestion track aim to collect open web results as a pre-task in early 2015, and use these submissions to construct a fixed open web collection shared to all track participants. The results of this paper give support to the creation of a fixed collection of open web results, and suggest that this will substantially increase the reusability of the benchmark for non-pooled runs in follow up experiments.",null,null
248,"Acknowledgments This research is party funded by the European Community's FP7 (project meSch, grant # 600851).",null,null
249,References,null,null
250,"[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information retrieval, 10(6):491­508, 2007.",null,null
251,"[2] B. Carterette. On rank correlation and the distance between rankings. In SIGIR, pages 436­443, 2009.",null,null
252,"[3] G. V. Cormack and T. R. Lynam. Power and bias of subset pooling strategies. In SIGIR, pages 837­838, 2007.",null,null
253,"[4] E. M. Voorhees. Evaluation by highly relevant documents. In SIGIR, SIGIR '01, pages 74­82. ACM, 2001.",null,null
254,"[5] E. M. Voorhees, J. Lin, and M. Efron. On run diversity in evaluation as a service. In SIGIR, pages 959­962, 2014.",null,null
255,"[6] E. Yilmaz, J. A. Aslam, and S. Robertson. A new rank correlation coefficient for information retrieval. In SIGIR, pages 587­594, 2008.",null,null
256,"[7] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, pages 307­314, 1998.",null,null
257,830,null,null
258,,null,null

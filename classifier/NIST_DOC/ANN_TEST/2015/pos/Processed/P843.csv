,sentence,label,data
0,Towards Quantifying the Impact of Non-Uniform,null,null
1,Information Access in Collaborative Information Retrieval,null,null
2,Nyi Nyi Htun,null,null
3,"SEBE Glasgow Caledonian University Glasgow, G4 0BA, Scotland, UK",null,null
4,nyinyi.htun@gcu.ac.uk,null,null
5,Martin Halvey,null,null
6,"Department of CIS University of Strathclyde Glasgow, G1 1XQ, Scotland, UK",null,null
7,martin.halvey@strath.ac.uk,null,null
8,Lynne Baillie,null,null
9,"Department of CS Heriot-Watt University Edinburgh, EH14 4AS, Scotland, UK",null,null
10,lynne.baillie@hw.ac.uk,null,null
11,ABSTRACT,null,null
12,"The majority of research into Collaborative Information Retrieval (CIR) has assumed a uniformity of information access and visibility between collaborators. However in a number of real world scenarios, information access is not uniform between all collaborators in a team e.g. security, health etc. This can be referred to as Multi-Level Collaborative Information Retrieval (MLCIR). To the best of our knowledge, there has not yet been any systematic investigation of the effect of MLCIR on search outcomes. To address this shortcoming, in this paper, we present the results of a simulated evaluation conducted over 4 different non-uniform information access scenarios and 3 different collaborative search strategies. Results indicate that there is some tolerance to removing access to the collection and that there may not always be a negative impact on performance. We also highlight how different access scenarios and search strategies impact on search outcomes.",null,null
13,Categories and Subject Descriptors,null,null
14,H.3.3 Information Search and Retrieval,null,null
15,General Terms,null,null
16,"Measurement, Performance, Experimentation.",null,null
17,Keywords,null,null
18,Collaborative search; non-uniform access; effectiveness measures,null,null
19,1. INTRODUCTION,null,null
20,"Collaborative Information Retrieval (CIR) involves people with common information needs working together, exploring and collecting useful information, and collectively making decisions that help them move toward their common goal. A simple example might be of a group of colleagues collaborating for a project where they may, individually or together, go through a number of information resources and then discuss their results, exchanging information and knowledge in order to contribute to the project.",null,null
21,"A common assumption in much of the research in CIR is that all members of a team have equal access to the information sources, tools etc., and that they may share any relevant information they find with each other without any restriction [4, 5, 11]. However, in reality it may not always be the case that all searchers have equal information access. There are numerous situations where societal, legal or security reasons may prevent a searcher from",null,null
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '15, August 09 - 13, 2015, Santiago, Chile © 2015 ACM. ISBN 978-1-4503-3621-5/15/08...$15.00 DOI: http://dx.doi.org/10.1145/2766462.2767779",null,null
23,"sharing information within or out with a group. Handel and Wang [6] presented an example of such a scenario involving two intelligence analysts engaged in collaborative search, where one analyst is a signal intelligence specialist and the other a human intelligence specialist. Despite their unequal access to intelligence databases and underlying intelligence, as well as differing information needs and shareability, the two analysts must collaborate to achieve an outcome. This type of scenario was referred to as Multi-Level Collaborative Information Retrieval (MLCIR) [6]. Similar scenarios have been examined by other researchers who have looked at the effect of organisational structure in legal search [2], crisis management [3] and healthcare [10] to gain a better understanding of how these can impede collaboration. Others have considered how different roles within a search team might be leveraged to assist with CIR. For example, Pickens et al. [12] studied the impact of having two different roles in a collaborative exploratory search team, and looked into developing algorithms to support this. However, the main focus of these studies has been on the division of labour in CIR and although, to date, having different roles has been viewed as positive in collaborative search tasks, it might not always be. In fact, MLCIR is different from division of labour in that any system that supports MLCIR has to be aware of information flow, accessibility and shareability between collaborators [6]. Thus many of the concepts previously used to support CIR such as awareness, sense-making and persistence [4, 5, 11] may need to be revised.",null,null
24,"Previous research [2, 3, 9, 10] has focused primarily on qualitative observations which may not be completely applicable in all nonuniform information access scenarios. To the best of our knowledge, there has yet to be a systematic evaluation on the impact of non-uniform information access within a team of searchers. We attempt to overcome this shortcoming by conducting a simulated user evaluation where we investigate the impact of two different kinds of non-uniformity in access, namely removing document access and search-term blacklisting for team members (Details are presented in Section 2.2). There are three main research questions that we attempt to answer in this paper:",null,null
25,1. What is the impact of non-uniform information access on the outcomes of CIR?,null,null
26,2. Do different types of non-uniformity have different impacts on CIR outcomes?,null,null
27,3. Are there scenarios where non-uniform access may be beneficial to CIR outcomes?,null,null
28,2. EXPERIMENTAL DESIGN,null,null
29,"As there are a number of potential parameters for collaboration and non-uniformity in information access, we decided to use a simulated study. This approach means that we can more easily compare different variables and combinations than in a user evaluation. In future work, we anticipate exploring the findings from this study in more depth with a user evaluation.",null,null
30,843,null,null
31,"2.1 Data, Topic and Search Strategies",null,null
32,"Our evaluation followed the same procedure as Joho et al.'s simulation of collaborative search [7], with some small changes as outlined below. We utilised the TREC HARD 2005 [1] collection (AQUAINT corpus) and topics. For their study, Joho et al. [8] generated a query pool through a user evaluation for 13 of the topics. We were provided with this query pool and thus use the same 13 topics (303, 344, 363, 367, 383, 393, 397, 439, 448, 625, 651, 658 689). The query pool has a total of 1157 queries across the 13 topics and each query contains up to 9 terms.",null,null
33,"Joho et al. [7] simulated teams of searchers (of variable size from 2 to 5) to carry out collaborative search tasks. Each team had 20 search iterations per topic. During each iteration, a team member selected a random query from the query pool and was assumed to judge 20 documents per iteration. For simplicity in our evaluation we simulate a pair of users rather than vary team size, as this would introduce extra complexity, whereby combining a multitude of possible access combinations could become intractable. In other words, we assume that there are always 2 people in a search team for any given search session and the team performs 20 search iterations. Thus each individual in a team would judge a maximum of 400 documents per topic, with a team judging a maximum of 800 documents. One of the goals of Joho et al. was to compare a number of collaborative search strategies [7]; we utilise 3 of these search strategies for our study. These 3 strategies are:",null,null
34,"1) Independent Search (IS): team members judge documents independently without any interaction between each other, and have their results merged at the end of each search iteration.",null,null
35,2) Independent Relevance Feedback (IRF): same as (1) but query expansion is performed based on their independent relevance feedback and then the expanded queries are resubmitted independently to the system. Team members do not share any knowledge on relevancy of documents.,null,null
36,"3) Shared Relevance Feedback (SRF): same as (2) but the query expansion is performed based on the relevance feedback of both members. Thus, team members share knowledge on relevancy of the documents.",null,null
37,"For Joho et al. [7], IS was the most basic and simplest search strategy whereas the other two were the most effective. Due to its simplicity, IS is also the easiest to compare directly with any other search strategies in terms of performance, collection coverage, etc. The other two strategies chosen were the best performing in their experiments.",null,null
38,2.2 Access Scenarios and Combinations,null,null
39,"We devised 4 scenarios to simulate non-uniform information access amongst team members completing a collaborative search task; these are summarised in Table 1 and outlined in detail below. For each scenario, we assumed that each of the two searchers have access to more or less of the collection relative to their search partner. For example, in one case, one searcher might be able to access only 10% of the collection while their partner can access 20% of the collection. Also, there is a possibility that one searcher cannot retrieve any documents that contain certain phrases or terms.",null,null
40,"Therefore, starting with S1 (document removal), we began by indexing a random selection of 10% of the documents from the document collection. Then an iterative process was adopted whereby we increased the percentage of documents indexed by 10% until 100% of the collection had been indexed. This resulted",null,null
41,"in 10 different indexes for each person and 55 possible access combinations of indexes for two people (i.e. combinations of 10%-10%, 10%-20%, 10%-30%, 10%-40%; up to 100%-100%). This simulates a scenario laid out by Handel and Wang [6] where a person with higher security clearance may have access to more documents than a subordinate.",null,null
42,Table 1. Information access scenarios,null,null
43,Code S1 S2,null,null
44,S3,null,null
45,S4,null,null
46,Scenario Remove access to documents from collection,null,null
47,Term blacklisting ­ remove access to random terms from the collection,null,null
48,Term blacklisting ­ remove access to terms based on their frequency in documents,null,null
49,Term blacklisting - remove access to terms based on their frequency in query pool,null,null
50,"Scenarios S2, S3 and S4 simulate term blacklisting, this is a major problem highlighted by Handel and Wang [6]. For S2, we began by analysing the collection for a list of terms. After that, we indexed the entire corpus meaning there is complete access. We then created other indexes by iteratively removing 10% of the terms randomly, until only 10% remained. This also resulted in 55 possible combinations of indexes for 2 individuals. Scenarios S3 and S4 took a more systematic approach. We analysed term frequencies in both collection and query pool, which contain 841498 and 591 unique terms respectively. We then followed the same procedure as S2 but instead of removing random terms we removed terms based on their frequencies in the collection and in the query pool respectively for S3 and S4. Therefore, for S3 the first 10% removed were the most frequent terms in the collection whereas for S4 those were the most frequent terms in the query pool. In each scenario we had 10 indexes for each team member and 55 different access combinations, although the indexes in S4 are of different size to S1, S2 and S3 because in S1, S2 and S3 we can theoretically exclude everything from the collection whereas for S4 this is dependent on the query pool.",null,null
51,"Thus for each scenario, there are 55 possible combinations; for each of these combinations, we conducted each search simulation 10 times in order to reduce randomness and inconsistencies. In total, there were 1,716,000 search sessions performed by teams in our simulation (i.e. 3 search strategies x 4 access scenarios x 10 runs x 55 combinations x 13 topics x 20 iterations). For all of the indexing and retrieval, we used the Inverted File indexing method and BM25 retrieval algorithm, these were developed using the Terrier1 library with out of the box settings.",null,null
52,2.3 Evaluation Measures,null,null
53,"For the evaluation we utilised traditional IR evaluation metrics: recall, precision and f-measure in conjunction with specific metrics for CIR proposed by Shah and González-Ibáñez [13]: coverage, relevant coverage, unique coverage and unique relevant coverage. Coverage is the average number of distinct documents discovered by the team throughout the entire search session. Relevant coverage is the average number of documents in coverage that are actually relevant. Unique coverage is the average number of distinct documents that are only discovered in a given access combination, and not in any other. Unique relevant coverage is the average number of documents in unique coverage that are actually relevant.",null,null
54,1 http://terrier.org,null,null
55,844,null,null
56,3. RESULTS,null,null
57,"Table 2 shows the access combinations which yield the highest values for recall, precision and f-measure across all access scenarios and search strategies and Table 3 shows those for coverage, relevant coverage, unique coverage and unique relevant coverage. As our data was not normally distributed, for each measure across 4 access scenarios and 3 search strategies, we conducted a Friedman analysis to compare the 55 access combinations (i.e. 10-10, 20-10, 20-20, 30-10, etc.) and found that there was a statistically significant difference in every case. Post hoc analysis with Wilcoxon signed-rank tests was conducted with a Bonferroni correction applied, resulting in a significance level set at p<0.00003367. We present more detailed results of the pairwise comparisons in the following sub-sections. For reasons of space as there were many comparisons we do not present all of these comparisons.",null,null
58,3.1 Search Performance,null,null
59,"Our first research question examined the impact of non-uniform information access on the outcomes of CIR. First of all, statistical analysis of recall, precision and f-measure values showed a number of access combinations that were not significantly different from the best performing access combinations. However, what was interesting among these is that for S1, S2 and S4, relevance feedback search strategies had a very high number of combinations that are not significantly different from their best performing access combinations (ranging from 50-20 to 90-60 for S1, 70-70 to 90-80 for S2, and 70-70 to 100-80 for S4) whereas the IS strategy had only a few (90-80, 90-90, 100-80, 100-100 for S1; 90-90, 100-10, 100-60 100-90 for S2; 90-90, 100-90 for S4). It suggests that in terms of recall, precision and f-measure non-uniform access for S1, S2 and S4 had very little effect when relevance feedback strategies were employed.",null,null
60,"Table 2. Highest recall, precision and f-measure values with their respective access combinations. * indicates those values",null,null
61,at full access (i.e. 100-100),null,null
62,Recall,null,null
63,Precision,null,null
64,Independent Search,null,null
65,S1,null,null
66,0.0859 (100-90) 0.0829*,null,null
67,0.2459 (100-90) 0.23898*,null,null
68,S2,null,null
69,0.0813 (100-100),null,null
70,0.2349 (100-100),null,null
71,S3,null,null
72,0.0818 (100-100),null,null
73,0.2446 (100-20) 0.2353*,null,null
74,S4,null,null
75,0.0830 (100-100),null,null
76,0.2389 (100-100),null,null
77,Independent Relevance Feedback,null,null
78,S1,null,null
79,0.1210 (90-90) 0.0383*,null,null
80,0.3576 (90-90) 0.1302*,null,null
81,S2,null,null
82,0.1110 (90-90) 0.0376*,null,null
83,0.3273 (90-90) 0.1266*,null,null
84,S3,null,null
85,0.1241 (90-90) 0.0370*,null,null
86,0.3931 (90-90) 0.1244*,null,null
87,S4,null,null
88,0.0904 (90-90) 0.0376*,null,null
89,0.2711 (90-90) 0.1295*,null,null
90,Shared Relevance Feedback,null,null
91,S1,null,null
92,0.1001 (90-30) 0.0325*,null,null
93,0.3317 (80-70) 0.1756*,null,null
94,S2,null,null
95,0.0836 (90-90) 0.0324*,null,null
96,0.4197 (90-90) 0.1748*,null,null
97,S3,null,null
98,0.1006 (90-90) 0.0323*,null,null
99,0.5208 (90-90) 0.1745*,null,null
100,S4,null,null
101,0.0762 (100-90) 0.0324*,null,null
102,0.3570 (90-90) 0.1748*,null,null
103,F-measure,null,null
104,0.1270 (100-90) 0.1227*,null,null
105,0.1204 (100-100),null,null
106,0.1210 (100-100),null,null
107,0.1228 (100-100),null,null
108,0.1802 (90-90) 0.0604*,null,null
109,0.1653 (90-90) 0.0588*,null,null
110,0.1878 (90-90) 0.0572*,null,null
111,0.1350 (90-90) 0.0580*,null,null
112,0.1502 (90-30) 0.0548*,null,null
113,0.1391 (90-90) 0.0554*,null,null
114,0.1683 (90-90) 0.0544*,null,null
115,0.1173 (100-90) 0.0551*,null,null
116,"Looking at Table 2, we found that when the IS strategy was employed for S1, the values of the 3 measures (recall, precision and f-measure) were highest at non-full access (i.e. 100-90) whereas for the rest of the scenarios (S2, S3 and S4) the values reached the highest at full access. When relevance feedback strategies were employed, however, it was found that the values reached the highest at non-full access (mostly at 90-90) for all 4 scenarios (S1, S2, S3 and S4). This suggests that there is some tolerance to removing access from the collection, and while it was expected that there would be a decrease in performance when access had been reduced, there were some cases which indicate that there may not always be a negative impact on performance. In addition, as mentioned earlier, our statistical test results revealed a number of combinations that are not significantly different from the best performing access combinations, which suggests that there are certain combinations that allow search performance to be comparable to the best performing access combination regardless of the users' unequal, or equal but not full (e.g. 90-90) access to the collection. This finding addresses our third research question. Moreover, the statistical test results also showed us that depending on the type of access scenario and search strategies being utilised, the resulting combinations were different, and thus resulted in different outcomes, addressing our second research question.",null,null
117,3.2 Collection Coverage,null,null
118,"In terms of coverage for the document removing scenario (S1), statistical test results showed that in all 3 search strategies, there were many access combinations which were not significantly different from the best performing access combination and also represent the case where team members had access to a very diverse amount of the collection from each other (these are 50-10, 60-10, 70-10, 80-10, 80-20, 90-10, 90-20, 100-10, 100-20, 10030). It appears that regardless of the search strategy, reducing access to documents for one member of the team means that a different member can make judgements about different parts of the collection thereby covering similar amount of documents as they would in the best performing access combinations. This finding is in contrast to term blacklisting scenarios (S2, S3 and S4) in which most combinations that are not significantly different from the best performing access combination represent the case where both team members had a higher access to the collection (e.g. 60-60, 100-80, etc.). Next, looking at coverage in Table 3, the fact that the highest values were obtained at non-full access again indicates that there may not always be a negative impact on performance when access has been reduced, addressing our third research question. In addition, statistical test results of coverage also showed that the resulting access combinations are different depending on the type of access scenario and search strategy being utilised which addresses our second research question.",null,null
119,"In terms of relevant coverage, Table 3 indicates that when the IS strategy was utilised, the highest values were obtained at full access (100-100) for all of the term blacklisting scenarios (S2, S3 and S4). However, statistical test results also indicated that there were non-full-access combinations where relevant coverage was as high as the full access. Besides, it also showed that the resulting access combinations and their outcomes are different depending on the type of access scenario and search strategy being utilised, again addressing our second research question. With respect to unique coverage for S1, it can be seen in Table 3 that across all search strategies the access combination that has highest value is the lowest access (10-10), and this is opposite to S3 where the full access has the highest unique coverage. In addition, it is interesting to note that for all 4 scenarios (S1, S2, S3 and S4) the SRF strategy was able to obtain very high unique coverage in all access",null,null
120,845,null,null
121,"combinations compared to the other two strategies. Statistical test results showed that for S2, when the IS and IRF strategies were utilised, many of the access combinations ranging from 20-10 to 100-100 showed no significant difference from the best performing access combinations (i.e. 50-40 and 10-10 respectively). A similar outcome was also found for S4, but across all 3 search strategies. Unique relevant coverage in Table 3 shows that for all scenarios (other than for S3 of the IS strategy), the highest values were not obtained at full access. However, it appears that reducing access to the collection has little or no effect in terms of unique relevant coverage as statistical test results indicated that for almost every access scenario and search strategy, none of the access combinations showed any significant difference from the best performing access combinations.",null,null
122,Table 3. Highest values of different CIR measures with their respective access combinations. * indicates values of those measures at full access (i.e. 100-100),null,null
123,Coverage,null,null
124,Relevant Coverage,null,null
125,Independent Search,null,null
126,S1,null,null
127,365.7769 (100-10) 297.7461*,null,null
128,44.6461 (100-80) 42.0769*,null,null
129,S2,null,null
130,355.7153 (80-80) 296.2615*,null,null
131,42.1615 (100-100),null,null
132,S3,null,null
133,304.2384 (100-90) 297.9615*,null,null
134,42.4769 (100-100),null,null
135,S4,null,null
136,418.6461 (90-60) 296.1538*,null,null
137,42.4307 (100-100),null,null
138,Independent Relevance Feedback,null,null
139,S1,null,null
140,349.2769 (100-80) 290.8846*,null,null
141,48.3769 (90-60) 19.4846*,null,null
142,S2,null,null
143,349.8692 (100-50) 292.4385*,null,null
144,47.5231 (90-80) 19.0538*,null,null
145,S3,null,null
146,326.6077 (100-90) 277.8692*,null,null
147,42.7538 (90-80) 18.2769*,null,null
148,S4,null,null
149,407.6769 (100-60) 281.6154*,null,null
150,40.3308 (90-90) 18.6*,null,null
151,Shared Relevance Feedback,null,null
152,S1,null,null
153,353.6153 (100-10) 244.5308*,null,null
154,43.5 (90-40) 17.1*,null,null
155,S2,null,null
156,361.1615 (100-40) 241.3*,null,null
157,41.4308 (100-90) 17.2385*,null,null
158,S3,null,null
159,304.0615 (100-90) 242.5308*,null,null
160,43.1615 (100-90) 17.0231*,null,null
161,S4,null,null
162,387.4077 (100-30) 249.3769*,null,null
163,40.1077 (100-90) 17.1308*,null,null
164,Unique Coverage,null,null
165,8.4923 (10-10) 2.0307*,null,null
166,14.7615 (50-40) 0.3769*,null,null
167,9.8846 (100-100),null,null
168,4.8461 (10-10) 1.8923*,null,null
169,81.5231 (10-10) 12.4615*,null,null
170,12.5923 (10-10) 7.9692*,null,null
171,17.1385 (100- 100),null,null
172,8.7231 (100-100),null,null
173,133.3615 (10-10) 58.6692*,null,null
174,74.7538 (40-40) 42.777*,null,null
175,67.2769 (100- 100),null,null
176,47.7692 (100-10) 45.7692*,null,null
177,Unique Rele- vant Coverage,null,null
178,0.0923 (80-20) 0.0461*,null,null
179,0.1923 (80-70) 0.0*,null,null
180,0.1615 (100-100),null,null
181,0.1153 (100-30) 0.0538*,null,null
182,0.3923 (10-10) 0.0*,null,null
183,0.3 (90-60) 0.0*,null,null
184,0.2231 (90-90) 0.0846*,null,null
185,0.0846 (100-80) 0.0615*,null,null
186,0.7923 (10-10) 0.0385*,null,null
187,1.2462 (80-20) 0.0769*,null,null
188,1.2692 (100-90) 0.2769*,null,null
189,0.4308 (100-90) 0.3077*,null,null
190,4. CONCLUSION AND FUTURE WORK,null,null
191,"While a great deal of research has focused on CIR, only a few papers have considered the impact of non-uniform information access on CIR outcomes. This paper is one of the first attempts to quantify the impact of non-uniform information access on CIR outcomes. To that end, we conducted a simulated user evaluation using established scenarios [6] and search strategies [7].",null,null
192,"In relation to our first research question it was found that in terms of recall, precision and f-measure that non-uniform access for S1, S2 and S4 had very little impact when relevance feedback strategies were employed. In addition, it was also found that in some cases, one member of the team having a high level of access can compensate for the other team member. Besides, our results have also highlighted that there is some tolerance to removing access from the collection and that there may not always be a negative",null,null
193,"impact on performance. This leads us into our second and third research questions. We have found that depending on the type of access scenario and search strategy, access combinations yield different outcomes. Removing access to documents and term blacklisting had different impacts in terms of coverage: for removing document access, coverage remained stable where at least one team member had high access, whereas for blacklisting both members needed high access to retain high coverage. We have also found that in some scenarios, performance is even increased due to non-uniformity. This may in part be because this ensures that parts of the collection which might otherwise be ignored due to overlap in retrieved documents are now examined. Thus, there can be some benefits to non-uniform access depending on the search task.",null,null
194,"To address our research questions in this paper we used 3 search strategies, 4 access scenarios, 7 different measures and teams of 2 simulated users. We anticipate extending this study in various ways to be able to produce findings that greatly generalise to a number of real situations. Thus, we intend to look at more complex strategies and access scenarios, and incorporate more users within each team. Furthermore, the findings from this study will be examined further via a user evaluation. To conclude, our findings provide a better understanding on the impact of non-uniform information access amongst searchers in collaborative information retrieval, as well as a roadmap for further user studies.",null,null
195,5. REFERENCES,null,null
196,"[1] Allan, J: HARD Track Overview in TREC 2005: High Accuracy Retrieval from Documents. TREC 2005, pp. 1-17",null,null
197,"[2] Attfield, S., Blandford, A., Makri, S.: Social and interactional practices for disseminating current awareness information in an organisational setting. IPM, 46(6), (2008)",null,null
198,"[3] Bjurling, B., Hansen, P.: Contracts for Information Sharing in Collaborative Networks. ISCRAM 2010 (Vol. 1)",null,null
199,"[4] González-Ibáñez, R., Shah, C.: Coagmento: A system for supporting collaborative information seeking. JASIST, 48(1), 1-4 (2011)",null,null
200,"[5] Halvey, M., Vallet, D., Hannah, D., Feng, Y., Jose, J. M.: An asynchronous collaborative search system for online video search. IPM, 46(6), 733-748 (2010)",null,null
201,"[6] Handel, M. J., Wang, E. Y.: I can't tell you what i found: problems in multi-level collaborative information retrieval. 3rd international workshop on Collaborative information retrieval, pp. 1-6. ACM CIKM 2011",null,null
202,"[7] Joho, H., Hannah, D., Jose, J. M.: Revisiting IR techniques for collaborative search strategies. ECIR 2009, pp. 66-77",null,null
203,"[8] Joho, H., Hannah, D., Jose, J. M.: Comparing collaborative and independent search in a recall-oriented task. ACM IIiX, 2008 pp. 89-96",null,null
204,"[9] Karunakaran, A., Reddy, M.: Barriers to collaborative information seeking in organizations. JASIST, 49(1), (2012)",null,null
205,"[10] Karunakaran, A., Reddy, M.: The Role of Narratives in Collaborative Information Seeking. ACM SIGGROUP 2012, pp. 273­276",null,null
206,"[11] Morris, M. R., Horvitz, E.: SearchTogether: an interface for collaborative web search. ACM UIST 2007, pp. 3-12",null,null
207,"[12] Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., Back, M.: Algorithmic mediation for collaborative exploratory search. ACM SIGIR 2008, pp. 315-322",null,null
208,"[13] Shah, C., González-Ibáñez, R.: Evaluating the synergic effect of collaboration in information seeking. ACM SIGIR 2011, pp. 913-922",null,null
209,846,null,null
210,,null,null

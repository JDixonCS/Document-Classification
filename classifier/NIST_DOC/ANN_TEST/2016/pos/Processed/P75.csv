,sentence,label,data
0,Engineering Quality and Reliability in Technology-Assisted Review,null,null
1,Gordon V. Cormack,null,null
2,University of Waterloo,null,null
3,gvcormac@uwaterloo.ca,null,null
4,Maura R. Grossman,null,null
5,"Wachtell, Lipton, Rosen & Katz",null,null
6,mrgrossman@wlrk.com,null,null
7,ABSTRACT,null,null
8,"The objective of technology-assisted review (""TAR"") is to find as much relevant information as possible with reasonable effort. Quality is a measure of the extent to which a TAR method achieves this objective, while reliability is a measure of how consistently it achieves an acceptable result. We are concerned with how to define, measure, and achieve high quality and high reliability in TAR. When quality is defined using the traditional goal-post method of specifying a minimum acceptable recall threshold, the quality and reliability of a TAR method are both, by definition, equal to the probability of achieving the threshold. Assuming this definition of quality and reliability, we show how to augment any TAR method to achieve guaranteed reliability, for a quantifiable level of additional review effort. We demonstrate this result by augmenting the TAR method supplied as the baseline model implementation for the TREC 2015 Total Recall Track, measuring reliability and effort for 555 topics from eight test collections. While our empirical results corroborate our claim of guaranteed reliability, we observe that the augmentation strategy may entail disproportionate effort, especially when the number of relevant documents is low. To address this limitation, we propose stopping criteria for the model implementation that may be applied with no additional review effort, while achieving empirical reliability that compares favorably to the provably reliable method. We further argue that optimizing reliability according to the traditional goal-post method is inconsistent with certain subjective aspects of quality, and that optimizing a Taguchi quality loss function may be more apt.",null,null
9,Keywords: Technology-assisted review; predictive coding; electronic discovery; e-discovery; test collections; relevance feedback; continuous active learning; reliability; quality; systematic review.,null,null
10,The views expressed herein are solely those of the author and should not be attributed to her firm or its clients.,null,null
11,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '16 July 17-21, 2016, Pisa, Italy c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4069-4/16/07. DOI: http://dx.doi.org/10.1145/2911451.2911510",null,null
12,1. INTRODUCTION,null,null
13,"A vexing question that has plagued the use of technologyassisted review (""TAR"") is ""when to stop""; that is, knowing when as much relevant information as possible has been found, with reasonable effort. We present a provably reliable method to achieve high recall using any search strategy that repeatedly retrieves documents and receives relevance feedback, continuing indefinitely until a decision is made to discontinue the review process. Amenable search strategies include traditional ranked retrieval,1 interactive searching and judging [8], move-to-front pooling [8], and continuos active learning (""CAL"") [5].",null,null
14,"For the particular implementation of CAL supplied as the baseline model implementation (""BMI"") [7] for the TREC 2015 Total Recall Track [13], we present two stopping procedures that achieve superior empirical reliability for comparable effort, and comparable empirical reliability for less effort, relative to our provably reliable method.",null,null
15,"Our primary motivation is to provide quality assurance for TAR applications, including electronic discovery (""eDiscovery"") in legal matters [5], systematic review in evidencebased medicine [10], and the creation of test collections for information retrieval (""IR"") evaluation [14]. Since these applications generally require that a human review each relevant document, we assume for this study that the effort to provide relevance feedback for relevant documents is a sunk cost. On the other hand, the effort to assess and provide relevance feedback for non-relevant documents is wasted. We measure review effort in terms of the total number of documents reviewed, whether relevant or not. An ideal search would find all of the relevant documents with effort equal to precisely that number. An acceptable search would find most of the relevant documents with minimal wasted effort.",null,null
16,"A reliable search method would achieve an acceptable search most of the time. More formally, if S is a random variable representing a search, and acceptable(s) is an indicator function denoting whether a particular search s has an acceptable result, we define:",null,null
17,"reliability ,def Pr[acceptable(S) , 1] .",null,null
18,"To this end, we define recall(s) and effort(s) to be the recall and effort associated with s. For simplicity, our primary",null,null
19,"1To be amenable, a retrieval method must be able to rank the entire collection. Incomplete rankings or set-based results may be extended by adding the remaining documents in any order.",null,null
20,75,null,null
21,Collection,null,null
22,At Home At Home At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05,null,null
23,Source,null,null
24,TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall TREC 2015 Total Recall Reuters TREC 2012 Filtering TREC 2012 Filtering TREC 2004 Robust TREC 2005 Robust,null,null
25,Description,null,null
26,"Jeb Bush public email Hacker forums Local news Tim Kaine non-public email MIMIC II Clinical Database News subject categories NIST topics Conjunction of RCV1-v2 subject pairs Amalgam of TREC ad-hoc topics 50 legacy topics, new dataset",null,null
27,# Docs,null,null
28,"290,000 465,147 902,434 401,953",null,null
29,"31,538 804,414 804,414 804,414 528,256 1,033,461",null,null
30,# Topics,null,null
31,10 10 10 4 19 103 50 50 249 50,null,null
32,# Rel (R),null,null
33,"227-17,135 179-9,517 23-2,094 14,341-166,118 180-19,182 5-381,327",null,null
34,12-610 21-349 4-161 17-376,null,null
35,"Table 1: Ten Evaluation Datasets. In our experiments, the three At Home datasets are treated as a single test collection, for a total of eight test collections.",null,null
36,results use a goal-post definition [18] of acceptability:,null,null
37,"acceptable(s) , 1 (recall(s)  0.70) . 0 (recall(s) < 0.70)",null,null
38,Our primary results further assume that 95% reliability is sufficiently high.,null,null
39,The methods and results detailed in this work are:,null,null
40,"· The target method : a provably reliable method that chooses ten random relevant documents as a target, and employs an independent search method to retrieve documents without knowledge of the target set, until each document in the target set has been retrieved. We prove that this method achieves 95% reliability for a minimum threshold recall of 70%.",null,null
41,"· The knee method : a geometric stopping procedure, based on the shape of the gain (i.e., recall versus effort) curve, that augments BMI to achieve similar empirical reliability to the target method, with substantially less effort. The knee method, in contrast to the target method, is practical regardless of the number of relevant documents in the collection.",null,null
42,"· The budget method: a variant of the knee method that achieves superior empirical reliability to the target method, with similar effort.",null,null
43,· Empirical validation: we assess the effectiveness and reliability of our methods on eight archival test collections consisting of 555 topics and 4.5 million documents (see Table 1).,null,null
44,"· Quality evaluation: As an alternative to binary relevance and fixed recall and reliability thresholds, we argue and provide evidence that quality loss functions [18] provide more nuanced measures that better reflect the tensions among consistency, effectiveness, and efficiency.",null,null
45,2. BACKGROUND,null,null
46,"The modern literature on the effectiveness and reliability of high-recall retrieval is largely confined to the problem of constructing test collections for IR evaluation, and eDiscovery in legal matters. A 1985 study by Blair and Maron [2] showed that teams of lawyers and paralegals, using iterative Boolean searches, believed they had achieved 75% recall, when in fact they had achieved 20%. Blair [3] later",null,null
47,"described the difficulty of measuring high recall in general, and the use of targeted searching, systematically constructed Boolean queries, and stratified sampling to estimate recall for the Blair and Maron study.",null,null
48,"The Text Retrieval Conference (""TREC"") [21] first addressed the problem of IR evaluation for ""large"" datasets, which at the time of TREC's inception in 1992, contained on the order of 500,000 documents. TREC follows the Cranfield paradigm [20], which evaluates the results of subject systems against a gold standard that identifies every relevant document. For large datasets, the effort to render a human assessment for each document is prohibitive, thus occasioning the use of automated or semi-automated methods to limit the human review effort required to label the dataset. TREC saw the introduction of the ""pooling method,"" which selects the top-ranked documents from a number of independent retrieval efforts for assessment, and deems all other documents to be non-relevant. A number of studies (see [19]) indicate that this method fails to identify a substantial number of documents, but even so, the resulting gold standard yields a stable evaluation of the relative effectiveness of candidate systems, as measured by Kendall's  rank correlation. We are unaware of any studies that address the effectiveness of pool-based gold standards for evaluating high-recall retrieval, or for simulating interactive relevance feedback. Studies suggest that greedy or machine-learning methods to select the pool yield a more nearly complete gold standard [8, 14].",null,null
49,"Interactive searching and judging (""ISJ""), in which a searcher repeatedly formulates queries and examines the top results from a relevance-ranking search engine, has been shown to yield gold standards with comparable quality to the pooling method, with considerably less effort [8]. Continuous active learning (""CAL"") [5] is essentially the same as ISJ, but uses machine learning instead of, or in addition to, manually formulated queries to rank the documents for review. An approach similar to CAL was used in the TREC 2012 Filtering Track (see [17]) to construct the gold standard that was used for evaluation, and also to simulate relevance feedback. A subsequent study based on pooling showed that the CAL-like approach had achieved high recall, and high effectiveness, as measured by Kendall's  [17]. CAL achieved superior results at the TREC 2009 Legal Track [4], and remains state of the art for eDiscovery.",null,null
50,"The TREC 2015 Total Recall Track [13] represents the first study of high-recall human-in-the-loop retrieval in which all aspects of human intervention are simulated, and",null,null
51,76,null,null
52,"hence controlled. Fully automated or semi-automated retrieval systems were tested through their interaction with an evaluation server. At the outset, the evaluation server provided a document collection and a topic description, after which the system under test submitted potentially relevant documents from the collection to the evaluation server. In response, the evaluation server provided an assessment (derived from a pre-computed gold standard) for the submitted documents, and the process continued until the documents were exhausted or the system chose to stop.",null,null
53,"Participants in the Total Recall Track were supplied with a CAL baseline model implementation2 (""BMI"") that, when connected to the evaluation server, performed all aspects of the task--other than deciding when to stop--without human intervention. Participating systems were allowed to run indefinitely, and were evaluated (primarily) on the quality of the ranking determined by the order in which the system presented documents to the server. Instead of actually terminating when they thought an acceptable result had been achieved, participants were invited to ""call their shot"" by indicating, in real time, when they would have stopped, had they been required to balance benefit with cost. The current study considers the addition of a call-your-shot mechanism to BMI, and, more generally, to any ranking system.",null,null
54,"The TREC 2015 Total Recall Track contributed five fully labeled archival datasets. The Jeb Bush, Hacker Forums, and Local News datasets were used for the At Home task, in which participants ran their systems on their own platforms, connecting via the internet to the evaluation server, which was run by the track coordinators. The Kaine and MIMIC II datasets were used for the Sandbox task, in which participants encapsulated their systems as a virtual machine, which was run by the track coordinators, along with the evaluation server, isolated from the internet.",null,null
55,"The reliability of methods for constructing gold standards for IR evaluation has typically been evaluated by how well the resulting gold standard ranks the relative effectiveness of precision-oriented retrieval systems, where the objective is to find as much relevant information as possible at low rank. For this purpose, a calibrated estimate is not required; it is sufficient to determine whether one system achieves higher recall than another, and the actual numerical value is ascribed little meaning. A number of studies (see [23]) eschew recall altogether, assuming that the user's information need will be satisfied by a tiny fraction of a vast sea of relevant documents. Zobel et al. [23] suggest that recall is a poor effectiveness measure, even for the ""high-recall applications"" where the user seeks ""total recall,"" and that only an extensive ad-hoc effort using multiple queries and tools will satisfy the user that their information need has been met.",null,null
56,"The reliability and effectiveness of TAR (also known as ""predictive coding"") is the subject of much interest in the legal community [9, 16]. A number of approaches to TAR, to deciding when to stop, and to quality assurance have been advanced, but no stopping procedure has previously been shown to be mathematically or empirically reliable. Perhaps the most commonly used approach to TAR involves the use of a supervised machine-learning algorithm trained using a set of documents from the collection (typically referred to as a ""seed set"") to partition the collection into a ""review set,"" which is subject to human review, and a ""null set,"" which",null,null
57,2See http://plg.uwaterloo.ca/~gvcormac/trecvm/.,null,null
58,"is not. This approach is referred to as either simple passive learning (""SPL"") or simple active learning (""SAL""), depending on whether or not the learning algorithm is involved in selecting the training documents [5]. Recently, CAL has been advanced as a superior alternative [5, 7].",null,null
59,"Regardless of the TAR method used, the question remains of when to stop. For SPL and SAL, two questions must be answered: when to stop training; and how many documents should be included in the review set. For CAL, the sole question is when to stop. One approach that has been advanced is to draw a random hold-out set (referred to as a ""control set"") that is used to measure the effectiveness of the classifier, in order to determine when to stop training, and then to measure recall, so as to determine how many documents should comprise the review set. The control set must be large enough to contain a sufficient number of relevant documents to yield a precise estimate. Bagdouri et al. [1] note that the use of a control set constitutes sequential sampling, with the net effect that it yields a biased estimate of recall, and cannot be used for quality assurance. As an alternative, they propose ""certified text classification,"" in which part of the review budget is set aside to conduct a frequentist acceptance test that will accept or reject the classifier. Bagdouri et al. are concerned with the problem of testing whether the classifier has achieved a threshold level of F1; they do not consider recall, or how to proceed in the event that the classifier is rejected by the test.",null,null
60,"The limitations of binary relevance may be of particular importance in evaluating the effectiveness and reliability of TAR systems. Binary relevance does not account for the differential importance of relevant documents, and there will necessarily be documents near the threshold about which competent assessors will disagree (see [19]). In evaluating the recall of a system against a gold standard, there will necessarily be uncertainty for some documents as to whether the system is correct, the gold standard is correct, or reasonable minds could disagree. If a system fails to meet a target recall threshold, is it because the system has missed important documents, because it has missed marginal documents about which reasonable minds could differ, or because it has missed documents that are incorrectly coded relevant in the gold standard? And, is the effort to remedy the shortfall proportionate to the importance of the missed documents?",null,null
61,"Binary relevance and fixed recall targets are examples of traditional goal-post methods in quality engineering ([18]), where success or failure is a binary quantity, and reliability is the probability of success. In quality engineering, a quadratic loss function blends reliability and effectiveness into a single quality measure, with targets, but no arbitrary thresholds [18].",null,null
62,3. GUARANTEED RELIABILITY,null,null
63,"Our target method involves drawing a target set T of k random relevant documents from the collection; for simplicity we fix k ,"" 10, but a different number could be chosen. In order to draw T , we retrieve and review documents selected at random until k relevant documents are found, or the collection is exhausted. The underlying search strategy retrieves documents for review without knowledge of T , until every document in T has been found. This method achieves 95% reliability, as shown below.""",null,null
64,Consider a document collection C and a function rel(d) indicating binary relevance. The number of relevant docu-,null,null
65,77,null,null
66,"ments in the collection is R ,"" |{d  C|rel(d)}|. A search strategy is a ranking on C where 1  rank(d)  |C| denotes the position of d in the ranking. It is important to note that the following argument holds for any such ranking, provided it is independent of T .""",null,null
67,The retrieved set of the target method is the shortest prefix P of the ranking that contains T :,null,null
68,"P , {d|rank(d)  max rank(d )} .",null,null
69,d T,null,null
70,Now consider the ranking relrank(d) of only relevant documents:,null,null
71,"relrank(d) , d  C|rel(d)  rank(d )  rank(d) .",null,null
72,The last retrieved document dlast is necessarily in T :,null,null
73,"dlast , arg max rank(d) , arg max relrank(d) .",null,null
74,dT,null,null
75,dT,null,null
76,The recall of our method is:,null,null
77,recall,null,null
78,",",null,null
79,relrank(dlast) R,null,null
80,.,null,null
81,"Taking T to be a random variable, the method is reliable if:",null,null
82,R,null,null
83,10,null,null
84,Pr[,null,null
85,relrank(dlast R,null,null
86,),null,null
87,0.7],null,null
88,0.95 .,null,null
89,"Assuming large R, consider the problem of determining a cutoff c such that:",null,null
90,Pr[,null,null
91,R,null,null
92,-,null,null
93,relrank(dlast) R,null,null
94,>,null,null
95,c],null,null
96,",",null,null
97,0.05,null,null
98,(1),null,null
99,"Pr[R - relrank(dlast) > cR] , 0.05",null,null
100,(2),null,null
101,"For the condition in Equation 2 to hold, it must be the case that the [numerically] top-ranked cR documents are absent from T , which occurs with probability",null,null
102,1-,null,null
103,10 R,null,null
104,cR,null,null
105,", 0.05 .",null,null
106,"It follows that: For all R > 10,",null,null
107,c,null,null
108,",",null,null
109,log 0.05,null,null
110,R log,null,null
111,1,null,null
112,-,null,null
113,10 R,null,null
114,.,null,null
115,c,null,null
116,<,null,null
117,lim,null,null
118,R,null,null
119,R,null,null
120,log 0.05,null,null
121,log,null,null
122,1,null,null
123,-,null,null
124,10 R,null,null
125,", 0.299573 < 0.3 .",null,null
126,(3),null,null
127,"Combining (1) and (3), we have:",null,null
128,Pr[,null,null
129,R,null,null
130,-,null,null
131,relrank(dlast) R,null,null
132,>,null,null
133,0.3],null,null
134,<,null,null
135,0.05,null,null
136,R,null,null
137,10,null,null
138,Pr[,null,null
139,relrank(dlast) R,null,null
140,0.7],null,null
141,0.95,null,null
142,.,null,null
143,Reliability is obtained at the cost of supplemental review,null,null
144,"effort inversely proportional to R, the number of relevant",null,null
145,documents. The number of randomly selected documents,null,null
146,that,null,null
147,need,null,null
148,to,null,null
149,be,null,null
150,reviewed,null,null
151,to,null,null
152,find,null,null
153,k,null,null
154,relevant,null,null
155,ones,null,null
156,is,null,null
157,k,null,null
158,|C| R,null,null
159,",",null,null
160,on,null,null
161,average for R,null,null
162,|C |.,null,null
163,For k,null,null
164,", 10",null,null
165,and prevalence,null,null
166,R |C|,null,null
167," 1%,",null,null
168,"the target method entails a review overhead of about 1,000",null,null
169,additional documents. Lower prevalence entails substan-,null,null
170,"tially more overhead, while higher prevalence entails less.",null,null
171,1,null,null
172,1,null,null
173,0.9,null,null
174,0.9,null,null
175,0.8,null,null
176,0.8,null,null
177,0.7,null,null
178,0.7,null,null
179,0.6,null,null
180,0.6,null,null
181,0.5,null,null
182,0.5,null,null
183,0.4,null,null
184,0.4,null,null
185,0.3,null,null
186,0.3,null,null
187,0.2,null,null
188,0.2,null,null
189,0.1,null,null
190,0.1,null,null
191,0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
192,0 0 0.1 0.2 0.3,null,null
193,(a),null,null
194,Fig. 2: KnFeiegdulerea1lg: oKrintheme Dfoetrecotniloinne[1k5n].ee detection. (a) depict,null,null
195,"4pro.etrapteeEndMd4ic5PuIldaRergIrdCeiesAsta.LnTcRheeEfmrLoamIgAnyBituI,dLeIxTofYwthiethsethbearsmcaoxrirmesupmonddisttoanthcee",null,null
196,"vaOluuerskannede tmheethcoodrrreeslpieosnodnintghetharsessuhmopldtiovnaltuheast(CwAitLh,Sin, 1). The kn",null,null
197,"aiinsccrsoairnmdkaipnnclgyemwaoitrfheu-ltnihkceetliyporrnoebloeavbfailntihttyed-rolacenunkmginetgnhtspsrobinefcfiotprhleee,lesssuisdc-lceiekseedloysf the triangl",null,null
198,"rwelietvhantthedopcouminetnstsa.sAvseartciconesse.qHueonwcee, vtheer,gaasinwcuersvhe opwloti-n Section IV",null,null
199,"twawdinnirthdaghiwnlreheenaicgraMfh-lzrl eeosvrnlmoeogrpsseeluiordsp(eicer.aaelooln.n,kscc""eeomilsnnyaeatriasagnsripulnuypmaoalreuoldplsxrrteeifomclueisvnbaiaoectnnettgi""soe)dnncoaecustru,armtvlilhtyaeentdcutooosruneehtvssaeefvxtone,,rotofwfloinrke",null,null
200,dat we,null,null
201,bfoeerntrheetrinevoeids.y online data sets typical of computing systems.,null,null
202,poAinEntWicdoerMarel sAgpao.inndTcuihnrgevetowEtoWhueldMrahnAakveaastlpowppherioc1ha.0cauhllndtiuolcsauenmsinenfltetesccthhioanndiques simila,null,null
203,"btoeenthroetsreieveemd,palondyesdlopbey0.B0 othlleirnegafeterr.BaAnndsact[u1a5l ]gaainnd Geometri",null,null
204,"cisMantulargonpvodvreoraiibtnrtydahgpb.miiclSAiatultyvlphyeparraodtansigvewkeeitrnehggeaa,ustlsgrtfeaorhnoreidmisotrhembttmhraiefsesaevciaddtfloeoaromrsln,edctauhthnheoadedntoagwmeelneirmoetdihisateyoabtdtelgeiocoonttlldioosogny",null,null
205,[16]. Th describe,null,null
206,"abcyhieAvelb7r0e%chrteceatll aaln.din70%thepirrecwisioonrkatosnompearrtainakl rb,aarsriiesrs [3], whic",null,null
207,"tbdyyepriecixvahleasfuosrftrimvoeomdmerpannruecavlalisorseiufivseierwsw[o1[11r9k],].oorTnahseMmsOliogpNhetE,bTuepa[ct1oh7iet]vh.eadEt WMA is a roannlkin(seloapleg<orr)i,twhomuldthbaet 0u.s7e, santdwtoheexslpopoenaefntetiratlhlyatwraenikghted movin",null,null
208,"(asvloeprea>gre)sw. ouTlhdeapfiprrosatchE, WbutMnoAt,eqcuaalll0e.dFoarrsrm,alilsvauluseesd to smoot wotshfeoBeRcuaoldsineneddpx|CopuEnet|,cWotwdutMerahteewaAx""o,ps,uelwloradpierheenrixcrcvpeaheatwicortii""t,sshlkonv,""epoieenepssw-llpoosppue0eetbd< >.r0larri,ccaadksnad1tohaff.osoesrtthtsa,elalwarRervioevwbrae-algteimdeevs.iatTioh""",null,null
209,"sferrovmed athrart,foarndR is 1a0n0,estima6t.0e (owfitthhesuivtaabrileanscmeooitnh-arrival times",null,null
210,"irtFtnaehiglrring)aeeabwstlihallmiysoty,ealtdtthghhoooeadostf.decaWiontrmwedripfoco+aarrvtmeo4adreld·uofaaftevhhrsoeirrgaahvhrbyaerlpyerouc,ttasholwele,dtshhiasaictcohtthhiaeacvctorhientmihepgveprrseeeuedscttaebehlnylraetatssnhmhdet-haeximmauxmimwuam",null,null
211,"oalmdsowuenret uonfivetrimsale; thtoat wthaeistamfoerthtrheeshonldexwtoupldoiwnotrktoforarrive. If th aqpuoweinindttleyavurasrreiidvetefyosrdaoafuttareseermtst,phiiirnsiccaltulhderivenasglhutaohtleidon,teo(nsretehthaTteabwtleehr1se)u.sbhseo-ld is reache 4wim.1itphoorNutatonistseeeaitAntrgbibatuthteeemnoeefnxttthiasrraivlgaol,riEthWmMisAthdaetclEarWesMaAkndeoee.sOnno edxiiIrsfetwsceatlnwyienrrfleeeptcootisrottnopwpoahitnettrhe1e<tmhiien<imksnusmeuecrhapntohkianst,tsiusc--h6,tiwhtaetowtnohlueyrldedetermines i aalmkonsetecehrtaasinblyeestnoppapsresmedat.uAreslyadureestuolcth, aEnWce.MMAoreiosvoern, ly applicabl",null,null
212,teihnffiosratnnaa¨isovnealaifnupnpecrotsiaeocnthtionwfgot.uhled,null,null
213,entail size of,null,null
214,quadratic computational the collection. To avoid,null,null
215,"both eventualities, we evaluate  only at values of s arising",null,null
216,"forfombattchheebsaitschpersopofodrtIoiIcoIun.maleKtnotNslEosegEle|DCctL|e,EdabsAytLhBeGMvOIa.RlTuIehTseHonfMums baerer",null,null
217,"sfeewpKaroanfteetedhdeblyceaaninsdeibxdpaatoseneedvnatoliuanellsythfionercrnseoawstiiinollgnbintethevarivtaabtlh.leeR, eeplvaoetininvetblsyy of maximum cchuarnvcae.tuArenyinreasidduaatlasesqeut--enttihale-teksntienegsb--iasarise oaffpspetrobxyima ately the se coofnFpsoerorevinaactthisvevinaclhuaoeiccouef rsov,fewthetrheeavsahtloaulardetefolor cata. lonmlyaoxniemia, cihfotshene ucsu- rve is rotate ingdeaggreeoemsetcrliocc""kknweies-deeatebcotiuont ""(xalmgoinri,tyhmmin[1)5t],hriloluusgtrhattehde line forme by the points (xmin, ymin) and (xmax, ymax). We choose this lin because we want to preserve the overall behavior of the dat",null,null
218,"78 set--using a line of best fit, for example, risks cutting off th end points due to a higher concentration of points in the middl",null,null
219,"of the curve. After rotating about this line, the local maxima--",null,null
220,Collection,null,null
221,At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05,null,null
222,Target Method,null,null
223,Reliability Recall Effort,null,null
224,1.00,null,null
225,0.91,null,null
226,"44,079",null,null
227,1.00,null,null
228,"0.92 119,644",null,null
229,1.00,null,null
230,0.89,null,null
231,"14,440",null,null
232,0.96,null,null
233,0.88,null,null
234,"83,412",null,null
235,0.98,null,null
236,"0.93 133,788",null,null
237,1.00,null,null
238,"0.92 174,415",null,null
239,0.89,null,null
240,"0.94 169,907",null,null
241,1.00,null,null
242,"0.92 155,405",null,null
243,Knee Method,null,null
244,Reliability Recall Effort,null,null
245,0.93,null,null
246,0.93,null,null
247,"5,244",null,null
248,1.00,null,null
249,"0.98 172,774",null,null
250,1.00,null,null
251,"0.97 19,387",null,null
252,0.99,null,null
253,"0.94 60,645",null,null
254,1.00,null,null
255,0.99,null,null
256,"3,857",null,null
257,0.98,null,null
258,"0.96 153,638",null,null
259,1.00,null,null
260,1.00,null,null
261,"7,575",null,null
262,1.00,null,null
263,0.96,null,null
264,"8,444",null,null
265,Budget Method,null,null
266,Reliability Recall Effort,null,null
267,"0.97 0.97 43,896 1.00 0.98 172,774 1.00 0.97 19,418 0.99 0.94 70,601 1.00 1.00 143,798 1.00 0.99 190,671 1.00 1.00 162,673 1.00 1.00 134,719",null,null
268,"Table 2: Reliability, Mean Recall, and Mean Effort for the Target, Knee, and Budget Methods.",null,null
269,"in Figure 1. We draw a line from the origin to the recall achieved at rank s, and compute the maximum perpendicular distance from this line to the gain curve. Our candidate value of i is the projection to the x-axis of the intersection between the perpendicular and the gain curve. Our rationale in choosing this point was that it would correctly choose the inflection point for an ideal curve, and would avoid anomalies associated with points very close to the origin or to rank s, while capturing our intuitive notion of a genuine tipping point.",null,null
270,We calculated the slope ratio as:,null,null
271,|{d|rank(d)irel(d)}|,null,null
272,",",null,null
273,i 1+|{d|i<rank(d)s}|,null,null
274,.,null,null
275,s-i,null,null
276,"Smoothing was accomplished by adding 1 to the number of relevant documents beyond the knee. This choice avoided the singularity of no relevant documents beyond the knee, and generally penalized situations in which the chosen inflection point was close to s. No smoothing was applied to the numerator, as we were not concerned with occasional underestimates.",null,null
277,4.2 Adjustment for Low Prevalence,null,null
278,The case of R 100 is more problematic. Any correction for small R faces a dual problem:,null,null
279,"1. the stopping procedure has no knowledge of the value of R, other than what can be estimated through relevance feedback from retrieved documents; and,",null,null
280,"2. even if it were known that R was small, the sparsity of relevant documents compromises the reliability of our slope-ratio calculation.",null,null
281,"The knee method relies entirely on the slope-ratio test, adjusted to compensate for low R. Initial tuning on the training collections from the TREC 2015 Total Recall Track indicated that a fixed lower bound  on the rank at which to stop, might be effective. For our submission to the TREC 2015 Total Recall At Home task, we conducted a parameter sweep of six combinations of   {100, 1000} and   {3, 6, 10}. Our results showed that combinations involving  , 100 or  ,"" 3 were unreliable, and we eliminated them from further consideration. Unsurprisingly, the combination of  "","" 1000,  "","" 10 proved most reliable, achieving the recall target for 29 of 30 topics (reliability "", 0.97 [0.830 - 0.999 95% c.i.]).",null,null
282,"We observed that recall and reliability appeared to be lower for smaller R, while effort (especially for  ,"" 10) appeared to be disproportionately higher for large R. This observation led us to seek more reliable methods for small R,""",null,null
283,"and to choose  ,"" 6 for large R. To aid in this endeavor, we used a non-public dataset consisting of about 300,000 documents reviewed by attorneys and labeled according to 63 criteria, with R ranging from 5 to 164, 000 (median 431). Based on tuning experiments using this dataset, we calibrated the slope-ratio cutoff as a function of relret, the number of relevant documents retrieved at any given rank:""",null,null
284," ,"" 156 - min(relret, 150) .""",null,null
285,"In other words, we set the threshold for the slope ratio to be 150 when no relevant documents have been retrieved, 6 whenever at least 150 relevant documents have been retrieved, and use linear interpolation between these values.",null,null
286,"We further observed that with this adjustment, the choice of  , 100 versus  ,"" 1000 became less critical. The lower value occasionally achieved lower effort than the higher value, and occasionally failed when the higher value did not. We chose to retain the value of  "", 1000 from our earlier experiments.",null,null
287,4.3 Effort Adjustment,null,null
288,A variant of our knee method--the budget method --,null,null
289,adjusts for small R by stopping only when a review budget,null,null
290,"comparable to that of the target method has been expended,",null,null
291,and the slope-ratio test   6 is also satisfied. This ad-,null,null
292,"justment substantially delays termination for small R, thus",null,null
293,ensuring reliability.,null,null
294,The approach is predicated on the hypothesis that the,null,null
295,supplemental review effort entailed by the target method,null,null
296,would be better spent reviewing more documents retrieved,null,null
297,by CAL. The target method entails the supplemental review,null,null
298,of,null,null
299,about,null,null
300,10|C| R,null,null
301,documents,null,null
302,in,null,null
303,order,null,null
304,to,null,null
305,find,null,null
306,10,null,null
307,relevant,null,null
308,ones.,null,null
309,"According to the probability-ranking principle, we would ex-",null,null
310,pect CAL to find more relevant documents than random,null,null
311,"selection,",null,null
312,for,null,null
313,any,null,null
314,level,null,null
315,of,null,null
316,"effort,",null,null
317,up,null,null
318,to,null,null
319,and,null,null
320,beyond,null,null
321,10|C| R,null,null
322,.,null,null
323,While the supplemental documents retrieved by the target,null,null
324,"method provide a statistical estimate of R, the documents",null,null
325,"retrieved by CAL provide a lower bound for R, and therefore",null,null
326,an upper bound for the expected effort entailed by the target,null,null
327,"method. At the outset, this upper bound is loose, but as the",null,null
328,"review progresses, it tightens. The budget method retrieves",null,null
329,documents using CAL until review effort exceeds this upper,null,null
330,"bound and   6.0, or until 0.75|C| documents are retrieved.",null,null
331,"For small R, the budget determines the stopping point.",null,null
332,"For large R, enough relevant documents will likely be discov-",null,null
333,ered to bound the review budget to an insubstantial fraction,null,null
334,"of R, and the slope-ratio test will determine when to stop.",null,null
335,"In any event, the review stops at 0.75|C|. This final cutoff",null,null
336,is predicated on the probability-ranking principle: random,null,null
337,79,null,null
338,TREC 2015 Total Recall Track -- Athome Tasks,null,null
339,1000000 1,null,null
340,Effort (Documents Reviewed),null,null
341,0.8 100000,null,null
342,0.6,null,null
343,Recall,null,null
344,0.4 10000,null,null
345,0.2,null,null
346,0,null,null
347,1000,null,null
348,23 26 66 76 113 179 182 227 252 255 265 506 589 629 661 1111 1256 1624 2036 2094 2299 2375 2375 3635 4542 4805 5725 5836 9517 17135,null,null
349,Number of Relevant Documents in Collection,null,null
350,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,null,null
351,TREC 2015 Total Recall Track -- Athome Tasks 1000000,null,null
352,1,null,null
353,Effort (Documents Reviewed),null,null
354,0.8 100000,null,null
355,0.6,null,null
356,Recall,null,null
357,0.4 10000,null,null
358,0.2,null,null
359,0,null,null
360,1000,null,null
361,23 26 66 76 113 179 182 227 252 255 265 506 589 629 661 1111 1256 1624 2036 2094 2299 2375 2375 3635 4542 4805 5725 5836 9517 17135,null,null
362,Number of Relevant Documents in Collection,null,null
363,Effort for Budget Method Effort for Target Method Recall for Budget Method Recall for Target Method,null,null
364,Figure 2: TREC 2015 Total Recall At Home Collection.,null,null
365,"selection of 75% of the collection would, with high probability, achieve 70% recall; the top-ranked 75% should achieve even higher recall.",null,null
366,5. EXPERIMENTS,null,null
367,"Testing the reliability of our stopping methods occasioned the use of ""fully assessed"" test collections, with a large number of topics and documents, where by ""fully assessed,"" we mean that the pooling method, ISJ, or a rule base was used, and the resulting documents were labeled by a human assessor. From the limited number of collections that met these criteria, we selected the TREC 2015 Total Recall Track collections, the Reuters RCV1-v2 news dataset, the TREC 2002 Filtering Track collections, and the TREC 2004 and 2005 Robust Track collections, as detailed in Table 1. We used our Total Recall At Home participation to conduct an initial parameter sweep with six combinations, as well as final testing; the other datasets were used solely for testing.",null,null
368,"The first phase of our experiments took place within the context of the TREC 2015 Total Recall Track, which had three distinct phases: training, At Home, and Sandbox. We conducted our initial development and tuning during the training phase, and submitted the knee method for evaluation in the At Home phase, but not the Sandbox phase. We captured the sequence of documents retrieved by BMI in both the At Home and Sandbox phases, and later used them",null,null
369,Effort (Documents Reviewed),null,null
370,Recall,null,null
371,TREC 2015 Total Recall Track -- Kaine Collection,null,null
372,1,null,null
373,1000000,null,null
374,0.8,null,null
375,100000 0.6,null,null
376,0.4 10000,null,null
377,0.2,null,null
378,0,null,null
379,1000,null,null
380,14341 20083 131698 166118,null,null
381,Number of Relevant Documents in Collection,null,null
382,Surplus Effort for Knee Method Surplus Effort for Target Method,null,null
383,Recall for Knee Method Recall for Target Method,null,null
384,TREC 2015 Total Recall Track -- ICD-9 Codes,null,null
385,1,null,null
386,1000000,null,null
387,0.8,null,null
388,100000 0.6,null,null
389,Effort (Documents Reviewed),null,null
390,Recall,null,null
391,0.4 10000,null,null
392,0.2,null,null
393,0,null,null
394,1000,null,null
395,179 2141 2575 3452 3851 5066 5140 5867 6113 6815 7806 8025 8678 8724 11081 11222 15046 16780 19095,null,null
396,Number of Relevant Documents in Collection,null,null
397,Surplus Effort for Knee Method Surplus Effort for Target Method,null,null
398,Recall for Knee Method Recall for Target Method,null,null
399,Figure 3: TREC 2015 Total Recall Sandbox Collections.,null,null
400,"to simulate the effect of the stopping methods whose results are presented here. After conducting further tuning on our non-public collection of 300,000 documents with 63 topics, we froze all parameters, and ran BMI on the other evaluation datasets, capturing the order in which the documents were retrieved. We then simulated our stopping methods by applying them to the ranking.",null,null
401,"Summary results showing reliability, average recall, and average effort for all collections are shown in Table 2. The overall reliability of the target method, the knee method, and the budget method are substantially higher than the target of 0.95. Considering reliability, alone, there is little to choose among the methods; but the recall achieved by the knee and budget methods is substantially higher, while the effort expended by the knee method is, for some datasets, dramatically lower.",null,null
402,"As illustrated in Figures 2 through 6, R (the number of relevant documents) appears to be the principal determinant of effort. For small R, effort for the target and budget methods approaches the size of the collection, while effort for the knee method, with one notable exception, generally diminishes with R, approaching the floor of  ,"" 1000 that we chose for this study. On the other hand, for large R, the effort for all methods appears proportional to R.""",null,null
403,"The top panel of Figure 2 compares recall and effort for the knee and target methods, for each topic in the At Home col-",null,null
404,80,null,null
405,Effort (Documents Reviewed),null,null
406,Recall,null,null
407,Reuters RCV1-v2 Subject Categories,null,null
408,1,null,null
409,1000000,null,null
410,0.8,null,null
411,100000 0.6,null,null
412,0.4 10000,null,null
413,0.2,null,null
414,0,null,null
415,1000,null,null
416,5 844 2107 2636 4835 7406 11878 21280 32153 47708 204820,null,null
417,Number of Relevant Documents in Collection Surplus Effort for Knee Method,null,null
418,Surplus Effort for Target Method Recall for Knee Method,null,null
419,Recall for Target Method,null,null
420,Figure 4: Reuters RCV1-v2 Subject Codes.,null,null
421,"lection, ordered by R. We see that 28 of the 30 recall points for the knee method (shown by the green curve) fall above 0.7, indicating reliability of 0.93, while all of the points for the target method (shown by the red curve) fall above 0.7, indicating reliability of 1.00 for this collection. We also see that the most of the recall points for the knee method fall above those for the target method, indicating higher median recall, and the (signed) area between the curves is positive, indicating higher mean recall. Per-topic effort is shown as a bar graph on a logarithmic scale spanning three orders of magnitude. For small R, the knee method entails about 100 times less effort than the target method, while for large R, the effort is comparable.",null,null
422,"The bottom panel of Figure 2 follows the same format, comparing the budget method (shown in blue) to the target method (shown in red). While the budget method achieves higher recall than the target method for nearly all topics, that superiority is not reflected in higher reliability. Effort for the two methods is very similar. The same observations apply to the results for the other collections: For low R, recall for the budget method exceeds that of the target method, while effort is indistinguishable; for large R, recall and effort are indistinguishable from the knee method. Both methods are reliable.",null,null
423,"For brevity, we show graphical results comparing only the knee and target methods for the other collections. Tabular results for all methods are presented in Table 2.",null,null
424,"Figure 3 shows results for the Sandbox task of the TREC 2015 Total Recall Track, which was notable in that participants had no prior access to the datasets or the topics, and their retrieval systems had to run fully autonomously. The top panel shows our results for the Kaine collection, which consisted of about 400,000 documents from Tim Kaine's eight-year tenure as Governor of Virginia. These documents had been previously reviewed and labeled by the archivist at the Library of Virginia according to four statutory categories: ""record"" (versus ""non-record""), ""open record,"" ""restricted record,"" and ""pertaining to the Virginia Tech shooting."" Two of the topics had moderately high R  104, and two had very high R  105. For all topics, the knee method achieved higher recall at the expense of somewhat higher effort. The bottom panel shows our results for the MIMIC II collection, which consisted of about 30,000 medical records",null,null
425,Recall 12 23 33 49 59 72 91 112 168 321 610,null,null
426,Effort (Documents Reviewed),null,null
427,TREC 2002 Filtering Track -- Assessor Topics,null,null
428,1000000 1,null,null
429,0.8 100000,null,null
430,0.6,null,null
431,0.4 10000,null,null
432,0.2,null,null
433,0,null,null
434,1000,null,null
435,Number of Relevant Documents in Collection,null,null
436,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,null,null
437,TREC 2002 Filtering Track -- Intersection Topics 1000000,null,null
438,1,null,null
439,0.8 100000,null,null
440,0.6,null,null
441,0.4 10000,null,null
442,0.2,null,null
443,0,null,null
444,1000,null,null
445,Number of Relevant Documents in Collection,null,null
446,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,null,null
447,Figure 5: TREC 2002 Filtering Track Collections.,null,null
448,Recall 21 28 39 45 50 59 78 89 137 218 349,null,null
449,Effort (Documents Reviewed),null,null
450,"collected from a hospital intensive care unit. The documents consisted of nurses' notes, radiology reports, and discharge summaries. The ""topics"" consisted of ICD-9 diagnostic codes extracted from non-textual database records. With one exception (R ,"" 179), all topics had moderately high R. The knee method generally achieved higher recall than the target method, at the expense of somewhat higher effort for most topics.""",null,null
451,"Figure 4 shows the results for the RCV1-v2 dataset, using the subject categories and descriptions published with the dataset as topics [11]. Over a very wide range 101 R 105, we observe a familiar pattern: The knee method has somewhat higher recall and lower variance, with dramatically lower effort, for small R.",null,null
452,"Figure 5 shows results for two sets of topics created for the TREC 2002 Filtering Track. The top panel shows results for topics that were created and assessed by NIST for the track. All topics had low R 610; the majority had very low R  100. For all topics, including those with the lowest R 100, the knee method achieved near-perfect recall. Recall for the target method showed much higher variance, suggesting that its reliability is actually lower. The knee method entails order(s) of magnitude less effort. The lower panel shows results for intersection topics, each of which was the conjunction of two RCV1-v2 subject categories. If rel1(d) and rel2(d) indicate relevance for two RCV1-v2 topics, rel1(d)  rel2(d) indicates relevance for the intersection",null,null
453,81,null,null
454,TREC 2004 Robust Track,null,null
455,1,null,null
456,1000000,null,null
457,0.8,null,null
458,100000 0.6,null,null
459,Recall 6 9 11 14 17 19 22 27 28 33 35 39 46 50 57 66 71 80 94 113 130 161 194 254,null,null
460,Effort (Documents Reviewed),null,null
461,0.4 10000,null,null
462,0.2,null,null
463,0,null,null
464,1000,null,null
465,Number of Relevant Documents in Collection,null,null
466,Surplus Effort for Knee Method Surplus Effort for Target Method,null,null
467,Recall for Knee Method Recall for Target Method,null,null
468,TREC 2005 Robust Track 1000000,null,null
469,1,null,null
470,Recall 9 21 32 42 52 60 65 71 83 88 97 109 111 121 127 151 153 163 165 177 183 232 242 280 356,null,null
471,Effort (Documents Reviewed),null,null
472,0.8 100000,null,null
473,0.6,null,null
474,0.4 10000,null,null
475,0.2,null,null
476,0,null,null
477,1000,null,null
478,Number of Relevant Documents in Collection,null,null
479,Effort for Knee Method Effort for Target Method Recall for Knee Method Recall for Target Method,null,null
480,Figure 6: TREC 2004-2005 Robust Track Collections.,null,null
481,"topic. The intersection topics were reported as a failed experiment [17], since no system achieved reasonable results on them. The results show that, while the effort to achieve high recall for these anomalous topics is inordinately large, our stopping methods are reliable.",null,null
482,"Figure 6 shows results for the TREC 2004 and 2005 Robust tracks. In 2004, the Robust Track aggregated 150 topics developed for the TREC 6, TREC 7, and TREC 8 Ad-Hoc tasks, 50 topics developed for the 2003 Robust Track, and 49 new topics, for a total of 249 topics. For 2005, 50 of these topics--those deemed to be ""difficult""--were reprised with a new dataset. The top panel reports our results for 2004; the bottom for 2005. The results further confirm that the target and knee methods both achieve high reliability, while the knee method entails dramatically less effort.",null,null
483,6. DIMINISHING LOSS,null,null
484,"As evidenced by the results above, reliability does not capture certain important aspects of effectiveness or efficiency. Moreover, empirical measurements of reliability lack statistical power, while parametric estimates depend on assumptions regarding the distribution of recall values. Since the choices of acceptable recall and acceptable reliability are both somewhat arbitrary, bias due to incorrect distributional assumptions may be of little consequence. We suggest that reporting the mean µ and standard deviation  of recall",null,null
485,"conveys more useful information, if not a provably accurate estimate of reliability. Such an estimate would have to be compared to one or more tacit thresholds to determine the reliability of the method; for example, assuming normality, any pair of µ and  such that µ - 1.64  0.70 would be 95% reliable. More generally, the value of Q ,"" µ - 1.64 is a quantitative measure of quality, which may be used to determine the threshold level of acceptable recall for which 95% reliability may be obtained. Alternatively, by substituting the appropriate z-score in place of 1.64, a threshold of reliability different from 95% may be tested.""",null,null
486,"We suggest that reliability and recall should be supplanted by quality estimates based on loss functions, of which recall and reliability are special cases. We define Q ,"" 1 - loss, where loss is the mean value of a loss function over all topics. If""",null,null
487,"loss ,"" 1 - recall , Q "","" recall ; if,""",null,null
488,"loss ,"" 0 (recall  0.7) , Q "", reliability . 1 (recall < 0.7)",null,null
489,A quadratic loss function such as:,null,null
490,"lossr , (1 - recall)2",null,null
491,(4),null,null
492,"captures the desirability of consistently high recall, subsuming the roles of µ and  in the previous discussion. Our aspirational goal is to achieve 100% recall. Any shortfall is penalized, and larger shortfalls are penalized more heavily.",null,null
493,"Quadratic loss further generalizes to other aspects of quality, such as graded relevance, facet relevance [6], and efficiency. For example, let a1, a2, . . ., an be categories of relevance, and rela(d) be the indicator function for category a. Define:",null,null
494,recalla,null,null
495,",",null,null
496,|{d,null,null
497,C|relret(d)  rela(d)}| |{d  C|rela(d)}|,null,null
498,"lossa , (1 - recalla)2",null,null
499,n,null,null
500,n,null,null
501,"loss ,"" ilossai , where 1 "", i .",null,null
502,(5),null,null
503,"i,i",null,null
504,"i,1",null,null
505,The,null,null
506,choice,null,null
507,of,null,null
508,weights,null,null
509,i,null,null
510,is,null,null
511,not,null,null
512,critical;,null,null
513,the,null,null
514,value,null,null
515,i,null,null
516,",",null,null
517,1 n,null,null
518,for,null,null
519,"all i will often suffice, as it rewards consistent recall over",null,null
520,"all categories, with the effect that documents belonging to",null,null
521,rarer categories are afforded more influence.,null,null
522,"Review effort may also be modeled as a category of loss,",null,null
523,"thus quantifying the notion of ""reasonable effort."" For the",null,null
524,"problem as we have framed it, an ideal method would entail",null,null
525,"effort , R. From the presentation of results in the TREC",null,null
526,"2015 Total Recall Track Overview [13], we observe that a",null,null
527,"reasonable effort might entail effort ,"" aR + b, where a  1""",null,null
528,"represents effort proportional to sunk review cost, and b ",null,null
529,0 represents fixed overhead. We suggest that a  2 and,null,null
530,b  1000 represent reasonable effort to achieve recall  0.70,null,null
531,with 95% reliability. The use of a quadratic loss replaces the,null,null
532,a and b thresholds by a soft target:,null,null
533,"losse ,",null,null
534,b2 |C |,null,null
535,ef f ort R+b,null,null
536,2,null,null
537,.,null,null
538,(6),null,null
539,"losse may be used to measure efficiency in its own right, or treated as a category loss in (5).",null,null
540,82,null,null
541,Collection,null,null
542,At Home Kaine MIMIC II RCV1-v2 Filtering Intersection Robust-04 Robust-05,null,null
543,Target Method,null,null
544,lossr,null,null
545,0.0132 0.0815 0.1229 0.1475 0.1011 0.1057 0.0870 0.1141,null,null
546,losse 0.0090 0.0016,null,null
547,0.0734 0.0883 0.2110 0.2499 0.4136 0.2368,null,null
548,lossre 0.0111 0.0577 0.1012 0.1216 0.1654 0.1919 0.2989 0.1858,null,null
549,Knee Method,null,null
550,lossr 0.0197 0.0252,null,null
551,0.0516 0.0947 0.0181 0.0818 0.0430 0.0570,null,null
552,losse 0.0000 0.0025 0.0862 0.0154 0.0079 0.2740 0.0481 0.0265,null,null
553,lossre 0.0099 0.0179 0.0710 0.0678 0.0140 0.2022 0.0456 0.0445,null,null
554,Budget Method,null,null
555,lossr 0.0056 0.0252 0.0516 0.0824 0.0015 0.0159 0.0025 0.0087,null,null
556,losse,null,null
557,0.0108 0.0025 0.0866 0.0795 0.2278 0.2947 0.3865 0.1843,null,null
558,lossre,null,null
559,0.0082,null,null
560,0.0179 0.0712 0.0809 0.1611 0.2087 0.2733 0.1305,null,null
561,"Table 3: Root Mean Loss for Relevance, Effort, and Combined.",null,null
562,Target Method,null,null
563,lossr,null,null
564,lossh,null,null
565,0.0837 0.0504,null,null
566,Knee Method,null,null
567,lossr,null,null
568,lossh,null,null
569,0.0134 0.0021,null,null
570,Budget Method,null,null
571,lossr 0.0007,null,null
572,lossh 0.0011,null,null
573,Table 4: Root Mean Loss for Relevance and High Relevance.,null,null
574,"In Table 3, we report, for each collection, the root mean",null,null
575,"loss ( loss) over all topics for relevance loss as defined in (4); effort loss as defined in (6); as well as their unweighted average, lossre ,"" 0.5 · lossr + 0.5 · losse. The results show conclusively the superiority of the budget method in terms of lossr. They show the general superiority of the knee method in terms of losse, while calling to our attention three collections where the target method is more efficient. On inspection, we see that two of the three collections have exclusively or nearly exclusively topics with high prevalence. We further see that that the the target method's narrow margin of superiority in terms of lossr is offset by a wide margin of inferiority in losse, as reflected in lossre. For the intersection collection, no system achieved acceptable losse.""",null,null
576,"The bottom line is that the quality loss results support our qualitative observation that the knee method affords the best balance between consistently high recall and consistently low effort; the budget method provides consistently higher recall at the expense of disproportionate effort for topics with few relevant documents; the target method, while provably reliable, yields empirical results that are generally inferior to the knee and budget methods.",null,null
577,"To illustrate the use of quality loss for graded relevance, we used a subset of 84 topics from Robust-04, for which relevance assessments were available for the categories ""highly",null,null
578,"relevant"" and ""relevant."" Table 4 shows lossr and lossh for these categories, respectively. The knee and target methods have lower lossh, than lossr, indicating they retrieve highly relevant documents more consistently than merely relevant documents. The budget method shows the opposite effect, but even so, is markedly superior to the target and knee methods. While we cannot draw any firm conclusions from this small experiment, the results do not support the proposition that TAR methods achieve high recall by ""bulking up"" on marginal documents at the expense of important ones (cf. [12]).",null,null
579,7. DISCUSSION,null,null
580,"To our knowledge, the target method is the first provably reliable method for TAR. The commonly used frequentist acceptance test (see [1, 9]) offers a p-value or confidence level which is a measure of the reliability of the test, not",null,null
581,"the reliability of the TAR method, not the probability that a given result is acceptable, and not the probability that a TAR method will pass the acceptance test. In eDiscovery, it is common to calculate a frequentist recall estimate, with a 5% margin of error and 95% confidence, and deem the result acceptable if the estimate exceeds 75%. Calculating such an estimate requires a sample of about 385 random relevant documents, entailing 38.5 times as much surplus effort as the target method.",null,null
582,"Our proof of reliability does not require that the target sample T be chosen at the outset, as long as it is independent of the retrieval method. The target method could be used as an acceptance test, such that the consequence of failing the test would be to continue to retrieve documents without knowledge of T , until all the documents in T are retrieved.",null,null
583,"Over test collections like the ones used in this study, there can be little doubt (p  0.00) that the knee and budget methods are reliable, that the budget method is more reliable than both the knee and target methods, and that the knee method is the most efficient. As with any empirical work, the test collections constitute convenience samples and ongoing research is necessary to characterize the scope of TAR tasks to which our results may be generalized.",null,null
584,"The target method is reliable regardless of the underlying review method; however, if the underlying method uses a human in the loop to formulate queries or to influence the selection of documents in any way, that human must be isolated from any knowledge of T . The simplest approach to accomplish this goal might be to complete all such human intervention before drawing T , and to rely on fully automated document selection thereafter. An alternative would be to establish an ""information barrier"" between those who draw T and those who conduct the search.",null,null
585,"This work establishes the reliability of the knee and budget methods as applied to BMI. It remains to be determined how well these approaches would work--possibly with different tuning parameters--for other CAL methods, including hybrid systems in which a human is afforded influence in the selection of documents for review. It is not obvious how to adapt the knee or budget method to SPL or SAL, for which an essential question is when to stop training.",null,null
586,"The target method, by design, targets less than 100% recall. It could be modified to continue past the point at which",null,null
587,83,null,null
588,"the last document in T is retrieved, thereby expending additional effort to increase the probability of achieving 100% recall. One might, for example, extrapolate from the distribution of rank(d  T ). The knee method, on the other hand, does target 100% recall, and only incidentally optimizes reliability. It appears that loss functions better characterize the tension among consistency, effectiveness, and efficiency, as compared to goal-post methods. Regardless of which measure is chosen for evaluation, systems should be tuned to optimize their suitability for their intended purpose, not the measure itself (cf. [22]).",null,null
589,8. CONCLUSIONS,null,null
590,"Reservations about the effectiveness and reliability of TAR have impeded its adoption for eDiscovery and other highrecall retrieval tasks. A primary area of concern has centered on the issue of ""when to stop,"" or knowing with reasonable certainty--and being able to show an adversary or the court--that a particular TAR effort has identified an acceptable amount of relevant information. Many approaches to validation in common use today are simply invalid, or require disproportionate effort compared to the information they yield, and are often misunderstood and misapplied [9, 16].",null,null
591,"We offer a method to determine when to stop that is guaranteed to be reliable, for the price of reviewing a number of random documents that is an order of magnitude less than acceptance tests that estimate recall, but neither determine when to stop nor guarantee reliability. We provide two other methods that entail no effort beyond that required by the underlying TAR method and, while not providing a guarantee of reliability, consistently demonstrate better reliability, and better recall, when evaluated on eight test collections, comprising 555 topics and 4.5M documents. Of particular interest is the knee method which, in contrast to the other methods, is demonstrated to be reliable and efficient when the collection contains few relevant documents.",null,null
592,"While our primary results are demonstrated using measures derived from traditional goal-post methods--binary relevance, a recall threshold, and a reliability floor--we describe how loss functions may be formulated to capture the tension among consistency, degrees of relevance, facets of relevance, and efficiency. We apply these formulae to show insights into our results that might not have been readily apparent from the goal-post measures.",null,null
593,9. REFERENCES,null,null
594,"[1] M. Bagdouri, W. Webber, D. D. Lewis, and D. W. Oard. Towards minimizing the annotation cost of certified text classification. In SIGIR 2013.",null,null
595,"[2] D. Blair and M. E. Maron. An evaluation of retrieval effectiveness for a full-text document-retrieval system. Commun. ACM, 28(3):289­299, 1985.",null,null
596,"[3] D. C. Blair. Stairs redux: Thoughts on the stairs evaluation, ten years after. J. Am. Soc. Inf. Sci., 47(1):4­22, Jan. 1996.",null,null
597,"[4] G. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In TREC 2009.",null,null
598,[5] G. V. Cormack and M. R. Grossman. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In SIGIR 2014.,null,null
599,[6] G. V. Cormack and M. R. Grossman. Multi-faceted recall of continuous active learning for technology-assisted review. In SIGIR 2015.,null,null
600,"[7] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv:1504.06868, 2015.",null,null
601,"[8] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR 1998.",null,null
602,"[9] M. R. Grossman and G. V. Cormack. Comments on ""The implications of Rule 26(g) on the use of technology-assisted review"". Fed. Cts. L. Rev., 7:285­312, 2014.",null,null
603,"[10] C. Lefebvre, E. Manheimer, and J. Glanville. Searching for studies. Cochrane Handbook for Systematic Reviews of Interventions, 2008.",null,null
604,"[11] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. RCV1: A new benchmark collection for text categorization research. J. Mach. Learn. Res., 5:361­397, 2004.",null,null
605,"[12] D. Remus and F. S. Levy. Can robots be lawyers? Computers, lawyers, and the practice of law. http://dx.doi.org/10.2139/ssrn.2701092, 2015.",null,null
606,"[13] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. Notebook Draft TREC 2015 Total Recall Track Overview. In TREC 2015.",null,null
607,[14] M. Sanderson and H. Joho. Forming test collections with no system pooling. In SIGIR 2004.,null,null
608,"[15] V. Satop¨a¨a, J. Albrecht, D. Irwin, and B. Raghavan. Finding a ""kneedle"" in a haystack: Detecting knee points in system behavior. In ICDCSW 2011.",null,null
609,"[16] K. Schieneman and T. Gricks. The implications of Rule 26(g) on the use of technology-assisted review. Fed. Cts. L. Rev., 7:239­274, 2013.",null,null
610,[17] I. Soboroff and S. Robertson. Building a filtering test collection for TREC 2002. In SIGIR 2003.,null,null
611,[18] G. Taguchi. Introduction to Quality Engineering: Designing Quality Into Products and Processes. 1986.,null,null
612,"[19] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, 2000.",null,null
613,"[20] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, pages 143­170. Springer, 2002.",null,null
614,"[21] E. M. Voorhees and D. K. Harman. The Text REtrieval Conference. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, pages 3­19. MIT Press, 2005.",null,null
615,"[22] E. Yilmaz and S. Robertson. On the choice of effectiveness measures for learning to rank. Information Retrieval, 13(3):271­290, 2010.",null,null
616,"[23] J. Zobel, A. Moffat, and L. A. Park. Against recall: Is it persistence, cardinality, density, coverage, or totality? In ACM SIGIR Forum, volume 43, pages 3­8. ACM, 2009.",null,null
617,84,null,null
618,,null,null

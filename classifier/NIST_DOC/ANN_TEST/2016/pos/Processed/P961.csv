,sentence,label,data
0,Ranking Documents Through Stochastic Sampling on Bayesian Network-based Models: A Pilot Study,null,null
1,"Xing Tan1, Jimmy Xiangji Huang1 and Aijun An2",null,null
2,"Information Retrieval and Knowledge Management Research Lab 1School of Information Technology, 2Department of Computer Science & Engineering",null,null
3,"York University, Toronto, Canada",null,null
4,"{xtan, jhuang}@yorku.ca, aan@cse.yorku.ca",null,null
5,ABSTRACT,null,null
6,"Using approximate inference techniques, we investigate in this paper the applicability of Bayesian Networks to the problem of ranking a large set of documents. Topology of the network is a bipartite. Network parameters (conditional probability distributions) are determined through an adoption of the weighting scheme tf -idf . Rank of a document with respect to a given query is defined as the corresponding posterior probability, which is estimated through performing Rejection Sampling. Experimental results suggest that performance of the model is at least comparable to the baseline ones such as BM 25. The framework of this model potentially offers new and novel ways in weighting documents. Integrating the model with other ranking algorithms, meanwhile, is expected to bring in performance improvement in document ranking.",null,null
7,Keywords,null,null
8,Info. Retrieval; Bayesian Networks; Stochastic Sampling,null,null
9,1. INTRODUCTION,null,null
10,"Probabilistic Graphical Models [10] in the form of Bayesian Networks (BN) [6] are widely used to represent knowledge with uncertainties. In the recent years, computational technologies and tools for BN-based models are becoming increasingly powerful. That being the case, modeling problems in Information Retrieval (IR), in particular for document ranking, as probabilistic inference problems in BNbased models has achieved only limited success to date. Major reasons for such relatively small progress in BN-based approaches for document ranking are, for one, conceptually it is challenging to appropriately identify causalities in document ranking (i.e., deciding network topology) and then to accurately capture the uncertainties (i.e., deciding network parameters) in order to construct a BN model for IR; and two, computationally exact inference algorithms associated with the model is bound to be intractable as practically the size of the BN model in terms of nodes representing both the number of documents and vocabulary size in words, can be easily in a few millions.",null,null
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",null,null
12,"SIGIR '16, July 17-21, 2016, Pisa, Italy",null,null
13,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,null,null
14,DOI: http://dx.doi.org/10.1145/2911451.2914750,null,null
15,"In this paper, we investigate the applicability of BNs to the problem of ranking a large set of documents. We propose a model, which specifically takes into considerations both the appropriateness in the semantics of causalities, and the computational tractability of probabilistic inferences. Topology of the network is a bipartite. Conditional probability distributions are determined through adopting the weighting scheme tf -idf . Experimental results, obtained from working on both computer-generated and standard document sets suggest that performance of the model is at least comparable to the baseline ones such as BM 25 ([2, 11]).",null,null
16,The remainder of this paper is organized as follows. Section 2 presents the background and preliminaries. Section 3 introduces the model. Experimental results are reported and analyzed in Section 4. Section 5 concludes the paper.,null,null
17,2. PRELIMINARIES,null,null
18,"In this section, document ranking in IR, and BN, are briefly reviewed.",null,null
19,2.1 Document Ranking in IR,null,null
20,"One of the fundamental tasks in IR is document-ranking: Given a set of documents D such that D ,"" {d1, . . . , dM } and |D| "","" M , a set of terms T "","" {t1, . . . , tN } and |T | "","" N , and a collection of query terms q such that q  T , documents in D need to be ranked in a complete order according to their respective relevance to q (other criteria such as """"diversity"""" might also be considered for ranking). To do this, a typical approach is to define a score function score(q, di) that returns a numeric score for each document di  D with respect to q. Documents can then be ranked on their scores in descending order. The top S elements will be selected to construct the set S, where |S| "", S.",null,null
21,"Let tft,d denote term frequency, the number of occurrences of term t in document d; dft denotes document frequency, the number of documents in D that contain the term t; and idft ,"" log (M/dft) denotes inverse document frequency. Summing up on tft,d × idft for each term t  q with respect to d defines a baseline score function for document ranking: score(q, d) "","" tq(tft,d ×idft). Definitions for variant score functions of tf -idf such as BM25, can be found in [11]. In addition, collection frequency in D and its subset S, are defined as the total number of occurrences of t in D and in S, and denoted by cft and sft, respectively.""",null,null
22,2.2 Bayesian Networks,null,null
23,"A Bayesian Network is a directed acyclic graph where nodes correspond to random variables [6]. Pairs of nodes in the graph might be connected by a directed edge. For example, given two nodes X and Y in the graph, if X enters Y , it is said that X is a parent of Y . Effect of the par-",null,null
24,961,null,null
25,ents of a node X in the graph is quantified by a conditional probability distribution P (X|P arents(X)).,null,null
26,"BNs are often used to carry out probabilistic inference: computing posterior distribution of a set of variables given a set of evidence variables ­ variables whose values are observed or assigned. Consider a simple example of Baby World, which consists of four random variables H, T , C, and R, corresponding to the variable facts that the baby is Hungary, Tired, Crying, and scReaming, respectively. A BN for this world is shown in Figure 1. After all four conditional probability distributions as listed in the figure are specified, we could compute, for example, the probability that the baby is hungry if we observe that it is crying but not screaming: P (H is true | C is true, and R is false).",null,null
27,P (H) Hungry? H,null,null
28,P (T ) T Tired?,null,null
29,"P (C|H, T ) C Crying?",null,null
30,"P (R|H, T ) scReaming? R",null,null
31,Figure 1: A Bayesian Network: Baby World.,null,null
32,3. MODEL,null,null
33,"This section presents the model, explains how samplings are performed, and justifies the merits of our model.",null,null
34,3.1 Network Topology and Conditional Probabilities,null,null
35,"Suppose a user specifies a set of terms as a query q for a set of documents D, a subset S  D of documents need to be retrieved and ranked in a complete order. To be explained in this section, we formulate this problem into the problems of calculating posterior probability values in a BN-based model where the original query is treated as the observed evidence.",null,null
36,"In our model, the probability space is induced accordingly by two sets of random variables D (document random variables) and T (term random variables), where Di  D for 1  i  M , and Tj  T for 1  j  N . Each document Di takes two values: V al(Di) ,"" {d1i , d0i }, which represents the values of """"Di selected with respect to a query"""" (d1i ) or not (d0i ); Similarly, V al(Tj) "","" {t1j , t0j }, which represents the values of term """"Tj is a query term"""" (t1j ) or not (t0j ). The BN, as shown in Figure 2, is a two-layer directed graph, which contains a node in the top layer for each document variable Di and a node in the bottom layer for each term variable Tj. In the graph, an edge from Di to Tj represents that term Tj appears in the document Di. We assume no edges between document variables in D, and no edges between term variables in T . In addition to the graph, two types of""",null,null
37,D,null,null
38,D1 D2,null,null
39,... ...,null,null
40,DM,null,null
41,T,null,null
42,T1,null,null
43,T2,null,null
44,... ...,null,null
45,... ...,null,null
46,TN,null,null
47,Figure 2: A BN graph for document ranking.,null,null
48,"probabilities need to be specified to capture the nature of the dependence of variables: P (Di), the prior distribution over a document Di, and P (Tj|D1, . . . , DM ), the conditional probability distribution of Tj given D1, D2, . . . , and DM . In our model, P (Di) represents the distribution that Di is selected in S or not, hence it is reasonable to define, for any",null,null
49,"document Di in D, the probability that Di is eventually selected into S equals the ratio of the size of S to the size of D, i.e., P (d1i ) , S/M .",null,null
50,"The conditional distribution P (Tj|D1, . . . , DM ) specifies the distribution over the term Tj, which depends on the actual content of S, the set of documents selected. That is to say, specifically, for each subset of D (totally 2M of them), we need to specify a distribution for t1j , the event that tj is actually a query term. Since the number of parents of a term in the network is not bounded by a constant, we know that exact inference here has exponential time complexity in worst cases. Nevertheless what we really need is to calculate, for any document variable Di where 1  i  M , the relevance of Di to evidence q~, i.e., the posterior probability of P (d1i |q~), the value of which can be estimated through stochastic sampling. For simplicity in explanation, we assume that q~ contains only one term t, without loss of generality.",null,null
51,3.2 Estimating Posteriors Through Sampling,null,null
52,General reasoning problems of probabilistic inference in,null,null
53,"BNs, and even their corresponding approximate versions,",null,null
54,"are NP-hard ([4], [5]). Due to this computational intractabil-",null,null
55,"ity, one often turns to randomized sampling methods (e.g.,",null,null
56,"Rejection Sampling [12], which is used in our current re-",null,null
57,search) to approximate posterior probabilities. Asymptoti-,null,null
58,cally accuracy of these sampling methods would usually be,null,null
59,improved as the number of samples increases.,null,null
60,Given a BN and its specified prior distributions for all n,null,null
61,"variables {X1, X2, . . . , Xn}, where Xi  X for 1  i  n,",null,null
62,forward sampling samples all nodes in X in an order consis-,null,null
63,"tent with the topological structure of the BN, and the prob-",null,null
64,"ability of a specific event, written as (x1, . . . , xn)1, generated",null,null
65,from forward sampling equals to,null,null
66,"n i,1",null,null
67,P,null,null
68,(xi|,null,null
69,"parents(Xi)),",null,null
70,"which in turn equals to the joint distribution P (x1, . . . , xn).",null,null
71,"Suppose totally N samples are taken, and the number of oc-",null,null
72,"currences of an event (x1, . . . , xn) equals to N(x1,...,xn), then the ratio N(x1,...,xn)/N is an approximation to P (x1, . . . , xn). With observation of evidence e for E, where E  X , the con-",null,null
73,"ditional probability P (X , x|e) can be further estimated",null,null
74,"through Rejection: first, all samples that do not match e",null,null
75,"are rejected in N , to obtain N1; second, all samples in N1",null,null
76,"and compatible with X ,"" x are put into N2; third, N2/N1""",null,null
77,"is an estimate to P (X , x|e).",null,null
78,Consider our model again and suppose there are P sam-,null,null
79,"ples where |P| ,"" P . During sampling, we dynamically main-""",null,null
80,"tain a vector of counters C, where |C| , M . All counters",null,null
81,in C are initialized to zero. Our sampling strategy say for,null,null
82,"the jth sample P j where 1  j  P : Step 1, each docu-",null,null
83,ment variable is sampled according to the distribution S/M ;,null,null
84,"Those selected document variables are put into Sj, thus the",null,null
85,"expected value of |Sj| is S. Step 2, we accept this sample",null,null
86,if and only if the collection frequency of Sj with respect to,null,null
87,"term t proportionally exceeds the one of D. Formally, the",null,null
88,sample is accepted iff,null,null
89,sftj cft,null,null
90,>,null,null
91,S M,null,null
92,.,null,null
93,If sample P j,null,null
94,is accepted,null,null
95,"and Di  Sj, ci  C, which is the corresponding counter for",null,null
96,"Di, would be increased by one.",null,null
97,"After completion of sampling, the set P would be mutual-",null,null
98,"exclusively partitioned into two sets, the set of accepted sam-",null,null
99,"ples Paccepted, and the set of rejected ones Prejected. The",null,null
100,vector C would be updated for |Paccepted| times. Values,null,null
101,"1The term (x1, . . . , xn) is an abbreviation for (X1 ,"" x1, . . . , Xn "", xn).",null,null
102,962,null,null
103,"stored in the counters of C, are actually scores for their corresponding documents with respect to the query term t. The documents can thus be ranked according to their scores.",null,null
104,3.3 Justification of Methodology,null,null
105,"In the literature, considerable research in investigating potential linkages between BNs and IR in general, has been reported (most notably [1], [7], [13], [14]). The originality and value of our research contribution lies in the following facts.",null,null
106,"Model Semantics. The model defines 2(M+N) different states, for different combinations of truth assignments to all random variables in D  T . A state specifies an instance of which random variables are true and which are false. For each state, its joint probability distribution theoretically can be calculated (although computationally it might be impractical). We are only interested in those states where statistically S out of D variables are true, since we only concern about the problem of selecting S out of D documents related to a given query.",null,null
107,"Causality. In the model, document variables in D are designed, in consistence with common perceptions, to have direct causal influence on term in T . For example, causal relation ""Di  Tj"" is interpreted as: If Di is selected to be a member of S (i.e., Di  S) then the term Ti should be of interest to the user.",null,null
108,"Scalability. The size of the problem of document ranking in practise is often in the magnitude of a few millions, if not more. A network built-up from these problems is large in size and multiply connected, making it dauntingly challenging to perform exact probabilistic inference. Consequently, it can be seen from the literature that experiments in earlier work (e.g., [7] and [1]), are restricted to cases with maximal a few thousand documents only. The development of BNs however has reached the point where approximate inference algorithms, such as randomized sampling, loopy propagation, or variational approximation, can make a practical difference. We adopt a direct sampling method in this research.",null,null
109,"Bipartite Network Structure. The underlying undirected graph of the network is a bipartite: nodes are grouped into two types, only connections between nodes from two different types are allowed. Recently, Bayesian models on bipartite graphs have found their ways to modeling real-world applications in social networks, with appealing properties demonstrated [3]. It remains to be investigated how these results can be utilized into our own framework of BN for IR. Nevertheless, due to this simplicity of topological structure, additional features (e.g., ontological/relational/logical representation and reasoning: see [8] for a mosaic of such proposals) can be incorporated into the current model.",null,null
110,4. EXPERIMENTAL RESULTS,null,null
111,"In this section we compare the performance of our proposed model with the ones of tf -idf , BM 25 and Golden (to be explained in Section 4.1). We first work on a set of computer-generated random documents (DR1 , DR2 , DR3 and DR4 ) and then on D1 and D2, which are two subsets of WT2G, a standard TREC test collection.",null,null
112,4.1 Documents Generation,null,null
113,"To simplify matters, we assume that all generated documents in DR are with same document length, which equals to the size of the vocabulary TR. For any document Dr  DR, occurrences of terms in Dr follow a Normal Distribution N (µ, ), where the values of mean µ and the standard de-",null,null
114,Term Frequency Term Frequency,null,null
115,An Example Document,null,null
116,3000,null,null
117,Center Shifted,null,null
118,3000,null,null
119,2500,null,null
120,2500,null,null
121,2000,null,null
122,"|D| , 100000",null,null
123,2000,null,null
124,"|D| , 100000",null,null
125,"µ , 50",null,null
126,"µ , 18",null,null
127," , 17",null,null
128," , 17",null,null
129,1500,null,null
130,1500,null,null
131,1000,null,null
132,1000,null,null
133,500,null,null
134,500,null,null
135,0 0 10 20 30 40 50 60 70 80 90 100,null,null
136,Terms,null,null
137,0 0 10 20 30 40 50 60 70 80 90 100,null,null
138,Terms,null,null
139,Figure 3: An example document (created according,null,null
140,"to a Normal Distribution with µ , 50 and  ,"" 17),""",null,null
141,"and its variant with the center shifted to 18 (µ , 18).",null,null
142,"viation  can be adjusted. However all documents in DR share the same µ and . Note that the smaller the value of , the terms cluster more closely to µ. The center of a given document is shifted to a random center before being stored into a vector. To illustrate the idea, an example document with µ ,"" 50,  "","" 17, |DR| "","" 10000, and |TR| "","" 100, and its variant with shifted center (now, µ "", 18) is respectively shown in the left subplot (and the right one) of Figure 3.",null,null
143,"Accordingly we introduce the baseline ranking method Golden. That is, given an input query q and a document d, the score(q, d) is defined as the difference between the center of d, and q. When the value of score(q, d) is smaller (greater), it means document d is more (less) related to the term q. Golden is used as one of the three methods for comparisons in Figure 4.",null,null
144,4.2 Experiments,null,null
145,"We first work on four sets of randomly generated documents: DR1 , where  ,"" 1250, DR2 ,  "","" 1000, DR3 ,  "","" 833, DR4 ,  "","" 416. Between these sets from DR1 up to DR4 , documents are more closely clustered around their means, thus intuitively more reliable in the sense of IR. Collection size, document length, and vocabulary size, are set to be all equal: |DR| "", |TR| , 10000. Experiments related to a specific set is pictorially summarized in the corresponding sub-figure in Figure 4.",null,null
146,"Consider Figure 4.a, for example, a term is queried against DR1 , three different ranking methods, i.e., Golden, BM25, and Rejection, return three different completely ordered sequences of 500 elements (the number 500 is obtained from |DR| × 0.05, where 0.05 is the pre-specified ratio, i.e., portion of all documents in DR1 need to be ranked). Results of pair-wise comparisons between these three methods are reported in Figure 4.a, where x-axis values indicate sample sizes and y-axis values indicate how many documents out of the 500 ranked ones are actually agreed between two given methods. For example, the red dot pointed by the arrow in Figure 4.a refers to the fact that totally 336 documents are shared by the 500 documents retrieved from applying BM25 on DR1 , and the 500 elements obtained from performing Rejection Sampling on DR1 (with the sample size equaling to 0.1 Million).",null,null
147,"Documents in both D1 and D2 (Figure 5) are drawn from dataset collection WT2G where |D1| , |D2| ,"" 2500, |T1| "", 50961 and |T2| ,"" 127487. First 100 elements obtained from three different ranking methods, tf -idf , BM 25, and Rejection are pair-wise compared in Figure 5.""",null,null
148,4.3 Brief Discussion,null,null
149,"This section draws the major observations from the present experimental study, and discusses some implications.",null,null
150,963,null,null
151,Num. of Agreed Documents Num. of Agreed Documents,null,null
152,"(a) DR1 and  , 1250",null,null
153,500,null,null
154,Golden-BM25,null,null
155,450,null,null
156,Golden-Rejection,null,null
157,BM25-Rejection,null,null
158,400,null,null
159,350,null,null
160,300 250,null,null
161,200,null,null
162,150,null,null
163,100,null,null
164,50,null,null
165,0,null,null
166,0,null,null
167,1K,null,null
168,10K 50K 0.1M 0.5M 1M 1.5M 2M,null,null
169,Sample Size,null,null
170,"(c) DR3 and  , 833",null,null
171,500,null,null
172,Golden-BM25,null,null
173,450,null,null
174,Golden-Rejection,null,null
175,BM25-Rejection,null,null
176,400,null,null
177,350,null,null
178,300,null,null
179,250,null,null
180,200,null,null
181,150,null,null
182,100,null,null
183,50,null,null
184,0,null,null
185,0,null,null
186,1K,null,null
187,10K 50K 0.1M 0.5M 1M 1.5M 2M,null,null
188,Sample Size,null,null
189,"(b) DR2 and  , 1000",null,null
190,500,null,null
191,Golden-BM25,null,null
192,450,null,null
193,Golden-Rejection,null,null
194,BM25-Rejection,null,null
195,400,null,null
196,350,null,null
197,300,null,null
198,250,null,null
199,200,null,null
200,150,null,null
201,100,null,null
202,50,null,null
203,0,null,null
204,0,null,null
205,1K,null,null
206,10K 50K 0.1M 0.5M 1M 1.5M 2M,null,null
207,Sample Size,null,null
208,"(d) DR4 and  , 416",null,null
209,500,null,null
210,Golden-BM25,null,null
211,450,null,null
212,Golden-Rejection,null,null
213,BM25-Rejection,null,null
214,400,null,null
215,350,null,null
216,300,null,null
217,250,null,null
218,200,null,null
219,150,null,null
220,100,null,null
221,50,null,null
222,0,null,null
223,0,null,null
224,1K,null,null
225,10K 50K 0.1M 0.5M 1M 1.5M 2M,null,null
226,Sample Size,null,null
227,"Figure 4: Pair-wise comparisons among the ranking methods Golden, BM25, and Rejection on data-sets with different standard deviations.",null,null
228,Num. of Agreed Documents,null,null
229,(a) Data Set: D1,null,null
230,100,null,null
231,90,null,null
232,80,null,null
233,70,null,null
234,60,null,null
235,50,null,null
236,40,null,null
237,30,null,null
238,20,null,null
239,TfIdf-BM25,null,null
240,10,null,null
241,TfIdf-Rejection,null,null
242,BM25-Rejection,null,null
243,0,null,null
244,0,null,null
245,1K,null,null
246,5k,null,null
247,7.5k 10K 50K 0.1M 0.5M 1M,null,null
248,Sample Size,null,null
249,Num. of Agreed Documents,null,null
250,(b) Data Set: D2,null,null
251,100,null,null
252,90,null,null
253,80,null,null
254,70,null,null
255,60,null,null
256,50,null,null
257,40,null,null
258,30,null,null
259,20,null,null
260,TfIdf-BM25,null,null
261,10,null,null
262,TfIdf-Rejection,null,null
263,BM25-Rejection,null,null
264,0,null,null
265,0,null,null
266,1K,null,null
267,5k,null,null
268,7.5k 10K 50K 0.1M 0.5M 1M,null,null
269,Sample Size,null,null
270,"Figure 5: Pair-wise comparisons among the ranking methods tf-idf, BM25, and Rejection on two datasets (2500 documents each) from WT2G.",null,null
271,"For more closely clustered documents, in essence all methods agree more on their rankings (as shown in Figure 4, they agree most on the set DR4 , but least on DR1 ); Following the same argument, we should claim that D1 is more clustered than D2.",null,null
272,"In our experimental settings, increasing sample size initially improves the performance sharply, but it tends to be leveling off after sample size is greater than certain value (e.g., 0.5M in the subplots of Figure 4). As the sample size increases, asymptotically the Rejection method agrees at least 80% with BM25 for almost all sets except for D2 (around 60% only). It seems that we should conclude that the proposed BN-based model can achieve competitive performance levels at relatively low cost in sampling.",null,null
273,"Since in Rejection, estimating posterior probabilities are based on the tf -idf ranking scheme, Rejection is a stochastic variant to the tf -idf ranking method. BM 25, meanwhile, can be deemed as a refinement to tf -idf . Hence, it is not a surprise that BM 25 agrees largely with Rejection. The more intriguing observation is that, with standard data-sets, the two methods disagree at least on 20% of their rankings. The reason as we speculate is either Rejection behaves somewhat differently from BM 25 in practise, or sampling with the current settings can get trapped and do not converge. In order to unravel this intricacy, a further investigation is necessary and desirable.",null,null
274,Num. of Agreed Documents Num. of Agreed Documents,null,null
275,5. SUMMARY & FUTURE WORK,null,null
276,"In this paper, document ranking in IR is transformed into the problems of estimating posterior probabilities through stochastic sampling on a BN designed for IR. Experimental results from this pilot study is quite encouraging in the sense that, with moderate sampling efforts, the model demonstrates its ranking capability comparable to BM 25. Additionally, graph-based structure and probability-based parameters of the model, together with other considerations in the model, suggest that new and novel weighing schemes for document ranking are conjecturally within a reach.",null,null
277,"Among many possible avenues, our direct future research includes 1) further evaluating performance of the model on WT2G, WT10G, and other standard dataset collections; 2) testing on parametric settings other than the current one that is based on term-frequency; 3) testing other sampling strategies (e.g., Gibbs Sampling [10], and the most recent ones [9]) to improve sampling efficiency and performance.",null,null
278,"Feedback to this work we have received encourages us to investigate in a broader context the relationships between the proposed BN model and other term weighting models ([15, 16]). With ease, most existing probability theory-based models in IR can actually be derived in this BN-based framework. It is thus rather promising to exploit this framework for a deeper understanding of existing term weighting models, and for the developments of new and better models in Information Retrieval.",null,null
279,6. ACKNOWLEDGEMENTS,null,null
280,"We gratefully acknowledge the anonymous reviewers for their insightful comments and suggestions. This research is supported by a Discovery grant from the Natural Sciences and Engineering Research Council of Canada (NSERC), an NSERC CREATE award in ADERSIM2 and an ORF-RE (Ontario Research Fund - Research Excellence) award in BRAIN Alliance3.",null,null
281,7. REFERENCES,null,null
282,[1],null,null
283,[2] [3],null,null
284,[4] [5] [6] [7] [8] [9],null,null
285,[10] [11] [12] [13] [14] [15] [16],null,null
286,"S. Acid, L. M. de Campos, J. M. Fern´andez-Luna, and J. F. Huete. An information retrieval model based on simple Bayesian networks. Int. J. Intell. Syst., 18(2):251­265, 2003.",null,null
287,"M. Beaulieu, M. Gatford, X. Huang, S. Robertson, S. Walker, and P. Williams. Okapi at TREC-5. In Proc. of TREC, pages 143­166, 1996.",null,null
288,"F. Caron. Bayesian nonparametric models for bipartite graphs. In Advances in Neural Information Processing Systems 25, pages 2051­2059. Curran Associates, Inc., 2012.",null,null
289,"G. F. Cooper. The computational complexity of probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42(2):393 ­ 405, 1990.",null,null
290,"P. Dagum and M. Luby. Approximating probabilistic inference in Bayesian belief networks is NP-hard. Artificial Intelligence, 60(1):141 ­ 153, 1993.",null,null
291,"P. A. Darwiche. Modeling and Reasoning with Bayesian Networks. Cambridge University Press, New York, NY, USA, 1st edition, 2009.",null,null
292,"R. Fung and B. Del Favero. Applying Bayesian networks to information retrieval. Commun. ACM, 38(3):42­ff., Mar. 1995.",null,null
293,"L. Getoor and B. Taskar. Introduction to Statistical Relational Learning (Adaptive Computation and Machine Learning). The MIT Press, 2007.",null,null
294,"K. Kandasamy, J. G. Schneider, and B. P´oczos. Bayesian active learning for posterior estimation - IJCAI-15 distinguished paper. In Proc. of the 24th IJCAI, pages 3605­3611, 2015.",null,null
295,"D. Koller and N. Friedman. Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning. The MIT Press, 2009.",null,null
296,"C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.",null,null
297,"S. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Prentice Hall Press, Upper Saddle River, NJ, USA, 3rd edition, 2009.",null,null
298,"H. Turtle and W. B. Croft. Inference networks for document retrieval. In Proc. of the 13th ACM SIGIR, pages 1­24, New York, NY, USA, 1990.",null,null
299,"H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187­222, July 1991.",null,null
300,"J. Zhao, J. X. Huang, and B. He. CRTER: Using Cross Terms to Enhance Probabilistic IR. In Proc. of the 34th ACM SIGIR, pages 155­164, 2011.",null,null
301,"J. Zhao, J. X. Huang, and Z. Ye. Modeling Term Associations for Probabilistic Information Retrieval. ACM Trans. Inf. Syst., 32(2):1­47, 2014.",null,null
302,2http://www.yorku.ca/adersim 3http://brainalliance.ca,null,null
303,964,null,null
304,,null,null

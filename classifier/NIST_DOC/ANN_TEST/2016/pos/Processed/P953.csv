,sentence,label,data
0,Quit While Ahead: Evaluating Truncated Rankings,null,null
1,"Fei Liu, Alistair Moffat, Timothy Baldwin",null,null
2,"The University of Melbourne Melbourne, Australia",null,null
3,"fliu3@student.unimelb.edu.au, ammoffat@unimelb.edu.au, tb@ldwin.net",null,null
4,Xiuzhen Zhang,null,null
5,"RMIT University Melbourne, Australia",null,null
6,xiuzhen.zhang@rmit.edu.au,null,null
7,ABSTRACT,null,null
8,"Many types of search tasks are answered through the computation of a ranked list of suggested answers. We re-examine the usual assumption that answer lists should be as long as possible, and suggest that when the number of matching items is potentially small ­ perhaps even zero ­ it may be more helpful to ""quit while ahead"", that is, to truncate the answer ranking earlier rather than later. To capture this effect, metrics are required which are attuned to the length of the ranking, and can handle cases in which there are no relevant documents. In this work we explore a generalized approach for representing truncated result sets, and propose modifications to a number of popular evaluation metrics.",null,null
9,1. INTRODUCTION AND BACKGROUND,null,null
10,"Ranked answer lists are a staple of search; and mechanisms for generating and evaluating them are widely known [1]. In most experimentation, ranked lists are taken to be of arbitrary length, that is, potentially spanning every item in the underlying collection; or to be of some fixed but large length, perhaps to depth d ,"" 1,000. But there are also situations in which there is only a small number of relevant answers (""""find the home page of . . . """") or no relevant answers to date (""""how do I get LATEX to . . . """"), for which generating a long list of unhelpful results is counter-productive. When confronted with such questions, an effective retrieval system might truncate its ranking after just a few suggestions, or even offer no answers at all, choosing to """"quit while ahead""""; assuming, of course, that the user understands the message being conveyed when a truncated ranking is generated by a system. Here we consider how to compute an effectiveness score for rankings that are of variable ­ and possibly zero ­ length, based on which we propose modifications to a range of popular evaluation metrics.""",null,null
11,"Effectiveness Metrics for Extended Rankings A large number of effectiveness metrics for ranked lists have been described, covering both binary relevance (the gain ri associated with position i in the ranking is either zero or one), and graded relevance (ri may take on arbitrary non-negative values). These include precision-focused metrics such as Precision@k and Reciprocal Rank (RR), which is",null,null
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17­21, 2016, Pisa, Italy",null,null
13,c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,null,null
14,DOI: http://dx.doi.org/10.1145/2911451.2914737,null,null
15,the precision at the first relevant document in the ranking. Other,null,null
16,"metrics add a recall component, such as Average Precision (AP).",null,null
17,Järvelin and Kekäläinen [2] describe a top-weighted evaluation,null,null
18,metric they call discounted cumulative gain (DCG). A key de-,null,null
19,velopment in this metric is that items near the top of the ranking,null,null
20,are explicitly given a greater influence on the final score than are,null,null
21,items later in the ranking. The formulation usually used is given,null,null
22,"by DCG@d ,",null,null
23,"d i,1",null,null
24,(ri/,null,null
25,log2,null,null
26,(1,null,null
27,+,null,null
28,"i)),",null,null
29,where,null,null
30,d,null,null
31,is,null,null
32,the,null,null
33,chosen,null,null
34,eval-,null,null
35,uation depth. An issue with DCG is that the values generated are,null,null
36,"unbounded; to address this, Järvelin and Kekäläinen also introduce",null,null
37,"a normalized version (NDCG), defined as the DCG score at that",null,null
38,depth divided by the DCG of a permuted ideal ranking in which,null,null
39,all relevant documents are returned at the head of the answer list:,null,null
40,"NDCG@d , DCG@d / DCGI @d. An NDCG@d score of 1.0",null,null
41,"indicates that, down to depth d, the ranking is as good as would",null,null
42,"have been attained by an omniscient system. Note, however, that",null,null
43,the DCG score of a ranking in which there are no relevant answers,null,null
44,is zero; and hence that NDCG is undefined on nil-answer queries.,null,null
45,"Other recall-based metrics, including Average Precision and the Q-",null,null
46,"measure [5], face the same challenge.",null,null
47,Moffat and Zobel [3] proposed an alternative top-weighted ap-,null,null
48,proach that avoids the need for the normalizing step. Their Rank-,null,null
49,"Biased Precision (RBP) metric is based on a simple user model,",null,null
50,"assuming that the user always looks at the first returned document,",null,null
51,and then continues from one depth i in the ranking to the next,null,null
52,"depth i + 1 with a fixed probability p, their persistence. The ex-",null,null
53,pected per-document rate at which gain is accrued is then given by,null,null
54,"RBP , (1 - p) ·",null,null
55," i,1",null,null
56,ri,null,null
57,· pi-1.,null,null
58,Rank-biased precision,null,null
59,assigns,null,null
60,"a score of zero to an empty ranking list, regardless of whether the",null,null
61,query that led to the ranking has answers or not.,null,null
62,"Effectiveness Metrics for Truncated Rankings Peñas and Rodrigo [4] note that in some question-answering (QA) scenarios, not responding is preferable to responding incorrectly, and propose a metric they denote c@1. Scores are based on having correct answers at the head of the ranked list, together with a component that is extrapolated for empty lists: c@1 ,"" nac/n + (nac/n) · (nu/n), where nac is the number of correctly answers across a set of n questions, and nu is the number of unanswered questions. However, c@1 is only applicable in cases where each question has a single correct answer, such as reading comprehension tests.""",null,null
63,"Another option for adding nil-answer assessment to an evaluation is to treat questions for which there are no answers differently from the has-answer queries. This may be appropriate if the distribution for the two classes of questions is imbalanced and nil-answer questions account for a small fraction of queries; the evaluation can then be one of correct classification between the two classes, followed by a standard evaluation within the has-answer class. For example, in the TREC 2001 QA track, there are 49 nil-answer ques-",null,null
64,953,null,null
65,"tions, out of 492 test questions. Similar statistics arise in the TREC 2002­2007 QA tracks. But note also that there are cases where nilanswer queries dominate. For example, in duplicate question detection for community question answering, the expectation is that most new questions will not have previously been asked.",null,null
66,"Sakai [5] proposed that NIL be regarded as a valid answer list of length one with positive gain, and showed that under this interpretation the Q-Measure (and other recall-based approaches) can be used to evaluate nil-answer questions. A similar approach was also used in the 2001 TREC QA track [6], where systems were permitted to return NIL in their answer lists. Any NIL's that appeared were assigned a gain of ri ,"" 1.0 if and only if there were no """"actual"""" answers to that query, and a gain of ri "","" 0 otherwise. Systems were free to continue listing documents after the NIL, meaning that a simple hedging strategy is to prefix NIL to every returned list; another, to insert NIL part way through every answer list. We explore the implications of this approach in more detail in Section 3.""",null,null
67,2. EVALUATING ARBITRARY RANKINGS,null,null
68,"All Rankings Are Different We propose that a system always be viewed as returning a ranking of documents, and that the length of that ranking always be regarded as having been determined by the system in response to the query. We then require that the evaluation process employed should be applicable to all rankings, including those of zero length.",null,null
69,"As a motivating example, consider the case of a query for which there are known to be R ,"" 3 relevant answers. For this query the five-document ranking (reading ri values from left to right, with """"1"""" representing relevant, and """"0"""" denoting non-relevant) """"10100"""" is almost certainly superior to the ranking """"01001"""", a relativity supported by all of RR, AP, NDCG, and RBP. Now consider the threeelement ranking """"101"""". It seems clear that """"101"""" must be regarded as superior (or, at the very least, not inferior) to """"10100"""", since it has the relevant documents in the same positions, and fewer nonrelevant documents. Next, consider the ranking """"011"""". Where does it fit in relation to the other three rankings? Most metrics would assess it as being inferior to """"101"""" and better than """"01001"""", but what about in comparison to """"10100""""? That is, is: """"101"""" > """"10100"""" > """"011"""" > """"01001"""" the preferred ordering from a user's point of view, where > is used as an abbreviation for numeric order, based on score? Or is: """"101"""" > """"011"""" > """"10100"""" > """"01001"""" the preferred relationship? And, what about the ranking """"1"""" ­ is one correct answer and no non-relevant answers better, or worse, than the rankings shown, all of which contain two correct answers? Finally, do any of these relativities change if instead of R "","" 3 relevant documents, there are known to be R "","" 5, or R "", 10?",null,null
70,"In the proposed new framework, in which ranking length is also regarded as being a factor that affects the score, dealing with nilanswer queries becomes a natural extension. If a query has no answers, then we would expect the evaluation metric to tell us that """" > ""0"" > ""00"" > ""000"" , and so on. Indeed, if a query has no answers, and a system returns a ranking containing no documents, would we not wish the score of that ranking to be 1.0, representing ""fully correct system response, and cannot be improved on""?",null,null
71,"Depth-Sensitive Evaluation To allow ranking length to influence assessed effectiveness, we modify every ranking to add a nominal terminal document at the first rank position after the last one supplied by the retrieval system. For example the ranking ""011"" is extended to make a new ranking ""011t"", where ""t"" represents the terminal document, and reflects that the system declined to provide an answer document in that or any subsequent position. Provided",null,null
72,"that a corresponding gain value rt is also assumed, any weightedprecision effectiveness metric, such as RR, Precision@k, or RBP,",null,null
73,can then be used to score the ranking.,null,null
74,The key to making this approach work is selecting a value for,null,null
75,"rt, the gain value associated with the terminal document. In the",null,null
76,"2001 TREC QA Track, and in the example presented by Sakai [5],",null,null
77,"rt ,"" 1.0 iff the question is a nil-answer one, and rt "", 0.0 if not.",null,null
78,We propose a more gradual approach. Suppose that the total gain,null,null
79,pool for the query is R  0. Then at depth d  0 in any given,null,null
80,ranking the fraction of the available gain that has been accrued is,null,null
81,given by,null,null
82,"d i,1",null,null
83,ri/R.,null,null
84,On,null,null
85,this,null,null
86,"basis,",null,null
87,we,null,null
88,define:,null,null
89,1,null,null
90,"if R , 0",null,null
91,"rt ,",null,null
92,"d i,1",null,null
93,ri,null,null
94,/R,null,null
95,if R > 0 .,null,null
96,(1),null,null
97,"To understand the implications of this definition, consider the metric RR, defined for binary gain values as the reciprocal of the first rank at which a relevant document appears. If a ranking of length d contains a relevant answer, then RR has the same value as it always does, since the terminal document at depth d + 1 has no bearing. If a ranking of length d does not contain a relevant answer, and if R > 0, then rt ,"" 0 and hence the value of RR is zero, as it should be ­ the system failed to return an answer that exists. But if R "","" 0, then rt "","" 1, and the value of RR is given by 1/(d + 1). That is, an empty ranking will be given a score of 1.0 if there are no relevant documents in the collection; the ranking """"0"""" will be given a score of 0.5 when R "","" 0, and so on. Overall, the adjusted RR computation that takes the terminal document into account smoothly adapts its score on nil-answer queries, as required; and has its previous behavior on has-answer queries.""",null,null
98,"In the case of RBP, rt is used in a slightly different way. Since RBP computes an infinite weighted sum over a geometric sequence of weights, it is appropriate to presume an arbitrary number of answers past the d th one, all with gain rt. That is, the finite truncated gain vector r1, r2, · · · , rd is treated as an infinite one, r1, r2, · · · , rd, rt, rt, rt, · · · , and the RBP score computed as normal. This has the same effect as taking the RBP residual at depth d, which is given by pd, and multiplying it by rt. That is, we define the adjusted RBP as",null,null
99,d,null,null
100,"RBP , RBP @d , (1 - p) · ri · pi-1 + rt · pd . (2)",null,null
101,"i,1",null,null
102,"As a third example, consider NDCG. To adjust this metric to handle truncated lists, we add rt as a (d + 1) th gain value, as for RR, and then use the usual scoring approach to depth d + 1 rather than to depth d:",null,null
103,"NDCG , NDCG @d ,"" DCG@(d + 1) r1, r2, · · · , rd, rt . DCGI @(d + 1) (3)""",null,null
104,"Note that this approach also means that d is no longer a parameter of the metric and is instead the length of the ranking supplied by the system; note also that the ideal (d + 1)-element ranking used in the denominator includes an extra gain of 1.0 in the first zero-gain position only if there are fewer than d + 1 full- or part-gain answers for the query. For example, if R ,"" 3, and all gain values are binary, then the ranking """"101"""" leads to rt "","" 2/3, and is scored as:""",null,null
105,"NDCG , 1/ log 2 + 1/ log 4 + (2/3)/ log 5 ,"" 0.698 , 1/ log 2 + 1/ log 3 + 1/ log 4 + 1/ log 5""",null,null
106,"where the final term in the denominator arises because in an ideal ranking of d ,"" 3 documents, the corresponding ideal rt value placed in the fourth position of the ranking would be 1.0.""",null,null
107,954,null,null
108,Ranking R,null,null
109,"00 ""000""",null,null
110,"R,0 R,0",null,null
111,"111 ""11"" ""11100"" ""101"" ""1"" ""10100"" ""011"" ""01001""",null,null
112,"R,3 R,3 R,3 R,3 R,3 R,3 R,3 R,3",null,null
113,rt,null,null
114,1.000 1.000,null,null
115,1.000 0.667 1.000 0.667 0.333 0.667 0.667 0.667,null,null
116,RR,null,null
117,0.333 0.250,null,null
118,1.000 1.000 1.000 1.000 1.000 1.000 0.500 0.500,null,null
119,RBP,null,null
120,0.250 0.125,null,null
121,1.000 0.917 0.906 0.708 0.667 0.646 0.458 0.302,null,null
122,NDCG,null,null
123,0.500 0.431,null,null
124,1.000 0.922 0.971 0.698 0.742 0.678 0.554 0.490,null,null
125,AP,null,null
126,0.333 0.250,null,null
127,1.000 0.648 0.917 0.528 0.306 0.491 0.403 0.299,null,null
128,"Table 1: Example truncated answer rankings and their modified scores, for two different queries, one with R , 0 and one with R , 3. The parameter p ,"" 0.5 is assumed for the RBP computation. Within each group, the results are sorted by RBP , which (by chance, for these examples) also corresponds to RR -order.""",null,null
129,"Average precision (AP) is handled similarly, by defining rd+1 ,"" rt, and then scoring the resulting extended-by-one ranking:""",null,null
130,1 d+1,null,null
131,AP,null,null
132,", R+1",null,null
133,ri,null,null
134,"i j,1",null,null
135,rj,null,null
136,.,null,null
137,i,null,null
138,(4),null,null
139,"i,1",null,null
140,"As is also the case with NDCG , the reference ranking used by AP contains R instances of ri ,"" 1, followed by a nominal terminating document with a gain of 1.0, that is, R + 1 values in total.""",null,null
141,"Table 1 shows scores computed for a range of rankings using the modified versions of RR, RBP, NDCG, and AP. The different adjusted metrics place different emphases on the tradeoff between recall and precision. All of the metrics respect the strict pairwise orderings noted earlier, for example, that ""101""  ""10100""; but they vary in their response to other relativities, such as the question as to whether ""1"" is better or worse than ""101"". Note how the different metrics place different emphases on the rankings, resulting in variations in their score orderings.",null,null
142,3. EXPERIMENTS AND RESULTS,null,null
143,"Tasks and Test Collections To explore the ramifications of the proposed approach, we employ the runs submitted to the main task of the TREC 2001 QA track. Participants were invited to submit a ranked list of [doc-id, answer-str] pairs of length up to five for each question; and for questions deemed to have no answer, were permitted to return ""NIL"" rather than one of the pairs. Overall, 36 groups contributed a total of 67 submissions to the QA main task; 47 of them are available for download.1 The question set consists of 492 queries, 49 of which are nil-answer queries. The 443 hasanswer questions have on average 25.7 relevant answers each.",null,null
144,"Interpretation of Truncation To evaluate the proposed approach, we transform each individual run using the rules shown in Table 2, so that we accurately capture any evidence of deliberate truncation. The first two rules, covering cases where fewer than five results are provided, or where an explicit ""NIL"" is provided, are evidence of system-initiated truncation, and are processed as such in our comparison; in the third case we cannot infer truncation, and those runs are retained intact and scored in the original manner by the unmodified metrics throughout our experimentation.",null,null
145,1http://trec.nist.gov/results/trec10/qa_main_input.html,null,null
146,aNIL n,null,null
147,"i 5 -1 < 5 -1 , 5",null,null
148,modified run,null,null
149,"a1, · · · , ai-1, t a1, · · · , an, t a1, · · · , a5",null,null
150,"Table 2: Transformation of a run a1, · · · , aNIL, · · · , an , where aNIL is the rank of an explicit NIL document (either rank i  [1, n], or -1 indicating not present) to a new ranked list.",null,null
151,#,null,null
152,"20,000 15,000",null,null
153,Original Transformed,null,null
154,"10,000",null,null
155,"5,000",null,null
156,0,null,null
157,0,null,null
158,1,null,null
159,2,null,null
160,3,null,null
161,4,null,null
162,5,null,null
163,n-answer response,null,null
164,"Figure 1: Distribution of lengths of 23,124 query responses.",null,null
165,"Figure 1 shows the distribution before and after transformation of the 23,124 runs submitted for the 492 queries by the 47 participants. The number of five-answer lists is reduced from around 20k to 16k, generating a total of approximately 7k truncated answer lists post-transformation. The number of zero-answer lists is zero before the transformation, because even when a system believes a query is a nil-answer question, it must return a ""NIL"" to indicate so. This also accounts for the decline in the number of single-answer responses post-transformation.",null,null
166,"Results and Analysis We first compare the TREC QA systems against each other using the TREC methodology (that is, with NIL in runs given a gain of 1.0 iff a query is nil-answer and otherwise given a gain of 0.0, and with metrics then applied in their standard form), and using our proposed modified approach applied to the transformed version of each run. Four different effectiveness metrics were explored, with the goal of determining the extent to which systems are affected by the proposed alteration in methodology. Each run for each system was scored using the two different approaches, and then system averages computed. In all of these evaluations, a [doc-id, answer-str] pair is considered correct iff the answer-str contains an answer to the question and is supported by the document specified by the doc-id.",null,null
167,"Table 3 compares the system orderings generated by the four pairs of original/modified metrics using Kendall's , which computes a correlation coefficient between pairs of ordered lists over the same domain. Three evaluation metric pairs give rise to  scores greater than 0.9, indicating strong agreement between the system ordering induced by the original metric and the system ordering generated by its modified version. The strong agreement between RR and RR was expected, because scores are primarily derived from just one relevant document, and because only a minority of the runs had explicit NIL markers. The similarly strong agreement between NDCG and NDCG was more surprising. At the other end of the scale, the pair AP/AP has the lowest  among the four metrics, but they are still strongly correlated.",null,null
168,955,null,null
169,RR Score,null,null
170,0.6,null,null
171,0.4,null,null
172,B,null,null
173,A 0.2,null,null
174,0,null,null
175,0,null,null
176,0.2,null,null
177,0.4,null,null
178,0.6,null,null
179,RR,null,null
180,"Figure 2: Relationship between RR and RR scores for 47 systems, with each system's score the mean over 492 queries.",null,null
181,Metric Pair,null,null
182,RR/RR NDCG/NDCG RBP@0.5/RBP @0.5 AP/AP,null,null
183,Kendall's ,null,null
184,0.960 0.958 0.916 0.870,null,null
185,Table 3: Kendall's  correlation coefficient calculated from the system orderings generated by pairs of original and modified metrics.,null,null
186,"Figure 2 provides details of the relationship between the RR and RR scores for the set of systems. Overall, RR and RR are in high agreement in regard to both system ordering (Table 3) and in terms of the actual scores assigned. However, there are also inverted pairs, where a system is ranked higher by the original metric but has inferior score in the modified. For example, the system marked with ""A"" has a slightly higher score than does ""B"" for RR, but is ranked lower than ""B"" by RR because of ""B""s aggressive (and effective) truncation strategy.",null,null
187,"We also investigated the impact of truncation on performance of individual systems. The horizontal axis in Figure 3 (% truncation) is the fraction of answer lists of length less than five, including NIL, but excluding terminal documents. Both of the top two systems receive a boost in score when truncation is taken into consideration. In the [0.15, 0.3] score range, despite the aggressive truncation, there are systems that obtain little improvement, in part due to their placement of a NIL at the end of every run. In addition, much of the truncation is a consequence of the system's inability to find a correct answer, rather than intentionally terminating the answer list. In such cases, even though there is explicit truncation, the system is not rewarded as there is no relevant document in the truncated answer list. Some systems sometimes prematurely truncate an answer list by placing a NIL before relevant documents. This causes the performance to drop when the modified metrics are employed. Two of the systems generated a NIL in the fifth position of all of their answer lists.",null,null
188,4. CONCLUSIONS AND FUTURE WORK,null,null
189,"We have identified an opportunity to refine the way in which truncated rankings are evaluated, and at the same time deal seam-",null,null
190,0.5,null,null
191,RBP,null,null
192,0.4,null,null
193,RBP,null,null
194,0.3,null,null
195,0.2,null,null
196,0.1 0,null,null
197,20 40 60 80 100 % Truncation,null,null
198,"Figure 3: Impact of proposed methodology on effectiveness scores of the top 20 systems. Percentage truncation (horizontal axis) is the fraction of truncated answers (length of answer list < 5, excluding the terminal document), with the two points marking pre- and post transformation scores. The RBP parameter is 0.5 throughout.",null,null
199,"lessly with a well-known shortcoming of recall-based evaluation metrics, namely, their inability to cope with queries with no relevant documents. By providing modified effectiveness approaches that provide subtle differentiation between runs of different lengths (for example, because ""110"" < ""11"" in our mechanism, but not in previous approaches to the problem) we are better able to nuance system evaluations. The approach we employ ­ the appending of a terminal document to every ranking, to indicate the truncation point, and modifications to a range of standard evaluation metrics including RR, RBP, NDCG and AP ­ is both intuitive, and also easy to implement and apply. In retrieval experiments over a large QA dataset, containing a non-trivial fraction of nil-answer queries, we illustrated the effectiveness of the modified metrics, and demonstrated that a refined evaluation of truncated document rankings can help differentiate system orderings.",null,null
200,"The obvious next step in our project is the development of methods for taking long document rankings and identifying, relative to the truncation-sensitive metrics, the point in each at which truncation is appropriate. One possible way of approaching this problem would be through analysis of the distribution of document scores in the ranking, in both relative and absolute terms. Query analysis could also be performed to predict the R value for a given query, for incorporation into the truncation process. We leave this exploration to future work.",null,null
201,Acknowledgments The authors thank MACE Engineering Group for their early support of this work. The third author was supported by ARC grant FT120100658.,null,null
202,References,null,null
203,"[1] C. Buckley and E. M. Voorhees. Retrieval system evaluation. In E. M. Voorhees and D. K. Harman, editors, TREC: Experiment and Evaluation in Information Retrieval, chapter 3, pages 53­75. MIT Press, 2005.",null,null
204,"[2] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Sys., 20(4):422­446, 2002.",null,null
205,"[3] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Sys., 27(1):2:1­2:27, 2008.",null,null
206,"[4] A. Peñas and A. Rodrigo. A simple measure to assess non-response. In Proc. ACL/HLT, pages 1415­1424, 2011.",null,null
207,"[5] T. Sakai. New performance metrics based on multigrade relevance: Their application to question answering. In Proc. NTCIR, 2004.",null,null
208,"[6] E. M. Voorhees. Overview of the TREC 2001 question answering track. In Proc. TREC, pages 42­51, 2002.",null,null
209,956,null,null
210,,null,null

,sentence,label,data
0,Document Retrieval Using Entity-Based Language Models,null,null
1,Hadas Raviv,null,null
2,Oren Kurland,null,null
3,David Carmel,null,null
4,"Technion, Israel",null,null
5,"Technion, Israel",null,null
6,"Yahoo Research, Israel",null,null
7,hadasrv@tx.technion.ac.il kurland@ie.technion.ac.il david.carmel@ymail.com,null,null
8,ABSTRACT,null,null
9,"We address the ad hoc document retrieval task by devising novel types of entity-based language models. The models utilize information about single terms in the query and documents as well as term sequences marked as entities by some entity-linking tool. The key principle of the language models is accounting, simultaneously, for the uncertainty inherent in the entity-markup process and the balance between using entity-based and term-based information. Empirical evaluation demonstrates the merits of using the language models for retrieval. For example, the performance transcends that of a state-of-the-art term proximity method. We also show that the language models can be effectively used for clusterbased document retrieval and query expansion.",null,null
10,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
11,Keywords: document retrieval; entity-based language models,null,null
12,1. INTRODUCTION,null,null
13,"Most ad hoc document retrieval methods compare query and document representations. To address the potential vocabulary mismatch between a short query and documents relevant to the query, various semantic document-query similarity measures have been proposed [28].",null,null
14,"Specifically, there is a growing body of work on retrieval methods that utilize information about entities in a repository (e.g., Wikipedia or Freebase) which appear in queries and documents (e.g., [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]). Most of these methods expand the query with terms or entities related to those appearing (or marked) in it [46, 35, 39, 7, 13, 31, 45, 29]; other methods project queries and documents onto a latent or explicit entity space [14, 33, 44].",null,null
15,"In this paper we take a step back, and address a more fundamental challenge regarding the use of entity-based information for document retrieval. We study whether using surface level entity-based query and document representations can help to improve retrieval effectiveness. By ""surface level"" we refer to representations based only on terms",null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",null,null
17,"SIGIR '16, July 17-21, 2016, Pisa, Italy",null,null
18,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,null,null
19,DOI: http://dx.doi.org/10.1145/2911451.2911508,null,null
20,"in the text and markups of entities in it, along with raw corpus-based occurrence statistics. This is in contrast to expansion-based and projection-based representations that utilize also terms and entities related to those (marked) in the text and which often use auxiliary information about entities from the entity repository; e.g., textual descriptions of entities, entities' categories and inter-entity relations [46, 35, 39, 7, 13, 31, 45, 29, 33, 44]. Put in simpler words, the question we address is whether the markup of entities in a query and documents is, by itself, sufficient information for",null,null
21,improving retrieval effectiveness. The reason for addressing the question just posed is two,null,null
22,"fold. First, it will shed light on the effectiveness of using entities in their most basic capacity; that is, special tokens marked in queries and documents. Indeed, findings in past work on ad hoc retrieval regarding the merits of using surface level entity-based representations are inconclusive [16, 42, 47, 3, 14]. Second, such representations can be naturally used in existing retrieval approaches and tasks to improve performance; e.g., query expansion and cluster-based document retrieval as we show in this paper.",null,null
23,"There are various potential merits in using surface level entity-based representations. For example, these can help to cope with the vocabulary mismatch problem; e.g., the entity United States of America can have different expressions in the text, including, ""U.S."", ""USA"", ""United States"" and more. Furthermore, expressions of entities in the text are variable-length n-grams that bear semantic meaning. Thus, entities can be used for effective modeling of term proximity information which goes beyond using fixed-length n-grams.",null,null
24,"An important challenge in inducing entity-based representations is accounting for the uncertainty inherent in the entity-markup process (a.k.a. entity linking); that is, associating term sequences with entities in a repository. Specifically, a term sequence can potentially be associated with multiple entities; e.g., the term ""Lincoln"" can be associated with the U.S. president, the car, the 2012 movie, etc. The uncertainty in entity linking has significant impact on retrieval effectiveness as we show in this paper.",null,null
25,"We present novel types of entity-based language models which consider both single terms in the text as well as term sequences marked as entities by an existing entity-linking tool. These language models are induced from the query and documents in the corpus and serve for retrieval in the language modeling framework. The main novelty of these language models is accounting, simultaneously, for (i) the uncertainty in entity linking -- specifically, the confidence levels of entity markups; and, (ii) the balance between using",null,null
26,65,null,null
27,"term-based and entity-based information. We demonstrate the importance of accounting for the mutual effects of these two aspects. For example, we show that using high recall entity markup, which is quite noisy, can help to significantly improve retrieval effectiveness if the noise is ""balanced"" by sufficient utilization of term-based information.",null,null
28,"Empirical evaluation demonstrates the merits of using our entity-based language models for retrieval. The performance significantly transcends that of a state-of-the-art term proximity method: the sequential dependence model (SDM) [36, 19]. Integrating the language models with SDM yields further performance improvements. The language models are also effective for two additional retrieval paradigms: clusterbased document retrieval and query expansion.",null,null
29,2. RELATED WORK,null,null
30,"The work most related to ours is that on devising surface level entity-based document and query representations for document retrieval [21, 16, 42, 47, 3, 41, 14]. The findings about the merits of these representations have been inconclusive. The few cases where the representations were shown to be somewhat effective for retrieval were when entity markups were devised in extreme care and were of very high quality [47, 3, 14]. In contrast to this past work that focused on vector space models, we demonstrate the clear merits of using our entity-based language models for retrieval. Also, in contrast to previously proposed representations [21, 16, 42, 47, 3, 41, 14], our language models account simultaneously for the uncertainty in the entity-markup process, and the balance between using term-based and entity-based information. Consequently, a highly important aspect that further differentiates our approach from past work is the effective utilization of high recall, noisy, entity markups.",null,null
31,"There is work on query expansion using entity-based information [43, 34, 30, 40, 8, 10, 18, 46, 6, 20, 7, 35, 39, 13, 31, 29, 45] and on projecting queries and documents onto an entity space to compare them [14, 33, 44]. There are two fundamental differences between all this past work and ours which focuses on surface level entity-based query and document representations. First, in these past methods, queries and documents are represented by external terms and entities which they do not contain1. Our surface level representations do not utilize such expansions. Second, auxiliary information about entities from the entity repository (e.g., textual descriptions of entities and their interrelations) is utilized in this past work, but not in our representations2.",null,null
32,"We show that our entity-based language models can be used to create effective expanded query forms by ""plugging"" them into an existing query expansion method: the relevance model [26, 1]. The resultant approach, which simultaneously expands the query with both terms and entities, is conceptually reminiscent of some methods recently proposed by Dalton et al. [13]. In their work, queries are expanded, independently, using terms and entities. The retrieval scores at-",null,null
33,"1Xiong and Callan [44] found that representing queries using only entities marked in them is of merit for their learningto-rank approach. However, features describing the queryentities relations rely on auxiliary information from the entity repository that is not used by our methods. 2The entity-linking process could use auxiliary information from the entity repository. However, our proposed representations utilize the entity markups simply as tokens with confidence levels, and do not use auxiliary information.",null,null
34,tained by using multiple term-only and entity-only expanded query forms are fused using a learning-to-rank method [13]. We show that our language models can be used to further improve the effectiveness of such expansion-based approaches by improving the quality of the pseudo relevant document list used for query expansion.,null,null
35,We also demonstrate the merits of using our language models for cluster-based document retrieval. Using entitybased representations for this task is novel to this study.,null,null
36,"In some studies, concepts (entities) in verbose queries were automatically weighted [2, 22, 4, 5]. In contrast to our approach, weights (confidence levels) of entities in documents were not accounted for. We demonstrate the importance of accounting for the confidence level of entity markups in both queries and documents. Further tuning of entities' weights in our proposed language models, using some of these approaches [2, 22, 4, 5], is interesting future work.",null,null
37,"There are language models that integrate word phrases and named entities based on their association with predefined classes [27, 23]. In contrast to our language models, which are not based on such classes, these language models were not designed and used for document retrieval.",null,null
38,3. RETRIEVAL FRAMEWORK,null,null
39,In what follows we present ad hoc document retrieval methods that rank documents in a corpus D in response to query q. The methods utilize information about entities mentioned in the query and in documents.,null,null
40,"To mark entities in texts, we use some entity-linking tool that utilizes a repository (e.g., Wikipedia or Freebase) where entities have unique IDs. The entity-linking tool takes as input a text, query or document in our case, and marks variable length sequences of terms as potential entities in the repository. The entity markup of a term sequence is composed of entity ID and a confidence level in [0, 1]. The confidence level reflects the likelihood that the term sequence corresponds to the entity. The confidence level relies on the term sequence and its context; e.g., its neighboring terms or other term sequences marked as entities [15, 38]. Using high confidence level results in high precision entity markup while low confidence level results in high recall.",null,null
41,"We assume that each position in a given text can be part of at most a single term sequence that is marked as an entity; i.e., the entity markups do not overlap. A specific occurrence of a term sequence in a text cannot be marked with more than one entity. Yet, a term sequence can appear several times in a text with different entity markups as the markups depend on the context of the sequence. Details of the entity linking tools we use are provided in Section 4.1.",null,null
42,The retrieval methods we present in Section 3.2 use entitybased query and document language models. We now turn to define these language models.,null,null
43,3.1 Entity-based language models,null,null
44,"We define unigram entity-based language models over a token space T ; i.e., tokens are generated by the language model independently of each other. The token space,",null,null
45,"T d,ef V  E",null,null
46,(1),null,null
47,is composed of the set V of all terms in the corpus D and the set E of entities in the entity repository which were marked at least once in a document in D with any confidence level.,null,null
48,66,null,null
49,"The language models we devise rely on a definition of pseudo counts for tokens. Two definitions of pseudo counts will be presented in Sections 3.1.1 and 3.1.2. Let pc(t, x) be the pseudo count of token t ( T ) in the text or text collection x. We define the pseudo length of x as:",null,null
50,"pl(x) d,ef",null,null
51,"X pc(t, x).",null,null
52,"tT :pc(t,x)>0",null,null
53,The maximum likelihood estimate (MLE) of token t ( T ) with respect to x is:,null,null
54,xMLE (t),null,null
55,"d,ef",null,null
56,"pc(t, x) pl(x)",null,null
57,.,null,null
58,(2),null,null
59,The MLE can be smoothed using Dirichlet priors [49]:,null,null
60,"xDir(t) d,ef",null,null
61,"pc(t,",null,null
62,x) + µDMLE pl(x) + µ,null,null
63,(t),null,null
64,;,null,null
65,(3),null,null
66,µ is a smoothing parameter. We next describe two types of language models defined,null,null
67,over T and induced using Equations 2 and 3. The language models differ by the definition of pseudo counts for tokens.,null,null
68,3.1.1 Hard confidence-level thresholding,null,null
69,"The hard confidence-level thresholding language model, HTLM in short, is based on fixing a threshold  ( [0, 1]) for entity markups. Entity-based information is used only for entity markups with confidence level   . In contrast, every term occurrence in a text, including those in entity markups with a confidence level <  , is accounted for.",null,null
70,"To formally define a HTLM using Equations 2 and 3, we have to define pseudo counts for tokens from T in a text or text collection x. To that end, we lay down a few definitions. If t ( T ) is a term, then cterm(t, x) is the number of occurrences of t in x. Let M(x) denote the set of all entity markups in x; i.e., all occurrences of term sequences in x that were marked as entities with some confidence level. For a markup m ( M(x)), E(m) is the entity and (m) is the confidence level. The equivalence relation t1  t2 holds iff the entity tokens t1 and t2 are identical (i.e., have the same ID). The pseudo count of t ( T ) in x is based on (i) the raw count of t in x if t is a term; and, (ii) the number of entity markups of t in x with a confidence level   if t is an entity. Formally,",null,null
71,"pcHT LM; (t, x) d,ef",null,null
72,"( cterm(t, x)",null,null
73,(1,null,null
74,-,null,null
75,),null,null
76,P,null,null
77,mM(x):E(m)t,null,null
78,[(m),null,null
79,],null,null
80,(4),null,null
81,if t  V; if t  E;,null,null
82," ( [0, 1]) is a free parameter which controls the relative importance attributed to term and entity tokens;  is Kronecker's delta function: for statement s, [s] , 1 if s is true and [s] , 0 otherwise.",null,null
83,"We note that using a Dirichlet smoothed HTLM (i.e., using Equation 4 in Equation 3) can still result in assigning zero probability to some tokens in T . These are entities with no corresponding markup of a term sequence in the corpus with confidence level   . We re-visit this point below.",null,null
84,"If we set  ,"" 1 in Equation 4, then the resultant HTLM reduces to a standard unigram term-based language model. Setting  "","" 0 results in HTEntLM which is a unigram language model that assigns non-zero probability only to entities: if the MLE from Equation 2 is used, then these are""",null,null
85,"the entities with at least one markup in x with a confidence level   ; if the Dirichlet smoothed language model is used (Equation 3), then these are the entities with at least one markup in the corpus with a confidence level   .",null,null
86,3.1.2 Soft confidence-level thresholding,null,null
87,"A potential drawback of HTLM is committing to a specific threshold  for entity markups. That is, information about entity markups with confidence level lower than  is ignored. Furthermore, all entity markups with confidence level   are counted equally as their confidence levels are ignored.",null,null
88,"Thus, we now turn to present a soft confidence-level thresholding language model, STLM. STLM accounts for all markups of an entity and weighs them by the corresponding confidence levels. Specifically, the pseudo count of t ( T ) in the text or text collection x is defined as:",null,null
89,(,null,null
90,"pcST LM (t, x) d,ef",null,null
91,"cterm(t, x)",null,null
92,(1,null,null
93,-,null,null
94,),null,null
95,P,null,null
96,mM(x):E(m)t,null,null
97,(m),null,null
98,if t  V; if t  E;,null,null
99,(5),null,null
100," ( [0, 1]) is a free parameter that, as in HTLM, controls",null,null
101,the relative importance attributed to term and entity to-,null,null
102,"kens. Thus, STLM addresses the uncertainty inherent in",null,null
103,the entity linking process by using expected entity occur-,null,null
104,rence counts; the corresponding confidence levels serve for,null,null
105,occurrence probabilities. These expected counts are then,null,null
106,integrated with deterministic term counts.,null,null
107,"If we set  ,"" 1 in Equation 5, then STLM reduces to""",null,null
108,a standard unigram term-based language model as was the,null,null
109,"case for HTLM. Setting  , 0 results in STEntLM. This",null,null
110,language model assigns a non-zero probability only to en-,null,null
111,tities that have at least one markup (with any confidence,null,null
112,level) in x when using the MLE (Equation 2) or in the corpus,null,null
113,when using the Dirichlet smoothed language model (Equa-,null,null
114,"tion 3). We note that in contrast to the case for HTLM,",null,null
115,there is no token in T that is assigned a zero probability by,null,null
116,a Dirichlet smoothed STLM.,null,null
117,3.2 Retrieval models,null,null
118,We rank document d by the cross entropy between the language models induced from the query (q) and d [25]:,null,null
119,X,null,null
120,"CE (q || d) , - q(t) log d(t);",null,null
121,(6),null,null
122,tT,null,null
123,higher values correspond to decreased similarity. Equation 6 is instantiated using the entity-based language,null,null
124,"models described in Section 3.1. Following common practice [48], we use an unsmoothed maximum likelihood estimate for the query language model (Equation 2) and a Dirichlet smoothed document language model (Equation 3). We obtain four retrieval methods : HT3, HTOEnt, ST and STOEnt4, which utilize the HTLM, HTEntLM, STLM and",null,null
125,"3In HT, the same confidence-level threshold, d, is used for all documents; the query threshold, q, can be different from d. Hence, an entity token assigned a non-zero probability by q could be assigned a zero probability by d; e.g., an entity marked in q with a confidence level  q but with no markup in the corpus with confidence level  d. In these cases, we zero the probability assigned to the entity token by q to avoid a log 0 in Equation 6. This is common practice in addressing term tokens that appear in a query but not in any document in the corpus. 4HTOEnt and STOEnt rely only on entity tokens. If all entities in E are assigned a zero probability by the unsmoothed",null,null
126,67,null,null
127,Table 1: TREC data used for experiments.,null,null
128,corpus,null,null
129,# of docs,null,null
130,data,null,null
131,queries,null,null
132,AP,null,null
133,ROBUST,null,null
134,WT10G GOV2 ClueB ClueBF,null,null
135,"242, 918 528, 155 1, 692, 096 25, 205, 179 50, 220, 423",null,null
136,Disks 1-3 Disks 4-5 (-CR),null,null
137,WT10g GOV2 ClueWeb09 (Cat. B),null,null
138,"51 - 150 301 - 450, 601 - 700 451 - 550 701 - 850",null,null
139,1 - 200,null,null
140,"STEntLM language models, respectively. HT and ST utilize entity and term tokens, while HTOEnt and STOEnt utilize only entity tokens, hence the ""O"" in the methods names.",null,null
141,3.2.1 Score-based fusion,null,null
142,"The HTLM and STLM language models integrate termbased and entity-based information at the language model level. Hence, the query-document comparison in Equation 6 simultaneously accounts for the appearance of the query terms and entities in a document.",null,null
143,"An alternative approach is integrating term and entity information at the retrieval score level. Inspired by approaches in the vector-space model [42], and in work on using a latent entity space [33], we consider a method that fuses document retrieval scores produced by utilizing, independently, termonly (xterm) and entity-only (xent) language models induced from text x. Document d is scored by:",null,null
144,CE `qterm || dterm´ + (1 - )CE `qent || dent´ ; (7),null,null
145,the  parameter balances the score fusion5. The query language models are unsmoothed maximum likelihood estimates (Equation 2) and the document language models are Dirichlet smoothed (Equation 3).,null,null
146,"Instantiating Equation 7 with an entity-only language model, HTEntLM or STEntLM, and with a standard unigram termbased language model yields the F-HT and F-ST methods, respectively. These are conceptually highly similar to the HT and ST methods which integrate term-based and entitybased information at the language-model level. However, HT and ST use a single smoothing parameter for both term and entity tokens (see Equation 3) while F-HT and F-ST can use a different smoothing parameter for each as they utilize separately term-only and entity-only language models. We could have used different smoothing parameters for entity and term tokens under the same language model, e.g., by applying term-specific smoothing [17], but we leave this exploration for future work.",null,null
147,4. EVALUATION,null,null
148,4.1 Experimental setup,null,null
149,"Experiments were conducted using the TREC datasets specified in Table 1. AP and ROBUST are mostly composed of news articles. WT10G is a small, noisy, Web collection. GOV2 is a much larger Web collection composed of high quality pages crawled from the .gov domain. ClueB is the",null,null
150,"query language model, then no documents are retrieved. This can happen for example when inducing HTEntLM from the query with a high confidence-level threshold or inducing a STEntLM from a query which has no entity markups. 5The  in the score-based fusion model has a conceptually similar role to that of  in STLM and HTLM: balancing the use of term-based and entity-based information.",null,null
151,English part of the Category B of the ClueWeb 2009 Web collection. ClueBF was created from ClueB by filtering from rankings suspected spam documents: those assigned a score below 50 by Waterloo's spam classifier [11].,null,null
152,Data processing. Titles of TREC topics served for queries.,null,null
153,Tokenization and Porter stemming were applied using the Lucene toolkit (lucene.apache.org) which was used for experiments. Stopwords on the INQUERY list were removed from queries but not from documents.,null,null
154,"Unless otherwise specified, the TagMe entity-linking tool (tagme.di.unipi.it) is used to annotate queries and documents. TagMe uses Wikipedia (a July 2014 dump) as the entity repository, and was shown to be highly effective and efficient in comparison to other publicly available entity-linking systems [12]. In Section 4.2.1 we also show the effectiveness of our methods using the Wikifier entity-linking tool6 [9, 12]. Wikifier was applied with an efficient configuration claimed to yield baseline entity linking effectiveness.",null,null
155,"TagMe and Wikifier cannot process very long texts. Thus, we split documents into non-overlapping term-window passages. We terminate a passage at the first space that appears at least 500 characters after the beginning of the previous passage. We let the tools mark the passages independently. The tools are applied on the non-stemmed and non-stopped queries and documents. Entity markup of a term sequence includes an entity ID and a confidence level (in [0, 1]). We scan each text left to right and remove overlapping entity markups so that each position can be part of at most a single markup. If two markups overlap, we select the one with the higher confidence level. We break ties of confidence levels by selecting the markup which starts at the leftmost position.",null,null
156,Baselines. We use standard term-based unigram language,null,null
157,"model retrieval [25], denoted TermsLM, for reference. This is a special case of the HT, ST, F-HT and F-ST methods with  , 1. Documents are ranked by the cross entropy between the unsmoothed (MLE) query language model and Dirichlet smoothed document language models.",null,null
158,"The HTCon method is a special case of HT with  , 0.5 and q , d ,"" 0 (q and d are the query and document thresholds, respectively). HTCon accounts uniformly for all entity mentions, and attributes the same importance to term and entity tokens. HTCon is conceptually reminiscent of methods representing documents and queries using concepts (e.g., from Wordnet) by concatenating with equal weights term-based and concept-based vector-space representations [41, 16, 42]. Accordingly, we consider F-HTCon: a special case of F-HT with  "", 0.5 and q , d , 0.",null,null
159,"Additional baseline is the state-of-the-art sequential dependence model, SDM, from the Markov Random Field framework which utilizes term proximities [36, 19]. The comparison with SDM, and its integration with our STLM is presented in Section 4.2.3.",null,null
160,Evaluation measures and free-parameters. Mean aver-,null,null
161,"age precision at cutoff 1000 (MAP), precision of the top 10 documents (p@10) and NDCG@10 (NDCG) serve as evaluation measures. Statistically significant performance differences are determined using the two-tailed paired t-test with a 95% confidence level.",null,null
162,6cogcomp.cs.illinois.edu/page/demo view/Wikifier,null,null
163,68,null,null
164,"Table 2: Comparison of methods instantiated from Equation 6 using term-only (TermsLM) and entitybased language models. Bold: the best result in a row. 't', 'h', 'o', 'c' and 's' mark statistically significant differences with TermsLM, HT, HTOEnt, HTCon and ST, respectively.",null,null
165,TermsLM HT HTOEnt HTCon ST STOEnt,null,null
166,MAP,null,null
167,AP,null,null
168,p@10,null,null
169,NDCG,null,null
170,MAP ROBUST p@10,null,null
171,NDCG,null,null
172,MAP WT10G p@10,null,null
173,NDCG,null,null
174,GOV2,null,null
175,MAP p@10,null,null
176,NDCG,null,null
177,ClueB,null,null
178,MAP p@10 NDCG,null,null
179,ClueBF,null,null
180,MAP p@10,null,null
181,NDCG,null,null
182,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8 17.1 22.7 16.5 18.8 33.6 24.3,null,null
183,23.1t 44 2t,null,null
184,. 45.3t 28 1t,null,null
185,. 45 5t,null,null
186,. 47 1t,null,null
187,.,null,null
188,21.9t 30.4t,null,null
189,32.7 32.1t 57.3t 47.4t 18.7t 25.9t 18.7t 20 5t,null,null
190,. 37.9t 28 4t,null,null
191,.,null,null
192,"15.6t,h 36.0h 37.6h 19.1t,h 35.7t,h 36.9t,h 13.3t,h 21.6t,h 21.2t,h 18.0t,h 39.4t,h 32.7t,h 14.0t,h",null,null
193,"23.9 18.3 14.4t,h 29.2h 22.2h",null,null
194,22.5o 43.4to 44.7to 27.4to 45.0to 46.3to 21.4to 30.5to 32.1o 30.6ho 56.8o,null,null
195,46.9o,null,null
196,18.5o 26.7to 19.2t,null,null
197,19.9o 38.2to 28.4to,null,null
198,"23.5to,c 17.5to,,hc,s",null,null
199,"43.8to 38.3hc,s",null,null
200,"45.5to 39.6hc,s",null,null
201,"28.1to,c 21.4to,,hc,s",null,null
202,"45.3to 38.0to,,hc,s",null,null
203,"46.9to 39.2to,,hc,s",null,null
204,"22.9to,,hc 16.7ho,c,s",null,null
205,"31.6to 25.3ho,c,s",null,null
206,"34.3to,c 25.4ho,c,s",null,null
207,"32.2to,c 20.7to,,hc,s",null,null
208,"57.7to 44.0to,,hc,s",null,null
209,"47.9to 35.7to,,hc,s",null,null
210,"19.5to 14.0tc,,hs",null,null
211,27 4t .,null,null
212,24.1,null,null
213,19 3t .,null,null
214,17.5,null,null
215,"20.3to 14.4tc,,hs",null,null
216,"37.9to 30.6hc,s",null,null
217,"27.5to 22.8hc,s",null,null
218,The free parameter values of all retrieval methods are set using 10-fold cross validation performed over the queries in a dataset. Query IDs are used to create the folds. The optimal parameter values for each of the 10 train sets are determined using a simple grid search applied to optimize MAP. The learned parameter values are then used for the queries in the corresponding test fold.,null,null
219,"The value of the Dirichlet smoothing parameter, µ, is selected from {100, 500, 1000, 1500, 2000, 2500, 3000}. The parameter , used in HTLM, STLM, F-HT and F-ST, is set to values in {0, 0.1, . . . , 1}. The document (d) and query (q) entity-markup confidence level thresholds, used in HT, HTOEnt and F-HT, are set to values in {0, 0.1, . . . , 0.9}.",null,null
220,4.2 Experimental results,null,null
221,4.2.1 Entity-based language models,null,null
222,"Table 2 presents the performance of the methods that use entity-based language models to instantiate Equation 6. Our first observation is that the HT and ST methods outperform the standard term-based language-model retrieval, TermsLM, in all relevant comparisons (6 corpora × 3 evaluation measures); most improvements are substantial and statistically significant. Furthermore, HT and ST outperform to a substantial and statistically significant degree their special cases which use only entity tokens: HTOEnt and STOEnt, respectively. These findings attest to the merits of using our proposed language models, HTLM and STLM, which integrate term-based and entity-based information.",null,null
223,We also see in Table 2 that HT and ST outperform HTCon in most relevant comparisons; most MAP improvements for ST are statistically significant. Recall from Section 4.1 that HTCon represents past practice of concept-based representations: accounting uniformly for all entity mentions and attributing equal importance to entity and term tokens. Below we further study the importance of accounting for the,null,null
224,MAP,null,null
225,MAP,null,null
226,MAP,null,null
227,24.0 23.0 22.0 21.0 20.0 19.0 18.0 17.0 16.0 15.0,null,null
228,0,null,null
229,24.0,null,null
230,AP,null,null
231,HT ST,null,null
232,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
233, WT10G,null,null
234,22.0,null,null
235,20.0,null,null
236,18.0,null,null
237,16.0,null,null
238,14.0 12.0,null,null
239,0,null,null
240,20.0,null,null
241,HT ST,null,null
242,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
243,ClueB,null,null
244,19.0,null,null
245,18.0,null,null
246,17.0,null,null
247,16.0,null,null
248,15.0 14.00,null,null
249,HT ST,null,null
250,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
251,MAP,null,null
252,MAP,null,null
253,MAP,null,null
254,30.0,null,null
255,ROBUST,null,null
256,28.0,null,null
257,26.0,null,null
258,24.0,null,null
259,22.0,null,null
260,20.0,null,null
261,18.0 0,null,null
262,34.0 32.0 30.0 28.0 26.0 24.0 22.0 20.0 18.0 16.0,null,null
263,0,null,null
264,22.0 21.0 20.0 19.0 18.0 17.0 16.0 15.0 14.00,null,null
265,HT ST,null,null
266,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
267, GOV2,null,null
268,HT ST,null,null
269,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
270, ClueBF,null,null
271,HT ST,null,null
272,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
273,"Figure 1: The effect of varying  on the MAP of HT and ST. For  ,"" 1, the methods amount to TermsLM (term-based language model retrieval). For  "","" 0, the methods use only entity tokens. The performance is reported for the test folds (i.e., all queries in a dataset) when fixing the value of  and using cross validation to set the values of all other free parameters. Note: figures are not to the same scale.""",null,null
274,"confidence level of entity markups, and attributing different weights to term and entity tokens as in HT and ST.",null,null
275,"Table 2 shows that ST outperforms HT in most relevant comparisons, although rarely to a statistically significant degree. In addition, ST posts more statistically significant improvements over HTCon than HT. We note that HT depends on four free parameters (, q, d and µ) while ST depends only on two ( and µ). Furthermore, the values learned for q and d in HT using the training folds are very low, attesting to the merits of using high recall entity markup. (We revisit this point below.) Overall, these findings attest to the potential merits of using a soft-thresholding approach for the confidence level of entity markups (STLM) with respect to a hard-thresholding approach (HTLM); i.e., accounting for all entity markups in a text and weighing their impact by their confidence levels is superior to accounting, uniformly, for entity markups with a confidence level above a threshold.",null,null
276,Terms vs. entities. Figure 1 depicts the MAP performance,null,null
277,"of HT and ST as a function of . Low and high values of  result in more importance attributed to entity-based and term-based information, respectively. For  ,"" 1, the two methods amount to TermsLM -- i.e., standard termbased language model retrieval. For  "","" 0, the methods use only entity-based information; specifically, HT reduces to HTOEnt and ST reduces to STOEnt.""",null,null
278,"We see in Figure 1 that optimal performance is always attained for   {0, 1}. This finding echoes those based on Table 2. That is, HT and ST outperform TermsLM,",null,null
279,69,null,null
280,MAP MAP,null,null
281,35.0 30.0,null,null
282,AP ROBUST,null,null
283,HT,null,null
284,WT10G GOV2,null,null
285,CLUEB09 CLUEB09F,null,null
286,35.0 30.0,null,null
287,AP ROBUST,null,null
288,HT,null,null
289,WT10G GOV2,null,null
290,CLUEB09 CLUEB09F,null,null
291,25.0,null,null
292,25.0,null,null
293,20.0,null,null
294,20.0,null,null
295,15.0 0,null,null
296,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
297,q,null,null
298,15.0 0,null,null
299,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
300,d,null,null
301,"Figure 2: The effect of varying q and d on the MAP performance of HT. The values of free parameters, except for that in the x-axis, are set using cross validation as in Figure 1.",null,null
302,"and HTOEnt and STOEnt, respectively. Thus, we find that there is much merit in integrating term-based and entitybased information for representing queries and documents.",null,null
303,"Figure 1 shows that the optimal value of  for HT is often higher than for ST. This can be attributed to the fact that HTLM, used to represent the query and documents in HT, uses a single confidence-level threshold for entity markups. Thus, potentially valuable information about entities is not utilized. As a result, HT calls for more reliance on termbased information to ""compensate"" for this potential information loss. In contrast, ST accounts for all entity markups, weighing their impact by their confidence levels. Hence, the ""risk"" in relying on entity-based information is lower7.",null,null
304,"To further explore the effect of using a hard threshold for the confidence level of entity markups in HT, we present in Figure 2 its MAP performance as a function of q and d -- the query and document thresholds, respectively. Recall that low threshold corresponds to high recall markup. Figure 2 shows that low values of q and d lead to improved performance. This finding can be attributed to the fact that increasing the confidence-level threshold amounts to loosing potentially valuable information about appearances of entities in the query and documents. To compensate for the lower precision (i.e., noisier) markup caused by using a low threshold, more weight is put on term-based information as is evident in the relatively high optimal values of  presented in Figure 1. Specifically, we note that the learned values of , d, and q, averaged over the train folds, for AP, ROBUST, WT10G, GOV2, ClueB and ClueBF are (0.6, 0.01, 0.11), (0.7,0.1,0.01), (0.55,0.1,0.2), (0.77,0.1,0.01), (0.7,0.15,0), and (0.81, 0.17, 0) respectively; namely, relatively high values of  and low values of d and q lead to improved performance.",null,null
305,Entity linking. Our main evaluation is based on using TagMe,null,null
306,"for entity linking. In Table 3 we compare the retrieval performance when using the entity markups of TagMe and Wikifier. Having Wikifier annotate large-scale collections is a challenging computational task. Thus, we present results only for AP, ROBUST and WT10G. We report MAP and NDCG; the performance patterns for p@10 are the same.",null,null
307,"Table 3 shows that using ST, our best performing method from above, with Wikifier, results in performance that transcends (often, significantly) that of the standard term-based language model (TermsLM) when using all queries in a dataset",null,null
308,"7Setting  on a per-query basis, in the spirit of work on fusing term-only-based and latent-entity-space-based retrieval scores [33], is a future direction we intend to explore.",null,null
309,"Table 3: Comparing entity-linking tools. Either all queries in a dataset are used (""All Queries""), or only those marked with at least one entity by both TagMe and Wikifier (""Marked Queries""). Bold: best result in a column in a block; 't', 's', 'w' and 'e': statistically significant differences with TermsLM, TagMeST, Wikifier-ST and TagMe-STOEnt, respectively.",null,null
310,AP,null,null
311,ROBUST,null,null
312,WT10G,null,null
313,MAP NDCG MAP NDCG MAP NDCG,null,null
314,All Queries,null,null
315,TermsLM 20.9 40.4,null,null
316,TagMe ST,null,null
317,23 5t 45 5t,null,null
318,.,null,null
319,.,null,null
320,Wikifier ST,null,null
321,23.3t 43.6,null,null
322,25.0 28 1t,null,null
323,. 27.2t,null,null
324,43.5 46 9t,null,null
325,. 45.6t,null,null
326,19.1 22 9t,null,null
327,". 19.7t,s",null,null
328,30.3 34 3t,null,null
329,. 30.9s,null,null
330,Marked Queries,null,null
331,TermsLM 22.2,null,null
332,TagMe ST,null,null
333,25 1t .,null,null
334,Wikifier ST,null,null
335,25 1t .,null,null
336,"TagMe STOEnt 18.5tw,s",null,null
337,"Wikifier STOEnt 17.5tw,s",null,null
338,41.7 48 4t,null,null
339,.,null,null
340,46.2t 41.4s 39.1sw,null,null
341,25.4 43.9 21.4 34.2,null,null
342,28 8t 47 3t 24 8t 36 2,null,null
343,.,null,null
344,.,null,null
345,.,null,null
346,.,null,null
347,28.0t 46.4t 21.9s 34.0,null,null
348,"22.9tw,s 41.1sw 18.1s 28.1s",null,null
349,"19.4tw,s,e 34.8tw,s,e 12.6tw,s,e 21.8tw,s",null,null
350,"(the ""All Queries"" block). However, the performance of using TagMe is consistently better.",null,null
351,"TagMe marks more queries with at least one entity than Wikifier: for AP, ROBUST and WT10G, Wikifier marked no entities in 17, 34 and 26 queries, respectively; TagMe did not mark entities in 0, 1 and 3 queries. (For GOV2 TagMe marked all queries with entities and for ClueB/ClueBF all queries except for one.) Recall that for queries with no marked entities, ST relies only on term-based information.",null,null
352,"To refine the comparison of TagMe and Wikifier, we report the performance of ST and STOEnt8 -- the latter relies only on entity tokens -- with these two tools over only queries in which both marked at least one entity. As can be seen in the ""Marked Queries"" block in Table 3, TagMe still outperforms Wikifier in almost all relevant comparisons; for STOEnt, several improvements are statistically significant.",null,null
353,"TagMe's superiority can be partially attributed to marking more entities (with confidence level > 0) on average than Wikifier: (2.4, 1.8, 2.0) with respect to (1.7, 1.2, 1.0) in queries over AP, ROBUST and WT10G; and, (157.2, 158.7, 207.0) with respect to (58.4, 50.5, 61.7) in documents.",null,null
354,"To conclude, our methods are effective with both TagMe and Wikifier. Using TagMe yields better performance that can be partially attributed to higher recall entity markup.",null,null
355,4.2.2 The score-based fusion methods,null,null
356,"Table 4 presents the performance of the F-HT and FST methods from Section 3.2.1 that perform score fusion of term-only-based and entity-only-based retrieval scores. The performance of TermsLM (term-only language model), HT and ST that integrate term and entity information at the language model level, and that of F-HTCon which is a special case of F-HT (see Section 4.1), is presented for reference. We see that F-HT and F-ST substantially outperform TermsLM. (F-ST posts the best performance in most relevant comparisons in Table 4.) Both methods also outperform F-HTCon in most relevant comparisons.",null,null
357,"8For queries for which a tool does not mark any entities, no documents are retrieved with STOEnt. Thus, we do not report the performance of STOEnt using all queries as the results are inherently biased in favor of TagMe which marks many more queries with entities than Wikifier.",null,null
358,70,null,null
359,"Table 4: Score-based fusion (""F-"" methods). Bold: best result in a row; 't', 'h', 's', 'f ' and 'c': statistically significant differences with TermsLM, HT, ST, F-HT and F-HTCon, respectively.",null,null
360,TermsLM HT ST F-HT F-HTCon F-ST,null,null
361,MAP,null,null
362,AP,null,null
363,p@10,null,null
364,NDCG,null,null
365,MAP ROBUST p@10,null,null
366,NDCG,null,null
367,MAP WT10G p@10,null,null
368,NDCG,null,null
369,GOV2,null,null
370,MAP p@10,null,null
371,NDCG,null,null
372,ClueB,null,null
373,MAP p@10,null,null
374,NDCG,null,null
375,ClueBF,null,null
376,MAP p@10,null,null
377,NDCG,null,null
378,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8,null,null
379,17.1,null,null
380,22.7 16.5 18.8 33.6 24.3,null,null
381,23.1t 23.5t 23.1t,null,null
382,44.2t 43.8t,null,null
383,44 5t .,null,null
384,45.3t 45.5t,null,null
385,46 2t .,null,null
386,28.1t 28.1t 28.1t 45.5t 45.3t 45.7t,null,null
387,47.1t 46.9t 47.3t,null,null
388,21.9t,null,null
389,"22 9t,h .",null,null
390,22.2t,null,null
391,30.4t 31.6t 30.0,null,null
392,32.7,null,null
393,34 3t .,null,null
394,32.7,null,null
395,32.1t,null,null
396,57.3t 47.4t,null,null
397,32.2t,null,null
398,57.7t 47.9t,null,null
399,"33.5ts,h 58 6t",null,null
400,. 48 7t,null,null
401,.,null,null
402,"18.7t 19.5t 19.6t,h",null,null
403,25.9t 27.4t 26.4t,null,null
404,18.7t 19.3t 19.1t,null,null
405,"20.5t 20.3t 21.3t,h",null,null
406,37.9t 37.9t,null,null
407,39 6t .,null,null
408,28.4t 27.5t 29.5ts,null,null
409,22.5s 43.5t 45.1t 27.7t 45.2t 46.6t 21.6ts 30.4t,null,null
410,"33.1 30.6hs,f",null,null
411,57.0,null,null
412,46.6 19.3t,null,null
413,27.5t 19.9t,null,null
414,19.7f,null,null
415,36.5f 27.6,null,null
416,"23 9t,h . f ,c",null,null
417,44.2t 45.8t,null,null
418,"28.4tc 46.7ts,c 47.8tc",null,null
419,22.9tc 31 8t,null,null
420,.,null,null
421,33.7t,null,null
422,"33.3ts,,hc 58.0t 48.2t",null,null
423,"20 8t,h . s,f ,c",null,null
424,"28 8t,h .f",null,null
425,"20 5t,h .f",null,null
426,"21.8ts,,hc 39.4tc 29.2ts",null,null
427,"Table 5: Comparison and integration with SDM [36]. Bold: the best result in a row. 't', 's', 'f ' and 'm' mark statistically significant differences with TermsLM, ST, F-ST and SDM, respectively.",null,null
428,AP ROBUST WT10G GOV2 ClueB ClueBF,null,null
429,TermsLM ST F-ST SDM SDM+STLM,null,null
430,MAP p@10 NDCG MAP p@10 NDCG MAP p@10 NDCG MAP p@10 NDCG,null,null
431,MAP p@10 NDCG MAP p@10 NDCG,null,null
432,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8,null,null
433,17.1 22.7 16.5 18.8 33.6 24.3,null,null
434,23.5t,null,null
435,43.8t 45.5t,null,null
436,23 9t .,null,null
437,44 2t .,null,null
438,45 8t .,null,null
439,21.6sf 40.6f 42.3f,null,null
440,28.1t,null,null
441,28 4t .,null,null
442,"25.7tf,s",null,null
443,45.3t,null,null
444,"46 7t,s .",null,null
445,43.9tf,null,null
446,46.9t,null,null
447,47 8t .,null,null
448,"44.8tf,s",null,null
449,22.9t 31.6t 34 3t,null,null
450,.,null,null
451,22.9t 31 8t,null,null
452,. 33.7t,null,null
453,20.2sf 27.7sf 30.7sf,null,null
454,"32.2t 33.3t,s 32.1t",null,null
455,57.7t 58.0t 58.3t,null,null
456,47.9t 48.2t 48.4t,null,null
457,19.5t 27.4t 19.3t,null,null
458,"20.8t,s 28.8t 20.5t",null,null
459,"18.2tf,s 23.8sf 16.9sf",null,null
460,"20.3t 21.8t,s 20.2tf",null,null
461,37.9t 39.4t 35.8tf,null,null
462,"27.5t 29.2t,s 25.9tf",null,null
463,23.9tm 44.2tm 45.8tm,null,null
464,"28.3tm 45.7tf,m 47.1tf,m 23.1tm 31.6tm 34.0tm 34 7t,s",null,null
465,". f ,m 61 4t,s",null,null
466,". f ,m 50 6t,s",null,null
467,". f ,m 21.5tm,s",null,null
468,"30 8t,s . f ,m",null,null
469,"21.9tm,s",null,null
470,"22 7t,s . f ,m",null,null
471,"42 8t,s . f ,m",null,null
472,"32 2t,s . f ,m",null,null
473,"In most relevant comparisons, F-HT outperforms HT and F-ST outperforms ST, although most performance differences are not statistically significant. The improvements can be attributed to the fact that F-HT and F-ST use a different smoothing parameter value for terms and entities while HT and ST use a joint one. (See Section 3.2.1 for details.)",null,null
474,"The potential effectiveness of using different smoothing parameters for term and entity tokens stems from the different number of terms and entity markups in a document. The average number of terms in a document for AP, ROBUST, WT10G, GOV2, and ClueB (ClueBF) is 455.4, 474.8, 588.2, 904.7 and 813.6, respectively. The average number of entity markups with a confidence level > 0 is much lower: 157.2, 158.7, 207.0, 291.9 and 307.8.",null,null
475,4.2.3 Comparison and integration with SDM,null,null
476,We next compare our entity-based approach with the sequential dependence model (SDM) [36] which scores d by:,null,null
477,"SSDM (d; q) d,""ef SSimS(d, q)+OSimO(d, q)+U SimU (d, q);""",null,null
478,"the sum of the S, O and U parameters is 1; SimS(d, q), SimO(d, q) and SimU (d, q) are cross-entropy based similarity estimates of the document to the query, utilizing information about occurrences of unigram, ordered bigrams, and unordered bigrams, respectively, of q's terms in d; un-ordered bigrams are confined to 8-terms windows in documents.",null,null
479,"Using entity tokens in our methods amounts to utilizing information about the occurrences of only some ordered variable-length n-grams of query terms in documents -- i.e., n-grams which constitute entities. Thus, in contrast to SDM, our methods do not utilize proximity information for query terms which are not in entity markups nor proximity information for unordered n-grams of query terms.",null,null
480,"In addition, we study the merit of integrating entity-based information, specifically, our soft-thresholding language model STLM, with SDM. To that end, we augment the SDM scoring function with an entity-based document-query similar-",null,null
481,"ity estimate, SimE(d, q). For this estimate, we use the score assigned to d by the STOEnt method; i.e., we use an entity-only language model since term-based information is accounted for in SimS(d, q). The resultant method, SDM+STLM, scores d by (S + O + U + E , 1):",null,null
482,"SSDM+ST LM (d; q) d,""ef S SimS (d, q) + OSimO(d, q)+""",null,null
483,"U SimU (d, q) + ESimE(d, q).",null,null
484,"SDM+STLM can be viewed as a novel instantiation of a weighted dependence model (WSDM) [4] with a novel concept type (i.e., entity). If O , U ,"" 0, SDM+STLM amounts to our F-ST method (see Section 3.2.1).""",null,null
485,"All free parameters of SDM and SDM+STLM: S, O, U , E and the Dirichlet smoothing parameter, µ, are set using cross validation as described in Section 4.1; S, O, U , and E are selected from {0, 0.1, . . . , 1} and µ is set to values in {100, 500, 1000, 1500, 2000, 2500, 3000}.",null,null
486,"Table 5 shows that ST and F-ST outperform SDM, often statistically significantly, in most relevant comparisons (6 corpora × 3 evaluation measures). This implies that using variable length n-grams which potentially bear semantic meaning (entities) can yield better performance than using ordered and unordered bigrams which do not necessarily have semantic meaning. Recall that in contrast to SDM, ST and F-ST do not account for proximities between terms which do not constitute entities and for unordered bigrams.",null,null
487,"In most relevant comparisons, SDM+STLM outperforms SDM and ST (which utilizes STLM) and is as effective as, and often posts statistically significant improvements over, F-ST -- its special case that fuses unigram term-only and entity-only retrieval scores. The few cases where F-ST outperforms SDM+STLM could be attributed to potential overfitting effects due to the high number of free parameters of SDM+STLM and the relatively low number of queries.",null,null
488,We also found that effective weights assigned to entityonly similarities in SDM+STLM (E) are much higher than those assigned to ordered (O) and un-ordered (U ) bigram,null,null
489,71,null,null
490,Table 6: Robustness analysis. Number of queries for which ST hurts (-) and improves (+) AP performance with respect to TermsLM and SDM.,null,null
491,AP ROBUST WT10G GOV2 ClueB ClueBF,null,null
492,-+- + - + - + - + - +,null,null
493,ST vs. TermsLM 38 61 75 173 31 63 50 99 54 137 75 112,null,null
494,ST vs. SDM,null,null
495,35 64 87 161 33 60 74 75 79 112 89 97,null,null
496,"term-based similarities. Furthermore, effective values of O and U are lower and higher, respectively, for SDM+STLM than for SDM. These findings further attest to the merits of using entity-based similarities with respect to (ordered and un-ordered) bigram similarities, and show that un-ordered bigram, in contrast to ordered bigram, similarities could be complementary to entity-based similarities.",null,null
497,4.2.4 Further analysis,null,null
498,"We now turn to further analyze merits, and shortcomings, of using entity-based query and document representations. To that end, we focus on the ST method that utilizes STLM.",null,null
499,"Table 6 presents performance robustness analysis: the number of queries for which ST improves or hurts average precision (AP) over TermsLM and SDM. In both cases, ST improves AP for more queries than it hurts; naturally, the differences with SDM are smaller than those with TermsLM.",null,null
500,"One advantage of STLM is that it represents the query and documents using entities which constitute variable length n-grams with semantic meaning. A case in point, query #41 in ClueWeb, ""orange county convention center"", refers to the primary public convention center for the Central Florida region. TermsLM, SDM and ST ranked the Web home page for this entity second. However, at the third rank in the lists retrieved by TermsLM and SDM appears a Wikipedia page titled ""list of convention and exhibition centers"", which is not specific to the entity of concern. The average precision (AP) of TermsLM, SDM and ST for the query in the ClueB dataset was 9, 13, and 30, respectively, attesting to the merit of the correct identification of the entity in the query and its utilization by ST.",null,null
501,"The ST method can suffer from incorrect entity identification in queries. For example, query #407 in ROBUST, ""poaching, wildlife preserves"", targets information about the impact of poaching on the world's various wildlife preserves. The entities identified by TagMe are ""poaching"", ""wildlife"" and ""preserves""; the latter refers to fruit preserves instead of nature preserves. Such erroneous entity identification can be attributed to the little context short queries provide. Consequently, the AP of ST for this query is only 8 while that of TermsLM and SDM is 31.4 and 30.0, respectively.",null,null
502,4.3 Using entity-based language models in additional retrieval paradigms,null,null
503,We next explore the effectiveness of using our entity-based language models in two additional retrieval paradigms: clusterbased document retrieval and query expansion.,null,null
504,4.3.1 Cluster-based document retrieval,null,null
505,"Let Dinit denote the list of top-n documents retrieved by TermsLM (standard language-model-based retrieval). Following common practice in work on cluster-based document retrieval [32, 24], we re-rank Dinit using information induced from nearest-neighbor clusters of documents in Dinit.",null,null
506,Table 7: Cluster-based document re-ranking. Bold:,null,null
507,"the best result in a row; 't', 's', '' and '' mark sta-",null,null
508,"tistically significant differences with TermsLM, ST,",null,null
509,"C-Term-Term and C-Term-Ent, respectively.",null,null
510,TermsLM ST C-Term-TermC-Term-EntC-Ent-Ent,null,null
511,AP,null,null
512,p@10 NDCG,null,null
513,ROBUST p@10 NDCG,null,null
514,WT10G,null,null
515,p@10 NDCG,null,null
516,GOV2,null,null
517,p@10 NDCG,null,null
518,ClueB,null,null
519,p@10 NDCG,null,null
520,ClueBF,null,null
521,p@10 NDCG,null,null
522,39.6 40.8 42.2,null,null
523,43.5,null,null
524,28.6 31.2 53.4 45.0 23.7 17.2 32.1 22.9,null,null
525,42.5 44.8t 44.3t,null,null
526,45.5t,null,null
527,30.6,null,null
528,33.4 57.0t,null,null
529,46.8 27.1t,null,null
530,19.1 36.9t 27.8t,null,null
531,43.2t 44.2t,null,null
532,43.1,null,null
533,44.2,null,null
534,30.2 32.1 55.1 45.8 23.7 17.2 31.2s 23.1s,null,null
535,44.3t,null,null
536,44.9,null,null
537,"46.0t 47.5t 3335.7.4tt,s 58 3t",null,null
538,. 48 9t,null,null
539,.,null,null
540,"33 0t,s 24.9t,s",null,null
541,. 3308.53tt,null,null
542,.,null,null
543,"46 5t,s .",null,null
544,46 8t .,null,null
545,"47 7t,s . ,",null,null
546,"49 1t,s . ,",null,null
547,"34 8t,s 36.3t,s",null,null
548,". 57.9t 47.8t 3212..59tt,,ss 39 0t 29..6t",null,null
549,"We use Sim(x, y) d,ef exp(-CE `xMLE || yDir´) to measure the similarity between texts x and y [24]; xMLE is an unsmoothed MLE induced from x and yDir is a Dirichlet smoothed language model induced from y. Each document",null,null
550,"d ( Dinit) and the k - 1 documents d (d ,"" d) in Dinit that yield the highest Sim(d, d) constitute a cluster.""",null,null
551,"We rank the (overlapping) clusters c, each contains k doc-",null,null
552,q,null,null
553,"uments, by:",null,null
554,k,null,null
555,Q,null,null
556,dc,null,null
557,S,null,null
558,"im(q,",null,null
559,d),null,null
560,[32].,null,null
561,This is a highly effective,null,null
562,simple cluster ranking method [24]. To induce document,null,null
563,"ranking, each cluster is replaced with its constituent docu-",null,null
564,ments omitting repeats; documents in a cluster are ordered,null,null
565,"by their query similarity: Sim(q, d).",null,null
566,The document (re-)ranking procedure just described re-,null,null
567,lies on the choice of the document language models used to,null,null
568,"induce clusters (i.e., in Sim(d, d)) and the choice of docu-",null,null
569,ment and query language models used to induce document-,null,null
570,"query similarities (Sim(q, d)); the latter are used for rank-",null,null
571,ing both clusters and documents within the clusters. We,null,null
572,use C-Term-Term to denote the standard method that,null,null
573,uses term-only language models for inducing clusters and,null,null
574,"document-query similarities [32, 24]. The C-Term-Ent",null,null
575,"method utilizes the same clusters used by C-Term-Term, but",null,null
576,"uses our entity-based language model, STLM, for inducing",null,null
577,document-query similarities to rank clusters and documents,null,null
578,"in them. In the C-Ent-Ent method, STLM is used to both",null,null
579,create clusters and induce document-query similarities. As a,null,null
580,"reference comparison, we re-rank Dinit using the ST method",null,null
581,that uses STLM but does not utilize clusters.,null,null
582,As the main goal of cluster-based re-ranking is improv-,null,null
583,"ing precision at top ranks [32, 24], we report p@10 and",null,null
584,NDCG@10 (NDCG). Free-parameter values are set using,null,null
585,cross validation; NDCG is the optimization criterion. Specif-,null,null
586,"ically, n is selected from {50, 100}; k is in {5, 10}; and, ",null,null
587,"(used in STLM) is in {0, 0.1, . . . , 1}; the Dirichlet smooth-",null,null
588,ing parameter is set to 1000. Table 7 presents the results.,null,null
589,"We see that all cluster-based methods (denoted ""C-X-Y"")",null,null
590,almost always outperform the initial term-based document,null,null
591,"ranking, TermsLM. C-Term-Ent substantially outperforms",null,null
592,C-Term-Term. This attests to the merits of using STLM,null,null
593,for inducing cluster ranking and within cluster document,null,null
594,"ranking. In most relevant comparisons, C-Ent-Ent outper-",null,null
595,forms (and is never statistically significantly outperformed,null,null
596,"by) C-Term-Ent, attesting to the potential merits of using",null,null
597,72,null,null
598,Table 8: Query expansion. Bold: the best result,null,null
599,"in a row. 't', 's', 'r', 'w', 'm' and 'n' mark sta-",null,null
600,"tistically significant differences with TermsLM, ST,",null,null
601,"RM3, WikiRM, SDM-RM and RMST, respectively.",null,null
602,TermsLM ST RM3 WikiRMSDM-RM RMST RMST-ST,null,null
603,MAP AP p@10,null,null
604,NDCG MAP,null,null
605,ROBU p@10,null,null
606,ST NDCG MAP,null,null
607,WT p@10 10G NDCG,null,null
608,MAP GOV2 p@10,null,null
609,NDCG MAP ClueB p@10 NDCG MAP Clue p@10 BF NDCG,null,null
610,20.9 39.1 40.4 25.0 42.2 43.5 19.1 27.3 30.3 29.6 53.9 44.8 17.1 22.7 16.5 18.8 33.6 24.3,null,null
611,23.5t 24.1t 24.0t,null,null
612,24.9t,null,null
613,"24.6t 27.4tw,s,m ,r,n",null,null
614,43.8t 42.5t 46.2t,null,null
615,43.9t,null,null
616,44.8t,null,null
617,"46 8t,r .",null,null
618,45.5t,null,null
619,43.2,null,null
620,"48 2t,r .",null,null
621,45.6t,null,null
622,45.0t,null,null
623,"47.4t,r",null,null
624,28.1t 28.3t 27.8t,null,null
625,28.4t,null,null
626,"29.0t 30.5tw,s,m ,r,n",null,null
627,45.3t 43.6 44.6t,null,null
628,"43.2 45.9tm,r 47.1tw,s,m ,r",null,null
629,"46.9t 43.8s 46.1t,r 43.6sw 46.5tm,r 47.2tm,r",null,null
630,22 9t .,null,null
631,19.6s,null,null
632,"21.9t,r",null,null
633,20.0s,null,null
634,"22.7tm,r",null,null
635,"22.8tm,r",null,null
636,31.6t 28.0s,null,null
637,"34 2t,r .",null,null
638,28.6w,null,null
639,"31.7tm,r",null,null
640,34 3t .,null,null
641,30.1s,null,null
642,"34 3t,r .",null,null
643,30.5sw,null,null
644,32.9,null,null
645,31.1t 31.8s,null,null
646,32.2t 32.4t 32.1t,null,null
647,33.7tw,null,null
648,33.1t,null,null
649,"33 7t,s .",null,null
650,57.7t 58.1t,null,null
651,60 1t .,null,null
652,58.0t,null,null
653,59.6t,null,null
654,58.5t,null,null
655,47.9t 48.0t,null,null
656,50 6t .,null,null
657,47.6,null,null
658,49.4t,null,null
659,48.8t,null,null
660,"19.5t 19.3t 21.9t,s,r 20.9t,r 20.7t,s,r 22.1tn,s,r",null,null
661,27.4t,null,null
662,30.6t,null,null
663,"35 3t,s,r .",null,null
664,"32.2tw,s",null,null
665,"32.2tw,s 34.9tn,s,r",null,null
666,"19.3t 22.6t,s 26.1t,s,r 24.3t,s 25.1t,s,r 27.1tn,s,r",null,null
667,"20.3t 20.4t 21.0t 21.8t,s,r 20.8t 21.9tn,s",null,null
668,37.9t 37.9t 38.5t,null,null
669,"39 7t,r .",null,null
670,38.2t,null,null
671,38.4t,null,null
672,27.5t 28.1t 28.2t,null,null
673,"29.8t,r",null,null
674,28.5t,null,null
675,"30 3t,s .",null,null
676,"entity-based information to also create clusters. However, only two improvements are statistically significant.",null,null
677,"Finally, Table 7 shows that in almost all relevant comparisons, ST outperforms TermsLM (often, statistically significantly) and C-Term-Term and is outperformed by C-TermEnt and C-Ent-Ent. This shows that while there is merit in using STLM for direct ranking of documents as shown in Section 4.2.1, the performance can be further improved by using STLM for cluster-based document ranking.",null,null
678,4.3.2 Query expansion,null,null
679,"As noted in Section 2, there is much work on expanding queries with terms and entities using entity-based information. In contrast, our entity-based language models, when induced from the query, utilize only query terms and entities marked in the query. Hence, we study the effectiveness of using our language models to perform query expansion.",null,null
680,We use the relevance model (RM3) [1] as a basis for instantiating expanded query forms. The probability assigned to token t by a relevance model RM is:,null,null
681,RM (t),null,null
682,"d,ef",null,null
683,qMLE (t),null,null
684,+,null,null
685,(1,null,null
686,-,null,null
687,),null,null
688,X,null,null
689,dL,null,null
690,dDir,null,null
691,(t),null,null
692,S(d; q),null,null
693,P,null,null
694,d L,null,null
695,S(d,null,null
696,;,null,null
697,q),null,null
698,;,null,null
699,(8),null,null
700, is a free parameter; L is a list of top-retrieved documents,null,null
701,used to construct RM ; S(d; q) is d's score. Due to computa-,null,null
702,"tional considerations, as in work on entity-based query ex-",null,null
703,"pansion [13, 45] we use RM to re-rank an initially retrieved",null,null
704,"document list; CE `RM || dDir´ serves for re-ranking. Using only terms as tokens, and applying standard language-",null,null
705,model-based retrieval (TermsLM) over the corpus to create,null,null
706,"L, yields the standard RM3 [1]. Creating L by applying",null,null
707,"TermsLM over Wikipedia results in WikiRM [46], an ex-",null,null
708,"ternal corpus expansion approach also used in [13, 45]. RM3",null,null
709,and WikiRM re-rank a document list retrieved by TermsLM.,null,null
710,(WikiRM is the only model where the list from which RM is,null,null
711,"constructed, L, is not a sub-set of the list to be re-ranked.)",null,null
712,"In both methods, S(d; q) d,""ef exp(-CE `qMLE || dDir´). The SDM-RM model [13] is constructed from, and used""",null,null
713,"to re-rank, lists retrieved by the sequential dependence model",null,null
714,"(SDM) [36]. dDir, and the resultant relevance model constructed by setting  ,"" 0 in Equation 8, are term-based unigram language models; S(d; q) is the exponent of the score assigned to d by SDM. Re-ranking is performed by linear interpolation of the SDM score assigned to d and CE `RM || dDir´, using a parameter . SDM-RM is, in fact, the highly effective Latent Concept Expansion method [37] without IDF-based weighting of expansion terms.""",null,null
715,"The next two relevance models, defined over T (the termentity token space from Equation 1), are novel to this study. They utilize our STLM language model which integrates terms and entities at the language model level. RMST is inspired by methods proposed by Dalton et al. [13]9 by the virtue of using both terms and entities for query expansion. qMLE and dDir are our STLM language models. S(d; q) d,ef exp(-CE `qMLE || dDir´). The TermsLM method is applied over the corpus to create the initial list to be re-ranked (cf. [45]) and from which L is derived.",null,null
716,"RMST-ST is constructed as RMST using STLM. The difference is that our entity-based ST method, rather than TermsLM, is used to create the initial list to be re-ranked and from which L is derived. The formal ease of using STLM in the relevance model (Equation 8), yielding RMST and RMST-ST, attests to the merits of using a single language model defined over terms and entities with respect to the alternative score-based fusion approach from Section 3.2.1.",null,null
717,"The free parameters of all methods are set using cross validation. The number of expansion terms (i.e., those assigned the highest probability by RM ), the number of documents in L, and  are set to values in {10, 30, 50, 100}, {50, 100} and {0, 0.1, . . . , 1}, respectively. (Only for WikiRM, the number of documents in L is selected from {1, 5, 10, 30, 50, 100} following [46].) All lists that are re-ranked contain 1000 documents. The values of the free parameters of ST and SDM are selected from the ranges specified in Section 4.1. The Dirichlet smoothing parameter, µ, is selected from {100, 500, 1000, 1500, 2000, 2500, 3000}; for relevance model construction (Equation 8) the value 0 is also used (yielding unsmoothed MLE). To reduce the number of free-parameter values configurations, we use the same value of µ for creating L, for re-ranking and for constructing the relevance model, unless 0 is used for relevance model construction.",null,null
718,"Table 8 presents the performance. Our ST method, which does not perform query expansion, is competitive with the term-based relevance model (RM3). We also see that RMST is an effective expansion method which often outperforms RM3 and SDM-RM. This finding echoes those from past work [13, 45] about the merits of using both terms and entities for query expansion. The best performing method in most relevant comparisons is RMST-ST which uses STLM to (i) create an effective initial list for re-ranking; (ii) create an effective list, L, for relevance model construction; and, (iii) induce ranking using the entity-based relevance model as in RMST. We conclude that our STLM language model can play different important roles in query expansion.",null,null
719,"Table 8 shows that expansion using Wikipedia as an external corpus (WikiRM) is effective. Our RMST and RMSTST expansion methods (as well as ST) utilize entity tokens marked by TagMe (i.e., Wikipedia concepts), but do no use",null,null
720,"9Various expansion methods, which utilize also auxiliary information about entities from the entity repository, were integrated in [13]. We do not use such auxiliary information.",null,null
721,73,null,null
722,"the text on their Wikipedia pages in contrast to WikiRM. Thus, integrating WikiRM with our methods, e.g., using score-based integration [13], is interesting future direction.",null,null
723,5. CONCLUSIONS,null,null
724,We presented novel entity-based language models induced using an entity linking tool. The models simultaneously account for the uncertainty in the entity-linking process and the balance between using term-based and entity-based information. We showed the merits of using the language models for document retrieval in several retrieval paradigms.,null,null
725,Acknowledgments. We thank the reviewers for their comments. This paper is based upon work supported in part by a Yahoo! faculty research and engagement award.,null,null
726,6. REFERENCES,null,null
727,"[1] N. Abdul-jaleel, J. Allan, W. B. Croft, O. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. Umass at TREC 2004: Novelty and hard. In Proc. of TREC-13, 2004.",null,null
728,"[2] J. Allan, J. P. Callan, W. B. Croft, L. Ballesteros, J. Broglio, J. Xu, and H. Shu. Inquery at TREC-5. In Proc. of TREC-5, pages 119­132, 1996.",null,null
729,"[3] A. R. Aronson, T. C. Rindflesch, and A. C. Browne. Exploiting a large thesaurus for information retrieval. In Proc. of RIAO, volume 94, pages 197­216, 1994.",null,null
730,"[4] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In Proc. of WSDM, pages 31­40, 2010.",null,null
731,"[5] M. Bendersky, D. Metzler, and W. B. Croft. Parameterized concept weighting in verbose queries. In Proc. of SIGIR, pages 605­614, 2011.",null,null
732,"[6] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proc. of WSDM, pages 443­452, 2012.",null,null
733,"[7] W. C. Brand~ao, R. L. T. Santos, N. Ziviani, E. S. de Moura, and A. S. da Silva. Learning to expand queries using entities. JASIST, 65(9):1870­1883, 2014.",null,null
734,"[8] G. Cao, J. Nie, and J. Bai. Integrating word relationships into language models. In Proc. of SIGIR, pages 298­305, 2005.",null,null
735,"[9] X. Cheng and D. Roth. Relational inference for wikification. In Proc. of EMNLP, pages 1787­1796, 2013.",null,null
736,"[10] K. Collins-Thompson and J. Callan. Query expansion using random walk models. In Proc. of CIKM, pages 704­711, 2005.",null,null
737,"[11] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.",null,null
738,"[12] M. Cornolti, P. Ferragina, and M. Ciaramita. A framework for benchmarking entity-annotation systems. In Proc. of WWW, pages 249­260, 2013.",null,null
739,"[13] J. Dalton, L. Dietz, and J. Allan. Entity query feature expansion using knowledge base links. In Proc. of SIGIR, pages 365­374, 2014.",null,null
740,"[14] O. Egozi, S. Markovitch, and E. Gabrilovich. Concept-based information retrieval using explicit semantic analysis. ACM Transactions on Information Systems (TOIS), 29(2):8, 2011.",null,null
741,"[15] P. Ferragina and U. Scaiella. Tagme: On-the-fly annotation of short text fragments (by Wikipedia entities). In Proc. of CIKM, pages 1625­1628, 2010.",null,null
742,"[16] W. R. Hersh, D. H. Hickam, and T. Leone. Words, concepts, or both: optimal indexing units for automated information retrieval. In Proc. of SCAMC, page 644, 1992.",null,null
743,"[17] D. Hiemstra. Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term. In Proc. of SIGIR, pages 35­41, 2002.",null,null
744,"[18] M. Hsu, M. Tsai, and H. Chen. Combining wordnet and conceptnet for automatic query expansion: A learning approach. In Proc. of AIRS, pages 213­224, 2008.",null,null
745,"[19] S. Huston and W. B. Croft. A comparison of retrieval models using term dependencies. In Proc. of CIKM, pages 111­120, 2014.",null,null
746,"[20] A. Kotov and C. Zhai. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries. In Proc. of WSDM, pages 403­412, 2012.",null,null
747,"[21] R. Krovetz and W. B. Croft. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems (TOIS), 10(2):115­141, 1992.",null,null
748,"[22] G. Kumaran and J. Allan. A case for shorter queries, and helping users create them. In Proc. of NAACL, pages 220­227, 2007.",null,null
749,"[23] H.-K. J. Kuo and W. Reichl. Phrase-based language models for speech recognition. In Proc. of EUROSPEECH, 1999.",null,null
750,"[24] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research (JAIR), 41:367­395, 2011.",null,null
751,"[25] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In Proc. of SIGIR, pages 111­119, 2001.",null,null
752,"[26] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.",null,null
753,"[27] M. Levit, S. Parthasarathy, S. Chang, A. Stolcke, and B. Dumoulin. Word-phrase-entity language models: getting more mileage out of n-grams. In Proc. of INTERSPEECH, pages 666­670, 2014.",null,null
754,"[28] H. Li and J. Xu. Semantic matching in search. Foundations and Trends in Information Retrieval, 7(5):343­469, 2014.",null,null
755,"[29] R. Li, L. Hao, P. Zhang, D. Song, and Y. Hou. A query expansion approach using entity distribution based on markov random fields. In Proc. of AIRS, 2015.",null,null
756,"[30] S. Liu, F. Liu, C. T. Yu, and W. Meng. An effective approach to document retrieval via utilizing wordnet and recognizing phrases. In Proc. of SIGIR, pages 266­272, 2004.",null,null
757,"[31] X. Liu, F. Chen, H. Fang, and M. Wang. Exploiting entity relationship for query expansion in enterprise search. Information Retrieval Journal, 17(3):265­294, 2014.",null,null
758,"[32] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proc. of ECIR, pages 454­462, 2008.",null,null
759,"[33] X. Liu and H. Fang. Latent entity space: a novel retrieval approach for entity-bearing queries. Information Retrieval Journal, 18(6):473­503, December 2015.",null,null
760,"[34] R. Mandala, T. Tokunaga, and H. Tanaka. Combining multiple evidence from different types of thesaurus for query expansion. In Proc. of SIGIR, pages 191­197, 1999.",null,null
761,"[35] E. Meij, D. Trieschnigg, M. de Rijke, and W. Kraaij. Conceptual language models for domain-specific retrieval. Information Processing & Management, 46(4):448­469, 2010.",null,null
762,"[36] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.",null,null
763,"[37] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In Proc. of SIGIR, pages 311­318, 2007.",null,null
764,"[38] D. Milne and I. H. Witten. Learning to link with Wikipedia. In Proc. of CIKM, pages 509­518, 2008.",null,null
765,"[39] D. Pan, P. Zhang, J. Li, D. Song, J. Wen, Y. Hou, B. Hu, Y. Jia, and A. N. D. Roeck. Using Dempster-Shafer's evidence theory for query expansion based on freebase knowledge. In Proc. of AIRS, pages 121­132, 2013.",null,null
766,"[40] C. Shah and W. B. Croft. Evaluating high accuracy retrieval techniques. In Proc. of SIGIR, pages 2­9, 2004.",null,null
767,"[41] P. Srinivasan. Query expansion and medline. Information Processing & Management, 32(4):431­443, 1996.",null,null
768,"[42] E. M. Voorhees. Using wordnet to disambiguate word senses for text retrieval. In Proc. of SIGIR, pages 171­180, 1993.",null,null
769,"[43] E. M. Voorhees. Query expansion using lexical-semantic relations. In Proc. of SIGIR, pages 61­69, 1994.",null,null
770,"[44] C. Xiong and J. Callan. EsdRank: Connecting query and documents through external semi-structured data. In Proc. of CIKM, pages 951­960, 2015.",null,null
771,"[45] C. Xiong and J. Callan. Query expansion with Freebase. In Proc. of ICTIR, pages 111­120, 2015.",null,null
772,"[46] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on Wikipedia. In Proc. of SIGIR, pages 59­66, 2009.",null,null
773,"[47] Y. Yang and C. G. Chute. Words or concepts: the features of indexing units and their optimal use in information retrieval. In Proc. of SCAMC, page 685, 1993.",null,null
774,"[48] C. Zhai. Statistical language models for information retrieval: A critical review. Foundations and Trends in Information Retrieval, 2(3):137­213, 2008.",null,null
775,"[49] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.",null,null
776,74,null,null
777,,null,null

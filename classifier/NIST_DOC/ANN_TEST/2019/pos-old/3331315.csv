,sentence,label,data
0,Short Research Papers 2B: Recommendation and Evaluation,null,null
1,,null,null
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
3,,null,null
4,A New Perspective on Score Standardization,null,null
5,,null,null
6,Julián Urbano,null,null
7,Delft University of Technology The Netherlands,null,null
8,urbano.julian@gmail.com,null,null
9,,null,null
10,Harlley Lima,null,null
11,Delft University of Technology The Netherlands,null,null
12,h.a.delima@tudelft.nl,null,null
13,,null,null
14,Alan Hanjalic,null,null
15,Delft University of Technology The Netherlands,null,null
16,a.hanjalic@tudelft.nl,null,null
17,,null,null
18,ABSTRACT,null,null
19,"In test collection based evaluation of IR systems, score standardization has been proposed to compare systems across collections and minimize the effect of outlier runs on specific topics. The underlying idea is to account for the difficulty of topics, so that systems are scored relative to it. Webber et al. first proposed standardization through a non-linear transformation with the standard normal distribution, and recently Sakai proposed a simple linear transformation. In this paper, we show that both approaches are actually special cases of a simple standardization which assumes specific distributions for the per-topic scores. From this viewpoint, we argue that a transformation based on the empirical distribution is the most appropriate choice for this kind of standardization. Through a series of experiments on TREC data, we show the benefits of our proposal in terms of score stability and statistical test behavior.",null,null
20,KEYWORDS,null,null
21,"Evaluation, test collection, score standardization, statistical testing",null,null
22,"ACM Reference Format: Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. A New Perspective on Score Standardization. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
23,1 INTRODUCTION,null,null
24,In the traditional Cranfield paradigm for test collection based evaluation in Information Retrieval,null,null
25,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331315",null,null
26,,null,null
27,"instance the bottom-left plot in Figure 1). Therefore, the observed",null,null
28,,null,null
29,differences in mean scores may be disproportionately due to a few,null,null
30,,null,null
31,"topics in the collection [2, 9].",null,null
32,,null,null
33,"To mitigate this problem, Webber et al. [9] proposed a two-step",null,null
34,,null,null
35,standardization process to look at scores relative to the difficulty of,null,null
36,,null,null
37,"the topic. First, given a raw effectiveness score x of some system",null,null
38,,null,null
39,"on some topic, a traditional z-score is computed",null,null
40,,null,null
41,"x -µ z=  ,",null,null
42,,null,null
43,(1),null,null
44,,null,null
45,where µ and  are the mean and standard deviation of the system,null,null
46,,null,null
47,scores for the topic. The effect is twofold: whether the topic is easy,null,null
48,,null,null
49,or hard,null,null
50,,null,null
51,zero; and whether systems perform similarly for the topic or not,null,null
52,,null,null
53,"(low or high  ), the z-scores have unit variance. Thanks to this first",null,null
54,,null,null
55,"step, all topics contribute equally to the final scores.",null,null
56,,null,null
57,The second step is a transformation of the z-score so that the final,null,null
58,,null,null
59,"standardized score is bounded between 0 and 1, as is customary",null,null
60,,null,null
61,in IR measures. Webber et al. [9] propose to use the cumulative,null,null
62,,null,null
63,distribution function,null,null
64,,null,null
65,which naturally maps z-scores on to the unit interval:,null,null
66,,null,null
67,=,null,null
68,,null,null
69,(2),null,null
70,,null,null
71,"Recently, Sakai [3] proposed a simple linear transformation of the z-score instead of the non-linear transformation applied by :",null,null
72,,null,null
73,= Az + B.,null,null
74,,null,null
75,(3),null,null
76,,null,null
77,"On the grounds of Chebyshev's inequality, they further suggested A= 0.15 and B = 0.5 so that at least 89% of the scores will fall within [0.05, 0.95]. Furthermore, and to ensure that standardized scores always stay within the unit interval, they proposed to simply censor",null,null
78,"between 0 and 1, computing = max(min(1, Az + B), 0) in reality. In this paper we show that the standardizations by Webber et al. [9] and Sakai [3] are actually special cases of a general class of standardizations consisting in assuming a specific distribution for the per-topic scores, and that they differ only in what distribution they assume. From this new perspective, we argue that the empirical distribution is actually the most appropriate choice because of its properties. We also carry out two experiments on TREC data that show how our proposal behaves better than both raw scores and the previous standardization schemes.",null,null
79,,null,null
80,2 SCORE STANDARDIZATION,null,null
81,"Let F be the distribution of scores by some population of systems on some particular topic and according to some specific measure like AP. If we knew this distribution, we could standardize a raw score x simply by computing = F",null,null
82,"between 0 and 1. The reasoning is that the cdf actually tells us where x is with respect to the rest of scores that one may expect for the topic, precisely computing the fraction of systems with lower",null,null
83,,null,null
84,1061,null,null
85,,null,null
86,Short Research Papers 2B: Recommendation and Evaluation,null,null
87,,null,null
88,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
89,,null,null
90,1062,null,null
91,,null,null
92,Short Research Papers 2B: Recommendation and Evaluation,null,null
93,,null,null
94,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
95,,null,null
96,"with slight deviations if censoring is needed. In the case of E-std, we see nearly constant mean and variance. This is achieved by design, because, in general, if X  F , then Y = F",null,null
97,The point still remains that µ and  are also unknown. The way around this limitation is to estimate them from a previous set of systems,null,null
98,3 EXPERIMENTS,null,null
99,"This section reports on two experiments to assess the effect of standardization. In the first one, we consider system comparisons using the same test collection",null,null
100,The data used in our experiments are the TREC 2004 Robust,null,null
101,3.1 Within-Collection Comparisons,null,null
102,"In order to investigate the effect of standardization on withincollection comparisons, we proceed as follows. We randomly sample 50 topics from the full set and compute the raw scores and the standardized scores as per each of the standardization schemes. From these data we compute three statistics. First, we compute the correlation between the ranking of systems by raw scores and the ranking by standardized scores, using Kendall's  and Yilmaz's ap [10]1. A high correlation indicates that the standardized scores are not much different from the raw scores, so in principle we look for lower coefficients. The third indicator evaluates the statistical power of the evaluation. In particular, we run a 2-tailed paired t-test between every pair of systems and, under the assumption that the null hypothesis is indeed false, look for schemes that maximize power. The process is repeated 10,000 times with both the RB and TB datasets, on both AP and nDCG.",null,null
103,"Figure 2 shows the results for a selection of collection-measure combinations2. The two plots in the first row show the distributions of  correlations. As expected, U-std and z-std perform very similarly because the former is simply a linear transformation of",null,null
104,"1In particular, we compute b and ap,b to deal with tied systems. See [5] for details. 2All plots, along with data and code to reproduce results, are available from https://github.com/julian-urbano/sigir2019-standardization.",null,null
105,,null,null
106,N-std U-std z-std E-std,null,null
107,,null,null
108,Robust - AP,null,null
109,,null,null
110,20,null,null
111,,null,null
112,Terabyte - AP,null,null
113,N-std U-std z-std E-std,null,null
114,,null,null
115,Density,null,null
116,,null,null
117,0 5 10,null,null
118,,null,null
119,Density 0 5 15 25,null,null
120,,null,null
121,Density 0 5 10 15,null,null
122,,null,null
123,Density 0 5 10 15 20 25,null,null
124,,null,null
125,0.86,null,null
126,,null,null
127,0.90,null,null
128,,null,null
129,0.94,null,null
130,,null,null
131,,null,null
132,,null,null
133,Robust - nDCG,null,null
134,,null,null
135,N-std U-std z-std E-std,null,null
136,,null,null
137,0.98,null,null
138,,null,null
139,0.86,null,null
140,,null,null
141,0.90,null,null
142,,null,null
143,0.94,null,null
144,,null,null
145,,null,null
146,,null,null
147,Terabyte - nDCG,null,null
148,,null,null
149,N-std U-std z-std E-std,null,null
150,,null,null
151,0.98,null,null
152,,null,null
153,0.75,null,null
154,,null,null
155,0.80,null,null
156,,null,null
157,0.85 0.90 ap,null,null
158,,null,null
159,0.95,null,null
160,,null,null
161,1.00,null,null
162,,null,null
163,Robust - AP,null,null
164,,null,null
165,0.75,null,null
166,,null,null
167,0.80,null,null
168,,null,null
169,0.85 0.90 ap,null,null
170,,null,null
171,0.95,null,null
172,,null,null
173,1.00,null,null
174,,null,null
175,Terabyte - nDCG,null,null
176,,null,null
177,Power 0.55 0.65 0.75,null,null
178,,null,null
179,Power 0.45 0.55 0.65,null,null
180,,null,null
181,0.001,null,null
182,,null,null
183,raw N-std U-std z-std E-std,null,null
184,,null,null
185,0.005,null,null
186,,null,null
187,0.020 0.050,null,null
188,,null,null
189,Significance level,null,null
190,,null,null
191,0.001,null,null
192,,null,null
193,raw N-std U-std z-std E-std,null,null
194,,null,null
195,0.005,null,null
196,,null,null
197,0.020 0.050,null,null
198,,null,null
199,Significance level,null,null
200,,null,null
201,Figure 2: Within-collection comparisons. First row:  correlation between rankings of systems with raw and standardized scores,null,null
202,,null,null
203,"the latter; differences come from the necessity to censor outliers in U-std. Indeed, because they are both a linear transformation of the raw scores, they produce the most similar rankings. N-std results in slightly lower correlations, but E-std sets itself clearly apart from the others, yielding significantly lower  scores. The plots in the second row show even clearer differences in terms of ap . We see that U-std and z-std are almost identical, but more importantly we see that N-std and E-std are even further away, likely because they eliminate outliers that could affect the top ranked systems.",null,null
204,"The two plots in the last row show statistical power for a range of significance levels. We can first observe that all standardization schemes achieve higher power than the raw scores, showing a clear advantage of the standardization principle. Once again, U-std and z-std perform nearly identically, and both are outperformed by N-std and, specially, E-std.",null,null
205,3.2 Between-Collection Comparisons,null,null
206,"Here we study how standardization affects between-collection comparisons. In this case, we randomly sample two disjoint subsets of 50 topics each and compute raw and standardized scores on both subsets. Because topics are sampled from the full set, both results can be regarded as coming from two different collections having different topics from the same population. In this case we are not interested in how standardized scores compare to raw scores, but rather on how stable the results are between both sets of topics, so we compute the following four statistics. First, we compute the  and ap correlations between both rankings. We seek high correlations, indicating high score stability across topic sets. Third, for",null,null
207,,null,null
208,1063,null,null
209,,null,null
210,Short Research Papers 2B: Recommendation and Evaluation,null,null
211,,null,null
212,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
213,,null,null
214,"every system we run a 2-tailed unpaired t-test between both sets. By definition, the null hypothesis is true because we are comparing a system to itself simply on a different sample, so we expect as many Type I errors as the significance level . Finally, we run another test between every system on one collection and every other system on the other collection, looking again to maximize statistical power under the assumption that all systems are different and thus null hypotheses are false. As before, this process is repeated 10,000 times with both the RB and TB datasets, on both AP and nDCG.",null,null
215,"Figure 3 shows the results for a selection of collection-measure combinations. The plots in the first two rows show that standardization generally produces more stable results, as evidenced by raw scores yielding lower correlations. U-std and z-std perform very similarly once again, and E-std generally outperforms the others, producing slightly more stable comparisons between collections. An exception can be noticed for ap on the TB dataset, which requires further investigation.",null,null
216,The third row of plots show the Type I error rates. We can see that all scoring schemes behave just as expected by the significance level . This evidences on the one hand the robustness of the t-test [7],null,null
217,4 CONCLUSIONS,null,null
218,"In this paper we revisit the problem of score standardization to make IR evaluation robust to variations in topic difficulty. We introduced a new scheme for standardization based on the distributions of pertopic scores, and showed that previous methods by Webber et al. [9] and Sakai [3] are special cases of this scheme. From this point of view we propose the empirical distribution as an alternative, and discuss a number of points that highlight its superiority.",null,null
219,"In experiments with TREC data, we showed that, even though the raw and standardized rankings are the same topic by topic, the rankings by mean scores may differ considerably. In addition, standardization achieves higher statistical power. Thus, standardization offers an alternative and quite different view on system comparisons. However, it is important to note that these comparisons are made on a different scale altogether, so one may not just use standardized scores to make statements about raw scores. Nonetheless, standardization with the empirical distribution is arguably more faithful to our notion of relative system effectiveness.",null,null
220,"Future work will follow three main lines. First, we will study additional datasets and measures for generality. However, because TREC collections are usually limited to 50 topics, we also plan on using recent simulation methods so that we can analyze more data [7]. Finally, we will study the stability of E-std for varying numbers of systems. This is interesting because, even though the empirical function converges to the true distribution, it is unclear how large the set of systems needs to be for the results to be stable.",null,null
221,,null,null
222,Density,null,null
223,,null,null
224,Density 0 2 4 6 8 10,null,null
225,,null,null
226,Robust - nDCG,null,null
227,raw N-std U-std z-std E-std,null,null
228,,null,null
229,10 15,null,null
230,,null,null
231,Terabyte - nDCG,null,null
232,raw N-std U-std z-std E-std,null,null
233,,null,null
234,5,null,null
235,,null,null
236,0,null,null
237,,null,null
238,0.65,null,null
239,,null,null
240,0.70 0.75 0.80 0.85,null,null
241,Robust - AP,null,null
242,raw N-std U-std z-std E-std,null,null
243,,null,null
244,0.90,null,null
245,,null,null
246,0.95,null,null
247,,null,null
248,0.65,null,null
249,,null,null
250,0.70 0.75 0.80 0.85,null,null
251,Terabyte - AP,null,null
252,raw N-std U-std z-std E-std,null,null
253,,null,null
254,0.90,null,null
255,,null,null
256,0.95,null,null
257,,null,null
258,Density 02468,null,null
259,,null,null
260,Density 02468,null,null
261,,null,null
262,0.050,null,null
263,,null,null
264,0.5,null,null
265,,null,null
266,0.6,null,null
267,,null,null
268,0.7,null,null
269,,null,null
270,0.8,null,null
271,,null,null
272,0.9,null,null
273,,null,null
274,0.5,null,null
275,,null,null
276,0.6,null,null
277,,null,null
278,0.7,null,null
279,,null,null
280,0.8,null,null
281,,null,null
282,0.9,null,null
283,,null,null
284,ap,null,null
285,,null,null
286,ap,null,null
287,,null,null
288,Robust - AP,null,null
289,,null,null
290,Terabyte - AP,null,null
291,,null,null
292,0.050,null,null
293,,null,null
294,raw N-std U-std z-std E-std,null,null
295,,null,null
296,raw N-std U-std z-std E-std,null,null
297,,null,null
298,Type I error rate,null,null
299,,null,null
300,Type I error rate,null,null
301,,null,null
302,0.001 0.005,null,null
303,,null,null
304,0.001 0.005,null,null
305,,null,null
306,0.001,null,null
307,,null,null
308,0.005,null,null
309,,null,null
310,0.020,null,null
311,,null,null
312,Significance level,null,null
313,,null,null
314,Robust - nDCG,null,null
315,,null,null
316,0.050,null,null
317,,null,null
318,0.001,null,null
319,,null,null
320,0.005,null,null
321,,null,null
322,0.020 0.050,null,null
323,,null,null
324,Significance level,null,null
325,,null,null
326,Terabyte - nDCG,null,null
327,,null,null
328,Power 0.4 0.5 0.6 0.7,null,null
329,,null,null
330,Power 0.3 0.4 0.5 0.6,null,null
331,,null,null
332,0.001,null,null
333,,null,null
334,raw N-std U-std z-std E-std,null,null
335,,null,null
336,0.005,null,null
337,,null,null
338,0.020 0.050,null,null
339,,null,null
340,Significance level,null,null
341,,null,null
342,0.001,null,null
343,,null,null
344,raw N-std U-std z-std E-std,null,null
345,,null,null
346,0.005,null,null
347,,null,null
348,0.020 0.050,null,null
349,,null,null
350,Significance level,null,null
351,,null,null
352,Figure 3: Between-collection comparisons. First row:  correlation between the rankings of systems produced by the two collections,null,null
353,,null,null
354,ACKNOWLEDGMENTS,null,null
355,Work carried out on the Dutch national e-infrastructure,null,null
356,REFERENCES,null,null
357,"[1] D. Bodoff. 2008. Test Theory for Evaluating Reliability of IR Test Collections. Information Processing and Management 44, 3",null,null
358,"[2] J. Guiver, S. Mizzaro, and S. Robertson. 2009. A Few Good Topics: Experiments in Topic Set Reduction for Retrieval Evaluation. ACM TOIS 27, 4",null,null
359,[3] T. Sakai. 2016. A Simple and Effective Approach to Score Standardization. In ACM ICTIR. 95­104.,null,null
360,"[4] M. Sanderson. 2010. Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval 4, 4",null,null
361,[5] J. Urbano and M. Marrero. 2017. The Treatment of Ties in AP Correlation. In SIGIR ICTIR. 321­324.,null,null
362,"[6] J. Urbano, M. Marrero, and D. Martín. 2013. On the Measurement of Test Collection Reliability. In ACM SIGIR. 393­402.",null,null
363,[7] J. Urbano and T. Nagler. 2018. Stochastic Simulation of Test Collections: Evaluation Scores. In ACM SIGIR.,null,null
364,"[8] E. Voorhees. 2005. Overview of the TREC 2005 Robust Retrieval Track. In TREC. [9] W. Webber, A. Moffat, and J. Zobel. 2008. Score Standardization for Inter-",null,null
365,"collection Comparison of Retrieval Systems. In AMC SIGIR. 51­58. [10] E. Yilmaz, J.A. Aslam, and S. Robertson. 2008. A New Rank Correlation Coefficient",null,null
366,for Information Retrieval. In AMC SIGIR. 587­594.,null,null
367,,null,null
368,1064,null,null
369,,null,null
370,,null,null

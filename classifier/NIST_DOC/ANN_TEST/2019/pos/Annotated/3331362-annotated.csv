,sentence,label,data
,,,
0,Short Research Papers 3B: Recommendation and Evaluation,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries,null,null
,,,
5,,null,null
,,,
6,Saar Kuzi,null,null
,,,
7,skuzi2@illinois.edu University of Illinois at Urbana-Champaign,null,null
,,,
8,Anusri Pampari,null,null
,,,
9,anusri@stanford.edu Stanford University,null,null
,,,
10,ABSTRACT,null,null
,,,
11,"In this paper, we address the problem of difficult queries by using a novel strategy of collaborative query construction where the search engine would actively engage users in an iterative process to continuously revise a query. This approach can be implemented in any search engine to provide search support for users via a ""Help Me Search"" button, which a user can click on as needed. We focus on studying a specific collaboration strategy where the search engine and the user work together to iteratively expand a query. We propose a possible implementation for this strategy in which the system generates candidate terms by utilizing the history of interactions of the user with the system. Evaluation using a simulated user study shows the great promise of the proposed approach. We also perform a case study with three real users which further illustrates the potential effectiveness of the approach.",null,null
,,,
12,"ACM Reference format: Saar Kuzi, Abhishek Narwekar, Anusri Pampari, and ChengXiang Zhai. 2019. Help Me Search: Leveraging User-System Collaboration for Query Construction to Improve Accuracy for Difficult Queries. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, Paris, France, July 21­25, 2019",null,null
,,,
13,1 INTRODUCTION,null,null
,,,
14,"The current search engines generally work well for popular queries where a large amount of click-through information can be leveraged. Such a strategy may fail for long-tail queries, which are entered by only a small number of users. Thus, for such queries, a search engine generally would have to rely mainly on matching the keywords in the query with those in documents. Unfortunately, such a method would not work well when the user's query does not include the ""right"" keywords. Users in such cases would often end up repeatedly reformulating a query, yet they still could not find the relevant",null,null
,,,
15,This work was done while the author was a student at UIUC.,null,null
,,,
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331362",null,null
,,,
17,,null,null
,,,
18,Abhishek Narwekar,null,null
,,,
19,narweka@amazon.com Amazon Alexa,null,null
,,,
20,ChengXiang Zhai,null,null
,,,
21,czhai@illinois.edu University of Illinois at Urbana-Champaign,null,null
,,,
22,"documents. Unfortunately, there are many such queries, making it a pressing challenge for search engines to improve their accuracy.",null,null
,,,
23,"In this paper, we address this problem and propose a general strategy of collaborative query construction where the search engine would actively engage users in an iterative process to revise a query. The proposed strategy attempts to optimize the collaboration between the user and the search engine and is based on the following assumptions:",null,null
,,,
24,"Our main idea is to optimize the user-system collaboration in order to perform a sequence of modifications to the query with the goal of reaching an ideal query. While the proposed strategy includes multiple ways to edit the query, we initially focus on studying a specific editing operator where the system suggests terms to the user to be added to the query at each step based on the history of interactions of the user with the system.",null,null
,,,
25,"We perform an evaluation with a simulated user which demonstrates the great promise of this novel collaborative search support strategy for improving the accuracy of difficult queries with minimum effort from the user. The results also show that suggesting terms based on user interaction history improves effectiveness without incurring additional user effort. Finally, we conduct a case study with three real users that demonstrates the potential effectiveness of our approach when real users are involved.",null,null
,,,
26,2 RELATED WORK,null,null
,,,
27,"The main novelty of our work is the idea of collaborative construction of an ideal query, specific algorithms for iterative query expansion, and the study of their effectiveness for difficult queries.",null,null
,,,
28,Previous works have studied approaches for interactive query expansion,null,null
,,,
29,,null,null
,,,
30,1221,null,null
,,,
31,,null,null
,,,
32,Short Research Papers 3B: Recommendation and Evaluation,null,null
,,,
33,,null,null
,,,
34,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
35,,null,null
,,,
36,"Furthermore, we propose methods which suggest terms to the user based on the history of interactions of the user with the system.",null,null
,,,
37,"On the surface, our approach is similar to query suggestion already studied in previous works [3]. However, there are two important differences:",null,null
,,,
38,"Other works focused on developing query suggestion approaches for difficult queries [6, 11]. In general, ideas from past works on query suggestion can be used in our approach for generating the set of query modifications that are suggested to the user.",null,null
,,,
39,"There is a large body of work on devising approaches for automatic query reformulation. One common method is to automatically add terms to the user's query [5]. Other approaches include, for example, substitution or deletion of terms [12]. The various ideas, which are suggested by works in this direction, can be integrated into our collaborative approach by devising sophisticated methods for term suggestion.",null,null
,,,
40,3 COLLABORATIVE QUERY CONSTRUCTION,null,null
,,,
41,Our suggested Collaborative Query Construction,null,null
,,,
42,"Our collaborative query construction process is represented by a sequence of queries, Q1, Q2, ..., Qn , where Q1 is the user's initial query, Qn is an ideal query, and Qi+1 is closer to Qn than Qi and the gap between Qi and Qi+1 is small enough for the user to recognize the improvement of Qi+1 over Qi . From the system's perspective, at any point of this process, the task is to suggest a set of candidate queries, while the user's task is to choose one of them. In this paper, we focus on a specific approach in which the query refinement is restricted to only adding one extra term to the query at each step. That is, a single collaborative iteration of revising a query Qi would be as follows:",null,null
,,,
43,,null,null
,,,
44,One advantage of using such an approach is that the gap be-,null,null
,,,
45,,null,null
,,,
46,tween two adjacent queries is expected to be small enough for the,null,null
,,,
47,,null,null
,,,
48,"user to recognize the correct choice. Furthermore, although this",null,null
,,,
49,,null,null
,,,
50,"implementation strategy is very simple, theoretically speaking, the",null,null
,,,
51,,null,null
,,,
52,process can guarantee the construction of any ideal query that,null,null
,,,
53,,null,null
,,,
54,contains all the original query terms if the system can suggest ad-,null,null
,,,
55,,null,null
,,,
56,ditional terms in the ideal query but not in the original query and,null,null
,,,
57,,null,null
,,,
58,the user can recognize the terms to be included in the ideal query.,null,null
,,,
59,,null,null
,,,
60,"We assume that the original query terms are all ""essential"" and",null,null
,,,
61,,null,null
,,,
62,"should all be included in the ideal query. While true in general, in",null,null
,,,
63,,null,null
,,,
64,"some cases this assumption may not hold, which would require the",null,null
,,,
65,,null,null
,,,
66,"removal or substitution of terms in the initial query. In this paper,",null,null
,,,
67,,null,null
,,,
68,"however, we focus on term addition as our first strategy and leave",null,null
,,,
69,,null,null
,,,
70,the incorporation of other operations for future work.,null,null
,,,
71,,null,null
,,,
72,"Following the game-theoretic framework for interactive IR [13],",null,null
,,,
73,,null,null
,,,
74,our approach can be framed as the following Bayesian decision,null,null
,,,
75,,null,null
,,,
76,problem where the goal is to decide a candidate set of terms Ti to,null,null
,,,
77,,null,null
,,,
78,suggest to the user in response to the current query Qi :,null,null
,,,
79,,null,null
,,,
80,,null,null
,,,
81,,null,null
,,,
82,Ti,null,null
,,,
83,,null,null
,,,
84,=,null,null
,,,
85,,null,null
,,,
86,arg min,null,null
,,,
87,T V -Qi,null,null
,,,
88,,null,null
,,,
89,"L(T , Hi , Q , U )p(Q |Hi , U )dQ ;",null,null
,,,
90,Q,null,null
,,,
91,,null,null
,,,
92,-1,null,null
,,,
93,,null,null
,,,
94,where,null,null
,,,
95,,null,null
,,,
96,denotes any relevant information about the user.,null,null
,,,
97,"query. The integral indicates the uncertainty about the ideal query,",null,null
,,,
98,,null,null
,,,
99,which can be expected to be reduced as we collect more information,null,null
,,,
100,,null,null
,,,
101,from the user.,null,null
,,,
102,,null,null
,,,
103,While in general we need to assess the loss of an entire can-,null,null
,,,
104,,null,null
,,,
105,"didate set T , in the much simplified method that we will actually",null,null
,,,
106,,null,null
,,,
107,"explore, we choose T by scoring each term and then applying a",null,null
,,,
108,,null,null
,,,
109,"threshold to control the number of terms. That is, we assume that",null,null
,,,
110,,null,null
,,,
111,the loss function on a term set T can be written as an aggregation,null,null
,,,
112,,null,null
,,,
113,of the loss on each individual term. As an additional simplifica-,null,null
,,,
114,,null,null
,,,
115,"tion, we approximate the integral with the mode of the posterior probability about the ideal query, ^ Q . Thus, our decision problem would become to compute the score of each term t, not already selected by the user, as follows: s(t) = -L(t, Hi , ^ Q , U ); where ^ Q = arg maxQ p(Q |Hi , U ). Computationally, the algorithm boils down to the following two steps:",null,null
,,,
116,,null,null
,,,
117,4 TERM SCORING,null,null
,,,
118,"According to the previous section, the optimal scoring function s(t) is based on the negative loss -L(t, Hi , ^ Q , U ). Intuitively, the loss of word t is negatively correlated with its probability according to ^ Q . We thus simply define our scoring function as s(t) = p(t |^ Q ). That is, our problem is now reduced to infer ^ Q given all of the observed information Hi and U .",null,null
,,,
119,"Next, we suggest a model for inferring ^ Q , which is based on Pseudo-Relevance Feedback",null,null
,,,
120,the relevance model RM1 [7] to incorporate Hi,null,null
,,,
121,,null,null
,,,
122,1222,null,null
,,,
123,,null,null
,,,
124,Short Research Papers 3B: Recommendation and Evaluation,null,null
,,,
125,,null,null
,,,
126,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
127,,null,null
,,,
128,p(t |^ Q ) =,null,null
,,,
129,,null,null
,,,
130,"p(t |d) · p(d |Q1, Hi ).",null,null
,,,
131,,null,null
,,,
132,-2,null,null
,,,
133,,null,null
,,,
134,d Di,null,null
,,,
135,,null,null
,,,
136,p(t |d) is estimated using the maximum likelihood approach. We,null,null
,,,
137,,null,null
,,,
138,"approximate p(d |Q1, Hi ) using a linear interpolation:",null,null
,,,
139,,null,null
,,,
140,"p(d |Q1, Hi ) =",null,null
,,,
141,,null,null
,,,
142,-3,null,null
,,,
143,,null,null
,,,
144,"p(d |Q1) is proportional to the reciprocal rank of d w.r.t Q1;   [0, 1].",null,null
,,,
145,"In order to estimate p(d |Hi ), two types of historical information are considered:",null,null
,,,
146,"In order to estimate p(d |HiD ), we assume that documents which appear in the result list presented to the user in the current iteration,",null,null
,,,
147,,null,null
,,,
148,"and that were absent in the previous result list, represent aspects",null,null
,,,
149,,null,null
,,,
150,of the information need that are more important to the user. We,null,null
,,,
151,,null,null
,,,
152,thus,null,null
,,,
153,,null,null
,,,
154,estimate p(d |HiD ) p(d |HiD ) =,null,null
,,,
155,,null,null
,,,
156,as follows: 1,null,null
,,,
157,rankDi,null,null
,,,
158,,null,null
,,,
159,Z,null,null
,,,
160,,null,null
,,,
161,D,null,null
,,,
162,,null,null
,,,
163,d,null,null
,,,
164,,null,null
,,,
165,,null,null
,,,
166,,null,null
,,,
167,Di,null,null
,,,
168,,null,null
,,,
169,\ Di-1;,null,null
,,,
170,,null,null
,,,
171,-4,null,null
,,,
172,,null,null
,,,
173,p(d |HiD ) = 0 for all other documents; rankDi,null,null
,,,
174,We estimate p(d |HiT ) such that high importance is attributed to documents in which terms that were previously selected by the,null,null
,,,
175,,null,null
,,,
176,user are prevalent.,null,null
,,,
177,,null,null
,,,
178,i -1,null,null
,,,
179,,null,null
,,,
180,"p(d |HiT ) = p(d |tj , HiT ) · p(tj |HiT );",null,null
,,,
181,,null,null
,,,
182,-5,null,null
,,,
183,,null,null
,,,
184,j =1,null,null
,,,
185,,null,null
,,,
186,"tj is the term selected by the user in the j'th iteration. p(d |tj , HiT ) is set to be proportional to the score of d with respect to tj as calculated by the system's ranking method. Assuming that terms",null,null
,,,
187,,null,null
,,,
188,selected in more recent iterations are more important than older,null,null
,,,
189,,null,null
,,,
190,"ones, we estimate p(tj |HiT ) as: p(tj |HiT )",null,null
,,,
191,,null,null
,,,
192,=,null,null
,,,
193,,null,null
,,,
194,exp(-µ ·(i ZT,null,null
,,,
195,,null,null
,,,
196,#NAME?,null,null
,,,
197,,null,null
,,,
198,)),null,null
,,,
199,,null,null
,,,
200,;,null,null
,,,
201,,null,null
,,,
202,ZT,null,null
,,,
203,,null,null
,,,
204,is a,null,null
,,,
205,,null,null
,,,
206,normalization factor; µ is a free parameter and is set to 0.5.,null,null
,,,
207,,null,null
,,,
208,"To conclude, we assign a probability to each term which is a lin-",null,null
,,,
209,,null,null
,,,
210,ear interpolation of its probabilities in the documents in the result,null,null
,,,
211,,null,null
,,,
212,"list, where the interpolation weights are influenced by:",null,null
,,,
213,,null,null
,,,
214,"of the document,",null,null
,,,
215,,null,null
,,,
216,"list, and",null,null
,,,
217,,null,null
,,,
218,"Query representation: According to our approach, the query Qi is composed of the original query Q1 and the terms selected by the user. The terms in Qi are weighted based on a probability distribution such that the probability of a term t in V is: p(t |Qi ) = i ·pmle",null,null
,,,
219,"was previously selected, and is set to 0 otherwise; pmle",null,null
,,,
220,,null,null
,,,
221,5 EVALUATION,null,null
,,,
222,The evaluation of the proposed strategy has two challenges:,null,null
,,,
223,,null,null
,,,
224,Table 1: Simulated user performance. Statistically significant differences with RM3 are marked with asterisk. All differences with the initial query are statistically significant.,null,null
,,,
225,,null,null
,,,
226,Initial RM3 CQC,null,null
,,,
227,,null,null
,,,
228,p @5,null,null
,,,
229,.000 .036 .057,null,null
,,,
230,,null,null
,,,
231,Single Term,null,null
,,,
232,,null,null
,,,
233,p@10 M RR success@10,null,null
,,,
234,,null,null
,,,
235,.000 .053,null,null
,,,
236,,null,null
,,,
237,0,null,null
,,,
238,,null,null
,,,
239,.040 .083,null,null
,,,
240,,null,null
,,,
241,0.238,null,null
,,,
242,,null,null
,,,
243,.090 .127,null,null
,,,
244,,null,null
,,,
245,0.457,null,null
,,,
246,,null,null
,,,
247,p @5,null,null
,,,
248,.000 .040 .137,null,null
,,,
249,,null,null
,,,
250,Five Terms,null,null
,,,
251,,null,null
,,,
252,p@10 M RR success@10,null,null
,,,
253,,null,null
,,,
254,.000 .053,null,null
,,,
255,,null,null
,,,
256,0,null,null
,,,
257,,null,null
,,,
258,.049 .090,null,null
,,,
259,,null,null
,,,
260,0.219,null,null
,,,
261,,null,null
,,,
262,.136 .209,null,null
,,,
263,,null,null
,,,
264,0.447,null,null
,,,
265,,null,null
,,,
266,"(TREC discs 4 and 5-{CR}). The collection is composed of 528,155",Y,TREC
,,,
267,,null,null
,,,
268,"newswire documents, along with 249 TREC topics which their titles",Y,TREC
,,,
269,,null,null
,,,
270,serve as queries,null,null
,,,
271,,null,null
,,,
272,stemming were applied to both documents and queries. The Lucene,null,null
,,,
273,,null,null
,,,
274,toolkit was used for experiments,null,null
,,,
275,,null,null
,,,
276,model was used for ranking [9]. We use the following strategy to,null,null
,,,
277,,null,null
,,,
278,construct our test set. We first perform retrieval for all queries.,null,null
,,,
279,,null,null
,,,
280,"Then, we remove from the collection the relevant documents that",null,null
,,,
281,,null,null
,,,
282,are among the top 10 documents in each result list. After doing,null,null
,,,
283,,null,null
,,,
284,"that, we remain with 105 queries for which p@10 = 0 when per-",null,null
,,,
285,,null,null
,,,
286,forming retrieval over the modified collection. We use these queries,null,null
,,,
287,,null,null
,,,
288,"for our evaluation, along with the modified collection. We report",null,null
,,,
289,,null,null
,,,
290,performance in terms of precision,null,null
,,,
291,,null,null
,,,
292,rocal Rank,null,null
,,,
293,,null,null
,,,
294,Average Precision in the case of such difficult queries,null,null
,,,
295,,null,null
,,,
296,how much effort a user has to make in order to reach the very,null,null
,,,
297,,null,null
,,,
298,first relevant document). We also report the fraction of queries for,null,null
,,,
299,,null,null
,,,
300,"which a method resulted in p@10 > 0, denoted success@10. The",null,null
,,,
301,,null,null
,,,
302,two-tailed paired t-test at 95% confidence level is used in order to,null,null
,,,
303,,null,null
,,,
304,determine significant differences in performance.,null,null
,,,
305,,null,null
,,,
306,"Our approach involves free parameters, which are set to effective",null,null
,,,
307,,null,null
,,,
308,ones following some preliminary experiments. We should point,null,null
,,,
309,,null,null
,,,
310,out that our research questions are mainly about how promising,null,null
,,,
311,,null,null
,,,
312,"the proposed approach is as a novel interaction strategy, which is",null,null
,,,
313,,null,null
,,,
314,generally orthogonal to the optimization of these parameters. The,null,null
,,,
315,,null,null
,,,
316,"number of terms suggested to the user, m, is set to 5. The number",null,null
,,,
317,,null,null
,,,
318,of documents used in our PRF-based term scoring method is set,null,null
,,,
319,,null,null
,,,
320,"to 100. The interpolation parameter in Equation 3, , is set to 0.8.",null,null
,,,
321,,null,null
,,,
322,"The value of i , the weight given to the original query, is set to",null,null
,,,
323,,null,null
,,,
324,"max(0.4,",null,null
,,,
325,,null,null
,,,
326,|Q1 |Qi,null,null
,,,
327,,null,null
,,,
328,| |,null,null
,,,
329,,null,null
,,,
330,);,null,null
,,,
331,,null,null
,,,
332,we,null,null
,,,
333,,null,null
,,,
334,chose,null,null
,,,
335,,null,null
,,,
336,this,null,null
,,,
337,,null,null
,,,
338,weighting,null,null
,,,
339,,null,null
,,,
340,function,null,null
,,,
341,,null,null
,,,
342,as,null,null
,,,
343,,null,null
,,,
344,to,null,null
,,,
345,,null,null
,,,
346,attribute,null,null
,,,
347,,null,null
,,,
348,high importance to the original query when a small amount of,null,null
,,,
349,,null,null
,,,
350,expansion is used. We compare the performance of our approach,null,null
,,,
351,,null,null
,,,
352,"with that of using the original query, and of using an automatic",null,null
,,,
353,,null,null
,,,
354,query expansion approach in which a set of terms is automatically,null,null
,,,
355,,null,null
,,,
356,added to the original query once. We set the number of expansion,null,null
,,,
357,,null,null
,,,
358,terms to be equal to the number of terms that were added by the,null,null
,,,
359,,null,null
,,,
360,user in the collaborative process. We use the RM3 [1] expansion,null,null
,,,
361,,null,null
,,,
362,model,null,null
,,,
363,,null,null
,,,
364,Simulation study: In order to do a controlled study of our ap-,null,null
,,,
365,,null,null
,,,
366,"proach, we experiment with a simulated user. Given a list of term",null,null
,,,
367,,null,null
,,,
368,"suggestions, the simulated user chooses a term with the highest",null,null
,,,
369,,null,null
,,,
370,"t f .id f score in the relevant documents. Specifically, for each query",null,null
,,,
371,,null,null
,,,
372,we concatenate all relevant documents and compute t f .id f based,null,null
,,,
373,,null,null
,,,
374,"on the single concatenated ""relevant document"". Our main result for",null,null
,,,
375,,null,null
,,,
376,the simulated user experiment is presented in Table 1. We report the,null,null
,,,
377,,null,null
,,,
378,performance when a single term or five terms are added. According,null,null
,,,
379,,null,null
,,,
380,"to the results, the collaborative approach",null,null
,,,
381,,null,null
,,,
382,"Specifically, after adding a single term to the query, users are able to",null,null
,,,
383,,null,null
,,,
384,1223,null,null
,,,
385,,null,null
,,,
386,Short Research Papers 3B: Recommendation and Evaluation,null,null
,,,
387,,null,null
,,,
388,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
389,,null,null
,,,
390,p@10 success@10,null,null
,,,
391,p@10 success@10,null,null
,,,
392,,null,null
,,,
393,CQC-Q,null,null
,,,
394,,null,null
,,,
395,CQC-H,null,null
,,,
396,,null,null
,,,
397,0.13,null,null
,,,
398,,null,null
,,,
399,CQC,null,null
,,,
400,,null,null
,,,
401,CQC-Q,null,null
,,,
402,,null,null
,,,
403,CQC-H,null,null
,,,
404,,null,null
,,,
405,0.52,null,null
,,,
406,,null,null
,,,
407,CQC,null,null
,,,
408,,null,null
,,,
409,0.50 0.12,null,null
,,,
410,,null,null
,,,
411,0.48 0.11,null,null
,,,
412,,null,null
,,,
413,0.46 0.10,null,null
,,,
414,,null,null
,,,
415,0.44,null,null
,,,
416,,null,null
,,,
417,0.09,null,null
,,,
418,,null,null
,,,
419,1,null,null
,,,
420,,null,null
,,,
421,2,null,null
,,,
422,,null,null
,,,
423,3,null,null
,,,
424,,null,null
,,,
425,4,null,null
,,,
426,,null,null
,,,
427,5,null,null
,,,
428,,null,null
,,,
429,Additional Terms,null,null
,,,
430,,null,null
,,,
431,1,null,null
,,,
432,,null,null
,,,
433,2,null,null
,,,
434,,null,null
,,,
435,3,null,null
,,,
436,,null,null
,,,
437,4,null,null
,,,
438,,null,null
,,,
439,5,null,null
,,,
440,,null,null
,,,
441,Additional Terms,null,null
,,,
442,,null,null
,,,
443,Figure 1: Performance of the different model components.,null,null
,,,
444,,null,null
,,,
445,see a noticeable improvement on the first page of search results in about 45% of these difficult queries that did not return any relevant document initially,null,null
,,,
446,"In Figure 2, we compare the performance of the real users with that of the simulated user. According to the results, retrieval performance can be very good when terms are selected by real users. Specifically, all users reach success@10 of around 0.5. That is, after adding a single term, at least one relevant result is obtained for about 50% of the queries. In Table 2, we present examples of queries along with the terms that were selected by a single real user and a simulated user. We also report the performance that resulted from adding a term. The first query serves as an example where the real user outperforms the simulated user by a better choice of terms. The second query is an example where the simulated user",null,null
,,,
447,,null,null
,,,
448,0.35,null,null
,,,
449,,null,null
,,,
450,Simulated User,null,null
,,,
451,,null,null
,,,
452,Real User,null,null
,,,
453,,null,null
,,,
454,0.3,null,null
,,,
455,,null,null
,,,
456,Real User,null,null
,,,
457,,null,null
,,,
458,0.8,null,null
,,,
459,,null,null
,,,
460,Real User,null,null
,,,
461,,null,null
,,,
462,0.25,null,null
,,,
463,,null,null
,,,
464,0.6 0.20,null,null
,,,
465,,null,null
,,,
466,0.15,null,null
,,,
467,,null,null
,,,
468,0.4,null,null
,,,
469,,null,null
,,,
470,0.1,null,null
,,,
471,,null,null
,,,
472,0.2,null,null
,,,
473,,null,null
,,,
474,Simulated User,null,null
,,,
475,,null,null
,,,
476,Real User,null,null
,,,
477,,null,null
,,,
478,Real User,null,null
,,,
479,,null,null
,,,
480,Real User,null,null
,,,
481,,null,null
,,,
482,0,null,null
,,,
483,,null,null
,,,
484,0,null,null
,,,
485,,null,null
,,,
486,0,null,null
,,,
487,,null,null
,,,
488,1,null,null
,,,
489,,null,null
,,,
490,2,null,null
,,,
491,,null,null
,,,
492,3,null,null
,,,
493,,null,null
,,,
494,0,null,null
,,,
495,,null,null
,,,
496,1,null,null
,,,
497,,null,null
,,,
498,2,null,null
,,,
499,,null,null
,,,
500,3,null,null
,,,
501,,null,null
,,,
502,Additional Terms,null,null
,,,
503,,null,null
,,,
504,Additional Terms,null,null
,,,
505,,null,null
,,,
506,Figure 2: Real users vs. Simulated user. Table 2: Query Examples. The performance of the query,null,null
,,,
507,,null,null
,,,
508,(p@10) after adding a term is reported in the brackets.,null,null
,,,
509,,null,null
,,,
510,curbing population growth Stirling engine antibiotics ineffectiveness,null,null
,,,
511,,null,null
,,,
512,Real User Simulated Real User Simulated Real User Simulated,null,null
,,,
513,,null,null
,,,
514,plan,null,null
,,,
515,cfc,null,null
,,,
516,drug,null,null
,,,
517,,null,null
,,,
518,family,null,null
,,,
519,hcfc,null,null
,,,
520,,null,null
,,,
521,birth,null,null
,,,
522,,null,null
,,,
523,"outperforms the real user presumably by recognizing the correct technical terms. Finally, the last query is an example where both users achieve similar performance, but using different terms.",null,null
,,,
524,6 CONCLUSIONS AND FUTURE WORK,null,null
,,,
525,"We proposed and studied a novel strategy for improving the accuracy of difficult queries by having the search engine and the user collaboratively expand the original query. Evaluation with simulated users and a case study with real users show the great promise of this strategy. In future work, we plan to devise more methods for term scoring, incorporate more operations for query modification, and perform a large-scale user study. Acknowledgments. This material is based upon work supported by the National Science Foundation under grant number 1801652.",null,null
,,,
526,REFERENCES,null,null
,,,
527,"[1] Nasreen Abdul-Jaleel, James Allan, W Bruce Croft, Fernando Diaz, Leah Larkey, Xiaoyan Li, Mark D Smucker, and Courtney Wade. [n. d.]. UMass at TREC 2004: Novelty and HARD. Technical Report.",null,null
,,,
528,"[2] Peter Anick. 2003. Using terminological feedback for web search refinement: a log-based study. In Proceedings of SIGIR. ACM, 88­95.",null,null
,,,
529,"[3] Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Mendoza. 2004. Query recommendation using query logs in search engines. In International Conference on Extending Database Technology. Springer, 588­596.",null,null
,,,
530,"[4] Nicholas J. Belkin, Colleen Cool, Diane Kelly, S-J Lin, SY Park, J Perez-Carballo, and C Sikora. 2001. Iterative exploration, design and evaluation of support for query reformulation in interactive information retrieval. Information Processing & Management 37, 3",null,null
,,,
531,[5] Claudio Carpineto and Giovanni Romano. 2012. A Survey of Automatic Query Expansion in Information Retrieval. ACM Comput. Surv.,null,null
,,,
532,"[6] Van Dang and Bruce W Croft. 2010. Query reformulation using anchor text. In Proceedings of WSDM. ACM, 41­50.",null,null
,,,
533,"[7] Victor Lavrenko and W Bruce Croft. 2001. Relevance based language models. In Proceedings of SIGIR. ACM, 120­127.",null,null
,,,
534,[8] David Dolan Lewis. 1992. Representation and learning in information retrieval. Ph.D. Dissertation. University of Massachusetts at Amherst.,null,null
,,,
535,"[9] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR. Springer-Verlag New York, Inc., 232­241.",null,null
,,,
536,"[10] Ian Ruthven. 2003. Re-examining the potential effectiveness of interactive query expansion. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM, 213­220.",null,null
,,,
537,"[11] Yang Song and Li-wei He. 2010. Optimal rare query suggestion with implicit user feedback. In Proceedings of WWW. ACM, 901­910.",null,null
,,,
538,"[12] Xuanhui Wang and ChengXiang Zhai. 2008. Mining term association patterns from search logs for effective query reformulation. In Proceedings of CIKM. ACM, 479­488.",null,null
,,,
539,"[13] ChengXiang Zhai. 2016. Towards a game-theoretic framework for text data retrieval. IEEE Data Eng. Bull. 39, 3",null,null
,,,
540,,null,null
,,,
541,1224,null,null
,,,
542,,null,null
,,,
543,,null,null

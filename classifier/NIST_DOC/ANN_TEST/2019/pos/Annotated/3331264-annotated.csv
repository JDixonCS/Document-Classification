,sentence,label,data
,,,
0,Session 9C: Learning to Rank 2,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Variance Reduction in Gradient Exploration for Online Learning to Rank,null,null
,,,
5,,null,null
,,,
6,"Huazheng Wang, Sonwoo Kim, Eric McCord-Snook, Qingyun Wu, Hongning Wang",null,null
,,,
7,"Department of Computer Science, University of Virginia Charlottesville, VA 22904, USA",null,null
,,,
8,"{hw7ww,sak2m,esm7ky,qw2ky,hw5x}@virginia.edu",null,null
,,,
9,,null,null
,,,
10,ABSTRACT,null,null
,,,
11,Online Learning to Rank,null,null
,,,
12,"In this work, we aim at reducing the variance of gradient estimation in OL2R algorithms. We project the selected updating direction",null,null
,,,
13,CCS CONCEPTS,null,null
,,,
14,· Information systems  Learning to rank; · Theory of computation  Online learning algorithms;,null,null
,,,
15,KEYWORDS,null,null
,,,
16,Online learning to rank; Dueling bandit; Variance Reduction,null,null
,,,
17,"ACM Reference Format: Huazheng Wang, Sonwoo Kim, Eric McCord-Snook, Qingyun Wu, Hongning Wang. 2019. Variance Reduction in Gradient Exploration for Online Learning to Rank . In Proceedings of the 42nd International ACM SIGIR",null,null
,,,
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331264",null,null
,,,
19,,null,null
,,,
20,Conference on Research and Development in Information Retrieval,null,null
,,,
21,1 INTRODUCTION,null,null
,,,
22,Online Learning to Rank,null,null
,,,
23,"One strain of OL2R algorithms, represented by Dueling Bandit Gradient Descent",null,null
,,,
24,"Recently, several works in OL2R have realized this deficiency of gradient exploration in DBGD, and propose various types of solutions to improve its learning efficiency. One type of studies explore multiple random directions in each iteration of model update. Unbiased estimate of gradient is maintained in this type of revisions of DBGD, as the directions are still uniformly sampled. Model estimation variance is expected to be reduced by testing more exploratory directions; but, in practice, as the users would only examine a finite number of documents under each query",null,null
,,,
25,,null,null
,,,
26,835,null,null
,,,
27,,null,null
,,,
28,Session 9C: Learning to Rank 2,null,null
,,,
29,,null,null
,,,
30,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
31,,null,null
,,,
32,"estimated gradient remains unbiased, and thus face high risk of converging towards a sub-optimal solution.",null,null
,,,
33,"Although empirically effective, previous OL2R solutions neglect an important property of click-based result utility evaluation: users only perceive utility from the documents that they actually examine. As a result, the true gradient is only revealed by features playing an essential role in ranking those examined documents under this query. Here we define essential features in ranking a particular set of documents as those features with non-zero variance among the documents. Assume in an interleaved test, one ranking feature takes a constant value in all examined documents under this query, such that it has no effect in differentiating the quality of those documents. Then, the proposed exploratory direction's contribution to the ranker update on this particular dimension cannot be justified by this test result. Random gradient exploration hence introduces an arbitrary update on this dimension, which inevitably leads to high estimation variance over time. This example can be generalized to situations where multiple",null,null
,,,
34,The above analysis suggests that an interleaved test only reveals the projection of true gradient in the spanned space of examined documents under a test query,null,null
,,,
35,2 RELATED WORK,null,null
,,,
36,One key family of OL2R methods root in Dueling Bandit Gradient Descent,null,null
,,,
37,"Recently, attempts have been made to improve the learning efficiency of DBGD-type algorithms. Schuth et al. [17] proposed a",null,null
,,,
38,,null,null
,,,
39,Multileave Gradient Descent,null,null
,,,
40,"Our solution falls into this second category of variance reduction for DBGD-type algorithms. Distinct from previous attempts to restrict gradient exploration before an interleaved test, we instead modify the selected direction after the test. As users' result examination is affected by the ranked results, which are in turn determined by the proposed exploratory directions, restricting the exploration space before the interleaved test potentially introduces bias in the subsequent interleaved test and model update. Our solution is based on the insight that only the projected true gradient in the document space can be revealed by an interleaved test. Hence, we decide to project the selected direction after each interleaved test, and thus guarantee an unbiased estimate of true gradient. Since the document space is expected to be smaller than the entire parameter space",null,null
,,,
41,3 METHOD,null,null
,,,
42,In this section we describe our proposed document space gradient projection method for online learning to rank. We first describe the problem setup in Section 3.1. And then we describe Document Space Projected Dueling Bandit Gradient Descent,null,null
,,,
43,3.1 Problem Setup,null,null
,,,
44,"The estimation of OL2R models can be formalized as a dueling bandit problem [24]. In iteration t, an OL2R algorithm receives a",null,null
,,,
45,,null,null
,,,
46,836,null,null
,,,
47,,null,null
,,,
48,Session 9C: Learning to Rank 2,null,null
,,,
49,,null,null
,,,
50,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
51,,null,null
,,,
52,"query and associated candidate documents, which are represented as a set of d-dimensional query-document pair feature vectors Xt = {x1, x2, ..., xs }. The algorithm takes two actions: first, it proposes two rankers, whose parameters are denoted as w, w ;",null,null
,,,
53,"second, it ranks the given documents with these two rankers ac-",null,null
,,,
54,,null,null
,,,
55,cordingly. An oracle,null,null
,,,
56,,null,null
,,,
57,"results and provides feedback. In practice, an interleaving method",null,null
,,,
58,,null,null
,,,
59,[15] is applied to merge the ranking lists of the two rankers and,null,null
,,,
60,,null,null
,,,
61,display the resulting ranked list to the user. User preference is in-,null,null
,,,
62,,null,null
,,,
63,"ferred from the click feedback. Thus, the ranker that contributes more clicked documents is preferred. We denote w  w  for the event that w is preferred over w . The comparison between two",null,null
,,,
64,individual rankers is determined independently of other comparisons performed before with a probability P,null,null
,,,
65,We quantify the performance of an online learning algorithm,null,null
,,,
66,,null,null
,,,
67,using cumulative regret defined as follows:,null,null
,,,
68,,null,null
,,,
69,T,null,null
,,,
70,,null,null
,,,
71,R(T ) = ft,null,null
,,,
72,,null,null
,,,
73,-1,null,null
,,,
74,,null,null
,,,
75,t =1,null,null
,,,
76,,null,null
,,,
77,"where wt and wt are rankers compared at time t, and w is the best",null,null
,,,
78,,null,null
,,,
79,"ranker in ground-truth. As a result, the distinguishability measure",null,null
,,,
80,ft,null,null
,,,
81,,null,null
,,,
82,ft,null,null
,,,
83,,null,null
,,,
84,decreasing to zero over time.,null,null
,,,
85,,null,null
,,,
86,"In this work, we make the following assumptions similar to [24].",null,null
,,,
87,We assume an unknown utility function vt,null,null
,,,
88,A link function  describes the probabilistic comparison of utili-,null,null
,,,
89,,null,null
,,,
90,"ties of two rankers as,",null,null
,,,
91,,null,null
,,,
92,Pt w  w  = ft,null,null
,,,
93,,null,null
,,,
94,"The link function should be rotation-symmetric, which means",null,null
,,,
95,(x) = 1 - ,null,null
,,,
96,"cumulative probability distribution function. For example, a com-",null,null
,,,
97,mon choice of link function is the standard logistic function ,null,null
,,,
98,1,null,null
,,,
99,"1+exp(-x) , which satisfies all the assumptions.",null,null
,,,
100,,null,null
,,,
101,3.2 Document Space Projected Dueling Bandit Gradient Descent,null,null
,,,
102,We describe our proposed Document Space Projected Dueling Ban-,null,null
,,,
103,dit Gradient Descent,null,null
,,,
104,,null,null
,,,
105,Algorithm 1 Document Space Projected Dueling Bandit Gradient,null,null
,,,
106,,null,null
,,,
107,Descent,null,null
,,,
108,,null,null
,,,
109,"1: Inputs:  ,",null,null
,,,
110,,null,null
,,,
111,2: Initiate w1 = sample_unit_vector() 3: for t = 1 to T do,null,null
,,,
112,,null,null
,,,
113,"4: Receive query Xt = {x1, x2, ..., xs } 5: ut = sample_unit_vector() 6: wt = wt + ut 7: Generate ranked lists l(Xt , wt ), l(Xt , wt) 8: Set Lt = Interleave {l(Xt , wt ), l(Xt , wt)} , and present Lt",null,null
,,,
114,,null,null
,,,
115,to user,null,null
,,,
116,,null,null
,,,
117,"9: Receive click positions Ct on Lt , and infer click credits",null,null
,,,
118,,null,null
,,,
119,10:00,null,null
,,,
120,,null,null
,,,
121,"{ct , ct } if ct",null,null
,,,
122,,null,null
,,,
123,ct,null,null
,,,
124,,null,null
,,,
125,then,null,null
,,,
126,,null,null
,,,
127,11:00,null,null
,,,
128,,null,null
,,,
129,wt +1 = wt,null,null
,,,
130,,null,null
,,,
131,12: else,null,null
,,,
132,,null,null
,,,
133,13:00,null,null
,,,
134,,null,null
,,,
135,"Based on Ct , infer user examined top mt documents in",null,null
,,,
136,,null,null
,,,
137,Lt .,null,null
,,,
138,,null,null
,,,
139,14:00,null,null
,,,
140,,null,null
,,,
141,Solve the orthogonal projection matrix At for document,null,null
,,,
142,,null,null
,,,
143,"space St = span({xLt,1, xLt,2, ..., xLt,mt }).",null,null
,,,
144,,null,null
,,,
145,15:00,null,null
,,,
146,,null,null
,,,
147,Project ut onto St by t = Atut,null,null
,,,
148,,null,null
,,,
149,16:00,null,null
,,,
150,,null,null
,,,
151,wt +1 = wt + t,null,null
,,,
152,,null,null
,,,
153,17: end if,null,null
,,,
154,,null,null
,,,
155,18: end for,null,null
,,,
156,,null,null
,,,
157,rankers,null,null
,,,
158,as Team Draft Interleaving [15] or Probabilistic Interleaving [8]. The,null,null
,,,
159,user examines the result list and provides implicit click feedback,null,null
,,,
160,to indicate their relevance evaluation of the results. The interleav-,null,null
,,,
161,ing method uses this implicit feedback to infer which ranker is,null,null
,,,
162,preferred by the user. If the exploratory ranker is preferred,null,null
,,,
163,"wins the duel), previous DBGD-style algorithms update the current ranker by wt+1 = wt + ut , where  is the learning rate; otherwise the current ranker stays intact. This gradient exploration strategy",null,null
,,,
164,"yields an unbiased estimate of the true gradient [5], in terms of",null,null
,,,
165,"expectation. However, since the exploratory gradient ut is required to be uni-",null,null
,,,
166,"formly sampled from the entire d dimensional unit sphere Sd-1, the model update suffers from high variance in its gradient estimation, especially when d is large, as in practice. Various improvements to this issue have been proposed in the past, but they still introduce",null,null
,,,
167,"other difficulties, such as variance and bias trade-off [7, 12, 20], and",null,null
,,,
168,"test sensitivity and efficiency [18, 25].",null,null
,,,
169,Unlike previous works that reduce the sampling space of gradi-,null,null
,,,
170,"ent exploration before the interleaved test [7, 12, 20], we change",null,null
,,,
171,the winning direction after the test. The key insight is that only the projected true gradient in the spanned space of examined documents under query Xt,null,null
,,,
172,,null,null
,,,
173,837,null,null
,,,
174,,null,null
,,,
175,Session 9C: Learning to Rank 2,null,null
,,,
176,,null,null
,,,
177,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
178,,null,null
,,,
179,ut gt,null,null
,,,
180,,null,null
,,,
181,S t',null,null
,,,
182,,null,null
,,,
183,gt',null,null
,,,
184,,null,null
,,,
185,w tS,null,null
,,,
186,t,null,null
,,,
187,,null,null
,,,
188,w t',null,null
,,,
189,,null,null
,,,
190,w*,null,null
,,,
191,,null,null
,,,
192,u t',null,null
,,,
193,,null,null
,,,
194,w,null,null
,,,
195,,null,null
,,,
196,w,null,null
,,,
197,,null,null
,,,
198,Figure 1: Illustration of model update for DBGD-DSP in a,null,null
,,,
199,"three dimensional space. Dashed lines represent the trajectory of DBGD following different update directions. ut is the selected direction by DBGD, which is in the 3-d space. Red bases present the document space St on a 2-d plane. ut is projected onto St to become t for model update.",null,null
,,,
200,,null,null
,,,
201,"the document space St = span{x1, ..xm } and the other component ut - t that is orthogonal to document space St . The orthogonal component ut -t does not affect the ranking among the examined documents, i.e.",null,null
,,,
202,guarantee is only obtained in expectation. The variance could po-,null,null
,,,
203,"tentially be large: for example, the blue and purple updating traces",null,null
,,,
204,"slow down model convergence, when the number of observations",null,null
,,,
205,is finite.,null,null
,,,
206,"As shown in line 14 to 16 of Algorithm 1, we solve for the orthogonal projection matrix At of document space St , and project the selected direction ut onto the document space St after each interleaved test. We leave the detailed design of constructing document space and solving projection matrix At in Section 3.5. Before that, we first rigorously prove the projection maintains an unbiased",null,null
,,,
207,estimate of true gradient in Section 3.3. Since the document space is,null,null
,,,
208,"constructed only by the examined documents, the rank of document",null,null
,,,
209,space is expected to be smaller than the entire parameter space.,null,null
,,,
210,This directly leads to lower variance and faster model convergence.,null,null
,,,
211,"We show that our document space projection reduces the variance of gradient estimation from d to Rank(At ) in Section 3.4, and then analyze its benefit for regret reduction from low-variance gradient",null,null
,,,
212,estimation.,null,null
,,,
213,,null,null
,,,
214,3.3 Unbiasedness of Gradient Estimation,null,null
,,,
215,,null,null
,,,
216,We now prove that our document space projected gradient is an,null,null
,,,
217,,null,null
,,,
218,unbiased estimate of true gradient in the sense of expectation [24]. We define Zt,null,null
,,,
219,,null,null
,,,
220,Zt,null,null
,,,
221,,null,null
,,,
222,1 w.p. 1 - Pt,null,null
,,,
223,,null,null
,,,
224,Then the gradient used for model update in DBGD-DSP,null,null
,,,
225,,null,null
,,,
226,ht = -Zt,null,null
,,,
227,,null,null
,,,
228,-2,null,null
,,,
229,,null,null
,,,
230,Note that by adding a negative sign we view our model update as online gradient descent wt +1 = wt - t .,null,null
,,,
231,We now show in the following theorem that this is an unbiased,null,null
,,,
232,gradient estimation of true gradient. By defining a smoothed version of ft as f^t,null,null
,,,
233,"Theorem 3.1. The projected gradient t in DBGD-DSP is an unbiased estimate of true gradient, i.e.,",null,null
,,,
234,,null,null
,,,
235,E[ht ],null,null
,,,
236,,null,null
,,,
237,=,null,null
,,,
238,,null,null
,,,
239,d,null,null
,,,
240,,null,null
,,,
241,,null,null
,,,
242,,null,null
,,,
243,f^t,null,null
,,,
244,,null,null
,,,
245,(w,null,null
,,,
246,,null,null
,,,
247,),null,null
,,,
248,,null,null
,,,
249,-3,null,null
,,,
250,,null,null
,,,
251,over random unit vector ut .,null,null
,,,
252,,null,null
,,,
253,"Proof. Based on the Lemma 1 of [24], we have",null,null
,,,
254,,null,null
,,,
255,E [ht ] = E [-Zt,null,null
,,,
256,,null,null
,,,
257,E[ht ] = Eut Sd-1 [ft,null,null
,,,
258,,null,null
,,,
259,= Eut Sd-1 [Ft,null,null
,,,
260,,null,null
,,,
261,=,null,null
,,,
262,,null,null
,,,
263,d,null,null
,,,
264,,null,null
,,,
265,Eut Bd [Ft,null,null
,,,
266,,null,null
,,,
267,+ ut )ut ],null,null
,,,
268,,null,null
,,,
269,=,null,null
,,,
270,,null,null
,,,
271,d,null,null
,,,
272,,null,null
,,,
273,F^t,null,null
,,,
274,,null,null
,,,
275,=,null,null
,,,
276,,null,null
,,,
277,d,null,null
,,,
278,,null,null
,,,
279,At f^t,null,null
,,,
280,,null,null
,,,
281,=,null,null
,,,
282,,null,null
,,,
283,d,null,null
,,,
284,,null,null
,,,
285,f^t,null,null
,,,
286,,null,null
,,,
287,where the fourth equality is based on Stokes' Theorem. The last,null,null
,,,
288,,null,null
,,,
289,equality holds because gradient f^t,null,null
,,,
290,,null,null
,,,
291,"St , and thus projecting it by At maps back to itself.",null,null
,,,
292,,null,null
,,,
293,,null,null
,,,
294,,null,null
,,,
295,"The guarantee of unbiased gradient estimation is a major advantage of our proposed document space gradient projection method, compared with previous attempts to reduce the gradient exploration space, such as Oosterhuis et. al [12] and Wang et al. [20]. Our method enjoys reduced variance of gradient estimate",null,null
,,,
296,,null,null
,,,
297,3.4 Regret Analysis of DBGD-DSP,null,null
,,,
298,"We now analyze the regret of our proposed DBGD-DSP algorithm, starting with its variance of gradient update.",null,null
,,,
299,,null,null
,,,
300,Lemma 3.2. The variance of gradient update in DBGD-DSP is bounded by,null,null
,,,
301,,null,null
,,,
302,E[|ht |2] = Eut Sd-1,null,null
,,,
303,,null,null
,,,
304,| - Zt,null,null
,,,
305,,null,null
,,,
306,Rank(At ) . d,null,null
,,,
307,,null,null
,,,
308,838,null,null
,,,
309,,null,null
,,,
310,Session 9C: Learning to Rank 2,null,null
,,,
311,,null,null
,,,
312,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
313,,null,null
,,,
314,Proof.,null,null
,,,
315,,null,null
,,,
316,E[|ht |2] = Eut | - Zt,null,null
,,,
317,Eut |At ut |2 = Eut,null,null
,,,
318,,null,null
,,,
319,#NAME?,null,null
,,,
320,,null,null
,,,
321,At,null,null
,,,
322,,null,null
,,,
323,1 d,null,null
,,,
324,,null,null
,,,
325,I,null,null
,,,
326,,null,null
,,,
327,At,null,null
,,,
328,,null,null
,,,
329,=,null,null
,,,
330,,null,null
,,,
331,1 tr,null,null
,,,
332,d,null,null
,,,
333,,null,null
,,,
334,At At,null,null
,,,
335,,null,null
,,,
336,1 = d tr,null,null
,,,
337,,null,null
,,,
338,=,null,null
,,,
339,,null,null
,,,
340,Rank(At ) d,null,null
,,,
341,,null,null
,,,
342,where tr(·) denotes the matrix trace operation. The sixth equality,null,null
,,,
343,,null,null
,,,
344,"holds because ut is uniformly sampled from a unit sphere, and",null,null
,,,
345,,null,null
,,,
346,its covariance matrix Eut,null,null
,,,
347,,null,null
,,,
348,ut ut,null,null
,,,
349,,null,null
,,,
350,is,null,null
,,,
351,,null,null
,,,
352,1 d,null,null
,,,
353,,null,null
,,,
354,I,null,null
,,,
355,,null,null
,,,
356,.,null,null
,,,
357,,null,null
,,,
358,Since,null,null
,,,
359,,null,null
,,,
360,At,null,null
,,,
361,,null,null
,,,
362,is,null,null
,,,
363,,null,null
,,,
364,an,null,null
,,,
365,,null,null
,,,
366,orthogonal,null,null
,,,
367,,null,null
,,,
368,"projection matrix, the eighth equality holds for At At = At .",null,null
,,,
369,,null,null
,,,
370,Remark. The variance of gradient update in DBGD [24] is bounded by Eut | - Zt,null,null
,,,
371,,null,null
,,,
372,Comparing the variance of gradient update in DBGD-DSP with,null,null
,,,
373,,null,null
,,,
374,"DBGD,",null,null
,,,
375,,null,null
,,,
376,our,null,null
,,,
377,,null,null
,,,
378,method,null,null
,,,
379,,null,null
,,,
380,reduces,null,null
,,,
381,,null,null
,,,
382,the,null,null
,,,
383,,null,null
,,,
384,variance,null,null
,,,
385,,null,null
,,,
386,from,null,null
,,,
387,,null,null
,,,
388,1,null,null
,,,
389,,null,null
,,,
390,to,null,null
,,,
391,,null,null
,,,
392,Rank(At d,null,null
,,,
393,,null,null
,,,
394,),null,null
,,,
395,,null,null
,,,
396,.,null,null
,,,
397,,null,null
,,,
398,Since,null,null
,,,
399,,null,null
,,,
400,"the dimension of projection matrix At is d-by-d, we have Rank(At )",null,null
,,,
401,,null,null
,,,
402,"d, which guarantees the reduction of variance in DBGD-DSP com-",null,null
,,,
403,,null,null
,,,
404,"paring to that in DBGD. The rank of At is also bounded by the number of examined documents mt , since document space St is constructed by these mt examined documents. In practice, users",null,null
,,,
405,,null,null
,,,
406,"would only examine a handful of documents [4, 9], while the rank-",null,null
,,,
407,,null,null
,,,
408,"ing feature dimension is expected to be much larger. We argue that mt  d, such that our document space projection achieves considerable variance reduction.",null,null
,,,
409,,null,null
,,,
410,The significance of this variance reduction can be intuitively,null,null
,,,
411,,null,null
,,,
412,understood from Figure 1: though different traces of model update,null,null
,,,
413,,null,null
,,,
414,"would eventually lead to the same converged model, if one has a",null,null
,,,
415,,null,null
,,,
416,"sufficiently large amount of interactions with users, the one with",null,null
,,,
417,,null,null
,,,
418,lower variance would always require less observations. A faster,null,null
,,,
419,,null,null
,,,
420,"converging algorithm leads to user satisfaction earlier. Next, we",null,null
,,,
421,,null,null
,,,
422,verify this benefit by proving the reduction of regret introduced by,null,null
,,,
423,,null,null
,,,
424,the reduced variance in gradient estimation.,null,null
,,,
425,,null,null
,,,
426,Theorem 3.3. By setting,null,null
,,,
427,,null,null
,,,
428,,null,null
,,,
429,,null,null
,,,
430,m,null,null
,,,
431,,null,null
,,,
432,=,null,null
,,,
433,,null,null
,,,
434,max,null,null
,,,
435,t,null,null
,,,
436,,null,null
,,,
437,mt,null,null
,,,
438,,null,null
,,,
439,",",null,null
,,,
440,,null,null
,,,
441,,null,null
,,,
442,,null,null
,,,
443,=,null,null
,,,
444,,null,null
,,,
445,"2Rm ,  13LT 1/4",null,null
,,,
446,,null,null
,,,
447,=,null,null
,,,
448,,null,null
,,,
449,Rm,null,null
,,,
450,,null,null
,,,
451,",",null,null
,,,
452,,null,null
,,,
453,T,null,null
,,,
454,,null,null
,,,
455,the expected regret of DBGD-DSP as defined in Eq,null,null
,,,
456,,null,null
,,,
457,"by,",null,null
,,,
458,,null,null
,,,
459,"E[Re]  2T T 3/426RmL,",null,null
,,,
460,,null,null
,,,
461,-4,null,null
,,,
462,,null,null
,,,
463,where,null,null
,,,
464,,null,null
,,,
465,T,null,null
,,,
466,,null,null
,,,
467,=,null,null
,,,
468,,null,null
,,,
469,,null,null
,,,
470,,null,null
,,,
471,L,null,null
,,,
472,,null,null
,,,
473,13LT,null,null
,,,
474,,null,null
,,,
475,4-Jan,null,null
,,,
476,,null,null
,,,
477,,null,null
,,,
478,,null,null
,,,
479,L 13LT 1/4 - Lv L2 2Rm,null,null
,,,
480,,null,null
,,,
481,"The proof is obtained by extending Theorem 2 in [24]. We omit the details due to space limit, and emphasize that the key difference is introduced by replacing variance of gradient estimation from",null,null
,,,
482,,null,null
,,,
483,Eut | - Zt,null,null
,,,
484,,null,null
,,,
485,the the,null,null
,,,
486,,null,null
,,,
487,variance regret of,null,null
,,,
488,,null,null
,,,
489,of gradient estimation is reduced,null,null
,,,
490,DBGD can be reduced from O( dT,null,null
,,,
491,,null,null
,,,
492,from 1 3/4) to,null,null
,,,
493,,null,null
,,,
494,"Oto(RmanTkd(3A/t4)),,",null,null
,,,
495,,null,null
,,,
496,where m is the maximum number of documents included in a docu-,null,null
,,,
497,,null,null
,,,
498,"ment space under a single query. Again, as the number of included",null,null
,,,
499,,null,null
,,,
500,ranking features is oftentimes much larger than the number of doc-,null,null
,,,
501,,null,null
,,,
502,"uments a user would examine under a single query, the reduction of",null,null
,,,
503,,null,null
,,,
504,"regret is considerable. Moreover, as the reduction of variance from",null,null
,,,
505,,null,null
,,,
506,our project-based method is independent from the way about how,null,null
,,,
507,,null,null
,,,
508,"the proposal directions are generated, our method can be generally",null,null
,,,
509,,null,null
,,,
510,applied to most existing DBGD-type OL2R algorithms to improve,null,null
,,,
511,,null,null
,,,
512,their learning convergence.,null,null
,,,
513,,null,null
,,,
514,3.5 Practical Treatments of Document Space Projection,null,null
,,,
515,Now we discuss several practical treatments of our proposed Docu-,null,null
,,,
516,"ment Space Projection method, including the construction of docu-",null,null
,,,
517,ment space and orthogonal projection matrix.,null,null
,,,
518,"In our theoretical analysis, we have assumed the knowledge of",null,null
,,,
519,users' examined documents and corresponding projection matrix.,null,null
,,,
520,"However, in practice, a user's result examination is unobserved.",null,null
,,,
521,A rich body of research has been developed to perform statistical,null,null
,,,
522,"inference of it, collectively known as click modeling [3, 4]. Any",null,null
,,,
523,of these existing click modeling solutions can be plugged into our,null,null
,,,
524,"solution framework, i.e., line 13 of Algorithm 1. In this work, we",null,null
,,,
525,"simply follow [9] to infer user examination by the last clicked position: given the click position list Ct , we use the last clicked position cl,t to approximate the last examined position Mt by setting Mt = cl,t + k, where k is a hyper-parameter. Based on sequential examination hypothesis of click modeling, every document before the last clicked position is examined, and we use k to approximate the number of positions following the last clicked position that",null,null
,,,
526,was still examined. We leave more comprehensive study of click,null,null
,,,
527,modeling in our solution as future work.,null,null
,,,
528,"The above treatment provides a reasonable inference of examined documents. However, it requires a careful choice of k for each query",null,null
,,,
529,at risk of introducing bias in gradient projection. To avoid bias in,null,null
,,,
530,"constructing the document space, we also consider adding histori-",null,null
,,,
531,"cally examined documents to the current query's document space. Specifically, we add r recently examined documents to the current document space St to compensate the potentially overlooked examined documents in the current query.",null,null
,,,
532,"In line 14 of Algorithm 1, we solve the orthogonal projection matrix At of document space St . At could be computed by several methods. Denote Dt as a d-by-mt matrix where each column is the feature vector for an examined document. One can use QR decom-",null,null
,,,
533,position or Singular Value Decomposition,null,null
,,,
534,efficient large-scale implementation. But the choice for the con-,null,null
,,,
535,struction of this project matrix does not affect the convergence nor,null,null
,,,
536,unbiasedness of our proposed solution.,null,null
,,,
537,,null,null
,,,
538,839,null,null
,,,
539,,null,null
,,,
540,Session 9C: Learning to Rank 2,null,null
,,,
541,,null,null
,,,
542,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
543,,null,null
,,,
544,4 EXPERIMENTS,null,null
,,,
545,"To demonstrate our proposed Document Space Projection method's empirical efficacy, we compare the performance of several state-ofthe-art OL2R algorithms on five public learning to rank datasets, with and without our document space projection method applied.",null,null
,,,
546,4.1 Experiment Setup,null,null
,,,
547,"· Datasets. We tested our algorithms and the baselines on five benchmark datasets: including MQ2007, MQ2008, NP2003 [11], MSLR-WEB10K [14], and the Yahoo! Learning to Rank Challenge dataset [2]. In each of the five datasets, each query-document pair is encoded as a vector of ranking features. These features include PageRank, TF.IDF, Okapi-BM25, URL length, language model score, and many more varied by dataset.",Y,
,,,
548,"The MQ2007 and MQ2008 datasets are collected from the 2007 and 2008 Million Query track at TREC [19]. MQ2007 contains about 1700 queries, and MQ2008 contains about 800 queries, which represent a mix of informational and navigational search intents. They both have 46-dimensional feature vectors to represent querydocument pairs, and the document relevance are labeled in three grades: 0",Y,
,,,
549,"The NP2003 dataset also comes from the TREC Web track, consisting of queries crawled from the .gov domain. It is comprised of about 150 navigational-focused queries, with over 1000 document relevance assessments per query. It uses 64 ranking features, and the document relevance labels are binary",Y,
,,,
550,"The MSLR-WEB10K dataset was released by Microsoft in 2010, and consists of 10,000 queries with relevance assessments coming from a labeling set from the Microsoft Bing search engine. It has 136 ranking features, and the relevance judgments range from 0",Y,
,,,
551,"The Yahoo! Learning to Rank Challenge dataset was also released in 2010, as an effort on part of Yahoo! to promote the dataset as well as research into better learning to rank algorithms. The dataset contains about 36,000 queries, 883,000 assessed documents, and 700 ranking featuress. Again, the relevance judgments range from 0",Y,
,,,
552,"This diversity in the structure of the datasets that we chose to test on helps us to evaluate our algorithms more holistically. While small, the MQ2007 and MQ2008 sets have been around for a long time and have a good mix of query types. NP2003 gives us insight into how the algorithms perform on navigational search intents specifically, which are markedly different in nature from informational search intents. MSLR-WEB10K and the Yahoo! dataset are large-scale datasets used by actual commercial search engines, which give us a better understanding of how the algorithms perform in practice. Since each dataset was split into training, testing, and validation subsets, we used the training sets for online experiments to measure cumulative performance, and used the testing sets for evaluating offline performance. · Simulated User Interactions. Based on an online learning to rank framework proposed in [13], we use the standard setup to simulate user interactions. Within this framework, we used the Cascade Click Model to simulate user click behavior. This model assumes that a user interacts with a set of search results by linearly scanning the list from top and making a decision for each document",Y,
,,,
553,,null,null
,,,
554,Table 1: Configurations of simulation click models.,null,null
,,,
555,,null,null
,,,
556,Click Probability,null,null
,,,
557,,null,null
,,,
558,Stop Probability,null,null
,,,
559,,null,null
,,,
560,R 0 123 4 01234,null,null
,,,
561,,null,null
,,,
562,Per 0.0 0.2 0.4 0.8 1.0 0.0 0.0 0.0 0.0 0.0,null,null
,,,
563,,null,null
,,,
564,Nav 0.05 0.3 0.5 0.7 0.95 0.2 0.3 0.5 0.7 0.9,null,null
,,,
565,,null,null
,,,
566,Inf 0.4 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5,null,null
,,,
567,,null,null
,,,
568,"as to whether or not to click. In the model, the probability of a click for a given document is conditioned on the relevance label of that document, as a user is expected to be more likely to click on relevant documents. After evaluating each document, the user must decide whether or not to continue perusing the list. This decision's probability distribution is again conditioned on the relevance of the current document, as a user is more likely to stop looking through the results if he/she has already satisfied their information need. These aforementioned probabilities can be altered to simulate different types of users and interactions.",null,null
,,,
569,"As illustrated in Table 1, we use three different click model probability configurations to represent three different types of users. First, we have the perfect user, who clicks on all relevant documents and does not stop browsing until they have visited all of the documents. This type of users contribute the least noise, as they make no mistakes and the feedback is entirely accurate. Second, we have the navigational user, who is very likely to click on the first highly relevant document that he/she sees and stops there. Third, we have the informational user, who, in his/her search for information, sometimes clicks on irrelevant documents, and as such contributes a significant amount of noise in click feedback. · Evaluation Metrics. As set forth in [16], cumulative",null,null
,,,
570,Q1: Can our proposed Document Space Projection method consistently improve the performance of state-of-the-art OL2R algorithms?,null,null
,,,
571,Q2: Do gradients rectified by our document space projection explore the gradient space more efficiently?,null,null
,,,
572,Q3: How do different hyper-parameter settings alter the performance of our document space projection?,null,null
,,,
573,· Baseline Algorithms. We choose the following three state-ofthe-art OL2R algorithms as our baselines for comparison:,null,null
,,,
574,,null,null
,,,
575,840,null,null
,,,
576,,null,null
,,,
577,Session 9C: Learning to Rank 2,null,null
,,,
578,,null,null
,,,
579,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
580,,null,null
,,,
581,NDCG NDCG NDCG,null,null
,,,
582,,null,null
,,,
583,0.72 perfect,null,null
,,,
584,,null,null
,,,
585,0.7,null,null
,,,
586,,null,null
,,,
587,0.68,null,null
,,,
588,,null,null
,,,
589,0.66,null,null
,,,
590,,null,null
,,,
591,0.64,null,null
,,,
592,,null,null
,,,
593,0.62 0,null,null
,,,
594,,null,null
,,,
595,2000,null,null
,,,
596,,null,null
,,,
597,4000,null,null
,,,
598,,null,null
,,,
599,6000,null,null
,,,
600,,null,null
,,,
601,Impressions,null,null
,,,
602,,null,null
,,,
603,(a) Perfect,null,null
,,,
604,,null,null
,,,
605,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
606,,null,null
,,,
607,8000,null,null
,,,
608,,null,null
,,,
609,10000,null,null
,,,
610,,null,null
,,,
611,0.72 navigational,null,null
,,,
612,0.7,null,null
,,,
613,,null,null
,,,
614,0.68,null,null
,,,
615,,null,null
,,,
616,0.66,null,null
,,,
617,,null,null
,,,
618,0.64,null,null
,,,
619,,null,null
,,,
620,0.62,null,null
,,,
621,,null,null
,,,
622,0.6,null,null
,,,
623,,null,null
,,,
624,0.58 0,null,null
,,,
625,,null,null
,,,
626,2000,null,null
,,,
627,,null,null
,,,
628,4000,null,null
,,,
629,,null,null
,,,
630,6000,null,null
,,,
631,,null,null
,,,
632,Impressions,null,null
,,,
633,,null,null
,,,
634,8000,null,null
,,,
635,,null,null
,,,
636,(b) Navigational,null,null
,,,
637,,null,null
,,,
638,10000,null,null
,,,
639,,null,null
,,,
640,0.70 informational,null,null
,,,
641,0.68,null,null
,,,
642,,null,null
,,,
643,0.66,null,null
,,,
644,,null,null
,,,
645,0.64,null,null
,,,
646,,null,null
,,,
647,0.62,null,null
,,,
648,,null,null
,,,
649,0.6,null,null
,,,
650,,null,null
,,,
651,0.58,null,null
,,,
652,,null,null
,,,
653,0.56 0,null,null
,,,
654,,null,null
,,,
655,2000,null,null
,,,
656,,null,null
,,,
657,4000,null,null
,,,
658,,null,null
,,,
659,6000,null,null
,,,
660,,null,null
,,,
661,Impressions,null,null
,,,
662,,null,null
,,,
663,8000,null,null
,,,
664,,null,null
,,,
665,(c) Informational,null,null
,,,
666,,null,null
,,,
667,10000,null,null
,,,
668,,null,null
,,,
669,Figure 2: Offline NDCG@10 on Yahoo! dataset.,null,null
,,,
670,,null,null
,,,
671,- DBGD [24]: A single direction uniformly sampled from the whole parameter space is explored.,null,null
,,,
672,"- MGD [17]: Multiple directions are explored in one iteration to reduce the gradient estimation variance. Multileaving is used to compare multiple rankers. If there is a tie, the model updates towards the mean of all winners.",null,null
,,,
673,- NSGD[20]: Multiple directions are sampled from the null space of previously poorly performing gradients. Ties are broken by evaluating the tied candidate rankers on a recent set of difficult queries.,null,null
,,,
674,"We apply our proposed Document Space Projection to the baseline algorithms, and compare them with DBGD-DSP, MGD-DSP and NSGD-DSP, respectively.",null,null
,,,
675,4.2 Performance of Document Space Projection,null,null
,,,
676,"We begin our experimental analysis by answering our first evaluation question. We compared all algorithms over 3 click models and 5 datasets. We set the hyper-parameters of DBGD, MGD and NSGD according to their original papers. Following [17, 24], we set the exploration step size  to 1 and learning rate  to 0.1. Both MGD and NSGD explore 9 proposal directions in one iteration. For our document space projection method, we consider k = 3 documents following the last clicked position as examined documents, and add r = 10 recently examined documents into document space St . We use SVD to solve for orthonormal basis Vt of the document space St , and compute the projection matrix by At = VtVt.",null,null
,,,
677,"We reported the offline NDCG@10 and online cumulative NDCG @10 after 10,000 iterations in Table 2 and Table 3. Due to space limit, we only reported the offline performance during the 10,000 iterations over 3 click models on Yahoo dataset, a large-scale realworld L2R dataset with 700 ranking features, in Figure 2. MGD improves the online performance over DBGD by exploring multiple rankers simultaneously, and NSGD further improves over MGD by exploring gradients in a constrained subspace, as shown in Table 2. We observe that our proposed document space projection method consistently improves the online performance of all baseline algorithms. Recall that in Section 3.4 our theoretical analysis suggested that document space projection reduces the gradient estimation variance and improves the regret",null,null
,,,
678,,null,null
,,,
679,dimensions,null,null
,,,
680,"From Figure 2 and Table 3 we notice that document space projection mostly improves offline performance over baseline algorithms. Figure 2 shows that document space projection significantly accelerates the convergence rate over the baseline algorithms, because of the reduced variance in gradient estimation. We also observe that applying document space projection under the perfect click model may lead to degraded performance, for example DBGD on MQ2007 and Yahoo dataset. This is because document space projection guarantees an unbiased gradient estimation under the assumption of known result examinations, as discussed in Section 3.3. However, since in practice a user's result examination is unobserved, we approximated the examined documents by including all documents before the last clicked position and k additional documents after the last clicked position. The perfect click model is an ideal case that users' stop probability is set to 0.0",null,null
,,,
681,4.3 Analysis of Document Space Projection,null,null
,,,
682,"To answer the second evaluation question, we design two experiments to show the effectiveness of document space projected gradient. In the first experiment, we study the utility of document space projected gradient. We compare the ranking performance of linearly interpolating the unrectified direction ut and its document space projected version t , i.e., t +",null,null
,,,
683,,null,null
,,,
684,841,null,null
,,,
685,,null,null
,,,
686,Session 9C: Learning to Rank 2,null,null
,,,
687,,null,null
,,,
688,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
689,,null,null
,,,
690,"Table 2: Online NDCG@10, standard deviation and relative improvement of document space projection of each algorithm after 10,000 queries.",null,null
,,,
691,,null,null
,,,
692,Click Model Algorithm MQ2007,Y,
,,,
693,,null,null
,,,
694,MQ2008,Y,
,,,
695,,null,null
,,,
696,MSLR-WEB10K,Y,
,,,
697,,null,null
,,,
698,NP2003,Y,
,,,
699,,null,null
,,,
700,Yahoo,Y,
,,,
701,,null,null
,,,
702,Perfect,null,null
,,,
703,,null,null
,,,
704,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
705,,null,null
,,,
706,679.3,null,null
,,,
707,,null,null
,,,
708,847.1,null,null
,,,
709,,null,null
,,,
710,532.2,null,null
,,,
711,,null,null
,,,
712,1130.2,null,null
,,,
713,,null,null
,,,
714,1165.5,null,null
,,,
715,,null,null
,,,
716,Navigational,null,null
,,,
717,,null,null
,,,
718,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
719,,null,null
,,,
720,646.1,null,null
,,,
721,,null,null
,,,
722,817.9,null,null
,,,
723,,null,null
,,,
724,517.5,null,null
,,,
725,,null,null
,,,
726,1062.3,null,null
,,,
727,,null,null
,,,
728,1133.3,null,null
,,,
729,,null,null
,,,
730,Informational,null,null
,,,
731,,null,null
,,,
732,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
733,,null,null
,,,
734,583.4,null,null
,,,
735,,null,null
,,,
736,763.9,null,null
,,,
737,,null,null
,,,
738,472.4,null,null
,,,
739,,null,null
,,,
740,849.8,null,null
,,,
741,,null,null
,,,
742,1107.3,null,null
,,,
743,,null,null
,,,
744,"Table 3: Offline NDCG@10, standard deviation and relative improvement of document space projection of each algorithm after 10,000 queries.",null,null
,,,
745,,null,null
,,,
746,Click Model Algorithm MQ2007,Y,
,,,
747,,null,null
,,,
748,MQ2008,Y,
,,,
749,,null,null
,,,
750,MSLR-WEB10K,Y,
,,,
751,,null,null
,,,
752,NP2003,Y,
,,,
753,,null,null
,,,
754,Yahoo,Y,
,,,
755,,null,null
,,,
756,Perfect,null,null
,,,
757,,null,null
,,,
758,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
759,,null,null
,,,
760,0.484,null,null
,,,
761,,null,null
,,,
762,0.683,null,null
,,,
763,,null,null
,,,
764,0.331,null,null
,,,
765,,null,null
,,,
766,0.737,null,null
,,,
767,,null,null
,,,
768,0.688,null,null
,,,
769,,null,null
,,,
770,Navigational,null,null
,,,
771,,null,null
,,,
772,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
773,,null,null
,,,
774,0.463,null,null
,,,
775,,null,null
,,,
776,0.667,null,null
,,,
777,,null,null
,,,
778,0.32,null,null
,,,
779,,null,null
,,,
780,0.728,null,null
,,,
781,,null,null
,,,
782,0.663,null,null
,,,
783,,null,null
,,,
784,Informational,null,null
,,,
785,,null,null
,,,
786,DBGD DBGD-DSP MGD MGD-DSP NSGD NSGD-DSP,null,null
,,,
787,,null,null
,,,
788,0.41,null,null
,,,
789,,null,null
,,,
790,0.641,null,null
,,,
791,,null,null
,,,
792,0.294,null,null
,,,
793,,null,null
,,,
794,0.699,null,null
,,,
795,,null,null
,,,
796,0.623,null,null
,,,
797,,null,null
,,,
798,"algorithm on MSLR-WEB10K dataset. Similar observations were obtained on other datasets, but due to space limit we have to omit those detailed results. We report the online and offline performance by varying  from 0",Y,
,,,
799,,null,null
,,,
800,"three click models when we increase , i.e., trust more on the projected direction t for model update. This confirms the effectiveness of the projected direction t within document space comparing with the unrectified direction ut from the entire parameter space. The offline performance is generally robust to the setting of  for",null,null
,,,
801,navigational and information click models. This is expected since,null,null
,,,
802,,null,null
,,,
803,842,null,null
,,,
804,,null,null
,,,
805,Session 9C: Learning to Rank 2,null,null
,,,
806,,null,null
,,,
807,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
808,,null,null
,,,
809,(a) Online performance of,null,null
,,,
810,linearly interpolating ut and its projection t,null,null
,,,
811,,null,null
,,,
812,Cosine Similarity to w*,null,null
,,,
813,NDCG,null,null
,,,
814,,null,null
,,,
815,0.5 informational,null,null
,,,
816,0.4,null,null
,,,
817,0.3,null,null
,,,
818,,null,null
,,,
819,0.75 perfect,null,null
,,,
820,0.7,null,null
,,,
821,0.65,null,null
,,,
822,,null,null
,,,
823,(b) Offline performance of,null,null
,,,
824,linearly interpolating ut and its projection t,null,null
,,,
825,,null,null
,,,
826,0.2,null,null
,,,
827,0.1,null,null
,,,
828,0.0 0,null,null
,,,
829,,null,null
,,,
830,MGD MGD-DSP,null,null
,,,
831,,null,null
,,,
832,1000,null,null
,,,
833,,null,null
,,,
834,2000,null,null
,,,
835,,null,null
,,,
836,3000,null,null
,,,
837,,null,null
,,,
838,Impressions,null,null
,,,
839,,null,null
,,,
840,4000,null,null
,,,
841,,null,null
,,,
842,5000,null,null
,,,
843,,null,null
,,,
844,0.6,null,null
,,,
845,0.55,null,null
,,,
846,0.50 0,null,null
,,,
847,,null,null
,,,
848,DBGD-DSP-GT DBGD-DSP MGD-DSP-GT MGD-DSP,null,null
,,,
849,,null,null
,,,
850,1000,null,null
,,,
851,,null,null
,,,
852,2000,null,null
,,,
853,,null,null
,,,
854,3000,null,null
,,,
855,,null,null
,,,
856,4000,null,null
,,,
857,,null,null
,,,
858,5000,null,null
,,,
859,,null,null
,,,
860,Impressions,null,null
,,,
861,,null,null
,,,
862,(c) Cosine similarity between offline best model w and online,null,null
,,,
863,document space model,null,null
,,,
864,,null,null
,,,
865,Figure 3: Analyzing Document Space Projection.,null,null
,,,
866,,null,null
,,,
867,(a) Including k documents following last clicked position,null,null
,,,
868,,null,null
,,,
869,(b) Including r recently examined documents,null,null
,,,
870,,null,null
,,,
871,Figure 4: Hyper-parameter tuning for Document Space Projection.,null,null
,,,
872,,null,null
,,,
873,both MGD and MGD-DSP are unbiased and will eventually converge to similar offline performance after sufficiently large number of iterations,null,null
,,,
874,"In the second experiment, we trained an offline LambdaRank model [1] using the complete annotated relevance labels in the largescale MSLR-WEB10K dataset. Then given this w, we compared cosine similarity between the online estimated model parameters with and without DSP in each iteration using MGD as the baseline. We show the result of first 5,000 iterations. In Figure 3",Y,
,,,
875,"To answer the third evaluation question, we compare different hyper-parameters used for constructing the document space on MSLR-WEB10K dataset. We vary k from 0 to 7 and report the result in Figure4",Y,
,,,
876,,null,null
,,,
877,construct the document space and guarantee an unbiased gradient estimate.,null,null
,,,
878,"In Figure 4(b), we vary r . As we discussed in Section 3.5, we are motivated to add recently examined documents to compensate for potentially overlooked examined documents in the current query. The effect of different choices of r is more noticeable under the perfect click model. This echoes our analysis above that under perfect click model some examined documents may be overlooked when k is not large enough. Thus correctly setting up r could reduce the bias in document space construction and compensate the final performance. From the result figure, we notice that setting r = 20 provides the best result. Under navigational and informational click models, the algorithm is generally robust to the choice of r . This is because the approximations of examined documents are already accurate with a reasonable setting of k.",null,null
,,,
879,5 CONCLUSION,null,null
,,,
880,"In this paper, we propose and develop the Document Space Projection",null,null
,,,
881,"Currently, we are using a heuristic method to construct the document space. However, we did observe that the performance of DSP varies under different click models for simulated user click feedback, i.e., different underlying examination behaviors. As for our future work, we plan to incorporate different click modeling solutions for more accurate document space construction. It would also be meaningful to study how to perform document space based exploratory direction generation, before the interleaved test. Exploratory direction pre-selection is expected to further accelerate the gradient exploration and improve user satisfaction during online learning, but we also need to ensure it is unbiased.",null,null
,,,
882,,null,null
,,,
883,843,null,null
,,,
884,,null,null
,,,
885,Session 9C: Learning to Rank 2,null,null
,,,
886,,null,null
,,,
887,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
888,,null,null
,,,
889,ACKNOWLEDGMENTS,null,null
,,,
890,We thank the anonymous reviewers for their insightful comments.,null,null
,,,
891,This work was supported in part by National Science Foundation,null,null
,,,
892,Grant IIS-1553568 and IIS-1618948 and Bloomberg Data Science,null,null
,,,
893,Ph.D. Fellowship.,null,null
,,,
894,REFERENCES,null,null
,,,
895,"[1] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581",null,null
,,,
896,[2] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Proceedings of the Learning to Rank Challenge. 1­24.,null,null
,,,
897,"[3] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model for web search ranking. In Proceedings of the 18th international conference on World wide web. ACM, 1­10.",null,null
,,,
898,"[4] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In Proceedings of the 2008 international conference on web search and data mining. ACM, 87­94.",null,null
,,,
899,"[5] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. 2005.",null,null
,,,
900,"Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 385­394. [6] Artem Grotov and Maarten de Rijke. 2016. Online learning to rank for information retrieval: SIGIR 2016 Tutorial. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 1215­ 1218.",null,null
,,,
901,"[7] Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. 2013.",null,null
,,,
902,"Reusing historical interaction data for faster online learning to rank for IR. In Proceedings of the sixth ACM international conference on WSDM. ACM, 183­192. [8] Katja Hofmann, Shimon Whiteson, and Maarten De Rijke. 2011. A probabilistic method for inferring preferences from clicks. In Proceedings of the 20th ACM international conference on Information and knowledge management. ACM, 249­ 258.",null,null
,,,
903,"[9] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2017. Accurately interpreting clickthrough data as implicit feedback. In ACM SIGIR Forum, Vol. 51. Acm, 4­11.",null,null
,,,
904,"[10] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3",null,null
,,,
905,"[11] Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li. 2007. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval, Vol. 310.",null,null
,,,
906,,null,null
,,,
907,"[12] Harrie Oosterhuis and Maarten de Rijke. 2017. Balancing Speed and Quality in Online Learning to Rank for Information Retrieval. In Proceedings of the 2017 ACM CIKM. ACM, 277­286.",null,null
,,,
908,[13] Harrie Oosterhuis and Maarten de Rijke. 2018. Differentiable Unbiased Online Learning to Rank. Proceedings of the 27th ACM International Conference on Information and Knowledge Management - CIKM '18,null,null
,,,
909,[14] Tao Qin and Tie-Yan Liu. 2013. Introducing LETOR 4.0 Datasets.,null,null
,,,
910,arXiv:cs.IR/1306.2597,null,null
,,,
911,"[15] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. 2008. How does clickthrough data reflect retrieval quality?. In Proceedings of the 17th ACM CIKM. ACM, 43­52.",null,null
,,,
912,"[16] Anne Schuth, Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Lerot: An online learning to rank framework. In Proceedings of the 2013 workshop on Living labs for information retrieval evaluation. ACM, 23­26.",null,null
,,,
913,"[17] Anne Schuth, Harrie Oosterhuis, Shimon Whiteson, and Maarten de Rijke. 2016. Multileave gradient descent for fast online learning to rank. In Proceedings of the Ninth ACM International Conference on WSDM. ACM, 457­466.",null,null
,,,
914,"[18] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten de Rijke. 2014. Multileaved comparisons for fast online evaluation. In Proceedings of the 23rd ACM CIKM. ACM, 71­80.",null,null
,,,
915,"[19] Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval. Vol. 1. MIT press Cambridge.",null,null
,,,
916,"[20] Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, and Hongn-",null,null
,,,
917,"ing Wang. 2018. Efficient exploration of gradient space for online learning to rank. In The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 145­154. [21] Xuanhui Wang, Michael Bendersky, Donald Metzler, and Marc Najork. 2016. Learning to rank with selection bias in personal search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 115­124. [22] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The k-armed dueling bandits problem. J. Comput. System Sci. 78, 5",null,null
,,,
918,"Learning more powerful test statistics for click-based retrieval evaluation. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 507­514. [24] Yisong Yue and Thorsten Joachims. 2009. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 1201­1208. [25] Tong Zhao and Irwin King. 2016. Constructing reliable gradient exploration for online learning to rank. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 1643­1652.",null,null
,,,
919,,null,null
,,,
920,844,null,null
,,,
921,,null,null
,,,
922,,null,null

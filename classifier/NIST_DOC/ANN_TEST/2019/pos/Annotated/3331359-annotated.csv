,sentence,label,data
,,,
0,"Short Research Papers 3A: AI, Mining, and others",null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Corpus-based Set Expansion with Lexical Features and Distributed Representations,null,null
,,,
5,,null,null
,,,
6,"Puxuan Yu, Zhiqi Huang, Razieh Rahimi, and James Allan",null,null
,,,
7,Center for Intelligent Information Retrieval University of Massachusetts Amherst,null,null
,,,
8,"{pxyu,zhiqihuang,rahimi,allan}@cs.umass.edu",null,null
,,,
9,,null,null
,,,
10,ABSTRACT,null,null
,,,
11,"Corpus-based set expansion refers to mining ""sibling"" entities of some given seed entities from a corpus. Previous works are limited to using either textual context matching or semantic matching to fulfill this task. Neither matching method takes full advantage of the rich information in free text. We present CaSE, an efficient unsupervised corpus-based set expansion framework that leverages lexical features as well as distributed representations of entities for the set expansion task. Experiments show that CaSE outperforms state-of-the-art set expansion algorithms in terms of expansion accuracy.",null,null
,,,
12,"ACM Reference Format: Puxuan Yu, Zhiqi Huang, Razieh Rahimi, and James Allan. 2019. Corpusbased Set Expansion with Lexical Features and Distributed Representations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
,,,
13,1 INTRODUCTION,null,null
,,,
14,"Corpus-based set expansion ­ i.e., finding in a given corpus the complete set of entities that belong to the same semantic class of a few seed entities ­ is a critical task in information retrieval and knowledge discovery. For example, given the input seed set {Massachusetts, Virginia, Washington}, a set expansion method is expected to output all other states in the United States. Set expansion is broadly useful for a number of downstream applications, such as question answering [14, 23], taxonomy construction [19], relation extraction [9], and query suggestion [1].",null,null
,,,
15,"Most corpus-based approaches [5, 12, 15­18] are based on the assumption of distributional similarity [6], which, in the context of set expansion, can be understood on two levels:",null,null
,,,
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331359",null,null
,,,
17,,null,null
,,,
18,"the task of finding sibling entities to finding optimal textual patterns. For an entity to be considered a candidate, it has to meet the ""hard match"" condition: sharing at least one textual pattern with at least one seed. Thus, many target entities end up with low relevance scores especially on smaller corpora. On the other side, distributed representations of entities do not require exact matching of textual patterns because they are calculated according to terms within a certain window, regardless of term arrangement. Therefore, not only sibling entities, but also other semantically related entities, such as twin or parent entities, are included in the final result.",null,null
,,,
19,"Different from prior methods which explored either side of the distributional hypothesis, we propose CaSE",null,null
,,,
20,The major contributions of this paper are:,null,null
,,,
21,2 RELATED WORK,null,null
,,,
22,"Web-based Set Expansion: Web-based methods ­ including Google Sets [22], SEAL [23] and Lyretail [2] ­ submit queries consisting of seed entities to search engines and analyze the retrieved documents. The assumption that top-ranked webpages cover other entities in the same semantic class is not always true. Also, extracting data from online platforms can be time-consuming at query time. Therefore, most recent studies are proposed in an offline setting.",null,null
,,,
23,"Corpus-based Set Expansion: Thelen and Riloff [21] described using certain contextual patterns to tag words with limited coarsegrained types. Roark and Charniak [15] first introduced a general set expansion solution based on co-occurrence of entities. Later,",null,null
,,,
24,1 https://github.com/PxYu/entity-expansion,null,null
,,,
25,,null,null
,,,
26,1153,null,null
,,,
27,,null,null
,,,
28,"Short Research Papers 3A: AI, Mining, and others",null,null
,,,
29,,null,null
,,,
30,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
31,,null,null
,,,
32,"methods that define membership functions based on co-occurrences of entities with contexts were proposed [5, 17]. Instead of text corpora, SEISA [7] uses offline query logs and web lists, and does set expansion with an iterative similarity aggregation function. EgoSet [16] constructs clusters of entities using textual patterns and user-generated ontology respectively, and outputs clusters after refinement.",null,null
,,,
33,"The most recent and comparable methods to our approach are SetExpan [18] and SetExpander [12]. Besides selecting contexts based on distributional similarity, SetExpan also leverages coarsegrained types from Wikipedia as features. SetExpan proposed resetting the context pool before each iteration to address the ""semantic drift"" problem, which turned out to be unsolved since false entities persist in later iterations. In addition, SetExpan takes hundreds of seconds per issued query, making it difficult to use with applications which involve user interaction. SetExpander takes the second perspective of distributional similarity, and generates variants of distributed representations from different patterns. Similarity scores of each candidate computed per representation with seed entities are treated as features, based on which an MLP binary classifier decides whether a candidate should be in the expanded set. Besides the limitation of solely using distributed representations, patterns such as explicit lists [17] cover only a small portion of entities.",null,null
,,,
34,,null,null
,,,
35,3 METHODOLOGY,null,null
,,,
36,,null,null
,,,
37,"Intuitively, CaSE expands input seed entities by semantically related",null,null
,,,
38,,null,null
,,,
39,entities that frequently share important contexts with seeds. The,null,null
,,,
40,,null,null
,,,
41,first step is to extract features from the contexts of seed entities in,null,null
,,,
42,,null,null
,,,
43,the corpus. Different features can be extracted from contexts of en-,null,null
,,,
44,,null,null
,,,
45,"tities. Potential features for entity e0 in sentence ""w-2w-1e0w1w2"" include unigrams",null,null
,,,
46,,null,null
,,,
47,"Skip-grams impose strong positional constraints [16], reducing the",null,null
,,,
48,,null,null
,,,
49,risk of finding relevant concepts rather than true sibling entities.,null,null
,,,
50,,null,null
,,,
51,"The other alternative is to directly use predefined patterns, e.g.,",null,null
,,,
52,,null,null
,,,
53,"""such as e0, e1 and e2"", for set expansion. However, Shi et al. [20]",null,null
,,,
54,,null,null
,,,
55,"showed that for large corpora, the construction of syntactic con-",null,null
,,,
56,,null,null
,,,
57,texts has better accuracy and introduces less noise compared to,null,null
,,,
58,,null,null
,,,
59,"pattern based methods. Therefore, we extract skip-gram features",null,null
,,,
60,,null,null
,,,
61,from entity contexts.,null,null
,,,
62,,null,null
,,,
63,Some preprocessing steps are performed on the text corpus to,null,null
,,,
64,,null,null
,,,
65,"improve run-time efficiency. First, we extract the set of entities",null,null
,,,
66,,null,null
,,,
67,"E = {ei | i = 1, 2, · · · , N } in the given text corpus. We then con-",null,null
,,,
68,,null,null
,,,
69,sider a window of size 4 around each entity mention in the corpus,null,null
,,,
70,,null,null
,,,
71,"and extract four skip-grams [-3, 0], [-2, 1], [-1, 2], and [0, 3] where",null,null
,,,
72,,null,null
,,,
73,"[-x, y] means keeping x words before and y words after the en-",null,null
,,,
74,,null,null
,,,
75,tity mention. This setting allows more matchings and thus creates,null,null
,,,
76,,null,null
,,,
77,"candidate pool with higher recall. Let i = {ij | j = 1, 2, · · · , Mi }",null,null
,,,
78,,null,null
,,,
79,"denote the extracted skip-grams for ei . Then, the set of all skip-",null,null
,,,
80,,null,null
,,,
81,grams in the corpus is  =,null,null
,,,
82,,null,null
,,,
83,N i =1,null,null
,,,
84,,null,null
,,,
85,i .,null,null
,,,
86,,null,null
,,,
87,Based,null,null
,,,
88,,null,null
,,,
89,on,null,null
,,,
90,,null,null
,,,
91,"these,",null,null
,,,
92,,null,null
,,,
93,we,null,null
,,,
94,,null,null
,,,
95,create,null,null
,,,
96,,null,null
,,,
97,a,null,null
,,,
98,,null,null
,,,
99,"frequency matrix N ×M = {ij | i = 1, 2, . . . , N ; j = 1, 2, . . . , M },",null,null
,,,
100,,null,null
,,,
101,"where N = |E|, M = ||, and cell value ij is the number of co-",null,null
,,,
102,,null,null
,,,
103,occurrences of entity i with skip-gram j.,null,null
,,,
104,,null,null
,,,
105,We also acquire a distributed representation for each entity either,null,null
,,,
106,,null,null
,,,
107,by training on the local corpus or using pre-trained representations.,null,null
,,,
108,,null,null
,,,
109,"Each entity ei is thus represented as a D dimensional embedding i · in matrix N ×D = {ik | i = 1, 2, . . . , N ; k = 1, 2, . . . , D}.",null,null
,,,
110,,null,null
,,,
111,3.1 Context Feature Selection,null,null
,,,
112,"At query time, we first build the set of candidate entities. Suppose the set of seeds S = {sq | q = 1, 2, . . . , L} is a subset of E, then the union of the skip-grams of seed entities, s , is a subset of . For a particular query, we derive a sub-matrix s from  by column projection; columns of s are the context features of seeds, s, and the rows represent all entities that share at least one context with",null,null
,,,
113,at least one seed. These entities are considered as candidate entities,null,null
,,,
114,for expansion.,null,null
,,,
115,"We use s to quantitatively measure the correlation between seeds and skip-grams. First, we compute cqj as the co-occurrences of seed entity sq with skip-gram j over the total occurrences of j in the corpus. Then, the c-weight for skip-gram j given the current query is defined as:",null,null
,,,
116,,null,null
,,,
117,L,null,null
,,,
118,,null,null
,,,
119,L,null,null
,,,
120,,null,null
,,,
121,cj = cqj =,null,null
,,,
122,,null,null
,,,
123,q=1,null,null
,,,
124,,null,null
,,,
125,q=1,null,null
,,,
126,,null,null
,,,
127,qj,null,null
,,,
128,,null,null
,,,
129,N i =1,null,null
,,,
130,,null,null
,,,
131,i,null,null
,,,
132,,null,null
,,,
133,j,null,null
,,,
134,,null,null
,,,
135,.,null,null
,,,
136,,null,null
,,,
137,-1,null,null
,,,
138,,null,null
,,,
139,"This weight shows the quality of skip-grams, in that the higher the c-weight, the more relevant the skip-gram is to the seeds. Since candidate entities are obtained by selecting entities that share skipgrams with seed entities, weighting skip-grams of seed entities can be used to rank candidate entities.",null,null
,,,
140,,null,null
,,,
141,3.2 Entity Search via Semantic Representation,null,null
,,,
142,"We use semantic similarity between seed and candidate entities to further evaluate candidate entities. In preprocessing steps, we acquire a D dimensional word embedding matrix . The comparison between a seed entity and a candidate entity is equivalent to computing the cosine similarity of two corresponding rows. Denoting the cosine similarity of seed entity sq and candidate entity ei as cos(ei , sq ), the relatedness of ei to all seeds is",null,null
,,,
143,,null,null
,,,
144,i,null,null
,,,
145,,null,null
,,,
146,=,null,null
,,,
147,,null,null
,,,
148,1 L,null,null
,,,
149,,null,null
,,,
150,L,null,null
,,,
151,"h(cos(ei , sq )),",null,null
,,,
152,q=1,null,null
,,,
153,,null,null
,,,
154,-2,null,null
,,,
155,,null,null
,,,
156,"where L is the length of the query and h(·) is an increasing and strictly positive function. The intuition behind h(·) is that the mathematical difference between cos(a, x) = 0.9 and cos(a, y) = 0.8 is not a sufficient description of the semantic difference between x and y. Finally, The score of entity ei with skip-gram j , denoted by ij , comprises three parts: the c-weight of j , the semantic similarity with seeds of ei , and the smoothed frequency of entity skip-gram co-occurrences. Formally, ij = cj · i ·",null,null
,,,
157,,null,null
,,,
158,i =,null,null
,,,
159,,null,null
,,,
160,j,null,null
,,,
161,,null,null
,,,
162,ij =,null,null
,,,
163,,null,null
,,,
164,1 L,null,null
,,,
165,,null,null
,,,
166,"h(cos(ei , sq ))",null,null
,,,
167,q,null,null
,,,
168,,null,null
,,,
169,j,null,null
,,,
170,,null,null
,,,
171,cqj,null,null
,,,
172,q,null,null
,,,
173,,null,null
,,,
174,"We compute i for each entity in the candidate pool. The set expansion result is the set of entities with top x highest scores, where",null,null
,,,
175,x is a predefined cutoff.,null,null
,,,
176,,null,null
,,,
177,1154,null,null
,,,
178,,null,null
,,,
179,"Short Research Papers 3A: AI, Mining, and others",null,null
,,,
180,,null,null
,,,
181,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
182,,null,null
,,,
183,4 EXPERIMENTS,null,null
,,,
184,4.1 Compared Methods,null,null
,,,
185,"· Word2Vec [13]: We trained word embedding on our corpus using skip-gram Word2Vec model, where window size and number of iterations are set to 6 and 15, respectively. We then use embedding vectors of entities to retrieve the K nearest neighbors of seed entities as the expansion result.",null,null
,,,
186,· BERT [4]: BERT is an empirically powerful embedding model for several NLP tasks. We use a pre-trained BERT model,null,null
,,,
187,"· SetExpander [12]: We perform preprocessing, training and inference in the default setting on evaluation corpora. Implementation is distributed under Intel's NLP Architect Framework 2.",null,null
,,,
188,· SetExpan [18]: We run SetExpan in its default settings with preprocessing steps identical to CaSE.,null,null
,,,
189,· CaSE: The unsupervised set expansion framework we proposed. Functions h(·) and,null,null
,,,
190,4.2 Experimental Setup,null,null
,,,
191,Datasets and Preprocessing: We use three corpora to evaluate CaSE.,null,null
,,,
192,"Constructing queries: We build a collection of 62 semantic sets for evaluating set expansion algorithms as the selected combination of MRSCs [16], INEX-XER sets [3], SemSearch sets [10], and 12 additional sets from web resources [8]. To evaluate the sensitivity of our algorithm to the number of seed entities, we build queries with length ranging from 2 to 5. For each set consisting of n entities, we build min 100, nCm queries with m random seeds.",null,null
,,,
193,"Evaluation Metrics: Set expansion algorithms retrieve a ranked list of entities in response to a query. We evaluate the top 100 retrieved entities for each query by all methods described in Section 4.1, except the SetExpan method where all retrieved entities after 10 iterations are evaluated. Mean Average Precision",null,null
,,,
194,2 http://nlp_architect.nervanasys.com/term_set_expansion.html,null,null
,,,
195,,null,null
,,,
196,1,null,null
,,,
197,,null,null
,,,
198,SetExpander,null,null
,,,
199,,null,null
,,,
200,SetExpan,null,null
,,,
201,,null,null
,,,
202,0.8,null,null
,,,
203,,null,null
,,,
204,CaSE-W2V,null,null
,,,
205,,null,null
,,,
206,0.6,null,null
,,,
207,,null,null
,,,
208,MAP,null,null
,,,
209,,null,null
,,,
210,0.4,null,null
,,,
211,,null,null
,,,
212,0.2,null,null
,,,
213,,null,null
,,,
214,0.0 1,null,null
,,,
215,,null,null
,,,
216,10,null,null
,,,
217,,null,null
,,,
218,20,null,null
,,,
219,,null,null
,,,
220,30,null,null
,,,
221,,null,null
,,,
222,40,null,null
,,,
223,,null,null
,,,
224,Set ID,null,null
,,,
225,,null,null
,,,
226,50,null,null
,,,
227,,null,null
,,,
228,60,null,null
,,,
229,,null,null
,,,
230,"Figure 1: Set-wise MAP of SetExpander, SetExpan and CaSE-W2V running 2-seed queries on Wiki corpus. Sets are ordered by MAP of CaSE-W2V decreasing.",null,null
,,,
231,,null,null
,,,
232,Figure 2: MAP of all compared methods on Wiki.,null,null
,,,
233,evaluation sets. Statistical significant tests are performed using the two-tailed paired t-test at the 0.05 level.,null,null
,,,
234,4.3 Results and Discussion,null,null
,,,
235,"Table 1 summarizes the overall performance of different methods for queries with different lengths on three corpora. The results indicate that the best variation of CaSE is CaSE-W2V, which shows robust improvements upon baselines on all corpora for queries of different length",null,null
,,,
236,"Robustness against input length: Intuitively, one might expect better performance given longer queries. SetExpan removes sub-optimal contexts in feature selection, thus showing the expected trend. Embeddings based methods demonstrate contrary behaviors, mainly because more seeds introduce more twin entities at top. CaSE does not remove features but weights them, and further weights entities with distributed similarity. As Table 1 shows, CaSE performs well even with few seeds, and improves slowly as the number of seeds increases.",null,null
,,,
237,"Gap among evaluation sets: Figure 1 shows that some semantic sets are easier to expand than others. This result partially confirms earlier work showing that the performance of set expansion models improves as the frequencies of candidate entities increase [17]. To specifically show the correlation between entity frequencies and performance of set expansion, we define a composite property for each set T . For each entity ei in T , we first calculate the average of number of entities that occur in each skip-gram associated with entity ei , which is denoted by ki . A higher k value means the entity occurs in general contexts shared by more entities. Then,",null,null
,,,
238,,null,null
,,,
239,1155,null,null
,,,
240,,null,null
,,,
241,"Short Research Papers 3A: AI, Mining, and others",null,null
,,,
242,,null,null
,,,
243,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
244,,null,null
,,,
245,Table 1: Retrieval accuracy,null,null
,,,
246,,null,null
,,,
247,AP89,Y,AP89
,,,
248,,null,null
,,,
249,WaPo,Y,WaPo
,,,
250,,null,null
,,,
251,Wiki,Y,Wiki
,,,
252,,null,null
,,,
253,#seeds,null,null
,,,
254,,null,null
,,,
255,2,null,null
,,,
256,,null,null
,,,
257,3,null,null
,,,
258,,null,null
,,,
259,4,null,null
,,,
260,,null,null
,,,
261,5,null,null
,,,
262,,null,null
,,,
263,2,null,null
,,,
264,,null,null
,,,
265,3,null,null
,,,
266,,null,null
,,,
267,4,null,null
,,,
268,,null,null
,,,
269,5,null,null
,,,
270,,null,null
,,,
271,2,null,null
,,,
272,,null,null
,,,
273,3,null,null
,,,
274,,null,null
,,,
275,4,null,null
,,,
276,,null,null
,,,
277,5,null,null
,,,
278,,null,null
,,,
279,Word2Vec BERT SetExpander SetExpan,null,null
,,,
280,,null,null
,,,
281,0.032 0.103 0.058 0.095,null,null
,,,
282,,null,null
,,,
283,0.030 0.094 0.067 0.103,null,null
,,,
284,,null,null
,,,
285,0.027 0.091 0.073 0.111,null,null
,,,
286,,null,null
,,,
287,0.027 0.087 0.076 0.117,null,null
,,,
288,,null,null
,,,
289,0.046 0.078 0.046 0.083,null,null
,,,
290,,null,null
,,,
291,0.041 0.072 0.054 0.094,null,null
,,,
292,,null,null
,,,
293,0.037 0.063 0.060 0.103,null,null
,,,
294,,null,null
,,,
295,0.035 0.061 0.065 0.111,null,null
,,,
296,,null,null
,,,
297,0.082 0.062 0.070 0.106,null,null
,,,
298,,null,null
,,,
299,0.075 0.058 0.079 0.119,null,null
,,,
300,,null,null
,,,
301,0.071 0.055 0.082 0.126,null,null
,,,
302,,null,null
,,,
303,0.066 0.050 0.086 0.131,null,null
,,,
304,,null,null
,,,
305,CaSE-mdr CaSE-BERT CaSE-W2V,null,null
,,,
306,,null,null
,,,
307,0.117 0.117 0.118 0.117 0.095 0.089 0.088 0.089 0.161 0.161 0.158 0.155 0.132 0.133 0.136 0.136 0.112 0.109 0.109 0.108 0.179 0.183 0.182 0.180 0.161 0.170 0.171 0.173 0.140 0.141 0.143 0.145 0.236 0.249 0.252 0.253,null,null
,,,
308,,null,null
,,,
309,1,null,null
,,,
310,,null,null
,,,
311,hypothetical curve,null,null
,,,
312,,null,null
,,,
313,CaSE-W2V,null,null
,,,
314,,null,null
,,,
315,0.8,null,null
,,,
316,,null,null
,,,
317,0.6,null,null
,,,
318,,null,null
,,,
319,MAP,null,null
,,,
320,,null,null
,,,
321,0.4,null,null
,,,
322,,null,null
,,,
323,0.2,null,null
,,,
324,,null,null
,,,
325,0.0 0,null,null
,,,
326,,null,null
,,,
327,2000,null,null
,,,
328,,null,null
,,,
329,4000,null,null
,,,
330,,null,null
,,,
331,6000,null,null
,,,
332,,null,null
,,,
333,8000,null,null
,,,
334,,null,null
,,,
335,K,null,null
,,,
336,,null,null
,,,
337,Figure 3: Relations between composite property K and setwise MAP of CaSE-W2V on Wiki corpus.,null,null
,,,
338,,null,null
,,,
339,the composite property of the set is defined as K = [ki /,null,null
,,,
340,,null,null
,,,
341,M j =1,null,null
,,,
342,,null,null
,,,
343,i,null,null
,,,
344,,null,null
,,,
345,j,null,null
,,,
346,,null,null
,,,
347,"],",null,null
,,,
348,,null,null
,,,
349,where,null,null
,,,
350,,null,null
,,,
351,M j =1,null,null
,,,
352,,null,null
,,,
353,i j,null,null
,,,
354,,null,null
,,,
355,is,null,null
,,,
356,,null,null
,,,
357,the,null,null
,,,
358,,null,null
,,,
359,frequency,null,null
,,,
360,,null,null
,,,
361,of,null,null
,,,
362,,null,null
,,,
363,ei,null,null
,,,
364,,null,null
,,,
365,in,null,null
,,,
366,,null,null
,,,
367,the,null,null
,,,
368,,null,null
,,,
369,corpus.,null,null
,,,
370,,null,null
,,,
371,Figure,null,null
,,,
372,,null,null
,,,
373,3,null,null
,,,
374,,null,null
,,,
375,shows,null,null
,,,
376,,null,null
,,,
377,the correlation between the defined metric K and set-wise MAP,null,null
,,,
378,,null,null
,,,
379,"performance of different sets using our proposed model. Intuitively,",null,null
,,,
380,,null,null
,,,
381,"lower MAP is expected for sets with higher K. Therefore, we fit",null,null
,,,
382,,null,null
,,,
383,an exponentially decreasing function to points in the diagram of,null,null
,,,
384,,null,null
,,,
385,Figure 3. There exists some outlier sets whose MAP performance is,null,null
,,,
386,,null,null
,,,
387,"low even with low K values. Investigating outlier sets, we discover",null,null
,,,
388,,null,null
,,,
389,"that these sets are conceptually subsets of some supersets, e.g., set",null,null
,,,
390,,null,null
,,,
391,"""allies of World War II"" is a subset of set ""all countries in the world"".",null,null
,,,
392,,null,null
,,,
393,The reason why outliers under-achieve in terms of MAP is that it,null,null
,,,
394,,null,null
,,,
395,is difficult for set expansion models to disambiguate more specific,null,null
,,,
396,,null,null
,,,
397,concepts from contexts unless directed to correct knowledge.,null,null
,,,
398,,null,null
,,,
399,5 CONCLUSION AND FUTURE WORK,null,null
,,,
400,"We present an unsupervised corpus-based set expansion framework, CaSE. We show that weighting entities directly with distributed embeddings and indirectly via lexical features significantly improves expansion accuracy of set expansion. In the future, we plan to improve CaSE's performance on less frequent sets by narrowing the scope of input, similar to a QA system.",null,null
,,,
401,,null,null
,,,
402,ACKNOWLEDGMENTS,null,null
,,,
403,"This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",null,null
,,,
404,,null,null
,,,
405,REFERENCES,null,null
,,,
406,"[1] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and H. Li. 2008. Contextaware query suggestion by mining click-through and session data. In Proceedings SIGKDD. ACM, 875­883.",null,null
,,,
407,"[2] Z. Chen, M. Cafarella, and H. Jagadish. 2016. Long-tail vocabulary dictionary extraction from the web. In Proceedings WSDM. ACM, 625­634.",null,null
,,,
408,"[3] A. P. De Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. 2007. Overview of the INEX 2007 entity ranking track. Springer, 245­251.",null,null
,,,
409,"[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805",null,null
,,,
410,[5] Z. Ghahramani and K. A. Heller. 2006. Bayesian sets. In Advances in neural information processing systems. 435­442.,null,null
,,,
411,"[6] Z. S. Harris. 1954. Distributional structure. Word 10, 2-3",null,null
,,,
412,"Proceedings of the 20th international conference on World wide web. ACM, 427­436. [8] C. Kelly and L. Kelly. 2019. http://www.manythings.org/ [9] J. Lang and J. Henderson. 2013. Graph-based seed set expansion for relation",null,null
,,,
413,"extraction using random walk hitting times. In Proceedings NAACL/HLT. 772­776. [10] Y. Lei, V. Uren, and E. Motta. 2006. Semsearch: A search engine for the semantic",null,null
,,,
414,"web. In KEOD. Springer, 238­245. [11] J. Liu, J. Shang, C. Wang, X. Ren, and J. Han. 2015. Mining quality phrases from",null,null
,,,
415,"massive text corpora. In Proceedings SIGMOD. ACM, 1729­1744. [12] J. Mamou, O. Pereg, M. Wasserblat, I. Dagan, Y. Goldberg, A. Eirew, Y. Green, S.",null,null
,,,
416,"Guskin, P. Izsak, and D. Korat. 2018. Term Set Expansion based on Multi-Context Term Embeddings: an End-to-end Workflow. arXiv:1807.10104",null,null
,,,
417,,null,null
,,,
418,1156,null,null
,,,
419,,null,null
,,,
420,,null,null

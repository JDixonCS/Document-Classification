,sentence,label,data
,,,
0,Short Research Papers 2B: Recommendation and Evaluation,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Quantifying Bias and Variance of System Rankings,null,null
,,,
5,,null,null
,,,
6,Gordon V. Cormack,null,null
,,,
7,University of Waterloo gvcormac@uwaterloo.ca,null,null
,,,
8,ABSTRACT,null,null
,,,
9,"When used to assess the accuracy of system rankings, Kendall's  and other rank correlation measures conflate bias and variance as sources of error. We derive from  a distance between rankings in Euclidean space, from which we can determine the magnitude of bias, variance, and error. Using bootstrap estimation, we show that shallow pooling has substantially higher bias and insubstantially lower variance than probability-proportional-to-size sampling, coupled with the recently released dynAP estimator.",null,null
,,,
10,ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2019. Quantifying Bias and Variance of System Rankings. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval,null,null
,,,
11,1 INTRODUCTION,null,null
,,,
12,"Since large-scale test collections were first applied in the context of TREC [10], concern has been expressed regarding the extent to which they afford ""fair"" or ""unbiased"" rankings of informationretrieval",null,null
,,,
13,b2 +  2 quantifies overall,null,null
,,,
14,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-6172-9/19/07. https://doi.org/10.1145/3331184.3331356",null,null
,,,
15,,null,null
,,,
16,Maura R. Grossman,null,null
,,,
17,University of Waterloo maura.grossman@uwaterloo.ca,null,null
,,,
18,,null,null
,,,
19,"We use bootstrap sampling to compare the official TREC 2018 Common Core test collection [1] to an alternate statistical collection created in advance by the University of Waterloo [9], along with its companion dynAP estimator1 [6]. We find that there are two sources of bias between the collections:",null,null
,,,
20,"This work contributes a new methodology for test-collection evaluation, and uses that methodology to show that shallow pooling introduces bias far beyond what is shown by rank-correlation measures over submitted TREC results. Of the statistical estimators, the older and more well established infAP [18] shows clear bias, while dynAP shows negligible bias. The results indicate that a statistical test collection can yield comparable accuracy over 50 topics, and considerably better accuracy over more, compared to exhaustive assessment.",null,null
,,,
21,,null,null
,,,
22,2 BIAS AND VARIANCE FOR RANKING,null,null
,,,
23,"In this paper, a ""test result"" [11] is a ranking of systems according to",null,null
,,,
24,"their measured effectiveness. Closeness of agreement is quantified by a rank-similarity coefficient like Kendall's  , with the maximum value 1 denoting equality.",null,null
,,,
25,"We interpret system rankings to be points in Euclidean space, where ",null,null
,,,
26,,null,null
,,,
27,"(X , Y ) = E  2(X , Y ) . The variance  2 of a given X is one-half the squared distance be-",null,null
,,,
28,,null,null
,,,
29,tween itself and an independent and identically distributed test,null,null
,,,
30,,null,null
,,,
31,result:,null,null
,,,
32,,null,null
,,,
33,2(X ),null,null
,,,
34,,null,null
,,,
35,=,null,null
,,,
36,,null,null
,,,
37,1,null,null
,,,
38,,null,null
,,,
39,(X,null,null
,,,
40,,null,null
,,,
41,",",null,null
,,,
42,,null,null
,,,
43,X,null,null
,,,
44,,null,null
,,,
45,,null,null
,,,
46,),null,null
,,,
47,,null,null
,,,
48,(i .i .d .,null,null
,,,
49,,null,null
,,,
50,X,null,null
,,,
51,,null,null
,,,
52,",",null,null
,,,
53,,null,null
,,,
54,X,null,null
,,,
55,,null,null
,,,
56,,null,null
,,,
57,),null,null
,,,
58,,null,null
,,,
59,.,null,null
,,,
60,,null,null
,,,
61,2,null,null
,,,
62,,null,null
,,,
63,"When G is the gold-standard ranking, squared bias b2 is the amount",null,null
,,,
64,,null,null
,,,
65,by which the squared distance between X and G exceeds that which,null,null
,,,
66,,null,null
,,,
67,is attributable to variance:,null,null
,,,
68,,null,null
,,,
69,b2(X ) =,null,null
,,,
70,,null,null
,,,
71,It follows that mean-squared error,null,null
,,,
72,,null,null
,,,
73,"MSE(X ) = b2(X ) +  2(X ) . Bias b(X ) is a vector whose direction is unspecified; however, its magnitude |b |(X ) is sufficient for our purpose.",null,null
,,,
74,"It is worth noting that the ground truth ranking is a random variable G, rather than the particular outcome G =  derived for the particular topics and assessments of the reference test collection from which G is derived. Under the assumption underlying virtually all reported statistical tests--that the set of topics in a collection",null,null
,,,
75,is a random sample of a population of topics--the distributions of,null,null
,,,
76,1See cormack.uwaterloo.ca/sample/.,null,null
,,,
77,,null,null
,,,
78,1089,null,null
,,,
79,,null,null
,,,
80,Short Research Papers 2B: Recommendation and Evaluation,null,null
,,,
81,,null,null
,,,
82,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
83,,null,null
,,,
84,G and X may estimated by bootstrap re-sampling,null,null
,,,
85,"When presenting empirical results, we report magnitude of bias |b|, standard deviation  , and root-mean-squared error RMSE.",null,null
,,,
86,3 THE BOOTSTRAP,null,null
,,,
87,"To estimate bias and variance, we conduct repeated tests in which random factors associated with the selection of topics, and the selection of documents to assess, are simulated. For statistical methods like infAP and dynAP, we first repeat the sampling method 100 times for each of nt topics in a reference collection. For each sample, we measure AP",null,null
,,,
88,"We then draw 1, 000 samples of nt topics, with replacement, from the nt topics of the reference collection. Typically, but not necessarily, nt = nt , thus preserving the variance of the mean. For each topic t within a sample, and for each system s, an AP measurement is drawn from the table at position",null,null
,,,
89,is estimated as the empirical distribution of ,null,null
,,,
90,4 EXPERIMENT 1: ASSESSMENT BUDGETS,null,null
,,,
91,"For our first experiment, we assume the TREC 8 Ad Hoc test collection to yield ground truth. We are concerned with ranking not only the 129 systems whose results were submitted to TREC 8 for evaluation, but also dissimilar systems. To simulate the results of dissimilar systems, we engineered an additional 129 results, each derived from a distinct TREC result by randomly permuting the order of the relevant documents. As a consequence, corresponding submitted and engineered results have identical AP and MAP scores, and identical ground-truth ranks.",null,null
,,,
92,"Using the ground-truth ranking for the submitted results, and additionally, the ground-truth ranking for the submitted and engineered results combined, we evaluate the bias, variance, and MSE of rankings derived from fewer assessments of the TREC collection.",null,null
,,,
93,"The TREC collection contains 50 topics and 86,830 assessments, an average of 1,737 assessments per topic. Initially, we evaluated four methods of reducing assessment cost seventeen-fold to about 5,000 assessments:",null,null
,,,
94,"· The well-known infAP estimator, applied to a probabilityproportional-to-size",null,null
,,,
95,"· The recently released dynAP estimator, applied to the same PPS sample, for a total of 5,000 assessments;",null,null
,,,
96,,null,null
,,,
97,"· The trec_eval evaluator, applied to a variable number of documents per topic selected by depth-5 pooling, for a total of 5,542 assessments",null,null
,,,
98,"· trec_eval, applied to 100 documents per topic, selected by hedge, for a total of 5,000 assessments.",null,null
,,,
99,We then evaluated the effect of quadrupling the assessment budget in two ways:,null,null
,,,
100,"Table 1 shows ranking accuracy with respect to the submitted TREC results; Table 2 shows ranking accuracy with respect to the submitted results augmented by the engineered results. The topleft panel of each table shows accuracy for 5, 000 assessments. The result of quadrupling the number of topics is shown to the right, while the results of quadrupling the number of assessments per topic is shown below. The bottom row shows the accuracy of the reference TREC collection, under the assumption that it is unbiased.",Y,TREC
,,,
101,"For 5, 000 assessments and the TREC results, the RMSE results show little to choose between dynAP and hedge, with between 15% and 20% higher error than the reference collection. The |b | and  results show that dynAP has less than one-quarter the bias, but hedge has lower variance, with the net effect that their overall error is similar.",null,null
,,,
102,4.1 Effect of More Topics,null,null
,,,
103,"From these results we can predict predict the effect of quadrupling the number of topics, which is borne out by the top-right panel: |b | is essentially unchanged, while  is approximately halved. The net effect is that RMSE for dynAP is approximately halved, about 17% higher than for the reference collection.  for hedge is similarly halved but |b | is unchanged, so RMSE is reduced by only one-third, about 55% higher than for the reference collection.",null,null
,,,
104,"From the initial results we can only bound the effect of quadrupling the number of assessments per topic: |b| and  will both generally be reduced, but b2 +  2 cannot fall below  for the reference collection. The bottom-left panel confirms this prediction, except for the fact that the bootstrap estimate of RMSE for hedge is slightly smaller than for the reference collection. This apparently preposterous result result may be explained by random error in the bootstrap estimate. Overall, this panel suggests that, with a budget of 400 assessments per topic, hedge has slightly lower overall error, but still four-times higher bias, than dynAP. Nevertheless, the results for hedge--and all other methods--are far superior when the same overall budget of 20, 000 is apportioned over 200 topics with 100 assessments per topic, instead of 50 topics with 400 assessments per topic.",null,null
,,,
105,"The effect of quadrupling the number of topics, with a budget of 400 assessments per topic, is the same as with a budget of 100 assessments per topic: |b | is invariant, while  is halved. Overall, for a budget of 80, 000 assessments, dynAP achieves an RMSE score that is insubstantially different from that achieved by a reference collection with 331,320 assessments.",null,null
,,,
106,,null,null
,,,
107,1090,null,null
,,,
108,,null,null
,,,
109,Short Research Papers 2B: Recommendation and Evaluation,null,null
,,,
110,,null,null
,,,
111,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
112,,null,null
,,,
113,infAP dynAP depth-5 hedge,null,null
,,,
114,infAP dynAP depth-20 hedge,null,null
,,,
115,Reference,null,null
,,,
116,,null,null
,,,
117,|b |,null,null
,,,
118,,null,null
,,,
119,,null,null
,,,
120,,null,null
,,,
121,RMSE,null,null
,,,
122,,null,null
,,,
123,"5,000 assessments, 50 topics",null,null
,,,
124,,null,null
,,,
125,0.0309 0.0952 0.1001 0.0117 0.0907 0.0914,null,null
,,,
126,,null,null
,,,
127,0.0800 0.0839 0.1160,null,null
,,,
128,,null,null
,,,
129,"0.0500 0.0806 0.0948 20,000 assessments, 50 topics",null,null
,,,
130,,null,null
,,,
131,0.0113 0.0826 0.0833,null,null
,,,
132,,null,null
,,,
133,0.0055 0.0806 0.0808,null,null
,,,
134,,null,null
,,,
135,0.0381 0.0783 0.0871,null,null
,,,
136,"0.0159 0.0777 0.0794 86,830 assessments, 50 topics",null,null
,,,
137,0 0.0797 0.0797,null,null
,,,
138,,null,null
,,,
139,|b |,null,null
,,,
140,,null,null
,,,
141,,null,null
,,,
142,,null,null
,,,
143,RMSE,null,null
,,,
144,,null,null
,,,
145,"20,000 assessments, 200 topics",null,null
,,,
146,,null,null
,,,
147,0.0293 0.0488 0.0124 0.0458,null,null
,,,
148,,null,null
,,,
149,0.0569 0.0475,null,null
,,,
150,,null,null
,,,
151,0.0815 0.0419 0.0916,null,null
,,,
152,,null,null
,,,
153,"0.0498 0.0398 0.0638 80,000 assessments, 200 topics",null,null
,,,
154,,null,null
,,,
155,0.0110 0.0419 0.0033 0.0408,null,null
,,,
156,,null,null
,,,
157,0.0433 0.0409,null,null
,,,
158,,null,null
,,,
159,0.0387 0.0394 0.0552,null,null
,,,
160,,null,null
,,,
161,"0.0154 0.0394 0.0423 331,320 assessments, 200 topics",null,null
,,,
162,0 0.0405 0.0405,null,null
,,,
163,,null,null
,,,
164,Table 1: Accuracy of ranking TREC system results.,Y,TREC
,,,
165,,null,null
,,,
166,infAP dynAP depth-5,null,null
,,,
167,,null,null
,,,
168,|b |,null,null
,,,
169,,null,null
,,,
170,RMSE,null,null
,,,
171,,null,null
,,,
172,"5,000 assessments, 50 topics",null,null
,,,
173,,null,null
,,,
174,0.1510 0.0939 0.1778,null,null
,,,
175,,null,null
,,,
176,0.0319 0.1078 0.1125,null,null
,,,
177,,null,null
,,,
178,0.3071 0.0695 0.3149,null,null
,,,
179,,null,null
,,,
180,|b |,null,null
,,,
181,,null,null
,,,
182,,null,null
,,,
183,,null,null
,,,
184,RMSE,null,null
,,,
185,,null,null
,,,
186,"20,000 assessments, 200 topics",null,null
,,,
187,,null,null
,,,
188,0.1517 0.0480 0.1591,null,null
,,,
189,,null,null
,,,
190,0.0347 0.0560 0.0658,null,null
,,,
191,,null,null
,,,
192,0.3007 0.0360 0.3029,null,null
,,,
193,,null,null
,,,
194,hedge,null,null
,,,
195,infAP dynAP depth-20,null,null
,,,
196,,null,null
,,,
197,"0.1263 0.0773 0.1481 20,000 assessments, 50 topics 0.0495 0.0847 0.0981 -0.0145 0.0856 0.0843 0.2013 0.0688 0.2127",null,null
,,,
198,,null,null
,,,
199,"0.1267 0.0389 0.1325 80,000 assessments, 200 topics 0.0503 0.0430 0.0662 -0.0093 0.0442 0.0432 0.1961 0.0338 0.1990",null,null
,,,
200,,null,null
,,,
201,"hedge 0.0866 0.0780 0.1166 86,830 assessments, 50 topics",null,null
,,,
202,Reference 0 0.0797 0.0797,null,null
,,,
203,,null,null
,,,
204,"0.0869 0.0398 0.0956 331,320 assessments, 200 topics",null,null
,,,
205,0 0.0405 0.0405,null,null
,,,
206,,null,null
,,,
207,Table 2: Accuracy of ranking TREC and dissimilar system results.,Y,TREC
,,,
208,,null,null
,,,
209,4.2 Adversarial Testing,null,null
,,,
210,Table 2 affirms the same predictions regarding assessment-budget,null,null
,,,
211,"allocation, but tells a different story regarding the accuracy of the",null,null
,,,
212,test collections. When the engineered results are ranked with the,null,null
,,,
213,"TREC results, bias is roughly tripled for all methods subject to a 5, 000-document assessment budget, while variance is increased moderately. For infAP, depth-5, and hedge, bias dominates variance,",Y,TREC
,,,
214,calling into question whether these methods provide reasonable,null,null
,,,
215,"accuracy, regardless of their RMSE. The dynAP results represent a closer call. |b | is one-third of  , rendering it a small but noticeable component of RMSE. Quadrupling the number of topics exacerbates the influence of |b|, but still yields RMSE scores better than the reference collection for 50 topics.",null,null
,,,
216,"Quadrupling the per-topic assessment budget dramatically reduces |b | for dynAP, to the point that the bootstrap estimate of b2 is negative",null,null
,,,
217,collection with 50 topics.,null,null
,,,
218,,null,null
,,,
219,5 EXPERIMENT 2: ALTERNATE ASSESSMENTS,null,null
,,,
220,"Before TREC 2018, the University of Waterloo used Dynamic Sampling [5], and 19, 161 of their own assessments to create relevance assessments",null,null
,,,
221,"Table 3 shows the accuracy with which the three test collections rank 69 of the 73 system results submitted to TREC, excluding the four submitted by Waterloo. The top panel shows inter-collection",null,null
,,,
222,,null,null
,,,
223,1091,null,null
,,,
224,,null,null
,,,
225,Short Research Papers 2B: Recommendation and Evaluation,null,null
,,,
226,,null,null
,,,
227,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
228,,null,null
,,,
229,G,null,null
,,,
230,UWcore18 UWcore18N,null,null
,,,
231,NIST,null,null
,,,
232,,null,null
,,,
233,UWcore18,null,null
,,,
234,0.0067 0.0364 0.0710,null,null
,,,
235,,null,null
,,,
236,UWcore18N --|b |-- 0.0887 0.0064 0.0369,null,null
,,,
237,,null,null
,,,
238,NIST,null,null
,,,
239,0.0752 0.0364 0.0084,null,null
,,,
240,,null,null
,,,
241,UWcore18 UWcore18N,null,null
,,,
242,NIST,null,null
,,,
243,,null,null
,,,
244,0.0828 0.1186 0.1089,null,null
,,,
245,,null,null
,,,
246,--RMSE-- 0.1210 0.0826 0.0903,null,null
,,,
247,,null,null
,,,
248,0.1090 0.0869 0.0794,null,null
,,,
249,,null,null
,,,
250,Table 3: Accuracy results for alternative TREC 2018 Common Core Collections. The top panel shows pairwise bias; the bottom panel shows RMSE.,Y,TREC
,,,
251,,null,null
,,,
252,"bias |b| using each collection as ground truth to measure the bias of each other collection, including itself. The top panel should be symmetric, and its diagonal should be 0. Deviations from this prediction may be attributed to random error in the bootstrap estimates. The diagonal of the bottom panel shows  for each collection, while the non-diagonal elements show inter-system RMSE. Our theory predicts that the bottom panel will be symmetric only if the  values are equal, which they nearly are.",null,null
,,,
253,"There is substantial bias--about equal to  --between UWcore18 and UWcore18N, indicating that some systems score better according to the Waterloo assessments, while others score better according to NIST's. We are not in a position to render an opinion on which set of assessments is better and suggest that evaluation using the two sets of assessments should be considered to be separate experiments, and both results considered when comparing system results.",null,null
,,,
254,"Bias between UWcore18N and NIST, while smaller in magnitude, may be more of a concern, because it reflects different measurements of ostensibly the same value. The results from our first experiment show that shallow pooling methods exhibit strong bias, while dynAP does not. Together, these results suggest that the intercollection bias |b |= 0.0364 may be attributed in large part to the NIST collection. If this were the case, it would offset an apparent advantage in  for NIST.",null,null
,,,
255,"We posit that better results would have been achieved, had the budget of 5, 767 NIST assessments allocated to shallow pooing been used to assess an additional 15 topics using Dynamic Sampling. Using our bootstrap simulation, we project that UWcore18N, if extended to 65 topics, would achieve  = 0.0724, lower than the official 50-topic collection.",null,null
,,,
256,"The evidence suggests the current UWcore18N test collection, notwithstanding its slightly higher variance, is preferable to the official TREC test collection, because it is less likely to be biased, particularly with respect to novel IR methods.",null,null
,,,
257,6 DISCUSSION AND LIMITATIONS,null,null
,,,
258,"Kutlu et al. [12] provide an excellent survey of rank-similarity measures and their shortcomings, in support of a ""significance",null,null
,,,
259,,null,null
,,,
260,"aware"" approach that takes into account the results of pairwise significance tests. Our approach exposes the variances of G and X directly--separate from bias--rather than obliquely through an amalgam of test results, distilled into a dimensionless overall score.",null,null
,,,
261,"Previous work has evaluated fairness and stability separately, by",null,null
,,,
262,perturbing the topics or the results to be ranked so as to calculate a,null,null
,,,
263,"separate summary measure for each [3, 14]. Our bootstrap sample was inadequate to quantify b when",null,null
,,,
264,"|b | 0. Further analytic and empirical work is needed to compute confidence intervals for the bootstrap estimates. In theory, the",null,null
,,,
265,"variance of the variance estimates can be determined from the distributions of G and X , but those estimates should be verified by meta-experiments.",null,null
,,,
266,Our experiments rely on on an assumption--contradicted by the,null,null
,,,
267,evidence [13]--that an assessor will give the same relevance deter-,null,null
,,,
268,"mination for a given document, regardless of the sampling strategy.",null,null
,,,
269,Whether real assessor behaviour would have any substantive posi-,null,null
,,,
270,tive or negative effect on rankings remains to be determined.,null,null
,,,
271,Orthogonal estimates of bias and variance have predictive ability,null,null
,,,
272,"lacking in a single score conflating the two, or separate uncalibrated",null,null
,,,
273,"scores. Harnessing this predictive ability, we offer evidence that",null,null
,,,
274,"shallow pooling methods introduce unreasonable amounts of bias,",null,null
,,,
275,"while offering hardly lower variance than dynAP, which represents",null,null
,,,
276,"a substantial improvement over infAP. Based on this evidence, we",null,null
,,,
277,have reason to suggest that the UWcore18 or UWcore18N statisti-,null,null
,,,
278,cal test collections are more accurate than the official TREC 2018,Y,TREC
,,,
279,Common Core test collection.,null,null
,,,
280,REFERENCES,null,null
,,,
281,"[1] Allan, J., Harman, D., Voorhees, E., and Kanoulas, E. TREC 2018 Common Core Track overview. In TREC 2018.",Y,TREC
,,,
282,"[2] Aslam, J. A., Pavlu, V., and Savell, R. A unified model for metasearch, pooling, and system evaluation. In CIKM 2003.",null,null
,,,
283,"[3] Buckley, C., and Voorhees, E. M. Evaluating evaluation measure stability. In SIGIR 2000.",null,null
,,,
284,"[4] Carterette, B. On rank correlation and the distance between rankings. In SIGIR 2009.",null,null
,,,
285,"[5] Cormack, G. V., and Grossman, M. R. Beyond pooling. In SIGIR 2018. [6] Cormack, G. V., and Grossman, M. R. Unbiased low-variance estimators for",null,null
,,,
286,"precision and related information retrieval effectiveness measures. In SIGIR 2019. [7] Cormack, G. V., and Lynam, T. R. Power and bias of subset pooling strategies.",null,null
,,,
287,"In SIGIR 2007. [8] Cormack, G. V., and Lynam, T. R. Statistical precision of information retrieval",null,null
,,,
288,"evaluation. In SIGIR 2006. [9] Cormack, G. V., Zhang, H., Ghelani, N., Abualsaud, M., Smucker, M. D.,",null,null
,,,
289,"Grossman, M. R., Rahbariasl, S., and Grossman, M. R. Dynamic sampling meets pooling. In SIGIR 2019. [10] Harman, D. K. The TREC Test Collections. In TREC: Experiment and Evaluation in Information Retrieval",null,null
,,,
290,,null,null
,,,
291,1092,null,null
,,,
292,,null,null
,,,
293,,null,null

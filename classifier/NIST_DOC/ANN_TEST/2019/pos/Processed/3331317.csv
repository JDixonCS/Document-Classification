,sentence,label,data
0,Short Research Papers 2C: Search,null,null
1,,null,null
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
3,,null,null
4,CEDR: Contextualized Embeddings for Document Ranking,null,null
5,,null,null
6,Sean MacAvaney,null,null
7,"IRLab, Georgetown University sean@ir.cs.georgetown.edu",null,null
8,Arman Cohan,null,null
9,Allen Institute for Artificial Intelligence armanc@allenai.org,null,null
10,ABSTRACT,null,null
11,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models",null,null
12,"ACM Reference Format: Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR: Contextualized Embeddings for Document Ranking. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
13,1 INTRODUCTION,null,null
14,"Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5, 6, 20]. Meanwhile, pretrained contextualized language models",null,null
15,"Prior work has suggested that contextual information can be valuable when ranking. ConvKNRM [1], a recent neural ranking model, uses a convolutional neural network atop word representations, allowing the model to learn representations aware of context in local proximity. In a similar vein, McDonald et al. [12] proposes",null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331317",null,null
17,,null,null
18,Andrew Yates,null,null
19,Max Planck Institute for Informatics ayates@mpi-inf.mpg.de,null,null
20,Nazli Goharian,null,null
21,"IRLab, Georgetown University nazli@ir.cs.georgetown.edu",null,null
22,"an approach that learns a recurrent neural network for term representations, thus being able to capture context from the entire text [12]. These approaches are inherently limited by the variability found in the training data. Since obtaining massive amounts of highquality relevance information can be difficult [24], we hypothesize that pretrained contextualized term representations will improve ad-hoc document ranking performance.",null,null
23,"We propose incorporating contextualized language models into existing neural ranking architectures by using multiple similarity matrices ­ one for each layer of the language model. We find that, at the expense of computation costs, this improves ranking performance considerably, achieving state-of-the-art performance on the Robust 2004 and WebTrack 2012­2014 datasets. We also show that combining each model with BERT's classification mechanism can further improve ranking performance. We call this approach CEDR",null,null
24,"In summary, our contributions are as follows: - We are the first to demonstrate that contextualized word repre-",null,null
25,sentations can be successfully incorporated into existing neural architectures,null,null
26,2 METHODOLOGY,null,null
27,2.1 Notation,null,null
28,"In ad-hoc ranking, documents are ranked for a given query according to a relevance estimate. Let Q be a query consisting of query terms {q1, q2, ..., q |Q | }, and let D be a document consisting of terms {d1, d2, ..., d |D | }. Let ranker",null,null
29,1 https://github.com/Georgetown- IR- Lab/cedr,null,null
30,,null,null
31,1101,null,null
32,,null,null
33,Short Research Papers 2C: Search,null,null
34,,null,null
35,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
36,,null,null
37,"a real-valued relevance estimate for the document to the query. Neural relevance ranking architectures generally use a similarity matrix as input S  R|Q |×|D |, where each cell represents a similarity score between the query and document: Si, j = sim(qi , dj ). These similarity values are usually the cosine similarity score between the word vectors of each term in the query and document.",null,null
38,2.2 Contextualized similarity tensors,null,null
39,Pretrained contextual language representations,null,null
40,"Although contextualized language models vary in particular architectures, they typically consist of multiple stacked layers of representations",null,null
41,"SQ,D[l, q, d] = cos(contextQ,D(q, l), contextQ,D(d, l))",null,null
42,"for each query term q  Q, document term d  D, and layer l  [1..L]. Note that when q and d are identical, they will likely not receive a similarity score of 1, as their representation depends on the surrounding context of the query and document. The layer dimension can be easily incorporated into existing neural models. For instance, soft n-gram based models, like PACRR, can perform convolutions with multiple input channels, and counting-based methods",null,null
43,2.3 Joint BERT approach,null,null
44,"Unlike ELMo, the BERT model encodes multiple text segments simultaneously, allowing it to make judgments about text pairs. It accomplishes this by encoding two meta-tokens",null,null
45,,null,null
46,We explore incorporating the [CLS] token's representation into existing neural ranking models as a joint approach. This allows neural rankers to benefit from deep semantic information from BERT in addition to individual contextualized token matches.,null,null
47,"Incorporating the [CLS] token into existing ranking models is straightforward. First, the given ranking model produces relevance scores",null,null
48,"We hypothesize that this approach will work because the BERT classification mechanism and existing rankers have different strengths. The BERT classification benefits from deep semantic understanding based on next-sentence prediction, whereas ranking architectures traditionally assume query term repetition indicates higher relevance. In reality, both are likely important for relevance ranking.",null,null
49,3 EXPERIMENT,null,null
50,3.1 Experimental setup,null,null
51,"Datasets. We evaluate our approaches using two datasets: Trec Robust 2004 and WebTrack 2012­14. For Robust, we use the five folds from [7] with three folds used for training, one fold for testing, and the previous fold for validation. For WebTrack, we test on 2012­14, training each year individually on all remaining years",null,null
52,"Models. Rather than building new models, in this work we use existing model architectures to test the effectiveness of various input representations. We evaluate our methods on three neural relevance matching methods: PACRR [6], KNRM [20], and DRMM [5]. Relevance matching models have generally shown to be more effective than semantic matching models, while not requiring massive amounts of behavioral data",null,null
53,Contextualized language models. We use the pretrained ELMo,null,null
54,"2520k documents; https://trec.nist.gov/data_disks.html 350M web pages, https://lemurproject.org/clueweb09/ 4733M web pages, https://lemurproject.org/clueweb12/",null,null
55,,null,null
56,1102,null,null
57,,null,null
58,Short Research Papers 2C: Search,null,null
59,,null,null
60,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
61,,null,null
62,same data using the Vanilla BERT classifier,null,null
63,Training and optimization. We train all models using pairwise hinge loss [2]. Positive and negative training documents are selected from the query relevance judgments,null,null
64,Baselines. We compare contextualized language model performance to the following strong baselines:,null,null
65,"- BM25 and SDM [13], as implemented by Anserini [21]. Finetuning is conducted on the test set, representing the maximum performance of the model when using static parameters over each dataset.6 We do not report SDM performance on WebTrack due to its high cost of retrieval on the large ClueWeb collections.",null,null
66,- Vanilla BERT ranker. We fine-tune a pretrained BERT model,null,null
67,- TREC-best: We also compare to the top-performing topic TREC run for each track in terms of nDCG@20. We use uogTrA44xu for WT12,null,null
68,"- ConvKNRM [1], our implementation with the same training pipeline as the evaluation models.",null,null
69,- Each evaluation model when using GloVe [15] vectors.8,null,null
70,3.2 Results & analysis,null,null
71,"Table 1 shows the ranking performance using our approach. We first note that the Vanilla BERT method significantly outperforms the tuned BM25 [V] and ConvKNRM [C] baselines on its own. This is encouraging, and shows the ranking power of the Vanilla BERT model. When using contextualized language term representations without tuning, PACRR and DRMM performance is comparable to that of GloVe [G], while KNRM sees a modest boost. This might be",null,null
72,5Pilot experiments showed that a learning rate of 2e-5 was more effective on this task than the other recommended values of 5e-5 and 3e-5 by [4]. 6k1 in 0.1­4,null,null
73,,null,null
74,try curb growth population order raise,null,null
75,,null,null
76,Relevant,null,null
77,,null,null
78,0.6,null,null
79,,null,null
80,0.3,null,null
81,,null,null
82,0.7,null,null
83,,null,null
84,Non-relevant,null,null
85,,null,null
86,tuesday,null,null
87,,null,null
88,abandoned,null,null
89,,null,null
90,plan,null,null
91,,null,null
92,citywide,null,null
93,,null,null
94,curb 0.6,null,null
95,,null,null
96,0.2,null,null
97,,null,null
98,0.5,null,null
99,,null,null
100,construction,null,null
101,,null,null
102,curbing population,null,null
103,growth curbing population growth,null,null
104,curb ##ing population growth curbing population growth curbing population growth curb ##ing population growth,null,null
105,,null,null
106,"Figure 1: Example similarity matrix excerpts from GloVe, ELMo, and BERT for relevant and non-relevant document for Robust query 435. Lighter values have higher similarity.",null,null
107,,null,null
108,(a),null,null
109,,null,null
110,(b),null,null
111,,null,null
112,Figure 2:,null,null
113,,null,null
114,"due to KNRM's ability to train its matching kernels, tuning to specific similarity ranges produced by the models.",null,null
115,"To gain a better understanding of how the contextual language model helps enhance the input representation, we plot example similarity matrices based on GloVe word embeddings, ELMo representations",null,null
116,"Although the contextualized language models yield ranking performance improvements, they come with a considerable cost at inference time--a practical issue ignored in previous ranking work [14, 21]. To demonstrate this, in Figure 2(a) we plot the processing rate of GloVe, ELMo, and BERT.9 Note that the processing",null,null
117,,null,null
118,"9Running time measured on single GeForce GTX 1080 Ti GPU, data in memory.",null,null
119,,null,null
120,1103,null,null
121,,null,null
122,Short Research Papers 2C: Search,null,null
123,,null,null
124,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
125,,null,null
126,"Table 1: Ranking performance on Robust04 and WebTrack 2012­14. Significant improvements to [B]M25, [C]onvKNRM, [V]anilla BERT, the model trained with [G]lOve embeddings, and the corresponding [N]on-CEDR system are indicated in brackets",null,null
127,,null,null
128,Robust04,null,null
129,,null,null
130,WebTrack 2012­14,null,null
131,,null,null
132,Ranker,null,null
133,,null,null
134,Input Representation,null,null
135,,null,null
136,P@20,null,null
137,,null,null
138,nDCG@20,null,null
139,,null,null
140,nDCG@20,null,null
141,,null,null
142,ERR@20,null,null
143,,null,null
144,BM25 SDM [13] TREC-Best ConvKNRM Vanilla BERT,null,null
145,,null,null
146,n/a n/a n/a GloVe BERT,null,null
147,,null,null
148,0.3123 0.3749 0.4386 0.3349 [BC] 0.4042,null,null
149,,null,null
150,0.4140 0.4353 0.5030 0.3806 [BC] 0.4541,null,null
151,,null,null
152,0.1970 -,null,null
153,0.2855 [B] 0.2547 [BC] 0.2895,null,null
154,,null,null
155,0.1472 -,null,null
156,0.2530 [B] 0.1833 [BC] 0.2218,null,null
157,,null,null
158,PACRR PACRR PACRR PACRR CEDR-PACRR,null,null
159,,null,null
160,GloVe ELMo BERT BERT,null,null
161,,null,null
162,0.3535 [C] 0.3554 [C] 0.3650 [BCVG] 0.4492 [BCVG] 0.4559,null,null
163,,null,null
164,[C] 0.4043 [C] 0.4101 [C] 0.4200 [BCVG] 0.5135 [BCVG] 0.5150,null,null
165,,null,null
166,0.2101 [BG] 0.2324,null,null
167,0.2225 [BCG] 0.3080 [BCVGN] 0.3373,null,null
168,,null,null
169,0.1608 [BG] 0.1885,null,null
170,0.1817 [BCG] 0.2334 [BCVGN] 0.2656,null,null
171,,null,null
172,KNRM KNRM KNRM KNRM CEDR-KNRM,null,null
173,,null,null
174,GloVe ELMo BERT BERT,null,null
175,,null,null
176,0.3408 [C] 0.3517 [BCG] 0.3817 [BCG] 0.4221 [BCVGN] 0.4667,null,null
177,,null,null
178,0.3871 [CG] 0.4089 [CG] 0.4318 [BCVG] 0.4858 [BCVGN] 0.5381,null,null
179,,null,null
180,[B] 0.2448 0.2227,null,null
181,[B] 0.2525 [BCVG] 0.3287 [BCVG] 0.3469,null,null
182,,null,null
183,0.1755 0.1689 [B] 0.1944 [BCVG] 0.2557 [BCVG] 0.2772,null,null
184,,null,null
185,DRMM DRMM DRMM DRMM CEDR-DRMM,null,null
186,,null,null
187,GloVe ELMo BERT BERT,null,null
188,,null,null
189,0.2892 0.2867 0.2878 [CG] 0.3641 [BCVGN] 0.4587,null,null
190,,null,null
191,0.3040 0.3137 0.3194 [CG] 0.4135 [BCVGN] 0.5259,null,null
192,,null,null
193,0.2215 [B] 0.2271 [BG] 0.2459 [BG] 0.2598 [BCVGN] 0.3497,null,null
194,,null,null
195,0.1603 0.1762 [BG] 0.1977 [B] 0.1856 [BCVGN] 0.2621,null,null
196,,null,null
197,"rate when using static GloVe vectors is orders of magnitude faster than when using the contextualized representations, with BERT outperforming ELMo because it uses the more efficient Transformer instead of an RNN. In an attempt to improve the running time of these systems, we propose limiting the number of layers processed by the model. The reasoning behind this approach is that the lower the layer, the more abstract the matching becomes, perhaps becoming less useful for ranking. We show the runtime and ranking performance of PACRR when only processing only up to a given layer in Figure 2(b). It shows that most of the performance benefits can be achieved by only running BERT through layer 5; the performance is comparable to running the full BERT model, while running more than twice as fast. While we acknowledge that our research code is not completely optimized, we argue that this approach is generally applicable because the processing of these layers are sequential, query-dependent, and dominate the processing time of the entire model. This approach is a simple time-saving measure.",null,null
198,4 CONCLUSION,null,null
199,We demonstrated that contextualized word embeddings can be effectively incorporated into existing neural ranking architectures and suggested an approach for improving runtime performance by limiting the number of layers processed.,null,null
200,REFERENCES,null,null
201,"[1] Zhuyun Dai, Chenyan Xiong, James P. Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search. In WSDM.",null,null
202,"[2] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In SIGIR.",null,null
203,"[3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition.",null,null
204,"[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805",null,null
205,,null,null
206,"[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM.",null,null
207,"[6] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2018. Co-PACRR: A Context-Aware Neural IR Model for Ad-hoc Retrieval. In WSDM.",null,null
208,[7] Samuel Huston and W Bruce Croft. 2014. Parameters learned in the comparison of retrieval models using term dependencies. Technical Report,null,null
209,[8] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR.,null,null
210,"[9] Kui-Lam Kwok, Laszlo Grunfeld, H. L. Sun, and Peter Deng. 2004. TREC 2004 Robust Track Experiments Using PIRCS. In TREC.",null,null
211,"[10] Nut Limsopatham, Richard McCreadie, M-Dyaa Albakour, Craig MacDonald, Rodrygo L. T. Santos, and Iadh Ounis. 2012. University of Glasgow at TREC 2012: Experiments with Terrier. In TREC.",null,null
212,"[11] Xitong Liu, Peilin Yang, and Hui Fang. 2014. Entity Came to Rescue - Leveraging Entities to Minimize Risks in Web Search. In TREC.",null,null
213,"[12] Ryan McDonald, Yichun Ding, and Ion Androutsopoulos. 2018. Deep Relevance Ranking using Enhanced Document-Query Interactions. In EMNLP.",null,null
214,[13] Donald Metzler and W. Bruce Croft. 2005. A Markov random field model for term dependencies. In SIGIR.,null,null
215,[14] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. CoRR abs/1901.04085,null,null
216,"[15] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP.",null,null
217,"[16] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.",null,null
218,[17] Fiana Raiber and Oren Kurland. 2013. The Technion at TREC 2013 Web Track: Cluster-based Document Retrieval. In TREC.,null,null
219,"[18] Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra, and Saurabh Tiwary. 2018. Optimizing Query Evaluations Using Reinforcement Learning for Web Search. In SIGIR.",null,null
220,"[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In NIPS.",null,null
221,"[20] Chenyan Xiong, Zhuyun Dai, James P. Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In SIGIR.",null,null
222,"[21] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR.",null,null
223,"[22] Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-End Open-Domain Question Answering with BERTserini. CoRR abs/1901.04085",null,null
224,"[23] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks?. In NIPS.",null,null
225,"[24] Hamed Zamani, Mostafa Dehghani, Fernando Diaz, Hang Li, and Nick Craswell. 2018. SIGIR 2018 Workshop on Learning from Limited or Noisy Data for Information Retrieval. In SIGIR.",null,null
226,,null,null
227,1104,null,null
228,,null,null
229,,null,null

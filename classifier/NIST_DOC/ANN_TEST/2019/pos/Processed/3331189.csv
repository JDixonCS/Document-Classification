,sentence
0,"Session 3C: Fact-checking, Privacy and Legal"
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,Privacy-aware Document Ranking with Neural Signals
5,
6,"Jinjin Shao, Shiyu Ji, Tao Yang"
7,"Department of Computer Science, University of California Santa Barbara, California"
8,
9,ABSTRACT
10,"The recent work on neural ranking has achieved solid relevance improvement, by exploring similarities between documents and queries using word embeddings. It is an open problem how to leverage such an advancement for privacy-aware ranking, which is important for top K document search on the cloud. Since neural ranking adds more complexity in score computation, it is difficult to prevent the server from discovering embedding-based semantic features and inferring privacy-sensitive information. This paper analyzes the critical leakages in interaction-based neural ranking and studies countermeasures to mitigate such a leakage. It proposes a privacy-aware neural ranking scheme that integrates tree ensembles with kernel value obfuscation and a soft match map based on adaptively-clustered term closures. The paper also presents an evaluation with two TREC datasets on the relevance of the proposed techniques and the trade-offs for privacy and storage efficiency."
11,"ACM Reference Format: Jinjin Shao, Shiyu Ji, Tao Yang. 2019. Privacy-aware Document Ranking with Neural Signals. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval"
12,1 INTRODUCTION AND RELATED WORK
13,"There is a growing demand for privacy protection in Internet or cloud-based information services [14, 26]. While searchable encryption"
14,"The previous research on neural ranking falls into the following two categories: interaction-based or representation-based models [42]. The earlier work has focused on the representation-based models [24, 47] where each document and a query are separately represented as vectors through neural computation and the final ranking is based on the similarity of the two representative vectors. The recent studies have focused on interaction-based neural ranking models [16, 22, 54] where the word or term-level similarity"
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331189"
16,
17,"of a query and a document is explored first based on their embedding vectors before applying additional neural computation. These studies have shown their interaction-based models outperform the earlier representation-based models and thus our paper addresses privacy issues for three interaction-based models, more specifically DRMM [22], KNRM [54], and CONV-KNRM [16]."
18,"Ranking requires arithmetic calculations based on feature vectors and homomorphic encryption [20, 41] is one idea offered to secure data while letting the server perform arithmetic calculations without decrypting the underlying data. But such a scheme is still not computationally feasible when many numbers are involved, because each addition or multiplication is extremely slow, meanwhile homomorphic encryption does not support the ability of comparing two results required by ranking. Neural ranking involves more computational complexity than a linear method or tree ensembles, and hiding feature computation becomes even harder."
19,"Recent secure neural net research [33] addresses image classification using homomorphic encryption and two-party communication with garbled circuits, to meet a different privacy requirement"
20,
21,305
22,
23,"Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France"
24,
25,2 BACKGROUNDS AND PROBLEM SETTINGS
26,"Problem definition and feature vectors. The problem of top K document ranking is defined as follows: given a query q with multiple terms and candidate documents, the server forms a feature vector for each document and ranks these documents. A ranking feature is called raw if it is explicitly stored"
27,"An interaction-based neural ranking [16, 22, 54] can be formalized as performing the following computation flow:"
28,
29,RankingScore = N N
30,
31,"where qì and dì are two sequences of embedding vectors which can be representations for a unigram or a n-gram in a query and a document [16, 22, 54], or can be entity embeddings for existing"
32,entities in a query and a document [55]. Those embedding vectors
33,can be learned from one of many existing neural network models
34,"such as Word2Vec [39], GloVe [44] and relevance based word embedding [56]. Embedding vectors for n-grams can be generated by a convolution operation described in [16]. N N is a forward neural network to compute the final ranking score."
35,"Operator  is the interaction between query q and document d and its output is the similarity of a query term and a document term for all possible pairs from q and d. In [16, 22, 54], cosine similarity is used for measuring term similarity with a score varying from -1 to 1. Let t, w denote the cosine similarity between the term vector of t and that of another term w."
36,"Operator Ker represents the kernel value calculation, extracts term-level matching signals based on the similarity of all term"
37,"pairs from a query and a document, and generates a vector of real"
38,"values being taken as input for the forward neural computation. There are two methods for kernel computation. In the Histogram Pooling [22] method, there are R kernels and each kernel associates with an interval within [-1, 1], e.g., [0.5, 0.6). The kernel value of the j-th kernel is the number of similarity values that fall into the j-th interval [bj , bj+1]: Kj"
39,associates with a Radial Basis Function
40,exponential function and log be the natural logarithm function. The kernel value of the j-th kernel for a query term t is:
41,
42,Kj
43,
44,"(t ,"
45,
46,d)
47,
48,=
49,
50,w
51,
52,d
53,
54,exp(-
55,
56,"(t ,"
57,
58,w -
59,
60,2
61,
62,2 j
63,
64,µj
65,
66,)2
67,
68,).
69,
70,"SIGIR '19, July 21­25, 2019, Paris, France Jinjin Shao, Shiyu Ji, Tao Yang"
71,
72,The output of kernel computation is a kernel vector of size R:
73,
74,"( log K1(t, d), · · · , log KR"
75,
76,t q
77,
78,t q
79,
80,"Privacy requirement and threat model. A client owns all data and wants to outsource the search service to a cloud server which is honest-but-curious, i.e., the server will honestly follow the client's protocol, but will also try to learn any private information from the client data. The client builds an encrypted but searchable index and lets a server host such index. This paper does not consider the dynamic addition of new documents to the existing index, assuming the client can periodically overwrite the index in a cloud host server to include new content. To conduct a search query, the client sends several encrypted keywords and related information to the server. Our design only uses one round of client-server communication since multi-round active communication between the server and client"
81,The biggest threat is the leakage of query and document plaintext. A server can also be interested in query access pattern and statistical information even if the query terms are encrypted. Finally the result patterns such as the overlapping of document IDs in multiple queries may also be interesting. This paper is focused on providing a privacy protection to avoid the leakage of document plaintext and also important feature values during ranking process.
82,"The previous works on plaintext or query attacks in [9, 27, 52] assume the adversary knows partial information on term occurrence in addition to a subset of plaintext documents. By preventing the leakage of the term occurrence information of documents in a hosted dataset, threats from these attacks can be removed or greatly alleviated. Islam et al. [27] proposed the query recovery attack called IKK which can be revised to launch a plaintext attack to identify some words in an encrypted document collection. This assumes that the adversary is the server who has some prior knowledge as follows. 1) The server knows plaintext of na words that appear in this document collection, but does not know the encrypted word IDs. 2) The server knows co-occurrence probabilities of these na words in this document collection. This can be approximated by using a public dataset. 3) The server has obtained encrypted IDs of documents that contain a subset of known na words. With the above three pieces of information, the server is able to recover the encrypted word IDs for a good percentage of these na words, and detect the set of document IDs containing these encrypted word IDs. Cash et al. [9] has improved the plain text recoverability of the IKK attack with extra information such as term frequency, and pointed out that the inverted index or occurrence probabilities for a set of words can be inferred when knowing the term frequency of English words in each document. There are also attacks exploiting leaked document similarities [52]. Their attacks only work if the adversary knows occurrence frequency and co-occurrence frequency of selected terms in the entire document set."
83,"Since all of the above attacks require term occurrence and use term frequency if possible, this paper will analyze the information leakage of interaction-based neural ranking methods on term frequency and occurrence in documents, and extend or redesign some of their components with a goal of hiding such statistical text information. It is worthy to note that some advanced techniques, e.g."
84,
85,306
86,
87,"Session 3C: Fact-checking, Privacy and Legal Privacy-aware Document Ranking with Neural Signals"
88,
89,"SIGIR '19, July 21­25, 2019, Paris, France SIGIR '19, July 21­25, 2019, Paris, France"
90,
91,"ranking based on Convolutional Neural Network [43], require term"
92,positions and such information can be leaked during computation.
93,Then term occurrences in a document can be easily inferred by a
94,server. Thus this paper does not investigate such ranking models.
95,3 LEAKAGE ANALYSIS AND DESIGN CONSIDERATIONS
96,We first examine the possible leakage of information in interaction-
97,"based neural ranking in terms of term occurrence and frequency. Hiding term vectors. As mentioned above, there are three steps in the interaction-based model: interaction between query and doc-"
98,"ument terms, kernel value calculation, and forward neural network"
99,computation. Our first thought is to hide term vectors using a hash-
100,ing function while preserving cosine similarities
101,metrics) between vectors. Such a protection can mask identities of
102,"term vectors; however, if a server is allowed to observe the result of interaction between a query term t and each term w of document d, it can easily infer frequencies and occurrences of all query terms as follows: T F"
103,Given the fact that we cannot store masked term vectors or
104,"explicitly store the interaction matrix elements, we resort the fol-"
105,lowing strategy where the result of interaction between query and
106,"document terms is not computed by the server. Kernel-level protection. Our next design idea is to provide a kernel-level protection of privacy by precomputing kernel values in advance. Thus only the output of Ker is exposed to the server, which hides the vector computation process and the result of in-"
107,teraction. Namely the owner of the dataset
108,which yields the leakage of term occurrence.
109,"Notice for both histogram pooling and kernel pooling, the last kernel KR"
110,"Proposition 3.1. Given query term t and document d, in histogram pooling for the R-th kernel whose interval is [1, 1], term frequency TF(t, d) = w d 1t,w =1 = exp(aR )."
111,"For the R-th kernel derived with kernel pooling [16, 54], µR is 1.0, and R is chosen to be a small positive real number, e.g., 0.001. We define the maximum cosine similarity between any two different terms in a vocabulary V where V is the collection of all terms in"
112,
113,the given dataset:
114,
115,"S¯ = max t, w ."
116,"t,w V ,t w"
117,
118,The following theorem shows a server can still approximate term frequency T F
119,
120,"Theorem 3.2. Given a query term t and a document d, in kernel"
121,
122,"pooling for the R-th kernel, if S¯ < 1.0 -"
123,
124,2R2
125,
126,ln
127,
128,n
129,
130,","
131,
132,then
133,
134,"|TF(t, d)"
135,
136,-
137,
138,"exp(aR )| < , where  is a small real value."
139,
140,"In our tested datasets in Section 6, S¯ is below 0.9. Using the"
141,
142,"above theorem, condition S¯  0.947 < 1.0 -"
143,
144,2R2
145,
146,ln
147,
148,n
149,
150,is
151,
152,true
153,
154,"when R  0.01,  = 0.01, and n  10, 000, and a server can easily"
155,
156,infer the frequency of a term in a document. Thus we are unable to
157,
158,use aR and the next section will present a solution to address this.
159,
160,It should be noted that the above analysis is true for computing
161,
162,the interaction between a unigram query term with all unigrams of
163,
164,a document in DRMM [22] and KNRM [54]. When computing the
165,"interaction of a h-gram from a query and a -gram from a document where h  in CONV-KNRM [16], the cosine similarity of such a"
166,
167,"pair cannot be 1. Thus for CONV-KNRM, we only need to worry"
168,
169,about the interaction of two terms with the same gram length.
170,
171,4 PRIVACY-AWARE NEURAL RANKING
172,"In this section, we propose three techniques for privacy-aware neural ranking: 1) replace the exact match kernel value with a traditional ranking method that uses exact word matching; 2) provide a soft match index"
173,
174,4.1 Replacement of the Exact Match Kernel
175,Neural signals can be considered to be composed of two parts: exact match component represented by last kernel value KR
176,We adopt a privacy-aware learning-to-rank tree ensemble model in [30] that encodes raw features with comparison preserving mapping
177,The above replacement is applied for computing the unigramto-unigram interaction in DRMM and KNRM. It is also applicable
178,
179,307
180,
181,"Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France"
182,
183,Relevance score
184,
185,Relevance score
186,
187,Neural network computation
188,
189,Neural network computation
190,
191,......
192,
193,......
194,
195,Soft match kernels
196,
197,Exact match kernel
198,
199,Obfuscated soft match kernels
200,
201,Private tree ensemble
202,
203,Encrypted
204,
205,(a)
206,
207,(b)
208,
209,raw features
210,
211,Figure 1: Replacement of the exact match kernel
212,
213,"to the interaction of a h-gram query term with a h-gram document term for CONV-KNRM. We discuss more on this in Section 6. This replacement comes with two advantages. First, it removes the source of term frequency leakage in aR since an adversary is not able to recover feature values encoded with CPM [30]. Second, it can potentially boost ranking performance. Currently there is no known method to combine a traditional ranking method with a neural ranking model for a better relevance. A tree ensemble method has been proven to be effective before neural models gain more attention. For example, in the Yahoo! learning-to-rank challenge [13] in 2010, all winners have used some forms of tree ensembles. This replacement can provide a natural way to effectively combine a tree ensemble with a representation based neural model and we will evaluate the relevance impact in Section 6."
214,
215,4.2 Obfuscation of Kernel Values
216,"Even though we have precomputed the kernel value computation to avoid the leakage of term frequency to the server, there still exists a term frequency attack described in Appendix A when the histogram pooling is used. In that attack, the frequency of a term queried can be uncovered by a server if it is able to find all or many of encrypted keys"
217,"To minimize the chance of leaking exact kernel values, we propose a many-to-one mapping to obfuscate kernel vector values. Intuitively, if kernel vector values from multiple different term document interactions are indistinguishable, the adversary has to make random guesses on real kernel vector values. In specific, we add the ceiling function to convert the floating point number to an integer in forming the kernel vector, and this change allows the revised soft match kernel values to accomplish k-anonymization [18, 50]. For the j-th element of a kernel vector based on R kernels,"
218,
219,aj =
220,
221,logr
222,
223,if Kj
224,
225,"where the logarithmic base r is a privacy parameter that can be adjusted. As we show later in Section 5.2, the anonymous factor k is r R-1 - 1, since all zero kernel values are converted to 1. A large r value will add more anonymity for privacy while it may degrade"
226,the relevance performance due to the lack of value differentiation
227,in kernel vectors.
228,
229,"SIGIR '19, July 21­25, 2019, Paris, France"
230,"Jinjin Shao, Shiyu Ji, Tao Yang"
231,"In DRMM, at the forward neural computation stage, the kernel values are re-scaled by document frequency weights of query terms. We use the same many-to-one function discussed above to obfuscate these weights. Our evaluation shows there is no visible relevance difference when r = 10."
232,4.3 Soft Match Maps
233,"Kernel vector values are precomputed before query processing and such offline processing can be done efficiently on a parallel platform and/or with LSH approximation [31]. However, it is too expensive to store kernel vectors for all possible"
234,"Inspired by the inverted index, we propose a soft match index structure that contains kernel vectors of term-document pair"
235,"Even though terms and documents in a soft match are encrypted and identified through numeral IDs, an adversary may infer the the occurrence of terms in a document which is a critical piece of information for privacy attack discussed in Section 2. In order to minimize the chance of leaking term occurrence in a document, we introduce the notion of  -similar term closure."
236,"Definition 1. A set of terms C under vocabulary V is called a  -similar term closure if for any term t  C and there exists another term w  V such that t, w   , then w  C."
237,Here V is the collection of terms in a given dataset and we will discuss a clustering algorithm shortly that groups a set of terms as a term closure.
238,Definition 2. A soft match map SMM is closed under term closures if for any
239,"There are two advantages of a closed soft match map. 1) From the relevance point view, a document that contains a word which is similar to a query word that can get some rank credit as the privacy-aware ranking only uses this soft match map to identify semantically related documents. 2) As shown in the next section, an adversary would have a hard time to detect if a word ID that appears in the document or not because other similar words in its term closure have all appeared in the soft match map. We will"
240,
241,308
242,
243,"Session 3C: Fact-checking, Privacy and Legal Privacy-aware Document Ranking with Neural Signals"
244,
245,"SIGIR '19, July 21­25, 2019, Paris, France SIGIR '19, July 21­25, 2019, Paris, France"
246,
247,Car
248,0.734 0.726
249,0.715
250,Vehicle
251,
252,0.279
253,
254,0.524
255,
256,Truck
257,
258,Flatbed
259,
260,0.305
261,
262,"Clustering with fixed threshold: C1: {Car, Truck, Vehicle, Flatbed, ...}"
263,"Adaptive clustering: C1: {Car, Truck, Vehicle}, C2: {FlatBed, ...}"
264,
265,Figure 2: Two clustering methods for term closure
266,
267,analyze its privacy implication based on the notion of statistical indistinguishability in the next section.
268,"In the rest of the paper, we will assume a soft match map is closed. We now describe how to partition a term vocabulary of a dataset into a disjoint set of term closures. There are trade-off factors to consider in controlling the size of term closures. With a larger size, the soft match map will accommodate more similar terms that can improve relevance, while creating more challenges for an adversary to distinguish and detect which term IDs in a soft match map appear in a document. On the other hand, a larger size demands more storage to host a soft match map. In the following, we discuss two algorithms that derive a disjoint set of term closures. Clustering with a fixed similarity threshold. Given a clustering threshold  , we cluster all terms in a closure C using a transitive closure computation as follows: if t  C, and t, w   , then w  C. This approach uses a uniform threshold for all clusters. With a small clustering threshold value, some clusters can have a very big size, which can result in a very large storage demand. With a large threshold value, some of term closures have a very small size, not big enough for the privacy purpose. Adaptive clustering with multiple thresholds and closure size control. Given p as a targeted closure size, and m sorted clustering thresholds 1 > 2 > · · · > m . We first apply a similarity clustering with a fixed threshold 1. We remove all clusters with the size no less than p. For the remaining terms, we apply a similarity clustering with a fixed threshold 2. Repeat this process until all cluster sizes are no less than p or we have applied clustering with all thresholds. This adaptive clustering provides a flexibility to group a sufficient number of similar terms in each closure while yielding a reduced storage demand. In our evaluation, p = 5 is used."
269,"Figure 2 shows the difference of the above two clustering for a partial similarity graph of 4 terms from one of our testing datasets. The edges represent pairwise similarity scores. With fixed similarity threshold at 0.5, all four terms ""Car"", ""Truck"", ""Vehicle"", and ""Flatbed"" are clustered transitively into the same term closure. With adaptive clustering using a threshold set {0.9, 0.8, 0.7, 0.6, 0.5}, and closure target size 3, term ""Flatbed"" is not grouped with ""Car"", ""Truck"", and ""Vehicle"". This is because that when threshold 0.7 is used, terms ""Car"", ""Truck"", and ""Vehicle"" are grouped, reaching closure size 3 and they are removed to form a separate closure. ""Flatbed"" will then be grouped with other terms in the rest of the graph"
270,
271,5 LEAKAGE AND PRIVACY OF SOFT MATCH MAPS
272,5.1 Search Process and Leakage Profile
273,The top K search process is described as follows. A client first sends randomized tokens for query terms and related information
274,"trapdoor information [1]) to a server, and these terms include query"
275,
276,"unigrams and/or multi-grams when needed. The server cannot map tokens into query terms since tokens are randomized. After the server receives tokens for all the query terms, a privacy-aware document retrieval model based on [1, 5, 6, 10, 11] produces a set of document candidates for further ranking. The server first computes the keys to access the CPM-encoded features and run a tree ensemble, where the leakage profile is studied in [1]. Then the server computes the keys to access the soft match map, and fetches associated kernel vectors to drive the forward neural computation."
277,"Depending on the query processing semantics and privacy requirement, there are two methods to pass the query term list and document list to neural ranking, and each of which has a different leakage profile. The first method is based on the private search work of [1, 11]. Based on trapdoor information sent from a client, the server can compute and obtain a key"
278,· Initially the server does not know any key
279,· Once the server knows a key
280,"The second method is based on the work in [5, 6, 10], the server receives a list of query term IDs and a list of candidate documents ID. It then computes each key"
281,"and kernel vectors, and also is able to build a partial soft inverted index for all queried terms. Based on the soft inverted index, the server is able to estimate partial document similarities based on the overlapping degree of soft terms between two documents. · Once all terms and documents are queried or processed, the server is able to build a complete soft forward index and soft inverted index. Namely give a list of documents that contain a soft term, and give a list of soft terms included in a document. By using the soft term postings, the server learns the membership information of each soft term closure."
282,"Since the information listed above is slowly leaked as more queries are processed, we propose to re-index the dataset periodically and replace the index in the cloud with different term IDs during each update to mitigate the chance of letting the server exploit the entire soft inverted index. The next two subsections study the privacy properties with respect to exact term occurrence even if the entire soft index is leaked to a server adversary, since such information is required for the plaintext attacks discussed in the last part of Section 2. We will also discuss k-anonymity for kernel vector values."
283,5.2 k-anonymization of Kernel Value Vectors
284,We show our obfuscation mapping achieves k-anonymization
285,
286,309
287,
288,"Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France"
289,
290,"Definition 3. [50] Let V be a vector of real values representing R - 1 kernel values namely, V = [K1(t, d), K2(t, d), · · · , KR-1(t, d)]. Let V  = F"
291,It is easy to show that the application of the above ceiling func-
292,"tion to the above logarithmic mapping is k-anonymous. In the first group, there are r values 0, 1, 2, ...,"
293,of kernel values that the adversary cannot distinguish from the
294,"real one. In our evaluation, we choose r = 10 and R = 20. Thus r R-1 = 1019, which is a very large number."
295,"Proposition 5.1. The logarithmic mapping with ceiling obfuscation is k-anonymous with respect to soft match kernel values, where k = r R-1."
296,
297,5.3 Obfuscation of Exact Term Occurrence
298,
299,"How strong a closed soft match map can be in avoiding the leakage of term occurrence? If an adversary including a server tries to detect an exact term of a document from a soft match map which contains both soft and exact terms with encrypted term IDs, we argue that other similar soft terms from the same closure behavior closely by looking at the structure and the kernel values in this soft match map for this document and the adversary should have a hard time to differentiate. We justify this argument based on the notion of statistical indistinguishability used in the cryptographic literature [4, 21]."
300,
301,Definition 4. -statistical indistinguishability. Any two dis-
302,
303,tributions P and Q over a finite set U are -statistically indistin-
304,
305,"guishable if their statistical distance SD(P, Q)  , where statistical"
306,
307,"distance is defined as [4, 21] SD(P, Q) = 1"
308,2
309,
310,x U |P(x) - Q(x)|.
311,
312,"If two distributions P and Q are -statistically indistinguishable,"
313,
314,then no adversary can successfully distinguish the samples from P
315,
316,and
317,
318,Q
319,
320,with
321,
322,probability
323,
324,more
325,
326,than
327,
328,1 2
329,
330,+
331,
332,
333,
334,(Theorem
335,
336,3.11
337,
338,in
339,
340,[4]).
341,
342,For
343,
344,"our context, we define the distribution of a soft kernel value vector"
345,
346,as a discrete distribution over the soft kernels with respect to a given
347,
348,"term paired with a different document. In particular, given the ker-"
349,
350,"nel vectors for documents d and d : fìt,d ="
351,
352,"fìt , d"
353,
354,=
355,
356,"(a , , ·"
357,1
358,
359,·
360,
361,·
362,
363,", aj,"
364,
365,·
366,
367,·
368,
369,·
370,
371,", aR )T"
372,
373,","
374,
375,where
376,
377,each
378,
379,ai
380,
381,and
382,
383,ai
384,
385,may
386,
387,be
388,
389,obfus-
390,
391,"cated, the statistical distance between fìt,d and fìt,d is defined as"
392,
393,"SD(fìt,d ,"
394,
395,"fìt,d )"
396,
397,=
398,
399,1 2
400,
401,R-1 i =1
402,
403,|ai
404,
405,- ai |.
406,
407,Definition 5. -statistically indistinguishable soft match
408,
409,"map. A soft match map is -statistically indistinguishable if for any document d and for any term w in d, for any document d  constructed"
410,
411,"by replacing each term w in d with a subset of the closure C such that w  C, the soft kernel values of"
412,
413,indistinguishable.
414,
415,"SIGIR '19, July 21­25, 2019, Paris, France"
416,"Jinjin Shao, Shiyu Ji, Tao Yang"
417,"In ClueWeb09 Dataset, using our algorithm where logarithmic base for kernel value obfuscation is r = 10, the derived soft match map is a -statistically indistinguishable with  being 0.004."
418,"Theorem 5.2. Given a -statistically indistinguishable soft match map for document set D in which each term closure has at least p terms, if a server can derive the document occurrence of exact terms with N term-document pairs, there exist"
419,"Theorem 5.2 shows that for any soft match map generated by a document set D, there are at least"
420,6 EVALUATION
421,"Here we evaluate relevance scores of the proposed privacy-aware neural ranking techniques using two TREC datasets and assess trade-offs of privacy and relevance, and storage and time cost. Datasets, features, and training. We use the following TREC test collections to do evaluations. 1) Robust04 uses TREC Disks 4 & 5"
422,"Candidate documents with their encrypted feature vectors are retrieved from the inverted index built for the above datasets, following the work in [1, 12, 34]. For a privacy-aware tree ensemble, we use CPM [30] with LambdaMART based on RankLib 2.5 [17]. In each fold of training ranking model, 5-fold cross validation is used to select the best model based on NDCG@20, varying the number of leaves from 2 to 30 and the number of trees from 100 to 500."
423,"To evaluate the impact of feature choices with the integration of the tree ensemble on the final ranking relevance, we have three options of features listed as follows."
424,"1) G0 with term frequency features: BM25 scores for query terms in the title field, and BM25 scores for query terms in the body field of each document. TF-IDF scores for query terms in the title field, and TF-IDF scores for query terms in the body field of each document. 2) G1 with term frequency and proximity features: All features from G0, the squared minimum distance reciprocal of query term pairs in the title field, and the squared minimum distance reciprocal [2, 19, 51, 57] of query term pairs in the body"
425,
426,310
427,
428,"Session 3C: Fact-checking, Privacy and Legal"
429,Privacy-aware Document Ranking with Neural Signals
430,"field of each document. 3) G2 with term frequency, proximity, and page quality features: All features from G1, PageRank, and a binary flag indicating whether a document is from Wikipedia. This group is only for ClueWeb09 Category B."
431,"The baseline models are DRMM, KNRM, and CONV-KNRM trained with 5-fold cross validation. We also choose a variant of CONV-KNRM, denoted by CONV-KNRM. For CONV-KNRM, we only use the interactions between query unigrams and document unigrams, between query unigrams and document bigrams, and between query bigrams and document unigrams. The interaction between query bigrams and document bigrams are not included to reduce storage space need. For both histogram pooling and kernel pooling, R=30 kernels are used. All soft match kernels are equally distributed in the cosine range. In kernel pooling,  is 0.10 for all soft match kernels. In CONV-KNRM, n-gram length is 2, and the number of CNN filters is 128 as used in the original work. All word embedding vectors are pre-trained, and are fixed in KNRM, CONVKNRM and CONV-KNRM. We use 300 dimension word embedding vectors trained on TREC Disks 4 & 5 or ClueWeb09 Category-Cat-B with Skip-gram + Negative sampling model [39]. All terms that appear less than 5 times are removed from embedding training."
432,"We assess the use of the following 3 techniques denoted with T, O, and C where T stands for the replacement of the exact match kernel with LambdaMART/CPM, O stands for kernel value obfuscation, and C stands for using a closed soft match map. Notation A/T means ranking A with technique T while A/TOC means ranking A with all 3 techniques. All NDCG [29] values are within confidence interval ±0.01 with p-value < 0.05. Impact of replacing the exact match kernel with a LambdaMART/CPM tree ensemble. Table 1 shows the NDCG relevance of the three neural ranking models as the baseline and relevance after the replacement of the exact match kernel with LambdaMART/CPM based on the three groups of features G0, G1, and G2. Soft match maps and kernel value obfuscation are not incorporated. The boldfaced numbers are the highest NDCG scores within each ranking model. From this table we observe that neural ranking with the use of LambdaMART/CPM trees outperforms the original baseline in NDCG at all Positions 1, 3, 5, and 10 for ClueWeb. For example, compared with CONV-KNRM, CONV-KNRM/T with G2 can improve NDCG@1, NDCG@3, NDCG@5 and NDCG@10 by 2.23%, 2.45%, 2.67% and 4.23% on ClueWeb. For Robust04, tree ensemble integration delivers up to 3.91% improvement for CONVKNRM and up to 8.02% for KNRM, but degrades by up to -8.18% for DRMM. Comparing the use of G0, G1, and G2 for neural ranking integration, G2 is still most effective for ClueWeb and G1 is most effective for Robust04, which shows traditional signals still make a good contribution."
433,"We examine NDCG scores reported in the previous work. For ClueWeb09-Cat-B, NDCG scores at Positions 1, 10 and 20 are 0.294, 0.289, 0.287 with CONV-KNRM in [16]. Our numbers are slightly higher, which can be caused by different data processing. Notice NDCG@20 in our run is 0.2950."
434,"By comparing CONV-KNRM and CONV-KNRM, the absence of bigram-bigram interaction does yield a loss of NDCG score. For example, the loss is 8.56%, 6.29%, 7.04% and 6.96% for ClueWeb at Positions 1, 3, 5, and 10, respectively. That represents a tradeoff of privacy and relevancy. Adding the tree ensemble integration, most"
435,
436,"SIGIR '19, July 21­25, 2019, Paris, France"
437,"SIGIR '19, July 21­25, 2019, Paris, France"
438,"NDCG scores for CONV-KNRM/T can be on par with those of CONV-KNRM and are 4.31% better for ClueWeb09 at Position 10. Impact of kernel value obfuscation. Table 2 shows the impact of kernel value obfuscation on NDCG scores incorporating LambdaMART/CPM with G2 for ClueWeb and G1 for Robust04. We choose two different logarithmic base r here: 5 and 10. Overall, the relevance with r = 5 is slightly better than r = 10, while both of them result in degradation in ranking accuracy compared with no obfuscation. For NDCG@1, the degradation with r = 10 is 1.7% for CONV-KNRM, 6.36% for KNRM, and 6.3% for DRMM. For CONVKNRM, its degradation is relatively smaller. Trade-offs between relevancy and storage efficiency. Table 3 studies the impact of using a closed soft match map with two clustering methods for term closures under different thresholds. This table is for CONV-KNRM/TOC only as this model delivers the highest NDCG scores with all privacy preserving techniques. For example, with the obfuscation base being 10, and the clustering threshold being 0.7, for DRMM/TOC in ClueWeb, NDCG@5 and NDCG@10 are 0.2704 and 0.2720, respectively. For KNRM/TOC in ClueWeb, NDCG@5 and NDCG@10 are 0.2972 and 0.2912, respectively. In Table 3, Column 6 in the middle shows the storage need in gigabytes to store a soft match map and related data after clustering with fixed thresholds while last column on the right is the storage demand with adaptive clustering. Each entry has two numbers X(Y). Y is the total storage for unigram-unigram interaction while X is the total storage for unigram-unigram, unigram-bigram, and bigram-unigram interaction. Number Y also represents the amount of storage space needed for KNRM and DRMM."
439,"While clustering with a fixed threshold yields some relevance improvement over adaptive clustering, it requires an excessive amount of storage space. The adaptive clustering threshold 0.7 is a good trade-off with an acceptable storage space for hosting the ClueWeb dataset. In this setting, the relevance of CONV-KNRM is on par with the original CONV-KNRM baseline, lower than CONVKNRM/T. That represents a trade-off of relevancy, privacy and storage cost. It still requires 7.627TB space and we can use a number of high-end SSDs with parallel I/O. The latest high-end SSD products from Intel [48] and Samsung [38, 46] have achieved 1015µs IO latency with up-to 750K I/O operations per second. Thus for a soft match map hosted at a high-end SSD, the I/O access time of processing one query can still be reasonable. Estimation of online query processing time. The online query processing time cost consists of 3 phases: 1) private result retrieval and preliminary ranking, 2) private tree ensemble scoring , 3) neural ranking. Based on [1], Phase I costs 460 ms on average for ClueWeb. For re-ranking top 1,000 candidate documents, there are about upto 10K IO operations needed to fetch kernel vectors and features, and the total I/O time for accessing SSDs can take around 100 to 150ms with the above fast SSD performance parameters. Our experiments show that private tree ensemble scoring takes less than 2ms and all three neural models with TOC take about 10 ms or less. Thus overall the online query processing time is about 572ms to 622ms on average for ClueWeb. Notice that CONV-KNRM requires computation of term vectors and interaction matrices which could take 4-5 seconds. Thus even though our design pays extra cost in space, it does remove the expensive time spent for interaction computation."
440,
441,311
442,
443,"Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France"
444,
445,"SIGIR '19, July 21­25, 2019, Paris, France Jinjin Shao, Shiyu Ji, Tao Yang"
446,
447,Model
448,
449,Table 1: Relevance impact of replacing exact match kernel with a tree ensemble
450,
451,Feature group
452,
453,ClueWeb09-Cat-B
454,
455,Robust04
456,
457,for ensemble NDCG@1 NDCG@3 NDCG@5 NDCG@10 NDCG@1 NDCG@3 NDCG@5 NDCG@10
458,
459,LambdaMART/CPM DRMM DRMM/T KNRM KNRM/T
460,CONV-KNRM CONV-KNRM/T CONV-KNRM CONV-KNRM/T
461,
462,G0 G1 G2
463,Baseline G0 G1 G2
464,Baseline G0 G1 G2
465,Baseline G0 G1 G2
466,G0 G1 G2
467,
468,0.2498 0.2818 0.2893
469,0.2586
470,0.2635 0.2838 0.2887
471,0.2663
472,0.2736 0.3036 0.2999
473,0.3155
474,0.3031 0.3254 0.3225
475,0.2884
476,0.3038 0.3276 0.3175
477,
478,0.2702 0.2725 0.2828
479,0.2659
480,0.2721 0.2778 0.2857
481,0.2739
482,0.2804 0.2974 0.3097
483,0.3124
484,0.3088 0.3187 0.3200
485,0.2927
486,0.2998 0.3099 0.3122
487,
488,0.2571 0.2688 0.2873
489,0.2659
490,0.2623 0.2772 0.2822
491,0.2693
492,0.2798 0.2951 0.3154
493,0.3126
494,0.3154 0.3177 0.3210
495,0.2906
496,0.2962 0.3099 0.3239
497,
498,0.2415 0.2653 0.2827
499,0.2634
500,0.2503 0.2645 0.2793
501,0.2681
502,0.2725 0.2903 0.3147
503,0.3085
504,0.3052 0.3099 0.3216
505,0.2870
506,0.2933 0.3117 0.3218
507,
508,0.4819 0.5181
509,-
510,0.5049
511,0.4993 0.5114
512,-
513,0.4983
514,0.5158 0.5382
515,-
516,0.5373
517,0.5402 0.5556
518,-
519,0.5007
520,0.5149 0.5404
521,-
522,
523,0.4465 0.4610
524,-
525,0.4872 0.4594 0.4658
526,-
527,0.4812
528,0.4908 0.5063
529,-
530,0.4875 0.5057 0.5042
531,-
532,0.4702
533,0.4827 0.5006
534,-
535,
536,0.4257 0.4346
537,-
538,0.4747 0.4425 0.4501
539,-
540,0.4647
541,0.4768 0.4906
542,-
543,0.4742
544,0.4894 0.4927
545,-
546,0.4601
547,0.4768 0.4892
548,-
549,
550,0.3982 0.4044
551,-
552,0.4528 0.4134 0.4158
553,-
554,0.4527
555,0.4592 0.4673
556,-
557,0.4586
558,0.4643 0.4693
559,-
560,0.4510
561,0.4535 0.4657
562,-
563,
564,Table 2: Impact of kernel value obfuscation with different logarithmic bases
565,
566,Model
567,
568,Obfuscation
569,
570,ClueWeb09-Cat-B
571,
572,Robust04
573,
574,base
575,
576,DRMM/TO
577,
578,Yes(10) Yes(5)
579,No
580,
581,0.2703 0.2769 0.2887
582,
583,0.2731 0.2757 0.2857
584,
585,0.2740 0.2753 0.2822
586,
587,0.2732 0.2722 0.2793
588,
589,0.5078 0.5110 0.5114
590,
591,0.4681 0.4669 0.4658
592,
593,0.4449 0.4446 0.4501
594,
595,0.4157 0.4221 0.4158
596,
597,KNRM/TO
598,
599,Yes(10) Yes(5)
600,No
601,
602,0.2808 0.2875 0.2999
603,
604,0.2929 0.2968 0.3097
605,
606,0.2947 0.2988 0.3154
607,
608,0.2906 0.2971 0.3147
609,
610,0.5117 0.5100 0.5382
611,
612,0.4639 0.4686 0.5063
613,
614,0.4393 0.4451 0.4906
615,
616,0.4130 0.4164 0.4673
617,
618,CONV-KNRM/TO
619,
620,Yes(10) Yes(5)
621,No
622,
623,0.3121 0.3178 0.3175
624,
625,0.3097 0.3067 0.3122
626,
627,0.3165 0.3161 0.3239
628,
629,0.3100 0.3100 0.3218
630,
631,0.5221 0.5306 0.5404
632,
633,0.4980 0.4987 0.5006
634,
635,0.4906 0.4893 0.4892
636,
637,0.4623 0.4613 0.4657
638,
639,Table 3: NDCG score and storage demand for CONV-KNRM/TOC
640,
641,Similarity
642,
643,Clustering with fixed threshold
644,
645,threshold NDCG@1 NDCG@3 NDCG@5 NDCG@10 Storage
646,
647,Adaptive clustering NDCG@1 NDCG@3 NDCG@5 NDCG@10 Storage
648,
649,Robust04
650,
651,0.3
652,
653,0.5225 0.4974 0.4915
654,
655,0.4621
656,
657,"45,021"
658,
659,0.5127 0.4892 0.4845
660,
661,0.4582
662,
663,"1,512"
664,
665,0.5
666,
667,0.5154 0.4883 0.4780
668,
669,0.4543
670,
671,"24,793"
672,
673,0.5078 0.4845 0.4756
674,
675,0.4498
676,
677,"1,133"
678,
679,0.7
680,
681,0.4886 0.4644 0.4486
682,
683,0.4169
684,
685,287
686,
687,0.4899 0.4608 0.4414
688,
689,0.4110
690,
691,278
692,
693,0.9
694,
695,0.4953 0.4594 0.4415
696,
697,0.4091
698,
699,261
700,
701,0.4913 0.4558 0.4397
702,
703,0.4090
704,
705,261
706,
707,ClueWeb09-Cat-B
708,
709,0.3
710,
711,0.3136 0.3078 0.3149
712,
713,0.3091 1.7 · 106
714,
715,0.3069
716,
717,0.3142
718,
719,"0.3120 46,811"
720,
721,0.5
722,
723,0.3073 0.3069 0.3114
724,
725,0.3069 1.3 · 106
726,
727,0.3037
728,
729,0.3113
730,
731,"0.3103 35,742"
732,
733,0.7
734,
735,0.3064 0.3048 0.3122
736,
737,0.3104
738,
739,"16,568"
740,
741,0.3067 0.3012 0.3088
742,
743,"0.3060 7,627"
744,
745,0.9
746,
747,0.3069 0.3041 0.3105
748,
749,0.3074
750,
751,"7,369"
752,
753,0.2963 0.3025 0.3117
754,
755,"0.3083 7,369"
756,
757,312
758,
759,"Session 3C: Fact-checking, Privacy and Legal"
760,Privacy-aware Document Ranking with Neural Signals
761,7 CONCLUSION
762,The main contribution of this paper is a privacy-aware neural ranking scheme integrated with a tree ensemble for server-side top K document search. The key techniques include the replacement of
763,"the exact kernel with a tree ensemble, a soft match map using obfus-"
764,"cated kernel values and term closures, and adaptive clustering for"
765,term occurrence obfuscation and storage optimization. Our design
766,for privacy enhancement is to prevent the leakage of two critical
767,text signals in terms of term frequency and occurrence needed for
768,the attacks shown in the previous work and this paper.
769,The evaluation with two TREC datasets shows that the NDCG
770,can be improved noticeably by replacing the exact match kernel
771,of neural ranking with a LambdaMART tree ensemble. The obfus-
772,cation of kernel values does carry a modest relevance trade-off
773,for privacy. The adaptive clustering for term closures significantly
774,reduces the storage demand with some trade-off in relevance.
775,ACKNOWLEDGMENTS
776,This work is supported in part by NSF IIS-1528041 and a Google
777,faculty research award. It has used the NSF-supported resource
778,in the Extreme Science and Engineering Discovery Environment
779,"(XSEDE) under allocation IRI190005. Any opinions, findings, con-"
780,clusions or recommendations expressed in this material are those
781,of the authors and do not necessarily reflect the views of the NSF.
782,REFERENCES
783,"[1] Daniel Agun, Jinjin Shao, Shiyu Ji, Stefano Tessaro, and Tao Yang. 2018. Privacy and efficiency tradeoffs for multiword top k search with linear additive rank scoring. In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee, 1725­1734."
784,"[2] Jing Bai, Yi Chang, Hang Cui, Zhaohui Zheng, Gordon Sun, and Xin Li. 2008. Investigation of partial query proximity in web search. In Proceedings of the 17th international conference on World Wide Web. ACM, 1183­1184."
785,"[3] Alexandra Boldyreva, Nathan Chenette, and Adam O'Neill. 2011. Orderpreserving encryption revisited: Improved security analysis and alternative solutions. In Annual Cryptology Conference. Springer, 578­595."
786,[4] Dan Boneh and Victor Shoup. 2015. A graduate course in applied cryptography. Draft 0.2
787,"[5] Raphael Bost. 2016. oo : Forward Secure Searchable Encryption. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. ACM, 1143­1154."
788,"[6] Raphael Bost and Pierre-Alain Fouque. 2017. Thwarting Leakage Abuse Attacks against Searchable Encryption ­ A Formal Approach and Applications to Database Padding. Cryptology ePrint Archive, Report 2017/1060."
789,"[7] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581"
790,"[8] Ning Cao, Cong Wang, Ming Li, Kui Ren, and Wenjing Lou. 2014. PrivacyPreserving Multi-Keyword Ranked Search over Encrypted Cloud Data. IEEE Trans. Parallel Distrib. Syst. 25, 1"
791,"[9] David Cash, Paul Grubbs, Jason Perry, and Thomas Ristenpart. 2015. Leakageabuse attacks against searchable encryption. In CCS'15. ACM, 668­679."
792,"[10] David Cash, Joseph Jaeger, Stanislaw Jarecki, Charanjit S Jutla, Hugo Krawczyk, Marcel-Catalin Rosu, and Michael Steiner. 2014. Dynamic Searchable Encryption in Very-Large Databases: Data Structures and Implementation.. In NDSS, Vol. 14. Citeseer, 23­26."
793,"[11] David Cash, Stanislaw Jarecki, Charanjit S. Jutla, Hugo Krawczyk, Marcel-Catalin Rosu, and Michael Steiner. 2013. Highly-Scalable Searchable Symmetric Encryption with Support for Boolean Queries. In CRYPTO 2013. 353­373."
794,[12] David Cash and Stefano Tessaro. 2014. The Locality of Searchable Symmetric Encryption. In EUROCRYPT 2014. 351­368.
795,[13] Olivier Chapelle and Yi Chang. 2011. Yahoo! Learning to Rank Challenge Overview. J. of Machine Learning Research
796,"[14] Benny Chor, Eyal Kushilevitz, Oded Goldreich, and Madhu Sudan. 1998. Private Information Retrieval. J. ACM 45, 6"
797,"[15] Reza Curtmola, Juan Garay, Seny Kamara, and Rafail Ostrovsky. 2011. Searchable symmetric encryption: improved definitions and efficient constructions. Journal"
798,
799,"SIGIR '19, July 21­25, 2019, Paris, France"
800,"SIGIR '19, July 21­25, 2019, Paris, France"
801,"of Computer Security 19, 5"
802,"neural networks for soft-matching n-grams in ad-hoc search. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 126­134."
803,[17] Van Dang. 2012. RankLib. https://sourceforge.net/p/lemur/wiki/RankLib/.
804,Accessed: 2018-05-20.
805,"[18] Dotan Di Castro, Liane Lewin-Eytan, Yoelle Maarek, Ran Wolff, and Eyal Zohar. 2016. Enforcing k-anonymity in web mail auditing. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, 327­336."
806,"[19] Tamer Elsayed, Nima Asadi, Lidan Wang, Jimmy J. Lin, and Donald Metzler. 2010. UMD and USC/ISI: TREC 2010 Web Track Experiments with Ivory. In Proceedings of the 19th Text REtrieval Conference, TREC 2010, Gaithersburg, Maryland, USA."
807,"[20] Craig Gentry. 2009. Fully Homomorphic Encryption Using Ideal Lattices. In STOC '09. ACM, 169­178."
808,"[21] Oded Goldreich. 2000. Foundations of Cryptography: Basic Tools. Cambridge University Press, New York, NY, USA."
809,"[22] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval. In Proceedings of CIKM'16. ACM, 55­64."
810,"[23] Haibo Hu, Jianliang Xu, Chushi Ren, and Byron Choi. 2011. Processing private queries over untrusted data cloud through privacy homomorphism. In ICDE. 601­612."
811,"[24] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry"
812,"Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of CIKM'13. ACM, 2333­2338. [25] Muhammad Ibrahim and Mark Carman. 2016. Comparing Pointwise and Listwise Objective Functions for Random-Forest-Based Learning-to-Rank. ACM Transactions on Information Systems"
813,https://www2.gemalto.com/cloud-security-research.
814,05-01.
815,"[27] Mohammad Saiful Islam, Mehmet Kuzu, and Murat Kantarcioglu. 2012. Access"
816,"Pattern disclosure on Searchable Encryption: Ramification, Attack and Mitigation. In NDSS 2012. [28] Geetha Jagannathan, Krishnan Pillaipakkamnatt, and Rebecca N Wright. 2009. A practical differentially private random decision tree classifier. In 2009 IEEE International Conference on Data Mining Workshops. IEEE, 114­121. [29] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems"
817,"[30] Shiyu Ji, Jinjin Shao, Daniel Agun, and Tao Yang. 2018. Privacy-aware Ranking with Tree Ensembles on the Cloud. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. ACM, 315­324."
818,"[31] Shiyu Ji, Jinjin Shao, and Tao Yang. 2019. Efficient Interaction-based Neural Ranking with Locality Sensitive Hashing. In The World Wide Web Conference"
819,"[32] Karen Spärck Jones, Steve Walker, and Stephen E. Robertson. 2000. A probabilistic"
820,"model of information retrieval: development and comparative experiments. In Information Processing and Management. 779­840. [33] Chiraag Juvekar, Vinod Vaikuntanathan, and Anantha Chandrakasan. 2018."
821,GAZELLE: A Low Latency Framework for Secure Neural Network Inference. In 27th USENIX Security Symposium
822,drives. https://www.theregister.co.uk/2018/01/30/samsung_launching_zssd_
823,attack_on_intel_optane_drives.
824,"[39] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013."
825,"Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111­3119. [40] Muhammad Naveed, Manoj Prabhakaran, and Carl A Gunter. 2014. Dynamic searchable encryption via blind storage. In 2014 IEEE Symposium on Security and Privacy. IEEE, 639­654. [41] Pascal Paillier. 1999. Public-Key Cryptosystems Based on Composite Degree Residuosity Classes. In EUROCRYPT '99. 223­238. [42] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2017. A deep investigation of deep IR models. In SIGIR 2017 Workshop on Neural Information Retrieval"
826,2017. Deeprank: A new deep architecture for relevance ranking in information
827,
828,313
829,
830,"Session 3C: Fact-checking, Privacy and Legal SIGIR '19, July 21­25, 2019, Paris, France"
831,
832,"retrieval. In Proceedings of CIKM'17. ACM, 257­266. [44] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:"
833,Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing
834,[46] Samsung. 2018. Samsung Electronics Begins Mass Production of Industry's
835,Largest Capacity SSD - 30.72TB - for Next-Generation Enterprise Systems. https:
836,//bit.ly/2EFKp5N.
837,"[47] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Grégoire Mesnil. 2014."
838,"Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd International Conference on World Wide Web. ACM, 373­374."
839,[48] Lyle Smith. 2018. Intel Optane 800P NVMe SSD Review. https://www.
840,storagereview.com/intel_optane_800p_nvme_ssd_review.
841,01-28.
842,"[49] Wenhai Sun, Bing Wang, Ning Cao, Ming Li, Wenjing Lou, Y. Thomas Hou, and"
843,"Hui Li. 2014. Verifiable Privacy-Preserving Multi-Keyword Text Search in the Cloud Supporting Similarity-Based Ranking. IEEE Trans. Parallel Distrib. Syst. 25, 11"
844,"[51] Tao Tao and ChengXiang Zhai. 2007. An exploration of proximity measures in information retrieval. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 295­302."
845,"[52] Guofeng Wang, Chuanyi Liu, Yingfei Dong, Kim-Kwang Raymond Choo, Peiyi"
846,"Han, Hezhong Pan, and Binxing Fang. 2018. Leakage Models and Inference Attacks on Searchable Encryption for Cyber-Physical Social Systems. IEEE Access 6"
847,"[53] Zhihua Xia, Xinhui Wang, Xingming Sun, and Qian Wang. 2016. A secure and dynamic multi-keyword ranked search scheme over encrypted cloud data. IEEE Transactions on Parallel and Distributed Systems 27, 2"
848,"[54] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 55­64."
849,"[55] Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie-Yan Liu. 2018. Towards"
850,Better Text Understanding and Retrieval through Kernel Entity Salience Modeling. In The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval
851,
852,A KERNEL VALUE RECOVERY ATTACK
853,
854,"In this section, we describe an attack to recover the term frequency"
855,
856,from closed soft match maps based on histogram pooling even
857,
858,though the exact match signals are removed. We assume that the in-
859,"terval [-1, 1) for similarity value is divided as [-1, b2, · · · , bR-1, 1) where -1 = b1  b2 < b3 < · · · < bR-1 < bR = 1. Each kernel Kj is associated with an interval [bj , bj+1) where 1  j  R - 1. Note that"
860,all intervals of these kernels are disjoint and their unions are inter-
861,"val [-1, 1). Given a term t, a document d, and a kernel vector fìt,d in a soft match map is"
862,
863,R-1
864,
865,R-1
866,
867,exp(aj ) =
868,
869,j =1
870,
871,j=1 w d
872,
873,"Since the union of all the disjoint intervals is [-1, 1), we can have"
874,
875,R-1
876,
877,exp(aj ) =
878,
879,"1-1 t,w <1."
880,
881,j =1
882,
883,w d
884,
885,"SIGIR '19, July 21­25, 2019, Paris, France Jinjin Shao, Shiyu Ji, Tao Yang"
886,
887,"Let the length of d be n, and term frequency of t contained in d is"
888,
889,"TF(t, d), we have"
890,
891,R j =1
892,
893,exp(aj )
894,
895,=
896,
897,n
898,
899,and
900,
901,exp(aR )
902,
903,=
904,
905,"TF(t, d)."
906,
907,Then
908,
909,"TF(t, d) = n -"
910,
911,R-1 j =1
912,
913,exp(aj
914,
915,).
916,
917,"We also notice for any term t  that is not in document d, TF(t , d) ="
918,
919,"0. But fìt,d is included in a soft match map because of term closure."
920,
921,Thus n =
922,
923,R-1 j =1
924,
925,exp(a j ).
926,
927,"To launch this attack, we assume an adversary can scan the"
928,
929,soft match map SMM to obtain all keys
930,
931,"SMM, and then take the following actions: 1) For each key"
932,
933,"obtain the kernel vector fìt,d ="
934,
935,Compute the sum of elements in this vector. St =
936,
937,R-1 j =1
938,
939,exp(a
940,
941,j
942,
943,"),"
944,
945,where aj = log Kj
946,
947,"as n = maxt :(t,d)S M M {St }. 3) Compute term frequency of any t in d as TF(t, d) = n - St ."
948,
949,B PROOFS
950,"Proof of Theorem 3.2 Let t f denote TF(t, d) for simplicity here."
951,|tf - exp(aR )| = |tf - exp(log KR
952,
953,= tf - exp(-
954,
955,w d
956,
957,2R2
958,
959,= tf -
960,
961,"(t, w - 1.0)2"
962,
963,exp(-
964,
965,)-
966,
967,"(t, w exp(-"
968,
969,- 1.0)2 )
970,
971,.
972,
973,"w d,w =t"
974,
975,2R2
976,
977,"w d,w t"
978,
979,2R2
980,
981,Since
982,
983,if
984,
985,w
986,
987,=
988,
989,"t,"
990,
991,then
992,
993,"t, w"
994,
995,=
996,
997,"1.0,"
998,
999,exp(-
1000,
1001,(
1002,
1003,t
1004,
1005,","
1006,
1007,w -1.0)2 2R2
1008,
1009,)
1010,
1011,=
1012,
1013,exp(0)
1014,
1015,=
1016,
1017,1.
1018,
1019,"Thus if the length of d is n, we have"
1020,
1021,"(t, w - 1.0)2"
1022,
1023,tf -
1024,
1025,1-
1026,
1027,exp(-
1028,
1029,)
1030,
1031,"w d,w =t w d,w t"
1032,
1033,2R2
1034,
1035,=
1036,
1037,"(t, w - 1.0)2"
1038,
1039,exp(-
1040,
1041,)
1042,
1043,(n - tf) exp(-
1044,
1045,"w d,w t"
1046,
1047,2R2
1048,
1049,2R2
1050,
1051,n exp(-
1052,Proof of Theorem 5.2
1053,
1054,"For each term closure, if at least one term in that closure appears"
1055,"in document d, all terms in that closure would have precomputed kernel values with d. For each closure C, there is a total of 2|C | - 1 non-empty subsets and thus there are totally 2|C | - 2 ways to"
1056,replace the exact terms in closure C appeared in d with another
1057,subset of C containing soft terms. When a server tries to guess the
1058,existence of exact term occurrence
1059,
1060,sets that produce the soft match maps with the same keys. The soft match maps of these
1061,
1062,closure for replacement and the original soft match map is closed and -statistically indistinguishable.
1063,
1064,314
1065,
1066,

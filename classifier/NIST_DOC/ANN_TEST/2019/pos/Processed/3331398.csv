,sentence
0,Demonstration Papers 2: Evaluation & Entities
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,cwl_eval: An Evaluation Tool for Information Retrieval
5,
6,Leif Azzopardi
7,"University of Strathclyde Glasgow, UK"
8,leifos@acm.org
9,
10,Paul Thomas
11,"Microsoft Canberra, Australia pathom@microsoft.com"
12,
13,Alistair Moffat
14,"The University of Melbourne Melbourne, Australia"
15,ammoffat@unimelb.edu.au
16,
17,ABSTRACT
18,We present a tool
19,"ACM Reference Format: Leif Azzopardi, Paul Thomas, and Alistair Moffat. 2019. cwl_eval: An Evaluation Tool for Information Retrieval. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval"
20,1 INTRODUCTION
21,"Effectiveness evaluation has played a central role in the development of information retrieval systems [11]. Over the years many metrics have been proposed, with the more recent ones employing multi-valued and/or discounted relevance values"
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331398"
23,
24,"metrics are more sophisticated and go beyond the typical assumptions made by trec_eval, and in part because of the complexity already present in trec_eval. The growth in the diversity of tools means that researchers wanting to use ""state-of-the-art"" metrics"
25,in their work need to obtain a variety of independent scripts
26,"they exist), and manage their differing input formats [9]. As a con-"
27,sequence there has been slow uptake of newer metrics despite
28,"evidence that metrics like INST, TBG, BPM, and IFT are able, in"
29,"various ways, to make more accurate predictions of performance,"
30,and/or are more correlated with user satisfaction than traditional approaches; and despite the standard set of trec_eval metrics making questionable modelling assumptions
31,"[8]), or having mathematical infelicities"
32,"We describe a common, extensible, open-source resource for evaluation metrics, ensuring back-compatibility with trec_eval, so that previous measurements of performance remain available in"
33,"a single tool, and allowing the community to also employ these newer measures. In its simplest form, cwl_eval takes the same input as trec_eval, but also provides additional options."
34,"To provide the theoretical underpinnings we draw upon the C/W/L framework proposed by Moffat et al. [8]. This framework provides a standardised and intuitive way to encode a wide range of metrics, and enables reporting of not just Expected Utility per item inspected, the rate at which gain is acquired, but also the related measurements of Expected Total Utility, the gain accumulated from the whole list; Expected Cost per item inspected; Expected Total Cost, the cost incurred in examining the results list; and Expected Depth, the number of items a searcher examines given the user model"
35,"encoded within the metric. The C/W/L framework is described in detail elsewhere [1, 7, 8]."
36,"In brief, it models the manner in which probabilistic users examine a"
37,"ranked list, assuming that each user reads top down and is governed by a conditional continuation probability 0  Ci  1 that indicates the fraction that proceed to examine the item at depth i + 1, given that they have examined the item at depth i. Different choices of C = Ci  then reflect different beliefs about user behaviour and hence define different metrics. A vector of weights W = Wi  can be derived from C, where Wi is an assessment of the expected proportion of user attention directed at the item at rank i. Using these weights the expected utility"
38,as the dot product between the relevance
39,can also calculate the expected cost
40,"the dot product between the weights and a cost vector [1, 12], and"
41,the expected total cost
42,"computations can be in units of documents, characters, reading"
43,"seconds, and so on. For example, the U-Measure [10] calculates"
44,"cost in characters, while TBG and IFT use seconds. Finally, we can"
45,
46,1321
47,
48,Demonstration Papers 2: Evaluation & Entities
49,
50,"SIGIR '19, July 21­25, 2019, Paris, France"
51,
52,calculate the expected depth
53,"Fundamentally, the C/W/L framework generates a wide range of measurements regarding the predicted user interactions with the ranked list of search results; and hence, conversely, allows for a wide range of observational information to be used to derive metrics that relate to user behaviour, and thus to useful measurement."
54,"2 TOOLKIT, APP AND NOTEBOOKS"
55,"Toolkit. cwl_eval defines three core classes. First, the Ranking class is minimally composed of two ordered lists representing the gains and costs of the items returned from the search. Attributes such as total gain and total relevant items are calculated from these two lists. The ranking is then passed to the CWLRuler, which is composed of a number of CWLMetrics; or directly to a single CWLMetric. The latter produces the measurements."
56,"The heart of the toolkit is the CWLMetric class, which is used as the basis of different measures. It requires the definition of the C, W and/or L vectors."
57,Graded Gain Values. Järvelin and Kekäläinen [3] make use of graded gains and non-binary relevance judgements; Moffat and Zobel [5] similarly allow fractional utility. cwl_eval also allows fractional gains to be input as part of a standard TREC qrels file.
58,"Residuals. In their description of RBP Moffat and Zobel [5] introduce the notion of residuals, the score uncertainty introduced by unjudged documents. In an ideal evaluation, all documents in the ranking would have associated gain values, and metric score measurements would be precise. Howevere, in most practical experimentation only a small subset of the documents in the ranking have been judged. The standard"
59,"In particular, high residual values indicate an experimental context in which there might be non-trivial imprecision in the measured values, with further relevance judgements being the only way to be sure whether that is in fact the case. Note that residuals should not be thought of as being confidence intervals, or as having any statistical basis. Their sole purpose is to provide a bounding range on the eventual score, based on the partial evidence supplied by incomplete relevance judgements."
60,"Application. The program cwl_eval.py provides a similar interface to trec_eval, and in simplest form is used via"
61,
62,python cwl_eval.py <TREC -QRELs > <TREC -RESULTS >
63,"where <TREC-QRELS> is the name of a standard four-column TRECformatted relevance file, and <TREC-RESULTS> a standard six-column TREC-formatted result file. When no cost file is specified, cwl_eval assumes the cost of each item to be 1.0. If a cost file is specified via the flag ""-c <COSTS>"", then document-specific costs are used to calculate expected"
64,Note the relationship between the measurements: EU
65,By default cwl_eval outputs a list of metric scores
66,PrecisionCWLMetrics(k=10) INSTCWLMetric(T=2.5) APCWLMetric
67,"It is straightforward to select and configure metrics. For example, cwl_bpm.py provides an implementation of the Bejewelled Player measure [14], which minimally takes two parameters: T , the total amount of benefit desired, and K, the total amount of cost available to be spent. Adding BPMCWLMetric(T=4, K=10) to the list of metrics means that the static Bejewelled Player Model will be computed."
68,"For convenience, three other flags can be set: ""-b <BIBFILE>"", to save the BibTEX associated with the metrics specified/used; ""-n"" to include the column names of each measurement in the output; and ""-r"" to compute and report residuals where it is appropriate to do so. Note that all output is provided on a per query/topic basis, the presumption being that a subsequent processing phase will compute average scores over topics and also"
69,"Tests. As part of the development, we checked cwl_eval's output of a range of traditional metrics"
70,"One issue that needed to be addressed was the internal sorting performed by trec_eval, which ignores the provided ranking, and sorts the list by score, and then reverse document identifier"
71,
72,1322
73,
74,Demonstration Papers 2: Evaluation & Entities
75,
76,"SIGIR '19, July 21­25, 2019, Paris, France"
77,
78,"the cwl_eval GitHub repository. We also checked the results against those of inst_eval1 [4], and the internal tests of the irmetrics package2, to verify that the scores were equivalent; scores agreed to within rounding error."
79,"Implementation. The C/W/L Evaluation Toolkit, and cwl_eval, is available at https://github.com/ireval/cwl. A demonstration of using the toolkit in Jupyter Notebooks is provided at https://github. com/ireval/cwl-examples."
80,3 C/W/L EXAMPLES
81,"To provide examples of how the C/W/L framework can be used to see the inner workings of the different metrics, we created a number of Jupyter Notebooks, which show:"
82,"Example Topics. For the examples below, we have assumed that we have two topics"
83,"rank i 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15 T1 Gain 0.0, 0.0, 0.2, 0.4, 1.0, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0"
84,"Cost 1.2, 0.6, 0.4, 0.6, 3.6, 1.6, 0.6, 2.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 1.8"
85,"T2 Gain 1.0, 0.0, 1.0, 0.4, 0.0, 0.2, 0.0, 0.0, 1.0, 0.2, 0.0, 0.4, 0.0, 0.0, 0.0 Cost 3.2, 1.6, 1.4, 0.6, 3.6, 1.6, 0.6, 1.6, 2.6, 0.2, 1.2, 0.2, 0.2, 0.6, 1.8"
86,
87,"Sample Output. Below we have included two examples of the output from cwl_eval, where:"
88,"Table 2 shows the output when the document costs are provided. Now the expected cost per item depends on how much time it takes to process each item, and the proportion of attention allocated to each item. In this example the expected total cost is now different from the expected depth, by a factor related to the weighted cost of processing the inspected items. Costs are in the units provided, so here the output units are seconds/document"
89,1 https://github.com/ielab/inst_eval 2 https://github.com/Microsoft/irmetrics- r
90,
91,Table 1: cwl_eval output without cost information.
92,
93,Topic Metric
94,
95,EU ETU EC ETC ED
96,
97,T1 AP
98,
99,0.2722 1.6000 1.0000 5.8776 5.8776
100,
101,T1 RR
102,
103,0.0667 0.2000 1.0000 3.0000 3.0000
104,
105,T1 P@5
106,
107,0.3200 1.6000 1.0000 5.0000 5.0000
108,
109,T1 NDCG-k@10 0.2270 1.0314 1.0000 4.5436 4.5436
110,
111,T1 INST-T=2
112,
113,0.1545 0.6069 1.0000 3.9220 3.9292
114,
115,T1 TBG-H@2
116,
117,0.1752 0.5981 1.0000 3.4142 3.4142
118,
119,T1 BPM-Dynamic-... 0.3200 1.6000 1.0000 5.0000 5.0000
120,
121,T1 IFT-...
122,
123,0.0659 0.1097 1.0000 1.6649 1.6649
124,
125,"Figure 1: C/W/L function plots for three metrics for Topics T1 and T2. Top row: RBP,  = 0.8; middle row, TBG, H = 2; bottom row, BPM, T = 2, K = 10.0, hc = 0.5, hb = 0.5"
126,
127,Inside Metrics. The repository includes a Jupyter Notebook to show how we can visualise and inspect the models within the metrics. Figure 1 shows the C/W/L functions for a subset of the metrics on the two example topics. For RBP the plots are the same for topics T1 and T2--this is because RBP is not sensitive to either the cost values or the gain values. Both TBG and BPM consider both the gain and cost vectors to determine how likely a user is to continue down the ranked list
128,"Visualising the Measurements. Since cwl_eval outputs a series of measurements given the metric, it is now possible to contextualise the usual Expected Utility measurement with respect to the"
129,
130,Table 2: cwl_eval output with costs information.
131,
132,Topic Metric
133,
134,EU ETU EC ETC ED
135,
136,T1 AP
137,
138,0.2722 1.6000 1.1681 6.8653 5.8776
139,
140,T1 RR
141,
142,0.0667 0.2000 0.7333 2.2000 3.0000
143,
144,T1 P@5
145,
146,0.3200 1.6000 1.2800 6.4000 5.0000
147,
148,T1 NDCG-k@10 0.2270 1.0314 1.1827 5.3738 4.5436
149,
150,T1 RBP@0.6
151,
152,0.1287 0.3218 1.0208 2.5520 2.5000
153,
154,T1 TBG-H@2
155,
156,0.2143 0.7195 1.1513 3.8663 3.3582
157,
158,T1 BPM-Dynamic... 0.3200 1.6000 1.2800 6.4000 5.0000
159,
160,T1 IFT...
161,
162,0.0748 0.1269 1.0857 1.8412 1.6959
163,
164,1323
165,
166,Demonstration Papers 2: Evaluation & Entities
167,
168,"SIGIR '19, July 21­25, 2019, Paris, France"
169,
170,Figure 2: Example of how measurements change over metric parameters. Left column: RBP
171,per item inspected vs Expected Depth
172,
173,"Expected Depth, and to visualise how the EU and ED change when the parameters of the metrics are varied. Figure 2 shows how Expected Utility and Expected Total Utility, and Expected Total Cost, vary as a function of ED for three metrics. Each plotted point represents a particular parameter setting combination for the metric and its corresponding predictions."
174,4 SUMMARY
175,"We have described the C/W/L evaluation framework, a toolkit and an application for evaluating information retrieval systems. This work represents the unification of various metrics within one package, enabling direct comparison between the estimates of such metrics. It also provides the foundations for the development of new utility- and cost-based metrics."
176,REFERENCES
177,"[1] Leif Azzopardi, Paul Thomas, and Nick Craswell. Measuring the utility of search engine result pages: An information foraging based measure. In Proc. SIGIR, pages 605­614, 2018."
178,"[2] Norbert Fuhr. Some common mistakes in IR evaluation, and how they can be avoided. SIGIR Forum, 52(2):32­41, 2017."
179,"[3] Kalervo Järvelin and Jaana Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002."
180,
181,"[4] Bevan Koopman and Guido Zuccon. A test collection for matching patient trials. In Proc. SIGIR, pages 669­672, 2016."
182,"[5] Alistair Moffat and Justin Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, 2008."
183,"[6] Alistair Moffat, Paul Thomas, and Falk Scholer. Users versus models: What observation tells us about effectiveness metrics. In Proc. CIKM, pages 659­668, 2013."
184,"[7] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. INST: An adaptive metric for information retrieval evaluation. In Proc. Aust. Doc. Comp. Symp., pages 5:1­5:4, 2015."
185,"[8] Alistair Moffat, Peter Bailey, Falk Scholer, and Paul Thomas. Incorporating user expectations and behavior into the measurement of search effectiveness. ACM Trans. Inf. Syst., 35(3):24:1­24:38, 2017."
186,"[9] Joao Palotti, Harrisen Scells, and Guido Zuccon. Trectools: an open-source python library for information retrieval practitioners involved in trec-like campaigns. In Proc. SIGIR, 2019."
187,"[10] Tetsuya Sakai and Zhicheng Dou. Summaries, ranked retrieval and sessions: A unified framework for information access evaluation. In Proc. SIGIR, pages 473­482, 2013."
188,"[11] Mark Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010."
189,"[12] Mark D. Smucker and Charles L. A. Clarke. Time-based calibration of effectiveness measures. In Proc. SIGIR, pages 95­104, 2012."
190,"[13] Ziying Yang, Alistair Moffat, and Andrew Turpin. How precise does document scoring need to be? In Proc. AIRS, pages 279­291, 2016."
191,"[14] Fan Zhang, Yiqun Liu, Xin Li, Min Zhang, Yinghui Xu, and Shaoping Ma. Evaluating web search with a bejeweled player model. In Proc. SIGIR, pages 425­434, 2017."
192,
193,1324
194,
195,

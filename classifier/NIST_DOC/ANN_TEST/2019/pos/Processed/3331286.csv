,sentence,label,data
0,Short Research Papers 1B: Recommendation and Evaluation,null,null
1,,null,null
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
3,,null,null
4,Query-Task Mapping,null,null
5,,null,null
6,Michael Völske Ehsan Fatehifar,null,null
7,Benno Stein,null,null
8,"<firstname>.<lastname>@uni-weimar.de Bauhaus-Universität Weimar Weimar, Germany",null,null
9,ABSTRACT,null,null
10,"Several recent task-based search studies aim at splitting query logs into sets of queries for the same task or information need. We address the natural next step: mapping a currently submitted query to an appropriate task in an already task-split log. This query-task mapping can, for instance, enhance query suggestions--rendering efficiency of the mapping, besides accuracy, a key objective.",null,null
11,Our main contributions are three large benchmark datasets and preliminary experiments with four query-task mapping approaches:,null,null
12,"ACM Reference Format: Michael Völske, Ehsan Fatehifar, Benno Stein, and Matthias Hagen. 2019. Query-Task Mapping. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
13,1 INTRODUCTION,null,null
14,"Users often turn to a search engine to fulfill an underlying task that led to the information need expressed in a query. The field of task-based search aims to understand the tasks behind information needs, in order to develop better support tools. Recent research has focused on observing user behavior during task-based search [15] or on splitting query logs into tasks and subtasks [24]. Given a task-split query log, we focus on the natural next step: map a new query to the most appropriate task. Query-task mapping may be used to derive task-based query embeddings [25] or to identify query suggestions [33]. Since query suggestions have to be derived in milliseconds, efficiency is a crucial factor besides effectiveness. Hence, our study analyzes runtime along with accuracy.",null,null
15,"We create three benchmarking datasets:1 one based on search session and mission detection corpora [8, 21], another based on the",null,null
16,1Data available from: https://webis.de/data/webis-qtm-19.html,null,null
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331286",null,null
18,,null,null
19,Matthias Hagen,null,null
20,matthias.hagen@informatik.uni-halle.de Martin-Luther-Universität Halle-Wittenberg,null,null
21,Halle,null,null
22,"TREC Sessions and Tasks tracks [4, 14] combined with a corpus of TREC-based search sessions [9], and the third built from wikiHow questions. We enlarge each dataset with query suggestions from Google and Bing to reach several tens of thousands of queries and annotate the task information. In a preliminary study, we test four query-task mapping methods on our new datasets:",null,null
23,2 RELATED WORK,null,null
24,"Research on matching queries with the same information need has recently shifted focus from single-user oriented session and mission detection [6­8, 10, 12, 13, 15, 18, 21] to the more multi-user oriented problem of splitting search logs into tasks [3, 11, 17, 22­24].",null,null
25,The studies on search sessions aimed to either match a current query to one of the previous queries of the same user submitted either within the same,null,null
26,"Recently, the focus has shifted away from the notion of individual users' search missions towards one of complex tasks that can re-occur across users. A complex search task is a multi-aspect or multi-step information need comprising subtasks which might recursively be complex; planning a journey is a typical example [1], and studies have aimed to subdivide query logs into clusters of same-task queries [19]. As before, the goal is to support individual users, but this time by leveraging what others have done in similar situations. One idea is to suggest related queries from the identified query-task clusters like in the TREC Task tracks' setting [14].",null,null
27,"Grouping the queries of some larger log into tasks and potentially subtasks has been tackled in different ways ranging from Hawkes processes [17], Bayesian Rose trees [24], entity relations [31], to DBpedia categories [32]. However, no large annotated datasets of logs split into tasks are available. And, maybe even more importantly, the problem of quickly mapping a currently submitted query to an appropriate task in a task-split background log has not been really studied in the literature so far. We address both issues by providing three large benchmarking datasets of task-split queries and an empirical study of four approaches for query-task mapping.",null,null
28,,null,null
29,969,null,null
30,,null,null
31,Short Research Papers 1B: Recommendation and Evaluation,null,null
32,,null,null
33,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
34,,null,null
35,3 BENCHMARKING DATASETS,null,null
36,"We provide three new datasets of queries split into tasks with different ""characteristics:""",null,null
37,3.1 Session-based Dataset,null,null
38,"Research on session and mission detection has produced three publicly available corpora of queries sampled from the AOL query log [28] annotated with individual users' search sessions [7, 8, 20]. Since the newer corpus of Hagen et al. [8] and Gayo-Avello's corpus [7] are based on the same sample, we use the corpora of Lucchese et al. [20] and of Hagen et al. [8] as our basis.",null,null
39,Lucchese et al. [20] sampled from the 500 time-based sessions with the largest number of queries from the first week of the AOL log. The 1424 queries from 13 users in this corpus are manually annotated with logical session information,null,null
40,"Hagen et al. [8] took all queries from the 215 AOL users in GayoAvello's corpus [7], removed 88 users with fewer than 4 queries, and annotated the remaining 8840 queries from 127 users with peruser logical session and search mission information",null,null
41,,null,null
42,"Table 1: Statistics of the benchmark datasets. Rows with ""+"" are cumulative, omitting duplicate task-query pairs.",null,null
43,,null,null
44,Tasks,null,null
45,,null,null
46,Session-based dataset Lucchese et al. [20] + Hagen et al. [8] + Google suggestions + Bing suggestions,null,null
47,TREC-based dataset Webis-TRC-12 [9] + TREC + Google suggestions + Bing suggestions,null,null
48,WikiHow-based dataset WikiHow + Google suggestions + Bing suggestions,null,null
49,,null,null
50,"223 1,423 1,423 1,423",null,null
51,150 276 276 276,null,null
52,"7,202 7,202 7,202",null,null
53,,null,null
54,Queries,null,null
55,"771 4,502 29,441 41,780",null,null
56,"3,848 7,771 38,478 47,514",null,null
57,"15,914 119,283 119,292",null,null
58,,null,null
59,Queries per Task min avg max,null,null
60,"1 3.5 55 1 3.2 147 1 20.7 924 1 29.4 1,368",null,null
61,1 25.7 122 1 28.2 144 8 139.4 858 8 172.2 997,null,null
62,1 2.2 22 1 16.6 197 1 16.6 197,null,null
63,,null,null
64,"To enlarge the dataset to tens of thousands of queries, we submit each original query to Google and Bing and scrape the query suggestions that we then add to the same task. We discard suggestions that have the original query as a prefix, but do not continue with a new term.2 Manual spot checks showed the task assignment to be reasonable for the remaining suggestions, with a small number of exceptions where the search engines returned suggestions in a different language, which were removed semi-automatically; further spot checks showed the remaining suggestions to be accurate. We gathered 24,939 unique suggestions from Google",null,null
65,3.2 TREC-based Dataset,null,null
66,"Our TREC-based dataset uses the queries from the TREC Session tracks 2012­2014 [4], from the TREC Tasks tracks 2015 and 2016 [14], and from the Webis-TRC-12 [9]. The Webis-TRC-12 is based on the search logs of writers who wrote essays on the 150 topics used at the TREC Web tracks 2009­2011 while doing their background research using a search engine",null,null
67,3.3 WikiHow-based Dataset,null,null
68,"Our third dataset is based on crawling 198,163 questions from wikiHow,3 inspired by Yang and Nyberg's idea of extracting steps for completing task-based search intents from the procedural knowledge collected at this platform [33]. However, we do not aim to extract steps, but to identify different questions on the same task.",null,null
69,"On wikiHow, each question is linked to other recommended questions, but spot checks showed that only those questions that mutually link to each other can be considered as on the same task such that we restrict the extraction to these cases. This way, we gathered 15,914 questions split into 7202 tasks. As before, we enlarge the dataset by obtaining 103,369 suggestions from Google and 9 additional ones Bing",null,null
70,4 EXPERIMENTAL ANALYSIS,null,null
71,"We compare four straightforward query-task mapping methods on our new benchmarking datasets with respect to their accuracy and efficiency, both in terms of preprocessing and online query processing.4 Table 2 summarizes the results.",null,null
72,"2For instance, for the original query [how to open a can], we would discard [how to open a canadian bank account] if returned as a suggestion. 3 www.wikihow.com 4Experiment machines had Intel Xeon 2608L-v4 CPUs and 128GB of DDR4 memory",null,null
73,,null,null
74,970,null,null
75,,null,null
76,Short Research Papers 1B: Recommendation and Evaluation,null,null
77,,null,null
78,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
79,,null,null
80,4.1 Query-Task Mapping Approaches,null,null
81,Our experiments take inspiration from the taxonomy of Metzler et al. [26] to cover a range of different short-text retrieval paradigms:,null,null
82,"Trie-based Approach. The trie data structure, first described by De La Briandais [5], matches strings based on prefixes. We construct a trie for all queries within the task-split dataset during pre-processing, and for query-task mapping assign a new query q to the task associated with the query found as the longest prefix of q. If queries from multiple tasks qualify, we choose the majority vote. We use the implementation from the Google Pygtrie library.5",null,null
83,MinHash LSH. MinHash,null,null
84,"Word Movers Distance. Using word embeddings, the word movers distance [16] measures the distance of two strings",null,null
85,"Index-based Search. As a final approach, we use an inverted indexbased method, whereby we store the queries in the log in an Elasticsearch index, with a field for their task.8 To perform query-task mapping for a new query, we simply submit it to the index, and assign the task of the top result.",null,null
86,4.2 Pre-Processing and Mapping Efficiency,null,null
87,"As for the pre-processing efficiency, we just measured the time needed to build the necessary data structures on the full datasets. Building the trie took about 10 seconds for the smaller datasets and 25 seconds for the largest. This is very close to the time needed to look up all query terms in the pre-trained word-embedding model for the WMD method, but quite a bit faster than computing the hashes for MinHash LSH Forest, which takes about one minute for the smaller datasets and three minutes for the largest. Building the inverted indexes takes about 30 seconds for the smaller corpora and about one minute for the largest.",null,null
88,"Query-task mapping runtime was averaged over 10,000 test queries left out from the pre-processing. Mapping a query to its",null,null
89,"5 https://github.com/google/pygtrie 6 https://ekzhu.github.io/datasketch/lsh.html 7 https://github.com/src- d/wmd- relax 8https://www.elastic.co/, version 5.6.14, retrieval model: BM25",null,null
90,,null,null
91,Table 2: Summary of our experimental results. Accuracy values shown with 95% confidence intervals.,null,null
92,,null,null
93,Dataset,null,null
94,,null,null
95,Trie LSH,null,null
96,,null,null
97,Preprocessing time,null,null
98,,null,null
99,Session-based,null,null
100,,null,null
101,10.03s 53.79s,null,null
102,,null,null
103,TREC-based,null,null
104,,null,null
105,13.26s 62.09s,null,null
106,,null,null
107,Wikihow-based 28.00s 141.65s,null,null
108,,null,null
109,Query-task mapping time,null,null
110,,null,null
111,Session-based 0.46ms 2.42ms,null,null
112,,null,null
113,TREC-based,null,null
114,,null,null
115,0.51ms 2.50ms,null,null
116,,null,null
117,Wikihow-based 0.33ms 2.28ms,null,null
118,,null,null
119,Query-task mapping accuracy,null,null
120,,null,null
121,Session-based 0.69±0.02 0.66±0.02,null,null
122,,null,null
123,TREC-based,null,null
124,,null,null
125,0.66±0.03 0.68±0.03,null,null
126,,null,null
127,Wikihow-based 0.48±0.02 0.41±0.02,null,null
128,,null,null
129,WMD,null,null
130,9.60s 11.14s 26.50s,null,null
131,7.16s 9.24s 22.65s,null,null
132,0.67±0.03 0.73±0.03 0.55±0.03,null,null
133,,null,null
134,Index,null,null
135,24.14s 26.90s 53.48s,null,null
136,2.80ms 2.95ms 4.21ms,null,null
137,0.78±0.03 0.80±0.03 0.63±0.02,null,null
138,,null,null
139,"tasks is a matter of milliseconds using the trie approach or MinHash LSH Forest. Compared to these runtimes, using WMD it took 23 seconds on average to map a single query to its task on the largest dataset--prohibitively slow for an online setup without any further efficiency tweaks that were beyond the scope of our study. Using the index-based method, determining the task of a query again only takes a few milliseconds.",null,null
140,4.3 Query-Task Mapping Accuracy,null,null
141,"We measure accuracy on every dataset as the ratio of correct task mappings across 50 runs of 100 independently sampled test queries in a leave-one-out manner: each test query is removed from its task individually, the datasets without that one query are pre-processed, and the methods are asked to map the now ""new"" query to a task. Overall, our approaches map at least one in three, and at most four out of five test queries to the correct task. The index-based method clearly performs best on all three datasets while the slow WMD approach is second best twice.",null,null
142,"Out of our three datasets, the smaller Session- and TREC-based ones pose easier query-task mapping problems, with all methods getting at least two thirds of the test queries correct. This is explained in part by the smaller datasets having fewer tasks, and comparatively more queries per task; beyond that, previous research on one of the underlying query logs [9] found related queries to often share prefixes, boosting not just the Trie-based method, but the other exact-word-match based ones",null,null
143,"By contrast, all four methods exhibit their worst query-task mapping performance on the WikiHow-based dataset--the largest both in terms of tasks and total number of queries, but with the smallest average number of queries per task. The fact that the distributional similarity",null,null
144,"To elaborate on this insight, Figure 1 shows the results of an additional experiment on two of our datasets. Here, we retrieve the top k",null,null
145,,null,null
146,971,null,null
147,,null,null
148,Short Research Papers 1B: Recommendation and Evaluation,null,null
149,,null,null
150,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
151,,null,null
152,Figure 1: Query-task mapping accuracy under a majority voting scheme. Bands show 95% confidence intervals.,null,null
153,"the majority task. Since the Trie data structure does not induce any ordering among the results in the same trie node, we use the deepest k trie nodes on the path to the input query that contain a larger number of individual result queries; with increasing k the Trie method thus approaches a simple majority",null,null
154,5 CONCLUSION,null,null
155,"We consider the problem of query-task mapping: given a query log split into tasks and a new query, identify the most appropriate task for the query. This problem is not as well studied as the problem of splitting a log into tasks while also larger datasets of task-split queries are missing. To close this gap and to foster research on query-task mapping, our first main contribution are three large publicly available benchmarking datasets",null,null
156,"Interesting directions for future research include the development of more accurate fast methods, and the generalization of our experiments to even larger datasets. Such larger datasets will most likely contain highly similar tasks that turned out to be the hardest for the tested baselines to distinguish; all methods performed worse on the bigger corpus compared to the smaller ones. In our experiments, all queries had an annotated ground truth task that was also shared by other queries. Also including queries not part of any task may form an interesting addition to the experimental setup",null,null
157,REFERENCES,null,null
158,"[1] Ahmed Hassan Awadallah, Ryen W. White, Patrick Pantel, Susan T. Dumais, and Yi-Min Wang. 2014. Supporting complex search tasks. In Proceedings of CIKM 2014, 829­838.",null,null
159,"[2] Mayank Bawa, Tyson Condie, and Prasanna Ganesan. 2005. LSH Forest: Selftuning indexes for similarity search. In Proceedings of WWW 2005, 651­660.",null,null
160,,null,null
161,"[3] Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. The query-flow graph: Model and applications. In Proceedings of CIKM 2008, 609­618.",null,null
162,"[4] Ben Carterette, Evangelos Kanoulas, Mark M. Hall, and Paul D. Clough. 2014. Overview of the TREC 2014 Session track. In Proceedings of TREC 2014.",null,null
163,"[5] Rene De La Briandais. 1959. File searching using variable length keys. In Proceedings of IRE-AIEE-ACM 1959, 295­298.",null,null
164,"[6] Debora Donato, Francesco Bonchi, Tom Chi, and Yoëlle S. Maarek. 2010. Do you want to take notes?: Identifying research missions in Yahoo! search pad. In Proceedings of WWW 2010, 321­330.",null,null
165,"[7] Daniel Gayo-Avello. 2009. A survey on session detection methods in query logs and a proposal for future evaluation. Information Sciences 179, 12",null,null
166,"[8] Matthias Hagen, Jakob Gomoll, Anna Beyer, and Benno Stein. 2013. From search session detection to search mission detection. In Proceedings of OAIR 2013, 85­92.",null,null
167,"[9] Matthias Hagen, Martin Potthast, Michael Völske, Jakob Gomoll, and Benno Stein. 2016. How writers search: Analyzing the search and writing logs of non-fictional essays. In Proceedings of CHIIR 2016, 193­202.",null,null
168,"[10] Daqing He, Ayse Göker, and David J. Harper. 2002. Combining evidence for automatic web session identification. Information Processing & Management 38, 5",null,null
169,"[11] Wen Hua, Yangqiu Song, Haixun Wang, and Xiaofang Zhou. 2013. Identifying users' topical tasks in web search. In Proceedings of WSDM 2013, 93­102.",null,null
170,"[12] Bernard J. Jansen, Amanda Spink, Chris Blakely, and Sherry Koshman. 2007. Defining a session on web search engines. JASIST 58, 6",null,null
171,"[13] Rosie Jones and Kristina Lisa Klinkner. 2008. Beyond the session timeout: Automatic hierarchical segmentation of search topics in query logs. In Proceedings of CIKM 2008, 699­708.",null,null
172,"[14] Evangelos Kanoulas, Emine Yilmaz, Rishabh Mehrotra, Ben Carterette, Nick Craswell, and Peter Bailey. 2017. TREC 2017 Tasks track overview. In Proceedings of TREC 2017.",null,null
173,"[15] Alexander Kotov, Paul N. Bennett, Ryen W. White, Susan T. Dumais, and Jaime Teevan. 2011. Modeling and analysis of cross-session search tasks. In Proceedings of SIGIR 2011, 5­14.",null,null
174,"[16] Matt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From word embeddings to document distances. In Proceedings of ICML 2015, 957­966.",null,null
175,"[17] Liangda Li, Hongbo Deng, Anlei Dong, Yi Chang, and Hongyuan Zha. 2014. Identifying and labeling search tasks via query-based Hawkes processes. In Proceedings of KDD 2014, 731­740.",null,null
176,"[18] Zhen Liao, Yang Song, Yalou Huang, Li-wei He, and Qi He. 2014. Task trail: An effective segmentation of user search behavior. IEEE Trans. Knowl. Data Eng. 26, 12",null,null
177,"[19] Zheng Lu, Hongyuan Zha, Xiaokang Yang, Weiyao Lin, and Zhaohui Zheng. 2013. A new algorithm for inferring user search goals with feedback sessions. IEEE Trans. Knowl. Data Eng. 25, 3",null,null
178,"[20] Claudio Lucchese, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2011. Identifying task-based sessions in search engine query logs. In Proceedings of WSDM 2011, 277­286.",null,null
179,"[21] Claudio Lucchese, Salvatore Orlando, Raffaele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2013. Discovering tasks from search engine query logs. ACM Trans. Inf. Syst. 31, 3",null,null
180,"[22] Rishabh Mehrotra, Prasanta Bhattacharya, and Emine Yilmaz. 2016. Deconstructing complex search tasks: A Bayesian nonparametric approach for extracting sub-tasks. In Proceedings of NAACL 2016, 599­605.",null,null
181,"[23] Rishabh Mehrotra and Emine Yilmaz. 2015. Terms, topics & tasks: Enhanced user modelling for better personalization. In Proceedings of ICTIR 2015, 131­140.",null,null
182,"[24] Rishabh Mehrotra and Emine Yilmaz. 2017. Extracting hierarchies of search tasks & subtasks via a Bayesian nonparametric approach. In Proceedings of SIGIR 2017, 285­294.",null,null
183,"[25] Rishabh Mehrotra and Emine Yilmaz. 2017. Task embeddings: Learning query embeddings using task context. In Proceedings of CIKM 2017, 2199­2202.",null,null
184,"[26] Donald Metzler, Susan T. Dumais, and Christopher Meek. 2007. Similarity measures for short segments of text. In Proceedings of ECIR 2007. 16­27.",null,null
185,"[27] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv abs/1301.3781",null,null
186,"[28] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A picture of search. In Proceedings of Infoscale 2006, 1.",null,null
187,"[29] Procheta Sen, Debasis Ganguly, and Gareth J. F. Jones. 2018. Tempo-lexical context driven word embedding for cross-session search task extraction. In Proceedings of NAACL 2018. 283­292.",null,null
188,"[30] Amanda Spink, Minsoo Park, Bernard J. Jansen, and Jan O. Pedersen. 2006. Multitasking during web search sessions. Inf. Process. Manage. 42, 1",null,null
189,"[31] Manisha Verma and Emine Yilmaz. 2014. Entity oriented task extraction from query logs. In Proceedings of CIKM 2014, 1975­1978.",null,null
190,"[32] Manisha Verma and Emine Yilmaz. 2016. Category oriented task extraction. In Proceedings of CHIIR 2016, 333­336.",null,null
191,"[33] Zi Yang and Eric Nyberg. 2015. Leveraging procedural knowledge for taskoriented search. In Proceedings of SIGIR 2015, 513­522.",null,null
192,,null,null
193,972,null,null
194,,null,null
195,,null,null

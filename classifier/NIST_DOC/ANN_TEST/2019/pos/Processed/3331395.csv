,sentence
0,Demonstration Papers 2: Evaluation & Entities
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,Information Retrieval Meets Scalable Text Analytics: Solr Integration with Spark
5,
6,"Ryan Clancy, Jaejun Lee, Zeynep Akkalyoncu Yilmaz, and Jimmy Lin"
7,David R. Cheriton School of Computer Science University of Waterloo
8,
9,ABSTRACT
10,"Despite the broad adoption of both Apache Spark and Apache Solr, there is little integration between these two platforms to support scalable, end-to-end text analytics. We believe this is a missed opportunity, as there is substantial synergy in building analytical pipelines where the results of potentially complex faceted queries feed downstream text processing components. This demonstration explores exactly such an integration: we evaluate performance under different analytical scenarios and present three simple case studies that illustrate the range of possible analyses enabled by seamlessly connecting Spark to Solr."
11,"ACM Reference Format: Ryan Clancy, Jaejun Lee, Zeynep Akkalyoncu Yilmaz, and Jimmy Lin. 2019. Information Retrieval Meets Scalable Text Analytics: Solr Integration with Spark. In 42nd Int'l ACM SIGIR Conference on Research and Development in Information Retrieval"
12,1 INTRODUCTION
13,"In the realm of data science, Apache Spark has emerged as the de facto platform for analytical processing, with broad adoption in both industry and academia. While not originally designed for scalable text analytics, it can nevertheless be applied to process large document collections in a scalable, distributed fashion. However, using Spark for text processing is hampered by the lack of integration with full-text indexes, particularly useful in applications where the data scientist wishes to analyze only a subset of the collection. By default, the only approach for selecting a collection subset is a brute-force scan over every document with a filter transformation to retain only the desired documents. For selective queries that only match a small number of documents, this is obviously inefficient."
14,"In the realm of search, Apache Solr has emerged as the de facto platform for building production applications. Other than a handful of commercial web search engines that deploy custom infrastructure to achieve the necessary scale, most organizations today take advantage of Solr, including Best Buy, Bloomberg, Comcast, Disney, eHarmony, Netflix, Reddit, and Wikipedia. Although Solr is designed to be scalable via a distributed, partitioned architecture, the platform is primarily engineered around providing low-latency"
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331395"
16,
17,"user-facing search. As such, it does not provide any analytics capabilities per se."
18,"The current state of the broader ecosystem sees little overlap between Spark for general-purpose analytical processing on the one hand and Solr for production search applications on the other. This is a missed opportunity in creating tremendous synergies for text analytics, which combines elements of search as well as analytical processing. As a simple example, the output of a"
19,"The contribution of this demonstration is an exploration of how Solr and Spark can be integrated to support scalable text analytics. We investigate the performance characteristics of using Solr as a document store, taking advantage of its powerful search functionality as a pushdown predicate to select documents for downstream processing. This is compared against an alternative where the raw documents are stored in the Hadoop Distributed File System"
20,"With Solr/Spark integration, we present three case studies that illustrate the range of interesting analyses enabled by end-to-end text processing pipelines. These examples include kernel density estimation to study the temporal distribution of tweets, namedentity recognition to visualize document content, and link analysis to explore hyperlink neighborhoods."
21,2 SOLR/SPARK INTEGRATION
22,"To integrate Solr with Spark, we adopt the most obvious architecture where Solr serves as the document store. We assume that a document collection has already been ingested [2]."
23,"Solr is best described as a REST-centric platform, whereas Spark programs define sequences of data-parallel transformations"
24,
25,1313
26,
27,Demonstration Papers 2: Evaluation & Entities
28,
29,"SIGIR '19, July 21­25, 2019, Paris, France"
30,
31,"filter, map, etc.) over a data abstraction called Resilient Distributed Datasets"
32,"As part of initial explorations, we compared the performance of SolrRDD to a much simpler approach of mapping over an RDD of docids"
33,"From a performance perspective, the baseline for comparing Solr/Spark integration is Spark processing over documents stored on HDFS. In this setup, all document manipulations require scanning the entire collection. Previous studies have found that such brute-force operations are not as inefficient as one might think at first glance; for example, researchers have explored document ranking using this architecture [4, 6]. Such designs lead to simple implementations that enjoy excellent data locality and can take advantage of high disk throughput."
34,"Intuitively, we can characterize the performance tradeoffs between the two alternatives as follows: Suppose a data scientist wishes to process the entire collection. Fetching documents from Solr will likely be slower than simply scanning the entire collection on HDFS sequentially, since Solr index structures come with associated overheads. At the other end of the spectrum, suppose a data scientist is only interested in analyzing a single document. In this case, Solr would be obviously much faster than scanning the entire collection. Thus, the selectivity of the document subset, in addition to other considerations such as hardware configurations and the processing workload, will determine the relative performance of the two approaches. The balance of these various factors, however, is an empirical question."
35,3 PERFORMANCE EVALUATION
36,"We set out to empirically characterize the performance tradeoffs between the designs discussed in the previous section, principally examining two characteristics: selectivity and workload."
37,3.1 Experimental Setup
38,Our experiments were conducted on a cluster with ten nodes. Each node has 2× Intel E5-2670 @ 2.60GHz
39,1 https://github.com/lucidworks/spark-solr
40,
41,"Note that our processors are of the Sandy Bridge architecture, which was introduced in 2012 and discontinued in 2015, and thus we can characterize these computing resources as both ""modest"" and ""dated"". Similar amounts of compute power could be found on a single high-end server today."
42,We examined the following document collections:
43,"· The New York Times Annotated Corpus, a collection of 1.8 million news article, used in the TREC 2017 Common Core Track."
44,"· Tweets2013, a collection of 243 million tweets gathered over February and March of 2013, used in the TREC Microblog Tracks [8]."
45,"· ClueWeb09b, a web crawl comprising 50.2 million pages gathered by CMU in 2009, used in several TREC Web Tracks."
46,"All collections were ingested into Solr using Anserini [2, 13, 14]. For comparison, all collections were also loaded into HDFS. In both cases, the same document processing"
47,Our performance evaluation focused on two characteristics of large-scale text analytics: the number of documents to process
48,"While running experiments, we used the master node as the driver while running Spark jobs in client mode. Each job used 9 executors with 16 cores and was allocated 48GB of RAM per executor. This allowed us to take full advantage of the available cluster resources and exploit data locality as Spark workers were co-located on the same nodes as HDFS DataNodes and Solr shards."
49,3.2 Experimental Results
50,"Figure 1 summarizes the results of our experiments on ClueWeb09b, varying input selectivity and processing workload. We report averages over five runs"
51,"The left bar graph in Figure 1 simulates no per-document processing and captures the raw I/O capacity of both architectures. As expected, total execution time does not vary much with selectivity when brute-force scanning the collection on HDFS, since the entire document collection needs to be read regardless. Also as expected, the performance of using Solr as a pushdown predicate to select subsets of the collection depends on the size of the results set. For small subsets, Solr is more efficient since it exploits index structures to avoid needless scans of the collection. Execution time for Solr grows as more and more documents are requested, and beyond a certain point, Solr is actually slower than a scan over the entire collection due to the overhead of traversing index structures. This crossover point occurs at around half the collection--that is, if an analytical query yields more results, a brute-force scan over the entire collection will be faster."
52,"The above results assume that no time is spent processing each document, which is obviously unrealistic. In the middle and right"
53,
54,1314
55,
56,Demonstration Papers 2: Evaluation & Entities
57,
58,"SIGIR '19, July 21­25, 2019, Paris, France"
59,
60,Figure 1: Average total execution time
61,
62,"bar graphs in Figure 1, we simulate per-document processing latencies of 3ms and 30ms. The takeaway from these results is that Solr is always faster than a brute-force scan over the entire collection on HDFS. As the per-document workload increases, processing time occupies a growing fraction of the overall execution time and masks latencies associated with fetching a large number of documents from Solr. Thus, from these experiments we can conclude that, except in the most extreme case where text analytics is dominated by I/O, predicate pushdown via Solr is beneficial."
63,
64,4 CASE STUDIES
65,"We present three case studies that illustrate the range of analyses enabled by our Solr/Spark integration, taking advantage of existing open-source tools. While these analyses are certainly possible without our platform, they would require more steps: issuing queries to Solr, extracting the result documents from the collection, and importing them into downstream processing tools. In practice, this would likely be accomplished using one-off scripts with limited generality and reusability. In contrast, we demonstrate end-to-end text analytics with seamless integration of Spark and Solr, with a Jupyter notebook frontend."
66,4.1 Temporal Analysis
67,Kernel density estimation
68,"The top graph in Figure 2 shows results for four keywords, aggregated by hour of day"
69,"The results show diurnal and weekly cycles of activity. Peaks for the three daily meals occur where we'd expect, although Twitter users appear to eat breakfast"
70,2 https://spark.apache.org/mllib/
71,
72,Figure 2: Results of Kernel Density Estimation on creation time of tweets to capture diurnal and weekly activity cycles.
73,"of the day. In terms of weekly cycles, unsurprisingly, ""church"" peaks on Sunday, ""party"" peaks on Saturday, and mentions of school drop off on weekends. The core of this analysis is around 15 lines of code, highlighting the expressivity of Solr/Spark integration."
74,4.2 Entity Analysis
75,We can take advantage of named-entity recognition
76,3 https://github.com/amueller/word_cloud
77,
78,1315
79,
80,Demonstration Papers 2: Evaluation & Entities
81,
82,"SIGIR '19, July 21­25, 2019, Paris, France"
83,
84,"Figure 3: Word cloud for ""music"" from the New York Times"
85,"The people mentioned are, perhaps unsurprisingly, famous musicians such as Bob Dylan, Frank Sinatra, and Michael Jackson, but the results do reveal the musical tastes of New York Times writers. All of this can be accomplished in around 20 lines of code."
86,4.3 Webgraph Analysis
87,"Network visualizations facilitate qualitative assessment by revealing relationships between entities. In this case study, we extracted links referenced by websites in the ClueWeb09b collection that contain the polysemous term ""jaguar"""
88,"By feeding the edge list to Gephi,4 we ended up with the network visualization in Figure 4 using a Multilevel Layout [7]. For better clarity in the visualization, we pruned nodes with small degrees. Unsurprisingly, the visualization features a large cluster centered around google.com, and multiple smaller clusters corresponding to websites associated with different meanings of the term."
89,5 CONCLUSIONS
90,"In this work we have demonstrated the integration of Solr and Spark to support end-to-end text analytics in a seamless and efficient manner. Our three usage scenarios only scratch the surface of what's possible, since we now have access to the rich ecosystem that has sprung up around Spark. With PySpark, which provides"
91,4 https://gephi.org
92,
93,"Figure 4: Network visualization for ""jaguar""."
94,"Python bindings for Spark, we gain further integration opportunities with PyTorch, TensorFlow, and other deep learning frameworks, enabling access to state-of-the-art models for many text processing tasks, all in a single unified platform."
95,Acknowledgments. This work was supported in part by the Natural Sciences and Engineering Research Council
96,REFERENCES
97,"[1] G. Ananthanarayanan, A. Ghodsi, S. Shenker, and I. Stoica. 2011. Disk-Locality in Datacenter Computing Considered Irrelevant. In HotOS."
98,"[2] R. Clancy, T. Eskildsen, N. Ruest, and J. Lin. 2019. Solr Integration in the Anserini Information Retrieval Toolkit. In SIGIR."
99,"[3] M. Efron, J. Lin, J. He, and A. de Vries. 2014. Temporal Feedback for Tweet Search with Non-Parametric Density Estimation. In SIGIR. 33­42."
100,"[4] T. Elsayed, F. Ture, and J. Lin. 2010. Brute-Force Approaches to Batch Retrieval: Scalable Indexing with MapReduce, or Why Bother? Technical Report HCIL-201023. University of Maryland, College Park, Maryland."
101,"[5] S. Hendrickson, S. Sturdevant, T. Harter, V. Venkataramani, A. Arpaci-Dusseau, and R. Arpaci-Dusseau. 2016. Serverless Computation with OpenLambda. In HotCloud."
102,"[6] D. Hiemstra and C. Hauff. 2010. MapReduce for Information Retrieval Evaluation: ""Let's Quickly Test This on 12 TB of Data"". In CLEF. 64­69."
103,"[7] Y. Hu. 2005. Efficient, High-Quality Force-Directed Graph Drawing. Mathematica Journal 10, 1"
104,"[8] J. Lin and M. Efron. 2013. Overview of the TREC-2013 Microblog Track. In TREC. [9] J. Lin, D. Ryaboy, and K. Weil. 2011. Full-Text Indexing for Optimizing Selection"
105,Operations in Large-Scale Data Analytics. In MAPREDUCE. 59­66. [10] C. Macdonald. 2018. Combining Terrier with Apache Spark to Create Agile
106,"Experimental Information Retrieval Pipelines. In SIGIR. 1309­1312. [11] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and D. McClosky. 2014."
107,"The Stanford CoreNLP Natural Language Processing Toolkit. In ACL Demos. 55­60. [12] F. Moretti. 2013. Distant Reading. [13] P. Yang, H. Fang, and J. Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR. 1253­1256. [14] P. Yang, H. Fang, and J. Lin. 2018. Anserini: Reproducible Ranking Baselines Using Lucene. JDIQ 10, 4"
108,
109,1316
110,
111,

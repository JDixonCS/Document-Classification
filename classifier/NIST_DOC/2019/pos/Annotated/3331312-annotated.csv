,sentence,label,data
,,,
0,"Short Research Papers 2A: AI, Mining, and others",null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,A study on the Interpretability of Neural Retrieval Models using DeepSHAP,null,null
,,,
5,,null,null
,,,
6,Zeon Trevor Fernando,null,null
,,,
7,"L3S Research Center Hannover, Germany",null,null
,,,
8,fernando@l3s.de,null,null
,,,
9,,null,null
,,,
10,Jaspreet Singh,null,null
,,,
11,"L3S Research Center Hannover, Germany",null,null
,,,
12,singh@l3s.de,null,null
,,,
13,,null,null
,,,
14,Avishek Anand,null,null
,,,
15,"L3S Research Center Hannover, Germany",null,null
,,,
16,anand@l3s.de,null,null
,,,
17,,null,null
,,,
18,ABSTRACT,null,null
,,,
19,"A recent trend in IR has been the usage of neural networks to learn retrieval models for text based adhoc search. While various approaches and architectures have yielded significantly better performance than traditional retrieval models such as BM25, it is still difficult to understand exactly why a document is relevant to a query. In the ML community several approaches for explaining decisions made by deep neural networks have been proposed ­ including DeepSHAP which modifies the DeepLift algorithm to estimate the relative importance",null,null
,,,
20,"ACM Reference Format: Zeon Trevor Fernando, Jaspreet Singh, and Avishek Anand. 2019. A study on the Interpretability of Neural Retrieval Models using DeepSHAP. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
,,,
21,1 INTRODUCTION,null,null
,,,
22,Deep neural networks have achieved state of the art results in several NLP and computer vision tasks in the last decade. Along with this spurt in performance has come a new wave of approaches trying to explain decisions made by these complex machine learning models. Explainablilty and interpretability are key to deploying,null,null
,,,
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331312",null,null
,,,
24,,null,null
,,,
25,"NNs in the wild and having them work in tandem with humans. Explanations can help debug models, determine training data bias and understand decisions made in simpler terms in order to foster trust. Recently in IR, models such as DRMM [5], MatchPyramid [10], PACRR-DRMM [8] and others have shown great promise in ranking for adhoc text retrieval. While these models do improve state-ofthe-art on certain benchmarks, it is sometimes hard to understand why exactly these models are performing better. With the increased scrutiny on automated decision making systems, including search engines, it is vital to be able to explain decisions made. In IR however, little to no work has been done on trying to explain the output of complex neural ranking models.",null,null
,,,
26,"In the ML community, several post-hoc non-intrusive methods have been suggested recently which enable us to train highly accurate and complex models while also being able to get a sense of their rationale. One of the more popular approaches to producing explanations is to determine the input feature attributions for a given instance and it's prediction according to a given model. The output of such a method is typically visualized as a heat map over the input words/pixels. Several approaches have been proposed in this direction for image and text classification but their applicability to adhoc text retrieval and ranking remains unexplored. In this paper we study the applicability of one such method designed specifically for neural networks ­ DeepSHAP [7], to explain the output of 3 different neural retrieval models. DeepSHAP is a modification of the DeepLift [15] algorithm to efficiently estimate the shapley values over the input feature space for a given instance. The shapley value is a term coined by Shapley [14] in cooperative game theory to refer to the contribution of a feature in a prediction. More specifically, shapley values explain the contribution of an input feature towards the difference in the prediction made vs the average prediction value.",null,null
,,,
27,"The objective of our work is to utilize DeepSHAP to explain NRMs which should ideally be a trivial pursuit since they are standard neural networks. However, in our experiments, we found that DeepSHAP's explanations are highly dependent on a reference input which is used to compute the average prediction. This is inline with recent work that suggests approaches like DeepLIFT lack robustness [4]. In this work, we ponder on the question, what makes a good reference input distribution for neural rankers? In computer vision, a plain black image is used as the reference input but what is the document equivalent of such an image in IR? Furthermore, we found that explanations produced by the model introspective DeepSHAP are considerably different from the model agnostic approach ­ LIME [12]. Although both models produce local explanations, the variability is concerning.",null,null
,,,
28,,null,null
,,,
29,1005,null,null
,,,
30,,null,null
,,,
31,"Short Research Papers 2A: AI, Mining, and others",null,null
,,,
32,,null,null
,,,
33,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
34,,null,null
,,,
35,2 RELATED WORK,null,null
,,,
36,"There are two main approaches to interpretability in machine learning models: model agnostic and model introspective approaches. Model agnostic approaches [12, 13] generate post-hoc explanations for the original model by treating it as a black box by learning an interpretable model on the output of the model or by perturbing the inputs or both. Model introspective approaches on one hand include ""interpretable"" models such as decision trees [6], attentionbased networks [20], and sparse linear models [19] where there is a possibility to inspect individual model components",null,null
,,,
37,"Interpretability in ranking models Recently there have been few works focused on interpretability [17, 18] and diagnosis of neural IR models [9, 11]. In the diagnostic approaches, they use the formal retrieval constraints",null,null
,,,
38,"To the best of our knowledge, this is the first work that looks at model introspective interpretability specifically for NRMs.",null,null
,,,
39,3 DEEPSHAP FOR IR,Y,DeepShap
,,,
40,DeepSHAP is a local model-introspective interpretability method to approximate the shapley values using DeepLIFT [15]. DeepLIFT explains the difference in output/prediction from some `reference' output with respect to the difference of the input,Y,DeepShap
,,,
41,"In the context of IR, DeepSHAP can be used to explain why a document is relevant to query",Y,DeepShap
,,,
42,"Unlike classification tasks, in ranking we have at least 2 inputs which are in most cases the query and document tokens. In this work we fix the reference input for the query to be same as that of",null,null
,,,
43,,null,null
,,,
44,the query-document instance to be explained and experiment with various reference inputs for the document. The intuition behind doing so is to gain an average reference output in the locality of the query.,null,null
,,,
45,The various document reference inputs that we considered in our experiments are:,null,null
,,,
46,"OOV The reference document consists of `OOV' tokens. For, DRMM and MatchPyramid models the embedding vector for `OOV' comprises of all-zeros which is similar to the background image used for MNIST. But for PACRR-DRMM, it is the average of all the embedding vectors in the vocabulary.",null,null
,,,
47,"IDF lowest The reference document is constructed by sampling words with low IDF scores. These words are generally stopwords or words that are similar to stop-words so they should, in general, be irrelevant to the query.",null,null
,,,
48,QL lowest The reference document comprises of sampled words with low query-likelihood scores that are derived from a language model of the top-1000 documents.,null,null
,,,
49,COLLECTION rand doc The reference document is randomly sampled from the rest of the collection minus the top-1000 documents retrieved for the query.,null,null
,,,
50,TOPK LIST rand doc from bottom The reference document is randomly sampled from the bottom of the top-1000 documents retrieved.,null,null
,,,
51,These variants were designed based on the intuition that the reference input document would comprise of words that are irrelevant to the query and thus DeepSHAP should be able to pick the most important terms from the input document that explain relevance to the query.,null,null
,,,
52,4 EXPERIMENTAL SETUP,null,null
,,,
53,"In our experiments, we aim to answer the following research questions:",null,null
,,,
54,· Are DeepSHAP explanations sensitive to the type of reference input in the case of NRMs?,Y,DeepShap
,,,
55,· Can we determine which reference input produces the most accurate local explanation?,null,null
,,,
56,"To this end, we describe the experimental setup we used to address these questions. We describe the various NRM's we considered and how we used LIME to evaluate the correctness of explanations produced by DeepSHAP.",null,null
,,,
57,4.1 Neural Retrieval Models,null,null
,,,
58,DRMM [5] This model uses a query-document term matching count histogram as input into a feed forward neural network,null,null
,,,
59,MatchPyramid [10] This model uses a query-document interaction matrix as input to a 2D CNN to extract matching patterns. The output is then fed into a MLP to get a relevance score.,null,null
,,,
60,"PACRR-DRMM [8] This model creates a query-document interaction matrix that is fed into multiple 2D CNNs with different kernel sizes to extract n-gram matches. Then, after k-max pooling across each q-term, the document aware q-term encodings are fed into a MLP, like in DRMM to obtain a relevance score.",null,null
,,,
61,,null,null
,,,
62,1006,null,null
,,,
63,,null,null
,,,
64,"Short Research Papers 2A: AI, Mining, and others",null,null
,,,
65,,null,null
,,,
66,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
67,,null,null
,,,
68,Figure 1: Confusion matrices of various DeepSHAP background document methods comparing Jaccard similarities.,null,null
,,,
69,,null,null
,,,
70,4.2 Evaluation,null,null
,,,
71,"To conduct the experiments, we used the Robust04 test collection from TREC. We used Lucene to index and retrieve documents. Next, we trained the NRMs using their implementations in MatchZoo1 [3]. All the hyparameters were tuned using the same experimental setup as described in the respective papers. We chose to study explanations for the distinguished set of hard topics2",null,null
,,,
72,"Evaluating explanations Since no ground truth explanations are available for a neural model, we use LIME based explanations as a proxy. We found that it can approximate the locality of a query document pair well, using a simple linear model that achieved an accuracy of over 90% across all NRMs considered in our experiments. To produce the explanations from LIME we used the implementation found in 3 along with the score-based modification suggested in [18]. The primary parameters for training a LIME explanation model are the number of perturbed samples to be considered and the number of words for the explanation. The number of perturbed samples is set to 5000 and the number of words is varied based on the experiment. We used the DeepSHAP implementation provided here 4. Note that we ignore the polarity of the explanation terms provided by both methods in our comparison since the semantics behind the polarities in LIME and DeepSHAP are different. We are more interested in the terms chosen as the explanations in both cases.",null,null
,,,
73,5 RESULTS AND DISCUSSION,null,null
,,,
74,5.1 Effect of reference input document,null,null
,,,
75,"Figure 1 illustrates the overlap between the explanation terms produced when varying the reference input. Immediately we observe that the overlap between explanations produced is low; below 50% in most cases and consistently across all NRMs. Each reference input method has its own distinct semantics and this is reflected by the low overlap scores. We also find that there is no consistent trend across NRMs. For MatchPyramid, OOV and QL have highest overlap whereas for PACRR-DRMM its OOV and COL that have highest overlap even though both models share similarities in input",null,null
,,,
76,1 https://github.com/N TMC- Community/MatchZoo/tree/1.0 2 https://trec.nist.gov/data/robust/04.guidelines.html 3 https://github.com/marcotcr/lime 4 https://github.com/slundberg/shap,null,null
,,,
77,,null,null
,,,
78,"representation and parts of the model architecture. Table 3 shows explanations for DRMM across variants. Once again we see how the explanations can differ significantly if we are not careful in selecting the reference input. For IR, finding the background image seems to be a much harder question.",null,null
,,,
79,Our results show how explanations are highly sensitive to the reference input for NRMs chosen in our experiments. This is also indication that a single reference input method may not be the best for every NRM.,null,null
,,,
80,5.2 Accuracy of reference input methods,null,null
,,,
81,"To help identify which reference input method is most accurate in explaining a given query-document pair, we compared the LIME explanations for the same against it's corresponding DeepSHAP explanations. In general we found that DeepSHAP produces more explanation terms whereas LIME's L1 regularizer constrains the explanations to only the most important terms. Additionally, the discrepancy between the explanations can be attributed to LIME being purely local, whereas DeepSHAP has some global context since it looks at activations for the whole network which may have captured some global patterns. Hence, to estimate which reference input surfaces the most `ground truth' explanation terms we only computed recall at top 50 and 100",null,null
,,,
82,"The first interesting insight is that some NRMs are easier to explain whereas others are more difficult. PACRR-DRMM consistently has a recall less than 0.7, whereas the DeepSHAP explanations of DRMM effectively capture almost all of the LIME explanation terms. When comparing reference input variants within each NRM we find that there is no consistent winner. For DRMM, QL is the best which indicates that sampling terms which are relatively generic for this query in particular is a better `background image' than sampling generic words from the collection",null,null
,,,
83,"In the case of MatchPyramid, TOPK LIST is the worst performing but it is more difficult to distinguish between the approaches here. The best approach surprisingly is OOV. This can be attributed to how MatchPyramid treats OOV terms. The OOV token is represented by an all-zeros embedding vector that is used for padding the input interaction matrix whereas in DRMM, OOV tokens are filtered out. These preprocessing considerations prove to be crucial when determining the right input reference. Moving on to PACRRDRMM, we once again find that QL is the best method even though DeepSHAP struggles to find all the LIME terms.",null,null
,,,
84,,null,null
,,,
85,1007,null,null
,,,
86,,null,null
,,,
87,"Short Research Papers 2A: AI, Mining, and others",null,null
,,,
88,,null,null
,,,
89,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
90,,null,null
,,,
91,Table 1: Comparison of recall measures at top-k,null,null
,,,
92,,null,null
,,,
93,SHAP variants OOV IDF QL COLLECTION TOPK LIST.,null,null
,,,
94,,null,null
,,,
95,DRMM,null,null
,,,
96,,null,null
,,,
97,MatchPyramid,null,null
,,,
98,,null,null
,,,
99,PACRR-DRMM,null,null
,,,
100,,null,null
,,,
101,top-10,null,null
,,,
102,,null,null
,,,
103,top-20,null,null
,,,
104,,null,null
,,,
105,top-30,null,null
,,,
106,,null,null
,,,
107,top-10,null,null
,,,
108,,null,null
,,,
109,top-20,null,null
,,,
110,,null,null
,,,
111,top-30,null,null
,,,
112,,null,null
,,,
113,top-10,null,null
,,,
114,,null,null
,,,
115,top-20,null,null
,,,
116,,null,null
,,,
117,top-30,null,null
,,,
118,,null,null
,,,
119,recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall recall @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100 @50 @100,null,null
,,,
120,,null,null
,,,
121,0.789 0.905 0.672 0.845 0.615 0.812 0.793 0.843 0.656 0.726 0.566 0.640 0.582 0.582 0.388 0.388 0.299 0.299,null,null
,,,
122,,null,null
,,,
123,0.830 0.917 0.723 0.871 0.658 0.841 0.795 0.832 0.653 0.711 0.565 0.633 0.633 0.633 0.446 0.446 0.362 0.362,null,null
,,,
124,,null,null
,,,
125,0.894 0.955 0.754 0.892 0.670 0.856 0.765 0.821 0.638 0.711 0.556 0.636 0.643 0.643 0.462 0.462 0.367 0.367,null,null
,,,
126,,null,null
,,,
127,0.760 0.881 0.673 0.841 0.620 0.815 0.783 0.824 0.639 0.709 0.552 0.630 0.621 0.621 0.429 0.429 0.343 0.343,null,null
,,,
128,,null,null
,,,
129,0.639 0.821 0.606 0.794 0.578 0.788 0.759 0.811 0.624 0.702 0.545 0.627 0.625 0.625 0.425 0.425 0.340 0.340,null,null
,,,
130,,null,null
,,,
131,Table 2: Comparison of mean squared error,null,null
,,,
132,,null,null
,,,
133,NRM,null,null
,,,
134,DRMM MatchPyramid PACRR-DRMM,null,null
,,,
135,,null,null
,,,
136,TRAIN MSE,null,null
,,,
137,0.00631 0.01827 0.00165,null,null
,,,
138,,null,null
,,,
139,Linear Regression TEST MSE TRAIN ACC,null,null
,,,
140,,null,null
,,,
141,0.00633 0.01839 0.00160,null,null
,,,
142,,null,null
,,,
143,0.92662 0.90367 0.98857,null,null
,,,
144,,null,null
,,,
145,TEST ACC,null,null
,,,
146,0.92654 0.90387 0.98980,null,null
,,,
147,,null,null
,,,
148,Table 3: An example of words selected by LIME and SHAP methods for the query `cult lifestyles' and document `FBIS3-843' which is about clashes between cult members and student union's activists at a university in Nigeria. Words unique to a particular explanation method are highlighted in bold.,null,null
,,,
149,,null,null
,,,
150,LIME,null,null
,,,
151,,null,null
,,,
152,OOV,null,null
,,,
153,,null,null
,,,
154,IDF,null,null
,,,
155,,null,null
,,,
156,QL,null,null
,,,
157,,null,null
,,,
158,COL.,null,null
,,,
159,,null,null
,,,
160,TOPK,null,null
,,,
161,,null,null
,,,
162,cult style followers elite saloon student home members march september,null,null
,,,
163,,null,null
,,,
164,cult followers,null,null
,,,
165,black fraternities degenerate,null,null
,,,
166,sons academic american,null,null
,,,
167,tried household,null,null
,,,
168,,null,null
,,,
169,cult style followers suspects belong reappearing household black fraternities degenerate,null,null
,,,
170,,null,null
,,,
171,cult style elite saloon final march friday september arms closed,null,null
,,,
172,,null,null
,,,
173,cult black fraternities degenerate sons followers style home household avoid,null,null
,,,
174,,null,null
,,,
175,cult numbers english college university fallouts buccaneers feudings activists troubles,null,null
,,,
176,,null,null
,,,
177,6 CONCLUSION AND FUTURE WORK,null,null
,,,
178,In this paper we suggested several reference input methods for DeepSHAP that take into account the unique semantics of document ranking and relevance in IR. Through quantitative experiments we found that it is indeed sensitive to the reference input. The distinct lack of overlap in most cases was surprising but in line with recent works on the lack of robustness in interpretability approaches. We also tried to evaluate which reference method is more accurate by comparing against LIME. Here we found that reference input method selection is highly dependent on the model at hand. We believe that this work exposes new problems when dealing with model introspective interpretability for NRMs. A worthwhile endeavor will be to investigate new approaches that explicitly take into account the discreteness of text and the model's preprocessing choices when generating explanations.,null,null
,,,
179,,null,null
,,,
180,This work was supported by the Amazon research award on,null,null
,,,
181,`Interpretability of Neural Rankers'.,null,null
,,,
182,REFERENCES,null,null
,,,
183,"[1] Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2017. ""What is relevant in a text document?"": An interpretable machine learning approach. PLOS ONE 12",null,null
,,,
184,"[2] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PLOS ONE 10",null,null
,,,
185,"[3] Yixing Fan, Liang Pang, Jianpeng Hou, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2017. MatchZoo: A Toolkit for Deep Text Matching.",null,null
,,,
186,"[4] Amirata Ghorbani, Abubakar Abid, and James Y. Zou. 2019. Interpretation of Neural Networks is Fragile. In AAAI '19.",null,null
,,,
187,"[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM '16. ACM, 55­64.",null,null
,,,
188,"[6] Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, and David Madigan. 2015. Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9, 3",null,null
,,,
189,[7] Scott M Lundberg and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems 30. 4765­4774.,null,null
,,,
190,"[8] Ryan McDonald, George Brokos, and Ion Androutsopoulos. 2018. Deep Relevance Ranking Using Enhanced Document-Query Interactions. In EMNLP '18.",null,null
,,,
191,"[9] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2017. A Deep Investigation of Deep IR Models. arXiv preprint",null,null
,,,
192,"[10] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching As Image Recognition. In AAAI'16. 2793­2799.",null,null
,,,
193,"[11] Daan Rennings, Felipe Moraes, and Claudia Hauff. 2019. An Axiomatic Approach to Diagnosing Neural IR Models. In ECIR '19. 489­503.",null,null
,,,
194,"[12] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ""Why Should I Trust You?"": Explaining the Predictions of Any Classifier. In KDD '16. ACM, 1135­1144.",null,null
,,,
195,"[13] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: HighPrecision Model-Agnostic Explanations. In AAAI '18.",null,null
,,,
196,"[14] Lloyd S Shapley. 1953. A value for n-person games. Contributions to the Theory of Games 2, 28",null,null
,,,
197,"[15] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating Activation Differences. arXiv preprint",null,null
,,,
198,"[16] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. ICLR Workshop",null,null
,,,
199,[17] Jaspreet Singh and Avishek Anand. 2018. Interpreting search result rankings through intent modeling. arXiv preprint,null,null
,,,
200,[18] Jaspreet Singh and Avishek Anand. 2019. EXS: Explainable Search Using Local Model Agnostic Interpretability. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining,null,null
,,,
201,"[19] Berk Ustun and Cynthia Rudin. 2016. Supersparse Linear Integer Models for Optimized Medical Scoring Systems. Machine Learning 102, 3",null,null
,,,
202,"[20] Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In International Conference on Machine Learning - Volume 37",null,null
,,,
203,,null,null
,,,
204,1008,null,null
,,,
205,,null,null
,,,
206,,null,null

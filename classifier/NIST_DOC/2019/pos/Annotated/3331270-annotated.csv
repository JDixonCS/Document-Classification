,sentence,label,data
,,,
0,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
1,,null,null
,,,
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
3,,null,null
,,,
4,Time-Limits and Summaries for Faster Relevance Assessing,null,null
,,,
5,,null,null
,,,
6,Shahin Rahbariasl,null,null
,,,
7,"srahbari@uwaterloo.ca University of Waterloo Waterloo, Ontario, Canada",null,null
,,,
8,ABSTRACT,null,null
,,,
9,"Relevance assessing is a critical part of test collection construction as well as applications such as high-recall retrieval that require large amounts of relevance feedback. In these applications, tens of thousands of relevance assessments are required and assessing costs are directly related to the speed at which assessments are made. We conducted a user study with 60 participants where we investigated the impact of time limits",null,null
,,,
10,KEYWORDS,null,null
,,,
11,Relevance Assessing; Time Limits; Document Summaries,null,null
,,,
12,ACM Reference Format: Shahin Rahbariasl and Mark D. Smucker. 2019. Time-Limits and Summaries for Faster Relevance Assessing. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval,null,null
,,,
13,1 INTRODUCTION,null,null
,,,
14,The idea of a speed accuracy tradeoff,null,null
,,,
15,"When assessors are hired to collect relevance judgments for tasks such as test collection construction [9], speed and accuracy are chief concerns. The faster that assessors judge, the sooner we are done and usually the lower the cost. While faster judgments are",null,null
,,,
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331270",null,null
,,,
17,,null,null
,,,
18,Mark D. Smucker,null,null
,,,
19,mark.smucker@uwaterloo.ca University of Waterloo,null,null
,,,
20,"Waterloo, Ontario, Canada",null,null
,,,
21,"better, we want to maintain accuracy or at least make an informed decision about the tradeoff between speed and accuracy.",null,null
,,,
22,"Many factors influence the speed and accuracy of relevance judging with the chief ones being the assessors, the search topic or task, and the items being judged. For example, Smucker and Jethani [10] found that crowdsourced assessors judged newswire articles at an average rate of 15 seconds per document while supervised assessors in a laboratory setting took 27 seconds per document. In their experiment, they found lab workers to have somewhat better ability to discriminate relevant from non-relevant documents, but this difference was not statistically significant. For legal e-discovery tasks, Oard and Webber [8] report that a few minutes per document is typical.",null,null
,,,
23,"In Heitz [4]'s review of the speed accuracy tradeoff, he notes that behavioral science has come to see the process as one where sensory information is accumulated in a sequential fashion and lower accuracy results from less accumulated information on which to make a decision. Assuming that we can extend this understanding of the speed accuracy tradeoff to relevance assessing, we can think of an assessor as scanning, skimming, and reading a document and at any moment, we can stop the sequential input and ask for a relevance judgment. The less time the assessor has, the less material has been consumed on which to make a judgment. Placing a time limit on judging is akin to limiting the assessor to a summary of the document, albeit a summary formed in their mind via their reading behavior up to the point at which time runs out.",null,null
,,,
24,"In this paper, we investigate the effect of time limits and document summaries on relevance assessing speed and accuracy. We conducted a controlled laboratory study that used a factorial design and varied the time allowed",null,null
,,,
25,"We found that providing more time does result in better discrimination between relevant and non-relevant documents, but most of this advantage was for full documents rather than summaries, i.e. time limits less than 60 seconds hurt the discrimination ability of assessors when judging full documents. Little difference was found in discrimination ability for short summaries at the various time limits. In addition, no statistically significant difference was found in accuracy of judging for the time limits or for full documents vs. document summaries.",null,null
,,,
26,"After each search task, we asked participants about the ease of judging and other factors, and judging summaries with a time limit of 60 seconds was most favored by participants. Indeed, the average time to judge a summary under the 60 second limit was only 13.4 seconds. In other words, with effectively no time limit, judging",null,null
,,,
27,,null,null
,,,
28,901,null,null
,,,
29,,null,null
,,,
30,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
31,,null,null
,,,
32,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
33,,null,null
,,,
34,"summaries can be fast with little loss in quality. Time limits with full documents can be used to force the relevance assessor to make a judgment based on their limited examination of the document with little loss in accuracy, but this comes with the additional cost of time pressure and stress on the assessors.",null,null
,,,
35,2 RELATED WORK,null,null
,,,
36,"In a series of four experiments, Maddalena et al. [6] investigated crowdsourced assessors' judging behavior and the effect of placing time limits on judging. In these experiments, the crowdworkers judged newswire articles from TREC 8. If workers were given a time limit, Maddalena et al. found that a shorter time of 15 seconds resulted in better assessor agreement with the official relevant judgments as compared to a longer 30 seconds time limit. Other experiments investigated judging without time limits, time limits from 3 to 30 seconds and the advantage of a fixed judging time vs. a time limit. Time limits less than 15 seconds resulted in lower judging quality. They also discovered that a fixed judging time of 25-30 seconds was best. A fixed judging time requires the assessor to submit the judgment at the end of the time period and likely encourages the assessor to keep reading a documment until the time is up. In contrast to our work, Maddalena et al. [6] focused solely on crowdsourced workers while we used university students in a supervised setting and also investigated summaries vs. full documents.",Y,TREC 8
,,,
37,"Wang and Soergel [12] performed a user study on different parameters in relevance assessment including agreement, speed, and accuracy. Moderate to a substantial agreement was reported between the two groups of students. There was no significant difference in the speed of the two groups of assessors but the speed varied between the individuals.",null,null
,,,
38,"Similarly, Wang [11] studied the accuracy, agreement, speed and perceived difficulty in relevance judgments for e-discovery. Wang found speed and perceived difficulty to be correlated as well as perceived difficulty and accuracy, but Wang did not find a correlation between accuracy and speed. Regarding speed, only a small fraction of documents slowed down the assessors. In contrast to our work and Maddalena et al. [6], Wang [11] did not apply time limits and thus working faster or slower would have been at the discretion of the assessors. Assuming most assessors make a legitimate attempt to do a good job, it is reasonable to assume that assessors without time limits will modulate their time to achieve a uniform level of accuracy.",null,null
,,,
39,3 MATERIALS AND METHODS,null,null
,,,
40,We conducted a 2x3 factorial experiment. Our two factors were document type,null,null
,,,
41,3.1 User Study,null,null
,,,
42,"After receiving ethics clearance from our university's office of research ethics, we recruited participants, and after pilot testing, we had 60 people participate in the final study. Each participant started with a tutorial describing the experiment and practice using the interface to judge the relevance of 5 documents.",null,null
,,,
43,,null,null
,,,
44,"For the main task, participants judged 20 documents for each of six search topics that were different from the topic used in the tutorial phase. In total, participants judged 120 documents. The 2x3 factorial design resulted in 6 treatments. For a given topic, a participant received a single treatment. Of the 20 documents for each topic, half were relevant and half were non-relevant. Each participant saw a randomized order of the topics and documents. The six treatments were balanced across users and task order with a Latin square. Before each search task, participants answered a pretask questionnaire that asked them about their knowledge about the topic and other matters. After each search task, participants answered a questionnaire about the judging experience. We paid each participant $20 for their participation.",null,null
,,,
45,3.2 User Interface,null,null
,,,
46,"We designed the user interface to show the participants one document at time. Participants could see the title of the document and either the full document or a short document summary. The only actions allowed were to submit a judgment of relevant or non-relevant. Through the whole study, participants could see the search topic and its description for that task. Participants were also provided with information about the number of documents and the time left for judging each document.",null,null
,,,
47,"The study involved three different time constraints for sets of 20 documents: 15, 30, and 60 seconds per document. When participants ran out of time, the document was hidden and participants had to submit their judgment to proceed. We recorded the overall time they spent on judging the document including the time after hiding the document.",null,null
,,,
48,3.3 Topics and Documents,null,null
,,,
49,"In total, we used seven topics from the 2017 TREC Common Core track [1] and documents from the New York Times annotated corpus. Six topics were for the main task",Y,TREC
,,,
50,"We carefully selected documents for the experiment to avoid biasing judging errors toward false positives or false negatives. For each topic, we first computed a document ordering using reciprocal rank fusion",null,null
,,,
51,,null,null
,,,
52,902,null,null
,,,
53,,null,null
,,,
54,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
55,,null,null
,,,
56,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
57,,null,null
,,,
58,3.4 Document Summaries,null,null
,,,
59,"For three out of the six main tasks, we showed the users paragraphlong summaries instead of full documents. We use a modification of the method of Zhang et al. [14] to create the summaries. This method trains a classifier to recognize relevant material and after dividing a document into paragraphs, selects the paragraph with the highest likelihood of being relevant as the summary. Our modification is that we trained the classifier based on the known relevant and non-relevant documents from the NIST judgments. As such, our document summaries represent an upper bound on the technique's potential for selecting the most relevant paragraph from a document.",null,null
,,,
60,,null,null
,,,
61,3.5 Measures of Accuracy and Discrimination,null,null
,,,
62,"We define the NIST relevance judgments to be the truth and measure performance against them. As such, a basic measure is accuracy:",null,null
,,,
63,,null,null
,,,
64,Accur acy,null,null
,,,
65,,null,null
,,,
66,=,null,null
,,,
67,,null,null
,,,
68,|T P |,null,null
,,,
69,,null,null
,,,
70,+,null,null
,,,
71,,null,null
,,,
72,|T P | + |T N | |F P | + |T N | +,null,null
,,,
73,,null,null
,,,
74,|FN |,null,null
,,,
75,,null,null
,,,
76,-1,null,null
,,,
77,,null,null
,,,
78,where |T P | is the number of true positives,null,null
,,,
79,"To measure statistical significance of results, we used generalized linear mixed-effects models, as implemented in the lme4 [2] package in R. Both the participants and topics are random effects and the experiment factors",null,null
,,,
80,,null,null
,,,
81,4 RESULTS AND DISCUSSION,null,null
,,,
82,"Table 1 shows the average time participants took to judge a document, and Table 2 shows the fraction of judgments that exceeded the time limit. As the time limit increases, the average time to judge a document increases regardless of whether it is a full document or a summary. When the time limit is 60 seconds, participants rarely exceed the time limit and we see that full documents take on average 22.6 seconds to judge in comparison to only 13.4 seconds for a summary. When a time limit of 15 seconds is imposed, both",null,null
,,,
83,,null,null
,,,
84,Time Limit,null,null
,,,
85,,null,null
,,,
86,Doc. Type 15 30,null,null
,,,
87,,null,null
,,,
88,60 Mean p-value,null,null
,,,
89,,null,null
,,,
90,Full doc. 9.8 15.2 22.6 Summary 9.1 11.5 13.4,null,null
,,,
91,,null,null
,,,
92,15.9 11.3,null,null
,,,
93,,null,null
,,,
94,p  0.001,null,null
,,,
95,,null,null
,,,
96,Mean 9.5 13.4 18.0 13.6,null,null
,,,
97,,null,null
,,,
98,p-value,null,null
,,,
99,,null,null
,,,
100,p  0.001,null,null
,,,
101,,null,null
,,,
102,Table 1: Average time in seconds to judge a document.,null,null
,,,
103,,null,null
,,,
104,Time Limit,null,null
,,,
105,,null,null
,,,
106,Doc. Type 15 30,null,null
,,,
107,,null,null
,,,
108,60 Mean,null,null
,,,
109,,null,null
,,,
110,Full doc. 0.25 0.14 Summary 0.17 0.04,null,null
,,,
111,,null,null
,,,
112,0.04 0.14 0.00 0.07,null,null
,,,
113,,null,null
,,,
114,Mean,null,null
,,,
115,,null,null
,,,
116,0.21 0.09,null,null
,,,
117,,null,null
,,,
118,0.02 0.11,null,null
,,,
119,,null,null
,,,
120,Table 2: Fraction of judgments that exceeded time limit.,null,null
,,,
121,,null,null
,,,
122,Time Limit,null,null
,,,
123,,null,null
,,,
124,Doc. Type 15 30 60 Mean,null,null
,,,
125,,null,null
,,,
126,Full doc. 0.70 0.71 0.73 0.71,null,null
,,,
127,,null,null
,,,
128,Summary 0.73 0.72 0.74 0.73,null,null
,,,
129,,null,null
,,,
130,Mean 0.71 0.71 0.74 0.72,null,null
,,,
131,,null,null
,,,
132,p-value,null,null
,,,
133,,null,null
,,,
134,p = 0.09,null,null
,,,
135,,null,null
,,,
136,Table 3: Average accuracy.,null,null
,,,
137,,null,null
,,,
138,p-value p = 0.18,null,null
,,,
139,,null,null
,,,
140,"full documents and summaries take only 9.8 and 9.1 seconds respectively. In effect, the 15 seconds time limit forces an assessor to consume material on par with a paragraph long summary. For similar judging tasks, Zhang [13] found average, unrestricted",null,null
,,,
141,"Table 3 shows the average accuracy for the 60 participants and the six experimental treatments. As explained in Section 3.5, we report the statistical significance of each of the experimental factors' effect on accuracy, and as Table 3 shows, neither the time limits nor the document type had a statistically significant effect on accuracy. In contrast to accuracy, Table 5 shows that time has a statistically significant effect on the d  measure of discrimination ability. As with accuracy, discrimination ability with summaries is not different from full documents at a statistically significant level.",null,null
,,,
142,"Tables 6 and 7 show the true positive and false positive rates for each of the six treatments. The higher true positive rate for summaries along with the effectively same false positive rate as for judging full documents shows that summaries have the potential to help assessors identify relevant material while not simply raising the false positive rate. With full documents, as the participants had more time, they were able to consume more material and suppress false positives. Thus, while there is little advantage to judging full documents in terms of accuracy, the evidence shows that full documents result in more conservative judgments.",null,null
,,,
143,"Finally, Table 4 shows the results for a selection of the posttask questionnaire's questions. In comparison to full documents, participants found judging summaries with a 60 second time limit, which is akin to having no time limit for such small amounts of text,",null,null
,,,
144,,null,null
,,,
145,903,null,null
,,,
146,,null,null
,,,
147,Short Research Papers 1B: Recommendation and Evaluation,null,null
,,,
148,,null,null
,,,
149,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
,,,
150,,null,null
,,,
151,Question,null,null
,,,
152,,null,null
,,,
153,Treatment,null,null
,,,
154,,null,null
,,,
155,"(15, F)",null,null
,,,
156,,null,null
,,,
157,Difficulty,null,null
,,,
158,,null,null
,,,
159,2.5,null,null
,,,
160,,null,null
,,,
161,3.1,null,null
,,,
162,,null,null
,,,
163,2.8,null,null
,,,
164,,null,null
,,,
165,3.2,null,null
,,,
166,,null,null
,,,
167,3.4,null,null
,,,
168,,null,null
,,,
169,3.6 3.1,null,null
,,,
170,,null,null
,,,
171,Experience,null,null
,,,
172,,null,null
,,,
173,3,null,null
,,,
174,,null,null
,,,
175,2.9,null,null
,,,
176,,null,null
,,,
177,3.2,null,null
,,,
178,,null,null
,,,
179,3.5,null,null
,,,
180,,null,null
,,,
181,3.4 3.2,null,null
,,,
182,,null,null
,,,
183,Mood,null,null
,,,
184,,null,null
,,,
185,3.3,null,null
,,,
186,,null,null
,,,
187,3.3,null,null
,,,
188,,null,null
,,,
189,3.2,null,null
,,,
190,,null,null
,,,
191,3.6,null,null
,,,
192,,null,null
,,,
193,3.3,null,null
,,,
194,,null,null
,,,
195,3.2 3.3,null,null
,,,
196,,null,null
,,,
197,Concentration,null,null
,,,
198,,null,null
,,,
199,2.9,null,null
,,,
200,,null,null
,,,
201,3,null,null
,,,
202,,null,null
,,,
203,3.1,null,null
,,,
204,,null,null
,,,
205,3.4,null,null
,,,
206,,null,null
,,,
207,3.5,null,null
,,,
208,,null,null
,,,
209,3.7 3.3,null,null
,,,
210,,null,null
,,,
211,Confidence,null,null
,,,
212,,null,null
,,,
213,3.2,null,null
,,,
214,,null,null
,,,
215,3.4,null,null
,,,
216,,null,null
,,,
217,3.3,null,null
,,,
218,,null,null
,,,
219,3.6,null,null
,,,
220,,null,null
,,,
221,3.8 3.4,null,null
,,,
222,,null,null
,,,
223,Time Pressure,null,null
,,,
224,,null,null
,,,
225,2.1,null,null
,,,
226,,null,null
,,,
227,2.4,null,null
,,,
228,,null,null
,,,
229,2.8,null,null
,,,
230,,null,null
,,,
231,2.3,null,null
,,,
232,,null,null
,,,
233,2.9,null,null
,,,
234,,null,null
,,,
235,3.0 2.6,null,null
,,,
236,,null,null
,,,
237,Accuracy,null,null
,,,
238,,null,null
,,,
239,3.2,null,null
,,,
240,,null,null
,,,
241,3.4,null,null
,,,
242,,null,null
,,,
243,3.4 3.4,null,null
,,,
244,,null,null
,,,
245,3.7 3.9 3.5,null,null
,,,
246,,null,null
,,,
247,"Table 4: Post task questionnaire. Participants rated their feelings and self-perceived performance. A value of 3 is ""neutral"".",null,null
,,,
248,,null,null
,,,
249,Time Limit,null,null
,,,
250,,null,null
,,,
251,Doc. Type 15 30 60 Mean p-value,null,null
,,,
252,,null,null
,,,
253,Full doc. 1.07 1.18 1.31 Summary 1.27 1.22 1.36,null,null
,,,
254,,null,null
,,,
255,1.19 1.28,null,null
,,,
256,,null,null
,,,
257,p = 0.13,null,null
,,,
258,,null,null
,,,
259,Mean 1.17 1.20 1.34 1.23,null,null
,,,
260,,null,null
,,,
261,p-value,null,null
,,,
262,,null,null
,,,
263,p = 0.047,null,null
,,,
264,,null,null
,,,
265,Table 5: Average ability to discriminate,null,null
,,,
266,,null,null
,,,
267,Doc. Type Full doc. Summary,null,null
,,,
268,,null,null
,,,
269,Time Limit,null,null
,,,
270,,null,null
,,,
271,Mean 0.64 0.68,null,null
,,,
272,,null,null
,,,
273,p-value p = 0.03,null,null
,,,
274,,null,null
,,,
275,Mean 0.67 0.65 0.65 0.66,null,null
,,,
276,,null,null
,,,
277,p-value,null,null
,,,
278,,null,null
,,,
279,p = 0.63,null,null
,,,
280,,null,null
,,,
281,Table 6: Estimated true positive rates.,null,null
,,,
282,,null,null
,,,
283,Time Limit,null,null
,,,
284,,null,null
,,,
285,Full doc. 0.28 0.26 0.22 Summary 0.28 0.27 0.23,null,null
,,,
286,,null,null
,,,
287,0.25 0.26,null,null
,,,
288,,null,null
,,,
289,p = 0.29,null,null
,,,
290,,null,null
,,,
291,Mean 0.28 0.26 0.23 0.26,null,null
,,,
292,,null,null
,,,
293,p-value,null,null
,,,
294,,null,null
,,,
295,p < 0.001,null,null
,,,
296,,null,null
,,,
297,Table 7: Estimated false positive rates.,null,null
,,,
298,,null,null
,,,
299,"to be easier, more enjoyable, and less stressful. At the same time, participants were more confident and believed they were more accurate in their judging of summaries under the 60 second limit.",null,null
,,,
300,5 CONCLUSION,null,null
,,,
301,"We conducted a user study and investigated the impact of time limits and document size on relevance assessing. We found no difference in the quality of judgments with summaries in comparison to full documents. As noted in Section 3.4, our summaries represent an upper bound on performance as we selected the paragraph most likely to be relevant given a classifier trained using known relevance judgments. Even so, these results are in line with past research that has shown little difference in relevance judging accuracy between summaries and full documents [7].",null,null
,,,
302,,null,null
,,,
303,"Imposing a time limit can speed judging of both full documents and summaries without a loss of accuracy, which shows how little material assessors need to consume from typical newswire documents for making judgments. An aggressive time limit of 15 seconds produced judgments at a rate of 9.5 seconds per document, but such time limits increased the stress on the participants and reduced their enjoyment of the task.",null,null
,,,
304,"Summaries appear to be a better solution to speeding relevance judging than time limits. With a generous time limit of 60 seconds, our assessors averaged 13.4 seconds per document and this treatment resulted in the best experience as measured by our post-task questionnaire.",null,null
,,,
305,ACKNOWLEDGMENTS,null,null
,,,
306,Thanks to Nimesh Ghelani and Adam Roegiest for their technical contributions. Thanks to Gordon Cormack and Maura Grossman for their feedback. This work was supported in part by the Natural Sciences and Engineering Research Council of Canada,null,null
,,,
307,REFERENCES,null,null
,,,
308,"[1] Allan, J., E. Kanoulas, D. Li, C. V. Gysel, D. Harman, and E. Voorhees",null,null
,,,
309,"[2] Bates, D., M. Mächler, B. Bolker, and S. Walker",null,null
,,,
310,"[3] Cormack, G. V., C. L. Clarke, and S. Buettcher",null,null
,,,
311,"[4] Heitz, R. P.",null,null
,,,
312,"[5] Macmillan, N. and C. Creelman",null,null
,,,
313,martini,null,null
,,,
314,,null,null
,,,
315,904,null,null
,,,
316,,null,null
,,,
317,,null,null

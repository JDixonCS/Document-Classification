,sentence,label,data
0,Short Research Papers 1B: Recommendation and Evaluation,null,null
1,,null,null
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
3,,null,null
4,Content-Based Weak Supervision for Ad-Hoc Re-Ranking,null,null
5,,null,null
6,Sean MacAvaney,null,null
7,"IRLab, Georgetown University sean@ir.cs.georgetown.edu",null,null
8,Kai Hui,null,null
9,Amazon kaihuibj@amazon.com,null,null
10,ABSTRACT,null,null
11,"One challenge with neural ranking is the need for a large amount of manually-labeled relevance judgments for training. In contrast with prior work, we examine the use of weak supervision sources for training that yield pseudo query-document pairs that already exhibit relevance",null,null
12,"ACM Reference Format: Sean MacAvaney, Andrew Yates, Kai Hui, and Ophir Frieder. 2019. ContentBased Weak Supervision for Ad-Hoc Re-Ranking. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
13,1 INTRODUCTION,null,null
14,"A lack of manual training data is a perennial problem in information retrieval [18]. To enable training supervised rankers for new domains, we propose a weak supervision approach based on pairs of text to train neural ranking models and a filtering technique to adapt the dataset to a given domain. Our approach eliminates the need for a query log or large amounts of manually-labeled indomain relevance judgments to train neural rankers, and exhibits stronger and more varied positive relevance signals than prior weak supervision work",null,null
15,Others have experimented with weak supervision for neural ranking,null,null
16,Work conducted while the author was at the Max Planck Institute for Informatics.,null,null
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331316",null,null
18,,null,null
19,Andrew Yates,null,null
20,Max Planck Institute for Informatics ayates@mpi-inf.mpg.de,null,null
21,Ophir Frieder,null,null
22,"IRLab, Georgetown University ophir@ir.cs.georgetown.edu",null,null
23,"using datasets of text pairs that exhibit relevance, rather than using a heuristic to find pseudo-relevant documents for queries. For instance, the text pair from a newswire dataset consisting of an article's headline and its content exhibits an inherent sense of relevance because a headline often provides a concise representation of an article's content. To overcome possible domain differences between the training data and the target domain, we propose an approach to filter the training data using a small set of queries",null,null
24,We evaluate our approaches by training several leading neural ranking architectures on two sources of weak supervision text pairs. We show that our methods can significantly outperform various neural rankers when trained using a query log source,null,null
25,2 BACKGROUND AND RELATED WORK 2.1 Neural IR models,null,null
26,Ad-hoc retrieval systems rank documents according to their relevance to a given query. A neural IR model,null,null
27,KNRM [16] uses Gaussian kernels applied to each individual similarity score and log-summed across the document dimension. A final dense learning-to-rank phase combines these features into a relevance score.,null,null
28,1,null,null
29,https://github.com/Georgetown- IR- Lab/neuir- weak- supervision,null,null
30,,null,null
31,993,null,null
32,,null,null
33,Short Research Papers 1B: Recommendation and Evaluation,null,null
34,,null,null
35,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
36,,null,null
37,Conv-KNRM [4] is a variant of KNRM which applies convolution filters of lengths 1­3 over word embeddings before building cross-matched,null,null
38,"PACRR [8] uses square convolutional kernels over the similarity matrix to capture soft n-gram matches. k-max pooling is applied to retain only the strongest signals for each query term, and signals are combined with a dense layer.",null,null
39,2.2 Weak supervision,null,null
40,"In IR, weak supervision uses pseudo-relevant information to train a ranking model in place of human judgments. Early work on weak supervision for IR focused on training learning-to-rank models [2], using web anchor text [1] and microblog hashtags [3] for weak supervision. More recently, Dehghani et al. [5] proposed a weak supervision approach that makes use of the AOL query log and BM25 results as a source of training data. Aside from limitations surrounding the availability of query logs, their approach suffers from limitations of BM25 itself: it assumes that documents ranked higher by BM25 are more relevant to the query than documents ranked lower. Others have suggested using a similar approach, but using news headlines [9], also assuming relevance from BM25 rankings. Still others have employed a Generative Adversarial Network to build training samples [15], but this limits the generated data to the types of relevance found in the training samples, making it a complementary approach. In contrast, our approach uses freelyavailable text pairs that exhibit both a high quality and large size.",null,null
41,3 METHOD 3.1 Ranking- and content-based sources,null,null
42,"Recall that pairwise training consists of a set of training triples, each consisting of a query q, relevant document d+, and non-relevant document d-. We describe two sources of weak supervision training data that replace human-generated relevance judgments: rankingbased and content-based training sources.",null,null
43,"Ranking-based training sources, first proposed by [5], are defined by a collection of texts T , a collection of documents D, and an unsupervised ranking function R(q, d)",null,null
44,"Content-based training sources are defined as a collection of text pairs P = {(a1, b1),",null,null
45,"2Our formulation of ranking-based sources is slightly different than what was proposed by Dehghani et al. [5]: we use cutoff thresholds for positive and negative training samples, whereas they suggest using random pairs. Pilot studies we conducted showed that the threshold technique usually performs better.",null,null
46,,null,null
47,selected,null,null
48,Although ranking-based and content-based training sources bear,null,null
49,"some similarities, important differences remain. Content-based",null,null
50,"sources use text pairs as a source of positive relevance, whereas",null,null
51,"ranking-based sources use the unsupervised ranking. Furthermore,",null,null
52,"content-based sources use documents from the pair's domain, not",null,null
53,the target domain. We hypothesize that the enhanced notion of rel-,null,null
54,evance that content-based sources gain from text pairs will improve,null,null
55,"ranking performance across domains, and show this in Section 4.",null,null
56,,null,null
57,3.2 Filter framework,null,null
58,,null,null
59,We propose a filtering framework to overcome domain mismatch,null,null
60,,null,null
61,that can exist between data found in a weak supervision training,null,null
62,,null,null
63,source and data found in the target dataset. The framework consists,null,null
64,of a filter function FD,null,null
65,set of pairs TD is assumed to be relevant in the given domain.3,null,null
66,We assert that these filters are easy to design and can have broad,null,null
67,,null,null
68,coverage of ranking architectures. We present two implementations,null,null
69,"of the filter framework: the kmax filter, and the Discriminator filter. k-Maximum Similarity",null,null
70,"filter consists of two components: a representation function rep(q, d) and a distance function dist(r1, r2). The representation function captures some matching signal between query q and document d",null,null
71,,null,null
72,as a vector. Since many neural ranking models consider similarity,null,null
73,,null,null
74,"scores between terms in the query and document to perform soft term matching [4, 7, 8, 16], this filter selects the k maximum cosine",null,null
75,,null,null
76,similarity scores between the word vectors of each query term and,null,null
77,,null,null
78,all,null,null
79,,null,null
80,terms,null,null
81,,null,null
82,in,null,null
83,,null,null
84,the,null,null
85,,null,null
86,document:,null,null
87,,null,null
88,maxk,null,null
89,d,null,null
90,,null,null
91,j,null,null
92,,null,null
93,d,null,null
94,,null,null
95,"sim(qi , dj )",null,null
96,,null,null
97,:,null,null
98,,null,null
99,qi,null,null
100,,null,null
101,,null,null
102,,null,null
103,q.,null,null
104,,null,null
105,Since neural models can capture local patterns,null,null
106,,null,null
107,we use an aligned mean square error. The aligned MSE iterates over,null,null
108,,null,null
109,possible configurations of elements in the representation by shifting,null,null
110,,null,null
111,the position to find the alignment that yields the smallest distance.,null,null
112,,null,null
113,"In other words, it represents the minimum mean squared error",null,null
114,,null,null
115,given all rotated configurations of the query. Based on the shift,null,null
116,"operation and given two interaction representation matrices r1 and r2, the aligned distkmax",null,null
117,"Using these two functions, the filter is simply defined as the min-",null,null
118,,null,null
119,imum distance between the representations of it and any template,null,null
120,,null,null
121,pair from the target domain:,null,null
122,,null,null
123,FD,null,null
124,,null,null
125,"(q,",null,null
126,,null,null
127,d,null,null
128,,null,null
129,),null,null
130,,null,null
131,=,null,null
132,,null,null
133,min,null,null
134,"(q , d  ) TD",null,null
135,,null,null
136,dist,null,null
137,,null,null
138,(r ep,null,null
139,,null,null
140,"d ),",null,null
141,,null,null
142,r ep,null,null
143,,null,null
144,",",null,null
145,,null,null
146,d,null,null
147,,null,null
148,)),null,null
149,,null,null
150,(1),null,null
151,,null,null
152,3,null,null
153,Templates do not require human judgments. We use sample queries and an unsupervised ranker to generate TD . Manual judgments can be used when available.,null,null
154,,null,null
155,994,null,null
156,,null,null
157,Short Research Papers 1B: Recommendation and Evaluation,null,null
158,,null,null
159,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
160,,null,null
161,"Discriminator filter. A second approach to interaction filtering is to use the ranking architecture R itself. Rather than training R to distinguish different degrees of relevance, here we use R to train a model to distinguish between samples found in the weak supervision source and TD . This technique employs the same pairwise loss approach used for relevance training and is akin to the",null,null
162,discriminator found in generative adversarial networks. Pairs are,null,null
163,"sampled uniformly from both templates and the weak supervision source. Once RD is trained, all weak supervision training samples are ranked with this model acting as FD",null,null
164,The intuition behind this approach is that the model should,null,null
165,learn characteristics that distinguish in-domain pairs from out-of-,null,null
166,"domain pairs, but it will have difficulty distinguishing between",null,null
167,cases where the two are similar. One advantage of this approach,null,null
168,"is that it allows for training an interaction filter for any arbitrary ranking architecture, although it requires a sufficiently large TD to avoid overfitting.",null,null
169,4 EVALUATION 4.1 Experimental setup,null,null
170,Training sources. We use the following four sources of training data to verify the effectiveness of our methods:,null,null
171,- Query Log,null,null
172,- Newswire,null,null
173,lines as pseudo queries and the corresponding content as pseudo,null,null
174,"relevant documents. We use BM25 to select the negative articles, retaining top c- = 100 articles for individual headlines. - Wikipedia",null,null
175,been employed as a training set for the Trec Complex Answer Re-,null,null
176,trieval,null,null
177,"source, assuming that the hierarchy of headings is a relevant",null,null
178,query for the paragraphs under the given heading. Heading-,null,null
179,paragraph pairs from train fold 1 of the Trec CAR dataset [6],null,null
180,(v1.5) are used. We generate negative heading-paragraph pairs for each heading using BM25,null,null
181,of relevance judgments generated by human assessors. In par-,null,null
182,"ticular, manual judgments from 2010 Trec Web Track ad-hoc task",null,null
183,with limited,null,null
184,4,null,null
185,"Distinct non-navigational queries from the AOL query log from March 1, 2006 to May 31, 2006 are selected. We randomly sample 100k of queries with length of at least 4. While Dehghani et al. [5] used a larger number of queries to train their model, the state-of-the-art relevance matching models we evaluate do not learn term embeddings",null,null
186,5,null,null
187,https://www.lemurproject.org/indri/,null,null
188,,null,null
189,Training neural IR models. We test our method using several state-of-the-art neural IR models,null,null
190,Interaction filters. We use the 2-maximum and discriminator filters for each ranking architecture to evaluate the effectiveness of the interaction filters. We use queries from the target domain,null,null
191,"Baselines and benchmarks. As baselines, we use the AOL ranking-based source as a weakly supervised baseline [5], WT10 as a manual relevance judgment baseline, and BM25 as an unsupervised baseline. The two supervised baselines are trained using the same conditions as our approach, and the BM25 baselines is tuned on each testing set with Anserini [17], representing the best-case performance of BM25.8 We measure the performance of the models using the Trec Web Track 2012­2014",null,null
192,4.2 Results,null,null
193,"In Table 1, we present the performance of the rankers when trained using content-based sources without filtering. In terms of absolute score, we observe that the two n-gram models",null,null
194,6,null,null
195,"By using these stat-of-the-art architectures, we are using stronger baselines than those used in [5, 9]. 7https://lemurproject.org/clueweb09.php, https://lemurproject.org/clueweb12.php 8Grid search: b  [0.05, 1]",null,null
196,,null,null
197,995,null,null
198,,null,null
199,Short Research Papers 1B: Recommendation and Evaluation,null,null
200,,null,null
201,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
202,,null,null
203,Table 1: Ranking performance when trained using contentbased sources,null,null
204,,null,null
205,Model,null,null
206,,null,null
207,Training,null,null
208,,null,null
209,BM25,null,null
210,,null,null
211,PACRR,null,null
212,,null,null
213,WT10 AOL,null,null
214,,null,null
215,NYT Wiki,null,null
216,,null,null
217,Conv-KNRM WT10 AOL,null,null
218,,null,null
219,NYT Wiki,null,null
220,,null,null
221,KNRM,null,null
222,,null,null
223,WT10 AOL,null,null
224,,null,null
225,NYT Wiki,null,null
226,,null,null
227,WT12,null,null
228,0.1087,null,null
229,B 0.1628 0.1910,null,null
230,W B 0.2135 W B 0.1955,null,null
231,B 0.1580 0.1498,null,null
232,A B 0.1792 0.1536,null,null
233,B 0.1764 B 0.1782 W 0.1455 A W 0.1417,null,null
234,,null,null
235,nDCG@20,null,null
236,WT13,null,null
237,0.2176,null,null
238,0.2513 0.2608 A W B 0.2919 A B 0.2881,null,null
239,0.2398 0.2155 A W B 0.2904 A 0.2680 0.2671 0.2648 A 0.2340 0.2409,null,null
240,,null,null
241,WT14,null,null
242,0.2646,null,null
243,0.2676 0.2802,null,null
244,W 0.3016 W 0.3002 B 0.3197,null,null
245,0.2889,null,null
246,B 0.3215 B 0.3206,null,null
247,0.2961 0.2998,null,null
248,0.2865 0.2959,null,null
249,,null,null
250,"are useful, and it is valuable for these models to see a wide variety of n-gram relevance signals when training. The n-gram models also often perform significantly better than the ranking-based AOL query log baseline. This makes sense because BM25's rankings do not consider term position, and thus cannot capture this important indicator of relevance. This provides further evidence that contentbased sources do a better job providing samples that include various notions of relevance than ranking-based sources.",null,null
251,"When comparing the performance of the content-based training sources, we observe that the NYT source usually performs better than Wiki. We suspect that this is due to the web domain being more similar to the newswire domain than the complex answer retrieval domain. For instance, the document lengths of news articles are more similar to web documents, and precise term matches are less common in the complex answer retrieval domain [10].",null,null
252,"We present filtering performance on NYT and Wiki for each ranking architecture in Table 2. In terms of absolute score, the filters almost always improve the content-based data sources, and in many cases this difference is statistically significant. The one exception is for Conv-KNRM on NYT. One possible explanation is that the filters caused the training data to become too homogeneous, reducing the ranker's ability to generalize. We suspect that Conv-KNRM is particularly susceptible to this problem because of language-dependent convolutional filters; the other two models rely only on term similarity scores. We note that Wiki tends to do better with the 2max filter, with significant improvements seen for Conv-KNRM and KNRM. In thse models, the discriminator filter may be learning surface characteristics of the dataset, rather than more valuable notions of relevance. We also note that cmax is an important",null,null
253,5 CONCLUSION,null,null
254,We presented an approach for employing content-based sources of pseudo relevance for training neural IR models. We demonstrated that our approach can match,null,null
255,,null,null
256,Table 2: Ranking performance using filtered NYT and Wiki. Significant improvements and reductions compared to unfiltered dataset are marked with  and ,null,null
257,,null,null
258,Model PACRR,null,null
259,Conv-KNRM,null,null
260,KNRM,null,null
261,,null,null
262,Training,null,null
263,NYT w/ 2max w/ discriminator,null,null
264,Wiki w/ 2max w/ discriminator,null,null
265,NYT w/ 2max w/ discriminator,null,null
266,Wiki w/ 2max w/ discriminator,null,null
267,NYT w/ 2max w/ discriminator,null,null
268,Wiki w/ 2max w/ discriminator,null,null
269,,null,null
270,kmax,null,null
271,200k 500k,null,null
272,700k 800k,null,null
273,100k 800k,null,null
274,400k 700k,null,null
275,100k 300k,null,null
276,600k 700k,null,null
277,,null,null
278,WebTrack 2012­14,null,null
279,,null,null
280,nDCG@20 ERR@20,null,null
281,,null,null
282,0.2690,null,null
283,0.2716  0.2875,null,null
284,,null,null
285,0.2136 0.2195 0.2273,null,null
286,,null,null
287,0.2613 0.2568 0.2680,null,null
288,,null,null
289,0.2038 0.2074 0.2151,null,null
290,,null,null
291,0.2637  0.2338 0.2697,null,null
292,,null,null
293,0.2031 0.2153 0.1937,null,null
294,,null,null
295,0.2474 0.2609 0.2572,null,null
296,,null,null
297,0.1614  0.1828,null,null
298,0.1753,null,null
299,,null,null
300,0.2220 0.2235 0.2274,null,null
301,,null,null
302,0.1536,null,null
303,0.1828  0.1671,null,null
304,,null,null
305,0.2262  0.2389,null,null
306,0.2366,null,null
307,,null,null
308,0.1635  0.1916,null,null
309,0.1740,null,null
310,,null,null
311,"sources of data. We also showed that performance can be boosted using two filtering techniques: one heuristic-based and one that re-purposes a neural ranker. By using our approach, one can effectively train neural ranking models on new domains without behavioral data and with only limited in-domain data.",null,null
312,,null,null
313,REFERENCES,null,null
314,"[1] Nima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo Test Collections for Learning Web Search Ranking Functions. In SIGIR.",null,null
315,"[2] Leif Azzopardi, Maarten de Rijke, and Krisztian Balog. 2007. Building Simulated",null,null
316,"Queries for Known-item Topics: An Analysis Using Six European Languages. In SIGIR. [3] Richard Berendsen, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke.",null,null
317,"2013. Pseudo Test Collections for Training and Tuning Microblog Rankers. In SIGIR. [4] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search. In WSDM '18. [5] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In SIGIR. [6] Laura Dietz and Ben Gamari. 2017. TREC CAR: A Data Set for Complex Answer",null,null
318,Retrieval.,null,null
319,"[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval. In CIKM '16.",null,null
320,"[8] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. PACRR: A Position-Aware Neural IR Model for Relevance Matching. In EMNLP.",null,null
321,"[9] Bo Li, Ping Cheng, and Le Jia. 2018. Joint Learning from Labeled and Unlabeled Data for Information Retrieval. In COLING '18.",null,null
322,"[10] Sean MacAvaney, Andrew Yates, Arman Cohan, Luca Soldaini, Kai Hui, Nazli",null,null
323,"Goharian, and Ophir Frieder. 2018. Overcoming low-utility facets for complex answer retrieval. Information Retrieval Journal",null,null
324,"Zhang, and Dell Zhang. 2017. IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. In SIGIR. [16] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-End Neural Ad-hoc Ranking with Kernel Pooling. In SIGIR. [17] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the Use of Lucene for Information Retrieval Research. In SIGIR. [18] Hamed Zamani, Mostafa Dehghani, Fernando Diaz, Hang Li, and Nick Craswell. 2018. Workshop on Learning from Limited or Noisy Data for IR. In SIGIR.",null,null
325,,null,null
326,996,null,null
327,,null,null
328,,null,null

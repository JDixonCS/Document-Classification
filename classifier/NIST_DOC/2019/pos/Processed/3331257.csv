,sentence
0,Session 2C: Knowledge and Entities
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations
5,
6,Laura Dietz
7,University of New Hampshire
8,"Durham, NH, USA"
9,dietz@cs.unh.edu
10,
11,ABSTRACT
12,Related work has demonstrated the helpfulness of utilizing information about entities in text retrieval; here we explore the converse: Utilizing information about text in entity retrieval. We model the relevance of Entity-Neighbor-Text
13,We focus on the task of retrieving
14,ACM Reference Format: Laura Dietz. 2019. ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval
15,1 INTRODUCTION
16,"Entity retrieval is important in many different applications where entities are sought in response to a textual description, type definition, or set of related entities. Information needs in natural language, structured SPARQL queries, or hybrids have been explored [6]. Often only a single entity is requested, such as in factoid question answering, conversational retrieval, or quizzes. In contrast, this work1 studies entity retrieval where, in response to a short information need, all topically related entities are to be retrieved. The motivation is to support authors in writing comprehensive articles about"
17,1Code and data available at https://www.cs.unh.edu/~dietz/appendix/ent-rank/
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07...$15.00 https://doi.org/10.1145/3331184.3331257"
19,
20,"topic. As our approach is modeling the context of relevant entities, this work also constitutes a first step towards fully automatic article composition approaches or even a new way to find information rather than documents through search engines [1]."
21,"An example topic is ""Zika fever"". Despite being a short unambiguous keyword query, several facets need to be covered such as ""Signs and Symptoms"", ""Causes"", or ""Epidemiology in Americas"". Several entities must be mentioned for this topic, such as ""Aedes Mosquitoes"" which are the vector for transmission, the ""2015­2016 Zika fever epidemic"" and other outbreaks, that Zika fever is a ""Flavivirus"", it causes muscle weakness due to nerve damage also called ""Guillain-Barré syndrome"", the ""Neonatal infection"" which is the most serious concern, and that ""Dengue Fever"" is a similar disease with confusable symptoms, and that ""Lethal ovitraps"" are used to trap adult Aedes mosquitos. Many, but not all of these relevant entities are mentioned on the Wikipedia page for Zika fever2."
22,"Topical entity retrieval task: Given an article title as query, retrieve a ranking of relevant entities. Relevance is defined based on whether the entity must, should, or could be mentioned in an article on this topic."
23,The entity retrieval task of the TREC Complex Answer Retrieval track [14]
24,The CAR benchmark includes an easily parsable dump of English Wikipedia pages
25,Current entity retrieval approaches focus on the development of relevance features. One set of features is derived from a knowledge graph
26,2 https://en.wikipedia.org/wiki/Zika_fever
27,
28,215
29,
30,Session 2C: Knowledge and Entities
31,"SIGIR '19, July 21­25, 2019, Paris, France"
32,"relations from the same source as equally important for the query. In contrast, this work models the relevance of neighbor relations through textual contexts with different measures of relevance."
33,"Contributions. We introduce the ENT Rank framework for integrating relevance information from the entity, its neighbors, and context. We provide a versatile learning-to-rank-entities algorithm that can be optimized for any rank evaluation metric, such as meanaverage precision. The ENT Rank framework can incorporate any existing entity relevance feature and can be easily customized. We demonstrate that even with simple features derived from unfielded unigram models, such as BM25 and RM3, ENT Rank provides a competitive retrieval method. In a comparison between ENT Rank and established methods on TREC Complex Answer Retrieval [14] and DBpedia-entity v2 [20], ENT Rank places best or second-best."
34,"The idea behind ENT Rank is to use text fragments with entity links, so-called contexts, to define neighbor relations between entities. This allows us to derive a hypergraph, where entities are represented as nodes, and context-neighbor relations are represented as edges. Preserving the association between each context and neighbor relation, allows us to use text-retrieval models to predict the relevance of a neighbor relation for the query. Furthermore, relevance information from context-neighbor relations is used to complement traditional entity relevance features."
35,"Outline. In Section 2 we provide an overview of the state-of-theart on this task. Section 3 introduces the ENT Rank framework and motivates different special cases through random walks. Section 4 discusses the entity, neighbor, and context features used in the experimental evaluation using entity retrieval benchmarks from Complex Answer Retrieval in Section 5 and DBpedia-entity v2 in Section 6."
36,2 RELATED WORK
37,"Entity retrieval was introduced to integrate information retrieval and semantic search [3]. It is often motivated by the large number of named entities mentioned in search requests [28]. Different flavors of this task are to retrieve related entities through a set of entities, type descriptions, or topics of expertise [4, 5, 12]. Entity retrieval can provide answers to questions, such as ""Who invented the paper clip?"" In the context of Complex Answer Retrieval, entity retrieval offers entities that should to be discussed in different parts of the answer [14]."
38,"Entity retrieval from knowledge base documents. Successful approaches to all these variants of entity retrieval center around a representation of each entity as a fielded document. After full-text indexing, entity retrieval can be addressed by traditional retrieval models for ad hoc document retrieval [28]. Tonon et al. combine full text search with structured queries [33]. Balog et al. [2] suggests a term-based and category-based entity representation, where term statistics are derived from documents representing the entity"
39,
40,"SIGIR '19, July 21­25, 2019, Paris, France"
41,Laura Dietz
42,Raviv et al. [29] extend the sequential dependence model
43,"Entity linking tools annotate unstructured text with mentions of entities, providing a new avenue for entity retrieval. Hasibi et al. [18] applies entity linking to queries, to extend the SDM approach with another dependency. Schuhmacher et al. uses entity links in web documents for entity retrieval in a pseudo-relevance feedback approach: Inspecting retrieved web documents, entities are ranked high if they are mentioned in"
44,"Ad hoc document retrieval with entities. By approaching entity retrieval as retrieval of fielded documents, combinations of ad hoc entity retrieval and document retrieval explored. Raviv et al. [30] suggests to represent queries and documents as bag-of-words and bag-of-entity-links for ad hoc document retrieval tasks. Liu et al. [23] rank documents through relevant entities. While the relevance of entities is latent, indicators of entity relevance are derived from entity links and Freebase abstracts. Xiong et al. suggests a discriminative machine learning approach to incorporate different meta information about entities into the document ranking model. Dalton et al. [11] compute an entity-term-category expansion model based on a feedback run of retrieved documents and sources of entity information: entity links in the query, a ranking of Wikipedia pages"
45,"Entity linking. Entity linking methods annotate unstructured text with hyperlink-like positional references to Wikipedia. Fast and reliably entity linking toolkits, such as TagMe and Nordlys [15, 19], are readily available. Entity linking combines spotting of possible entity mentions with the disambiguation among similarly named entities. Several information retrieval approaches to entity linking use the fielded entity representations discussed above [17]. Text surrounding the spot can be cast as a search query for entity retrieval [10]. Special features of short text such as tweets [25] can be incorporated."
46,"Graphs, Relations, and Neighbors. Knowledge graphs contain information about how entities are related through RDF triples with relation types. Alternatively, relations with ""cheap semantics"" [3] can be derived from hyperlinks on Wikipedia, or entities that are mentioned in the same document. Kotov et al. [21] combines both explicit relations available from ConceptNet together with information which entities are mentioned near one another"
47,
48,216
49,
50,Session 2C: Knowledge and Entities
51,ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations
52,
53,"SIGIR '19, July 21­25, 2019, Paris, France"
54,"SIGIR '19, July 21­25, 2019, Paris, France"
55,
56,weakly supervised relation extraction to generate training data for relevant relations.
57,3 ENT-RANK APPROACH
58,"The difficulty of using neighbor relations for entity retrieval lies in the presence of many connections of which the majority are typically not relevant for the query. One example is the entity ""South America"" which is relevant for the query ""Zika fever"" as a location of a major outbreak. However, many contexts about South America are unrelated to the Zika fever, such as political incidents or environmental issues due to the loss of rainforest. In fact, there are so many interesting topics to discuss about South America that there is no room to mention the Zika fever outbreak on South America's Wikipedia page."
59,"We notice an asymmetry of relevance: just because South America is relevant for a discussion of the Zika fever, it does not mean that the Zika fever is equally relevant for a discussion about South America. Therefore, short entity descriptions, such as the introductory Wikipedia paragraph, are often not mentioning relevant connections. We compensate this lack with a text-oriented approach. We hypothesize that whenever two entities are mentioned in a relevant context, it is a strong indicator that both entities are topically relevant. The relevance of the context is predicted through textbased retrieval models. We define the relevance of context-neighbor relations based on the relevance of contexts and entities. We use entity links in context to estimate"
60,"The remainder of this section introduces the construction of the ENT Rank framework. In response to a query,"
61,3.1 ENT Hypergraph and Binary Multi-Graph
62,All Wikipedia pages
63,This approach offers the option of a full-text search index from which hyperedges can be retrieved with different retrieval models such as BM25. We suggest to create the graph from several input rankings of entities e and contexts t. The hypergraph forms the basis for reasoning about the relevance of edges.
64,"Given a search query, the ENT Rank approach formalizes the connections between entities ei , contexts tk , and neighboring entities ej as a binary multi-graph G ="
65,
66,Derived ENT hyper-graph
67,
68,Pages with entity links are split into contexts which induce neighbor relations
69,
70,Legend: Context Entity Neighbors Entity link Owner Identity
71,
72,"Figure 1: ENT Hypergraph is created from contexts with entity links. Example contexts are paragraphs, pages, and sections on Wikipedia pages."
73,relations r =
74,3.2 ENT Feature Vector Graphs
75,We endow nodes ei  V and context-neighbor edges
76,3.3 ENT Learning-to-Rank-Entities Model
77,"The major challenge in using the ENT Rank model for entity ranking is the vast amount of heterogeneous feature choices: Offering multiple sources for contexts, different neighbor roles, different entity"
78,
79,217
80,
81,Session 2C: Knowledge and Entities
82,"SIGIR '19, July 21­25, 2019, Paris, France"
83,
84,Input contexts t1
85,e1 e2 e3
86,
87,t2
88,e2 e3
89,
90,Derived ENT multi-graph
91,
92,e2
93,
94,e1
95,
96,e3
97,
98,"SIGIR '19, July 21­25, 2019, Paris, France"
99,Laura Dietz
100,
101,Legend:
102,
103,Entity mentions in context
104,
105,Node associated with entity ei
106,
107,ei
108,
109,Neighbor relation
110,
111,"Figure 2: The binarized ENT-multi graph is derived from contexts, where each context t1 and t2 induces a"
112,"contexts, here e2 and e3."
113,
114,"weighted combination of these choices, given sufficient training data."
115,The entity ranking is derived from the ENT feature vector graph
116,G using a learning-to-rank model with weight parameters ì and
117,"ì. As customary in learning-to-rank, the weight parameters are trained across many queries; dependence on the query is expressed through the features. We first discuss the prediction of a ranking, second how to train the weight parameters, and finally give a motivation that is based on random walks. We define features gì of an entity pair as,"
118,
119,
120,
121,"gì(ei , ej ) ="
122,
123,"fì(ei,tk,ej )"
124,
125,(1)
126,
127,"k :ei,ej tk"
128,
129,Ranking prediction. Given trained node and multi-edge parame-
130,ters ì and ì and a query-specific feature vector graph G. We define the rank score of an entity
131,
132,score(ej )
133,
134,=
135,
136,ì
137,
138,fìej
139,
140,1 +
141,|V | )(
142,
143,"ì gì(ei , ej )"
144,i
145,
146,)
147,
148,(2)
149,
150,=
151,
152,ì ì
153,
154,1 |V
155,
156,|
157,
158,"ifìegìj(ei , ej )"
159,
160,Here |V | is the number of nodes in the graph. The second line follows after rearranging inner vector products and stacking weight
161,parameters ì and ì into a single weight parameter vector which
162,"contains all entries in ì followed by all entries in ì. Likewise, node and multi-edge feature vectors are stacked, after summing vectors across all multi-edges"
163,
164,"Training. The weight parameters are trained to achieve optimal entity ranking performance on the training set. In this work, we use mini-batched coordinate-ascent as a training algorithm, but other training algorithms are equally applicable. Coordinate-ascent is an iterative algorithm that optimizes the weight of one feature at at time in a round-robin fashion until no further improvement in ranking performance can be achieved. The ranking performance is evaluated with mean-average precision"
165,
166,"iteration on a different random subset of 150 training queries. The algorithm is stopped when the relative change in MAP is less than 1%. This convergence is usually achieved within 5 iterations, since our features are all positively correlated with relevance."
167,
168,Motivation. The ENT learning-to-rank-entities model is inspired
169,
170,"by random walks with restarts [32], where P(ej ) is the probability of chosing node ej during restart. Due to space constraints, this work only discusses the simple case of weighted degree central-"
171,
172,"ity, i.e., random walks with only one step, for which an analytic"
173,
174,solution to the optimization problem is available.
175,
176,Nodes are initialized uniformly at random
177,
178,1 |V
179,
180,|
181,
182,).
183,
184,The
185,
186,transi-
187,
188,tion from node ei to ej is given by the transition probability P(ei
189,
190,ej |ei ) given that the random surfer is on the start node ei . Using
191,
192,teleportation probability  
193,
194,"noted as matrix T, where"
195,
196,Tij = P(ej ) +
197,
198,Under degree centrality
199,
200,of the receiving node ej is
201,
202,score(ej ) =
203,
204,1 |V |
205,
206,Ti j
207,i
208,
209,=
210,
211,1 P(ej )+(1- ) |V |
212,
213,( P
214,i
215,
216,)  ej |ei )
217,
218,"The fraction of |V | vanishes from the first term, when the teleport"
219,
220,is summed over all sending nodes.
221,
222,For learning-to-rank-entities we model the restart probabilities
223,
224,and transition probabilities as linear models of node feature vec-
225,
226,tors ìf and edge feature vectors gì. The ratio of teleportation versus
227,
228,otrfatnhseittiroanin( i1n-g
229,
230,) is absorbed into parameters and estimated as part process. Since only a rank-equivalent rank score is
231,
232,"necessary, we let"
233,
234,P
235,
236,(3)
237,
238,(1 -  )P
239,
240,(4)
241,
242,score(ej )
243,
244,ra=nk ììfej
245,
246,+
247,
248,1 |V |
249,
250,"ì gì(ei , ej )"
251,i
252,
253,(5)
254,
255,"These are combined into Equation 5. Thereby, we arrive at the formulation which is given in Equation 2."
256,
257,218
258,
259,Session 2C: Knowledge and Entities
260,
261,"SIGIR '19, July 21­25, 2019, Paris, France"
262,
263,ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations
264,
265,"SIGIR '19, July 21­25, 2019, Paris, France"
266,
267,3.4 Options for Multi-edge Feature Vectors
268,"We envision features ìf and gì in the ENT Rank feature graph G to be tailored to the application domain. Before providing details on the set of features used in this work, we discuss how we envision information about contexts, neighbors, and relation types to be integrated into the ENT Rank framework. Our suggestions are based on probabilistic random walks."
269,
270,"3.4.1 Neighbor features. When entity features of the sending neighbor ei are available, the feature vector of the multi-edge"
271,"fì(ei,tk,ej ) = ìf(ei ) Following Equation 2, this results in a rank score for ej that is"
272,
273,score(ej )
274,
275,ra=nk ììfej
276,
277,+
278,
279,1 |V |
280,
281,
282,"i k :ei,ej tk"
283,
284,ììfei
285,
286,"where ì and ì control the importance of different neighbor features versus entity features. This formulation naturally incorporates the multiplicity of multi-edges between ei and ej . In the context of semi-supervised classification, this model is also known as linear neighborhood propagation [34]."
287,
288,"3.4.2 Relation-typed neighbor features. As mentioned earlier, different entities can play different roles in the context, such as being the owner of the context versus being mentioned in the context. These roles can define a type of the neighbor relation"
289,0
290,...
291,"0 ìf(ej , r ) = ìf(ej )  block for relation type r"
292,0
293,...
294,0
295,
296,"If across multiple contexts, entity ej and ei have different roles, we suggest to copy the entity feature vector of neighbor ei into all corresponding relation type blocks. The consequence is that the training algorithm would then not only learn to balance entity features of ej versus neighbor ei , but also assign different importance weights depending on neighbor relation type and context type."
297,
298,"3.4.3 Context-Relevance Features. When contexts are retrieved from a full-text index, contexts tk are naturally associated with features from text retrieval models, such as the retrieval score of the context under a BM25 model or the RM3 expansion models with different hyperparameters. Each of these retrieval models would contribute a separate relevance feature for the context tk"
299,
300,We incorporate the case of context feature vectors based on re-
301,
302,lated work [37] on random walks for hypergraphs. Zhou et al. sug-
303,
304,gest the following random surfer process: A random surfer on node
305,
306,ei first surfs to an adjacent hyper edge tk proportionally to its edge weight
307,
308,"possibility of surfing back to the starting node ei . Under this model, the transition probability from node ei to ej"
309,
310,"vnisieagcTtithvkeieisinstcopboryerojrPp,eot(shepriteoio nmndaaserlgjtto|ioeniEa)|tlq1ktu|raatn(itoskkint):i.o4(Wneiinp,htrekoon,ubemjra)bfuie|ltl1aitktityp|ulrfe(retohkgmy)rp.aneporheddefgoeerismtcouolenaj--"
311,
312,tion where the feature vector for transition from ei to ej can be expressed as features of connecting hyperedges tk :
313,
314,"ìf(ei , ej ) ="
315,"k :ei,ej tk"
316,
317,1 |tk
318,
319,|
320,
321,ìf(tk
322,
323,)
324,
325,"Here |tk | denotes the number of entities mentioned in the context. In relation to Equation 1, it follows that"
326,
327,"fì(ei,tk,ej )"
328,
329,=
330,
331,1 |tk
332,
333,|
334,
335,ìf(tk
336,
337,)
338,
339,(6)
340,
341,Our experiments empirically confirm that dividing hyperedge feature vectors by the number of neighbors provides slightly better
342,"results than the unnormalized alternative, fì(ei,tk,ej ) = ìf(tk )."
343,
344,3.4.4 Combinations of Multi-edge Features Vectors. We envision
345,"that multi-edge feature vectors ìf are composed of both relationtyped neighbor features, context features, and many other feature sources by stacking feature vectors into one combined feature vector,"
346,
347,"fì(ei,tk,ej ) ="
348,
349,"ìf(ej , r )"
350,
351,1 |tk
352,
353,|
354,
355,ìf(tk
356,
357,)
358,
359,...
360,
361,We use this representation to construct the edge feature vector for ENT Learning-to-rank-entities for use in Equation 1.
362,
363,4 WIKIPEDIA FEATURES FOR ENT RANK
364,"In this study we use the following set of retrieval-based features for entities feature vectors ìf(ej ) and multi-edge feature vectors fì(ei , tk , ej ) as described in Section 3.4.4. The features used in the evaluation are derived from a 2016 Wikipedia dump and a corpus of paragraphs"
365,"From the Wikipedia dump and a text corpus we extract the following types of information which are used as a source of contexts and/or entity relevance, from which we derive ENT feature vector graphs."
366,"· Page: Full-text of Wikipedia pages, including all visible text including title, headings, and content paragraphs. For the graph only bi-directional entity links are included as neighbors"
367,"· Entity: Knowledge graph representation of entities using only head information such as title, lead text, and name variations"
368,
369,219
370,
371,Session 2C: Knowledge and Entities
372,"SIGIR '19, July 21­25, 2019, Paris, France"
373,
374,"derived from anchor text of incoming links, redirects, and disambiguations. This is the typical representation commonly used by entity linking methods such as TagMe [15]. The graph structure is derived only from bi-directional entity links. · Section: Sections"
375,"In this work use the TREC CAR benchmark. We derive page, entity, and section from the allButBenchmark data"
376,
377,4.1 Entity and Context-Relevance Features
378,
379,"For each representation of page, entity, section, paragraph, we create an full-text search index with a single text field. Using this index and the keyword query"
380,
381,· BM25: The Lucene-BM25 model with default parameters without expansion.
382,· BM25-RM: A BM25 ranking with RM3-style query expansion on a BM25 feedback run.
383,· QL: The Querylikelihood model with Dirichlet smoothing
384,· QL-RM: QL ranking with RM3-style query expansion on a QL feedback run.
385,
386,We use a fixed interpolation for RM variations for input runs: query
387,
388,terms weighted by 1.0; expansion terms weighted by expansion
389,
390,probability. We learn a refined interpolation between QL and QL-
391,
392,RM as part of an larger learning-to-rank-entities model.
393,
394,Drawing inspiration from the entity context model described by
395,
396,"Dalton et al. [11], we further include the following entity-expansion"
397,
398,model: We represent a pseudo-relevance feedback run of contexts
399,
400,"d as a bags-of-entities e. Using entity links instead of words, the"
401,
402,relevance model [22] is used to compute expansion entities as in
403,
404,Equation 7.
405,
406,
407,
408,pEcmX(e |q) = p(d |q)p(e |d)
409,
410,(7)
411,
412,d
413,
414,We use entity-expansion model in two variations:
415,
416,· EcmX: A ranking of expansion entities ranked by their expansion probability p(e |q).
417,· EcmPsg: Expanding BM25 or QL with top 20 expansion entities under p(e |q) to retrieve a new ranking of contexts via an RM3-like combination of query term matches in text field and expansion entity matches in the entity link field.
418,
419,"When multiple rankings are to be combined, an effective alterna-"
420,
421,tive to learning to rank is unsupervised rank aggregation. All dis-
422,
423,tinct rank
424,
425,items score
426,
427,d across all rankings R from reciprocal ranks
428,
429,are
430,R
431,
432,assigned a new aggregated
433,
434,1 rank(d
435,
436,)
437,
438,.
439,
440,We
441,
442,include
443,
444,aggre-
445,
446,gated rank features for entities and each context type:
447,
448,"SIGIR '19, July 21­25, 2019, Paris, France"
449,Laura Dietz
450,· Entity feature Aggr: Rank aggregation across all entity rankings
451,"· For each context type, Aggr: Rank aggregation across all context rankings of this type"
452,Feature vectors are derived from all of these rankings:
453,"· Entity relevance: Features ìf are derived from rank scores. We use BM25, QL, BM25-RM, QL-RM, BM25-EcmPsg and QL-EcmPsg scores when retrieving from page and entity indexes in addition to the scores of the EcmX model on all representations."
454,"· Context relevance: Features gì use rank scores of BM25, QL, BM25-RM, QL-RM, BM25-EcmPsg, and QL-EcmPsg retrieval from the context representation"
455,"The ENT Hypergraph is created from the top 1000 of all entity rankings and context rankings. As we use retrieval models that only assign positive retrieval scores, missing features are set to zero. Finally, Z-score normalization is applied."
456,4.2 Relation-typed Neighbor Features
457,We include neighbor features as described in Section 3.4.1 based on entity features described above. The relation type is based on the context-type
458,· Link-Link: when both entities are mentioned in the same context
459,· Owner-Link: when entity ej is linked in a context owned by entity ei
460,· Owner-Self: modelling loops of an entity with itself through the context.
461,"Owner roles are not available for paragraph contexts, as these are derived from the paragraphCorpus of the CAR data set."
462,5 EVALUATION ON COMPLEX ANSWER RETRIEVAL
463,The TREC Complex Answer Retrieval track
464,The CAR dataset [13] comes with a large collection of about 5.41 million
465,"3Resource ""unprocessedAllButBenchmark"", available at http://trec-car.cs.unh.edu"
466,
467,220
468,
469,Session 2C: Knowledge and Entities
470,
471,"SIGIR '19, July 21­25, 2019, Paris, France"
472,
473,ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations
474,
475,"SIGIR '19, July 21­25, 2019, Paris, France"
476,
477,CAR queries are hierarchical page outlines that consist of a page title and headings. These outlines are to be populated with passages from a paragraph corpus and/or entities from a provided Wikipedia dump
478,"· BenchmarkY1train-auto: 117 title queries, 1,816 title-heading queries, and 13,031 automatic entity assessments."
479,"· BenchmarkY2test-manual: 271 title-heading queries, and 8,415 manual entity assessments."
480,"· BenchmarkY2test-auto: 976 title-heading queries and 17,044 automatic entity assessments."
481,"List of experiments. While the goal of this paper is to retrieve entities in response to short title queries, we evaluate our ENT Rank model both on title queries and title-heading queries for which official baselines are available. Following the track guidelines, we always train on the benchmarkY1train queries. In the page-level experiment we train/test on title queries from benchmarkY1trainauto using 5-fold cross validation. To compare to the state-of-theart in CAR, we conduct a section-level experiment trained titleheading queries and qrels from benchmarkY1train-auto and evaluated on benchmarkY2test-manual and benchmarkY2test-auto. The keyword query in the section-level experiment is formed by concatenating the title, the heading, and parent headings of the section. We complement the experiments with a study of the running example ""Zika fever"", before continuing with experiments on DBpedia-Entity in Section 6."
482,We evaluate resulting entity rankings by metrics R-Precision
483,"Experimental Setup. To carry out these experiments, full-text indexes and rankings were created with Lucene 7, using the English analyzer for tokenization of text and whitespace tokenization for entity ids."
484,We apply our mini-batched coordinate ascent learning-to-rankentities algorithm
485,4 http://trec- car.cs.unh.edu/datareleases/v2.1/
486,
487,Table 1: Page-level results on benchmarkY1train title
488,queries measured in MAP. Comparison of feature subsets
489,"and context types: paragraph, section, and page. Significantly higheror lowerthan AllExp"
490,paired-t-test.
491,
492,Run
493,AllExp JustAggr
494,No Entity No Neighbor No Context
495,Only Entity ExpEcm No Expansion
496,
497,Paragraph
498,0.311 0.274
499,0.280 0.318 0.299
500,0.287 0.226 0.227
501,
502,Section
503,0.291 0.158
504,0.156 0.278 0.280
505,0.287 0.220 0.148
506,
507,Page
508,0.287 0.211
509,0.235 0.274 0.275
510,0.282 0.260 0.113
511,
512,"Next, feature vectors ìf for nodes and gì for binarized edges are constructed using retrieval models as detailed in Section 4. We apply Z-score normalization to all feature vectors during training, which is inverted to obtain graph visualizations."
513,5.1 Page-level Experiment on TREC CAR
514,We study the advantage of the ENT learning-to-rank-entities model with respect to the features described in Section 4. We analyze the retrieval performance achieved on the following subsets of features described:
515,(1) AllExp: Use all described features.
516,with unsupervised rank aggregation. The relative weight between different context rankings and entity features needs to be trained.
517,"For comparison, the strongest input entity retrieval feature is QL EcmX on the Paragraph index, with MAP of 0.21. Inspecting the trained model parameters, we find that this feature also receives one of the highest weights among EcmX entity features. One of the strongest edge features is QL without expansion. This is interesting, because the EcmX entity feature is equivalent to using QL edge features in the ENT Rank framework with Owner-Self roles. This can be seen when Equation 6 is inserted into Equations 1 and 2. Of course, ENT Rank is a more flexible and powerful model as will be demontrated in the evaluation on the DBpedia v2 dataset in Section 6."
518,Table 1 displays the ranking performance in MAP across different feature subsets and context types. The best performance is achieved for paragraph contexts with all features included. Neither
519,
520,221
521,
522,Session 2C: Knowledge and Entities
523,"SIGIR '19, July 21­25, 2019, Paris, France"
524,"Figure 3: The 2-hop neighbor relation graph for example query ""Zika fever"" and entity South America. Edge weights are predicted with the ENT Rank model using paragraph contexts. The graph was not manually cleaned."
525,"section nor page contexts are not significantly improving over entity features alone. We conclude that large contexts, such as pages, are not effective to model neighbor relations. Generally the inclusion of more features, as in AllExp, does not hurt. For paragraph contexts, the lower values in No Entity, No Neighbor, and No Context demonstrate that all components of the ENT Rank model provide value. Lower score of JustAggr demonstrates the benefits of machine learning."
526,"The best variant of ENT Rank, AllExp on paragraph contexts achieves Rprec of 0.356 and ndcg@100 of 0.674."
527,"While excluded for brevity, similar results are also obtained for title-queries in benchmarkY1test and benchmarkY2test."
528,5.2 Case Study: Zika fever
529,"We demonstrate the algorithm by analyzing the results for our motivating example query ""Zika fever""."
530,"Figure 3 displays the 2-hop neighborhood relation graph for the example query entity ""Zika virus"" and entity South America. Edge weights are predicted with the ENT Rank model on paragraph contexts with the AllExp subset. The resulting graph includes many topical connections between South America and Infection, World Health Organization, and Mosquito."
531,We want to remark that the knowledge base provided with TREC CAR v2.1 does not include the page Zika fever
532,
533,"SIGIR '19, July 21­25, 2019, Paris, France"
534,Laura Dietz
535,"Table 2: Rank at which the target entity South America is found for example query ""Zika fever"". As the query terms are not mentioned on the target's Wikipedia page, it is not in runs with RM, EcmPsg, or no expansion."
536,
537,Input ranking
538,
539,rank
540,
541,Paragraph BM25 EcmX 55
542,
543,Page BM25 EcmX
544,
545,102
546,
547,Section BM25 EcmX
548,
549,119
550,
551,Entity BM25 EcmX
552,
553,146
554,
555,ENT Rank rank
556,
557,ExpEcmX
558,
559,49
560,
561,Only Entity 66
562,
563,AllExp
564,
565,86
566,
567,JustAggr
568,
569,109
570,
571,"does not mention the connection to Zika fever. Therefore, all connections in the ENT graph in Figure 3 are identified through contexts, neighbor relations, and EcmX features."
572,Table 2 displays the rank at which South America can be found in different input rankings
573,5.3 Section-level Experiment on TREC CAR
574,"To compare ENT Rank to baseline systems from the TREC CAR challenge, we train ENT Rank models on title-heading queries from benchmarkY1train, then predict entities for benchmarkY2test outlines. Table 3 compares the two best ENT Rank variants with the two best entity retrieval systems from TREC CAR, ""UNH-e-L2R"" and ""UNH-e-mixed"".5 ENT Rank either outperforms or is equivalent to the CAR baseline systems. We want to remark, that ENT Rank runs did not contribute to the pool for manual assessments, giving baseline systems a slight advantage."
575,6 EVALUATION ON DBPEDIA-ENTITY V2
576,"We further evaluate our approach on several established entity retrieval datasets, provided in the DBpedia-Entity v2 benchmark [20]. The benchmark includes the following categories of queries with updated relevance judgments using a pool of methods:"
577,"SemSearch ES are short and ambiguous named entity queries. 113 queries such as ""brooklyn bridge""."
578,"INEX-LD are IR-style keyword queries for linked data. 99 queries such as ""electronic music genres""."
579,"List Search contain list search queries. 115 queries such as ""Professional sports teams in Philadelphia""."
580,"QALD-2 is comprised of questions for linked data. 140 queries such as ""Who is the mayor of Berlin?"". Question-specific stopwords were removed by Hasibi et al."
581,The benchmark is designed for the English part of DBpedia from October 2015. As our algorithm makes heavy use of the Wikipedia article structure
582,5Available at http://trec-car.cs.unh.edu/results/trec-car-y2-appendix.html
583,
584,222
585,
586,Session 2C: Knowledge and Entities
587,
588,"SIGIR '19, July 21­25, 2019, Paris, France"
589,
590,ENT Rank: Retrieving Entities for Topical Information Needs through Entity-Neighbor-Text Relations
591,
592,"SIGIR '19, July 21­25, 2019, Paris, France"
593,
594,Table 3: Comparison of section-level retrieval on TREC CAR benchmarkY2test between best performing ENT Rank variants in comparison to two best entity retrieval baseline systems. Significantly higher or lower according to 5% paired-t-test.
595,
596,CAR Rank 1: UNH-e-L2R CAR Rank 2: UNH-e-mixed ENT Rank AllExp ENT Rank ExpEcm ENT Rank JustAggr ENT Rank No Expansion
597,
598,MAP 0.146
599,0.142 0.136 0.156 0.161 0.152
600,
601,Automatic
602,Rprec ndcg@10 0.181 0.258 0.175 0.275 0.161 0.239 0.180 0.254 0.186 0.270
603,0.179 0.255
604,
605,ndcg@100 0.316 0.298 0.391 0.427 0.428 0.416
606,
607,MAP 0.310 0.260 0.276
608,0.307
609,0.322 0.323
610,
611,Manual
612,Rprec ndcg@10 0.315 0.453 0.278 0.386 0.275 0.395 0.304 0.443
613,0.312 0.443
614,0.317 0.448
615,
616,ndcg@100 0.514 0.435 0.538 0.578 0.592 0.590
617,
618,"from the ranking. The benchmark also provides contributed baseline runs. These were also projected onto the 2016 Wikipedia dump, and likewise unjudged entities were removed to obtain a fair comparison"
619,"Datasets are merged for training with 5-fold cross validation. Table 4 displays the result of our suggested ENT Rank model in comparison to the best-performing baselines BM25F-CA and FSDMELR. With the exception of the SemSearch ES subset, our ENT Rank method outperforms all twelve baseline systems. ENT Rank especially improves on recall-oriented measures MAP and ndcg@100."
620,"Inspecting the feature weights reveal that--in comparison to complex answer retrieval­these datasets require that weight is placed on entity features. Restricting the features to the ExpEcmX subset does drastically hurt the performance. In contrast, limiting features to only access un-expanded BM25 and QL runs, obtains relatively good results. When entity features are removed, ENT rank increases the weight of neighbor features, thereby practically recovering a retrieval performance of 0.671 ndcg@100."
621,7 CONCLUSION
622,"We propose ENT Rank, a framework for modeling entity-neighbortext relations for entity retrieval. While ENT Rank can incorporate a wide range of context, neighbor, and entity features, here we focus on features that are derived from traditional text retrieval methods, such as BM25, and neighbor relations that are based on co-occuring entity links. We explore different sizes of contexts and find that paragraph-sized contexts work best."
623,"The approach is evaluated through several experiments on the TREC Complex Answer Retrieval and DBpedia-Entity v2 benchmarks which include title-heading queries, semantic search queries, and question answering queries. ENT Rank is consistently the best or second-best method among a set of eleven baseline systems that participated in TREC CAR and twelve systems from DBpediaEntity. A case study on the running example ""Zika fever"" demonstrates the ability to detect relevant entities even when their relevance cannot be concluded from their Wikipedia page alone."
624,"In future, we would like to use ENT Rank to not only rank entities, but also to provide a useful order among entities and support them with text. Such a system could support both human article authors and automated conversational agents with background knowledge. One day, such systems might respond to web search requests with automatically written Wikipedia articles that do not exist yet."
625,
626,Table 4: Results of ENT Rank on the DBpedia-Entity v2
627,dataset. Baselines BM25F-CA and FSDM+ELR [20]. Significantly higheror lowerthan BM25F-CA
628,ing to 5% paired-t-test.
629,
630,All
631,BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr
632,
633,MAP
634,0.454 0.440 0.465 0.476
635,
636,Rprec
637,0.433 0.416 0.430 0.438
638,
639,ndcg@100
640,0.680 0.663 0.702 0.711
641,
642,ndcg@10
643,0.545 0.537 0.536 0.544
644,
645,SemSearch_ES
646,BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr
647,
648,0.606 0.620 0.601 0.590
649,
650,0.549 0.550 0.532 0.506
651,
652,0.782 0.791 0.783 0.779
653,
654,0.671 0.694 0.666 0.658
655,
656,ListSearch
657,BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr
658,
659,0.441 0.422 0.478 0.493
660,
661,0.427 0.404 0.450 0.471
662,
663,0.689 0.665 0.733 0.744
664,
665,0.550 0.533 0.542 0.549
666,
667,INEX_LD
668,BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr ENT Rank Only Entity
669,
670,0.420 0.399 0.437 0.439 0.443
671,
672,0.414 0.395 0.412 0.422 0.425
673,
674,0.666 0.645 0.693 0.696 0.702
675,
676,0.525 0.511 0.520 0.519 0.532
677,
678,QALD2
679,BM25F-CA FSDM-ELR ENT Rank AllExp ENT Rank JustAggr
680,
681,0.366 0.339 0.366 0.396
682,
683,0.359 0.332 0.346 0.366
684,
685,0.600 0.572
686,0.618 0.639
687,
688,0.455 0.432 0.439 0.465
689,
690,Acknowledgements
691,"This material is based upon work supported by the National Science Foundation under Grant No. 1846017. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
692,
693,223
694,
695,Session 2C: Knowledge and Entities
696,"SIGIR '19, July 21­25, 2019, Paris, France"
697,REFERENCES
698,"[1] James Allan, Bruce Croft, Alistair Moffat, and Mark Sanderson. 2012. Frontiers, challenges, and opportunities for information retrieval: Report from SWIRL 2012 the second strategic workshop on information retrieval in Lorne. In ACM SIGIR Forum, Vol. 46. ACM, 2­32."
699,"[2] Krisztian Balog, Marc Bron, and Maarten De Rijke. 2011. Query modeling for entity search based on terms, categories, and examples. ACM Transactions on Information Systems"
700,"[3] Krisztian Balog, Edgar Meij, and Maarten De Rijke. 2010. Entity search: building bridges between two worlds. In Proceedings of the 3rd International Semantic Search Workshop. 9."
701,"[4] Krisztian Balog, Pavel Serdyuko, and Arjen de Vries. 2011. A neighborhood relevance model for entity linking. In TREC."
702,"[5] Krisztian Balog, Pavel Serdyuko, Arjen de Vries, Paul Thomas, and Thijs Westerveld. 2009. Overview of the TREC 2009 Entity Track. In TREC."
703,"[6] Holger Bast, Alexandru Chitea, Fabian Suchanek, and Ingmar Weber. 2007. Ester: efficient search on text, entities, and relations. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 671­678."
704,[7] Hannah Bast and Elmar Haussmann. 2015. More accurate question answering on freebase. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. 1431­1440.
705,"[8] Michael Bendersky, Donald Metzler, and W Bruce Croft. 2010. Learning concept importance using a weighted dependence model. In Proceedings of the third ACM international conference on Web search and data mining. ACM, 31­40."
706,"[9] Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An empirical study of learning to rank for entity search. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 737­740."
707,[10] Jeffrey Dalton and Laura Dietz. 2013. A Neighborhood Relevance Model for Entity Linking. In Proceedings of the 10th Conference on Open Research Areas in Information Retrieval. 149­156.
708,"[11] Jeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity Query Feature Expansion Using Knowledge Base Links. In SIGIR."
709,"[12] Gianluca Demartini, Tereza Iofciu, and Arjen P De Vries. 2009. Overview of the INEX 2009 entity ranking track. In International Workshop of the Initiative for the Evaluation of XML Retrieval. Springer, 254­264."
710,[13] Laura Dietz and Ben Gamari. 2018. TREC CAR 2.0: A Data Set for Complex Answer Retrieval. http://trec-car.cs.unh.edu. Version 2.0.
711,"[14] Laura Dietz, Ben Gamari, Jeff Dalton, and Nick Craswell. 2018. TREC Complex Answer Retrieval Overview. TREC."
712,[15] Paolo Ferragina and Ugo Scaiella. 2010. Tagme: on-the-fly annotation of short text fragments
713,[16] Darío Garigliotti and Krisztian Balog. 2017. On Type-Aware Entity Retrieval. In SIGIR. 27­34.
714,"[17] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2015. Entity Linking in Queries: Tasks and Evaluation. In ICTIR. 171­180."
715,"[18] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2016. Exploiting Entity Linking in Queries for Entity Retrieval. In ICTIR. 209­218."
716,"[19] Faegheh Hasibi, Krisztian Balog, Darío Garigliotti, and Shuo Zhang. 2017. Nordlys: A toolkit for entity-oriented and semantic search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1289­1292."
717,"[20] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. DBpedia-entity v2: a test"
718,
719,"SIGIR '19, July 21­25, 2019, Paris, France"
720,Laura Dietz
721,"collection for entity search. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1265­ 1268. [21] Alexander Kotov and ChengXiang Zhai. 2012. Tapping into knowledge base for concept feedback: Leveraging ConceptNet to improve search results for difficult queries. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining"
722,
723,224
724,
725,

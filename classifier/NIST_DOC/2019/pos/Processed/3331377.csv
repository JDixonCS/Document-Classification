,sentence
0,Short Research Papers 3C: Search
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,LIRME: Locally Interpretable Ranking Model Explanation
5,
6,Manisha Verma
7,"Verizon Media, New York, USA manishav@verizonmedia.com"
8,ABSTRACT
9,Information retrieval
10,CCS CONCEPTS
11,· Information systems  Information retrieval; Content analysis and feature selection; Retrieval models and ranking;
12,KEYWORDS
13,"Interpretability, Ranking, Point-wise explanations"
14,"ACM Reference format: Manisha Verma and Debasis Ganguly. 2019. LIRME: Locally Interpretable Ranking Model Explanation. In Proceedings of Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, Paris, France, July 21­25, 2019"
15,1 INTRODUCTION
16,It has been shown that complex machine learning
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn"
18,
19,Debasis Ganguly
20,"IBM Research, Dublin, Ireland debasis.ganguly1@ie.ibm.com"
21,"features [3, 10], or a weighted distribution of feature importance [6, 9]. While the explanation space itself and methods to generate explanations are widely known in practise for classification tasks, their utility is largely unexplored for ranking tasks. There is little existing work in the IR community to systematically investigate ways of generating explanations for an IR model. Given that IR models involve complex variations in term weighting functions for scoring query-document pairs, some models may not be easy to `explain' to a search engine user, who may have questions such as `Why does a search engine retrieve document D at rank k?'."
22,"In this work, with the motivation of `explanations' in IR, we explore ways of generating and evaluating explanations. We focus on model-agnostic point-wise explanations, i.e. estimating `explanation vectors' with respect to a retrieval model without any knowledge about its internals. The weight of each token in the term importance vector indicates its contribution to a ranking model's output for a given query-document pair. This vector can then be analyzed to see the relative importance of terms contributing positively"
23,To estimate the explanation vector for a query-document pair
24,"We propose two metrics for evaluating the stability and correctness of the generated explanations. The first metric evaluates the sensitivity of the explanation model, i.e. the scale of change in explanations with change in model parameters. The second evaluation metric is more specific to IR, in which we evaluate the effectiveness of the explanations in terms of document relevance. Our experiments on the TREC ad hoc dataset indicate that sampling methods that are biased with tf-idf or positional information produce weaker explanations than those generated by uniformly sampling words from documents. We also found that explanation stability decreases with an increase in the number of explanation words; and that this effect is more pronounced for non-relevant documents."
25,
26,1281
27,
28,Short Research Papers 3C: Search
29,
30,"SIGIR '19, July 21­25, 2019, Paris, France"
31,
32,2 RELATED WORK
33,"Singh et. al. [12] investigate methods to train an explanation model for a given base ranker. The document rankings generated from the explanation model are then compared to those generated by the base model. Contrastingly, in our work, the focus is to evaluate different sampling strategies and automatically evaluate the consistency and effectiveness of explanation models. Specifically, our work explores methods to generate data for explanation model training and evaluate its effectiveness across queries. The metrics and sampling methods explored in this work can be adapted easily to train and evaluate new explanation models such as those proposed in [11, 12]. We do not investigate ways of transforming document scores into class probabilities [13], as explanation models can be trained directly on `scores' assigned by any ranker"
34,3 LOCAL EXPLANATION OF IR MODELS
35,"In classification tasks, model predictions can be understood by analyzing the predictions of simpler"
36,"Since the working principle of a ranking task is different from that of classification, in this work we investigate different ways of generating model-agnostic interpretable explanations for ranked lists. Formally, given a set of documents D and a query Q, the ranking function S(D, Q) induces a total order on the set D. For traditional IR models, such as BM25 or language models"
37,"To generate explanations, it is required to select set of simple instances or sub-instances, where each sub-instance is comprised of partial information extracted from a particular document. We employ a weighted squared loss to predict the score of the entire document D with respect to the input query. We call this method locally interpretable ranking model explanation"
38,
39,M
40,"L(D, Q,  ; ) ="
41,
42,i =1
43,
44,M
45,
46,p
47,
48,=
49,
50,i =1
51,
52,j =1
53,
54,(1)
55,
56,"In Equation 1, Di = i"
57,
58,document D comprised of p unique terms;  is an L1 regularization
59,
60,term; and   Rp denotes a vector of p real-valued parameters used
61,
62,"to approximate the score query Q. Additionally, the"
63,
64,of the sub-sample weight of the loss
65,
66,"D(iD w, Ditih),riessapseicmt itloartihtye"
67,
68,between the document D and its sub-sample Di. A standard way to define  in Equation 1 is with a kernel function of the form
69,
70,"(D, D)"
71,
72,=
73,
74,exp(-
75,
76,x2 h
77,
78,"),"
79,
80,x
81,
82,"= arccos(D, D)"
83,
84,(2)
85,
86,"where arccos(D, D) denotes the cosine-distance"
87,
88,"document D and a sub-document sampled from it, and h denotes"
89,
90,the width of a Gaussian kernel.
91,
92,"The weighted loss function of Equation 1 predicts S(D, Q) using"
93,
94,the given samples. Since a retrieval model computes the score of an
95,
96,"entire document and also the scores of its sub-samples, the predicted vector ^  Rp estimates the importance of each term, e.g. the jth component of ^ denotes the likelihood of term tj in contributing positively to the overall score S(D, Q)."
97,It is expected that weights in ^ that correspond to a query term
98,
99,will have larger weights
100,
101,semantically related to the query and hence are likely to be relevant
102,
103,to its underlying information need. A visualization of these terms
104,
105,may then provide the desired explanation of an observed score of a
106,
107,document D with respect to Q
108,
109,3.1 Sampling of Explanation Instances
110,
111,We now describe three different ways to define the sampling func-
112,
113,tion 
114,
115,"neighbourhood of D for the purpose of predicting the parameter vector, ^ , to explain a retrieval model."
116,
117,Uniform Sampling:. A simple way to sample from the neigh-
118,
119,bourhood of an given document D is to sample terms with a uniform
120,
121,likelihood
122,
123,towards term selection leading to likely generation of a diverse set
124,
125,of samples for a document.
126,
127,Biased Sampling:. Another way to sample terms is to set the
128,
129,sampling probability of a term proportional to its tf-idf weight
130,
131,seeking to generate sub-samples with informative terms.
132,
133,Masked Sampling:. In contrast to a bag-of-words based sam-
134,
135,"pling approach, an alternative way is to extract segments of text"
136,
137,"from a document, somewhat analogous to selecting regions from"
138,
139,"an image [9]. More specifically, in this sampling method we first"
140,
141,"specify a segment size, say k, and then segment a document D"
142,
143,prised of
144,
145,|D|
146,
147,tokens) into
148,
149,|D | k
150,
151,number of chunks. A
152,
153,chunk is then
154,
155,made visible in the sub-sample with probability v
156,
157,4 EXPLANATION EVALUATION METRIC
158,"We now consider ways of automatically evaluating the quality of an explanations generated using different sampling methods. Since it is costly and laborious to manually label the quality of explanations for each query-document pair, we propose two metrics that exploit relevance judgments to measure explanation quality at scale. We focus on 2C's ­ consistency and correctness for evaluating explanations described in following sections."
159,
160,4.1 Explanation Consistency
161,"An explanation vector ^ Q,D can be used to determine which terms are important for explaining the score of a document D with respect to a query Q, i.e. S(D, Q). The first desirable quality of an explanation method is that the relative ranking of important terms should"
162,
163,1282
164,
165,Short Research Papers 3C: Search
166,
167,"SIGIR '19, July 21­25, 2019, Paris, France"
168,
169,"not change significantly with variations in the parameters of the model, or in other words, a particular choice of samples around the pivot document, D, should not result in considerable differences in the predicted explanation vector."
170,Variances in term rankings
171,
172,R(Q)(w) =
173,
174,f
175,v R(Q )
176,
177,R
178,
179,+
180,
181,(1
182,
183,-
184,
185,") cf(w) , cs"
186,
187,(3)
188,
189,"where R(Q) denotes the set of relevant terms extracted from R(Q), f and cf respectively denote term and collection frequencies, and"
190,cs denotes collection size. We then assume that an ideal explanation
191,system should seek to predict the same ranking of terms as induced
192,"by the decreasing order of term weights. Formally speaking, if "
193,respect to the ground-truth ranking of terms as
194,
195,"(Q, D)"
196,
197,=
198,
199,1 ||
200,
201,(
202,
203,"^ Q, D"
204,
205,(4)
206,
207,ECON = 1
208,
209,"(Q, D),"
210,
211,|Q|
212,
213,Q  Q D TOP(Q )
214,
215,"where  represents the set of different explanation vectors obtained with different samples, e.g. variations in the L1-regularization and kernel widths of LIRME"
216,
217,4.2 Explanation Correctness
218,"Intuitively, an explanation may be considered to be effective if it attributes higher weights to the components of ^ Q,D that correspond to relevant terms, i.e. the terms occurring in documents that are"
219,"judged relevant by assessors. We measure explanation correctness by computing similarity between explanation vector terms ^ Q,D and relevant terms R(Q). In particular, for a query-document pair"
220,
221,ECOR
222,
223,=
224,
225,1 |Q|
226,
227,Q  Q D TOP(Q )
228,
229,"^ Q,D · R(Q) |^ Q,D ||R(Q)|"
230,
231,(5)
232,
233,(a) Consistency
234,
235,(b) Correctness
236,
237,Figure 1: ECON and ECOR for top-5 retrieved documents.
238,
239,(a) Relevant documents
240,
241,(b) Non-relevant documents
242,
243,Figure 2: ECON for relevant and non-relevant documents.
244,
245,(a) Relevant documents
246,
247,(b) Non-relevant documents
248,
249,Figure 3: ECOR for relevant and non-relevant documents. where R(Q) represents the distribution of terms in the judged relevant documents. Similar to consistency ECON. we aggregate the relevance similarity values over a set of queries and number of top documents retrieved for each query.
250,
251,5 EXPERIMENTS
252,The objectives of our experiments are to investigate - a) what term sampling approaches are effective in terms of the metrics consistency and ECOR
253,"For our experiments, we use a standard benchmark dataset, namely the TREC-8, comprising 50 topics. To generate different sub-samples for explanation, we also employ uniform kernel in addition to Gaussian kernel, i.e. apply"
254,"In Figure 1a, we report expected consistency"
255,
256,1283
257,
258,Short Research Papers 3C: Search
259,
260,"SIGIR '19, July 21­25, 2019, Paris, France"
261,
262,(a) Uniform sampling
263,
264,(b) Tf-idf sampling
265,
266,(c) Masked samples
267,
268,Figure 4: Visualization of explanation vectors ^
269,
270,query
271,
272,"higher consistency than the masking-based sampling. This indicates that on an average the relative ranks of the term weights in the explanation vector ^ are more stable for these two sampling methods in comparison to the masking based sampler. Our experiments indicate that samples generated from a masking-based sampler exhibit less diversity because of a smaller degree of freedom in choosing individual terms independent of its context. Moreover, in the case of bag-of-words, uniform sampling yields higher consistency"
273,Biased sampling with tf-idf weights results in higher ECOR
274,"For each sampling strategy investigated, Figure 4 plots the terms with their associated weights from explanation vectors, ^ , as histograms for a document judged relevant for the query `counterfeiting money'"
275,
276,the local explanations generated by any LIRME. Another observation is that sampling approaches were mostly able to find terms
277,6 CONCLUSION
278,"While research in explaining outputs of classification models exists, there is little work on explaining results of a ranking model. In this work, we addressed the research question: Why does an IR model assign a certain score"
279,REFERENCES
280,"[1] E. Alepis, E. Politou, and C. Patsakis. Forgetting personal data and revoking consent under the GDPR: Challenges and proposed solutions. Journal of Cybersecurity, 4(1), 03 2018."
281,"[2] T. Bolukbasi, K. Chang, J. Y. Zou, V. Saligrama, and A. Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. CoRR, abs/1607.06520, 2016."
282,"[3] J. Chen, L. Song, M. J. Wainwright, and M. I. Jordan. Learning to explain: An information-theoretic perspective on model interpretation. arXiv preprint arXiv:1802.07814, 2018."
283,"[4] L. Dixon, J. Li, J. Sorensen, N. Thain, and L. Vasserman. Measuring and mitigating unintended bias in text classification. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 67­73. ACM, 2018."
284,"[5] Z. C. Lipton. The mythos of model interpretability. arXiv:1606.03490, 2016. [6] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions."
285,"In Advances in Neural Information Processing Systems, pages 4765­4774, 2017. [7] D. Metzler and W. Bruce Croft. Linear feature-based models for information"
286,"retrieval. Inf. Retr., 10(3):257­274, June 2007. [8] G. Montavon, W. Samek, and K. Müller. Methods for interpreting and under-"
287,"standing deep neural networks. CoRR, abs/1706.07979, 2017. [9] M. T. Ribeiro, S. Singh, and C. Guestrin. Why should i trust you?: Explaining the"
288,"predictions of any classifier. In Proc. of KDD'16, pages 1135­1144, 2016. [10] M. T. Ribeiro, S. Singh, and C. Guestrin. Anchors: High-precision model-agnostic"
289,"explanations. In AAAI Conference on Artificial Intelligence, 2018. [11] J. Singh and A. Anand. Interpreting search result rankings through intent model-"
290,"ing. arXiv preprint arXiv:1809.05190, 2018. [12] J. Singh and A. Anand. Posthoc interpretability of learning to rank models using"
291,"secondary training data. arXiv preprint arXiv:1806.11330, 2018. [13] J. Singh and A. Anand. EXS: Explainable Search Using Local Model Agnostic"
292,"Interpretability. In Proc. of WSDM '19, pages 770­773, 2019."
293,
294,1284
295,
296,

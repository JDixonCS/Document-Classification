,sentence,label,data
0,Short Research Papers 3C: Search,null,null
1,,null,null
2,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
3,,null,null
4,Selecting Discriminative Terms for Relevance Model,null,null
5,,null,null
6,Dwaipayan Roy,null,null
7,"Indian Statistical Institute, Kolkata dwaipayan_r@isical.ac.in",null,null
8,,null,null
9,Sumit Bhatia,null,null
10,"IBM Research, Delhi, India sumitbhatia@in.ibm.com",null,null
11,,null,null
12,Mandar Mitra,null,null
13,"Indian Statistical Institute, Kolkata mandar@isical.ac.in",null,null
14,,null,null
15,ABSTRACT,null,null
16,"Pseudo-relevance feedback based on the relevance model does not take into account the inverse document frequency of candidate terms when selecting expansion terms. As a result, common terms are often included in the expanded query constructed by this model. We propose three possible extensions of the relevance model that address this drawback. Our proposed extensions are simple to compute and are independent of the base retrieval model. Experiments on several TREC news and web collections show that the proposed modifications yield significantly better MAP, precision, NDCG, and recall values than the original relevance model as well as its two recently proposed state-of-the-art variants.",null,null
17,CCS CONCEPTS,null,null
18,· Information systems  Query representation; Query reformulation.,null,null
19,"ACM Reference Format: Dwaipayan Roy, Sumit Bhatia, and Mandar Mitra. 2019. Selecting Discriminative Terms for Relevance Model. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",null,null
20,1 INTRODUCTION,null,null
21,Query expansion,null,null
22,RM3 [8] is one of the most commonly used method for PRF and experiments reported by Lv and Zhai [11] demonstrated its robustness on different collections when compared with other feedback methods. They also found that RM3 term weighing is prone to select generic words as expansion terms that cannot help identify relevant documents. This occurrence of noise is a well-known shortcoming of PRF methods in general due to presence of non-relevant documents in the pseudo-relevant set,null,null
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331357",null,null
24,,null,null
25,"noise that can lead to the expanded query drifting away from the original query [10]. Cao et al. [2] studied the utility of terms selected for expansion and found that only about a fifth of the expansion terms identified for query expansion contributed positively to an improvement in retrieval performance. Rest of the terms either had no impact on retrieval performance or led to the reduction in final retrieval performance. Background Work: In order to prevent the inclusion of common terms in expanded queries, different methods have been proposed to compute better term weights for PRF models such as incorporating proximity of expansion terms to query terms [12], classifying expansion terms as good and bad by using features derived from term distributions, co-occurrences, proximity, etc. [2]. Parapar and Barreiro [15] proposed RM3-DT, an alternative estimation of term weighting in RM by subtracting the collection probability of the term from its document probability, thus giving a high weight to terms that have a higher probability of being present in the pseudorelevant document set rather than the whole collection.",null,null
26,"Clinchant and Gaussier [4] described an axiomatic framework and discussed five axioms for characterizing different PRF models. Building upon this framework, various improvements and modifications of the original relevance model have been proposed [1, 13, 14] to select better terms for query expansion and compute better weights by incorporating new constraints and assumptions in the computation of scores of terms present in the set of feedback documents. Recently, Montazeralghaem et al. [14] proposed two additional constraints to consider interdependence among previously established characteristics of pseudo-relevant feedback",null,null
27,1 https://github.com/dwaipayanroy/rm3idf,null,null
28,,null,null
29,1253,null,null
30,,null,null
31,Short Research Papers 3C: Search,null,null
32,,null,null
33,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
34,,null,null
35,2 PROPOSED METHOD,null,null
36,2.1 Intuition Behind Proposed Approaches,null,null
37,According to the relevance model,null,null
38,,null,null
39,FWrm,null,null
40,,null,null
41,(1),null,null
42,,null,null
43,d D,null,null
44,,null,null
45,q Q,null,null
46,,null,null
47,"Here, D is the pseudo-relevant document set.",null,null
48,"In RM3 [8], the feedback weight is estimated by linearly interpolating the query likelihood model of w",null,null
49,,null,null
50,FWrm3(w ) = P,null,null
51,,null,null
52,(2),null,null
53,,null,null
54,"Here,   [0, 1] is the interpolation parameter, and P(w |Q) is estimated using maximum likelihood estimation.",null,null
55,"Let us consider a term t that is present in a large number of documents in the collection. If t is present in the pseudo-relevant document set, the value of P(t |R)",null,null
56,,null,null
57,2.2 RM3+1,null,null
58,"In our first proposed approach, we compute the weights of candidate expansion terms by using P(w |R)/P(w |N ). The probability of selection of a term from the relevance class",null,null
59,,null,null
60,Document Collection #Docs Collection Type,null,null
61,,null,null
62,Query Query Set Fields,null,null
63,,null,null
64,Query Dev Test Ids Set Set,null,null
65,,null,null
66,"TREC Disk 1, 2",null,null
67,,null,null
68,News,null,null
69,,null,null
70,"741,856",null,null
71,,null,null
72,Title,null,null
73,,null,null
74,"TREC 1 ad-hoc TREC 2, 3 ad-hoc",null,null
75,,null,null
76,51-100  101-200,null,null
77,,null,null
78,,null,null
79,,null,null
80,"TREC Disks 4, 5 News exclude CR",null,null
81,,null,null
82,TREC 6 ad-hoc 301-350,null,null
83,,null,null
84,"528,155 Title TREC 7, 8 ad-hoc 351-450",null,null
85,,null,null
86,TREC Robust 601-700,null,null
87,,null,null
88,,null,null
89,,null,null
90,GOV2 Web,null,null
91,,null,null
92,"25,205,179 Title",null,null
93,,null,null
94,"TREC Terabyte 1 701-750  TREC Terabyte 2,3 751-850",null,null
95,,null,null
96,,null,null
97,,null,null
98,WT10G CW09B-S70,null,null
99,,null,null
100,Web,null,null
101,,null,null
102,"1,692,096 50,220,423",null,null
103,,null,null
104,Title,null,null
105,,null,null
106,WT10G ClueWeb09,null,null
107,,null,null
108,451-550,null,null
109,,null,null
110,1-200,null,null
111,,null,null
112,,null,null
113,,null,null
114,Table 1: Dataset Overview,null,null
115,,null,null
116,expanded query is computed as follows:,null,null
117,,null,null
118,FW,null,null
119,,null,null
120,=,null,null
121,,null,null
122,P,null,null
123,,null,null
124,,null,null
125,,null,null
126,P,null,null
127,,null,null
128,,null,null
129,,null,null
130,P,null,null
131,,null,null
132,(3),null,null
133,,null,null
134,where r,null,null
135,"For our experiments, we use the traditional inverse document frequency factor in place of r",null,null
136,"effectiveness). Next, the weights are sum-normalized and the top N terms with the highest FW",null,null
137,terms. The normalized feedback weights,null,null
138,,null,null
139,FWrm3+1,null,null
140,,null,null
141,(4),null,null
142,,null,null
143,If Dirichlet smoothing is used,null,null
144,"ing retrieval using the expanded query Q, the score of a document d is computed in practice as follows:",null,null
145,,null,null
146,"Score(d,",null,null
147,,null,null
148,Q),null,null
149,,null,null
150,=,null,null
151,,null,null
152,NFW rm31+,null,null
153,q Q,null,null
154,,null,null
155,×,null,null
156,,null,null
157,log,null,null
158,,null,null
159,µ .P,null,null
160,,null,null
161,(5),null,null
162,,null,null
163,where NFW,null,null
164,,null,null
165,2.3 RM3+2,null,null
166,"In RM3+1 , a term w is weighted on the basis of a linear combination between the query language model",null,null
167,term selection probability from RM,null,null
168,,null,null
169,"non-relevance model. As an alternative approach, we consider the RM3 model",null,null
170,,null,null
171,likely w is from the query-relevance model,null,null
172,,null,null
173,"non-relevance model. Formally, candidate terms are weighted using",null,null
174,,null,null
175,a P,null,null
176,,null,null
177,ratio similar to,null,null
178,,null,null
179,"the one used in RM3+1 , but unlike to formulate the expansion term",null,null
180,,null,null
181,"RM3+1 , we use weight. Here,",null,null
182,,null,null
183,"P(w |R) is approximated by Equation 2, and P(w |N ) is estimated",null,null
184,,null,null
185,"as in RM3+1 . Thus, the weight of term w in the expanded query is",null,null
186,,null,null
187,computed as follows.,null,null
188,,null,null
189,P,null,null
190,,null,null
191,FW rm32+,null,null
192,,null,null
193,(6),null,null
194,,null,null
195,P,null,null
196,,null,null
197,"Note that in this approach, the final score for a document incorporates the rareness information ""twice"": first, as a part of the computation of FW",null,null
198,,null,null
199,1254,null,null
200,,null,null
201,Short Research Papers 3C: Search,null,null
202,,null,null
203,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
204,,null,null
205,This overemphasis on the rareness of query terms may promote highly rare,null,null
206,2.4 RM3+3,null,null
207,"To address the potential issues due to double application of rareness information we adopt an approach similar to Carpineto et al. [3] where candidate expansion terms are ranked using one weighing function, and a different function is used to determine the final weight of the selected expansion terms. Specifically, we first rank the candidate expansion terms using Equation",null,null
208,FW rm33+,null,null
209,3 EXPERIMENTS,null,null
210,3.1 Datasets and Experimental Settings,null,null
211,"We evaluate the proposed modifications to RM3 using standard TREC news and web collections. For parameter tuning, we split the topic sets into development and testing sets as summarized in Table 1.All the collections are stemmed using Porter's stemmer and stopwords are removed prior to indexing using Apache Lucene. For the initial retrieval using the original, unexpanded queries, we used the Dirichlet smoothed language model.All the baselines and the proposed methods were implemented using Lucene.",null,null
212,"Baselines: For comparing with expansion based methods, we choose three baselines ­",null,null
213,Parameter Tuning: There are three parameters associated with the RM3 based QE method: number of documents,null,null
214,3.2 Results and discussion,null,null
215,"Retrieval Performance: Table 2 presents the results of retrieval experiments with all the baselines over all the test collections. We observe that for all the test collections, all of the three proposed modifications outperform the traditional RM3 [8], as well as the",null,null
216,,null,null
217,"two baselines, for most of the evaluation measures. We also note",null,null
218,,null,null
219,that the second proposed compared to RM3+1 and,null,null
220,,null,null
221,RmMet3h+3o.dF(oRrMT3R+2E)Cis,null,null
222,,null,null
223,"seen to be less effective news topics, the mean",null,null
224,,null,null
225,average precision,null,null
226,,null,null
227,always seen to be significantly better for both RM3+1 and RM3+3 than,null,null
228,,null,null
229,"the baselines. Improvement is also observed for P@10, however",null,null
230,,null,null
231,they are not significant for most of the topic sets. Results for TREC,null,null
232,,null,null
233,"Terabyte track 2 and 3 topics exhibit similar improvements over the baselines. For ClueWeb09 topics, RM3+3 achieves significant improvements in MAP, recall, and P@10 values over RM3 and RM3DT. Compared to RM3+All, RM3+3 achieves significantly better MAP and recall values. We also report robustness index",null,null
234,,null,null
235,to compare the robustness of methods when compared with the,null,null
236,,null,null
237,"original RM3 model. We observe that the performance of both RM3+1 and RM3+3 is consistently more robust than the baselines. RM3DT achieves negative RI values for Terabyte 2,3 and ClueWeb",null,null
238,,null,null
239,collections indicating that more queries suffered in terms of retrieval,null,null
240,,null,null
241,"performance than the queries that gained in retrieval performance. On the other hand, RM3+3 consistently achieved best RI values across all collections, except for the ClueWeb collection where it is a very close second. Thus, in terms of retrieval performance, RM3+3 achieves overall best performance among the baselines and the three proposed heuristics, followed by RM3+1 .",null,null
242,,null,null
243,"Computational Latency: For each test collection, Table 4 reports",null,null
244,,null,null
245,the total time taken by different methods to execute all the queries,null,null
246,,null,null
247,"(averaged over ten runs). Note that, we only compare the elapsed",null,null
248,,null,null
249,time for expansion term selection as the actual retrieval times,null,null
250,,null,null
251,(traversing inverted lists) would be common for all the methods.,null,null
252,,null,null
253,"We observe that the execution time for the three proposed methods are consistently less than RM3+All. Further, RM3+3 takes the least amount of additional time over RM3 for finding expansion terms for",null,null
254,,null,null
255,"Terabyte 2,3 and ClueWeb collections. Reading results in Tables 4 and 2 together, we conclude that the RM3+3 method, for most cases, achieves best retrieval performance, is robust, and computes the",null,null
256,,null,null
257,expansion terms faster than other methods studied.,null,null
258,,null,null
259,"Qualitative Analysis: We compare expansion terms selected by RM3, RM3DT, RM3+All and RM3+3",null,null
260,nuclear proliferation,null,null
261,"methods. Observe that as discussed before, RM3 is prone to se-",null,null
262,lecting common terms,null,null
263,"list.On the other hand, RM3+All selects terms with significantly",null,null
264,"high IDF values. For example, expansion terms kalayeh, qazvin",null,null
265,"and yongbyon have collection frequency 1, 4 and 8 respectively",null,null
266,"in TREC 123 collection. However, such extremely rare terms may",null,null
267,"often be due to noise rather than topical relevance to the original query. In contrast, RM3+3 strikes a balance by preventing frequent terms from occupying top weights as well as not prioritizing very",null,null
268,rare terms.,null,null
269,,null,null
270,4 CONCLUSION,null,null
271,We proposed three modifications to RM3 model to promote selection of discriminative terms for query expansion. A thorough empirical evaluation was performed using TREC news and Web collections and results compared with two state-of-the-art RM3 variants for promoting high IDF terms for query expansion. The,null,null
272,,null,null
273,1255,null,null
274,,null,null
275,Short Research Papers 3C: Search,null,null
276,,null,null
277,"SIGIR '19, July 21­25, 2019, Paris, France",null,null
278,,null,null
279,Metrics,null,null
280,,null,null
281,TREC23,null,null
282,,null,null
283,MAP P@10 Recall NDCG@10,null,null
284,RI,null,null
285,,null,null
286,TREC78Rb,null,null
287,,null,null
288,MAP P@10 Recall NDCG@10,null,null
289,RI,null,null
290,,null,null
291,"MAP P@10 Terabyte 2,3 Recall NDCG@10",null,null
292,RI,null,null
293,,null,null
294,CW09B,null,null
295,,null,null
296,MAP P@10 Recall NDCG@10,null,null
297,RI,null,null
298,,null,null
299,LM,null,null
300,0.2325 0.4990 0.6142 0.5065,null,null
301,-,null,null
302,0.2550 0.4372 0.7172 0.4406,null,null
303,-,null,null
304,0.2918 0.5680 0.7076 0.4943,null,null
305,-,null,null
306,0.1065 0.2258 0.4494 0.1693,null,null
307,-,null,null
308,,null,null
309,RM3,null,null
310,0.29360 0.54400 0.67340 0.54490,null,null
311,-,null,null
312,0.29060 0.45130 0.78280 0.4478,null,null
313,-,null,null
314,0.32010 0.60100 0.73640 0.51550,null,null
315,-,null,null
316,0.1081 0.23010 0.45390 0.1699,null,null
317,-,null,null
318,,null,null
319,RM3DT,null,null
320,0.29890 0.54380 0.67830 0.54400,null,null
321,0.11,null,null
322,"0.29310, 1 0.45910 0.78810 0.45030, 1",null,null
323,0.12,null,null
324,0.31680 0.59950 0.73850 0.51340,null,null
325,-0.05,null,null
326,0.1078 0.22970 0.45140,null,null
327,0.1695,null,null
328,-0.01,null,null
329,,null,null
330,RM3+All,null,null
331,"0.29960, 1 0.54100 0.67960 0.55110, 1, 2",null,null
332,0.16,null,null
333,"0.29460, 1, 2 0.46170 0.78910 0.45490, 1",null,null
334,0.19,null,null
335,"0.32120, 2 0.60040 0.73910 0.52310, 1, 2",null,null
336,0.14,null,null
337,0.1081 0.23210 0.45910,null,null
338,0.1702,null,null
339,0.10,null,null
340,,null,null
341,"RM3+1 0.30540, 1, 2, 3",null,null
342,"0.55400 0.69060, 1, 2, 3 0.55380, 1, 2, 3",null,null
343,0.12,null,null
344,"0.29980, 1 0.47040, 1, 2",null,null
345,"0.79630 0.45510, 1",null,null
346,0.11,null,null
347,"0.33720, 1, 2, 3 0.61300",null,null
348,"0.75030, 1, 2, 3 0.52340, 1, 2",null,null
349,0.24,null,null
350,"0.11370, 1, 2, 3 0.23180",null,null
351,"0.47020, 1, 2, 3",null,null
352,0.1692,null,null
353,0.23,null,null
354,,null,null
355,"RM3+2 0.30300, 1 0.54900 0.68590, 1, 2, 3 0.55100, 1",null,null
356,0.11,null,null
357,"0.29520, 1 0.47140, 1, 2, 3",null,null
358,0.79230 0.45600,null,null
359,0.06,null,null
360,"0.32370, 2 0.61200 0.73390 0.52020, 2",null,null
361,0.02,null,null
362,0.1080,null,null
363,0.2328,null,null
364,0.4487,null,null
365,0.1664,null,null
366,0.00,null,null
367,,null,null
368,"RM3+3 0.30750, 1, 2, 3",null,null
369,"0.54100 0.68300, 1, 2, 3",null,null
370,0.53990,null,null
371,0.30,null,null
372,"0.30360, 1, 2, 3 0.46780, 1, 2 0.79760, 1, 2, 3 0.45770, 1, 2",null,null
373,0.27,null,null
374,"0.33230, 1, 2, 3 0.61700, 1, 2, 3 0.74460, 1, 2 0.52520, 1, 2",null,null
375,0.32,null,null
376,"0.11480, 1, 2, 3 0.23740, 1, 2 0.46870, 1, 2, 3",null,null
377,0.1737,null,null
378,0.22,null,null
379,,null,null
380,Table 2: Performance of the proposed methods and different baselines. Statistically significant improvements,null,null
381,,null,null
382,nuclear proliferation,null,null
383,,null,null
384,RM3 RM3DT RM3-All,null,null
385,,null,null
386,"nuclear. prolifer, weapon, 10, nation, year, soviet, 1, state, spread, date, regim, call, goal, limit nuclear, prolifer, intellig, cia, gate, mr, countri, weapon, soviet, iraq, chemic, effort, commun, agenc nuclear, prolifer, treati, weapon, farwick, pakistan, ntg, spector, qazvin, signatori, southasia, argentina, kalayeh, yongbyon, mcgoldrick",null,null
387,,null,null
388,"RM3+3 nuclear, prolifer, treati, weapon, soviet, intern, signatori, nation, armamaent, assess, review, regim, union, spread, peac",null,null
389,,null,null
390,"Table 3: Top 15 expansion terms selected by RM3, RM3+All and RM3+3",null,null
391,,null,null
392,Topics,null,null
393,,null,null
394,RM3,null,null
395,,null,null
396,Latency,null,null
397,,null,null
398,RM3DT RM3+ALL,null,null
399,,null,null
400,RM3+1,null,null
401,,null,null
402,RM3+2,null,null
403,,null,null
404,RM3+3,null,null
405,,null,null
406,TREC23,null,null
407,,null,null
408,314 320,null,null
409,,null,null
410,TREC78Rb 756 773,null,null
411,,null,null
412,"Tb 2,3",null,null
413,,null,null
414,997 1005,null,null
415,,null,null
416,CW09B 1547 1789,null,null
417,,null,null
418,Table 4: Average computational latency in milliseconds for different methods on test queries. Percentage increase over RM3,null,null
419,,null,null
420,results suggested that the proposed heuristics,null,null
421,"with the baselines. Further, the improvements are more robust and",null,null
422,the proposed heuristics are computationally more efficient than the,null,null
423,baselines.,null,null
424,REFERENCES,null,null
425,[1] M. Ariannezhad et al. 2017. Iterative Estimation of Document Relevance Score for Pseudo-Relevance Feedback. In Proc. of ECIR. 676­683.,null,null
426,"[2] G. Cao, J. Nie, J. Gao, and S. Robertson. 2008. Selecting Good Expansion Terms for Pseudo-relevance Feedback. In Proc. of 31st ACM SIGIR. ACM, 243­250.",null,null
427,"[3] C. Carpineto, R. de Mori, G. Romano, and B. Bigi. 2001. An information-theoretic approach to automatic query expansion. ACM Trans. Inf. Syst. 19, 1",null,null
428,"[4] Stéphane Clinchant and Eric Gaussier. 2013. A Theoretical Analysis of PseudoRelevance Feedback Models. In Proc. of ICTIR 2013. Article 6, 8 pages.",null,null
429,,null,null
430,[5] Ronan Cummins. 2017. Improved Query-Topic Models Using Pseudo-Relevant PóLya Document Models. In Proc. of the ACM ICTIR 2017. 101­108.,null,null
431,"[6] G. Furnas, T. Landauer, L. Gomez, and S. Dumais. 1987. The Vocabulary Problem in Human-system Communication. Commun. ACM 30, 11",null,null
432,[7] H. Hazimeh and C. Zhai. 2015. Axiomatic Analysis of Smoothing Methods in Language Models for Pseudo-Relevance Feedback. In Proc. of ICTIR 2015. 141­150.,null,null
433,"[8] N. A. Jaleel, J. Allan, W. B. Croft, F. Diaz, L. S. Larkey, X. Li, M. D. Smucker, and C. Wade. 2004. UMass at TREC 2004: Novelty and HARD. In Proc. TREC '04.",null,null
434,[9] Victor Lavrenko and W. Bruce Croft. 2001. Relevance Based Language Models. In Proc. of 24th SIGIR,null,null
435,"[10] Kyung Soon Lee, W B. Croft, and J. Allan. 2008. A cluster-based resampling method for pseudo-relevance feedback. In SIGIR. 235­242.",null,null
436,[11] Y. Lv and C. Zhai. 2009. A Comparative Study of Methods for Estimating Query Language Models with Pseudo Feedback. In Proc. of 18th ACM CIKM. 1895­1898.,null,null
437,"[12] Y. Lv and C. Zhai. 2010. Positional relevance model for pseudo-relevance feedback. In Proc. of 33rd ACM SIGIR. ACM, 579­586.",null,null
438,"[13] A. Montazeralghaem, H. Zamani, and A. Shakery. 2016. Axiomatic Analysis for Improving the Log-Logistic Feedback Model. In Proc. of 39th ACM SIGIR. 765­768.",null,null
439,"[14] A. Montazeralghaem, H. Zamani, and A. Shakery. 2018. Theoretical Analysis of Interdependent Constraints in Pseudo-Relevance Feedback. In SIGIR. 1249­1252.",null,null
440,[15] Javier Parapar and Álvaro Barreiro. 2011. Promoting Divergent Terms in the Estimation of Relevance Models. In Proc. of Third ICTIR'11. 77­88.,null,null
441,"[16] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4",null,null
442,"[17] T. Sakai, T. Manabe, and M. Koyama. 2005. Flexible pseudo-relevance feedback via selective sampling. ACM TALIP 4, 2",null,null
443,"[18] C. Zhai and J. Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM TOIS. 22, 2",null,null
444,,null,null
445,1256,null,null
446,,null,null
447,,null,null

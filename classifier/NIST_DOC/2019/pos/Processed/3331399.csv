,sentence
0,Demonstration Papers 2: Evaluation & Entities
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,TrecTools: an Open-source Python Library for Information Retrieval Practitioners Involved in TREC-like Campaigns
5,
6,João Palotti
7,"Qatar Computing Research Institute Doha, Qatar"
8,jpalotti@hbku.edu.qa
9,
10,Harrisen Scells
11,"The University of Queensland Brisbane, Australia h.scells@uq.net.au"
12,
13,Guido Zuccon
14,"The University of Queensland Brisbane, Australia g.zuccon@uq.edu.au"
15,
16,ABSTRACT
17,"This paper introduces TrecTools, a Python library for assisting Information Retrieval"
18,"Written in the most popular programming language for Data Science, Python, TrecTools offers an object-oriented, easily extensible library. Existing systems, e.g., trec_eval, have considerable barrier to entry when it comes to modify or extend them. Furthermore, many existing IR measures and tools are implemented independently of each other, in different programming languages. TrecTools seeks to lower the barrier to entry and to unify existing tools, frameworks and activities into one common umbrella. Widespread adoption of a centralised solution for developing, evaluating, and analysing TREC-like campaigns will ease the burden on organisers and provide participants and users with a standard environment for common IR experimental activities."
19,TrecTools is distributed as an open source library under the MIT license at https://github.com/joaopalotti/trectools
20,CCS CONCEPTS
21,· Information systems  Evaluation of retrieval results; Test collections;
22,"ACM Reference Format: João Palotti, Harrisen Scells, and Guido Zuccon. 2019. TrecTools: an Opensource Python Library for Information Retrieval Practitioners Involved in TREC-like Campaigns. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval"
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331399"
24,
25,1 INTRODUCTION
26,Rigorous empirical evaluation is a cornerstone of Information Retrieval
27,"Creating test collections for IR evaluation and participating to such shared tasks is time consuming ­ with a large amount of effort spent in implementing the evaluation settings, rather than the methods to address the IR task, e.g., creating baselines, selecting documents for assessment, performing relevance assessment, measuring system effectiveness, etc. In this paper we present TrecTools1, an open source Python software package to support the creation and use of IR evaluation resources. TrecTools aims to support both IR campaign organizers and participants to deal with a number of recurrent, tedious and time-consuming procedures. For evaluation campaigns organisers, TrecTools allows one to"
28,"Before TrecTools, IR practitioners were required to use a series of separate, independent and unlinked tools such as trec_eval for computing evaluation measures, workspaces e.g., in R/Matlab for statistical analysis and result plotting, etc., and were required to implement their own routines for other tasks, such as pooling. TrecTools aims to provide practitioners with a unified environment to perform these common tasks, as well as standard and verified"
29,1See http://www.ielab.io/trectools
30,
31,1325
32,
33,Demonstration Papers 2: Evaluation & Entities
34,
35,"SIGIR '19, July 21­25, 2019, Paris, France"
36,
37,"supported. TrecTools is built upon standard Data Science libraries in Python, such as Numpy, Scipy, Pandas and Matplotlib, intending to allow for a rapid and smooth learning curve for new users."
38,2 RELATED WORK
39,trec_eval is perhaps the most popular software used by IR researchers. This tool takes as input a run and a relevance assessments
40,"Similar to TrecTools, the Java-based EvALL tool provides native and verified implementations of most IR evaluation measures, and includes features such as statistical significance analysis, results visualisation, and LATEXtables generation [1]. TrecTools goes however beyond EvALL's functionalities by supporting other tasks in the collection creation pipeline, e.g, providing utilities for creating retrieval baselines and assessment pools."
41,"PyIndri [23] and Pytrec_eval [22] are Python interfaces to the Indri IR toolkit and trec_eval, respectively. While comparable to TrecTools with respect to baselines creation and IR measures provision"
42,"Other tools have been released to aid evaluation campaign organisers, including VisualPool [10] and ircor [21]. VisualPool allows to visualise the results of using different document pooling strategies, thus informing collection creators about the effect of using a strategy over another for selecting documents for relevance assessment. Many of the popular pooling strategies implemented in VisualPool are also available in TrecTools, e.g., Depth@K [19], CombMAXTake@N or CombMNZTake@N [13], RRFTake@N [6] and RBPTake@N [14]. ircor is an R package that provides implementations of correlation measures for comparing results rankings or system rankings. This tool implements for example  -AP [26], a extension of the Kendall- correlation that puts higher weight to matches found at higher rank positions. TrecTool also provides an implementation of  -AP"
43,3 TRECTOOLS FEATURES
44,TrecTools is implemented in Python using standard Data Science libraries and using the object-oriented paradigm. Each of the key
45,
46,components of an evaluation campaign is mapped to a class: classes for runs
47,"Querying IR Systems. Benchmark runs can be obtained directly from one of the IR toolkits that are integrated in TrecTools. There is support for issuing full-text queries to Indri, Terrier2 and PISA3 toolkits. Future releases will include other toolkits"
48,"Pooling Techniques. The following techniques for pool creation from a set of runs are implemented: Depth@K [19], Take@N [11], Comb-Min/Max/Med/Sum/ANZ/MNZ [13], RRFTake@N [6], RBPTake@N [14]. Examples are shown in Figure 3."
49,"Evaluation Measures. Currently implemented and verified measures include widely used metrics such as Precision at depth K, Recall at depth K, MAP, NDCG, Bpref and RBP [14], as well as recently developed ones, such as uBpref [15], uRBP [27] and the MM framework [17]. Implemented in TrecTools is the option to break ties using document score"
50,"Correlation and Agreement Analysis. The Pearson, Spearman, Kendall and ap correlations between system rankings can be computed directly using TrecTools. Agreement measures between relevance assessment sets can be obtained with Kappa or Jaccard. Examples are provided in Figures 5 and 6."
51,Fusion Techniques. Runs can be fused using the following techniques: Comb-Max/Min/Sum/Mnz/Anz/Med
52,4 CONCLUSION
53,"In this paper we introduced TrecTools, an open source Python library for assisting IR practitioners with TREC-like evaluation campaigns. Some of the use cases for campaign organisers using TrecTools include automatically creating baselines by querying the Indri and Terrier IR toolkits"
54,2Thanks to Craig Macdonald for implementing support for Terrier v5.0. 3Thanks to Antonio Mallia for implementing support for PISA
55,
56,1326
57,
58,Demonstration Papers 2: Evaluation & Entities
59,
60,"SIGIR '19, July 21­25, 2019, Paris, France"
61,
62,"from trectools import TrecQrel , procedures"
63,"qrels_file = ""./qrel/robust03_qrels.txt"" qrels = TrecQrel(qrels_file)"
64,"# Generates a P@10 graph with all the runs in a directory path_to_runs = ""./robust03/runs/"" runs = procedures.list_of_runs_from_path(path_to_runs , ""*.gz"")"
65,"results = procedures.evaluate_runs(runs , qrels , per_query=True) p10 = procedures.extract_metric_from_results(results , ""P_10"") procedures.plot_system_rank(p10 , display_metric=""P@10"") # Sample output with one run for each participating team in robust03:"
66,Figure 1: Code Snippets and toy examples with TrecTools. Note the plot is generated by simply calling the method procedures.plot_system_rank().
67,"does not help with obtaining relevance assessments, it can be integrated into existing tools such as Revelation! [8]. TrecTools is by no means a `completed' package: it is open to new evaluation measures and activities as suggested and contributed by the community ­ and it is fully extensible. Despite the care taken for correctness of results, no software is bullet proof -- although, for TrecTools, unit tests are written for each component of the library and methods are also validated against previously released tools for an activity, if any. TrecTools has already been successfully used throughout the creation and results analysis of the CLEF eHealth evaluation campaigns [7, 16, 18, 28]."
68,Acknowledgements. Guido Zuccon is the recipient of an Australian Research Council DECRA Research Fellowship
69,REFERENCES
70,"[1] Enrique Amigó, Jorge Carrillo-de Albornoz, Mario Almagro-Cádiz, Julio Gonzalo, Javier Rodríguez-Vidal, and Felisa Verdejo. 2017. Evall: Open access evaluation for information access systems. In SIGIR. ACM, 1301­1304."
71,"[2] Javed A. Aslam and Mark Montague. 2001. Models for Metasearch. In SIGIR. ACM, 276­284. https://doi.org/10.1145/383952.384007"
72,"[3] Leif Azzopardi, Paul Thomas, and Alistair Moffat. 2019. cwl_eval: An Evaluation Tool for Information Retrieval. In SIGIR. ACM."
73,"[4] Chris Buckley et al. 2004. The trec_eval evaluation package. [5] Charles LA Clarke, Maheedhar Kolla, Gordon V Cormack, Olga Vechtomova,"
74,"Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and diversity in information retrieval evaluation. In SIGIR. ACM, 659­666. [6] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. 2009. Reciprocal rank fusion outperforms condorcet and individual rank learning methods.. In SIGIR, Vol. 9. 758­759. [7] Jimmy, Guido Zuccon, João Palotti, Lorraine Goeuriot, and Liadh Kelly. 2018. Overview of the CLEF 2018 Consumer Health Search Task. In CLEF. http: //ceur- ws.org/Vol- 2125/invited_paper_17.pdf [8] Bevan Koopman and Guido Zuccon. 2014. Relevation!: An open source system for information retrieval relevance assessment. In SIGIR. 1243­1244."
75,
76,"from trectools import TrecTopics , TrecTerrier , TrecIndri"
77,# Loads some topics from a file
78,"the way of the future , including one recent model called a Raspberry Pi. You start thinking about"
79,"buying one , and wonder how much they cost. </description > </topic > </topics > """""" topics = TrecTopics().read_topics_from_file(""topics.txt"") # Or...load topics from a Python dictionary topics = TrecTopics(topics={'201': u'amazonraspberrypi'}) topics.printfile(fileformat=""terrier"") #<topics > # <top > # <num >201</num > # <title >amazon raspberry pi </title > # </top > # </topics >"
80,"topics.printfile(fileformat=""indri"") #<parameters > # <trecFormat >true </trecFormat > # <query > # <id >201 </id > # <text ># combine( amazon raspberry pi ) </text > # </query > # </parameters >"
81,"topics.printfile(fileformat=""indribaseline"") #<parameters > # <trecFormat >true </trecFormat > # <query > # <id >201 </id > # <text >amazon raspberry pi </text > # </query > # </parameters >"
82,"tt = TrecTerrier(bin_path=""<PATH >/terrier/bin/"") # where trec_terrier.sh is located"
83,"# Runs PL2 model from Terrier with Query Expansion tr = tt.run(index=""<PATH >/terrier/var/index"", topics=""topics.xml.gz"","
84,"qexp=True , model=""PL2"", result_file=""terrier.baseline"", expTerms=5, expDocs=3,"
85,"expModel=""Bo1"")"
86,"ti = TrecIndri(bin_path=""~/<PATH >/indri/bin/"") # where IndriRunQuery is located"
87,"ti.run(index=""<PATH >/indriindex"", topics , model=""dirichlet"", parameters ={ "" mu "" :2500} ,"
88,"result_file=""trec_indri.run"", ndocs=1000, qexp=True , expTerms=5, expDocs =3)"
89,Figure 2: Code Snippets for manipulating topic formats and
90,querying IR toolkits
91,"[9] Joon Ho Lee. 1997. Analyses of Multiple Evidence Combination. In SIGIR. ACM, 267­276. https://doi.org/10.1145/258525.258587"
92,"[10] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2017. Visual Pool: A Tool to Visualize and Interact with the Pooling Method. In SIGIR. ACM, 1321­1324. https://doi.org/10.1145/3077136.3084146"
93,"[11] Aldo Lipani, Joao Palotti, Mihai Lupu, Florina Piroi, Guido Zuccon, and Allan Hanbury. 2017. Fixed-cost pooling strategies based on IR evaluation measures. In ECIR. Springer, 357­368."
94,"[12] Craig Macdonald, Richard McCreadie, Rodrygo LT Santos, and Iadh Ounis. 2012. From puppy to maturity: Experiences in developing Terrier. Proc. of OSIR at SIGIR"
95,"[13] Craig Macdonald and Iadh Ounis. 2006. Voting for candidates: adapting data fusion techniques for an expert search task. In CIKM. ACM, 387­396."
96,"[14] Alistair Moffat and Justin Zobel. 2008. Rank-biased Precision for Measurement of Retrieval Effectiveness. ACM Trans. Inf. Syst. 27, 1, Article 2"
97,"[15] Joao Palotti, Lorraine Goeuriot, Guido Zuccon, and Allan Hanbury. 2016. Ranking health web pages with relevance and understandability. In SIGIR. ACM, 965­968."
98,"[16] João Palotti, Guido Zuccon, Lorraine Goeuriot, Liadh Kelly, Allan Hanbury, Gareth J. F. Jones, Mihai Lupu, and Pavel Pecina. 2015. ShARe/CLEF eHealth Evaluation Lab 2015, Task 2: User-centred Health Information Retrieval. In CLEF."
99,"[17] Joao Palotti, Guido Zuccon, and Allan Hanbury. 2018. MM: A new Framework for Multidimensional Evaluation of Search Engines. In CIKM. ACM, 1699­1702."
100,
101,1327
102,
103,Demonstration Papers 2: Evaluation & Entities
104,
105,"SIGIR '19, July 21­25, 2019, Paris, France"
106,
107,"from trectools import TrecPool , TrecRun"
108,"r1 = TrecRun(""./robust03/runs/input.aplrob03a.gz"") r2 = TrecRun(""./robust03/runs/input.UIUC03Rd1.gz"")"
109,len(r1.topics()) # 100 topics
110,# Creates document pools with r1 and r2 using different strategies:
111,# Strategy1: Creates a pool with top 10 documents of each run: pool1 = TrecPool.make_pool
112,1636 unique documents.
113,# Strategy2: Creates a pool with 2000 documents
114,pool2 = TrecPool.make_pool
115,"# Check to see which pool covers better my run r1 pool1.check_coverage(r1 , topX =10) # 10.0 pool2.check_coverage(r1 , topX =10) # 8.35"
116,"# Export documents to be judged using Relevation! visual assessing system pool1.export_document_list(filename=""mypool.txt"", with_format=""relevation"")"
117,Figure 3: Code Snippets for generating and exporting document pools using different pooling strategies.
118,
119,"from trectools import TrecQrel , TrecRun , TrecEval"
120,
121,"# A typical evaluation workflow r1 = TrecRun(""./robust03/runs/input.aplrob03a.gz"") r1.topics()[:5] # Shows the first 5 topics: 601, 602, 603, 604, 605"
122,
123,"qrels = TrecQrel(""./robust03/qrel/robust03_qrels.txt"")"
124,
125,"te = TrecEval(r1 , qrels) rbp , residuals = te.getRBP() p100 = te.getPrecisionAtDepth"
126,
127,"# RBP: 0.474, Residuals: 0.001 # P@100: 0.186"
128,
129,"# Check if documents retrieved by the system were judged: r1.get_mean_coverage(qrels , topX=10) # 9.99 r1.get_mean_coverage(qrels , topX=1000) # 481.390 # On average for system 'input.aplrob03a' participating in robust03 , 480"
130,documents out of 1000 were judged.
131,
132,"# Loads another run r2 = TrecRun(""./robust03/runs/input.UIUC03Rd1.gz"")"
133,
134,"# Check how many documents , on average , in the top 10 of r1 were retrieved in the top 10 of r2"
135,"r1.check_run_coverage(r2 , topX =10) # 3.64"
136,
137,"# Evaluates r1 and r2 using all implemented evaluation metrics result_r1 = r1.evaluate_run(qrels , per_query=True) result_r2 = r2.evaluate_run(qrels , per_query=True)"
138,
139,# Inspect for statistically significant differences between the two runs for P_10 using two -tailed Student t-test
140,"pvalue = result_r1.compare_with(result_r2 , metric=""P_10"") # pvalue: 0.0167"
141,
142,Figure 4: Code snippets showing evaluation options avail-
143,able in TrecTools.
144,"[18] João Palotti, Guido Zuccon, Jimmy, Pavel Pecina, Mihai Lupu, Lorraine Goeuriot, Liadh Kelly, and Allan Hanbury. 2017. CLEF 2017 Task Overview: The IR Task at the eHealth Evaluation Lab - Evaluating Retrieval Methods for Consumer Health Search. In CLEF. http://ceur-ws.org/Vol-1866/invited_paper_16.pdf"
145,[19] K Spark-Jones. 1975. Report on the need for and provision of an'ideal'information retrieval test collection. Computer Laboratory
146,"[20] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. 2­6."
147,[21] Julián Urbano and Mónica Marrero. 2017. The Treatment of Ties in AP Correlation. In SIGIR. 321­324.
148,"[22] Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_Eval: An Extremely Fast Python Interface to Trec_Eval. In SIGIR. ACM, 873­876."
149,"[23] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. 2017. Pyndri: a Python Interface to the Indri Search Engine. In ECIR, Vol. 2017. Springer."
150,"[24] Lidan Wang, Paul N Bennett, and Kevyn Collins-Thompson. 2012. Robust ranking models via risk-sensitive optimization. In SIGIR. ACM, 761­770."
151,
152,"from trectools import misc , TrecRun , TrecQrel , procedures"
153,"qrels_file = ""./robust03/qrel/robust03_qrels.txt"" path_to_runs = ""./robust03/runs/"""
154,qrels = TrecQrel(qrels_file)
155,"runs = procedures.list_of_runs_from_path(path_to_runs , ""*.gz"")"
156,"results = procedures.evaluate_runs(runs , qrels , per_query=True)"
157,# check the system correlation between P@10 and MAP using Kendall's tau for all systems participating in a campaign
158,"misc.get_correlation( misc.sort_systems_by(results , ""P_10""), misc.sort_systems_by(results , ""map""), correlation = ""kendall"") # Correlation: 0.7647"
159,# check the system correlation between P@10 and MAP using Tau's ap for all systems participating in a campaign
160,"misc.get_correlation( misc.sort_systems_by(results , ""P_10""), misc.sort_systems_by(results , ""map""), correlation = ""tauap"") # Correlation: 0.77413"
161,
162,Figure 5: Code Snippets for obtaining correlation measures from a set of runs.
163,
164,# Code snippet to check correlation between two sets of relevance assessment
165,from trectools import TrecQrel
166,
167,"original_qrels_file = ""./robust03/qrel/robust03_qrels.txt"" # Changed the first 10 assessments from 0 to 1 modified_qrels_file = ""./robust03/qrel/mod_robust03_qrels.txt"""
168,
169,original_qrels = TrecQrel(original_qrels_file) modified_qrels = TrecQrel(modified_qrels_file)
170,
171,# Overall agreement
172,
173,original_qrels.check_agreement(modified_qrels) # 0.99
174,
175,# Fleiss' kappa agreement
176,
177,"original_qrels.check_kappa(modified_qrels) # P0: 1.00, Pe = 0.90"
178,
179,# Jaccard similarity coefficient
180,
181,original_qrels.check_jaccard(modified_qrels) # 0.99
182,
183,# 3x3 confusion matrix
184,
185,original_qrels.check_confusion_matrix(modified_qrels)
186,
187,# [[122712
188,
189,10
190,
191,0]
192,
193,#[
194,
195,0 5667
196,
197,0]
198,
199,#[
200,
201,0
202,
203,0 407]]
204,
205,Figure 6: Code Snippets for obtaining agreement measures from a pair of relevance assessments.
206,
207,"from trectools import TrecRun , TrecEval , fusion"
208,
209,"r1 = TrecRun(""./robust03/runs/input.aplrob03a.gz"") r2 = TrecRun(""./robust03/runs/input.UIUC03Rd1.gz"")"
210,
211,# Easy way to create new baselines by fusing existing runs:
212,
213,fused_run = fusion.reciprocal_rank_fusion
214,
215,"TrecEval(r1 , qrels).getPrecisionAtDepth"
216,
217,# P@25: 0.3392
218,
219,"TrecEval(r2 , qrels).getPrecisionAtDepth"
220,
221,# P@25: 0.2872
222,
223,"TrecEval(fused_run , qrels).getPrecisionAtDepth(25) # P@25: 0.3436"
224,
225,"# Save run to disk with all its topics fused_run.print_subset(""my_fused_run.txt"", topics=fused_run.topics())"
226,
227,Figure 7: Code Snippets for generating fusing two runs
228,"[25] Peilin Yang, Hui Fang, and Jimmy Lin. 2018. Anserini: Reproducible ranking baselines using Lucene. JDIQ 10, 4"
229,"[26] Emine Yilmaz, Javed A Aslam, and Stephen Robertson. 2008. A new rank correlation coefficient for information retrieval. In SIGIR. ACM, 587­594."
230,"[27] Guido Zuccon. 2016. Understandability biased evaluation for information retrieval. In ECIR. Springer, 280­292."
231,"[28] Guido Zuccon, João Palotti, Lorraine Goeuriot, Liadh Kelly, Mihai Lupu, Pavel Pecina, Henning Mueller, Julie Budaher, and Anthony Deacon. 2016. The IR Task at the CLEF eHealth Evaluation Lab 2016: User-centred Health Information Retrieval. In CLEF, Vol. 1609. 15­27."
232,
233,1328
234,
235,

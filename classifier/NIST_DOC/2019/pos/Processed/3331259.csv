,sentence
0,"Session 5B: Efficiency, Effectiveness and Performance"
1,
2,"SIGIR '19, July 21­25, 2019, Paris, France"
3,
4,"Statistical Significance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors"
5,
6,Julián Urbano
7,Delft University of Technology The Netherlands
8,urbano.julian@gmail.com
9,
10,Harlley Lima
11,Delft University of Technology The Netherlands
12,h.a.delima@tudelft.nl
13,
14,Alan Hanjalic
15,Delft University of Technology The Netherlands
16,a.hanjalic@tudelft.nl
17,
18,ABSTRACT
19,"Statistical signi cance testing is widely accepted as a means to assess how well a di erence in e ectiveness re ects an actual di erence between systems, as opposed to random noise because of the selection of topics. According to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is the most popular choice among IR researchers. However, previous work has suggested computer intensive tests like the bootstrap or the permutation test, based mainly on theoretical arguments. On empirical grounds, others have suggested non-parametric alternatives such as the Wilcoxon test. Indeed, the question of which tests we should use has accompanied IR and related elds for decades now. Previous theoretical studies on this matter were limited in that we know that test assumptions are not met in IR experiments, and empirical studies were limited in that we do not have the necessary control over the null hypotheses to compute actual Type I and Type II error rates under realistic conditions. Therefore, not only is it unclear which test to use, but also how much trust we should put in them. In contrast to past studies, in this paper we employ a recent simulation methodology from TREC data to go around these limitations. Our study comprises over 500 million p-values computed for a range of tests, systems, e ectiveness measures, topic set sizes and e ect sizes, and for both the 2-tail and 1-tail cases. Having such a large supply of IR evaluation data with full knowledge of the null hypotheses, we are nally in a position to evaluate how well statistical signi cance tests really behave with IR data, and make sound recommendations for practitioners."
20,KEYWORDS
21,"Statistical signi cance, Student's t-test, Wilcoxon test, Sign test, Bootstrap, Permutation, Simulation, Type I and Type II errors"
22,"ACM Reference Format: Julián Urbano, Harlley Lima, and Alan Hanjalic. 2019. Statistical Signi cance Testing in Information Retrieval: An Empirical Analysis of Type I, Type II and Type III Errors. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval"
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21­25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331259"
24,
25,1 INTRODUCTION
26,In the traditional test collection based evaluation of Information Retrieval
27,1.1 Motivation
28,"In a recent survey of 1,055 SIGIR and TOIS papers, Sakai [27] reported that signi cance testing is increasingly popular in our eld, with about 75% of papers currently following one way of testing or another. In a similar study of 5,792 SIGIR, CIKM and ECIR papers, Carterette [5] also observed an increasing trend with about 60% of full papers and 40% of short papers using signi cance testing. The most typical situation is that in which two IR systems are compared on the same collection, for which a paired test is in order to control for topic di culty. According to their results, the t-test is the most popular with about 65% of the papers, followed by the Wilcoxon test in about 25% of them, and to a lesser extent by the sign, bootstrap-based and permutation tests."
29,"It appears as if our community has made a de facto choice for the t and Wilcoxon tests. Notwithstanding, the issue of statistical testing in IR has been extensively debated in the past, with roughly three main periods re ecting our understanding and practice at the time. In the 1990s and before, signi cance testing was not very popular in our eld, and the discussion largely involved theoretical considerations of classical parametric and non-parametric tests, such as the t-test, Wilcoxon test and sign test [16, 40]. During the late 1990s and the 2000s, empirical studies became to be published, and suggestions were made to move towards resampling tests based on the bootstrap or permutation tests, while at the same time advocating for the simplicity of the t-test [31­33, 46]. Lastly, the 2010s witnessed the wide adoption of statistical testing by our community, while at the same time it embarked in a long-pending discussion about statistical practice at large [3, 4, 26]."
30,"Even though it is clear that statistical testing is common nowadays, the literature is still rather inconclusive as to which tests are more appropriate. Previous work followed empirical and theoretical"
31,
32,505
33,
34,"Session 5B: Efficiency, Effectiveness and Performance"
35,
36,"SIGIR '19, July 21­25, 2019, Paris, France"
37,
38,"arguments. Studies of the rst type usually employ a topic splitting methodology with past TREC data. The idea is to repeatedly split the topic set in two, analyze both separately, and then check whether the tests are concordant"
39,1.2 Contributions and Recommendations
40,"The question we ask in this paper is: which is the test that, maintaining Type I errors at the  level, achieves the highest statistical power with IR-like data? In contrast to previous work, we follow a simulation approach that allows us to evaluate tests with unlimited IR-like data and under full control of the null hypothesis. In particular, we use a recent method for stochastic simulation of evaluation data [36, 39]. In essence, the idea is to build a generative stochastic model of the joint distribution of e ectiveness scores for a pair of systems, so that we can simulate an arbitrary number of new random topics from it. The model contains, among other things, the true distributions of both systems, so we have full knowledge of the true mean scores needed to compute test error rates."
41,"We initially devised this simulation method to study the problem of topic set size design [36], but after further developments we recently presented preliminary results about the t-test with IR data [39]. The present paper provides a thorough study in continuation. Our main contributions are as follows:"
42,"· A description of a methodology that allows us, for the rst time, to study the behavior of statistical tests with IR data."
43,· A large empirical study of 500 million p-values computed for simulated data resembling TREC Adhoc and Web runs.
44,· A comparison of the typical paired tests
45,· A comparison across several measures
46,"Based on the results of this study, we make the following conclusions and recommendations:"
47,"· The t-test and the permutation test behave as expected and maintain the Type I error rate at the  level across measures, topic set sizes and signi cance levels. Therefore, our eld"
48,"1For instance, under H0 a test that always returns 1 minus the p-value of the permutation test, will always be discordant with it but have the same Type I error rate. 2Type III errors refer to incorrect directional conclusions due to correctly rejected non-directional hypotheses"
49,
50,"is not being conservative in terms of decisions made on the grounds of statistical testing. · Both the t-test and the permutation test behave almost identically. However, the t-test is simpler and remarkably robust to sample size, so it becomes our top recommendation. The permutation test is still useful though, because it can accommodate other test statistics besides the mean. · The bootstrap-shift test shows a systematic bias towards small p-values and is therefore more prone to Type I errors. Even though large sample sizes tend to correct this e ect, we propose its discontinuation as well. · We agree with previous work in that the Wilcoxon and sign tests should be discontinued for being unreliable. · The rate of Type III errors is not negligible, and for measures like P@10 and RR, or small topic sets, it can be as high as 2%. Testing in these cases should be carried with caution."
51,"We provide online fast implementations of these tests, as well as all the code and data to replicate the results found in the paper3."
52,2 RELATED WORK
53,"One of the rst accounts of statistical signi cance testing in IR was given by van Rijsbergen [40], with a detailed description of the t, Wilcoxon and sign tests. Because IR evaluation data violates the assumptions of the rst two, the sign test was suggested as the test of choice. Hull [16] later described a wider range of tests for IR, and argued that the t-test is robust to violations of the normality assumption, specially with large samples. However, they did not provide empirical results. Shortly after, Savoy [32] stressed van Rijsbergen's concerns and proposed bootstrapping as a general method for testing in IR because it is free of distributional assumptions."
54,"An early empirical comparison of tests was carried out by Wilbur [44], who simulated system runs using a simple model of the rate at which relevant documents are retrieved [20]. Albeit unrealistic, this model allowed a preliminary study of statistical tests under knowledge of the null hypothesis. They showed that non-parametric tests, as well as those based on resampling, behaved quite well in terms of Type I error rate, also indicating preference for bootstrapping. Zobel [46] compared the t-test, Wilcoxon and ANOVA with a random 25­25 split of TREC Ad hoc topics, and found lower discordance rates with the t-test. However, they suggested the Wilcoxon test because it showed higher power and it has more relaxed assumptions. Although they did not study signi cance tests, Voorhees and Buckley [43] used TREC Ad hoc and Web data to analyze the rate of discordance given the observed di erence between two systems. Inspired by these two studies, Sanderson and Zobel [31] used several 25­25 splits of TREC Ad hoc topics and found that the t-test has lower discordance rates than the Wilcoxon test, followed by the sign test. Cormack and Lynam [11] later used 124­124 topic splits from the TREC Robust track to compare actual and expected discordance rates, and found the Wilcoxon test to be more powerful than the t and sign tests, though with higher discordance rates. Voorhees [42] similarly used 50­50 splits of the TREC Robust topics to study concordance rates of the t-test with score standardization. Finally, Sakai [25] also advocated for the bootstrap when evaluating e ectiveness measures, but did not compare to other tests."
55,3 https://github.com/julian-urbano/sigir2019-statistical
56,
57,506
58,
59,"Session 5B: Efficiency, Effectiveness and Performance"
60,
61,"SIGIR '19, July 21­25, 2019, Paris, France"
62,
63,"The most comprehensive comparison of statistical tests for IR is probably the one by Smucker et al. [33]. From both theoretical and empirical angles, they compared the t, Wilcoxon, sign, bootstrapshift and permutation tests. From a theoretical standpoint, they recommend the permutation test and compare the others to it using TREC Ad hoc data. They propose the discontinuation of the Wilcoxon and sign tests because they tend to disagree, while the bootstrap and t-test show very high agreement with the permutation test. In a later study, they showed that this agreement is reduced with smaller samples, and that the bootstrap test appears to have a bias towards small p-values [34]. Inspired by Voorhees [42] and Smucker et al. [33], Urbano et al. [37] performed a largescale study with 50­50 splits of the TREC Robust data to compare statistical tests under di erent optimality criteria. They found the bootstrap test to be the most powerful, the t-test the one with lowest discordance rates, and the Wilcoxon test the most stable."
64,"In summary, we nd in the literature:"
65,· Both theoretical and empirical arguments for and against speci c tests.
66,"· Even though discordance rates can not be used as proxies to the Type I error rate because the null hypothesis is unknown [11], several studies made a direct or indirect comparison, suggesting that tests are too conservative because discordance rates are below the signi cance level  = 0.05."
67,"· Most studies analyze statistical tests with AP scores, with few exceptions also using P@10. In general, they found higher discordance rates with P@10, arguing that AP is a more stable measure and conclusions based on it are more reliable."
68,"· Studies of 2-tailed tests at  = 0.05 almost exclusively. · Except [29] and [44], no empirical study was carried out with"
69,"control of the null hypothesis. However, the rst one does not study the paired test case, and the second was based on unrealistic generative models for simulation."
70,"Although previous work substantially contributed to our understanding of signi cance testing in IR, a comprehensive study of actual error rates is still missing in our eld. An attempt at lling this gap was very recently presented by Parapar et al. [21]. Their approach exploits score distribution models from which new relevance pro les may be simulated"
71,3 METHODS
72,"This section provides a brief description of the ve statistical tests we consider, and then outlines the method we follow to simulate evaluation data with which to compute actual error rates."
73,
74,3.1 Statistical Tests
75,"Assuming some e ectiveness measure, let B1, . . . , Bn be the scores achieved by a baseline system on each of the n topics in a collection, and let E1, . . . , En be the scores achieved by an experimental system on the same topics. For simplicity, let Di = Ei - Bi be the di erence for a topic, and let B, E and D be the mean scores over topics. Under normal conditions, researchers compute the mean scores, and if"
76,"D > 0 they test for the statistical signi cance of the result using a paired test. At this point, a distinction is made between directional and non-directional null hypotheses."
77,"A non-directional null hypothesis has the form H0 : µB = µE , meaning that the mean score of both systems is the same; the alternative is H1 : µB µE . This is called non-directional because it tests for equality of means, but if the hypothesis is rejected it does not say anything about the direction of the di erence. In this case, one uses a test to compute a 2-tailed or 2-sided p-value. A directional null hypothesis has the form H0 : µB  µE , meaning that the mean performance of the baseline system is larger than or equal to that of the experimental system4; the alternative is H1 : µB < µE . In this case, one computes a 1-tailed or 1-sided p-value."
78,"For simplicity, let us assume D > 0 unless otherwise stated."
79,
80,"3.1.1 Student's t-test. The case of a paired two-sample test for µB = µE is equivalent to the one-sample test for µD = 0. In general, the distribution of a sample mean has variance  2/n. When this mean is normally distributed, and  is unknown but estimated with the sample standard deviation s, the standardized mean follows a t-distribution with n - 1 degrees of freedom [35]. The test statistic is therefore de ned as"
81,
82,t=
83,
84,D .
85,
86,(1)
87,
88,sD/ n
89,
90,"Using the cdf of the t-distribution, the 1-tailed p-value is calculated"
91,
92,as the probability of observing an even larger t statistic:
93,
94,p1 = 1 - Ft
95,
96,(2)
97,
98,"Because the t-distribution is symmetric, the 2-tailed p-value is simply twice as large: p2 = 2 · p1. When the data are normally distributed, one can safely use the t-test because the mean is then normally distributed too. If not, and by virtue of the Central Limit Theorem, it can also be used if the sample size is not too small."
99,In our experiments we use the standard R implementation in function t.test.
100,
101,3.1.2 Wilcoxon Signed Rank test. This is a non-parametric test that
102,"disregards the raw magnitudes of the di erences, using their ranks instead [45]. First, every zero-valued observation is discarded5, and"
103,"every other Di is converted to its rank Ri based on the absolute values, but maintaining the correct sign: sign(Ri ) = sign(Di ). The test statistic is calculated as the sum of positive ranks"
104,
105,W = Ri .
106,
107,(3)
108,
109,Ri >0
110,
111,"Under the null hypothesis, W follows a Wilcoxon Signed Rank distribution with sample size n0"
112,
113,"4H0 : µB  µE is also valid, but irrelevant to an IR researcher. 5Other ways of dealing with zeros have been proposed, but they are well out of the scope of this paper. See for instance [23], and [7] for a comparison."
114,
115,507
116,
117,"Session 5B: Efficiency, Effectiveness and Performance"
118,
119,"SIGIR '19, July 21­25, 2019, Paris, France"
120,
121,The 1-tailed p-value is computed as
122,
123,"p1 = 1 - FWSR(W ; n0),"
124,
125,(4)
126,
127,and the 2-tailed p-value is simply twice as large: p2 = 2 · p1. In our experiments we use the standard R implementation in
128,function wilcox.test.
129,
130,3.1.3 Sign test. In this case the data are looked at as if they were
131,
132,"coin tosses where the possible outcomes are Di > 0 or Di < 0, therefore having complete disregard for magnitudes [8, §3.4]. The"
133,
134,"test statistic is the number of successes, that is, the number of topics"
135,
136,where Di > 0:
137,
138,"S = I[Di > 0],"
139,
140,(5)
141,
142,"where I[·] is 1 if · is true or 0 otherwise. Under the null hypothesis, S follows a Binomial distribution with 50% probability of success and n0 trials, where n0 is again the number of topics where Di 0. The 1-tailed p-value is the probability of obtaining at least S successes:"
143,
144,"p1 = 1 - FBinom(S - 1; n0, 0.5)."
145,
146,(6)
147,
148,The 2-tailed p-value is simply twice as large: p2 = 2 · p1. van Rijsbergen [40] proposed to use a small threshold h such that if
149,"|Di |  h then we consider that systems are tied for topic i. Following Smucker et al. [34], we set h = 0.01."
150,In our experiments we use our own implementation to compute
151,"S and n0, and then use the standard R implementation in function binom.test to compute the p-values."
152,
153,"3.1.4 Permutation test. This test is based on the exchangeability principle: under the null hypothesis both systems are the same, so the two scores observed for a topic actually come from the same system and we simply happened to label them di erently [14, 22, §II]. With n topics, there are 2n di erent permutations of the labels, all equally likely. The one we actually observed is just one of them,"
154,
155,so the goal is to calculate how extreme the observed D is.
156,"In practice, the distribution under the null hypothesis is esti-"
157,"mated via Monte Carlo methods. In particular, the following may be repeated T times. A permutation replica Dj is created by randomly swapping the sign of each Di with probability 0.5"
158,
159,the observed mean:
160,
161,1 p1 = T
162,
163,j
164,
165,I Dj  D .
166,
167,(7)
168,
169,The 2-tailed p-value is similarly the fraction of replicas where the magnitude of the mean is at least as large as the one observed:
170,
171,1 p2 = T
172,
173,j
174,
175,I
176,
177,|D
178,
179,j
180,
181,|
182,
183,|D|
184,
185,.
186,
187,(8)
188,
189,The precision of the p-values depends on how many replicas we
190,"create. As noted by Efron and Tibshirani [12, ch. 15], an answer to"
191,this issue may be given by realizing that T · p1 has a Binomial distribution with T trials and probability of success p1. The coe cient of variation for the estimated p^1 is
192,
193,c
194,
195,p^1(1 - p^1) T.
196,
197,(9)
198,
199,If we do not want our estimate of p1 to vary more than % of its
200,
201,"value, then we need to set T"
202,
203,=
204,
205,(1-p1 )  2p1
206,
207,.
208,
209,For
210,
211,"instance,"
212,
213,for
214,
215,a
216,
217,target
218,
219,= 1% error on a p1 = 0.05
220,
221,"T = 190,000 replicas. Under symmetricity assumptions, for p2 we"
222,
223,may assume p2 = 2 · p1. Smucker et al. [34] and Parapar et al. [21]
224,
225,"used 100,000 replicas in their experiments, which yield an error of"
226,
227,0.00045 for a target p2 = 0.01
228,
229,target p2 = 0.05
230,
231,"levels, in our experiments we use T = 1 million replicas. This yields"
232,
233,an error of 0.00014 for a target p2 = 0.01
234,
235,p2 = 0.05
236,
237,0.0002
238,
239,"Because this test is computationally expensive, we used our"
240,
241,"own implementation in C++, using the modern Mersenne Twister"
242,
243,pseudo-random number generator in the C++11 standard.
244,
245,3.1.5 Bootstrap test ­ Shi method. This test follows the bootstrap
246,
247,principle to estimate the sampling distribution of the mean under
248,
249,"the null hypothesis [12, §16.4]. The following is repeated T times."
250,
251,A
252,
253,bootstrap
254,
255,sample
256,
257,D
258,
259,j
260,
261,is
262,
263,created
264,
265,by
266,
267,resampling
268,
269,n
270,
271,observations
272,
273,with replacement from D = 1/T j Dj be the
274,
275,"{Di }in=1, mean of"
276,
277,"and its mean these means,"
278,
279,Dj is recorded. Let which will be used
280,
281,to shift the bootstrap distribution to have zero mean. The 1-tailed
282,
283,p-value is computed as the fraction of bootstrap samples where the
284,
285,shifted bootstrap mean is at least as large as the observed mean:
286,
287,1 p1 = T
288,
289,j
290,
291,I Dj - D  D .
292,
293,(10)
294,
295,The 2-tailed p-value is similarly the fraction of shifted bootstrap samples where the magnitude of the mean is at least as large as the one observed:
296,
297,1 p2 = T
298,
299,j
300,
301,I |Dj - D|  |D| .
302,
303,(11)
304,
305,"As with the permutation test, we compute T = 1 million bootstrap samples. Wilbur [44] and Sakai [25] used 1,000 samples, Cormack and Lynam [10] used 2,000, Savoy [32] used up to 5,000, and both Smucker et al. [33] and Parapar et al. [21] used 100,000. Again, we use our own implementation in C++11 for e ciency."
306,
307,3.2 Stochastic Simulation
308,"In order to simulate realistic IR evaluation that allows us to compare statistical tests, we use the method recently proposed by Urbano and Nagler [39], which extends an initial method by Urbano [36] to study topic set size design methods. In essence, they build a generative stochastic model M that captures the joint distribution of e ectiveness scores for a number of systems. An observation from this model would be the vector of scores achieved by the system on the same topic. This model contains two parts: the marginal distribution of each system, that is, its distribution of e ectiveness scores regardless of the other systems, and a copula [17] to model the dependence among systems, that is, how they tend to behave for the same topic. Because we will focus on paired statistical testing, in our case we will only contemplate bivariate models of only two systems B and E."
309,
310,508
311,
312,"Session 5B: Efficiency, Effectiveness and Performance"
313,
314,"SIGIR '19, July 21­25, 2019, Paris, France"
315,
316,Direction fit
317,simulate
318,
319,Ei
320,
321,E = FE-1(V)
322,
323,margin fE
324,
325,Vi= FE(Ei)
326,
327,V
328,
329,U
330,
331,B = F-B1(U)
332,
333,Ui= FB(Bi)
334,
335,Bi
336,
337,margin fB
338,
339,copula c
340,
341,Figure 1: Stochastic simulation model. To t the model
342,
343,"Given a model M, we can inde nitely simulate evaluation scores for the same two systems but on new, random topics. The model de nes the two individual marginal distributions FB and FE , so it provides full knowledge about the null hypothesis because we know µB and µE beforehand. For the simulated data to be realistic, Urbano and Nagler [39] argue that the model rst needs to be exible enough to accommodate arbitrarily complex data; this is ensured by allowing a range of non-parametric models for both the margins and the copula. Finally, a realistic model can be instantiated by tting it to existing TREC data using a model selection criterion that rewards"
344,t over simplicity
345,Figure 1 summarizes how these stochastic models are built and used. To t the model
346,"ow), a new pseudo-observation"
347,"to obtain the nal observation B = FB-1(U ), E = FE-1(V ) . By construction, we have B  FB and E  FE , so that we simulate new scores under full knowledge of the true system distributions."
348,"We use this simulation method in two di erent ways. First, in order to study Type I error rates we need to generate data under the"
349,
350,"null hypothesis H0 : µB = µE . We achieve this by assigning FE  FB after tting the margins and the copula. This way, we simulate data from two systems that have a certain dependence structure but the same margin distribution, so that µB = µE by construction. Second, and in order to study Type II and Type III error rates, we need to generate data with di erent e ect sizes. In particular, for a xed di erence  , we will need to make sure that µE = µB +  . Given an already tted stochastic model, Urbano and Nagler [39, §3.4] show how this requirement can be met by performing a slight transformation of FE that still maintains underlying properties of the distribution such as its support. For instance, if FE corresponds to P@10 scores, the transformed distribution will also generate valid P@10 scores. This way, we simulate data from two systems that have a certain dependence structure and whose expected scores are at a xed distance  ."
351,"In this paper we use version 1.0 of the simIReff R package6 by Urbano and Nagler [39]. simIReff o ers a high degree of exibility to model IR data. In particular, it implements 6 distribution families for the margins"
352,4 EVALUATION
353,"Because our goal is to evaluate the behavior of statistical tests on a variety of IR data, we chose systems and measures representative of ad hoc retrieval. In particular, we used the runs from the TREC 5­8 Ad hoc and TREC 2010­13 Web tracks. In terms of measures, we chose AP, P@10 and RR for the Ad hoc runs, and nDCG@20 and ERR@20 for the Web runs. We therefore evaluate tests on measures producing smooth but asymmetric distributions"
354,"As is customary in this kind of experiments, we only use the top 90% of the runs to avoid errorful system implementations. This results in a total of 326 Ad hoc runs and 193 Web runs, from which we can use about 14,000 and 5,000 run pairs, respectively. In terms of topic set sizes, we will simulate results on n = 25, 50, 100 topics, representing small, typical and large collections."
355,4.1 Type I Errors
356,"In order to evaluate the actual Type I error rate of the statistical tests, we proceed as follows. For a target measure and topic set size, we randomly select two systems from the same collection and"
357,t the corresponding stochastic model as described in section 3.2
358,6 https://cran.r-project.org/package=simIReff
359,
360,509
361,
362,"Session 5B: Efficiency, Effectiveness and Performance"
363,
364,"SIGIR '19, July 21­25, 2019, Paris, France"
365,
366,AP - 25 topics
367,
368,nDCG@20 - 25 topics
369,
370,ERR@20 - 25 topics
371,
372,P@10 - 25 topics
373,
374,RR - 25 topics
375,
376,.05 .1
377,
378,.05 .1
379,
380,.05 .1
381,
382,.05 .1
383,
384,.05 .1
385,
386,.001 .005 .01
387,
388,Type I error rate
389,
390,.001 .005 .01
391,
392,Type I error rate
393,
394,.001 .005 .01
395,
396,Type I error rate
397,
398,.001 .005 .01
399,
400,Type I error rate
401,
402,.001 .005 .01
403,
404,Type I error rate
405,
406,t-test Wilcoxon Sign Permutation Bootstrap
407,
408,.001
409,
410,.005 .01
411,
412,.05 .1 .001
413,
414,.005 .01
415,
416,.05 .1 .001
417,
418,.005 .01
419,
420,.05 .1 .001
421,
422,.005 .01
423,
424,.05 .1 .001
425,
426,.005 .01
427,
428,.05 .1
429,
430,Significance level
431,
432,Significance level
433,
434,Significance level
435,
436,Significance level
437,
438,Significance level
439,
440,AP - 50 topics
441,
442,nDCG@20 - 50 topics
443,
444,ERR@20 - 50 topics
445,
446,P@10 - 50 topics
447,
448,RR - 50 topics
449,
450,.05 .1
451,
452,.05 .1
453,
454,.05 .1
455,
456,.05 .1
457,
458,.05 .1
459,
460,.001 .005 .01
461,
462,Type I error rate
463,
464,.001 .005 .01
465,
466,Type I error rate
467,
468,.001 .005 .01
469,
470,Type I error rate
471,
472,.001 .005 .01
473,
474,Type I error rate
475,
476,.001 .005 .01
477,
478,Type I error rate
479,
480,.001
481,
482,.005 .01
483,
484,.05 .1 .001
485,
486,.005 .01
487,
488,.05 .1 .001
489,
490,.005 .01
491,
492,.05 .1 .001
493,
494,.005 .01
495,
496,.05 .1 .001
497,
498,.005 .01
499,
500,.05 .1
501,
502,Significance level
503,
504,Significance level
505,
506,Significance level
507,
508,Significance level
509,
510,Significance level
511,
512,AP - 100 topics
513,
514,nDCG@20 - 100 topics
515,
516,ERR@20 - 100 topics
517,
518,P@10 - 100 topics
519,
520,RR - 100 topics
521,
522,.05 .1
523,
524,.05 .1
525,
526,.05 .1
527,
528,.05 .1
529,
530,.05 .1
531,
532,.001 .005 .01
533,
534,Type I error rate
535,
536,.001 .005 .01
537,
538,Type I error rate
539,
540,.001 .005 .01
541,
542,Type I error rate
543,
544,.001 .005 .01
545,
546,Type I error rate
547,
548,.001 .005 .01
549,
550,Type I error rate
551,
552,.001
553,
554,.005 .01
555,
556,.05 .1 .001
557,
558,.005 .01
559,
560,.05 .1 .001
561,
562,.005 .01
563,
564,.05 .1 .001
565,
566,.005 .01
567,
568,.05 .1 .001
569,
570,.005 .01
571,
572,.05 .1
573,
574,Significance level
575,
576,Significance level
577,
578,Significance level
579,
580,Significance level
581,
582,Significance level
583,
584,"Figure 2: Type I error rates of 2-tailed tests. Plots on the same column correspond to the same e ectiveness measure, and plots on the same row correspond to the same topic set size. When indiscernible, the t, permutation and bootstrap tests overlap."
585,
586,AP - 50 topics
587,
588,nDCG@20 - 50 topics
589,
590,ERR@20 - 50 topics
591,
592,P@10 - 50 topics
593,
594,RR - 50 topics
595,
596,.05 .1
597,
598,.05 .1
599,
600,.05 .1
601,
602,.05 .1
603,
604,.05 .1
605,
606,.001 .005 .01
607,
608,Type I error rate
609,
610,.001 .005 .01
611,
612,Type I error rate
613,
614,.001 .005 .01
615,
616,Type I error rate
617,
618,.001 .005 .01
619,
620,Type I error rate
621,
622,.001 .005 .01
623,
624,Type I error rate
625,
626,.001
627,
628,t-test Wilcoxon Sign Permutation Bootstrap
629,
630,.005 .01
631,
632,.05 .1
633,
634,Significance level
635,
636,.001
637,
638,.005 .01
639,
640,.05 .1 .001
641,
642,Significance level
643,
644,.005 .01
645,
646,.05 .1 .001
647,
648,Significance level
649,
650,.005 .01
651,
652,.05 .1 .001
653,
654,Significance level
655,
656,.005 .01
657,
658,.05 .1
659,
660,Significance level
661,
662,"Figure 3: Type I error rates of 1-tailed tests with 50 topics. When indiscernible, the t, permutation and bootstrap tests overlap."
663,
664,1-tailed p-values for every measure and topic set size combination. The grand total is therefore just over 250 million p-values.
665,"Figure 2 shows the actual error rates for the 2-tailed tests and for signi cance levels  in [0.001, 0.1]; each plot summarizes  8.3 million p-values. Ideally all lines should go through the diagonal, meaning that the actual error rate"
666,
667,"with large samples. The permutation test behaves quite better and also approaches the diagonal as sample size increases, and the t-test is remarkably close to ideal behavior even with small sample sizes. With nDCG@20 and ERR@20 we see very similar behavior, with the t-test tracking the ideal error rate nearly perfectly, perhaps with the exception of the t-test being conservative at low  levels with small samples in ERR@20."
668,"When we look at a measure like P@10 that produces clearly discrete distributions, it is remarkable how well the t and permutation tests behave. This is probably because the distributions of di erences in P@10 are very symmetric in practice, which facilitates the e ect of the Central Limit Theorem. For the same reason, the Wilcoxon test behaves better than before, but again becomes overcon dent with large samples. The sign test also su ers from large sample sizes by increasing error rates, while the bootstrap test behaves better with more topics. For RR we see very similar"
669,
670,510
671,
672,"Session 5B: Efficiency, Effectiveness and Performance"
673,
674,"SIGIR '19, July 21­25, 2019, Paris, France"
675,
676,AP - 25 topics
677,
678,nDCG@20 - 25 topics
679,
680,ERR@20 - 25 topics
681,
682,P@10 - 25 topics
683,
684,RR - 25 topics
685,
686,Power 0.0 0.2 0.4 0.6 0.8 1.0
687,
688,Power 0.0 0.2 0.4 0.6 0.8 1.0
689,
690,Power 0.0 0.2 0.4 0.6 0.8 1.0
691,
692,Power 0.0 0.2 0.4 0.6 0.8 1.0
693,
694,Power 0.0 0.2 0.4 0.6 0.8 1.0
695,
696,t-test Wilcoxon Sign Permutation Bootstrap
697,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
698,AP - 50 topics
699,
700,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
701,nDCG@20 - 50 topics
702,
703,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
704,ERR@20 - 50 topics
705,
706,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
707,P@10 - 50 topics
708,
709,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
710,RR - 50 topics
711,
712,Power 0.0 0.2 0.4 0.6 0.8 1.0
713,
714,Power 0.0 0.2 0.4 0.6 0.8 1.0
715,
716,Power 0.0 0.2 0.4 0.6 0.8 1.0
717,
718,Power 0.0 0.2 0.4 0.6 0.8 1.0
719,
720,Power 0.0 0.2 0.4 0.6 0.8 1.0
721,
722,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
723,AP - 100 topics
724,
725,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
726,nDCG@20 - 100 topics
727,
728,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
729,ERR@20 - 100 topics
730,
731,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
732,P@10 - 100 topics
733,
734,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
735,RR - 100 topics
736,
737,Power 0.0 0.2 0.4 0.6 0.8 1.0
738,
739,Power 0.0 0.2 0.4 0.6 0.8 1.0
740,
741,Power 0.0 0.2 0.4 0.6 0.8 1.0
742,
743,Power 0.0 0.2 0.4 0.6 0.8 1.0
744,
745,Power 0.0 0.2 0.4 0.6 0.8 1.0
746,
747,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
748,
749,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
750,
751,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
752,
753,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
754,
755,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
756,
757,"Figure 4: Power of 2-tailed tests as a function of  and at  = 0.05. Plots on the same column correspond to the same topic set size, and plots on the same row correspond to the same e ectiveness measure."
758,
759,"results, again with the exception of the t-test being conservative at low  levels with small samples. The permutation test shows nearly ideal behavior for RR too."
760,Figure 3 shows similar plots but for the 1-tailed case and only with n = 50 topics. The results are consistent with the 2-tailed case: the permutation and t-tests show nearly ideal behavior and the bootstrap test has a bias towards small p-values. When sample size increases
761,"For the typical IR case of n = 50 topics and  = 0.05, we see that the t-test and the permutation tests maintain a Type I error rate of just 0.05, while the bootstrap test is at 0.059 in the 2-tailed case and 0.054 in the 1-tailed case. At the  = 0.01 level, the t-test and the permutation test maintain a 0.01 error rate, with the bootstrap going up to 0.014."
762,4.2 Type II Errors
763,"In order to evaluate the actual Type II error rate, we need to simulate from systems whose true mean scores are at a distance  from each other, and then observe how error rates are a ected by the magnitude of this distance. We rst randomly select a baseline system B from the bottom 75% of runs for a collection and measure, and then pre-select the 10 systems whose mean score is closest to the target µB +  . From these 10, we randomly choose the experimental system E and t the stochastic model to simulate, but we additionally transform FE so that its expected value is µE = µB +  ,"
764,7Plots for other topic set sizes are provided as supplementary material.
765,
766,"as described in section 3.2. Under this model, the null hypothesis is known to be false, so any result that is not statistically signi cant would therefore count as a Type II error. This is repeated 167,000 times for each value of  in 0.01, 0.02, . . . , 0.1. In total, this leads to 8.35 million 2-tailed p-values and 8.35 million 1-tailed p-values for every measure and topic set size combination. The grand total is therefore just over 250 million p-values as well."
767,"As is customary in this kind of analysis, we report power curves instead of Type II error rates, and do so only for the typical signi cance level  = 0.05.8 Figure 4 shows the power curves of the 2-tailed tests as a function of the e ect size  . The rst thing we notice is that there are clear di erences across measures; this is due to the variability of their distributions. For instance, di erences in P@10 or RR have between 2 and 4 times the standard deviation of the other measures. Across all measures we can also see clearly how power increases with the sample size and with the e ect size. These plots clearly visualize the e ect of the three main factors that a ect statistical power: variability, e ect size and sample size. The de nition of the t statistic in eq."
768,"We can see nearly the same pattern across measures. The sign test is consistently less powerful because it does not take magnitudes into account, and the bootstrap test is nearly consistently the most powerful, specially with small samples. The second place is disputed between the Wilcoxon, t and permutation tests. For instance, the Wilcoxon test is slightly more powerful for AP or ERR@20, but less so for nDCG@20 or P@10. For the most part though, these three"
769,8Plots for other  levels are provided online as supplementary material.
770,
771,511
772,
773,"Session 5B: Efficiency, Effectiveness and Performance"
774,
775,AP -  = 0.01
776,t-test Wilcoxon Sign Permutation Bootstrap
777,
778,nDCG@20 -  = 0.01
779,
780,ERR@20 -  = 0.01
781,
782,"SIGIR '19, July 21­25, 2019, Paris, France"
783,
784,P@10 -  = 0.01
785,
786,RR -  = 0.01
787,
788,Power 0.0 0.1 0.2 0.3 0.4 0.5
789,
790,Power 0.0 0.1 0.2 0.3 0.4 0.5
791,
792,Power 0.0 0.1 0.2 0.3 0.4 0.5
793,
794,Power 0.0 0.1 0.2 0.3 0.4 0.5
795,
796,Power 0.0 0.1 0.2 0.3 0.4 0.5
797,
798,.001
799,
800,.005 .01
801,
802,.05 .1 .001
803,
804,.005 .01
805,
806,.05 .1 .001
807,
808,.005 .01
809,
810,.05 .1 .001
811,
812,.005 .01
813,
814,.05 .1 .001
815,
816,.005 .01
817,
818,.05 .1
819,
820,Significance level
821,
822,Significance level
823,
824,Significance level
825,
826,Significance level
827,
828,Significance level
829,
830,AP -  = 0.03
831,
832,nDCG@20 -  = 0.03
833,
834,ERR@20 -  = 0.03
835,
836,P@10 -  = 0.03
837,
838,RR -  = 0.03
839,
840,Power 0.0 0.1 0.2 0.3 0.4 0.5
841,
842,Power 0.0 0.1 0.2 0.3 0.4 0.5
843,
844,Power 0.0 0.1 0.2 0.3 0.4 0.5
845,
846,Power 0.0 0.1 0.2 0.3 0.4 0.5
847,
848,Power 0.0 0.1 0.2 0.3 0.4 0.5
849,
850,.001
851,
852,.005 .01
853,
854,.05 .1 .001
855,
856,.005 .01
857,
858,.05 .1 .001
859,
860,.005 .01
861,
862,.05 .1 .001
863,
864,.005 .01
865,
866,.05 .1 .001
867,
868,.005 .01
869,
870,.05 .1
871,
872,Significance level
873,
874,Significance level
875,
876,Significance level
877,
878,Significance level
879,
880,Significance level
881,
882,"Figure 5: Power of 2-tailed tests as a function of  and at  = 0.01, 0.03, with 50 topics."
883,
884,"tests perform very similarly, specially the t and permutation tests. When the sample size is large, all tests are nearly indistinguishable in terms of power, with the clear exception of the sign test. Results in the 1-tailed case are virtually the same, except that power is of course higher. The reader is referred to the online supplementary material for the full set of results."
885,"Figure 5 o ers a di erent perspective by plotting power as a function of the signi cance level, and for the selection  = 0.01, 0.03 and 50 topics. These plots con rm that the bootstrap and Wilcoxon tests have the highest power, specially for AP, nDCG@20 and ERR@20. That the bootstrap-shift test appears to be more powerful was also noted for instance by Smucker et al. [33], Urbano et al. [37], but the slightly higher power of the Wilcoxon test comes as a surprise to these authors. Conover [8, §5.7,§5.11] points out that, for certain heavy-tailed distributions, the Wilcoxon test may indeed have higher asymptotic relative e ciency compared to the t and permutation tests. For small samples, Conover et al. [9] report similar"
886,"ndings in the two-sample case. In summary, all tests except the sign test behave very similarly,"
887,"with very small di erences in practice. The bootstrap and Wilcoxon tests are consistently a little more powerful, but the results from last section indicate that they are also more likely to make Type I errors. Given that the t-test and the permutation test behave almost ideally in terms of Type I errors, and similar to the others in terms of power, it seems clear that they are the best choice, also consistently across measures and sample sizes."
888,4.3 Type III Errors
889,"The majority of literature on statistical testing is devoted to analyzing the Type I error rate, while the study of Type II errors and power has received considerably less attention. There is however a third type of errors that is virtually neglected in the literature. Type III errors refer to cases in which a wrong directional decision is made after correctly rejecting a non-directional hypothesis9 [19]. Imagine"
890,"9The related concept of major con ict was studied for instance by Voorhees [42] and Urbano et al. [37], but without control over the null hypothesis."
891,
892,we observe a positive improvement of our experimental system
893,"E over the baseline B, that is, D > 0. If we follow the common procedure of running a 2-tailed test for the non-directional hypothesis H0 : µB = µE and reject it, we typically make the directional decision that µE > µB on the grounds of D > 0 and p2   . In such case, we make a directional decision based on a non-directional hypothesis, but that decision could very well be wrong. In our case, it could be that the baseline system really is better and the set of topics was just unfortunate. That is, H0 would still be correctly rejected, but for the wrong reason. In modern terminology, Type III errors are sometimes referred to as Type S errors"
894,"Using the same data from section 4.2, we can study the rate of Type III errors, coined  by Kaiser [19]. Because our simulated data always had a positive e ect  , we actually count cases in which"
895,"D < 0 and the non-directional hypothesis was rejected by p2  . In these cases we would incorrectly conclude that the baseline is signi cantly better than the experimental system. Again, we only report here results at  = 0.05."
896,"Figure 6 shows the rate of Type III errors as a function of the e ect size  . As anticipated by the Type I error rates, the t and permutation tests lead to much fewer errors than the other tests, with the Wilcoxon and sign tests being again more error-prone and the bootstrap test somewhere in between. The e ect of the topic set size is clearly visible again in that the Wilcoxon and sign test become even more unreliable while the others signi cantly reduce the error rate. In particular, the bootstrap test catches up with the t and permutation tests. Di erences across measures are once again caused by the variability of their distributions. RR is by far the most variable measure and hence more likely to observe outliers that lead to wrong conclusions. On the other hand, ERR@20 is the most stable measure and the tests show better behavior with it."
897,"We note that the error rates reported in Figure 6 are over the total number of cases. An arguably more informative rate can be computed over the number of signi cant cases, that is, the fraction of statistically signi cant results that could actually lead to Type III errors. Even though space constraints do not permit to report these plots, Figures 4 and 6 together provide a rough idea. For"
898,
899,512
900,
901,"Session 5B: Efficiency, Effectiveness and Performance"
902,
903,AP - 25 topics
904,t-test Wilcoxon Sign Permutation Bootstrap
905,
906,nDCG@20 - 25 topics
907,
908,ERR@20 - 25 topics
909,
910,"SIGIR '19, July 21­25, 2019, Paris, France"
911,
912,P@10 - 25 topics
913,
914,RR - 25 topics
915,
916,Type III error rate 0.000 0.004 0.008 0.012
917,
918,Type III error rate 0.000 0.004 0.008 0.012
919,
920,Type III error rate 0.000 0.004 0.008 0.012
921,
922,Type III error rate 0.000 0.004 0.008 0.012
923,
924,Type III error rate 0.000 0.004 0.008 0.012
925,
926,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
927,AP - 50 topics
928,
929,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
930,nDCG@20 - 50 topics
931,
932,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
933,ERR@20 - 50 topics
934,
935,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
936,P@10 - 50 topics
937,
938,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
939,RR - 50 topics
940,
941,Type III error rate 0.000 0.004 0.008 0.012
942,
943,Type III error rate 0.000 0.004 0.008 0.012
944,
945,Type III error rate 0.000 0.004 0.008 0.012
946,
947,Type III error rate 0.000 0.004 0.008 0.012
948,
949,Type III error rate 0.000 0.004 0.008 0.012
950,
951,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
952,AP - 100 topics
953,
954,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
955,nDCG@20 - 100 topics
956,
957,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
958,ERR@20 - 100 topics
959,
960,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
961,P@10 - 100 topics
962,
963,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
964,RR - 100 topics
965,
966,Type III error rate 0.000 0.004 0.008 0.012
967,
968,Type III error rate 0.000 0.004 0.008 0.012
969,
970,Type III error rate 0.000 0.004 0.008 0.012
971,
972,Type III error rate 0.000 0.004 0.008 0.012
973,
974,Type III error rate 0.000 0.004 0.008 0.012
975,
976,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
977,
978,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
979,
980,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
981,
982,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
983,
984,.01 .02 .03 .04 .05 .06 .07 .08 .09 .1 Effect size
985,
986,"Figure 6: Type III error rates in directional 2-tailed tests at  = 0.05. Plots on the same column correspond to the same topic set size, and plots on the same row correspond to the same e ectiveness measure. When indiscernible, the t, permutation and bootstrap tests overlap."
987,
988,"instance, with AP and 50 topics, the Type III error rate at the typical  = 0.01 is 0.0069 with the t-test. With this e ect size, 9.47% of comparisons come up statistically signi cant, which means that 7.3% of signi cant results could lead to a Type III error. As another example, with P@10 and 50 topics the Type III error rate is 0.0064, with power at 8.9%. This means that 7.2% of signi cant results could lead to erroneous conclusions."
989,5 CONCLUSIONS
990,"To the best of our knowledge, this paper presents the rst empirical study of actual Type I and Type II error rates of typical paired statistical tests with IR data. Using a method for stochastic simulation of evaluation scores, we compared the t-test, Wilcoxon, sign, bootstrap-shift and permutation tests on more than 500 million p-values, making it also a 10-fold increase over the largest study to date. Our analysis also comprises di erent e ectiveness measures"
991,"Our results con rm that the sign and Wilcoxon tests are unreliable for hypotheses about mean e ectiveness, specially with large sample sizes. One could argue that this is because they do not test hypotheses about means, but about medians. However, because of the symmetricity assumption, they are legitimately used as alternatives to the t-test with less strict assumptions. As suggested by previous research, the t-test and permutation test largely agree"
992,
993,"with each other, but the t-test appears to be slightly more robust to variations of sample size. On the other hand, the bootstrap-shift test is shown to have a clear bias towards small p-values. While this leads to higher power, our results con rm that it also has higher Type I error rates, so there is virtually no gain over the t or permutation tests. This bias decreases with sample size, so the only situation favorable to the bootstrap appears to be that of"
994,"An advantage of our study is that it allows us to move past the typical discordance ratios used so far in the literature to compare tests. As discussed here and in previous work, without knowledge of the null hypothesis these discordances do not imply Type I errors. Thanks to our methodology based on simulation, we computed actual Type I error rates and found that both the t-test and the permutation test maintain errors at the nominal  level remarkably well, and they do so across measures and sample sizes. The tests are not being too conservative as previously suggested, and even though some e ectiveness measures are indeed more unstable than others, that does not imply a higher error rate in the tests."
995,"Based on our ndings, we strongly recommend the use of the ttest for typical hypotheses pertaining to mean system e ectiveness, and the permutation test for others. We provide further evidence that the bootstrap-shift test, although with nice theoretical properties, does not behave well unless the sample size is large, so we also recommend its discontinuation."
996,
997,513
998,
999,"Session 5B: Efficiency, Effectiveness and Performance"
1000,
1001,"SIGIR '19, July 21­25, 2019, Paris, France"
1002,
1003,ACKNOWLEDGMENTS
1004,Work carried out on the Dutch national e-infrastructure
1005,Cooperative) and funded by European Union's H2020 programme
1006,(770376-2­TROMPA).
1007,"Whichever tree, plant, or bird you're now part of, thank you, Mom."
1008,REFERENCES
1009,"[1] Omar Alonso and Stefano Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Information Processing & Management 48, 6"
1010,"[2] Peter Bailey, Nick Craswell, Ian Soboro , Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. 2008. Relevance Assessment: Are Judges Exchangeable and Does it Matter?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 667­674."
1011,"[3] Ben Carterette. 2012. Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments. ACM Transactions on Information Systems 30, 1"
1012,[4] Ben Carterette. 2015. Bayesian Inference for Information Retrieval Evaluation. In International Conference on the Theory of Information Retrieval. 31­40.
1013,"[5] Ben Carterette. 2017. But Is It Statistically Signi cant?: Statistical Signi cance in IR Research, 1995-2014. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 1125­1128."
1014,"[6] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas, Javed A. Aslam, and James Allan. 2008. Evaluation Over Thousands of Queries. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 651­658."
1015,"[7] William J. Conover. 1973. On Methods of Handling Ties in the Wilcoxon SignedRank Test. J. Amer. Statist. Assoc. 68, 344"
1016,"[8] William J. Conover. 1999. Practical Nonparametric Statistics. Wiley. [9] William J. Conover, Oscar Wehmanen, and Fred L. Ramsey. 1978. A Note on"
1017,"the Small-Sample Power Functions for Nonparametric Tests of Location in the Double Exponential Family. J. Amer. Statist. Assoc. 73, 361"
1018,
1019,"[22] Edwin J.G. Pitman. 1937. Signi cance Tests Which May be Applied to Samples From any Populations. Journal of the Royal Statistical Society 4, 1"
1020,"[23] John W. Pratt. 1959. Remarks on Zeros and Ties in the Wilcoxon Signed Rank Procedures. J. Amer. Statist. Assoc. 54, 287"
1021,[24] Stephen Robertson and Evangelos Kanoulas. 2012. On Per-Topic Variance in IR Evaluation. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 891­900.
1022,[25] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics Based on the Bootstrap. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 525­532.
1023,"[26] Tetsuya Sakai. 2014. Statistical Reform in Information Retrieval? ACM SIGIR Forum 48, 1"
1024,"[27] Tetsuya Sakai. 2016. Statistical Signi cance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 5­14."
1025,"[28] Tetsuya Sakai. 2016. Topic Set Size Design. Information Retrieval 19, 3"
1026,[29] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 1045­1048.
1027,"[30] Mark Sanderson, Andrew Turpin, Ying Zhang, and Falk Scholer. 2012. Di erences in E ectiveness Across Sub-collections. In ACM International Conference on Information and Knowledge Management. 1965­1969."
1028,"[31] Mark Sanderson and Justin Zobel. 2005. Information Retrieval System Evaluation: E ort, Sensitivity, and Reliability. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 162­169."
1029,"[32] Jacques Savoy. 1997. Statistical Inference in Retrieval E ectiveness Evaluation. Information Processing and Management 33, 4"
1030,"[33] Mark D. Smucker, James Allan, and Ben Carterette. 2007. A Comparison of Statistical Signi cance Tests for Information Retrieval Evaluation. In ACM International Conference on Information and Knowledge Management. 623­632."
1031,"[34] Mark D. Smucker, James Allan, and Ben Carterette. 2009. Agreement Among Statistical Signi cance Tests for Information Retrieval Evaluation at Varying Sample Sizes. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 630­631."
1032,"[35] Student. 1908. The Probable Error of a Mean. Biometrika 6, 1"
1033,"Statistical Assumptions via Stochastic Simulation. Information Retrieval Journal 19, 3"
1034,
1035,514
1036,
1037,

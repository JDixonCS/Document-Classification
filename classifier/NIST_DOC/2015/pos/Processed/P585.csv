,sentence,label,data
0,A Probabilistic Model for Information Retrieval Based on Maximum Value Distribution,null,null
1,Jiaul H. Paik,null,null
2,"University of Maryland, College Park, USA",null,null
3,jia.paik@gmail.com,null,null
4,ABSTRACT,null,null
5,"The main goal of a retrieval model is to measure the degree of relevance of a document with respect to the given query. Probabilistic models are widely used to measure the likelihood of relevance of a document by combining within document term frequency and term specificity in a formal way. Recent research shows that tf normalization that factors in multiple aspects of term salience is an effective scheme. However, existing models do not fully utilize these tf normalization components in a principled way. Moreover, most state of the art models ignore the distribution of a term in the part of the collection that contains the term. In this article, we introduce a new probabilistic model of ranking that addresses the above issues. We argue that, since the relevance of a document increases with the frequency of the query term, this assumption can be used to measure the likelihood that the normalized frequency of a term in a particular document will be maximum with respect to its distribution in the elite set. Thus, the weight of a term in a document is proportional to the probability that the normalized frequency of that term is maximum under the hypothesis that the frequencies are generated randomly. To that end, we introduce a ranking function based on maximum value distribution that uses two aspects of tf normalization. The merit of the proposed model is demonstrated on a number of recent large web collections. Results show that the proposed model outperforms the state of the art models by significantly large margin.",null,null
6,Categories and Subject Descriptors,null,null
7,H.3.3 [Information Systems]: Information Search and Retrieval: Retrieval Models,null,null
8,General Terms,null,null
9,Algorithm; Experimentation; Performance,null,null
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",null,null
11,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,null,null
12,DOI: http://dx.doi.org/10.1145/2766462.2767762.,null,null
13,Keywords,null,null
14,Document ranking; Retrieval model; Extreme value theory,null,null
15,1. INTRODUCTION,null,null
16,"To measure the weight of a term in a document, most well known functions combine three major components the term frequency, the inverse document frequency and the document length. The term frequency factor is a key evidence for determining a term's salience in a document, while inverse document frequency is used for attenuating the effect of terms that occur too often in the collection to be meaningful for relevance determination. On the other hand, term frequency is closely related with document length, since long documents tend to use a term repeatedly. Thus, term frequency normalization, in accordance with the document length, is necessary to remove the advantage that the long documents have in retrieval over the short documents.",null,null
17,"Given these three major components, the key question is then how these components can be integrated to produce a composite weight for each query term in each document and that is where one model differs from the other. Most well known weighting functions under the vector space model compute the composite weight by taking the product of the tf factor and the idf factor, where the tf factor is some combination of tf and the document length. Classical probabilistic models (for example BM25 [24]), adopt somewhat the same strategy. Although, they have the same objective, the two models have very different ways of determining the functional form of the tf factor. The nature of the tf functions under the vector space framework are generally constructed empirically, which are primarily guided by the experimental results, while BM25 formula is derived by approximating the logarithm of odds ratio of two Poisson distributions- one for relevant documents and the other for non-relevant documents. On the other hand, language models (LM) [21] differ from the above models in a fundamental way in the sense that the documents are ranked based on the likelihood that the query has been generated from the document in consideration. In addition, unlike tf.idf models, language models do not use explicit length normalization. The length of the document is an integral part of the probability estimation. Non-parametric probabilistic models are also known to be very effective in information retrieval. One of the widely used non-parametric probabilistic model is divergence from randomness (DFR) [1] based approaches, where the term weight is computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution. One major deficiency with",null,null
18,585,null,null
19,"these models is that they consider only the document length normalized tf and ignore within document relative tf distributions. Recent research [20] shows that integration of within document relative tf into scoring model improves performance significantly. However, it is yet not clear how this variable can be added into the existing models formally.",null,null
20,"This article describes a probabilistic retrieval model that obviates empirical way of determining a ranking function, unlike existing tf-idf models [29, 20]. The model introduces a tf factor based on the distribution of maximum values of normalized tf. The model achieves a number of important goals. First, it integrates the recent multi-aspect tf normalization schemes into a probabilistic framework. Second, the model automatically factors in the distribution of normalized tf in a term specific way, unlike many standard models. Third, it uses a mixture of two maximum value distribution to better model distributions of terms having varying heaviness of tails. To the best of our knowledge, this work is the first to address the ranking problem using the distribution of maximum values.",null,null
21,"The effectiveness of the proposed model is evaluated on a number of recent web test collections containing millions of documents. We compare the performance of the proposed method to the state of the art representative baselines from tf-idf model, classical probabilistic model, language model and divergence from randomness model. Our primary experimental results show that the proposed model almost always outperforms the state of the art baselines by a significantly large margin. We carry out additional set of experiments to compare the performance of the proposed model against a log-logistic (LL) based model that uses multi-aspect tf normalization. Once again, the results suggest that the proposed model is often significantly better than LL model. Moreover, the results demonstrate that our model is more precise than the state of the art models, thereby making it a potential choice for web search.",null,null
22,"We organize the article as follows. Section 2 reviews the state of the art. The proposed approach is described in Section 3. The experimental setup is detailed in Section 4. In Section 5, we present the experimental results. Finally, we conclude in Section 6.",null,null
23,2. PRIOR WORK,null,null
24,"Modeling term weight is the central issue in an information retrieval system. Three widely used models in IR are the probabilistic models [23], the vector space model [28, 27], and the inference network based model [31]. Furthermore, probabilistic models can be broadly classified into three groups, namely the classical probabilistic model, language model and a non-parametric divergence from randomness model. A large number of instances of these models exist in the literature. In this section we mainly review the state of the art representatives from each of these categories.",null,null
25,2.1 Classical Probabilistic Model,null,null
26,"The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic models differ from one another. Since the introduction of full text search, a large number of weighting formulae have been developed that attempt to measure document relevance probabilistically and BM25 [22] seems to be the most effective weighting function from among them. BM25 model approximates the two Pois-",null,null
27,"son model of relevance. The approximation is done using a increasing asymptotic tf function. Although, structurally, BM25 and tf-idf functions are very similar (in the sense that they both use tf and idf factor), they differ in many respects. First, BM25 has a well grounded theory, while most of the tf-idf models have an empirical background. Second, anatomically, IDF factor of BM25 discounts the collection size by the document frequency of the term, which is different from the standard IDF factor. Third, BM25 uses a different query term frequency function, unlike tf-idf models where that function is linear. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.",null,null
28,2.2 Language Model,null,null
29,"Probabilistic language modeling approaches [21, 15] follow a different principle in estimating the relevance of a document, unlike classical probabilistic models. Typically, language modeling approaches compute the probability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TF-IDF models, language modeling approaches do not explicitly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the length normalization factor and term specificity in pivoted normalization or BM25 model. Three major smoothing techniques (Dirichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [32].",null,null
30,"Although, query likelihood model is reasonably effective, one major deficiency with using a multinomial distribution as a language model is that all term occurrences are treated independently. The term-independence assumption in information retrieval is often adopted in theory and practice, as it renders the retrieval problem tractable. It is well known that once a term occurs in a document, it is more likely to reappear in the same document. This phenomenon is known as word burstiness [18] and is a type of dependency that is not modelled in the multinomial language model. Cummins et al. [8] present a Smoothed Polya Urn Document language model, which incorporates word burstiness only into the document model. They use the Dirichlet compound multinomial (DCM) to model documents in place of the standard multinomial distribution, whereas the standard multinomial is used to model query generation.",null,null
31,2.3 Divergence from Randomness Model,null,null
32,Amati and Rijsbergen [1] proposed a class of non-parametric probabilistic approaches to term weighting called divergence from randomness (DFR) model. The weight of a term in DFR models is the amount of divergence between a term distribution produced by a random process and the actual term distribution. The anatomy of the weighting function of DFR is defined as follows,null,null
33,"w(t, d) , -log2(P rob1) · (1 - P rob2).",null,null
34,(1),null,null
35,"The left factor measures the information content of the term in a document based on its distribution in the entire collection, while the right factor measures the information gain of the term with respect to its occurrence in the elite set (set of documents that contains the term). P rob1 is com-",null,null
36,586,null,null
37,"puted using various well known distributions (such as BoseEinstein statistics, Poisson distributions etc), while P rob2 is measured using Laplace law of succession or the ratio of two Binomial distributions. Like other models, DFR models use the same basic components. However, the integration of various component are derived theoretically. DFR models use explicit length normalization and following standard practice, average document length is considered as the ideal document length.",null,null
38,2.4 Vector Space Models,null,null
39,"In vector space model, the search problem is viewed in a different way. Queries and documents are represented as the vectors of terms. To compute a score between a document and a query, the model measures the similarity between the query and document vector using cosine function. The central part of the vector space model is to determine the weight of the terms that are present in the query and the documents. Salton and Buckley [26] summarize a number of term weighting approaches which use various types of normalization. It is evident that document length is an important component in effective term weighting. Singhal et al. [29] identify a number of weaknesses of cosine and maximum tf normalization and they observe that a weighting formula that retrieves documents with chances similar to their probability of relevance performs better. Following this observation, they propose a pivoted normalization scheme that acts as a correction factor of old normalization and is one of the most effective term weighting schemes in the vector space framework. Typically, the term weighting functions in vector space model are constructed empirically. Several work tried to go beyond purely empirical approaches and use the data instead to learn the patterns that satisfy the data. For example, Greiff [12] uses exploratory data analysis to uncover some important relationship between the document frequency and the relevance of a document.",null,null
40,"Most of the earlier work on vector space model normalizes the term frequency in accordance with the length of the documents. Paik [20] argued that the length based normalization alone is not sufficient to capture the different aspects of term salience and that within document distribution of the terms plays an important role. He then proposed a two-aspect normalization scheme. An asymptotic bounded increasing function (much in spirit with BM tf function) is then used to transform the normalized tf values. Two tf components are then combined using query length information. However, the main weakness of the model is its highly empirical nature and that is where the model proposed in this article differs from [20]. The proposed model has a formal probabilistic foundation that directly produces the weighting function.",null,null
41,2.5 Other Models,null,null
42,"In inference network, document retrieval is modeled as an inference process [31]. A document instantiates a term with a certain strength and given a query, the credit from multiple terms is accumulated to compute a relevance, which is very much equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.",null,null
43,"Some models go beyond the use of bag of words features only and incorporates the proximity/phrases of query terms in the documents [6, 9]. Metzler and Croft [19] develop a general formal framework for modeling term dependencies via Markov Random Fields. The model allows arbitrary text features, such as occurrence of single term, ordered phrases and unordered phrases to be incorporated as the potential evidences of relevance. They explore full independence (bag of words) , full dependence (between every pair of query terms) and sequential dependence (between consecutive query terms) in the language modeling framework. Since, the model has to compute the positional information during query processing time, it is more computationally complex than our model.",null,null
44,"Fang et al. [10] give a comprehensive analysis of four retrieval models by defining a set of constraints that needs to be satisfied for effective retrieval. Using these constraints the strengths and weaknesses of some well known models are analyzed and some of the models are modified. There are also a number of recent works that focus on the constraint based analysis of the retrieval models [4, 7].",null,null
45,3. PROPOSED WORK,null,null
46,"In this section we describe the proposed ranking model. We first revisit the key variables used in a typical ranking model and describe the roles they play. We then describe how maximum value can be used for ranking. Finally, we turn on to present the maximum value based models and their parameter estimation.",null,null
47,3.1 TF-IDF Model: A Probabilistic View,null,null
48,"Within document Term frequency and inverse document frequency (idf) are the two main building blocks of information retrieval models that measure query-document similarity. These two variables play a complementary role in ranking documents in response to a query. The idf factor of a term t (idf (t)) measures the information gain of randomly picking a document that will fall in the elite set for t (the set of document that contains t and henceforth we denote it as E(t)). On the other hand, tf factor of t for a document d, (tf f (t, d)) measures the relative weights of documents within E(t). Thus, from an operational perspective, idf (t) balances the weight between different E(t), while tf f (t, d) adjusts the relative weights of documents within the same elite set. Term frequency hypothesis suggests that tf f (t, d) is an increasing function of normalized term frequency. Intuitively, this means, if the rank of a document d having ntf (t, d) (normalized tf of t in d) is relatively high in E(t), the contribution made by tf f (t, d) is also high. Hence, given the distribution of normalized tf of a term in E(t), a natural way to measure tf f (t, d) is to take the percentage of documents in E(t) having normalized tf not higher than ntf (t, d). Thus, tf f (t, d) can be defined as follows:",null,null
49,"tf f (t, d)  P (X  ntf (t, d))",null,null
50,(2),null,null
51,where X is the random variable on normalized tf values in E(t).,null,null
52,"Lv and Zhai [17] argued that straightforward non-parametric (plain percentile based) way of estimating this probability does not fully factor in the main objective of tf hypothesis, since it ignores the quantum of differences of normalized tf values. Thus, they advocate the use of parametric probability distribution functions to circumvent this limitation.",null,null
53,587,null,null
54,"They use log-logistic distribution for computing tf f (t, d) as follows",null,null
55,"tf f (t, d) ,"" P (X  ntf (t, d)|c, ) "","" F (ntf (t, d)|c, ) ntf (t, d)""",null,null
56,","" c + ntf (t, d) (3)""",null,null
57,"where c > 0 and  > 0 are the model parameters which can be estimated from the normalized tf values in E(t). The main issue in this approach is to choose the right distribution function that captures the distribution of normalized values properly. We use maximum value distribution of two aspect normalized tf values in the above framework to measure the tf f (t, d). In the next two sections, we describe multi-aspect tf normalization scheme followed by the maximum value based model.",null,null
58,3.2 Term Frequency Normalization,null,null
59,"Raw term frequencies are known to be less effective because of its correlation with the document length. Thus, a long document enjoys preference over a short document if the term frequency is used as is. A document becomes longer if it contains many unrelated contents together. Therefore, although the frequency of a term may not increase in this case, the document uses many distinct terms. Since, the chance of a random match of a term between a query and a document is approximately proportional to the number of distinct terms in the document, long documents get an additional advantage over shorter documents. On the other hand, documents also become longer if they repeat the same content, thereby resulting in higher term frequencies without giving any additional useful information.",null,null
60,"Therefore, to enhance retrieval accuracy, it is imperative to regularize the term frequency in accordance with the document length. A standard and successful approach for doing this is to compare the length of the concerned document to the length of an ideal document (pivotal document). Both, pivoted tf-idf and BM25 effectively use this strategy where the length of the pivotal document is the average document length of the retrieval collection. Thus, the tf of an average length document remain unchanged, while tf of the documents longer (shorter) than average length document are punished (rewarded).",null,null
61,"Recently, Paik [20] argued that the traditional length based normalization alone is not sufficient to capture the different aspects of term importance and proposed two normalization formulae- one is based on within document average term frequency, while the other makes use of the traditional length based approach. These two normalized tf s are then combined. We use the same normalization schemes as described in [20], since it gives state of the art results. For convenience, the normalization factors are called ritf (t, d) (relative intra-document frequency of term t in the document d) and lrtf (t, d) (length normalized frequency of term t in the document d). The following equations formally define the normalization schemes.",null,null
62,"ritf (t, d) ,"" log(1 + tf (t, d))""",null,null
63,(4),null,null
64,log(k + mtf (d)),null,null
65,adl,null,null
66,"lrtf (t, d) ,"" tf (t, d) log(1 + )""",null,null
67,(5),null,null
68,l(d),null,null
69,"The terms mtf , adl and l(d) denote the mean term frequency of the document that contains t, the average docu-",null,null
70,"ment length of the collection and the length of the document d, and k ( 1) is a smoothing parameter. The proposed model combines these frequency normalizations in a probabilistic framework.",null,null
71,3.3 Limitations of Existing Models,null,null
72,In the last section we have described multi-aspect tf nor-,null,null
73,malization scheme. In this section we discuss the potential,null,null
74,limitations of existing methods and the major difficulties in,null,null
75,integrating multi-aspect tf normalization into the state of,null,null
76,the art probabilistic models.,null,null
77,We start our discussion with the MATF model. We reiter-,null,null
78,"ate that, although, idf function does not vary much from one",null,null
79,"model to the other, it is the tf function that often makes",null,null
80,the main difference.,null,null
81,In,null,null
82,"[20],",null,null
83,the,null,null
84,function,null,null
85,x 1+x,null,null
86,is,null,null
87,used,null,null
88,to,null,null
89,transform the normalized tf values to enforce term cover-,null,null
90,"age. However, the function has a number of notable short-",null,null
91,"comings. First, the choice of the function is purely empirical",null,null
92,"in nature. Second, the function does not have the knowl-",null,null
93,"edge of the distribution of tf in the elite set. Third, since",null,null
94,the function operates on the tf values having incompatible,null,null
95,"range (range of ritf is much smaller than that of lrtf ), one",null,null
96,"component overpowers the other component, thereby com-",null,null
97,promising the ultimate effectiveness.,null,null
98,BM25 model is a nice bridge between tf.idf and probabilis-,null,null
99,"tic model. Anatomically, BM25 is clearly separable into tf",null,null
100,"and idf component, where the tf function is a special case of",null,null
101,log-logistic model and is guided by 2-Poisson model. BM25,null,null
102,normalizes tf in accordance with the document length where,null,null
103,average document length is used as an ideal (or pivotal) doc-,null,null
104,"ument. However, it is not clear how to integrate relative",null,null
105,"intra-document tf into this model, since the notion of pivot",null,null
106,"for relative intra-document tf is hard to define. Moreover,",null,null
107,BM25's tf function is also distribution independent.,null,null
108,"Unlike the previous two models, divergence from random-",null,null
109,ness model (DFR) takes a more principled approach in terms,null,null
110,"of factoring in the term distribution. Once again, it is yet",null,null
111,unknown how relative intra document tf (ritf (t)) can be,null,null
112,added to this model that will be theoretically consistent with,null,null
113,"DFR's basic principle. Moreover, normalized tf values are",null,null
114,"continuous valued random variable and thus, an attempt to",null,null
115,"integrate it into DRF will give rise to theoretical anomaly,",null,null
116,since DFR uses discrete distributions to measure informa-,null,null
117,tion gain.,null,null
118,Language model is very different from all the models dis-,null,null
119,cussed above primarily because it neither uses idf explicitly,null,null
120,nor it uses length normalization. Thus we confine our dis-,null,null
121,cussion on the models that have explicit tf and idf factors.,null,null
122,In the next section we describe the maximum value based,null,null
123,model and how it can be used to circumvent some of the,null,null
124,"problems outlined above, followed by the development of a",null,null
125,model that uses two aspect tf normalization in a probabilis-,null,null
126,tic framework.,null,null
127,3.4 Maximum Value Model,null,null
128,"Unlike existing ranking models, we attempt to measure tf f (t, d) based on the nature of some of the largest values of normalized tf for that term. A natural consequence of using maximum value based ranking is that it makes the weight of a term in a document dependent upon the distribution of normalized tfs in E(t).",null,null
129,"To that direction, the simplest possible approach could be to take the maximum value of normalized tf for a term t",null,null
130,588,null,null
131,"and then measure tf f (t, d) relative to the maximum value. Clearly, this scoring is perfectly consistent with tf hypothesis, where the document having highest normalized tf gets highest weight. We can easily think of two naive approaches to measure tf f (t, d) that are based on maximum values. One potentially feasible approach can be percentile based scoring that we have outlined before, while the other simple approach can be to measure tf f (t, d) as a ratio of ntf (t, d) (or some increasing function of ratio) and the maximum normalized tf for that term in the collection. To understand the limitations of these two approaches, let us consider the following examples.",null,null
132,"Let x1, x2, . . . , xn-1, xn be the normalized tf values for a term t in ascending order. As our first case, let us assume that (xn - xn-1)  0. The percentile based method may give higher weight for xn compared to xn-1 even if they are nearly the same. This happens because percentile based method does not factor in the magnitude of difference, which consequently violates the tf hypothesis. As a second case, if it happens that xn xn-1, scoring based on ratio gives too much priority on the maximum value alone, which results in sharp discount of scores of other documents. As a consequence, a document even if genuinely relevant, is undesirably punished.",null,null
133,"These problems are addressed using a sampling based technique which exclusively focuses on maximum values of samples. Rather than relying on a single value, we attempt to measure the distribution of values at the right tail where some of the largest values fall. Hence, our main goal is to model the nature of the right tail of ntf (t, d). We hypothesize that the most potentially relevant documents for a term fall on that part of the distribution. Quite clearly, this hypothesis is consistent with the standard tf hypothesis. Thus, the main challenge is to model the nature of the right most tail as accurately as possible. In other words, this model measures the likelihood that ntf (t, d) will fall on the right most tail. Thus, if the probability is higher, likelihood of d being relevant will also be higher.",null,null
134,"We now focus on the models for maximum values. We reiterate that in order to avoid the influence of a single quantity (maximum value), the following sampling based approach is taken to derive maximum value distributions. Let us assume that N samples, each of size n are drawn from the same population. From each sample we can get the largest value. Thus in nN observations we have N largest values corresponding to each random sample. The distribution of the largest values in nN observations will tend to follow the same asymptotic expression as the distribution of the largest value in samples of size n. Consequently, the asymptote must be such that the largest value of a sample of size n taken from it must have the same asymptotic distribution. Formally, the maximum value distribution is defined as follows. Let X1, X2, . . . , Xn be independent and identically distributed random variable with distribution F . Let Mn ,"" max(X1, X2, . . . Xn). Then,""",null,null
135,"P r(Mn  x) ,"" P r(X1  x, X2  x . . . Xn  x) (6)""",null,null
136,", F n(x)",null,null
137,(7),null,null
138,"Since a linear transformation does not change the form of the distribution, the probability that the largest value is less than x should be equal to the probability of a linear function",null,null
139,prob,null,null
140,0.6 mitchell travel,null,null
141,0.5,null,null
142,0.4,null,null
143,0.3,null,null
144,0.2,null,null
145,0.1,null,null
146,0,null,null
147,1,null,null
148,2,null,null
149,3,null,null
150,4,null,null
151,5,null,null
152,6,null,null
153,7,null,null
154,8,null,null
155,9,null,null
156,10,null,null
157,normalized tf,null,null
158,Figure 1: Distributions of random samples of normalized elite set term frequency of mitchell and travel.,null,null
159,"of x. Thus, the above equation is equivalent to",null,null
160,P r( Mn - bn an,null,null
161," x) , F n(anx + bn).",null,null
162,(8),null,null
163,"Fisher-Tippett-Gnedenko theorem [11] states that if a pairs of real numbers (an, bn) (an and bn must be functions of n) exist such that an > 0 and",null,null
164,lim F n(anx + bn)  D(x),null,null
165,(9),null,null
166,n,null,null
167,"for a distribution F , then D(x) can be Type I or Type II distribution defined below.",null,null
168,The type I distribution [13] (known as Gumbel distribution) is defined as,null,null
169,x-µ,null,null
170,"Fg(x) ,"" exp(- exp(-  )), µ  R;  > 0.""",null,null
171,(10),null,null
172,while type II distribution [13] (Frechet distribution) for pos-,null,null
173,itive random variable is defined as,null,null
174,µ,null,null
175,"Ff (x) ,"" exp(- x ), x  0; µ > 0;  > 0.""",null,null
176,(11),null,null
177,"Having defined the maximum value distribution, our next major goal is to verify that the maximum value distributions satisfy the mandatory preconditions in order to be applicable in our task. Specifically, the data must be coming from a distribution F that satisfies Fisher-Tippett-Gnedenko theorem. Thus, our primary goal is to fix the underlying distribution function from which the data have been supposedly generated. In order to guess F , we first examine the distributions of normalized frequencies for a few randomly chosen terms. We noticed that the density graphs near the extreme right tail are not monotonically decreasing and it happens primarily because of the presence of random noise or extreme outliers. We empirically (by plotting) identify the points at which the density graphs violate this smoothness for the first time and ignore all the data larger than this particular point. On Clueweb collections, our analysis suggests that normalized tf values between 70-80 seem to be",null,null
178,589,null,null
179,"a reasonable cut-off point and thus, in our experiments we set it to 75 empirically (but that value may depend on the nature of the collection). We then plot the distributions of the truncated data. As an example, Figure 1 shows distributions for two selected terms. To better understand the relationship between the pattern of distributions and term's collection level occurrence, we choose two terms (""mitchell"" and ""travel"") of varying specificity. Figure 1 clearly shows that both the terms seem to be following long tail distributions with monotonically decreasing density functions. We consider two such long tail distributions ­ namely, exponential distribution and Pareto distribution. Note that the nature of the tails are different in these two cases.",null,null
180,Case 1.,null,null
181,"Suppose the data have been distributed from exponential distribution. Thus, F (x) ,"" 1 - exp(-x/),  > 0. If we choose an "", 1 and bn ,"" ln n, Then""",null,null
182,"F n(anx + bn) ,",null,null
183,-x - ln n n,null,null
184,1 - exp(-,null,null
185,),null,null
186,(12),null,null
187,exp(-x/) n,null,null
188,", lim 1 -",null,null
189,(13),null,null
190,n,null,null
191,n,null,null
192,", exp(- exp(-x/)).",null,null
193,(14),null,null
194,"Thus, if the data is generated from exponential distribution, for an , 1 and bn ,"" ln n, maximum value distribution converges to Gumbel distribution.""",null,null
195,Case 2.,null,null
196,"Suppose now the data have Pareto tail. Thus, 1 - F (x) ,"" cx- as x  , with c > 0 and  > 0. Again if we set""",null,null
197,1,null,null
198,"an , n  and bn ,"" 0, then for x > 0 we have""",null,null
199,"F n(anx) , 1 - c(anx)- n",null,null
200,(15),null,null
201,x- n,null,null
202,", lim 1 - c",null,null
203,n,null,null
204,n,null,null
205,(16),null,null
206,", exp(-( µ )) (setting c , µ) (17) x",null,null
207,"which turned out to be Frechet distribution. Hence, the above results provide us the necessary evidence that the maximum value distributions can be applied on our data.",null,null
208,Mixture Model.,null,null
209,"Although, Fg and Ff are the asymptotic approximations to maximum value models, the shapes of their distributions are not identical. Frechet distribution has longer right tail (Pareto tail) than Gumbel. This has some interesting corelation with the distribution of term frequencies in a large collection. If a term is more general (but not really stopwords), the frequency distribution for that term likely to have a longer tail than that of more specific term. Figure 1 illustrates this point clearly: the density curve of ""mitchell "" (which is a rare term) touches the x-axis much before that of ""travel "" (which is a more general term). Thus, an attempt to model the distribution of a term using only one of Gumbel and Frechet may lead to lower accuracy. Any real query contains terms having varying collection frequency and this motivates us to use a weighted mixture of the two distributions. Thus, our resulting distribution is defined as",null,null
210,"G(x) ,"" p · Fg(x) + (1 - p) · Ff (x), 0 < p < 1 (18)""",null,null
211,"where p can be considered as prior of Fg(x). A straightforward way to estimate p is to use a standard method such as gradient ascent method that directly optimizes a target retrieval metric (such as NDCG@20). Indeed, we adopt such an approach, but not directly on p. As we have discussed earlier, Fg (Gumbel) distribution seems better in modeling the distribution of a term having relatively smaller df values (more specific). Thus, instead of optimizing the value of p independently, we make the value of p dependent on df . Specifically, if a term has low df (high idf ) we give higher weight to Fg(x). In other words, p should be higher for high idf terms. We formalize this intuition using the following well known linear model",null,null
212,"p ,  · idf",null,null
213,(19),null,null
214,1-p,null,null
215,which gives the following solution for p,null,null
216,"p ,  · idf .",null,null
217,(20),null,null
218,1 +  · idf,null,null
219,where  (> 0) is a free parameter.,null,null
220,3.5 Scoring Function,null,null
221,"We are now ready to define our final scoring function. Our scoring function uses two aspect tf normalization in maximum value distribution framework. Formally, if X and Y be the random variables corresponding to ritf (t) and lrtf (t) in E(t) respectively, then tf f (t, d) is defined as",null,null
222,"tf f (t, d) ,"" ·P (X  ritf (t, d))+(1-)·P (Y  lrtf (t, d)) "",""  · G(ritf (t, d)) + (1 - ) · G(lrtf (t, d)) (21)""",null,null
223,"where 0 <  < 1, is the interpolation parameter. Consequently, the final scoring function for a query Q , q1q2 . . . qn and a document d is defined as",null,null
224,"S(Q, d) ,"" tf f (t, q) · idf (q)""",null,null
225,(22),null,null
226,qQ,null,null
227,"where idf (t) ,"" log(N/df (t)). The parameter   (0, 1) in Equation 21 is set empirically.""",null,null
228,3.6 Model Parameter Estimation,null,null
229,"In this section we detail our method for estimating the parameters of the two maximum value distribution models described in the last section. These parameters play important role in determining the actual shape of the distributions which in turn make them term dependent. There are many methods for parameter estimation including maximum likelihood estimation (MLE), which perhaps is an obvious choice. However, in our case, MLE does not seem to be a good choice for the reason we detail next.",null,null
230,"We explain the difficulty with Gumbel distribution only (similar argument holds for Frechet). The log-likelihood function of Gumbel based on random sample x1, x2, . . . , xn is given by",null,null
231,"L(, µ) , - n xi - µ - n ln  - n exp(- xi - µ ). (23)",null,null
232,"i,1",null,null
233,"i,1",null,null
234,The system of differential equations (used for MLE),null,null
235,"L , L , 0",null,null
236,(24),null,null
237,µ ,null,null
238,590,null,null
239,yields the following estimates for µ and ,null,null
240,"µ , (ln n - ln n exp(- xi ))",null,null
241,(25),null,null
242,"i,1",null,null
243,and,null,null
244,n,null,null
245,xi,null,null
246,exp(-,null,null
247,xi ,null,null
248,),null,null
249,x¯,null,null
250,",",null,null
251,+,null,null
252,"i,1 n",null,null
253,.,null,null
254,exp(-,null,null
255,xi ,null,null
256,),null,null
257,"i,1",null,null
258,(26),null,null
259,"Clearly, Equation 26 shows that  does not have closed form expression. Thus, we need to apply iterative numerical methods to find value of . Iterative methods may take substantial amount of time for very large collection such as Clueweb, since it needs to iterate over the set of maximum values from each random sample for each distinct term in the collection. This is precisely the reason we use point estimates (with a somewhat empirical transformation) of central tendencies for these models.",null,null
260,3.6.1 Parameter Estimation for Gumbel,null,null
261,The mean of Gumbel distribution is,null,null
262,µ + 0.57 · ,null,null
263,(27),null,null
264,while the standard deviation is,null,null
265, .,null,null
266,(28),null,null
267,6,null,null
268,To estimate the values of  and µ we equate them with,null,null
269,"corresponding sample mean and standard deviation, which",null,null
270,finally gives the following estimates.,null,null
271,6,null,null
272,6,null,null
273," , s and µ , x¯ - 0.58 · s",null,null
274,(29),null,null
275,"where x¯ and s are sample mean and standard deviation respectively. Since our data is positive random variable and originates from exponential distribution we use Equation 14 for final ranking. Thus, we do not need to worry about the parameter µ. Our only concern is the parameter . Surprisingly, point estimate of  as is does not perform well in practice. Thus, in practice, we use a linear transformation,  ,"" z1 + z2 · s, where z1 and z2 are set empirically to 2.5 and 0.04 respectively.""",null,null
276,3.6.2 Parameter Estimation for Frechet,null,null
277,Mean and variance for Frechet are defined respectively as,null,null
278,"µ(1 - 1/),  > 1",null,null
279,(30),null,null
280,and,null,null
281,"µ2((1 - 2/) - 2(1 - 1/)),  > 2.",null,null
282,(31),null,null
283,"Once again, the above two expressions are not very convenient to use since the improper integral (.) needs to be evaluated in order to compute the parameter. Fortunately, median and mode of Frechet distribution have much manageable expressions. Median is defined as",null,null
284,µ0.69-1/,null,null
285,(32),null,null
286,and mode is defined as,null,null
287,µ(1 + 1 )-1/.,null,null
288,(33),null,null
289,"As in Gumbel, we can equate these two expressions to sample median and mode to estimate the model parameters.",null,null
290,"However, unlike Gumbel, the parameters do not have closed form solution, which can be achieved by using any standard numerical method. Note that, in this case we do not need to iterate over the sample of maximum values, instead mode and median computed once for a term is enough. It is also important to note that although median for a sample is easy to determine, we need to do a little processing to compute mode from a set of real numbers. To compute mode of a sample, we create non-overlapping bins of numbers having 0.5 as the interval. We then take the median of the bin having highest frequency as our sample mode. We have adopted computationally efficient parameter estimation methods. However, a large number of other methods exist in the literature. Thus, it may be interesting to see whether other estimation strategies can improve the retrieval results without sacrificing efficiency too much.",null,null
291,4. EXPERIMENT SETUP,null,null
292,"In this section, we describe the experiment setup used to evaluate the proposed model. Our experiments have the following two major objectives.",null,null
293,1. To compare the performance of the model against the state of the art probabilistic models (Section 5.1).,null,null
294,2. To compare against a recently proposed multi-aspect tf-idf weighting scheme [20] (Section 5.2).,null,null
295,Table 1: Summary of the test collections and topics,null,null
296,used in our experiments. `M' stands for million.,null,null
297,Collection,null,null
298,# doc topics,null,null
299,# topics,null,null
300,Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009 Clueweb.A-09 & 10 Clueweb.A-11 & 12,null,null
301,50M 50M 50M 500M 500M,null,null
302,1-100,null,null
303,100,null,null
304,101-200,null,null
305,100,null,null
306,20001-30000 684,null,null
307,1-100,null,null
308,100,null,null
309,101-200,null,null
310,100,null,null
311,"We summarize the test collections used in our experiments in Table 4. The test collections are taken from TREC web tasks of recent years (2009-2012) as well as from million query 2009 (MQ-2009). The collections contain web documents and real web queries sampled from a search engine log. The documents are crawled from web and hence they have variety of content quality. Clueweb.B collection contains nearly 50 million documents, while ClueWeb.A collection contains approximately 500 million web pages. In MQ-2009 collection, although many queries available, not all queries have been judged. Thus, we use 684 queries for which judgments are available. All the collections have graded relevance assessment. It is important to note that, MQ-2009 queries have incomplete relevance assessment. Therefore, our evaluation methodology skips the unjudged documents from the ranked lists in order to compute the values of well known metrics following the recommendation made in [25].",null,null
312,Documents and queries are stemmed via Porter stemmer. Stopwords are removed from documents and queries. Statistically significant performance differences are determined using a paired t-test at 95% confidence level (p < 0.05). All our experiments are done using title field of the topics.,null,null
313,591,null,null
314,Table 2: Retrieval effectiveness of the proposed method (MVD) compared to probabilistic models. Statisti-,null,null
315,cally significant improvements are indicated using the first letter of the less effective method. The highest,null,null
316,"value per column is boldfaced. The numbers in parenthesis indicate relative improvement over LM, PL2 and",null,null
317,"BM25, respectively.",null,null
318,Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009,null,null
319,Clueweb.A-09 & 10 Clueweb.A-11 & 12,null,null
320,LM 0.309,null,null
321,0.264,null,null
322,0.367,null,null
323,0.254,null,null
324,0.219,null,null
325,PL2 0.312,null,null
326,0.263,null,null
327,0.373,null,null
328,0.256,null,null
329,0.219,null,null
330,ERR@20,null,null
331,BM25 0.306 MVD 0.337lpb,null,null
332,0.253 0.286lpb,null,null
333,0.372 0.408lpb,null,null
334,0.248 0.286lpb,null,null
335,0.221 0.257lpb,null,null
336,"(8.9, 7.9, 9.9)",null,null
337,"(8.2, 8.8, 12.9)",null,null
338,"(11.2, 9.5, 9.8) (12.7, 11.4, 15.0) (17.8, 17.5, 16.4)",null,null
339,NDCG@10,null,null
340,LM PL2 BM25 MVD,null,null
341,"0.282 0.285 0.284 0.332lpb (17.9, 16.5, 16.8)",null,null
342,"0.228 0.231 0.222 0.268lpb (17.3, 16.1, 20.4)",null,null
343,"0.395 0.393 0.391 0.422lpb (7.0, 7.4, 7.9)",null,null
344,"0.200 0.205 0.208 0.261lpb (30.7, 27.0, 25.3)",null,null
345,"0.194 0.196 0.191 0.231lpb (19.0, 17.8, 21.3)",null,null
346,NDCG@20,null,null
347,LM PL2 BM25 MVD,null,null
348,"0.275 0.278 0.280 0.325lpb (18.4, 17.0, 16.4)",null,null
349,"0.228 0.228 0.225 0.265lpb (15.9, 16.2, 17.8)",null,null
350,"0.459 0.458 0.453 0.479b (4.5, 4.5, 5.8)",null,null
351,"0.193 0.195 0.208 0.248lpb (28.7, 27.4, 19.0)",null,null
352,"0.196 0.198 0.186 0.228lpb (16.4, 15.2, 22.7)",null,null
353,4.1 Baselines,null,null
354,"The performance of the proposed model is compared to a number of state of the art retrieval models from different families. BM25 [24] is chosen as the representative baseline from the classical probabilistic model. From language model, we choose Dirichlet smooth version [32], since it is known to be the most effective among the language models [10]. From divergence from randomness family, we choose PL2 [1] as the baseline, following recent work [10, 14].",null,null
355,"Pivoted document length normalization is chosen as a basic TF-IDF baseline. MATF [20] is chosen as another state of the art tf-idf model. Note that, MATF is a highly effective empirical tf-idf model and one of the major objectives of the proposed model is to advance the multi-aspect TF model using a probabilistic foundation. Finally, since our model attempts to capture the distribution of normalized tf, we also compare to multi-aspect TF normalization with a log-logistic distribution which has similar purpose. Thus, our set of baselines contains members from all state of the art families.",null,null
356,4.2 Free parameters and evaluation metrics,null,null
357,"All the baseline models (except MATF) and the proposed model contain one or more free parameters. It is important to note that the parameters of these models often influence the performance to a statistically significant degree. Hence, for the sake of reliable and competitive comparison, the parameters are optimized using 5-fold cross validation with the corresponding evaluation measure (ERR@20 [2], NDCG@10 [16] or NDCG@20) as the target metric.",null,null
358,"We choose expected reciprocal rank (ERR), NDCG@10, and NDCG@20 as our evaluation measures. ERR has been the primary evaluation metric for recent TREC web tracks [3]. NDCG@k leverages graded relevance and also has a position wise discounting. Thus, it reflects the overall quality of the documents at top k. On the other hand, ERR@k is a precision bias metric that leverages graded relevance assessment. Thus, ERR is more suitable metric for web search.",null,null
359,"Clueweb collections contain substantial number of spam documents. Thus, following previous work [5], we have filtered out spam documents from the collections. Specifically, documents assigned by Waterloo's spam classifier [5] with a score below 70 were filtered out from the initial corpus. The score indicates the percentage of all documents in ClueWeb that are presumably ""spammier"" than the document at hand. The models are then run on the residual corpus to produce final ranked lists.",null,null
360,5. RESULTS,null,null
361,In this section we summarize retrieval performance of the proposed method and the baseline methods. Throughout the result section MVD denotes the proposed model.,null,null
362,5.1 Comparison to Probabilistic Models,null,null
363,"Table 2 compares the performance of MVD to that of the three probabilistic models, namely, language model with Dirichlet prior, BM25 and PL2. First, we compare the performances measured by ERR@20. Table 2 shows that, on two Clueweb.B collections, MVD outperforms LM, PL2 and BM25 by a margin of 8% to 12% and all the differences are statistically significant. On MQ-2009 collection, MVD is once again always statistically significant compared to all the baselines with a margin more than 9%. Similarly, on two Clueweb.A datasets, MVD is unequivocally superior to the baselines and quite clearly the performance differences are even larger than that on Clueweb.B and MQ-2009. The baseline methods seem to be performing nearly equally and in none of the cases, the performance differences among the baselines found to be statistically significant.",null,null
364,"Our next goal is to analyze the results measured in terms of NDCG@10. Once again, MVD gives consistent performance improvement over LM, BM25 and PL2 on Clueweb.B collections. The performance differences are always statistically significant with more than 15% relative improvements. Results on MQ-2009 collection also show that MVD is significantly more effective than all the baselines, however the relative differences are smaller compared to Clueweb collec-",null,null
365,592,null,null
366,Table 3: Retrieval effectiveness of the proposed method (MVD) compared to tf-idf models. Statistically,null,null
367,significant improvements are indicated using the first letter of the less effective method. The highest value,null,null
368,"per column is boldfaced. The numbers in parenthesis indicate relative improvement over PIVOT, MATF and",null,null
369,"LL, respectively.",null,null
370,Clueweb.B-09 & 10 Clueweb.B-11 & 12 MQ-2009,null,null
371,Clueweb.A-09 & 10 Clueweb.A-11 & 12,null,null
372,PIVOT 0.263,null,null
373,0.234,null,null
374,0.367,null,null
375,0.169,null,null
376,0.196,null,null
377,MATF 0.283,null,null
378,0.282,null,null
379,0.388,null,null
380,0.227,null,null
381,0.251,null,null
382,ERR@20,null,null
383,LL MVD,null,null
384,0.290 0.337pml,null,null
385,0.275 0.286pl,null,null
386,0.391 0.408pml,null,null
387,0.244 0.286pml,null,null
388,0.240 0.257pl,null,null
389,"(27.9, 18.8, 16.0) (22.0, 1.5, 3.9)",null,null
390,"(11.2, 5.1, 4.4) (69.6, 26.1, 16.9) (31.2, 2.7, 7.1)",null,null
391,NDCG@10,null,null
392,PIVOT MATF LL MVD,null,null
393,"0.219 0.276 0.287 0.332pml (51.6, 20.5, 15.8)",null,null
394,"0.196 0.240 0.234 0.268pml (36.6, 11.4, 14.5)",null,null
395,"0.381 0.402 0.418 0.422pm (10.8, 5.0, 1.0)",null,null
396,"0.177 0.197 0.207 0.261pml (47.5, 32.3, 25.8)",null,null
397,"0.175 0.213 0.206 0.231pml (32.5, 8.7, 12.5)",null,null
398,NDCG@20,null,null
399,PIVOT MATF LL MVD,null,null
400,"0.212 0.286 0.284 0.325pml (53.5, 13.5, 14.4)",null,null
401,"0.200 0.243 0.235 0.265pml (32.0, 9.0, 12.7)",null,null
402,"0.442 0.466 0.477 0.479pm (8.3, 2.8, 0.4)",null,null
403,"0.181 0.202 0.201 0.248pml (37.2, 22.7, 23.0)",null,null
404,"0.171 0.209 0.198 0.228pml (33.2, 9.4, 15.0)",null,null
405,"tions. One reason for smaller difference is that the baseline NDCG@10 numbers are very high, which makes the relative improvements smaller. The effectiveness of MVD on Clueweb.A collections is even more encouraging. MVD surpasses the baselines on Clueweb.A-09 & 10 collection by more than 20% margin which is clearly highly significant. We observe similar trend on the other Clueweb.A collection. As in ERR@20, the baselines seem to be performing with equal effectiveness.",null,null
406,"We notice very similar (as in ERR@20 and NDCG@20) behavior of MVD on Clueweb.B collections measured by NDCG@20. Once again, MVD is consistently and significantly better than all the baselines with noticeably large margin of relative improvement. The picture is slightly different on MQ-2009 collection. Although, MVD is better than all the baselines, difference against BM25 only found to be significant. We suspect that sparser relevance judgements of MQ-2009 collection is a possible reason behind smaller differences. Finally, MVD beats the baselines by a convincingly large margin thereby maintaining its consistency as in the previous cases.",null,null
407,"Overall, the results indicate that the proposed model based on distribution of maximum values yields consistent and significant retrieval performance improvement over the three state of the art probabilistic baselines from different categories measured by NDCG measures. We conclude that the proposed model is significantly more precise than the baselines on all the collections, thereby making it a very suitable for web search. The experiments also reveal that the performance of the baselines are very similar to each other, irrespective of the collection, which corroborates earlier findings that if parameters of the models are properly optimized, language model, BM25 and divergence from randomness model are closely comparable.",null,null
408,5.2 Comparison to TF-IDF Models,null,null
409,"The experiments in this section are designed to compare the proposed method to a number of tf-idf models. By this set of experiments, we intend to achieve the following major goals.",null,null
410,1. How does the proposed model perform compare to a basic tf-idf model that uses only pivoted document length normalization?,null,null
411,"2. Since MVD is based on multi-aspect term frequency normalization in a new probabilistic framework, how does it compare against a recent tf-idf model (MATF) that introduced multi-aspect tf normalization? We reiterate that this is the main issue we sought to address using maximum value based model.",null,null
412,"3. We mentioned before that MATF combines the two normalized tf using an empirical tf function that transforms normalized tf values. Moreover, it does not factor in the distribution of normalized tf in the elite set for the particular term. Thus, in this section we compare the performance of MVD to a method that uses log-logistic probability distribution of two normalized tfs. The method is denoted as LL in the table. The parameters of this model is estimated using the method detailed in [17].",null,null
413,"Table 5.2 compares the performance of tf-idf methods and MVD. First, it is clear from the table that MVD is highly significantly better than PIVOT. This holds for all collection and measured by all three evaluation measures. The performance differences are unequivocally statistically significant. On Clueweb (both A and B) collections, MVD gives upto 50% relative improvement over PIVOT. Second, MATF, which is based on relative intra-document tf normalization and length based normalization (which we call multi-aspect tf normalization), is always poorer than MVD and the differences are almost always statistically significant. More importantly, the margin of improvement by MVD is often noticeably high.",null,null
414,"Thus, we conclude that maximum value distribution has large impact on retrieval performance. Finally, we compare the proposed method to log-logistic distribution based method denoted as LL in the table. Note that LL uses distribution of multi-aspect tf normalization for estimating rel-",null,null
415,593,null,null
416,evance and thus has probabilistic interpretation. Table 5.2 once again shows that MVD often significantly surpasses LL.,null,null
417,6. CONCLUSION,null,null
418,"In this paper we introduce a probabilistic information retrieval model. The proposed model is guided by the principle that given the normalized frequency of a term in a document, the score is proportional to the likelihood that the normalized tf is maximum with respect to its distribution in the elite set for the corresponding term. We use a mixture of two maximum value distribution, that factors in varying specificity of query terms. The proposed model, integrates multi-aspect tf normalization scheme proposed recently in a probabilistic framework. Unlike many existing models, the proposed model takes into account the term specific distribution in the elite set. However, the unique contribution is that the model measures the likelihood of relevance focussing on the maximum values of the distribution, which we believe the first such effort to view ranking problem from this perspective. An empirical evaluation on large web collections containing millions of documents and hundreds of real world web queries demonstrates that the model significantly outperforms the state of the art probabilistic models from different families. As a future work, we plan to incorporate term proximity (ordered and un-ordered bigram) information into our model.",null,null
419,Acknowledgments,null,null
420,I thank Doug Oard for useful discussions and suggestions. Without his support and advice this work would not have been possible. This research was supported in part by DARPA contract HR0011-12-C-0015 and NSF award 1065250.,null,null
421,7. REFERENCES,null,null
422,"[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 2002.",null,null
423,"[2] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In ACM CIKM, 2009.",null,null
424,"[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In TREC, 2011.",null,null
425,"[4] S. Clinchant and E. Gaussier. Retrieval constraints and word frequency distributions a log-logistic model for ir. Inf. Retr., 2011.",null,null
426,"[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval, 2011.",null,null
427,"[6] W. B. Croft, H. R. Turtle, and D. D. Lewis. The use of phrases and structured queries in information retrieval. In ACM SIGIR, 1991.",null,null
428,"[7] R. Cummins and C. O'Riordan. A constraint to automatically regulate document-length normalisation. In ACM CIKM, 2012",null,null
429,"[8] R. Cummins, J. H. Paik, and Y. Lv. A polya urn document language model for improved information retrieval. ACM Trans. Inf. Syst., 2015.",null,null
430,"[9] J. Fagan. Automatic phrase indexing for document retrieval. In ACM SIGIR, 1987.",null,null
431,"[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Trans. Inf. Syst., 2011.",null,null
432,"[11] R. A. Fisher and L. H. C. Tippett. Limiting forms of the frequency distribution of the largest or smallest member of a sample. In Mathematical Proceedings of the Cambridge Philosophical Society, 1928.",null,null
433,"[12] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In ACM SIGIR, 1998.",null,null
434,"[13] E. J. Gumbel. Statistics of extremes. Courier Dover Publications, 2012.",null,null
435,"[14] B. He and I. Ounis. A study of the dirichlet priors for term frequency normalisation. In ACM SIGIR, 2005.",null,null
436,"[15] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsimonious language models for information retrieval. In ACM SIGIR, 2004.",null,null
437,"[16] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4), Oct. 2002.",null,null
438,"[17] Y. Lv and C. Zhai. A log-logistic model-based interpretation of tf normalization of bm25. In ECIR, 2012",null,null
439,"[18] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word burstiness using the dirichlet distribution. In ICML, 2005.",null,null
440,"[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In ACM SIGIR, 2005.",null,null
441,"[20] J. H. Paik. A novel tf-idf weighting scheme for effective ranking. In ACM SIGIR, 2013.",null,null
442,"[21] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In ACM SIGIR, 1998.",null,null
443,"[22] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4), Apr. 2009.",null,null
444,[23] S. E. Robertson. Readings in information retrieval. chapter The probability ranking principle in IR. 1997.,null,null
445,"[24] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In ACM SIGIR, 1994.",null,null
446,"[25] T. Sakai. Alternatives to bpref. In ACM SIGIR, 2007. [26] G. Salton and C. Buckley. Term-weighting approaches",null,null
447,"in automatic text retrieval. Inf. Process. Manage., 24(5), Aug. 1988. [27] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., 1986. [28] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11), Nov. 1975. [29] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In ACM SIGIR, 1996. [30] K. Sparck Jones. Document retrieval systems. chapter A statistical interpretation of term specificity and its application in retrieval. 1988. [31] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3), July 1991. [32] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), Apr. 2004.",null,null
448,594,null,null
449,,null,null

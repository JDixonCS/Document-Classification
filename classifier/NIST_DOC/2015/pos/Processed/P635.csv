,sentence,label,data
0,An Entity Class-Dependent Discriminative Mixture Model for Cumulative Citation Recommendation,null,null
1,Jingang Wang,null,null
2,School of Computer Science Beijing Institute of Technology,null,null
3,bitwjg@bit.edu.cn,null,null
4,Zhiwei Zhang,null,null
5,Dept. of Computer Science Purdue University,null,null
6,zhan1187@purdue.edu,null,null
7,Dandan Song,null,null
8,School of Computer Science Beijing Institute of Technology,null,null
9,sdd@bit.edu.cn,null,null
10,Luo Si,null,null
11,Dept. of Computer Science Purdue University,null,null
12,lsi@purdue.edu,null,null
13,Chin-Yew Lin,null,null
14,Knowledge Mining Group Microsoft Research,null,null
15,cyl@microsoft.com,null,null
16,Qifan Wang,null,null
17,Dept. of Computer Science Purdue University,null,null
18,wang868@purdue.edu,null,null
19,Lejian Liao,null,null
20,School of Computer Science Beijing Institute of Technology,null,null
21,liaolj@bit.edu.cn,null,null
22,ABSTRACT,null,null
23,"This paper studies Cumulative Citation Recommendation (CCR) for Knowledge Base Acceleration (KBA). The CCR task aims to detect potential citations of a set of target entities with priorities from a volume of temporally-ordered stream corpus. Previous approaches for CCR that build an individual relevance model for each entity fail to handle unseen entities without annotation. A baseline solution is to build a global entity-unspecific model for all entities regardless of the relationship information among entities, which cannot guarantee to achieve satisfactory result for each entity. In this paper, we propose a novel entity class-dependent discriminative mixture model by introducing a latent entity class layer to model the correlations between entities and latent entity classes. The model can better adjust to different types of entities and achieve better performance when dealing with a broad range of entities. An extensive set of experiments has been conducted on TREC-KBA-2013 dataset, and the experimental results demonstrate that the proposed model can achieve the state-of-the-art performance.",null,null
24,Categories and Subject Descriptors,null,null
25,H.3.3 [Information Search and Retrieval]: Retrieval Models,null,null
26,This work was partially done when the first author was visiting Purdue University and Microsoft Research Asia. Corresponding Author,null,null
27,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09-13, 2015, Santiago, Chile.",null,null
28,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,null,null
29,DOI: http://dx.doi.org/10.1145/2766462.2767698.,null,null
30,Keywords,null,null
31,Cumulative Citation Recommendation; Knowledge Base Acceleration; Mixture Model; Information Filtering,null,null
32,1. INTRODUCTION,null,null
33,"In recent years, we have witnessed a proliferation of open domain Knowledge Bases (KBs) such as Freebase1 and Yago2. They have been used in many applications such as query answering, entity search and entity linking and have shown great promises. These KBs are usually organized around entities such as persons, organizations, locations, and so on. Currently, the maintenance of a KB mainly relies on human editors. However, with the explosion of information, largescale KBs are hard to be kept up-to-date solely by human editors. Taking English Wikipedia for example, there are approximately 4.7 million entities but merely 132,938 active editors3. The less popular entities cannot be updated in time because they are not spotlighted. As reported in [14], the median time delay between a cited document's publishing and its citation in Wikipedia is almost one year. An outdated KB severely limits the effectiveness of applications depending on it. This gap could be bridged if relevant documents of KB entries can be automatically detected as soon as they emerge online and then be recommended to the editors with various levels of relevance. This is called the Cumulative Citation Recommendation (CCR). Formally, given a KB entity, CCR is a task to filter highly relevant documents from a chronological stream corpus and evaluate their citation-worthiness to the target entity.",null,null
34,"Most previous approaches (e.g., [2, 25]) for CCR are highly supervised and require sufficient training data to build an individual relevance model for each entity. These approaches are infeasible when dealing with a large-scale KB, since the",null,null
35,1https://www.freebase.com/ 2http://www.mpi-inf.mpg.de/departments/ databases-and-information-systems/research/ yago-naga/yago/ 3http://meta.wikimedia.org/wiki/List_of_ Wikipedias#1_000_000.2B_articles,null,null
36,635,null,null
37,"labeling work is labor intensive. One solution is to build a global entity-unspecific discriminative model and optimize it to achieve an overall optimal performance for all entities [29, 36]. However, these models ignore the distinctions between different entities and learn a set of fixed model parameters for all entities, which leads unsatisfactory performance when dealing with a diverse entity set. For instance, it is not intuitve to apply the same discriminative model for Geoffery Hinton and Appleton Museum of Art. The former entity is a computer scientist, while the latter one is a museum. Nevertheless, the global model treats them equally without considering the prior entity class knowledge. We assume that entities from a same class have similar tastes and preferences when citing relevant documents, which means they have similar combination weights in the discriminative model. Therefore, for an entity with little training data, the training data of its similar entities from the same class can be utilized to learn the combination weights. In comparison to the global model, more accurate combination weights are learned for each entity by this manner.",null,null
38,"Based on this observation, we build an adaptive discriminative model for different types of entities by utilizing the underlying entity class information, i.e. entity class dependent discriminative mixture model. We introduce an intermediate latent entity class layer and define a joint distribution over the entity-document pairs and latent classes conditioned on the observations. The aim is to achieve relevance estimation through learning a mixture model which is expected to outperform the global model, while maintaining the capability to reveal the hidden correlations between entities and entity classes. The model can be viewed as a hierarchical combination of a discriminative component and a mixing component, so two types of features are required: entity-document features for the discriminative component and entity-class features for the mixing component.",null,null
39,"For the discriminative component, we develop a set of bursty features as temporal features in addition to semantic features. The bursty features are detected from two independent data sources: the stream corpus (internal) and certain third-party data (external) like Google Trends.",null,null
40,"For the mixing component, we explore two types of entityclass features to model the correlations between entities and hidden classes, including profile-based features and categorybased features. Profile-based features are constructed from the entity's profile in KB, while category-based features rely on the existing category labels for the entity in KB.",null,null
41,"To the best of our knowledge, this is the first research work that focuses on modeling correlations between entities and hidden entity classes in discriminative model for CCR. Our model is capable of tackling less popular entities with little training data and unseen entities that do not exist in the training set, which is indispensable in a practical CCR system. Empirical studies have been conducted on TREC-KBA-2013 dataset to show the effectiveness and robustness of the proposed mixture model. Experimental results demonstrate that our model achieves the state-ofthe-art performance on TREC-KBA-2013 dataset.",null,null
42,"The rest of this paper is organized as follows. Section 2 summarizes related works. Section 3 introduces an entity class-dependent discriminative mixture model for CCR. Section 4 describes features required in our model, especially the temporally bursty features and their detection methods. Section 5 presents the detailed experimental results and pro-",null,null
43,vides some discussion. Section 6 concludes this paper and points out possible future work.,null,null
44,2. RELATED WORK,null,null
45,"Although CCR was first proposed in TREC-KBA tracks, the similar research problem has been studied in several topics of information retrieval.",null,null
46,Topic/Event Detection and Tracking.,null,null
47,"Topic Detection and Tracking (TDT) is a track hosted by TREC from 1997 to 2004 [1]. A similar research topic in recent years is event detection. Both TDT and event detection are concerned with the development of techniques for finding and following events in broadcast news or social media. The techniques adopted for TDT and event detection can be broadly classified into two categories: (1) clustering documents based on the semantic distance between them [34], or (2) grouping the frequent words together to represent events [22]. In [22], a finite automaton model is proposed to detect events in stream by modeling events as state transitions. This method has been validated widely by lots of other studies [18, 17, 35]. We also adopt this model to detect KB entities' bursts in the stream corpus and then extract bursty features for them. Different from above works, we model entities' occurrences to capture bursty activities instead of words' occurrences. Another difference between CCR and TDT is that CCR needs to make fine-grained citation-worthiness distinctions between relevant documents further.",null,null
48,Cumulative Citation Recommendation.,null,null
49,"TREC has launched the KBA-CCR track since 2012. Participants treat CCR as either a ranking problem [3, 2, 4] or a classification problem [3, 5, 29]. Classification and Learning to Rank methods have been compared and evaluated [2, 15], and both of them can achieve the state-of-art performance with a powerful feature set. Several supervised learning techniques, such as SVM [21], language models [26, 10], Markov Random Fields [7], and Random Forests [4, 5, 29] are utilized. Meanwhile, a variety of relevance scoring methods have been tried, including standard Lucene scoring [6], and custom ranking functions based on entity cooccurrences [25]. A time-aware evaluation paradigm is developed to study time-dependent characteristics of CCR [9].",null,null
50,"However, some highly supervised methods require training instances for each entity to build a relevance model, limiting their scalabilities. Entity-unspecific methods, regardless of entity distinctions, are employed to address this problem [29, 28]. Nevertheless, characteristics of different entities are lost in the entity-unspecific methods. Some other researchers employ transfer learning techniques to learn across entities by using entity-unspecific meta-features [36], or utilize a semi-supervised approach to profile an entity by leveraging its related entities and weighting them with the training data [23]. These methods have demonstrated that the correlations between entities are useful for CCR. Nevertheless, all these methods are empirically designed and the performance can be improved further.",null,null
51,"What's more, query expansion is often employed because the name of the target entity is too sparse to be a good query. Other name variants and contextual information of terms or related entities from Wikipedia or from the document stream",null,null
52,636,null,null
53,"[7, 6] are used to enrich the semantic features of entities. In addition to semantic features, temporal features have been proved especially helpful in CCR [2, 5, 29].",null,null
54,Mixture Model.,null,null
55,"Mixture model has been proved effective to address the problem of data insufficiency in several information retrieval tasks, including expert search [11], federated search [19], collaborative filtering [20] and image retrieval [30]. By introducing latent layers to learn flexible combination weights for different feature vectors, mixture model can always outperform simple discriminative models with fixed combination weights. Hence, we propose an entity class-dependent discriminative mixture model to deal with the entities with little training data, which will be described in next section.",null,null
56,3. DISCRIMINATIVE MODEL FOR CCR,null,null
57,"This section proposes a novel learning framework by modeling each entity's distribution across hidden entity classes and combining it with a logistic regression model to form a final discriminative model. First we provide a formal definition of the research problem and model it as a classification task, and then present two discriminative models: a global model and an entity class-dependent mixture model.",null,null
58,3.1 Problem Statement,null,null
59,"We consider CCR as a binary classification problem that treats the relevant entity-document pairs as positive instances and irrelevant ones as negative instances. Many probabilistic classification techniques in the literature generally fall into two categories: generative models and discriminative models. Discriminative models have attractive theoretical properties [24] and generally perform better than their generative counterparts in the field of information retrieval [16, 32]. Therefore, we adopt discriminative probabilistic models in this paper.",null,null
60,"Given a set of KB entities E , {eu}(u ,"" 1,    , M ) and a document collection D "", {dv}(v ,"" 1,    , N ), our objective is to estimate the relevance of a document d to a given entity e. In other words, we need estimate the conditional probability of relevance P (r|e, d) with respect to an entity-document pair (e, d). Each entity-document pair (e, d) is represented as a feature vector f (e, d) "","" (f1(e, d),    , fK (e, d)), where K indicates the number of entity-document features. Moreover, to model the hidden entity class information, each entity can be represented as an entity-class feature vector g(e) "","" (g1(e),    , gL(e)), where L indicates the number of entity-class features. The entity-document features and entity-class features will be introduced in Section 4 later.""",null,null
61,3.2 Global Discriminative Model,null,null
62,"This paper utilizes logistic regression, a traditional discriminative model, to estimate the conditional probability P (r|e, d), in which r(r  {1, -1}) is a binary label to indicate the relevance of the entity-document pair (e, d). The value of r is 1 if the document d is relevant to the entity e, otherwise r ,"" -1. Formally, the parametric form of P (r "","" 1|e, d) can be expressed as follows in terms of logistic functions over a linear combination of features,""",null,null
63,K,null,null
64,"P (r ,"" 1|e, d) "","" ( ifi(e, d))""",null,null
65,(1),null,null
66,"i,1",null,null
67,"where (x) ,"" 1/(1 + exp(-x)) is the standard logistic function, and i is the combination parameter for the ith entry of the feature vector. For the irrelevant class, we have""",null,null
68,K,null,null
69,"P (r ,"" -1|e, d)"",1 - P (r ,"" 1|e, d)"",""(- ifi(e, d)) (2)""",null,null
70,"i,1",null,null
71,"It is worth noting that for different values of r, the only",null,null
72,"difference in P (r|e, d) is the sign within the logistic func-",null,null
73,"tion. Therefore, we adopt the general representation of",null,null
74,"P (r|e, d) , (r",null,null
75,"K i,1",null,null
76,i,null,null
77,"fi(e,",null,null
78,d)),null,null
79,in,null,null
80,the,null,null
81,following,null,null
82,sections.,null,null
83,"The conditional probability of relevance P (r|e, d) represents",null,null
84,the extent to which the document d is relevant to the entity,null,null
85,e. The entity-documents pairs are then classified as positive,null,null
86,"or negative according to the value of P (r ,"" 1|e, q). Since the""",null,null
87,learned weights are identical for all entity-document pairs,null,null
88,"and regardless of specific entities, this model is also denoted",null,null
89,as global discriminative model (GDM) in this paper.,null,null
90,"Several other approaches for CCR [5, 29] can be deemed as",null,null
91,global discriminative models adopting different classification,null,null
92,functions such as decision trees and Support Vector Machine,null,null
93,(SVM).,null,null
94,3.3 Entity Class-Dependent Mixture Model,null,null
95,"In the GDM introduced in Subsection 3.2, a fixed set of combination weights (i.e., ) are learned to optimize the overall performance for all entities. However, the best combination strategy for a given entity is not always the best for others. The entities stored in KBs are extremely diverse, including persons, organizations, locations, events, etc. Different entities have personalized criteria to detect relevant documents.",null,null
96,"We propose an entity class-dependent discriminative mixture model (ECDMM) by introducing an intermediate latent class layer to capture the entity class information in the learning framework. A latent variable z is utilized to indicate which entity class the combination weights z ,"" (z1,    , zK ) are drawn from. The choice of z depends on the target entity e in the entity-document pair (e, d). The joint probability of relevance r and the latent variable z is represented as""",null,null
97,"P (r, z|e, d; , ) ,"" P (z|e; )P (r|e, d, z; )""",null,null
98,(3),null,null
99,"where P (z|e; ) is the mixing coefficient, representing the probability of choosing hidden entity class z given entity e, and  is the corresponding parameter. P (e, d, z; ) denotes the mixture component which takes a logistic functions for r , 1 (or r , -1).  ,"" {zi} is the set of combination parameters where zi is the weight for the ith feature vector entry for the given training instance (e, d) under the hidden class z. By marginalizing out the latent variable z, the corresponding mixture model can be written as""",null,null
100,Nz,null,null
101,K,null,null
102,"P (r|e, d; , ) ,"" P (z|e; ) r zifi(e, d) (4)""",null,null
103,z,null,null
104,"i,1",null,null
105,where Nz is the number of latent entity classes. If P (z|e; ),null,null
106,"follows the multinomial distribution, the model cannot eas-",null,null
107,ily generalize the combination weights to unseen entities,null,null
108,beyond the training set since each parameter in multino-,null,null
109,mial distribution specifically corresponding to a training en-,null,null
110,"tity. To address this problem, we adopt a soft-max function",null,null
111,1 Ze,null,null
112,exp(,null,null
113,"Lz j,1",null,null
114,zj,null,null
115,gj,null,null
116,(e)),null,null
117,to,null,null
118,model,null,null
119,P (z|e; ),null,null
120,instead.,null,null
121,zj is,null,null
122,the weight parameter associated with the jth entity feature,null,null
123,637,null,null
124,"in the latent entity class z and Ze is the normalization factor that scales the exponential function to be a proper probability distribution. In this representation, each entity e is denoted by a bag of entity-class features (g1(e),    , gLz (e)) where Lz is the number of entity features. By plugging the soft-max function into Eq. 4, we can get",null,null
125,1 Nz,null,null
126,Lz,null,null
127,K,null,null
128,"P (r|e, d; , ),",null,null
129,exp,null,null
130,"Ze z,1",null,null
131,zj gj (e),null,null
132,"j,1",null,null
133,"r zifi(e, d)",null,null
134,"i,1",null,null
135,(5),null,null
136,"Because zj is associated with each entity feature instead of each entity, the above model allows the estimated zj to be applied to less popular entities and even unseen entities.",null,null
137,"Suppose entity-document pairs in training set are represented as T ,"" {(eu, dv)}, and R "","" {ruv} denotes the corresponding relevance judgment (i.e., +1 or -1) of (eu, dv), where u "","" 1,    , M and v "","" 1,    , N . Assume training instances in T are independently generated, the conditional likelihood of the training data is written as follows.""",null,null
138,MN,null,null
139,"P (R|T ),",null,null
140,"P (ruv|eu, dv),",null,null
141,"u,1 v,1",null,null
142,"MN u,1v,1",null,null
143,1 Nz,null,null
144,Lz,null,null
145,K,null,null
146,Zeu,null,null
147,exp(,null,null
148,"z,1",null,null
149,"j,1",null,null
150,zj gj (eu))(ruv,null,null
151,"zifi(eu, dv))",null,null
152,"i,1",null,null
153,(6),null,null
154,3.4 Parameter Estimation,null,null
155,"The parameters (i.e.  and ) in Eq. 6 can be estimated by maximizing the following data log-likelihood function,",null,null
156,MN,null,null
157,Nz 1,null,null
158,"L(, ),",null,null
159,log,null,null
160,"u,1 v,1",null,null
161,"( z,1 Zeu",null,null
162,exp( zj gj (eu))),null,null
163,"j,1",null,null
164,(7),null,null
165,K,null,null
166,"(ruv zifi(eu, dv))",null,null
167,"i,1",null,null
168,"where M is the number of the entities and N is the number of the documents in training set. gj(eu) denotes the jth feature for the uth entity and ruv denotes the relevance judgment for the pair (eu, dv). A typical approach to maximize Eq. 7 is to use Expectation-Maximization (EM) algorithm [8].",null,null
169,E-Step.,null,null
170,The E-step can be derived as follows by computing the posterior probability of z given entity eu and document dv.,null,null
171,"P (z|eu, dv) ,",null,null
172,exp(,null,null
173,"Lz j,1",null,null
174,zj,null,null
175,gj,null,null
176,(eu))(ruv,null,null
177,"K i,1",null,null
178,"zifi(eu,",null,null
179,dv )),null,null
180,(8),null,null
181,z exp(,null,null
182,"Lz j,1",null,null
183,zj,null,null
184,gj,null,null
185,(eu))(ruv,null,null
186,"K i,1",null,null
187,"zifi(eu,",null,null
188,dv )),null,null
189,M-Step.,null,null
190,"By optimize the auxiliary Q function, we can derive the following parameter update rules.",null,null
191,"z ,",null,null
192,K,null,null
193,(9),null,null
194,"arg max P (z|eu, dv) log (ruv zifi(eu, dv))",null,null
195,z uv,null,null
196,"i,1",null,null
197,"z , arg max",null,null
198,z u,null,null
199,1,null,null
200,Lz,null,null
201,"P (z|eu, dv)",null,null
202,v,null,null
203,log,null,null
204,Zeu,null,null
205,exp(,null,null
206,"j,1",null,null
207,zj,null,null
208,gj,null,null
209,(eu)),null,null
210,(10),null,null
211,"The M-step can be optimized by any gradient descent method. To optimize Eq. 9 and Eq. 10, we employ the minFunc toolkit4, a collection of Matlab functions for solving optimization problems using Quasi-Newton strategy. When the value of L(, ) converges to a local optima, the estimated parameters can be plugged back into the model to compute the probability of relevance for entity-document pairs. Since EM is only guaranteed to converge to local optima given different starting points, we try several starting points and choose the model that leads to the greatest log-likelihood.",null,null
212,3.5 Discussion,null,null
213,The ECDMM can exploit the following two advantages over the GDM: (1) the combination weights are able to change across entities and hence lead to a gain of flexibility. (2) it offers probabilistic semantics for the latent entity classes and thus each entity can be associated with multiple classes.,null,null
214,Determining the number of latent Variables.,null,null
215,"The number of hidden entity classes can be determined by some model selection criterion. We choose Akaike Information Criteria (AIC), which has been shown suitable in determining the number of latent classes in mixture models. As a measure of the goodness of fit of an estimated statistical model, AIC is defined as",null,null
216,"2m - 2L(, )",null,null
217,(11),null,null
218,"where m is the total number of parameters in the model. AIC offers a relative estimation of the information loss when a given model is used to represent the process that generates the data. Given a set of models, the preferred model is the one with the minimum AIC value.",null,null
219,4. FEATURES,null,null
220,"In this section, we present the two types of features used in the discriminative models. Entity-document features f (e, d) are used in the discriminative components of GDM and ECDMM. In addition, ECDMM requires entity-class features g(e) to learn the mixing coefficients in the mixture component.",null,null
221,4.1 Entity-Document Features,null,null
222,"Entity-document features (i.e., f (e, d)) are composed of semantic and temporal features.",null,null
223,4.1.1 Semantic Features,null,null
224,"We adopt the semantic features listed in Table 1, which have been proved effective in CCR [28, 29]. Semantic features can model semantic characteristics of document-entity pairs.",null,null
225,4.1.2 Temporal Features,null,null
226,"Entities are evolving in the stream corpus as time goes by, yet semantic features are not capable of portraying the",null,null
227,4http://www.cs.ubc.ca/~schmidtm/Software/minFunc. html,null,null
228,638,null,null
229,Table 1: Semantic features.,null,null
230,Feature,null,null
231,Description,null,null
232,N (erel),null,null
233,# of entity e's related entities found in its profile page,null,null
234,"N (d, e)",null,null
235,# of occurrences of e in document d,null,null
236,"N (d, erel)",null,null
237,# of occurrences of the related entities in document d,null,null
238,"F P OS(d, e)",null,null
239,First occurrence position of e in d,null,null
240,"F P OSn(d, e)",null,null
241,"F P OS(d, e) normalized by the document",null,null
242,length,null,null
243,"LP OS(d, e)",null,null
244,Last occurrence position of e in d,null,null
245,"LP OSn(d, e)",null,null
246,"LP OS(d, e) normalized by the document",null,null
247,length,null,null
248,"Spread(d, e)",null,null
249,"LP OS(d, e) - F P OS(d, e)",null,null
250,"Spreadn(d, e) Spread(d, e) normalized by document",null,null
251,length,null,null
252,"Simcos(d, si(e)) Cosine similarity between d and the ith section of e's profile",null,null
253,"Simjac(d, si(e)) Jaccard similarity between d and the ith section of e's profile",null,null
254,"Simcos(d, ci)",null,null
255,Cosine similarity between d and the ith citation of e in the KB,null,null
256,"Simjac(d, ci)",null,null
257,Jaccard similarity between d and the ith citation of e in the KB,null,null
258,"dynamic characteristics of entities. So we resort temporal features to make up this deficiency. Previous work [2, 5, 29] considering temporal features can be summarized as a straightforward strategy that counts the daily (or hourly) occurrences of target entities in the stream corpus and calculates some statistical indicators as temporal features. To exploit the effectiveness of temporal features, novel bursty features are introduced in this paper. The underlying intuition is that the occurrences of entities in the stream do not distribute uniformly. If the amount of documents referring to an entity increases sharply in a short time period when something important is happening around the entity, this time period is detected as one bursty period of this entity. We make an assumption that documents occur in a bursty period of an entity are more likely to be related to it than those not.",null,null
259,"The bursty periods of an entity can be detected either from stream corpus or from third-party data sources, denoted as internal bursty periods and external bursty periods respectively. Due to the heterogeneity of data sources, we use different burst detection methods to identify internal bursty periods and external bursty periods for entities.",null,null
260,Internal Burst Detection.,null,null
261,Burst detection from a stream of documents have been,null,null
262,"thoroughly investigated in TDT and event detection [22, 17,",null,null
263,31].,null,null
264,Since our goal is not to develop a new burst detection algo-,null,null
265,"rithm, we simply adopt Kleinberg's 2-state finite automaton",null,null
266,model [22] to identify bursty periods of entities. There are,null,null
267,two states q0 and q1 in the finite automaton A. For every,null,null
268,"target entity e, when A in state q0, it has low emission rate",null,null
269,0,null,null
270,",",null,null
271,|Rd (e)| T,null,null
272,",",null,null
273,where,null,null
274,Rd(e),null,null
275,is,null,null
276,the,null,null
277,number,null,null
278,of,null,null
279,all,null,null
280,documents,null,null
281,referring to e over the whole time range T . When A in state,null,null
282,"q1,",null,null
283,the,null,null
284,rate,null,null
285,is,null,null
286,increased,null,null
287,to,null,null
288,1,null,null
289,",",null,null
290,s,null,null
291,|Rd (e)| T,null,null
292,",",null,null
293,where,null,null
294,1,null,null
295,>,null,null
296,0,null,null
297,be-,null,null
298,cause s is a scaling factor larger than 1.0 and s is empirically,null,null
299,set as 2.0 in our work. The larger the number of documents,null,null
300,"referring to entity e at time t, the higher the likelihood of e",null,null
301,being identified as a bursty entity at t.,null,null
302,"After performing the burst detection algorithm, if the au-",null,null
303,"tomaton of entity e is in the state q1 during a time period [tstart, tend], [tstart, tend] is a burtsy period of e with a bursty weight bw(tstart,tend)(e). The bursty weight is defined as the cost improvement incurred by assigning state q1 over the bursty period instead of q0, and can be found in [22].",null,null
304,External Burst Detection.,null,null
305,"External resources, such as daily view statistics of entities' profile pages, are utilized as temporal features in previous work [2, 29]. Since some KBs do not provide page view statistics for entities as Wikipedia, we also include Google Trends5 to detect external bursts. Akin to Wikipedia statistics, Google Trends can provide a numeric sequence v ,"" (v1,    , vT ) for each entity e, where vi denotes the normalized search volume of e in the ith day.""",null,null
306,"We detect external bursts of entity e from v with a tailored moving average (MA) method [27]. More concretely, for each vi in v,",null,null
307,1. Calculate a moving average sequence of length w as,null,null
308,M Aw(i),null,null
309,",",null,null
310,vi,null,null
311,+ vi-1,null,null
312,+    + vi-w+1 w,null,null
313,"2. Calculate a cutoff c(i) based on previous MA sequences P reMA ,"" (M Aw(1),    , M Aw(i)) as""",null,null
314,"c(i) , mean(P reMA) +   std(P reMA)",null,null
315,"3. Detect bursty day sequence d, where d , {i|M Aw(i)  c(i)}",null,null
316,"4. Calculate the bursty weight sequence w ,"" (w1,    , wT ) for e as follows.""",null,null
317," 0, i  d",null,null
318,wi,null,null
319,",",null,null
320,"M Aw(i) , c(i)",null,null
321,i,null,null
322,d,null,null
323,5. Compact each segment of consecutive days in d into a,null,null
324,"bursty period [tstart, tend] of entity e, and the bursty weight bw(tstart,tend) is calculated as the average weight of all the bursts in this period.",null,null
325,"The moving average length can be varied to detect long-term or short-term bursts. We set the moving average length as 7 days (i.e., w ,"" 7). The cutoff value is empirically set as 2 times the standard deviation of the M A (i.e.,  "", 2).",null,null
326,Bursty Feature Representation.,null,null
327,"Given an entity-document pair (e, d), we define a bursty value b(e, d) to represent the temporal correlation between d and e. Let t be the timestamp of d. If t falls in one of e's bursty periods, say [tstart, tend], then b(d, e) is calculated as Eq. 12 shows. If t is not in any bursty period of e, b(d, e) is set as 0.",null,null
328,"b(d, e)",null,null
329,",",null,null
330,(1,null,null
331,-,null,null
332,t - tstart ) tend - tstart,null,null
333,,null,null
334,"bw(tstart ,tend ) (e),",null,null
335,(12),null,null
336,"t  [tstart, tend]",null,null
337,In,null,null
338,"Eq. 12, 1 -",null,null
339,t-tstart tend -tstart,null,null
340,is,null,null
341,a decaying coefficient,null,null
342,reflecting,null,null
343,the intuition that the documents appear at the beginning,null,null
344,of a bursty period are more informative than those appear,null,null
345,5http://www.google.com/trends/,null,null
346,639,null,null
347,Geoffery Hinton,null,null
348,Labeled Categories,null,null
349,Canadian Computer Scientists,null,null
350,AI Researchers,null,null
351,Fellows of AAAI,null,null
352,Parent Categories,null,null
353,Computer Scientists,null,null
354,Researchers,null,null
355,Fellows of Learned Societies,null,null
356,Labeled Categories,null,null
357,American Computer Scientists,null,null
358,Programming Language Researchers,null,null
359,Fellows of ACM,null,null
360,Barbara Liskov,null,null
361,Figure 1: Two entities without common labeled categories but with shared parent categories.,null,null
362,"at the end. Please note that b(d, e) can be calculated based on external bursts and internal bursts respectively. To avoid using future information during burst detection, we carefully perform burst detection algorithm (either internal or external) in a daily incremental manner. When dealing with an entity-document pair, the bursty periods are determined by the data before the timestamp of this document.",null,null
363,4.2 Entity-Class Features,null,null
364,"In ECDMM, besides entity-document features, entity-class features (i.e., g(e) in Eq. 5) are required to learn the mixing coefficients. Here we consider two types of prior knowledge to design entity-class features.",null,null
365,4.2.1 Profile-based features,null,null
366,"Each entity in KBs is uniquely identified by its profile page, which contains the basic information of this entity, such as name, address and experiences. We crawl the profile pages of all the entities as a profile collection. After removing stop words, we represent each entity as a feature vector with the bag-of-words model, where term weights are determined by the TF-IDF scheme.",null,null
367,4.2.2 Category-based features,null,null
368,"Some KBs like Wikipedia organize entities with hierarchical categories. For example, Geoffrey Hinton in Wikipedia, is labeled with categories such as Canadian computer scientists, Artificial intelligence researchers, and Fellows of AAAI. Besides these labeled categories, we take the parent categories of the labeled categories into consideration to deal with the circumstance in Figure 1. The two alike entities can not be correlated if we only consider labeled categories.",null,null
369,"Similar to profile-based feature vector, we leverage a ""bagof-categories"" model to represent each entity as a categorybased feature vector. Given an entity without category information, we manually assign a meta-category for it according to its profile. We supplement three meta-categories: person, facility and organization, which can cover all the entities in our dataset. The category-based feature vector of entity e is denoted as gc(e) ,"" (c1(e),    , cN (e)), where N is the total number of categories. ci(e) equals to 1 if e is labeled with category ci, otherwise ci(e) is 0.""",null,null
370,"Therefore, given a target entity set E, we can generate two feature vectors for each e  E: profile-based vector gp(e) and category-based vector gc(e) respectively.",null,null
371,5. EXPERIMENTS,null,null
372,"In this section, we first introduce the dataset for experiments. After that, we report an extensive set of experimental results of our proposed models and baselines in two scenarios of CCR. At last, analysis and discussion are presented based on the experimental results.",null,null
373,5.1 Dataset,null,null
374,"We conduct our experiments on TREC-KBA-2013 dataset6, a standard test bed provided by TREC. The data set is composed of a target entity set and a document collection called stream corpus.",null,null
375,Entity Set.,null,null
376,"The target entity set includes 121 Wikipedia entities and 20 Twitter entities, more specifically, 98 people, 19 organizations, and 24 facilities from 14 inter-related communities such as small towns like Danville, KY and academic communities like Turing award winners.",null,null
377,Stream Corpus.,null,null
378,"The temporally-ordered stream corpus, containing approximately 1 billion documents crawled from October 2011 to the end of February 2013. Each document is associated with a timestamp indicating its time of crawling. The corpus have been split with documents from October 2011 to February 2012 as training instances and the remainder for evaluation. We adopt the same training/test range setting in our experiments.",null,null
379,Annotation.,null,null
380,"The relevance of entity-document pairs are labeled following a four-point scale relevance setting, including vital, useful, neural and garbage. The definitions are listed in Table 2.",null,null
381,Table 2: Four-point scale relevance estimation in TREC-KBA-2013.,null,null
382,"Vital timely info about the entity's current state, actions, or situation. This would motivate a change to an already up-to-date KB article.",null,null
383,"Useful possibly citable but not timely, e.g., background biography, secondary source information.",null,null
384,"Neutral informative but not citable, e.g., tertiary source like Wikipedia article itself.",null,null
385,"Garbage no information about the target entity could be learned from the document, e.g., spam.",null,null
386,The details of the annotations for Wikipedia and Twitter entities are demonstrated in Table 3.,null,null
387,5.2 Evaluation Scenarios,null,null
388,"According to different granularity settings, we evaluate the proposed models in two classification scenarios respectively.",null,null
389,6http://trec-kba.org/kba-stream-corpus-2013.shtml,null,null
390,640,null,null
391,Table 3: The number of training and test instances (entity-document pairs) for Wikipedia and Twitter entities,null,null
392,respectively.,null,null
393,Training,null,null
394,Test,null,null
395,Vital Useful Neutral Garbage Vital Useful Neutral Garbage,null,null
396,Wikipedia 2096 2257 1162,null,null
397,1756 8639 16053 5649 18694,null,null
398,Twitter 182 326,null,null
399,72,null,null
400,569 1808 2953 1491,null,null
401,4103,null,null
402,Total 2278 2583 1234,null,null
403,2325 10447 19006 7140 22797,null,null
404,Vital Only.,null,null
405,"Only vital entity-document pairs are treated as positive instances, and the others are negative instances. This scenario is the essential task of CCR.",null,null
406,Vital + Useful.,null,null
407,"Both vital and useful entity-document pairs are treated as positive instances, and the others are negative ones.",null,null
408,5.3 Experimental Methodology,null,null
409,Experiments in this section investigate the effectiveness of our proposed mixture model and baseline methods in the two scenarios. The following methods are compared:,null,null
410," Global Discriminative Model (GDM). As presented in Subsection 3.2, this approach learns a set of fixed weights for all entity-documents pairs.",null,null
411," Na쮑ve Entity Class-Dependent Discriminative Mixture Model (Na쮑ve ECDMM). This approach uses entitydocument features instead of entity-class features for the mixing component (i.e., g(e) :,"" f (e, d) in Eq. 5) of ECDMM.""",null,null
412, Profile-based Entity Class-Dependent Discriminative Mixture Model (profile ECDMM). This approach utilizes profile-based features as entity-class features for the mixing component of ECDMM.,null,null
413, Category-based Entity Class-Dependent Discriminative Mixture Model (category ECDMM). This approach utilizes category-based features as entity-class features for the mixing component of ECDMM.,null,null
414," Combination Entity Class-Dependent Discriminative Mixture Model (combine ECDMM). This approach utilizes profile-based and category-based features together as entity-class features for the mixing component of ECDMM. In our experimental setting, we simply union the two feature vectors together into an integral feature vector.",null,null
415,"For reference, we also include three top-ranked approaches in the TREC-KBA-2013 track as baselines.",null,null
416," Official Baseline [13]. A string matching approach implemented by TREC-KBA organizers. For each target entity, they split the entity's name into different tokens and manually composite them into reliable aliases of the entity. These alias are utilized to filter relevant documents from the stream corpus with the strategy that documents referring to any alias are rated as vital to the target entity. A relevance score is estimated according to the length of matched string.",null,null
417," BIT-MSRA [29]. An entity-unspecific random forests classification model, which is the first place approach in TREC-KBA-2013 track. This approach can be considered as a variant of GDM utilizing a different kernel.",null,null
418," UDEL [23]. An entity-centric query expansion approach that achieves the second best performance in TREC-KBA-2013 track. Given a target entity, the approach first detect related entities from the profile page of the entity. Then, these related entities are utilized as expansion terms and combine with the target entity as a new query to detect and rank the relevant documents. The optimal weights of query terms are learned from the training data. The relevance score of a document is estimated according to its position in the ranking list.",null,null
419,5.4 Hidden Classes Analysis,null,null
420,"For all mixture models, the number of hidden classes are determined according to AIC value. The optimal numbers of latent classes of all variants of ECDMM are reported in Table 4. The number of optimal classes of category ECDMM is obviously larger than the optimal numbers of the other mixture models, which possibly caused by the hierarchical structures of categories in our category-based feature set. Although the incorporation of parent categories can build the correlation between two similar entities without common labeled categories, it brings some noisy correlations in the meantime. For instance, a politician and a business man both living in Florida share a common parent category ""Living people from Ocala, FLorida"", this correlation will mislead the model and come to an non-optimal fit to the data.",null,null
421,Table 4: Number of hidden classes determined by,null,null
422,AIC for each mixture model.,null,null
423,Model,null,null
424,Vital Vital + Useful,null,null
425,na쮑ve ECDMM,null,null
426,9,null,null
427,10,null,null
428,profile ECDMM 7,null,null
429,6,null,null
430,category ECDMM 13,null,null
431,12,null,null
432,combine ECDMM 9,null,null
433,8,null,null
434,5.5 Overall Results,null,null
435,"This section presents the overall performance of all experimental methods. We adopt F1 (harmonic mean between precision and recall), accuracy and AUC (Area Under Curve) [12] as the evaluation measurements. All the measurements are computed in an entity-insensitive manner. In other words, the measurements are computed based on the test pool of all entity-document pairs regardless of specific entities. The results are reported in Table 5.",null,null
436,641,null,null
437,Table 5: Overall classification results of evaluated models.,null,null
438,Methods,null,null
439,Vital Only,null,null
440,Vital + Useful,null,null
441,P,null,null
442,R F1 Accu AUC P,null,null
443,R F1 Accu,null,null
444,Official Baseline .171 .942 .290 .175 .475 .540 .972 .694 .532,null,null
445,BIT-MSRA,null,null
446,.214 .790 .337 .445 .580 .589 .974 .734 .615,null,null
447,UDEL,null,null
448,.169 .806 .280 .259 .473 .573 .893 .698 .579,null,null
449,GDM,null,null
450,.218 .507 .304 .587 .556 .604 .913 .727 .565,null,null
451,na쮑ve ECDMM .223 .400 .286 .644 .548 .627 .912 .744 .656,null,null
452,profile ECDMM .332 .376 .353 .754 .606 .669 .866 .755 .692,null,null
453,category ECDMM .316 .422 .362 ..734 .612 .672 .894 .767 .704,null,null
454,combine ECDMM .397 .418 .407 .783 .640 .703 .877 .780 .731,null,null
455,AUC .488 .578 .547 .588 .631 .675 .685 .716,null,null
456,"We notice that combine ECDMM achieves best on all measurements except recall. The official baseline achieve the best recall of all methods, which is not surprising since the official baseline is a manual method to detect as many relevant documents as possible by manually selecting reliable aliases of an entity in advance.",null,null
457,"Compared with GDM regardless of entity class information, all the mixture models employing entity-class features explicitly (i.e., profile ECDMM, category ECDMM and combine ECDMM) achieve better classification performance in both scenarios. Even Na쮑ve ECDMM which does not employ entity-class features explicitly can outperform GDM and other three baselines. This reveals that the mixture model is an effective strategy to enhance the straightforward discriminative model. Na쮑ve ECDMM is not robust in two scenarios. Although it outperforms GDM in vital + useful scenario, it cannot beat GDM in vital only scenario. This is possibly caused by its implicitly employment of entityclass features. The entity-document features are noisy, because the document-related counterpart contributes nothing to capture hidden entity classes.",null,null
458,"Both profile ECDMM and category ECDMM outperform na쮑ve ECDMM remarkably, revealing that profile-based features and category-based features are effective in modeling hidden entity classes. Category-based features are more promising than profile-based features, which is reasonable because the category labels in KBs contain prior human knowledge on entity class and taxonomy information. Even though combine ECDMM combines profile-based features and category-based features in a straightforward manner, it achieves the best performance. In comparison to GDM, combine ECDMM improve F1 more than 10 percent and AU C approximately 10 percent. We believe that the performance can be enhanced further with more comprehensive entity class information and combination strategy.",null,null
459,5.6 Fine-grained Results,null,null
460,This section compares the methods in a fine-grained level.,null,null
461,We need guarantee our mixture models not only achieve,null,null
462,"remarkable overall performance, but also perform well in",null,null
463,"entity-level. Hence, we recomputed the measurements in an",null,null
464,entity-sensitive manner.,null,null
465,"Based on the classification results for each entity ei (i ,",null,null
466,"1,    , M ) in the test set, precision and recall of each model",null,null
467,"are first calculated as P (ei) and R(ei). Then, we compute",null,null
468,"the macro-averaged precision and recall over all entities, de-",null,null
469,"noted as macro P ,",null,null
470,M 1,null,null
471,P (ei),null,null
472,M,null,null
473,and,null,null
474,macro,null,null
475,R,null,null
476,",",null,null
477,M 1,null,null
478,R(ei ),null,null
479,M,null,null
480,"respectively. At last, macro-averaged F1 is computed ac-",null,null
481,cording to macro P and macro R.,null,null
482,"The macro-averaged measurements in two scenarios are reported in 2(a) and 2(b) respectively. The three baselines are labeled with red color, and the blue dots represent our proposed methods. The best method is labeled with pentagram in both figures. The parallel solid curves are contour lines of F1 value, which means the dots in the same curve achieve same F1 values. The dots lying in upper right achieve higher F1 than the lower left ones. Obviously, combine ECDMM achieves the best F1 in both scenarios. We also find our mixture models (i.e., blue dots) achieve higher precision in vital only scenario, demonstrating our models can detect vital documents more accurately than the baselines.",null,null
483,5.7 Performance on Unseen Entities,null,null
484,"This section evaluates the generalization ability of our proposed models to handle unseen entities in the training set. A robust model is able to handle unseen entities as well as training entities. As listed in Table 6, there are 10 unseen entities in the TREC-KBA-2013 dataset. We evaluate the performance of our models on the unseen entity set composed of these 10 entities. We choose macro-averaged accuracy as the evaluation measurement. Due to the sparse positive instances for some unseen entities, it is improper to adopt precision, recall and F1 for evaluation because they possibly become 0, in which case these measurements cannot reflect the performance suitably. The results are reported in Table 7.",null,null
485,Table 7: The averages of accuracies over 10 unseen,null,null
486,entities. Methods,null,null
487,accu@(vital) accu@(vital + useful),null,null
488,Official Baseline,null,null
489,.175,null,null
490,.532,null,null
491,BIT-MSRA,null,null
492,.445,null,null
493,.614,null,null
494,UDEL,null,null
495,.259,null,null
496,.579,null,null
497,GDM,null,null
498,.552,null,null
499,.565,null,null
500,na쮑ve ECDMM,null,null
501,.587,null,null
502,.608,null,null
503,profile ECDMM,null,null
504,.623,null,null
505,.647,null,null
506,category ECDMM,null,null
507,.565,null,null
508,.431,null,null
509,combine ECDMM,null,null
510,.580,null,null
511,.582,null,null
512,"In both scenarios, the best classification results are achieved by profile ECDMM, which outperforms category ECDMM and combine ECDMM. A possible explanation for the unsatisfactory performance of category ECDMM is that the category information of unseen entities are not covered well in the training set, especially the Twitter entities. For these entities, there is too little category information to model their hidden classes accurately.",null,null
513,642,null,null
514,Recall Recall,null,null
515,0.9,null,null
516,Official Baseline,null,null
517,BIT-MSRA,null,null
518,UDEL,null,null
519,GDM,null,null
520,0.8,null,null
521,nave_ECDMM,null,null
522,profile_ECDMM,null,null
523,category_ECDMM,null,null
524,combine_ECDMM,null,null
525,0.7,null,null
526,0.6,null,null
527,0.5,null,null
528,0.4,null,null
529,0.3,null,null
530,0.15,null,null
531,0.2,null,null
532,0.25,null,null
533,0.3,null,null
534,0.35,null,null
535,0.4,null,null
536,0.45,null,null
537,0.5,null,null
538,Precision,null,null
539,(a) Vital Only,null,null
540,0.8 0.75,null,null
541,0.7 0.65,null,null
542,0.6 0.45,null,null
543,Official Baseline BIT-MSRA UDEL GDM nave_ECDMM profile_ECDMM category_ECDMM combine_ECDMM,null,null
544,0.5,null,null
545,0.55,null,null
546,0.6,null,null
547,0.65,null,null
548,0.7,null,null
549,Precision,null,null
550,(b) Vital + Useful,null,null
551,Figure 2: Macro-averaged recall VS. macro-averaged precision over all test entities. The best approach is dotted as pentagram.,null,null
552,Table 6: The statistics of test instances for 10 unseen entities.,null,null
553,Entity,null,null
554,KB vital useful neutral/garbage,null,null
555,"The Ritz Apartment (Ocala,Florida) Wiki 4",null,null
556,1,null,null
557,5,null,null
558,Keri Hehn,null,null
559,Wiki 3,null,null
560,0,null,null
561,0,null,null
562,Chiara Nappi,null,null
563,Wiki 2,null,null
564,3,null,null
565,55,null,null
566,Chuck Pankow,null,null
567,Wiki 7,null,null
568,0,null,null
569,10,null,null
570,John H. Lang,null,null
571,Wiki 2,null,null
572,0,null,null
573,1,null,null
574,Joshua Boschee,null,null
575,Wiki 191 23,null,null
576,5,null,null
577,MissMarcel,null,null
578,Twitter 52 13,null,null
579,3,null,null
580,evvnt,null,null
581,Twitter 1,null,null
582,3,null,null
583,40,null,null
584,GandBcoffee,null,null
585,Twitter 0,null,null
586,2,null,null
587,2,null,null
588,BartowMcDonald,null,null
589,Twitter 1,null,null
590,18,null,null
591,9,null,null
592,total 10 3 60 17 3 219 68 44 4 28,null,null
593,"In vital only scenario, all the variants of our mixture model can achieve better classification performance than GDM and the other baselines. The results validate the flexibility of our mixture model as expectation, which is essential for a practical CCR system. Our mixture model is not only good at handling existing entities in the training set, but also capable of dealing with unseen entities.",null,null
594,5.8 Bursty Feature Analysis,null,null
595,"To further validate the effectiveness of the proposed bursty features, we evaluate them with the help of Information Gain (IG). Table 8 reports the IGs of the proposed features in two scenarios. All the IGs are computed following the method proposed in [33]. The higher of the IG achieved by a feature, the more powerful role it plays in the classification. The maximum, mean and median IGs of semantic features are also presented for reference. Since the bursty features are only used in the discriminative counterpart of ECDMM, we evaluate them with GDM.",null,null
596,"In Table 8, external bursty features perform best out of all features in both scenarios, conforming that external bursts of an entity are accompanied with occurrences of its relevant documents. However, internal bursts are not so helpful in vital + useful scenario as in vital only scenario. This is possibly caused by the incompleteness of the stream corpus. As we know, the stream corpus are crawled from the web, so it is possibly a biased snapshot of the true web. In addition, we only utilize the occurrences of a target entity itself in the stream corpus to detect its internal bursts currently. We can include more evidences to improve the accuracy of",null,null
597,Table 8: Information gain values of features.,null,null
598,Information Gain,null,null
599,Feature,null,null
600,Vital Only Vital + Useful,null,null
601,external bursty feature 0.130,null,null
602,0.286,null,null
603,internal bursty feature 0.020,null,null
604,0.008,null,null
605,max1 mean 2 median3,null,null
606,0.121 0.046 0.039,null,null
607,0.175 0.081 0.067,null,null
608,1 maximum IG of all semantic features 2 mean IG of all semantic features 3 median IG of all semantic features,null,null
609,"internal burst detection. For instance, contextual related entities can be resorted to enhance the detection accuracy of internal burst.",null,null
610,6. CONCLUSIONS AND FUTURE WORK,null,null
611,"The objective of Cumulative Citation Recommendation (CCR) is to detect citation-worthy documents for a set of KB entities from a chronological stream corpus. To address the problem of training data insufficiency for less popular entities, we propose an entity class-dependent discriminative mixture model (ECDMM) by introducing a latent entity class layer to model the hidden entity class information. The model can be adjusted to different types of entities by learning flexible combination parameters according to underlying entity classes. Experimental results demonstrate that ECDMM can improve the performance of CCR. Entity-",null,null
612,643,null,null
613,"document features and entity-class features are developed for the discriminative and mixing components of ECDMM respectively. In terms of entity-class features, profile-based and category-based features are validated separately and in a combination strategy. The novel bursty features developed as entity-document features are proved rewarding. Our ECDMM with proposed semantic and temporal features can achieve the state-of-the-art performance on TREC-KBA2013 dataset.",null,null
614,"For future work, we wish to explore more useful entityclass features and apply more proper combination strategies to improve the entity class-dependent mixture model.",null,null
615,Acknowledgement,null,null
616,"The authors would like to thank Jing Liu and Ning Zhang for their valuable suggestions and the anonymous reviewers for their helpful comments. This work is funded by the National Program on Key Basic Research Project (973 Program, Grant No. 2013CB329600), National Natural Science Foundation of China (NSFC, Grant Nos. 61472040 and 60873237), and Beijing Higher Education Young Elite Teacher Project (Grant No. YETP1198).",null,null
617,7. REFERENCES,null,null
618,"[1] J. Allan. Introduction to topic detection and tracking. In Topic Detection and Tracking, volume 12 of The Information Retrieval Series, pages 116. Springer US, 2002.",null,null
619,"[2] K. Balog and H. Ramampiaro. Cumulative citation recommendation: classification vs. ranking. In SIGIR, pages 941944. ACM, 2013.",null,null
620,"[3] K. Balog, H. Ramampiaro, N. Takhirov, and K. Nrv캻g. Multi-step classification approaches to cumulative citation recommendation. In OAIR, pages 121128. ACM, 2013.",null,null
621,"[4] R. Berendsen, E. Meij, D. Odijk, M. d. Rijke, and W. Weerkamp. The university of amsterdam at trec 2012. In TREC. NIST, 2012.",null,null
622,"[5] L. Bonnefoy, V. Bouvier, and P. Bellot. A weakly-supervised detection of entity central documents in a stream. In SIGIR, pages 769772. ACM, 2013.",null,null
623,"[6] Z. W. C. Tompkins and S. G. Small. Sawus: Siena's automatic wikipedia update system. In TREC. NIST, 2012.",null,null
624,"[7] J. Dalton and L. Dietz. Bi-directional linkability from wikipedia to documents and back again: Umass at trec 2012 knowledge base acceleration track. In TREC. NIST, 2012.",null,null
625,"[8] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society. Series B (Methodological), pages 138, 1977.",null,null
626,"[9] L. Dietz and J. Dalton. Time-aware evaluation of cumulative citation recommendation systems. In SIGIR 2013 Workshop on Time-aware Information Access (TAIA2013), 2013.",null,null
627,"[10] L. Dietz, J. Dalton, and K. Balog. Umass at trec 2013 knowledge base acceleration track. In TREC. NIST, 2013.",null,null
628,"[11] Y. Fang, L. Si, and A. Mathur. Discriminative probabilistic models for expert search in heterogeneous information sources. Information Retrieval, 14(2):158177, 2011.",null,null
629,"[12] T. Fawcett. An introduction to roc analysis. Pattern Recogn. Lett., 27(8):861874, June 2006.",null,null
630,"[13] J. Frank, S. J. Bauer, M. Kleiman-Weiner, D. A. Roberts, N. Triouraneni, C. Zhang, and C. R`e. Evaluating stream filtering for entity profile updates for trec 2013. In TREC. NIST, 2013.",null,null
631,"[14] J. R. Frank, M. Kleiman-Weiner, D. A. Roberts, F. Niu, C. Zhang, C. Re, and I. Soboroff. Building an Entity-Centric Stream Filtering Test Collection for TREC 2012. In TREC. NIST, 2012.",null,null
632,"[15] G. G. Gebremeskel, J. He, A. P. d. Vries, and J. Lin. Cumulative citation recommendation: A feature-aware comparison of approaches. In Database and Expert Systems Applications (DEXA), pages 193197. IEEE, 2014.",null,null
633,"[16] A. Genkin, D. D. Lewis, and D. Madigan. Large-scale bayesian logistic regression for text categorization. Technometrics, 2007.",null,null
634,"[17] Q. He, K. Chang, and E.-P. Lim. Using burstiness to improve clustering of topics in news streams. In ICDM, pages 493498. IEEE, 2007.",null,null
635,"[18] Q. He, K. Chang, E.-P. Lim, and J. Zhang. Bursty feature representation for clustering text streams. In SDM, pages 491496. SIAM, 2007.",null,null
636,"[19] D. Hong and L. Si. Mixture model with multiple centralized retrieval algorithms for result merging in federated search. In SIGIR, pages 821830. ACM, 2012.",null,null
637,"[20] R. Jin, L. Si, and C. Zhai. A study of mixture models for collaborative filtering. Information Retrieval, 9(3):357382, 2006.",null,null
638,"[21] B. Kjersten and P. McNamee. The hltcoe approach to the trec 2012 kba track. In TREC. NIST, 2012.",null,null
639,"[22] J. Kleinberg. Bursty and hierarchical structure in streams. In KDD, pages 91101. ACM, 2002.",null,null
640,"[23] X. Liu, J. Darko, and H. Fang. A related entity based approach for knowledge base acceleration. In TREC. NIST, 2013.",null,null
641,"[24] A. Y. Ng and M. I. Jordan. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. In T. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 841848. MIT Press, 2002.",null,null
642,"[25] A. D. O. Gross and H. Toivonen. Term association analysis for named entity filtering. In TREC. NIST, 2012.",null,null
643,"[26] J. H. C. B. S. Araujo, G. Gebremeskel and A. de Vries. Cwi at trec 2012 kba track and session track. In TREC. NIST, 2012.",null,null
644,"[27] M. Vlachos, C. Meek, Z. Vagena, and D. Gunopulos. Identifying similarities, periodicities and bursts for online search queries. In SIGMOD, pages 131142. ACM, 2004.",null,null
645,"[28] J. Wang, L. Liao, D. Song, L. Ma, C.-Y. Lin, and Y. Rui. Resorting relevance evidences to cumulative citation recommendation for knowledge base acceleration. In WAIM, 2015.",null,null
646,"[29] J. Wang, D. Song, C.-Y. Lin, and L. Liao. Bit and msra at trec kba ccr track 2013. In TREC. NIST, 2013.",null,null
647,"[30] Q. Wang, L. Si, and D. Zhang. A discriminative data-dependent mixture-model approach for multiple instance learning in image classification. In ECCV, pages 660673. 2012.",null,null
648,"[31] J. Weng and B.-S. Lee. Event detection in twitter. In ICWSM, volume 11, pages 401408. AAAI, 2011.",null,null
649,"[32] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, pages 4249. ACM, 1999.",null,null
650,"[33] Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization. In ICML, pages 412420, 1997.",null,null
651,"[34] Y. Yang, T. Pierce, and J. Carbonell. A study of retrospective and on-line event detection. In SIGIR, pages 2836. ACM, 1998.",null,null
652,"[35] W. X. Zhao, R. Chen, K. Fan, H. Yan, and X. Li. A novel burst-based text representation model for scalable event detection. In ACL, pages 4347. ACL, 2012.",null,null
653,"[36] M. Zhou and K. C.-C. Chang. Entity-centric document filtering: boosting feature mapping through meta-features. In CIKM, pages 119128. ACM, 2013.",null,null
654,644,null,null
655,,null,null

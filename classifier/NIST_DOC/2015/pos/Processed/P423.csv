,sentence,label,data
0,Learning to Extract Local Events from the Web,null,null
1,John Foley,null,null
2,Center for Intelligent Information Retrieval University of Massachusetts,null,null
3,jfoley@cs.umass.edu,null,null
4,Michael Bendersky,null,null
5,Google,null,null
6,bemike@google.com,null,null
7,Vanja Josifovski,null,null
8,Pinterest,null,null
9,vanja@pinterest.com,null,null
10,ABSTRACT,null,null
11,"The goal of this work is extraction and retrieval of local events from web pages. Examples of local events include small venue concerts, theater performances, garage sales, movie screenings, etc. We collect these events in the form of retrievable calendar entries that include structured information about event name, date, time and location.",null,null
12,"Between existing information extraction techniques and the availability of information on social media and semantic web technologies, there are numerous ways to collect commercial, high-profile events. However, most extraction techniques require domain-level supervision, which is not attainable at web scale. Similarly, while the adoption of the semantic web has grown, there will always be organizations without the resources or the expertise to add machine-readable annotations to their pages. Therefore, our approach bootstraps these explicit annotations to massively scale up local event extraction.",null,null
13,"We propose a novel event extraction model that uses distant supervision to assign scores to individual event fields (event name, date, time and location) and a structural algorithm to optimally group these fields into event records. Our model integrates information from both the entire source document and its relevant sub-regions, and is highly scalable.",null,null
14,"We evaluate our extraction model on all 700 million documents in a large publicly available web corpus, ClueWeb12. Using the 217,000 unique explicitly annotated events as distant supervision, we are able to double recall with 85% precision and quadruple it with 65% precision, with no additional human supervision. We also show that our model can be bootstrapped for a fully supervised approach, which can further improve the precision by 30%.",null,null
15,"In addition, we evaluate the geographic coverage of the extracted events. We find that there is a significant increase in the geo-diversity of extracted events compared to existing explicit annotations, while maintaining high precision levels.",null,null
16,Work done while at Google.,null,null
17,"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author(s). Copyright is held by the owner/author(s). SIGIR'15, August 09-13, 2015, Santiago, Chile. ACM 978-1-4503-3621-5/15/08. DOI: http://dx.doi.org/10.1145/2766462.2767739 .",null,null
18,Categories and Subject Descriptors,null,null
19,H3.3 [Information Storage And Retrieval]: Information Search and Retrieval,null,null
20,Keywords,null,null
21,Information Retrieval; Information Extraction,null,null
22,1. INTRODUCTION,null,null
23,"With the increasing trend toward personalized mobile applications and user experiences, there is a need for information systems that react to user preferences and location. In the past few years, this challenge has gathered more attention in the research community. Lagun et al. find that not only is local context useful in search, but that users are interested in explicit feedback in locality-sensitive tasks [16]. The Contextual Suggestion Track in the Text Retrieval Conference (TREC) presents the task of recommending establishments or venues to users given the preferences of a user in another city [5]. In this work, we explore a similar task, presenting users with events near them, rather than locations. Unlike the contextual suggestion track, we move away from wholepage relevance judgments toward extracting relevant atomic event records.",null,null
24,"We define an event as an object having three mandatory properties, keeping in mind our goal: to recommend, display, and make searchable all events that can be extracted from the web.",null,null
25,"Definition An event occurs at a certain location, has a start date and time, and a title or description. In other words, to be useful to a user, an event must be able to answer the questions: What?, When?, and Where?",null,null
26,"Succintly, we are interested in events that users may want to add to their calendars to be reminded of and potentially attend. This is in contrast to many other definitions of an event, such as those in works discussing real-time detection of natural disasters, riots, pandemics or terrorist attacks in microblog streams [26, 36, 20], or the classic event definition in computational linguistics, which can be as broad as ""a situation that occurs"" [25].",null,null
27,"Before a recommendation system for events can be created and evaluated, there is the information extraction challenge of discovering and collecting all available events in all areas.",null,null
28,Simple approaches to this problem include:,null,null
29,· Mining and recommending events from social media [14].,null,null
30,423,null,null
31,· Leveraging semantic web annotations like Schema.org1.,null,null
32,· Traditional wrapper induction and data mining.,null,null
33,"Unfortunately, both semantic web and social media approaches require organizations to maintain their data in a particular format. With social media, this means an updated organization page, and with semantic web technologies, this means marking up the page with microdata (e.g., Schema.org). Unfortunately, smaller businesses, charities, and truly local organizations will lack the funding or the expertise required to fully and correctly adopt semantic web technologies.",null,null
34,"Similarly, most existing approaches to information extraction require supervision at either the page or domain level, or some sort of repeated element structure [34]. As it would be infeasible and costly to annotate all such pages - or even a single page per domain, existing systems that mine for events or other structured data fall short of our goal of local event extraction from all web pages.",null,null
35,"Examples of events that we consider local and that are unlikely to have existing markup, or sufficient social media presence are farmer's markets, poetry readings, library book sales, charity dinners, garage sales, community band concerts, etc. These events are of interest to a smaller, local community, and are unlikely to be selling tickets on high-profile sites or paying for advertisement.",null,null
36,"In this work, our goal is to leverage the well-advertised, high-profile events to learn to extract a greater variety and depth of events, including the kinds of local events described above. Specifically, we leverage the existing Schema.org microdata annotations (there is an example of how these annotations appear in Figure 1) as a source of data for distant supervision, allowing us to learn to extract events that do not have semantic web annotations, including local events, without actually collecting judgments specifically for our task.",null,null
37,"We introduce a model for scoring event field extractions and an algorithm that groups these fields into complete event records. We scale up our technique to the entire ClueWeb12 corpus (700 million pages), extracting 2.7 million events. We evaluate our precision at various recall-levels, and show that we can double event coverage of a system with respect to the available semantic web annotations at an 85% precision level. We briefly explore using our judgments for a supervised approach to this task and are able to improve precision by 30% on another million events with only 30 annotator-hours.",null,null
38,"We also explore the geographic diversity of our dataset, finding that existing markup is heavily biased toward large cities (e.g. New York, Chicago, and Los Angeles) and that the results of our extraction cover a wider variety of locations. We validate this hypothesis via visual mapping, and by showing that we have good precision in a random sample of 200 cities across the world.",null,null
39,"In the next section, we introduce related work in detail. In Section 3, we introduce our event field extraction and scoring model, and our event record grouping algorithm. In Section 4, we discuss our corpus and our judgments in more detail, and we present the results of our experiments in Section 5. We end with our conclusions in Section 6.",null,null
40,1http://schema.org/Event,null,null
41,Figure 1: Example Microdata adapted from Schema.org,null,null
42,"<div itemscope itemtype,``http://schema.org/Event''> <span itemprop,``name''> Miami Heat at Philadelphia 76ers </span> <meta itemprop,``startDate'' content,""``2016-04-21T20:00''> Thu, 04/21/16 8:00 p.m. <div itemprop"",``location'' itemscope itemtype,``http://schema.org/Place''> Wells Fargo Center <div itemprop,``address'' itemscope itemtype,``http://schema.org/PostalAddress''> <span itemprop,""``addressLocality''> Philadelphia </span>, <span itemprop"",``addressRegion''>PA</span> </div> </div>",null,null
43,</div>,null,null
44,2. RELATED WORK,null,null
45,"Our work is characterized by using the explicitly annotated Schema.org as training data to learn to extract local events from the web. While there is work looking at events on social media, work leveraging semantic web annotations, and work on extraction in general, to our knowledge, our work is the first to leverage this data in this way, and the first to attempt this task.",null,null
46,2.1 Similar Tasks,null,null
47,"The Contextual Suggestion Track [5] considers the task of a known user spending time and looking for entertainment in a new city. Evaluation is done on the snippet and page level, with users judging sites as interesting or not, effectively making the task about retrieving venues or establishments. In a similar motivation, we would like to consider the task of finding events relevant to a user in their current location, but because no large corpora of events exist, we consider first the task of extracting local events.",null,null
48,"There are numerous works that identify the availability of semantic web information [27, 21, 3] but there is very little prior work on using this information as a source of distant supervision. Petrovski et al. use Schema.org annotations for products to learn regular expressions that help identify product attributes such as CPU speed, version, and product number [22]. Gentile et al. work on dictionary-based wrapper induction methods that learn interesting XPaths using linked data [7, 8]. Because Linked Data websites like DBPedia and Freebase are not typical web pages as those with Schema.org data, the structural features we are able to learn are not available in these works. We also attempt to learn about less structured fields, in particular, our What? aspect of events.",null,null
49,"Another similar work comes from the historical domain. Smith worked on detecting, and disambiguating places and dates within historical documents in a digital libraries setting. He looked at collocations between these places and dates as events, and ranked them to identify significant date-place collocations that might merit further study by historians [32]. In a follow-up work, he looked at associating terms with these collocations and building interfaces for browsing [31].",null,null
50,424,null,null
51,2.2 Event Detection in other Domains,null,null
52,"There is an entire class of work on detecting events within microblogs or real-time social media updates [26, 36, 20]. Becker describes identification of unknown events and their content in her thesis, but focuses on trending events on social media sites, and classification is used to separate event from non-event content clusters [2]. Our work, in contrast, is looking to find events in an offline manner that are explicitly described so as to present them in a search or recommendation system to users.",null,null
53,"Kayaalp et al. discuss an event-based recommendation system integrated with a social media site, but focus on events already available through Facebook and Last.fm [14]. In this work, we consider these social media sites as two examples of the larger concept of ""head domains"" which are likely to have or adopt semantic web technologies, or be worthwhile candidates for supervised wrapper induction techniques.",null,null
54,"Extraction of named events in news is another topic in which there has been work, e.g., [15]. Again, the definition of these news events is different from our local event definition.",null,null
55,2.3 Information Extraction,null,null
56,"In the extraction domain, there is an immense amount of work on reducing boilerplate, whether directly, through template extraction [10, 23, 18], or through the more general idea of region extraction and classification. Sleimen and Corchuelo provide a detailed survey of region extractors in web documents, including those that leverage visual features [29]. Region extractors, in general, attempt to classify portions of a page into their purpose, i.e. navigation links, main content, and are studied in part to index only the content present on web pages. At least part of our task could be phrased within this context: an attempt to classify regions as either containing an event or not.",null,null
57,"Work on temporal knowledge bases and extraction needed for automatic construction is similar to our task [11], except in a different domain and context. Most popular knowledge bases, have some ties to Wikipedia, which requires articles to be notable. By definition, we are seeking to extract events that do not have global relevance.",null,null
58,"Additionally, there are a number of works that focus on repeated structure for extraction. Taking cues from HTML tables [9, 17, 4, 1, 39], or repetitive command tags or terms in general [34], these techniques do not require microdata, but require repetitive structure and make the assumption that there will always be a multitude of records to extract [35]. Recently, there has been a number of works about extracting data from web documents that were generated from the same templates [12, 30, 28]. In contrast to these works, we are interested in extracting local event records, which may or may not be generated from a pre-specified template.",null,null
59,3. EVENT EXTRACTION MODEL,null,null
60,"We approach the task of extracting structured event information as a scoring or ranking method. We do this because we do not expect our target data ­ the listings of events on the web ­ to be particularly clean or perfectly aligned with any schema. Therefore, we wish to have a technique that joins several field scoring functions, which together give a multi-faceted score of how well the extracted structure fits our model of an event. The scoring functions take into account the source web page (Section 3.1), extraction",null,null
61,"sub-region (Section 3.2), and the individual extracted fields (Section 3.3).",null,null
62,"The fact that there may be multiple extractions from overlapping regions on the page, poses an additional challenge of grouping the extracted fields into disjoint event records that contain event name, date, time and location. To this end, we propose a greedy selection and grouping algorithm that provides an efficient way to optimize the quality of extracted event records. This algorithm is described in detail in Section 3.4.",null,null
63,"Formally, we represent each extracted event record as a set of fields (F ,"" {f1, f2 . . . fn}), their enclosing region (R), and the source document (D).""",null,null
64,"Figure 2: Example of an HTML document structure, presented visually. The boxes present represent nodes, the white boxes are the ones that are plausibly detected as fields in this example. Subsets of these fields represent possible event extractions.",null,null
65,"An example HTML document is shown in Figure 2. If we consider the case where this document represents only one event, it is likely that Fields A,B, and C (F ) are the key fields in our event, and that the date in the navigation bar and in the copyright are false-positive fields. An event extraction only including the copyright field should get a low score, and similarly, the event containing all the fields should be considered too big and also receive a low score. We define the region of a fieldset to be the first shared parent of its fields, so for fields A and B, the R should be the box that contains all lettered fields, the image and the text node.",null,null
66,"We formulate our scoring function, , which relates all our available context, as follows",null,null
67,"(F , R, D) ,"" (D)(R)(F ) We discuss the scoring components in order, and note where we adapted this object-general model to our event domain. Our document scoring, (D), our region scoring (R), and our field scoring (F ) are discussed in depth in the following sections.""",null,null
68,3.1 Document Scoring,null,null
69,"We want our event scoring function to take into account the page's likelihood of discussing an event, therefore we include a document-level score ((D)).",null,null
70,425,null,null
71,"We take a naive-Bayes approach to this problem, and examine the probability of a web page discussing an event (belonging to the event class). We denote the candidate document D and the event class E, and consider the following ratio",null,null
72,P (E|D) > 1.,null,null
73,P (E|D),null,null
74,"In other words, we consider a page worth investigating if the probability of it belonging to the event class (E) is higher than the probability that it does not belong to the event class (E). In practice, the division of small fractions can lead to underflow, so we consider the equivalent relationship, asking whether the so-called log-odds of a page belonging to an event class is greater than zero.",null,null
75,logP (E|D) - logP (E|D) > 0,null,null
76,"We estimate these probabilities based upon the commonlyused language modeling framework introduced by Ponte and Croft [24]. In the language modeling framework, we estimate the probability of a word given a model X (which will be one of {E, E}) as the probability of drawing it randomly from the bag of words that is that model.",null,null
77,"tf (w, X) P (w  X) ,",null,null
78,"tf (, X)",null,null
79,"The bag of words assumption means that we can treat all our terms as being independent, and we estimate the probability of a whole document by the probability of all its component terms (w  D) under the model.",null,null
80,"tf (w, X) P (D  X) ,",null,null
81,"tf (, X)",null,null
82,wD,null,null
83,"Because our event class may be sparse in comparsion to any given document, we apply linear smoothing [13] to our positive class to avoid zero probabilities.",null,null
84,"P (E|D) , P (w  E) + (1 - )P (w  C)",null,null
85,wD,null,null
86,"In contrast, because we approximate our non-event class by the language model of the entire collection, E ,"" C, no smoothing is needed because all terms are present by construction.""",null,null
87,"P (E|D) , P (w  C)",null,null
88,wD,null,null
89,"Since we run our algorithm on millions of pages, we chose to use this page scoring mechanism as a first pass filter, restricting our calculation of other scoring components to those whose (D) > 0, where  is defined as follows:",null,null
90,"(D) ,",null,null
91,1 logP (E|D) - logP (E|D) > 0,null,null
92,0,null,null
93,otherwise,null,null
94,"What remains now is identifying an initial set of documents that are used to construct the language model for the event class E. For this, we bootstrap existing semantic annotations on the web. Specifically, we consider all the",null,null
95,"pages that contain any http://schema.org/Event annotations (see Figure 1 for an example) as belonging to the event class E, since mark-up by their creators suggests that they discuss events. Overall, we have close to 150,000 such pages, which allows creating a robust event model.",null,null
96,3.2 Region Scoring,null,null
97,"Region scoring ((R)) is considered on the enclosing region R in the document. Since this region encloses all of event potential fields, it is a good unstructured representation of the extracted event. In fact, we present this enclosing region to annotators to understand our event prediction performance separately from our field prediction.",null,null
98,"Therefore, we decided on a simple region filtering approach, which simply removed from consideration regions above certain length",null,null
99,"(R) ,",null,null
100,1 |R| <  0 otherwise,null,null
101," is set to 212 in all the subsequent experiments. Our empirical evaluation have shown that this simple approach effectively removed the majority of bad regions. In fact, we considered a number of more sophisticated approaches, including learning probability distributions of the size of the region and enclosing region tags. However, in experiments not listed here due to space constraints, we found the effects of such additional information negligible, especially since region features were included on a per-field basis within our field-scoring functions (see Section 3.3).",null,null
102,3.3 Field Set Scoring,null,null
103,"We explore field scoring in a way that requires at least one of each required field to be part of our extracted field set (F ). Therefore our formulation for (F ) includes both a scoring function S and an indicator function that tests for required fields, R.",null,null
104,"(F ) ,",null,null
105,S(F ) R(F ),null,null
106,0,null,null
107,otherwise,null,null
108,"We will discuss the breakdown of the S and R functions below. Generally, we jointly score field occurrences, and this joint scoring considers the independent scores, k(f ), assigned to each field f of type k  {What, When, Where}.",null,null
109,3.3.1 Independently Scoring Fields,null,null
110,"In this work, we consider a number of ways to assign independent scores to fields that allows us to define k(f ) for any f and k  {What, When, Where}. Formally, we define f as a tuple with a (begin, end) index range within the source document D. Because the document itself has some HTML structure, the offsets within the field allow us to understand where in the structure of the page it occurs and leverage that as part of our scoring features.",null,null
111,"Pattern-based approaches to tagging of dates, times and postal addresses in text yields results of reasonable accuracy, as evidenced by the approaches in HeidelTime [33], StanfordNLP [19], as well as some prior work on address detection [38]. Therefore, we consider pattern-based baselines for our extraction of event `When' and `Where' fields. While dates, times, and places have inherent and sometimes obvious structure, patterns do not make sense as a baseline for the `What' of an event, so we assign an equal score to all candidates.",null,null
112,426,null,null
113,Table 1: Features used in Field Classification.,null,null
114,Category Feature,null,null
115,Description,null,null
116,Text,null,null
117,Unigrams,null,null
118,Stopped and stemmed,null,null
119,"unigrams, hashed to",null,null
120,"10,000 count-features.",null,null
121,Bigrams,null,null
122,Stopped and stemmed,null,null
123,"bigrams, hashed to",null,null
124,"10,000 count-features.",null,null
125,NLP,null,null
126,Capitalization Ratio of terms capital-,null,null
127,"ized, first term capital-",null,null
128,ized.,null,null
129,Address overlap Number of address fields,null,null
130,that overlap the target,null,null
131,span.,null,null
132,Date overlap,null,null
133,Number of date fields,null,null
134,that overlap the target,null,null
135,span.,null,null
136,Time overlap,null,null
137,Number of time fields,null,null
138,that overlap the target,null,null
139,span.,null,null
140,Structural Size,null,null
141,Ratio of size to parent,null,null
142,and to page.,null,null
143,Location,null,null
144,Ratio of start and end,null,null
145,locations to page.,null,null
146,Attribute Text Unigrams present in at-,null,null
147,tributes; can capture,null,null
148,style information.,null,null
149,Parent tag,null,null
150,Hashed vocabulary of,null,null
151,size 10 of lower-case par-,null,null
152,ent tag.,null,null
153,GrandParent tag Hashed vocabulary of,null,null
154,size 10 of lower-case par-,null,null
155,ent's parent tag.,null,null
156,Sibling tags,null,null
157,Hashed vocabulary of,null,null
158,size 1000 sibling tags.,null,null
159,Reverse XPath 10 features for each,null,null
160,XPath entry going back-,null,null
161,wards toward HTML.,null,null
162,No classification uses baseline approaches for all fields.,null,null
163,"What(f ) , 0.5 Where(f ) ,"" matches(f, Address) When(f ) "","" matches(f, Date/Time)""",null,null
164,What classification uses baseline approaches except for What fields.,null,null
165,"What(f ) , WW T hat · Xf Where(f ) ,"" matches(f, Address) When(f ) "","" matches(f, Date/Time)""",null,null
166,What-When-Where classification uses multiclass classification to rescore the boolean baseline approaches for all fields.,null,null
167,"What(f ) , WW T hat · Xf Where(f ) ,"" matches(f, Address) · WW T here · Xf When(f ) "","" matches(f, Date/Time) · WW T hen · Xf""",null,null
168,"Our baseline methods are implemented by the function matches(f, rk) where f is a field, and r is a set of regular expressions for field type k, returning 1 if a field f is considered a match for field type k, and 0 otherwise.",null,null
169,Our classification methods leverage features Xf extracted,null,null
170,from the candidate field f and weights Wk learned using LI-,null,null
171,"BLINEAR [6]. The features used encompass textual, naturallanguage, and structural features that are more fully described in Table 1. Evaluation of these prediction methods is discussed in Section 5.1. All other evaluations consider only the What-When-Where classification method for independent field scoring.",null,null
172,"In order to train our classification methods we turn once again to the pages in the event class E, described in Section 3.1. Using these pages, we label all the HTML tags with semantic mark-up related to one of our three target fields (`What', `When', `Where') with their respective field type. In addition, we label all other textual HTML tags on these pages as `Other'. We then use this bootstrapped training data to construct a multiclass classifier with label set",null,null
173,"K ,"" [ W hat , W hen , W here , `Other ],""",null,null
174,"and learn a weight vector Wk, for each k  K. See Section 4.3 for more details on this process.",null,null
175,3.3.2 Jointly Scoring Fields,null,null
176,"Given F R ,"" {What, When, Where} as the set of required fields for an event, a field set F has all its required fields if and only if R(F ) is true. We make the assumption that the field type k, with the maximum score k(f ) is the PredictedType of the given field f . Formally:""",null,null
177,"PredictedType(f ) , argmax k(f )",null,null
178,kF R,null,null
179,"We test for the presence of all required fields by containment; the required fields should be a subset of the predicted types of the entire field set we are scoring. As an example, a set of fields may comprise an event if it has an extra date or time, but it may not if it doesn't have a date or time at all.",null,null
180,"R(F ) , F R  {PredictedType(f )|f  F }",null,null
181,"We combine the individual field scores within S(F ), our field set scoring function, using the same notation as before for per-field-type scorers (k(f )).",null,null
182,"S(F ) ,",null,null
183,max k(f ),null,null
184,kF R,null,null
185,f F,null,null
186,"The individual score for each field is still labeled with a function, k(f ), which computes the score for the maximallylikely class for each field, reusing F R to describe the set of required classes.",null,null
187,"In this formulation, we expect the output range of k to be [0, 1]. Since our independent scores are all in this range, it means that our function for S will tend to prefer field sets with fewer higher-scored fields, with R ensuring that we do not consistently predict incomplete events.",null,null
188,3.4 Event Record Grouping Algorithm,null,null
189,"We consider all HTML tags that contain any baseline (regular expression pattern-based) matches on the page to be candidates for field scoring, rather than exhaustively iterating over all the subsets of text fields on the page. Even with this relatively-smaller number of candidate fields, the prediction algorithm is computationally difficult. Recall that we have formulated our field scoring as a ranking problem. Therefore, grouping these ranked fields into complete event records is a variation of the general subset selection problem, which is known to be NP-hard in most cases [37].",null,null
190,427,null,null
191,"To ameliorate this problem, we choose to add a constraint that no field may be used twice (predicted events should not overlap) and to use a greedy approach to assigning fields to event records, so that we at least are able to predict the best events correctly. This greedy algorithm for event record grouping is presented in Algorithm 1. We rank our predicted nodes by score, and greedily select the highest scoring events, whose enclosing regions (R) do not overlap.",null,null
192,Algorithm 1 Greedy event record grouping algorithm.,null,null
193,"# All tags and all regex matches on the page: candidate fields ,"" {span, . . .} # All tags on the page containing candidatef ields. candidate tags "","" {span, . . .}""",null,null
194,"# Score and collect candidates possible events ,  for R in each candidate tag:",null,null
195,"F , set(candidate fields inside R) score ,"" (F , R, D) if score > 0:""",null,null
196,"add (score, F , R, D) to possible events",null,null
197,"# Greedily select highest scoring, non-overlapping tags output ,  for event in sorted(possible events):",null,null
198,if event fields do not overlap with any fields in output: add event to output,null,null
199,return output;,null,null
200,"As another nod to efficiency, we implement  by considering the parts independently, in terms of cost. Because (D), (R), and R(F ) are simple to calculate and may return zero scores, we calculate these parts first and skip the relatively-more expensive independent field calculations (S(F )). Additionally, this reduces the number of field sets that must be considered, lowering the overall cost of the algorithm in practice.",null,null
201,"An example HTML structure is shown in Figure 3. Nodes D,E,F, and G are textual nodes, which contain fields. D has been detected as a What field, E has been detected as a When field, F has both a Where and a partial When (only a relative date) and maybe a What field. Node G is an example of a date that is nearby but irrelevant to the event itself.",null,null
202,"Running on this example, our algorithm would walk through the nodes, assigning  scores to each node. Node D would receive a score of zero, because it is missing required field types (R). Similarly, nodes E,F, and G would be removed from consideration due to missing fields. The best selection for this event would either be Node B or Node C, depending on whether Node D is required to describe the event or if Node F is considered sufficient What explanation. Because of our joint field scoring, even if we do not apply a What field classifier, Node A will receive a lower score since it has an extra date field, which will lower its S score and overall score.",null,null
203,3.4.1 Deduplication,null,null
204,"During development, our annotators reported seeing numerous instances of duplicate events, particularly of high-profile events that were advertised on many sites. Therefore, we",null,null
205,Figure 3: Example document structure handled by our algorithm. The boxes labeled with letters represent nodes in the HTML structure. Nodes D-G are textual nodes whose text is present. The lines display the parent-child relationships in the tree.,null,null
206,"applied an exact-string matching duplicate detection method that ignored differences in capitalization, HTML tags and whitespace as a baseline and found that it was difficult to improve upon this baseline as our annotators no longer reported finding duplicates in our sampling of events. In all following analyses, we deduplicate all the extracted events using this method. We leave using extracted date/time and place information to perform a more domain-specific duplicate detection for future work.",null,null
207,4. EXPERIMENTAL SETUP,null,null
208,"We evaluate our technique for extracting events on the ClueWeb122 corpus, which contains about 733 million pages. We extract http://schema.org/event records as microdata and rDFa in a first pass. This generates 145,000 unique web pages with events, and about 900,000 events on those pages. After deduplication, there are 430,000 unique events. Of these unique events, we map Schema.org properties to our three fields: What, Where, and When, and we are left with 217,000 complete events without any further processing.",null,null
209,"We use incomplete events to train our field classifiers, but we will later consider recall in respect to the number of unique, complete Schema.org events, and we remove all pages with Schema.org events from our processing of the corpus moving forward, except for training data generation purposes.",null,null
210,4.1 Collecting Judgments,null,null
211,"During the development of our system, we collected 2,485 binary judgments for understanding precision of event predictions. Each of these judgments contains the ClueWeb12 id, the start and end offsets of the events (in bytes after the WARC header), and a judgment: 1 if it is an event, and 0 otherwise.",null,null
212,"For a large number of events in our best method, and in our method comparsion, we evaluate at the field level as well as at the event level, but only if the extraction actually is an event. For these judgments, as well as the agreement judgments, we turn to a crowdsourcing platform (Amazon",null,null
213,2http://lemurproject.org/clueweb12/,null,null
214,428,null,null
215,"Mechanical Turk) to rapidly accumulate high-quality perfield judgments. We paid annotators $0.05 for event-level judgments and $0.10 for field-level judgments. We allowed annotators to mark I can't tell from the available information for any field or event, and removed these from our evaluation (Only 46 events in total out of the 2500).",null,null
216,"For the fields, we asked annotators if they believed the selected snippet answered the given question about the event. As an example, we asked annotators to be picky about the ""When"" field in particular: two of our options were No, it only includes a date and No, it only includes a time.",null,null
217,4.2 Annotator Agreement,null,null
218,Figure 4: Agreement on Events and Fields,null,null
219,"We calculated agreement on a set of thirty events randomly selected from our best-performing technique. Results are displayed in Figure 4. Because we asked our annotators for reasons when a field was inaccurate, we have some data on which the annotators agreed the field was incorrect, but disagreed on the reason: for example, one reviewer said the field was too short and the other said that they could not tell.",null,null
220,"Our agreement is relatively high (>80%) on all but the ""What"" field, and our local reviewers found that agreement tended to be correlated with quality. The relative field performance here displays the ambiguity inherent in deciding what should describe an event in comparison to the stricter definitions of location and time.",null,null
221,4.3 Training Field Classifiers,null,null
222,"Earlier, in Section 3.3.1, we discuss our approach to assigning scores to fields, and our classification methods. We list the features that we extract in Table 1, and we have already mentioned that we were able to parse 430,000 unique events from websites with Schema.org markup.",null,null
223,"The question remains - how do we use this data as training for What Classification and What-When-Where Classification? Our first attempt to learn from this data involved a cross-fold evaluation setup, where we split events by domain into train, validate and test splits, extract features, and try to predict the fields in our validate and test sets. We split by domain here to better prepare ourselves for new data in our cross-validation setup, to avoid overtfitting to head domains that would have been present in all cross-validation sets.",null,null
224,"While Schema.org gives us plenty of positive examples for each field, we don't necessarily have obvious negative examples. In the end, we found that taking all the tags, including other Schema.org properties, gave us the richest, most accurate representation of our problem space. We want",null,null
225,"our field classifiers to be able to select tags that actually correspond to an event from all the other tags on a page that has events. In early experiments, we found that using a singleclass classifier was a mistake - the best negative examples for What include the positives for When and Where, as judged by the way our What field initially predicted dates and times and places. We moved to a multiclass setup, and this seemed to improve on this problem. Even for our What Classification setup, we emitted training, validation, and test data. We used our validation data to tune the C and parameters for LIBLINEAR and to choose between L1 and L2 normalization.",null,null
226,"Even with the care taken to select negative examples, our cross-validation estimate of field classifier performance was still extremely high (F1 > 75%) for our three fields, which as we discuss later, is not the case when evaluated on pages without Schema.org.",null,null
227,5. RESULTS,null,null
228,5.1 Evaluating our Field Prediction Methods,null,null
229,"Table 2: Fields Classified by Method. (N,W) denote significant",null,null
230,"improvements, where 99% of bootstrap samples of the target",null,null
231,method have higher precision than 99% of samples from the None,null,null
232,"and What methods, respectively.",null,null
233,None What What-Where-When,null,null
234,Event 0.54 0.51,null,null
235,"0.76 (N,W)",null,null
236,What 0.09 0.30 (N) 0.36 (N),null,null
237,When 0.17 0.20,null,null
238,0.32,null,null
239,Where 0.32 0.32,null,null
240,"0.66 (N,W)",null,null
241,Figure 5: Comparison of Precision of Field Detection Methods,null,null
242,"A key part of our overall event extraction system is the ability to assign meaningful independent scores to fields. We introduced three field scoring systems in Section 3.3.1 based on our baseline ability to detect Where/When aspects of events through regular expressions, and combining these baselines with classification.",null,null
243,"In the No Classification system, we don't use any training data, and end up leveraging text near our Where/When fields for our What predictions. In our What Classification system, we use classification on only the What field, for which the baseline is weakest, and in our What/When/Where Classification system, we combine our baseline and classification approaches for all fields, leveraging the semi-supervision of our Schema.org extracted fields to learn weights for features as described earlier.",null,null
244,429,null,null
245,Table 3: Precision Levels used in evaluation.,null,null
246,V. High High Medium Low,null,null
247,"Predicted 25,833 201,531 452,274 1,575,909",null,null
248,Increase 12%,null,null
249,93%,null,null
250,208% 725%,null,null
251,Judgments 155,null,null
252,567,null,null
253,482,null,null
254,491,null,null
255,Precision 0.92,null,null
256,0.85,null,null
257,0.65,null,null
258,0.55,null,null
259,"In order to compare these methods, we first attempted to use our semi-supervised data - the Schema.org events and fields. Unfortunately, on such an event-focused dataset, all of our methods performed well. We believe that because of the format of microdata, and how it requires that each field be represented succinctly and separately in its own tag, these training instances were actually too simple. As a result, we were not able to leverage the Schema.org data to compare methods, and we had to create our own judgments.",null,null
260,"We chose the top 30,000 pages by our document scoring method: (D), and ran our algorithm three times on each page, once with each of our classification methods, and generated a pool of events from each. From each pool we sampled 100 random events (any attempt at a pairwise comparison would not have accurately compared the different methods).",null,null
261,"We used bootstrap sampling with 10,000 samples to obtain 99% confidence intervals for each of our evaluations. We mark improvements where the 1% (worst-case) of the treatment is greater than the 99% (best-case) of the baseline. The improvements are marked in terms of the None baseline (N) and the What baseline (W), and the raw, mean values are presented in the Table 2.",null,null
262,5.2 Exploring Precision and Recall,null,null
263,"In Figure 6, precision for each component of the event is illustrated. As overall precision drops, we can see how the weakest part of our detection is still the Event title, or the What field. Intuitively, this makes sense, as it is less structured and even our annotators had trouble agreeing on What field with respect to the other fields. Another finding that's interesting here is that even though the field precision degrades rapidly, (partly because it is conditioned on the event being correct) our event precision is significantly higher, particularly in the ""Low"" precision level. This suggests that even our noisy field detection has value, particularly to a selection algorithm like ours.",null,null
264,5.3 Supervised Evaluation,null,null
265,"As a qualitative observation, when we had false-positives for event classification, they were often near-misses, in the form of album releases, band histories, obituaries, or biographies. They tended to be rich in field information, but from the wrong domain. In general, our technique introduced some false positives as the domain broadened from the focus on commercial music events of the Schema.org events.",null,null
266,"Since we had collected a large number of judgments, we decided to briefly explore a human-supervised approach. In all other sections, we discuss distant supervision results, where we had no gold-truth human assigned labels, but only the silver-truth of the Schema.org data. This evaluation serves two purposes: (1) determining whether our impression of easily-detected domain mismatches is true, and (2) evaluating the amount of effort needed to apply a supervised approach to this problem.",null,null
267,"Since we were interested in understanding annotator effort, we only used our 1100 Mechanical Turk annotations for this part, as they were automatically timed. We break our judgments into two parts, taking 800 events from medium to very-high precision, as our training and validation data, and 300 events in our low-precision bracket (original Precision of 0.55). We build a simple unigram classifier with 10,000 hashed term features as well as document, region, and individual field scores for the best field of each kind from those predictions, again using LIBLINEAR.",null,null
268,Figure 6: Precision Evaluated at Recall Levels,null,null
269,"As we discussed earlier, there were only about 217,000 unique, complete events used in our semi-supervised approach for training. We cannot evaluate our recall perfectly because we simply do not know how many events are on the internet, or even in ClueWeb12's 700 million pages. Additionally, we think the most interesting way to evaluate recall is to consider a number of interesting recall points (precision categories) for evaluation based on the ratio of events predicted to the size of our semi-supervised input (Schema.org) events).",null,null
270,"We consider a ""Very High"" precision category, chosen because it was a cutoff found to have over 90% precision. Our ""High"" precision category is chosen because it represents about a 1:1 prediction to training ratio. Our ""Medium"" precision category captures the case where we predict two events for each training instance, and our ""Low"" precision category contains the lowest score range in which we collected judgments, at about a 7:1 prediction ratio. These pseudo-recall levels are listed in Table 3.",null,null
271,"Figure 7: Supervised Event Prediction of Low-precision data based on other judgments. On the left, we have the training/validation data, and their precision represented (Very High, High, and Medium), and on the right, we have the test data, and its original precision (Low) and its new precision under supervision (Supervised).",null,null
272,"This classifier works extremely well over our 300 evaluation events from the low range, boosting the precision from 0.55 to 0.72, with a 92% relative recall (the new classifier rejected another 8% of the 300 events). This experiment is displayed",null,null
273,430,null,null
274,"in Figure 7, where the blue bars correspond to the training data, the red bar with the original performance of the lowprecision events, and the green bar with the supervised precision of the low-precision events.",null,null
275,This experiment supports our claim that a large amount of false-positives from our technique are simple to eliminate with minimal training data and annotation effort (~30 total hours of annotator time for the 1100 train/validate/evaluation judgments used).,null,null
276,5.4 Geographic Evaluation,null,null
277,"We use a publicly-available geocoding and address API3 to convert our extracted ""Where"" fields into full street addresses, and latitude/longitude information. The street addresses are used to evaluate our performance on future retrieval tasks, and the latitude and longitude pairs are used to generate maps to better understand our coverage and that of the Schema.org data.",null,null
278,5.4.1 City Coverage around the World,null,null
279,Table 4: Retrieval Metrics for 200 random cities.,null,null
280,MRR@5 P@1 P@2 P@3 P@4 P@5,null,null
281,0.78,null,null
282,0.71 0.70 0.69 0.70 0.70,null,null
283,"The TREC Contextual Suggestion Track considers the task of recommending venues or destinations to users in new cities [5]. In this work, we briefly evaluate the suitability of our dataset to be applied to a similar task: the task of recommending events to users based on their location. Because the work we present is not about the recommendation task itself, but rather the extraction of events, we evaluate our ability to provide event coverage in a random city.",null,null
284,"We group our events by city, and select events from 200 cities at random. We then judge, using Amazon Mechanical Turk, the 5 highest scoring events from each city. We evaluate as a retrieval task, where our query is the random city, and our ranking function for now is our confidence in the event prediction. The mean reciprocal rank (MRR) of finding an event is 0.78 on this data: on average there is an event at either rank 1 or rank 2. Our event precision at 1 is 0.71. Full results are listed in Table 4. The precision numbers here seem flat because there tend to be five or so valid events for each location. It is nice to see this consistency, because it suggests that our system is able to find many events for randomly selected locations.",null,null
285,5.4.2 Geographic Display,null,null
286,"In Figure 8, we display a small world map where our events are plotted to understand our international coverage. Since the dataset we used, ClueWeb12 is biased toward English pages, it is understandable that the events we have extracted are mostly found in the United States, and north-western Europe. There is also some token coverage in other parts of the world, including India and Australia, but we focus on areas that we expect to have good coverage based on the ClueWeb12 English-speaking bias.",null,null
287,"In Figure 9, you can see our events plotted on a US Map in two colors: green for high and very-high confidence P >,"" 0.65, and yellow for 0.55 < P < 0.65. The blue dots are events parsed from Schema.org pages. The key takeaway from this""",null,null
288,3https://developers.google.com/maps,null,null
289,"Figure 8: Events across the world. Note that we have good coverage of countries where English is the dominant language, in a reflection of our corpus, ClueWeb12.",null,null
290,"Figure 9: Events within the United States. Blue data points indicate Schema.org data, Green represents high confidence (P,""0.65..0.85+), and Yellow represents low to medium confidence points (P"",""0.55..0.65). All points have more than 5 unique events, and are logarithmically scaled according to weight. map is that Schema.org events are definitely more prevalent in large cities, specifically in New York, Los Angeles, and Chicago, and visually, our technique does quite a good job of expanding coverage geographically. Note that we only plot a point if there are more than five unique event occurrences at that location, and we have restricted our sampled precision to be above 0.55, and there are many more events at the larger dots, as their size is logarithmically scaled.""",null,null
291,6. CONCLUSION,null,null
292,"In this work, we introduce a new task inspired by the TREC Contextual Suggestion Track and the increasing population of mobile users: to retrieve and recommend events that users might want to attend. Because we are particularly interested in local events, our work focuses on the identification and extraction of events on the open internet.",null,null
293,"We show that using semantic web technologies, and specifically Schema.org microdata to collect distant supervision data can be useful for training semi-supervised approaches to extraction problems like ours. This setup has the advantage that as more organizations adopt such technologies, the performance of our extractions will increase over time without any additional labeling effort.",null,null
294,"We propose a novel model of event extraction based on independent field scoring and a greedy algorithm for grouping these fields into complete event records, with confidence scores assigned to them. Our features for field classification",null,null
295,431,null,null
296,"allow us to combine elements commonly seen in region extraction approaches with both information retrieval-based and linguistically-inspired features. As the proposed model depends heavily upon the ability to assign individual field scores, we evaluate a number of score assignment methods, and find that classification that learns from using the distant supervision of the Schema.org data is significantly beneficial.",null,null
297,7. ACKNOWLEDGEMENTS,null,null
298,"This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",null,null
299,8. REFERENCES,null,null
300,"[1] M. D. Adelfio and H. Samet. Schema extraction for tabular data on the web. VLDB'13, 6(6):421­432, 2013.",null,null
301,"[2] H. Becker. Identification and characterization of events in social media. PhD thesis, Columbia University, 2011.",null,null
302,"[3] C. Bizer, K. Eckert, R. Meusel, H. Mu¨hleisen, M. Schuhmacher, and J. V¨olker. Deployment of rDFa, microdata, and microformats on the web­A quantitative analysis. ISWC, pages 17­32, 2013.",null,null
303,"[4] M. J. Cafarella, A. Halevy, and J. Madhavan. Structured data on the web. CACM'11, 54(2):72­79, 2011.",null,null
304,"[5] A. Dean-Hall, C. L. Clarke, J. Kamps, P. Thomas, N. Simone, and E. Voorhees. Overview of the trec 2013 contextual suggestion track. Technical report, DTIC Document, 2013.",null,null
305,"[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871­1874, 2008.",null,null
306,"[7] A. L. Gentile, Z. Zhang, I. Augenstein, and F. Ciravegna. Unsupervised wrapper induction using linked data. In K-CAP '13, pages 41­48, New York, NY, USA, 2013. ACM.",null,null
307,"[8] A. L. Gentile, Z. Zhang, and F. Ciravegna. Self training wrapper induction with linked data. In Text, Speech and Dialogue, pages 285­292. Springer, 2014.",null,null
308,"[9] R. Gupta and S. Sarawagi. Answering table augmentation queries from unstructured lists on the web. VLDB'09, 2(1):289­300, 2009.",null,null
309,"[10] S. Gupta, G. Kaiser, D. Neistadt, and P. Grimm. Dom-based content extraction of html documents. In WWW'03, pages 207­214. ACM, 2003.",null,null
310,"[11] J. Hoffart, F. M. Suchanek, K. Berberich, and G. Weikum. Yago2: a spatially and temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28­61, 2013.",null,null
311,"[12] J. L. Hong, E.-G. Siew, and S. Egerton. Information extraction for search engines using fast heuristic techniques. Data & Knowledge Engineering, 69(2):169­196, 2010.",null,null
312,"[13] F. Jelinek and R. L. Mercer. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, 1980.",null,null
313,"[14] M. Kayaalp, T. Ozyer, and S. Ozyer. A collaborative and content based event recommendation system integrated with data collection scrapers and services at a social networking site. In ASONAM'09, pages 113­118. IEEE, 2009.",null,null
314,"[15] E. Kuzey, J. Vreeken, and G. Weikum. A fresh look on knowledge bases: Distilling named events from news. In CIKM'14, pages 1689­1698. ACM, 2014.",null,null
315,"[16] D. Lagun, A. Sud, R. W. White, P. Bailey, and G. Buscher. Explicit feedback in local search tasks. In SIGIR'13, pages 1065­1068. ACM, 2013.",null,null
316,"[17] G. Limaye, S. Sarawagi, and S. Chakrabarti. Annotating and searching web tables using entities, types and relationships. VLDB'10, 3(1-2):1338­1347, 2010.",null,null
317,"[18] R. Manjula and A. Chilambuchelvan. Extracting templates from web pages. In Green Computing, Communication and Conservation of Energy (ICGCE), 2013 International Conference on, pages 788­791. IEEE, 2013.",null,null
318,"[19] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. The Stanford CoreNLP natural language processing toolkit. In ACL, pages 55­60, 2014.",null,null
319,"[20] D. Metzler, C. Cai, and E. Hovy. Structured event retrieval over microblog archives. In ACL'12, pages 646­655. Association for Computational Linguistics, 2012.",null,null
320,"[21] H. Mu¨hleisen and C. Bizer. Web data commons-extracting structured data from two large web corpora. LDOW, 937, 2012.",null,null
321,"[22] P. Petrovski, V. Bryl, and C. Bizer. Learning regular expressions for the extraction of product attributes from e-commerce microdata. 2014.",null,null
322,"[23] J. Pomik´alek. Removing boilerplate and duplicate content from web corpora. Disertacni pra´ce, Masarykova univerzita, Fakulta informatiky, 2011.",null,null
323,"[24] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR'98, pages 275­281. ACM, 1998.",null,null
324,"[25] J. Pustejovsky, P. Hanks, R. Sauri, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. Day, L. Ferro, et al. The timebank corpus. In Corpus linguistics, volume 2003, page 40, 2003.",null,null
325,"[26] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake shakes twitter users: real-time event detection by social sensors. In Proceedings of the 19th international conference on World wide web, pages 851­860. ACM, 2010.",null,null
326,"[27] M. Schmachtenberg, C. Bizer, and H. Paulheim. Adoption of the linked data best practices in different topical domains. ISWC, pages 245­260, 2014.",null,null
327,[28] H. Sleiman and R. Corchuelo. Trinity: on using trinary trees for unsupervised web data extraction. 2013.,null,null
328,"[29] H. A. Sleiman and R. Corchuelo. A survey on region extractors from web documents. Knowledge and Data Engineering, IEEE, 25(9):1960­1981, 2013.",null,null
329,"[30] H. A. Sleiman and R. Corchuelo. Tex: An efficient and effective unsupervised web information extractor. Knowledge-Based Systems, 39:109­123, 2013.",null,null
330,"[31] D. A. Smith. Detecting and browsing events in unstructured text. In SIGIR'02, pages 73­80. ACM, 2002.",null,null
331,"[32] D. A. Smith. Detecting events with date and place information in unstructured text. In Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries, pages 191­196. ACM, 2002.",null,null
332,"[33] J. Str¨otgen and M. Gertz. Heideltime: High quality rule-based extraction and normalization of temporal expressions. In Workshop on Semantic Evaluation, ACL, pages 321­324, 2010.",null,null
333,"[34] W. Thamviset and S. Wongthanavasu. Information extraction for deep web using repetitive subject pattern. WWW'13, pages 1­31, 2013.",null,null
334,"[35] W. Thamviset and S. Wongthanavasu. Bottom-up region extractor for semi-structured web pages. In ICSEC'14, pages 284­289. IEEE, 2014.",null,null
335,"[36] K. Watanabe, M. Ochi, M. Okabe, and R. Onai. Jasmine: a real-time local-event detection system based on geolocation information propagated to microblogs. In CIKM'11, pages 2541­2544. ACM, 2011.",null,null
336,"[37] W. J. Welch. Algorithmic complexity: three np-hard problems in computational statistics. Journal of Statistical Computation and Simulation, 15(1):17­25, 1982.",null,null
337,"[38] Z. Yu. High accuracy postal address extraction from web pages. Master's thesis, Dalhousie University, Halifax, Nova Scotia, 2007.",null,null
338,"[39] Z. Zhang. Towards efficient and effective semantic table interpretation. In ISWC'14, pages 487­502. Springer, 2014.",null,null
339,432,null,null
340,,null,null

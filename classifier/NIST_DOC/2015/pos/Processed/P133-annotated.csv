,sentence,label,data
,,,
0,Learning by Example: Training Users with High-quality Query Suggestions,null,null
,,,
1,Morgan Harvey,null,null
,,,
2,MIS Department Northumbria University,null,null
,,,
3,"pwhq2@ Nenwocartshtlue,mUbKria.ac.uk",null,null
,,,
4,Claudia Hauff,null,null
,,,
5,Web Information Systems TU Delft,null,null
,,,
6,c.hThaeufNf@ethteurdlaenldfts.nl,null,null
,,,
7,David Elsweiler,null,null
,,,
8,I:IMSK University of Regensburg Germany,null,null
,,,
9,david@elsweiler.co.uk,null,null
,,,
10,ABSTRACT,null,null
,,,
11,"The queries submitted by users to search engines often poorly describe their information needs and represent a potential bottleneck in the system. In this paper we investigate to what extent it is possible to aid users in learning how to formulate better queries by providing examples of high-quality queries interactively during a number of search sessions. By means of several controlled user studies we collect quantitative and qualitative evidence that shows: (1) study participants are able to identify and abstract qualities of queries that make them highly eective, (2) after seeing high-quality example queries participants are able to themselves create queries that are highly eective, and, (3) those queries look similar to expert queries as defined in the literature. We conclude by discussing what the findings mean in the context of the design of interactive search systems.",null,null
,,,
12,"Categories and Subject Descriptors: H.3.3 Information Search and Retrieval General Terms: Measurement, Experimentation, Human Factors Keywords: Search expertise; Reflection; Behavioural Change; User Study",null,null
,,,
13,1. INTRODUCTION,null,null
,,,
14,"Much of the IR research in the last half century has, with great success, focused on developing improved retrieval models to enhance the utility of retrieval systems for the end user [41]. In this line of research search queries submitted to a retrieval system are considered as a given. The focus is placed on what to do systematically to return relevant documents given this limited representation of the user's information need. A complementary approach with potentially more scope for future performance gains is to focus on giving the system more to work with by assisting users in creating better queries for specific search systems [32, 24].",null,null
,,,
15,All three authors contributed equally to this work.,null,null
,,,
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile.",null,null
,,,
17,c 2015 ACM. ISBN 978-1-4503-3621-5/15/08 ...$15.00.,null,null
,,,
18,DOI: http://dx.doi.org/10.1145/2766462.2767731.,null,null
,,,
19,"Considerable evidence exists showing that many users do not know how to generate good queries. Analyses of search transaction logs show that people use short queries, especially on the Web [2] and even in this familiar domain a good proportion of searches fail completely [13]. In many search domains, including Web and Email search (with domainspecific search systems and interfaces), expert users achieve better retrieval eectiveness than novices and demonstrate dierent querying behaviour [3, 12, 43]. Moreover, despite the fact that most users today have to navigate through a range of search systems in their digital life, it has been reported that many users are inflexible in their approach and tend to use the same querying strategies regardless of task and available search system [29].",null,null
,,,
20,"Typical solutions to assist users in creating eective search queries are the use of search UI features, such as query suggestions [35], related searches [36] or query autocompletion [5]. Alternatively, systems can employ context and personalisation techniques [11], which involve storing (and learning from) personal search histories and preferences to understand what a user knows and likes [18].",null,null
,,,
21,"A third approach is to educate users about how to become better searchers [28] or to help users reflect on their own behaviour by comparing it to experts [6]. This method has the advantage that it is complementary to technical solutions. Our research continues along this path by investigating to what extent we can teach users how to pose better search queries to a particular search system. In contrast to existing approaches, we aim to understand if users are able to recognise, compare and contrast the properties of their own queries with good queries (provided by the system) and make changes to the queries they generate based on these insights. This is a new way of thinking about query suggestions; instead of providing automated examples for users to simply click on, we present them in a way that leads the user to reflect on his own behaviour, positively influencing his actions as a result.",null,null
,,,
22,The two principle research questions we answer are:,null,null
,,,
23,"RQ1 Are users able to notice dierences between good queries and their own and abstract these dierences to change their own behaviour? If so, what are the noticeable dierences?",null,null
,,,
24,"RQ2 How eectively can users learn and abstract from good queries; do users who are ""trained"" perform better than users who did not receive the training? Which properties of their queries do users adapt after training?",null,null
,,,
25,133,null,null
,,,
26,2. RELATED WORK,null,null
,,,
27,"It is well-recognised that searchers have di culties communicating their information needs [7, 38, 24]. Taylor writes of a series of stages a user goes through when seeking information. These range from experiencing a visceral need, which is ""probably inexpressible in linguistic terms"" to a compromised need - a ""representation of the inquirer's need within the constraints of the system and its files"" [38]. Therefore, in order to generate successful queries, the user must overcome several cognitive challenges: 1) to determine himself what the need is and what kind of document will solve it; 2) to choose terms that describe that document well out of a very large set of possibilities [15] and 3) to communicate using the system's vocabulary and not his own [9].",null,null
,,,
28,Many interactive solutions have been designed to help the user overcome these challenges and improve the representations of information needs systems have to work with. The following subsection briefly reviews such work.,null,null
,,,
29,2.1 Interactive Query Support,null,null
,,,
30,"IR systems can attain better descriptions of information needs by explicitly asking for certain details. The I3R system oered a means for users to provide terms and concepts they felt were important and identify relationships between these terms and other concepts in the domain [10]. Similarly, Kelly and Fu [24] used clarification forms to elicit additional information about the search context from users. The forms queried users on what they knew and what they would like to know about the topic and why. These were shown to be helpful in achieving improved retrieval performance.",null,null
,,,
31,"A second technique is to assist the user to iteratively improve their own queries by adding additional terms suggested by the system, commonly referred to as interactive query expansion (IQE) [17]. This approach gives the user much more control over the search than if the query were to be expanded automatically (i.e. where the system selects expansion terms without user input [31]). Although IQE can oer improved performance [25], it has been shown that users are poor at identifying the terms that will oer the best improvement to their queries [33, 1]. This finding is intriguing with respect to our aims as it begs the question of whether or not users are able to identify qualities of good terms or whether they just assume terms suggested by a system will automatically be of a high quality.",null,null
,,,
32,"Relevance feedback systems [34] are a further means to expand queries without explicitly choosing terms. Instead, relevance judgements are solicited on the returned documents. In addition to expanding queries, other scholars have investigated the performance of systems suggesting similar or related queries e.g. [36].",null,null
,,,
33,"Improving user queries need not be achieved via technical solutions. One group in the 2007 SIGIR workshop breakout group identified a spectrum of possible solutions from manually-led approaches (based on improved information literacy and teaching) through to automatic, system-based approaches (based on more intelligent systems) [32]. The following section reviews literature on changing user behaviour via primarily non-technical means.",null,null
,,,
34,2.2 Changing Behaviour,null,null
,,,
35,"Behaviour change support systems are ""information systems designed to form, alter, or reinforce attitudes or behaviours or both without using coercion or deception"" [30].",null,null
,,,
36,"Within the context of search, changes can be made to the underlying retrieval engine or to the interface to `nudge' people towards submitting longer or better queries or to look deeper in the results list [6]. Altering the size [14] and wording [8] of the search box, for example, has been shown to influence the length of queries submitted. Moreover providing a simple ""Google-like"" search interface as opposed to a complicated multi-field catalogue search can radically alter user behaviour [27]. Training users on how to construct queries can improve search behaviour [26]. For example, providing guidance on the advanced features that can help with specific search tasks can improve performance for these tasks and users are able to preserve and use the knowledge gained weeks later [28]. Moreover, allowing users to reflect on their own behaviour and, importantly, compare their behaviour to other, expert users, enables individuals to improve their own habits. In [6] users, after reflection, spent longer considering search results and issued longer queries. They also used a wider range of techniques and search engine features.",null,null
,,,
37,"We extend some of the ideas in [6] here. Rather than inviting users to compare their behaviour with that of experts, however, we investigate if they are able to learn by comparing their own queries to examples generated by the system to be near optimal for the task at hand. In doing so we relate the kinds of approaches shown in Section 2.1 with the approaches in this section. We attempt to 'nudge' users to improve their queries via high-quality examples shown via widgets similar to those described above.",null,null
,,,
38,3. RESEARCH METHODOLOGY,null,null
,,,
39,The aim of our work is to establish whether showing users of an unfamiliar search system examples of high-quality queries (for a small number of information needs) enables them to create better-performing queries themselves. We investigate to what extent users learn more successful querying behaviours from those examples.,null,null
,,,
40,Based on our two research questions (Section 1) we devised the following research hypotheses:,null,null
,,,
41,H1 Users are able to adapt their querying behaviour to pose good queries to an unfamiliar search system.,null,null
,,,
42,H2 Users are able to identify characteristics of highperforming queries that allow them to perform so well.,null,null
,,,
43,"H3 A small number of ""training queries"" is su cient to enable a user to learn how to pose good queries themselves.",null,null
,,,
44,"H4 A user who receives training with queries he can relate to (i.e. that are anticipated to perform well), learns better than a user receiving training with queries that are not predicted to perform well.",null,null
,,,
45,"H5 A user who receives training with queries he can relate to, learns faster than a user receiving training with queries that are not predicted to perform well.",null,null
,,,
46,"We conducted a number of user studies (Figure 1), each requiring the automatic generation of high-quality queries for a given information need and search system (described in Section 4.1). To address the issue of predicted performance, we performed an initial user study (Section 4.2) to investigate participants' perceptions of the generated queries. In contrast to the later studies, participants were not given access to our search system, their judgement was solely based on their own past experience.",null,null
,,,
47,134,null,null
,,,
48,Figure 1: Overview of our experimental design.,null,null
,,,
49,"Concurrently with the User Perception Study, we performed a Pilot Study (Section 4.3) which gave us qualitative insights into the characteristics of good queries that users were able to identify. The results of these two studies then allowed us to conduct a larger Main Study (Section 4.4) and a follow-up Variable Training Size Study (Section 4.5) with a consistent design, but dierent training parameters. The aim here was to better understand how much training is required to achieve an eect. In each of these studies participants were asked to perform a series of ad-hoc retrieval tasks using our search system.",null,null
,,,
50,"To maintain maximum control over the experiments and have access to complete statistics of the collection the participants were searching over, we used a standard test collection: AQUAINT1 together with the 50 TREC 2005 Robust track queries [40]. As our indexing and search engine we chose Apache SOLR2. To provide our study participants with a familiar user interface for searching the collection, we developed a web-based front-end in PHP (Figure 3).",Y,null
,,,
51,4. EXPERIMENTS & RESULTS,null,null
,,,
52,"In this section, we present an overview of each study and its results in turn.",null,null
,,,
53,4.1 Generating High-Performing Queries,null,null
,,,
54,"In generating the ""high-performing"" query examples, we make the assumption that a query qa is better than another query qb for a given information need if qa returns a higher Average Precision (AP) score. It is also important that the queries are understandable by humans and are not excessively long. Therefore, we are not interested in queries that happen to return good results because of a statistical anomaly or because they are overly verbose and specific.",null,null
,,,
55,"Candidate queries were obtained via a recursive, greedy search algorithm. For each topic and its corresponding set of relevant documents, a collection was built consisting of only those relevant documents. The query building process is initiated by first considering only queries of length 1 (i.e. single-term queries) and choosing each of the top 100 terms from the topic-specific document collection (after stop words had been removed). Each initialisation of the recursive method takes in a base query and adds each of the top 100 terms to it. All 100 new potential queries are run against the entire collection using the standard SOLR search system and the AP score of the top 50 returned documents is computed. The list of queries is then ranked by their AP values and the top 10 are added to the candidate query list. Subsequently, the algorithm is initiated again with new base query having the newly-selected term added to the end. This recursive process was continued up to a",null,null
,,,
56,"1We removed duplicate documents in a pre-processing step, to provide a better and more familiar user experience.",null,null
,,,
57,2,null,null
,,,
58,http://lucene.apache.org/solr/,null,null
,,,
59,query length of 4. At the end of the process any duplicate queries were removed and the top 100 queries (according to AP) were selected as the final list of candidates.,null,null
,,,
60,"Note that this approach diers significantly from previous methods proposed in the literature for generating queries, e.g. [4], as our goal is fundamentally dierent. Rather than generating queries which appear to be samples from the collection (i.e. stochastically drawn from collection statistics), we are specifically interested in queries which yield high performance, are understandable and would, potentially, be posed by real users. Other related approaches used to find optimal queries in Boolean systems (e.g. [37]) were inappropriate due to dierences in the underlying retrieval system. While our greedy approach does not produce globally optimal queries, it quickly produces large numbers of queries with an AP score of around 0.4. Concrete examples of generated queries can be found in the last column of Table 4.",null,null
,,,
61,"Considering the top 100 queries for each topic, the median AP obtained by the generated queries over the first 20 returned results was 0.389. On a per-topic basis, the median was 0.391, the lowest average achieved was 0.054 and the maximum was 0.948 (IQR,""0.31). Overall, 28 out of 50 topics had at least one query with an AP score greater than 0.5 and only 11 topics had any queries in the top 100 with an AP score below 0.2.""",null,null
,,,
62,4.2 User Perception of Queries,null,null
,,,
63,"To gain insights into how users perceive our high-quality queries (and as a precursor to answering hypotheses H4 and H5), we conducted a crowd-sourcing experiment on the CrowdFlower3 platform.",null,null
,,,
64,4.2.1 Study Overview,null,null
,,,
65,"Each crowd-sourced task consisted of one search topic (in natural language form) and one of the queries generated in Section 4.1. Specifically, the workers were instructed as follows:",null,null
,,,
66,You already know query suggestions from search engines such as Google that present you with suggested queries while you type or show related queries alongside search results.,null,null
,,,
67,"In this task, you will be given an information need (in natural language form) and a query suggestion that has been derived for this information need. You are asked to judge the query suggestion along three dimensions - surprise, usage and relevance.",null,null
,,,
68,Four questions had to be answered on a 5-point Likert scale:,null,null
,,,
69,"1. How much do you know about the topic of the information need? (1: Very little, 5: A lot)",null,null
,,,
70,"2. How surprised are you about the generated query suggestion? (1: Not at all surprised, 5: Extremely surprised)",null,null
,,,
71,"3. Would you use this suggestion in an actual search? (1: No, I would not use it, 5: Yes I would use it)",null,null
,,,
72,"4. What do you think the search result quality will be if this suggestion is used as query? (1: Very low quality, 5: Very high quality)",null,null
,,,
73,3,null,null
,,,
74,http://www.crowdflower.com,null,null
,,,
75,135,null,null
,,,
76,Each job consisted of 10 tasks and workers were paid 12 cents (a standard rate). In this and all following CrowdFlower experiments the participants were restricted to countries where English is a native language.,null,null
,,,
77,"For each of the Robust track topics, the 15 most eective queries generated were judged by CrowdFlower workers. Each query was judged by 3 workers, and thus, for each topic 45 judgements were collected. Three examples of topics, generated suggestions and worker ratings are shown in Table 4.",null,null
,,,
78,4.2.2 Results,null,null
,,,
79,"Our workers found many of the search topics rather challenging with an average topic knowledge rating of 2.21. The most familiar topics tended to be of broad interest to many dierent communities; the two with the highest average knowledge ratings (3.00 and 2.89 respectively) were What factors contributed to the growth of consumer on-line shopping? (topic 639) and Identify drugs used in the treatment of mental illness. (topic 383). In contrast, search topics focusing on very specific themes or entities tended to elicit the lowest familiarity ratings; the topic with the lowest average knowledge rating (1.58) was What is the status of The Three Gorges Project? (topic 416).",null,null
,,,
80,"When considering how unexpected the presented suggestions were (i.e. the ""surprise"" factor) we found that the vast majority of queries (more than 80%) were at least somewhat expected, receiving a rating between 1 and 3 (top-left of Figure 2). Only a small number of suggestions were considered to be extremely surprising and those were mostly found in topics our study participants knew little about. This indicates that our query generation approach is achieving its goal of generating queries understandable to humans.",null,null
,,,
81,Number of ratings Number of ratings,null,null
,,,
82,Number of ratings,null,null
,,,
83,600,null,null
,,,
84,500,null,null
,,,
85,400,null,null
,,,
86,300,null,null
,,,
87,200,null,null
,,,
88,100,null,null
,,,
89,0,null,null
,,,
90,1,null,null
,,,
91,2,null,null
,,,
92,3,null,null
,,,
93,4,null,null
,,,
94,5,null,null
,,,
95,Rating,null,null
,,,
96,800,null,null
,,,
97,800,null,null
,,,
98,600,null,null
,,,
99,400,null,null
,,,
100,200,null,null
,,,
101,0,null,null
,,,
102,1,null,null
,,,
103,2,null,null
,,,
104,3,null,null
,,,
105,4,null,null
,,,
106,5,null,null
,,,
107,Rating,null,null
,,,
108,600,null,null
,,,
109,400,null,null
,,,
110,200,null,null
,,,
111,0,null,null
,,,
112,1,null,null
,,,
113,2,null,null
,,,
114,3,null,null
,,,
115,4,null,null
,,,
116,5,null,null
,,,
117,Rating,null,null
,,,
118,"Figure 2: Histograms of ""surprise"" (top-left), ""search quality"" (top-right) and ""suggestion usage"" (bottom) ratings across the 750 dierent generated query suggestions, each rated by three users.",null,null
,,,
119,"Of note is that fewer than 7% of judgements estimated the queries to achieve a very high quality of search results (top-right of Figure 2), while in contrast nearly 17% of the judgements were rated as likely to return very low quality search results. This result indicates that users are not able to judge the quality of query suggestions well, corroborating previous findings that users are unable to dierentiate good search terms from bad ones [33, 1]. This result can only be partially explained by their lack of topical domain knowledge as the correlation between knowledge ratings and",null,null
,,,
120,"search quality ratings was moderate (but significant) with r , 0.35.",null,null
,,,
121,"Lastly, we consider the question of to what extent users would use the shown suggestions in an actual search (bottom of Figure 2). Not unexpectedly, the correlation between the estimated search result quality and the potential usage is high (r ,"" 0.77). Based on the ratings we have to conclude that many suggestions are not convincing, only a small number would definitely be used (9% of those rated 5) while 30% would definitely not be used (ratings of 1).""",null,null
,,,
122,"In summary, we find that user perception of our highquality queries varies; many of them are not recognised as being eective. We make use of this result in the Main Study: one group of users receives high-quality suggestions recognised as high quality in this study, while another group of users receives high-quality suggestions that were rated as low quality in this study.",null,null
,,,
123,4.3 Pilot Study,null,null
,,,
124,"The pilot study had three goals: (i) to test the validity of our system and task setup, (ii) to learn more about experimental factors such as participant fatigue, and (iii) most importantly, to collect qualitative data in order to establish whether participants are able to notice qualities of example queries that make them so eective as hypothesised in H2.",null,null
,,,
125,4.3.1 Study overview,null,null
,,,
126,"The participants (n,""22) consisted of university students and sta members recruited via email lists and announcements in lectures from a major European university. Although the participants were not native English speakers, all had advanced English language skills. They were given access to our search system and asked to complete 10 search tasks. As seen in Figure 3 the information need was prominently displayed to the participants. Each time they issued a query (1), its retrieval eectiveness was displayed (5) in terms of the number of returned relevant documents within the top 20 results and the average precision (which was referred to as """"search performance score""""). Any relevant documents returned by the search were highlighted in blue (4).""",null,null
,,,
127,"The participants were instructed to submit queries that they believed would return relevant documents (i.e. useful and containing information pertinent to the task). They were told that the documents had already been evaluated for relevance and that each submitted query would be scored in terms of how many relevant documents were returned in the top 20 results and the positions of those documents within the ranked list. This second score is simply average precision as used during the automatic query generation process and users were encouraged to focus on this to determine how well they were doing in the task. Users could move on to the next topic with a click on the New topic, please button (6). Due to the interactive nature of the study, we selected 10 of the 50 Robust TREC topics by first eliminating those that that were either very di cult or very easy for our search system (measured in average precision achieved when using the title of the search topic as query) and then drawing randomly from the remaining topics4.",Y,null
,,,
128,The study participants were provided with query suggestions as shown in Figure 3 (3) similarly to how Web search,null,null
,,,
129,"4The topics used were 303*, 362, 367*, 375*, 378, 383*, 401, 426*, 638* and 689. * indicates those that were later also used in the Main Study.",null,null
,,,
130,136,null,null
,,,
131,2,null,null
,,,
132,3,null,null
,,,
133,1 6,null,null
,,,
134,5 4,null,null
,,,
135,Figure 3: Screenshot of search interface showing a list of search results as well as some query suggestions.,null,null
,,,
136,"engines often present query suggestions. After participants have posed their first two queries to the system for a particular topic, they are shown a number of our high-quality query suggestions. All displayed suggestions are more eective (achieve an AP at least 10% higher) than the participant's previous queries. Thus, dierent participants receive dierent suggestions, depending on the quality of their initial queries. The interface conveys to the participant that these are high-quality queries and they are encouraged to use them (Figure 3 (3)).",null,null
,,,
137,"To test hypothesis H2, i.e. to establish whether users are able to learn from high quality query examples, after every use of a suggestion participants were prompted to describe in a text box why they considered it to be eective: ""You used the suggested query [query]. Considering your previous queries for this topic (shown below), what do you think is it about the suggested query that makes it so eective? "".",null,null
,,,
138,4.3.2 Results,null,null
,,,
139,"The pilot findings help fine-tune our setup for the Main Study. Overall, the setup worked well, however we did establish fatigue to be a considerable factor. Figure 4 plots for each topic in sequence (recall that study participants receive the 10 topics in random order) the AP achieved by all queries submitted for the nth topic across all study participants. It is evident that over time (i.e. queries submitted for later topics) the retrieval eectiveness degrades. In particular after the 7th topic, the median AP is close to zero.",null,null
,,,
140,"To investigate hypothesis H2 we analysed the free-text explanations from participants describing why they believe the example queries performed so well. The responses show that participants were indeed able to identify positive query characteristics. In total 81 descriptions were supplied and out of the 22 participants, 15 gave at least one description of a suggestion. 3 participants gave descriptions for all of the suggested queries they used.",null,null
,,,
141,"We analysed the responses qualitatively using an a nity diagramming technique, a process allowing the discovery and validation of patterns in qualitative data [16]. 12 codes were generated describing qualities participants assigned to high-performing suggestions. These are shown in Table 1.",null,null
,,,
142,Category,null,null
,,,
143,C1: Specific query terms (specification),null,null
,,,
144,C2: More general query terms (generalisation),null,null
,,,
145,C3: Queries not in topic description,null,null
,,,
146,C4: Unexpected or surprising vocabulary,null,null
,,,
147,C5: Surprising non-use of vocabulary,null,null
,,,
148,"C6: Uses term the user was surprised at the usefulness of (i.e. perhaps not surprising given the topic, but surprising that it was good for performance)",null,null
,,,
149,C7: Thinking creatively,null,null
,,,
150,C8: Advanced vocabulary (rare but not on a specialist subject relating to the topic),null,null
,,,
151,C9: Specialist vocabulary (rare and to do with a specialist subject relating to the topic),null,null
,,,
152,C10: Good combination of search terms,null,null
,,,
153,C11: Using synonyms and related concepts,null,null
,,,
154,C12: Query requires specialist or background knowledge,null,null
,,,
155,Table 1: Overview of the query categories identified during the pilot study,null,null
,,,
156,Not only does the established coding scheme provide evidence that users are capable of noticing and abstracting dierences between the suggested queries and their own - a prerequisite to learning - but the responses given are similar to those reported in the literature as being useful query reformulation strategies [23] or typical for queries submitted,null,null
,,,
157,137,null,null
,,,
158,"Figure 4: Pilot study: Average precision over sequences of topics showing fatigue. The nth element of the box plot contains the AP achieved over all queries across all users submitted for the nth topic the users worked on (since topics were issued in randomised order, the topic sequence diers per user).",null,null
,,,
159,"by system and domain experts. For example, a common way to improve queries is to either make them more specfic (C1) or general (C2) [23]. Experts submit queries which are more elaborate [21] (C7, C11, C12), use broader and more varied vocabulary [39] (C1, C2), exploit synonyms and related concepts [22](C11), and include terms not used in topic descriptions [21]. Moreover, domain experts often search with queries containing specialist or domain knowledge [42] (C9, C12).",null,null
,,,
160,"We take this as evidence to accept hypothesis H2. It is important to point out, however, that some of the participants explicitly mentioned in their responses that they would not be able to create some of the examples due to lack of domain knowledge or vocabulary (C10, C13).",null,null
,,,
161,"We conclude that, despite the fact that participants are not universally able to recognise good queries (Section 4.2.2), our pilot data show that for many queries people can determine a range of properties that explain good performance.",null,null
,,,
162,4.4 Main Study,null,null
,,,
163,"The main study addresses hypotheses H1, H3, H4 & H5 and draws from the outcomes of the two previously discussed user studies.",null,null
,,,
164,4.4.1 Study overview,null,null
,,,
165,"In this study, we use search topics that our workers in the user perception study considered themselves knowledgeable about to reduce the potential influence of domain knowledge on our results. We base our choice of experimental conditions on the reported perceptions of queries to reflect H2 and we reduced the number of tasks to six in an eort to counteract the fatigue eect observed in the pilot.",null,null
,,,
166,We use a between-groups design with participants randomly assigned to one of three experimental conditions:,null,null
,,,
167,· Group G,null,null
,,,
168,: this experimental group receives high-,null,null
,,,
169,exp high,null,null
,,,
170,quality query suggestions in the training phase which,null,null
,,,
171,were predicted to be eective in the user perceptions,null,null
,,,
172,study (Section 4.2).,null,null
,,,
173,· Group G,null,null
,,,
174,: this experimental group receives high-,null,null
,,,
175,exp low,null,null
,,,
176,quality query suggestions in the training phase which,null,null
,,,
177,were predicted to be ineective in the user perceptions,null,null
,,,
178,study (Section 4.2).,null,null
,,,
179,· Group G : the control group does not receive any control query suggestions.,null,null
,,,
180,For groups G,null,null
,,,
181,and G,null,null
,,,
182,", where suggestions are",null,null
,,,
183,exp high,null,null
,,,
184,exp low,null,null
,,,
185,"given, we split tasks into two phases: the first four top-",null,null
,,,
186,"ics are considered the training phase, where suggestions are",null,null
,,,
187,"shown, and the final two tasks are referred to as the test",null,null
,,,
188,"phase, where no suggestions are presented. Suggestions are",null,null
,,,
189,provided using the same approach and interface as in the Pi-,null,null
,,,
190,"lot Study, i.e. suggestions were only given after two freely-",null,null
,,,
191,created queries had been submitted and when there were,null,null
,,,
192,queries available that would increase the AP score by at,null,null
,,,
193,"least 10%. Again, topics were issued in random order.",null,null
,,,
194,"The participants (n,""91, 29 in G""",null,null
,,,
195,", 34 in G",null,null
,,,
196,and,null,null
,,,
197,exp high,null,null
,,,
198,exp low,null,null
,,,
199,28 in G ) were also recruited via CrowdFlower and were,null,null
,,,
200,control,null,null
,,,
201,paid 50 cents for the completion of a job. A job consisted,null,null
,,,
202,of using our search system on six ad-hoc retrieval tasks; the,null,null
,,,
203,study participants were not informed about the two phases,null,null
,,,
204,"of the study, they simply performed six search tasks (after",null,null
,,,
205,four of which the query suggestion UI element was removed).,null,null
,,,
206,4.4.2 Results,null,null
,,,
207,"We first compare the eectiveness of the issued queries, before looking at properties of the submitted queries and the fatigue factor.",null,null
,,,
208,Average Precision,null,null
,,,
209,0.4,null,null
,,,
210,0.5,null,null
,,,
211,0.4 0.3,null,null
,,,
212,0.3 0.2,null,null
,,,
213,0.2,null,null
,,,
214,0.1 0.1,null,null
,,,
215,0,null,null
,,,
216,0,null,null
,,,
217,Gexp_high Gexp_low Gcontrol,null,null
,,,
218,Gexp_high Gexp_low Gcontrol,null,null
,,,
219,Figure 5: Main study: Querying performance over groups. Left: training topics. Right: test topics.,null,null
,,,
220,Effectiveness of Submitted Queries.,null,null
,,,
221,The fairest way to compare the performance across groups,null,null
,,,
222,is to consider only the first 2 queries submitted by each,null,null
,,,
223,participant for each topic. Doing so means we only con-,null,null
,,,
224,sider queries submitted before suggestions are provided for,null,null
,,,
225,a topic. Kruskal-Wallace rank sum tests show no signifi-,null,null
,,,
226,cant dierence between the groups on the training topics (p-,null,null
,,,
227,"value,0.320) but a significant dierence for the test topics",null,null
,,,
228,"(p-value,""0.002), with both experimental groups (G""",null,null
,,,
229,exp high,null,null
,,,
230,and G,null,null
,,,
231,) performing significantly better than G .,null,null
,,,
232,exp low,null,null
,,,
233,control,null,null
,,,
234,"If we consider all queries submitted for the test topics, not",null,null
,,,
235,just the first 2 (as now no suggestions are shown to any user,null,null
,,,
236,"group), then these results become even clearer as shown in",null,null
,,,
237,138,null,null
,,,
238,the top half of Table 2; participants of the G,null,null
,,,
239,group,null,null
,,,
240,exp high,null,null
,,,
241,"issue on average queries achieving an AP of 0.10, while par-",null,null
,,,
242,ticipants of the alternative experimental condition G,null,null
,,,
243,exp low,null,null
,,,
244,achieve an AP of 0.06. The control group G,null,null
,,,
245,at this,null,null
,,,
246,control,null,null
,,,
247,stage submits queries which are an order of a magnitude,null,null
,,,
248,"worse, with a mean AP of 0.004.",null,null
,,,
249,Figure 5 presents an alternative view of the submitted,null,null
,,,
250,queries' eectiveness across groups; the left boxplot shows,null,null
,,,
251,the retrieval eectiveness for the training topics whereas on,null,null
,,,
252,the right the eectiveness for the testing topics is shown. It,null,null
,,,
253,is evident that participants who receive high-quality training,null,null
,,,
254,"suggestions perform better on average, but also that they",null,null
,,,
255,are able to achieve much higher maximum average precision,null,null
,,,
256,scores.,null,null
,,,
257,Main study,null,null
,,,
258,1: G,null,null
,,,
259,exp high,null,null
,,,
260,2: G,null,null
,,,
261,exp low,null,null
,,,
262,3: G,null,null
,,,
263,control,null,null
,,,
264,Training-size study,null,null
,,,
265,1: G,null,null
,,,
266,exp high,null,null
,,,
267,2: G,null,null
,,,
268,exp low,null,null
,,,
269,3: G,null,null
,,,
270,control,null,null
,,,
271,Training 2 Queries,null,null
,,,
272,0.0560 0.0238 0.025 Training 2 Queries,null,null
,,,
273,0.0922 0.014 0.04,null,null
,,,
274,Testing 2 Queries,null,null
,,,
275,0.043 0.041 0.024,null,null
,,,
276,Testing 2 Queries 0.055 0.0343 0.0132,null,null
,,,
277,Testing All Queries,null,null
,,,
278,0.0998,null,null
,,,
279,0.0571,null,null
,,,
280,0.0039,null,null
,,,
281,Testing All Queries,null,null
,,,
282,0.0591,null,null
,,,
283,0.0666,null,null
,,,
284,0.0132,null,null
,,,
285,Table 2: Average AP values aggregated across the,null,null
,,,
286,"first two queries of the training topics (column II),",null,null
,,,
287,the first two queries of the test topics (column III),null,null
,,,
288,and all queries submitted for the test topics (column,null,null
,,,
289,IV).  indicates a significant improvement over the,null,null
,,,
290,G,null,null
,,,
291,"condition (Kruskal-Wallace rank sum test, p-",null,null
,,,
292,control,null,null
,,,
293,value  0.01).,null,null
,,,
294,If we look at how retrieval eectiveness changes as partic-,null,null
,,,
295,"ipants query more on the same topic, we see a strong trend",null,null
,,,
296,where G,null,null
,,,
297,and G,null,null
,,,
298,continue to improve while those,null,null
,,,
299,exp high,null,null
,,,
300,exp low,null,null
,,,
301,in G,null,null
,,,
302,do not (Figure 6). At query position 1 there is very,null,null
,,,
303,control,null,null
,,,
304,little dierence between the groups; G,null,null
,,,
305,is only scoring,null,null
,,,
306,control,null,null
,,,
307,on average 0.005 worse than G,null,null
,,,
308,". However, this pat-",null,null
,,,
309,exp high,null,null
,,,
310,tern changes quickly with the experimental groups able to,null,null
,,,
311,"achieve steadily more eective queries the more they submit,",null,null
,,,
312,which is not the case for the control group G . By the,null,null
,,,
313,control,null,null
,,,
314,4th query the dierence between G,null,null
,,,
315,and G,null,null
,,,
316,widens,null,null
,,,
317,exp high,null,null
,,,
318,control,null,null
,,,
319,considerably to 0.135.,null,null
,,,
320,These findings provide strong evidence of retrieval eec-,null,null
,,,
321,tiveness improvements for the experimental groups over the,null,null
,,,
322,"control group. The analyses so far, however, do not evi-",null,null
,,,
323,dence a significant dierence in performance gain between,null,null
,,,
324,experimental conditions G,null,null
,,,
325,and G,null,null
,,,
326,.,null,null
,,,
327,exp high,null,null
,,,
328,exp low,null,null
,,,
329,Properties of Submitted Queries.,null,null
,,,
330,Beyond simply considering the retrieval eectiveness at-,null,null
,,,
331,"tained by a given query, we can also look at other properties",null,null
,,,
332,of it that relate to its eectiveness or quality. These prop-,null,null
,,,
333,erties (shown in Table 3) go some way towards explaining,null,null
,,,
334,the observed improvements in performance achieved by both,null,null
,,,
335,experimental groups. We evaluated the submitted queries,null,null
,,,
336,with metrics reflecting the literature on expert querying be-,null,null
,,,
337,haviour (see Section 4.3.2). On many of these metrics the ex-,null,null
,,,
338,perimental groups G,null,null
,,,
339,and G,null,null
,,,
340,significantly outper-,null,null
,,,
341,exp high,null,null
,,,
342,exp low,null,null
,,,
343,form the control group G . The trend is generally that,null,null
,,,
344,control,null,null
,,,
345,group G,null,null
,,,
346,"scores highest, group G",null,null
,,,
347,scores slightly,null,null
,,,
348,exp high,null,null
,,,
349,exp low,null,null
,,,
350,0.4 Control,null,null
,,,
351,Exp_High,null,null
,,,
352,0.3 Exp_Low,null,null
,,,
353,Average Precision,null,null
,,,
354,0.2,null,null
,,,
355,0.1,null,null
,,,
356,0,null,null
,,,
357,1,null,null
,,,
358,2,null,null
,,,
359,3,null,null
,,,
360,4,null,null
,,,
361,5,null,null
,,,
362,6,null,null
,,,
363,7,null,null
,,,
364,8,null,null
,,,
365,9,null,null
,,,
366,10,null,null
,,,
367,Query sequence,null,null
,,,
368,Figure 6: Main study: Average precision over sequences of queries on test topics. Each point in the plot represents the mean AP of all queries submitted as nth query. Truncated at query 10 as later queries have very few data points associated with them.,null,null
,,,
369,"lower, but often not significantly so, and group G",null,null
,,,
370,control,null,null
,,,
371,achieves the poorest scores. Participants in G,null,null
,,,
372,and,null,null
,,,
373,exp high,null,null
,,,
374,G,null,null
,,,
375,", for example, tended to submit longer queries (in",null,null
,,,
376,exp low,null,null
,,,
377,"both words and characters), which is noteworthy as the",null,null
,,,
378,example queries they were shown were designed not to be,null,null
,,,
379,overly long.,null,null
,,,
380,"Out of all three groups, participants in G",null,null
,,,
381,submitted,null,null
,,,
382,exp high,null,null
,,,
383,the rarest query terms. We measured this both in terms of,null,null
,,,
384,the IDF statistics for the collection (i.e. their query terms,null,null
,,,
385,feature significantly less often in the test corpus as a whole),null,null
,,,
386,and in terms of the number of overall participants who sub-,null,null
,,,
387,mitted those terms (we refer to this as median UserCount-,null,null
,,,
388,Term in Table 3). Comparing the Jaccard-coe cient scores,null,null
,,,
389,for query and topic description terms across the experimen-,null,null
,,,
390,tal conditions reveals that participants of G,null,null
,,,
391,were also,null,null
,,,
392,exp high,null,null
,,,
393,the most likely to take terms from the topic descriptions,null,null
,,,
394,given to them. These results suggest that a good query cre-,null,null
,,,
395,ation strategy was to use rare terms and seek inspiration,null,null
,,,
396,"from the topic descriptions, echoing findings from the liter-",null,null
,,,
397,"ature [44]. While this could be negatively construed, since",null,null
,,,
398,topic descriptions do not exist in real-life and users actu-,null,null
,,,
399,"ally have di culties in describing what they want [38], this",null,null
,,,
400,finding does not explain the whole picture as there is no sig-,null,null
,,,
401,"nificant correlation (r,0.21) between AP and the overlap of",null,null
,,,
402,queries with the topic descriptions (Jaccard score).,null,null
,,,
403,G,null,null
,,,
404,participants also submitted significantly more,null,null
,,,
405,exp high,null,null
,,,
406,queries per topic than G,null,null
,,,
407,"participants. However, this",null,null
,,,
408,control,null,null
,,,
409,is less likely to explain the performance gains as there is no,null,null
,,,
410,significant dierence in the median number of queries sub-,null,null
,,,
411,mitted between G,null,null
,,,
412,and G,null,null
,,,
413,nor between G,null,null
,,,
414,exp high,null,null
,,,
415,exp low,null,null
,,,
416,exp low,null,null
,,,
417,and G . From the median time per topics it is also,null,null
,,,
418,control,null,null
,,,
419,evident that G,null,null
,,,
420,and G,null,null
,,,
421,spend significantly more,null,null
,,,
422,exp high,null,null
,,,
423,exp low,null,null
,,,
424,time working on each topic than G .,null,null
,,,
425,control,null,null
,,,
426,Fatigue.,null,null
,,,
427,One factor that could potentially aect the results is that,null,null
,,,
428,of fatigue; are groups G,null,null
,,,
429,and G,null,null
,,,
430,doing better be-,null,null
,,,
431,exp high,null,null
,,,
432,exp low,null,null
,,,
433,"cause they are feeling less fatigued by the task, perhaps as",null,null
,,,
434,a result of getting some assistance in the early topics or by,null,null
,,,
435,being shown that high performance was possible and thus in-,null,null
,,,
436,creasing motivation? There are a number of metrics we can,null,null
,,,
437,consider to try to ascertain if fatigue is present: the amount,null,null
,,,
438,of time spent per query (query duration) and the amount,null,null
,,,
439,of time spent per topic (topic duration) and the number of,null,null
,,,
440,queries submitted. For all 3 groups the median query dura-,null,null
,,,
441,tion does seem to decrease slightly over the topics - linear,null,null
,,,
442,models show a significant negative coe cient over the top-,null,null
,,,
443,139,null,null
,,,
444,"Median query length (words) Median query length (chars) Median #queries per topic Median time per topic (seconds) Median time per query (seconds) Median query term IDFs Median UserCountTerm Jaccard coe cient (query terms, topic descr. terms)",null,null
,,,
445,G1:,null,null
,,,
446,G exp high,null,null
,,,
447,"5.4 29 3, IQR,4 165 13 4.9 43 0.3",null,null
,,,
448,G2:,null,null
,,,
449,G exp low,null,null
,,,
450,"4.4 28 2, IQR,4 150 11 5.24 45 0.25",null,null
,,,
451,G3:,null,null
,,,
452,G control,null,null
,,,
453,"4.3 23 2, IQR,2 97 13 5.15 51 0.25",null,null
,,,
454,Wilcox (p-value) G1/G2 G1/G3 G2/G3,null,null
,,,
455,p < 0.05 -- --,null,null
,,,
456,p < 0.01 p < 0.01 p < 0.01,null,null
,,,
457,#NAME?,null,null
,,,
458,p < 0.01 --,null,null
,,,
459,p < 0.05 p < 0.01,null,null
,,,
460,-- -- p < 0.01 p < 0.01,null,null
,,,
461,#NAME?,null,null
,,,
462,#NAME?,null,null
,,,
463,-- -- -- --,null,null
,,,
464,Table 3: Main study: Overview of query properties aggregated for each user group across the two topics issued during the test phase.,null,null
,,,
465,ics of between -0.6 and -0.99. This is not the case for topic,null,null
,,,
466,"duration, however, as there is no significant trend for any of",null,null
,,,
467,the 3 groups meaning that they all spend roughly the same,null,null
,,,
468,amount of time on each topic. The same consistency is also,null,null
,,,
469,present when looking at the number of queries submitted.,null,null
,,,
470,There is no significant correlation between topic sequence,null,null
,,,
471,and number of queries for any of the groups although groups,null,null
,,,
472,G,null,null
,,,
473,and G,null,null
,,,
474,do submit more queries overall. These,null,null
,,,
475,exp high,null,null
,,,
476,exp low,null,null
,,,
477,"factors do not point strongly to fatigue being a factor, al-",null,null
,,,
478,though the subtle changes in query duration do suggest that,null,null
,,,
479,users are spending less time thinking about each query as,null,null
,,,
480,"time goes on, which may explain the consistent reduction in",null,null
,,,
481,average precision.,null,null
,,,
482,4.5 Variable Training Size Study,null,null
,,,
483,"An obvious question to ask, given these results, is what impact does the number of training topics given to the test groups have on performance. A final study investigated to what extent the number of training topics (hypothesis H3) influences a user's ability to formulate good queries.",null,null
,,,
484,0.4 Control,null,null
,,,
485,Exp_High,null,null
,,,
486,0.3 Exp_Low,null,null
,,,
487,Average Precision,null,null
,,,
488,0.2,null,null
,,,
489,0.1,null,null
,,,
490,0,null,null
,,,
491,1,null,null
,,,
492,2,null,null
,,,
493,3,null,null
,,,
494,4,null,null
,,,
495,5,null,null
,,,
496,6,null,null
,,,
497,7,null,null
,,,
498,8,null,null
,,,
499,9,null,null
,,,
500,10,null,null
,,,
501,Query sequence,null,null
,,,
502,Figure 7: Training-size study: Average precision over sequences of queries on test topics. Each point in the plot represents the mean AP of all queries submitted as nth query. Truncated at query 10 as later queries have very few data points associated with them.,null,null
,,,
503,4.5.1 Study overview,null,null
,,,
504,"We used the same setup and experimental design as in the Main Study and varied only the ratio between training and test topics: in this study we used two topics for training, and the remaining four topics for testing. As in the Main Study,",null,null
,,,
505,"participants (n,""57, 19 participants in each condition) were recruited via CrowdFlower.""",null,null
,,,
506,4.5.2 Results,null,null
,,,
507,The results from this study were analysed in the same,null,null
,,,
508,fashion as those from the main study as can be seen in the,null,null
,,,
509,bottom half of Table 2. The major finding of the Main Study,null,null
,,,
510,holds in this experiment as well: both experimental groups,null,null
,,,
511,"outperform the control group wrt. eectiveness. Thus, even",null,null
,,,
512,a very small amount of training (2 topics) is useful and aids,null,null
,,,
513,users in learning to formulate better queries.,null,null
,,,
514,"In contrast to the Main Study, and unsurprising given the",null,null
,,,
515,"lower amount of training, we observe a smaller dierence in",null,null
,,,
516,retrieval eectiveness across the test topics: 0.05 (G,null,null
,,,
517,"),",null,null
,,,
518,exp high,null,null
,,,
519,0.054 (G ) and 0.024 (G ) respectively.,null,null
,,,
520,exp low,null,null
,,,
521,control,null,null
,,,
522,These results suggest that some form of learning is tak-,null,null
,,,
523,ing place and that the relative improvements are smaller if,null,null
,,,
524,less training is given. They also serve to further highlight,null,null
,,,
525,the unexpected finding that there is little dierence between,null,null
,,,
526,G,null,null
,,,
527,and G,null,null
,,,
528,.,null,null
,,,
529,exp high,null,null
,,,
530,exp low,null,null
,,,
531,5. SUMMARY AND DISCUSSION,null,null
,,,
532,Our findings vs. our research hypotheses.,null,null
,,,
533,Hypothesis H1 has been shown to hold - users are in-,null,null
,,,
534,deed able to adapt their search behaviour to an unfamiliar,null,null
,,,
535,search system. While G,null,null
,,,
536,(which received no training),null,null
,,,
537,control,null,null
,,,
538,"does not adapt, we clearly see significant changes in query-",null,null
,,,
539,ing behaviour in both experimental groups (i.e. those who,null,null
,,,
540,received training).,null,null
,,,
541,Our pilot study served to confirm hypothesis H2; the,null,null
,,,
542,study participants were indeed able to determine a set of,null,null
,,,
543,characteristics that well-performing queries contain. Recog-,null,null
,,,
544,nising such characteristics is a necessary requirement for,null,null
,,,
545,learning how to create better queries in general and not just,null,null
,,,
546,for specific topics.,null,null
,,,
547,The main study and the follow-up focusing on the train-,null,null
,,,
548,ing set size provide evidence for hypothesis H3. The two,null,null
,,,
549,experimental groups outperform G,null,null
,,,
550,"significantly, both",null,null
,,,
551,control,null,null
,,,
552,when being shown two and four training topics respectively.,null,null
,,,
553,"Thus, even a very small set of training topics is su cient to",null,null
,,,
554,improve users' ability to pose good queries.,null,null
,,,
555,"Our results do not support H4. In terms of AP, although",null,null
,,,
556,Figure 5 hints that G,null,null
,,,
557,may have outperformed G,null,null
,,,
558,",",null,null
,,,
559,exp high,null,null
,,,
560,exp low,null,null
,,,
561,140,null,null
,,,
562,ID Information need,null,null
,,,
563,av. KNOW av. SUR av. QUAL Query suggestion examples,null,null
,,,
564,Identify positive accomplishments,null,null
,,,
565,303,null,null
,,,
566,2.64,null,null
,,,
567,of the Hubble telescope since it was,null,null
,,,
568,launched in 1991,null,null
,,,
569,Identify drugs used in the treat-,null,null
,,,
570,383,null,null
,,,
571,2.89,null,null
,,,
572,ment of mental illness.,null,null
,,,
573,What is the status of The Three,null,null
,,,
574,416,null,null
,,,
575,1.58,null,null
,,,
576,Gorges Project?,null,null
,,,
577,universe astronomer faint hubble in-,null,null
,,,
578,2.62,null,null
,,,
579,2.98,null,null
,,,
580,[,null,null
,,,
581,"], [",null,null
,,,
582,frared galaxies universe hubble infrared,null,null
,,,
583,"], [",null,null
,,,
584,stars universe hubble,null,null
,,,
585,],null,null
,,,
586,antidepressant risk zoloft prozac zoloft,null,null
,,,
587,2.45,null,null
,,,
588,3.36,null,null
,,,
589,[,null,null
,,,
590,"], [",null,null
,,,
591,studies prozac antidepressant eective,null,null
,,,
592,"], [",null,null
,,,
593,zoloft prozac,null,null
,,,
594,],null,null
,,,
595,coerdams damming generating 2009,null,null
,,,
596,3.09,null,null
,,,
597,2.6,null,null
,,,
598,[,null,null
,,,
599,"],",null,null
,,,
600,dam corporation phase 2009 2009 river,null,null
,,,
601,[,null,null
,,,
602,"], [",null,null
,,,
603,construction,null,null
,,,
604,],null,null
,,,
605,Table 4: Examples of Robust track search tasks and the generated high-quality query suggestions. Columns 3-5 contain user rating data from our study on user perceptions of queries. Column 3 (KNOW) contains the average knowledge rating of the information need across all users of the study. Columns 4 and 5 contain the average rating users assigned to all query suggestions of the topic with respect to the surprise (SUR) factor and the estimated result quality (QUAL).,Y,null
,,,
606,"the dierence is not significant. There were some features of the queries that were statistically distinguishable between these groups, but we feel that the evidence is not strong enough to claim that H4 holds.",null,null
,,,
607,"Finally, based on the evidence in Figures 6 and 7, we have to reject H5 - our participants in both experimental groups had a comparable learning rate (though with dierent absolute performance scores).",null,null
,,,
608,Our findings vs. prior work.,null,null
,,,
609,"Previous work has presented mixed evidence for people's ability to accurately determine which query terms will have utility. Our findings suggest this is a complex behaviour. Although participants were able to identify positive characteristics of queries shown to be eective (Section 4.3.2), many high-performing queries were not predicted to be such (Section 4.2.2). Perhaps these potentially contradictory findings indicate a potential systems bias, i.e. do users implicitly trust suggestions presented by the system as good? Is it only when doubt is introduced by explicitly questioning users about the queries that they perceive suggestions to be potentially not of good quality? What does this mean for the learning eect? This line of thought opens up many fascinating questions of how query suggestions are presented.",null,null
,,,
610,"Our work has added to the small base of literature demonstrating means for users to learn how to provide higherquality queries. One limitation of our work has to do with the time period of learning. Our findings support the claim that being shown good suggestions can lead to users learning how to produce better queries, however this is only demonstrated over the period of a session i.e. the test groups achieved better performance for later queries and for later topics. Ideally, however, what we want to show is learning over longer periods of time, such as weeks [28] and months [6] as previous studies have done. This requires a dierent mode of evaluation as crowd-sourcing is not suited to such tasks and represents an important next stage in our project.",null,null
,,,
611,"A further limitation, with respect to how our findings may be used, is that in a real-life scenario a search system would normally not have access to relevance judgements. This means our method of creating queries cannot typically be applied. We argue that there are situations, though, that may be ideally suited to such an approach. For example in web search we have implicit indicators for di cult tasks (i.e. where better queries might be required) [20] and we also have good models for determining search success based",null,null
,,,
612,"on user behaviour [19]. When such instances combine (i.e. when users are successful in tasks they have been struggling with), this might be the perfect time to present a query suggestion, perhaps along the line of ""The following query would get this page further up the ranking"". Another potential use-case might be to present examples when a user switches context or to a new search-engine where new strategies are required. It has been suggested that users tend not to vary their strategy [29] and our approach might help encourage more diverse or tailored behaviour.",null,null
,,,
613,6. CONCLUSIONS,null,null
,,,
614,"The set of user studies described in this paper have demonstrated that it is possible to use high-quality query examples to influence the queries users submit themselves. We have shown that users can recognise and abstract positive qualities of good queries. Users change the properties of the queries they submit and achieve better retrieval performance after seeing good examples for other tasks. Our findings open up a range of interesting questions relating to how query examples should be presented and how this affects learning and the influence of learning duration, i.e. is user behaviour influenced over the longer term? Finally is domain knowledge an important factor? We hope to address these issues in upcoming work.",null,null
,,,
615,7. REFERENCES,null,null
,,,
616,"[1] P. Anick, Using terminological feedback for web search refinement: a log-based study, SIGIR '03, ACM, 2003, pp. 88­95.",null,null
,,,
617,"[2] A. Arampatzis and J. Kamps, A study of query length, SIGIR '08, ACM, 2008, pp. 811­812.",null,null
,,,
618,"[3] A. Aula, N. Jhaveri, and M. K¨aki, Information search and re-access strategies of experienced web users, WWW '05, ACM, 2005, pp. 583­592.",null,null
,,,
619,"[4] L. Azzopardi, M. De Rijke, and K. Balog, Building simulated queries for known-item topics: an analysis using six european languages, SIGIR '07, ACM, 2007, pp. 455­462.",null,null
,,,
620,"[5] H. Bast and I. Weber, Type less, find more: fast autocompletion search with a succinct index, SIGIR '06, ACM, 2006, pp. 364­371.",null,null
,,,
621,"[6] S. Bateman, J. Teevan, and R.W. White, The search dashboard: how reflection and comparison impact search behavior, SIGCHI, ACM, 2012, pp. 1785­1794.",null,null
,,,
622,141,null,null
,,,
623,"[7] N.J. Belkin, Helping people find what they don't know, Commun. ACM 43 (2000), no. 8, 58­61.",null,null
,,,
624,"[8] N.J. Belkin, D. Kelly, G Kim, J-Y Kim, H-J Lee, G. Muresan, M-C Tang, X-J Yuan, and C. Cool, Query length in interactive information retrieval, SIGIR '03, ACM, 2003, pp. 205­212.",null,null
,,,
625,"[9] J.L. Bennett, The user interface in interactive systems, Annual review of information science and technology 7 (1972), no. 159-196.",null,null
,,,
626,"[10] W.B. Croft and R.H. Thompson, I3r: A new approach to the design of document retrieval systems, JASIST 38 (1987), no. 6, 389­404.",null,null
,,,
627,"[11] Z. Dou, R. Song, and J. Wen, A large-scale evaluation and analysis of personalized search strategies, WWW '07, ACM, 2007, pp. 581­590.",null,null
,,,
628,"[12] D. Elsweiler, Supporting human memory in personal information management, Ph.D. thesis, University of Strathclyde, 2007.",null,null
,,,
629,"[13] B.M. Evans and E.H. Chi, An elaborated model of social search, IP&M 46 (2010), no. 6, 656­678.",null,null
,,,
630,"[14] K. Franzen and J. Karlgren, Verbosity and interface design, SICS Research Report (2000).",null,null
,,,
631,"[15] G.W. Furnas, T.K. Landauer, L.M. Gomez, and S.T. Dumais, The vocabulary problem in human-system communication, Communications of the ACM 30 (1987), no. 11, 964­971.",null,null
,,,
632,"[16] J. Hackos and J. Redish, User and task analysis for interface design, John Wiley & Sons, Inc., 1998.",null,null
,,,
633,"[17] D. Harman, Towards interactive query expansion, SIGIR '88, ACM, 1988, pp. 321­331.",null,null
,,,
634,"[18] M. Harvey, F. Crestani, and M.J. Carman, Building user profiles from topic models for personalised search, CIKM '13, ACM, 2013, pp. 2309­2314.",null,null
,,,
635,"[19] A. Hassan, R. Jones, and K. L. Klinkner, Beyond dcg: user behavior as a predictor of a successful search, WSDM, ACM, 2010, pp. 221­230.",null,null
,,,
636,"[20] A. Hassan, R. W. White, S. T Dumais, and Y. Wang, Struggling or exploring?: disambiguating long search sessions, WSDM, ACM, 2014, pp. 53­62.",null,null
,,,
637,"[21] H.A. Hembrooke, L.A. Granka, G.K. Gay, and E.D. Liddy, The eects of expertise and feedback on search term selection and subsequent learning, JASIST 56 (2005), no. 8, 861­871.",null,null
,,,
638,"[22] I. Hsieh-yee, Eects of search experience and subject knowledge on the search tactics of novice and experienced searchers, JASIST 44 (1993), 161­174.",null,null
,,,
639,"[23] B.J. Jansen, D.L. Booth, and A. Spink, Patterns of query reformulation during web searching, Journal of the American Society for Information Science and Technology 60 (2009), no. 7, 1358­1371.",null,null
,,,
640,"[24] D. Kelly and X. Fu, Eliciting better information need descriptions from users of information search systems, IP&M 43 (2007), no. 1, 30­46.",null,null
,,,
641,"[25] J. Koenemann and N.J. Belkin, A case for interaction: a study of interactive information retrieval behavior and eectiveness, SIGCHI '96, ACM, 1996, pp. 205­212.",null,null
,,,
642,"[26] W. Lucas and H. Topi, Training for web search: Will it get you in shape?, JASIST 55 (2004), no. 13, 1183­1198.",null,null
,,,
643,"[27] D. McKay and G. Buchanan, Boxing clever: how searchers use and adapt to a one-box library search, OzCHIO~ 13, ACM, 2013, pp. 497­506.",null,null
,,,
644,"[28] N. Moraveji, D. Russell, J. Bien, and D. Mease, Measuring improvement in user search performance resulting from optimal search tips, SIGIR '11, ACM, 2011, pp. 355­364.",null,null
,,,
645,"[29] J. Nielsen, Incompetent research skills curb users' problem solving http://www.useit.com/alertbox/search-skills.html last accessed january, 2015, Alertbox (2011).",null,null
,,,
646,"[30] H. Oinas-Kukkonen and M. Harjumaa, Towards deeper understanding of persuasion in software and information systems, ACHI '08, IEEE, 2008, pp. 200­205.",null,null
,,,
647,"[31] S.E. Robertson, On term selection for query expansion, Journal of Documentation 46 (1990), no. 4, 359­364.",null,null
,,,
648,"[32] K. Rodden, I. Ruthven, and R.W. White, Workshop on web information seeking and interaction, ACM SIGIR Forum, vol. 41, ACM, 2007, pp. 63­67.",null,null
,,,
649,"[33] I. Ruthven, Re-examining the potential eectiveness of interactive query expansion.",null,null
,,,
650,"[34] I. Ruthven and M. Lalmas, A survey on the use of relevance feedback for information access systems, The Knowledge Engineering Review 18 (2003), no. 02, 95­145.",null,null
,,,
651,"[35] B. Shneiderman, Dynamic queries for visual information seeking, Software, IEEE 11 (1994), no. 6, 70­77.",null,null
,,,
652,"[36] B. Smyth, E. Balfe, J. Freyne, P. Briggs, M. Coyle, and O. Boydell, Exploiting query repetition and regularity in an adaptive community-based web search engine, User Modeling and User-Adapted Interaction 14 (2004), no. 5, 383­423.",null,null
,,,
653,"[37] E. Sormunen, A method for measuring wide range performance of boolean queries in full-text databases, Tampere University Press, 2000.",null,null
,,,
654,"[38] R.S. Taylor, Question-negotiation and information seeking in libraries, College & Research Libraries 29 (1968), no. 3, 178­194.",null,null
,,,
655,"[39] P. Vakkari, Changes in search tactics and relevance judgements when preparing a research proposal a summary of the findings of a longitudinal study, Information Retrieval 4 (2001), no. 3-4, 295­310.",null,null
,,,
656,"[40] E.M. Voorhees, The trec 2005 robust track, SIGIR Forum 40 (2006), no. 1, 41­48.",null,null
,,,
657,"[41] E.M. Voorhees, D.K. Harman, et al., Trec: Experiment and evaluation in information retrieval, vol. 63, MIT press Cambridge, 2005.",null,null
,,,
658,"[42] R.W. White, S.T. Dumais, and J. Teevan, Characterizing the influence of domain expertise on web search behavior, WSDM '09, ACM, 2009, pp. 132­141.",null,null
,,,
659,"[43] R.W. White and D. Morris, Investigating the querying and browsing behavior of advanced search engine users, SIGIR '07, ACM, 2007, pp. 255­262.",null,null
,,,
660,"[44] P. Willett and I. Ruthven, Relevance behaviour in trec, Journal of Documentation 70 (2014), no. 6, 1098­1117.",null,null
,,,
661,142,null,null
,,,
662,,null,null

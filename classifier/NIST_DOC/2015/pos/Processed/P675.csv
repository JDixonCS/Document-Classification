,sentence,label,data
0,Evaluating Streams of Evolving News Events,null,null
1,Gaurav Baruah,null,null
2,Computer Science,null,null
3,University of Waterloo gbaruah@uwaterloo.ca,null,null
4,Mark D. Smucker,null,null
5,Charles L. A. Clarke,null,null
6,Management Sciences,null,null
7,Computer Science,null,null
8,University of Waterloo,null,null
9,University of Waterloo,null,null
10,mark.smucker@uwaterloo.ca claclark@plg.uwaterloo.ca,null,null
11,ABSTRACT,null,null
12,"People track news events according to their interests and available time. For a major event of great personal interest, they might check for updates several times an hour, taking time to keep abreast of all aspects of the evolving event. For minor events of more marginal interest, they might check back once or twice a day for a few minutes to learn about the most significant developments. Systems generating streams of updates about evolving events can improve user performance by appropriately filtering these updates, making it easy for users to track events in a timely manner without undue information overload. Unfortunately, predicting user performance on these systems poses a significant challenge. Standard evaluation methodology, designed for Web search and other adhoc retrieval tasks, adapts poorly to this context. In this paper, we develop a simple model that simulates users checking the system from time to time to read updates. For each simulated user, we generate a trace of their activities alternating between away times and reading times. These traces are then applied to measure system effectiveness. We test our model using data from the TREC 2013 Temporal Summarization Track (TST) comparing it to the effectiveness measures used in that track. The primary TST measure corresponds most closely with a modeled user that checks back once a day on average for an average of one minute. Users checking more frequently for longer times may view the relative performance of participating systems quite differently. In light of this sensitivity to user behavior, we recommend that future experiments be built around clearly stated assumptions regarding user interfaces and access patterns, with effectiveness measures reflecting these assumptions.",null,null
13,Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software -- Performance evaluation (efficiency and effectiveness),null,null
14,Keywords: search; streams; evaluation,null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'15, August 09 - 13, 2015, Santiago, Chile. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3621-5/15/08 ...$15.00. DOI: http://dx.doi.org/10.1145/2766462.2767751.",null,null
16,1. INTRODUCTION,null,null
17,"On December 4, 2012, Typhoon Bopha made landfall on the eastern coast of the Philippine island of Mindanao. The strongest typhoon in the island's recorded history, the storm system had formed in the Pacific in late November, growing into a tropical storm by November 28th, and into a severe tropical storm by November 30th. On December 1st, as the storm moved west towards the island, it rapidly intensified into a category 4 typhoon. Two days later, only miles from shore, it reached category 5, a super typhoon, with winds up to 150 mph. At 4:45AM it hit. Heavy rains and waves exceeding 25 feet washed across villages and towns, destroying homes, flooding roads, and cutting communications. Initial reports limited the death toll to one, but by the next day it had increased to over 200, mostly killed by flooding and debris. Over the next week, the number of fatalities grew to a final total of over 600, with more than 200,000 people displaced or otherwise impacted.",null,null
18,"In this paper we consider how to evaluate a search engine that aggregates a stream of information and provides a means for people to obtain the latest, most relevant information regarding an event of interest. As time permits, a user of such a search engine will check back multiple times in order to find out new information about the event.",null,null
19,"We envision that there will be a wide variety of users of such a search engine. Government and health care workers, as well as people in the immediate vicinity, would likely need to learn of new information soon after it becomes available. People interested in the event, but not directly affected, would likely have a less urgent need for new information. Some people would be willing to consume a large amount of content in the hope of not missing any relevant information, while others may desire a minimal amount of content.",null,null
20,"One way of designing an information retrieval system for streams of evolving events would be to allow users to visit and query the system whenever they want the latest information. In this type of system, the user pulls information. An alternative approach would be to create a system that pushes select pieces of information to users that have set up a search query or profile. In either case, users would want to control the frequency and duration of their interaction with the system. In the pull scenario, the user controls the frequency by how often they visit and controls the duration by how long they consume content during a visit's session. In the push scenario, the user would need to specify up front how often content should be sent to them as well as the amount of content to be sent.",null,null
21,675,null,null
22,"In this paper, we propose a new effectiveness measure for the evaluation of retrieval systems that produce streams of content for evolving events. We call this measure modeled stream utility (MSU). This new effectiveness measure models users in terms of the time spent away from the system and the time spent with the system. In effect, we model users in terms of their frequency and duration of interaction with the system, which is a model applicable to either push or pull variants of this type of retrieval system. We measure utility in the number of relevant information nuggets the user is likely to find. Search users naturally vary, and we model this variation as well by modeling a population of users and simulating many different users' interaction with the retrieval system. A significant advantage of MSU as an effectiveness measure is that by making it user-centered, the measure has the potential to be easily calibrated using recorded user behavior.",null,null
23,"We demonstrate MSU using the TREC 2013 Temporal Summarization Track [4]. Participating groups in the temporal summarization track (TST) take a large stream of content and attempt to produce a filtered stream of content relevant to a given search topic. For each topic, the track has determined a set of relevant nuggets of information and has judged a pool of content and determined which nuggets are present in the content.",null,null
24,"To evaluate the runs submitted to TST, we use MSU in a scenario where users pull information from the retrieval system. MSU simulates user interaction with the retrieval system. When a simulated user visits the retrieval system and queries it for a given search topic, the user is shown the most recently filtered content in reverse chronological order. In TST, filtered content consists of sentence length material extracted from a stream of larger documents. After a simulated user has read an element of content, we measure the user's gain in terms of the number of nuggets the user is likely to have found relevant in the content. If a user has read a nugget earlier, MSU assumes that a repeated nugget will not be found relevant and has a gain value of zero. The temporal summarization track has estimated when nuggets are first known in the aggregate stream, and in the MSU simulation, nuggets delivered late to a user are less likely to be found relevant.",null,null
25,"In addition to demonstrating MSU applied to the TREC 2013 temporal summarization track, we compare MSU to the track's existing primary measure, ELG, as well as conduct a parameter sweep over a range of the MSU user model's parameters to better understand how the measure behaves. We find that:",null,null
26,"· When we set MSU's parameters to that of a reasonably interested user who visits the system for 2 minutes every 3 hours on average, MSU's ranking of TST submitted runs is significantly different from TST's primary measure, ELG (Kendall's  , 0.47).",null,null
27,"· Of the parameter settings included in our sweep, ELG correlates best with a model of users who visit for only 1 minute per day (Kendall's  ,"" 0.62). In terms of content consumed, a minute-long visit amounts to a user reading approximately 4-5 sentences.""",null,null
28,· The ranking of systems produced by the MSU effectiveness measure appears to be most sensitive to the amount of content consumed by the simulated users.,null,null
29,"Given that the amount of content consumed appears to affect system rankings the most, this result has implications for the design of retrieval systems that generate streams of information updates. The temporal summarization track gave participating groups the task of filtering a stream, but different users are going to be interested in different amounts of material. Rather than filter a stream, retrieval systems for evolving events may be better designed as best-match rankers that must balance recency and relevancy. No matter how such systems are designed, MSU provides a usercentered measure that allows for calibration with actual measured user behavior.",null,null
30,2. BACKGROUND,null,null
31,"The news about an event, such as Typhoon Bopha, has an impact on large sections of society. The sudden onset of such events can generate related information needs among governments, aid agencies, observer groups, as well as those affected and their families. Usually such events are unexpected and can be dynamic and evolve over time.",null,null
32,"A large number of documents will be authored in connection with significant events and made available for consumption via the Internet in near real time. Publishers of this content include social networking services such as Twitter and Facebook as well as newspapers, news feeds, and blogs. Authors can range from journalists to government agencies to persons experiencing the event first hand. We consider all newly created content in aggregate as a stream of material from which a person might want to find relevant information.",null,null
33,"The closest analogues to this type of retrieval are microblog search [12] and temporal summarization [2, 4]. The TREC 2011 Microblog track [12] included a single adhoc retrieval task where the retrieval system receives a query at time t and must retrieve the most relevant and recent Twitter postings prior to time t. The track organizers used precision at rank 30 to evaluate submissions, but found that the task was under-specified with respect to how results should be ranked in terms of relevance and recency, making comparisons between participating groups difficult.",null,null
34,"In the TREC 2012 Microblog track [14], the adhoc retrieval task remained and was adjusted so that participants were simply to score the microblog posts for ranking purposes. The only notion of recency was that the postings had to have occurred before the time of the query. Relevance of a posting was formulated with respect to the informativeness of the posting without considerations of recency or novelty. To evaluate a ranking, the organizers computed its precision at 30 and its receiver operating characteristic (ROC). For TREC 2013, the microblog track continued to use precision at 30 as its primary retrieval measure [11].",null,null
35,"Temporal summarization systems attempt to extract relevant information from an aggregate stream and relay this information to the user as soon as possible. In contrast to a microblog search task, a temporal summarization task extends over a period that roughly corresponds to the period that an event is in the news. Being a summarization task, the evaluation of temporal summarization systems has focused on determining the overall quality of the resulting summary even though the nature of temporal summarization means that the summary grows over time and is likely being consumed by an end user at regular intervals, e.g. for",null,null
36,676,null,null
37,"a multi-day event, we would expect a user to seek new information daily.",null,null
38,"The temporal summarization work of Allan, Gupta, and Khandelwal [2] defined new precision and recall measures that incorporate notions of usefulness and novelty. A useful sentence is one that would be helpful for use in a summary and a novel sentence is one that is new and contains information not previously seen. In addition, they examine the importance of summary size and recency of information.",null,null
39,"Guo et al. [10] primarily focus on finding updates for rapidly evolving news events for which the information need is urgent (or time-critical). They consider an event to be composed of a number of subtopics. Each update may contain one or more subtopics as well. Accordingly, the precision for an update is the fraction of the update's subtopics that are also subtopics for the event, and, the recall for an update is the the fraction of the event's subtopics contained in the update. They define measures expected precision and expected recall to be the average precision (and recall respectively) over the complete set of updates.",null,null
40,"The TREC 2013 Temporal Summarization Track (TST) [4] follows the temporal summarization framework defined by Allan et al. [2] with two significant changes. The first change is that while Allan et al. required that summarization happen on-the-fly as each news story was received, in TST, systems are only restricted to be certain that summaries produced at time t only use content available at t or earlier. For example, if a system wanted to, it could wait to process several days of content before producing a single result. The TST directly evaluates the recency of information and thus does not have to restrict systems in when or how they produce summaries. The second change is that the TST uses a web-scale aggregate stream of world wide web content as opposed to the comparatively small collection available to Allan et al. in 2001.",null,null
41,The TST uses sentences (updates in TST terminology) as the primary retrieval unit. Groups participating in TST had the task of filtering the aggregate stream and emitting sentences containing timely and novel information regarding a news event. The TST judges the relevance of sentences by noting the presence of individual nuggets of information. Sentences are annotated such that it is known what pieces (nuggets) of relevant information the sentences contain.,null,null
42,"The TST assessors identified nuggets about an event from Wikipedia. For example, Typhoon Bopha has its own detailed Wikipedia page1. TST attached a timestamp to every nugget as determined by the first occurrence of the nugget in the Wikipedia edit history for the topic's article.",null,null
43,"The TST organizers crafted effectiveness measures that are nugget-based variants of precision and recall that incorporate several key criteria for the updates produced by a system. In addition to an update needing to contain novel nuggets, the nuggets should be emitted as early as possible (latency) and shorter updates are better than longer updates (verbosity). Each update is assigned a score based on these criteria and the track's set-based effectiveness measures are computed as averages over all updates.",null,null
44,"Participating groups produced runs consisting of emitted updates. When an update is emitted, it is given a timestamp equal to the most recent material consumed by the system from the aggregate stream of material. Thus, a system that",null,null
45,1http://en.wikipedia.org/wiki/Typhoon_Bopha,null,null
46,"delayed emitting any updates until it had consumed all of the material in the period of interest, would have all of its updates given a timestamp equal to the very end of the period. If an update contains a nugget, the nugget's latency is the difference between the update's timestamp and the nugget's Wikipedia assigned time. The gain for a reported nugget is discounted as a function of its latency, such that, with increasing latency, the gain for a nugget drops substantially within the first 24 hours, after which its discounted gain stays under 20%. It is possible for an update to be ""earlier"" than the nugget's timestamp, in which case the latency discount function awards the update a bonus for reporting the nugget early. Reflecting novelty, repeated nuggets have no value, providing zero gain.",null,null
47,"Finally, sentences are penalized for being overly verbose. To incorporate verbosity into the track's measures, an update may count as more than a single update in the computation of the average. The verbosity of an update is based on the extra number of nuggets the update could have contained. For example, if an update contains zero nuggets and is 60 words long, and if the average number of words per nugget is 15, then this update has a verbosity penalty of 60 / 15 ,"" 4 and when averaged counts as 5 updates, i.e. itself (1) plus its verbosity penalty (4). If the number of words in an update equals the sum of the words in the nuggets in the update, then the update counts as a single update.""",null,null
48,"TST's evaluation has two primary metrics, expected latency gain (ELG), which measures the gain per update while discounting latency and penalizing verbosity, and latency comprehensiveness (LC), which measures coverage of nuggets relating to the topic. ELG and LC are analogous to precision and recall respectively. While both ELG and LC are important to be considered together, we consider the ELG measure to be the primary measure of the track given that runs were presented from best to worst ELG in the track overview [4]. ELG for a set D of system generated updates is formulated as,",null,null
49,"ELGV(D) ,",null,null
50,1,null,null
51,"G(d, D)",null,null
52,dD V(d) dD,null,null
53,(1),null,null
54,"where, V(d) is the verbosity normalization of update d, G(d, D) is the latency discounted gain for update d, and G(d, D) is non zero when d is the earliest update from the set D to report one or more nuggets for the topic. Once a nugget is reported, it does not contribute to gain if it appears again in later updates. The LC metric replaces the denominator in equation (1), with nN R(n), where N is the set of nuggets identified by the assessors and R(n) is the relevance for n based on its importance (R(n) , 1 for binary relevance). Essentially LC computes the recall of relevant material by the system. Unjudged sentences are elided from run submissions.",null,null
55,"It is clear that users are kept in mind in the formulation of effectiveness measures for temporal summarization. For example, an update should be timely and contain relevant, novel information without extraneous material. In addition to these qualities, we believe that it important to also consider the user's desired frequency and duration of interaction with the system. A government agency may well assemble a team of people to monitor a stream of updates 24 hours a day for the duration of a crisis. An individual in the midst of a crisis may be limited by extrinsic circumstances to only having a few moments each day to check for updates.",null,null
56,677,null,null
57,ID Time Nugget n9 12/05/12 The typhoon destroyed 70-80% of planta-,null,null
58,"15:13:56 tions, mostly bananas for export. n10 12/06/12 Damage to agriculture and infrastructure in",null,null
59,"17:45:12 Compostela Valley province could reach at least 4 billion pesos, equivalent to 75 million or $98 million U.S.",null,null
60,n11 12/04/12 About 40 people were killed or missing in 18:31:18 flash floods and landslides near a mining area on Mindanao.,null,null
61,n12 12/04/12 Typhoon Bopha made landfall on Mindanao 03:17:18 early on December 4 as a category 4.,null,null
62,"n13 12/05/12 Late on December 3, Bopha made landfall 10:31:21 over Baganga, Mindanao, as a category 5 super typhoon.",null,null
63,"n14 12/05/12 As of 5 December, 238 deaths had been re13:55:42 ported on Mindana with hundreds missing.",null,null
64,Table 1: Nuggets reported to user (read by user) in the example session (Table 2). Times for each nugget are their time of first occurrence in Wikipedia's edit history.,null,null
65,3. MODELED STREAM UTILITY,null,null
66,"We have designed our new effectiveness measure, modeled stream utility (MSU), to be an easily calibrated usercentered effectiveness measure. A user-centered effectiveness measure must take into consideration both the user interface of the retrieval system and user behavior with the user interface. For an effectiveness measures to be easily calibrated to user behavior, the measure's user model must be designed such that the model's parameters can be set based on actual measurements of user behavior.",null,null
67,3.1 User Interface and User Behavior,null,null
68,"Our hypothetical user interface provides a means for the user to query the system and receive a ranked list of results (i.e. updates about an event). Each result is a short piece of text. The retrieval system produces a stream of information that is consumed by the user in the order produced. In other words, after issuing a query and receiving the ranked list of results, the user reads the results in rank order. No other interaction with the retrieval system is possible.",null,null
69,"Our simulated user is not limited to one visit with the retrieval system. Our simulated user will repeatedly visit the search engine with a frequency reflective of their interest in the evolving event or reflective of their availability to visit. On each visit, the simulated user enters the same query, and expects to receive the most recent and relevant updates concerning their event of interest. As with their frequency of visits, the simulated user will read during a visit for an amount of time reflective of their interest or time available. We will call visits sessions, with each having a duration.",null,null
70,No two users are the same. Each user will visit the search engine with different frequencies and durations. Each user will have their own reading speed. We model frequency and duration by modeling the time a user spends away from the system and by separately modeling the time a user spends with the system. We model each user with three parameters:,null,null
71,· A: Average time away.,null,null
72,· D: Average session duration.,null,null
73,"· V : Reading speed, e.g. words per minute.",null,null
74,"To estimate user performance with the retrieval system, we draw multiple simulated users from population distributions and average user performance over all simulated users. We model the user population's time away and their session duration with two separate log-normal distributions. Lognormal distributions are such that when one takes the natural log of the data, a normal distribution fits the resulting distribution. Usually one describes a log-normal distribution in terms of the mean, , and standard deviation, , of the normal distribution fit to the log of the data.",null,null
75,"For both the time-away and the session-duration distributions, we will describe them in this paper in terms of the underlying distribution's mean and standard deviation. If the underlying data has mean M and standard deviation S, then, for the log-normal distribution, the variance is 2 ,"" log(1 + S2/M 2), and the mean is  "","" log(M ) - 0.52. As we describe later in the paper, we consider a wide range of user behavior by varying the time away and session duration population distributions. To model the population's reading speed, we use the log-normal parameters from Clarke and Smucker [8] wherein, the distribution of reading speed across users is described by a log-normal distribution with  "", 1.29 and  , 0.558.",null,null
76,"Given these three population distributions, we can generate as many simulated users as needed for numerical precision in our estimate of average user performance. For example, a possible reasonable setting of the population parameters is:",null,null
77,"· Time away mean, MA ,"" 3 hours, and standard deviation, SA "", 1.5 hours.",null,null
78,"· Session duration mean, MD ,"" 2 minutes, and standard deviation, SD "", 1 minute.",null,null
79,"· Reading speed log-normal (words per second):  , 1.29 and  ,"" 0.558, i.e. a mean reading speed of 255 words per minute.""",null,null
80,"A random user drawn from this distribution will spend more or less time away, have longer or shorter session durations, and read faster or slower than the above population averages. Overall, the population of simulated users will have average behavior equal to the population means. To simulate a single user's interaction with the search engine over a given period of time, we construct a user-trace (Figure 1d) consisting of alternating sessions with the search engine and time intervals spent away from the search engine.",null,null
81,"It would be unusual for a user to visit on a fixed frequency and visit for the exact same amount of time on each visit. To model variation in a user's time away and session duration, we use their mean time away and mean session duration as parameters to two separate exponential distributions. An exponential distribution is commonly used to model phenomena such as the time between radioactive decay of nuclei or the time between events in a Poisson process. For example, to simulate random session duration times with a mean of D, we draw random deviates from an exponential distribution [9] with a rate parameter of  , 1/D.",null,null
82,3.2 Gain,null,null
83,"A simulated user will repeatedly visit the search engine. During a visit, the user will read the search results in rank order. Each search result has an amount of gain (possibly zero) associated with it. The simulated user will accumulate",null,null
84,678,null,null
85,Time 9:52,null,null
86,9:52,null,null
87,9:52 9:52,null,null
88,9:50,null,null
89,9:15 7:45,null,null
90,7:31,null,null
91,Conf. 0.95,null,null
92,0.95,null,null
93,0.95 0.91,null,null
94,0.87,null,null
95,0.91 0.87,null,null
96,0.87,null,null
97,Update Sentence Produced by System at Time Shown in Column 1,null,null
98,"Typhoon Bopha , with central winds of 75 mph and gusts of up to 93 mph, battered beach resorts and dive spots in northern Palawan on Wednesday, but there was little damage as the storm began to weaken.",null,null
99,"Hardest hit were the coastal, farming and mining towns in the southern Mindanao region, where Bopha made landfall on Tuesday, destroying homes, causing landslides and flash flooding and killing at least 230 people.",null,null
100,"About 60 people died in the municipality of New Bataan alone and around 245 were still missing, Uy said, adding the area was initially cut off by road blocks.",null,null
101,"Damage to agriculture and infrastructure in Compostela Valley province could reach at least 4 billion pesos ($98 million), with the typhoon destroying 70-80 percentof plantations, mostly bananas for export, Uy said.",null,null
102,A man looks at the dead bodies of relatives killed by landslides after Typhoon Bopha hit Compostela town. / Getty Disaster-response agencies reported 13 other typhoon-related deaths elsewhere.,null,null
103,Typhoon Bopha : Philippines Storm Kills 238 - Yahoo!,null,null
104,Most Popular Today's five most popular stories World's oldest person dies at age 116 Snake on a plane forces emergency landing What city has world's best quality of life?,null,null
105,"( AP Photo)&lt;/em&gt ; May 1960 A magnitude 9.5 earthquake in southern Chile and ensuing tsunami kill at least 1,716 people.&lt; br&gt; &lt;em&gt ;Caption: A soldier stands guard nearrubble strewn around an electrical shop which was shattered by an earthquake in Concepcion , Chile , on May 24, 1960.",null,null
106,TtR 10.1 s,null,null
107,8.8 s,null,null
108,7.7 s 8.5 s,null,null
109,7.5 s,null,null
110,2.4 s 7.7 s,null,null
111,13.1 s,null,null
112,CTtR 10.1 s,null,null
113,18.9 s,null,null
114,26.7 s 35.2 s,null,null
115,42.7 s,null,null
116,45.1 s 52.8 s,null,null
117,65.9 s,null,null
118,"Nuggets n11, n12, n13, n14 n9, n10",null,null
119,n14,null,null
120,"Table 2: Example user session: User reads at 225 words per minute, spending 60 seconds reading, starting at 9:55, on Dec 06, 2012. The column heading ""Time"" indicates the time at which the update was emitted, ""Conf."" is short for ""Confidence"", ""TtR"" means ""Time to Read"" and ""CTtR"" means ""Cumulative Time to Read"".",null,null
121,"gain at the end of each search result. If the simulated user does not finish reading a search result during a session, the result is considered unread and no gain is recorded. We consider unjudged updates as non-relevant, and we do not elide them.",null,null
122,We measure the gain of a search result as the number of nuggets of relevant information contained in the result. A nugget can be considered an atomic piece of information. Example nuggets from the TST 2013 qrels are shown in Table 1. Our simulated users consider only novel nuggets to be relevant. Previously seen nuggets provide no gain.,null,null
123,"Users interested in evolving events want the update produced by the search engine to be recent as well as relevant. Nuggets delivered late to a user are less likely to be considered relevant by the user. Given that different users will visit the search engine at different times, our notion of nugget timeliness must be relative to the simulated user. To make nugget timeliness relative to the user, we define a nugget as being late if it existed in the aggregate stream at a time equal to or before the start of the previous visit. In other words, if a user reads a result that contains a nugget that the search engine could have delivered during a previous visit, the nugget is late. To measure how late a nugget is, we define a function (n) that returns how many sessions ago the nugget could have been reported (Figure 1f). A nugget that is reported on time has an (n) of zero.",null,null
124,"While we do not know how the probability that a user will consider a nugget relevant changes with its lateness, an exponential decay seems reasonable. Therefore we compute the gain for every read nugget n as:",null,null
125,"g(n) , 1 × L(n)",null,null
126,(2),null,null
127,"where, L is the decay parameter for late reporting of nuggets and can vary between 0 and 1. L is a pre-determined value representing how much less a user is likely to consider the nugget to be relevant if it is reported late. For our exper-",null,null
128,"iments, we vary L from 0 to 1, where 0 indicates that a nugget loses all value if it is reported late, and 1 indicates that the nugget does not lose any value ever regardless of how late the reporting.",null,null
129,The gain from each read nugget is summed to get the cumulative gain for a user over the user trace (Figure 1d). Thus a simulated user's MSU for a search topic is given by:,null,null
130,"M SU , g(n)",null,null
131,(3),null,null
132,n,null,null
133,"where n is the set of nuggets read by the user. We compute the MSU for a system by computing for each simulated user their mean MSU over all topics and then averaging all users' mean MSU to produce a system mean MSU. To increase the numerical precision of our system's MSU estimate, we only need to increase the number of simulated users.",null,null
134,3.3 MSU and TST,null,null
135,"To demonstrate modeled stream utility (MSU), we use the TST 2013 test collection. The TST is a close but imperfect fit for MSU. The primary issue with using TST to demonstrate MSU is that the runs submitted by the participating groups were guided by the set-based effectiveness measures of TST.",null,null
136,"As described in section 2, each participating group submitted runs to the TST that consisted of a set of updates. Each update has both a timestamp and a confidence associated with it. The timestamp is when the system emitted the update and the confidence is a system generated score indicating how relevant the update is.",null,null
137,"For MSU, the simulated users check the retrieval system as per their user-trace to read the system's updates. At the start of every session in the user-trace, the updates emitted between the end of the last session and the start of the current session are presented to the user in a reverse chronological order, so that most recent information is read first.",null,null
138,679,null,null
139,(a) Input: A time ordered document stream.,null,null
140,(b) Output: Stream of updates d1..d10 emitted at various times by a system.,null,null
141,(c) Update-trace: Times of first occurrence of nuggets are identified. Updates containing nuggets are noted.,null,null
142,(d) User-trace: Simulated behavior of a user who reads updates from the stream from time to time.,null,null
143,"(e) Reading-trace: Determines which updates are available to read for every session. e.g. d3, d2 are available to read at the start of session 2. The user's reading speed V determines which",null,null
144,nuggets are actually read. Reading updates that contain nuggets adds to gain.,null,null
145,(f) (n): Gain is discounted by L(n). (n) is the number of sessions between the first occurrence of nugget n and the,null,null
146,current session within which an update reporting n is read by the user.  is only computed only if the update containing n is,null,null
147,read by the user. Figure 1: Evaluation Model,null,null
148,"Updates with the same timestamp are shown in descending order given their confidence. The simulated user starts reading the latest update and then reads the next older update, and so on, until the user runs out of time or encounters an already read update and stops reading further. In case the session ends, the last update that is partially read in the session, is considered as unread. The first user session always begins at the start of the query duration. To determine if a nugget is late, we use the TST assigned Wikipedia times as described in section 2. As with the TST, the gain of a novel, on-time nugget is 1.",null,null
149,3.4 MSU Example for User Gain in a Session,null,null
150,"Figure 1 illustrates the MSU model of evaluation. Figures 1a and 1b show the input to and output of a system that generates a stream of updates. The evaluation process begins at Figure 1c, where the nuggets, their time of first occurrence and the updates containing the nuggets are identified. The user-trace in Figure 1d is generated for a modeled user by alternatively sampling the exponential distributions for the session duration and the away time respectively. With the user-trace overlaid over the update-trace we get a readingtrace (Figure 1e). The reading-trace identifies updates available to read at each user session, and by incorporating the reading speed of the user it determines which updates are actually read. Thus the reading-trace determines if any relevant updates were read by the user or not, for every user session.",null,null
151,"For a more concrete example, consider a modeled user with A of 1 day, D of 60 seconds and a reading speed V , of 225 words per minutes. Let us also assume that this user considers late information to be half as likely to be relevant, for every session where it is unreported (L ,"" 0.5). Suppose that this user checks for updates at about 10:00 am every day and had previously checked for updates at 10:02 am on Dec 4, at 10:11 am on Dec 5, and at 9:50 am on Dec 6, 2012.""",null,null
152,"On Dec 7, 2012, the user starts a session at 9:55 am. The user finds the updates listed in Table 2 at the start of the session. The times at which the updates were emitted by the system are listed in the first column. In this case, all updates were emitted on the morning of Dec 7 before the user started the session. The updates are presented to the user in reverse chronological order. In case multiple updates are emitted at the same time, the updates are ordered by their system assigned confidence (as for the 4 updates emitted at 9:52).",null,null
153,"The user starts by reading the most recent update (delivered at 9:52 am). The user continues on to read the second update and finds 4 nuggets n11, n12, n13, n14. Table 1 lists the nuggets found in the session. These are nuggets that the user had not seen before and therefore experiences an increase in gain.",null,null
154,"However, as per each nugget's Wikipedia time, n11 first occurred at 6:31 p.m. on Dec 4, 2012. It should ideally have been reported to the user at the session starting at 10:11 am on Dec 5. The system further missed reporting the nugget at the session at 9:50 on Dec 6. Therefore (n11) is 2. By similar calculation (n12) ,"" 3, as it should have been reported for the user session at 10:02 on Dec 4. For nuggets n13 and n14, (n13) and (n14) are 1.""",null,null
155,"As the user reads further down the list, more nuggets (n9, n10) are read and gain increases accordingly. n10 is reported in time at the current session and (n10) , 0. The user gets no gain from reading the update emitted at 9:15 as",null,null
156,680,null,null
157,RunID (GroupID),null,null
158,cluster5 (PRIS) run2 (ICTNET) run1 (ICTNET) TuneExternal2 (hltcoe) TuneBasePred2 (hltcoe) cluster3 (PRIS) cluster2 (PRIS) uogTrNMTm1MM3 cluster1 (PRIS) cluster4 (hltcoe) BasePred (PRIS) Baseline (hltcoe) uogTrNSQ1 EXTERNAL (hltcoe) uogTrNMTm3FMM4 uogTrNMM uogTrEMMQ2 SUS1 (wim GY 2013) rg4 (UWaterlooMDS) rg3 (UWaterlooMDS) rg2 (UWaterlooMDS) rg1 (UWaterlooMDS) UWMDSqlec4t50 UWMDSqlec2t25 CosineEgrep (UWMDS) NormEgrep (UWMDS),null,null
159,#Upd. / topic,null,null
160,"21.9 93.8 97.8 799.4 2,696.1 42.3 122.1 358.8 164.8 163.0 8,790.7 12,743.0 139.0 22,476.1 168.3 954.7 2,077.8 2,338.7 41,863.3 42,534.1 299,559.6 312,863.3 213,735.7 230,056.0 11.9 151.3",null,null
161,ELG,null,null
162,0.136 0.127 0.125 0.118 0.114 0.103 0.074 0.069 0.067 0.067 0.067 0.063 0.060 0.055 0.049 0.045 0.040 0.036 0.028 0.026 0.022 0.021 0.018 0.017 0.010 0.001,null,null
163,Reason. MSU,null,null
164,4.35 9.45 9.46 5.34 5.49 5.99 9.31 7.28 9.57 9.55 5.84 5.87 6.85 5.60 6.33 7.63 6.88 3.62 1.44 1.45 0.32 0.30 1.02 0.61 0.55 0.73,null,null
165,ELG Rank,null,null
166,1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26,null,null
167,Reason. MSU Rank,null,null
168,17 4 3 16 15 11 5 7 1 2 13 12 9 14 10 6 8 18 20 19 25 26 21 23 24 22,null,null
169,Best Rank,null,null
170,8 1 1 13 11 5 1 6 1 1 1 1 4 1 5 1 2 17 1 2 18 19 6 14 19 19,null,null
171,Best Ranks achieved by Systems,null,null
172,MSU Parameter values for Best Rank,null,null
173,@Best MA,null,null
174,SA MD,null,null
175,SD L,null,null
176,4.06 1 d 12 h 30 s 30 s 0.9,null,null
177,9.27 1 d 12 h 5 m 2.5 m 0.1,null,null
178,14.65 1 d 12 h 30 m 15 m 0.9,null,null
179,9.77 3 h 1.5 h 30 m 15 m 0.9,null,null
180,9.89 1 h 30 m 15 m 7.5 m 0.9,null,null
181,5.83 1 d 12 h 30 s 15 s 1.0,null,null
182,12.02 3 h 1.5 h 30 s 30 s 1.0,null,null
183,12.79 1 h 30 m 30 s 15 s 1.0,null,null
184,16.45 5 m 10 m 30 s 30 s 1.0,null,null
185,16.15 10 m 20 m 30 s 30 s 1.0,null,null
186,14.17 6 h 6 h 30 m 15 m 0.9,null,null
187,15.02 30 m 15 m 30 m 15 m 0.9,null,null
188,11.35 3 h 3 h 30 s 15 s 1.0,null,null
189,20.38 30 m 1 h 30 m 15 m 1.0,null,null
190,10.52 3 h 1.5 h 30 s 15 s 1.0,null,null
191,19.07 30 m 15 m 15 m 7.5 m 1.0,null,null
192,18.55 30 m 15 m 15 m 15 m 1.0,null,null
193,7.44 1 h 1 h 2 m 60 s 1.0,null,null
194,22.11 5 m 10 m 30 m 15 m 1.0,null,null
195,21.54 5 m 5 m 30 m 15 m 1.0,null,null
196,14.02 5 m 10 m 30 m 15 m 1.0,null,null
197,13.39 5 m 10 m 30 m 15 m 1.0,null,null
198,12.20 5 m 5 m 30 m 15 m 0.9,null,null
199,15.87 5 m 10 m 30 m 15 m 1.0,null,null
200,0.30 1 d 12 h 60 s 30 s 0.3,null,null
201,0.52 3 h 1.5 h 30 s 15 s 0.5,null,null
202,"Table 3: Main results. The 26 runs submitted to the TREC 2013 Temporal Summarization Track (TST) are ordered by the TST primary measure, ELG. The column ""#Upd./topic"" shows the average number of updates produced by the run per topic. The ""Reason. MSU"" column shows the score for modeled stream utility (MSU) with a reasonable set of parameters as described in section 4.2. On the right side of the table we report an experiment where we selected parameter settings from our parameter sweep that give a run the best rank possible relative to the other runs (section 4.3).",null,null
203,"nugget n14 was read earlier. Since the session duration was 60 seconds, the last update was partially read and the user gets no gain on reading it. Thus the total gain (MSU) for this user, from this session on Dec 7, is 2.875.",null,null
204,4. EXPERIMENTS,null,null
205,"We explore the parameter space of MSU by applying it to evaluate the runs submitted to the TST 2013. We compare MSU's ranking of the TST runs to the TST's track's primary measure, ELG. The ELG measure is described in section 2.",null,null
206,"The 2013 TST evaluated 26 runs from 6 groups over 9 topics. Topics in the track are instances of event types from the set {accident, bombing, earthquake, shooting, storm}. Each topic had a time period (query duration) of 10 days. In other words, a run must summarize 10 days of material from the TREC KBA stream corpus [1] for each topic.",null,null
207,"The KBA corpus is essentially a time ordered document stream spanning from October 2011 to January 2013, containing over 1 billion documents crawled from the web. The corpus contains documents with 3 categories, i.e. the documents were crawled from URLs of public newswires (news), blogs and forums (social), and the URLs submitted for shortening at www.bitly.com. Documents are segmented into sentences and some documents are tagged with named entities using NLP tools. There are on average 93,037 documents per hour [5] in the corpus.",null,null
208,4.1 Parameter Sweep,null,null
209,"Keeping the user population's reading speed distribution the same across all parameter sets, we sweep the parameter",null,null
210,space by setting MSU's parameters to the following values:,null,null
211,"· User population mean session duration, MD ,"" {0.5, 1, 2, 5, 15, 30} minutes.""",null,null
212,"· User population mean time away, MA ,"" {5, 10, 30 minutes, 1, 3, 6, 24 hours}).""",null,null
213,"· Lateness decay parameter, L ,"" {0, 0.1, 0.25, 0.5, 0.75, 0.9, 1}.""",null,null
214,"For the standard deviations of the session duration, SD, and the time away, SA, we multiplied the mean by the values 0.5, 1, 2. For example, a mean session duration of 30 seconds, was associated with standard deviations of 15, 30 and 60 seconds. In total we generated 7 (MA) x 3 (SA) x 6 (MD) x 3 (SD) x 7 (L) ,"" 2646 parameter-tuples (points in the parameter space). For instance, the parameter tuple (MA"","" 6 hours, SA"","" 3 hours, MD"","" 5 minutes,SD"","" 5 minutes, L "","" 1), is a point in the parameter space that represents users who spend an average of 5 minutes every 6 hours reading updates, unaffected by late reporting of nuggets.""",null,null
215,"For each tuple (selected point from the parameter space) in the parameter sweep, we simulated 1000 users. For each simulated user, we generated their user-trace, determined which updates they read, and computed their average MSU across the topics for every run submitted to TST 2013.",null,null
216,4.2 Reasonable Parameters,null,null
217,We believe that a reasonable setting of MSU's parameters is a user population where the users visit on average for 2 minutes every 3 hours and for whom late material quickly,null,null
218,681,null,null
219,8,null,null
220,Modeled Stream Utility (MSU),null,null
221,6,null,null
222,Modeled Stream Utility (MSU),null,null
223,"MSU vs ELG Kendall's tau ,"" 0.471 , tau_AP "", 0.405",null,null
224,cluster4  cluster1 cluster2 ,null,null
225,run1  run2,null,null
226,uogTrNMM ,null,null
227,uogTrNMTm1MM3 ,null,null
228,uogTrEMMQ2 ,null,null
229, uogTrNSQ1,null,null
230,uogTrNMTm3FMM4 ,null,null
231,BaselinBeasePred cluster3 ,null,null
232,EXTERNAL ,null,null
233,TuneBasePred2  ,null,null
234,TuneExternal2,null,null
235,cluster5 ,null,null
236,SUS1 ,null,null
237,4,null,null
238,6,null,null
239,8,null,null
240,"MSU vs LC Kendall's tau ,"" -0.108 , tau_AP "", -0.244",null,null
241,rruucnnl1u2stecrl4usctelur2ster1,null,null
242,uogTrNMM ,null,null
243,uogTrNMTm1MM3 ,null,null
244,uogTrNSQ1 ,null,null
245, uogTrEMMQ2,null,null
246,uogTrNMTm3FMM4  cluster3 ,null,null
247,TuneBasePred2,null,null
248,TuneExternal2,null,null
249,BasePred   Baseline  EXTERNAL,null,null
250,cluster5 ,null,null
251,SUS1 ,null,null
252,4,null,null
253,2,null,null
254,2,null,null
255,0,null,null
256,rg3  rg4 ormEgrep  UWMDSqlec4t50,null,null
257,CosineEgreUpWrgM2DSqlec2t25 rg1,null,null
258,0.00,null,null
259,0.02,null,null
260,0.04,null,null
261,0.06,null,null
262,0.08,null,null
263,0.10,null,null
264,0.12,null,null
265,0.14,null,null
266,Expected Latency Gain (ELG),null,null
267,"(a) Reasonable Users, MSU vs ELG Correlation.",null,null
268,0,null,null
269,NormEgrep,null,null
270,CosineEgrep,null,null
271,0.0,null,null
272,0.1,null,null
273,rg3  rg4,null,null
274,UWMDSqlec4t50 ,null,null
275,UWMDSqlec2t25  rg2  rg1,null,null
276,0.2,null,null
277,0.3,null,null
278,0.4,null,null
279,0.5,null,null
280,Latency Comprehensiveness (LC),null,null
281,"(b) Reasonable Users, MSU vs LC Correlation.",null,null
282,"Figure 2: MSU with reasonable parameter settings (MA,"" 3 hours, SA"","" 1.5 hours, MD"","" 2 minutes, SD"","" 1 minute, L"","" 0.5), compared to TST 2013 measures, ELG and LC.""",null,null
283,"becomes less likely to be considered relevant. This corresponds to the parameter tuple (MD ,"" 2 minutes, SD "","" 1 minute, MA "","" 3 hours, SA "","" 1.5 hours, L "", 0.5).",null,null
284,"Both Figure 2a and Table 3 show the results of MSU vs. the TST measure ELG, given these reasonable parameter settings. For both MSU and ELG, larger values indicate better runs. At these parameter settings, MSU and ELG do not rank systems the same (Kendall's  ,"" 0.471). As Figure 2a shows, the runs in the middle-lower positions of the ELG ranking jump to the top positions. A similar observation can be made in Figure 2b for MSU vs. the LC measure. Here as well, MSU and LC rank systems differently (Kendall's  "", -0.108). ELG and LC are themselves negatively correlated (Kendall's  ,"" -0.28). The AP correlation coefficient (AP ) [16, 13], which is more sensitive to changes in higher ranks is also reported in Figure 2.""",null,null
285,"Across the entire set of swept parameter settings, the maximum Kendall's  correlation was found to be 0.625 for the parameters: MA,"" 24 hours, SA"","" 12 hours, MD"","" 1 minute, SD"","" 2 minutes, L "","" 0.1. These results are shown in Figure 3. While not a high correlation, this result shows us that ELG best correlates with a simulated user population where users on average visit once a day for 1 minute. In other words, over the 10 days of the query period, an average user reads about 10 minutes of material. If we were to understand ELG in terms of the users that would prefer its top ranked runs, then ELG is tuned for highly time constrained and selective users with low tolerance for late reporting.""",null,null
286,"A correlation of 0.625 correlation is not high, and MSU and ELG are behaving quite differently. We believe that there are likely two main reasons for the different rank order of the runs. The first reason is that run performance with MSU is affected by the amount of material that the simulated user reads. If a run does not supply enough ma-",null,null
287,"terial for a session visit, then the simulated user will stop reading and also stop accumulating gain.",null,null
288,"For example, Table 3 shows that the top ELG run cluster5 averages only 21.9 updates per topic over a 10 day query duration. With the reasonable parameter settings, MSU simulated users spend about 2 minutes every 3 hours, i.e. about 160 minutes reading, on average, over a 10 day query duration. With an average length of each TST update being 63 words, and the average reading speed being 4.3 words per second, it takes on average just 319 seconds to read all the 21.9 updates of cluster5. A simulated user, who is willing to read for 160 minutes in total over 10 days, thus derives very low gain from cluster5 because such a user's visit is cut short from a lack of material to read.",null,null
289,"Figure 4 shows the user performance at the point (MA,"" 5 minutes, SA"","" 10 minutes, MD"","" 30 minutes, SD"","" 15 minutes, L "", 1) that has the minimum correlation of MSU with ELG (Kendall's  ,"" -0.04), in our parameter set. This set of users seem inclined to spend almost all their time with the system reading updates taking a 5 minute break every 30 minutes on average, without any dissatisfaction for late reporting of information. Unsurprisingly, these users achieve the highest amount of MSU (22.11) with run rg4. While seemingly an unreasonable parameter setting for MSU, such user behavior could be achieved by a team interested in constant monitoring of a stream of updates.""",null,null
290,"This result also shows that MSU scores are related to the amount of material read by the simulated users. MSU measures gain as the number of relevant nuggets read in the total content consumed by a user, while discounting for lateness if required. The amount of material read depends on the characteristic behavior of the user (or user population) as well as the number of updates emitted by the system. MSU for a system is simply the average over all simulated users.",null,null
291,A second reason for differences between ELG and MSU is likely that ELG is a set-based measure that measures av-,null,null
292,682,null,null
293,4,null,null
294,"MSU vs ELG Kendall's tau ,"" 0.625 , tau_AP "", 0.522",null,null
295,cluster2  cluster4  cluster1,null,null
296,uogTrNSQ1 ,null,null
297,run1  run2,null,null
298,cluster3 ,null,null
299,uogTrNMTm3FMM4,null,null
300,uogTrNMTm1MM3,null,null
301,cluster5 ,null,null
302,3,null,null
303,Modeled Stream Utility (MSU),null,null
304,2,null,null
305,uogTrNMM ,null,null
306,Baseline,null,null
307,BasePred  EXTERNAL  uogTrEMMQ2 ,null,null
308,TuneExternal2 TuneBasePred2  ,null,null
309,SUS1 ,null,null
310,1,null,null
311,0,null,null
312,DorSmCqEolesgcirn2eetp2E5grreUgp3WrgM2rDg4Sqlec4t50 rg1,null,null
313,0.00,null,null
314,0.02,null,null
315,0.04,null,null
316,0.06,null,null
317,0.08,null,null
318,0.10,null,null
319,0.12,null,null
320,0.14,null,null
321,Expected Latency Gain (ELG),null,null
322,"Figure 3: Maximum correlation of MSU with ELG is obtained with parameters (MA,"" 24 hours, SA"","" 12 hours, MD"","" 1 minute, SD"","" 2 minutes, L"", 0.1).",null,null
323,20,null,null
324,15,null,null
325,Modeled Stream Utility (MSU),null,null
326,"MSU vs ELG Kendall's tau ,"" -0.04 , tau_AP "", 0.0362",null,null
327,rg4  rg3  EXTERNAL ,null,null
328,Baseline   BasePred,null,null
329,UWMDSqlec4t50,null,null
330, uogTrNMM,null,null
331,uogTrEMMQ2,null,null
332,cluster4  cluster1,null,null
333,UWMDSqlec2t25,null,null
334,uogTrNMTm1MM3 ,null,null
335, cluster2,null,null
336,run1  run2,null,null
337,uogTrNMTm3FMM4,null,null
338,rg2 ,null,null
339,rg1 ,null,null
340,uogTrNSQ1 ,null,null
341,TuneBasePred2  TuneExternal2 ,null,null
342,SUS1 ,null,null
343,cluster3 ,null,null
344,cluster5 ,null,null
345,10,null,null
346,5,null,null
347, NormEgrep  CosineEgrep,null,null
348,0,null,null
349,0.00,null,null
350,0.02,null,null
351,0.04,null,null
352,0.06,null,null
353,0.08,null,null
354,0.10,null,null
355,0.12,null,null
356,0.14,null,null
357,Expected Latency Gain (ELG),null,null
358,"Figure 4: Minimum correlation of MSU with ELG is obtained with parameters (MA,"" 5 minutes, SA"","" 10 minutes, MD"","" 30 minutes, SD"","" 15 minutes, L"", 1).",null,null
359,"erage quality of an update. In contrast, MSU is estimating the total number of nuggets read and considered by the simulated user to be relevant. To see how much the set-based nature of ELG is causing it to be different than MSU, we transformed MSU into a similar measure by taking MSU and dividing it by the time the simulated user spent reading (MSU/second).",null,null
360,"In our experiments, MSU/second has the highest correlation (Kendall's  ,"" 0.754) with ELG (Figure 5), with parameters (MA"","" 24 hours, SA"","" 12 hours, MD"","" 1 minute,SD"","" 1 minutes, L "","" 0.1). The correlation between MSU/second and ELG is greater than the maximum correlation between MSU and ELG. Of note, the top ranked run for MSU/second is now the same as for ELG: cluster5. The MSU/second top 4 runs in Figure 5 are 4 of the 5 runs with the lowest number of updates submitted (Table 3).""",null,null
361,4.3 Everyone's a Winner (Almost),null,null
362,"Noting how the ranking of systems can change greatly based on the wide range of parameter settings in our sweep, we decided to find the instances in our parameter sweep for which a particular system was ranked the highest across all parameter sets. Table 3 shows the results of this analysis on its right side. Some systems achieved their best rank for multiple parameter sets. In such cases, we chose the parameter set for which the system had the highest MSU.",null,null
363,"The relationship between time spent and user performance can be seen as we look at the parameter settings from top to bottom of Table 3. Systems that had very few updates submitted, performed well for users who might visit a system about once a day for 30 seconds to 30 minutes on average. The run cluster2 seems to be the best performing system for users who return to the system about every 3 hours for 30 seconds on average. As we go lower in the table, we see that spending more time reading (larger MD) and taking shorter breaks (smaller MA) improves performance of systems that",null,null
364,ranked lower on ELG. These systems are typically those that submitted a large number of updates.,null,null
365,"As Table 3 shows, almost all groups have at least one run that is able to achieve a rank 1 performance under some setting of MSU's parameters. If we want to know which system is the best, we must calibrate MSU given actual user behavior. This result may also show that submitted TST systems had very different notions of the appropriate amount of material to make available to users. TST's ELG measure is set-based, but the decision of how large of a set to return to users is left to system designers. In contrast, MSU guides system designers to return a suitable amount of material based on the model of user behavior.",null,null
366,5. OTHER RELATED WORK,null,null
367,"Our work builds directly on recent research in modeling user behavior for improved evaluation of information retrieval systems [7]. Of these new effectiveness measures, Clarke and Smucker's [8] Time Well Spent (TWS) measure is the most similar. Like modeled stream utility (MSU), TWS provides a means to evaluate systems that produce a stream of content. Unlike TWS, MSU models multiple visits to the same system, and MSU models variation in each simulated user's behavior. MSU should be amenable to the same statistical analyses that TWS enables.",null,null
368,"Other researchers have also evaluated systems by defining a hypothetical user interface and then simulating user behavior with the interface. Of the more recent work that goes beyond a single query and results list is that of Baskaya, Keskustalo, and J¨arvelin [6] who conducted an experiment that considered many different parameter settings for their model and investigated various query reformulation strategies that could be utilized by a user. Yang and Lad [15] also employ a user model based evaluation for information distillation systems and it would be interesting to compare MSU with their method in future work.",null,null
369,683,null,null
370,"MSU/second vs ELG Kendall's tau ,"" 0.754 , tau_AP "", 0.599",null,null
371,cluster5 ,null,null
372,0.08,null,null
373,0.06,null,null
374,Modeled Stream Utility per Second (MSU/second),null,null
375,cluster3  run1  run2,null,null
376,0.04,null,null
377,0.02,null,null
378,cluster2 ,null,null
379,cluster4  cluster1 Baseline,null,null
380,uogTrNSQ1BasePrueodgTrNMTm1MMTu3neExternal2,null,null
381,uogTrNMTm3FMM4 ,null,null
382,EXTERNAL ,null,null
383,TuneBasePred2,null,null
384,SUS1   uogTrNMM mDDESCSgqqorleelsecpinc42et5tE20g5rerrpgg12rgr3g4  uogTrEMMQ2,null,null
385,0.00,null,null
386,0.02,null,null
387,0.04,null,null
388,0.06,null,null
389,0.08,null,null
390,0.10,null,null
391,0.12,null,null
392,0.14,null,null
393,Expected Latency Gain (ELG),null,null
394,0.00,null,null
395,"Figure 5: Maximum correlation of MSU/second with ELG is obtained with parameters (MA,"" 24 hours, SA"","" 12 hours, MD"","" 1 minute, SD"","" 1 minutes, L"", 0.1).",null,null
396,6. DISCUSSION,null,null
397,"ELG has aspects that try to capture characteristics of good updates, i.e. updates should be on time, not long, and have relevant material. ELG seems to be a measure oriented towards measuring the performance of systems that push highly relevant updates with low frequency to the user. ELG does not provide an easy means to be calibrated to known user behavior.",null,null
398,"Our experiments and analysis show that it matters how much material is read. By specifying the amount of material in user terms, we have a way of then calibrating a measure once we know actual user behavior. Observing actual user behavior while evolving events are actually taking place would involve monitoring users' browsing histories. The sudden nature of news events makes a live user study difficult to organize. Search log-analysis may provide some indirect insight into user behavior when such events are running. Our analysis also shows that there may be a case for personalization of stream filtering systems for different user behaviors. As other future work, we hope to extend our understanding of MSU, both in terms of its formal properties and in terms of empirical meta-evaluation criteria, including robustness to noise, discriminativeness between systems, and strictness [3].",null,null
399,7. CONCLUSION,null,null
400,"We introduced an effectiveness measure that utilizes a model of user behavior for evaluating systems producing streams of information about evolving events. Our measure is designed to be calibrated based on actual usage data. Our user model simulates a user checking back with the system to read the most recent information from time to time. Users can check back with different frequencies and for different amounts of time depending on various factors. By modeling user behavior, our effectiveness measure produces a score that is easily interpretable as the number of relevant nuggets",null,null
401,"of information read by the user. As would be expected, we found that for streams of updates, the gain is sensitive to the amount of time a user spends for reading updates. While temporal summarization systems have traditionally been evaluated in term of their precision and recall, we believe that it is important to consider both the user interface and the user behavior with this interface when evaluating such systems. Given the degree to which the rankings of systems changed as we modified the behavior of the user population, we believe that an effectiveness measure such as ours when calibrated with user data will allow system developers and researchers to build better performing systems tuned to user behavior.",null,null
402,8. ACKNOWLEDGMENTS,null,null
403,"This work was made possible by the facilities of SHARCNET (www.sharcnet.ca) and Compute/Calcul Canada, and was supported in part by GRAND NCE, in part by an Amazon AWS in Education Research Grant, in part by NSERC, in part by a Google Founders Grant, and in part by the University of Waterloo.",null,null
404,9. REFERENCES,null,null
405,[1] KBA Stream Corpus 2013. http://trec-kba.org/kba-stream-corpus-2013.shtml.,null,null
406,"[2] J. Allan, R. Gupta, and V. Khandelwal. Temporal Summaries of New Topics. In SIGIR, pp. 10­18, 2001.",null,null
407,"[3] E. Amigo´, J. Gonzalo, and S. Mizzaro. A Formal Approach to Effectiveness Metrics for Information Access: Retrieval, Filtering, and Clustering. In ECIR, pp. 817­821. 2015.",null,null
408,"[4] J. Aslam, F. Diaz, M. Ekstrand-Abueg, V. Pavlu, and T. Sakai. TREC 2013 Temporal Summarization. In TREC, 2013.",null,null
409,"[5] G. Baruah, A. Roegiest, and M. D. Smucker. The Effect of Expanding Relevance Judgements with Duplicates. In SIGIR, pp. 1159­1162, 2014.",null,null
410,"[6] F. Baskaya, H. Keskustalo, and K. J¨arvelin. Modeling Behavioral Factors in Interactive Information Retrieval. In CIKM, pp. 2297­2302, 2013.",null,null
411,"[7] C. L. Clarke, L. Freund, M. D. Smucker, and E. Yilmaz. Report on the SIGIR 2013 workshop on Modeling User Behavior for Information Retrieval Evaluation (MUBE 2013). SIGIR Forum, 47(2):84­95, Jan. 2013.",null,null
412,"[8] C. L. A. Clarke and M. D. Smucker. Time Well Spent. In IIiX, pp. 205­214, 2014.",null,null
413,"[9] B. P. Flannery, W. H. Press, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C, pp. 214­215, Cambridge University Press, 1988.",null,null
414,"[10] Q. Guo, F. Diaz, and E. Yom-Tov. Updating Users About Time Critical Events. In ECIR, pp. 483­494, 2013.",null,null
415,"[11] J. Lin and M. Efron. Overview of the TREC-2013 Microblog Track. In TREC, 2013.",null,null
416,"[12] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 Microblog Track. In TREC, 2011.",null,null
417,"[13] M. D. Smucker, G. Kazai, and M. Lease. Overview of the TREC 2013 Crowdsourcing Track. In TREC, 2013.",null,null
418,"[14] I. Soboroff, I. Ounis, J. Lin, and I. Soboroff. Overview of the TREC-2012 Microblog Track. In TREC, 2012.",null,null
419,"[15] Y. Yang and A. Lad. Modeling Expected Utility of Multi-Session Information Distillation. In ICTIR, pp. 164­175, 2009.",null,null
420,"[16] E. Yilmaz, J. A. Aslam, and S. Robertson. A new Rank Correlation Coefficient for Information Retrieval. In SIGIR, pp. 587­594, 2008.",null,null
421,684,null,null
422,,null,null

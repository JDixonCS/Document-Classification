,sentence,label,data
0,Generalized BROOF-L2R: A General Framework for Learning to Rank Based on Boosting and Random Forests ,null,null
1,Clebson C. A. de Sá Marcos A. Gonçalves Daniel X. Sousa Thiago Salles,null,null
2,"Federal University of Minas Gerais Computer Science Department Belo Horizonte, Brazil",null,null
3,"{clebsonc, mgoncalv, danielxs, tsalles}@dcc.ufmg.br",null,null
4,ABSTRACT,null,null
5,"The task of retrieving information that really matters to the users is considered hard when taking into consideration the current and increasingly amount of available information. To improve the effectiveness of this information seeking task, systems have relied on the combination of many predictors by means of machine learning methods, a task also known as learning to rank (L2R). The most effective learning methods for this task are based on ensembles of tress (e.g., Random Forests) and/or boosting techniques (e.g., RankBoost, MART, LambdaMART). In this paper, we propose a general framework that smoothly combines ensembles of additive trees, specifically Random Forests, with Boosting in a original way for the task of L2R. In particular, we exploit out-of-bag samples as well as a selective weight updating strategy (according to the out-of-bag samples) to effectively enhance the ranking performance. We instantiate such a general framework by considering different loss functions, different ways of weighting the weak learners as well as different types of weak learners. In our experiments our rankers were able to outperform all state-of-the-art baselines in all considered datasets, using just a small percentage of the original training set and faster convergence rates.",null,null
6,Keywords,null,null
7,Learning to Ranking; Random Forests; Boosting,null,null
8,1. INTRODUCTION,null,null
9,"Today, we live in an era of massive available information, with a never-seen-before (and increasing) rate of information production. It is not surprising that such a scenario imposes hard to tackle challenges. For example, the availability of massive amounts of data is not of great help if one is not",null,null
10,"This work was partially funded by projects InWeb (grant MCT/CNPq 573871/2008- 6) and MASWeb (grant FAPEMIG/PRONEX APQ-01400-14), and by the authors' individual grants from CNPq, FAPEMIG, Capes and Google Inc.",null,null
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",null,null
12,"SIGIR '16, July 17­21, 2016, Pisa, Italy.",null,null
13,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911540,null,null
14,"able to effectively access relevant information that satisfies her information needs. Retrieval systems, such as search engines, question and answer, and expert search systems serve exactly this purpose: given an information need, expressed in the form of a query, and a set of possible information units (e.g., documents), the main goal is to provide an ordered list of information units according to their relevance with relation to the query. The desideratum is to increase the likelihood of satisfying an user's information need in an effective manner, which translates to maintain the truly relevant results on top of the less relevant ones.",null,null
15,"One of the key aspects that influence retrieval systems is how they determine the relative relevance among candidate results in order to produce a ranked list based on their relevance with regard to some information need, posed in the form of a query. The quality of those rankings is thus paramount to guarantee efficient and effective access to relevant information (and, hopefully, the satisfaction of the user's information needs). Several approaches to generate such ranked lists do exist, being traditionally performed by the specification of a function that is able to relate some user's query to the set of known (indexed) information units. Usually, ranking functions consider several features, such as those that rely on the relatedness between query and possible results (e.g., BM25, edit distance, similarities in vector space models) or on link analysis information (e.g., PageRank, HITS). Such features must be somehow combined to provide accurate relevance scores (and, thus, a properly ranked list of results).",null,null
16,"Unfortunately, to specify and tune ranking functions turns out to be a major problem, specially when the number of features becomes large, with non-trivial interactions. This motivates the use of supervised machine learning techniques to devise such functions, since machine learning techniques are effective to combine multiple pieces of evidence towards optimizing some goal. This is the direction pursued by Learning to Rank (L2R) techniques, the primary focus of this work.",null,null
17,"More specifically, based on a set of query-document pairs with known relevance judgments, the goal is to learn a function f (d, q) that is able to accurately devise the relevance scores for a document d, with respect to a query q. Due to its importance, several approaches for L2R have been proposed in the literature. Ensemble methods, such as RankBoost [7], AdaRank [32] and Random Forests [1] (and the variations thereof, such as [11]), are deemed to be the techniques of choice for L2R, achieving higher effectiveness in published benchmarks when compared to other algorithms [11, 3]. Both RankBoost and AdaBoost are based on boosting [26], an",null,null
18,95,null,null
19,"iterative meta-algorithm that combines a series of weaklearners in order to come up with a strong final learner, focusing on hard-to-classify regions of the input space as the iterations go by. The strategies based on Random Forest rely on the combination of several decision trees, learned using bootstrapped samples of the training set, together with additional sources of randomization (such as random feature selection) to produce decorrelated-correlated trees--a requirement to guarantee its effectiveness.",null,null
20,"In this work, we propose a general framework for L2R, named Generalized BROOF-L2R that explores the advantages of boosting and Random Forests, by combining them in a non-trivial fashion. More specifically, at each iteration of the boosting algorithm, a Random Forest model is learned, considering training examples sampled according to a probability distribution. Such probability distribution is updated at the end of each iteration, in order to force the subsequent learners to focus on hard to classify regions of the input space. In particular, the use of RF models as weak learners has its own advantages, since it is capable of providing robust estimates of expected error through out of bag error estimates and, by means of selectively updating the weights of out of bag samples, one can effectively slow down the tendency of boosting strategies to overfit (a well known phenomenon that becomes critical as the noise level of the dataset being analyzed increases).",null,null
21,"As we shall detail in the next sections, the key aspects of the proposed Generalized BROOF-L2R have to do with how to update the probability distribution and how such update should be performed, as well as the underlying ranker to be used to produce the final set of results. In this work, we discuss a set of possible instances of the proposed general framework, in order to highlight the behavior and potential of the proposed L2R solution. In fact, the instances that makes use of out-of-bag samples and optimizes through gradient descent [12] over the residues is able to achieve the strongest results, in terms of Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG), with significant improvements over the explored adversary algorithms, considering 5 traditional benchmark datasets. Our alternative instances were also able to achieve competitive (or superior) results when compared to the baselines. Moreover, as our experimental evaluation shows, our approaches based on the proposed general framework are able to produce topnotch results with substantially less training samples when compared to the baselines. Such data efficiency is key to guarantee practical feasibility as obtaining labeled data is still a costly process.",null,null
22,"To summarize, the contributions of this work are threefold. We provide a general framework for L2R that is able to combine two strong methods (boosting and Random Forests) in an original way, which can be specialized in several ways and produce highly effective L2R solutions. We propose and discuss a set of alternative instantiations of such a framework, in order to highlight the behavior and effectiveness of each possible choice. Finally, we advance the state of the art in L2R by means of some instantiations of our proposed framework that are able to outperform top-notch solutions, according to an extensive benchmark evaluation considering five datasets and seven L2R baseline algorithms.",null,null
23,"Roadmap: Section 2 discusses related work. Section 3 presents our proposed Generalized BROOF-L2R framework, as well as outlines our proposed set of possible instantiations",null,null
24,"of the proposed framework. We clarify our experimental setup and discuss the obtained results in Section 4. Finally, Section 5 concludes and highlights some future work.",null,null
25,2. RELATED WORK,null,null
26,"Learn to Rank (L2R) [17] is the focus of active developments due to its cross-industry and society importance. Here, we review some relevant work on this topic, positioning our work in the literature.",null,null
27,"L2R attempts to improve traditional strategies for ranking query results according to some relevance criteria, by exploring supervised machine learning algorithms to combine various relevance related features into a more effective ranking function, based on a set of queries and associated documents with relevance judgments. L2R have been successfully applied to a variety of tasks, such as Question and Answer [28], Recommender [29, 16] and Document Retrieval [14] systems.",null,null
28,"Solutions specifically tailored to improve document retrieval have been extensively studied in the past years [4, 19]. In general, there are three major L2R approaches: the pointwise, pairwise and listwise approaches. Pointwise L2R algorithms are probably the simplest (yet successful) approaches, directly translating the ranking problem to a classification/regression one. In this case, the training set for the supervised learning algorithm consists of pairs qi, (xi,j, yi,j) of queries qi and a list of associated documents xi,j, each one with its relevance judgment yi,j. In this case, each triple qi, xi,j, yi,j is considered to be a single training example. The goal is to learn a classifier/regressor model capable of accurately predicting the relevance score of a document x , with relation to a query qi, thus producing a partial ordering over documents. Pairwise algorithms, on the other hand, transform the ranking problem into a pairwise classification/regression problem. In this case, learning algorithms are used to predict orders of document pairs, thus exploring more ground-truth information than the pointwise approaches. Unlike both mentioned strategies, the listwise approaches essentially treat qi, (xi,j, yi,j)j as a single training instance (that is, considering a ranked list of documents for a query qi as a single training example), capturing more information from the training set (namely, group structure) than the previous alternatives. Of course, being able to better capture training data information when learning a ranking function comes with a price: usually, pairwise and mainly listwise approaches are harder to train, since they require more sophisticated (e.g. query-level) loss functions [17].",null,null
29,"In terms of the state-of-the-art in L2R, methods based on Random Forests (RFs) and boosting were shown to be strong solutions according to already published benchmarks [22, 11, 3]. More specifically, RFs (and the variations thereof [11]) as well as boosting algorithms such as Gradient Boosted Regression Trees (GBRT) [9] and LambdaMART [33], are considered by many [3, 22, 18] to be the state of the art in L2R tasks. This work is based on both RFs and boosting strategies. Thus, in the following we briefly review some previous literature on them.",null,null
30,"The RF algorithm was proposed in [1] as a variation of bagging of low-correlated decision/regression trees, built with a series of random procedures, such as bootstrapping of training data and random attribute selection. The popularity of RFs is highlighted by their successful application in several domains, such as tag recommendation [3], object segmentation [27], human pose recognition [30] and L2R [3, 22],",null,null
31,96,null,null
32,"to name a few. Thus, it is natural to expect several extensions to it, in order to improve its effectiveness even more. One such extension is the extremely randomized trees (ERT) model [10] and its application to L2R [11]. The ultimate goal of ERTs is to reduce the correlation between the trees composing the ensemble, a requirement to guarantee high effectiveness of RF models. This is achieved by modifying the RF algorithm in, essentially, two aspects: each tree is learned considering the entire training set, instead of bootstrapped samples. Furthermore, in order to determine the decision splits after the random attribute selection, instead of selecting a cut-point that optimizes node purity, ERTs simply select a random cut-point threshold. This ultimately reduces tree correlation, potentially improving generalization capability of the learned model. As a final remark, such RF based models can be regarded as nonlinear pointwise approaches for L2R.",null,null
33,"Boosting strategies have also been shown to produce state of the art results on L2R tasks, with GBRT [9] (a.k.a, MART1 (Multiple Additive Regression Trees) and Lambda-MART [33] as the two perhaps most widely used strategies. Both algorithms are additive ensembles of regression trees. GBRT learns a ranking function by approximating the root mean squared error (RMSE) on the training set through gradient descent. As with typical boosting algorithms, the goal of GBRT is to focus on regions of the input space where predicting the correct relevance score is a hard task. Since this algorithm aims at approximating the RMSE on the training data, it can be regarded as a pointwise approach. The Lambda-MART algorithm, on the other hand, is a listwise approach that directly optimizes the ranked list of documents according to some retrieval measure, such as NDCG (instead of simply approximating the RMSE of the training documents relevance scores in isolation). To this end, Lambda-MART learns a ranking function that generates a list of relevant documents to a query that is as close as possible to the correct rank. As GBRT, it is based on gradient descent to optimize such metric.",null,null
34,"Due to the successful application of RFs and boosting in machine learning tasks (such as classification and L2R), some authors propose to use both strategies in order to come up with better learned models. For example, in [22] GBRTs and RFs are independently explored in order to learn better ranking functions. More specifically, the GBRT model is initialized with the residues of the RF algorithm, followed by the traditional iterations of a GBRT model. The main motivation behind this approach is that RFs are less prone to overfitting, being ideal to initialize the GBRT algorithm instead of the usual uniform initialization. According to the reported benchmark, such strategy was shown to be superior to the GBRT algorithm.",null,null
35,"Unlike [22], in [21] the authors propose an enhanced RF model for classification by boosting the decision trees composing the ensemble. In this case, each tree is learned with training examples weighted by wi, resembling boosting by re-weighting. In particular, training instances with higher weights influence more when determining the decision nodes (and cut-point threshold definition). Furthermore, each tree is evaluated according to this weighted training set, which enables the ensemble to focus on hard-to-predict regions. The observed effect of such combination is the ability to",null,null
36,"1From now on, we will use MART and GBRT as synonyms.",null,null
37,"come up with high quality models with substantially reduced training sets. As we shall detail, our proposed framework is tailored for the L2R task and, instead of introducing boosting into random forests, we apply boosting to several RF models, which act as weak learners.",null,null
38,"Differently from the aforementioned previous work, we base ourselves in a recent development for text classification, namely, the BROOF algorithm [25]. In BROOF algorithm, RF and boosting strategies are tightly coupled in order to exploit their unique advantages: by exploiting out of bag error estimates as well as selectively updating training weights according to out of bag samples, the BROOF model is able to focus on hard-to-classify regions of the input space, without being compromised by the boosting tendency to overfit. This ultimately leads to competitive results when compared to state of the art algorithms. In here, we generalize such approach specifically for L2R tasks in order to come up with better ranking functions: the Generalized BROOF-L2R. As we shall see, this general framework is flexible enough so that it can be instantiated in several ways, exploiting distinct characteristics of the ranking tasks being addressed. In special, with this general framework we are able to achieve state of the art results, with rankers superior to the top notch algorithms proposed so far in all evaluated cases.",null,null
39,3. GENERALIZED BROOF-L2R,null,null
40,"In this section, we detail our proposed Generalized BROOFL2R framework. Briefly speaking, this framework allows the definition of learners based on the combination of Random Forests and the Boosting meta-algorithm, in a non-trivial fashion. As we shall see, this framework establishes a set of operations to be performed during the boosting iterations, in a well defined order of application. The goal is to drive the weak learners towards hard to predict regions of the underlying data representation, in order to come up with an optimized additive combination of weak learners to form the final predictor. The extension points of the proposed framework can produce a heterogeneous set of instantiations that typically produces very competitive results for L2R. In the following, we present the generalized framework for L2R, as well as some pointwise instantiations. We stress that the set of instantiations discussed here is far from exhaustive, being possible to elaborate even better possibilities in future work.",null,null
41,3.1 Framework Description,null,null
42,"Based on the BROOF algorithm, proposed in [25] to solve text classification tasks, we here extend the proposed ideas in order to exploit the combination of Random Forests and Boosting for the specific task of L2R. However, instead of directly adapting the original algorithm to a single L2R method, we here generalize it into an extensible framework that is flexible enough to permit a series of possible instantiations. The proposed framework, named Generalized BROOF-L2R is an additive model composed of several Random Forest models, which act as weak-learners. Each fitted model influences the final decision proportionally to its accuracy, focusing -- as the boosting iterations go by -- on ever more complex regions of the input space, in order to drive down the expected error. As usual in a boosting strategy, two aspects play a key role: (i) the influence t of each learner in the fitted additive model, and (ii) the strategy to update the sample distribution wi,j in each iteration t of the boosting meta-algorithm.",null,null
43,97,null,null
44,"The basic structure of the framework is outlined in Algorithm 1, together with a brief explanation of what we call its extension points--the general functions exploited by the framework to determine how the optimization process works. There are 5 general functions whose purpose is to specify the weight distribution update process, the error estimation and the underlying input representation. Particularly, the use of the Random Forest classifier as a weak learner extends the range of possible instantiations of the framework, since it enables us to come up with better error rate estimates and a more selective approach to update the examples' weights, through the use of the so-called out-of-bag samples.",null,null
45,"Let Qtrn ,"" {(qi, {xi,j , yi,j })|m j"",""i1} be the training set, descriptively the set of documents xi,j, with associated graded relevance judgment yi,j with relation to a query qi. Initially, associate a weight wi,j with each training example xi,j according to the general function InitializeWeights. For each boosting iteration t, the input data representation may be updated, through the general function UpdateExamples. This general function can considerably extend the range of possible implementations of the framework, allowing us for example, to instantiate a Gradient Boosting Machine algorithm [20, 12]. Then, a Random Forest regressor model RFt is learned considering this data representation.""",null,null
46,"In order to evaluate the generalization capabilities of RFt, predict y^ for a set of training documents given by ValidationSet. The output of this step is paramount to guide the optimization process towards hard to classify regions of the input space. Although being of great importance to boosting effectiveness, this focus on hard to classify regions of the input space may also be harmful to the optimization process, specially when dealing with noisy data. As noted by [8, 13], boosting tends to increase the weights of few hard-to-classify examples (e.g., noisy ones). Thus, the decision boundary may only be suitable for those noisy regions of the input space while not necessarily general enough for general examples. In order to offer a greater robustness against such a drawback, our framework exposes an intermediary step related to how the examples weights get updates as the boosting iterations go by. The general function ValidationSet serves the purpose of specifying which training examples should be used during error estimation and weights update. The main goal here is to provide some mechanism to slowdown overfitting as well as provide more robust estimates of error weight (to capture the generalization power of each weak learner and to determine how they should influence the final predictor).",null,null
47,"The selected training examples are then used to compute both the error rate of the model and the influence t of the weak learner on the final model, through ComputeLearnerWeights. Finally, the training examples' weight distribution is updated by UpdateExampleWeights. This update process should, ideally, take into account the generalization capability of the current weak learner RFt, as well as how hard is to correctly predict the ranked lists of the validation examples. Validation examples whose outcome is hard to predict by an accurate learner should influence more in the following boosting iterations. An early stopping strategy is adopted, terminating the boosting iterations if the current learner has an estimated error rate greater than 0.5. The final prediction rule is then given by an additive combination of the weak-learners RFt, weighted by t.",null,null
48,Instantiation BROOFabsolute BROOFmedian BROOFheight BROOFgradient,null,null
49,Description,null,null
50,Extension Point,null,null
51,InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights,null,null
52,InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights,null,null
53,InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights,null,null
54,InitializeWeights UpdateExamples ValidationSet ComputeLearnerWeights UpdateExampleWeights,null,null
55,Variation,null,null
56,Uniform Identity OOB Absolute OOB,null,null
57,Uniform Identity OOB Median OOB,null,null
58,Uniform Identity OOB Height OOB,null,null
59,Uniform Residue OOB Constant Constant,null,null
60,Table 1: Generalized BROOF-L2R: Possible instantiations.,null,null
61,3.2 Possible Instantiations,null,null
62,"In this section, we describe a set of possible instantiations of the proposed framework. Due to space limitations, we here focus on four possible instantiations, stressing that this is far from being an exhaustive list of possibilities. In fact, we consider some representative alternatives that highlight the flexibility of the proposed framework to produce L2R solutions that typically produces very competitive results.",null,null
63,"In order to induce a L2R algorithm based on the Generalized BROOF-L2R framework, one needs to specify the 5 generic functions discussed earlier. Our proposed instantiations can be found in Table 1. In that table, we specify which alternative was chosen for each generic function, providing details on how they are implemented.",null,null
64,"As it can be observed, BROOFabsolute, BROOFmedian and BROOFheight rely on out-of-bag samples in order to drive the boosting meta-algorithm further on hard to predict regions of the input space. Such samples are explored when estimating the weak-learner's error rate through out-of-bag estimates. Recall that in boosting, the usual way of assessing the errors is to use the training to measure the error. This is too optimistic, since the same data that was used to train the model is used as a measure of error. By using the out-of-bag samples we are able to produce better error estimates, since the out-of-bag are an independent set of samples that was left apart during the construction of the model. Thus, it is able to better approximate the expected error rate of the learner and is a more reliable measure than the usual training error rate [1].",null,null
65,"In addition, the out-of-bag errors estimates are used to identify the weights' distribution that should be applied on following iterations of the boosting procedure; allowing the model to focus on hard to predict regions of the input space. We hypothesize that such selective update strategy can slowdown the algorithm's tendency to overfit. The major difference between them relates on how each weaklearner influence on the final predictor. The proposed instantiations can be found outlined in Algorithms 2 to 4. More specifically, we considered the absolute regression loss, |yi,j - y^i,j|, computed for the out-of-bag samples. We call",null,null
66,98,null,null
67,Algorithm 1 Generalized BROOF-L2R: Pseudocode,null,null
68,"1: function Fit(Qtrn ,"" {(qi, {xi,j , yi,j }|m j"",""i1}, max iter"",""M , num trees"",""N , shrinkage"",)",null,null
69,"2: wi,j ,InitializeWeights(Qtrn )",null,null
70,3:,null,null
71,"xi,j  yi,j",null,null
72,"4: for each t , 1 to M do",null,null
73,5:,null,null
74,"xi,j UpdateExamples(Qtrn , xi,j )",null,null
75,6:,null,null
76,"RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )",null,null
77,7:,null,null
78,"{(y^it,j , yit,j )} ValidationSet(RFt, Qtrn )",null,null
79,8:,null,null
80,"eti,j , t ComputeLearnerWeights(RFt, {(y^it,j , yit,j )})",null,null
81,9:,null,null
82,"if i,j eti,j wi,j  0.5 then",null,null
83,10:,null,null
84,break,null,null
85,11:,null,null
86,end if,null,null
87,12:,null,null
88,"wi,j UpdateExampleWeights(eti,j , t, {(y^it,j , yit,j )})",null,null
89,13: end for,null,null
90,"14: return {(RFt, t)}|M t,1 15: end function",null,null
91,Function,null,null
92,Description,null,null
93,InitializeWeights UpdateExamples,null,null
94,"Initial weights associated to each example, ressambling boosting by re-weighting.",null,null
95,"Uniform: Equal weights for each example, wi,j ,",null,null
96,"1 i,j mi,j",null,null
97,.,null,null
98,"Random: Randomly initialized weights, wi,j ,""Random(), 0  wi,j  1.""",null,null
99,"Determines the underlying representation of the input data, directly defining what the algorithm should",null,null
100,optimize for.,null,null
101,"Identity: Maintains the original representation of input data, xi,j ,"" xi,j .""",null,null
102,"Residue: Optimizes for the residues: xi,j ,",null,null
103,"xi,j - y^it,-j1 if t > 1 , where  is a shrinkage factor. yi,j otherwise",null,null
104,ValidationSet ComputeLearnerWeights,null,null
105,"Determines which training data will be considered during weight update and error rate estimation, with direct influence on the algorithm robustness to overfitting. OOB: The set of out of bag examples OOBt related to RFt. Train: The entire training set Qtrn .",null,null
106,Determines how to compute the influence of the current weak learner on the final predictor.,null,null
107,"Absolute: t ,""  1- , where "","" i,j eti,j wi,j , eti,j "","" |yi,j - y^i,j | and  is a shrinkage factor. Median: Similarly to the above variant, t "",  1- and ,"" i,j eti,j wi,j . However, the errors are given by eti,j "","" |Median(Ry^i,j ) - y^i,j | where Ri denotes the list of predictions y^i,j associated to documents""",null,null
108,whose real relevance score is i.,null,null
109,"Height: Similarly to the variants above, both t ,  1- and ,"" i,j eti,j wi,j . Unlike them, eti,j "",",null,null
110,"# irrelevant documents above xi,j # relevant documents below xi,j",null,null
111,"if xi,j is relevant otherwise",null,null
112,", in the ordered list of results.",null,null
113,"Constant: Produces constant coefficients, t , .",null,null
114,UpdateExampleWeights,null,null
115,Specifies how to update the training examples weights to be used in the next iteration.,null,null
116,"OOB: Updates the weights associated to the out of bag samples according to t and the difficulty involved in predicting the samples' outcomes. More specifically, wi,j ,"" wi,j t1-eti,j Train: Updates the weights associated to the entire training set. Similarly to the above variant, the update strategy considers both the coefficient t and the error eti,j .""",null,null
117,"Constant: Keeps the same weights during the boosting iterations, wi,j ,"" wi,j .""",null,null
118,"this variant BROOFabsolute. We also considered two other alternatives, that rely on the position of documents in the predicted ranked lists. One alternative, named BROOFL2Rmedian, relies on the intuition that documents with the same relevance judgment should be as nearer as possible to each other on the current ranked list. We thus consider as loss |Median(Ry^i,j ) - y^i,j | where Ri denotes the list of predictions y^i,j associated to documents whose real relevance score is i. The second alternative, named BROOFheight, is inspired on ideas of [5]. We define the height of a document xi,j as the total number of irrelevant documents ranked higher then xi,j if xi,j is relevant, or the total number of relevant documents ranked below xi,j if it is an irrelevant one.",null,null
119,"Finally, in order to illustrate the generality of our proposed framework, we provide a fourth instantiation, BROOFgradient, that resembles the gradient boosting machines (GBM), that optimizes through gradient descent [12] over the residues. More specifically, by a suitable combination of alternative implementations for each general function outlined in Algorithm 1, one can come up with an algorithm that could be named Gradient Boosted Random Forests (GBRF). This",null,null
120,"is achieved by considering an alternative representation of input data, that optimizes for the residues, such as y - y^, instead of the original input representation, updating them according to the negative gradient of the cost function (in this case, RMSE). Such alternative is outlined in Algorithm 5.",null,null
121,"As we shall see in our experimental evaluation (Section 5), our proposed instantiations achieve very strong results compared to seven state-of-the-art baselines algorithms in five representative datasets. In particular, BROOFabsolute and BROOFgradient were shown to be the strongest algorithms, obtaining significant improvements over the best baselines.",null,null
122,4. EXPERIMENTAL EVALUATION,null,null
123,"We conducted extensive experiments in well-known L2R benchmarks. In the following, we describe the characteristics of the used datasets, the baseline algorithms, the experimental protocol/setup and the experimental results.",null,null
124,4.1 Datasets,null,null
125,The corpus we use are freely available online for scientific purposes. Such datasets can be divided into two groups,null,null
126,99,null,null
127,Algorithm 2 BROOFabsolute: Pseudocode,null,null
128,"1: function Fit({(qi, {xi,j , yi,j }|m j,""i1}, M , N , )""",null,null
129,2:,null,null
130,"wi,j ,",null,null
131,"1 i,j mi,j",null,null
132,3:,null,null
133,"xi,j  yi,j",null,null
134,"4: for each t , 1 to M do",null,null
135,5:,null,null
136,"xi,j  xi,j",null,null
137,6:,null,null
138,"RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )",null,null
139,7:,null,null
140,"{(y^it,j , yit,j )}  RFt.OOB",null,null
141,8:,null,null
142,"eti,j  |yi,j - y^i,j |",null,null
143,9:,null,null
144," i,j eti,j wi,j",null,null
145,10:,null,null
146,t   1-,null,null
147,11:,null,null
148,if  0.5 break,null,null
149,12:,null,null
150,"wi,j  wi,j t1-eti,j",null,null
151,13: end for,null,null
152,"14: return {(RFt, t)}|M t,1 15: end function",null,null
153,Algorithm 3 BROOFmedian: Pseudocode,null,null
154,"1: function Fit({(qi, {xi,j , yi,j }|m j,""i1}, M , N , )""",null,null
155,2:,null,null
156,"wi,j ,",null,null
157,"1 i,j mi,j",null,null
158,3:,null,null
159,"xi,j  yi,j",null,null
160,"4: for each t , 1 to M do",null,null
161,5:,null,null
162,"xi,j  xi,j",null,null
163,6:,null,null
164,"RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )",null,null
165,7:,null,null
166,"{(y^it,j , yit,j )}  RFt.OOB",null,null
167,8:,null,null
168,"eti,j  Median(pos(yi,j ) - pos(y^i,j ))",null,null
169,9:,null,null
170," i,j eti,j wi,j",null,null
171,10:,null,null
172,t   1-,null,null
173,11:,null,null
174,if  0.5 break,null,null
175,12:,null,null
176,"wi,j  wi,j t1-eti,j",null,null
177,13: end for,null,null
178,"14: return {(RFt, t)}|M t,1 15: end function",null,null
179,"considering the relevance judgments and their sizes. The two largest datasets contain query, document pairs with five relevance levels, ranging from 0 (completely irrelevant) to 4 (highly relevant). In this group we have one dataset from the ""YAHOO! Webscope Learning to Rank Challenge"", divided into three partitions for training, validation and test. The second largest dataset, WEB10K, consists of 10, 000 queries released by Microsoft. In contrast to the YAHOO! datasets, the Microsoft dataset is partitioned into 5 folds for crossvalidation purposes, with 3 partitions used for training, 1 for validation and 1 for test.",null,null
180,"The second group of datasets corresponds to well-known LETOR 3.0 Topic distillation tasks, TD2003 and TD2004 (a.k.a., informational queries), of the Web track of the Text Retrieval Conference 2003 and 2004. These datasets contain binary relevance judgments. Similarly to the WEB10K benchmark, these datasets are partitioned into 5 folds to be used in a folded cross-validation procedure.",null,null
181,"For comparative purposes, considering that the Microsoft and LETOR datasets were designed for a folded cross-validation procedure, we applied this same strategy to the YAHOO! dataset by merging the original partitions into a single set, and splitting the sorted queries into 5 folds, distributed using the same proportions: 3 folds for training, 1 for validation and 1 for test. We report results for both splits: the original one (called YAHOOV1S2) and the new 5-fold split (called YAHOOV1S2-F5).",null,null
182,4.2 Baselines,null,null
183,In our experiments we consider as baselines freely avail-,null,null
184,Algorithm 4 BROOF-L2Rheight: Pseudocode,null,null
185,"1: function Fit({(qi, {xi,j , yi,j }|m j,""i1}, M , N , )""",null,null
186,2:,null,null
187,"wi,j ,",null,null
188,"1 i,j mi,j",null,null
189,3:,null,null
190,"xi,j  yi,j",null,null
191,"4: for each t , 1 to M do",null,null
192,5:,null,null
193,"xi,j  xi,j",null,null
194,6:,null,null
195,"RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )",null,null
196,7:,null,null
197,"{(y^it,j , yit,j )}  RFt.OOB",null,null
198,8:,null,null
199,"eti,j ",null,null
200,"# irrelevant documents above xi,j # relevant documents below xi,j",null,null
201,9:,null,null
202," i,j eti,j wi,j",null,null
203,10:,null,null
204,t   1-,null,null
205,11:,null,null
206,if  0.5 break,null,null
207,12: 13:,null,null
208,"wi,j  wi,j t1-eti,j end for",null,null
209,14: 15:,null,null
210,"return {(RFt, t)}|M t,1 end function",null,null
211,"if xi,j is relevant otherwise",null,null
212,Algorithm 5 BROOFgradient: Pseudocode,null,null
213,"1: function Fit({(qi, {xi,j , yi,j }|m j,""i1}, M , N , )""",null,null
214,2:,null,null
215,"wi,j ,",null,null
216,"1 i,j mi,j",null,null
217,3:,null,null
218,"xi,j  yi,j",null,null
219,"4: for each t , 1 to M do",null,null
220,5:,null,null
221,"xi,j ",null,null
222,"xi,j - y^it,-j1 if t > 1 yi,j otherwise",null,null
223,6:,null,null
224,"RFt  RFRegressor .Fit({(xi,j , yi,j )}, N )",null,null
225,7:,null,null
226,"{(y^it,j , yit,j )}  RFt.OOB",null,null
227,8:,null,null
228,"eti,j  |yi,j - y^i,j |",null,null
229,9:,null,null
230," i,j eti,j wi,j",null,null
231,10:,null,null
232,t  ,null,null
233,11:,null,null
234,if  0.5 break,null,null
235,12:,null,null
236,"wi,j  wi,j",null,null
237,13: end for,null,null
238,14: 15:,null,null
239,"return {(RFt, t)}|M t,1 end function",null,null
240,"able implementations of state-of-the-art L2R methods, including AdaRank (with MAP and NDCG as loss functions), Random Forests, SVMrank, MART, LambdaMART and RankBoost. We used the RankLib2 (under the Lemur project) implementations of RankBoost, MART and LambdaMART. For AdaRank we used the implementation freely available at Microsoft Research3. For SVMrank, we used the original implementation of [15]4. Finally, for Random Forests, we used the implementation available in Scikit-Learn[24] library, which is also the basis of our implementations.",null,null
241,4.3 Experimental Protocol and Setup,null,null
242,"To validate the performance of our approaches, we use two statistical tests to assess the statistical significance of our results, namely, the Wilcoxon signed-rank test and the paired Student's t-test. We consider the Wilcoxon signedrank test since it is a non-parametric statistical hypothesis testing procedure that requires no previous knowledge of the samples distribution. In fact, some authors believe that it is one of the best choices for the analysis of two independent samples [6]. However, there is also some discussion in the literature favoring the Student's t-test when comparing L2R methods [23]. Due to the lack of consensus, we perform our",null,null
243,2http://sourceforge.net/p/lemur/wiki/RankLib/ 3http://research.microsoft.com/en-us/downloads/ 0eae7224-8c9b-4f1e-b515-515c71675d5c/ 4https://www.cs.cornell.edu/people/tj/svm light/svm rank.html,null,null
244,100,null,null
245,"analysis with both tests, considering a two-sided hypothesis with significance level of 0.95% in both tests.",null,null
246,"The statistical tests are computed over the values for Mean Average Precision (MAP) and the Normalized Discounted Cumulative Gain at the top 10 retrieved documents (hereafter, NDCG@10), the two most important and frequently used performance metrics to evaluate a given permutation of a ranked list using binary and multi-relevance order [31]. To compute these metrics we used the standard evaluation tool available for the LETOR 3.0 benchmark (for binary datasets), as well the tool available for the Microsoft dataset for all multi-label relevance judgment datasets 5. For MAP, let Q be the set of all queries. These tools simply compute",null,null
247,"M AP , AveragePrecision(q) . |Q|",null,null
248,qQ,null,null
249,"Regarding NDCG, we assume that NDCG@p is 0 (zero) for empty queries, i.e., queries with no relevant documents. Some of the available evaluations tools (e.g., the one from YAHOO!) assume the value of 1 for these cases, which may lead to higher values of NDCG [2]. We chose to standardize this issue, using the same criterion used by most evaluation tools, e.g., those available for the Letor (3.0 and 4.0) and Microsoft datasets, in order to allow fairer comparisons. Accordingly, let IDCGp be the maximum possible discounted cumulative gain for a given query. These tools implement NDCG@p as follows:",null,null
250,N DCG@p,null,null
251,",",null,null
252,"DCGp , where I DC Gp",null,null
253,DC Gp,null,null
254,",",null,null
255,"p i,1",null,null
256,2reli - 1 .,null,null
257,log2(i + 1),null,null
258,"In terms of algorithm tuning, we follow the usual procedure of tuning the hyper-parameters using training and validation sets. Considering the Random Forest based approaches we vary the number of trees ranging from 10 to 1000. We achieved convergence around 300 trees, We also optimized the percentage of features to be considered as candidates during node splitting, as well as the maximum allowed number of leaf nodes. The optimal values were 0.3 and 100, respectively.",null,null
259,"For BROOFabsolute, BROOFmedian and BROOFheight, we limited the number of iterations to 500, reminding that the algorithms have an early stopping criterion that prevents further boosting iterations when the error rate exceeds 0.5. On average, our strategies converge at about 15 iterations on the LETOR datasets, and around 5 to 10 iterations on the multi-relevance judgment datasets. An exception was BROOFgradient which converged at about 100 iterations for the largest datasets.",null,null
260,"Concerning the SVMrank baseline, we favored the use of a linear kernel considering the fact that we verified in our analysis that a polynomial kernel is infeasible on large scale benchmarks such as WEB10K. The cost parameter C was calibrated using the training and validation sets with the explored values: 0.001, 0.01, 0.1, 1, 10, 100 and 1000. For the boosting methods Mart and LambdaMART, we tuned, always considering the validation set, the number of iterations ranging from one to a hundred, with a step of 1, and then scaling it up to 1000 iterations, with steps of 100. For the shrinkage factor of the predictive models, we tested the",null,null
261,"5Reminding that, at the time of the writing of this paper, the evaluation tool used in the YAHOO! competition was not available online.",null,null
262,"values of 0.025, 0.05, 0.075 and 0.1. The best found values for the MART and LambdaMART were ensembles of 1000 trees with shrinkage factor  of 0.1. For the AdaRankMAP , AdaRankNDCG@5 and for the RankBoost algorithm, similar procedures were performed in the validation set to configure the number of iterations.",null,null
263,"Finally, we performed 5, 10 and 30 runs of the 5-fold cross validation procedure for WEB10K, YAHOO! and LETOR datasets, respectively. The differences in the number of repetitions are due to the size of the datasets and the need to properly address the variance of the results. The reported results on Tables 2 and 3 are the average of all these runs, being the statistical tests applied to these results.",null,null
264,4.4 Results,null,null
265,"In this section we analyze our proposals in terms of effectiveness, comparing them to the 7 explored baseline algorithms on the 5 described datasets. The results are reported on Tables 2 and 3.",null,null
266,"We start by considering the MAP metric (Table 2). Briefly, the MAP results show that, overall, our proposed framework outperforms or ties with the strongest baselines in all cases. More specifically, with the TD2003 dataset, BROOFheight outperformed the strongest baseline (RF) considering both statistical tests, with BROOFabsolute and BROOFmedian as the winners according to at least one statistical test. In this dataset, BROOFgradient was statistically tied with the best baseline. Considering TD2004, BROOFabsolute was considered the top performer amongst the proposed solutions, being tied with the strongest baseline ­ RankBoost ­ in this dataset. Regarding the WEB10K dataset, we can see that BROOFgradient was the top performer, according to both statistical tests, being superior to MART, the strongest baseline. Finally, in the YAHOOV1S2 dataset all four proposed algorithms were statistically superior to the strongest baseline (RF) according to both statistical tests, whereas in the YAHOOV1S2-F5 dataset BROOFgradient was the best approach. In sum, according to the MAP metric, our results clearly show that the proposed instantiations of the Generalized BROOF framework produced very competitive results as the best algorithm, being superior in the majority of the cases (and tying in the others) ­ a very significant result.",null,null
267,"Turning our attention to the NDCG results, reported on Table 3, a similar behavior can be observed: our proposed instantiations are no worse than the strongest baselines in all cases, being superior in the majority of cases. Considering the TD2003 and TD2004 datasets, our solutions were no worse than any baseline, being statistically tied with the strongest one (RF, in both cases). BROOFgradient was the best algorithm in the three remaining datasets, according to both employed statistical tests. Furthermore, BROOFmedian was also superior to the best baseline (MART) in the YAHOOV1S2 dataset (according to the Student's t-test), with BROOFabsolute and BROOFheight tied with the MART algorithm. Again, this set of results highlights the effectiveness of the proposed approaches.",null,null
268,"We now turn our attention to some behavioral aspects of our algorithms, namely, convergence and learning efficiency. In order to better understand the convergence rate of our proposals, we provide an empirical evaluation of our most effective solution (i.e., BROOFgradient), by analyzing the obtained NDCG@10 as we vary the number of boosting iterations, contrasting these results with the boosting",null,null
269,101,null,null
270,Baselines,null,null
271,Algorithm,null,null
272,Mart LambdaMart RF RankBoost AdaRank-MAP Adarank-NDCG SVM-Rank,null,null
273,BROOFabsolute BROOFmedian BROOFheight BROOFgradient,null,null
274,TD2003,null,null
275,0.192633 0.165181 0.278644 0.235189 0.2003 0.121672 0.257490,null,null
276,0.288039+ 0.282427 0.285937+ 0.280634,null,null
277,TD2004,null,null
278,0.193744 0.169605 0.2522 0.255467 0.196801 0.132435 0.220392,null,null
279,0.263288 0.259941 0.259058 0.252342,null,null
280,Datasets,null,null
281,WEB10K YAHOOV1S2,null,null
282,0.352491 0.350263 0.337702 0.316201 0.294792 0.304359 0.324552,null,null
283,0.342437 0.347665 0.340340 0.36251+,null,null
284,0.559721 0.5545,null,null
285,0.563355 0.544887 0.413846 0.540243 0.544887,null,null
286,0.565486+ 0.567696+ 0.564774+ 0.572918+,null,null
287,YAHOOV1S2-5F,null,null
288,0.568821 0.563694 0.559019 0.547524 0.480190 0.538514 0.551333,null,null
289,0.563729 0.567284 0.557727 0.57656+,null,null
290,BROOF,null,null
291,"`': better than the strongest baseline, with statistical significance according to Wilcoxon Test `+': better than the strongest baseline with statistical significance according to Student's t-test `n': statistically tied results considering both tests",null,null
292,Table 2: Mean Average Precision (MAP): Obtained results.,null,null
293,Baselines,null,null
294,Algorithm,null,null
295,Mart LambdaMart RF RankBoost AdaRank-MAP AdaRank-NDCG SVM-Rank,null,null
296,BROOFabsolute BROOFmedian BROOFheight BROOFgradient,null,null
297,TD2003,null,null
298,0.271274 0.224536 0.36346 0.31613 0.271921 0.166241 0.344177,null,null
299,0.360802 0.36798 0.368195 0.368695,null,null
300,TD2004,null,null
301,0.263926 0.237338 0.350582 0.33399 0.281035 0.182031 0.303471,null,null
302,0.358146 0.350466 0.355356 0.348532,null,null
303,Datasets,null,null
304,WEB10K YAHOOV1S2,null,null
305,0.4404 0.445437 0.424498 0.397071 0.35732 0.385761 0.399902,null,null
306,0.703757 0.69619 0.703139 0.682478 0.51767 0.66309 0.682478,null,null
307,0.434964,null,null
308,0.436284,null,null
309,0.42882 0.456081+,null,null
310,0.70633,null,null
311,0.708538+ 0.70383,null,null
312,0.717271+,null,null
313,YAHOOV1S2-5F,null,null
314,0.714763 0.706287 0.702384 0.681796 0.607867 0.664115 0.691064,null,null
315,0.706954 0.709148 0.701985 0.725129+,null,null
316,BROOF,null,null
317,"`': better than the strongest baseline, with statistical significance according to Wilcoxon Test `+': better than the strongest baseline, with statistical significance according to Student's t-test `n': statistically tied results considering both tests",null,null
318,Table 3: Normalized Discounted Cumulative Gain (NDCG@10): Obtained results.,null,null
319,"baselines. We here focus on the three largest datasets: YAHOOV1S2, YAHOOV1S2-F5 and WEB10K. Results can be found on Figure 1. As it can be observed, BROOFgradient share similar behavior with three explored boosting algorithms, namely, MART, RankBoost and AdaRank-NDCG: the four algorithms show fast convergence rates. The two key differences are: (i) our approach is able to achieve significantly better results at the initial boosting iterations and (ii ) BROOFgradient converges to a higher asymptote than the other algorithms. On the other hand, the convergence rate of LambdaMART was significantly slower than the convergence rate of the mentioned algorithms. In sum, BROOFgradient enjoys faster convergence rates, with higher NDCG values at the initial boosting iterations and higher asymptote. This is paramount to guarantee practical feasibility of our solution: although high effectiveness is a requirement, achieving such high effectiveness with just a few boosting iterations is key to minimize running time.",null,null
320,"Another aspect of direct impact on the practical feasibility of the solutions is to what extent the algorithms are ""data efficient"". That is, to what extent each algorithm is capable of delivering highly effective rankings with reduced training sets. We evaluate the solutions under this dimension by analyzing each algorithm's learning curve. To this end, we measure the effectiveness of each algorithm as we vary training set size. We randomly sample s% examples from the training set, selected at random. We vary s from 10% to 100%, with steps of 10%. The obtained results can be found",null,null
321,"on Figure 2. Considering the WEB10K dataset, we can observe a surprising result: BROOFgradient is able to outperform all algorithms with just 20% of the training set, even considering the other algorithms trained with larger training sets (including the entire training set). Also, it can be noted that BROOFabsolute is no worse than the baseline algorithms, even with 10% of the training set. In fact, with about 40% of the training set BROOFgradient is able to achieve its maximum effectiveness, whereas for BROOFabsolute 10% is enough. For the YAHOO datasets, a similar behavior was observed: with about 20% to 30% of the training set our approaches were able to outperform the baseline algorithms (or match, in the case of BROOFabsolute), even considering the baseline algorithms trained with the entire training set. In these datasets, our algorithms were able to achieve maximum effectiveness at 50% to 80% of the training set. Considering the TD2003 and TD2004 datasets, the RF baseline was a bit more competitive to our approaches, exhibiting a similar behavior in terms of effectiveness as the training set size varies. In these datasets, 50% to 60% of the training set was enough to produce the best effectiveness on the TD2003, while 40% was enough to surpass all baselines on TD2004. These findings have also a direct influence on the practical feasibility of our solutions. First, smaller training sets translates to smaller runtimes. Second, obtaining labeled data is critical but also costly. Clearly, being able to produce highly effective models from reduced training sets is an important characteristic of a successful approach.",null,null
322,102,null,null
323,Figure 1: Convergence analysis: NDCG as the number of boosting iterations increases.,null,null
324,Figure 2: Learning curve analysis for the boosting algorithms.,null,null
325,"Finally, we turn our attention to the effect of the use of out-of-bag samples by our approaches. Due to space restrictions, we here focus on BROOFgradient, considering the WEB10K dataset. We analyze the effect of weak-learner error rate estimation through out-of-bag samples by contrasting it with a variant whose generic function ValidationSet equals to Train. The effectiveness of BROOFgradient and the mentioned variation, as the boosting iterations go by, can be found on Figure 3. From that figure, it is clear that the out-of-bag error estimation produces more effective results than the simple training error estimate. In fact, for all boosting iterations, the BROOFgradient variation with ValidationSet set to OOB produces better results than the results obtained with ValidationSet set to Train. This highlights the importance of exploiting the out-of-bag error estimates in our proposed framework instantiations. As a final remark, as it can be observed in Figure 3, even the variant that uses the training error rate is able to outperform the explored baselines. This is also an important aspect that highlights the quality of the proposed framework.",null,null
326,5. CONCLUSIONS AND FUTURE WORK,null,null
327,"In this work, we propose an extensible framework for L2R, called Generalized BROOF-L2R, which smoothly combines two successful strategies for Learning to Rank, namely, Ran-",null,null
328,Figure 3: BROOFgradient: Effect of out-of-bag samples versus entire training set.,null,null
329,"dom Forests and Boosting. Such combination, that uses Random Forests models as weak-learners for the boosting algorithm, relies on the use of the out of bag samples produced by the Random Forests to (i) determine the influence of each weak-learner in the final additive model and (ii) update the sample distribution weights by means of a more reliable error rate estimate. In fact, the framework is general enough to provide a rather heterogeneous set of instantiations that, according to our empirical evaluation, are able to",null,null
330,103,null,null
331,"achieve competitive results compared to state-of-the-art algorithms for L2R. We proposed four different instantiations. Three instantiations closely follows the ideas of a recently proposed algorithm for text classification, namely, BROOF. The fourth instantiation is based on gradient descent optimization, resembling gradient boosting machines. In fact, such instantiation can be seen as a gradient boosted random forests model. As our results show, despite the fact that all the four algorithms provide very competitive results, two of them are consistently the top-performers, highlighting the quality and effectiveness of our proposed framework. Also, our proposals have two properties that are paramount to guarantee their practical feasibility, namely, data efficiency and fast convergence rates.",null,null
332,"The space of possible instantiations of the proposed general framework for L2R is rather large. This clearly makes room for further investigations regarding such possibilities. In fact, one can come up with improved instantiations of the framework, by means of extending the set of possible implementations for each generic function composing the framework. This is under investigation. We also plan to study a more comprehensive set of instantiations, in order to build a substantially larger catalog of algorithms based on the Generalized BROOF-L2R to better understand the effects of each choice on model effectiveness.",null,null
333,References,null,null
334,"[1] L. Breiman. Random forests. Mach. Learn., 45(1):5­32, 2001.",null,null
335,"[2] R. Busa-Fekete, B. K´egl, T. E´lteto, and G. Szarvas. Tune and mix: learning to rank using ensembles of calibrated multi-class classifiers. Machine Learning, 93(2):261­292, 2013.",null,null
336,"[3] S. D. Canuto, F. M. Bel´em, J. M. Almeida, and M. A. Gonc¸alves. A comparative study of learning-to-rank techniques for tag recommendation. JIDM, 4(3):453­468, 2013.",null,null
337,"[4] O. Chapelle and Y. Chang. Yahoo! learning to rank challenge overview. JMLR - Proceedings Track, 14:1­24, 2011.",null,null
338,"[5] K. Christakopoulou and A. Banerjee. Collaborative ranking with a push at the top. In WWW, pages 205­215, 2015.",null,null
339,"[6] J. Demsar. Statistical comparisons of classifiers over multiple data sets. J. Mach. Learn. Res., 7:1­30, 2006.",null,null
340,"[7] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4:933­969, 2003.",null,null
341,"[8] Y. Freund and R. E. Schapire. Experiments with a New Boosting Algorithm. In ICML, pages 148­156, 1996.",null,null
342,"[9] J. H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29:1189­1232, 2000.",null,null
343,"[10] P. Geurts, D. Ernst, and L. Wehenkel. Extremely randomized trees. Mach. Learn., 63(1):3­42, 2006.",null,null
344,"[11] P. Geurts and G. Louppe. Learning to rank with extremely randomized trees. In Proc. of the Yahoo! L2R Challenge, held at ICML 2010, Haifa, Israel, June 25, 2010, volume 14 of JMLR Proceedings Track, pages 49­61, 2011.",null,null
345,"[12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of Statistical Learning. Springer, 2009.",null,null
346,"[13] R. Jin, Y. Liu, L. Si, J. Carbonell, and A. G. Hauptmann. A new boosting algorithm using input-dependent regularizer. In ICML, 2003.",null,null
347,"[14] T. Joachims. Optimizing search engines using clickthrough data. In ACM SIGKDD, pages 133­142, 2002.",null,null
348,"[15] T. Joachims. Training linear svms in linear time. In ACM SIGKDD, pages 217­226, 2006.",null,null
349,"[16] A. Karatzoglou, L. Baltrunas, and Y. Shi. Learning to rank for recommender systems. In ACM RecSys, pages 493­494, 2013.",null,null
350,"[17] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225­331, 2009.",null,null
351,"[18] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonellotto, and R. Venturini. Quickscorer: A fast algorithm to rank documents with additive ensembles of regression trees. In SIGIR, pages 73­82, 2015.",null,null
352,"[19] C. Macdonald, R. L. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.",null,null
353,"[20] L. Mason, J. Baxter, P. Bartlett, and M. Frean. Boosting algorithms as gradient descent. In In Advances in Neural Information Processing Systems, pages 512­518, 2000.",null,null
354,"[21] Y. Mishina, R. Murata, Y. Yamauchi, T. Yamashita, and H. Fujiyoshi. Boosted random forests. IEICE Transactions, 98-D(9):1630­1636, 2015.",null,null
355,"[22] A. Mohan, Z. Chen, and K. Weinberger. Web-search ranking with initialized gradient boosted regression trees. JMLR Workshop and Conference Proceedings: Proceedings of the Yahoo! Learning to Rank Challenge, 14:77­89, 2011.",null,null
356,"[23] L. a. F. Park. Confidence Intervals for Information Retrieval Evaluation. Australiasian Document Computing Symposium, 2010.",null,null
357,"[24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res., 12:2825­2830, 2011.",null,null
358,"[25] T. Salles, M. Gonc¸alves, V. Rodrigues, and L. Rocha. Broof: Exploiting out-of-bag errors, boosting and random forests for effective automated classification. In SIGIR, pages 353­362, 2015.",null,null
359,[26] R. E. Schapire and Y. Freund. Boosting: Foundations and Algorithms. 2012.,null,null
360,"[27] F. Schroff, A. Criminisi, and A. Zisserman. Object class segmentation using random forests. In British Machine Vision Conf., pages 1­10, 2008.",null,null
361,"[28] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In ACM SIGIR, pages 373­382, 2015.",null,null
362,"[29] Y. Shi, M. Larson, and A. Hanjalic. List-wise learning to rank with matrix factorization for collaborative filtering. In ACM RecSys, pages 269­272, 2010.",null,null
363,"[30] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake. Real-time human pose recognition in parts from single depth images. In CVPR, pages 1297­1304, 2011.",null,null
364,"[31] N. Tax, S. Bockting, and D. Hiemstra. A cross-benchmark comparison of 87 learning to rank. Information Processing & Management, 51(6):757­772, 2015.",null,null
365,"[32] J. Xu and H. Li. Adarank: A boosting algorithm for information retrieval. In ACM SIGIR, pages 391­398, 2007.",null,null
366,"[33] Z. E. Xu, K. Q. Weinberger, and O. Chapelle. The greedy miser: Learning under test-time budgets. In ICML, 2012.",null,null
367,104,null,null
368,,null,null

,sentence,label,data
0,That's Not My Question: Learning to Weight Unmatched Terms in CQA Vertical Search,null,null
1,Boaz Petersil,null,null
2,"Dep. of Electrical Eng. Technion, Haifa, Israel boaz.petersil@gmail.com",null,null
3,Avihai Mejer,null,null
4,"Yahoo Research Haifa, 31905, Israel amejer@yahoo-inc.com",null,null
5,Idan Szpektor,null,null
6,"Yahoo Research Haifa, 31905, Israel idan@yahoo-inc.com",null,null
7,Koby Crammer,null,null
8,"Dep. of Electrical Eng. Technion, Haifa, Israel koby@ee.technion.ac.il",null,null
9,ABSTRACT,null,null
10,"A fundamental task in Information Retrieval (IR) is term weighting. Early IR theory considered both the presence or absence of all terms in the lexicon for ranking and needed to weight them all. Yet, as the size of lexicons grew and models became too complex, common weighting models preferred to aggregate only the weights of the query terms that are matched in candidate documents. Thus, unmatched term contribution in these models is only considered indirectly, such as in probability smoothing with corpus distribution, or in weight normalization by document length. In this work we propose a novel term weighting model that directly assesses the weights of unmatched terms, and show its benefits. Specifically, we propose a Learning To Rank framework, in which features corresponding to matched terms are also ""mirrored"" in similar features that account only for unmatched terms. The relative importance of each feature is learned via a click-through query log. As a test case, we consider vertical search in Community-based Question Answering (CQA) sites from Web queries. Queries that result in viewing CQA content often contain fine grained information needs and benefit more from unmatched term weighting. We assess our model both via manual evaluation and via automatic evaluation over a clickthrough log. Our results show consistent improvement in retrieval when unmatched information is taken into account. This holds both when only identical terms are considered matched, and when related terms are matched via distributional similarity.",null,null
11,Keywords: Unmatched Terms; Document Ranking; Communitybased Question Answering,null,null
12,1. INTRODUCTION,null,null
13,"One of the fundamental tasks in Information Retrieval (IR) is term weighting, which refers to the assessment of a weight for each term appearing in the document collection, and similarly in the input query. Early Probabilistic IR theories considered the presence or absence of all terms in the lexicon for ranking, both in the query and the documents",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '16, July 17-21, 2016, Pisa, Italy",null,null
15,© 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2911496,null,null
16,"[29, 26]. However, term weights in these models were found difficult to compute, and the main line of research around weighting models chose to consider mainly the weights of the query terms that are matched in candidate documents. Indeed, weighting schemes such as TF-IDF [31], BM25 [28] and statistical language models [33, 24, 40], as well as Learning To Rank (LTR) methods [20], are primarily based on considering the contribution of the matched terms, those query terms that appear also in the document. Though unmatched terms, i.e. terms that appear only in the query or only in the document, are not completely ignored, they are considered indirectly in these models, such as by using the document length for weight normalization or via corpus-based smoothing of maximum likelihood estimations.",null,null
17,"As suggested by early probabilistic models we argue that analyzing directly unmatched terms may provide additional cues to the relevance of a candidate document to the query. Indeed, while the contribution of stop-words, such as determiners and modals, can be largely ignored, unmatched named entities are strong indicators of semantic differences between the query and the document. For example, for the query ""most deadliest snake"", the document title ""where can I find a list of the deadliest snakes"" is more relevant than ""which is the most deadliest snake in Russia"", though the first title is longer than the second, and second title contains all of the query terms in the right order.",null,null
18,"Another intuition regarding direct modeling of unmatched terms refers to the percentage of query terms that are covered in the document. We would like to explicitly indicate that for two queries, a short one and a long one, if both match the same set of terms within a candidate document, this document is likely to be of less relevance for the longer query, which contains more unmatched terms, compared to the shorter one. As an example consider the queries ""most deadliest snake"" and ""most deadliest snake in Russia"" and the candidate document ""where can I find a list of the deadliest snakes"". We would like to explicitly express the lack of relevance of the document to the second query due to unmatched query terms.",null,null
19,"We expect the subtleties between different types of unmatched terms to show especially for Web queries with finegrained information-need. Therefore, we focus in this paper on Web queries with question intent, which constitute 10% of the Web queries issued to a search engine [36]. Examples for such Web queries are those resulting in the searcher clicking on a question page belonging to Community-based Question Answering (CQA) sites, such as Yahoo Answers, StackExchange and Quora, and are called here CQA queries.",null,null
20,225,null,null
21,"Our retrieval scenario is vertical search [25, 2], in which content of a CQA sub-collection should be retrieved on top of general Web search.",null,null
22,"In this work, we address this vertical search task by introducing a term weighting model that directly considers the contribution of unmatched terms for ranking. However, instead of a probabilistic framework, we utilize LTR as a ranking framework. To this end, we employ a large set of state-of-the-art features, which capture various attributes of matched terms, both statistical ones (such as variants of TF-IDF) as well as syntactic ones (such as Part-Of-Speech (POS) tags) [20, 9]. We then design ""mirror"" features that evaluate similar attributes, but for the unmatched terms. These novel mirror features are provided, together with the features that correspond to matched terms, as input to an LTR algorithm, which learns the relative weights of the different features using click-through training data.",null,null
23,"Prior work in document ranking noted that, occasionally, different terms in the query and the document actually convey related meanings or even the same meaning (e.g. `guy' ­ `man', `drink ' ­ `alcohol ') and should be considered as matching for document ranking. One common approach to handle this lexical gap is via translation models, which include some similarity measure between query and document terms as part of term matching [4, 17, 39, 14]. In order to analyze the contribution of our unmatched term modeling under such ""soft matching"" schemes, we introduce a ""soft"" variant of all the features we compute for matched terms in our LTR framework, which is based on distributional similarity between terms. We then provide a similar soft variant of our unmatched-term features that complement the soft matched-term features. This should enable the unmatchbased features to better account for only semantically unmatched terms instead of terms that have similar meanings.",null,null
24,"We conducted experiments on a vertical search setting that searches a Web query over a large collection of question pages from Yahoo Answers. The contribution of our unmatch-based features for term weighting was evaluated under two setups: a) large-scale automatic evaluation over a click-through query log; b) manual evaluation of the top retrieved documents for a set of tested queries. We compared our model to a state-of-the-art LTR model that utilizes only features that correspond to matched terms. The tested models were assessed both under the exact matching modeling, in which only identical terms are considered matched, and the soft matching modeling, where terms may be partially matched via distributional similarity. Our novel features provided consistent improvement in document ranking on both scenarios, showing the benefit of directly considering unmatched terms for term weighting.",null,null
25,2. RELATED WORK,null,null
26,"Unmatched terms were addressed in prior IR ranking models in different ways, both for general search and for CQA search. We distinguish between two types of unmatched terms: a) terms that appear in the candidate document but not in the query, denoted as excessive terms; b) terms that appear in the query but not in the candidate document for ranking, denoted as missing terms.",null,null
27,"Probabilistic information retrieval theory accounts for presence or absence of all terms in the lexicon, both in the query and in a candidate document for ranking [29]. Similarly, early reformulation of language models for IR (LMIR) [26]",null,null
28,"considered the query as a set of words, and modeled excessive terms in the document by their ability to generate terms not in the query. However, term weighting computation becomes a difficult problem under these frameworks [30, 33, 40], especially when no relevance feedback is considered. Therefore, recent ranking models, and specifically term weighting models, focus mainly on the matched terms between the query and a candidate document.",null,null
29,"The overall ranking score of a document is typically the sum of the weights of the terms in the document that match (to some extent) the query terms. Therefore, document term weights in popular weighting schemes are non-negative and the effect of missing terms in a candidate document is considered indirectly by not contributing their weights to the document ranking score. This is the case in common probabilistic and vector space models, such as Binary Independence Model (BIM) [30], TF-IDF [31], Okapi BM25 [28], divergence from randomness [1], and multinomial language models, which view the query as a sequence of terms [33, 24, 41, 40]. Specifically, language models were extensively explored for CQA retrieval and were extended in different ways to incorporate meta data like categories [7], and the question focus and topic [13]. The same principle of scoring documents by summing matching term weights is also behind different weighting terms at the query side [3, 43].",null,null
30,"In another line of ranking research, Learning to rank (LTR) approaches [20, 19] were introduced for learning to combine many features in a supervised way. Various learning algorithms were proposed, such as SVMRank [8] and LambdaMart [38], which may assign negative weights to some features. Still, the features themselves are typically derived for the matched terms, and therefore LTR algorithms learn the relative contribution of each feature with respect to the matched terms. Most derived features are statistical in nature, such as variants of term frequency and document frequency scores [20], and were also utilized in supervised ranking models in CQA [18, 37]. Carmel et al. [9] showed that utilizing features derived from syntactic analysis of the document title improves ranking performance for CQA queries. In a related task of answer sentence ranking within the field of Question Answering, tree kernels that incorporate semantic and syntactic features of the words provide state-of-theart performance [34]. Still, the overall approach weigh in the number of matched sub-trees but not the unmatched ones.",null,null
31,"Missing terms, which appear in the query but not in the document, received considerable attention within attempts to address the lexical gap problem: improving the matching between query and document terms that are not lexically identical but convey similar meaning. One common approach incorporates a translation model as part of term weighting [4, 17]. This approach was found useful also for retrieving CQA content, where translation models were used within LMIR for retrieving related questions [16, 39, 42] as well as for ranking CQA documents for Web queries [37]. Recently, lexical semantic similarity between terms via distributed representations, such as word2vec [23], was found helpful in several IR tasks, including query term weighting [43] and as features in a LTR framework for answer retrieval [10]. Ganguly et al [14] employed similarity between word embedding vectors within a translation model for LMIR as means to overcome the lexical gap between queries and documents, where it outperformed a language model extended with latent topics.",null,null
32,226,null,null
33,Figure 2: POS tagging and dependency parse tree for the question Can someone suggest fun party games?. The upper label of each token is its POS tag and the lower label is its syntactic role.,null,null
34,"Figure 1: Sum of IDF values (normalized) for the matched, excessive and missing terms, computed separately over all non-clicked documents ranked at positions 1 to 20 (the three plots) and over the clicked documents (three horizontal lines).",null,null
35,"Prior work captured the effect of excessive terms (appearing only in the document) on the ranking score mainly by their contribution to overall document context or structure. Many vector space and probabilistic models (e.g. TF-IDF, BM25, language models) utilize the document's length as a degrading parameter for term weights, e.g. as the denominator of maximum likelihood estimation or in 2 vector normalization. Other models include all document terms when modeling a global latent representation for each document. One line of works uses latent topics (e.g. LDA [5]) as additional smoothing elements within LMIR [35]. This extension was shown useful also in retrieval of related CQA questions [6]. Another approach is to embed the document in a latent space. Latent Semantic Indexing [12] utilizes SVD to represent documents and queries within a reduced dimension space based on the main singular values of a term/document co-occurrence matrix. Lately, deep learning was shown useful for ranking by embedding the query and document texts into a shared latent space, within Web search [15] and within Question Answering [32]. Such approaches were not evaluated under the CQA vertical search setting yet, whose query length distribution and query attributes is quite distinguishable from general Web search and from question/answer datasets [9, 36].",null,null
36,3. YAHOO ANSWERS DATASET,null,null
37,"In this work we perform our analysis and experiments on a large document collection taken from Yahoo Answers. Yahoo Answers is a popular CQA website containing questions about diverse topics, such as sports, healthcare, politics, science and many others. Each question page in the site consist of: a) a title, which is typically a short summary of the question, b) a body, containing a detailed description of the question, and c) all the answers provided for this question. We collected 54 million question pages from Yahoo Answers (referred to as our corpus) and indexed them using Lucene1.",null,null
38,We also randomly sampled Web queries that were issued to a popular search engine and resulted in a click on one of the pages in our corpus by analyzing the search log. For each sampled query the top 100 results from our corpus were re-,null,null
39,1lucene.apache.org,null,null
40,"trieved using Lucene's BM25 ranking function over all fields (title, body, answers). We retained the set of queries for which the clicked page for the query (as extracted from the search log) was found among the top 100 Lucene results. After this process, our click-based query collection consists of 136,000 queries.",null,null
41,"Since search-engines show mostly the title of a question page on the search result page, Carmel et al. [9] reason that the relevance of the title to the query is one of the main reasons for a user to click on the page. This especially makes sense as the title is often a good summary of the question in the page. Furthermore, both title and query are concise and usually do not contain redundant information. Therefore, we expect that unmatched term weighting would help in retrieval under this scenario. Following them [9], we analyze and model unmatched terms only between the title of a Yahoo Answers question page and the target query.",null,null
42,4. UNMATCHED TERM ANALYSIS,null,null
43,"To further motivate our modeling approach we analyze the properties of the unmatched terms: terms that appear in the candidate document but not in the query (excessive), and terms that appear in the query but not in the candidate document (missing). To this end, we sampled 20,000 queries2 from our query collection and examined the matched and unmatched terms between each query and the titles of the retrieved documents (using Lucene). We compared the analysis statistics between documents that were clicked by the user who issued the query, denoted as clicked documents, and the other top retrieved documents, denoted as non-clicked documents.",null,null
44,4.1 IDF Distribution Analysis,null,null
45,"First we examined the distribution of IDF values among the matched and unmatched terms. To this end, we computed the sum of IDF values for the matched terms and the excessive terms in each title (normalized by the title length). We also computed the IDF sum for the query's missing terms (normalized by the query length). These three indicators are plotted in Fig. 1, where each point in the plot represents the value averaged over all non-clicked titles ranked at the i'th position by the BM25 ranking, for i,""1..20. The three horizontal lines in Fig. 1 correspond to the values of the three indicators averaged over the clicked documents. Note, higher values intuitively reflect more relevance in the matched term curve but less relevance in excessive and missing term statistics.""",null,null
46,2Taken from our training set ­ see Sec. 5.4,null,null
47,227,null,null
48,Figure 3: Probability of a term in the title not to match any query term given its POS tag (top chart) or syntactic role (bottom chart),null,null
49,"The matching-term IDF values (the diamond shape points) indicate that the clicked document (the horizontal line) is comparable, on average, only to the document at the 4 th position. This means that, with respect to matched terms, top ranked non-clicked documents usually contain as much and even more IDF volume compared to the clicked document. Yet, excessive and missing term analyses reveal complementing phenomena. First, on average, only non-clicked documents that are ranked first have the excessive term indicator lower than the value for clicked documents. This may indicate that while several non-clicked documents contain ""important"" (high-IDF) query terms in their title, driving them to high ranking positions, they also contain additional ""important"" terms that do not appear in the query and may change the meaning of the title compared to the query.",null,null
50,"Second, the indicator for missing terms in clicked documents stands out even more, as it is lower compared to all non-clicked documents. This could indicate that in CQA queries it takes more than one term to express the gist of the information needed. While some non-clicked documents may include in their title high-IDF query terms, which correspond to ""important"" terms, they also tend to leave-out more ""important"" terms compared to the clicked document.",null,null
51,"Assuming that click analysis is a useful approximation of relevance analysis, these results suggest that matched term statistics reveal only some aspects of the title's relevance to a searcher's information need. More relevance aspects may be further exposed by explicitly modeling unmatched terms.",null,null
52,4.2 Syntactic Analysis of Excessive Terms,null,null
53,"Carmel et al. [9] showed that document terms with different syntactic properties should be weighted differently for retrieval. Hence, we examine similar syntactic properties of excessive terms, namely POS tags and dependency roles.",null,null
54,"To this end, all titles in our corpus were syntactically analyzed using the Stanford parser3 under the ""all typed dependencies"" setting. Then, for each title term in a retrieved document we extracted its POS tag and syntactic role (the",null,null
55,3http://nlp.stanford.edu/software/lex-parser.shtml,null,null
56,"dependency relation in which the term is the dependent). Fig. 2 presents an example for this analysis. Finally, for each syntactic property we counted its total occurrences in each title and its occurrences within excessive terms, and computed their ratio. The ratio of these two counts represents the probability of each syntactic property to be an excessive term. In Fig. 3 we depict two probability families, averaged across all analyzed queries: a) for all clicked documents; and b) for the three highest ranking non-clicked documents (representing the ""toughest"" competitors to beat for ranking the clicked documents on top of non-clicked ones). For clarity, the charts contain only the results for the 15 most common POS tags and syntactic roles, and they are sorted in decreasing order of the probability value.",null,null
57,"Looking at Fig. 3 we observe large differences in excessive term probability between different syntactic tags. For example, in clicked items, this probability for pronouns (PRP usually a low IDF stopword) is 0.75, while for proper nouns (NNP) it is only 0.35. Such large differences echoes previous observation [9] that there is a possible gain in modeling differently terms with different syntactic tags.",null,null
58,"Comparing the statistics between clicked titles and the top non-clicked titles, we can see that in quite a few properties there are distinguishable differences between clicked and non-clicked titles. These differences suggest that excessive term weighting may improve if syntactic properties will be considered. As an example we look at verbs, which are important terms in a query (usually capturing the main activity asked about). The syntactic roles that are often associated with verbs in the bottom chart are `aux ', `cop', `root' and sometimes `conj '. These roles can be partitioned into two groups. The first group contains `aux ' and `cop', whose excessive probability is higher in non-clicked titles. The second group contains `root' and `conj ', which are more likely to be excessive in clicked titles. This shows that syntactic properties can provide more fine-grained distinctions between similar terms or even for the same term when assuming different roles.",null,null
59,"As another example, the charts in Fig. 3 also show that nouns (POS tags: NN*, Dep roles: conj, nsubj, dobj, pobj, nn) are more likely to be excessive in non-clicked documents. As nouns typically contain the main participants of a question, it is important to match all (or most) of them to align the exact semantics of the query to that of a title. Therefore, directly assessing both matched and unmatched nouns could improve retrieval. In Fig. 3 the only reverse case is with `conj ', which is more likely to be excessive in clicked titles. Yet, conjunctive nouns, such as in the example ""good websites that stream movies and tv shows"", may be skipped (and become excessive terms) while maintaining the same semantic gist of the question.",null,null
60,"The analysis in this section suggests that modeling statistical properties as well as syntactic properties of unmatched title terms may lead to better assessment of document relevance for CQA queries. We next explicitly construct features for unmatched terms, within a Learning To Rank (LTR) framework, which take these properties into consideration.",null,null
61,5. MODELING UNMATCHED TERMS,null,null
62,"The task of our ranking algorithm is to rank a set of candidate documents D given a CQA query q. We follow a standard LTR scheme [19, 20, 9] and define a mapping function",null,null
63,228,null,null
64,Feature L1 L2 L3,null,null
65,L4,null,null
66,L5,null,null
67,L6,null,null
68,L7,null,null
69,L8,null,null
70,L9,null,null
71,L10,null,null
72,G1(p) G2(p) G3(p) G4(p) G5(sr) G6(sr) H1 H2 H3,null,null
73,Formulation,null,null
74,"tq c(t, d) × (t  d)",null,null
75,"tq log(c(t, d) + 1) × (t  d)",null,null
76,tq,null,null
77,"c(t,d) |d|",null,null
78,× (t,null,null
79,d),null,null
80,tq log,null,null
81,"c(t,d) |d|",null,null
82,+1,null,null
83,× (t  d),null,null
84,tq log,null,null
85,|C| df (t),null,null
86,× (t  d),null,null
87,tq log,null,null
88,log(,null,null
89,|C| df (t),null,null
90,),null,null
91,× (t  d),null,null
92,tq log,null,null
93,"|C| c(t,C)",null,null
94,+,null,null
95,1,null,null
96,× (t  d),null,null
97,tq log,null,null
98,"c(t,d) |d|",null,null
99,log(,null,null
100,|C| df (t),null,null
101,),null,null
102,+,null,null
103,1,null,null
104,× (t  d),null,null
105,"tq c(t, d) log",null,null
106,|C| df (t),null,null
107,× (t  d),null,null
108,tq log,null,null
109,"c(t,d) |d|",null,null
110,"|C| c(t,C)",null,null
111,+,null,null
112,1,null,null
113,× (t  d),null,null
114,"tdP OS(t),""p c(t, q) tdP OS(t)"",""p idf (t) × c(t, q) tdCP OS(t)"",""p c(t, q) tdCP OS(t)"",""p idf (t) × c(t, q) tdSR(t)"",""sr c(t, q) tdSR(t)"",""sr idf (t) × c(t, q)""",null,null
115,BM25 score log(BM25 score) LMIR with Dirichlet smoothing parameter ,null,null
116,"Table 1: Matched term features used in [9]; c(t, X) ­",null,null
117,term frequency of t in X; df (t) ­ document frequency,null,null
118,"of t in our corpus C; idf (t) , log",null,null
119,|C| df (t),null,null
120,; |X| ­ total,null,null
121,"number of terms in X; P OS(t), CP OS(t) and SR(t)",null,null
122,"are the POS tag, coarse-POS tag and syntactic role",null,null
123,of t respectively; () is the indicator function.,null,null
124,"(q, d)",null,null
125,n,null,null
126,R,null,null
127,from,null,null
128,pairs,null,null
129,of,null,null
130,a,null,null
131,query,null,null
132,q,null,null
133,and,null,null
134,a,null,null
135,candidate,null,null
136,docu-,null,null
137,ment d to the vector space Rn. Our algorithm uses a weight,null,null
138,vector w to compute a score for each d  D via the inner,null,null
139,"product s(q, d) ,"" w · (q, d). Finally, the candidate docu-""",null,null
140,"ments are ranked according to the value of s(q, d), where the",null,null
141,"higher the score for some document d, the higher its rank in",null,null
142,the retrieved list. The goal of the learning algorithm is to,null,null
143,find weights w such that more relevant documents will have,null,null
144,high score compared to less relevant ones.,null,null
145,Our work focuses in the design of a new feature mapping,null,null
146,"(q, d) that captures both missing and excessive terms. We",null,null
147,build on the work of Carmel et al [9] who proposed only fea-,null,null
148,tures that consider matched terms and extend their mapping,null,null
149,in two ways: (1) taking into account unmatched terms; (2),null,null
150,"relaxing the notion of matched/unmatched terms, and al-",null,null
151,lowing soft-matching between terms in the query and the,null,null
152,"candidate, based on distributional similarity.",null,null
153,We describe in Sec. 5.1 the state-of-the-art features pro-,null,null
154,"posed by Carmel et al [9], which are our baseline and starting",null,null
155,"point. Additionally, we present in Sec. 5.2 an abstraction of",null,null
156,these features having in mind our goal to introduce their,null,null
157,"corresponding new features for unmatched terms. Finally,",null,null
158,"in Sec. 5.3, we incorporate soft-matching into all features",null,null
159,presented until then. We use a previous weight learning,null,null
160,scheme [9] (Sec. 5.4) in order to replicate their work as a,null,null
161,baseline and have a fair comparison of our new features.,null,null
162,5.1 Matched Term Features,null,null
163,"Carmel et al [9] also addressed the task of vertical search for CQA queries within an LTR framework (see Sec. 2). They proposed two types of features that analyze matched terms: standard statistical features [20], such as TF-IDF, and new syntactic-based features that collect matched term statistics for each POS tag and syntactic role separately. All these features are summarized in Tab. 1.",null,null
164,"Carmel et al mainly analyzed the performance of the document title for matching the query, arguing that in CQA content, the title is a good summary of the question being answered within the document. Therefore, all statistics are derived only from the document's title, except for the BM25-related features (H1-2) which are computed over the whole document. Under this formulation, which we follow, q and d represent the list of terms in the query and the document's title respectively. Features L1-10 and H1-3 refer to standard statistical features, while G1-4(p) and G5-6(sr) are feature families that are generated for each POS tag p and syntactic role sr, respectively. For example G1(IN ) is the feature generated for the IN (preposition) POS tag and G5(root) is the feature generated for the root syntactic role.",null,null
165,"Each of the features L1-10 and G1-6 can be viewed more abstractly as (possibly conditional) term summing of a product of two terms: (1) a count (or a function of it) of some event, denoted by fFi ; and (2) a boolean predicate or a numeric value indicating some matching between q and d:",null,null
166,"Li(q, d) ,",null,null
167,tq,null,null
168,"fLi (t, d) × (t  d)",null,null
169,(1),null,null
170,"Gi(q, d) ,",null,null
171,"fGi (t, d) × c(t, q)",null,null
172,(2),null,null
173,tdcondGi (t),null,null
174,"The matching indication part in Li is (t  d) ­ whether the query term appears in the document. The indication part of Gi is c(t, q) ­ the occurrence count of the title term in the query, which is 0 for unmatched. Examples for instantiating these abstractions with specific statistics are: a) for L2, fL2 :,"" log(c(t, d) + 1); and b) for G1, fG1 :"", 1 and condG1 :, (P OS(t) , p).",null,null
175,5.2 Unmatched Term Features,null,null
176,"We now introduce our novel features, which induce the fFi signals in parallel to their counterpart matched term features L1-10 and G1-6. Yet, instead for the matched terms, the new feature families do so for the set of excessive terms, denoted by EXL and EXG, and for the set of missing terms, denoted by M IL and M IG. We present the generic representations of these feature families similarly to (1) and (2):",null,null
177,"EXLi(q, d) ,",null,null
178,tu(d),null,null
179,"fLi (t, d) × (1 - (t  q))",null,null
180,"EXGi(q, d) ,",null,null
181,"fGi (t, d) × (1 - (t  q))",null,null
182,tdcondGi (t),null,null
183,"and,",null,null
184,"M ILi(q, d) ,",null,null
185,tu(q),null,null
186,"fLi (t, q) × (1 - (t  d))",null,null
187,"M IGi(q, d) ,",null,null
188,"fGi (t, q) × (1 - (t  d))",null,null
189,tqcondGi (t),null,null
190,229,null,null
191,Figure 4: Percentage of queries by length,null,null
192,"The differences between these new feature families and their matched term counterparts are in: a) the matching indicators, which turn into unmatching indicators; and b) the term sets over which the summation is performed. For example, the unmatching indicator in the excessive feature family EXLi is (1 - (t  q)), which is 1 only if the document term is not in the query. In addition, the summation in EXLi is over u(d), which stands for the set of unique4 terms in d from which we pick the excessive terms.",null,null
193,"We note that for M IGi, syntactic analysis of the query is required. We used the Stanford parser for query parsing as well, but found that the query dependency trees were of low quality. Therefore, in our experiments we only use the POS tags for queries, and hence only features M IG1-4, leaving the reliable generation of features M IG5-6 for future work.",null,null
194,5.3 Soft Matching Formulation,null,null
195,"As discussed in Sec. 2, some mismatches in exact matching scheme should actually be accounted as (at least partial) matches, such as in the case of synonyms or related words, e.g. `guitar ' and `riff '. We extend our proposed features and present a novel soft matching formulation of all our matched and unmatched features (except BM25-related H1-2). To the best of our knowledge, this is the first formulation in the context of the standard set of LETOR features [20].",null,null
196,"We start with a lexical similarity function sim(tq, td)  [0, 1] between a query term tq and a document term td. The closer the function's value is to 1 the more similar the two terms are. We follow recent successes with word embedding similarity and use in this work:",null,null
197,"sim(tq, td) :,"" max(cos(sg(tq), sg(dq)), 0) ,""",null,null
198,"where sg(t) is the word embedding vector of term t learned by the SkipGram algorithm [23]. We define sim(t, t) ,"" 1 for every word similarity with itself and sim(t, u) "", 0 if t , u and either t or u are not in the lexicon.",null,null
199,"To incorporate the similarity score sim(tq, td) into our features we find the best matching counterpart term for each query term and for each document term:",null,null
200,"bmd(tq) ,"" arg max sim(tq, td)""",null,null
201,td d,null,null
202,"bmq(td) ,"" arg max sim(tq, td)""",null,null
203,tq q,null,null
204,"s(t, d) ,"" sim(t, bmd(t))""",null,null
205,"s(q, t) ,"" sim(bmq(t), t) ,""",null,null
206,4Term repetition is avoided since the number of occurrences of the term t in d is already counted in fLi .,null,null
207,"where s(t, d) and s(q, t) are soft indicator functions that capture how well a query (document) term is matched against the document (query) via its similarity score with its best match. We note that if sim() would only return 1 for exact match and 0 otherwise, s() would become ().",null,null
208,"Finally, we extend our features using bm() and s():",null,null
209,"Lsi (q, d) ,",null,null
210,tq,null,null
211,"fLi (bmd(t), d) × s(t, d)",null,null
212,"Gsi (q, d) ,",null,null
213,"fGi (t, d) × c(bmq(t), q) × s(q, t)",null,null
214,tdcondGi (t),null,null
215,"EXLsi (q, d) ,",null,null
216,"fLi (t, d) × (1 - s(q, t))",null,null
217,tu(d),null,null
218,"EXGsi (q, d) ,",null,null
219,"fGi (t, d) × (1 - s(q, t))",null,null
220,tdcondGi (t),null,null
221,"M ILsi (q, d) ,",null,null
222,"fLi (t, q) × (1 - s(t, d))",null,null
223,tu(q),null,null
224,"M IGsi (q, d) ,",null,null
225,"fGi (t, q) × (1 - s(t, d)) ,",null,null
226,tqcondGi (t),null,null
227,"where we simply replace (or augment where necessary) the indicator function with the soft indicator variant, and instead of gathering statistics from exact match occurrences, we gather them from the occurrences of the best-match. If a query term appears as-is in the document (exact match), our feature scores are exactly as for exact matching. Yet, when a query term does not exactly appear in the document (or vice versa) instead of returning a matched feature value of zero, we resort to counting with respect to its best soft match instead. We note that a similar formulation using best-matches is utilized by Liu et al [21] for computing similarity between short documents.",null,null
228,5.3.1 Language Model with Soft Matching,null,null
229,"Prior work showed that extending LMIR with some similarity notion between terms improves retrieval results [39, 14]. We therefore extend our language model feature H3 in a similar way, following the formalism of Xue et al [39]:",null,null
230,"H3s(q, d) , log(P (t|d))",null,null
231,tq,null,null
232,|d|,null,null
233,"c (t, d)",null,null
234,"P (t|d) , |d|+",null,null
235,(1 - ),null,null
236,|d|,null,null
237,+ Ptt (t|d) + |d|+ Pc (t),null,null
238,"Ptt(t|d) ,",null,null
239,"sim(t, td) × c (td, d) Z(t, d) ,",null,null
240,"Z(t, d)",null,null
241,|d|,null,null
242,"sim(t, td) ,",null,null
243,td d,null,null
244,td d,null,null
245,where q is the query term list; d is the document term list,null,null
246,"and |d| is its length; c(t, d) is the term-frequency of t in d;",null,null
247,Pc(t) is maximum likelihood estimation (MLE) of t in our corpus; Z is a probability normalizer; and  and  are hyper,null,null
248,parameters to be tuned. We note that H3s is a variant of Xue et al's language model.,null,null
249,"Instead of using a translation table as Ptt, we followed Ganguly et al [14], who suggested a variant of Ptt based on a similarity function sim() (normalized into a probability distribution). Note, when sim() represents exact matching, H3s becomes H3.",null,null
250,230,null,null
251,5.4 Model Weight Learning,null,null
252,"We learned the weights w of our ranking algorithm in a semi-supervised manner based on clickthrough data. The goal of the learning algorithm is to find a vector w such that for each query in the training data, the corresponding clicked document will be ranked as high as possible. We used the online variant of SVMRank [8] with the AROW update [11] as done before [9]. Specifically, for each training query the algorithm first re-ranks the top 100 documents retrieved by Lucene using the currently learned ranker. Then, it selects the top K ranking documents, excluding the clicked document. The algorithm then updates w such as for this query the clicked document would increase its ranking score compared to the selected K documents.",null,null
253,"We split our query collection (see Sec. 3) into a 61,000 query training-set, a 14,000 query validation-set and a 61,000 query test-set. The validation-set was used to tune the various hyper-parameters for each tested model separately, namely, the number of training rounds, the value of K, and the AROW hyper-parameter r. The only hyper parameters that were tuned once for all models are  ,"" 1,  "","" 0.5 for the H3 and H3s LMIR features. Specifically,  was tuned on the LETOR model and  was then tuned on a soft version of the LETOR. See Sec. 6.1 for details on the configuration of each tested model. Finally, to compute term similarity we used publicly available5 pre-trained word embedding vectors.""",null,null
254,6. EXPERIMENTS,null,null
255,"We evaluated our proposed models against several baselines via two settings: first, based on a large scale clickthrough data, and second, based on manual judgments.",null,null
256,6.1 Tested Models,null,null
257,We consider six baseline models:,null,null
258,"· LSI: Latent Semantic Indexing [12], where ranking score is computed as the dot product between the LSI representations of the query and the document title. We utilized the top 200 dimensions of the SVD decomposition of our corpus6 as the latent space.",null,null
259,"· BIM: Binary Independence Model [22] with unmatched probability estimation using pseudo relevance, taking the 'Relevant set' as 1, 3 or 5 top ranking documents by BM25.",null,null
260,· BM25: Using only the relevance score as provided by Lucene (feature H1 in Tab. 1).,null,null
261,· LETOR: Using only statistical features associated with matched terms (features L1-10 and H1-3 in Tab. 1).,null,null
262,· Matched: Using all the features associated with matched terms (all features in Tab. 1).,null,null
263,"· SoftMatched: Using soft-matching formulation for the matched features, i.e. feature families Lsi , Gsi and feature H3s (as in Sec. 5.3), and H1-2 from Tab. 1.",null,null
264,We compare the baselines to our proposed models:,null,null
265,"· Full: Combining unmatched and matched features under exact matching formulation, i.e. all features in Tab. 1 as well as feature families EXLi, EXGi,M ILi and M IGi (described in Sec. 5.2).",null,null
266,5https://code.google.com/p/word2vec/ 6Using RedSVD: http://code.google.com/p/redsvd/,null,null
267,"· SoftFull: Combining unmatched and matched features under soft-matching formulation, i.e. feature families Lsi , Gsi , EXLsi , EXGsi , M ILsi , M IGsi and feature H3s (described in Sec. 5.3), and features H1-2 in Tab. 1.",null,null
268,We trained separately each of the LTR-based models using the algorithm in Sec. 5.4.,null,null
269,6.2 Automatic Evaluation,null,null
270,"We conducted a large scale automatic evaluation using our 61,000 query test-set (see Sec. 5.4). For each query we retrieved the top 100 results from the document collection using Lucene, and then re-ranked the top results using each of the tested models. We report Mean Reciprocal Rank (MRR) and Binary Recall at position K (R@K), all derived from the rank position of the clicked document associated with each query. Fig. 4 depicts the query length distribution of our test-set. We remind the reader that CQA queries are usually longer than typical Web queries.",null,null
271,6.3 Manual Evaluation,null,null
272,"We randomly sampled 1,000 queries of length 3 or more words from our test-set (shorter queries are scarce in our query collection - see Fig. 4). For each query we collected the top 10 documents as ranked by each of the tested models. Professional editors assessed the relevance of each document in the pool on three Likert-scale levels: (1) non-relevant, (2) partially-relevant, and (3) highly-relevant. Inspecting the evaluations, we found that usually only highly-relevant documents refer to relevant content. Hence, we report NDCG with weights of 0 for non-relevant, 1 for partially-relevant and 10 for highly-relevant. We also report Precision at K (P@K) taking only highly-relevant documents as relevant.",null,null
273,7. RESULTS,null,null
274,"The results for the automatic and manual evaluations are summarized in Tab. 2 and Tab. 3, respectively. All statistical significance figures are computed using t-test. The results in both tables indicate a trend similar to the one reported by Cramel et el [9]. Namely, LETOR outperforms BM25 by a large margin (e.g. 10.5% increase in MRR) and adding syntactic features (Gi) on top of statistical features (Li, Hi) in the Matched model consistently provides additional improvement, e.g. 1.5% increase in MRR across all query lengths (Fig. 5). We thus refer to Matched as our main baseline. We note in passing that the performance of the LSI and BIM models was significantly lower than the LTR models (e.g. MRR of 0.202 for LSI, 0.164 for BIM) and adding them as additional features did not help either. We therefore excluded their performance report.",null,null
275,"We next observe that adding soft term-matching to address the lexical gap between queries and documents (SoftMatched model) shows a nice improvement under the clickthrough automatic evaluation. For example, MRR is increased by 2.1% compared to Matched. In addition, manual evaluation also shows some improvement using soft matching, specifically at high rank positions. For example, P@3 is increased by 2.9% compared to Matched. On the other hand, the results for P@5 and P@10 are comparable to Matched. Analyzing our soft matching model, we found quite a few queries where exact matching provided better ranking than soft matching. For example, under the automatic evaluation setting, SoftMatched ranked the clicked",null,null
276,231,null,null
277,Model,null,null
278,BM25 LETOR Matched Full SoftMatched SoftFull,null,null
279,MRR,null,null
280,0.465 0.514 0.522 0.537 0.533 0.543,null,null
281,(-10.9%) (-1.5%),null,null
282,(2.9%) (2.1%) (4.0%),null,null
283,R@1,null,null
284,0.339 0.386 0.391 0.405 0.401 0.411,null,null
285,(-13.3%) (-1.3%),null,null
286,(3.6%) (2.6%) (5.1%),null,null
287,R@3,null,null
288,0.525 0.581 0.596 0.609 0.604 0.616,null,null
289,(-11.9%) (-2.5%),null,null
290,(2.2%) (1.3%) (3.4%),null,null
291,R@5,null,null
292,0.605 0.663 0.680 0.692 0.690 0.700,null,null
293,(-11.0%) (-2.5%),null,null
294,(1.8%) (1.5%) (2.9%),null,null
295,R@10,null,null
296,0.709 0.763 0.783 0.792 0.790 0.800,null,null
297,(-9.5%) (-2.6%),null,null
298,(1.1%) (0.9%) (2.2%),null,null
299,Table 2: Automatic evaluation results (and percentage of change compared to Matched model). All differences are statistically significant at p < 0.001.,null,null
300,Model,null,null
301,BM25 LETOR Matched Full SoftMatched SoftFull,null,null
302,NDCG,null,null
303,0.685 0.711 0.713 0.714,null,null
304,(-3.9%) (-0.3%),null,null
305,(0.1%),null,null
306,0.716 (0.4%),null,null
307,0.719 (0.8%),null,null
308,P@1,null,null
309,0.522 0.562 0.567 0.567,null,null
310,(-7.9%) (-0.9%),null,null
311,(0.0%),null,null
312,0.575 (1.4%),null,null
313,0.577 (1.8%),null,null
314,P@3,null,null
315,0.396 0.417 0.419 0.423 0.431 0.431,null,null
316,(-5.5%) (-0.5%),null,null
317,(1.0%) (2.9%) (2.9%),null,null
318,P@5,null,null
319,0.336 0.357 0.365 0.366,null,null
320,(-7.9%) (-2.2%),null,null
321,(0.3%),null,null
322,0.366 (0.3%),null,null
323,0.369 (1.1%),null,null
324,P@10,null,null
325,0.268 0.280 0.284 0.287,null,null
326,(-5.6%) (-1.4%),null,null
327,(1.1%),null,null
328,0.284,null,null
329,(0.0%),null,null
330,0.289 (1.8%),null,null
331,"Table 3: Manual evaluation results (and percentage of change compared to Matched model). Values marked with / indicate differences that are statistically significant at p < 0.05 and p < 0.01, respectively, compared to the Matched model.",null,null
332,query: stick a fork in it Full: what does the phrase stick a fork in it mean? Matched: is it time to stick a fork in Angels and Dodgers?,null,null
333,query: doctorate vs phd Full: what is the difference between phd and doctorate? Matched: phd in nutrition or naturopathic doctorate?,null,null
334,query: my family in french Full: how do you say my family in french Matched: describe a family member in french?,null,null
335,Table 4: Examples where Full promoted better content at the top compared to Matched,null,null
336,"document higher than Matched on 25% of the queries but that Matched ranked the clicked document higher than SoftMatched on 18.3% of the test-set. This may indicate that our current similarity function is noisy and could be improved in future work. While soft matching for retrieval was studied before, this is the first time it is applied in the CQA vertical search scenario. In addition, we are not aware of prior work that directly applies it to a large set of standard LTR features, specifically using similarity between word embedding vectors for lexical semantics (compared to the well studied translation models for this usage).",null,null
337,"We now get to our main result, which is split into two parts, corresponding to the exact matching and soft matching settings. Under the exact matching setting, when adding features that directly address unmatched terms (Full model) we see a significant improvement in performance in the automatic evaluation compared to only using matched term features (Matched model). For example, MRR is increased by 2.9%, and similar trends occur for R@K. In Fig. 5 we plot MRR vs query length from which we observe that the MRR gap is maintained across all query lengths. The gap is",null,null
338,"slightly decreasing towards longer queries, perhaps because matching many of the terms for longer queries within a question title contains enough information to indicate relevant content. Under the manual evaluation, some improvement is shown. Specifically P@3 and P@10 show an increase of  1% compared to Matched while P@1 and P@5 show comparable results. Tab. 4 shows examples where Full promoted better content at the top compared to Matched. These examples demonstrate how Full downgrades titles containing excessive information that changes the meaning of the title, such as `Angles' and `Dodgers' in the first example.",null,null
339,"Both unmatched term features (Full) and soft matched term features (SoftMatched) provide a rather similar improvement over exact matching (Matched). However, they capture different aspects of query/document ranking, one is addressing the lexical gap between the two and the other is addressing the importance of terms that were not matched from either side. Combining both model approaches together (SoftFull model) shows that they convey somewhat complementing elements for ranking. Indeed, SoftFull is the best performing model under all metrics. Under automatic evaluation the improvement is rather additive with a nice gap in performance maintained across all query lengths from both SoftMatched and Matched. Under manual evaluation the improvement is more significant. It seems that under all metrics, but P@3, the combination of soft matching with direct unmatched term assessment is more powerful than each of its parts. This result may indicate that soft matching helps pinpointing the ""true"" unmatched terms and therefore improves the modeling of their contribution to ranking.",null,null
340,7.1 Excessive vs Missing Features,null,null
341,"Our unmatched term features are composed of two types: a) those that address excessive terms, which occur only in the document (EX{L, G}i); and b) those that address missing terms, which occur only in the query (M I{L, G}i). We evaluated the contribution of each feature type independently by constructing two auxiliary models, both augmenting all matched term features (Matched). The first model adds only EX{L, G}i features, denoted MatchedAndExces-",null,null
342,232,null,null
343,Figure 5: MRR by query length,null,null
344,Query what is candys american dream,null,null
345,no virgin birth,null,null
346,92 accord misfire,null,null
347,Document Title,null,null
348,"the story ""Of Mice and Men"", how does the excerpt from Candy relate to the American Dream and Garden of Eden? No Virgin birth? Was the Virgin Mary really a virgin or was this a mistranslation of the Greek? My '92 Accord misfires and loses power at about 2500 rpm. Possible causes?",null,null
349,Table 5: Examples where Full ranked a relevant clicked document low due to many excessive terms,null,null
350,"sive, and the second model adds only the M I{L, G}i features, denoted MatchedAndMissing.",null,null
351,"Fig. 6 presents the performance of the two new models compared to Matched and to Full on the automatic evaluation setting, measured via the MRR metric. The graph shows that while missing term features are not as strong indicators for irrelevance compared to excessive term features, they still directly improve MRR compared to Matched. In addition, while excessive term features contribute more to detecting irrelevant documents, a small improvement in MRR is gained when excessive and missing features are combined in the Full model.",null,null
352,"Another observation from Fig. 6 is that MatchedAndExcessive improves over Matched for short queries much more than for long queries. One reason may be that for short queries there are more candidates in the corpus that contain the query terms, but many of them may have a lot of excessive information. As opposed to short web queries, in which the information need is generally wide, CQA queries tend to refer to very specific information needs even in such short queries, e.g. ""characteristics of enzymes"". If this hypothesis is true and our algorithm learned that a lot of excessive information indicates an irrelevant candidate, we expect the model to be particularly effective in filtering out such candidates. We note that for short queries of length 1-2 there is no improvement using missing features. This is not surprising, since there are no missing terms when matching queries of length 1, and the amount of missing information in queries of length two is at most a single term.",null,null
353,Figure 6: MRR by query length with the addition of Excessive vs. Missing Features,null,null
354,7.2 Error Analysis,null,null
355,"To better understand the performance of our features, we conducted error analysis on cases in which Full, which considers both matched and unmatched term features, ranks a clicked document significantly lower than Matched, which employs only matched term features. To this end, we considered queries from our validation set in which Matched ranked the clicked document for the query in one of the top 3 positions while Full ranked it far below. We sorted the examples by the rank margin between Matched and Full and analyzed the 100 queries with the largest rank margin.",null,null
356,"We found out that in 35 of the analyzed queries the top document ranked by Full was relevant and in 46 queries at least one of the top 3 documents was relevant. This is a known issue when using clickthrough logs as proxy to document relevance, as some unclicked documents may also be relevant to the query. Thus absolute model performance under such evaluation is biased. Yet, comparing ranking algorithms over a large-scale click-based gold labeling is useful for differentiating between their ranking performance [27].",null,null
357,"Out of the 65 queries where the top candidate by Full was not relevant, we recognized two main reasons for this ranking failure. The most prominent reason, which occurred in 36 cases, is that some terms in the document title were not matched due to the exact-matching scheme used in Full, but would have considered matched under proper soft matching. Out of these 36 case, 13 queries had spelling errors and other phenomena, such as unigram/bigram variations (e.g. `countertop' vs. `counter top'). Such lexical variations are not recognized by our current soft term matching which uses word embedding.",null,null
358,"The second phenomenon occurred in 16 out of the 65 queries. The clicked document title contains a lot of excessive terms, yet still fulfills the information need behind the query. Tab. 5 presents such examples. While our results show the potential in directly modeling unmatched terms, and specifically excessive terms as negative signals, a large number of such terms may accumulate into an unnecessary downgrading of the ranking score, and further research is required to develop more robust models.",null,null
359,233,null,null
360,8. CONCLUSIONS,null,null
361,"In this work we proposed novel features in a Learning To Rank framework that directly assess the importance of missing and excessive terms within the task of term weighting for vertical search on CQA content. To better model truly unmatched terms we also presented a ""soft matching"" variant of all our features, basing it on distributional similarity between terms, where similar terms are considered partially both matched, and unmatched. Our experiments show improvement in document retrieval in all settings when unmatched information is taken into account.",null,null
362,"In future research we plan to test whether our approach may contribute to other types of Web documents. One direction could be to explore how unmatched terms can be modeled in other parts of the document, which may require different features than the ones used in this paper.",null,null
363,9. ACKNOWLEDGEMENTS,null,null
364,The resrach was supported in part by the Yahoo Faculty Research and Engagement Program.,null,null
365,10. REFERENCES,null,null
366,"[1] G. Amati, V. Rijsbergen, and C. Joost. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4), Oct. 2002.",null,null
367,"[2] J. Arguello, F. Diaz, J. Callan, and J.-F. Crespo. Sources of evidence for vertical selection. In SIGIR, 2009.",null,null
368,"[3] M. Bendersky, D. Metzler, and W. B. Croft. Learning concept importance using a weighted dependence model. In WSDM, 2010.",null,null
369,"[4] A. Berger and J. Lafferty. Information retrieval as statistical translation. In SIGIR, 1999.",null,null
370,"[5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. JMLR, 3:993­1022, 2003.",null,null
371,"[6] L. Cai, G. Zhou, K. Liu, and J. Zhao. Learning the latent topics for question retrieval in community qa. In AFNLP, 2011.",null,null
372,"[7] X. Cao, G. Cong, B. Cui, C. S. Jensen, and C. Zhang. The use of categorization information in language models for question retrieval. In CIKM, 2009.",null,null
373,"[8] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. Adapting ranking svm to document retrieval. In SIGIR, 2006.",null,null
374,"[9] D. Carmel, A. Mejer, Y. Pinter, and I. Szpektor. Improving term weighting for community question answering search using syntactic analysis. In CIKM, 2014.",null,null
375,"[10] R.-C. Chen, D. Spina, W. B. Croft, M. Sanderson, and F. Scholer. Harnessing semantics for answer sentence retrieval. In ESAIR Workshop, 2015.",null,null
376,"[11] K. Crammer, A. Kulesza, and M. Dredze. Adaptive regularization of weight vectors. MLJ, 91(2):155­187, 2013.",null,null
377,"[12] S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. Indexing by latent semantic analysis. JAsIs, 41(6):391­407, 1990.",null,null
378,"[13] H. Duan, Y. Cao, C.-Y. Lin, and Y. Yu. Searching questions by identifying question topic and question focus. In ACL, 2008.",null,null
379,"[14] D. Ganguly, D. Roy, M. Mitra, and G. J. Jones. Word embedding based generalized language model for information retrieval. In SIGIR, 2015.",null,null
380,"[15] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In CIKM, 2013.",null,null
381,"[16] J. Jeon, W. B. Croft, and J. H. Lee. Finding similar questions in large question and answer archives. In CIKM, 2005.",null,null
382,"[17] R. Jin, A. G. Hauptmann, and C. X. Zhai. Language model for information retrieval. In SIGIR, 2002.",null,null
383,"[18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, Y. Maarek, D. Pelleg, and I. Szpektor. Predicting web searcher satisfaction with existing community-based answers. In SIGIR, 2011.",null,null
384,"[19] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends Inf. Retr., 3(3):225­331, Mar. 2009.",null,null
385,"[20] T. y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR Workshop on Learning to Rank for Information Retrieval, 2007.",null,null
386,"[21] Y. Liu, C. Sun, L. Lin, Y. Zhao, and X. Wang. Computing semantic text similarity using rich features. In PACLIC, 2015.",null,null
387,"[22] C. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.",null,null
388,"[23] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In NIPS. 2013.",null,null
389,"[24] D. R. Miller, T. Leek, and R. M. Schwartz. A hidden markov model information retrieval system. In SIGIR, 1999.",null,null
390,"[25] V. Murdock and M. Lalmas. Workshop on aggregated search. SIGIR Forum, 42(2):80­83, Nov. 2008.",null,null
391,"[26] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, 1998.",null,null
392,"[27] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In CIKM, 2008.",null,null
393,"[28] S. Robertson and H. Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333­389, Apr. 2009.",null,null
394,"[29] S. E. Robertson and K. S. Jones. Relevance weighting of search terms. Journal of the American Society for Information science, 27(3):129­146, 1976.",null,null
395,"[30] S. E. Robertson, C. J. van Rijsbergen, and M. F. Porter. Probabilistic models of indexing and searching. In SIGIR, 1980.",null,null
396,"[31] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513­523, Aug. 1988.",null,null
397,"[32] A. Severyn and A. Moschitti. Learning to rank short text pairs with convolutional deep neural networks. In SIGIR, 2015.",null,null
398,"[33] F. Song and W. B. Croft. A general language model for information retrieval. In CIKM, 1999.",null,null
399,"[34] K. Tymoshenko and A. Moschitti. Assessing the impact of syntactic and semantic structures for answer passages reranking. In CIKM, 2015.",null,null
400,"[35] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In SIGIR, 2006.",null,null
401,"[36] R. W. White, M. Richardson, and W.-t. Yih. Questions vs. queries in informational search tasks. In WWW Companion, 2015.",null,null
402,"[37] H. Wu, W. Wu, M. Zhou, E. Chen, L. Duan, and H.-Y. Shum. Improving search relevance for short queries in community question answering. In WSDM, 2014.",null,null
403,"[38] Q. Wu, C. J. Burges, K. M. Svore, and J. Gao. Adapting boosting for information retrieval measures. Inf. Retr., 13(3):254­270, June 2010.",null,null
404,"[39] X. Xue, J. Jeon, and W. B. Croft. Retrieval models for question and answer archives. In SIGIR, 2008.",null,null
405,"[40] C. Zhai. Statistical language models for information retrieval. Synthesis Lectures on HLT, 1(1):1­141, 2008.",null,null
406,"[41] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR, 2001.",null,null
407,"[42] W. Zhang, Z. Ming, Y. Zhang, L. Nie, T. Liu, and T. Chua. The use of dependency relation graph to enhance the term weighting in question retrieval. In COLING, 2012.",null,null
408,"[43] G. Zheng and J. Callan. Learning to reweight terms with distributed representations. In SIGIR, 2015.",null,null
409,234,null,null
410,,null,null

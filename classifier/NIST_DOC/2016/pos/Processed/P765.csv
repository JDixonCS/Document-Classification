,sentence,label,data
0,Axiomatic Analysis for Improving the Log-Logistic Feedback Model,null,null
1,"Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery",null,null
2,"School of Electrical and Computer Engineering, College of Engineering, University of Tehran, Iran",null,null
3,"Center for Intelligent Information Retrieval, College of Information and Computer Sciences,",null,null
4,"University of Massachusetts Amherst, MA 01003",null,null
5,"{ali.montazer,shakery}@ut.ac.ir",null,null
6,zamani@cs.umass.edu,null,null
7,ABSTRACT,null,null
8,"Pseudo-relevance feedback (PRF) has been proven to be an effective query expansion strategy to improve retrieval performance. Several PRF methods have so far been proposed for many retrieval models. Recent theoretical studies of PRF methods show that most of the PRF methods do not satisfy all necessary constraints. Among all, the log-logistic model has been shown to be an effective method that satisfies most of the PRF constraints. In this paper, we first introduce two new PRF constraints. We further analyze the log-logistic feedback model and show that it does not satisfy these two constraints as well as the previously proposed ""relevance effect"" constraint. We then modify the log-logistic formulation to satisfy all these constraints. Experiments on three TREC newswire and web collections demonstrate that the proposed modification significantly outperforms the original log-logistic model, in all collections.",null,null
9,CCS Concepts,null,null
10,·Information systems  Query representation; Query reformulation;,null,null
11,Keywords,null,null
12,Pseudo-relevance feedback; axiomatic analysis; theoretical analysis; query expansion; semantic similarity,null,null
13,1. INTRODUCTION,null,null
14,"Pseudo-relevance feedback (PRF) refers to a query expansion strategy to address the vocabulary mismatch problem in information retrieval (IR). PRF assumes that a number of top-retrieved documents are relevant to the initial query. Based on this assumption, it updates the query model using these pseudo-relevant documents to improve the retrieval performance. PRF has been shown to be highly effective in many retrieval models [1, 5, 8, 10].",null,null
15,Several PRF models with different assumptions and formulations have so far been proposed. Clinchant and Gaussi-,null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",null,null
17,"SIGIR '16, July 17-21, 2016, Pisa, Italy",null,null
18,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2911451.2914768,null,null
19,"er [2] theoretically analyzed a number of effective PRF models. To this end, they proposed five constraints (axioms) for PRF models and showed that the log-logistic feedback model [1] is the only PRF model (among the studied ones) that satisfies all the constraints. They also showed that its performance is superior to the other PRF methods, including the mixture model [10] and the geometric relevance model [9]. Effectiveness of the log-logistic model motivates us, in this paper, to study this state-of-the-art PRF model.",null,null
20,"Recently, Pal et al. [8] proposed a sixth constraint for PRF models to improve the PRF performance in the divergence from randomness framework. This constraint, which is called ""relevance effect"", indicates that the terms in the feedback documents with high relevance scores (i.e., relevance of document to the initial query) should have higher weights in the feedback model compared to those with exactly similar statistics, but appear in the documents with lower relevance scores. Formally writing, if a term w occurs in two documents d1, d2  F (F denotes the set of feedback documents) such that d1 is more relevant to the initial query than d2. Then, we can say that the feedback weight of w given the F - {d1} feedback documents is lower than the weight of the same word in the F - {d2} feedback documents [8]. It can be shown that the log-logistic feedback model does not satisfy the relevance effect constraint.",null,null
21,"In this paper, we propose two additional constraints for PRF models. The first constraint considers the semantic similarity of feedback terms to the initial query. Although previous work, such as [4], proposed similar constraints for retrieval models, to the best of our knowledge, it is the first time to study a semantic-related constraint for the PRF task. The second constraint indicates that the weight of each term w in the feedback model not only depends on the distribution of w in the feedback documents, but is also related to the distribution of the other terms in those documents. We further show that the log-logistic model does not satisfy the two proposed constraints. We then propose a modification to the log-logistic feedback model to satisfy the proposed constraints as well as the relevance effect constraint [8].",null,null
22,"We evaluate the modified log-logistic model using three standard TREC collections: AP (Associated Press 1988-89), Robust (TREC 2004 Robust track), and WT10g (TREC 910 Web track). The experimental results demonstrate that the proposed method significantly outperforms the original log-logistic feedback model in all collections. The proposed method is also shown to be more robust than the original log-logistic model, especially in the web collection.",null,null
23,765,null,null
24,2. METHODOLOGY,null,null
25,"In this section, we introduce two constraints that (pseudo) relevance feedback methods should satisfy (in addition to those proposed in [2, 8]). We further analyze the log-logistic model, a state-of-the-art feedback model, and figure out that this model does not satisfy the proposed constraints as well as the ""relevance effect"" constraint introduced in [8]. Based on these observations, we modify the log-logistic feedback model to satisfy all the constraints.",null,null
26,"We first introduce our notation. Let F W (w; F, Pw, Q) be the feedback weight function that assigns a real-value weight to each feedback term w for a given query Q. F and Pw respectively denote the set of feedback documents for the query Q and a set of term-dependent parameters. For simplicity, we henceforth use F W (w). In the following equations, T F and IDF denote term frequency and inverse document frequency, respectively. The notation | · | is also used for query/document length or size of a given set.",null,null
27,2.1 Constraints,null,null
28,"In this subsection, we introduce two constraints for feedback models.",null,null
29,"[Semantic effect] Let Q be a single-term query (i.e., Q ,"" {q}), w1 and w2 be two terms such that IDF (w1) "","" IDF (w2), D  F : T F (w1, D) "","" T F (w2, D), and""",null,null
30,"sem(q, w1) < sem(q, w2)",null,null
31,"where sem(·, ·) denotes the semantic similarity of the given terms. Then, we can say:",null,null
32,F W (w1) < F W (w2),null,null
33,The intuition behind this constraint is that the feedback terms should be semantically similar to the initial query.,null,null
34,"[Distribution effect] Let w1 and w2 be two vocabulary terms such that T F (w1, D1) ,"" T F (w2, D2), T F (w1, D2) "","" T F (w2, D1) "","" 0, and |D1| "","" |D2|, where D1 and D2 are two documents in the feedback set F . Also, assume that w1 and w2 do not occur in other feedback documents, and""",null,null
35,U niqueT erms(D1) < U niqueT erms(D2),null,null
36,"where U niqueT erms(·) denotes the number of unique terms in the given document. Then, we can say:1",null,null
37,F W (w1) < F W (w2),null,null
38,"In other words, this constraint implies that for computing the feedback weight of a term w, the distribution of other terms in the feedback documents should also be considered.",null,null
39,2.2 Modifying the Log-Logistic Model,null,null
40,The feedback weight of each term w in the log-logistic feedback model [1] is computed as follows:,null,null
41,F W (w),null,null
42,",",null,null
43,1 |F |,null,null
44,DF,null,null
45,"F W (w, D)",null,null
46,",",null,null
47,1 |F |,null,null
48,DF,null,null
49,log(,null,null
50,"t(w,",null,null
51,D) w,null,null
52,+,null,null
53,w,null,null
54,),null,null
55,(1),null,null
56,where w,null,null
57,",",null,null
58,Nw N,null,null
59,(Nw,null,null
60,and N,null,null
61,denote the number of docu-,null,null
62,ments in the collection that contain w and the total number,null,null
63,"of documents in the collection, respectively), and t(w, D) ,",null,null
64,"T F (w,",null,null
65,D),null,null
66,log(1,null,null
67,+,null,null
68,c,null,null
69,avgl |D|,null,null
70,),null,null
71,(avgl,null,null
72,denotes,null,null
73,the,null,null
74,average,null,null
75,document,null,null
76,length and c is a free hyper-parameter). It is shown that,null,null
77,1The intuition behind this constraint comes from the definition of information in information theory literature.,null,null
78,"the log-logistic model satisfies all the PRF constraints introduced in [2]. It can be easily proved that this model cannot satisfy the constraints proposed in this paper. In more detail, there is no semantic-related or relevance-related components in the log-logistic formulation and thus it cannot satisfy the proposed ""semantic effect"" and the ""relevance effect"" [8] constraints. In addition, the log-logistic formula does not consider the distribution of other terms in computing the weight of each term w, and thus it does not satisfy the ""distribution effect"" constraint.",null,null
79,"To satisfy the ""semantic effect"" constraint, we modify the log-logistic feedback weight function as follows:",null,null
80,F Wsem(w),null,null
81,",",null,null
82,F W (w),null,null
83,1 |Q|,null,null
84,"s(w, q) s(q, q)",null,null
85,(2),null,null
86,qQ,null,null
87,"where s(·, ·) denotes the semantic similarity between the given two terms. The parameter  controls the effect of semantic similarity in the feedback weight function. The semantic weighting component comes from the query-growth function, which was previously proposed by Fang and Zhai [4]. Note that in Equation (2), we can ignore the 1/|Q| term and the  parameter, since they are equal for all terms and the feedback weighting function will be normalized. Several methods have so far been proposed to incorporate semantic similarity of terms in various retrieval tasks. In this paper, we consider the mutual information as a basic semantic similarity metric to compute s(·, ·). The mutual information (MI) of two terms w and w is computed as follows:",null,null
88,"I (Xw ,",null,null
89,Xw ),null,null
90,",",null,null
91,"Xw ,Xw {0,1}",null,null
92,"p(Xw ,",null,null
93,Xw ),null,null
94,log,null,null
95,"p(Xw, Xw ) p(Xw)p(Xw )",null,null
96,"where Xw and Xw are two binary random variables that represent the presence or absence of the terms w and w in each document. A simple way to compute the mutual information is to consider the whole collection; but, this choice may not be ideal for ambiguous terms. Another way is to compute the mutual information from the pseudo-relevant documents. However, the top-retrieved documents could be a biased corpus for this goal. Therefore, similar to [4], we extract the mutual information from a corpus containing the top m retrieved documents and r × m documents randomly selected from the collection, where r is a free parameter that controls the generality of mutual information scores.",null,null
97,"To satisfy the ""distribution effect"" constraint, we re-define the function t(w, D) as follows:",null,null
98,"t(w, D)",null,null
99,",",null,null
100,"t(w, D)",null,null
101,log(,null,null
102,|D| ut(D),null,null
103,),null,null
104,(3),null,null
105,where ut(D) denotes the number of unique terms in the document D. A similar approach for modifying the raw TF formula was previously used in [7].,null,null
106,"To satisfy the ""relevance effect"" constraint, we re-define the function F W (w, D) (see Equation (1)) as follows:",null,null
107,"F W (w, D) ,"" F W (w, D)  RS(Q, D)""",null,null
108,(4),null,null
109,"where RS(Q, D) denotes the relevance score of the document D to the query Q. This function can be computed using the relevance score of D in the first ranking phase in PRF. A similar idea was previously proposed by Lavrenko and Croft [5]. They used the query likelihood similarity as a posterior probability in the relevance models. Lv and",null,null
110,766,null,null
111,ID AP Robust,null,null
112,WT10g,null,null
113,Collection,null,null
114,Associated Press 88-89 TREC Disks 4 & 5 minus,null,null
115,Congressional Record,null,null
116,TREC Web Collection,null,null
117,Table 1: Collections statistics.,null,null
118,"Queries (title only) TREC 1-3 Ad Hoc Track, topics 51-200",null,null
119,"TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 9-10 Web Track, topics 451-550",null,null
120,#docs 165k 528k,null,null
121,1692k,null,null
122,doc length 287 254,null,null
123,399,null,null
124,"#qrels 15,838 17,412",null,null
125,5931,null,null
126,Table 2: Performance of the proposed modifications and the baselines. Superscripts 0/1 denote that the MAP improvements over NoPRF/LL are statistically significant. The highest value in each column is marked in bold.,null,null
127,Method,null,null
128,AP MAP P@10 RI,null,null
129,Robust MAP P@10 RI,null,null
130,WT10g MAP P@10 RI,null,null
131,NoPRF LL,null,null
132,LL+Sem LL+Rel LL+Dis,null,null
133,0.2642,null,null
134,0.3385,null,null
135,0.34220 0.34250 0.33860,null,null
136,0.4260 0.4622,null,null
137,0.4702 0.4681 0.4671,null,null
138,­ 0.15,null,null
139,0.18 0.20 0.16,null,null
140,0.2490,null,null
141,0.2829,null,null
142,0.294001 0.289701 0.28310,null,null
143,0.4237 ­ 0.4393 0.33,null,null
144,0.4474 0.31 0.4490 0.35 0.4401 0.32,null,null
145,0.2080 0.2127,null,null
146,0.2247 0.228901 0.2194,null,null
147,0.3030 ­ 0.3187 0.08,null,null
148,0.3188 0.10 0.3289 0.17 0.3207 0.13,null,null
149,LL+All 0.344501 0.4722 0.20 0.297901 0.4486 0.36 0.230001 0.3177 0.19,null,null
150,Zhai [6] also used a similar technique to improve the divergence minimization feedback model [10].,null,null
151,"Considering the aforementioned modifications, we can rewrite the log-logistic feedback weighting formula as follows:",null,null
152,F W (w),null,null
153,",",null,null
154,1 |F |,null,null
155,DF,null,null
156,log(,null,null
157,"t(w,",null,null
158,D) w,null,null
159,+,null,null
160,w,null,null
161,),null,null
162,RS,null,null
163,"(Q,",null,null
164,D),null,null
165,"s(w, q) s(q, q)",null,null
166,(5),null,null
167,qQ,null,null
168,3. EXPERIMENTS,null,null
169,3.1 Experimental Setup,null,null
170,"We used three standard TREC collections in our experiments: AP (Associated Press 1988-89), Robust (TREC Robust Track 2004 collection), and WT10g (TREC Web Track 2001-2002). The first two collections are newswire collections, and the third collection is a web collection with more noisy documents. The statistics of these datasets are reported in Table 1. We consider the title of topics as queries. All documents are stemmed using the Porter stemmer. Stopwords are removed in all the experiments. We used the standard INQUERY stopword list. All experiments were carried out using the Lemur toolkit2.",null,null
171,3.1.1 Parameter Setting,null,null
172,"The number of feedback documents, the number of feedback terms, the feedback coefficient and the parameter that controls the generally of mutual information scores (parameter r) are set using 2-fold cross validation over each collection. We sweep the number of feedback documents and feedback terms between {10, 25, 50, 75, 100}, the feedback coefficient between {0, 0.1, · · · , 1}, and the parameter r between {2, 4, 6, 8, 10}.",null,null
173,3.1.2 Evaluation Metrics,null,null
174,"To evaluate retrieval effectiveness, we use mean average precision (MAP) of the top-ranked 1000 documents as the",null,null
175,2http://lemurproject.org/,null,null
176,"main evaluation metric. In addition, we also report the pre-",null,null
177,cision of the top 10 retrieved documents (P@10). Statisti-,null,null
178,cally significant differences of performance are determined,null,null
179,using the two-tailed paired t-test computed at a 95% confi-,null,null
180,dence level over average precision per query.,null,null
181,"To evaluate the robustness of methods, we consider the ro-",null,null
182,bustness,null,null
183,index,null,null
184,(RI),null,null
185,[3],null,null
186,which,null,null
187,is,null,null
188,defined,null,null
189,as,null,null
190,N+ -N- N,null,null
191,",",null,null
192,where,null,null
193,N,null,null
194,denotes the number of queries and N+/N- shows the num-,null,null
195,ber of queries improved/decreased by the feedback method.3,null,null
196,"The RI value is always in the [-1, 1] interval and the method",null,null
197,with higher value is more robust.,null,null
198,3.2 Results and Discussion,null,null
199,"In this subsection, we first evaluate the proposed modifications to the log-logistic model. We further study the sensitivity of the proposed method to the free parameters.",null,null
200,3.2.1 Evaluating the Modified Log-Logistic Model,null,null
201,"We consider two baselines: (1) the document retrieval method without feedback (NoPRF), and (2) the original loglogistic feedback model (LL). Although several other PRF methods have already been proposed, since in this paper, we propose a modification of the log-logistic model, we do not compare the proposed method with other existing PRF models.",null,null
202,"To study the effect of each constraint in the retrieval performance, we modify the log-logistic model based on each constraint, separately. LL+Sem, LL+Rel, and LL+Dis denote the modified log-logistic model based on the ""semantic effect"", the ""relevance effect"", and the ""distribution effect"" constraints, respectively. We also modify the log-logistic model by considering all of these constraints (called LL+All) as introduced in Equation (5). The results obtained by the baselines and those achieved by the proposed modifications are reported in Table 2. According to this table, LL outperforms the NoPRF baseline in all cases, which shows the effectiveness of the log-logistic model. The improvements on the WT10g collection is lower than those on the AP and the Robust collections. This observation demonstrates that the",null,null
203,"3To avoid the influence of very small average precision changes in the RI value, we only consider the improvements/losses higher than 10% (relatively).",null,null
204,767,null,null
205,0.32  Robust WT10g,null,null
206,0.32  Robust WT10g,null,null
207,0.30,null,null
208,0.30 ,null,null
209,0.28,null,null
210,0.28,null,null
211,MAP MAP,null,null
212,0.26,null,null
213,0.26,null,null
214,0.24,null,null
215,0.24,null,null
216,0.22,null,null
217,0.22,null,null
218,25 50 75 100 125 150 175 200,null,null
219,2,null,null
220,4,null,null
221,6,null,null
222,8,null,null
223,# of feedback terms,null,null
224,r,null,null
225,Figure 1: Sensitivity of the proposed method to the number of feedback terms and the parameter r.,null,null
226,log-logistic model is less effective and robust in improving,null,null
227,"the retrieval performance in the web collection, compared",null,null
228,to the newswire collections. LL+Sem and LL+Rel perform,null,null
229,"better than LL in terms of MAP and P@10, in all collec-",null,null
230,tions. The MAP improvements are statistically significant,null,null
231,"in many cases, especially in the LL+Rel method. Except",null,null
232,"in one case (i.e., LL+Sem in Robust), both LL+Sem and",null,null
233,LL+Rel models are shown to be more robust than the LL,null,null
234,baseline. It is worth noting that we use very simple modifi-,null,null
235,"cations to satisfy these two constraints, and thus using more",null,null
236,accurate methods to satisfy these constraints can potentially,null,null
237,improve the performance. LL+Dis method in general per-,null,null
238,forms comparable to or sometimes slightly better than the,null,null
239,LL baseline. The results achieved on the WT10g collection,null,null
240,"shows that LL+Dis can be more effective in noisy conditions,",null,null
241,"such as web collections. Overall, although the theoretical",null,null
242,"analysis shows that PRF methods should satisfy the ""distri-",null,null
243,"bution effect"" constraint, it does not substantially affect the",null,null
244,retrieval performance in the AP and the Robust collections.,null,null
245,The,null,null
246,reason,null,null
247,is,null,null
248,that,null,null
249,the,null,null
250,values,null,null
251,of,null,null
252,|D| ut(D),null,null
253,(see,null,null
254,Equation,null,null
255,(3)),null,null
256,are,null,null
257,"very close to each other for different documents, especially",null,null
258,"in newswire collections. Thus, our modification to the log-",null,null
259,"logistic regarding the ""distribution effect"" constraint cannot",null,null
260,substantially affect the retrieval performance.,null,null
261,"As shown in Table 2, the LL+All method, which is our fi-",null,null
262,"nal modification to the log-logistic model, outperforms both",null,null
263,baselines in all collections in terms of MAP and P@10. The,null,null
264,MAP improvements are always statistically significant. The,null,null
265,LL+All method is also shown to be more robust than the,null,null
266,"LL method, in particular in the WT10g collection.",null,null
267,3.2.2 Parameter Sensitivity,null,null
268,"In this set of experiments, we fix one of the parameters r (the generality control parameter for mutual information) and n (the number of feedback terms), and then sweep the other one to show the sensitivity of the method to the input parameters. The results are reported in Figure 1.4 According to this figure, the method is quite stable w.r.t. the changes in the values of these two parameters, especially for the parameter r. The results also indicate that by increasing the number of feedback terms, performance in the Robust collection generally increases, but in the WT10g collection it is not the case. The reason could be related to the noisy nature of this collection compared to the newswire collections.",null,null
269,"4For the sake of visualization, we only report the results for the Robust and the WT10g collections. The behaviour of the method in AP is similar to the Robust collection.",null,null
270,4. CONCLUSIONS AND FUTURE WORK,null,null
271,"In this paper, we proposed two new constraints for pseudorelevance feedback models. The first constraint considers semantic similarity of the feedback terms to the initial query. The second constraint focuses on the effect of distribution of all terms in the feedback documents on each term. We further studied the log-logistic model, a state-of-the-art feedback model, and showed that this model does not satisfy the proposed constraints as well as the previously proposed ""relevance effect"" constraint [8]. We then modified the loglogistic model to satisfy all of these constraints. The proposed modification was evaluated using three TREC newswire and web collections. Experimental results suggest that the modified model significantly outperforms the original log-logistic model, in all collections.",null,null
272,"An interesting future direction is to study other feedback methods, such as the language model-based feedback methods, and modify them in order to satisfy the constraints. In this paper, we only consider simple approaches to satisfy the constraints, such as using mutual information for capturing semantic similarities. Future work can focus on more complex and accurate approaches to improve the retrieval performance.",null,null
273,5. ACKNOWLEDGEMENTS,null,null
274,"This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",null,null
275,6. REFERENCES,null,null
276,"[1] S. Clinchant and E. Gaussier. Information-based Models for Ad Hoc IR. In SIGIR '10, pages 234­241, 2010.",null,null
277,"[2] S. Clinchant and E. Gaussier. A Theoretical Analysis of Pseudo-Relevance Feedback Models. In ICTIR '13, pages 6­13, 2013.",null,null
278,"[3] K. Collins-Thompson. Reducing the Risk of Query Expansion via Robust Constrained Optimization. In CIKM '09, pages 837­846, 2009.",null,null
279,"[4] H. Fang and C. Zhai. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In SIGIR '06, pages 115­122, 2006.",null,null
280,"[5] V. Lavrenko and W. B. Croft. Relevance Based Language Models. In SIGIR '01, pages 120­127, 2001.",null,null
281,"[6] Y. Lv and C. Zhai. Revisiting the Divergence Minimization Feedback Model. In CIKM '14, pages 1863­1866, 2014.",null,null
282,"[7] J. H. Paik. A Novel TF-IDF Weighting Scheme for Effective Ranking. In SIGIR '13, pages 343­352, 2013.",null,null
283,"[8] D. Pal, M. Mitra, and S. Bhattacharya. Improving Pseudo Relevance Feedback in the Divergence from Randomness Model. In ICTIR '15, pages 325­328, 2015.",null,null
284,"[9] J. Seo and W. B. Croft. Geometric Representations for Multiple Documents. In SIGIR '10, pages 251­258, 2010.",null,null
285,"[10] C. Zhai and J. Lafferty. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01, pages 403­410, 2001.",null,null
286,768,null,null
287,,null,null

,sentence,label,data
0,Risk-Sensitive Evaluation and Learning to Rank using Multiple Baselines,null,null
1,"B. Taner Dinçer1, Craig Macdonald2, Iadh Ounis2",null,null
2,"1 Sitki Kocman University of Mugla, Mugla, Turkey 2 University of Glasgow, Glasgow, UK",null,null
3,"dtaner@mu.edu.tr1,{craig.macdonald, iadh.ounis}@glasgow.ac.uk2",null,null
4,ABSTRACT,null,null
5,"A robust retrieval system ensures that user experience is not damaged by the presence of poorly-performing queries. Such robustness can be measured by risk-sensitive evaluation measures, which assess the extent to which a system performs worse than a given baseline system. However, using a particular, single system as the baseline suffers from the fact that retrieval performance highly varies among IR systems across topics. Thus, a single system would in general fail in providing enough information about the real baseline performance for every topic under consideration, and hence it would in general fail in measuring the real risk associated with any given system. Based upon the Chi-squared statistic, we propose a new measure ZRisk that exhibits more promise since it takes into account multiple baselines when measuring risk, and a derivative measure called GeoRisk, which enhances ZRisk by also taking into account the overall magnitude of effectiveness. This paper demonstrates the benefits of ZRisk and GeoRisk upon TREC data, and how to exploit GeoRisk for risk-sensitive learning to rank, thereby making use of multiple baselines within the learning objective function to obtain effective yet risk-averse/robust ranking systems. Experiments using 10,000 topics from the MSLR learning to rank dataset demonstrate the efficacy of the proposed Chi-square statistic-based objective function.",null,null
6,1. INTRODUCTION,null,null
7,"The classical evaluation of information retrieval (IR) systems has focused upon the arithmetic mean of their effectiveness upon a sample of queries. However, this does not address the robustness of the system, i.e. its effectiveness upon the worst performing queries. For example, while some retrieval techniques (e.g. query expansion [2, 8]) perform effectively for some queries, they can orthogonally cause a decrease in effectiveness for other queries. To address this, various research into robust and risk-sensitive measures has taken place. For instance, in the TREC Robust track, systems were measured by geometric mean average precision [23, 25] to determine the extent to which they perform well on all queries. More recently, the notion of risksensitivity has been introduced, in that an evaluation mea-",null,null
8,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.",null,null
9,"SIGIR '16, July 17­21, 2016, Pisa, Italy.",null,null
10,c 2016 ACM. ISBN 978-1-4503-4069-4/16/07. . . $15.00,null,null
11,DOI: http://dx.doi.org/10.1145/2911451.2911511,null,null
12,"sure should consider per-query losses and gains compared to a particular baseline technique [11]. Within this framework, measures such as URisk [27] and TRisk [13] have been proposed. Both measures can be adapted to integrate with the state-of-the-art LambdaMART learning to rank technique.",null,null
13,"Since risk-sensitive measures compare to a specific baseline, such measures are most naturally applied in experiments using a before-and-after design, where different treatments are applied to a particular baseline system, e.g. query expansion. However, when simply considering a single baseline, a full knowledge of the difficulty of a particular query cannot be obtained. For instance, a single baseline system may perform lowly for a query that other systems typically perform well. For this reason, the inference of risk based upon a population of baseline systems is attractive. One can easily draw an analogy with the building of ranking methods that combine multiple weighting models, such as data fusion or learning to rank, to obtain a more effective final ranking. Moreover, the use of multiple baselines permits a deployed search engine to evaluate the risk of an alternative retrieval approach not only with respect to its own baseline, but also to other competitor systems.",null,null
14,"In this paper, we show how a risk-sensitive evaluation based on the Chi-square test statistic permits the consideration of multiple baselines, unlike the existing measures URisk & TRisk which can only consider a single baseline. In doing so, we argue that a robust system should not be less effective for a given topic than an expectation of performance given a population of other (baseline) systems upon that topic. In particular, this paper contributes: a new risksensitive evaluation measure, namely ZRisk, based on Chisquare test statistic, and a derivative called GeoRisk that enhances ZRisk by also taking into account the overall magnitude of effectiveness; Moreover, we demonstrate the use of ZRisk and GeoRisk upon a TREC comparative evaluation of Web retrieval systems; Finally, we show how to directly and effectively integrate GeoRisk within the state-of-the-art LambdaMART learning to rank technique.",null,null
15,"This paper is organised as follows: Section 2 provides a background on robust and risk-sensitive evaluation; Section 3 defines ZRisk based upon Chi-squared statistic, as well as the GeoRisk derivative; Section 4 & Section 5 demonstrate the proposed measures upon synthetic & real TREC data, while Section 6 shows the integration of GeoRisk within the LambdaMART learning to rank technique; Related work and concluding remarks follow in Sections 7 & 8.",null,null
16,2. RISK-SENSITIVE EVALUATION,null,null
17,Risk-sensitive evaluation [11] aims at quantifying the tradeoff between risk and reward for any given retrieval strat-,null,null
18,483,null,null
19,"egy. Information retrieval performance, which is usually measured by a given retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [9]) over a set of topics Q, can be expressed in terms of risk and reward as a risk function. Such a risk function takes into account the downside-risk of a new system s with respect to a given baseline system b (i.e. a loss: performing a topic q worse than the baseline according to the effectiveness measure, sq < bq) and an orthogonal reward function that takes into account the upside-risk (i.e. a win: performing a topic better than the baseline, sq > bq).",null,null
20,"A single measure, URisk [27], which allows the tradeoff between risk and reward to be adjusted, is defined as:",null,null
21,1,null,null
22,URisk,null,null
23,",",null,null
24, c,null,null
25,q + (1 + ),null,null
26,"q , (1)",null,null
27,qQ+,null,null
28,qQ-,null,null
29,"where c , |Q| and q ,"" sq - bq. The left summand in the square brackets, which is the sum of the score differences q for all q where sq > bq (i.e. q  Q+), gives the total win (or upside-risk) with respect to the baseline. On the other hand, the right summand, which is the sum of the score differences q for all q where sq < bq, gives the total loss (or downsiderisk). The risk sensitivity parameter   0 controls the tradeoff between reward and risk (or win and loss):  "","" 0 calculates the average change in effectiveness between s and b, while for higher , the penalty for under-performing with respect to the baseline is increased: typically  "","" 1, 5, 10 [12] to penalise risky systems, where  "", 1 doubles the emphasis of down-side risk compared to  , 0.",null,null
30,"Recently, Din¸cer et al. [13] introduced a statistically-grounded risk-reward tradeoff measure, TRisk, as a generalisation of URisk, for the purposes of hypothesis testing:",null,null
31,TRisk,null,null
32,",",null,null
33,"URisk , S E (URisk )",null,null
34,(2),null,null
35,"where SE(URisk) is the standard error in the risk-reward tradeoff score URisk. Here, TRisk is a linear monotonic transformation of URisk. This transformation is called studentisation in statistics (c.f., t-scores) [16], and TRisk can be used as the test statistic of the Student's t-test. Moreover, the aforementioned work shows that TRisk permits a state-of-the-art learning to rank algorithm (LambdaMART) to focus on those topics that lead to a significant level of risk in order to learn effective yet risk-averse ranking systems.",null,null
36,"On the other hand, the comparative risk-sensitive evaluation of different IR systems is challenging, as the systems may be based upon a variety of different (base) retrieval models ­ such as learning to rank or language models ­ or upon different IR platforms (Indri, Terrier etc.). It has been shown that using a particular system as the baseline in a comparative risk-sensitive evaluation of a set of diverse IR systems ­ as attempted by the TREC 2013 and 2014 Web track ­ yields biased risk-reward tradeoff measurements [14], especially when the systems under evaluation are not variations of the provided baseline system. To address this, the use of the within-topic mean system performance was proposed as an unbiased baseline (as well as the within-topic median system performance and the within-topic maximum system performance). Given a particular topic q and a set of r systems, the arithmetic mean of the r performance scores according to an evaluation measure observed on q is the unbiased baseline score:",null,null
37,1r,null,null
38,"Meanq ,"" r si(q),""",null,null
39,(3),null,null
40,"i,1",null,null
41,"where si(q) is the performance score of system i on topic q measured by a given evaluation measure (e.g. ERR@20) for i ,"" 1, 2, . . . , r. Since the arithmetic mean gives equal weight to every retrieval strategy in determining the within-topic mean system performance, a baseline system that is determined by the Meanq scores will be unbiased with respect to the retrieval strategies yielding the r system scores.""",null,null
42,"However, as shown in [14], the use of Meanq exposes a problem about the validity of the comparative risk-sensitive evaluation of different IR systems. This issue is related to the risk-based rankings of the systems obtained using Meanq. Indeed, such a comparison of the risk-sensitive performances of different IR systems actually implies the comparison of the retrieval effectiveness of the individual systems based on the underlying effectiveness measure, i.e. ERR@20 [14]. That is, the ranking of the systems obtained by using the underlying effectiveness measure will be the same as the risk-based ranking of the systems obtained using the unbiased baseline Meanq, irrespective of the value of the risk sensitivity parameter .",null,null
43,"Most importantly, the previously proposed risk measures are only sensitive to the mean and the variance of the observed losses and wins, i.e. URisk is sensitive to mean and TRisk is sensitive to mean and variance (c.f. SE(URisk)). However in a comparative risk-sensitive evaluation, we argue that it is necessary to be sensitive to the shape of the score distributions, as well as the mean and the variance. As such, in the next section, we propose the ZRisk measure, which satisfies the aforementioned variance and shape requirements of a comparative risk-sensitive IR evaluation, while the derivative GeoRisk measure enhances ZRisk by naturally incorporating the overall effectiveness of the considered system.",null,null
44,3. MEASURES OF RISK FROM CHI-SQUARE,null,null
45,"Each existing robust and risk-sensitive evaluation measure each encodes properties about what a good (or bad) IR system should exhibit. Firstly, the classical mean measure (e.g. MAP or mean NDCG) stipulates that a good system should perform well on a population of topics on average; The geometric mean (e.g. as proposed in [24] for Mean Average Precision as GMAP) says that a good system should avoid performing lowly on any topics, while comparing GMAP values permits identifying improvements in low performing topics, in contrast to mean, which gives equal weight to absolute changes in per-topic scores, regardless of the relative size of the change [4]. Risk-sensitive evaluation measures such as URisk and TRisk use the notion of a baseline - a good system should perform well, but preferably no worse than the given baseline. Hence URisk responds to changes in the mean effectiveness of the system, but emphasises those worse than the baseline. Building upon URisk, TRisk is also sensitive to the variance exhibited by a system across the population of topics. These attributes are highlighted in Table 1.",null,null
46,"In this section, we argue for a risk measure that considers the `shape' of a system's performance across topics. In particular, we consider that the distribution of the effectiveness scores of a set of baseline systems across the topics, mapped to the same overall mean effectiveness as the system at hand, represents an expected performance for each topic that the system should not underperform. In other words, we calculate the expectation of the system's performance for each topic, by considering the overall performance of the current system and the observed performances of other baseline systems. This allows to determine topics that the system should be performing better on. It follows that our proposal encap-",null,null
47,484,null,null
48,Measure,null,null
49,Mean AP Geo. MAP,null,null
50,URisk TRisk ZRisk GeoRisk,null,null
51,Baseline,null,null
52,None None,null,null
53,Single Single Multiple Multiple,null,null
54,Penalty of low topics None Focus on lowest topics 1+ 1+ 1+ 1+,null,null
55,Sensitive to:,null,null
56,Mean Var. Shape,null,null
57,Table 1: Comparison of existing and proposed robustness/risk-sensitive measures.,null,null
58,Topics,null,null
59,Systems t1 t2 t3 . . . tc Total,null,null
60,s1,null,null
61,x11 x12 x13 . . . x1c S1,null,null
62,s2,null,null
63,x21 x22 x23 . . . x2c S2,null,null
64,"X,",null,null
65,s3,null,null
66,x31 x32 x33 . . . x3c S3,null,null
67,...,null,null
68,...,null,null
69,...,null,null
70,...,null,null
71,...,null,null
72,...,null,null
73,...,null,null
74,sr,null,null
75,xr1 xr2 xr3 . . . xrc Sr,null,null
76,Total T1 T2 T3 . . . Tc N,null,null
77,Table 2: Data matrix for an IR experiment.,null,null
78,"sulates two separate measures: ZRisk, introduced in Section 3.1, which measures the shape of the system's performance irrespective of the overall magnitude of effectiveness; and later in Section 3.2 we show how to create a risk-measure responsive to mean effectiveness called GeoRisk. The subsequent Section 4 & Section 5 demonstrate the ZRisk and GeoRisk measures upon artificial and TREC data.",null,null
79,"The first measure, ZRisk, is inspired by the Chi-square statistic used in the Chi-square test for goodness-of-fit, which is one of the well-established nonparametric hypothesis tests in categorical data analysis [1]. In statistics, goodness-of-fit tests are used to decide whether two distributions are significantly different from each other in shape/form. In relation to risk-sensitive evaluation, this means that, given a sample of topics, a risk measure based on Chi-square statistic permits quantifying the difference in the performance profiles of two IR systems across the topics. As mentioned above, none of the previously proposed risk measures are sensitive to the score distributions of IR systems on topics. However, risksensitive evaluation, by nature, should take into account all of shape, mean and variance, while ZRisk is independent of overall mean effectiveness. Hence, building upon ZRisk, we propose the GeoRisk measure, which covers all of the aforementioned aspects including the overall mean effectiveness of the system at hand, as highlighted in Table 1.",null,null
80,3.1 The Chi-square Statistics & ZRisk,null,null
81,ZRisk is best explained by deriving it directly from the,null,null
82,Chi-square statistic used in the Chi-square test for goodness-,null,null
83,"of-fit. In particular, the Chi-square statistic is calculated",null,null
84,"over a data matrix of r × c cells, called the contingency ta-",null,null
85,ble. The result of an IR experiment involving r systems,null,null
86,"and c topics can be represented by a r × c data matrix X,",null,null
87,whose rows and columns correspond respectively to the r,null,null
88,"systems and c topics, where the cells xij (for i ,"" 1, 2, . . . , r""",null,null
89,"and j ,"" 1, 2, . . . , c) contain the observed performances of""",null,null
90,"the corresponding systems for the associated topics, mea-",null,null
91,sured by an effectiveness measure such as ERR@20. Table 2,null,null
92,provides a graphical portrayal of data matrix X.,null,null
93,"For such a data matrix, the row and the column marginal",null,null
94,"totals are given by Si ,",null,null
95,"c j,1",null,null
96,xij,null,null
97,and,null,null
98,Tj,null,null
99,",",null,null
100,"r i,1",null,null
101,xij,null,null
102,respec-,null,null
103,"tively, and the grand total is given by N ,",null,null
104,"r i,1",null,null
105,"c j,1",null,null
106,xij,null,null
107,.,null,null
108,The average effectiveness of a system i over c topics is given,null,null
109,"by Si/c and similarly, the within-topic mean system effec-",null,null
110,"tiveness is given by Tj/r. Given a data matrix X, the Chi-square statistic, G2, can",null,null
111,be expressed as,null,null
112,r,null,null
113,"G2 ,",null,null
114,"c (xij - eij )2 ,",null,null
115,(4),null,null
116,"i,1 j,1",null,null
117,eij,null,null
118,"where the expected value for cell (i, j), eij, is given by",null,null
119,eij,null,null
120,",",null,null
121,Si × Tj N,null,null
122,", Si ×",null,null
123,Tj N,null,null
124,", Si × pj .",null,null
125,(5),null,null
126,"In Equation (5), pj",null,null
127,",",null,null
128,Tj N,null,null
129,can be described as the density,null,null
130,"or mass of column j, for j ,"" 1, 2, . . . , c. If a row total Si is""",null,null
131,distributed on columns proportional to the column masses,null,null
132,"pj, then the Chi-squared differences of the associated cell",null,null
133,values from the corresponding expected values will sum up to,null,null
134,"zero, i.e.",null,null
135,"c j,1",null,null
136,(xij,null,null
137,-,null,null
138,eij,null,null
139,)2,null,null
140,",",null,null
141,0.,null,null
142,Note that,null,null
143,eij,null,null
144,",",null,null
145,xij,null,null
146,when,null,null
147,r,null,null
148,",",null,null
149,"1, where pj , xij/Si since N ,"" Si. Intuitively, when there""",null,null
150,"is only one IR system, the expected system performance for",null,null
151,any topic j will be equal to the score observed for that sys-,null,null
152,"tem. When r ,"" 1, G2 "","" 0, meaning that the observed score""",null,null
153,distribution of the system across topics is perfectly fit to it-,null,null
154,"self. Thus, G2 values that are greater than zero indicate a",null,null
155,"discordance between two distributions, above or below ex-",null,null
156,pectations. This makes G2 not directly applicable as a risk-,null,null
157,"sensitive evaluation measure, since it equally and uniformly",null,null
158,penalises both downside (losses) and upside risk (wins). In,null,null
159,"contrast, risk-sensitive measures should favour wins and or-",null,null
160,"thogonally penalise losses. Hence, we propose below a mea-",null,null
161,sure derived from G2 that addresses this limitation.,null,null
162,"For large samples, the Pearson's Chi-square statistic G2 in",null,null
163,Eq. (4) follows a Chi-square distribution with (r - 1)(c - 1),null,null
164,degrees of freedom and the observed cell values xij follow a,null,null
165,Poisson distribution with mean eij and variance eij [1]. This,null,null
166,means that the Chi-square statistic can also be expressed as,null,null
167,the sum of the square of standard normal deviates [1]:,null,null
168,rc,null,null
169,"G2 ,",null,null
170,zi2j,null,null
171,"i,1 j,1",null,null
172,where,null,null
173,"zij , xij -eijeij .",null,null
174,"The square root of the components of Chi-square statistic, zij, gives the standardised deviation in cell (i, j) from the expected value eij (i.e. z-scores). Thus, for large samples, the distribution of zij values on the population can be approximated by the standard normal distribution with zero mean and unit variance.",null,null
175,"It follows that a risk-reward tradeoff measure can be expressed in terms of the standard normal deviates from the expected effectiveness, as given by:",null,null
176,"ZRisk , ",null,null
177,ziq + (1 + ),null,null
178,"ziq ,",null,null
179,(6),null,null
180,qQ+,null,null
181,qQ-,null,null
182,"for any system i, i ,"" 1, 2, . . . , r. Q+ (Q-) is the set of queries where ziq > 0 (ziq < 0, respectively), determined by whether system i outperforms its expectation on topic j (c.f. xij - eij ).""",null,null
183,"ZRisk takes the classical form of a risk-sensitive evaluation measure, in that upside risk is rewarded and the effectiveness penalty of downside risk is amplified by  - i.e. the higher the ZRisk, the more safe and less risky a system is. In addition, ZRisk calculates the risk of a system in relation to the shape of effectiveness across topics exhibited by multiple baselines. In this way, ZRisk brings a new dimension to the measurement of robustness, originally defined by",null,null
184,485,null,null
185,"Voorhees [24] as ""the ability of the system to return reasonable results for every topic"", in that for ZRisk, robustness is measured compared to a per-topic expectation calculated from a population of baseline systems.",null,null
186,3.2 GeoRisk,null,null
187,"As noted before, a limitation of ZRisk is that it measures robustness irrespective of the mean effectiveness of IR systems. Indeed, one may consider that the baseline for any given system i is composed of the expected per-topic scores of the system, eij, such that the sum of expected per-topic scores is equal to the sum of the observed per-topic scores of the system, i.e. j eij ,"" Si. This means that ZRisk measures robustness using individual baselines for every system, each of which is derived on the basis of the observed total effectiveness of the system (i.e. Si) and the observed topic masses (i.e. Tj). This makes the robustness/risk measurements of ZRisk independent of the observed mean effectiveness of the systems, i.e. j xij "", j eij for i ,"" 1, 2, . . . , r.""",null,null
188,"On the other hand, for the purposes of the comparative risk-sensitive evaluation of different IR systems, we combine the risk measure with the effectiveness measure in use, ZRisk and ERR@20 for example, into a final measure. A natural method for such a combination is the geometric mean, which is expressed as the nth root of the product of n numbers. The geometric mean is a type of average, like arithmetic mean, that represents the central tendency in a given set of numbers. In contrast to the arithmetic mean, the geometric mean normalises the ranges of the variables, so that each datum has an equal impact on the resulting geometric mean. Hence, the geometric mean of the ERR@20 scores and the ZRisk scores represents, evenly, both the effectiveness and the robustness of system si under evaluation:",null,null
189,"GeoRisk (si) ,"" Si/c × (ZRisk/c),""",null,null
190,(7),null,null
191,"where 0  ()  1 is the cumulative distribution function of the standard normal distribution. In this way, we use () to normalise ZRisk into [0,1], because -  ZRisk/c   .",null,null
192,4. DEMONSTRATION,null,null
193,"To illustrate ZRisk and GeoRisk introduced in Section 3, Table 3 presents an example data matrix X composed of 8 systems and 5 topics. The effectiveness scores of the example systems are artificially determined so that the resulting performance profiles of the systems across the topics serve as a basis to exemplify some potential differences in performance profiles of IR systems in relation to their mean effectiveness. Figure 1 shows the performance profiles of the 8 systems, which can be characterised as follows:",null,null
194,"· Systems s1 and s2 have the same mean effectiveness over the 5 topics (i.e. 0.3000) but the scores of s1 are monotonically increasing in magnitude across the topics, whereas, the scores of s2 are monotonically decreasing. That is, s1 and s2 have contrasting performance profiles across the topics, with respect to the same mean effectiveness score of 0.3000.",null,null
195,"· Systems s3 and s4 have constant scores across the topics that are equal to their respective mean effectiveness scores. In other words, these systems have constant performance profiles, while system s3 has the same mean effectiveness as both s1 and s2.",null,null
196,"· Systems s5 and s6 again have the same mean effectiveness as systems s1 and s2, but have alternating scores across the topics, such that one has a higher score in magnitude",null,null
197,s1 s2 s3 s4 s5 s6 s7 s8 Tj Mean,null,null
198,t1 0.0500 0.4000 0.3000 0.2500 0.4000 0.2000 0.2542 0.2918 2.1460 0.2683,null,null
199,t2 0.1500 0.3500 0.3000 0.2500 0.1500 0.4500 0.2629 0.2994 2.2123 0.2765,null,null
200,t3 0.3000 0.3000 0.3000 0.2500 0.4000 0.2000 0.2802 0.3147 2.3449 0.2931,null,null
201,t4 0.4500 0.2500 0.3000 0.2500 0.1500 0.4500 0.2975 0.3301 2.4776 0.3097,null,null
202,t5 0.5500 0.2000 0.3000 0.2500 0.4000 0.2000 0.3061 0.3378 2.5440 0.3180,null,null
203,Si 1.5000 1.5000 1.5000 1.2500 1.5000 1.5000 1.4009 1.5738 11.7248,null,null
204,Mean 0.3000 0.3000 0.3000 0.2500 0.3000 0.3000 0.2802 0.3148,null,null
205,Table 3: Example data matrix X.,null,null
206,Score,null,null
207,0.5,null,null
208,0.4,null,null
209,S2,null,null
210,S5,null,null
211,0.3,null,null
212,S3 S8,null,null
213,S7,null,null
214,0.2,null,null
215,S6,null,null
216,S4,null,null
217,0.1 S1,null,null
218,01,null,null
219,s1 s2 s3 s4 s5 s6 s7 s8,null,null
220,2,null,null
221,3,null,null
222,4,null,null
223,5,null,null
224,Topics,null,null
225,Figure 1: Example systems' performance profiles.,null,null
226,than the other for one topic and vice versa for the next topic. We describe such systems as having alternating performance profiles across the topics.,null,null
227,"· Systems s7 and s8 have different mean effectiveness scores from each other and also from that of the other systems. Their performance profiles are visually parallel to each other, and concordant with the profile of the mean topic scores, i.e. the row ""Mean"" of Table 3.",null,null
228,4.1 Single Baseline,null,null
229,"Measuring the level of risk associated with a given IR system s with respect to a particular single baseline system b means that, in total, there are two systems under consideration, i.e. r ,"" 2. For such a risk-sensitive evaluation, the Chi-square statistic G2 is given by""",null,null
230,c,null,null
231,"G2 ,",null,null
232,"(xsj - esj )2 + (xbj - ebj )2 ,",null,null
233,"j,1",null,null
234,esj,null,null
235,ebj,null,null
236,"and, under the null hypothesis that the observed score distri-",null,null
237,"butions of both systems follow a common distribution with mean µ and variance 2, it can be expressed as",null,null
238,"c (xsj - xbj )2 ,",null,null
239,(8),null,null
240,"j,1 xsj + xbj",null,null
241,"where xsj is the observed score of the system s for topic j, and xbj is the observed score of the baseline system b. Note that, when there are only two systems, Tj ,"" xsj + xbj, and hence xbj "", Tj - xsj and xsj ,"" Tj - xbj . Here,""",null,null
242,esj,null,null
243,",",null,null
244,Ss × Tj N,null,null
245,",",null,null
246,Ss N,null,null
247,× (xsj,null,null
248,+ xbj ),null,null
249,ebj,null,null
250,",",null,null
251,Sb × Tj N,null,null
252,",",null,null
253,N - Ss N,null,null
254,× (xsj + xbj ),null,null
255,"where N ,"" Ss + Sb. In fact, given two IR systems, the level of risk associated""",null,null
256,"with any one of the two systems can be measured by taking the other system as the baseline, as implied by Eq. (8). Most importantly, Eq. (8) suggests, in this respect, that, if the",null,null
257,486,null,null
258,",0",null,null
259,s2 vs. s1 s1 vs. s2 s3 vs. s1 s1 vs. s3 s4 vs. s1 s1 vs. s4 s5 vs. s1 s1 vs. s5 s6 vs. s1 s1 vs. s6 s7 vs. s1 s1 vs. s7 s8 vs. s1 s1 vs. s8,null,null
260,t1 0.3689 -0.3689 0.2988 -0.2988 0.3077 -0.2809 0.3689 -0.3689 0.2121 -0.2121 0.2799 -0.2705 0.2792 -0.2860,null,null
261,t2 0.2000 -0.2000 0.1581 -0.1581 0.1599 -0.1460 0.0000 0.0000 0.2739 -0.2739 0.1422 -0.1374 0.1445 -0.1480,null,null
262,t3 0.0000 0.0000 0.0000 0.0000 0.0000 0.0000 0.0845 -0.0845 -0.1000 0.1000 0.0000 0.0000 -0.0001 0.0001,null,null
263,t4 -0.1690 0.1690 -0.1225 0.1225 -0.1209 0.1103 -0.2739 0.2739 0.0000 0.0000 -0.1057 0.1021 -0.1097 0.1123,null,null
264,t5 -0.2858 0.2858 -0.1917 0.1917 -0.1884 0.1720 -0.1088 0.1088 -0.2858 0.2858 -0.1669 0.1613 -0.1732 0.1774,null,null
265,ZRisk 0.1141 -0.1141 0.1427 -0.1427 0.1583 -0.1445 0.0708 -0.0708 0.1002 -0.1002 0.1496 -0.1446 0.1408 -0.1442,null,null
266,Table 4: Single baseline example.,null,null
267,"systems show an equal mean performance over a given set of c topics (i.e. Ss ,"" Sb), the measured level of risk will be the same for both systems. In risk-sensitive evaluations, a baseline system defines what is a robust system, so that risk can be quantified as the degree of divergence from that baseline. However, given a set of IR systems, taking every system as a baseline, actually contributes information for the qualification of a robust (i.e. not `risky' or safe) system on the population of topics. In this regard, multiple baselines can provide more information about the real level of risk associated with any IR system.""",null,null
268,"Let system s1 in Table 3 be the baseline system. The level of risk associated with system s2, which has the same mean performance with s1, is ZRisk ,"" 0.1141, while the level of risk associated with s1 for baseline s2 is the same in magnitude but different in sign, i.e. -0.1141. The sign of the ZRisk scores indicates the direction of the observed level of riskreward tradeoff, where minus indicates down-side risk and plus indicates up-side risk. Table 4 shows the calculated values of ZRisk at  "", 0 for each system i ,"" 2, 3, . . . , 8. As can be seen, for those systems whose mean performances are equal to the mean performance of s1, only the sign of the calculated ZRisk values changes when the baseline is swapped.""",null,null
269,"Based on the calculated ZRisk values when the baseline is s1, system s4 is the least `risky' system among the 8 example runs with the highest ZRisk value of 0.1583 (i.e. the s4 vs. s1 row of the table). However, as can be seen in Figure 1, s3, s7, or s8 are relatively less `risky' than s4. That is, those three systems have performance profiles that are concordant/parallel with that of s1 and also they have relatively higher mean effectiveness scores than s4: thus, s4 could not be considered less ""risky"" than s3, s7, or s8. The reason behind this counter-intuitive result is two-fold. Firstly, baseline system s1 has performance scores that are monotonically increasing in magnitude across the topics. Thus, as a baseline, it suggests that the expected system performance on the population of topics that is represented by the sample topic t1 would be low, and for the population of topics represented by t2 it would be relatively higher than that of t1, and so on. However, as seen from Figure 1, considering the observed scores of the other systems, it would appear that the expected per-topic system performances are in general different from those that the system s1 suggests, i.e. the `mean' row of Table 3. Secondly, the risk that is measured by ZRisk is related to the distribution of the total system performance Si on topics with respect to the expected per-topic system performances, and is not dependent on the magnitude of the mean performance of the systems across topics.",null,null
270,"These two issues explain the above counter-intuitive result that s4 is declared as the least `risky' system. Indeed, the former issue can be resolved by employing multiple baselines",null,null
271,",0",null,null
272,",1",null,null
273,",5",null,null
274," , 10",null,null
275,Mean ZRisk Geo ZRisk Geo ZRisk Geo ZRisk Geo,null,null
276,s1 0.300 -0.049 0.386 -0.727 0.364 -3.442 0.271 -6.835 0.160,null,null
277,s2 0.300 0.026 0.388 -0.312 0.378 -1.668 0.333 -3.362 0.274,null,null
278,s3 0.300 0.006 0.387 -0.069 0.385 -0.368 0.376 -0.742 0.364,null,null
279,s4 0.250 0.005 0.354 -0.063 0.352 -0.336 0.344 -0.677 0.334,null,null
280,s5 0.300 0.006 0.387 -0.541 0.370 -2.727 0.296 -5.460 0.203,null,null
281,s6 0.300 0.005 0.387 -0.539 0.370 -2.718 0.297 -5.442 0.204,null,null
282,s7 0.280 -0.001 0.374 -0.008 0.374 -0.036 0.373 -0.072 0.372,null,null
283,s8 0.315 0.001 0.397 -0.010 0.396 -0.052 0.395 -0.106 0.393,null,null
284,Table 5: ZRisk and GeoRisk for the example systems.,null,null
285,"as shown in the following Section 4.2, and the latter issue of independence from the magnitude of mean effectiveness can be resolved as shown in Section 4.3, where the risk measure ZRisk and the measure of effectiveness are combined into a single measure of effectiveness, GeoRisk.",null,null
286,4.2 Multiple Baselines,null,null
287,"Chi-square statistic allows the use of all systems in data matrix X as multiple baselines for risk-reward tradeoff measurements using ZRisk. Recall that the expected value for cell (i, j), eij, is given by",null,null
288,eij,null,null
289,", Si ×",null,null
290,Tj N,null,null
291,", Si × pj .",null,null
292,"For the case of a single baseline system b, given a particular system s, to calculate the mass or density pj of topic j, the within topic total performance score Tj is taken as xsj + xbj, i.e. pj ,"" (xsj + xbj)/N . Similarly, given a set of baselines, the topic masses can be calculated as""",null,null
293,1r,null,null
294,"pj , N",null,null
295,"xij ,",null,null
296,"i,1",null,null
297,"for each topic j ,"" 1, 2, . . . , c. Intuitively this means that, given a set of r systems, the level of risk associated with every system is measured by taking the remaining (r - 1) systems as the baseline. Compared to the case of taking a particular system as the baseline, as the number of baseline systems increases, the accuracy of the estimates of expected system performance for each topic increases, and hence the accuracy of the estimates of real risk increases.""",null,null
298,"Table 5 shows the calculated ZRisk values for each of the 8 example runs at  ,"" 0, 1, 5, 10. We observe from the table that, as the risk sensitivity parameter  increases, example systems s7 and s8 exhibit the lowest levels of risk relative to the other systems, (i.e.  "","" 1, 5, 10), while s1 exhibits the highest level of risk (ZRisk "", -6.835 at  ,"" 10). As can be seen, using multiple baselines resolves the effect of the lack of information about the expected per-topic system performance in assessing the risk levels of systems, i.e. s4 vs. s7 and s4 vs s8. In the following section, we show how to combine ZRisk with mean system effectiveness in order to solve the last issue about ZRisk, i.e. the counter-intuitive case of s4 vs. s3, where the measured level of risk for s3 is higher than that of s4 (e.g. the ZRisk score of s3 is -0.368 and it is -0.336 for s4 at  "","" 5), while s3 has higher effectiveness score than s4 (i.e. 0.300 vs. 0.250) and it has also a performance profile concordant with that of s4.""",null,null
299,4.3 Effectiveness vs. Risk,null,null
300,"Table 5 shows the calculated GeoRisk values for each of the 8 example runs. As can be seen, for the case of s4 vs. s3, the issue of the independence of ZRisk measurements from the magnitude of the mean effectiveness of IR systems is solved. The example system s3 is now measured as less",null,null
301,487,null,null
302,GeoRisk,null,null
303,0.45 S 8,null,null
304,0.4,null,null
305,0.35,null,null
306,0.3,null,null
307,s 1,null,null
308,s,null,null
309,0.25,null,null
310,2,null,null
311,s 3,null,null
312,0.2,null,null
313,s,null,null
314,4,null,null
315,0.15,null,null
316,s 5,null,null
317,s,null,null
318,0.1,null,null
319,6,null,null
320,s 7,null,null
321,0.05,null,null
322,s,null,null
323,8,null,null
324,S 4,null,null
325,S 3,null,null
326,S,null,null
327,2,null,null
328,S,null,null
329,6,null,null
330,S,null,null
331,5,null,null
332,S 1,null,null
333,0,null,null
334,0,null,null
335,1,null,null
336,2,null,null
337,3,null,null
338,4,null,null
339,5,null,null
340,6,null,null
341,7,null,null
342,8,null,null
343,9,null,null
344,10,null,null
345,Figure 2: Example systems' GeoRisk as 0    10.,null,null
346,GeoRisk,null,null
347,0.45,null,null
348,TREC 2012 Runs,null,null
349,0.4,null,null
350,uogTrA44xu,null,null
351,srchvrs12c00,null,null
352,0.35,null,null
353,DFalah121A,null,null
354,0.3,null,null
355,QUTparaBline,null,null
356,utw2012c1,null,null
357,0.25,null,null
358,ICTNET12ADR2,null,null
359,indriCASP,null,null
360,0.2,null,null
361,autoSTA,null,null
362,irra12c,null,null
363,0.15,null,null
364,0.1,null,null
365,0.05,null,null
366,0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20,null,null
367,Figure 3: GeoRisk plot for 8 TREC 2012 runs.,null,null
368,"`risky' than s4, as suggested by the magnitude of the observed mean effectiveness scores.",null,null
369,"Figure 2 shows the plot of GeoRisk scores for each example system for  ,"" 0, 1, 2, . . . , 10, where the systems with lines sloping downward along the increasing values of  (i.e. xaxis) are those that exhibit a risk of abject failure, (i.e. s1, s2, s5, and s6) while, in contrast, the robust systems such as s3, s4, s7 and s8 have nearly a straight, horizontal lines.""",null,null
370,"In summary, the GeoRisk measure takes into account both the mean effectiveness of IR systems and the difference in the shapes of their performance profiles. As a result, GeoRisk is sensitive to mean (i.e. the component Si/c), variance and the shape of the observed effectiveness scores across topics (i.e. the ZRisk component).",null,null
371,5. ANALYSIS OF TREC DATA,null,null
372,"In this section, we demonstrate the use of the risk-reward tradeoff measure derived from the Chi-square statistic, ZRisk, and the aggregate measure GeoRisk, on real systems submitted to the TREC 2012 Web track [10]1, in comparison with the existing measures URisk and TRisk. In the subsequent year of the Web track [12], a standard baseline called indriCASP and based on the Indri retrieval platform was provided. Similar to [13, 14], we use the same indriCASP system as the nominal single baseline on the 2012 Web track topics.",null,null
373,"In particular, out of the 48 runs submitted to TREC 2012, we select the top runs of the highest 8 performing groups, based on the mean ERR@20 score, While we omit other submitted runs for brevity, the following analysis would be equally applicable to them. For each run, we report the riskreward tradeoff scores obtained using the official TREC 2012 evaluation measure, ERR@20.",null,null
374,"1Although our analysis is equally applicable to the TREC 2013 Web track, due to the lack of space, we report results from TREC 2012, which are also directly comparable to that of previous works [13, 14].",null,null
375,"Table 6 lists the URisk, TRisk, TRisk, ZRisk and GeoRisk risk-reward tradeoff scores for the 8 runs. For the measures URisk and TRisk, the baseline run is indriCASP ; for the measures ZRisk and GeoRisk, we use as multiple baselines all 48+1 TREC 2012 runs including indriCASP ; TRisk denotes TRisk calculated using the per-topic mean effectiveness of the 49 runs as the baseline, i.e. Meanq in Eq. (3). Note that Dinc¸er et al. [13] showed that TRisk is inferential, i.e. the TRisk scores correspond to scores of the Student's t statistic. For this reason, for the URisk scores in TREC 2012 in Table 6 (where c ,"" 50), TRisk > ±2 indicates that the observed URisk score exhibits a significant level of risk.""",null,null
376,"Table 6 shows in general that the notion of risk quantified by the Chi-square statistic-based risk measure ZRisk differs from that of the URisk, TRisk and TRisk measures, as illustrated by the contrasting systems' rankings (the column R next to each measure) for ZRisk. In particular, at  ,"" 0, URisk and TRisk agree with the effectiveness measure ERR@20 on the rankings of the 8 TREC 2012 runs. However, at  "","" 5, although URisk and TRisk still agree with each other, they both diverge from the agreement with ERR@20. On the other hand, the risk measure ZRisk agrees neither with ERR@20 nor with the risk measures URisk and TRisk. Note that, except for the determination of baselines, the three risk measures URisk, TRisk, and ZRisk rely on the same notion of risk and reward, i.e. down-side risk and upside risk. Thus, comparing ZRisk with URisk and TRisk, it follows that multiple baselines (i.e. 49 TREC 2012 runs) provide information that is different from the information provided by the single baseline system indriCASP.""",null,null
377,"According to ZRisk, the most robust run is ""uogTrA44xu"" with a ZRisk value of 0.962 at  ,"" 0, and the next is """"irra12c"""" with ZRisk "","" 0.265, and so on, given the expected pertopic performance scores representing the baselines for each system. Based on the definition of ZRisk, it is expected that """"uogTrA44xu"""" would perform any given topic with an ERR@20 score that is better than or equal to the expected score for that topic on a population of systems with mean ERR@20 scores equal to 0.3406. Conversely, the least robust or most `risky' run is """"srchvrs12c00"""" with a ZRisk "", -0.912.",null,null
378,"Recall that ZRisk is independent of the observed mean effectiveness scores of the systems, which is, by definition, inappropriate for the purpose of a comparative IR evaluation. Thus, as an aggregate measure, GeoRisk, the geometric mean of ZRisk and ERR@20, can be used to tackle this challenge. As can be seen in Table 6, GeoRisk agrees with ERR@20 at  ,"" 0 on the rankings of the 8 TREC 2012 runs. Here, GeoRisk gives equal weights to ERR@20 and ZRisk, and similarly, at  "","" 0, ZRisk gives equal weights to downside risk and up-side risk. Thus, the observed agreement between GeoRisk and ERR@20 implies that the measured ZRisk scores for each of the 8 TREC 2012 runs at  "","" 0 are negligible compared to the observed differences in effectiveness between the runs. In other words, every TREC 2012 run exhibits risk, to a certain extent, but none of the measured risk levels are high enough to compensate for the observed difference in mean effectiveness between two systems, so that a swap between risk and reward for a given topic is likely to occur for two systems on the population of topics. Note that the agreements of TRisk, as well as URisk, at  "","" 0, with ERR@20 also give support in favour of the same conclusion, i.e. the practical insignificance of the measured levels of risk at  "", 0.",null,null
379,"On the other hand, as  increases (i.e. as the emphasis of down-side risk increases in ZRisk measurements), GeoRisk",null,null
380,488,null,null
381,uogTrA44xu,null,null
382,",0",null,null
383,",5",null,null
384," , 20",null,null
385,ERR URisk TRisk TRisk ZRisk R Geo R URisk R TRisk R TRisk R ZRisk R Geo R Geo R,null,null
386,0.3406 0.146 2.822 4.833 0.962 1 0.4158 1 -0.130 2 -0.798 2 2.767 1 -29.255 1 0.308 1 0.053 1,null,null
387,srchvrs12c00 0.3067 0.112 2.332 2.985 -0.912 9 0.3887 2 -0.100 1 -0.673 1 -0.678 2 -37.069 7 0.265 4 0.024 7,null,null
388,DFalah121A 0.2920 0.097 2.290 2.895 -0.328 7 0.3811 3 -0.156 3 -0.981 3 -0.861 3 -32.635 5 0.274 3 0.037 4,null,null
389,QUTparaBline 0.2901 0.095 2.130 2.870 0.004 4 0.3809 4 -0.189 4 -1.112 4 -1.006 4 -30.966 4 0.279 2 0.044 3,null,null
390,utw2012c1,null,null
391,0.2203 0.026 0.561 0.917 -0.172 6 0.3314 5 -0.388 6 -2.046 6 -2.987 5 -29.807 2 0.246 5 0.044 2,null,null
392,ICT. . . DR2 0.2149 0.020 0.487 0.569 0.233 3 0.3284 6 -0.329 5 -1.994 5 -3.238 6 -32.887 6 0.234 6 0.030 6,null,null
393,indriCASP,null,null
394,0.1947 *,null,null
395,* -0.120 -0.339 8 0.3111 7 * * *,null,null
396,* * -38.619 9 0.207 8 0.014 8,null,null
397,autoSTA,null,null
398,0.1735 -0.021 -0.498 -0.968 -0.143 5 0.2942 8 -0.509 8 -2.518 7 -4.195 7 -38.215 8 0.196 9 0.014 9,null,null
399,irra12c,null,null
400,0.1723 -0.022 -0.545 -1.214 0.265 2 0.2942 9 -0.501 7 -2.634 8 -4.510 8 -30.410 3 0.216 7 0.035 5,null,null
401,"Table 6: URisk, TRisk, TRisk and ZRisk risk-reward tradeoff scores for the top 8 TREC 2012 runs, along with GeoRisk at  ,"" 0, 1, 5, 10, 20. For URisk and TRisk the baseline is indriCASP, and for TRisk it is Meanq in Eq. (3) over all 48 + 1 TREC 2012 runs including indriCASP, and for ZRisk and GeoRisk, the baselines are estimated for the 8 runs over the same set of 49 runs and 50 Web track topics. The underlined TRisk scores correspond to those URisk scores for which a two-tailed paired t-test gives significance with p-value < 0.05 - i.e. TRisk > ±2.""",null,null
402,"diverges from ERR@20 and ranks the 8 TREC 2012 runs in a way that is different from all of the risk measures under consideration including ZRisk (for example,  ,"" 5), and at a high value of , it converges to an agreement with ZRisk, e.g. the systems' rankings in Table 6 at  "", 20 for GeoRisk and at  , 5 for ZRisk. The tendency of GeoRisk towards ZRisk as  increases is expected from the definition of GeoRisk.",null,null
403,"Figure 3 plots the GeoRisk scores calculated for each run at  ,"" 0, 1, . . . , 20. As can be seen, each of the 8 TREC 2012 runs has a decreasing GeoRisk score in magnitude, as the risk sensitivity parameter  increases. This means in general that - to a varying extent - every run under evaluation is subject to the risk of committing an abject failure, as the importance of getting a reasonable result for every topic increases. In particular, the runs """"uogTrA44xu"""" and """"ICTNET12ADR2"""" keep their relative positions in the observed rankings across all  values, while the ranks of the other runs considerably change. For example, the rank of """"srchvrs12c00"""" changes from 2 at  "", 0 to 7 at  , 5. At  ,"" 0, the run with rank 7 is indriCASP. However, the calculated risk for """"srchvrs12c00"""" at any level of  cannot be considered as empirical evidence to favour indriCASP over """"srchvrs12c00"""" for any given topic from the population, since the mean effectiveness of """"srchvrs12c00"""" is significantly higher than the mean effectiveness of indriCASP (p < 0.0239, paired t-test).""",null,null
404,"Note that, for two IR systems whose mean effectiveness scores are significantly different from each other, a measured risk level could have no particular meaning from a user perspective. This is because the system with higher mean effectiveness would be the one that can fulfil the users' information needs better than the other on average, no matter what level of risk is associated with it. The system with significantly low mean effectiveness would, on average, fail to return a ""reasonable"" result for any given topic, compared to the other system's effectiveness. For a declared significance with a p-value of 0.05, a swap in scores between the two systems for a topic (i.e. a transition from risk to reward or vice versa between the systems) is likely to occur 5% of the time on average [26].",null,null
405,"Nevertheless, the same case is not true for runs ""DFalah121A"" and ""QUTparaBline"", whose observed mean effectiveness scores are not significantly different from the mean effectiveness of ""srchvrs12c00"". The paired t-test, which is performed at a significance level of 0.05, fails to give significance to the observed difference in mean effectiveness between ""DFalah121A"" and ""srchvrs12c00"" with a p-value of 0.7592, and similarly for ""QUTparaBline"" with a p-value of 0.7003. This means that, for a given topic, a transition from risk to reward, or vice versa, between the runs ""DFalah121A"" and ""srchvrs12c00"", or between runs ""QUTparaBline"" and",null,null
406,"srchvrs12c00, is highly likely to occur on the population of topics. Thus, both systems can be considered less ""risky"" or more robust than ""srchvrs12c00"".",null,null
407,"In summary, this analysis of the TREC 2012 Web track runs demonstrates the suitability of GeoRisk for balancing risk-sensitivity and overall effectiveness, and the importance of using multiple baselines within the appropriate statistical framework represented by ZRisk. The analysis performed for the 8 TREC runs shows overall that, in a comparative IR evaluation effort, relying only on the observed mean effectiveness of the systems may be misleading, even for a best performer system like ""srchvrs12c00"", where the risk associated with such a system is high enough that it can cause an over-estimation of the real performance of the system. However, we showed that GeoRisk provides a solution for identifying systems exhibiting such risks.",null,null
408,6. RISK-SENSITIVE OPTIMISATION,null,null
409,"In this section, we show how GeoRisk can be integrated within the state-of-the-art LambdaMART learning to rank technique [7, 28]. Indeed, Wang et al. [27] showed how their URisk measure could be integrated within LambdaMART. Similarly, Din¸cer et al. [13] proposed variants of TRisk that resulted in learned models that exhibited less risk.",null,null
410,"The method of integration of risk-sensitive measures into LambdaMART requires adaptation of its objective function. In short, LambdaMART's objective function is a product of (i) the derivative of a cross-entropy that was originally defined in the RankNet learning to rank technique [6], based on the scores of two documents a and b, and (ii) the absolute change Mab in an evaluation measure M due to the swapping of documents a and b [28]. Various IR measures (e.g. NDCG) can be used as M , as long as the measure is consistent: for each pair of documents a and b with differing relevance labels, making an ""improving swap"" (moving the higher labelled document above the lesser) must result in Mab  0, and orthogonally for ""degrading swaps"".",null,null
411,"In adapting LambdaMART to be more robust within a risk-sensitive setting, the M is replaced by a variant M that considers the change in risk observed by swapping documents a and b, according to the underlying evaluation measure M , e.g. NDCG. In the following, we summarise existing instantiations of M arising from URisk and TRisk (called U-CRO, T-SARO and T-FARO), followed by our proposed instantiation of GeoRisk within LambdaMART. U-CRO: Constant Risk Optimisation based upon the URisk measure [27] (U-CRO) maintains a constant risk-level, regardless of the topic. In particular, let Mm define the effectiveness of the learned model m according to measure M , and let Mb define the effectiveness of the baseline b. Corre-",null,null
412,489,null,null
413,"spondingly, Mm(j) (Mb(j)) is the effectiveness of the learned model (baseline) for query j, then:",null,null
414,"M ,",null,null
415,M,null,null
416,if Mm(j) + M  Mb(j);,null,null
417,M · (1 + ) otherwise.,null,null
418,(9),null,null
419,"T-SARO & T-FARO: Adaptive Risk-sensitive Optimisation makes use of the fact that TRisk can identify queries that exhibit real (significant) levels of risk [13] compared to the baseline b. Dinc¸er et al. [13] proposed two Adaptive Risk-sensitive Optimisation adaptations of LambdaMART, namely T-SARO and T-FARO, which use this observation to focus on improving those risky queries. Indeed, in U-CRO, M is multiplied by (1 + ), for a static   0. In T-SARO and T-FARO,  is replaced by a variable  , which varies according to the probability of observing a query with a risk-reward score greater than that observed. By modelling this probability using the standard normal cumulative distribution function P r Z  TRj  1 -  TRj , T-SARO replaces the original  in Eq. (9) with  as:",null,null
420," ,"" [1 -  TRj ] · ,""",null,null
421,(10),null,null
422,"where TRj ,"" j/SE(URisk) determines the level of risk exhibited by topic j. T-SARO and T-FARO contrast on the topics for which  is adjusted ­ while T-SARO only adjusts topics with downside risk as per Eq. (9), T-FARO adjusts all topics:""",null,null
423,"M , M · (1 +  )",null,null
424,"The experiments in [13] found that T-FARO exhibited higher effectiveness than T-SARO, thus we compare GeoRisk to only U-CRO and T-FARO in our experiments. GeoRisk: Our adaptation of M for the newly proposed GeoRisk is more straightforward than the TRisk Adaptive Risk-sensitive Optimisations, in that we use GeoRisk directly as the measure within LambdaMART.",null,null
425,"M , GeoRisk(Mm + M ) - GeoRisk(M ) , (Si + M )/c × (ZRisk/c) - GeoRisk(M )",null,null
426,"Indeed, GeoRisk is suitable for LambdaMART as it exhibits the consistency property: an improving `swap' will increase both the left factor (Si/c) and the right factor ZRisk and therefore the value of GeoRisk for Mm +M will likewise increase. Moreover, as M is calculated repeatedly during the learning process, the speed of the implementation is critical for efficient learning. In this respect, it is important to note that retaining the values of the separate zij summands for each query in the ZRisk calculation (see Equation (6)) allows the new ZRisk value to be calculated by only recomputing the zij for the query affected, then recalculating GeoRisk.",null,null
427,"Recall from Section 3 that zij encapsulates differential weighting of downside and upside risks, but with respect to the expected performance on the topic. Hence, by using GeoRisk, the objective function of LambdaMART will favour learned models that make improving swaps of documents on topics where the learned model performs below expectation as calculated on the set of baselines.",null,null
428,"Naturally, the instantiation of GeoRisk within a learning to rank setting depends on the set of baselines X, to allow the estimation of the topic expectations eij (see Eq. (5)). The choice of baselines to provide for learning can impact upon which topics the learner aims to improve. Previous works on risk-sensitive learning [13, 27] have used the performance of a single BM25 retrieval feature as the baseline. Indeed, single weighting models such as BM25 are typically",null,null
429,"used to identify the initial set of documents, which are then re-ranked by the learned model [19, 20], and hence it is a baseline that the learner should outperform. However, it does not represent the typical performance of a learned approach upon the queries, as it cannot encapsulate the effectiveness of refined ranking models using many other features.",null,null
430,"Hence, instead of using GeoRisk to learn a model more effective than a set of BM25-like baselines, we argue for the use of state-of-the-art baselines, which portray representative estimations of query difficulty to the learner. Such baselines are more akin to the systems submitted to TREC (which themselves have been trained on previous datasets), rather than a single weighting model feature. In a deployed search engine, such state-of-the-art-baselines could represent the effectiveness of the currently deployed search engine, or other deployed search engines for the same query. In an experimental setting, such as in this paper, we use held-out data to pre-learn several learned models before conducting the main comparative experiments. Finally, for comparison purposes, we also deploy T*-FARO in our experiments, where the mean performance of the state-of-the-art baselines for each topic is used as the single learning baseline.",null,null
431,6.1 Experimental Setup,null,null
432,"Our experiments use the open source Jforests learning to rank tool [15]2, which implements U-CRO, and T-FARO, as well as plain LambdaMART. Our implementations of T*FARO & GeoRisk are also built upon Jforests3. As baselines, in addition to LambdaMART, we also deploy a plain gradient boosted regression trees learner (also implemented by Jforests), and two linear learned models, Automatic Feature Selection (AFS) [21] & AdaRank [19, Ch. 4].",null,null
433,"We conduct experiments using the MSLR-Web10k dataset4. This learning to rank dataset contains 136 query-dependent and query-independent feature values for documents retrieved for 10,000 queries, along with corresponding relevance labels. As highlighted above, our baselines require separate training. For this reason, we hold out 2000 queries for initial training (two thirds) and validation (one third). The remaining 8000 queries are then split into 5 folds, each with a balance of 60% queries for training, 20% for validation, and 20% for testing. Hence, our reported results are not comparable to previous works using all 10000 queries of the MSLR dataset, but instead performances for LambdaMART, UCRO & T-FARO are presented on the 8000 queries. The underlying evaluation measure M in each learner is NDCG.",null,null
434,"For GeoRisk & T*-FARO, the multiple baselines are evaluated for each query in the main 5 folds, which represent `unseen' queries for those systems. For U-CRO, T-SARO and T-FARO, the baseline is depicted by the performance of the BM25.wholedocument feature.",null,null
435,"Finally, we note that LambdaMART has several hyperparameters, namely the minimum number of documents in each leaf m, the number of leaves l, the number of trees in the ensemble nt and the learning rate r. Our experiments use a uniform setting for all parameters across all folds, namely m ,"" 1500, l "","" 50, nt "","" 500, l "","" 0.1, which are similar to those reported in [27] for the same dataset.""",null,null
436,6.2 Results,null,null
437,"In Table 7, we report the NDCG@10 effectiveness and robustness for LambdaMART, U-CRO, T-FARO, T*-FARO",null,null
438,2https://github.com/yasserg/jforests 3We have contributed GeoRisk as open source to Jforests. 4http://research.microsoft.com/en-us/projects/mslr/,null,null
439,490,null,null
440,"and GeoRisk for  ,"" {1, 5}5. The table is split into two halves: comparison to the effectiveness of the single BM25 baseline, and comparison viz. the 4 baseline learned models. For each of the baselines, we report the reward to risk ratio (denoted """"Reward/Risk""""), which measures the gain over the effectiveness of the baseline. Similarly, the win to loss ratio (denoted """"W/L"""") measures the number of queries that the risk-sensitive optimisation contributed to reward against risk. Finally, the number of queries that each model wins or looses relative to the baseline are also shown for each  value, along with the number of queries that experience a relative loss greater than 20% NDCG@10. For clarity, the header of Table 7 denote arrows to show the favourable direction of each measure, e.g.  means that higher is better.""",null,null
441,"On analysis of the top half of Table 7, we observe that GeoRisk generates the highest mean NDCG effectiveness, marginally improving over LambdaMART. This is also observable for the Reward measure, in comparison to the BM25 baseline. However, for the risk measures, T-FARO and UCRO obtain the best values. For ,  ,"" 1 is deemed the appropriate setting across all risk-sensitive learners, which has the effect of doubling the penalty of a query underperforming the corresponding baseline for that learner.""",null,null
442,"In the bottom half of Table 7, we examine the performance profiles of the different learners compared to the effectiveness of the 4 learned model baselines - the measures reported are the macro-average, i.e. the mean when each measure is calculated with respect to each learned baseline in turn (rather than compared to the micro-averaged effectiveness of the 4 learned baselines). In this half of the table, we note that, for  ,"" 1, GeoRisk demonstrates the highest Reward and number of Wins and lowest Losses (and the resulting best Reward/Risk & Win/Loss ratios (the latter is a 2.7% improvement over LambdaMART). These improvements in the risk profile of the systems are achieved while still attaining the highest mean NDCG effectiveness among the systems. All differences are statistically significant (n "", 8000 queries).",null,null
443,"Finally, we note that the effectiveness and risk profiles attained by T*-FARO are markedly different, with T*-FARO attaining the lowest Reward/Risk & Win/Loss ratios. This verifies that the use of expected topic performance by GeoRisk rather than a mean per-topic performance (as used by T*-FARO) results in a learned model more attuned to the normal performances of state-of-the-art baseline systems.",null,null
444,"Overall, this empirical evidence confirms our claim that the new risk-sensitive objective function GeoRisk for the LambdaMART learning to rank technique allows effective yet robust learned models to be obtained using multiple baselines. Moreover, we would highlight the more impressive risk-profiles attained in the bottom half of Table 7, which demonstrate that given a set of state-of-the-art baselines, using GeoRisk can generate an effective model that is as effective as LambdaMART with better risk profiles, and allows learning-to-rank to benefit from natural incremental improvements as practiced in real deployment settings.",null,null
445,"vestigated the use of the Student's t-test for risk-sensitive evaluation, this is the first work to investigate the use of Pearson's Chi-square statistic for risk-sensitive evaluation, thereby facilitating the use of multiple baselines. Instead, previous usages of the Chi-square statistic has encompassed index term weighing [18] and document classification [22].",null,null
446,"In IR experimentation, Armstrong et al. [3] noted that many papers appeared to show improvement upon older, weaker baselines. More recent work by Kharazmi et al. [17] showed that testing upon state-of-the-art baselines is necessary to demonstrate an advance. Similarly, this paper advocates the use of multiple state-of-the-art baselines, both in experimental and learning settings.",null,null
447,"We also note several attempts to develop robust learning to rank techniques: of note, the AdaRank technique [19, Ch. 4] focuses on improving hard queries using boosting. Since then, risk-sensitive optimisation techniques such as UCRO [27], T-SARO & T-FARO [13] have aimed to adapt the LambdaMART technique by identifying risky topics with respect to a single baseline. Our work goes further by making use of multiple state-of-the-art baseline systems when calculating risk-estimation in the learning to rank objective function. Finally, Bennett et al.[5] take a different route, by developing personalised risk-averse ranking strategies upon the LambdaMART technique. As they build upon U-CRO, it may be possible to combine both approaches in the future.",null,null
448,8. CONCLUSIONS,null,null
449,"This is the first paper that thoroughly investigated the use of multiple baselines in risk-sensitive evaluation. It argued for a new definition of risk-sensitivity related to the expected performance upon a given topic, calculated from a population of existing baseline systems. In particular, the paper introduced two new risk-sensitive evaluation measures, ZRisk and GeoRisk that are based upon the Chi-square statistic. Moreover, while ZRisk estimates risk independent of the overall mean retrieval effectiveness, GeoRisk enhances ZRisk by additionally accounting for overall effectiveness.",null,null
450,"Our new measures were demonstrated on the results of a comparative system evaluation from the TREC Web track. Finally, the paper showed how the proposed GeoRisk measure can be directly integrated within the objective function of the state-of-the-art LambdaMART learning to rank technique. Experiments upon 8000 queries from a learning to rank dataset showed that the resulting learned models were as effective as LambdaMART, but also more risk-averse when compared to four learned baselines.",null,null
451,9. ACKNOWLEDGMENTS,null,null
452,"This work is partially supported by TUBITAK, The Scientific and Technological Research Council of Turkey (Project No: 114E558). All opinions are that of the authors.",null,null
453,7. RELATED WORK,null,null
454,"One aspect of this paper is the assessment of the robustness of IR systems, initiated first by the TREC Robust track [24] based on the geometric mean average precision measure [23, 25], and developed further by the introduction of new measures of risk/robustness, such as URisk [27] and TRisk [13]. In this respect, while Dinc¸er et al. [13] in-",null,null
455,10. REFERENCES,null,null
456,"[1] A. Agresti. Categorical Data Analysis. Wiley, 2002. [2] G. Amati, C. Carpineto, and G. Romano. Query",null,null
457,"difficulty, robustness, and selective application of query expansion. In Proceedings of ECIR, 2004. [3] T. Armstrong, A. Moffat, W. Webber, and J. Zobel. Improvements that don't add up: ad-hoc retrieval results since 1998. In Proceedings of ACM CIKM, 2009.",null,null
458,"5,0 is equivalent to the normal LambdaMART algorithm.",null,null
459,"[4] S. Beitzel, E. Jensen, and O. Frieder. GMAP. In",null,null
460,"L. Liu and M. O¨ zsu, eds., Encyclopedia of Database",null,null
461,491,null,null
462,"Systems, pp 1256­1256, 2009.",null,null
463, NDCG Reward Risk Reward/Risk Wins Losses W/L L > 20%,null,null
464,Compared to BM25 Baseline,null,null
465,L'MART 0 0.4578 0.195 0.036,null,null
466,5.401,null,null
467,5892 1715 3.44,null,null
468,982,null,null
469,U-CRO 1 0.4558 0.192 0.035,null,null
470,5.472,null,null
471,5913 1684 3.51,null,null
472,965,null,null
473,U-CRO 5 0.4461 0.180 0.032,null,null
474,5.552,null,null
475,5913 1666 3.55,null,null
476,880,null,null
477,T-FARO 1 0.4576 0.194 0.035,null,null
478,5.568,null,null
479,5928 1671 3.55,null,null
480,963,null,null
481,T-FARO 5 0.4569 0.194 0.036,null,null
482,5.435,null,null
483,5889 1719 3.43,null,null
484,995,null,null
485,T*-FARO 1 0.4557 0.194 0.037,null,null
486,5.25,null,null
487,5857 1753 3.34,null,null
488,1016,null,null
489,T*-FARO 5 0.4562 0.194 0.037,null,null
490,5.26,null,null
491,5861 1737 3.37,null,null
492,999,null,null
493,GeoRisk 1 0.4581 0.196 0.037,null,null
494,5.327,null,null
495,5868 1732 3.39,null,null
496,992,null,null
497,GeoRisk 5 0.4557 0.195 0.038,null,null
498,5.179,null,null
499,5876 1728 3.40,null,null
500,1037,null,null
501,Compared to 4 Learned Models Baselines,null,null
502,L'MART 0 0.4578 0.061 0.053,null,null
503,1.138,null,null
504,3978.3 3669.0 1.12 1722.5,null,null
505,U-CRO 1 0.4558 0.059 0.053,null,null
506,1.103,null,null
507,3954.5 3690.0 1.11 1718.8,null,null
508,U-CRO 5 0.4461 0.051 0.056,null,null
509,0.941,null,null
510,3748.8 3887.3 1.01 1825.8,null,null
511,T-FARO 1 0.4576 0.059 0.052,null,null
512,1.137,null,null
513,3965.5 3674.3 1.12 1677.0,null,null
514,T-FARO 5 0.4569 0.059 0.052,null,null
515,1.121,null,null
516,3967.3 3676.5 1.12 1692.0,null,null
517,T*-FARO 1 0.4557 0.042 0.051,null,null
518,0.820,null,null
519,3562.0 4079.5 0.87 1821.5,null,null
520,T*-FARO 5 0.4562 0.041 0.053,null,null
521,0.823,null,null
522,3539.0 4099.5 0.86 1783.5,null,null
523,GeoRisk 1 0.4581 0.061 0.054,null,null
524,1.142,null,null
525,4017.0 3628.8 1.15 1709.8,null,null
526,GeoRisk 5 0.4557 0.060 0.055,null,null
527,1.095,null,null
528,3974.0 3674.0 1.12 1756.5,null,null
529,"Table 7: Learning to rank results, with risk results calculated w.r.t. BM25 & the 4 learned models. All differences are statistically significant over the n , 8000 queries.",null,null
530,"[5] P. N. Bennett, M. Shokouhi, and R. Caruana. Implicit preference labels for learning highly selective personalized rankers. In Proceedings of ACM ICTIR, 2015.",null,null
531,"[6] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proceedings of ICML, 2005.",null,null
532,"[7] C. J. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.",null,null
533,"[8] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query refinement using lexical affinities with maximal information gain. In Proceedings of ACM SIGIR, 2002.",null,null
534,"[9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of ACM CIKM, 2009.",null,null
535,"[10] C. L. A. Clarke, N. Craswell, and E. Voorhees. Overview of the TREC 2012 Web track. In Proceedings of TREC, 2012.",null,null
536,"[11] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In Proceedings of ACM CIKM, 2009.",null,null
537,"[12] K. Collins-Thompson, P. Bennett, F. Diaz, C. Clarke, and E. M. Voorhees. Overview of the TREC 2013 Web track. In Proceedings of TREC, 2013.",null,null
538,"[13] B. T. Din¸cer, C. Macdonald, and I. Ounis. Hypothesis testing for the risk-sensitive evaluation of retrieval systems. In Proceedings of ACM SIGIR, 2014.",null,null
539,"[14] B. T. Din¸cer, I. Ounis, and C. Macdonald. Tackling biased baselines in the risk-sensitive evaluation of retrieval systems. In Proceedings of ECIR, 2014.",null,null
540,"[15] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. In Proceedings of ACM SIGIR, 2011.",null,null
541,"[16] D. Hoaglin, F. Mosteller, and J. Tukey, eds. Understanding robust & exploratory data analysis. Wiley, 1983.",null,null
542,"[17] S. Kharazmi, F. Scholer, D. Vallet and M. Sanderson. Examining Additivity and Weak Baselines. TOIS, to appear, 2016.",null,null
543,"[18] I. Kocaba¸s, B. T. Din¸cer, and B. Karaoglan. A nonparametric term weighting method for information retrieval based on measuring the divergence from independence. Information Retrieval, 17(2):153­176, 2014.",null,null
544,"[19] T.-Y. Liu. Learning to rank for information retrieval. Foundation and Trends in Information Retrieval, 3(3):225­331, 2009.",null,null
545,"[20] C. Macdonald, R. L. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Information Retrieval., 16(5):584­628, 2013.",null,null
546,"[21] D. A. Metzler. Automatic feature selection in the markov random field model for information retrieval. In Proceedings of ACM CIKM, 2007.",null,null
547,"[22] M. Oakes, R. Gaaizauskas, H. Fowkes, A. Jonsson, V. Wan, and M. Beaulieu. A method based on the chi-square test for document classification. In Proceedings of ACM SIGIR, 2001.",null,null
548,"[23] S. Robertson. On GMAP - and other transformations. In Proceedings of ACM CIKM, 2006.",null,null
549,"[24] E. M. Voorhees. Overview of the TREC 2003 Robust retrieval track. In Proceedings of TREC, 2003.",null,null
550,"[25] E. M. Voorhees. The TREC Robust retrieval track. SIGIR Forum, 39(1):11­20, June 2005.",null,null
551,"[26] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proceedings of ACM SIGIR, 2002.",null,null
552,"[27] L. Wang, P. N. Bennett, and K. Collins-Thompson. Robust ranking models via risk-sensitive optimization. In Proceedings of ACM SIGIR, 2012.",null,null
553,"[28] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Ranking, boosting, and model adaptation. Technical Report MSR-TR-2008-109, Microsoft, 2008.",null,null
554,492,null,null
555,,null,null

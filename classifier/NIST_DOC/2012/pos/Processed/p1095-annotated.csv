,sentence,label,data
,,,
0,Inferring Missing Relevance Judgments from Crowd Workers via Probabilistic Matrix Factorization,null,null
,,,
1,Hyun Joon Jung,null,null
,,,
2,Dept. of Electrical and Computer Engineering University of Texas at Austin,null,null
,,,
3,hyunJoon@utexas.edu,null,null
,,,
4,Matthew Lease,null,null
,,,
5,School of Information Science University of Texas at Austin,null,null
,,,
6,ml@ischool.utexas.edu,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"In crowdsourced relevance judging, each crowd worker typically judges only a small number of examples, yielding a sparse and imbalanced set of judgments in which relatively few workers influence output consensus labels, particularly with simple consensus methods like majority voting. We show how probabilistic matrix factorization, a standard approach in collaborative filtering, can be used to infer missing worker judgments such that all workers influence output labels. Given complete worker judgments inferred by PMF, we evaluate impact in unsupervised and supervised scenarios. In the supervised case, we consider both weighted voting and worker selection strategies based on worker accuracy. Experiments on crowd judgments from the 2010 TREC Relevance Feedback Track show promise of the PMF approach merits further investigation and analysis.",Y,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,I.2.6 [Artificial Intelligence]: Learning,null,null
,,,
11,General Terms,null,null
,,,
12,"Algorithms, Design, Experimentation, Performance",null,null
,,,
13,Keywords,null,null
,,,
14,"Crowdsourcing, label aggregation, matrix Factorization",null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"Crowdsourced relevance judging offers potential to reduce time, cost, and effort of relevance judging [1] and benefit from greater diversity of crowd judges. However, quality of judgments from non-workers continues to be a concern, motivating continuing work in quality assurance methods based on statistical label aggregation methods or greater attention to human factors. A common approach is to collect multiple, redundant judgments from workers and aggregate them via methods like majority voting (MV) or expectation maximization (EM) to produce consensus labels [4].",null,null
,,,
17,"Because each crowd worker typically judges only a small number of examples, collected judgments are typically sparse and imbalanced, with relatively few workers influencing output consensus labels. MV is completely susceptible to this problem. EM addresses this indirectly: while only workers",null,null
,,,
18,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
19,Figure 1: Crowdsourcing workers judgments (Left) are copied to a sparse worker-task matrix (Middle). Missing judgments are inferred via PMF (Right).,null,null
,,,
20,"labeling an example vote on it, global worker judgments are used to infer class priors and worker confusion matrices.",null,null
,,,
21,"We propose to tackle this issue directly by adopting a collaborative filtering approach, which routinely deals with the issue of each user rating only a small number of items (e.g., movies, books, etc.) vs. the complete set. In particular, we employ probabilistic matrix factorization (PMF), which induces a latent feature vector for each worker and example [6] in order to infers unobserved worker judgments for all examples. Figure 1 depicts our approach graphically.",null,null
,,,
22,"We are not familiar with any prior work investigating PMF, or collaborative filtering approaches more generally, toward crowdsourcing quality assurance. Related prior work has investigated other ways to infer bias corrected labels in place of raw labels [4], as well as inference of missing labels by estimating a unique classifier for each worker [3].",null,null
,,,
23,"Probabilistic Matrix Factorization (PMF). Suppose we have M tasks (examples to be labeled), N workers, and a label matrix R in which Rij indicates the label of worker i for task j. Let U  RDM and V  RDN be latent feature matrices for workers and tasks, with column vectors Ui and Vj representing D-dimensional worker-specific and taskspecific latent feature vectors, respectively. The conditional probability distribution over the observed labels R  RNM is given by Equation 1. Indicator Iij equals 1 iff worker i labeled task j. We place zero-mean spherical Gaussian priors on worker and task feature vectors (Equations 2 and 3).",null,null
,,,
24,NM,null,null
,,,
25,"p(R|U, V, 2) ,",null,null
,,,
26,"[N (Rij |UiT Vj , 2)]Iij",null,null
,,,
27,-1,null,null
,,,
28,"i,1 j,1",null,null
,,,
29,N,null,null
,,,
30,"p(U |U2 ) ,"" [N (Ui|0, U2 I)]""",null,null
,,,
31,-2,null,null
,,,
32,"i,1",null,null
,,,
33,M,null,null
,,,
34,"p(V |V2 ) ,"" [N (Vj |0, V2 I)]""",null,null
,,,
35,-3,null,null
,,,
36,"j,1",null,null
,,,
37,1095,null,null
,,,
38,Method 1 2 3 4 5 6 7,null,null
,,,
39,Supervised No No No Yes Yes Yes Yes,null,null
,,,
40,Worker Labels raw (sparse) raw (sparse) PMF (complete) raw (sparse) raw (sparse) raw (sparse) PMF (complete),null,null
,,,
41,Label Aggregation MV EM MV WV,null,null
,,,
42,"Filtering(,0.67) WV & Filtering(,0.67) WV & Filtering(,0.7)",null,null
,,,
43,ACC 0.603 0.644 0.643 0.642 0.752 0.750 0.673,null,null
,,,
44,Rank 4 3 3 3 1 1 2,null,null
,,,
45,RMSE 0.63 0.596 0.598 0.598 0.498 0.500 0.571,null,null
,,,
46,Rank 4 3 3 3 1 1 2,null,null
,,,
47,SPE 0.332 0.418 0.440 0.900 0.838 0.848 0.542,null,null
,,,
48,Rank 6 4 5 1 2 2 3,null,null
,,,
49,"Table 1: Results of PMF-based inference of missing worker labels. For the unsupervised case, majority voting (MV) with PMF (Method 3) is compared to MV and EM approaches using input (sparse) worker labels (Methods 1-2). With supervision, we compare weighted voting (WV) and/or filtering with and without PMF. Ranks shown indicate statistically significant differences at p <, 0.05 using a two-tailed paired t-test.",null,null
,,,
50,"To estimate model parameters, we maximize the log-posterior over task and worker features with fixed hyper-parameters. Maximizing the posterior with respect to U and V is equivalent to minimizing squared error with L2 regularization:",null,null
,,,
51,1N 2,null,null
,,,
52,M,null,null
,,,
53,Iij (Rij,null,null
,,,
54,-,null,null
,,,
55,UiT,null,null
,,,
56,Vj )2,null,null
,,,
57,+,null,null
,,,
58,U 2,null,null
,,,
59,N,null,null
,,,
60,Ui,null,null
,,,
61,2 F,null,null
,,,
62,+,null,null
,,,
63,V 2,null,null
,,,
64,M,null,null
,,,
65,Vj,null,null
,,,
66,2 F,null,null
,,,
67,"i,1 j,1",null,null
,,,
68,"i,1",null,null
,,,
69,"i,1",null,null
,,,
70,"where U ,"" U /, V "","" V /, and""",null,null
,,,
71,2 F,null,null
,,,
72,denotes,null,null
,,,
73,the,null,null
,,,
74,Frobe-,null,null
,,,
75,nius Norm. We use gradient descent to find a local mini-,null,null
,,,
76,"mum of the objective for U and V . Finally, we infer missing",null,null
,,,
77,worker judgments in the worker-task matrix R by taking,null,null
,,,
78,"the scalar product of U and V. Note that as in [4], we also",null,null
,,,
79,replace actual labels with bias-corrected inferred labels.,null,null
,,,
80,Label Aggregation. Given the complete set of inferred,null,null
,,,
81,"worker relevance judgments in matrix R, we next aggregate",null,null
,,,
82,worker judgments to induce consensus labels. We consider,null,null
,,,
83,"both unsupervised supervised scenarios. In the former, we",null,null
,,,
84,consider majority voting with raw (sparse) labels (Method,null,null
,,,
85,"1), expectation maximization with raw labels (Method 2),",null,null
,,,
86,"and PMF-based MV (Method 3). In the supervised case, we",null,null
,,,
87,"measure each worker's accuracy based on expert judgments,",null,null
,,,
88,with labels of anti-correlated workers flipped such that ac-,null,null
,,,
89,curacy is always  50%. We use supervision in two distinct,null,null
,,,
90,"ways: weighted voting (WV) and worker filtering, in which",null,null
,,,
91,only workers with accuracy   participate in voting.,null,null
,,,
92,2. EVALUATION,null,null
,,,
93,"Experiments are performed on crowd judgments collected in the 2010 TREC Relevance Feedback Track [2] from Amazon Mechanical Turk. 762 crowd workers judged 19033 querydocument tasks (examples), and 89624 judgments were collected. Our worker-task matrix thus has 762 columns (workers) and 19,033 rows (tasks); only 89,624 out of 14,503,146 labels (0.6%) are observed, so data is extremely sparse. 3,275 expert relevance judgments by NIST are partitioned into training (2,275) and test (1,000) sets. The test set is evenlybalanced between relevant and non-relevant classes.",Y,null
,,,
94,"Parameters. For dimensionality of task and worker latent feature vectors, we consider D  10, 30, 50 and select D ,"" 30 based on cross-validation on the entire set of labels (unsupervised). We similarly tune regularization parameter   {0.001, 0.01, 0.1, 0.5} and select  "","" 0.1. We tune the worker filtering threshold   [0.6, 0.99] by cross-validation on the training set using a linear sweep with step-size 0.01.""",null,null
,,,
95,"Metrics and Results. Table 1 reports accuracy (ACC), RMSE, and specificity achieved by each method.",null,null
,,,
96,Unsupervised Methods. Method 2 of PMF with ma-,null,null
,,,
97,jority voting (MV) outperforms the MV baseline (Method 1) and performs equivalently to EM (Method 2).,null,null
,,,
98,"Supervised vs. Unsupervised Methods. While supervised methods tend to dominate, unsupervised EM and PMF both match performance of the supervised weighted voting (WV) method without filtering or PMF (Method 4).",null,null
,,,
99,"Supervised Methods. Worker filtering is clearly seen to provide the greatest benefit, and surprisingly performs better without PMF than with PMF (Methods 6 vs. 7). When filtering is used, use of WV is not seen to further improve performance (Methods 5 vs. 6). We do see PMF-based modeling outperform non-PMF modeling when worker filtering is not employed (Methods 7 vs. 4).",null,null
,,,
100,3. CONCLUSION,null,null
,,,
101,"While unsupervised consensus labeling accuracy with PMF only matched EM performance, PMF is advantageous in that once complete worker judgments are inferred, they might be used for a variety of other purposes, such as better routing or recommending appropriate tasks to workers.",null,null
,,,
102,"Intuitively, an accurate worker's empirical label distribution should resemble the actual class prior. This suggests an alternative, more weakly supervised scenario to consider in which class priors are known while example labels are not. In the unsupervised case, we might instead simply examine the distribution of empirical priors for each worker and detect outliers [5]. In future work, we plan to investigate these ideas further in combination with those described here.",null,null
,,,
103,4. REFERENCES,null,null
,,,
104,"[1] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. SIGIR Forum, 42(2):9­15, 2008.",null,null
,,,
105,"[2] C. Buckley, M. Lease, and M. D. Smucker. Overview of the TREC 2010 Relevance Feedback Track (Notebook). In Proc. of the 19th Text Retrieval Conference, 2010.",null,null
,,,
106,"[3] S. Chen, J. Zhang, G. Chen, and C. Zhang. What if the irresponsible teachers are dominating? a method of training on samples and clustering on teachers. In 24th AAAI Conference, pages 419­424, 2010.",null,null
,,,
107,"[4] P. Ipeirotis, F. Provost, and J. Wang. Quality management on amazon mechanical turk. In Proceedings of the ACM SIGKDD workshop on human computation, pages 64­67. ACM, 2010.",null,null
,,,
108,"[5] H. J. Jung and M. Lease. Improving Consensus Accuracy via Z-score and Weighted Voting. In AAAI Workshop on Human Computation (HComp), 2011.",null,null
,,,
109,"[6] R. Salakhutdinov and et al. Probabilistic matrix factorization. In NIPS 2008, volume 20, January 2008.",null,null
,,,
110,1096,null,null
,,,
111,,null,null

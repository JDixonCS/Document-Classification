,sentence,label,data
,,,
0,Incorporating Statistical Topic Information in Relevance Feedback,null,null
,,,
1,Karla Caballero,null,null
,,,
2,"UC, Santa Cruz Santa Cruz CA, USA",null,null
,,,
3,karla@soe.ucsc.edu,null,null
,,,
4,Ram Akella,null,null
,,,
5,"UC, Santa Cruz Santa Cruz CA, USA",null,null
,,,
6,akella@soe.ucsc.edu,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"Most of the relevance feedback algorithms only use document terms as feedback (local features) in order to update the query and re-rank the documents to show to the user. This approach is limited by the terms of those documents without any global context. We propose to use statistical topic modeling techniques in relevance feedback to incorporate a better estimate of context by including global information about the document. This is particularly helpful for difficult queries where learning the context from the interactions with the user is crucial. We propose to use the topic mixture information obtained to characterize the documents and learn their topics. Then, we rank documents incorporating positive and negative feedback by fitting a latent distribution for each class of documents online and combining all the features using Bayesian Logistic Regression. We show results using the OHSUMED dataset for 3 different variants and obtain higher performance, up to 12.5% in Mean Average Precision (MAP).",null,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,"H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Relevance Feedback, Document Filtering",null,null
,,,
11,General Terms,null,null
,,,
12,"Algorithms, Experimentation",null,null
,,,
13,Keywords,null,null
,,,
14,"Relevance Feedback,Topic Models, Language Models",null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"Relevance feedback has been studied extensively in Information Retrieval as a form of incorporating feedback from the user to refine the results retrieved. The authors in [5] concluded that negative feedback is also valuable to improve the ranking. However, the need to capture broader context in difficult queries is still a challenge. The authors in [2] have showed that including global features and using clusters can improve the retrieval performance significantly. Thus, statistical topic modeling provides a robust and automatic method to incorporate context to the user feedback.",null,null
,,,
17,Main contact.,null,null
,,,
18,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
19,"Previous approaches have used statistical topic models to represent documents according to their latent topic content and use this representation in information retrieval [1, 4]. Authors in [4], use topics as a form of smoothing the Language Model used in retrieval. However, this approach does not address the incorporation of relevance feedback. Recent work from [1] explores the use of topics as a form to perform the query expansion for relevance feedback. However, this action might make the query noisier because the top topic terms might not contribute to a better discrimination of the relevant documents. In addition, these terms might not be distinctive across different topics.",null,null
,,,
20,"We propose to include the topic information as feedback using the document topic mixture instead of the document word mixture. We first estimate the topic mixture for each document in the corpus using LDA and save it as meta data. Given an initial query we use a standard retrieval engine, Language Models for this case, to show the first set of documents to the user and obtain relevance judgments. Then, we assume that topic mixtures for feedback documents are observed. We then define two latent Dirichlet distributions: one for relevant documents and another for nonrelevant documents. We fit these distributions iteratively, by finding a sufficient statistic and maximizing the likelihood of observing this statistic. To score the documents, we use Bayesian Logistic Regression. This function results in a very efficient scoring function, and incorporates the benefits of active learning. Under this model, we incorporate positive and negative feedback, and context based on topics in the interaction without changing the query. We also provide efficient updates of the latent distributions based on topics.",null,null
,,,
21,2. METHODOLOGY,null,null
,,,
22,In this section we describe how we incorporate the topic,null,null
,,,
23,mixture of feedback documents as a global measure in con-,null,null
,,,
24,"trast to query expansion. To achieve this, we estimate the",null,null
,,,
25,"topic mixtures of the documents, i, for K topics in the cor-",null,null
,,,
26,"pus using LDA off-line. Given a initial ranking, the user",null,null
,,,
27,provides relevance feedback which is used to fit the latent,null,null
,,,
28,"relevant/non-relevant distributions of topic mixtures. Thus,",null,null
,,,
29,we assume two Dirichlet distributions: one for the relevant,null,null
,,,
30,"set of documents R, and one for the non-relevant documents R¯ . Therefore, we have:",null,null
,,,
31,P (i|R),null,null
,,,
32,Dirichlet(R),null,null
,,,
33,",",null,null
,,,
34,(,null,null
,,,
35,"K k,1",null,null
,,,
36,"k,R",null,null
,,,
37,),null,null
,,,
38,"K k,1",null,null
,,,
39,"(k,R)",null,null
,,,
40,K,null,null
,,,
41,"k,R -1 i,k k,1",null,null
,,,
42,"for R and R¯ . Then, we calculate the log-probability of the document being generated by those distributions:",null,null
,,,
43,1093,null,null
,,,
44,K,null,null
,,,
45,K,null,null
,,,
46,K,null,null
,,,
47,"LogP (i|) ,"" log ( k)- log (k)+ (k-1) log(i,k)""",null,null
,,,
48,"k,1",null,null
,,,
49,"k,1",null,null
,,,
50,"k,1",null,null
,,,
51,We denote these scores as P Ri and P R¯i respectively. To,null,null
,,,
52,update the latent distributions of the relevant R and non-,null,null
,,,
53,"relevant topics R¯, we use the topic content from the docu-",null,null
,,,
54,"ments labeled by the user as relevant, R, and non-relevant,",null,null
,,,
55,"R¯ , after each interaction. The Dirichlet distribution guar-",null,null
,,,
56,antees a unique maximum when the Maximum Likelihood,null,null
,,,
57,"(ML) is estimated for . Moreover, a sufficient statistics,",null,null
,,,
58,"SS, can be estimated to update this distribution as more",null,null
,,,
59,observations are available. We can update SS efficiently,null,null
,,,
60,"without keeping previous document feedback. The initial value of the sufficient statistic SSk(0,R) for the relevant topic k and its update from the interaction j is described by:",null,null
,,,
61,"S Sk(0,R)",null,null
,,,
62,",",null,null
,,,
63,1 NR(0),null,null
,,,
64,"log i,k",null,null
,,,
65,iR0,null,null
,,,
66,"S Sk(j,R)",null,null
,,,
67,",",null,null
,,,
68,NR(j-1) NR(j),null,null
,,,
69,S,null,null
,,,
70,"Sk(j,R-1)",null,null
,,,
71,+,null,null
,,,
72,1 NR(j),null,null
,,,
73,"log i,k",null,null
,,,
74,iRj,null,null
,,,
75,"where NR(j) ,"" NR(j-1) + |Rj |, and |Rj | is the total number of relevant documents at the j-th interaction. Given SSR(j) and SSR(¯j), we use the method proposed in [3] to calculate the ML estimator for (Rj) and (R¯j). In addition to these distributions, we use the topic-based Language Model PT W for document i as follows:""",null,null
,,,
76,K,null,null
,,,
77,"PT W,i(w|i, ^) , P (w|z ,"" k, ^k)P (z "", k|i)",null,null
,,,
78,"k,1",null,null
,,,
79,where ^ are the word mixture for the topics obtained from,null,null
,,,
80,"LDA. Thus, the score ST W,i for query Q with terms q is defined as:",null,null
,,,
81,"ST W,i ,",null,null
,,,
82,"PT W,i(w ,"" q|i, ^)""",null,null
,,,
83,qQ,null,null
,,,
84,To combine the scores from the latent relevant/non-relevant,null,null
,,,
85,"topic mixtures and the topic-based Language Model, we use",null,null
,,,
86,"the Bayesian Logistic Regression approach [5]. Let yi ,"" {+1, -1} be the relevant/non-relevant label for document,""",null,null
,,,
87,we have the score function:,null,null
,,,
88,"P (yi|, di)",null,null
,,,
89,",",null,null
,,,
90,1,null,null
,,,
91,+,null,null
,,,
92,1 exp(-T diyi),null,null
,,,
93,"where di is the feature vector scores: P Ri, P R¯i, ST W,i.  is a parameter vector assumed to be normally distributed and updated in a Bayesian form. Here, the distribution of  from the j-th iteration is taken as prior distribution for the next iteration. To approximate the posterior distribution we",null,null
,,,
94,use the Laplace approximation as discussed in [5].,null,null
,,,
95,3. RESULTS,null,null
,,,
96,"We test our method using the OHSUMED dataset which consists of 196, 000 medical abstracts and 3, 506 relevance labels for 63 queries from the Document Filtering Track from TREC 4. As suggested in the track, we assume unobserved labels as non-relevant. We fit the LDA model using K ,"" 50 topics, which is the number of topics with highest performance based on Empirical Likelihood. To test the impact of topic information, we use standard Language Model (LM) with Dirichlet smoothing described in [4] as baseline. This score is used with Bayesian Logistic Regression. We test 3 variants of the model and the baseline: LM as baseline;""",Y,null
,,,
97,Table 1: Results of Topic feedback using 50 topics,null,null
,,,
98,in the OHSUMED dataset,Y,null
,,,
99,Method,null,null
,,,
100,P@10 MAP DiscGain,null,null
,,,
101,LM,null,null
,,,
102,0.3968 0.4286 0.5660,null,null
,,,
103,LM +ST W,null,null
,,,
104,0.4206 0.4557,null,null
,,,
105,LM +ST W +P R,null,null
,,,
106,0.2968 0.3307,null,null
,,,
107,LM +ST W +P R+P R¯ 0.4698 0.5141,null,null
,,,
108,0.6315 0.5590 0.6580,null,null
,,,
109,LM+ST W ; LM+ST W +P R; LM+ST W +P R+P R¯. We calculate the initial ranking using LM and asked for feedback,null,null
,,,
110,"until we have at least one relevant and one non-relevant documents. We use 10 feedback documents and estimate precision at 10 (P@10), Mean Average Precision (MAP), and Discounted Gain (DiscGain). There are two relevance level labels available in the dataset, {1, 2}, that are assumed equally, {+1} for P@10 and MAP. However for DiscGain, we use both labels in the evaluation.",null,null
,,,
111,"Table 1 shows the results for the variants tested. We observe that the LM+ST W performs better than the baseline. This score is similar to the LDA-based retrieval proposed in [4] but the value of the linear combination parameters  is fitted based on the feedback as opposed to a corpus-wide parameter. When we incorporate only the score from the relevant distribution of topics P R, the performance decreases. However, when the score for the non-relevant distribution P R¯ is incorporated, the performance is the highest. This shows the value of negative feedback reported previously in [5]. We notice that, the combination of P R and P R¯ is equivalent to the log of the likelihood ratio test (probabilistic ranking principle) weighted by . This also explains why both scores should be included in the model.",null,null
,,,
112,"We observe that the combination of the four scores improves the general performance by: 11.6% P@10, 12.8%",null,null
,,,
113,"MAP, and 4.6% DiscGain respect topic-based language model (LM+ST W ). This demonstrates, the power of statistical topic modeling in relevance feedback.",null,null
,,,
114,4. CONCLUSION AND FUTURE WORK,null,null
,,,
115,"We have presented a method to incorporate statistical topic information in relevance feedback without changing the query. Results show that including the mixture of topics in relevance feedback improves the performance by pruning the search space, and adding context to the query. As future work, we plan to incorporate a policy to decide when to update the parameters of the relevant and non-relevant topic distribution optimally.",null,null
,,,
116,5. ACKNOWLEDGMENTS,null,null
,,,
117,This work is partially funded by CONACYT grant 207751 and SAP Gift Support,null,null
,,,
118,6. REFERENCES,null,null
,,,
119,"[1] D. Andrzejewski and D. Buttler. Latent topic feedback for information retrieval. In Proceedings of the 17th ACM SIGKDD conference, KDD '11, pages 600­608, 2011.",null,null
,,,
120,"[2] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of the SIGIR conference, pages 235­242, 2008.",null,null
,,,
121,"[3] T. Minka. Estimating a dirichlet distribution. Technical report, 2003.",null,null
,,,
122,"[4] X. Wei and W. B. Croft. Lda-based document models for ad-hoc retrieval. In Proceedings of the 29th ACM SIGIR conference, SIGIR '06, pages 178­185, 2006.",null,null
,,,
123,"[5] Z. Xu and R. Akella. A bayesian logistic regression model for active relevance feedback. In Proceedings of the 31st ACM SIGIR conference, SIGIR '08, pages 227­234, 2008.",null,null
,,,
124,1094,null,null
,,,
125,,null,null

,sentence,label,data
0,"When Web Search Fails, Searchers Become Askers: Understanding the Transition",null,null
1,"Qiaoling Liu§, Eugene Agichtein§, Gideon Dror, Yoelle Maarek, Idan Szpektor,",null,null
2,"§Emory University, Atlanta, GA, USA Yahoo! Research, Haifa, Israel",null,null
3,"{qliu26, eugene}@mathcs.emory.edu, {gideondr, idan}@yahoo-inc.com, yoelle@ymail.com",null,null
4,ABSTRACT,null,null
5,"While Web search has become increasingly effective over the last decade, for many users' needs the required answers may be spread across many documents, or may not exist on the Web at all. Yet, many of these needs could be addressed by asking people via popular Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. In this paper, we perform the first large-scale analysis of how searchers become askers. For this, we study the logs of a major web search engine to trace the transformation of a large number of failed searches into questions posted on a popular CQA site. Specifically, we analyze the characteristics of the queries, and of the patterns of search behavior that precede posting a question; the relationship between the content of the attempted queries and of the posted questions; and the subsequent actions the user performs on the CQA site. Our work develops novel insights into searcher intent and behavior that lead to asking questions to the community, providing a foundation for more effective integration of automated web search and social information seeking.",null,null
6,Categories and Subject Descriptors,null,null
7,H.3.3 [Information Systems]: Information Storage and Retrieval,null,null
8,Keywords,null,null
9,"query analysis, community question answering",null,null
10,1. INTRODUCTION,null,null
11,"While Web search engines have significantly progressed in effectiveness and efficiency over the last decade, there still exist certain user needs that cannot be satisfied. This could be due to a number of reasons, such as the difficulty of expressing a complex need as a short search query, the lack of existing relevant content on the Web (e.g., for unique or ""tail"" needs that keep appearing), and for more ""social"" needs, for which the user prefers to interact with a real human.",null,null
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",null,null
13,"In fact, Hitwise in August 2011 reported that only 6680% of the searches are successful1, and Hassan et al. [15] obtained a similar success rate of search goals (73%) via human labeling. We argue that many such unsatisfied searches could be addressed by asking people via Community Question Answering (CQA) services, such as Baidu Knows, Quora, or Yahoo! Answers. It already happens in practice. For example, we have observed that about 2% of web search sessions performed by users who are also members of the Yahoo! Answers community, lead to a question posted to the community. Consider Figure 1a, which depicts a sample search submitted to a major search engine. The searcher is not satisfied with the results, and eventually posts a related question on the Yahoo! Answers site, which is then answered to the searcher's satisfaction. Understanding and improving the synergy between searching and community question answering is at the heart of this work.",null,null
14,"Specifically, our goal is to better understand the behavior of these users, as well as characterize the types of Web searches that could be effectively handled by CQA sites. Insights acquired during such analysis should bring multiple benefits to both search engines and CQA systems. On one hand, search engines always need to better understand when searchers are unsatisfied by the returned results. More specifically, Web search engines would find value in analyzing the search session patterns of such unsuccessful queries, the associated underlying query intents, and possibly reflect these findings in search effectiveness metrics. One can even imagine new search experiences that would allow users to turn to the community for certain types of needs better addressed by people than by traditional Web search. Additionally, CQA systems could potentially improve the asking experience by taking advantage of the context provided by unsuccessful queries preceding a posted question. One can imagine several ways to leverage this context, such as automatically giving examples of irrelevant answers to clarify the question to the community.",null,null
15,"To the best of our knowledge, our work is the first to perform a large-scale study of the transformation of searchers into askers. That is, we start our analysis with web search sessions, trace the searcher through her visit to a CQA site, and analyze the resulting questions posted for the community.",null,null
16,"We focus on one of the most visited, and more mature, CQA systems existing today, namely Yahoo! Answers, which",null,null
17,1www.hitwise.com/us/about-us/ press-center/press-releases/ experian-hitwise-reports-google-share-of-searche/,null,null
18,801,null,null
19,(a),null,null
20,(b),null,null
21,Figure 1: Example search (a) followed by a question posted by the same user on the Yahoo! Answers site with a satisfactory answer from the community (b).,null,null
22,with more than 1 billion posted answers2 is highly visible,null,null
23,such as occurrence of personal pronouns or sentiment,null,null
24,in most search engines result pages. We built a corpus of,null,null
25,indicators (Section 4).,null,null
26,query-to-question transitions and studied it in order to un-,null,null
27,derstand when and why searchers become askers. A privacy-,null,null
28,preserving subset of the data has recently been made publicly available through Yahoo's Webscope program3.,null,null
29,Research Question 3: How do searchers behave after transferring to the CQA site?,null,null
30,"More specifically, our study is organized around the following three research questions, each associated with a set of hypotheses:",null,null
31,· Hypothesis 5: We further hypothesize that the content and the topics of the questions posted after a search session differ substantially from the general ques-,null,null
32,Research Question 1: When do searchers turn to CQA,null,null
33,tion distribution (Section 5.1).,null,null
34,for answers?,null,null
35,· Hypothesis 6: We hypothesize that the question ses-,null,null
36,"· Hypothesis 1: Queries and information needs of search sessions that lead to posting questions are hypothesized to share common characteristics, and differ from general web searches in words and information needs (Section 3.1).",null,null
37,"sions after switching from searching, exhibit different characteristics than general question sessions and search sessions explored in depth in previous research work [17][10]. We study these different types of sessions in terms of duration and persistence for specific users and examine their behavior over time (Section 5.2).",null,null
38,"· Hypothesis 2: We hypothesize that searchers who switch to CQA exhibit common search behavior. For instance, they tend to click more on CQA results on",null,null
39,The rest of this paper is dedicated to answering the above questions and verifying the associated hypotheses.,null,null
40,"the search result page, and their search sessions are",null,null
41,"longer, allowing to characterize different types of users in the same spirit as [7] (Section 3.2).",null,null
42,2. ACQUIRING DATA,null,null
43,"In order to understand how searchers become askers, we",null,null
44,collected a dataset that contains both the search session part,null,null
45,Research Question 2: How do search queries relate to the associated questions posted on CQA sites?,null,null
46,and asking session part of each user who conducted a search session that resulted in posting a question. Our dataset is derived from joining a sample of the query logs of the Yahoo!,null,null
47,· Hypothesis 3: Queries and questions follow different,null,null
48,"search engine and the Yahoo! Answers question logs, both",null,null
49,"word distributions. More specifically, words in queries",null,null
50,for June 2011.,null,null
51,are hypothesized to follow different distributions than,null,null
52,"To create this dataset, we first created a mapping of users",null,null
53,"those appearing in questions, and a clear vocabulary",null,null
54,"between the two logs, based on the clicks on the same Yahoo!",null,null
55,gap between these can be observed (Section 4).,null,null
56,Answers question page following the same query on the same,null,null
57,"time frame, as they appear in both logs. Then, we extracted",null,null
58,· Hypothesis 4: Questions are typically more specific,null,null
59,"user actions from the query and question logs, e.g. posting",null,null
60,"than queries and include additional context (e.g., per-",null,null
61,"queries and clicking on results from query logs, as well as",null,null
62,sonal background) absent from the original queries.,null,null
63,posting questions and re-viewing them from the question,null,null
64,We hypothesize that these differences are reflected in,null,null
65,logs. We constructed search sessions from these extracted,null,null
66,"the lexicographic differences between questions and queries, actions, with a 30 minutes timeout as a session boundary.",null,null
67,2http://yanswersblog.com/index.php/archives/2010/,null,null
68,05/03/1-billion-answers-served/ 3http://webscope.sandbox.yahoo.com/,null,null
69,"Question sessions have no temporal boundary, since every action in the session unambiguously refers to the question posted by the asker.",null,null
70,802,null,null
71,Table 1: Statistics of the constructed datasets. Description,null,null
72,Total sampled search sessions SearchOnly sessions Search sessions that include question sessions SearchAsk sessions: search sessions with a single relevant question posted after searching,null,null
73,Number of sessions,null,null
74,"1,287,238 (100%) 1,233,279 (95.8%)",null,null
75,53959 (4.2%) 21231 (1.65%),null,null
76,Query length distribution (cumulative) 0.0 0.2 0.4 0.6 0.8 1.0,null,null
77,"Once we had search sessions and question sessions, and mapping between some of them, we created two datasets. The first, termed SearchAsk dataset, contains search sessions that turned into question sessions. We only kept such sessions that resulted in posting one and only one question for simplicity of analysis later on. In addition, we only kept sessions in which the posted question is ""relevant"" to a previously issued query (if the query and the question share at least one non-stopword, they were considered relevant). By observing that some users actually searched for Yahoo! Answers to navigate to its home page before they posted a question there, we deleted such special navigational clicks and corresponding queries from the user action sequences. The second dataset, termed SearchOnly dataset, consists of search sessions that did not turn into question sessions. In both datasets, we only kept sessions for users that posted at least once in Yahoo! Answers, since these users are aware of the site and know how to post a question there, thus removing the potential investment of effort for newcomers to join the site, and filtering our the users that simply do not know where to ask questions.",null,null
78,"Table 1 reports the statistics of the datasets we obtained. As shown in the table, 95.8% of all the search sessions are SearchOnly sessions, while SearchAsk sessions account for 1.65%. Despite the sparsity of the SearchAsk sessions, we still believe that understanding how searchers become askers in such sessions can be helpful for improving the search experience of these users and perhaps more users. Indeed, the two datasets allow us to investigate the differences between sessions in which users posted a question following attempted searches, mainly due to search failure or searcher frustration, and sessions in which users that have experience of asking questions on Yahoo! Answers did not bother or did not need to ask questions at that time. Recall, that the users in these datasets satisfy two conditions: (1) having clicked at least a Yahoo! Answers question page within this month; and (2) having asked at least one question on Yahoo! Answers within the month. Yet, we believe that such users still represent the general, though somewhat experienced, web searchers.",null,null
79,3. FROM SEARCHING TO ASKING: QUERY AND BEHAVIOR ANALYSIS,null,null
80,"As a first step, we study the characteristics of queries leading to a question post on Yahoo! Answers (Section 3.1), and the characteristics of searcher behavior before question asking (Section 3.2).",null,null
81,3.1 Characteristics of Queries leading to Questions,null,null
82,"The first interesting question is which queries are more likely to be unsuccessful for automated search, but instead are more amenable to be answered by a CQA site. To get such queries, we examine each SearchAsk session, and",null,null
83,SearchAsk SearchOnly,null,null
84,1,null,null
85,2,null,null
86,5 10 20,null,null
87,50,null,null
88,Query length (number of words),null,null
89,Figure 2: Distribution of query length,null,null
90,Table 2: Statistics of words per query,null,null
91,Avg # Avg # Avg % Avg,null,null
92,words stop- stop- word,null,null
93,words words length,null,null
94,SearchAsk queries 6.5,null,null
95,2.4,null,null
96,28% 5.1,null,null
97,SearchOnly queries 3.4,null,null
98,0.72 11% 6.0,null,null
99,"extract the queries that are issued before the question is posted, and are relevant to the question. We call such queries SearchAsk queries. For comparison, we also extract the queries in each SearchOnly session which are called SearchOnly queries. In the following, we explore how SearchAsk queries are different from SearchOnly queries in terms of length, words, frequency, and results.",null,null
100,Query Length Distribution.,null,null
101,"Figure 2 compares the distribution of query length (in terms of number of words in the query) for the SearchAsk and SearchOnly queries. We can see that SearchAsk queries tend to be longer than SearchOnly queries, as 85% of the SearchOnly queries contain at most 5 words, while about 50% of the SearchAsk queries contain more than 5 words. Therefore, searchers issuing longer queries are more likely to turn to Yahoo! Answers to post a relevant question.",null,null
102,"Table 2 compares the average word length per query and the average number of stopwords for the SearchAsk queries and SearchOnly queries. We can see that, on average, queries turning to questions tend to contain more words (but shorter words) than queries that do not turn to questions. The main reason could be that SearchAsk queries contain more stopwords (which are often short) than SearchOnly queries. Indeed, the percentage of stopwords in SearchAsk queries is over 2.5 times higher than in SearchOnly queries.",null,null
103,803,null,null
104,Query frequency distribution (cumulative) 0.5 0.6 0.7 0.8 0.9 1.0 Search Search Only Ask,null,null
105,Query SERP with url from Yahoo! Answers Query SERP without url from Yahoo! Answers,null,null
106,SearchAsk SearchOnly,null,null
107,1,null,null
108,10 100 1000 10000,null,null
109,Query frequency in 1-month query log,null,null
110,Figure 3: Distribution of query frequency,null,null
111,Query Words Distribution.,null,null
112,"To better understand the difference between the content of SearchAsk queries and SearchOnly queries, we compare their word distributions and show the main difference in Table 3. We can see that SearchOnly queries are more likely to be navigational, e.g., to reach websites like Facebook or YouTube, or to find information related to the searcher's common tasks such as looking up the weather, hunting for coupons, or finding a cooking recipe. In contrast, SearchAsk quries are more likely to start with question words (e.g., `how', `what'), and tend to use more verbose natural language to express the needs of the searchers (e.g., `want', `to', `know') rather than using only keywords.",null,null
113,Query Frequency Distribution.,null,null
114,"To verify the hypothesis from the above word distribution analysis that SearchAsk queries are more likely to be unique, we compute the frequency4 of SearchAsk queries and SearchOnly queries in our 1-month query log. Figure 3 shows the results. We can see that over 90% of SearchAsk queries are tail (actually unique) queries, indicating the variety of the needs of searchers and the ways to express them. In contrast, SearchOnly queries contain more popular queries, e.g., around 20% of SearchOnly queries occur in more than 100 search sessions.",null,null
115,Query Results Distribution.,null,null
116,"To better understand user needs behind SearchAsk queries, we further examine the results returned in their search engine result pages (SERPs). We found a significant difference between SearchAsk and SearchOnly queries based on whether a SERP contains a Yahoo! Answers question page. As shown in Figure 4, a Yahoo! Answers question page occurs in the SERPs for half of the queries that eventually turn to questions, but for only 13% of SearchOnly queries. It is clear that SearchAsk queries are more likely to have a Yahoo! Answers question page in the SERP. This is not surprising. First, having a Yahoo! Answers question page in search results indicates that the query could be relevant to an existing Yahoo! Answers question. Therefore, answers from a human might be more suitable to address the need behind the query, encouraging the searcher to post a question on Yahoo! Answers. Second, more impressions often leads to more clicks. After landing on the Yahoo! Answers",null,null
117,4The frequency of a query in this paper is computed as the number of search sessions containing the query.,null,null
118,0.0,null,null
119,0.2,null,null
120,0.4,null,null
121,0.6,null,null
122,0.8,null,null
123,1.0,null,null
124,Probability,null,null
125,Figure 4: Distribution of query results,null,null
126,"site, the searcher might realize that a community might be able to answer her information need, and try posting a question.",null,null
127,Summary of Query Characteristics.,null,null
128,"As a summary of the above analysis, we conclude that queries that are more likely to fail in search and lead to a question post on Yahoo! Answers tend to be longer, and use more verbose natural language to express the searchers' needs. The needs behind such queries tend to be more unique and complex than those associated with SearchOnly queries.",null,null
129,3.2 Searcher Behavior Before Asking Questions,null,null
130,"To understand how searchers become askers, we analyze the searcher behavior in search sessions, with an associated question posted by the same user on Yahoo! Answers.",null,null
131,Last Action Before Question Asking.,null,null
132,"First, we examine what searchers do right before they start question asking, i.e., we examine the last user action prior to a question being posted. We found that the last search action before question asking is a click on Yahoo! Answers question result in 47.8% of the sessions, a click on other result in 31.2% of the sessions, and a query in 17.4% of the sessions. We notice that in about half of the sessions, the searcher posts a question right after clicking on a Yahoo! Answers question page from the search engine results. There may be several reasons for this. First, such a click indicates that the query is relevant to the clicked question, and therefore it probably carries an information need that would benefit from a human response. Second, when the clicked Yahoo! Answers question page cannot satisfy the search need, it encourages the user to post a new question on Yahoo! Answers. Of course, it is also possible that a searcher had already decided to post a question when seeing the original SERP, and she then clicked on a Yahoo! Answers question result simply to navigate to the Yahoo! Answers site.",null,null
133,Distribution of Clicks.,null,null
134,"To better understand the effects of clicking on a Yahoo! Answers question result on the transformation of searchers into askers, we compute and compare the likelihood of such clicks in SearchAsk and SearchOnly sessions. Figure 5 shows the results. First, 21% of SearchOnly sessions and 81% of SearchAsk sessions contain a Yahoo! Answers question page in the SERPs. Next, after seeing a Yahoo! Answers question page in the SERPs, 81% of the searchers who turned to",null,null
135,804,null,null
136,Words First words Content words,null,null
137,Words First words Content words,null,null
138,Table 3: Frequent words in SearchAsk queries and SearchOnly queries More likely in SearchAsk queries,null,null
139,"to, a, be, i, how, do, my, can, what, on, in, the, for, have, get, with, you, if, yahoo, it how, what, can, be, why, i, do, my, where, yahoo, if, when, 0000, a, will, 00, best, who, which, should yahoo, 00, use, 0, work, song, old, help, make, need, like, change, year, good, long, mail, answer, email, want, know",null,null
140,"More likely in SearchOnly queries facebook, youtube, google, lyric, craigslist, free, online, new, bank, game, map, ebay, county, porn, tube, coupon, recipe, home, city, park facebook, youtube, google, craigslist, ebay, the, you, gmail, casey, walmart, amazon, *rnrd, justin, facebook.com, mapquest, netflix, face, fb, selena, home facebook, youtube, google, craigslist, lyric, free, bank, map, ebay, online, county, porn, tube, coupon, recipe, anthony, weather, login, park, ca",null,null
141,Search Search Only Ask,null,null
142,1+ clicks on Yahoo! Answers question result session SERPs with url from Yahoo! Answers session SERPs without url from Yahoo! Answers,null,null
143,1/1 Begin,null,null
144,0.12 / 0.13,null,null
145,0.15 / 0.03,null,null
146,Click ques result,null,null
147,0.47 / 0.21,null,null
148,0.25 / 0.40,null,null
149,0.06 / 0.01,null,null
150,0.14 / 0.23,null,null
151,Query,null,null
152,0.04 / 0.05,null,null
153,Ask /End,null,null
154,0.0,null,null
155,0.2,null,null
156,0.4,null,null
157,0.6,null,null
158,0.8,null,null
159,1.0,null,null
160,Probability,null,null
161,Figure 5: Distribution of clicks,null,null
162,0.30 / 0.23,null,null
163,0.51 / 0.43,null,null
164,0.48 / 0.66,null,null
165,Click other result,null,null
166,0.30 / 0.25,null,null
167,0.09 / 0.29,null,null
168,"askers had clicked on a Yahoo! Answers question result while 19% of them hadn't; in contrast, 43% of the searchers in SearchOnly sessions seeing a Yahoo! Answers question result clicked on it while 57% of them didn't. Therefore, users in SearchAsk sessions are about twice as likely as in SearchOnly sessions to click on a Yahoo! Answers question page in the search results once seeing it. This indicates that searchers are more likely to post a question once clicking on a Yahoo! Answers question result.",null,null
169,Transitions between Actions.,null,null
170,"To better understand searcher actions, we further compute the probability of transitions between actions in SearchAsk and SearchOnly sessions respectively, and compare them in Figure 6. The transition probability between two actions ai and aj in SearchAsk (SearchOnly) sessions is computed using Maximum Likelihood estimation: P (ai, aj) ,"" Nai,aj /Nai , where Nai,aj is the number of transitions from action ai to action aj in all SearchAsk (SearchOnly) sessions, and Nai "","" ak Nai,ak . SearchAsk transition probabilities are shown in red before the slash symbol, while SearchOnly transition probabilities are shown in black after the slash symbol. If we look at the transitions for SearchOnly sessions from the figure, we can see that after issuing a query, the searcher is very likely to click on other result, then with perhaps more queries and clicks on other result, and then ends the session. Clicking on a Yahoo! Answers question result is very unlikely. However, in SearchAsk sessions, the searcher has a higher probability on clicking a Yahoo! Answers question result. After the click, the searcher in SearchAsk sessions would post a question on Yahoo! Answers for around half of the time.""",null,null
171,"Figure 6: Transition probabilities for actions in SearchAsk (in red, before the slash symbol) and SearchOnly (in black, after the slash symbol) sessions. Note that two other actions (Pagination and Click interface) are ignored for simplicity.",null,null
172,Action Sequences Before Question Asking.,null,null
173,"To better understand how searchers become askers, we examine the user action sequences in SearchAsk sessions before the question post, and compare them with action sequences in SearchOnly sessions. Table 4 shows a sample of top frequent user action sequences. The top frequent path in SearchOnly sessions indicates navigational needs of the searchers, i.e., they issue a query, click on a search result and leave the session. Such navigational cases account for 30% of total SearchOnly sessions. In contrast, the top frequent path in SearchAsk sessions indicates more ""social"" needs of the searchers, i.e., they issue a query, click on a search result of Yahoo! Answers question page, and then ask a question on Yahoo! Answers. Yet, the path distribution is more balanced for SearchAsk sessions. Moreover, clicks on Yahoo! Answers question results are common in the paths.",null,null
174,Session Size Distribution.,null,null
175,"Finally, we compare the distribution of session sizes for SearchAsk and SearchOnly sessions. Session size can be measured in several ways, e.g., by the number of (unique) queries issued by the searcher in the session, by the number of actions performed in the session, or by the duration that the session lasts. We use the first option in this paper. The results are shown in Figure 7. While only one query",null,null
176,805,null,null
177,Table 4: Top frequent user action sequences in,null,null
178,SearchAsk sessions and SearchOnly sessions (B: Be-,null,null
179,"gin a session, Q: Query, Cqr: Click on a Yahoo! Answers question result, Cor: Click on other result, A: Ask a question, E: End a session)",null,null
180,SearchAsk sessions Distribution,null,null
181,B Q Cqr A B Q Cor A B Q Q Cqr A BQA,null,null
182,10% 3.8% 3.3% 2.8%,null,null
183,B Q Cor Q Cqr A,null,null
184,2.0%,null,null
185,SearchOnly sessions Distribution,null,null
186,B Q Cor E B Q Cor Q Cor E BQE,null,null
187,30.2% 7.1% 6.1%,null,null
188,B Q Cor Cor E B Q Q Cor E,null,null
189,3.6% 3.6%,null,null
190,Table 5: Statistics of length difference between a,null,null
191,query and its associated question (number of words).,null,null
192,Median Avg Max,null,null
193,|question| - |query| 42,null,null
194,66 1431,null,null
195,|subject| - |query|,null,null
196,3,null,null
197,4 27,null,null
198,|content| - |query| 31,null,null
199,55 1428,null,null
200,Table 6: Overlap of content words (CW) between a query and its associated question.,null,null
201,"CW?  CWquery CW? , CWquery CW?  CWquery CW?  CWquery CW?  CWquery",null,null
202,"?,question",null,null
203,31.4% 1.8% 0.7% 66.1%,null,null
204,"?,subject",null,null
205,14.6% 6.2% 3.7% 75.5%,null,null
206,"?,content",null,null
207,14% 0.4% 17.1% 68.5%,null,null
208,Sessioin size distribution (cumulative) 0.0 0.2 0.4 0.6 0.8 1.0,null,null
209,SearchAsk SearchOnly,null,null
210,1,null,null
211,2 3 5 10 20,null,null
212,50,null,null
213,Session size (number of unique queries),null,null
214,Figure 7: Distribution of session size,null,null
215,"is issued in the half of SearchOnly sessions, at least three different queries are issued in the half of SearchAsk sessions. The average session size is 2.5 for SearchOnly sessions and 3.8 for SearchAsk sessions. This shows that searchers tend to issue more queries in SearchAsk sessions, possibly because SearchOnly sessions contain more navigational needs, while SearchAsk sessions are associated with more difficult or complex needs, and thus require more effort in finding answers.",null,null
216,4. QUERIES VS. QUESTIONS: CONTENT ANALYSIS,null,null
217,"After discovering the unique attributes of queries that lead to asking a question, we next want to understand better the process of turning a search session, as captured by a query, into a question posted on Yahoo! Answers.",null,null
218,"The most expected difference between queries and questions is their length. Table 5 shows these differences. From the table we can see that a question has 66 more words than its associated query on average. This indicates two things: first, as expected, questions are much more verbose, being natural language expressions, compared to the concise queries; second, since Yahoo! Answers questions are not",null,null
219,Figure 8: Word distributions over question words,null,null
220,"limited in length, additional knowledge of the problem to be solved is added. Interestingly, the subject of the question is very close in length to the query, which shows that searchers still think in search-style writing for the subject. However, the content part of the question is significantly longer, and much more information is added in this question part.",null,null
221,"We next look at word distribution differences, since they may point at the lexical gap between queries leading to questions and their associated posted questions. Figure 8 depicts the word occurrence distribution over word ranking by frequency for search-related questions. The most notable difference between the two distributions is that questions tend to be more personal and verbose, as captured by the abundant usage of the pronouns such as `I', `me', `it' and `this', connectives such as `but', `because', `recently', and `just', as well as sentiment indicators such as `help', `please', and `thanks'. Queries, on the other hand, tend to focus more on the things or actions that are searched for, with content words like `best', `free', `download' and `games' as well as question words like `how', `why' and `what' occurring more frequently than in the associated questions corpus. Interestingly, one to four digit figures, such as car model years,",null,null
222,806,null,null
223,Table 7: Examples showing semantics difference between the query and the question.,null,null
224,ID Type of Query,null,null
225,"Question (Category, Subject, and Content)",null,null
226,context,null,null
227,added,null,null
228,1 N/A,null,null
229,what to serve with Food & Drink>Cooking & Recipes,null,null
230,chicken salad,null,null
231,what can you serve with chicken salad?,null,null
232,2 thought best nba players with- Sports>Basketball,null,null
233,out a championship,null,null
234,Greatest NBA players to never win championship?,null,null
235,"Patrcik ewing, reggie miller, charles barkley, karl malone? Who else?",null,null
236,3 task,null,null
237,pt cruiser ac fix,null,null
238,Cars & Transportation>Car Makes>Chrysler,null,null
239,how much does it cost to fix an ac system in a pt cruiser?,null,null
240,4 task,null,null
241,"solve n^2-2n-3,5000 Education & Reference>Homework Help",null,null
242,"Algebra question, Need Help Pls!!!!?",null,null
243,An owner of a key rings company found that the profit earned (in thousands of,null,null
244,"dollars) per day by selling n number of key rings is given by n^2 - 2n - 3, where",null,null
245,n is the number of key rings in thousands. Find the number of key rings sold on,null,null
246,a particular day when the total profit is $5000. Thanx,null,null
247,5 limit,null,null
248,chocolate croissant Dining Out>United States>San Jose,null,null
249,menlo park,null,null
250,"Where can I get a good Chocolate Croissant near Menlo Park, CA?",null,null
251,"Something with thick, dark chocolate? And please, don't say La Boulanger.",null,null
252,"6 situation, chicago fried chicken Dining Out>United States>Chicago",null,null
253,task,null,null
254,Where can I get really good fried chicken in the Lakeview area in,null,null
255,Chicago?,null,null
256,I really want fried chicken after watchin a special on TV. But I cant find any place,null,null
257,near me that has decent priced chicken thats not fast food and is homemade and,null,null
258,delicious. Any one know of a place?,null,null
259,"7 situation, douglas az",null,null
260,Education & Reference>Higher Education (University +),null,null
261,task,null,null
262,Radiology schools in Arizona?,null,null
263,"Does any one know any schools in az that offer radiology degree programs, I",null,null
264,moved to Douglas az and don't know any schools near to study radiology. If any,null,null
265,one can help that would be great :),null,null
266,"8 attribute, how many bottles to Pregnancy & Parenting>Newborn & Baby",null,null
267,"situation, buy for a newborn",null,null
268,How many bottles should I purchase for my new baby? And what,null,null
269,task,null,null
270,brand is best?,null,null
271,I am 9 mo. pregnant and still need to buy bottles. I will be trying to breast,null,null
272,feed but I am unsure of how many bottles and what sizes I should buy. Is there,null,null
273,anything else I will need for feeding and what brand do you recommend? Thanks!,null,null
274,"also appear more in question-related queries than in their associated questions, probably since they capture much of the essence of the target information need.",null,null
275,"To further understand the semantic difference between composing a query and its related question, we measured the distribution of query-question pairs in which the same words are used for both query and question, the pairs in which one is included in the other, and those pairs in which each contains words that do not occur in the other. Table 6 presents these statistics, while Table 7 provides examples of such pairs, annotated with the type of context added when switching from query to question, as been classified by [23], i.e., task, situation, attribute, limit, and thought.",null,null
276,"Some interesting question composition patterns are evident from this analysis. First, in the majority of pairs (66%), both queries and questions contain unique words that do not occur in the other. This is somewhat surprising, since we would expect more complete inclusion of the query terms in the question. However, it seems that with the freedom of writing a free text question, searchers tend to rephrase some of the terms they used in their queries. For example, abbreviations and short terms are turned into their more complete forms, e.g. `AZ' into `Arizona' and `newborn' into `new baby' (see example 7 and 8 in Table 7). In addition, while 31% of the pairs do show complete inclusion of the query terms in the question, many times the query terms do not all appear in the question's subject or content, but spread in both question parts. Table 7 shows that most of the extensions of",null,null
277,"the query into a question include additional details that are related to the search task. Yet, many times details of the personal situation are added, such as the state of mind, e.g. ""after watching a special on TV "" (example 6 in Table 7).",null,null
278,"One interesting future research is to automatically generate questions from queries [25][26]. However, adding context information to the question, such as the situation or limit is a difficult challenge. Still, expanding the query expression to an explicit question form may be possible for many cases, e.g. examples 1 and 3 in Table 7.",null,null
279,5. ASKING AFTER SEARCHING: QUESTION ANALYSIS,null,null
280,"As our final analysis, we are interested in discovering unique activity patterns in Yahoo! Answers that searchers posting a question have, compared to typical asker behavior in Yahoo! Answers. Specifically, we first examine the differences in lexicon, that is whether different words are used when composing a question (Section 5.1). Then, we analyze the difference in asker behavior after posting such questions, in terms of ""traditional"" CQA activities (Section 5.2).",null,null
281,5.1 Characterizing Questions Posted after a Search Session,null,null
282,"As expected, we find that there is a large difference between the word distribution for the corpus of all questions posted in June 2011 and the distribution of the corpus of",null,null
283,807,null,null
284,Table 8: Categories with largest differences in,null,null
285,assignment probability between questions coming,null,null
286,from search and general questions,null,null
287,Categories more likely for Categories more likely for,null,null
288,general questions,null,null
289,questions following search,null,null
290,Polls & Surveys (Entertain- Maintenance & Repairs,null,null
291,ment & Music),null,null
292,(Cars),null,null
293,Singles & Dating,null,null
294,Law & Ethics,null,null
295,Religion & Spirituality,null,null
296,Dogs (Pets),null,null
297,Politics,null,null
298,Pregnancy,null,null
299,Friends,null,null
300,Maintenance & Repairs,null,null
301,(Home & Garden),null,null
302,Mathematics,null,null
303,Renting & Real Estate,null,null
304,Diet & Fitness,null,null
305,Accounts & Passwords,null,null
306,"Lesbian, Gay, Bisexual, and Other - Yahoo! Mail",null,null
307,Transgendered,null,null
308,Other - Beauty & Style,null,null
309,Military,null,null
310,Basketball,null,null
311,Problems with Service,null,null
312,Baby Names,null,null
313,Garden & Landscape,null,null
314,Adolescent,null,null
315,Cooking & Recipes,null,null
316,"questions posted by searchers. In addition, the entropy of generating a word from the search-related question corpus is much lower, showing a more focused vocabulary. But what are the reasons for this large difference? It turned out to be mainly topical.",null,null
317,"To measure this topical difference between the two types of questions, we looked at the distribution of categories to which the questions in the two compared corpora are assigned. Table 8 shows the categories with largest differences in assignment probability, those that are preferred more in the general question corpus and in the search-related question corpus respectively. These lists show that searchers tend to ask informational questions [14] to get fact- or adviceoriented answers, such as how to fix the car or maintain one's garden, how to bake cookies, but also questions related to Yahoo products, such as Yahoo! Mail. On the other hand, regular askers are more likely to ask conversational questions [14] with a social flavor, such as discussions around music or sports events, politics and religions, and opinions on possible baby names. We manually labeled 100 questions randomly sampled from the search-related question corpus, and found none are conversational, showing a very different distribution compared to that 38% of Yahoo! Answers questions are conversational as reported in [14]. We conjecture that this is because searchers usually turn to search engines to find information instead of starting conversations. Another kind of questions that are less likely searched first over the web are personal questions, in which the asker is interested in adding very personal details. These include topics such as diet and fitness advices, dating and style opinions. Finally, there are questions that are too complex, for which the asker knows the answer cannot be found on the web. A good example are Math questions, such as example 4 in Table 7.",null,null
318,"To further investigate the differences between the two question types, we removed the strong bias caused by the different category distributions within the two corpora by sampling questions from the general question corpus based on the category distribution of the search-related question corpus. By comparing the word distribution between the sampled corpus and the search-related question corpus, we found that hardly no topical differences remained. That is, the topical variation in the two corpora is more or less completely captured by the level of assigned categories, without",null,null
319,Table 9: Statistics of words in SearchAsk questions,null,null
320,and sampled general questions,null,null
321,Avg. corpus,null,null
322,Sampled general SearchAsk,null,null
323,statistics,null,null
324,questions,null,null
325,questions,null,null
326,# words,null,null
327,78.3,null,null
328,73.7,null,null
329,# words per sentence,null,null
330,13.5,null,null
331,13.7,null,null
332,# sentences,null,null
333,5.8,null,null
334,5.4,null,null
335,% stopwords,null,null
336,66.3,null,null
337,65.0,null,null
338,word length,null,null
339,4.22,null,null
340,4.17,null,null
341,Table 10: Statistics about user follow-up activities around their posted questions.,null,null
342,SearchAsk Ask Search,null,null
343,Avg duration,null,null
344,30h,null,null
345,32h 19.4m,null,null
346,Median duration,null,null
347,2.2h,null,null
348,3.7h 11.6m,null,null
349,Avg #actions,null,null
350,6.41,null,null
351,7.45,null,null
352,-,null,null
353,Median #actions,null,null
354,5,null,null
355,5,null,null
356,-,null,null
357,"more subtle topical differences evident. Still, there may be stylish variations in question composition between searchers and typical askers. Table 9 provides the stylish statistics for the general-sampled and search-related question corpora. The significant difference between the two corpora is the number of words per question: for the same topics, general questions contain 6% more words compared to searchrelated questions. Yet, interestingly, this attribute is due to more sentences that are written on average per general question, while if we look at the number of words per sentence, we see that surprisingly search-related questions have slightly more words in each sentence. This could be related to more information-focused nature of the questions posted after a search session, and suggests further investigation.",null,null
358,5.2 Asker Follow-up Activity after a Search,null,null
359,"As our final question behavior analysis, we wanted to test whether a searcher interacts more or less with Yahoo! Answers after posting the question. To that end, we measured both the number of actions that both searchers and regular askers perform around a specific question they posted, as well as the duration of this set of actions. Follow-up actions after a posted question include: browsing the question page (e.g. checking for new answers), adding more details to the question, selecting a best answer, reporting abusive answers, voting for answers, and deleting the question.",null,null
360,"Table 10 provides the average statistics of these actions, while Figures 9 and 10 depict the distribution of number of actions and their duration for searchers and regular askers. From the table we can see that searchers perform fewer yet similar number of actions as typical askers do, but in a much shorter duration. As can be seen by Figure 9, in terms of number of actions, the difference of about one more action on average for regular askers is small though constant. For the duration of the interaction, regular askers spend about 7% more time on average around the question, but looking at the median, the difference is substantially larger, with half the searchers spending 2.2 hours or less while the typical askers tend to spend about 68% more time, or 3.7 hours, at the median. As an interesting comparison, we also measured the average time the searchers spent searching before asking questions, to show the substantial difference between",null,null
361,808,null,null
362,Cumulative distribution function 0.0 0.2 0.4 0.6 0.8 1.0,null,null
363,SearchAsk Ask,null,null
364,1,null,null
365,2,null,null
366,5 10 20,null,null
367,50,null,null
368,# of user actions on question after it is posted,null,null
369,Figure 9: Count of user actions in questions,null,null
370,1.0,null,null
371,0.8,null,null
372,0.6,null,null
373,Cumulative distribution function,null,null
374,0.4,null,null
375,0.2,null,null
376,0.0,null,null
377,SearchAsk Ask Search,null,null
378,0,null,null
379,10m 1h,null,null
380,1d,null,null
381,1m,null,null
382,Duration of user actions on the question after it is posted,null,null
383,Figure 10: Duration of user actions in questions,null,null
384,"an interactive search session and an offline asking session, a difference that is clear to the searchers, since they are willing to spend several hours waiting for an answer to arrive, compared to a few minutes actively searching.",null,null
385,"In summary, we showed that searchers are expecting a faster response time for their questions, which often aim to address practical problem solving tasks. On the other hand, general Yahoo! Answers askers are willing to put more effort in following up their questions. One possible reason for this behavior is that Yahoo! Answers site is often viewed by users from a more social perspective, as indicated by many users asking socially-focused (e.g., conversational) questions.",null,null
386,6. RELATED WORK,null,null
387,"As we study the transformation of unsatisfied searches into questions posted on a popular CQA site, our work is related to the work on query log analysis, searcher behavior and satisfaction prediction, and CQA question analysis.",null,null
388,"On the search side, significant research has been done on analysis of queries and searcher behavior based on query logs. For example, understanding query intent and user goals has attracted much research effort [22][6]. Difficult queries [9][8], long and tail queries [4][5], and question-like",null,null
389,"queries [21] have also received special research attention. Besides, searcher satisfaction and frustration [11][15][2][16] has also been actively studied, which utilized query log information for satisfaction prediction, such as relevance measures, as well as user behavior during the search session, including mouse clicks and time spent between user actions.",null,null
390,"Donato et al. [10] identified the research missions that often associate with complex information needs and require collecting information from many pages. In our work we focused on studying the types of queries that arguably are difficult for a web search engine to satisfy, often require human to answer [19][20][18], and thus could be better handled by CQA sites. Liu et al. [18] argued that some of these needs can be satisfied with existing answers from CQA archives by harnessing the unique structure of such archives for detecting web searcher satisfaction. Our work in this paper further observed that many searchers not satisfied with search results finally posted a related question on a CQA site, which inspired our analysis of how searchers become askers.",null,null
391,"White and Dumais [24][12] studied search engine switching behavior and developed models to predict the switching and its rationale. Although different types of searchers are focused on (they focused on searchers who turn to another search engine and issue more queries, while we focused on searchers who turn to CQA sites and post questions), we are both interested in characterizing the types of queries and searcher behavior that lead to the switchings. Our analysis shows both similar (e.g. longer sessions are more likely to involve a switch) and different characteristics (e.g. different last action before switching) compared to their study.",null,null
392,"On the CQA side, there is also research effort devoted to question analysis, e.g. distinguishing conversational and informational questions [14], identifying high quality questions [3], and investigating the effects of contexts in questions on answer quality [23]. In our work, we use their classification of contextual factors to analyze the semantic difference between the query and question posted by the same user for the same need. There is also some previous work related to asker behavior analysis. For example, Adamic et al. [1] analyzed the content properties and user interaction patterns across different Yahoo! Answers categories. Gyongyi et al. [13] studied several aspects of user behavior in Yahoo! Answers, including users' activity levels, interests, and reputation. Yet, they did not study the effort that askers spend in tracing their posted questions as we studied in this paper.",null,null
393,7. CONCLUSIONS,null,null
394,"Web search needs are becoming increasingly sophisticated, and the expectations have grown accordingly. As a result, quite a few search sessions end in posting a question in a Community Question Answering service, as the searcher realizes that such a service could better answer her need.",null,null
395,"This work studies the unique properties of SearchAsk sessions: search sessions that turn into question composition. To the best of our knowledge, this paper presents the first large-scale analysis of the user transition from searching to asking. What makes our work unique is the study of the explicit connection between the search query and the corresponding question from the same user for the same need. It provides insights into some specific needs that searchers try to express on search engines, yet are not satisfied by search results, and turn to human answerers instead. We analyzed the various aspects of SearchAsk sessions, includ-",null,null
396,809,null,null
397,"ing the differences between general search-engine queries and those belonging to a SearchAsk session, the transformation of a query into a natural language question and the question composition patterns, as well as other asking behavior of searchers, compared to general askers in a CQA service.",null,null
398,"Our findings may contribute both to search-engine optimization, as well as to better user experience in CQA sites. For example, we found out that searchers are not as patient as regular askers when waiting for answers to their questions. This finding may influence CQA sites to promote questions coming from searchers, if they want to retain their engagement. As another example, our analysis of the transitions between user actions in SearchAsk sessions, and especially the fact that question asking is typically preceded by viewing a CQA page, may help search engines. They might decide to detect such cases and explicitly promote the option of asking a question to the searcher, even before she resorts into doing it on her own. Furthermore, as this paper demonstrates, modeling the transformation of a query meant for an automated search engine into a fully specified question meant for human, provides a valuable tool for query intent and satisfaction analysis.",null,null
399,"In future work, we intend to develop some of the directions mentioned above. One of the most intriguing ones in our view is for search engines to automatically trigger a dialog for posting questions in the right CQA forum, whenever a SearchAsk need is detected. This is just one application made possible by our study, which lays a foundation for more effective integration of automated web search and social information seeking.",null,null
400,8. ACKNOWLEDGMENTS,null,null
401,This work was supported by the National Science Foundation grant IIS-1018321 and by the Yahoo! Faculty Research and Engagement Program. The authors also would like to thank Elad Yom-Tov for the helpful discussion and the anonymous reviewers for their valuable comments.,null,null
402,9. REFERENCES,null,null
403,"[1] L. A. Adamic, J. Zhang, E. Bakshy, and M. S. Ackerman. Knowledge sharing and yahoo! answers: everyone knows something. In WWW, 2008.",null,null
404,"[2] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: a game for modeling different types of web search success using interaction data. In SIGIR, pages 345­354, 2011.",null,null
405,"[3] E. Agichtein, C. Castillo, D. Donato, A. Gionis, and G. Mishne. Finding high-quality content in social media. In WSDM, pages 183­194, 2008.",null,null
406,"[4] P. Bailey, R. W. White, H. Liu, and G. Kumaran. Mining historic query trails to label long and rare search engine queries. ACM Trans. Web, 4:15:1­15:27, September 2010.",null,null
407,"[5] M. Bendersky and W. B. Croft. Analysis of long queries in a large scale search log. In WSCD, 2009.",null,null
408,"[6] A. Broder. A taxonomy of web search. SIGIR Forum, 36:3­10, September 2002.",null,null
409,"[7] G. Buscher, R. W. White, S. Dumais, and J. Huang. Large-scale analysis of individual and task differences in search result page examination strategies. In WSDM, pages 373­382, 2012.",null,null
410,"[8] D. Carmel, E. Yom-Tov, A. Darlow, and D. Pelleg. What makes a query difficult? In SIGIR, 2006.",null,null
411,"[9] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In SIGIR, 2002.",null,null
412,"[10] D. Donato, F. Bonchi, T. Chi, and Y. Maarek. Do you want to take notes?: identifying research missions in yahoo! search pad. In WWW, pages 321­330, 2010.",null,null
413,"[11] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR, pages 34­41, 2010.",null,null
414,"[12] Q. Guo, R. W. White, Y. Zhang, B. Anderson, and S. T. Dumais. Why searchers switch: understanding and predicting engine switching rationales. In SIGIR, pages 335­344, 2011.",null,null
415,"[13] Z. Gyongyi, G. Koutrika, J. Pedersen, and H. Garcia-Molina. Questioning yahoo! answers. Evolution, 2008.",null,null
416,"[14] F. M. Harper, D. Moy, and J. A. Konstan. Facts or friends?: distinguishing informational and conversational questions in social q&a sites. In CHI, pages 759­768, 2009.",null,null
417,"[15] A. Hassan, R. Jones, and K. L. Klinkner. Beyond dcg: user behavior as a predictor of a successful search. In WSDM, pages 221­230, 2010.",null,null
418,"[16] S. B. Huffman and M. Hochster. How well does result relevance predict session satisfaction? In SIGIR, 2007.",null,null
419,"[17] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM, 2008.",null,null
420,"[18] Q. Liu, E. Agichtein, G. Dror, E. Gabrilovich, Y. Maarek, D. Pelleg, and I. Szpektor. Predicting web searcher satisfaction with existing community-based answers. In SIGIR, pages 415­424, 2011.",null,null
421,"[19] M. R. Morris, J. Teevan, and K. Panovich. A comparison of information seeking using search engines and social networks. In ICWSM, 2010.",null,null
422,"[20] M. R. Morris, J. Teevan, and K. Panovich. What do people ask their social networks, and why?: a survey study of status message q&a behavior. In CHI, 2010.",null,null
423,"[21] B. Pang and R. Kumar. Search in the lost sense of ""query"": question formulation in web search queries and its temporal changes. In HLT, 2011.",null,null
424,"[22] D. E. Rose and D. Levinson. Understanding user goals in web search. In WWW, pages 13­19, 2004.",null,null
425,"[23] S. Suzuki, S. Nakayama, and H. Joho. Formulating effective questions for community-based question answering. In SIGIR, pages 1261­1262, 2011.",null,null
426,"[24] R. W. White and S. T. Dumais. Characterizing and predicting search engine switching behavior. In CIKM, pages 87­96. ACM, 2009.",null,null
427,"[25] S. Zhao, H. Wang, C. Li, T. Liu, and Y. Guan. Automatically generating questions from queries for community-based question answering. In IJCNLP, pages 929­937, 2011.",null,null
428,"[26] Z. Zheng, X. Si, E. Y. Chang, and X. Zhu. K2q: Generating natural language questions from keywords with user refinements. In IJCNLP, 2011.",null,null
429,810,null,null
430,,null,null

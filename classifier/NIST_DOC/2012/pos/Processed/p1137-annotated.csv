,sentence,label,data
,,,
0,Rewarding Term Location Information to Enhance Probabilistic Information Retrieval,null,null
,,,
1,"Jiashu Zhao1, Jimmy Xiangji Huang2, Shicheng Wu2",null,null
,,,
2,"Information Retrieval and Knowledge Management Research Lab 1Department of Computer Science & Engineering, 2School of Information Technology",null,null
,,,
3,"York University, Toronto, Canada",null,null
,,,
4,"jessie@cse.yorku.ca, {jhuang, scwu}@yorku.ca",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"We investigate the effect of rewarding terms according to their locations in documents for probabilistic information retrieval. The intuition behind our approach is that a large amount of authors would summarize their ideas in some particular parts of documents. In this paper, we focus on the beginning part of documents. Several shape functions are defined to simulate the influence of term location information. We propose a Reward Term Retrieval model that combines the reward terms' information with BM25 to enhance probabilistic information retrieval performance.",null,null
,,,
7,Categories and Subject Descriptors,null,null
,,,
8,H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval,null,null
,,,
9,Keywords,null,null
,,,
10,"BM25-RT, Influence Shape Function",null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"Using document semantic information is usually regarded as an effective approach for improving Information Retrieval (IR) performance. In this paper, we propose to reward terms according to their location information in documents.",null,null
,,,
13,"Human beings do not treat each word equally when reading a document. People pay more attention on certain parts of the document. This is due to the fact that many authors would summarize their ideas in some particular locations, e.g. the beginning and/or the end. It particularly fits articles that have abstract and introduction at the beginning and conclude the summary in the end. In this paper, we focus on the situation that the beginning of the documents should be rewarded during retrieval.",null,null
,,,
14,"Many researchers have studied using semantic information in some particular domains in IR. For example, Ontology Web Language and Information Retrieval (OWLIR) system [4] is designed for Semantic Web documents, using several techniques such as information extraction, annotation and inference. Wang and Li [5] investigated semantically annotated Wikipedia XML corpus to improve retrieval performance. Zhao and Callan [6] utilized query structure expansion to enhance structured retrieval in a question answering (QA) application. Gao and Toutanova [1] learned semantic models from large amounts of query-title pairs con-",null,null
,,,
15,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
16,Influence,null,null
,,,
17,1,null,null
,,,
18,Cosine,null,null
,,,
19,0.8,null,null
,,,
20,Linear,null,null
,,,
21,Parabolic,null,null
,,,
22,0.6,null,null
,,,
23,0.4,null,null
,,,
24,0.2,null,null
,,,
25,0,null,null
,,,
26,0,null,null
,,,
27,20,null,null
,,,
28,40,null,null
,,,
29,60,null,null
,,,
30,80,null,null
,,,
31,100,null,null
,,,
32,Position in Document,null,null
,,,
33,Figure 1: Influence Shape Functions,null,null
,,,
34,"structed from clickthrough data. Compared with the above approaches, our approach does not require any domain-specific knowledge.",null,null
,,,
35,"For structured data, different fields could be given different weights. Robertson and Zaragoza [3] proposed an extension of BM25 to weight multiple fields separately, such as title text, body text, and anchor text. However, each part of a document is treated equally for unstructured documents. Compared to their approach, our approach rewards terms smoothly according to their locations in documents, and doesn't incorporate any field information.",null,null
,,,
36,"The remainder of this paper is organized as follows. In Section 2, we propose several Influence Shape functions and extend BM25 by rewarding terms according to their locations in documents. Section 3 presents the experimental results and parameter sensitivity in ISF function. Section 4 concludes the findings in this paper and discusses possible future directions.",null,null
,,,
37,2. OUR APPROACH,null,null
,,,
38,"We firstly build the Influence Shape Functions (ISF) that could represent the term influence at the locations closer to the beginning part, and then integrate the influence into BM25 by adding an extra part to term frequency. Under our assumption, when a term occurs closer to the beginning part in the document, it would be more likely to reveal the meaning of the document. Therefore, an ISF should have the highest value at the first position and gradually decreases. We build three functions with different gradients which satisfy this property.",null,null
,,,
39,"p ISFP arabola(p, D) , (  · |D|",null,null
,,,
40,"- 1)2,",null,null
,,,
41,when,null,null
,,,
42,p <  · |D|,null,null
,,,
43,p,null,null
,,,
44,"ISFLinear(p, D) , 1 -",null,null
,,,
45,",  · |D|",null,null
,,,
46,when,null,null
,,,
47,p <  · |D|,null,null
,,,
48,1137,null,null
,,,
49,"·p ISFCosine(p, D) ,"" cos( 2 ·  · |D| ), when p <  · |D|""",null,null
,,,
50,"where p is the position that a term occurs, |D| is the length",null,null
,,,
51,"of the document, and  is a parameter controlling the per-",null,null
,,,
52,centage of the document that should be considered as the,null,null
,,,
53,"beginning part. It ranges from 0 to 1. When  ,"" 0, p is""",null,null
,,,
54,always greater than  · |D|. Therefore the value of each ISF,null,null
,,,
55,is always 0. It becomes a special case that ISFs do not affect,null,null
,,,
56,the retrieval process. The above ISFs are normalized to 0,null,null
,,,
57,"to 1, and their shapes are shown in Figure 1. In this figure,",null,null
,,,
58,the document length |D| is 100 and  is 0.8.,null,null
,,,
59,"In [3], the field information is integrated into BM25 by us-",null,null
,,,
60,ing weighted term frequency on each field. Here we enhance,null,null
,,,
61,the within-document term frequency of a query term by the,null,null
,,,
62,ISFs to reward query terms occurring closer to the begin-,null,null
,,,
63,ning of the document. The within-document query term,null,null
,,,
64,frequency is linearly combined with the accumulation of a,null,null
,,,
65,term's ISF value.,null,null
,,,
66,tf (i),null,null
,,,
67,"tfRT (i, D) ,"" tf (i, D) + ISF (pj, D)""",null,null
,,,
68,-1,null,null
,,,
69,"j,1",null,null
,,,
70,where ISF could adopt any of the above defined shape func-,null,null
,,,
71,"tion. The only parameter that is affected is k1, which controls the non-linear tf function.",null,null
,,,
72,"k1RT (i) , k1 ·",null,null
,,,
73,"DIndex tfRT (i, D) DIndex tf (i, D)",null,null
,,,
74,-2,null,null
,,,
75,The weighting function of BM25 with Rewarded Terms (BM25RT) is as follows.,null,null
,,,
76,"wRT (qi, D)",null,null
,,,
77,",",null,null
,,,
78,(k1RT + 1)  tfRT K + tfRT,null,null
,,,
79, (k3 + 1)  qtf k3 + qtf,null,null
,,,
80,log,null,null
,,,
81,N,null,null
,,,
82,- n + 0.5 n + 0.5,null,null
,,,
83,"where N is the number of documents in the collection, n is the number of documents containing qi, tfRT is withindocument term frequency, qtf is within-query term frequency, dl is the length of the document, avdl is the average document length, nq is the number of query terms, k3 is tuning constant defaulted to be 8, K equals to k1RT  ((1 - b) + b  dl/avdl). In this paper, we set b , 0.75 and k1 ,"" 1.2, which are regarded as default parameter settings in many BM25 applications [2].""",null,null
,,,
84,3. EXPERIMENTAL RESULTS,null,null
,,,
85,"We evaluate the proposed approach on three data sets: WT2G (topic 401-450), TREC8 (topic 401-450), and Blog06 (topic 851-950). The WT2G collection is a 2G size crawl of Web documents. The TREC8 contains 528,155 newswire articles from various sources, such as Financial Times (FT), the Federal Register (FR) etc., which are usually considered as high-quality text data with little noise. The Blog06 collection includes 100,649 blog feeds collected over an 11 week period from December 2005 to February 2006. For all test collections used, each term is stemmed using Porter's English stemmer, and standard English stopwords are removed. We use the MAP and P@10 as measurements in our experiments.",Y,null
,,,
86,"Table 1 shows the result of BM25-RT on the above three data sets using Cosine ISF, Linear ISF, and Parabolic ISF. We compare BM25-RT with BM25, since BM25-RT doesn't incorporate any field or annotation information. Significant improvement is observed on all the data sets.",null,null
,,,
87,Figure 2 shows how the parameter  in ISF affects the retrieval performance. We have discussed in Section 2 that,null,null
,,,
88,BM25,null,null
,,,
89,Cosine Linear Parabola,null,null
,,,
90,WT2G,null,null
,,,
91,MAP,null,null
,,,
92,P@10,null,null
,,,
93,0.2694,null,null
,,,
94,0.44,null,null
,,,
95,0.2812* 0.2814* 0.2787,null,null
,,,
96,0.4500 0.4440 0.4460,null,null
,,,
97,TREC8,Y,null
,,,
98,MAP,null,null
,,,
99,P@10,null,null
,,,
100,0.241,null,null
,,,
101,0.446,null,null
,,,
102,0.2467* 0.2460* 0.2460*,null,null
,,,
103,0.4580 0.4580 0.4560,null,null
,,,
104,Blog06,null,null
,,,
105,MAP,null,null
,,,
106,P@10,null,null
,,,
107,0.2966,null,null
,,,
108,0.6033,null,null
,,,
109,0.2968 0.2874 0.2982*,null,null
,,,
110,0.6173* 0.6200* 0.6180*,null,null
,,,
111,Table 1: MAP and P@10 for BM25-RT with different ISFs (* indicates significant improvements compared to BM25),null,null
,,,
112,MAP P@10,null,null
,,,
113,0.282 0.28,null,null
,,,
114,Cosine Linear Parabola,null,null
,,,
115,0.278,null,null
,,,
116,0.276 0.274,null,null
,,,
117,0.272,null,null
,,,
118,0.27,null,null
,,,
119,0.268,null,null
,,,
120,0.266,null,null
,,,
121,0,null,null
,,,
122,0.2 0.4 0.6 0.8,null,null
,,,
123,1,null,null
,,,
124,(a) MAP,null,null
,,,
125,0.455 0.45,null,null
,,,
126,0.445 0.44,null,null
,,,
127,0.435 0.43,null,null
,,,
128,0.425 0.42 0,null,null
,,,
129,Cosine Linear Parabola,null,null
,,,
130,0.2 0.4 0.6 0.8,null,null
,,,
131,1,null,null
,,,
132,(b) P@10,null,null
,,,
133,"Figure 2: Parameter Sensitivity of  on WT2G for BM25-RT with Cosine ISF, Linear ISF, and Parabola ISF.",null,null
,,,
134,"BM25-RT becomes BM25 when  ,"" 0. When  is increasing, it means BM25-RT takes into more proportion of document into account. We can see that MAP and P@10 of BM25-RT increase at first when  increments from 0. It means the term rewarding technique boosts BM25-RT's performance. As  keeps incrementing, both MAP and P@10 decrease after reaching optimal values.""",null,null
,,,
135,4. CONCLUSIONS AND FUTURE WORK,null,null
,,,
136,"In this paper, we propose a BM25-RT model which tries to reward terms according to their location information during retrieval. A term occurrence is regarded to be more important when it is closer to the beginning of the document. In the future, we plan to discuss more possible document distributions, for instance, to reward the endings of documents.",null,null
,,,
137,5. ACKNOWLEDGEMENTS,null,null
,,,
138,This research is supported in part by the research grant from the Natural Sciences & Engineering Resarch Council (NSREC) of Canada and the Early Researcher/Premier's Research Excellence Award.,null,null
,,,
139,6. REFERENCES,null,null
,,,
140,"[1] J. Gao, K. Toutanova, and W. Yih. Clickthrough-based latent semantic models for web search. In SIGIR'11, pages 675­684, New York, NY, USA, 2011. ACM.",null,null
,,,
141,"[2] S. Robertson, S. Walker, and M. Beaulieu. Okapi at TREC-7: automatic ad hoc, filtering, vlc and interactive track. In TREC'99, pages 253­264, 1999.",null,null
,,,
142,"[3] S. Robertson, H. Zaragoza, and M. Taylor. Simple BM25 extension to multiple weighted fields. In CIKM'04, pages 42­49, New York, NY, USA, 2004. ACM.",null,null
,,,
143,"[4] U. Shah, T. Finin, A. Joshi, R. S. Cost, and J. Matfield. Information retrieval on the semantic web. In CIKM'02, pages 461­468, New York, NY, USA, 2002. ACM.",null,null
,,,
144,"[5] Q. Wang, Q. Li, S. Wang, and X. Du. Exploiting semantic tags in xml retrieval. In INEX'09, pages 133­144, Berlin, 2010.",null,null
,,,
145,"[6] L. Zhao and J. Callan. Effective and efficient structured retrieval. In CIKM'09, pages 1573­1576, New York, NY, USA, 2009. ACM.",null,null
,,,
146,1138,null,null
,,,
147,,null,null

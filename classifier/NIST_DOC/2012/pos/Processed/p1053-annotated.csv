,sentence,label,data
,,,
0,Effect of Written Instructions on Assessor Agreement,null,null
,,,
1,William Webber,null,null
,,,
2,College of Information Studies University of Maryland,null,null
,,,
3,United States of America wew@umd.edu,null,null
,,,
4,Bryan Toth and Marjorie Desamito ,null,null
,,,
5,"Eleanor Roosevelt High School Greenbelt, Maryland",null,null
,,,
6,United States of America bryan.n.toth@gmail.com magicaura2000@yahoo.com,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"Assessors frequently disagree on the topical relevance of documents. How much of this disagreement is due to ambiguity in assessment instructions? We have two assessors assess TREC Legal Track documents for relevance, some to a general topic description, others to detailed assessment guidelines. We find that detailed guidelines lead to no significant increase in agreement amongst assessors or between assessors and the official qrels.",null,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.,null,null
,,,
11,Keywords,null,null
,,,
12,"Retrieval experiment, evaluation, e-discovery",null,null
,,,
13,General Terms,null,null
,,,
14,"Measurement, performance, experimentation",null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"Assessors frequently disagree on the relevance of a document to a topic. Voorhees [2000] finds that TREC adhoc assessors have mutual F1 scores of around 0.6, while Roitblat et al. [2010] report mutual F1 as low as 0.35 for professional e-discovery reviewers. Such low agreement is of serious practical concern in e-discovery, where large-scale, delegated manual review is still widely used. Possible causes of disagreement include assessor error and ambiguity in instructions. We examine whether detailed relevance guidelines increase agreement amongst assessors and with the guideline author, and find no significant increase in either form of agreement",null,null
,,,
17,2. METHODS AND MATERIALS,null,null
,,,
18,"We measure inter-assessor agreement by Cohen's , for which 1 is perfect and 0 is random agreement [Cohen, 1960]. Unassessable documents (too long or misrendered) are ignored. A trial experiment of 75 documents per treatment, on Topic 301 from the TREC 2010 Legal Track, indicated that a sample size of 215 documents per treatment, with even proportions relevant and irrelevant, was required to achieve 80% power for a true  delta of 0.23, being the difference in agreement with official assessments between first and second tercile assessors in the TREC 2009 Legal Track. Work performed while interns at the University of Maryland.",null,null
,,,
19,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
20,Message type,null,null
,,,
21,Messages,null,null
,,,
22,> 1 relevant document,null,null
,,,
23,5,null,null
,,,
24,Relevant appealed,null,null
,,,
25,170,null,null
,,,
26," unappealed

38

Irrelevant appealed

58

 unappealed returned",null,null
,,,
27,32,null,null
,,,
28,  unreturned,null,null
,,,
29,16,null,null
,,,
30,Total,null,null
,,,
31,319,null,null
,,,
32,Documents,null,null
,,,
33,rel irrel unass,null,null
,,,
34,13 12,null,null
,,,
35,0,null,null
,,,
36,170 82,null,null
,,,
37,4,null,null
,,,
38,38 7,null,null
,,,
39,0,null,null
,,,
40,0 78,null,null
,,,
41,1,null,null
,,,
42,0 44,null,null
,,,
43,1,null,null
,,,
44,0 17,null,null
,,,
45,1,null,null
,,,
46,221 240,null,null
,,,
47,7,null,null
,,,
48,"Table 1: Number and types of messages and documents sampled from Topic 204 for re-assessment. A message is classed as ""relevant"" if it contains a single relevant document (body or attachment). Counts of relevant, irrelevant, and unassessable documents are using the official, post-appeal assessments.",null,null
,,,
49,"Topic 204 from the interactive task of the TREC 2009 Legal Track [Hedin et al., 2009] was used for the full experiment. The corpus is the EDRM Enron emails. Whole messages were sampled, but each email body and attachment was separately assessed. A stratified sample was taken, as described in Table 1. The strata were divided evenly and randomly into two batches. Each batch was assessed in document id order, with the parts of a message being assessed sequentially, as in TREC.",Y,null
,,,
50,"At TREC, a senior lawyer called the topic authority develops the topic, writes the detailed guidelines, and adjudicates appeals against first-round assessments. The appeals process for this topic was thorough [Webber, 2011], and the majority of sampled documents were appealed; we regard the assessments as an accurate representation of the topic authority's conception of relevance. We measure agreement for each batch between the two experimental assessors and the official, post-appeal assessments.",Y,null
,,,
51,"The latter two authors of this paper acted as experimental assessors. Each assessor assessed all documents in each batch. For the first batch, assessors were given the 42-word topic statement to guide their assessments; for the second, they received the 5-page detailed relevance guidelines. A third pass was then made, in which the two assessors jointly reviewed both batches, in light of the detailed guidelines, and tried to agree on a conception of relevance.",null,null
,,,
52,3. EXPERIMENTAL RESULTS,null,null
,,,
53,"Table 2 shows the results of our experiments. The provision of detailed assessment guidelines (Batch 2) did not improve agreement, significantly or otherwise, over topic-only instructions (Batch 1), either amongst assessors or with the official assessments, in either the full or the trial experiment. Message-level analysis (in",null,null
,,,
54,1053,null,null
,,,
55,Batch,null,null
,,,
56,1 2 Jnt-1 Jnt-2,null,null
,,,
57,Full experiment,null,null
,,,
58,AvB AvO BvO,null,null
,,,
59,0.519 0.528 0 .992,null,null
,,,
60,0 .950,null,null
,,,
61,0.454ab,null,null
,,,
62,0.555 0.677a 0.665b,null,null
,,,
63,0.710 0.637 0.686 0.674,null,null
,,,
64,Trial experiment,null,null
,,,
65,AvB AvO BvO,null,null
,,,
66,0.229 0.275,null,null
,,,
67,- -,null,null
,,,
68,0.557 0.439,null,null
,,,
69,- -,null,null
,,,
70,0.417 0.294,null,null
,,,
71,- -,null,null
,,,
72,"Table 2: Cohen's  values between official and two experimental assessors, for full and trial experiments, on single-assessed Batch 1 (with topic statement only), single-assessed Batch 2 (with detailed guidelines), and (for full experiment only) jointassessed Batches 1 and 2 (with topic guidelines and consultation between assessors). Columnar value pairs significantly different at  , 0.05 (excepting inter-experimenter joint review) are marked by superscripts.",null,null
,,,
73,Assessors,null,null
,,,
74,A v. B A v. Official B v. Official,null,null
,,,
75,Confidence interval,null,null
,,,
76,"[-0.155, 0.173] [-0.061, 0.263] [-0.211, 0.065]",null,null
,,,
77,"Table 3: Two-tailed 95% normal-approximation confidence intervals on the true change in  between Batch 1 and Batch 2 amongst different assessor pairs, for the full experiment.",null,null
,,,
78,"which a message is relevant if any part of it is relevant) gives similar results. Inter-assessor  values are high for the full experiment's joint assessment, since assessors reached agreement on all save a handful of documents (1 for Batch 1, and 5 for Batch 2). Assessor A's agreement with the official assessments increases significantly under joint review, but this may be due to Assessor A's assessments moving closer to Assessor B's; Assessor A's self-agreement on Batch 1 is 0.399 post-consultation, whereas Assessor B's is 0.739.",null,null
,,,
79,"Table 3 gives 95% confidence intervals on the true change in  values with the addition of assessor guidelines. A substantial improvement is still plausible in agreement between Assessor A and the official assessments, but not for Assessor B and official, nor for inter-assessor agreement.",null,null
,,,
80,"Agreement between the original TREC assessors and the authoritative assessment on the documents examined in our experiment is 0.102 for Batch 1 and 0.024 for Batch 2, much lower than for our experimental assessors; however, this is a biased comparison, since sampling was heavily weighted towards appealed documents. Over the 7,289 documents sampled for assessment at TREC, though, the original assessors achieved a  of 0.320, still well below that of the experimental assessors. The relatively high reliabilty of the assessor is reflected in their high mutual F1 scores (Table 4).",Y,null
,,,
81,"Qualitatively, the experimental assessors described the full experiment topic description by itself as being clear, and the detailed guidelines as being very clear and easy to relate to the documents. As can be seen in Table 2, agreement for this topic was generally higher than for the trial experiment.",null,null
,,,
82,4. DISCUSSION,null,null
,,,
83,"Our initial, seemingly common-sense, hypothesis was that more detailed instructions would raise agreement between assessors and the authoritative conception of relevance, and therefore amongst assessors themselves. The results of this experiment have failed to confirm this hypothesis, or even to show a general trend in this direction. The only significant improvement occurred when Asses-",null,null
,,,
84,Batch,null,null
,,,
85,1 2,null,null
,,,
86,AvB,null,null
,,,
87,0.679 0.769,null,null
,,,
88,AvO,null,null
,,,
89,0.648 0.791,null,null
,,,
90,BvO,null,null
,,,
91,0.828 0.823,null,null
,,,
92,Table 4: Assessor mutual F1 scores for the full experiment.,null,null
,,,
93,"sor A consulted with Assessor B, but that may be attributable to the former's assessments moving closer to the latter's. Indeed, confidence intervals indicate that a substantial increase in agreement is not plausible, except possibly between one assessor and the official view. We can conclude that, for this topic and these assessors, the provision of more detailed assessment guidelines did not lead to any marked increase in assessor reliability.",null,null
,,,
94,"It is also notable that our experimental assessors, who were high school students with no legal training, appear to have produced assessments much more in line with the authoritative conception of relevance than the original TREC assessors, who were legally trained, professional document reviewers.",null,null
,,,
95,"Our findings are not reassuring for the widespread practice of using delegated manual review in e-discovery. If assessors do no better with detailed guidelines than with a general outline of the topic, then there is an irreducible loss of signal in transmitting the relevance conception of an authoritative reviewer into the minds of other assessors. E-discovery practice is moving towards the use of statistical classification tools [Grossman and Cormack, 2011]; it may well be that the lawyer overseeing a case is better able to convey their conception of relevance by personally training a machine classifier, than by instructing delegated human reviewers.",null,null
,,,
96,Acknowledgments.,null,null
,,,
97,"Venkat Rangan of Symantec eDiscovery provided the TIFF images of Enron documents used in the TREC 2009 Legal Track assessments. Maura Grossman and Gord Cormack advised on the choice of TREC topics. This material is based upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",null,null
,,,
98,References,null,null
,,,
99,"Jacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37­46, 1960.",null,null
,,,
100,"Maura R. Grossman and Gordon V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17(3):11:1­ 48, 2011.",null,null
,,,
101,"Bruce Hedin, Stephen Tomlinson, Jason R. Baron, and Douglas W. Oard. Overview of the TREC 2009 legal track. In Ellen Voorhees and Lori P. Buckland, editors, Proc. 18th Text REtrieval Conference, pages 1:4:1­ 40, Gaithersburg, Maryland, USA, November 2009. NIST Special Publication 500-278.",null,null
,,,
102,"Herbert L. Roitblat, Anne Kershaw, and Patrick Oot. Document categorization in legal electronic discovery: computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70­80, 2010.",null,null
,,,
103,"Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5): 697­716, September 2000.",null,null
,,,
104,"William Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1­8, Beijing, China, July 2011.",null,null
,,,
105,1054,null,null
,,,
106,,null,null

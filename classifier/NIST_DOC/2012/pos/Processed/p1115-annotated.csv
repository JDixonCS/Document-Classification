,sentence,label,data
,,,
0,On Judgments Obtained from a Commercial Search Engine,null,null
,,,
1,"Emine Yilmaz, Gabriella Kazai",null,null
,,,
2,"Microsoft Research Cambridge, UK",null,null
,,,
3,"{eminey,v-gabkaz }@microsoft.com",null,null
,,,
4,"Nick Craswell, S.M.M. Tahaghoghi",null,null
,,,
5,"Microsoft, Bellevue, WA, USA",null,null
,,,
6,"{nickcr,saied.tahaghoghi}@microsoft.com",null,null
,,,
7,ABSTRACT,null,null
,,,
8,"In information retrieval, relevance judgments play an important role as they are used for evaluating the quality of retrieval systems. Numerous papers have been published using judgments obtained from a commercial search engine by researchers in industry. As typically no information is provided about the quality of these judgments, their reliability for evaluating retrieval systems remains questionable. In this paper, we analyze the reliability of such judgments for evaluating the quality of retrieval systems by comparing them to judgments by NIST judges at TREC.",null,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,H.3 [Information Storage and Retrieval]: H.3.3[Information Search and Retrieval],null,null
,,,
11,General Terms,null,null
,,,
12,"Experimentation, Human Factors, Measurement",null,null
,,,
13,Keywords,null,null
,,,
14,"Crowdsourcing, Evaluation, Test Collection",null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"In information retrieval (IR), test collections are typically used to evaluate and optimize the performance of IR systems. The quality of a test collection can impact the conclusions of the evaluation, where the quality of the relevance judgments is a key factor. For example, evaluation outcomes are shown to be affected by using different judge populations [1] and different judging guidelines [3]. On the other hand, using different judges from the same population of NIST judges employed by TREC has been shown to lead to relatively stable conclusions as to which retrieval algorithm beats another [4].",Y,null
,,,
17,"In recent years, several papers using judgments obtained from a commercial search engine have been published [2]. Most of these papers use such judgments (which are typically not publicly available) to validate the superiority of their proposed methods over existing algorithms. Since judges employed by commercial search engine companies are likely to come from different populations than NIST judges and are likely to be subjected to different training and judging",null,null
,,,
18,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
19,"procedures, we may reason that judgments from a commercial search engine are likely to lead to different conclusions than judgments from NIST judges.",null,null
,,,
20,"We analyze whether judgments obtained from a commercial search engine are reliable, in terms of leading to the same evaluation conclusions as when using NIST judgements.",null,null
,,,
21,2. EXPERIMENTAL RESULTS,null,null
,,,
22,"We use the test collection from the TREC Web Track Adhoc tasks from 2009 and 2010. This dataset consists of nearly 50K NIST relevance labels, roughly 25K in each year, for 50 topics in each year. We took these 100 topics and using the topic titles as queries we scraped the top 10 search results from Google and Bing for each query. This gave us a total of 1603 unique query-URL pairs for the 100 topics.",Y,null
,,,
23,"We constructed three different collections by obtaining judgments from three judge groups : judges from (1) the TREC Web track ad-hoc task (NIST), (2) a commercial search engine (ProWeb), and (3) crowdsourcing (Crowd). ProWeb judges were experienced and highly trained judges, employed by the search engine company, while crowd workers, recruited via Clickworker, received no prior training on relevance assessing.",Y,null
,,,
24,"The NIST judgments differ across the two years. In 2009, three relevance levels were used (Highly Relevant, Relevant, and Not Relevant), while in 2010, five grades of relevance were used (Navigational, Key, Relevant, Non-relevant, and Junk). The ProWeb and Crowd judgments were obtained using a simple interface that asked judges to rate a search result's usefulness to a query using a five point scale that can be viewed as a variation of the 2010 Web Track scale (Ideal, Highly Relevant, Relevant, Somewhat Relevant, Nonrelevant). Unlike the ProWeb and Crowd judges, the NIST judges were given descriptions (topic narrative) about what information need is associated with a particular query.",null,null
,,,
25,"Using the different sets of judgments and the NDCG measure, we evaluate the effectiveness of the runs submitted to the TREC 2009 and 2010 Web Track ad-hoc task. Since we only have labels for a subset of the retrieved documents, we remove unjudged documents from the runs. To avoid variance due to having different documents labeled across the different judge groups, we only consider documents that were judged by all three groups.",null,null
,,,
26,"To remove the inconsistency across different judge groups due to the different levels of relevance scales used, we converted all the judgments to the Web Track 2009 scale using the following mapping:Navigational (or Ideal ) judgments to Highly Relevant, Key and Relevant (or Highly Relevant and",Y,null
,,,
27,1115,null,null
,,,
28,1.0,null,null
,,,
29,1,null,null
,,,
30,1,null,null
,,,
31,"Kendall's tau , 0.896955",null,null
,,,
32,"Kendall's tau , 0.860887",null,null
,,,
33,"Kendall's tau , 0.928039",null,null
,,,
34,0.8,null,null
,,,
35,0.8,null,null
,,,
36,0.8,null,null
,,,
37,0.6,null,null
,,,
38,NDCG Crowd,null,null
,,,
39,0.6,null,null
,,,
40,NDCG Crowd,null,null
,,,
41,0.6,null,null
,,,
42,NDCG ProWeb,null,null
,,,
43,0.4,null,null
,,,
44,0.4,null,null
,,,
45,0.4,null,null
,,,
46,0.2,null,null
,,,
47,0.2,null,null
,,,
48,0.2,null,null
,,,
49,0,null,null
,,,
50,0,null,null
,,,
51,0,null,null
,,,
52,1,null,null
,,,
53,0,null,null
,,,
54,0.2,null,null
,,,
55,0.4,null,null
,,,
56,0.6,null,null
,,,
57,0.8,null,null
,,,
58,1,null,null
,,,
59,NDCG NIST,null,null
,,,
60,"Kendall's tau , 0.639488",null,null
,,,
61,1,null,null
,,,
62,0,null,null
,,,
63,0.2,null,null
,,,
64,0.4,null,null
,,,
65,0.6,null,null
,,,
66,0.8,null,null
,,,
67,1,null,null
,,,
68,NDCG NIST,null,null
,,,
69,"Kendall's tau , 0.538047",null,null
,,,
70,1,null,null
,,,
71,0,null,null
,,,
72,0.2,null,null
,,,
73,0.4,null,null
,,,
74,0.6,null,null
,,,
75,0.8,null,null
,,,
76,1,null,null
,,,
77,NDCG ProWeb,null,null
,,,
78,"Kendall's tau , 0.783693",null,null
,,,
79,0.8,null,null
,,,
80,0.8,null,null
,,,
81,0.8,null,null
,,,
82,0.6,null,null
,,,
83,NDCG Crowd,null,null
,,,
84,0.6,null,null
,,,
85,NDCG Crowd,null,null
,,,
86,0.6,null,null
,,,
87,NDCG ProWeb,null,null
,,,
88,0.4,null,null
,,,
89,0.4,null,null
,,,
90,0.4,null,null
,,,
91,0.2,null,null
,,,
92,0.2,null,null
,,,
93,0.2,null,null
,,,
94,0,null,null
,,,
95,0,null,null
,,,
96,0,null,null
,,,
97,0,null,null
,,,
98,0.2,null,null
,,,
99,0.4,null,null
,,,
100,0.6,null,null
,,,
101,0.8,null,null
,,,
102,1,null,null
,,,
103,0,null,null
,,,
104,0.2,null,null
,,,
105,0.4,null,null
,,,
106,0.6,null,null
,,,
107,0.8,null,null
,,,
108,1,null,null
,,,
109,0,null,null
,,,
110,0.2,null,null
,,,
111,0.4,null,null
,,,
112,0.6,null,null
,,,
113,0.8,null,null
,,,
114,1,null,null
,,,
115,NDCG NIST,null,null
,,,
116,NDCG NIST,null,null
,,,
117,NDCG ProWeb,null,null
,,,
118,Figure 1: Comparisons of evaluation results for runs submitted to TREC 2009 (top) and TREC 2010 (bottom),Y,null
,,,
119,using three different sets of judges and their judgements,null,null
,,,
120,"Relevant) to Relevant, and Non-relevant and Junk (or Somewhat Relevant and Non-relevant) to Not Relevant. We have also experimented with various other mappings but all supported the same conclusions of this poster.",null,null
,,,
121,"Figure 1 shows the obtained scatter plots: TREC 2009 results in the top row and TREC 2010 in the bottom row. For each year, we plot the results obtained from evaluating the runs using the NIST vs ProWeb (left plot), NIST vs. Crowd (middle plot) and Crowd vs. ProWeb (right plot) judgments. In each plot, we also include the Kendall's  correlation between the resulting system rankings, obtained using the two respective sets of judgments.",Y,null
,,,
122,"We see that for TREC 2009, evaluations using the NIST, ProWeb and Crowd judgments mostly agree with each other (Kendall's  of 0.86-0.93). This suggests that the judgments obtained from a commercial search engine are more or less consistent with NIST: using them will not cause major differences in the evaluation. Crowd judgments may be somewhat noisier, but still lead to stable evaluations. The differences in the three plots could be caused by the consistency in the number and description of relevance grades between the Crowd and ProWeb judges as compared to the NIST judges.",Y,null
,,,
123,"On the other hand, for TREC 2010, evaluations using the ProWeb and NIST judgments are quite different (Kendall's  ,"" 0.639). At first glance, one might think that ProWeb judgments are not reliable at evaluating the systems and the high agreement on the TREC 2009 data is due to chance. However, if we compare the agreements between the evaluations using the Crowd and ProWeb judgments, we also see low agreements. This suggests that the low correlation is specific to this particular TREC. As the figure suggests, systems that were submitted to this particular TREC have very similar performance. Hence, the Kendall's  statistic may be affected by these systems. Furthermore, when we""",Y,null
,,,
124,"considered the cases where NIST judgments highly disagree with the Crowd and ProWeb judgments, we found that there are quite a few documents that got the best rating by the Crowd and ProWeb judges but were labeled as non-relevant by the NIST judges. When we analyzed the disagreement cases, we realized that these differences could be caused by the specific topic description that was given to the NIST judges, which limited the possible intents associated with a query. Since the Crowd and ProWeb judges were not given such topic descriptions, they would have considered all possible intents for a query when assigning labels to the documents.",null,null
,,,
125,"Overall, our conclusion is that even though judgments from a commercial search engine could lead to slightly different conclusions than NIST judges in some settings, evaluations using the two judge groups seem mostly consistent.",null,null
,,,
126,3. REFERENCES,null,null
,,,
127,"[1] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P. de Vries, and Emine Yilmaz. Relevance assessment: are judges exchangeable and does it matter. In Proc. of ACM SIGIR Conference, pages 667­674. ACM, 2008.",null,null
,,,
128,"[2] Christopher J. C. Burges, Robert Ragno, and Quoc Viet Le. Learning to rank with nonsmooth cost functions. In NIPS, pages 193­200. MIT Press, 2006.",null,null
,,,
129,"[3] Charles L.A. Clarke, Nick Craswell, Ian Soboroff, and Azin Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proc. of ACM WSDM Conference, pages 75­84. ACM, 2011.",null,null
,,,
130,"[4] Ellen M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proc. of ACM SIGIR Conference, pages 315­323. ACM, 1998.",null,null
,,,
131,1116,null,null
,,,
132,,null,null

,sentence,label,data
0,Active Query Selection for Learning Rankers,null,null
1,Mustafa Bilgic,null,null
2,"Illinois Institute of Technology, Chicago, IL",null,null
3,mbilgic@iit.edu,null,null
4,ABSTRACT,null,null
5,"Methods that reduce the amount of labeled data needed for training have focused more on selecting which documents to label than on which queries should be labeled. One exception to this [4] uses expected loss optimization (ELO) to estimate which queries should be selected but is limited to rankers that predict absolute graded relevance. In this work, we demonstrate how to easily adapt ELO to work with any ranker and show that estimating expected loss in DCG is more robust than NDCG even when the final performance measure is NDCG.",null,null
6,Categories and Subject Descriptors,null,null
7,H.3.3 [Information Search and Retrieval]: Information Search and Retrieval--active learning,null,null
8,Keywords,null,null
9,"Active learning, query selection",null,null
10,1. INTRODUCTION,null,null
11,"Research in information retrieval evaluation has examined how to construct minimal test collections [2], and the balance between the number of queries judged and the depth of judging [3]. With respect to training rankers, most work has focused on document selection [1] or balancing number of queries with depth of documents judged using random query selection [5]. In this paper, we focus on selecting queries in order to most rapidly increase ranker retrieval performance.",null,null
12,"In particular, we focus on the application of expected loss optimization (ELO) to query selection. Long et al. [4] used ELO to select queries for training but relied on having a ranker that estimated absolute graded relevance. We generalize this to work with any ranker ­ many of which induce a ranking but not absolute labels. To generalize to any ranker, we introduce a calibration phase over validation data.",null,null
13,"In addition, although in theory ELO can be used with any performance measure for active learning, we show using DCG loss (as done in [4]) leads to better performance whether DCG or NDCG is used as the final evaluation of ranker performance. We provide evidence this is because ELO using DCG loss tends toward queries that have both more relevant examples and many degrees of relevance.",null,null
14,2. APPROACH,null,null
15,"ELO suggests that, given a set of candidate queries C, one pick the query q  C for labeling where the expected loss is the greatest. Mathematically, we have:",null,null
16,max,null,null
17,qC,null,null
18,EP,null,null
19,(Y,null,null
20,|Xq,null,null
21,",D)",null,null
22,"max M((Xq), y) - M(R(Xq), y)",null,null
23,(1),null,null
24,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
25,Paul N. Bennett,null,null
26,"Microsoft Research, Redmond, WA",null,null
27,pauben@microsoft.com,null,null
28,"where D is the given training data, and P (Y | Xq, D) is a distribution over graded relevance labels Y for the documents, Xq, to be ranked for the query q. M(r, y) is a retrieval performance measure such as DCG that can evaluate the quality of a ranking, r, for a set of documents given a particular labeling of the documents, y. (Xq) is simply a permutation of the documents and R(Xq) denotes the current ranking of the documents. For most retrieval performance measures, the inner max on the left-hand side of the difference is easily found by sorting from highest relevance to the lowest.",null,null
29,"In order to estimate the label distribution P (Y | Xq, D) Long et al. [4] relied on training an ensemble of models to predict absolute graded relevance. We generalize ELO to work with any ranker by mapping the current ranking model to a distribution over graded labels. To do so, we introduce a calibration phase where a classification model is trained over the labels of the top k documents according to the ranker in the validation data.1 During active learning, the classification model is used to estimate the P (Y | xq, D) for each document xq  Xq. The quantity in Eq. 1 is then estimated through sampling of the labels from this distribution.",null,null
30,3. EXPERIMENTS,null,null
31,"Like most active learning evaluation settings, we start with some labeled data D that is randomly chosen, train the models on D, pick a number of queries to be labeled from the candidate set C, add those to D, and repeat this process for a number of iterations. The performance of the active learning strategy in augmenting D is judged at each iteration by evaluating the current induced rankers on held-out test data. We perform 20 iterations of labeling and based on the findings reported in [5] we label 15 documents per query (or the maximum available). We repeat this process five times, each time starting with a different set of labeled data and report averages. We report both DCG@10 and NDCG@10 (one can use DCG for selection but NDCG for evaluation and so forth).",null,null
32,"We use the publicly available Yahoo! LETOR challenge dataset that has 3 splits: we treat the train split as the candidate data C, utilize the validation split for tuning the rankers' parameters and training the calibration models, and use the test split for evaluation. We experiment with two rankers: one that does not produce absolute graded relevance (SVMRank) and one that does (Additive Regression (AR)). SVMRank labels 750 query-document pairs per iteration where AR labels only 150. The difference is due to AR having a steeper learning curve.",null,null
33,We examine four query/document selection methods: (1) select queries and documents randomly (rndQ-rndD); (2) se-,null,null
34,1We use the same validation data that is used for model parameter search ­ this ensures our method does not require any additional labeled data.,null,null
35,1033,null,null
36,DCG@10 NDCG@10,null,null
37,Active Learning Experiments - DCG@10,null,null
38,13.8,null,null
39,Active Learning Experiments - NDCG@10,null,null
40,0.72,null,null
41,13.6,null,null
42,0.71 13.4,null,null
43,13.2 13.0 12.8 12.6 12.4 12.2,null,null
44,0,null,null
45,5,null,null
46,10,null,null
47,15,null,null
48,Number of active training batches,null,null
49,0.70,null,null
50,svm-rndQ-rndD,null,null
51,ar-rndQ-rndD,null,null
52,0.69,null,null
53,svm-rndQ-topD,null,null
54,ar-rndQ-topD,null,null
55,0.68,null,null
56,svm-dcqELOq,null,null
57,ar-dcqELOq,null,null
58,0.67,null,null
59,20,null,null
60,svm-ndcgELOq,null,null
61,0,null,null
62,ar-ndcgELOq,null,null
63,5,null,null
64,10,null,null
65,15,null,null
66,20,null,null
67,Number of active training batches,null,null
68,Figure 1: Comparison of query-selection strategies.,null,null
69,lect queries randomly and select the top documents according to the current ranker (rndQ-topD); (3) select queries according to ELO with DCG@10 as the selection measure and the top documents according to the current ranker (dcgELOq); (4) same as 3 but with NDCG@10 instead (ndcgELOq).,null,null
70,4. RESULTS AND DISCUSSION,null,null
71,"Figure 1 shows the results for SVMRank (solid) and AR (dashed) when the evaluation measure is DCG@10 (left) and NDCG@10 (right).2 Error bars are standard error about the mean over the five trials. As reported elsewhere [4], selecting the top documents performs as well or better than selecting documents randomly. Note that regardless of whether evaluating by DCG or NDCG, using NDCG for selection (ndcgELOq) leads to the worst performance. In contrast, using DCG for selection leads to the best performance across both rankers and both evaluation measures. Finally, we note that the differences between methods are less significant when evaluated by NDCG than DCG. This suggests that while the learners are more effective ­ finding more relevant results per query ­ they are contributing to the marginal relevance for each query according to NDCG. The perceived impact on user utility will likely depend on the scenario and the degree to which the task is recall-oriented.",null,null
72,Rating Distribution - SVMRank,null,null
73,60%,null,null
74,50%,null,null
75,40%,null,null
76,Percentage,null,null
77,30%,null,null
78,20%,null,null
79,10%,null,null
80,0%,null,null
81,0,null,null
82,1,null,null
83,2,null,null
84,3,null,null
85,4,null,null
86,Ratings,null,null
87,rndQ-rndD rndQ-topD dcgELOq ndcgELOq,null,null
88,Figure 2: Rating distribution of selected queries.,null,null
89,Figure 2 displays the rating distribution of the training data collected at the last step of active learning as a per-,null,null
90,2SVMRank and AR are displayed together for space and to emphasize the similarity in trends. We are interested in comparisons within each and not across the two.,null,null
91,"centage for the 15,000 labeled instances for SVMRank (the distribution trends for AR were nearly identical). Note that ndcgELOq selects far more irrelevant items than the other methods. This may seem surprising since NDCG selection is the same as DCG but normalized by the max estimate on the left-hand side of Eq. 1. However, for a poorly performing query with a single relevant document, NDCG's max will be 1 but current performance will be near zero. Thus, the selection method often selects queries with very few relevant documents. In contrast, dcgELOq not only obtains the largest percent of documents at the relevant side (labels 3,4) and fewest on the irrelevant side (label 0), it selects queries where a variety of relevance grades exist. This is consistent with the literature that biasing toward relevant documents is not sufficient in itself [1] ­ one also needs a variety of relevance grades present.",null,null
92,5. SUMMARY,null,null
93,"We presented a method that generalizes the applicability of ELO for query selection to any ranker. Our method also has the benefit of being less of a computational burden than training ensembles at each step prior to labeling. We also demonstrated that whether one cares about DCG or NDCG for performance, using DCG provides a more stable query selection method. This is because the nature of NDCG as a ratio pushes the selection toward queries that often have few relevant documents. In contrast, using DCG in the selection mechanism promotes queries that have more relevant documents, and the expected loss component ensures that there will be a variety of relevance grades ­ since current performance is far below the max. These insights may be useful in developing new query selection methods.",null,null
94,6. REFERENCES,null,null
95,"[1] J. Aslam, E. Kanoulas, V. Pavlu, S. Savev, and E. Yilmaz. Document selection methodologies for efficient and effective learning-to-rank. In SIGIR '09.",null,null
96,"[2] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06.",null,null
97,"[3] B. Carterette, V. Pavlu, E. Kanoulas, J. Aslam, and J. Allan. If i had a million queries. In ECIR '09.",null,null
98,"[4] B. Long, O. Chapelle, Y. Zhang, Y. Chang, Z. Zheng, and B. Tseng. Active learning for ranking through expected loss optimization. In SIGIR '10.",null,null
99,[5] E. Yilmaz and S. Robertson. Deep versus shallow judgments in learning to rank. In SIGIR '09.,null,null
100,1034,null,null
101,,null,null

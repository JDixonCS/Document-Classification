,sentence,label,data
,,,
0,Utilizing Inter-Document Similarities in Federated Search,null,null
,,,
1,Savva Khalaman,null,null
,,,
2,Oren Kurland,null,null
,,,
3,savvakh@tx.technion.ac.il kurland@ie.technion.ac.il,null,null
,,,
4,"Faculty of Industrial Engineering and Management Technion -- Israel Institute of Technology Haifa 32000, Israel",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"We demonstrate the merits of using inter-document similarities for federated search. Specifically, we study a resultsmerging method that utilizes information induced from clusters of similar documents created across the lists retrieved from the collections. The method significantly outperforms state-of-the-art results merging approaches.",null,null
,,,
7,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
,,,
8,"General Terms: Algorithms, Experimentation",null,null
,,,
9,"Keywords: inter-document similarities, federated search",null,null
,,,
10,1. INTRODUCTION,null,null
,,,
11,"Federated search is the task of retrieving documents from multiple (possibly non-overlapping) collections in response to a query [1]. The task is typically composed of three steps: attaining resource (collection) description, selecting resources (collections), and merging the results retrieved from the selected collections [1]. We focus on the resultsmerging step; specifically, we study the merits of using information induced from inter-document similarities.",null,null
,,,
12,"While there is much work on utilizing inter-document similarities for the single-corpus retrieval setting, there is little work along that venue for federated retrieval. For example, clustering was used to transform a single-collection retrieval setting into that of multiple collections [8]. Clusters of sampled documents were used for performing query expansion in federated search [5]; yet, inter-document similarities were not used for results merging. Furthermore, it was shown that among the clusters created across the lists retrieved from different collections there are some that contain a high percentage of relevant documents [3]; still, a results merging method exploiting these clusters was not proposed",null,null
,,,
13,"The only work, to the best of our knowledge, that uses inter-document-similarities for (direct) results merging is based on scoring a document by its similarity with other documents in the retrieved lists [6]. We show that the method we study substantially outperforms this approach.",null,null
,,,
14,"The method we present for merging results in federated search is adapted from recent work on fusing lists that were retrieved from the same collection [4]. In contrast to the non-overlapping collections setting we explore here, the retrieved lists in this work [4] were (partially) overlapping and",null,null
,,,
15,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
16,"the merging (fusion) methods used to assign initial scores to documents exploited this overlap. The adapted method that we study integrates retrieval scores assigned by a state-ofthe-art results-merging approach (e.g., CORI [1] and SSL [7]) with information induced from clusters created from similar documents across the retrieved lists. Specifically, a document can provide relevance-status support to documents in the same list or in other lists that it is similar to. The resultant retrieval performance is substantially better than that of using only the initial retrieval scores assigned by the state-of-the-art merging approach.",null,null
,,,
17,2. RESULTS-MERGING METHOD,null,null
,,,
18,Suppose that some resource (collection) selection method,null,null
,,,
19,was applied in response to query q [1]. We assume that docu-,null,null
,,,
20,ment lists were retrieved from the selected (non-overlapping),null,null
,,,
21,"collections and merged by some previously proposed resultsmerging algorithm [1, 7]. Let Di[nn]it denote the list of the n documents most highly ranked by the merging algorithm",null,null
,,,
22,that assigns the (initial) score Finit(d; q) to document d. Our goal is re-ranking Di[nn]it to improve ranking effec-,null,null
,,,
23,"tiveness. (Documents not in Di[nn]it remain at their original ranks.) To that end, we study the utilization of inter-",null,null
,,,
24,document similarities by adapting a method proposed in,null,null
,,,
25,"work on fusing lists retrieved from the same corpus [4]. Let Cl be the set of document clusters created from Di[nn]it using some clustering algorithm; c will denote a cluster. Then, the score assigned to document d ( Di[nn]it) by the Clust method is:",null,null
,,,
26,"FClust(d; q) d,ef (1 X",null,null
,,,
27,P,null,null
,,,
28,- ) P,null,null
,,,
29,d,null,null
,,,
30,F (c; q),null,null
,,,
31,Finit(d; q),null,null
,,,
32,+,null,null
,,,
33,Di[nn]it,null,null
,,,
34,Finit(d P,null,null
,,,
35,;,null,null
,,,
36,q),null,null
,,,
37,P,null,null
,,,
38,"diPc Sim(di,",null,null
,,,
39,d),null,null
,,,
40,;,null,null
,,,
41,"cCl c Cl F (c ; q) d Di[nn]it dic Sim(di, d )",null,null
,,,
42,F (c; q),null,null
,,,
43,"d,ef",null,null
,,,
44,Q,null,null
,,,
45,di c,null,null
,,,
46,Finit(di; q);,null,null
,,,
47,as,null,null
,,,
48,all,null,null
,,,
49,the,null,null
,,,
50,clusters,null,null
,,,
51,in,null,null
,,,
52,Cl,null,null
,,,
53,contain,null,null
,,,
54,"the same number of documents (see details below), there is",null,null
,,,
55,"no cluster-size bias incurred; Sim(·, ·) is the inter-document",null,null
,,,
56,similarity measure used to create Cl;  is a free parameter.,null,null
,,,
57,"Thus, d is highly ranked if (i) its (normalized) initial score",null,null
,,,
58,"is high; and, (ii) it is similar to documents in clusters c",null,null
,,,
59,that contain documents that were initially highly ranked.,null,null
,,,
60,"In other words, similar documents provide relevance-status",null,null
,,,
61,"support to each other via the clusters to which they belong. Note that if  ,"" 0, then cluster-based information is not used and FClust(d; q) is d's normalized initial score.""",null,null
,,,
62,1169,null,null
,,,
63,Uni Rel NRel Rep KM,null,null
,,,
64,CORI SSL CORI SSL CORI SSL CORI SSL,null,null
,,,
65,initial CRSC Clust initial CRSC Clust,null,null
,,,
66,initial CRSC Clust initial CRSC Clust,null,null
,,,
67,initial CRSC Clust initial CRSC Clust,null,null
,,,
68,initial CRSC Clust initial CRSC Clust,null,null
,,,
69,CORI SSL,null,null
,,,
70,initial CRSC Clust initial CRSC Clust,null,null
,,,
71,Queries: 51-100 p@5 p@10 MAP,null,null
,,,
72,0.46,null,null
,,,
73,.524 .536i .432 .492,null,null
,,,
74,0.504,null,null
,,,
75,.420 .060 .474i .062 .498i .064i,null,null
,,,
76,.410 .060,null,null
,,,
77,.458 .062 .490i .064i,null,null
,,,
78,.424 .384 .059,null,null
,,,
79,.428 .476 .412,null,null
,,,
80,.402 .061 .442i .063ic .384 .095,null,null
,,,
81,.412 .404 .096 .480ic .454ic .099ic,null,null
,,,
82,.452 .446 .064,null,null
,,,
83,.476 .476 .066,null,null
,,,
84,.552ic .516ic .069ic .464 .448 .065,null,null
,,,
85,.488 .462 .067,null,null
,,,
86,.536i .516ic .070ic,null,null
,,,
87,.428 .408 .060,null,null
,,,
88,.416 .418 .060,null,null
,,,
89,.492c .474ic .064ic .444 .408 .061,null,null
,,,
90,.448 .434 .060,null,null
,,,
91,.476 .470i .064ic,null,null
,,,
92,Queries: 201-250,null,null
,,,
93,p@5 p@10 MAP,null,null
,,,
94,.468 .402 .135,null,null
,,,
95,.484 .396 .138,null,null
,,,
96,.496 .460ic .147i .480 .412 .152,null,null
,,,
97,.512 .412 .150 .532 .478ic .165ic,null,null
,,,
98,Queries: 101-150 p@5 p@10 MAP,null,null
,,,
99,.396 .376 .480ic .412 .400 .492c,null,null
,,,
100,.432 .456 .496 .440 .452 .492,null,null
,,,
101,.380 .054,null,null
,,,
102,.372 .052 .454ic .059ic .396 .055 .346 .052 .468ic .060ic,null,null
,,,
103,.370 .064 .442 .072i .466i .073i,null,null
,,,
104,.376 .095 .436 .103i .474i .105i,null,null
,,,
105,.416 .472 .500i .396,null,null
,,,
106,.456 .504i,null,null
,,,
107,.394 .414,null,null
,,,
108,.450 .398 .412,null,null
,,,
109,0.452,null,null
,,,
110,0.054,null,null
,,,
111,.057 .059ic .055,null,null
,,,
112,.058 .060i,null,null
,,,
113,.428 .432,null,null
,,,
114,.496 .452 .396,null,null
,,,
115,.508c,null,null
,,,
116,.388 .050,null,null
,,,
117,.424 .052 .474i .056ic .408 .052,null,null
,,,
118,.418 .052 .484ic .058ic,null,null
,,,
119,"Table 1: Results. Boldface: the best result per testbed, evaluation measure, and initial merging method; 'i' and 'c': statistically significant differences with initial and CRSC, respectively.",null,null
,,,
120,3. EVALUATION,null,null
,,,
121,"We conducted experiments with testbeds that are commonly used in work on federated search [1, 2, 5, 8, 7]: (i) Uni: Trec123-100col-bysource (Uniform), (ii) KM: Trec4kmeans (K-means), (iii) Rep: Trec123-2ldb-60col (Representative), (iv) Rel: Trec123-AP-WSJ-60col (Relevant), and (v) NRel: Trec123-FR-DOE-81col (non-relevant). Titles of the TREC topics 51-150 served for queries for all testbeds except for KM where the description fields of TREC topics 201-250 were used. Tokenization, Porter stemming, and stopword removal were applied using the Lemur toolkit (www.lemurproject.org), which was used for experiments. To acquire resource (collection) description, we adopt the query-based sampling method from [2] which was also used in [1, 7, 5]. Following common practice [1, 7], the 10 highest ranked collections are selected in the resource-selection phase using CORI's resource selection method. As in previous report [7], 1000 documents are retrieved from each selected collection using the INQUERY search engine. Then, the retrieved lists are merged using either CORI's merging method [1] or the single-model SSL merging approach [7]. The initial score, Finit(d; q), assigned to d by these methods is used in our Clust method.",Y,null
,,,
122,"To cluster Di[nn]it, we use a simple nearest-neighbors-based clustering approach [4]. For each d  Di[nn]it we create a cluster that is composed of d and the  - 1 ( , 5) documents d in Di[nn]it (d ,"" d) that yield the highest Sim(d, d ) d"",ef",null,null
,,,
123,"

",null,null
,,,
124,exp -D(p[d0](·)||p[dµ](·)) ; p[xµ] is the Dirichlet-smoothed un-,null,null
,,,
125,igram language model induced from x with the smooth-,null,null
,,,
126,"ing parameter  (, 1000); D is the KL divergence; the",null,null
,,,
127,term-counts statistics used for smoothing language models,null,null
,,,
128,is based on the query-based sampling mentioned above.,null,null
,,,
129,The initial ranking induced by the CORI and SSL results-,null,null
,,,
130,merging methods serves for a baseline. Additional reference,null,null
,,,
131,comparison that we use is the Cross Rank Similarity Com-,null,null
,,,
132,parison scoring,null,null
,,,
133,(CRSC) approach [6] that,null,null
,,,
134,P,null,null
,,,
135,d with,null,null
,,,
136,d,null,null
,,,
137,"(,d)Di[nn]it",null,null
,,,
138,P d,null,null
,,,
139,"(,d",null,null
,,,
140,re-ranks Di[nn]it,null,null
,,,
141,"Sim(d ,d)",null,null
,,,
142,")Di[nn]it Sim(d ,d",null,null
,,,
143,here by ) . Our,null,null
,,,
144,"Clust method incorporates two free parameters: n ( {10, 30,",null,null
,,,
145,"50, 100}), the number of documents in Di[nn]it, and  ( {0, 0.1, . . . , 1}), the interpolation parameter; CRSC only depends",null,null
,,,
146,on n. We set the free-parameter values for each method us-,null,null
,,,
147,ing leave-one-out cross validation performed over the queries,null,null
,,,
148,per testbad; MAP@1000 serves as the optimization criterion,null,null
,,,
149,"in the learning phase. In addition to MAP, we also present",null,null
,,,
150,p@5 and p@10 performance numbers. Statistically signif-,null,null
,,,
151,icant differences in performance are determined using the,null,null
,,,
152,two-tailed paired t-test at a 95% confidence level.,null,null
,,,
153,Results and Conclusions. We see in Table 1 that Clust,null,null
,,,
154,"always outperforms the initial ranking that was induced by a state-of-the-art results-merging method; often, the improvements are statistically significantly. This finding attests to the merits of integrating the initial results-merging score with information induced from clusters of similar documents. Further exploration reveals that   {0, 1} often yields optimal performance. This shows that the integration just mentioned yields performance that is better than that of using each of the two integrated components alone. We also see that Clust consistently outperforms CRSC, which does not utilize the initial results-merging scores, nor uses a cluster-based approach.",null,null
,,,
155,"Acknowledgments We thank the reviewers for their comments. This paper is based upon work supported in part by the Israel Science Foundation under grant no. 557/09. Any opinions, findings and conclusions or recommendations expressed here are the authors' and do not necessarily reflect those of the sponsors.",null,null
,,,
156,4. REFERENCES,null,null
,,,
157,"[1] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.",null,null
,,,
158,"[2] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97­130, 2001.",null,null
,,,
159,"[3] F. Crestani and S. Wu. Testing the cluster hypothesis in distributed information retrieval. Information Processing and Management, 42(5):1137­1150, 2006.",null,null
,,,
160,"[4] A. Khudyak Kozorovitsky and O. Kurland. Cluster-based fusion of retrieved lists. In Proceedings of SIGIR, pages 893­902, 2011.",null,null
,,,
161,"[5] M. Shokouhi, L. Azzopardi, and P. Thomas. Effective query expansion for federated search. In Proceedings of SIGIR, pages 427­434, 2009.",null,null
,,,
162,"[6] X. M. Shou and M. Sanderson. Experiments on data fusion using headline information. In Proceedings of SIGIR, pages 413­414, 2002.",null,null
,,,
163,"[7] L. Si and J. Callan. A semisupervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457­491, October 2003.",null,null
,,,
164,"[8] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of SIGIR, 1999.",null,null
,,,
165,1170,null,null
,,,
166,,null,null

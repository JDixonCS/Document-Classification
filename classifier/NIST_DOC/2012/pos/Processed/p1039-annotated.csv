,sentence,label,data
,,,
0,Clarity Re-Visited,null,null
,,,
1,"Shay Hummel, Anna Shtok, Fiana Raiber, Oren Kurland",null,null
,,,
2,"Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel {hummels,annabel,fiana}@tx.technion.ac.il,",null,null
,,,
3,kurland@ie.technion.ac.il,null,null
,,,
4,David Carmel,null,null
,,,
5,"IBM Research Lab, Haifa 31905, Israel carmel@il.ibm.com",null,null
,,,
6,ABSTRACT,null,null
,,,
7,"We present a novel interpretation of Clarity [5], a widely used query performance predictor. While Clarity is commonly described as a measure of the ""distance"" between the language model of the top-retrieved documents and that of the collection, we show that it actually quantifies an additional property of the result list, namely, its diversity. This analysis, along with empirical evaluation, helps to explain the low prediction quality of Clarity for large-scale Web collections.",null,null
,,,
8,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
,,,
9,"General Terms: Algorithms, Experimentation",null,null
,,,
10,"Keywords: query-performance prediction, Clarity",null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"Many query performance predictors were proposed over the years [2]. Clarity [5] is a well known, commonly used, state-of-the-art predictor that measures the ""coherence"" of the top-retrieved documents with respect to the collection. Specifically, the more distinguishable the language used in the retrieved documents from the general language used in the collection, the better the retrieval is assumed to be1. Clarity was shown to be highly effective for most TREC benchmarks [6]. However, low prediction quality is observed when using Clarity for large scale, noisy, Web corpora [1].",Y,null
,,,
13,"We present a novel formal analysis of Clarity that sheds some light on its underlying components and the properties of the result list of top-retrieved documents that it quantifies. While Clarity is commonly described as a measure of the ""distance"" between a language model induced from the result list and that induced from the collection, we show that Clarity actually quantifies an additional property of the result list, namely, its diversity. Our empirical analysis shows that the diversity of the result list has a negative correlation with retrieval performance for older TREC benchmarks and a positive correlation for the new ClueWeb collection. These findings, along with the formal analysis, help to explain the poor prediction quality of Clarity over ClueWeb.",Y,null
,,,
14,"1There are are several variants of clarity, among which is a pre-retrieval method (SCS [7]) that considers only the query and the corpus and not the retrieved documents.",null,null
,,,
15,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
,,,
16,"In addition, the novel interpretation we present suggests new integration approaches of Clarity's building blocks.",null,null
,,,
17,2. INSIDE CLARITY,null,null
,,,
18,"Let q and D denote a query and a corpus of documents, respectively. In what follows we use p(w|x) to denote the probability assigned to term w by a unigram (smoothed) language model induced from x.",null,null
,,,
19,The query likelihood (QL) retrieval method scores doc-,null,null
,,,
20,"ument d by log p(q|d) d,""ef log qiq p(qi|d), where qi is a term in q. Let Dq[k] denote the result list of the k highest ranked documents. The assumption behind Clarity is that the higher the divergence of a model of Dq[k] from that of the corpus, the higher the effectiveness of Dq[k] is, and thereby, the better the quality of the QL-based retrieval. The KL divergence between a relevance language model, R, induced from Dq[k], and a language model induced from D, is used to quantify this divergence. R is a weighted linear mixture of the models of documents in Dq[k] [5]. Accordingly, the Clarity of q is defined as:""",null,null
,,,
21,"Clarity(q) d,ef KL p(·|R) p(·|D) ,",null,null
,,,
22,p(w|R),null,null
,,,
23,log,null,null
,,,
24,p(w|R) p(w|D),null,null
,,,
25,.,null,null
,,,
26,w,null,null
,,,
27,"As is the case for any two probability distributions, we can write the KL divergence as follows:",null,null
,,,
28,"KL p(·|R) p(·|D) , CE p(·|R) p(·|D) -H (p(·|R)) ;",null,null
,,,
29,"CE is the cross entropy between R and D,",null,null
,,,
30,"CE p(·|R) p(·|D) d,ef - p(w|R) log p(w|D);",null,null
,,,
31,w,null,null
,,,
32,"H is the entropy of the relevance model,",null,null
,,,
33,"H (p(·|R)) d,ef - p(w|R) log p(w|R).",null,null
,,,
34,w,null,null
,,,
35,"Under this decomposition, Clarity integrates two measures (building blocks). The first is the ""distance"" of R from the corpus, as measured by the cross entropy. The more distant R from D, the higher the cross entropy is. We use CDistance (for ""corpus distance"") to refer to the cross entropy between R and D.",null,null
,,,
36,"The second measure used by Clarity is the entropy of R. High entropy means that R assigns relatively low weights (i.e., probabilities) to a large number of terms; thereby, Dq[k] is highly diverse. In contrast, low entropy means that only",null,null
,,,
37,1039,null,null
,,,
38,"a few terms are highly weighted, hence Dq[k] is more focused.",null,null
,,,
39,setup that affects the estimation of the corpus language,null,null
,,,
40,"We use LDiversity (for ""list diversity"") to refer to R's en-",null,null
,,,
41,"model. Evidently, spam removal did not improve prediction",null,null
,,,
42,tropy. Next we study the prediction quality of each of these,null,null
,,,
43,quality over ClueWeb.,Y,null
,,,
44,building blocks and compare it with that of Clarity which,null,null
,,,
45,"Second, LDiversity and retrieval performance have posi-",null,null
,,,
46,amounts to their equal-weight linear interpolation:,null,null
,,,
47,"tive correlation for ""ClueWebs"", yet negative correlation is",Y,null
,,,
48,"Clarity(q) , CDistance(q) - LDiversity(q). (1)",null,null
,,,
49,"observed for ""SmallScales"". We presume that list coherence can attest to improved retrieval effectiveness, as is implied",null,null
,,,
50,3. EXPERIMENTS,null,null
,,,
51,"by the findings for ""SmallScales"", which are mainly composed of unambiguous (coherent) queries. On the other",null,null
,,,
52,"We conducted experiments using the following TREC bench- hand, list diversity might correspond to improved retrieval",Y,null
,,,
53,marks (disks and topics are indicated in the parentheses):,null,null
,,,
54,"performance, as is implied by the findings for ""ClueWebs"",",Y,null
,,,
55,"TREC4 (disks 2-3; 201-250), TREC5 (disks 2,4; 251-300),",Y,null
,,,
56,"for ambiguous queries, if it attests to coverage of various",null,null
,,,
57,"WT10G (WT10G; 451-550), Robust (disks 4-5 - {CR}; 301-",Y,null
,,,
58,query aspects. It was found that for Clue09 topical diver-,Y,null
,,,
59,"450, 601-700), GOV2 (GOV2; 701-850), and the ClueWeb",Y,null
,,,
60,sity and retrieval performance are strongly correlated [3].,null,null
,,,
61,collection (category B). Two sets of topics were used for,null,null
,,,
62,"Following the observations made above, we can explain",null,null
,,,
63,ClueWeb: Clue09 (1-50) and Clue10 (51-100). We applied,Y,null
,,,
64,"the low effectiveness of Clarity for ""ClueWebs""; Clarity, as",Y,null
,,,
65,Porter stemming and stopword removal upon queries and,null,null
,,,
66,"presented in Equation 1, is the subtraction of prediction",null,null
,,,
67,documents using the Lemur/Indri toolkit.,null,null
,,,
68,values assigned by two predictors which are both positively,null,null
,,,
69,Previous work hypothesized that the low prediction qual-,null,null
,,,
70,"correlated with retrieval effectiveness. Usually, two predic-",null,null
,,,
71,ity of Clarity over Web collections is due to the large amount,null,null
,,,
72,tors that are positively correlated with retrieval performance,null,null
,,,
73,"of noise (e.g. spam) [6]. To address spam effects in ClueWeb,",Y,null
,,,
74,"are integrated by multiplication or summation [2]. Thus,",null,null
,,,
75,we filtered out the spammiest documents from the result lists,null,null
,,,
76,"the subtractive integration of CDistance and LDiversity,",null,null
,,,
77,(those assigned a spam score below 50 by Waterloo's clas-,null,null
,,,
78,"as implemented by Clarity, yields low quality prediction over",null,null
,,,
79,sifier [4]) and retained the original ranking for the residual,null,null
,,,
80,ClueWebs.,Y,null
,,,
81,"corpus. Thus, we get two additional experimental setups for",null,null
,,,
82,ClueWeb: Clue09+SpRM and Clue10+SpRM.,Y,null
,,,
83,4. SUMMARY,null,null
,,,
84,"We use the predictors to predict the performance of the QL retrieval method specified above; unigram Dirichlet-smoothed document language models are used with the smoothing parameter set to 1000. We study three predictors: CDistance, LDiversity, and Clarity which interpolates the two (see Equation 1). Following the common practice to evaluating prediction quality [2], we report Pearson's correlation between the values assigned by the predictor and retrieval effectiveness measured by average precision computed using TREC's relevance judgments. The size of the result list, k, was set to 500 following previous recommendations [6]. The relevance models were clipped to use only the 100 highest",Y,null
,,,
85,"We showed that Clarity amounts to an equal weight interpolation of two predictors; one measures the ""distance"" of the result list from the collection, while the second measures the list's ""diversity"". We used this formal finding to help explain the low prediction quality of Clarity over ClueWeb, in contrast to its high effectiveness over other TREC benchmarks. Preliminary results of using non-equal weights for the interpolation mentioned above, and independently optimizing free-parameter values for each predictor, attest to the merits of these future directions. (Actual numbers are omitted due to space considerations.)",Y,null
,,,
86,weighted terms. The language models of documents used to construct the relevance model were not smoothed.,null,null
,,,
87,Acknowledgments. We thank the reviewers for their com-,null,null
,,,
88,ments. This paper is based upon work supported in part,null,null
,,,
89,"by the Israel Science Foundation under grant no. 557/09,",null,null
,,,
90,"by IBM's SUR award, by an IBM Ph.D. Fellowship and by Miriam and Aaron Gutwirth Memorial Fellowship. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.",null,null
,,,
91,Figure 1: Comparing the prediction quality of Clarity and its building blocks.,null,null
,,,
92,"Figure 1 presents our main results. To simplify the presentation, we refer to the ClueWeb benchmarks as ""ClueWebs"" (on the left of the graph) and to all the other benchmarks as ""SmallScales"" (on the right). The differences of the patterns observed for ""ClueWebs"" and ""SmallScales"" are as follows. First, CDistance is not very effective over ""ClueWebs"" in comparison to ""SmallScales"". We attribute this finding to the low quality of the collection statistics in a noisy Web",Y,null
,,,
93,5. REFERENCES,null,null
,,,
94,"[1] N. Balasubramanian, G. Kumaran, and V. R. Carvalho. Predicting query performance on the web. In Proceedings of SIGIR, pages 785­786, 2010.",null,null
,,,
95,"[2] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool, 2010.",null,null
,,,
96,"[3] C. L. A. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 Web track. In Proceedings of TREC, 2009.",Y,null
,,,
97,"[4] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. CoRR, abs/1004.5168, 2010.",null,null
,,,
98,"[5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. Predicting query performance. In Proceedings of SIGIR, pages 299­306, 2002.",null,null
,,,
99,"[6] C. Hauff, V. Murdock, and R. Baeza-Yates. Improved query difficulty prediction for the web. In Proceedings of CIKM, pages 439­448, 2008.",null,null
,,,
100,"[7] B. He and I. Ounis. Inferring query performance using pre-retrieval predictors. In Proceedings of SPIRE, pages 43­54, 2004.",null,null
,,,
101,1040,null,null
,,,
102,,null,null

,sentence,label,data
0,Using Preference Judgments for Novel Document Retrieval,null,null
1,"Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu",null,null
2,Department of Computer and Information Sciences University of Delaware,null,null
3,"Newark, DE, USA 19716",null,null
4,ABSTRACT,null,null
5,"There has been considerable interest in incorporating diversity in search results to account for redundancy and the space of possible user needs. Most work on this problem is based on subtopics: diversity rankers score documents against a set of hypothesized subtopics, and diversity rankings are evaluated by assigning a value to each ranked document based on the number of novel (and redundant) subtopics it is relevant to. This can be seen as modeling a user who is always interested in seeing more novel subtopics, with progressively decreasing interest in seeing the same subtopic multiple times. We put this model to test: if it is correct, then users, when given a choice, should prefer to see a document that has more value to the evaluation. We formulate some specific hypotheses from this model and test them with actual users in a novel preference-based design in which users express a preference for document A or document B given document C. We argue that while the user study shows the subtopic model is good, there are many other factors apart from novelty and redundancy that may be influencing user preferences. From this, we introduce a new framework to construct an ideal diversity ranking using only preference judgments, with no explicit subtopic judgments whatsoever.",null,null
6,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval],null,null
7,"Keywords: diversity, user study, preference judgments",null,null
8,1. INTRODUCTION,null,null
9,"Research on novelty and diversity aims to improve the effectiveness of search engines by providing results that serve a range of possible user intents for the given query. These problems have been the subject of much interest in IR and web search recently, including the focus of a TREC task1. Batch effectiveness evaluation of retrieval systems serves sev-",null,null
10,1TREC 2009-2011 Web Track Diversity task focuses on novelty and diversity in search results.,null,null
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$10.00.",null,null
12,"eral important purposes: first, giving developers and researchers a measurable objective; second, allowing for failure analysis and troubleshooting; and third, trying to estimate how useful search results will be to users. For the last of these, it is helpful to think of the evaluation measure and relevance judgments as a model of user utility. Measures like precision and recall can be seen as modeling utility in terms of the proportion of retrieved documents that are relevant and the proportion of relevant documents retrieved; measures like discounted cumulative gain or expected reciprocal rank offer a more refined model of user utility that incorporates graded judgments and rank-based discounts.",null,null
13,"None of these models capture novelty or redundancy in ranked results. All of them will reward a system for retrieving the same relevant document 10 times in a row, or 10 relevant documents that are only superficially different from each other; while that may be useful for knowing whether retrieval features are working correctly, it is not likely to be very useful to a user. Diversity evaluation attempts to model novelty and redundancy in ranked results so as to give a more precise model of user utility.",null,null
14,"Current diversity evaluation measures in the literature require judgments of relevance to subtopics (also called aspects, facets, or nuggets) of a topic. For example, judgments for the query Windows would include binary relevance judgments for each document for the subtopics window panes, windows operating system, etc. These subtopic judgments are used to determine whether a document is redundant with a previously-ranked document, or whether it contains some new information that a user might find interesting, or whether it is relevant to an alternative intent and perhaps not useful to this user but still useful enough to a different user. Measures like -nDCG, ERR-IA, subtopic recall, and D-measures [?, ?, ?, ?] all use this same basic model, assigning more value to a document with more novel subtopics and less value to one with more redundant subtopics.",null,null
15,"Like any model, the subtopic model surely has shortcomings. Novelty and redundancy are certainly not the only reasons a user might prefer one relevant document over another. Apart from a study by Sanderson et al. that showed that user preferences for rankings correlate with -nDCG [?], there has not been much work on validating this model against real user preferences. And if the model does not track user preferences, then it is hard to justify its continued use: it conflates various aspects of effectiveness in such a way that, if used as an objective function, it can be difficult to understand the precise effect of change in the ranker.",null,null
16,861,null,null
17,"Fortunately these measures produce directly-testable hypotheses about user preferences. In this work we describe a novel user study to test these hypotheses. In Section 2 we start by describing the diversity retrieval problem in more detail and define the model more precisely. In Section 3 we present a user study, including a crowdsourced design; we show that while the model is not perfect, it is certainly not invalid. Section 4 builds on this by presenting a preferencebased method for determining a diversity-aware ranking of documents. We conclude in Section 5.",null,null
18,2. NOVELTY RANKING TASK,null,null
19,"Consider a user that has an unambiguous but broad information need and goes to a search engine to help satisfy it. This user will input a query and then see a ranked list of results, some of which will be relevant and some of which will not. The user will presumably click the relevant results to view and absorb the information they contain. Ideally, each relevant result would provide some new information that was not provided by previous relevant results; in other words, the relevant results would not be redundant with each other. The idea is that, each time a user clicks on a new relevant document, the amount of knowledge a user gains must be maximized by the novel content in the document.",null,null
20,"The goal of ranking documents with novelty is to ensure that each relevant document a user sees as they progress down a ranked list provides new, non-redundant information that will help them satisfy their need. This means that a ranking of documents cannot be based solely on the probability of relevance; the novelty of a document depends to no small degree on the documents that have been ranked above it. Similarly, evaluation of these results cannot be based solely on binary or even graded relevance judgments, since these judgments are made to individual documents independently of all the other documents that might have been ranked. Part of studying the task is defining evaluation measures that can model redundancy and novelty.",null,null
21,2.1 Relationship With Other Tasks,null,null
22,"The novelty task has similarities with some existing tasks such as the diversity task studied as part of the TREC Web track. Diversity aims at retrieving a subset of documents that has the maximum coverage of subtopics with the assumption that different users may be interested in different subtopics. In novelty ranking, the goal is to provide a set of documents for a single topic from which the user can get as much information as possible for that particular topic. We assume all users are interested in all of the subtopics, like the standard ad hoc assumption that all users are interested in all of the relevant material.",null,null
23,2.2 Intrinsic vs Extrinsic Diversity,null,null
24,"Researchers in the past have identified two types of diversity: extrinsic and intrinsic [?]. Extrinsic diversity addresses the uncertainty in an ambiguous query where the intent is unclear and is best served by a ranking of documents covering several intents. Intrinsic diversity can be described as diversification that focuses on reducing redundancy and providing novel information for an unambiguous but still underspecified information need. In our work, we focus on intrinsic diversity which we refer to as novelty ranking, as we believe it will be easier for assessors to express preferences when there is no ambiguity of intent.",null,null
25,2.3 Evaluation,null,null
26,Evaluation measures for novelty and diversity must account for both relevance and novelty in the result set. It is important that redundancy caused by documents containing previously retrieved subtopics be penalized and documents containing novel information be rewarded. Most evaluation measures solve this problem by requiring that the subtopics for a query be known and that documents have been judged with respect to subtopics.,null,null
27,2.3.1 Existing Evaluation Measures,null,null
28,"Subtopic recall. Subtopic recall measures the number of unique subtopics retrieved at a given rank [?]. Given that a query q has m subtopics, the subtopic recall at rank k is given by the ratio of number of unique subtopics contained by the subset of document up to rank k to the total number of subtopics m.",null,null
29,"k i,1",null,null
30,subtopics(di),null,null
31,"S-recall@k ,",null,null
32,(1),null,null
33,m,null,null
34,-nDCG scores a result set by rewarding newly found subtopics,null,null
35,and penalizing redundant subtopics. In order to calculate -,null,null
36,nDCG we must first compute the gain vector [?]. The gain,null,null
37,vector is computed by summing over subtopics appearing in,null,null
38,the document at rank k:,null,null
39,m,null,null
40,"G[i] ,"" (1 - )cj,i-1""",null,null
41,(2),null,null
42,"j,1",null,null
43,"where cj,i is the number of times subtopic j has appeared in documents up to (and including) rank i. Once the gain vector is computed, a discount is applied at each rank to penalize documents as the rank decreases. The most commonly used discount function is the log2(1 + i), although other discount functions are possible. The discounted cumulative gain is given by",null,null
44,k,null,null
45,G[i],null,null
46,"DCG@k ,",null,null
47,(3),null,null
48,"i,1 log2(1 + i)",null,null
49,"-DCG must be normalized to compare the scores against various topics. This is done by finding an ""ideal"" ranking that maximizes -DCG, which can be done using a greedy algorithm. The ratio of -DCG to that ideal gives -nDCG.",null,null
50,"Intent-aware family. Agrawal et al. studied the problem of answering ambiguous web queries, which is similar to the subtopic retrieval problem [?]. The focus of their evaluation measure is to measure the coverage of each intent separately for each query and combine them with a probability distribution of the user intents. They call this the intent-aware family of measures. It can be used with most of the traditional measures for evaluations such as precision@k, MAP, nDCG, and so on.",null,null
51,"ERR-IA. Expected Reciprocal Rank (ERR) is a measure based on ""diminishing returns"" for relevant documents [?]. According to this measure, the contribution of each document is based on the relevance of documents ranked above it. The discount function is therefore not just dependent on the rank but also on relevance of previously ranked documents. A weighted average of the ERR measures for each interpretation would give the intent-aware version of ERR [?].",null,null
52,862,null,null
53,D-Measure. The D and the D# measures described by Sakai et al. [?] aims to combine two properties into a single evaluation measure. The first property is to retrieval documents covering as many intents as possible and the second is to rank documents relevant to more popular intents higher than documents relevant to less popular intents.,null,null
54,2.3.2 Principles of Existing Evaluation Measures,null,null
55,"All of these measures estimate effectiveness of a system's ranking by iterating over the ranking, rewarding relevant documents containing a unseen subtopic(s) and penalizing relevant documents containing subtopic(s) seen before in the ranking. They are all based on a few principles in general:",null,null
56,1. A document with more unseen subtopics is worth more than a document with fewer unseen subtopics;,null,null
57,2. A document with both unseen and already-seen subtopics is worth more than a document with only the same unseen subtopic;,null,null
58,3. A document with unseen subtopics is worth more than a document with only redundant subtopics.,null,null
59,One of our goals with this work is to test whether these principles hold for real users.,null,null
60,2.4 Data,null,null
61,"Our analysis was conducted primarily on the Newswire data created by Allan et al. [?] to investigate the relationship between system performance and human performance on a subtopic retrieval task. The data consists of 61 topics, each with a short (3-6 word) query, and judgments of relevance to documents in a subset of the TDT5 corpus. The Newswire data includes relevance judgments for the top 130 documents retrieved by a query-likelihood language model for the short query for each query. The judgments consists of binary relevance judgments for each document, and for each relevant document, a list of subtopics contained in that document. This data reflects an intrinsic diversity task and is therefore most appropriate to this work.",null,null
62,3. FACTORS INFLUENCING USER PREFERENCES,null,null
63,"In Section 2.3.2, we identified some principles on which the evaluations for diversity are based on. In this section we tests if these principles hold for real users and further study in detail the role of subtopics in influencing user preference. Although in practice the same evaluation measures are used for both intrinsic and extrinsic diversity, our focus is on intrinsic diversity as it is easier for assessors to understand the concept of relevance when there is no ambiguity of intent. We explore the factors that influence user preference for novelty ranking using a preference based framework.",null,null
64,3.1 Triplet Framework,null,null
65,"The idea of pairwise preference judgments is relatively new in the IR literature, having been introduced by Rorvig in 1990 [?] but not subject to empirical study until the past several years [?, ?]. Comparison studies between absolute and preference judgments show that preference judgments can often be made faster than graded judgments, with better agreement between assessors (and more consistency with individual assessors) [?]. Also with preferences tassessors can make much finer distinctions between documents.",null,null
66,"We propose a preference-based framework to study novelty consisting of a set up in which three relevant documents that we refer to as a triplet are displayed such that one of them appears at the top and the other two are displayed as a pair below the top document. We will use DT , DL, and DR to denote the top, left, and right documents respectively, and a triplet as DL, DR|DT . An assessor shown such a triplet would be asked to choose which of DL or DR they would prefer to see as the second document in a ranking given that DT is first, or in other words, they would express a preference for DL or DR conditional on DT . For the purpose of this study we will assume we have relevance judgments to a topic, and for each relevant document, binary judgments of relevance to a set of subtopics. Thus we can represent a document as the set of subtopics it has been judged relevant to, e.g. Di ,"" {Sj, Sk} means document i is relevant to subtopics j and k. Varying the number of subtopics in top, left and right documents yields specific hypotheses about preferences for novelty over redundancy.""",null,null
67,3.2 Hypotheses,null,null
68,"The triplet framework allows us to collect judgments for novelty based on preferences and also enables us to test various hypotheses. As discussed above, varying the number of subtopics in DT , DL and DR it is possible to enumerate various hypotheses concerning the effect of subtopics in a document. We define two types of hypotheses; one very specific with respect to subtopic counts and redundancy, and the other more general.",null,null
69,"Hypothesis Set 1 : First we propose the simplest possible hypotheses that capture the three principles above. We will denote a preference between two documents using , e.g. DL DR means document DL is preferred to document DR. Then the three hypotheses stated formally are:",null,null
70,"H1: if DL, DR|DT ,"" {S2}, {S1}|{S1} , then DL DR (novelty is better than redundancy)""",null,null
71,"H2: if DL, DR|DT ,"" {S1, S2}, {S2}|{S1} , then DL DR (novelty+redundancy is better than novelty alone)""",null,null
72,"H3: if DL, DR|DT ,"" {S2, S3}, {S2}|{S1} , then DL DR (novelty+novelty is better than novelty alone)""",null,null
73,Hypothesis Set 2 : Here we define a class of hypotheses in which the number of subtopics contained in each document in a triplet is categorized by relative quantity. We identify six variables based on number of subtopics that almost completely describe the novelty and redundancy present in the triplet. The six variable are as follows:,null,null
74,1. T n - Number of subtopics in DT ; 2. N Ln - Number of subtopic in DL not present in DT ; 3. N Rn - Number of subtopic in DR not present in DT ; 4. Sn - Number of subtopics shared between DL and DR; 5. RLn - Number of subtopics in DL and present in DT ; 6. RRn - Number of subtopics in DR and present in DT .,null,null
75,"The number of subtopics for each of the six variables are categorized as low or high. The six variables enable us to test the effect of novelty and redundancy w.r.t the number of subtopics in a triplet. The variables N Ln and N Rn focus on novelty whereas RLn and RRn focuses on redundancy. For instance, by varying N Ln and N Rn and holding the other variables constant, it is possible to test the effect of the relative quantity of novel subtopics in a document.",null,null
76,863,null,null
77,3.3 Experimental Design,null,null
78,"In this section we describe the experimental design used to test the hypotheses defined above. Notice that the defined hypotheses are based on the number of subtopics contained in the documents and they fit into the triplet framework which requires conditional preference judgments. Therefore, to test our hypotheses, two kinds of judgments were needed: subtopic level judgments and conditional preference judgments. The subtopic level judgments were obtained from the data described in Section 2.4. Conditional preference judgments were collected using crowd sourcing as it is a fast, easy and a low cost way of collection user judgments [?].",null,null
79,"We used Amazon Mechanical Turk (AMT) [?]; an online labor marketplace to collect user judgments. AMT works as follow: requestor create a group of Human Intelligence Task (HITs) with various constraints and worker from the marketplace works on these task to complete the task. In this work we, use a design similar to the one used by Chandar and Cartertte [?]. Designing a user study using AMT involves deciding on the HIT layout and HIT properties.",null,null
80,3.3.1 HIT Layout,null,null
81,"In order to collect user judgments for our hypotheses using AMT, we had to organize the triplets satisfying a given hypothesis into HITs. Each HIT consisted of the following (in order of display): a set of instructions about the task, original keyword query, topic description, five preference triplets, and a comment field allowing worker to provide feedback. A brief description about each element is given below:",null,null
82,"Guidelines: Workers were provided with a set of instructions and guidelines prior to judging. Guidelines specified that workers should assume that everything they know about the topic is in the top document and are trying to find a document that would be most useful for learning more about the topic. Guidelines did not mention anything about subtopics, or even novelty/redundancy except as examples of properties assessors might take into account in their preferences (along with recency, ease of reading, and relevance).",null,null
83,"Query text and topic description: The query text described the topic in a few words (we used the topic ""titles"" in the traditional TREC jargon) and topic description provided a more verbose and informative description about the topic. Again, there was no mention of explicit subtopics.",null,null
84,"Preference triplet: Figure 1 shows an example preference triplet with the query text and topic description. Each preference triplet consists of three documents, all of which were relevant to the topic and the document were picked randomly from the data described in Section 2.4 to meet the constraints of a given hypothesis. One document appeared at the top followed by two documents below it, the triplets were chosen randomly such that the hypothesis constraints were satisfied. A HIT consisted of five preference triplets belonging to the same query shown one below the other.",null,null
85,"The triplets were followed by a preference option for the workers to indicate which of the two documents they preferred. The workers were asked to pick the document from the lower two that provided the most new information, assuming that all the information they know about the topic is in the top document. They could express a preference based on whatever criteria they liked; we listed some examples in the guidelines. Note that we do not show them any subtopics, nor do we ask them to try to determine subtopics and make a preference based on that.",null,null
86,"Comments Field was provided at the end, so that the workers could to provide a common feedback for all the five triplets, if they chose to do so.",null,null
87,3.3.2 HIT Properties,null,null
88,"Workers are paid for each HIT they complete and picking an appropriate amount for each task is always tricky. In our study, workers were paid $0.80 for each completed HIT. Also each HIT had a time limit of three hours before which the HIT had to be completed. While the actual task might not take three hours to complete; the extra time allows them to take breaks if needed, since the workers had to read fifteen documents per HIT. We had five separate workers judge each HIT for the our first set of hypotheses and three separate workers judge each HIT for the our second set of hypotheses.",null,null
89,3.3.3 Triplets,null,null
90,"Triplets were generated by randomly picking the three relevant documents for a given query and representing them as subtopic(s). Triplets for the first set of hypotheses in Section 3.2 were considered such that the constraints are satisfied for each hypothesis. For example, for hypothesis H1 given a query x the triplet would consist of DT containing only the subtopic S1 and DL containing the subtopics S1 and S2. Six queries were used to test the first set of hypotheses with four triplets for each query.",null,null
91,"The triplets were generated in a similar way for the second set of hypotheses but the constraints for each hypothesis were based on the six variables described in 3.2. For example, a triplet with a variable setting of T n ,"" High, Sn "","" High, N Ln "", High and N Rn , High would contain 5 or more subtopics in the top document DT and 3 or more subtopics in the left and right documents (DL and DR) such that there are 1 or more subtopics shared between DL and DR. The details of the number of subtopics for each categories of high and low levels for each variables are provided in the Table 1. Eight queries were used to test the second set of hypotheses with four different triplets for each query.",null,null
92,3.3.4 Quality Control,null,null
93,"There are two major concerns in collecting judgments through crowdsourcing platform such as AMT. One is ""Do the workers really understand the task?"" and the other is ""Are they making faithful effort to do the work or clicking randomly?"". We address these concerns using three techniques: majority vote, trap questions, and qualifications.",null,null
94,"Majority vote: Since novelty judgments to be made by the workers are subjective and it is possible some workers are clicking randomly, having more than one person judge a triplet is common practice to improve the quality of judgments. In our study, each HIT was judged by 5 or 3 different workers (depending on hypothesis set). We look at the individual preferences as well as the majority preference.",null,null
95,"Trap questions: Triplets for which answers are obvious were included to assess the validity of the results. We included two kinds of trap questions: ""non-relevant document trap"" and ""identical document trap"". For the former, one of the bottom two documents was not relevant to the topic and should never be preferred. For the latter, the top document and one of the bottom two documents were the same. The workers were expected to pick the non-identical document as it provides novel information. One of the five triplets in a HIT was a trap and the type was chosen randomly.",null,null
96,864,null,null
97,Figure 1: Screenshot of the preference triple along with the query text and description.,null,null
98,variable,null,null
99,Tn Sn NLn NRn RLn RRn,null,null
100,number of subtopics,null,null
101,low,null,null
102,high,null,null
103,1-4,null,null
104,5-9,null,null
105,0,null,null
106,1-2,null,null
107,0-2,null,null
108,3-6,null,null
109,0-2,null,null
110,3-6,null,null
111,0,null,null
112,1-2,null,null
113,0,null,null
114,1-2,null,null
115,Table 1: Number of subtopics corresponding to the high and low categories for each variable in our second set of hypotheses.,null,null
116,"Qualifications: It is possible to qualify workers before they are allowed to work on your HITs in AMT. Worker qualifications can be determined based on historical performance such as percentage of approved HITs. Also, worker's qualification can be based on a short questionnaire or a test. The two qualifications used in are study are explained below:",null,null
117,1. Approval rate: HITs can be restricted to workers with an overall minimum percentage of approval. It is commonly used for improving accuracy and reducing spammer from working on your task. An overall approval rate of 95% was required to work on our HITs.,null,null
118,"2. Qualification test: Qualification tests can be used to ensure that workers have the required skill and knowledge to perform the task. In our case, workers had to be trained to look for documents that provide novel information given the top document. We created a qualification test having the same design layout as the actual task but had only three triplets. Two of the three triplets were identical document traps and the other was a non-relevant trap with instructions for each triplet aiding in making a preference.",null,null
119,3.4 Results and Analysis,null,null
120,"Judgments for a total of 60 triplets (out of which 12 triplets were traps) were obtained for the hypothesis set 1. Since we had each triplet assessed by five separate assessors, a total of 300 judgments were collected out of which 60 were traps. We had 39 unique workers (identified by worker ID) on AMT judge these triplets across six topics.",null,null
121,Table 2 shows results for H1. It turns out that there is no clear preference for either redundant or novel documents,null,null
122,H1 topic childhood obesity terrorism indonesia earthquakes weapons for urban fighting total,null,null
123,all prefs same new,null,null
124,6 14 8 12 15 5 15 5 44 36,null,null
125,consensus same new,null,null
126,13 13 31 31 88,null,null
127,"Table 2: Results for H1: that novelty is preferred to redundancy. The ""all prefs"" columns give the number of preferences for the redundant and the novel document for all assessors. The ""consensus"" columns take a majority vote for each triplet and report the resulting number of preferences.",null,null
128,H2 topic kerry endorsement childhood obesity terrorism indonesia libya sanctions total,null,null
129,all prefs,null,null
130,new same+new,null,null
131,9,null,null
132,11,null,null
133,4,null,null
134,16,null,null
135,13,null,null
136,7,null,null
137,4,null,null
138,16,null,null
139,30,null,null
140,50,null,null
141,consensus,null,null
142,new same+new,null,null
143,2,null,null
144,2,null,null
145,0,null,null
146,4,null,null
147,4,null,null
148,0,null,null
149,0,null,null
150,4,null,null
151,6,null,null
152,10,null,null
153,"Table 3: Results for H2: that novelty and redundancy together are preferred to novelty alone. The ""all prefs"" columns give the number of preferences for the redundant+novel document and the novel document for all assessors. The ""consensus"" columns take a majority vote for each triplet and report the resulting number of preferences.",null,null
154,"for the four queries. For two of our queries assessors tended to prefer the novel choice; for the other two they tended to prefer the redundant choice. When we use majority vote to determine a consensus for each triplet, we find that the outcomes are exactly equal. Thus while we cannot reject H1, we have to admit that if it holds it is much less strong than we expected.",null,null
155,"Table 3 shows a clearer (but still not transparent) preference for H2, novelty and redundancy together over novelty alone. Over all assessors and all triplets, the preference is significant by a binomial test (50 successes out of 80 trials; p < 0.05). Still, there is one query (""john kerry endorsement"") for which the difference is insubstantial, and one that has the opposite result (""terrorism indonesia""). The latter",null,null
156,865,null,null
157,H3 topic kerry endorsement childhood obesity terrorism indonesia libya sanctions total,null,null
158,all prefs,null,null
159,new new+new,null,null
160,9,null,null
161,11,null,null
162,3,null,null
163,17,null,null
164,2,null,null
165,18,null,null
166,8,null,null
167,12,null,null
168,22,null,null
169,58,null,null
170,consensus,null,null
171,new new+new,null,null
172,1,null,null
173,3,null,null
174,0,null,null
175,4,null,null
176,0,null,null
177,4,null,null
178,1,null,null
179,3,null,null
180,2,null,null
181,14,null,null
182,"Table 4: Results for H3: that two novel subtopics are preferred to one. The ""all prefs"" columns give the number of preferences for the novel+novel document and the novel document for all assessors. The ""consensus"" columns take a majority vote for each triplet and report the resulting number of preferences.",null,null
183,topic earthquakes terry nichols guilt evidence medicare drug coverage oil producing countries no child left behind european union member german headscarf court ohio highway shooting,null,null
184,total,null,null
185,high low 76 - 20 (79%) 75 - 21 (78%) 73 - 23 (76%) 65 - 31 (68%) 62 - 34 (65%) 61 - 35 (64%) 59 - 37 (61%) 51 - 45 (53%) 522 - 246 (68%),null,null
186,left right 96 - 96 (50%) 100 - 92 (52%) 86 - 106 (45%) 89 - 103 (46%) 81 - 111 (42%) 103 - 89 (54%) 84 - 108 (44%) 104 - 88 (54%) 743 - 793 (48%),null,null
187,"Table 5: Results of preference judgments by the number of new subtopics in DL, DR over DT (variables N Ln, N Rn). Counts are aggregated over all values of T n, Sn per query. The first column gives preference counts for the document with more new subtopics over the document with fewer when N Ln N Rn. The second column is the baseline, giving counts for preferences for left over right.",null,null
188,"case is particularly interesting because it is the opposite of what we would expect after seeing the results in Table 2: given that assessors preferred redundant documents to novel documents for that query, why would they prefer novel documents to documents with both novelty and redundancy?",null,null
189,"Table 4, with results for H3, is the strongest positive result: a clear preference for documents with two new subtopics over documents with just one. In this case both results are significant (58 successes out of 80 trials and p < 0.0001 over all triplets and all assessors; 14 successes out of 16 trials and p < 0.01 for majority voting). Nevertheless, there are still queries for which the preference is weak.",null,null
190,"Based on this, it seems like novelty + novelty > novelty, novelty+redundancy  novelty, but not novelty  redundancy.",null,null
191,"There were a total of 640 triplets (out of which 128 triplets were traps) for the second part of our study. Each of these triplets were judged by three separate assessors, thus a total of 1920 judgments were made out of which 384 were traps. And for this study we had 38 unique workers (identified by worker ID) on AMT working on our triplets. Some of these workers had worked on the first study as well. Almost 70% of the judgments were completed by 15% of the workers and about 93% of the irrelevant traps were passed by the workers. This power law distribution for our task has been observed earlier for other tasks as well [?], we hope to investigate on this issue in the future.",null,null
192,"Triplets were generated by controlling four variable: T n, Sn, N Ln and N Rn, we obtained sixteen unique settings for the four variable combination as each of the four variables",null,null
193,"were categorized into low and high with equal number of triplet in each setting. This allowed us to perform ANOVA such that the number of new subtopics in the left or right document was the primary predictor of preference, with the number of subtopics in the four variables as the secondary predictors. ANOVA indicated that there is a lot of residual variance, suggesting there are various factors influencing preferences that we have not included in the model.",null,null
194,"Table 5 analyzes preferences for more new subtopics in DL or DR over fewer new subtopics (variables N Ln and N Rn) by topic. We looked at four cases: the first two (N Ln high, N Rn low; N Ln low, N Rn high) can tell us whether users prefer to see more new subtopics over fewer, while the second (N Ln high, N Rn high; N Ln low, N Ln low) along with the first two give us a baseline preference for left over right. While we would expect the baseline preference to be 50% (since which document appears on the left versus right is randomized), there may be other unmodeled factors that cause it to be more or less than 50%, so it is useful to compare to this baseline.",null,null
195,"It is clear from this table that users as a group prefer to see more new subtopics, just as we saw in the results for H3 above. Still, there are individual queries for which that preference is not strong, especially when compared to the baseline (e.g. the ""Ohio highway shooting"" topic), and even when the preference is strong in aggregate there are cases where they do not hold.",null,null
196,"There is some effect due to the number of subtopics in DT , with preferences for more new subtopics stronger when T n is low. When it is low, the preference for high versus low is 271 to 113 (70%) against a baseline preference for left over right of 347 to 421 (45%)2. When T n is high, the preference for high versus low is 251 to 133 (65%) against a baseline of 396 to 372 (52%). We conjecture that when the top document already has a lot of information about the topic, there is a little less reason to prefer either left or right regardless of how many subtopics they contain.",null,null
197,"There is not much effect due to the number of shared subtopics between DL and DR. When Sn is low, the preference for more new subtopics over fewer is 268 to 116 (70%) against a baseline of 370 to 398 (48%); when it is high, the preference for more new is 254 to 130 (66%) against a baseline of 373 to 395 (49%). This may be because fewer shared subtopics makes it easier to express a preference. However, the effect is too small to draw any firm conclusion.",null,null
198,"There is interesting interaction between the number of new subtopics and the number of redundant subtopics in DL and DR. When one has a high number of new subtopics and the other has a low number of new subtopics, number of redundant subtopics seems to influence the strength of preference for the one with more new subtopics: if there are more redundant subtopics along with the new subtopics, the preference is 118 to 44 (73%), but when there are fewer redundant subtopics with more new subtopics and more redundant subtopics with fewer new subtopics, the preference is even at 51 to 51 (50%). This suggests again that users like redundancy, sometimes enough to overcome a lack of novelty. However we must note that data here is sparse, also the two variables RRn and RLn were not the ones that we controlled for in our experiment.",null,null
199,2We presume that the greater-than-expected preference for the right document is just due to random chance.,null,null
200,866,null,null
201,Topic childhood obesity weapons for urban fighting kerry endorsement libya sanctions earthquake terrorism indonesia,null,null
202,Mean,null,null
203,Agreement 0.71 0.92 0.58 0.62 0.72 0.71 0.69,null,null
204,No. triplets 15 5 10 10 5 15 60,null,null
205,Table 6: Interassessor agreement scores for each topic for the first study.,null,null
206,Topic oil producing countries terry nichols guilt evidence no child left behind german headscarf court medicare drug coverage earthquakes european union member ohio highway shooting,null,null
207,Mean,null,null
208,Agreement 0.63 0.72 0.61 0.57 0.66 0.65 0.59 0.59 0.63,null,null
209,No. triplets 80 80 80 80 80 80 80 80 640,null,null
210,Table 7: Interassessor agreement scores for each topic for the second study.,null,null
211,3.5 Interassessor Agreement,null,null
212,"As described above, each triplet was judged by five different workers for the first study (hypotheses set 1) and by three workers for the second study (hypotheses set 2). We calculated an inter-assessor agreement score for each triplet for the first study as follows. The judgments were considered as 10 pairs of answers given for a single triplet, adding 1 points to the score if the two workers agreed (complete agreement); and adding nothing if they judged different documents (no agreement). The perfect agreement would sum up 10 points, so we divided the score obtained by 10 and normalized from 0 (no agreement at all) to 1 (perfect agreement). Mean agreement for the first study is given in Table 6 for each query. Overall a high mean agreement of 0.7 was found across all triplets and the scores are close to the agreement observed previously [?]. Since the mean agreement was quite high for the first study, it encouraged us to reduce the number of workers for each triplet and increase the number of queries for the second study. Similar mean agreement can be seen for the second study in Table 7.",null,null
213,3.6 Possible Confounding Effects in Display,null,null
214,"The way the hits were displayed may introduce some confounding effects, possibly causing assessors to choose documents for reasons other than novelty or redundancy. We investigated two such effects: Document length A preference towards shorter documents was observed in general, though the preference gets weaker over the three hypotheses. For H1, assessors preferred the shorter document in 79% of triplets. For H2, that decreased to 71% of triplets, and for H3 it dropped steeply to only 44%. However, it is also true that the mean difference in length for the pair of documents they were choosing between was greatest for H1 triplets and least for H3 triplets (H1:158 terms, H2:126 terms, H3:47 terms). Therefore its safe to conclude there seems to be a preference towards shorter documents.",null,null
215,"Highlighted terms It turns out that assessors tended to prefer the document with fewer highlighted query terms. For H1, assessors preferred the document with more query terms only 35% of the time. For H2 that drops to 13%, and for H3 it comes back up to 29%. The mean difference in number of query term occurrences is quite low, only on the order of one additional occurrence on average for H1 and H3 documents, and only 0.2 additional occurrences for H2. While the effect is significant, it seems unlikely that assessors can pick up on such small differences. We think the effect is more likely due to the distribution of subtopics in documents.",null,null
216,3.7 Additional Investigation,null,null
217,"While the results suggest that the number of subtopics influences user preferences, it is also clear that from the analysis that other factors are affecting preferences. The results from H1 and the weaker preference in H2 were not what we expected. We investigated this more by looking at a number of triplets ourselves and identifying some new hypotheses about why assessors were making the preferences they were. From looking at triplets for the ""earthquakes"" topic, we identified three possible reasons for preferring a document with a redundant subtopic:",null,null
218,· it updates or corrects information in the top document; · it significantly expands on the information in the top,null,null
219,"document; · despite having a novel subtopic, the other choice pro-",null,null
220,vides little information of value.,null,null
221,"This suggests to us that there are other factors that affect user preferences, in particular recency, completeness, and value. It may also suggest that there are implicit subtopics (at finer levels of granularity) that the original assessors did not identify, but that make a difference in preferences. None of this is surprising, but there is currently no evaluation paradigm of note that take all of these factors into account in a holistic way. Preference judgments can, and this analysis suggests additional hypotheses for testing with preferences.",null,null
222,4. PREFERENCE JUDGMENTS FOR AN IDEAL NOVELTY RANKING,null,null
223,"The user study shows that although users tend to prefer documents containing more novel subtopics, it is also evident that factors other than subtopics play a vital role. The study also shows that the presence of subtopic in a document is taken into account implicitly and preferences are based not only on the number of subtopics but also on several other factors that include subtopic importance, relevance of the subtopic, readability of the document, etc. In this section, we propose an approach that attempts to capture these factors implicitly using a preference based framework to form a full ranking of documents with novelty as an implicit quality.",null,null
224,"Our approach involves a series of sets of preference comparisons. Each set is essentially a comparison sort algorithm, with the comparison function a simple preference conditional on information contained in top-ranked documents from prior sets of comparisons, generalizing the triplet framework we introduced above.",null,null
225,"The first set of preferences is meant to produce a relevance ranking: given a choice between two documents, assessors select the one they prefer, with topical relevance being the primary consideration in the judgment. Once these comparisons are done for all pairs, it is possible to obtain the best or",null,null
226,867,null,null
227,"""most relevant"" document, i.e. the most preferred document based on the number of times a document was selected.",null,null
228,"For the second set of preferences, the assessor needs to consider the novelty of information in the document along with relevance. This leads to exactly the triplet framework we used previously. For this second set, the assessor will see the top-ranked document from the previous set as DT , then pick from two documents DL, DR conditional on that.",null,null
229,"The sequence continues by adding more documents to the top. For the third set, the comparison involves information in two previously ranked documents along with a pair of documents; for the fourth, it involves information in three previously ranked documents along with a pair. This continues to the final set, in which there are only two documents to compare conditional on n - 2 previous top documents.",null,null
230,"When complete, the most preferred document in the first set takes rank 1, the most preferred document in the second set takes rank 2, and so on. Observe that the first set of judgments correspond to relevance judgments and sets 2 through n - 1 correspond to novelty.",null,null
231,"This method asks for a very large number of preferences: if fully judged, there would be O(n2) preferences in the first set, O((n - 1)2) in the second, and so on, for a total of O(n3) judgments, which is almost certainly infeasible. We hypothesize that the first two sets of preferences (one for relevance and one for novelty) will provide a near-optimal approximation to the full set and if judgments are transitive (that is, if document A is preferred to B and B is preferred to C, then A should be preferred to C as well), the number of judgements needed can be reduced drastically. We will test both of these hypotheses below.",null,null
232,4.1 Experimental Design,null,null
233,"As described above, we asked assessors to make the first two sets of judgments for each topic. The first set of judgments attempts to rank documents by relevance to the topic; intuitively, these judgments could be used to find the most relevant document in the ranked list: that which is preferred to everything else (assuming judgments are transitive) is most relevant. The second set of judgments attempts to rank the remaining documents by the degree of novelty they provide given that we know the document that is ranked at position one from the first set.",null,null
234,"For this experiment we elected not to use MTurk. We wanted a single assessor to do all the preferences for a single topic, first so they would be able to build a familiarity with the topic as they judge, and second so we could assess their self-consistency. Thus we asked students at our institution to participate in the study. These students are mostly in computer science, mostly studying NLP and language technologies. Like the workers in the previous section, they were not given explicit instruction regarding subtopics; they were only asked to express a preference. We had 6 assessors complete preferences for at least one topic.",null,null
235,"We designed two new web interfaces running on a local server to be used by assessors to collect preferences for both relevance and novelty, the first two sets of preferences described above. Common elements in both interfaces are the original keyword query, topic description, article texts (with query keywords highlighted), preference buttons for indicating which of the two documents the assessor prefers, a progress bar with a rough estimate of the percentage of preferences completed, and a comment field allowing them",null,null
236,to say why they made their choice (if they wish). Elements specific to each experiment are described in more detail in the respective sections below.,null,null
237,"For this study, we asked assessors to judge all pairs of documents in the first two sets. Topics were chosen from the data described in Section 2.4. We wanted to include all known relevant documents for the topic in the preference experiment. Since we were asking assessors for all pairs, we limited our selection to topics with a relatively small number of relevant documents. We then added a randomlyselected set of nonrelevant documents from among the topranked documents for the topic. We kept the total number of preferences in an experiment to less than 200.",null,null
238,"The first two documents shown to an assessor were chosen randomly from the set of all documents to be ranked. After that, whichever document the assessor preferred remained fixed in the interface; only the other document changed. This way the assessor only had to read one new document after each judgment, just as they would in normal singledocument assessing. Furthermore after the first O(n) judgments we know the top-ranked document for the current set, and thus if transitivity holds it follows that we only need a linear number of preferences at each set.",null,null
239,4.1.1 First Level Judgments: Relevance Preferences,null,null
240,"In the first set of judgments, the assessor was shown two documents (news articles) and a statement of an information need (a topic); the task was to pick the most preferred document using the ""prefer left"" or ""prefer right"" buttons. A screenshot of the first level judgments is shown in Figure 2",null,null
241,"The assessor was provided with a set of instructions and guidelines prior to judging. The guidelines specified that the assessor should assume they know nothing about the topic and are trying to find documents that are topically relevant, that is, that provide some information about it. If a document contains no topical information, the assessor could judge it ""not relevant""; if they do so, the system will assume they prefer every other document to that one and remove it from this set as well as all subsequent sets so it will not be seen in future comparisons. Assessors could also judge ""both not relevant"" to remove both from the set and see a new pair. These buttons can make the task easier by reducing the total number of preference judgments the assessors need to make.",null,null
242,"If both documents were topically relevant, the assessor could express a preference based on whatever criteria they liked. Some suggestions included in the guidelines were: one document is more focused on the topic than the other; one document has more information about the topic than the other; one document has more detailed information than the other; one document is easier to read than the other. Assessors could exit for a break as long as they liked and return at the point where they stopped. A progress indicator let them know roughly how close they were to the end",null,null
243,4.1.2 Second Level Judgments: Novelty Preferences,null,null
244,"For the second set of preferences, the assessor was shown three documents and a statement of an information need (a topic); the task was to pick the most useful document from two of the three to learn more about the topic given what is presented in the third.",null,null
245,The interface for the second level judgment was very similar to the triplet layout shown is Figure 1. One document,null,null
246,868,null,null
247,Figure 2: Screenshot of the preference collection interface for relevance preferences.,null,null
248,appeared at the top of the screen; this was the most preferred document as identified by the assessor after the first set of preferences. The assessors were asked to pick a document from a pair of documents (appearing below the top document) that provided the most novel information given that they know all the information in the top document.,null,null
249,"Guidelines specified that the assessor should pretend that the top document is the entirety of what they know about the topic, and their goal is now to find the best document for learning more about the topic. Beyond that, they could express a preference based on whatever criteria they liked, including those listed above.",null,null
250,"There are no nonrelevant judgment buttons in this interface. Any document that was judged nonrelevant in the first set of preferences will not be seen in this set. Anything that was relevant in the first set is assumed to still be relevant; if a relevant documents provides no new relevant information, we assume the assessor's preferences will result in that document being ranked near the bottom of this set.",null,null
251,4.2 Experimental Analysis,null,null
252,"As described above, we conducted novelty preference judging with five topics from the data described in Section 2.4. On average, 16.8 documents were judged for each topic. A total of 605 pairs were judged for 4 topics by 6 assessors for experiment levels 1 and 2. We compared these judgments to the original subtopic-based judgments in the data.",null,null
253,"Agreement on Relevance. To assess agreement on the relevance of a document, we assume that any document not explicitly judged not relevant must be relevant. We consider the original relevance judgments derived from the subtopic judgments as the ground truth and assess the performance of our assessors relative to that. Table 9 shows the confusion matrix between assessors making preference judgments and the original assessors making subtopic judgments; broad agreement on the two classes is 71%. Preference assessors identified 76% of the relevant documents that the original assessors found, and 60% of the documents judged relevant by at least one assessor were judged relevant by both. This is a high level of agreement for IR tasks; compare to the 40% agreement on relevance reported by Voorhees [?].",null,null
254,topic,null,null
255,OPEC actions OPEC actions - Alternate childhood obesity childhood obesity - Alternate suicide bombers teens women foreign students visa restrictions,null,null
256,Rank Correlation Level 1 Level 2 0.563 0.534 0.568 0.377 0.467 0.264 0.403 0.394 0.320 0.200 0.532 0.030,null,null
257,Table 8: Kendall's  correlations between rankings from real preference judgments and rankings from simulated preference judgments (for the relevance ranking (level 1) and the novelty ranking (level 2)).,null,null
258,Preference Judgments,null,null
259,Relevant Non-Relevant,null,null
260,Subtopic Judgments,null,null
261,Relevant Non-Relevant,null,null
262,58,null,null
263,20,null,null
264,18,null,null
265,37,null,null
266,Table 9: Confusion matrix for relevance judgments derived from the subtopic judgments in the original Newswire collection and derived from our preference judgments (all queries aggregated).,null,null
267,"Rank Correlation. Another way to compare preference judgments to the original subtopic judgments is by using both to construct a ranking of documents, then computing a rank correlation statistic between the two rankings. The subtopic judgments included in the Newswire data were obtained by assessors explicitly labeling subtopics for each relevant document. We use the subtopic information to simulate preference judgments that might have been obtained via our experiment. For the first set, we always prefer the document with the greatest number of subtopics. (Except in the case of a tie, when we prefer a random document.) For the second set, the top-ranked document from the first set becomes the ""top document"", and then for each pair we prefer the document that contains the greatest number of subtopics that are not in that top-ranked document. The final ranking has the most-preferred document from the first set of preferences at rank 1 followed by the ranking obtained from the second set of preferences.",null,null
268,"Kendall's  rank correlation for each topic for both level 1 and level 2 preference judgments is shown in Table 8. Kendall's  ranges from -1 (lists are reversed) to 1 (lists are exactly the same), with 0 indicating a random reordering. The values we observe are positive and statistically significant (except for level 2 judgments for topic foreign students visa restrictions). Kendall's  is based on pairwise swaps, and thus can be converted into agreement on pairwise preferences by adding 1 and dividing by 2. When doing this we see that agreement is again high for the relevance ranking, and also high for the novelty ranking, well over the 40% observed by Voorhees (except for topic foreign students visa restrictions). We believe this validates our second set of preferences, though certainly the question is not closed.",null,null
269,4.2.1 Transitivity in Preference Judgments,null,null
270,"One issue in using our preference judgments for novelty is that the number of pairwise judgments increases quickly with number of documents. Increase in number of judgments means increase in assessor time, but if the assessors are consistent i.e. if their judgments are transitive, then we can",null,null
271,869,null,null
272,srecall@20 0.70 0.75 0.80 0.85 0.90 0.95 1.00,null,null
273,1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20,null,null
274,Levels,null,null
275,"Figure 3: S-recall increases as we simulate deeper levels of preference judgments, but the first set of novelty preferences (level 2) gives an increase that nearly exceeds all subsequent levels combined.",null,null
276,"reduce the number of preferences from O(n2) to O(n log n) at each level; furthermore, since we really only need the ""best"" document at each level, transitivity would allow us to reduce the number of preferences to O(n) at each level.",null,null
277,"We performed experiments to check for transitivity in the novelty task by looking at triplets of documents. A triplet of documents i, j, k is transitive if and only if i is preferred to j, j is preferred to k, and i is preferred to k. The ratio of number of triplets found to be transitive to the total number of triplets give a measure of transitivity in the preference judgments. On average transitivity holds for 98% across all queries with each query being transitive 96% of the time. This suggests that the assessors are highly consistent in their judgments; thus using a sorting algorithm with minimum information loss could further reduce the number of judgments required. It also suggests that whatever other features of documents (apart from topical relevance and novelty) the assessors are using in their decision process, they are consistent in their use of those features.",null,null
278,4.2.2 How many levels of judgments are needed?,null,null
279,"In this section we show that the first two sets of preferences, i.e. experiments 1 and 2, are approximately sufficient to produce an optimal ranking. We again use preferences simulated from subtopic judgments: a relevance ranking is found by always preferring the document with more subtopics (""level 1""); a first approximation to a novelty ranking is found by always preferring the document with the most subtopics that are not in the top document (""level 2""); a second approximation by always preferring the document with the most subtopics that are not in the first two documents (""level 3""); and so on up to level 20.",null,null
280,"Figure 3 shows the S-recall scores increasing as the number of preference sets increases. Clearly the increase in S-recall from level 1 to level 2 is the largest, nearly exceeding the total increase obtained from all subsequent levels put together. This suggests that the first approximation novelty ranking is likely to be sufficient; this has the benefit of reducing the amount of assessor effort needed to produce the data.",null,null
281,5. CONCLUSION AND FUTURE WORK,null,null
282,We have taken initial steps into investigating the use of preference judgments for novelty ranking tasks. We have proposed a novel framework for obtaining preference judgments for the novelty task and explicated the pros and cons of using preference judgments. Preliminary results for com-,null,null
283,paring explicit subtopic labels with preference judgments suggest that preference judgments can give similar information about both relevance and novelty as the subtopic judgments that are typically used.,null,null
284,"Based on this, we proposed a preference-based approach to obtaining a full ranking for relevance, novelty, and all other factors that contribute to user preferences. We showed that rankings obtained in this way correlate well to rankings based on subtopic judgments, and since assessors are highly self-consistent, probably capturing a great deal of other information as well. Of course, if subtopic judgments were replaced with preferences, we would need a new set of evaluation measures. This clearly is a direction for future work.",null,null
285,"6. REFERENCES [1] Amazon mechanical turk. http://www.mturk.com. [2] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proc. of WSDM, pages 5­14, 2009. [3] J. Allan, B. Carterette, and J. Lewis. When will information retrieval be ""good enough""? In Proc. of SIGIR, pages 433­440, 2005. [4] O. Alonso, D. Rose, and B. Stewart. Crowdsourcing for relevance evaluation. In ACM SIGIR Forum, number 2, pages 9­15, Nov. 2008. [5] J. Arguello, F. Diaz, J. Callan, and B. Carterette. A methodology for evaluating aggregated search results. In Proceedings of the 33rd European Conference on Information Retrieval (ECIR, 2011. [6] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: preference judgments for relevance. In Proceedings of the IR research, 30th European conference on Advances in information retrieval, Proceedings of ECIR, pages 16­27, 2008. [7] P. Chandar and B. Carterette. What qualities do users prefer in diversity ranking. In Proceedings of the 2nd Workshop on Diversity in Document Retrieval, 2012. [8] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results. Information Retrieval, pages 1­21, 2011. [9] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceeding of CIKM, pages 621­630, 2009. [10] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova,",null,null
286,"A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of SIGIR, pages 659­666, 2008. [11] T. Moore and R. Clayton. Evaluating the wisdom of crowds in assessing phishing websites. In In 12th International Financial Cryptography and Data Security Conference, pages 16­30. Springer-Verlag, 2008. [12] F. Radlinski, P. N. Bennett, B. Carterette, and T. Joachims. Redundancy, diversity and interdependent document relevance. SIGIR Forum, 43:46­52, Dec 2009. [13] M. E. Rorvig. The simple scalability of documents. JASIS, 41(8):590­598, 1990. [14] T. Sakai, N. Craswell, R. Song, S. Robertson, Z. Dou, and C. Y. Lin. Simple evaluation metrics for diversified search results. In Proc. EVIA, 2010. [15] M. Sanderson, M. L. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? In Proceedings of SIGIR, pages 555­562, 2010. [16] E. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. In Proceedings of SIGIR, pages 315­323, 1998. [17] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR, pages 10­17, 2003.",null,null
287,870,null,null
288,,null,null

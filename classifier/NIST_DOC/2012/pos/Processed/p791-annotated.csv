,sentence,label,data
,,,
0,Category Hierarchy Maintenance: a Data-Driven Approach,null,null
,,,
1,"Quan Yuan, Gao Cong, Aixin Sun, Chin-Yew Lin, Nadia Magnenat-Thalmann",null,null
,,,
2,"School of Computer Engineering, Nanyang Technological University, Singapore 639798",null,null
,,,
3,"{qyuan1@e., gaocong@, axsun@, nadiathalmann@}ntu.edu.sg",null,null
,,,
4,"Microsoft Research Asia, Beijing, China 100080",null,null
,,,
5,{cyl@microsoft.com},null,null
,,,
6,ABSTRACT,null,null
,,,
7,"Category hierarchies often evolve at a much slower pace than the documents reside in. With newly available documents kept adding into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. In this paper, we propose a novel automatic approach to modifying a given category hierarchy by redistributing its documents into more topically cohesive categories. The modification is achieved with three operations (namely, sprout, merge, and assign) with reference to an auxiliary hierarchy for additional semantic information; the auxiliary hierarchy covers a similar set of topics as the hierarchy to be modified. Our user study shows that the modified category hierarchy is semantically meaningful. As an extrinsic evaluation, we conduct experiments on document classification using real data from Yahoo! Answers and AnswerBag hierarchies, and compare the classification accuracies obtained on the original and the modified hierarchies. Our experiments show that the proposed method achieves much larger classification accuracy improvement compared with several baseline methods for hierarchy modification.",null,null
,,,
8,Categories and Subject Descriptors,null,null
,,,
9,H.3.3 [Information Storage and Retrieval]: Information Filtering,null,null
,,,
10,Keywords,null,null
,,,
11,"Category Hierarchy, Hierarchy Maintenance, Classification",null,null
,,,
12,1. INTRODUCTION,null,null
,,,
13,"With the exponential growth of textual information accessible, category hierarchy becomes one of the most widely-adopted and effective solutions in organizing large volume of documents. Hierarchy provides an organization of data by different levels of abstraction, in which each node (or category) represents a topic that is shared by the data in it. The connection between two nodes denotes supertype-subtype relation. Examples include Web directories provided by Yahoo! and Open Directory Project (ODP), hierarchies for community-based question-answering services by Yahoo! Answers (YA) and AnswerBag (AB), product hierarchies by online retailers like Amazon and eBay, as well as the hierarchies for news",null,null
,,,
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",null,null
,,,
15,browsing at many news portal websites. Figure 1 shows a small portion of Yahoo! Answers hierarchy. Questions in the same category are usually relevant to the same topic.,null,null
,,,
16,"Hierarchy enables not only easy document browsing, but also searching of the documents within user defined categories or subtrees of categories. Additionally, hierarchy information can be utilized to enhance retrieval models to improve the search accuracy [4]. On the other hand, users' information needs can only be satisfied with the documents accessible through the hierarchy (but not the category hierarchy itself). That is, the usefulness of a hierarchy heavily relies on the effectiveness of the hierarchy in properly organizing the existing data, and more importantly accommodating the newly available data into the hierarchy. Given the fast growth of text data, continuously accommodating large volume of newly available text data into a hierarchy is nontrivial. Automatic text classification techniques are often employed for efficient categorization of newly available documents into category hierarchies. However, hierarchy often evolves at a much slower pace than its documents. Two major problems often arise after adding many documents into a hierarchy after some time.",null,null
,,,
17,"· Structure Irrelevance. A category hierarchy may well reflect the topical distribution of its data at the time of construction. However, as new topics always emerge from the newly coming documents, there is no proper category in the hierarchy to accommodate these new documents, leading to putting these documents in less relevant categories. As the result, some categories contain less topically cohesive documents. Moreover, some categories become less discriminative with respect to the current data distribution. One example is the two categories Printers and Scanners in YA, for there emerged many questions about multi-functional devices which are related to both printers and scanners, leading to ambiguity between these two categories.",null,null
,,,
18,"· Semantics Irrelevance. Semantics may change over time which calls for a better organization of the documents [17]. For instance, when creating the hierarchy, experts are more likely to put category Petroleum under Geography. However, after the disaster of BP Gulf Oil Spill, a lot of news articles in category Petroleum are about the responsibility of the Obama Administration. These documents have stronger connection to category Politics than Geography. It is therefore more reasonable to put these documents under Politics for better document organization.",null,null
,,,
19,"These two problems not only hurt user experiences in accessing information through the hierarchy, but also result in poorer classification accuracy for the classifiers categorizing newly available documents because of the less topically cohesive categories [15].",null,null
,,,
20,791,null,null
,,,
21,,null,null
,,,
22,,null,null
,,,
23,Figure 1: Portion of Yahoo! Answers Hierarchy,null,null
,,,
24,"Consequently, the poorer classification accuracy further hurts user experience in browsing and searching documents through the hierarchy. This calls for category hierarchy maintenance, a task to modify the hierarchy to make it better reflect the topics of its documents, which in turn would improve the classification accuracy. Although a category hierarchy is relatively stable, many websites have modified or adjusted their hierarchies in the past. In May 2007, Yahoo! Answers added a new category Environment into her hierarchy, and added several categories like Facebook and Google under Internet later. By comparison, eBay adjusted her hierarchy more frequently, because there always emerge new types of items, like tablets and eReaders.",null,null
,,,
25,"Hierarchy modification is nontrivial. Manual modification of category hierarchy is a tedious and difficult task, because it is hard to detect the semantic changes as well as the newly emerged topics. This motivates the data-driven automatic modification of a given hierarchy to cope with semantic changes and newly emerged topics. This is a challenging task because of at least two reasons, among others. First, the resultant modified category hierarchy (hereafter called modified hierarchy for short) should largely retain the semantics of the existing hierarchy and keep its category labels semantically meaningful. Second, the categories in the modified hierarchy shall demonstrate much higher topical cohesiveness, which in turn enables better classification accuracy in putting new documents into the modified hierarchy.",null,null
,,,
26,"To the best of our knowledge, very few work has addressed the hierarchy modification problem (see Section 2). Tang et al. propose a novel approach to modifying the relations between categories aiming to improve the classification accuracy [17]. However, their proposed method does not change the leaf categories of the given hierarchy, and thus cannot solve the aforementioned problems. For example, the method may move the leaf category Petroleum to be child category of Politics. However, it is more reasonable to partition the documents in Petroleum into two categories: one being the child category of Geography, and the other child category of Politics. The method [17] fails to do so since it is unable to detect the newly emerged hidden topic ""Petroleum politics"".",null,null
,,,
27,"In this paper, we propose a data-driven approach to modify a given hierarchy (also called as the original hierarchy) with reference to an auxiliary hierarchy using three operations (namely, sprout, merge, and assign). An auxiliary hierarchy is a category hierarchy that covers a similar set of topics as does the given hierarchy (e.g., the Yahoo! hierarchy and ODP can be used as auxiliary hierarchy to each other). Similar to the concept of bisociation [8], our approach discovers finer and more elaborate categories (also known as hidden topics) by projecting the documents in the given hierarchy to the auxiliary hierarchy. This operation, similar to a cross-product operation between the categories from the given hierarchy and the categories from the auxiliary hierarchy, is named sprout 1. The similar hidden topics are then merged to form new",null,null
,,,
28,1We would like to thank an anonymous reviewer for suggesting the connection with bisociation [8] and the name sprout,null,null
,,,
29,"categories in the modified hierarchy. The assign operation rebuilds the parent-child relations in the modified hierarchy. The category labels in the modified hierarchy are either borrowed from or generated based on both the original and the auxiliary hierarchies. We emphasize that the reuse of category labels from original and auxiliary hierarchies largely ensures semantically meaningful category labels in the modified hierarchy. When such an auxiliary hierarchy is unavailable, the given hierarchy can be used as an auxiliary hierarchy. Because of the three operations (i.e., sprout, merged, and assign), we name our approach the SMA approach. The main contributions are summarized as follows.",null,null
,,,
30,"1) We propose a novel data-driven approach SMA to automatically modify a category hierarchy making it better reflect the topics of its documents. The proposed approach exploits the semantics of the given hierarchy and an auxiliary hierarchy, to guide the modification of the given hierarchy.",null,null
,,,
31,"2) We evaluate the proposed approach using data from three realworld hierarchies, Yahoo! Answers, Answerbag, and ODP. The user study shows that the modified hierarchy fits with the data better than the original one does. As we argue that the categories in the modified hierarchy are more topically cohesive compared to the original hierarchy, we employ text classification as an extrinsic evaluation. Our experimental results show that the classifiers trained on the modified hierarchy achieve much higher classification accuracy (measured by both macro-F1 and micro-F1), than the classifier built on the original hierarchy, or the classifiers modeled on the hierarchies generated by three baseline methods, including the state-of-the-art method in [17] and the hierarchy generation method in [2].",null,null
,,,
32,"The rest of this paper is organized as follows. Section 2 surveys the related work. We describe the research problem and overview the proposed approach in Section 3. The three operations are detailed in Section 4. The experimental evaluation and discussion of the results are presented in Section 5. Finally, we conclude this paper in Section 6.",null,null
,,,
33,2. RELATED WORK,null,null
,,,
34,"Hierarchy Generation. Hierarchy generation focuses on extracting a hierarchical structure from a set of categories, each containing a set of documents. The generation process can be either fully automatic [2, 5, 11] or semi-automatic [1, 7, 22]. The semi-automatic approaches involve interaction with domain experts in the hierarchy generation process. In the following, we review the fully automatic approaches in more detail.",null,null
,,,
35,Aggarwal et al. use the category labels of documents to supervise hierarchy generation [2]. They first calculate the centroids of all categories and use them as the initial seeds. Similar categories are merged and clusters with few documents are discarded. The process is iterated to build the hierarchy. User study is employed to evaluate the quality of the generated hierarchy.,null,null
,,,
36,"Punera et al. utilizes a divisive hierarchical clustering approach, which first splits the given set of categories into two sets of categories, and each such set is partitioned recursively until it contains only one category [11].",null,null
,,,
37,"An algorithm for generating hierarchy for short text is proposed by Chuang et al. [5]. They first create a binary-tree hierarchy by hierarchical agglomerative clustering, and then construct a multiway-tree hierarchy from the binary-tree hierarchy. They use both classification measurement and user evaluation to evaluate the generated hierarchy.",null,null
,,,
38,"Recently, Qi et al. employ genetic algorithms to generate hierarchy [12]. Given a set of leaf categories, a group of hierarchies are randomly generated as seeds, and genetic operators are applied",null,null
,,,
39,792,null,null
,,,
40,to each hierarchy to generate new ones. The newly generated hierarchies are evaluated and the hierarchies with poor classification accuracy are removed. The process is repeated until the classification accuracy is not improved.,null,null
,,,
41,"Different from hierarchy generation which assumes a set of categories as input, our hierarchy modification method takes a hierarchy as the input. Hierarchy generation does not change the given categories hence it cannot solve the structure irrelevance problem.",null,null
,,,
42,"Hierarchy Modification. Tang et al. present a method of modifying a hierarchy to improve the classification accuracy [17]. The method introduces three operations. The promote operation lifts a category to upper level; the merge operation generates a new parent for a category and its most similar sibling; the demote operation either demotes a category as a child of its most similar sibling, or makes the sibling a child of the category. For each category in the given hierarchy, promote operation is tested, followed by merge and demote operations, in a top-down manner. The operation comes into effect if it can improve the classification accuracy. The approach iterates the process until no improvement can be observed or some criterion is met. In experiments, this method outperforms clustering-based hierarchy generation method in terms of classification accuracy. However, this method does not change the leaf categories, leaving the topically incohesive leaf categories untouched. In addition, the method [17] has a high time-complexity. Due to the high time complexity of the method [17], Kiyoshi et al. propose an approach [10] to addressing the efficiency issue.",null,null
,,,
43,"Discussion. With the existing work on either hierarchy generation or hierarchy modification, the leaf categories in the modified hierarchy (i.e., either generated or modified) remain unchanged. Clearly, without changing leaf categories, the topical incohesiveness among documents in the same leaf category remains unaddressed. Consequently, the likely poorer classification accuracy for these topically incohesive categories results in poorer document organization in the hierarchy. In this paper, we therefore propose an automatic approach to modify a given hierarchy where the leaf categories could be split or merged so as to better reflect the topics of the documents in the hierarchy.",null,null
,,,
44,3. SMA APPROACH OVERVIEW,null,null
,,,
45,"We observe that each category in a hierarchy may contain several ""hidden topics"", each of which is topically cohesive, e.g., category Computer contains hidden topics like Internet Programming, Operating Systems, etc. With more documents adding to a category hierarchy, new ""hidden topics"" emerge within a single category leading to topical incohesiveness among its documents (see Section 1). Our proposed approach, therefore, aims to find the hidden topics within each category and then sprout categories based on its hidden topics, merge similar hidden topics to form new categories, and then assign the parent-child relation among categories. We name our approach SMA after its three major operations.",null,null
,,,
46,"The key challenges in the approach include: (i) How to detect the ""hidden topics"" at the appropriate granularity? (ii) How to evaluate the similarity between ""hidden topics""? and (iii) How to assign the parent-child relation between the unmodified and modified categories? Further, recall from Section 1, the modified hierarchy has to largely retain the semantics of the existing hierarchy, with meaningful category labels and topically cohesive categories. In the following, we give a high-level overview of the SMA approach and then detail the three major operations in the next Section.",null,null
,,,
47,"The framework of our SMA algorithm is illustrated in both Figure 2 and Algorithm 1, where Hc is the category hierarchy to be modified, Hn is the modified hierarchy, Hn is the intermediate hi-",null,null
,,,
48, ,null,null
,,,
49,Figure 2: Overview of SMA,null,null
,,,
50,Algorithm 1: SMA algorithm for hierarchy modification,null,null
,,,
51,Input: Hc: category hierarchy to be modified Ha: auxiliary hierarchy  : minimum coverage ratio  : maximum loss ratio,null,null
,,,
52,Output: Hn: modified category hierarchy,null,null
,,,
53,1 Hn  Hc;,null,null
,,,
54,2 h  number of levels of Hc;,null,null
,,,
55,3 foreach Level from 2 to h of Hn do,null,null
,,,
56,4 foreach Category Ci of Hn on level do,null,null
,,,
57,5,null,null
,,,
58,"Ci  ProjectedCategories(Ci, Ha,  ,  );",null,null
,,,
59,6,null,null
,,,
60,"Sprout(Ci, Ci , Hn, Ha);",null,null
,,,
61,7 n  number of categories on level of Hc;,null,null
,,,
62,"8 Merge( , n , Hn); 9 Assign( , Hn);",null,null
,,,
63,"10 HnRelabel(Hc, Ha, Hn); 11 return Hn",null,null
,,,
64,"erarchy during the modification process, and Ha is the auxiliary hierarchy.",null,null
,,,
65,"Auxiliary Hierarchy. Briefly introduced in Section 1, an auxiliary hierarchy Ha is a hierarchy covering similar topics as the given hierarchy Hc. For example, Yahoo! hierarchy and ODP hierarchy can be auxiliary hierarchy to each other. Similarly, Yahoo! Answers and AnswerBag can be auxiliary hierarchy to each other.",null,null
,,,
66,"The auxiliary hierarchy Ha plays an essential role in finding hidden topics. Note that the hidden topics are not readily present in the auxiliary hierarchy, and our approach does not simply use the structure of auxiliary hierarchy as part of the modified hierarchy. Instead, they contain semantics from both the original hierarchy and the auxiliary hierarchy. We use the following example to illustrate. Suppose that the original hierarchy has two categories, Action movie and Comedy movie, and the auxiliary hierarchy contains two categories America and Asia. Our approach will find that Action movie has two hidden topics, namely American action movie and Asian active movie; Comedy movie also has two hidden topics, namely American comedy movie and Asian comedy movie.",null,null
,,,
67,"The auxiliary hierarchy also plays an important role in guiding the merge operation, which is to merge similar hidden topics to generate the categories of the modified hierarchy. Continue with the earlier example, after merging the generated hidden topics, we may get new categories­American movie and Asian movie (if ""action vs. comedy"" is evaluated to be less discriminative compared with ""American vs. Asian""). The semantics of the hierarchy to be modified, together with the semantics of the auxiliary hierarchy, will be exploited to define the similarity between hidden topics.",null,null
,,,
68,"Validated in our experiments (Section 5), our approach is equally applicable when the original hierarchy Hc is used as the auxiliary hierarchy to itself.",null,null
,,,
69,"Algorithm Overview. Shown in Figure 2 and Algorithm 1, Hn is first initialized to Hc (line 1). In a top-down manner, the SMA algorithm modifies the hierarchy level by level. Note that the root",null,null
,,,
70,793,null,null
,,,
71,"category is the only category at level 1. Starting from level 2, for each category Ci in this level, the documents contained in Ci is projected to the auxiliary hierarchy Ha. A set of categories from Ha each of which contains a reasonable number of documents originally from Ci is identified to represent Ci's hidden topics (line 5). The two parameters, minimum coverage ratio  and maximum loss ratio  , adjust the number of hidden topics. New finer categories are then sprouted from Ci according to the hidden topics and the documents in category Ci are assigned to these finer categories (or hidden topics) (line 6). Given the expected number of categories n on level (line 7), the merge operation forms n number of new categories on level of the intermediate hierarchy Hn (line 8). If the current level is not the lowest level in the hierarchy, the parentchild relations between the modified categories and the unmodified categories on the next level are assigned (line 9). The last step in the SMA algorithm is to generate category labels with reference to both the original and auxiliary hierarchies (line 10).",null,null
,,,
72,"4. SPROUT, MERGE, AND ASSIGN",null,null
,,,
73,"We detail the three operations to address the challenges in the SMA framework (i.e., to identify hidden topics, evaluate the similarity between hidden topics, and assign the parent-child relation).",null,null
,,,
74,4.1 Sprout Operation,null,null
,,,
75,"The sprout operation first discovers the hidden topics for the documents in a category Ci and then sprouts the category. Without loss of generalization, a leaf category is represented by all documents belonging to the category; a non-leaf category is represented by all documents belonging to any of its descendent categories.",null,null
,,,
76,4.1.1 Discovery of Hidden Topics,null,null
,,,
77,"Ideally, for a category we find a set of its hidden topics, which are comprehensive and cohesive, and have no overlap. This is however a challenging task. We proceed to give an overview of the proposed method. Given a category Ci in the intermediate hierarchy Hn during the modification process (see Algorithm 1), we assign all its documents into the categories of the auxiliary hierarchy Ha, and get a set of candidate categories from Ha in a tree-structure. Each candidate category contains a number of documents from Ci. Then, with the consideration of both cohesion and separation, we select a set of categories from the tree as hidden topics. The selection process is modeled as an optimization problem. We now elaborate the details.",null,null
,,,
78,"Document Projection. To assign documents from Ci to Ha, we represent a document by its word feature vector, and a category in Ha by its centroid vector. Based on cosine similarity between the document and the centroids, we recursively assign each document d  Ci to Ha from its root to a leaf category along a single path of categories. If a good number of documents from Ci are assigned to a category Ca in Ha, then the topic of Ca is relevant to Ci, and the semantics of Ca can be used to describe a hidden topic of Ci. Thus, multiple categories in Ca can be identified to describe all hidden topics of Ci. For example, large number of documents from category Programming assigned into two categories Security and Network in an auxiliary hierarchy, implies that Programming has two hidden topics: Network Programming and Security Programming. We have also tried to build a classifier on Ha to assign documents from Ci to Ha using Naive Bayes and support vector machine, respectively, and the set of generated hidden topics is almost the same.",null,null
,,,
79,The process of assigning documents from a category Ci in Hn to categories in Ha is called projection. We denote the set of docu-,null,null
,,,
80,"ments projected from category Ci to category Ca by (Ci  Ca). If Ca is a leaf category, then (Ci  Ca) denotes the set of documents from Ci that are projected into Ca; if Ca is a non-leaf category, then (Ci  Ca) denotes the set of documents projected into any of the descendent leaf categories of Ca in Ha.",null,null
,,,
81,"Candidate Topic Tree. Based on the projection, we select categories from Ha to represent the hidden topics of Ci. A selected category can be either a leaf category or a non-leaf category. Before describing the selection process, we introduce the notions of major category and minor category. Let  denote the minimum coverage ratio parameter.",null,null
,,,
82,Definition 1 (Major Category). A category Ca from Ha is a major category for category Ci if |(Ci  Ca)|/|Ci|   .,null,null
,,,
83,Definition 2 (Minor Category). A category Ca from Ha is a minor category for category Ci if |(Ci  Ca)|/|Ci| <  .,null,null
,,,
84, ,null,null
,,,
85,  !,null,null
,,,
86,"
#$ %

 


 


 
  


 
  
  






   '!(
!""'

'&#",null,null
,,,
87,$% !,null,null
,,,
88,Figure 3: Generate candidate topic tree for Ci using Ha,null,null
,,,
89,"For example, suppose  ,"" 15%. As shown in Figure 3, category Ci is projected to the categories in Ha. In the left tree, in which each number besides a node represents the percentage of documents of Ci projected to the node, the nodes in dark color are major categories while the others are minor categories.""",null,null
,,,
90,"Naturally, only the major categories are considered candidate categories to represent hidden topics of Ci because a good number of documents in Ci are projected into them. However, not all major categories need to be selected because of two reasons. First, let Cp be the parent of a major category Ca. By definition, the parent of a major category is also a major category. Selecting both Ca and Cp would lead to semantic overlap. Second, assume all Cp's other child categories are minor categories, but altogether those minor categories contain a large number of documents. Then selecting Ca but not Cp would lead to a significant loss of documents from Ci (hence semantic loss). We therefore define the notion of loss ratio.",null,null
,,,
91,Definition 3 (Loss Ratio). The loss ratio of a leaf category is,null,null
,,,
92,"defined as 0. For a non-leaf category Ca, let Ca be the set of minor",null,null
,,,
93,categories among Ca's child categories. The loss ratio of Ca with,null,null
,,,
94,"respect to Ci, the category being projected, is the ratio between",null,null
,,,
95,the projected documents in all its child minor categories and Ca's,null,null
,,,
96,"projected documents, i.e.,",null,null
,,,
97,. C Ca |(CiC )|,null,null
,,,
98,| (Ci Ca )|,null,null
,,,
99,We set a threshold maximum loss ratio  . After projecting doc-,null,null
,,,
100,"uments from Ci to categories in Ha, we only keep the major categories whose parent's loss ratio is smaller than  . Note that, if a",null,null
,,,
101,"non-leaf category is not selected in the above process, the subtree",null,null
,,,
102,"rooted at this category is not selected. After the selection, we obtain",null,null
,,,
103,"a sub-hierarchy from Ha containing only eligible major categories,",null,null
,,,
104,"which is called candidate topic tree for Ci, denoted by TCi . For example, suppose  , 30%. The candidate topic tree for Ci",null,null
,,,
105,is shown on the right hand side of Figure 3. Although node C5 is,null,null
,,,
106,"a major category, it is not part of the candidate topic tree since the",null,null
,,,
107,"loss ratio ((10%+10%)/50% , 40%) of its parent node C3 is larger than  .",null,null
,,,
108,794,null,null
,,,
109,Hidden Topic Selection. We next present how to choose a set of,null,null
,,,
110,"nodes from TCi to represent hidden topics of Ci. Ideally, we expect the hidden topics to be comprehensive but not overlap with each",null,null
,,,
111,"other. Hence, we use tree-cut to define the selection [18].",null,null
,,,
112,"Definition 4 (Tree-Cut). A tree-cut is a partition of a tree. It is a list of nodes in the tree, and each node represents a set of all leaf nodes in a subtree rooted by the node. The sets in a tree-cut exhaustively cover all leaf nodes of the tree, and they are mutually disjoint.",null,null
,,,
113,"There exist many possible tree-cuts for TCi to generate hidden topics. Two example tree cuts for the candidate topic tree in Figure 3 are {C1, C2} and {C1, C3}. Among all possible tree-cuts, we aim to choose the tree-cut such that each resultant hidden topic (cate-",null,null
,,,
114,gory) is cohesive and well separated from other categories in the,null,null
,,,
115,"tree-cut. In the following, we prove that the tree-cut containing",null,null
,,,
116,only leaf nodes of the candidate topic tree satisfies this require-,null,null
,,,
117,"ment. Note that a leaf node in TCi is not necessary a leaf category in Ha. For example, in Figure 3, C3 is leaf node of the candidate topic tree, but not a leaf category in the auxiliary hierarchy.",null,null
,,,
118,We proceed to show the proof. We first define the Sum of Square,null,null
,,,
119,Error (SSE) of cohesion for a category Ca.,null,null
,,,
120,"SSE(Ca) ,"" (d - ca)2,""",null,null
,,,
121,dCa,null,null
,,,
122,where d is a document and ca is the centroid of category Ca.,null,null
,,,
123,"Given a set of categories {Ca} (1  a  k), the Total-SSE and",null,null
,,,
124,"Total Sum of Square Between (Total-SSB), denoted by E and B",null,null
,,,
125,"respectively, are E ,",null,null
,,,
126,"k a,1",null,null
,,,
127,SSE,null,null
,,,
128,(Ca),null,null
,,,
129,and,null,null
,,,
130,B,null,null
,,,
131,",",null,null
,,,
132,"k a,1",null,null
,,,
133,|Ca|(c,null,null
,,,
134,-,null,null
,,,
135,"ca)2,",null,null
,,,
136,where c is the centroid of documents in all categories {Ca}. It is,null,null
,,,
137,"verified that, given a set of documents, the sum of E and B is a",null,null
,,,
138,"constant value [16]: E +B ,",null,null
,,,
139,"k a,1",null,null
,,,
140,"dCa (d-c)2. Thus, maximizing",null,null
,,,
141,separation is equivalent to minimizing cohesion error. We therefore,null,null
,,,
142,formulate the problem of selecting categories from TCi to represent hidden topics for category Ci as following:,null,null
,,,
143,"Ci ,"" arg min SSE(Ca), where S is a tree-cut on TCi . (1)""",null,null
,,,
144,S,null,null
,,,
145,Ca S,null,null
,,,
146,This problem can be reduced to the maximum flow problem by,null,null
,,,
147,"viewing TCi as a flow network. Thus, it can be solved directly by Ford-Fulkerson method [6]. However, its complexity is relatively",null,null
,,,
148,high. Note that we need to solve the optimization problem for every,null,null
,,,
149,"category in the original hierarchy, and thus an efficient algorithm is",null,null
,,,
150,essential.,null,null
,,,
151,Lemma 1: The SSE of a category is not smaller than the Total-SSE,null,null
,,,
152,"of its child categories. Proof: Suppose there is a category Cp with k child categories {Ci}ki,1.",null,null
,,,
153,"For a child category Ci, the Sum of Square Distance (SSD) of its",null,null
,,,
154,"data to a data point x is: SSD(Ci) , dCi (d - x)2. We get the mini-",null,null
,,,
155,mum value when x,null,null
,,,
156,",",null,null
,,,
157,1 |Ci |,null,null
,,,
158,dCi,null,null
,,,
159,d,null,null
,,,
160,",",null,null
,,,
161,ci,null,null
,,,
162,which,null,null
,,,
163,let,null,null
,,,
164,d dx,null,null
,,,
165,"dCi (d-x)2 , 0.",null,null
,,,
166,"Thus, when x is the mean of data in Ci (or ci), the SSD of Ci be-",null,null
,,,
167,"comes SSE of Ci, and gets its minimum value. One step further, we",null,null
,,,
168,have,null,null
,,,
169,"(d - ci)2  (d - cp)2,",null,null
,,,
170,dCi,null,null
,,,
171,dCi,null,null
,,,
172,where cp is the mean of data of Cp. This demonstrates that the SSE,null,null
,,,
173,"of Ci is smaller than the SSD of data of Ci to the overall mean, and",null,null
,,,
174,this result leads to,null,null
,,,
175,k,null,null
,,,
176,k,null,null
,,,
177,(d - ci)2 ,null,null
,,,
178,(d - cp)2.,null,null
,,,
179,"i,1 dCi",null,null
,,,
180,"i,1 dCi",null,null
,,,
181,This demonstrates that the SSE of a category is not smaller than the,null,null
,,,
182,Total-SSE of its child categories.,null,null
,,,
183,Procedure ProjectedCategories,null,null
,,,
184,Input: Ci: the category to be sprouted Ha: the auxiliary hierarchy  : minimum coverage ratio  : maximum loss ratio,null,null
,,,
185,Result: Ci []: the list of projected categories for Ci,null,null
,,,
186,1 Ci []  {root category of Ha}; 2 repeat,null,null
,,,
187,3 Ca  Ci [].getNextUnprocessedCategory(); 4 C_List[]  child categories of Ca;,null,null
,,,
188,5 M_List[]  {};,null,null
,,,
189,6 foreach Category C of C_List[] do,null,null
,,,
190,7,null,null
,,,
191,if,null,null
,,,
192,| (Ci C)| |Ci |,null,null
,,,
193,then,null,null
,,,
194,8,null,null
,,,
195,M_List[].add(C);,null,null
,,,
196,9,null,null
,,,
197,if,null,null
,,,
198,1-,null,null
,,,
199,CM_List [] | (Ci C)| | (Ci Ca )|,null,null
,,,
200,<,null,null
,,,
201,then,null,null
,,,
202,10,null,null
,,,
203,Ci [].add(M_List[]);,null,null
,,,
204,11,null,null
,,,
205,Ci [].remove(Ca);,null,null
,,,
206,12 else,null,null
,,,
207,13,null,null
,,,
208,mark Ca as processed,null,null
,,,
209,14 until No more unprocessed category in Ci [] 15 return Ci [],null,null
,,,
210,"Lemma 1 enables us to use an efficient method to solve Eq.1 as follows. According to Lemma 1, specializing a category by its child categories can reduce Total-SSE. That is, among all possible tree-cuts in TCi , the cut that contains only leaf categories has the minimum value of Total-SSE.",null,null
,,,
211,"In summary, for a category Ci in Hn, we build a candidate topic tree TCi and the leaf nodes of TCi are used to represent the hidden topics of Ci. The pseudocode is given in Procedure 2 ProjectedCategories. As discussed, according to Lemma 1, we only need the leaf nodes of the candidate topic tree TCi as the result. Instead of explicitly building TCi and then finding TCi 's tree cut containing only leaf nodes, we find TCi 's leaf nodes directly in our procedure. More specifically, we start from the root category of Ha in a topdown manner (the root node is a major category by definition as its coverage ratio is 1). Each time we get a unprocessed categories Ca from the list of projected categories Ci [], and check its child categories (lines 3-4). The major categories among the child categories are put into a major category list (lines 6-8) for further testing on loss ratio. If the loss ratio of Ca is smaller than maximum loss ratio  , then Ca is replaced by its child major categories (lines 9-11); otherwise, Ca is selected as a candidate category (line 13). We iterate the process until all major categories are processed (line 14).",null,null
,,,
212,4.1.2 Sprout Category,null,null
,,,
213,"For a category Ci of Hn, we sprout it based on the projected categories Ci returned by Procedure ProjectedCategories. Recall that each of the projected category Ca  Ci represents a hidden topic of Ci and contains a good number of documents projected from Ci, i.e., (Ci  Ca). We sprout Ci with |Ci | number of categories. However, not all documents from Ci are contained in all these newly sprouted categories, i.e., CaCi |(Ci  Ca)|  |Ci|. For those documents in Ci but not contained in any of the newly sprouted categories, we assign them to their nearest sprouted categories. As the result, each document in Ci is now contained in one and only one sprouted category of Ci.",null,null
,,,
214,795,null,null
,,,
215,$X[LOLDU\ KLHUDUFK\,null,null
,,,
216,&RPSXWHU,null,null
,,,
217,6HFXULW\,null,null
,,,
218,1HWZRUN,null,null
,,,
219,3URWRFRO,null,null
,,,
220,0RGLI\LQJ &RPSXWHU KLHUDUFK\,null,null
,,,
221,&DEOH  +LGGHQ WRSLFV,null,null
,,,
222,1HWZRUN,null,null
,,,
223,3URJUDPPLQJ,null,null
,,,
224, 6SOLW FDWHJRU\  0HUJH FDWHJRU\,null,null
,,,
225,&RPSXWHU,null,null
,,,
226,1HWZRUN 6HFXULW\,null,null
,,,
227,1HWZRUN 1HWZRUN,null,null
,,,
228,6HFXULW\,null,null
,,,
229,3URWRFRO,null,null
,,,
230,3URWRFRO &DEOH 3URJUDPPLQJ 3URJUDPPLQJ,null,null
,,,
231,1HWZRUN 6HFXULW\ 6HFXULW\ 3URJUDPPLQJ,null,null
,,,
232,&RPSXWHU,null,null
,,,
233,1HWZRUN &DEOH,null,null
,,,
234,1HWZRUN 3URWRFRO 3URWRFRO 3URJUDPPLQJ,null,null
,,,
235,:$1,null,null
,,,
236,:$1 6HFXULW\,null,null
,,,
237, $VVLJQ FDWHJRU\ UHODWLRQ,null,null
,,,
238,:$1 3URWRFRO 0RGLILHG KLHUDUFK\ EHIRUH UHODEHO,null,null
,,,
239,"Figure 4: SMA operations by example. The hidden topics and sprouted categories for a category of the original hierarchy are in the same color. The Network and Programming categories have 3 and 2 hidden topics, respectively, leading to 5 sibling sprouted categories. These 5 categories are merged into 3 categories and the category WAN is reassigned to 2 of the merged categories.",null,null
,,,
240,"Example 4.1: Shown in Figure 42, suppose after applying procedure ProjectedCategories, we find Network is projected to Security, Protocol, and Cable. According to the three hidden topics, we sprout Network into three categories Network Security, Network Protocol, and Network Cable.",null,null
,,,
241,"The sprout operation may be reminiscent of the work on hierarchy integration, aiming to integrate a category from a source hierarchy into similar categories in the target hierarchy, which has a different purpose from our mapping. Most of proposals (e.g., [3, 21]) on hierarchy integration employ a hierarchical classifier built on the target hierarchy to classify each document in the source hierarchy into a leaf node of the target hierarchy, which is too fine a granularity to represent hidden topics as in our approach. Frameworks that can map a category to categories on proper levels in target hierarchy are proposed (e.g., [19]). However, they do not take the cohesion and separation between mapping categories into account, which are essential to find good hidden topics in our approach. Thus, they cannot be applied to our work.",null,null
,,,
242,4.2 Merge Operation,null,null
,,,
243,"The sprout operation in Section 4.1 generates a set of sprouted categories, each representing a hidden topic. The merge operation aims to combine the newly sprouted categories with similar hidden topics.",null,null
,,,
244,"Suppose we are now working on level of the intermediate modified hierarchy Hn and we have a set of sprouted categories originated from the categories on level . Our task is to merge some of these sprouted categories such that the number of resultant categories on level is the same as before (i.e., n ). Note that the number of resultant categories can also be specified by users. To ease the presentation, the modified hierarchy has the same size as the given hierarchy in our discussion.",null,null
,,,
245,"During merge, we need to consider an important constraint -- we can only merge categories under the same parent category. Thus, existing clustering algorithms need to be modified to accommodate such a constraint. Another key issue here is how to define the similarity between two sprouted categories by considering their semantics enclosed in Hc and Ha. In the following, we first define a similarity measure and then describe our merge method.",null,null
,,,
246,We consider two aspects when defining the similarity for a pair of sprouted categories C1 and C2 on level : (i) the distribution of their,null,null
,,,
247,"2For clarity, we recommend viewing all diagrams in this paper from a color copy.",null,null
,,,
248,"documents over categories of Hc and Ha, and (ii) the similarity",null,null
,,,
249,"between the categories within Hc and Ha, respectively.",null,null
,,,
250,Let Lc be the set of categories on level in the original hierarch,null,null
,,,
251,"Hc, Ls be the set of categories sprouted from Lc, and La be the",null,null
,,,
252,set of projected categories in auxiliary hierarchy representing the,null,null
,,,
253,"hidden topics of the categories in Lc. That is, La ,"" CiLc Ci . For a sprouted category Cs  Ls, its document distribution over""",null,null
,,,
254,Lc is defined to be the ratios of its documents in each of the cate-,null,null
,,,
255,"gories in Lc. That is, the document distribution of Cs can be mod-",null,null
,,,
256,eled as a |Lc|-dimensional vector vcs. The j-th element of vcs is,null,null
,,,
257,|CsC j | |Cs |,null,null
,,,
258,"(i.e.,",null,null
,,,
259,the,null,null
,,,
260,portion,null,null
,,,
261,of Cs's,null,null
,,,
262,documents,null,null
,,,
263,also,null,null
,,,
264,contained,null,null
,,,
265,"in Cj),",null,null
,,,
266,"where Cj is the j-th category of Lc. Similarly, we get the data dis-",null,null
,,,
267,tribution vector vas for Cs over La based on the ratio of documents,null,null
,,,
268,in Cs projected to each of the categories in La. Because vcs and,null,null
,,,
269,vas usually have different dimensionality for different sprouted cat-,null,null
,,,
270,"egory Cs's, we extend vcs to be |Hc|-dimensional (each category is",null,null
,,,
271,one dimension) by filling up zeros for corresponding categories in,null,null
,,,
272,"Hc but not in Lc. Similarly vas is extended to be |Ha|-dimensional. We use two matrices Mc, Ma to represent the similarity between",null,null
,,,
273,"categories of Hc and Ha, respectively. Mc is a |Hc|- by-|Hc| matrix and Ma is a |Ha|-by-|Ha| matrix. Each element mi j in a matrix",null,null
,,,
274,represents the similarity in the corresponding hierarchy between,null,null
,,,
275,"a pair of categories Ci and Cj, which is defined by Inverted Path",null,null
,,,
276,Length [14]:,null,null
,,,
277,mi j,null,null
,,,
278,",",null,null
,,,
279,"1 1+path(Ci,C j )",null,null
,,,
280,",",null,null
,,,
281,where,null,null
,,,
282,"path(Ci, Cj) is",null,null
,,,
283,the length,null,null
,,,
284,of,null,null
,,,
285,path between Ci and Cj in the hierarchy.,null,null
,,,
286,Considering both document distribution and structural similar-,null,null
,,,
287,"ity from the two hierarchies, the similarity between two sprouted",null,null
,,,
288,categories C1 and C2 on level of Hn is defined as:,null,null
,,,
289,"Sim(C1, C2) , (vTc1 · Mc · vc2) + (vTa1 · Ma · va2).",null,null
,,,
290,"This similarity definition considers both the similarity estimated based on Hc and the similarity estimated based on Ha. With similarity between two sprouted categories defined, we proceed to detail the merge operation.",null,null
,,,
291,"We first explain the notion of sibling sprouted category through an example. Let Ci1 and Ci2 be the two categories sprouted from category Ci, and Cj1 and Cj2 be the two categories sprouted from Cj. If Ci and Cj in Hm are both children of category Cp, then naturally, all the newly sprouted categories Ci1, Ci2, Cj1, Cj2 are children of Cp. These four example categories are known as sibling sprouted categories. All the five sprouted categories shown in Figure 4 are sibling sprouted categories.",null,null
,,,
292,"The merge operation is as follows. We first calculate the similarity between sibling sprouted categories on level . Then, we pick up the pair of categories with the largest similarity, and merge them into a category, and recompute its similarities with its sibling sprouted categories. The process iterates until the number of remaining categories on equals n , the number of categories on level of the original hierarchy Hc. When all the sibling sprouted categories under the same parent node are merged into a single category, we shrink the single category into its parent node. Note that we cannot merge two sprouted categories on level if they have different parent node.",null,null
,,,
293,"Example 4.2: Recall Example 4.1. We sprout Network into three categories Network Security, Network Protocol, and Network Cable. Suppose there is another category Programming on the same level of Network and sprouted into Security Programming, and Protocol Programming (see Figure 4). Based on the similarity, Network Security and Security Programming are merged together (both are about the Security topic), and Network Protocol and Protocol Programming are merged to generate a new category about protocol.",null,null
,,,
294,796,null,null
,,,
295,4.3 Assign Operation,null,null
,,,
296,"After modifying categories (by sprout and merge) on level of H n, the original parent-child relations between the categories on level and the categories on next level + 1 do not hold any longer. Hence we need to reassign the parent-child relation.",null,null
,,,
297,"Based on the fact that a non-leaf category of a hierarchy subsumes the data of all its descendants, we rebuild the children for each of the modified categories on by checking document containment. If the documents of a category on level are also contained in a category on level + 1, then the latter category is assigned to be the children of the former category. In other words, for each category C on , we calculate the intersection of documents between C and the categories on + 1. The intersections form new children for category C. Because one category has only one parent category, if a category has intersections with more than one category on level , the category will be split into multiple categories, each containing the intersection with one category on level .",null,null
,,,
298,"Example 4.3: Recall Example 4.2. Suppose WAN was a child category of Network before sprout (see Figure 4). After sprout and merge, Network no longer exists and WAN lost its parent. We compare the documents of WAN and the two newly formed categories after merge (i.e., Network Security & Security Programming and Network Protocol & Protocol Programming). If WAN has overlap with both categories, then WAN have two hidden topics (about security and protocol). Thus, we divide WAN into two categories and assign them to different parent nodes, shown in Figure 4.",null,null
,,,
299,4.4 Relabel,null,null
,,,
300,"Unlike most of previous work, our approach is able to automatically generate readable labels for every modified category. By projecting documents from Hn to Ha, we can consider the three hierarchies Hn, Hc and Ha, which contain the same set of data. For every document in Hn, we trace its labels in Hc and Ha, and use them together as the label of the document; the semantic of the new label is the intersection of semantics of its two component labels. We then aggregate such labels for all documents in a category of Hn, and use them as candidate labels for the category. The candidate labels for a category are ranked according to the proportion of documents in their corresponding original categories from Hc and Ha. In this paper, the top-1 ranked label is chosen.",null,null
,,,
301,"The labels generated in this way are mostly readable and semantically meaningful, as reflected in our user study (see Section 5.3) and case study (see Section 5.4). Nevertheless, a manual verification of the labels for the newly generated categories can be employed when the proposed technique is used in real applications.",null,null
,,,
302,5. EXPERIMENTS,null,null
,,,
303,"We designed two sets of experiments. The first set of experiment, similar to that in [17], is to evaluate whether the modified hierarchy improves the classification accuracy. Discussed in Section 1, if a category hierarchy better reflects the topics of its contained documents and each category in the hierarchy is topically cohesive, then better classification accuracy is expected than that on a hierarchy with less topically cohesive categories. The second set of experiments employs a user study to manually evaluate the semantic quality of the modified hierarchy following the settings in [2,5]. Finally, we report a case study comparing a part of Yahoo! Answers hierarchy with its modified hierarchy.",null,null
,,,
304,5.1 Data Set,null,null
,,,
305,"We use data from three real-world hierarchies: Yahoo! Answers, AnswerBag, Open Directory Project, denoted by HYA, HAB and",Y,null
,,,
306,Table 1: Statistics of the three hierarchies,null,null
,,,
307,Hierarchy,null,null
,,,
308,HYA,null,null
,,,
309,HAB,null,null
,,,
310,HODP,null,null
,,,
311,Number of documents,null,null
,,,
312,"421,163 148,822 203,448",null,null
,,,
313,Number of leaf nodes,null,null
,,,
314,75,null,null
,,,
315,195,null,null
,,,
316,460,null,null
,,,
317,Number of non-leaf nodes 40,null,null
,,,
318,70,null,null
,,,
319,98,null,null
,,,
320,Height,null,null
,,,
321,4,null,null
,,,
322,5,null,null
,,,
323,4,null,null
,,,
324,"HODP, respectively. Since the modified category hierarchy contains a different set of leaf nodes, the labels for documents given in the original dataset do not stand in modified hierarchy. Manual annotation of documents in the modified hierarchy is therefore unavoidable. To make the annotation manageable, we selected the documents from two major topics Sports and Computers from these three hierarchies (because the annotators are familiar with both topics). Nevertheless, the number of documents in the two major topics in the three hierarchies ranges from 148,822 to 421,163, and the number of categories ranges from 115 to 558. These numbers are large enough for a valid evaluation. Table 1 reports the statistics on the three hierarchies.",null,null
,,,
325,"HYA: Obtained from the Yahoo! Webscope datatset3, HYA contains 421,163 documents (or questions) from Sports and Computers & Internet categories.",Y,null
,,,
326,"HAB: We collected 148,822 questions from Recreation & Sports and Computers categories from AnswerBag to form HAB. Categories with fewer than 100 questions are pruned and all affected questions are moved to their parent categories.",Y,null
,,,
327,"HODP: The set of 203,448 documents from Sports and Computers categories are collected4 in HODP. Categories containing fewer than 15 documents or located on level 5 or deeper are removed in our experiments.",Y,null
,,,
328,The preprocessing of the documents in all three hierarchies includes stopword removal and stemming. Terms occurred no more than 3 times across the datasets are also removed.,null,null
,,,
329,5.2 Evaluation by Classification,null,null
,,,
330,"The proposed SMA algorithm modifies a category hierarchy to better reflect the topics of its documents, which in turn should improve the classification performance. Following the experimental setting in [17], we evaluate the effectiveness of hierarchy modification by comparing the classification accuracies obtained by the same hierarchical classification model applied on the original category hierarchy and the modified hierarchy, respectively.",null,null
,,,
331,"Another three methods for hierarchy modification are employed as the baselines, namely, Bottom Up Clustering (BUC), Hierarchical Acclimatization (HA) [17], and Supervised Clustering (SC) [2]. Table 2 gives a summarized comparison of the three baselines with the proposed SMA, and Section 5.2.1 briefs the baseline methods.",null,null
,,,
332,"The modified hierarchies by all the methods evaluated in this paper have the same size as the original hierarchy (i.e., same number of levels, and same number of categories in each level). For each hierarchy modification method, we evaluate the percentage of classification accuracy increment obtained by the same classification model (e.g., Support Vector Machine) on the modified hierarchy over the original hierarchy. The classification accuracy is measured by both micro-average F1 (Micro-F1) and macro-averaged F1 (Macro-F1) [20]. The former gives equal weight to every document while the latter weighs categories equally regardless the number of documents in each category.",null,null
,,,
333,"We remark that this is a fair evaluation for all the methods, each generating a hierarchy with the same size as that of the original",null,null
,,,
334,3Available at http://research.yahoo.com/Academic_Relations. 4Available at http://www.dmoz.org/rdf.html.,null,null
,,,
335,797,null,null
,,,
336,Table 2: Comparison of baseline methods with SMA,null,null
,,,
337,Aspect/Methods Utilize original hierarchy Change leaf category,null,null
,,,
338,BUC HA[17] SC [2],null,null
,,,
339,×,null,null
,,,
340,×,null,null
,,,
341,×,null,null
,,,
342,×,null,null
,,,
343,SMA ,null,null
,,,
344,Utilize auxiliary hierarchy ×,null,null
,,,
345,×,null,null
,,,
346,× Optional,null,null
,,,
347,"hierarchy, where the same classification method is applied to the modified hierarchies to evaluate the improvement of each modified hierarchy over the original one in terms of classification accuracy.",null,null
,,,
348,5.2.1 Baseline Methods,null,null
,,,
349,"Baseline 1: Bottom Up Clustering (BUC). In this method, each leaf category is represented by the mean vector of its contained documents. The categories are then clustered in a bottom-up manner using K-means to form a hierarchy.",null,null
,,,
350,"Baseline 2: Hierarchical Acclimatization (HA). The HA algorithm is reviewed in Section 2, In simple words, it employs promote, demote and merge operations to adjust the internal structure, but leaves the leaf nodes unchanged [17].",null,null
,,,
351,"Baseline 3: Supervised Clustering (SC). Given a set of documents with labels, SC first calculates the mean vector of each category as the initial centroid and then reassigns the documents to the categories based on the cosine similarity with their centroids. Then, similar categories are merged and minor categories are removed. These procedures are repeated, and during each iteration, a constant portion of features with smallest term-frequencies are set to zero (projected out) [2]. The process stops when the number of features left is smaller than a pre-defined threshold. This method cannot generate category labels. We take the most frequent words in a category to name it.",null,null
,,,
352,5.2.2 Experiments on Yahoo! Answers,Y,null
,,,
353,"From the data of HYA, we randomly selected 500 questions as test data (used for classification evaluation with manual annotations). To evaluate the possible improvement in classification accuracy, the same set of test documents are classified on the original (or unmodified) HYA, and the modified HnYA's. Two classifiers, multinominal Naive Bayes (NB) and Support Vector Machine (SVM) classifiers are used as base classifiers for hierarchical classification. We build Single Path Hierarchical Classifier (SPH) [9] as it performs better than other hierarchical classification methods for question classification according to the evaluation [13]. In the training phase of SPH, for each internal node of the category tree, SPH trains a classifier using the documents belonging to its descendent nodes. In the testing phase, a test document is classified from the root to a leaf node in the hierarchy along a single path.",null,null
,,,
354,"SMA Settings. Recall that SMA uses auxiliary hierarchy in the modification process. We evaluated SMA with three settings, to modify HYA using HYA, HAB, and HODP as auxiliary hierarchy, respectively. The three settings are denoted by SMAYA|YA, SMAYA|AB, and SMAYA|ODP, respectively. The first setting is to evaluate the effectiveness of using the original hierarchy as auxiliary hierarchy, and the last two are to evaluate the effectiveness of using external hierarchies.",null,null
,,,
355,"Parameter Setting. Before evaluating the test documents on the modified hierarchies, we set the parameters required by SMA for hierarchy modification. Recall that SMA requires two parameters: minimum coverage ratio  and maximum loss ratio  . Usually parameters are set using a development set or through crossvalidation. In our case, however, there is no ground truth on how",null,null
,,,
356,good a modified hierarchy is and manual assessment of every modified hierarchy for parameter tuning is impractical. We therefore adopt a bootstrapping like approach described below.,null,null
,,,
357,"After the test data selected, the remaining data is used for hierarchy modification. We split the remaining data of HYA into 3 parts: P1, P2, P3, and the proportion of their sizes is 12:3:1. Using P1 for HYA and a given auxiliary hierarchy, we obtain a modified hierarchy HnYA. Naturally, all documents in P1 have category labels from hierarchy HnYA. We then build a classifier using all documents in P1 and their labels from HnYA. The classifier classifies documents in P2 and P3. Assume that the classifier gives reasonably good classification accuracy, then all documents in P2 and P3 have their category labels assigned according to HnYA. With these labels, we can evaluate the classification accuracy of documents in P3 by the classifier built using P2 on HnYA. Intuitively, if a hierarchy H1 better organizes documents than another hierarchy H2, then the classifier trained on H1 is expected to have higher classification accuracy for P3 than a classifier built on H2. We then select the parameters leading to the best classification accuracy for P3. In our experiments, the parameters (i.e.,  and  ) set for SMAYA|YA, SMAYA|AB, and SMAYA|ODP are (0.29 and 0.11), (0.17 and 0.17), (0.38 and 0.08), respectively.",null,null
,,,
358,"Test Data Annotation. With the chosen parameters, each SMA setting generated a modified hierarchy using P1 as HYA and its corresponding auxiliary hierarchy. The preselected 500 test questions are used as test data to fairly evaluate the modified hierarchies by the three SMA settings and the baseline methods. Recall that the 500 questions are not included in the three parts (P1, P2, and P3) for parameter setting. Because BUC and HA do not change the leaf categories, the original labels of the 500 questions remain applicable. For SC and SMA, both changing leaf categories, we invited two annotators to label the 500 questions to their most relevant leaf categories in the modified hierarchies. We synthesized the results of the annotators, and assigned the labels for questions. If two annotators conflicted about a label, a third person made the final judgment. The dataset and their annotations are available online 5.",null,null
,,,
359,"Classification Results. Classification accuracy measured by MicroF1 using NB and SVM as base classifiers for the six methods (i.e., three baselines BUC, HA, SC, and the three SMA settings) on modified hierarchies is reported in Figure 5(a). For comparison, the classification accuracy on the original (or unmodified) Yahoo! Answers hierarchy is also reported under column named HYA. Figure 5(b) reports Macro-F1.",null,null
,,,
360,"As shown in Figures 5(a) and 5(b), all the three settings of SMA achieve significant improvement over the results obtained on the original hierarchy. For example, using NB as the base classifier, SMAYA|YA improves Micro-F1 over the results on the original hierarchy by 41.0% and improves Macro-F1 by 40.3%. NB achieves better accuracy than SVM probably because NB was used as the base classifier for parameter setting. The three baseline modification methods only slightly improve the classification accuracy over the original hierarchy and even deteriorate the accuracy in some cases. Recall that SC and SMA modify leaf categories while BUC and HA only modify internal structures of hierarchy without changing leaf categories. All the methods that change leaf categories outperform the methods that keep leaf categories unchanged. Note that SMAYA|YA significantly outperforms SC. One possible reason could be that SMAYA|YA utilizes the semantics of the original hierarchy in hierarchy modification while SC does not.",null,null
,,,
361,We observe that the auxiliary hierarchies employed by SMA have effect on the classification accuracy of the modified hierarchy.,null,null
,,,
362,5 http://www.ntu.edu.sg/home/gaocong/datacode.htm,null,null
,,,
363,798,null,null
,,,
364,MicroF1,null,null
,,,
365,0.9 NB,null,null
,,,
366,0.85 SVM 0.8,null,null
,,,
367,0.75 0.7,null,null
,,,
368,0.65 0.6,null,null
,,,
369,0.55 0.5 HYA,null,null
,,,
370,0.8 NB,null,null
,,,
371,0.75 SVM 0.7,null,null
,,,
372,0.65 0.6,null,null
,,,
373,0.55 0.5,null,null
,,,
374,0.45 0.4 HYA,null,null
,,,
375,BUC,null,null
,,,
376,HA,null,null
,,,
377,SC,null,null
,,,
378,SMAYA|YA SMAYA|AB SMAYA|ODP,null,null
,,,
379,(a) Micro-F1,null,null
,,,
380,BUC,null,null
,,,
381,HA,null,null
,,,
382,SC,null,null
,,,
383,SMAYA|YA SMAYA|AB SMAYA|ODP,null,null
,,,
384,(b) Macro-F1,null,null
,,,
385,MacroF1,null,null
,,,
386,"Figure 5: Micro-F1 and Macro-F1 on the modified hierarchies by three baselines and three SMA settings, and on the original Yahoo! Answers hierarchy.",null,null
,,,
387,Table 3: Classification accuracy on modifying AnswerBag,null,null
,,,
388,Measure/Hierarchy HAB SMAAB|YA Improvement(%),null,null
,,,
389,Macro-F1 (NB),null,null
,,,
390,0.3444 0.5638,null,null
,,,
391,63.70%,null,null
,,,
392,Macro-F1 (SVM) 0.3691 0.4933,null,null
,,,
393,33.60%,null,null
,,,
394,Micro-F1 (NB),null,null
,,,
395,0.4671 0.6669,null,null
,,,
396,42.80%,null,null
,,,
397,Micro-F1 (SVM) 0.4371 0.5697,null,null
,,,
398,30.30%,null,null
,,,
399,"Measured by Macro-F1, SMAYA|YA without using an external hierarchy slightly outperforms its counterpart SMAYA|AB or SMAYA|ODP, which uses an external hierarchy; while in terms of Micro-F1, SMAYA|YA performs worse than do its counterparts.",null,null
,,,
400,5.2.3 Experiments on AnswerBag,null,null
,,,
401,"In this set of experiments, SMA is used to modify the AnswerBag hierarchy. The main purpose is to evaluate whether SMA remains effective when the size of the auxiliary hierarchy is smaller than the one to be modified. Specifically, we use HYA as auxiliary hierarchy to modify HAB and evaluate the classification accuracy as we did in the earlier set of experiments. Note that the HAB has 265 categories which is more than twice of the 115 categories contained in HYA. The parameters  and  were set as 0.17 and 0.08, respectively, using the parameter setting approach described earlier. The classification accuracy is reported in Table 3. Observe that SMAAB|YA improves the classification accuracy (Macro- and Micro-F1) by 30% to 63%, compared with the result obtained before hierarchy modification. This demonstrates that the proposed SMA approach is effective for different hierarchies, even if the size of the auxiliary hierarchy is smaller than the hierarchy to be modified.",null,null
,,,
402,5.3 User Study,null,null
,,,
403,"A good category hierarchy must be semantically meaningful: (i) Its category labels should be easy to understand, facilitating data browsing; and (ii) Its category structure should reflect the topics of its data. We would like to note that it is challenging to evaluate these. We evaluate the modified hierarchy by SMAYA|AB through two types of user study by following the methods [2, 5], respectively.",null,null
,,,
404,Table 4: Comparison on appropriateness of category labels,null,null
,,,
405,Judgement,null,null
,,,
406,Number of documents,null,null
,,,
407,HnYA is better than HYA,null,null
,,,
408,12,null,null
,,,
409,HnYA is not as good as HYA,null,null
,,,
410,1,null,null
,,,
411,Both are equally good,null,null
,,,
412,81,null,null
,,,
413,Neither is good,null,null
,,,
414,6,null,null
,,,
415,Table 5: Averaged scores of HYA and HnYA (by SMAYA|AB),null,null
,,,
416,Measure/Hierarchy HYA HnYA,null,null
,,,
417,Cohesiveness,null,null
,,,
418,5.00 6.00,null,null
,,,
419,Isolation,null,null
,,,
420,4.00 4.67,null,null
,,,
421,Hierarchy,null,null
,,,
422,5.00 5.33,null,null
,,,
423,Navigation Balance 4.50 4.50,null,null
,,,
424,Readability,null,null
,,,
425,6.00 5.67,null,null
,,,
426,"Through the study, we aim to quantify both the appropriateness of the category labels and the structure of the modified hierarchy.",null,null
,,,
427,"Category Labels. Following a similar setting as in [2], we randomly selected 100 questions from the labeled test set originated from Yahoo! Answers. For each question, we gave the path of the categories in HYA from the second level category to the leaf category, and similarly the category path from HnYA (by SMAYA|AB). We asked three students to annotate which category path better reflects the topic of the question. Which hierarchy a category path was originated from was not provided to the annotators. Given a question, each volunteer is asked to rate each path from 1(lowest) to 5(highest) based on its quality. hen, we select one of the following choices based on the averaged ratings (rYA and rnYA for the two paths, respectively). (1)HnYA is better than HYA, if rnYA, rYA  [3, 5] and rnYA > rYA; (2)HnYA is not as good as HYA, if rnYA, rYA  [3, 5]; (3)Both are equally good, if rnYA, rYA  [3, 5] and rnYA ,"" rYA; (4)Neither is good, if rnYA, rYA  [1, 3). The statistics of the labels are reported in Table 4. The table shows that the number of questions having better labels in HnYA is larger than that in HYA although for majority of questions, the category paths from the two hierarchy are equally good. This result also suggests that the generated labels well reflects the content of categories.""",null,null
,,,
428,"Category Structure. Following the evaluation approaches in [5], we evaluate the quality of Yahoo! Answers hierarchy and the modified hierarchy by five measures. Cohesiveness: Judge whether the instances in each category are semantically similar. Since it is impractical to read all questions in a large category, we randomly select 50 questions from each category for cohesiveness evaluation. Isolation: Judge whether categories on the same level are discriminative from each other.We also use the 50 randomly selected questions to represent each category. Hierarchy: Judge whether the concepts represented by the categories become finer from top to bottom. Navigation Balance: Judge whether the number of child categories for each internal category is appropriate. Readability: Judge whether the concept represented by each category is easy to understand.",null,null
,,,
429,"We invited three students to evaluate the two hierarchies, HYA and HnYA (by SMAYA|AB) and assigned scores ranging from 0 to 7 on each measure. The mean of the scores is reported in Table 5.",null,null
,,,
430,"The cohesiveness of the modified hierarchy is better than the original one. A possible reason is that our approach detected the hidden topics and merged the most similar ones together. The isolation of the modified hierarchy is slightly better. This is probably because the proposed method takes isolation into consideration. To find out the reasons that caused the relatively low isolation of the original hierarchy, we get the list of categories with low scores from",null,null
,,,
431,799,null,null
,,,
432, ,null,null
,,,
433,,null,null
,,,
434,&&&,null,null
,,,
435, ,null,null
,,,
436," !"" #",null,null
,,,
437,!,null,null
,,,
438, #,null,null
,,,
439, $,null,null
,,,
440,#,null,null
,,,
441,,null,null
,,,
442,"% "" ",null,null
,,,
443,  ,null,null
,,,
444,&&&,null,null
,,,
445,   ,null,null
,,,
446,Figure 6: Portion of Yahoo! Answers hierarchy and its modified hierarchy.,Y,null
,,,
447,"the annotators. As an example, a number of questions that are related to motor-cycling were put under Other - Auto Racing by their askers, resulting in low isolation between the two categories. The modified hierarchy does not deteriorate hierarchy quality, navigation balance, and readability of the original hierarchy on average. In summary, the modified hierarchy is of high quality comparable to the original hierarchy generated by domain experts.",null,null
,,,
448,5.4 Case Study,null,null
,,,
449,"As a case study, we select three categories Software, Internet and Hardware from Yahoo! Answers as an example to illustrate the differences before and after modifying HYA. The modified hierarchy HnYA is by SMAYA|AB utilizing AnswerBag as auxiliary hierarchy.",Y,null
,,,
450,"The two hierarchies are shown in Figure 6. We make the following observations: 1) Different from the original hierarchy, Software and Internet become three categories ­ Operating System & Application Software, Internet & E-mail and Internet Software. The third category is formed based on the overlapping part of the original two categories, which contains questions about instant messaging (IM) and blog software. This demonstrates that the proposed approach can discover and detach the overlapping hidden topics. 2) Two pairs of categories of the original hierarchy, (Laptops & Notebooks and Desktops), and (Printers and Scanners), are merged into two categories in the modified hierarchy, because of the high similarity between the categories within each pair. This shows that categories with high overlap in semantics are merged. 3) For Hardware, some hidden topics are discovered and new categories are formed, like Storage and CPU & Memory & Motherboard, whose questions come from Desktops, Add-ons and Other - Hardwares in the original hierarchy. These newly formed categories are more isolated from each other.",null,null
,,,
451,6. CONCLUSION,null,null
,,,
452,"Category hierarchy plays a very important role in organizing data automatically (through classifiers built on the hierarchy) or manually. However, with newly available documents added into a hierarchy, new topics emerge and documents within the same category become less topically cohesive. Thus the hierarchies suffer from problems of structure irrelevance and semantic irrelevance, leading to poor classification accuracy of the classifiers developed for automatically categorizing the newly available documents into the hierarchy, which in turn leads to poorer document organization. To address these problems, we propose a novel approach SMA to modify a hierarchy. SMA comprises three non-trivial operations (namely, sprout, merge, and assign) to modify a hierarchy. Experimental results demonstrate that SMA is able to generate a modified",null,null
,,,
453,"hierarchy with better classification accuracy improvement over the original hierarchy than baseline methods. Additionally, user study shows that the modified category hierarchy is topically cohesive and semantically meaningful.",null,null
,,,
454,7. ACKNOWLEDGEMENTS,null,null
,,,
455,"Quan Yuan would like to acknowledge the Ph.D. grant from the Institute for Media Innovation, Nanyang Technological University, Singapore. Gao Cong is supported in part by a grant awarded by Microsoft Research Asia and by a Singapore MOE AcRF Tier 1 Grant (RG16/10).",null,null
,,,
456,8. REFERENCES,null,null
,,,
457,"[1] G. Adami, P. Avesani, and D. Sona. Bootstrapping for hierarchical document classification. In CIKM, pages 295­302, 2003.",null,null
,,,
458,"[2] C. C. Aggarwal, S. C. Gates, and P. S. Yu. On the merits of building categorization systems by supervised clustering. In KDD, pages 352­356, 1999.",null,null
,,,
459,"[3] R. Agrawal and R. Srikant. On integrating catalogs. In WWW, pages 603­612, 2001.",null,null
,,,
460,"[4] X. Cao, G. Cong, B. Cui, C. S. Jensen, and Q. Yuan. Approaches to exploring category information for question retrieval in community question-answer archives. ACM Trans. Inf. Syst., 30(2):1­38, 2012.",null,null
,,,
461,"[5] S.-L. Chuang and L.-F. Chien. A practical web-based approach to generating topic hierarchy for text segments. In CIKM, pages 127­136, 2004.",null,null
,,,
462,"[6] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Second Edition. The MIT Press and McGraw-Hill Book Company, 2001.",null,null
,,,
463,"[7] S. C. Gates, W. Teiken, and K.-S. F. Cheng. Taxonomies by the numbers: building high-performance taxonomies. In CIKM, pages 568­577, 2005.",null,null
,,,
464,"[8] A. Koestler. The Act of Creation. Penguin Books, New York, 1964. [9] D. Koller and M. Sahami. Hierarchically classifying documents",null,null
,,,
465,"using very few words. In ICML, pages 170­178, 1997. [10] K. Nitta. Improving taxonomies for large-scale hierarchical",null,null
,,,
466,"classifiers of web documents. In CIKM, pages 1649­1652, 2010. [11] K. Punera, S. Rajan, and J. Ghosh. Automatically learning document",null,null
,,,
467,"taxonomies for hierarchical classification. In WWW (Special interest tracks and posters), pages 1010­1011, 2005. [12] X. Qi and B. D. Davison. Hierarchy evolution for improved classification. In CIKM, pages 2193­2196, 2011. [13] B. Qu, G. Cong, C. Li, A. Sun, and H. Chen. An evaluation of classification models for question topic categorization. JASIST, 63(5):889­903, 2012. [14] G. Siolas and F. d'Alché Buc. Support vector machines based on a semantic kernel for text categorization. In IJCNN (5), pages 205­209, 2000. [15] A. Sun, E.-P. Lim, and Y. Liu. What makes categories difficult to classify?: a study on predicting classification performance for categories. In CIKM, pages 1891­1894, 2009. [16] P.-N. Tan, M. Steinbach, and V. Kumar. Introduction to Data Mining. Addison-Wesley, 2005. [17] L. Tang, J. Zhang, and H. Liu. Acclimatizing taxonomic semantics for hierarchical content classification from semantics to data-driven taxonomy. In KDD, pages 384­393, 2006. [18] N. Tomuro. Tree-cut and a lexicon based on systematic polysemy. In NAACL, 2001. [19] W. Wei, G. Cong, X. Li, S.-K. Ng, and G. Li. Integrating community question and answer archives. In AAAI, 2011. [20] Y. Yang and X. Liu. A re-examination of text categorization methods. In SIGIR, pages 42­49, 1999. [21] D. Zhang and W. S. Lee. Web taxonomy integration through co-bootstrapping. In SIGIR, pages 410­417, 2004. [22] L. Zhang, S. Liu, Y. Pan, and L. Yang. Infoanalyzer: a computer-aided tool for building enterprise taxonomies. In CIKM, pages 477­483, 2004.",null,null
,,,
468,800,null,null
,,,
469,,null,null

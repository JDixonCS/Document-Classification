,sentence,label,data
0,Dual Role Model for Question Recommendation in Community Question Answering,null,null
1,"Fei Xu1,2, Zongcheng Ji1,2, Bin Wang1,3",null,null
2,"1Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China 2Graduate University of Chinese Academy of Sciences, Beijing, China",null,null
3,"2{feixu1966, jizongcheng}@gmail.com 3wangbin@ict.ac.cn",null,null
4,ABSTRACT,null,null
5,"Question recommendation that automatically recommends a new question to suitable users to answer is an appealing and challenging problem in the research area of Community Question Answering (CQA). Unlike in general recommender systems where a user has only a single role, each user in CQA can play two different roles (dual roles) simultaneously: as an asker and as an answerer. To the best of our knowledge, this paper is the first to systematically investigate the distinctions between the two roles and their different influences on the performance of question recommendation in CQA. Moreover, we propose a Dual Role Model (DRM) to model the dual roles of users effectively. With different independence assumptions, two variants of DRM are achieved. Finally, we present the DRM based approach to question recommendation which provides a mechanism for naturally integrating the user relation between the answerer and the asker with the content relevance between the answerer and the question into a unified probabilistic framework. Experiments using a real-world data crawled from Yahoo! Answers show that: (1) there are evident distinctions between the two roles of users in CQA. Additionally, the answerer role is more effective than the asker role for modeling candidate users in question recommendation; (2) compared with baselines utilizing a single role or blended roles based methods, our DRM based approach consistently and significantly improves the performance of question recommendation, demonstrating that our approach can model the user in CQA more reasonably and precisely.",null,null
6,Categories and Subject Descriptors,null,null
7,H.3.3 [Information Search and Retrieval]: Information Filtering,null,null
8,General Terms,null,null
9,"Algorithms, Design, Experimentation",null,null
10,Keywords,null,null
11,"Community Question Answering, Role Analysis, Question Reommendation, Dual Role Model, PLSA",null,null
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. Copyright 2012 ACM 978-1-4503-1472-5/12/08 ...$15.00.",null,null
13,1. INTRODUCTION,null,null
14,"Community Question Aswering (CQA) is a web service where people can seek information (posting a question and getting the answer of it from others) and share knowledge (answering a question). Yahoo! Answers1 and Baidu Zhidao2 are two typical examples of CQA system. Compared with the traditional information retrieval, CQA bases on the community, which is a form of social network, so it can make best of user's collective wisdom to meet the information needs of users more easily and accurately.",null,null
15,"In CQA system, there are a large number of questions posted every day. Take Yahoo! Answers for example, there are about 207 thousands new questions asked daily [1]. If we can automatically recommend the new question to appropriate users to answer, it will help the question be resolved as soon as possible, which will improve the CQA system's performance. In addition, it will meet the answerers' needs to answer questions. As we can see, question recommendation is a very important component in a CQA system.",null,null
16,"The core issue of question recommendation is how to represent the users' interests (profile) and the questions, which is called the representation model. Based on that, we can assess the match between a question and each user, and then recommend the question to top N users who are the most consistent with it. Of course, we can solve question recommendation from another perspective, which is matching a user with each question and recommending the appropriate questions to him. Both of these types of recommendation tasks aim to make new questions answered as early as possible and satisfy the user better. Essentially, the key issues of both of them are the representation models for users and questions. As our target is recommending a new question to the appropriate users to answer, this paper focuses on the first type of recommendation task. At present, a lot of representation models have been proposed. Dror et al [5] represented the user and question as vectors consisting of multi-channel features and casted question recommendation as a classification problem. Other methods [2, 4, 6] utilized latent semantic models (PLSA, LDA, etc.) to model the user and question as the distribution of several topics.",null,null
17,"As we can see, each user in CQA plays two different roles (dual roles) simultaneously: the asker and the answerer. That is, a user not only posts his questions, but also is able to answer someone else's questions. Intuitively, the profiles of the two roles of users are different from each other, which existing methods have not paid attention to. For example, a piano teacher wants to learn some computer knowledge which he is not familiar with. Thus he is most likely to ask lots of questions related to computer, and answer many piano-related questions based on his specialty. As an asker, a user may post some questions in",null,null
18,1 http://answers.yahoo.com/,null,null
19,2 http://zhidao.baidu.com/,null,null
20,771,null,null
21,"the field that he is not familiar with. In contrast, as an answer, the user will solve the question which he is good at and interested in.",null,null
22,"Are there distinctions between users' roles? How do different roles affect the performance of question recommendation? Whether we can legitimately combine the characteristics of different roles to improve the effectiveness of the recommendation system? All of these important issues are worthy of our concern. However, current recommendation methods have not in-depth studied the different characteristics of users' roles and their different influences on question recommendation. All of previous methods only modeled the user using a single role, or simply mixed the two roles together to represent the user without considering the distinctions between roles.",null,null
23,"This paper systematically investigates the distinctions between users' dual roles and how they affect the performance of question recommendation differently. To the best of our knowledge, this is the first work on studying these important issues. While Nam et al. [30] observed that users in CQA are divided into askers and answerers and only a few of them both ask and answer in the same category through statistics, they have not theoretically analyzed the distinctions between users' different roles and their different influences on question recommendation. Moreover, we propose the Dual Role Model (DRM) to model the dual roles of users effectively. Finally, we present the DRM based approach to question recommendation, which takes full advantage of users' different roles to improve the effect of question recommendation. There are three primary contributions of our work.",null,null
24,"First, DRM which considers the two different roles of users separately provides a more precise and appropriate user representation model for question recommendation in CQA. Specifically, we utilize DRM to analyze the latent topic information of different roles for modeling the user. According to different independence assumptions, two variants of DRM are achieved: (1) independent DRM that assumes that users are independent of each other and models each user individually; (2) dependent DRM which considers the dependence between users.",null,null
25,"Next, we carried out systematic experiments on a real-world data to explore the distinctions between users' roles and compare the effects of recommendation methods that are based on asker role, answerer role or blending both of these roles. The results show that not only the two roles but also their influences on question recommendation are different from each other distinctly. In addition, simply mixing the roles together will impair the performance of recommender methods.",null,null
26,"Finally, our DRM based recommendation approach allows us to naturally integrate the user relation between the answerer and the asker with the content relevance between the answerer and the question into a unified probabilistic framework, which is more interpretable. Most previous methods only consider the content relevance. There have been several approaches that make use of the user relation [3, 5], however in these approaches, the user relation is either obtained through somewhat heuristic statistics outside of the model or combined with the content relevance by a linear interpolation.",null,null
27,"The remainder of this paper is organized as follows: Section 2 introduces some prior work related to our approach. Section 3 is the preliminary description of question recommendation in CQA. Section 4 discusses our dual role model and how to use it in question recommendation. Experimental results are presented in Section 5. At last, we conclude the paper and discuss about the future work in Section 6.",null,null
28,2. RELATED WORK,null,null
29,"In this part, we review previous work which is related to our approach: recommender system, question recommendation.",null,null
30,2.1 Recommender System,null,null
31,"Because question recommendation is a type of recommender system, we first review general recommender systems. Recommender systems can be divided into three stages based on how recommendations are made: content-based recommendations, collaborative filtering and hybrid approaches [18]. In contentbased recommendations, the user will be recommended items similar to the ones the user preferred in the past. In collaborative filtering, the user will be recommended items that people with the similar tastes and preferences liked in the past, that is, user will help each other find what they may like. In order to combine the advantage of both previous methods together, hybrid approaches are proposed. All these recommender systems firstly attempt to profile user preferences based on his history logs, and then recommend items according to the relevancy between him and items. Different kinds of methods are used to capture the model of users, such as classifying [24, 28], PLSA [13], matrix factorization [29], and ranking-oriented approach [17]. However, the user in these general recommender systems only plays one single role, which is significantly different from question recommendation. Therefore, we should pay close attention to this difference as we have mentioned in the above section.",null,null
32,2.2 Question Recommendation,null,null
33,"With CQA system becoming popular in recent years, many people turn their attention to question recommendation in CQA, e.g., [2, 3, 4, 5, 6, 16]. Overall, there are two main lines to solve this problem in previous work.",null,null
34,"On one hand, question recommendation is consider as a classifier problem which is similar to [5]. In [5], Dror et al. proposed a representation model based on multi-channel vector space model, where the user and question are represented as the vector with multiple dimension features from multi-channel data. Then, the matching degree between a user and a question is learned from their respective features using a binary classifier. Although this model treats user attributes in the answered-channel and askedchannel as two groups of features respectively, all the features are integrated into a single vector space model to represent the user's dual roles without considering the distinctions between user's different roles and their different influences on question recommendation.",null,null
35,"On the other hand, we can learn a ranking model to generate a recommendation list for question recommendation. In these earlier works, various extensions of Probabilistic Latent Semantic Analysis (PLSA) or other topic models are developed. Wu et al. [2] presented an incremental automatic question recommendation framework based on PLSA. Question recommendation in their work considered both the users' interests and feedback. Guo et al. [4] developed a general generative model based on basic Latent Dirichlet Allocation (LDA) model for questions and answers in CQA. In this approach, they combined topic-level information about questions and users with word-level information to improve question recommendation. In order to deal with the data sparsity, Qu et al. [6] used a user-word aspect model instead of direct aspect model [9] to model user preferences. However, all of these methods have used a single role, or simply blended roles to represent the user, which have not distinguished user's different roles and considered how they affect the performance of question recommendation differently.",null,null
36,772,null,null
37,3. PRELIMINARIES,null,null
38,Given the question set,null,null
39,... | | and the user set,null,null
40,"... | | , where | | is the number of questions and | | is the",null,null
41,number of users. Each question in Q is denoted as a triple,null,null
42,",,",null,null
43,". The is the text content of the question. For instance,",null,null
44,may include the title or the detailed description of the question.,null,null
45,"If we assume that words are independent, can be denoted as a",null,null
46,"bag of words ... | | , where | | is the number of words in .",null,null
47,The denotes the answerer of the question (it is also the an-,null,null
48,swerer role of user ). The denotes the asker of the question (it,null,null
49,is also the asker role of user ). If there are multiple answerers in,null,null
50,"the question, all answerers will be separated. If the question is not",null,null
51,"answered, is null.",null,null
52,"Based on the previous discussion in the section of related work,",null,null
53,we choose the idea of ranking to solve the problem of question,null,null
54,"recommendation. For a new posted question, the question an-",null,null
55,swerer recommendation task is to suggest a ranked list of users,null,null
56,"who are suitable to answer it. To tackle with this problem, we",null,null
57,need to resolve the two sub-problems: question and user represen-,null,null
58,"tation, the method of ranking recommendation candidates.",null,null
59,Since Probabilistic Latent Semantic Analysis (PLSA) [20],null,null
60,can effectively mines the latent semantic information of users and,null,null
61,"questions, it has been widely used to obtain the question and user",null,null
62,"representation in question recommendation, e.g., [2, 3, 4, 6].",null,null
63,PLSA assumes that users and questions are generated from a mix-,null,null
64,ture of some latent topics. We can compute the consisten-,null,null
65,cy between the distribution on topics of a user and a question to,null,null
66,determine whether to recommend the question to the user.,null,null
67,We summarized the previous PLSA based methods for question,null,null
68,recommendation and discovered that they can be divided into two,null,null
69,main categories: (1) methods that model the user indirectly. Simi-,null,null
70,"larly to [2], it takes a question as one document and use PLSA to",null,null
71,model the question to gain its distribution on topics at first. Then,null,null
72,the user can be represented as the average of topic distributions of,null,null
73,all the questions that he accesses; (2) methods that obtain the,null,null
74,"model of the user directly. In these methods, all the questions that",null,null
75,a user accesses are treated as one document. Then PLSA is used,null,null
76,directly to get the topic information of the user. A typical ap-,null,null
77,proach is the user-word aspect model applied by Qu et al. [6].,null,null
78,"This model is proposed by Popescul et al. [7], which improves",null,null
79,Hofmann's aspect model [9] for collaborative filtering.,null,null
80,"However, when these PLSA based methods modeling the user,",null,null
81,they did not pay attention to the user's dual roles and their distinc-,null,null
82,tions. In order to effectively analyze characteristics of different,null,null
83,roles and make use of both of user roles to improve the perfor-,null,null
84,"mance of question recommendation, we propose a Du-",null,null
85,al Role Model (DRM) based on PLSA to model the user in CQA,null,null
86,"precisely. According to different independence assumptions, we",null,null
87,"implement two variants of DRM. In the next section, we will de-",null,null
88,tail generation processes of these variants and describe the DRM,null,null
89,based method for question recommendation.,null,null
90,4. MODEL DESCRIPTION,null,null
91,4.1 Independent DRM,null,null
92,"With the assumption that all users are independent of each other in independent DRM (IDRM), we separately model the dual roles of each user. As Figure 1 illustrates, the IDRM can be divided into two steps. First, we employ the PLSA to analyze the topic information of all the questions, and then model the answerer role and asker role of each user based on questions which he answers or asks.",null,null
93,z,null,null
94,q |t| |Q|,null,null
95,Figure 1: Independent DRM.,null,null
96,We introduce the latent variable,null,null
97,to indicate each,null,null
98,topic under users and questions. The model of user's answerer,null,null
99,"role can be represented as its topic distribution | . Similarly,",null,null
100,the asker role is characterized by,null,null
101,| and the latent topic,null,null
102,information of the question is | . According to the first step,null,null
103,"of IDRM in the Figure 1, the generative model for question/word",null,null
104,co-occurrences is defined as: a latent topic is obtained with,null,null
105,"probability , and then a question q is generated with probabil-",null,null
106,ity | and a word is generated with probability | .,null,null
107,"Therefore, we can compute the joint probability , of ob-",null,null
108,serving a question together with a word based on topic varia-,null,null
109,ble as follows:,null,null
110,",",null,null
111,|,null,null
112,|,null,null
113,"Then considering all question/word pairs , set , the log likelihood is",null,null
114,in question,null,null
115,",",null,null
116,",",null,null
117,",",null,null
118,"where q, w is the frequency of word in the question . We use the Expectation Maximization (EM) method to learn",null,null
119,"the model parameters , | and | : E-Step,",null,null
120,"|,",null,null
121,|,null,null
122,|,null,null
123,|,null,null
124,|,null,null
125,"M-Step,",null,null
126,",",null,null
127,"|,",null,null
128,",",null,null
129,|,null,null
130,",",null,null
131,"|,",null,null
132,|,null,null
133,",",null,null
134,"|,",null,null
135,"After obtaining all questions' representations, we perform the second step to get the representations of users' different roles. The user's answerer role is defined as the combination of topic distributions of all questions that he answers, and the modeling method is similar for the asker role. Intuitively, we can give an example to illustrate the feasibility of this approach. For example, if a user answers lots of questions related to using computer, so the profile",null,null
136,773,null,null
137,"of his answerer role is very likely to be related to this topic. Specifically, the role models | and | are estimated as:",null,null
138,|,null,null
139,| |,null,null
140,|,null,null
141,| |,null,null
142,"where, and",null,null
143,"is the set of questions that the user answers, is the set of question he asks.",null,null
144,4.2 Dependent DRM,null,null
145,"Different from the IDRM, the assumption made in dependent",null,null
146,DRM (DDRM) is that there is dependence between users. As we,null,null
147,"can see in Figure 2, DDRM assumes that the answer and the asker",null,null
148,are dependent on each other when not observing the latent varia-,null,null
149,ble. The assumed generative model is as follows. We first pick a,null,null
150,"latent topic to some prior . We then generate the answerer ,",null,null
151,"the asker , and the content",null,null
152,of question with corre-,null,null
153,"sponding probability | , | , and ",null,null
154,|,null,null
155,",.",null,null
156,"Thus, the joint probability distribution of a triple , ,",null,null
157,of,null,null
158,question is defined as:,null,null
159,",,",null,null
160,|,null,null
161,|,null,null
162,|,null,null
163,",",null,null
164,"In the above equation, , is the frequency of word in the content .",null,null
165,"Accordingly, the log likelihood in DDRM is",null,null
166,"log , ,",null,null
167,",,",null,null
168,"and we can also train the model using EM method as follows: E-step,",null,null
169,"|, ,",null,null
170,|,null,null
171,|,null,null
172,|,null,null
173,",",null,null
174,|,null,null
175,|,null,null
176,|,null,null
177,",",null,null
178,"M-step,",null,null
179,"|, ,",null,null
180,",,",null,null
181,|,null,null
182,",",null,null
183,"|, ,",null,null
184,",,",null,null
185,|,null,null
186,"|, ,",null,null
187,",",null,null
188,|,null,null
189,"|, ,",null,null
190,",",null,null
191,"The IDRM and the DDRM respectively model the user's dual roles from different perspectives. Compared with previous models that do not take the dual roles and their distinctions into account, DRM provides a more precise and appropriate user representation model for question recommendation. Apart from different independence assumptions between users, we can see that the IDRM is a type of method modeling user role indirectly while the DDRM is a method which learns the role model directly.",null,null
192,4.3 Question Recommendation,null,null
193,"Based on any one of the above DRM variants, we build the",null,null
194,DRM based method for question recommendation that takes full,null,null
195,"advantage the characteristics of different user roles. When a new question arriving, we compute posterior probability | for",null,null
196,z,null,null
197,|t|,null,null
198,q,null,null
199,|Q|,null,null
200,Figure 2: Dependent DRM.,null,null
201,"each candidate user , and then recommend this question to the top N users. | is obtained by:",null,null
202,|,null,null
203,",",null,null
204,",,",null,null
205,|,null,null
206,||,null,null
207,|,null,null
208,|,null,null
209,/| |,null,null
210,|,null,null
211,",",null,null
212,"where, the first step uses the Bayesian formula for an equivalent",null,null
213,"transformation. In the second step, the question is decomposed",null,null
214,"into its content and its asker . In addition, we only need to",null,null
215,consider the candidate's answerer role when we evaluate whether,null,null
216,"he is suitable to answer this question. Therefore, the is repre-",null,null
217,sented as his answerer role . The third step and fourth step,null,null
218,is based on the role models and the question model obtained in,null,null
219,"DRM, where the generation probability | is normalized by",null,null
220,the length of question content | |.,null,null
221,"In this recommendation approach, |",null,null
222,| denotes the,null,null
223,"consistency of the answerer and the asker over topics, which",null,null
224,models the user relation between the answerer and the asker. Cor-,null,null
225,"respondingly,",null,null
226,|,null,null
227,"| , measures the con-",null,null
228,"sistency of the answerer and the question content over topics,",null,null
229,which models the content relevance between the answerer and the,null,null
230,"question. As we can see, our DRM based method takes full ad-",null,null
231,vantage of users' dual roles to improve the performance of ques-,null,null
232,"tion recommendation. Moreover, this method utilizes a unified",null,null
233,probabilistic framework to naturally associate the user relation,null,null
234,"with the content relevance together, which is more interpretable.",null,null
235,"Compared with the DRM, both the methods described in [2]",null,null
236,and [6] employed a single role model to represent the user and,null,null
237,ignored the user relation when recommending question to users.,null,null
238,"In the next section, these methods will be used as two groups of",null,null
239,baselines in our experiments.,null,null
240,5. EXPERIMENTS,null,null
241,We evaluate the proposed approach using a real-world data from Yahoo! Answers and conduct different experiments to address the following questions: (1) Are there any distinctions between users' dual roles and how they affect the result of question recommendation? (2) Does the proposed DRM improve the effectiveness of question recommendation compared with other base-,null,null
242,774,null,null
243,"line methods? (3) Which of the two variants of DRM for question recommendation, namely IDRM and DDRM, is more effective?",null,null
244,5.1 Data Sets,null,null
245,"In order to obtain the data sets for experiments, we used Yahoo! Answers API3 to crawl 246490 resolved questions posted in 2011 from Yahoo! Answers. All the questions are lowercased and all stop words are removed from questions using a standard list of 418 common terms before further experiments.",null,null
246,"In our question set , we divide the whole question set and user set , into three subsets according to the user participation degree. For each subset, we split it into the training set and the testing set based on the asked time of questions. The training set is used solely for parameter estimation and the test set is used for evaluation purposes. In each subset, we take about 9/10 of questions as training set, and the rest as testing set. The data set statistics of all subsets are listed in Table 1. Each dataset contains a question set and a user set. For instance, the question set and user set of User-10 are and . We selected users who asked or answered more than 10 questions as the user set and then collected questions which were asked or answered by users in as the question set . Other subsets are similar to User-10.",null,null
247,User-10 User-15 User-20,null,null
248,Question Number,null,null
249,32009,null,null
250,28404,null,null
251,25690,null,null
252,Answer Number,null,null
253,97911,null,null
254,89144,null,null
255,80677,null,null
256,User Number 2515,null,null
257,1339,null,null
258,870,null,null
259,Table 1: Statistic of Yahoo! Answers data set.,null,null
260,5.2 Evaluation Metric,null,null
261,"In traditional recommender systems, precision is a commonly used measure to evaluate the performance. However, precision is not suitable in the CQA context. There are so many questions asked in a CQA community every day [1] that the user can only access a very small portion of all questions. While the questions one accessed are those he is interested in, we can not guarantee that the remaining unaccessed questions are those he does not like. That is, in some cases, a user did not access a question just because he had no chance to see the question in CQA system. Therefore, we employ a new metric proposed in [6] to evaluate the effectiveness of question recommendation in CQA.",null,null
262,"For a question in testing set, the user who provides the best answer (named the best answerer, Adamic et al. [27] have verified that answers selected as the best ones are mostly indeed the most suitable for the questions.) of this question is seemlier to answer it compared with other answerers, so it is more reasonable to recommend this question to the best answerer than other answerers. Based on this intuition, we only recommend the question to the users who actually answered it instead of all possible users in the whole dataset. Then the recommendation accuracy for this question is defined according to the rank of the user who provides the best answer. (We only keep the questions which have more than one answer and are already labeled with the best answers in the testing set.) Therefore, according to the evaluation metric applied",null,null
263,3 http://developer.yahoo.com/answers,null,null
264,"in [6], we utilize the best answerer's rank as the ground truth of",null,null
265,our evaluation metric:,null,null
266,||,null,null
267,1,null,null
268,|| 1,null,null
269,"where | | is the length of recommending list, which is equally the",null,null
270,"number of answers, and",null,null
271,is the rank of the best answerer.,null,null
272,5.3 Role Analysis,null,null
273,"We first discuss an interesting subject: the distinction between the user's two roles. Based on latent topic analysis of user roles in DRM, the distinction between the answer role and the asker role is defined as the difference between their topic distributions. The larger the difference between the topic distributions is, the greater the distinction between roles is. In information theory, the Kullback-Leibler divergence (KL divergence) is a commonly used non-symmetric measure of the difference between two probability distributions. We apply the KL divergence to assess the distinction between user roles. According to the modeling results of user roles in DRM, we obtain the latent topic distributions of each user role:",null,null
274,|,null,null
275,|,null,null
276,|,null,null
277,|,null,null
278,|,null,null
279,|,null,null
280,|,null,null
281,|,null,null
282,"Based on the above two equations, the distinction between answer role and asker role of the user is",null,null
283,||,null,null
284,|,null,null
285,log,null,null
286,| |,null,null
287,"In DRM, the number of topics is a parameter that has siginificant impact on the performance. We utilize crossvalidation to estimate the parameters. Based on experiments of tuning parameter, we empirically set topic number to 70 to train our DRM.",null,null
288,"First, we analyze the average of KL divergence of all users in the user set of each subset to measure the overall distinction between user roles. The results are summarized in Table 2. Across",null,null
289,data subsetsthe overall role distinction in IDRM is about 1.3 to,null,null
290,"1.5, and that in DDRM is about 2.4. Compared with IDRM, the role distinction in DDRM is greater and relatively more stable over different data subsets.",null,null
291,IDRM DDRM,null,null
292,User-10 1.349 2.440,null,null
293,User-15 1.379 2.467,null,null
294,User-20 1.502 2.441,null,null
295,Table 2: The overall distinction between user roles.,null,null
296,"Furthermore, we take User-10 as an example to detail the distribution of role distinction, which is illustrated in Figure 3. For the DDRM, role distinction of 65.5% of users is more than 1.0, and most of them is in the range of [0.5, 3.5]. For the DDRM, role distinction of 74.3% of users is more than 2, most of which is in the range of [1.5, 4.5]. This shows that there are clear differences between different roles of most of users in CQA.",null,null
297,"Another important result to note is that the role distinction in DDRM is more obvious and relatively more stable than that in IDRM, which can be observed in both of overall and detailed role analysis. This result may be due to that DDRM models the depen-",null,null
298,775,null,null
299,Figure 3: Distribution of the role distinction.,null,null
300,User Ro1e Answerer,null,null
301,Asker,null,null
302,Topic ID 41 (using computer),null,null
303,30 (programming),null,null
304,Top Words,null,null
305,"free, best, windows, antivirus, virus, anti, software, spyware, download, program, xp, vista, hard, drive",null,null
306,"c, program, file, java, programming, write, language, convert, system, net, code, array, php, number",null,null
307,Table 3: An example of the difference between user roles.,null,null
308,Role,null,null
309,Answerer Role Asker Role,null,null
310,Blended Roles,null,null
311,VSM VSM-an VSM-as VSM-bl,null,null
312,PLSA1 (modeling user indi-,null,null
313,rectly),null,null
314,PLSA2 (modeling user directly),null,null
315,PLSA1-an,null,null
316,PLSA2-an,null,null
317,PLSA1-as,null,null
318,PLSA2-as,null,null
319,PLSA1-bl,null,null
320,PLSA2-bl,null,null
321,Table 4: All versions of baselines considering users' different roles.,null,null
322,dence between users which is more effective to capture the peculiarities of different user roles.,null,null
323,"After discussing the role analysis of all users, we take a typical user in DDRM as an illustrative example to show the modeling results of user roles. Table 3 lists the topic with maximum probability and corresponding top words of the topic for both of user roles. As the result shows, this user is most likely to be a junior programmer (such as junior college students from school of computer science). He has some basic knowledge of computer and is familiar with using computer, so he solved many questions about that topic. While he may be just getting started with computer programming, a lot of questions he asked are related to programming.",null,null
324,5.4 Question Recommendation,null,null
325,5.4.1 Result Comparison,null,null
326,"In this section, we explored how different roles of users affect the result of question recommendation. Moreover, we compared our DRM-based question recommendation method with other methods.",null,null
327,"The models proposed in previous work are classified into two main categories. The first one is the word-level Vector Space Model (VSM) which is directly used to compute the similarity between users and questions. VSM only make use of word-level information to model users and questions. For example, the user and question are represented as vectors with tf.idf word weights, and then cosine similarity between them is defined as:",null,null
328,",",null,null
329,· || || · || ||,null,null
330,.,null,null
331,",.",null,null
332,",",null,null
333,.,null,null
334,",",null,null
335,.,null,null
336,",",null,null
337,where .,null,null
338,", is the word 's tf.idf weight in question ,",null,null
339,and .,null,null
340,", is the sum of 's tf.idf weights in questions that",null,null
341,asks or answers.,null,null
342,The second one is PLSA based methods. As we have specified,null,null
343,"in section 3, these methods model the user either indirectly or",null,null
344,"directly. For the former, we took the model in [2] (PLSA1) as",null,null
345,776,null,null
346,VSM PLSA1 PLSA2 DRM,null,null
347,VSM-an VSM-as VSM-bl PLSA1-an PLSA1-as PLSA1-bl PLSA2-an PLSA2-as PLSA2-bl IDRM DDRM,null,null
348,User-10 0.555 0.456 0.541 0.639 0.445 0.622 0.638 0.447 0.619 0.669* 0.685* 7.2%,null,null
349,User-15 0.600 0.492 0.591 0.650 0.482 0.642 0.651 0.485 0.647 0.675* 0.690* 6%,null,null
350,User-20 0.612 0.466 0.581 0.671 0.448 0.628 0.674 0.441 0.622 0.683* 0.697* 3.4%,null,null
351,Table 5: Recommendation accuracies of different methods for question recommendation. Each underlined value means the best result for each baseline group. `*' means the corresponding improvement over all baselines is statistically significant.,null,null
352,"baseline. For the latter, we implemented the ""user-word aspect model"" presented in [6] (PLSA2) as another baseline. The details of PLSA1 and PLSA2 have been described in section 3.",null,null
353,"In order to explore the impact of different user roles on question recommendation, we implemented the different versions of the three groups of baselines. These versions are based on the answerer role, the asker role, or the blended roles. Table 4 shows the labels of all baseline methods. Each version of a baseline is trained on the question set that users access under the corresponding role. Specifically, the blended roles mean all the question that each user answers or asks are simply mixed together as one set.",null,null
354,"Based on cross-validation, we selected the best topic number for PLSA1 and PLSA2. The recommendation results of baselines and our DRM are summarized in Table 5, where the best result for each baseline group is underlined and the best result in each data subset is highlighted.",null,null
355,"We first compare the performances of different roles in each baseline group. From Table 5, we observe that the answerer role always wins the best result in all baseline groups across data subsets. Especially, the answerer role is obviously better than the asker role over all the recommendation results. When data sets become denser and denser from User-10 to User-20, the effect of the answerer role becomes better and better as we expect. On the contrary, the result of the asker role appears an unexpected decrease in User-20. Furthermore, we examine the recommendation results of blended roles. As we can see, simply mixing the asker role into the answerer role not only fails to improve but worsens recommendation results of answerer role instead. According to above experiments, we conclude that different user roles reflect the different aspects of the user, moreover, there are clear distinctions between their influences on question recommendation. When modeling the user in CQA, we must distinguish the different user roles. When recommending new questions to users, it would be more appropriate to use the answerer role model to represent the candidate users.",null,null
356,"Since the recommendation methods based on blended roles do not work well, whether our DRM based method can make full use of user's dual roles to improve the recommendation result? Tested",null,null
357,"on each data subset, our model exhibits good performance, significantly outperforming all baselines. The relative improvement of DDRM over the best baseline result is 7.2% for User-10, 6% for User-15, and 3.4% for User-20. In addition, DDRM is significantly better than IDRM across data sets, which means considering the dependence between uses is more effective to model user roles. This result is also consistent with the above role analysis.",null,null
358,"Another interesting result to note is that the PLSA1 which models the user indirectly is almost equivalent to PLSA2 which models the user directly on three data subsets, suggesting that it is feasible to model the user indirectly by combining the topic information questions that he accesses. Additionally, it is clear that all methods based on latent topic analysis (PLSA1, PLSA2, and DRM) always perform better than word-level VSM, which demonstrates that the latent topic based model can be more effective to represent the profile of user. And this result also verifies the conclusion drew in [6]. Moreover, it is part of the reason for that Guo et al. [4] introduced topic-level model to improve heuristic word-level methods.",null,null
359,5.4.2 Parameter Sensitivity,null,null
360,"We note that the topic number K is an important parameter in our proposed DRM. Therefore, we are interested in analyzing the sensitivity of the recommendation performance of DRM with respect to the topic number. We tested these two DRM variants with 8 different values of K, which is illustrated in Figure 4. Like previous role analysis, we only present the final results in data subset User-10. The results for other subsets are similar. As we can see from Figure 4, the recommendation accuracy gradually increases when the topic number varies from 10 to 40. Then we observe that the effectiveness of both DRM based recommendation approaches begins to be relatively stable when topic number is more than 40.",null,null
361,6. CONCLUSION & FUTURE WORK,null,null
362,"The user in CQA plays two different roles (dual roles) simultaneously, which is different from the user in a general recommender system. In this paper, we have systematically investigated the",null,null
363,777,null,null
364,Figure 4: Sensitivity to the topic number of DRM.,null,null
365,"distinctions between users' dual roles and how they affect the performance of question recommendation differently. Moreover, in order to represent the user in CQA with dual roles more reasonably and precisely, we proposed a Dual Role Model (DRM) to model the user's different roles. With different independence assumptions, two variants of DRM were achieved, which were independent DRM (IDRM) and dependent DRM (DDRM). Finally, we presented the DRM based approach to question recommendation which can take full advantage of the particularities of users' different roles. Based on a unified probabilistic framework, our DRM based method naturally combines the user relation between the answerer and the asker with the content relevance between the answerer and the question.",null,null
366,"Our experiments were carried out on a real-world data crawled from Yahoo! Answers. First, the results of user role analysis showed that there are evident differences between the answerer role and asker role of users in CQA. Comparing the effects of the two roles on question recommendation, we discovered that the answerer role model is more appropriate to represent the candidate users when recommending new questions to users. Additionally, an interesting result was that simply mixing the asker role into the answerer role not only failed to improve but impaired recommendation results of answerer role instead. Furthermore, we compared our DRM based recommendation methods with baseline methods based on a single role or blended roles. Experiment results on three data subsets showed our DRM significantly outperforms all baselines, where the relative improvement of DRM over the best baseline result is 7.2% for User-10, 6% for User-15, and 3.4% for User-20. In addition, DDRM is more effective than IDRM across data subsets, suggesting it is more effective to model users' dual roles. Finally, the parameter sensitivity analysis showed our DRM approach is robust.",null,null
367,"There are two interesting future research directions to explore. One of the most interesting directions is to further study how the roles of users will vary over time, and whether that will have influence on question recommendation. The other interesting direction is how to diversify the recommendation results to satisfy users better.",null,null
368,7. ACKNOWLEDGMENTS,null,null
369,We thank the anonymous reviewers for their useful comments. This work is supported by the National Science Foundation of China under Grant No. 61070111.,null,null
370,8. REFERENCES,null,null
371,"[1] L. Rao. Yahoo mail and im users update their status 800 million times a month. TechCrunch, Oct282009. http://techcrunch.com/2009/10/28/yahoo-mail-and-imusersupdate-their-status-800-million-times-a-month/.",null,null
372,"[2] Hu Wu, Yongji Wang and Xiang Cheng. Incremental Probabilistic Latent Semantic Analysis for automatic question recommendation. In RecSys'08, pages 99-106, 2008.",null,null
373,"[3] Damon Horowitz and Sepandar D. Kamvar. The anatomy of a large-scale social search engine. In WWW'10, pages 431440, 2010.",null,null
374,"[4] Jinwen Guo, Shengliang Xu, Shenghua Bao, and Yong Yu. Tapping on the potential of Q&A community by recommending answer providers. In CIKM'08, pages 921-930, 2008.",null,null
375,"[5] Gideon Dror, Yehuda Koren, Yoelle Maarek and Idan Szpektor. I want to answer, who has a question? Yahoo! Answers recommender system. In SIGKDD'11, pages 11091117, 2011.",null,null
376,"[6] Mingcheng Qu, Guang Qiu, Xiaofei He, Cheng Zhang, Hao Wu, Jiajun Bu, and Chun Chen. Probabilistic question recommendation for question answering communities. In WWW'09, pages 1229-1230, 2009.",null,null
377,"[7] Alexandrin Popescul, Lyle H. Ungar, David M. Pennock and Steve Lawrence. Probabilistic models for unified collaborative and content-based recommendation in sparse-data environments. In UAI'01, pages 437-444, 2001.",null,null
378,"[8] Baichuan Li, Irwin King and Michael R. Lyu. Question routing in community question answering- putting category in its place. In CIKM'11, pages 2041-2044, 2011.",null,null
379,[9] Thomas Hofmann and Jan Puzicha. Latent class models for,null,null
380,"collaborative filtering. In IJCAI'99, pages 688­693. 1999.",null,null
381,"[10] Thomas Hofmann. Probabilistic latent semantic indexing. In SIGIR'99, pages 50-57, 1999.",null,null
382,"[11] Luo Si and Rong Jin. Flexible mixture model for collaborative filtering. In ICML'03, 2003.",null,null
383,"[12] David M. Blei , Andrew Y. Ng and Michael I. Jordan. Latent dirichlet allocation. In Journal of Machine Learning Research, pages 993-1022, 2003.",null,null
384,"[13] Thomas Hofmann. Collaborative filtering via gaussian probabilistic latent semantic analysis. In SIGIR'03, pages 259-266, 2003.",null,null
385,"[14] Tom Chao Zhou, Chin-Yew Lin, IrwinKing, Michael R. Lyu, Young-In Song and Yunbo Cao. Learning to suggest questions in online forums. In AAAI'11, pages 1298-1303, 2011.",null,null
386,"[15] Qiaoling Liu and Eugene Agichtein. Modeling answerer behavior in collaborative question answering systems. In ECIR'11, pages 67-79, 2011.",null,null
387,"[16] Ke Sun, Yunbo Cao, Xinying Song, Young-In Song, Xiaolong Wang and Chin-Yew Lin. Learning to recommend questions based on user rating. In CIKM'09, pages 751-758, 2009.",null,null
388,"[17] Nathan N. Liu and Qiang Yang. EigenRank: A rankingoriented approach to collaborative filtering. In SIGIR'08, pages 83-90, 2008.",null,null
389,778,null,null
390,"[18] Adomavicius G., and Tuzhilin A. Toward the next generation of recommender systems: A survey of the state-of-theart and possible extensions. In IEEE Trans. on Knowledge and Data Engineering, 17(6), 2005.",null,null
391,"[19] P. Kantor, F. Ricci, L. Rokach, and B. Shapira. Recommender Systems Handbook: A complete guide for research scientists and practitioners. Springer, 2010.",null,null
392,"[20] Thomas Hofmann. Unsupervised learning by probabilistic latent semantic analysis. Maching Learning Journal, Vol. 42, No. 1-2, pages. 177-196, 2001.",null,null
393,"[21] Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee and Soyeon Park. A framework to predict the quality of answers with non-textual features. In SIGIR'06, pages 228-235, 2006.",null,null
394,"[22] Jiwoon Jeon, W. Bruce Croft and Joon Ho Lee. Finding semantically similar questions based on their answers. In SIGIR'05, pages 617-618, 2005.",null,null
395,"[23] G. Linden, B. Smith, and J. York. Amazon.com recommendations: Item-to-item collaborative filtering. In IEEE Internet Computing, 07(1):76-80, 2003.",null,null
396,"[24] Jiahui Liu, Peter Dolan, Elin Rønby Pedersen. Personalized news recommendation based on click behavior. In IUI'10, pages 31-40, 2010.",null,null
397,"[25] Y. Cao, H. Duan, Chin-Yew Lin, Y. Yu and Hsiao-Wuen Hon. Recommending questions using the MDL-based tree cut model. In WWW'08, pages 81-90, 2008.",null,null
398,"[26] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, and R. Harshman. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391407, 1990.",null,null
399,"[27] J. Zhang, L. A. Adamic, E. Bakshy and Mark S. Ackerman. Every-one knows something: Examining knowledge sharing on Yahoo! Answers. In WWW'08, pages 665-674, 2008.",null,null
400,"[28] M. Pazzani and D. Billsus. Learning and revising user profiles: The identification of interesting web sites. Machine Learning. vol. 27, pages 313-331, 1997.",null,null
401,"[29] Y. Koren, R. M. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Journal of Computer, 42(8):30-37, 2009.",null,null
402,"[30] Kevin K. Nam, Mark S. Ackerman, Lada A. Adamic. Questions in, knowledge in? A study of Naver's question answering community. In CHI'09, pages 779-788, 2009.",null,null
403,"[31] Pawel Jurczyk, Eugene Agichtein. Discovering authorities in question answer communities by using link analysis. In CIKM'07, pages 919-922, 2007.",null,null
404,779,null,null
405,,null,null

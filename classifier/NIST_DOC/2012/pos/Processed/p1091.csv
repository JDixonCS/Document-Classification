,sentence,label,data
0,Impact of Assessor Disagreement on Ranking Performance,null,null
1,Pavel Metrikov Virgil Pavlu Javed A. Aslam,null,null
2,"College of Computer and Information Science Northeastern University, Boston, MA, USA",null,null
3,"{metpavel, vip, jaa}@ccs.neu.edu",null,null
4,ABSTRACT,null,null
5,"We consider the impact of inter-assessor disagreement on the maximum performance that a ranker can hope to achieve. We demonstrate that even if a ranker were to achieve perfect performance with respect to a given assessor, when evaluated with respect to a different assessor, the measured performance of the ranker decreases significantly. This decrease in performance may largely account for observed limits on the performance of learning-to-rank algorithms.",null,null
6,Categories and Subject Descriptors: H. Information Systems; H.3 Information Storage and Retrieval; H.3.3 Information Search and Retrieval:Retrieval models,null,null
7,"General Terms: Experimentation, Measurement, Theory",null,null
8,"Keywords: Inter-assessor Disagreement, Learning-to-Rank, Evaluation",null,null
9,1. INTRODUCTION,null,null
10,"In both Machine Learning and Information Retrieval, it is well known that limitations in the performance of ranking algorithms can result from several sources, such as insufficient training data, inherent limitations of the learning/ranking algorithm, poor instance features, and label errors. In this paper we focus on performance limitations due solely to label ""errors"" which arise due to inter-assessor disagreement.",null,null
11,"Consider a training assessor A that provides labels for training data and a testing assessor B that provides labels for testing data. Even if a ranker can produce a perfect list as judged by A, its performance will be suboptimal with respect to B, given inevitable inter-assessor disagreement. In effect, no ranking algorithm can simultaneously satisfy two or more disagreeing assessors (or users). Thus, there are inherent limitations in the performance of ranking algorithms, independent of the quality of the learning/ranking algorithm, the availability of sufficient training data, the quality of extracted instance features, and so on.",null,null
12,"We model inter-assessor disagreement with a confusion matrix C, where cij corresponds to the (conditional) probability that a document labeled i by testing assessor B will be labeled j by training assessor A, for some given set of label grades such as {0, 1, 2, 3, 4}. Given such a model of interassessor disagreement, we ask the question, ""What is the",null,null
13,9This work supported by NSF grant IIS-1017903.,null,null
14,"Copyright is held by the author/owner(s). SIGIR'12, August 12­16, 2012, Portland, Oregon, USA. ACM 978-1-4503-1472-5/12/08.",null,null
15,"expected performance of a ranked list optimized for training assessor A but evaluated with respect to testing assessor B?"" We approach this question in two ways, via simulation and closed form approximation. In the former case, we use the confusion matrix C to probabilistically generate training labels A from testing labels B, optimally rank documents according to A, and evaluate with respect to B. In the latter case, we analytically derive a closed-form approximation to this limiting performance, as measured by nDCG.",null,null
16,"Given a confusion matrix C modeling inter-assessor disagreement, one can apply our results to any learning-to-rank dataset. The limiting nDCG values obtained correspond to reasonable upper bounds on the nDCG performance of any learning-to-rank algorithm, even one given unlimited training data and perfect features. Considering the performance of existing algorithms on these datasets, and comparing with the upper bounds we derive, one can argue that learning-torank is approaching reasonable limits on achievable performance.",null,null
17,2. ASSESSOR DISAGREEMENT MODEL,null,null
18,"Much research in the IR community has focused on addressing the problem of system evaluation in the context of missing, incomplete, or incorrect document judgments [1]. Soboroff and Carterette [4] provide an in-depth analysis of the effect of assessor disagreement on the Million Query Track evaluation techniques. Both assessors and users often disagree on the degree of document relevance to a given query, and we model such disagreement with a confusion matrix C as described above. On data sets with multiple assessments per query-document pair, such as the TREC Enterprise Track [2], these confusion matrices can be directly estimated from data, and they can be obtained from user studies as well [3, 5].",null,null
19,"For any ranked list returned by a system, the expected limiting nDCG due to assessor disagreement can be formulated as a function of (1) the disagreement model C and (2) the number of assessed documents and their distribution over the label classes. One way to compute this expected nDCG is numerical simulation: For every document d having testing label id in the ranked list, we randomly draw an alternative label jd with the probability cij; we then sort the ranked list in decreasing order of {jd} and evaluate its nDCG performance with respect to labels {id}. This simulation is repeated multiple times and the results averaged to obtain an accurate estimate of the expected limiting nDCG.",null,null
20,"In our first experiment, we test whether the inter-assessor confusion matrix C alone can be used to estimate the limiting nDCG value. We do so by considering data sets that have multiple judgments per query-document pair, such as were collected in the TREC Enterprise Track where each",null,null
21,1091,null,null
22,Real NDCG Bound Real NDCG Bound,null,null
23,Real vs. Simulated NDCG Bounds (Enterprise C),null,null
24,Real vs. Simulated NDCG Bounds (MSR C),null,null
25,1,null,null
26,1,null,null
27,0.9,null,null
28,0.9,null,null
29,0.8,null,null
30,0.8,null,null
31,0.7,null,null
32,0.7,null,null
33,0.6,null,null
34,SG,null,null
35,0.6,null,null
36,SG,null,null
37,BG,null,null
38,BG,null,null
39,0.5,null,null
40,GS,null,null
41,0.5,null,null
42,GS,null,null
43,BS,null,null
44,BS,null,null
45,0.4,null,null
46,GB,null,null
47,0.4,null,null
48,GB,null,null
49,SB,null,null
50,SB,null,null
51,0.3,null,null
52,0.3,null,null
53,0.3,null,null
54,0.4,null,null
55,0.5,null,null
56,0.6,null,null
57,0.7,null,null
58,0.8,null,null
59,0.9,null,null
60,1,null,null
61,0.3,null,null
62,0.4,null,null
63,0.5,null,null
64,0.6,null,null
65,0.7,null,null
66,0.8,null,null
67,0.9,null,null
68,1,null,null
69,Simulated NDCG Bound,null,null
70,Simulated NDCG Bound,null,null
71,"Figure 1: Applying C ENT model (left) and C MSR model (right) to TREC enterprise data. X-axis is the simulated nDCG upper bound, while Y-axis is the actual nDCG assessor disagreement measured between 2 TREC assessors; pairs of assessor type (""Gold-Silver"" as GS) are indicated by colors.",null,null
72,"topic was judged by three assessors: a gold assessor G (expert on task and topic), a silver assessor S (expert at task but not at topic), and a bronze assessor B (expert at neither). Each G, S, and B set of assessments can take on the role of training or testing assessor, as described above, giving rise to six possible combinations: GS, GB, SG, SB, BG, BS. For each such combination, such as GS, the optimal ranked list can be computed with respect to G and evaluated with respect to S, resulting in a real suboptimal nDCG. The GS confusion matrix can also be computed from the data given and the simulation described above performed, yielding an estimated limiting nDCG. These actual and estimated limiting nDCG values can then be compared.",null,null
73,"Using the TREC Enterprise data, Figure 1 compares the estimated limiting nDCG obtained through simulation with a confusion matrix (x-axis) with the real suboptimal nDCG (y-axis) obtained from different assessors. The left plot uses a confusion matrix CENT obtained from the TREC Enterprise data itself, as described above, while the right plot uses a confusion matrix CMSR obtained from a user study conducted by Microsoft Research [3]. Note that the more accurate confusion matrix yields better simulated results, as expected, and that the confusion matrix alone can be used to accurately estimate limiting nDCG values in most cases.",null,null
74,"Given that a confusion matrix alone can be used, via simulation, to estimate limiting nDCG performance, we next consider other data sets and their associated real or estimated confusion matrices. Yandex [5] conducted user studies to obtain confusion matrices specific to Russian search (CY anR) and to Ukrainian search (CY anU ), and these were shown to improve learning-to-rank performance if the learner was given such models as input. Table 1 presents the estimated limiting nDCG values when applying three different confusion matrices to two learning-to-rank data sets. For comparison, the last column in the table presents the actual best known performance of a learning algorithm on these data sets. Consider the difference between the estimated limiting nDCG bounds (middle three columns) and known ranking performance (last column): If CMSR is a good model of assessor disagreement for these data sets, then the known learning-to-rank performance is reasonably close to the limiting bound, and little improvement is possible. On the other hand, if CYanU is a better model of inter-assessor disagreement, then learning algorithms have room for improvement.",null,null
75,3. CLOSED FORM APPROXIMATION,null,null
76,"Let L ,"" {0, 1, 2, 3, 4} be the set of relevance grades, nk the number of documents with reference label k  L, n the total""",null,null
77,Collection MSLR30K(SIM) MSLR30K(CFA),null,null
78,Yahoo(SIM) Yahoo(CFA),null,null
79,C MSR 0.780 0.794 0.861 0.887,null,null
80,C YanR 0.867 0.869 0.920 0.919,null,null
81,C YanU 0.900 0.898 0.944 0.938,null,null
82,LearnToRank 0.741,null,null
83,0.801,null,null
84,Table 1: nDCG upper bounds derived from disagreement models C applied to popular learning-torank data sets. SIM rows are simulated values; the CFA rows are closed form approx. Last column is best known learning-to-rank performance.,null,null
85,"number of documents in the rank-list, and Prank(i, r) the probability that the rank of a given document with reference label i is r, as ordered by the alternative labels j. One can then show that the expected nDCG as measured by reference labels is",null,null
86,E [nDC G],null,null
87,",",null,null
88,h,null,null
89,I,null,null
90,1 dealDC,null,null
91,G,null,null
92,·PiL,null,null
93,ni,null,null
94,·,null,null
95,gain(i),null,null
96,·,null,null
97,Pn,null,null
98,"r,1",null,null
99,"i Prank (i,r)",null,null
100,discount(r),null,null
101,where,null,null
102,"Prank(i, r)",null,null
103,",",null,null
104,P,null,null
105,jL,null,null
106,h cij,null,null
107,·,null,null
108,"Pr-1
h=0

Pn-h-1
s=r-1-h

ij (h,s) i",null,null
109,s+1,null,null
110,"and ij(h, s) is the probability that other s documents have",null,null
111,"the same alternative label j, and other h documents have",null,null
112,"alternative label higher than j, given that a particular doc-",null,null
113,ument with reference label i has alternative label j. Com-,null,null
114,"puting ij(h, s) straightforwardly is inefficient for even moderately long rank-lists, with a running time of O(n2|L|).",null,null
115,We instead employ a closed form approximation (CFA),null,null
116,"based on approximating , a sum-product of binomial con-",null,null
117,"ditional distributions, with a Gaussian joint distribution of",null,null
118,"two variables (h + s, s). This approximation becomes more",null,null
119,accurate as rank-lists get longer. For a fixed i and j we have,null,null
120,`h+s´,null,null
121,s,null,null
122,"N ij (µ,",null,null
123,"),",null,null
124,µ,null,null
125,",",null,null
126,`µh+s,null,null
127,µs,null,null
128,"´,",null,null
129,and,null,null
130,",",null,null
131,"` h2+s cov h+s,s",null,null
132,"cov s,h+s s2",null,null
133,´,null,null
134,where,null,null
135,(1),null,null
136,aij,null,null
137,",",null,null
138,P,null,null
139,kj,null,null
140,"cik ,",null,null
141,(2),null,null
142,µh+s,null,null
143,",",null,null
144,-aij,null,null
145,+,null,null
146,P,null,null
147,kL,null,null
148,nk,null,null
149,·,null,null
150,"akj ,",null,null
151,(3),null,null
152,µs,null,null
153,",",null,null
154,-cij,null,null
155,+,null,null
156,P,null,null
157,kL,null,null
158,nk,null,null
159,·,null,null
160,"ckj ,",null,null
161,and,null,null
162,h2+s,null,null
163,",",null,null
164,-aij,null,null
165,·,null,null
166,(1,null,null
167,-,null,null
168,aij ),null,null
169,+,null,null
170,P,null,null
171,kL,null,null
172,nk,null,null
173,·,null,null
174,akj,null,null
175,·,null,null
176,(1,null,null
177,-,null,null
178,akj ),null,null
179,s2,null,null
180,",",null,null
181,-cij,null,null
182,·,null,null
183,(1,null,null
184,-,null,null
185,cij ),null,null
186,+,null,null
187,P,null,null
188,kL,null,null
189,nk,null,null
190,·,null,null
191,ckj,null,null
192,·,null,null
193,(1,null,null
194,-,null,null
195,ckj ),null,null
196,"cov s,h+s",null,null
197,",",null,null
198,-cij,null,null
199,·,null,null
200,(1,null,null
201,-,null,null
202,aij ),null,null
203,+,null,null
204,P,null,null
205,kL,null,null
206,nk,null,null
207,·,null,null
208,ckj,null,null
209,·,null,null
210,(1,null,null
211,-,null,null
212,akj ).,null,null
213,"We can approximate E[nDCG] in O(n2) time given that the ""spread"" of the Gaussian grows as O( n) per component. The CFA rows of Table 1 show closed form approximations for comparison with simulated nDCG upper bounds.",null,null
214,4. CONCLUSION,null,null
215,We present a simple probabilistic model of assessor disagreement and results which indicate that the performance of learning-to-rank algorithms may be approaching inherent limits imposed by such disagreement.,null,null
216,5. REFERENCES,null,null
217,"[1] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. P.",null,null
218,"de Vries, and E. Yilmaz. Relevance assessment: Are",null,null
219,"judges exchangeable and does it matter? SIGIR, 2008.",null,null
220,"[2] P. Bailey, A. P. De Vries, N. Craswell, and I. Soboroff. Overview of the TREC-2007 Enterprise Track.",null,null
221,"[3] B. Carterette, P. N. Bennett, D. M. Chickering, and S. T. Dumais. Here or there: Preference judgments for relevance. In ECIR, 2008.",null,null
222,"[4] B. Carterette and I. Soboroff. The effect of assessor error on ir system evaluation. In SIGIR, 2010.",null,null
223,"[5] A. Gulin, I. Kuralenok, and D. Pavlov. Winning the transfer learning track of Yahoo!'s learning to rank challenge with YetiRank. Journal of Machine Learning Research, 2011.",null,null
224,1092,null,null
225,,null,null

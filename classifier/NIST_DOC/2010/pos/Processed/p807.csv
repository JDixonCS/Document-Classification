,sentence,label,data
0,Learning to Rank Query Reformulations,null,null
1,"Van Dang, Michael Bendersky and W. Bruce Croft",null,null
2,"Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts Amherst, MA 01003",null,null
3,"{vdang, bemike, croft}@cs.umass.edu",null,null
4,ABSTRACT,null,null
5,"Query reformulation techniques based on query logs have recently proven to be effective for web queries. However, when initial queries have reasonably good quality, these techniques are often not reliable enough to identify the helpful reformulations among the suggested queries. In this paper, we show that we can use as few as two features to rerank a list of reformulated queries, or expanded queries to be specific, generated by a log-based query reformulation technique. Our results across five TREC collections suggest that there are consistently more useful reformulations in the first positions in the new ranked list than there were initially, which leads to statistically significant improvements in retrieval effectiveness.",null,null
6,Categories and Subject Descriptors,null,null
7,H.3.3 [Information Search and Retrieval]: Query Formulation,null,null
8,General Terms,null,null
9,"Algorithms, Measurement, Performance, Experimentation.",null,null
10,Keywords,null,null
11,"Query reformulation, query expansion, query log, query performance predictor, learning to rank.",null,null
12,1. INTRODUCTION,null,null
13,"Query logs have become an important resource for many tasks including query reformulation [3, 6]. Most log-based reformulation techniques, however, are evaluated using nonstandard approaches and proprietary query logs, making it hard to compare one to another. A more recent study [2] compares different techniques using TREC collections and finds that when intial queries have relatively high quality, query expansion is much more reliable than substitution.",null,null
14,"Although the log-based expansion technique [2] can generate some good reformulations for high-quality TREC queries, it also produces many bad reformulations and it does not generate a reliable ranking of the reformulations by quality.",null,null
15,"In this paper, we show that we can effectively rerank the list of reformulated queries obtained with this log-based expansion approach. By using as few as two features, SCQ",null,null
16,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
17,"(Similarity Collection Query) [8] and query clarity [1], we can substantially improve the ranking of reformulated queries in terms of the quality of the reformulations in the top two ranks (measured by NDCG@2 ), which then leads to significant improvements in retrieval effectiveness.",null,null
18,2. METHOD,null,null
19,2.1 Log-based Query Expansion,null,null
20,"The log-based query expansion method [2] (referred to as LQE) is a slight modification of the query substitution method proposed by Wang and Zhai [6]. It first estimates a context distribution for terms occuring in a query log. It then constructs a translation model that can suggest similar words based on their distributional similarity. Given any query, the expansion model will try to expand it with candidates suggested by the translation model for each query term. The model decides whether to expand the query based on how similar the candidate is to the query term and how appropriate it is to the context of the query. For more details, see [2].",null,null
21,2.2 The Reranking Approach,null,null
22,"Query quality predictors aim to predict a query's quality without explicit relevance judgements. Thus, given a ranked list of reformulated queries, it is intuitive to think about reorganizing this list based on the ""quality"" score given by some predictor.",null,null
23,We tried some of the top-performing predictors that Kumaran and Carvalho [4] used in a similar task and found that,null,null
24,"[8] and clarity score [1] are the most effective for our problem. Therefore, we rerank the list of expanded queries by",null,null
25,"( ), 1×",null,null
26,( )+ 2×,null,null
27,(),null,null
28,where 1 and 2 are weight of the two predictors.,null,null
29,Table 1: Statistics of queries used for reformulation,null,null
30,AP WSJ Robust-04 WT10G Gov-2,null,null
31,Title Q. 133 133,null,null
32,200,null,null
33,66,null,null
34,119,null,null
35,Desc. Q. 150 150,null,null
36,246,null,null
37,94,null,null
38,134,null,null
39,807,null,null
40,Table 3: Evaluation of retrieval effectiveness in terms of MAP.  and  indicate significant difference to the,null,null
41,original query and LQE's ranked list respectively. Best result in each column is marked in bold.,null,null
42,Title Query,null,null
43,Description Query,null,null
44,AP,null,null
45,WSJ,null,null
46,RBT-04 WT10G Gov-2,null,null
47,AP,null,null
48,WSJ RBT-04 WT10G Gov-2,null,null
49,Orig-Q 0.1694 0.2594,null,null
50,0.2247,null,null
51,0.1904 0.2829,null,null
52,0.1660 0.2358 0.2519 0.1770 0.2518,null,null
53,LQE,null,null
54,0.1741 0.2563,null,null
55,0.2297,null,null
56,0.1911 0.2559,null,null
57,0.1694 0.2391 0.2538,null,null
58,0.1775,null,null
59,0.2497,null,null
60,Rerank 0.1749 0.2663 0.2382 0.1962 0.2901 0.1820 0.2374 0.2584 0.1836 0.2579,null,null
61,"Table 2: Our approach (""Rerank"") consistently out-",null,null
62,performs LQE in NDCG@2. All differences are signif-,null,null
63,icant at < 0.05,null,null
64,Collection,null,null
65,Title Query,null,null
66,Desc. Query,null,null
67,LQE Rerank LQE Rerank,null,null
68,AP,null,null
69,0.2434 0.4805 0.2307 0.3728,null,null
70,WSJ,null,null
71,0.2318 0.5040 0.2250 0.3296,null,null
72,Robust-04 0.2905 0.5559 0.2138 0.3687,null,null
73,WT10G 0.2673 0.5499 0.1680 0.3847,null,null
74,Gov-2 0.1933 0.5830 0.2059 0.4093,null,null
75,3. EVALUATION,null,null
76,3.1 Experiment Settings,null,null
77,"In this section, we evaluate the performance of our reranking technique. Evaluation is done on five TREC collections: AP, WSJ, Robust-04, WT10G and Gov-2, with both title and description queries. We use the language modeling framework and remove all stop words at indexing time. We adopt the parameter settings for LQE from the authors [2].",null,null
78,"Due to the limited coverage of the available query log [5], we use only a subset of TREC queries where the LQE can generate at least one reformulation. Information about these subsets is given in Table 1.",null,null
79,"On each collection, we first use LQE to generate a list of expanded queries ( ,"" 30) for each original query. We append to this list the original query - in the case when all generated reformulations are bad, the reranking approach has a chance to choose not to reformulate. We then use our approach to rerank this list and compare its performance with that of the intial list as well as original query.""",null,null
80,3.2 Training Data,null,null
81,"We run LQE with the MSN log to obtain a list of reformulations for each original query. We use all these queries to do retrieval and record their MAP and use them to create our dataset. Training and testing are done using 5-fold cross validation on this dataset. 1 and 2 are learned using AdaRank [7] to maximize the average NDCG@2. The algorithm ends up choosing either ( 1 ,"" 1, 2 "", 0) or ( 1 ,"" 0, 2 "", 1) depending on the collection.",null,null
82,3.3 Reranking Effectiveness,null,null
83,"We use NDCG@2 to measure the quality of the ranked list of reformulations given by our approach. Reformulations are graded on a scale from zero to four with respect to the improvement they provide over the original query. In particular, improvement larger than 0.03 corresponds to a 4, or ( > 0.03)  4. Similarly, (0.01 <  0.03)  3, (0 <  0.01)  2, ( , 0)  1 and ( < 0)  0.",null,null
84,Table 2 summarizes the result: the list of reformulations ranked by our approach has a much higher average NDCG@2,null,null
85,than the initial list. All improvements are statistically significant at < 0.05 using a two-tailed t-test.,null,null
86,3.4 Retrieval Effectiveness,null,null
87,"We define the MAP of a ranked list of reformulations as the best MAP observed among its top queries. In this section, we compare the MAP obtained by (i) the original query, (ii) the list of reformulations generated by LQE, and (iii) the list reranked by our method.",null,null
88,"As can be seen in Table 3, the best of the top two reformulated queries ranked by our approach is almost always significantly better than the original query. This is not the case in LQE. In many cases, our method also provides significant improvements over LQE. This result suggests that the reranking can push better reformulations to the first two positions in the ranked list.",null,null
89,4. CONCLUSIONS,null,null
90,"In this paper, we have shown that by reranking the list of reformulations generated by the log-based query expansion technique [2] with only two features, we can push more good reformulations into the first two positions in the list. This is reflected in the huge gain of NDCG@2 and statistically significant improvement in retrieval effectiveness. In the future, we will investigate more features. We hope this will lead to greater improvement in NDCG@1, helping retrieval systems to reformulate queries implicitly without user involvement.",null,null
91,5. ACKNOWLEDGMENTS,null,null
92,"This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by ARRA NSF IIS-9014442. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",null,null
93,6. REFERENCES,null,null
94,"[1] S. Cronen-Townsend, Y. Zhou, and W.B. Croft. Predicting Query Performance. In Proc. of SIGIR, pages 299-306, 2002.",null,null
95,"[2] V. Dang and W.B. Croft. Query Reformulation Using Anchor Text. In Proc. of WSDM, pages 41-50, 2010.",null,null
96,"[3] R. Jones, B. Rey and O. Madani. Generating Query Substitutions. In Proc. of WWW, pages 387-396, 2006.",null,null
97,"[4] G. Kumaran and V.R. Carvalho. Reducing Long Queries Using Query Quality Predictors. In Proc. of SIGIR, pages 564-571, 2009.",null,null
98,"[5] Proc. of the 2009 workshop on Web Search Click Data, Barcelona, Spain. ACM New York, NY, USA, 2009.",null,null
99,"[6] X. Wang and C. Zhai. Mining Term Association Patterns from Search Logs for Effective Query Reformulation. In Proc. of CIKM, pages 479-488, 2008.",null,null
100,"[7] J. Xu and H. Li. AdaRank: A Boosting Algorithm for Information Retrieval. In Proc. of SIGIR, pages 391-398, 2007.",null,null
101,"[8] Y. Zhao, F. Scholer, and Y. Tsegay. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proc. of ECIR, pages 52-64, 2008.",null,null
102,808,null,null
103,,null,null

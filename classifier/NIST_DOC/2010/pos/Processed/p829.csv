,sentence,label,data
0,Query Term Ranking based on Dependency Parsing of Verbose Queries,null,null
1,Jae-Hyun Park and W. Bruce Croft,null,null
2,Center for Intelligent Information Retrieval Department of Computer Science,null,null
3,"University of Massachusetts, Amherst, MA, 01003, USA",null,null
4,"{jhpark,croft}@cs.umass.edu",null,null
5,ABSTRACT,null,null
6,"Query term ranking approaches are used to select effective terms from a verbose query by ranking terms. Features used for query term ranking and selection in previous work do not consider grammatical relationships between terms. To address this issue, we use syntactic features extracted from dependency parsing results of verbose queries. We also modify the method for measuring the effectiveness of query terms for query term ranking.",null,null
7,Categories and Subject Descriptors,null,null
8,H.3.3 [Information Search and Retrieval]: Query formulation,null,null
9,General Terms,null,null
10,"Algorithm, Experimentation, Performance",null,null
11,Keywords,null,null
12,"Dependency Parse, Query Reformulation, Query Term Ranking",null,null
13,1. INTRODUCTION,null,null
14,"Most search engines have a tendency to show better retrieval results with keyword queries than with verbose queries. Verbose queries tend to contain more redundant terms and these terms have grammatical meaning for communication between humans to help identify the important concepts.. Search engines do not typically use syntactic information.. For example, given a verbose query, ""Identify positive accomplishments of the Hubble telescope since it was launched ..."", search engines cannot recognize that ""Hubble telescope"" is the key concept of the query whereas ""accomplishments"" should be considered as a complementary concept, while people can readily identify this by analyzing the grammatical structure of the query. Therefore, search engines potentially need a method for exploiting this structure.",null,null
15,"In this work, we rank terms in a verbose query and reformulate a new query using selected highly ranked terms. Good selection methods should be able to leverage the grammatical roles of terms within a query. To do this, we use syntactic features extracted from dependency parsing trees of queries. In addition, we suggest a new method for measuring the effectiveness of terms for query term ranking.",null,null
16,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
17,2. QUERY TERM RANKING,null,null
18,2.1 Features extracted from Dependency Parsing,null,null
19,"We use syntactic features extracted from dependency parsing to capture the grammatical properties of terms for a query. Features used by previous work in query term ranking [1, 6] are inadequate to reflect these characteristics. The limitation of these features is that they are based on individual terms. Features such as tf, idf, part-of-speech (PoS) tag, etc. will not change even if the role of the term changes according to the syntactic structure of queries. Even features for sub-queries [5] are also unlikely to reflect grammatical characteristics because they are not affected by the structure of queries.",null,null
20,"Therefore, we propose to overcome this limitation by using dependency parsing trees. A typed dependency parse labels dependencies with grammatical relations [3]. Figure 1 shows an example of a typed dependency parse tree. Dependency parsing tree fragments of terms can provide grammatical information about terms in queries [2].",null,null
21,"It is infeasible to use all dependency parse tree fragments as syntactic features. We limit the number of arcs in syntactic features to two arcs. Even if we limit the number of arcs, some of collected tree fragments are too specific to",null,null
22,Sentence: Identify positive accomplishments of the Hubble telescope since it was launched in 1991.,null,null
23,Identify,null,null
24,dobj,null,null
25,amod accomplishments prep_of,null,null
26,positive,null,null
27,telescope nn,null,null
28,Hubble,null,null
29,Figure 1: An example of dependency parsing trees. Labels attached to arcs are types of dependencies.,null,null
30,Identify,null,null
31,dobj,null,null
32,*,null,null
33,dobj,null,null
34,Identify,null,null
35,*,null,null
36,accomplishments accomplishments,null,null
37,(a),null,null
38,(b),null,null
39,accomplishments (c),null,null
40,"Figure 2: Three types of syntactic features for the term ""accomplishments"". (a) An original syntactic feature (b) The word is generalized to a * (c) The type of the dependency is generalized to a *",null,null
41,829,null,null
42,"have a reliable amount of training data and not all of them are useful. We generalize syntactic features which consist of arcs labeled with dependency types and nodes representing words which are dependent. Figure 2 shows an example of an original syntactic feature and its generalized features. In the figure, ""*"" means any word or any type of dependency.",null,null
43,2.2 Query Term Ranking,null,null
44,"Our approach aims to rank terms in a query and to reformulate the query using the ranking. To build training data for a ranking model, Bendersky and Croft [1] manually annotate the concept from each query that had the most impact on effectiveness. For given terms ,"" { 1, 2, ..., }, they used labeled instances ( , ), where is a binary label, as training data. However, queries can have more than one effective term or concept. In addition, it is difficult for annotators to judge the effectiveness of a term. Therefore, we estimate the effectiveness of terms, i.e., the labels for training data, by evaluating the search results of terms in training data. By using these estimated scores, we expect that a ranking model can take account of all terms in a query and consider how effective they are.""",null,null
45,"Lee et al. [6] point out the importance of underlying correlations between terms. Previous work has evaluated the effectiveness of groups of terms instead of individual terms to capture these relationships [5, 6]. The problem is that the number of unique groups will grow exponentially with the size of the term groups and it will cause a data sparseness problem. We used the following equation for ( ), the effectiveness of a term to reflect the effects of relationships between terms in training labels.",null,null
46,"( ),",null,null
47,1,null,null
48, (,null,null
49,"(,",null,null
50,)-,null,null
51,"( )),",null,null
52,(1),null,null
53,where is all possible combinations of m terms except . N is the number of elements in and ( ) is the search performance of . Eq. (1) estimates the effectiveness of term,null,null
54,"through aggregating the impacts of term on effectiveness when using it with other terms in . Thus, the scores of Eq. (1) reflects the correlations between and other terms.",null,null
55,3. EXPERIMENTS AND ANALYSIS,null,null
56,"We evaluated our proposed method using two TREC collections: Robust 2004 (topic numbers are 301-450 and 601700) and Wt10g (topic numbers are 450-550). The average number of nouns, adjectives and verbs in queries of Robust2004 and Wt10g are 8.7 and 6.5 per a query, respectively. We used the language model framework with Dirichlet smoothing ( set to 1,500). Indexing and retrieval were conducted using the Indri toolkit.",null,null
57,"To rank query terms, we used RankSVM [4]. We trained query term ranking models for each query using leave-oneout cross-validation in which one query was used for test data and the others were used for training data. We labeled training data based on Key concepts [1] and the effectiveness measured by Eq. 1 in which we chose nDCG as the performance measure. We used syntactic features in addition to tf, idf, and PoS tag features.",null,null
58,"When we combined selected terms with original queries, we used two approaches. First, we assigned uniform weights to selected terms (binary). Alternatively, we used query term ranking scores as the weight for selected terms (weight).",null,null
59,Table 1: Mean Average Precision (MAP) of Ro-,null,null
60,"bust04 and Wt10g collections, Key-Concept: using",null,null
61,"key concept [1] as labels of training data, Auto: us-",null,null
62,ing effectiveness in retrieval as labels of training data,null,null
63,Robust04 Wt10g,null,null
64,<title>,null,null
65,25.17,null,null
66,18.55,null,null
67,<desc>,null,null
68,24.07,null,null
69,17.52,null,null
70,Key-Concept,null,null
71,binary weight,null,null
72,23.98 24.24,null,null
73,18.55 19.45,null,null
74,Auto,null,null
75,binary weight,null,null
76,25.40 26.21,null,null
77,17.91 19.15,null,null
78,"Experimental results in Table 1 shows that selected terms by using query term ranking have better performance than description queries except for one result in which we used key concepts and uniform weighting. In this case, only the most important concepts in queries are labeled, whereas the effectiveness in retrieval is measured for all terms in queries. This difference makes the method using the effectiveness of terms (Auto) superior for the relatively longer queries in Robust2004, and the method using key concepts (Key Concept) better for the shorter queries in Wt10g.",null,null
79,4. CONCLUSIONS,null,null
80,"In this paper, we propose a query term ranking method that uses syntactic features extracted from dependency parsing trees. By using syntactic features, we can take into account grammatical relationships between terms. We also modify the query term ranking method to measure the effectiveness of terms based on combinations of terms. Experimental results showed that the terms selected by the query term ranking method improved retrieval performance.",null,null
81,5. ACKNOWLEDGMENTS,null,null
82,"This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0711348. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",null,null
83,6. REFERENCES,null,null
84,"[1] M. Bendersky and W. B. Croft. Discovering key concepts in verbose queries. In Proc. ACM SIGIR, pages 491­498, 2008.",null,null
85,"[2] A. Chanen. A comparison of human and computationally generated document features for automatic text classification. PhD thesis, The University of Sydney, 2009.",null,null
86,"[3] M. De Marneffe, B. MacCartney, and C. Manning. Generating typed dependency parses from phrase structure parses. In Proc. LREC 2006, 2006.",null,null
87,"[4] T. Joachims. Optimizing Search Engines Using Clickthrough Data. In Proc. ACM KDD, pages 133­142, 2002.",null,null
88,"[5] G. Kumaran and V. Carvalho. Reducing long queries using query quality predictors. In Proc. ACM SIGIR, pages 564­571, 2009.",null,null
89,"[6] C. Lee, R. Chen, S. Kao, and P. Cheng. A term dependency-based approach for query terms ranking. In Proc. CIKM, pages 1267­1276, 2009.",null,null
90,830,null,null
91,,null,null

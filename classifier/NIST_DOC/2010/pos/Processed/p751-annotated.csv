,sentence,label,data
,,,
0,Semi-Supervised Spam Filtering using Aggressive Consistency Learning,null,null
,,,
1,Mona Mojdeh and Gordon V. Cormack,null,null
,,,
2,Cheriton School of Computer Science University of Waterloo,null,null
,,,
3,"Waterloo, Ontario, Canada",null,null
,,,
4,"{mmojdeh,gvcormac}@uwaterloo.ca",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"A graph based semi-supervised method for email spam filtering, based on the local and global consistency method, yields low error rates with very few labeled examples. The motivating application of this method is spam filters with access to very few labeled message. For example, during the initial deployment of a spam filter, only a handful of labeled examples are available but unlabeled examples are plentiful. We demonstrate the performance of our approach on TREC 2007 and CEAS 2008 email corpora. Our results compare favorably with the best-known methods, using as few as just two labeled examples: one spam and one non-spam.",Y,TREC 2007
,,,
7,Categories and Subject Descriptors,null,null
,,,
8,H.3.3 [Information Search and Retrieval]: Information Filtering,null,null
,,,
9,General Terms,null,null
,,,
10,"Experimentation, Measurement",null,null
,,,
11,Keywords,null,null
,,,
12,"Spam, Email, Filtering, Classification",null,null
,,,
13,1. INTRODUCTION,null,null
,,,
14,"Semi-supervised methods are of special interest when there are very few training samples available. In many machine learning applications, there is always great human effort involved in labeling samples, while obtaining unlabeled data is fairly simple. This is the case for spam filters. During the initial deployment of spam filters, a normal user may be willing to provide only a few labeled examples for training but will still expect correct classification of a large number of emails. Another application is personalized spam filtering with low label cost, using per-user semi-supervised filters with few labeled examples to augment a global filter.",null,null
,,,
15,In this paper we address the problem of email spam filtering with very few correct training samples using graph based semi-supervised learning methods. Previous semisupervised methods such as Transductive SVM and Logistic Regression and Dynamic Markov Compression with self training for spam filtering have yielded mixed results [4]. In this paper we are focused on the special situation in which,null,null
,,,
16,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
17,the first handful of messages are labeled and used to filter the rest.,null,null
,,,
18,We present an aggressive graph-based iterative solution modeled after the local and global consistency learning method of Zhou et al. [5]. The same method is applied for detecting web spam in [3]. Local consistency guarantees that the nearby points are likely to have the same label; while the global consistency guarantees that the points on the same structure are likely to have the same label. We have also applied Single Value Decomposition to find the most informative terms. Our experiments show a comparatively high performance of our method in the presence of very few training samples.,null,null
,,,
19,2. AGGRESSIVE CONSISTENCY LEARNING METHOD,null,null
,,,
20,"Given a sequence of n email messages and labels denoting the true class ­ spam or nonspam ­ of each of the first nlabeled n, we consider the problem of finding the class of the remaining n - nlabeled messages. Algorithm 1 demonstrates the details of our method. The input matrix Xnm represents the feature vector of the messages; n is number of messages and m is the number of terms, and Yn1 is the labels of messages; {yi  {-1, 1} for i  nlabeled and yi ,"" 0 for i > nlabeled}. The output of the algorithm is {yi  {-1, 1} for i > nlabeled}.""",null,null
,,,
21,"The n × n symmetric Gaussian affinity matrix A captures the similarity between each pair of messages xi and xj, where xi - xj 2 is the Euclidian distance between messages xi and xj. A is then normalized by constructing L ,"" D-1/2AD-1/2 [5]. The   (0, 1) parameter in line 4 of the algorithm, determines the relative amount of information that each node in the graph receives from its neighbors. It is worth mentioning that self-reinforcement is avoided since the diagonal elements of the affinity matrix are set to zero in the first step.""",null,null
,,,
22,"The main contribution of this algorithm is the aggressive approach in updating the affinity matrix. A large number of elements in the affinity matrix are approximately zero due to the large Euclidean distances between messages meaning that the messages do not share many terms. In our aggressive definition of affinity matrix, for all zero rows or columns in equation (1), a ""1"" (equivalently, a link in the graph) is inserted where the distance between the two corresponding messages is minimum in that column or row. Although adding a link in this case may seem too ""aggressive"", the simulation results show the improved performance.",null,null
,,,
23,"Moreover, in order to better handle the sparsity of the",null,null
,,,
24,751,null,null
,,,
25,Algorithm 1 Aggressive Consistency Learning Method (ACLM),null,null
,,,
26,"Input: X, Y , ,  1: Compute Affinity matrix",null,null
,,,
27,"Aij ,",null,null
,,,
28,e-,null,null
,,,
29,xi -xj 22,null,null
,,,
30,2,null,null
,,,
31,"for i , j",null,null
,,,
32,-1,null,null
,,,
33,0,null,null
,,,
34,"for i ,"" j,""",null,null
,,,
35,"2: For all j such that i Ai,j  0 : Arj , 1 where r , arg minj xr - xj",null,null
,,,
36,"3: Compute L , D-1/2AD-1/2 where",null,null
,,,
37,n,null,null
,,,
38,"Dii , Aij .",null,null
,,,
39,-2,null,null
,,,
40,"j,1",null,null
,,,
41,"4: Y , (1 - )(I - L)-1L",null,null
,,,
42,"affinity matrix A, we also propose to reduce the dimensionality of matrix X. By applying Singular Value Decomposition (SVD)[2] on matrix X; we find the most informative terms in X and replace X with its approximate. In other words, X ,"" U  V -1, we only keep the rank highest singular values of X; so {i,i "", 0  i > rank}.",null,null
,,,
43,3. EXPERIMENTS AND RESULTS,null,null
,,,
44,"We compare the effectiveness of ACLM with the supervised and transductive modes of SV M light [1] (denoted SVM and TSVM). We have compared these methods on two email corpora, TREC 2007 Public Corpus 1 and CEAS 2008 Public Corpus 2. From each corpus we have selected the first 10, 000 from which the first 1000 were used for tuning purposes to figure out the three main parameters , , and rank.",Y,TREC 2007
,,,
45,"For the actual experiment, we divided the remaining 9000 messages into batches of 1000, getting 9 batches. For each batch we used the first 100 messages to select a balanced training set (same number of spam and non-spam) and the remaining 900 messages as the test set. We report mean error rate, as average over all batches.",null,null
,,,
46,"Each message was abstracted as a binary feature vector representing word occurrences within the whole email, including headers. We removed terms with document frequency of less than 5 in the training and test sets combined. Binary term frequency was then used for the terms. Raw term frequency was also investigated, but did not provide better results than binary weights.",null,null
,,,
47,"For parameters of SVM and TSVM, several values were adjusted but no improvement over their default values was observed. The p parameter in TSVM, representing the proportion of spam messages to be expected, was tuned using our tuning set of emails.",null,null
,,,
48,"Fig. 1 shows the results of the methods on CEAS08 and TREC07 corpora. ACLM with SVD gives best performance of all methods between 4 and 32 labeled examples, mostly having less than 0.01 error rate. TSVM only performs best with fewer than 4 examples. We have previously seen similar results in [4] where TSVM was performing better than SVM only when the train and test sets were from two completely different sources. SVM does not give best performance on CEAS08 even with 30 labeled examples.",null,null
,,,
49,1trec.nist.gov/data/spam.html 2www.ceas.cc/challenge,null,null
,,,
50,"Figure 1: Error rate for ACLM (with SVD and without), SVM, TSVM on CEAS08 (up) and TREC07 (bottom) corpora",null,null
,,,
51,0.35,null,null
,,,
52,ACLM no SVD,null,null
,,,
53,0.3,null,null
,,,
54,ACLM with SVD,null,null
,,,
55,SVM 0.25,null,null
,,,
56,TSVM,null,null
,,,
57,0.2,null,null
,,,
58,Error Rate,null,null
,,,
59,0.15,null,null
,,,
60,0.1,null,null
,,,
61,0.05,null,null
,,,
62,0,null,null
,,,
63,2,null,null
,,,
64,4,null,null
,,,
65,8,null,null
,,,
66,16,null,null
,,,
67,32,null,null
,,,
68,Number of Labeled Examples (log scale),null,null
,,,
69,0.3 ACLM no SVD,null,null
,,,
70,ACLM with SVD 0.25,null,null
,,,
71,SVM,null,null
,,,
72,TSVM 0.2,null,null
,,,
73,Error Rate,null,null
,,,
74,0.15,null,null
,,,
75,0.1,null,null
,,,
76,0.05 2,null,null
,,,
77,4,null,null
,,,
78,8,null,null
,,,
79,16,null,null
,,,
80,32,null,null
,,,
81,Number of Labeled Examples (log scale),null,null
,,,
82,4. REFERENCES,null,null
,,,
83,[1] SVM Light. http://svmlight.joachims.org/.,null,null
,,,
84,"[2] O. Alter, P. Brown, and D. Botstein. Singular value decomposition for genome-wide expression data processing and modeling. In Proc Natl Acad Sci, USA, 2000.",null,null
,,,
85,"[3] C. Castillo, D. Donato, V. Murdock, and F. Silvestri. Know your neighbors: Web spam detection using the web topology. In 30st ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007), Netherlands, 2007.",null,null
,,,
86,"[4] M. Mojdeh and G. Cormack. Semi supervised spam filtering: Does it work? In 31st ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2008), Singapore, 2008.",null,null
,,,
87,"[5] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. In Advances in Neural Information Processing Systems 16 (NIPS 2003), pages 321­328. MIT Press.",null,null
,,,
88,752,null,null
,,,
89,,null,null

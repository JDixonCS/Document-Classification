,sentence,label,data
,,,
0,Aspect Presence Verification Conditional on Other,null,null
,,,
1,Aspects,null,null
,,,
2,Dmitri Roussinov,null,null
,,,
3,"University of Strathclyde 16 Richmond Street, Glasgow, UK G1 1XQ",null,null
,,,
4,+44 141 548 3706,null,null
,,,
5,dmitri.roussinov@cis.strath.ac.uk,null,null
,,,
6,ABSTRACT,null,null
,,,
7,"I have shown that the presence of difficult query aspects that are revealed only implicitly (e.g. exploration, opposition, achievements, cooperation, risks) can be improved by taking advantage of the known presence of other, easier to verify query aspects. The approach proceeds by mining a large external corpus and results in substantial improvements in re-ranking the subset of the top retrieved documents.",null,null
,,,
8,Categories and Subject Descriptors,null,null
,,,
9,H.3.3 [Information Search and Retrieval]: Retrieval models; ­ Retrieval Models,null,null
,,,
10,General Terms,null,null
,,,
11,"Algorithms, Experimentation, Theory.",null,null
,,,
12,Keywords,null,null
,,,
13,"Information retrieval, machine learning, external corpus.",null,null
,,,
14,1. INTRODUCTION,null,null
,,,
15,"It has been noticed that a common reason for the search results to be of poor quality is missing one or more aspects of the user information need [1], where an aspect can be represented by a subset of query words. E.g., in the query antarctica exploration, the word antarctica is relatively rare and specific. Along with its same-stem variants, it explicitly occurs in virtually all documents about Antarctica. On the other side, the word exploration, capturing the other aspect of the query, is not only more frequent, but also represents a higher level concept (theme, topic, etc.), often revealing itself only implicitly by such statements like ""... scientists began to collect data..."" The ranking of the retrieved documents by currently state-of-the-art algorithms would be primarily determined by antarctica, mildly affected by the explicit occurrence of the word exploration, and not affected by the implicit presence of the exploration theme. Although techniques aimed at detecting and representing missing aspects have been suggested [2][3] they have not been yet methodologically studied or revealed convincing improvements. I suggest that it was because they were essentially limited to the bag-of-words representations and the query expansion models. On the contrary, this research builds on the prior works studying the prediction of occurrence of given words (concepts) in a given text context (e.g. a sentence) such as [5][7]. Specifically, I proceeded by the exploring the following innovations: 1) Going beyond the bag of words by looking for the indicators of implicit aspect presence among all the sequences of words",null,null
,,,
16,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
17,"(up to a certain length) from the document (e.g. polar station, oil drilling proposed, expedition ships, etc.) rather than relying on a finite number of expansion terms. 2) Considering the problem of aspect verification conditional on the presence of other, often easier to establish aspects. E.g., in a document about Antarctica, station is a good indication of the exploration theme, while not necessary so in unrestricted context. 3) Modeling subsumption of indicators, e.g. if the word station is a part of train station then its connection to the exploration theme is weaker.",null,null
,,,
18,2. FRAMEWORK,null,null
,,,
19,"Here, I considered the simplest, but a common situation with the two aspects in a query, each can be specified by one or more keywords: 1) Ap is already known to be present in the document d and 2) the other aspect Am, explicit representation of which is missing in the document, thus simply referred below as the ""missing"" aspect. While Am is typically a difficult aspect to verify, as I have demonstrated here, the task of estimating the presence of Am conditional on the presence of Ap happens to be easier. As the possible indicators of the aspect implicit presence, I considered all the sequences of words (up to length of 3) in a document. For each indicator i, the algorithm estimated P(Am|i,Ap) , the probability of the occurrence of Am within the proximity (e.g. same sentence) of i, conditional on the occurrence of Ap. In order to do that, a regression (M5P model tree [6]) was trained independently for each topic, using the normalized frequency counts obtained from Microsoft's Bing search engine as independent variables. For example, the high ratio #((station NEAR exploration) AND Antarctica)/#(station AND A ntarctica) would indicate the high probability of occurrence of exploration near the word station in the external corpus, conditional on the presence of Antarctica. The snippets returned by Bing for the query representing both aspects (e.g. antarctica NEAR exploration) served as training examples, thus no parameter tuning was necessary. The advantage of using M5P was learning automatically the reliability thresholds for the frequency counts. The subsumption step of the algorithm discarded all the subsequences (e.g. station) within the word sequences (e.g. train station) for which the reliable estimates of the conditional probabilities were obtained. This resulted in the average of 531 preserved indicators per document, with 90% of the strongest 100 indicators having 2 or more words, some of those presented in Table 2 below. To rank the documents, the sum of all the conditional probability estimates over all the remaining indicators was treated as the total number of ""implied"" occurrences of the missing aspect:",null,null
,,,
20,865,null,null
,,,
21," tf m ,",null,null
,,,
22,"P ( Am | i , Ap ) , and the bm25 formula was",null,null
,,,
23,i d,null,null
,,,
24,applied.,null,null
,,,
25,3. EMPIRICAL EVALUATION,null,null
,,,
26,B1,null,null
,,,
27,B2,null,null
,,,
28,Entire Wikipedi,null,null
,,,
29,Web,null,null
,,,
30,a only,null,null
,,,
31,MAP,null,null
,,,
32,0.39,null,null
,,,
33,0.43,null,null
,,,
34,0.65**,null,null
,,,
35,.53*,null,null
,,,
36,Standard,null,null
,,,
37,0.13,null,null
,,,
38,0.11,null,null
,,,
39,0.16,null,null
,,,
40,0.14,null,null
,,,
41,Deviation,null,null
,,,
42,% change -9% n/a,null,null
,,,
43,51%,null,null
,,,
44,23%,null,null
,,,
45,over B2,null,null
,,,
46,Table 1. The results of the evaluation. Statistically significant differences at the levels of 0.05 and .1 are marked with ** and * accordingly.,null,null
,,,
47,"Since the approaches studied here involved a large number of time-consuming external queries, I built a small but ""clean"" data set that was sensitive enough to evaluate various configurations and the overall verification accuracy of the techniques suggested here based on the top ranked documents retrieved by bm25 ranking function (described as B1 below) using the HARD 2005 TREC topics, which are known to be difficult and often result in missing aspects [1]. I chose only those topics in which 1) It was possible to interpret the title as consisting of missing and present aspects. 2) The present aspect was occurring in at least 90% of the top 20 documents. 3) The missing aspect was not explicitly occurring in more than half of the top 20 documents. This left me with 18 topics, some examples of them listed in Table 2. Only up to 20 top ranked irrelevant and relevant documents were selected, and only those that did not have the missing aspect mentioned explicitly. I also removed (approx. 10% of) the documents assessed by TREC as irrelevant but still having both aspects present in order to reduce the sensitivity of the evaluation to the details supplied only in the narratives of the topics and thus not available to neither the baseline nor the suggested algorithms. The resulting data set was almost ""balanced"" with approximately 15 negative and 10 positive examples on average per each topic. I compared the performance against two strong baselines: B1 was obtained by applying bm25 ranking formula using the topic titles only, with",Y,null
,,,
48,"stemming and pseudo-relevance feedback using Lemur retrieval engine default settings. Since the approach here involves an external corpus, to make a fair comparison, I also involved B2 obtained and optimized the same way as in [4], which was essentially an application of a relevance model with an external corpus (here, using Bing's snippets).",null,null
,,,
49,4. CONCLUSIONS,null,null
,,,
50,"As the results in Table 1 indicate, it was possible to significantly improve verifying the presence of the difficult implicit aspects by the techniques suggested here. Also, no specific topic has been harmed as a result. While the actual impact on a larger set of topics remains to be seen, I have nevertheless been able to handle a very common scenario with the implicit presence of a difficult aspect (e.g. exploration) and the explicit presence of an easier aspect (e.g. Antarctica). Although the dataset involved in experiments here was very small, it still represented the top ranked results from the state of the art techniques, and thus being able to improve them by re-ranking has great practical implications. The ablation studies (not reported here due to the space limitations) confirmed that each of the novelties 1-4 stated in the introduction was crucial. While sending queries to a search portal delayed the processing substantially, in order to achieve the real time speeds, future implementations can make use of the processed large corpus data such as Google's 1T or a faster access to a search engine index.",null,null
,,,
51,5. REFERENCES,null,null
,,,
52,"[1] Buckley, C. Why current IR engines fail. SIGIR 2004. [2] Collins-Thompson, K., Callan, J. Query expansion using",null,null
,,,
53,"random walk models. CIKM 2005. [3] Crabtree, D.W., Andreae, P. Gao, X. Exploiting",null,null
,,,
54,"underrepresented query aspects for automatic query expansion. SIGKDD 2007. [4] Diaz, F. and Metzler, D. Improving the estimation of relevance models using large external corpora. SIGIR 2006. [5] Edmonds, P. Choosing the word most typical in context using a lexical co-occurrence network, ACL 1997. [6] Monz, C. Model Tree Learning for Query Term Weighting in Question Answering. Advances in Information Retrieval, Volume 4425/2007, pp. 589-596. [7] SzeWang F., Roussinov, D., Skillicorn, D.B. Detecting Word Substitutions in Text. IEEE TKDE, 2008.",null,null
,,,
55,Table 2. Top indicators of implicit aspect presence for some topics. The missing aspects are underlined. Bold font highlights the,null,null
,,,
56,indicators that are specific to the present aspect.,null,null
,,,
57,Topic,null,null
,,,
58,Strongest Indicators of Presence,null,null
,,,
59,"Transportation train caught fire, people were killed, caused by sparks, the midst of, described the crash, in spite",null,null
,,,
60,Tunnel,null,null
,,,
61,"of, millions of dollars, quickly as possible, billions of dollars, around the corner, was caused by,",null,null
,,,
62,Disasters,null,null
,,,
63,"turned into a, the worst, rescue, emergency, explosion, coal, killed, fire",null,null
,,,
64,Black Bear,null,null
,,,
65,"were forced to, bear attempted to, fire, the bear was, officials say, reduced, kill, defense,",null,null
,,,
66,Attacks,null,null
,,,
67,"occurred in, officials said, carcass, attract, documented, bear was sighted, dangerous, killed",null,null
,,,
68,"by, ripped, threatening",null,null
,,,
69,Iran Iraq,null,null
,,,
70,"Iran will ship, minister to visit, delegation will visit, positive, visit to Iraq, oil exports, Iranian",null,null
,,,
71,"Cooperation delegation, normalizing, relations, the joint, agreement with Iran, mutual, to bilateral, visit to Iran,",null,null
,,,
72,"accepted an invitation, invitation to visit, normalization, war ended in, the withdrawal of, gesture",null,null
,,,
73,"Journalist Risks journalist was shot, press freedom, murdered, circumstances, in prison, Iraq, Russia, body, killer,",null,null
,,,
74,"gunned down, win, posed, detained",null,null
,,,
75,866,null,null
,,,
76,,null,null

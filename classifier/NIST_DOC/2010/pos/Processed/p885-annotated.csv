,sentence,label,data
,,,
0,Achieving High Accuracy Retrieval using Intra-Document Term Ranking,null,null
,,,
1,Hyun-Wook Woo hwwoo@nlp.korea.ac.kr,null,null
,,,
2,Jung-Tae Lee jtlee@nlp.korea.ac.kr,null,null
,,,
3,Seung-Wook Lee swlee@nlp.korea.ac.kr,null,null
,,,
4,Young-In Song yosong@microsoft.com,null,null
,,,
5,Hae-Chang Rim rim@nlp.korea.ac.kr,null,null
,,,
6," Dept. of Computer & Radio Comms. Engineering, Korea University, Seoul, South Korea  Microsoft Research Asia, Beijing, China",null,null
,,,
7,ABSTRACT,null,null
,,,
8,"Most traditional ranking models roughly score the relevance of a given document by observing simple term statistics, such as the occurrence of query terms within the document or within the collection. Intuitively, the relative importance of query terms with regard to other individual non-query terms in a document can also be exploited to promote the ranks of documents in which the query is dedicated as the main topic. In this paper, we introduce a simple technique named intra-document term ranking, which involves ranking all the terms in a document according to their relative importance within that particular document. We demonstrate that the information regarding the rank positions of given query terms within the intra-document term ranking can be useful for enhancing the precision of top-retrieved results by traditional ranking models. Experiments are conducted on three standard TREC test collections.",Y,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Retrieval models,null,null
,,,
11,General Terms,null,null
,,,
12,"Algorithms, Experimentation",null,null
,,,
13,Keywords,null,null
,,,
14,"inter-document term ranking, precision at top ranks",null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"With the rapid growth of Web document collection sizes in recent years, achieving high precision at the top of the retrieved result has become a major issue for search engine users [1]. In traditional IR ranking models [2, 3], the relevance of a document for a given query is scored primarily based on simple term statistics, such as the number of occurrence of query terms within the document or within the whole document collection. Basically, the more query terms occur in a document, the higher the chance that the document would be relevant to the query and thus would be",null,null
,,,
17,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
18,"provided at the top part of the ranked list. Despite its simplicity, such a scoring method has been effective in many previous document ranking experiments.",null,null
,,,
19,"However, traditional models virtually do not consider the relative importance of query terms compared to other individual non-query terms within a document in ranking. For example, consider the query word ""q"" and the following two equal-length documents at top ranks in which ""q"" occurs twice: ""q q b c d e f "" and ""q q b b b b c"". Traditional ranking models would assume that the weight of ""q"" is roughly the same for both documents. However, this is indeed not true if we examine the relative importance of ""q"" within individual documents. In the first document, ""q"" occurs relatively more than any of the other non-query terms, which implies that the document may be mainly about ""q"". In contrast, in the second document, ""q"" occurs relatively less than the word ""b"", which implies that ""q"" may be a subtopic of that document. Intuitively, given two documents that both contain the same number of query terms, we would like to rank the document in which the query is dedicated as a main topic above the one where the query is regarded as a minor topic.",null,null
,,,
20,"The relative importance of query terms with regard to other terms in a document can be observed by ranking all the terms in the document with some weighting scheme, such as the tf-idf method. We refer to this technique as intra-document term ranking. In this paper, we present two heuristic document ranking methods based on the rank positions of given query terms in intra-document term ranking lists of individual documents. Our hypothesis is two-fold:",null,null
,,,
21,"· First, when query terms are ranked in relatively higher term rank positions than other terms in a given document, it implies that the query is dedicated as the central topic of the document. Thus, there is a higher chance that the document is relevant to the query.",null,null
,,,
22,"· Second, individual query terms having similar term rank positions to each other in a document reflect that they are treated equally. Thus, there is a chance that the query terms have close relationships to each other within the document, which implies higher relevance.",null,null
,,,
23,The succeeding section will elaborate on how we rank documents based on the two hypotheses. We validate our method on three standard TREC test collections.,null,null
,,,
24,885,null,null
,,,
25,2. PROPOSED METHOD,null,null
,,,
26,"To analyze the relative importance of individual terms that occurred in a given document, we rank the terms by their standard tf-idf weights. We use the term frequency normalized with the document length as the tf component, and the logarithm of the inverse document frequency as the idf component. We normalize the ranks in range 0 to 1 so that the term with highest tf-idf weight will be at rank 0 and the term with the lowest weight at rank 1.",null,null
,,,
27,"Given this ranked list of terms within a document, we derive two document ranking heuristics based on the rank positions of query terms within the list. The first heuristic, which corresponds to our first hypothesis, ranks documents according to the average rank position of query terms. Here we basically assume that the higher the rank of the query term within a document, the more likely that the document is relevant. Given a query Q and the term ranking result of a document D, the average rank position of query terms is calculated as follows:",null,null
,,,
28,"R1(Q, D) ,",null,null
,,,
29,"qiQ{1 - rank(qi, D)} |Q|",null,null
,,,
30,-1,null,null
,,,
31,"where qi represents ith query term, |Q| is the size of the query in words, and rank(qi, D) is a function that returns the normalized term rank of qi in D.",null,null
,,,
32,"The second heuristic, which corresponds to our second hypothesis, assumes that the more cohesive the rank positions of query terms (i.e. have similar rank positions to each other) in a document, the higher the probability that the document is relevant. There are a number of ways to measure the cohesiveness of the term ranks. In this paper, we calculate such measure by the maximum rank difference of all pairs of query terms as follows:",null,null
,,,
33,"R2(Q, D) ,"" 1 - { max {dif f (qi, qj , D)}} (2) qi ,qj QD,qi"",qj",null,null
,,,
34,"where dif f (qi, qj , D) is a function that returns the absolute value of the difference in rank positions of qi and qj . For example, if three query terms were ranked at 0, 0.1, and 0.4, the maximum difference would be 0.3.",null,null
,,,
35,3. EXPERIMENTS,null,null
,,,
36,"For test collections, we use the following standard TREC collections: AP88-90 (Associated Press news 1988-90), WT2g (2 gigabyte Web data), and Blogs06 (Web data used in TREC Blog Track 2006-08). For queries, we use the title field of the TREC topics 50-150 for AP88-90, 401-450 for WT2g, and 851-950 and 1001-1050 for Blogs06.",Y,null
,,,
37,"To investigate whether the two new heuristics can infer the true relevance for documents at top ranks of the retrieved list by traditional ranking models, we retrieved the top 20 documents from the collections using query-likelihood language models [2] with dirichlet prior1 (LM) and computed the correlation between the output value of either of the heuristics and the relevance. We observe that both have positive correlations with document relevance (0.21 for R1 and 0.31 for R2). This result is consistent with our two hypotheses.",null,null
,,,
38,"We validate the effectiveness of the two ranking heuristics in a re-ranking scheme. First, given a query, we generate a",null,null
,,,
39,"1We tuned µ to be optimal for each collection (1000, 3000, and 5000 for AP88-90, WT2g, and Blogs06, respectively).",Y,null
,,,
40,Table 1: Retrieval performance.,null,null
,,,
41,Corpus Method MRR,null,null
,,,
42,P@1,null,null
,,,
43,P@5,null,null
,,,
44,LM,null,null
,,,
45,0.4598 0.3100 0.2840,null,null
,,,
46,0,null,null
,,,
47,0.4433 0.3000 0.2740 (-3.59%) (-3.23%) (-3.52%),null,null
,,,
48,AP88-90 +R2,null,null
,,,
49,0.4671 0.3200 0.2900 (1.59%) (3.23%) (2.11%),null,null
,,,
50,0,null,null
,,,
51,0.4743 (3.15%),null,null
,,,
52,0.3200 (3.23%),null,null
,,,
53,0.3020 (6.34%),null,null
,,,
54,LM,null,null
,,,
55,0.6342 0.6000 0.4880,null,null
,,,
56,0,null,null
,,,
57,0.6874 0.5800 0.4640 (8.39%) (-3.33%) (-4.92%),null,null
,,,
58,WT2g +R2,Y,null
,,,
59,0.7151 0.6200 0.4640 (12.76%) (3.33%) (-4.92%),null,null
,,,
60,0,null,null
,,,
61,0.7097 (11.90%),null,null
,,,
62,0.6200 (3.33%),null,null
,,,
63,0.5000 (2.64%),null,null
,,,
64,LM,null,null
,,,
65,0.6634 0.5333 0.5747,null,null
,,,
66,0,null,null
,,,
67,0.7090 0.6000 0.5987 (6.87%) (12.51%) (4.18%),null,null
,,,
68,Blogs06 +R2,null,null
,,,
69,0.7404 0.6467 0.6067 (11.61%) (21.26%) (5.57%),null,null
,,,
70,0,null,null
,,,
71,0.7384 (11.31%),null,null
,,,
72,0.6400 (20.01%),null,null
,,,
73,0.6453 (12.28%),null,null
,,,
74,"ranked list consisting of top n documents from a test collection using LM, which represents a state-of-the-art baseline. Then, we create a new ranked list of the n documents using each heuristic function. We finally re-rank the documents in the initial list in the ascending order of the mean rank of documents among individual ranked lists. If a tie occurs, a document that has a higher rank in the initial ranked list is promoted. We compare the top results of the re-ranked result with the baseline result using Mean Reciprocal Rank (MRR) and Precision at k ranks (P@1 and P@5). The Indri implementation2 is used for indexing and retrieval; stemming and stopword removal are performed.",null,null
,,,
75,"The retrieval performances of the baseline and the proposed method are presented in Table 1. We observe that when the baseline ranking is aggregated with either one of the heuristic rankings, the improvement is not consistent across different data collections. However, when the baseline ranking is merged with all two heuristic rankings, the improvement over the baseline is consistent across all collections for all evaluation measures. This demonstrates that the analysis of rank positions of query terms in interdocument term ranking is effective in improving the precision at top ranks. For future work, we plan to explore other features that capture various aspects of inter-document term ranking. We also plan to investigate on designing a unified ranking model that combines the proposed heuristics with the features of the traditional retrieval models.",null,null
,,,
76,4. REFERENCES,null,null
,,,
77,"[1] I. Matveeva, C. Burges, T. Burkard, A. Laucius, and L. Wong. High accuracy retrieval with multiple nested ranker. In SIGIR '06, 2006.",null,null
,,,
78,"[2] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR '98, 1998.",null,null
,,,
79,"[3] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, 1975.",null,null
,,,
80,2http://www.lemurproject.org/indri/,null,null
,,,
81,886,null,null
,,,
82,,null,null

,sentence,label,data
,,,
0,Where to Start Filtering Redundancy? A Cluster-Based Approach,null,null
,,,
1,"Ronald T. Fernández1, Javier Parapar2, David E. Losada1, Álvaro Barreiro2",null,null
,,,
2,"1Department of Electronics and Computer Science, University of Santiago de Compostela, Spain",null,null
,,,
3,"{ronald.teijeira, david.losada} @usc.es",null,null
,,,
4,"2Information Retrieval Lab, Department of Computer Science, University of A Coruña, Spain",null,null
,,,
5,"{javierparapar, barreiro} @udc.es",null,null
,,,
6,ABSTRACT,null,null
,,,
7,"Novelty detection is a difficult task, particularly at sentence level. Most of the approaches proposed in the past consist of re-ordering all sentences following their novelty scores. However, this re-ordering has usually little value. In fact, a naive baseline with no novelty detection capabilities yields often better performance than any state-of-the-art novelty detection mechanism. We argue here that this is because current methods initiate too early the novelty detection process. When few sentences have been seen, it is unlikely that the user is negatively affected by redundancy. Therefore, re-ordering the first sentences may be harmful in terms of performance. We propose here a query-dependent method based on cluster analysis to determine where we must start filtering redundancy.",null,null
,,,
8,"Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Information Filtering, Clustering, Retrieval Models",null,null
,,,
9,General Terms: Experimentation,null,null
,,,
10,"Keywords: Novelty Detection, Sentence Clustering",null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"Novelty detection (ND) consists of filtering out redundant material from a ranked list of texts. This is an important task that has recently become of interest in many scenarios, such as text summarization, web information access, etc. However, the performance of current ND methods is not satisfactory.",null,null
,,,
13,"In ND at sentence level, current mechanisms are based on filtering out redundancy starting at the beginning of the rank. There is often little overlapping among the first sentences seen by the user. In fact, we demonstrate here that the performance of these methods is poor because, usually, it is better to leave the ranking as it is (i.e. do not apply redundancy altogether). Redundancy that affects severely to the user comes likely at later stages (e.g. when the user has already seen a bunch of sentences). Therefore, filtering out information from the top ranked positions may be harmful. We propose here an approach based on starting the ND process only when there is strong evidence about redundancy. Moreover, because different queries may pro-",null,null
,,,
14,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
15,Table 1: Comparison of performance between differ-,null,null
,,,
16,ent state-of-the-art novelty detection methods and,null,null
,,,
17,the baseline (do nothing). Best values are bolded.,null,null
,,,
18,DN (basel.) NW SD CD,null,null
,,,
19,TREC 2003,null,null
,,,
20,P@10,null,null
,,,
21,0.876,null,null
,,,
22,.8800 .8460 .8060,null,null
,,,
23,MAP,null,null
,,,
24,0.7411,null,null
,,,
25,.8188* .7902 .8046*,null,null
,,,
26,TREC 2004,null,null
,,,
27,P@10,null,null
,,,
28,0.764,null,null
,,,
29,.6760 .5900 .6460,null,null
,,,
30,MAP,null,null
,,,
31,0.6103,null,null
,,,
32,.6086 .5574 .5865,null,null
,,,
33,"duce document ranks with diverse redundancy levels, it is necessary to do this process in a query-dependent way. To this aim, we propose here a method based on clustering.",null,null
,,,
34,2. THE METHOD,null,null
,,,
35,"First, we study the performance of state-of-the-art ND methods, i.e. NewWords, SetDif and CosDist [1] and show that they are often outperformed by a do nothing (DN) baseline (leave the relevance ranking as is). NewWords (NW) counts the number of terms of a sentence that have not been seen in any previous sentence. SetDif (SD) counts the number of different words between a current sentence and the most similar previously seen sentence. CosDist (CD) is a vector-space measure where the novelty score of a sentence si is the negative cosine of the angle between si and the most similar sentence in the history. These measures have shown their merits in past evaluations of ND [1]. However, when we compare these methods against a DN baseline, i.e. no novelty detection, we find that the application of ND is not justified. This is illustrated in Table 1, where we report the results found for Task 2 of TREC Novelty Tracks 2003 and 2004 (given the relevant sentences in 25 retrieved documents, identify all novel sentences). Statistical significant improvements between the performance of these ND methods and the baseline (t-test with 95% confidence level) are marked with .",null,null
,,,
36,"The ND mechanisms are only helpful in a couple of cases and, furthermore, most of the ND methods lead to a performance that is worse than the baseline's performance. This indicates that current ND methods produce a strong reordering of the information presented to the user, which is problematic in terms of performance. We claim that this poor performance comes from initiating ND too early.",null,null
,,,
37,"We propose here a method that drives the process so that,",null,null
,,,
38,735,null,null
,,,
39,Table 2: Comparison of performance between our,null,null
,,,
40,approach over the three ND methods and their orig-,null,null
,,,
41,inal formulations. Statistical significant differences,null,null
,,,
42,of each version w.r.t. the corresponding original,null,null
,,,
43,method are marked with  (at 95% of confidence,null,null
,,,
44,level). Best values are bolded.,null,null
,,,
45,NW NWr SD SDr CD CDr,null,null
,,,
46,testing: TREC 2003 (training: TREC 2004),Y,
,,,
47,P@10 .8800 .8980 .8460 .8920* .8060 .8940*,null,null
,,,
48,%,null,null
,,,
49,(+2 .05 ),null,null
,,,
50,(+5 .44 ),null,null
,,,
51,(+10 .92 ),null,null
,,,
52,MAP .8188 .8237 .7902 .8074* .8046 .8182*,null,null
,,,
53,%,null,null
,,,
54,(+0 .60 ),null,null
,,,
55,(+2 .18 ),null,null
,,,
56,(+1 .69 ),null,null
,,,
57,testing: TREC 2004 (training: TREC 2003),Y,
,,,
58,P@10 .6760 .7840* .5900 .7640* .6460 .7760*,null,null
,,,
59,%,null,null
,,,
60,(+15 .98 ),null,null
,,,
61,(+29 .49 ),null,null
,,,
62,(+20 .12 ),null,null
,,,
63,MAP .6086 .6389* .5574 .6169* .5865 .6318*,null,null
,,,
64,%,null,null
,,,
65,(+4 .98 ),null,null
,,,
66,(+10 .67 ),null,null
,,,
67,(+7 .72 ),null,null
,,,
68,"depending on the query, ND is triggered starting at a given position r in the ranking of sentences (the sentences in previous positions preserve their order). To determine the value of r we propose a cluster-based approach. The intuition behind this idea is that ND should only be started when we find some evidence about redundancy, i.e. a sentence is strongly thematically related to a previous one, and this can be detected using clustering. The k-NN clustering algorithm was widely used for cluster-based document retrieval, see [2] for instance. Here we use a variant of the k-NN algorithm: instead of setting the number k of neighbors for a sentence we set the minimum similarity threshold t for the given metric (in our case cosine distance). Given a sentence si, its neighborhood is the set of sentences sk such that sim(si, sk)  t. The method works as follows: first, for each query we cluster all its relevant sentences using tNN. Next, we scan sequentially the ranking of sentences (as provided by the task) and fix r to the position of the first sentence whose cluster (neighborhood) contains a sentence already seen before. This means that positions from 1 to r - 1 are frozen, while sentences starting at the r position are re-ranked using the ND methods described above.",null,null
,,,
69,3. EXPERIMENTS,null,null
,,,
70,"In our evaluation we considered the TREC 2003 [4] and 2004 [3] novelty datasets. These test collections supply relevance and novelty judgments at sentence level for each topic. Sentences were clustered by applying the t-NN clustering algorithm. We considered values for t between 0 and 0.95 (in steps of 0.05). In order to assess the parameter stability we performed double cross-evaluation by swapping training and testing collections. Runs are compared using precision at 10 (P@10) and mean average precision (MAP) computed with the novelty judgments. In the training stage we obtained the optimal t (in terms of MAP) for the best novelty technique, i.e. NewWords, (t , 0.5 and t ,"" 0.4 training in TREC 2003 and 2004, respectively) and this value was fixed for all novelty methods in the test collection.""",Y,
,,,
71,"Table 2 shows a comparison between the original ND methods (NW, SD and CD) and our variants (NWr, SDr and CDr, respectively). The modified ND methods outperform significantly the original ones. Only when NWr is evaluated against the TREC 2003 dataset significant differences w.r.t. NW are not obtained but, anyway, performance does not decrease. Therefore, the new methods are promising.",Y,
,,,
72,0.82 0.80 0.78 0.76 0.74,null,null
,,,
73,0,null,null
,,,
74,TREC 2003 MAP,Y,
,,,
75,DN NW NWr,null,null
,,,
76,0.2 0.4 0.6 0.8 t,null,null
,,,
77,0.68 0.66 0.64 0.62 0.60,null,null
,,,
78,0,null,null
,,,
79,TREC 2004 MAP DN NW NWr,null,null
,,,
80,0.2 0.4 0.6 0.8 t,null,null
,,,
81,"Figure 1: Performance of our approach considering t ,"" 0 . . . 0.95, NW and DN with TREC 2003 and TREC 2004.""",Y,
,,,
82,Observe also that the new performance figures are substantially higher than the values yielded by the DN baseline (Table 1). This means that the ND techniques are still useful (provided that they are executed from lower rank positions).,null,null
,,,
83,"Figure 1 shows the impact of t on NWr, in terms of MAP (trends are similar for the rest of methods). With low t values the performance is equivalent to the original NW (clusters are large and, therefore, the ND process is initiated at early positions in the ranking). As t increases, we obtain better performance and t around 0.5 seems a good configuration.",null,null
,,,
84,We also tested a simple approach based on training r in one collection (same r for all queries) and using the learnt value in the the testing collection. This query-independent approach performs worse than our cluster-based method.,null,null
,,,
85,4. CONCLUSIONS,null,null
,,,
86,"In this poster we analyzed the performance of current state-of-the-art novelty detection methods at sentence level. We showed that, usually, these methods perform worse than doing nothing. This happens because, when the user has seen few sentences, the information tends to be novel and, therefore, applying a novelty detection process that re-orders these sentences may be harmful. Therefore, we proposed a mechanism that consists of starting the novelty detection process from a given rank position. To this aim, we followed a query-dependent cluster-based method that predicts a good ND starting position. We showed that statistically significant improvements between this variant and the stateof-the-art ND methods were obtained. As future work, we propose to study the performance of our approach for novelty detection at document level.",null,null
,,,
87,"Acknowledgments: This work was partially supported by FEDER, Ministerio de Ciencia e Innovaci´on and Xunta de Galicia under projects TIN2008-06566-C04-04, 2008/068 and 07SIN005206PR.",null,null
,,,
88,5. REFERENCES,null,null
,,,
89,"[1] J. Allan, C. Wade, and A. Bolivar. Retrieval and novelty detection at the sentence level. In Proceedings of the 26th ACM SIGIR, pp. 314­321, Canada, 2003.",null,null
,,,
90,"[2] K. S. Lee, W. B. Croft, and J. Allan. A cluster-based resampling method for pseudo-relevance feedback. In Proceedings of the 31st ACM SIGIR, pp. 235­242, USA, 2008.",null,null
,,,
91,"[3] I. Soboroff. Overview of the TREC 2004 Novelty Track. In Proceedings of the 13th TREC, USA, 2004.",null,null
,,,
92,"[4] I. Soboroff and D. Harman. Overview of the TREC 2003 Novelty Track. In Proceedings of the 12th TREC, USA, 2003.",null,null
,,,
93,736,null,null
,,,
94,,null,null

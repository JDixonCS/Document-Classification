,sentence,label,data
,,,
0,EUSUM: Extracting Easy-to-Understand English Summaries for Non-Native Readers,null,null
,,,
1,"Xiaojun Wan, Huiying Li and Jianguo Xiao",null,null
,,,
2,"Institute of Computer Science and Technology, Peking University, Beijing 100871, China Key Laboratory of Computational Linguistics (Peking University), MOE, China",null,null
,,,
3,"{wanxiaojun, lihuiying, xiaojianguo}@icst.pku.edu.cn",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"In this paper we investigate a novel and important problem in multi-document summarization, i.e., how to extract an easy-tounderstand English summary for non-native readers. Existing summarization systems extract the same kind of English summaries from English news documents for both native and nonnative readers. However, the non-native readers have different English reading skills because they have different English education and learning backgrounds. An English summary which can be easily understood by native readers may be hardly understood by non-native readers. We propose to add the dimension of reading easiness or difficulty to multi-document summarization, and the proposed EUSUM system can produce easy-to-understand summaries according to the English reading skills of the readers. The sentence-level reading easiness (or difficulty) is predicted by using the SVM regression method. And the reading easiness score of each sentence is then incorporated into the summarization process. Empirical evaluation and user study have been performed and the results demonstrate that the EUSUM system can produce more easy-to-understand summaries for non-native readers than existing summarization systems, with very little sacrifice of the summary's informativeness.",null,null
,,,
6,Categories and Subject Descriptors,null,null
,,,
7,H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing ­ abstracting methods; I.2.7 [Artificial Intelligence]: Natural Language Processing ­ text analysis,null,null
,,,
8,General Terms,null,null
,,,
9,"Algorithms, Experimentation, Design, Human Factors.",null,null
,,,
10,Keywords,null,null
,,,
11,"EUSUM, multi-document summarization, reading easiness",null,null
,,,
12,1. INTRODUCTION,null,null
,,,
13,Document summarization is a task of producing a condensed version of a document or document set. A summary is usually required to be informative and fluent. Users can easily understand the main content of the document or document set by reading the summary.,null,null
,,,
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07...$10.00.",null,null
,,,
15,"To date, various summarization methods and a number of summarization systems have been developed, such as MEAD, NewsInEssence and NewsBlaster. These methods and systems focus on how to improve the informativeness, diversity or fluency of the English summary, and they usually produce the same English summaries for all users, including native readers and nonnative readers. However, different users usually have different English reading levels because they have different English education backgrounds and learning environments. And native readers usually have higher English reading levels than non-native readers. In particular, Chinese readers usually have less ability to read English summaries than native English readers. For example, Chinese college students usually have passed the National English Test Band 4 (CET-4), and they have learned English for several years, but they still have more or less difficulty to read original English news and summaries. The difficulty lies in unknown or difficult English words (e.g. ""seismographs"", ""woodbine""), or the complex sentence structure (e.g. ""The chairman of the House Agriculture Committee says hearings are planned next year into how the U.S. Forest Service handled last summer's stubborn wildfires that scorched the West, including one-third of Yellowstone National Park.""). Therefore, they have to slow down the reading speed in order to understand the news text or summary, or give up the reading process.",null,null
,,,
16,"In this study, we argue that the English summaries produced by existing methods and systems are not fit for non-native readers (i.e. Chinese readers). We examine a new factor - reading easiness (or difficulty) 1 for document summarization, and the factor can indicate whether the summary is easy to understand by non-native readers or not. The reading easiness of a summary is dependent on the reading easiness of each sentence in the summary. And we propose a novel summarization system ­ EUSUM (Easy-toUnderstand Summarization) for incorporating the reading easiness factor into the final summary. The proposed system first predicts the reading easiness score for each sentence, and then incorporates the reading easiness score into the final sentence ranking process. Both informative and easy-to-understand sentences are selected into the summary. Both automatic evaluation and user study have been performed and the evaluation results verify the effectiveness of the proposed EUSUM system.",null,null
,,,
17,The contribution of this paper is summarized as follows: 1) We examine a new factor of reading easiness for document summarization. 2) We propose a novel summarization system ­ EUSUM for incorporating the new factor and producing easy-to-,null,null
,,,
18,"1 In this paper, ""reading easiness"" and ""reading difficulty"" refer to the same factor, and we use them interchangeably.",null,null
,,,
19,491,null,null
,,,
20,understand summaries for non-native readers. 3) We conduct both automatic evaluation and user study to verify the effectiveness of the proposed system.,null,null
,,,
21,The paper is organized as follows: Section 2 introduces related work. Section 3 describes the details of the EUSUM system. Sections 4 and 5 present experimental results and discussions. Lastly we conclude our paper in Section 6.,null,null
,,,
22,2. RELATED WORK,null,null
,,,
23,2.1 Document Summarization,null,null
,,,
24,"Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set.",null,null
,,,
25,"For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, topic signature [19, 22]. The summary sentences can also be selected by using machine learning methods [1, 17] or graphbased methods [8, 23]. Other methods include mutual reinforcement principle [33].",null,null
,,,
26,"For multi-document summarization, the centroid-based method [27] is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS [20] makes use of new features such as topic signature to select important sentences. Machine Learning-based approaches have also been proposed for combining various sentence features [34]. Themes (or topics, clusters) discovery in documents has been used for sentence selection [10]. The influences of input difficulty on summarization performance have been investigated in [25]. Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau [24] extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences [32]. Sentence ordering in summaries has been investigated in [2, 3] to improve the summary fluency.",null,null
,,,
27,"Other summarization tasks include topic-focused (query-biased) document summarization [31], update summarization [18]. All these summarization tasks do not consider the reading easiness factor of the summary for non-native readers.",null,null
,,,
28,2.2 Reading Difficulty Prediction,null,null
,,,
29,"A reading difficulty measure can be originally described as a function or model that maps a text to a numerical value corresponding to a difficulty or grade level [12]. And reading difficulty prediction can be viewed as a regression of difficulty grade level based on a set of features derived from the text. Earlier work on reading difficulty prediction is conducted for the purpose of education or language learning. For example, one purpose is to find appropriate reading materials of the appropriate difficulty level, in terms of both vocabulary and grammar, for English as a First or Second Language students. And almost all earlier work focuses on document-level reading difficulty prediction.",null,null
,,,
30,A variety of features have been investigated in reading difficulty measures. Average sentence length and word length are simple,null,null
,,,
31,"proxies for grammatical and lexical complexity of a text, as in the Dale-Chall model [5]. The Flesch-Kincaid meaure [15] is probably the most common reading difficulty in use in earlier days. The Lexile Framework [29] uses individual word frequency estimates as a measure of lexical difficulty, and it uses a Rasch model based on the features of word frequency and sentence length. In recent years, more sophisticated features and models are used. Smoothed unigram language modeling has been developed to capture the predictive ability of individual words based on their frequency at each reading difficulty level [7]. A statistical approach is proposed to infer the distribution of a word's likely acquisition age automatically from authentic texts collected from the Web, and then an effective semantic component for predicting reading difficulty of news texts is provided by combining the acquisition age distributions for all words in a document [14]. Schwarm and Ostendorf [28] incorporate syntactic features derived from syntactic parses of text, and their system performs better than the Flesch-Kincaid and Lexile measures. The frequency of grammatical constructions has been used as a measure of grammatical difficulty, and the final prediction function is a linear function of the lexical and grammatical components [11, 12]. Pitler and Nenkova [26] combine lexical, syntactic and discourse features to produce a highly predictive model of text readability. In addition to English language, François [9] presents an approach to assessing the readability of French texts. More recently, a machine learning approach is used for predicting the readability of web search summaries or snippets [13].",null,null
,,,
32,"In this study, we investigate the reading difficulty (or easiness) prediction of English sentences for Chinese readers, i.e. whether an English sentence is easy to understand by Chinese readers or not.",null,null
,,,
33,"Note that sentence ordering in a long summary also has influences on the reading difficulty or readability of the summary, and proper order of extracted sentences can improve their readability [2]. However, sentence ordering is another research problem and we do not take into account this factor in this study.",null,null
,,,
34,3. THE EUSUM SYSTEM 3.1 System Overview,null,null
,,,
35,"The main idea of the proposed EUSUM system is to incorporate the sentence-level reading easiness factor into the summary extraction process. Each sentence is associated with two factors: informativeness and reading easiness. The informativeness of a sentence is computed by using previous summarization methods. The reading easiness of a sentence is measured by an EU (easy-tounderstand) score, which is predicted by using statistical regression methods. The two scores are then combined and both informative and easy-to-understand sentences are chosen into the summary. The three steps of the EUSUM system will be described in details in next two sections.",null,null
,,,
36,"As mentioned in Section 2.2, we do not consider the fluency factor of the whole summary in this study, which has been investigated in related research areas (e.g. sentence ordering [2, 3]).",null,null
,,,
37,492,null,null
,,,
38,3.2 Sentence-Level Reading Easiness Prediction,null,null
,,,
39,"In this study, reading easiness refers to how easily a text can be understood by non-native readers. Reading easiness prediction is a task of mapping a text to a numerical value corresponding to a reading easiness. The larger the value is, the more easily the text can be understood. We focus on predicting the reading easiness score of an English sentence for Chinese college students.",null,null
,,,
40,"As mentioned earlier, Chinese college students usually have studied English for several years and they usually have passed the CET-4 test2 or above, which means that they have some ability to read ordinary English articles. However, because of different English learning environments and different learning abilities, these students may have different English reading levels. Many students have some difficulty to read original English news or summaries. The two factors most influencing the reading process are as follows:",null,null
,,,
41,"1) Unknown or difficult English words: for example, most Chinese college students do not know the words such as ""seismographs"", ""woodbine"".",null,null
,,,
42,"2) Complex sentence structure: for example, a sentence with two or more clauses introduced by a subordinating conjunction is usually difficult to read.",null,null
,,,
43,"As introduced in Section 2.2, various regression methods have been used for reading difficulty prediction. In this study, we adopt the -support vector regression (-SVR) method [30] for the reading easiness prediction task. The SVR algorithm is firmly grounded in the framework of statistical learning theory (VC theory). The goal of a regression algorithm is to fit a flat function to the given training data points.",null,null
,,,
44,"In the experiments, we use the LIBSVM tool [6] with the RBF kernel for the regression task, and we use the parameter selection tool of 10-fold cross validation via grid search to find the best parameters with respect to mean square error (MSE), and then use the best parameters to train the whole training set.",null,null
,,,
45,"We use the following two groups of features for each sentence: the first group includes surface features, and the second group includes parse based features.",null,null
,,,
46,The four surface features are as follows:,null,null
,,,
47,1) Sentence length: It refers to the number of words in the sentence. A long sentence may be more difficult to understand than a short sentence.,null,null
,,,
48,"2) Average word length: It refers to the average length of words in the sentence. Usually, an English word with few characters is more easily recognized and remembered than that with many characters.",null,null
,,,
49,"3) CET-4 word percentage: It refers to the percentage of how many words in the sentence appear in the CET-4 word list (690 words). As mentioned earlier, most Chinese college students have passed CET-4, and the words appearing in the CET-4 word list are likely to be recognized by the students.",null,null
,,,
50,"2 CET-4 is College English Test Band 4, which is a national English level test in China, and all college students are required to pass this test before graduation.",null,null
,,,
51,"4) Number of peculiar words: It refers to the number of infrequently occurring words in the sentence. We collect all words in the experimental corpus, and choose the top 2000 words with low frequency as the peculiar words. The frequency of each word is extracted from the Google Web 1T 1-gram database [4].",null,null
,,,
52,We use the Stanford Lexicalized Parser [16] with the provided English PCFG model to parse a sentence into a parse tree. The output tree is a context-free phrase structure grammar representation of the sentence. The four parse features are as follows:,null,null
,,,
53,"1) Depth of the parse tree: It refers to the depth of the generated parse tree. Usually the higher the parse tree is, the more complex the sentence is.",null,null
,,,
54,"2) Number of SBARs in the parse tree: SBAR is defined as a clause introduced by a (possibly empty) subordinating conjunction. It is an indictor of sentence complexity, especially for Chinese readers.",null,null
,,,
55,3) Number of NPs in the parse tree: It refers to the number of noun phrases in the parse tree.,null,null
,,,
56,4) Number of VPs in the parse tree: It refers to the number of verb phrases in the parse tree.,null,null
,,,
57,All the above feature values are scaled by using the provided svmscale program.,null,null
,,,
58,"At this step, each sentence si can be associated with a reading easiness score EaseScore(si) predicted by the -SVR method. The larger the score is, the more easily the sentence is understood. The score is finally normalized by dividing by the maximum score.",null,null
,,,
59,3.3 Sentence-Level Informativeness Evaluation,null,null
,,,
60,"In this study, we adopt two typical methods for evaluating the informativeness of each sentence in a document set. The two methods are described briefly in the following sections.",null,null
,,,
61,3.3.1 Centroid-Based Method,null,null
,,,
62,"The centroid-based method is the algorithm used in the MEAD system. The method uses a heuristic and simple way to sum the sentence scores computed based on different features. In our implementation, the score for each sentence is a linear combination of the weights computed based on the following three features: 1) Centroid-based Weight. The weight C(si) of sentence si is calculated as the cosine similarity between the sentence text and the concatenated text for the whole document set D. The weight is then normalized by dividing by the maximal weight. 2) Sentence Position. The weight P(si) is calculated for sentence si to reflect its position priority as P(si),""1-(posi-1)/ni, where posi is the position number of sentence si in a particular document and ni is the total number of sentences in the document. Obviously, posi ranges from 1 to ni. 3) First Sentence Similarity. The weight F(si) is computed as the cosine similarity value between sentence si and the corresponding first sentence in the same document.""",null,null
,,,
63,"After all the above weights are calculated for each sentence, we sum the three weights and get the overall score InfoScore(si) for sentence si. After the scores for all sentences are computed, the",null,null
,,,
64,493,null,null
,,,
65,score of each sentence is normalized by dividing by the maximum score.,null,null
,,,
66,3.3.2 Graph-Based Method,null,null
,,,
67,"The basic idea of the graph-based method is that of ""voting"" or ""recommendation"" between sentences. Formally, given a document set D, let G,""(V, E) be an undirected graph to reflect the relationships between sentences in the document set. V is the set of vertices and each vertex si in V is a sentence in the document set. E is the set of edges. Each edge eij in E is associated with an affinity weight f(si, sj) between sentences si and sj (ij). The weight is computed using the standard cosine measure between the two sentences. Here, we have f(si, sj)"",""f(sj, si) and let f(si, si)"",0 to avoid self transition.",null,null
,,,
68,"We use an affinity matrix M to describe G with each entry corresponding to the weight of an edge in the graph. M ,",null,null
,,,
69,"~ (Mi,j)|V|×|V| is defined as Mi,j,""f(si,sj). Then M is normalized to M to make the sum of each row equal to 1.""",null,null
,,,
70,"Based on matrix M~ , the saliency score InfoScore(si) for sentence si can be deduced from those of all other sentences linked with it and it can be formulated in a recursive form as in the PageRank algorithm:",null,null
,,,
71, InfoScore(si,null,null
,,,
72,),null,null
,,,
73,",",null,null
,,,
74,µ,null,null
,,,
75,all,null,null
,,,
76,ji,null,null
,,,
77,InfoScore(s j,null,null
,,,
78,),null,null
,,,
79,M~,null,null
,,,
80,"j,i",null,null
,,,
81,+,null,null
,,,
82,(1- µ) |V |,null,null
,,,
83,"where µ is the damping factor usually set to 0.85, as in the PageRank algorithm.",null,null
,,,
84,"After the scores for all sentences are computed, the score of each sentence is normalized by dividing by the maximum score.",null,null
,,,
85,3.4 Summary Extraction,null,null
,,,
86,"After we obtain the reading easiness score and the informativeness score of each sentence in the document set, we linearly combine the two scores to get the combined score of each sentence.",null,null
,,,
87,"Formally, let EaseScore(si)[0,1] and InfoScore(si)[0,1] denote the reading easiness score and the informativeness score of sentence si, the combined score of the sentence is:",null,null
,,,
88,"CombinedSc ore(si ) , InfoScore (si ) +  × EaseScore (si )",null,null
,,,
89,"where 0 is a parameter controlling the influences of the reading easiness factor. If  is set to 0, the summary is extracted without considering the reading easiness factor. Usually,  is not set to a large value because we must maintain the content informativeness in the extracted summary. Therefore, we choose the parameter value empirically in order to balance the two factors of content informativeness and reading easiness.",null,null
,,,
90,"For multi-document summarization, some sentences are highly overlapping with each other, and thus we apply the same greedy algorithm in [31] to penalize the sentences highly overlapping with other highly scored sentences, and finally the informative, novel, and easy-to-understand sentences are chosen into the summary.",null,null
,,,
91,"In the algorithm, the final rank score RankScore(si) of each sentence si is initialized to its combined score CombinedScore(si). And at each iteration, the highly ranked sentence (e.g. si) is selected into the summary, and the rank score of each remaining sentence sj is penalized by using the following formula:",null,null
,,,
92,"RankScore(s j ) ,"" RankScore(s j ) -   M~ j,i  CombinedScore(si )""",null,null
,,,
93,"where >0 is the penalty degree factor. The larger  is, the greater penalty is imposed to the rank score. If ,""0, no diversity penalty is imposed at all. The iteration is stopped after the summary length limit is reached.""",null,null
,,,
94,4. EXPERIMENTS,null,null
,,,
95,4.1 Reading Easiness Prediction,null,null
,,,
96,4.1.1 Experimental Setup,null,null
,,,
97,"In the experiments, we first constructed the gold-standard dataset in the following way.",null,null
,,,
98,"DUC2001 provided 309 news articles for document summarization tasks, and the articles were grouped into 30 document sets. The news articles were selected from TREC-9. We chose five document sets (d04, d05, d06, d08, d11) with 54 news articles out of the DUC2001 test set. The documents were then split into sentences and there were totally 1736 sentences.",Y,null
,,,
99,"Two college students (one undergraduate student and one graduate student) manually labeled the reading easiness score for each sentence separately. The score ranges between 1 and 5, and 1 means ""very hard to understand"", and 5 means ""very easy to understand"", and 3 means ""mostly understandable"". The final reading easiness score was the average of the scores provided by the two annotators.",null,null
,,,
100,"After annotation, we randomly separated the labeled sentence set into a training set of 1482 sentences and a test set of 254 sentences. We then used the LIBSVM tool for training and testing.",null,null
,,,
101,Two standard metrics were used for evaluating the prediction results. The two metrics are as follows:,null,null
,,,
102,"Mean Square Error (MSE): This metric is a measure of how correct each of the prediction values is on average, penalizing more severe errors more heavily. Pearson's Correlation Coefficient (): This metric is a measure of whether the trends of prediction values matched the trends for human-labeled data.",null,null
,,,
103,4.1.2 Experimental Results,null,null
,,,
104,"Table 1 shows the prediction results. For comparison, the result for the Flesch-Kincaid measure3 is also reported in the table. We can see that the overall results of our method are very promising. And the correlation is high. The results guarantee that the use of reading easiness scores in the summarization process is feasible.",null,null
,,,
105,Table 1. Reading easiness prediction results,null,null
,,,
106,Method Flesch-Kincaid SVR (Surface features) SVR (Parse features) SVR (Surface features + Parse features),null,null
,,,
107,MSE 0.704 0.121 0.227 0.112,null,null
,,,
108, 0.377 0.929 0.853 0.931,null,null
,,,
109,3 The reading easiness scores based on the FK measure are directly obtained by accessing the following web service: http://www.standards-schmandards.com/exhibits/rix/index.php,null,null
,,,
110,494,null,null
,,,
111,"We can also see that either the surface feature set or the parse feature set can achieve good prediction result, and the two feature sets can contribute to the overall prediction results. However, the Flesch-Kincaid measure does not perform well.",null,null
,,,
112,4.2 Document Summarization,null,null
,,,
113,4.2.1 Experimental Setup,null,null
,,,
114,"In this study, we used the multi-document summarization task (task 2) in DUC2001 for evaluation. As mentioned in Section 4.1.1, DUC2001 provided 30 document sets. Because we have used five document sets (d04-d11) for training and testing in the task of reading easiness prediction, we used the remaining 25 document sets for summarization evaluation, and the average document number per document set is 10. The sentences in each article have been separated and the sentence information has been stored into files. A summary was required to be created for each document set and the summary length was 100 words. Generic reference summaries were provided by NIST annotators for evaluation.",Y,null
,,,
115,"We used the LIBSVM tool with the learned model to predict the reading easiness score for each sentence in the documents, and then used the scores for summary extraction.",null,null
,,,
116,"Different from traditional summarization tasks, our task is to incorporate the reading easiness factor into multi-document summary, and the easy-to-understand summarization can be considered as a novel summarization task. Therefore, we evaluate a summary from the following two aspects:",null,null
,,,
117,"Content Informativeness: This aspect is widely evaluated in almost all traditional summarization tasks. It refers to how much a summary reflects the major content of the document set. Usually, it can be measured by comparing the system summary with the reference summary.",null,null
,,,
118,"We used the ROUGE-1.5.5 toolkit for automatic evaluation of the content informativeness, and the toolkit was officially adopted by DUC for automatic summarization evaluation. The toolkit measures summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary [21]. The ROUGE-1.5.5 toolkit reports separate F-measure scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences. In this study, we show four ROUGE F-measure scores in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), ROUGE-W (based on weighted longest common subsequence, weight,""1.2), and ROUGE-SU* (based on skip bigram with unigram)4.""",null,null
,,,
119,"Reading Easiness: This aspect is not evaluated by previous summarization tasks. We aim to evaluate the reading easiness level of the whole summary. Because the reading easiness level of a summary is dependent on the reading easiness scores of the sentences in the summary, we use the average reading easiness score of the sentences in a summary as the summary's reading easiness level. The overall reading easiness score is the average across all 25 document sets.",null,null
,,,
120,"4 We also used the option ""-l 100"" for truncating the summary and used the option ""-m"" for word stemming when using the ROUGE-1.5.5 toolkit.",null,null
,,,
121,"In addition to the above automatic evaluation procedures, we also performed pilot user studies for evaluation. Four Chinese college students participated in the user studies. We have developed a user study tool for facilitating the subjects to evaluate each summary from the two aspects of content informativeness and reading easiness. Each subject can assign a score from 1 to 5 on each aspect for each summary. For reading easiness, 1 means ""very hard to understand"", and 5 means ""very easy to understand"". For content informativeness, 1 means ""least informative"", and 5 means ""very informative"". During each user study procedure, we compared two summarization systems' results. And the two summaries produced by the two systems for the same document set were presented in the same interface, and then the four subjects assigned scores to each summary after they read and compared the two summaries. The final score of a summary on one aspect was the average of the scores assigned by the four subjects. And the overall scores were averaged across all subjects and all 25 document sets.",null,null
,,,
122,4.2.2 Experimental Results,null,null
,,,
123,4.2.2.1 Automatic Evaluation Results,null,null
,,,
124,"In this section, we report the automatic evaluation results of EUSUM from both two aspects. Though the combination weight  in EUSUM can be set to any non-negative value, it ranges from 0 to 1 in our experiments, because a much larger  will lead to a big sacrifice of the content informativeness for the summary. The penalty degree factor  for EUSUM is set to 10, as in [31]. Table 2 shows the ROUGE scores and the reading easiness score of the EUSUM system with the centroid-based method, which is denoted as EUSUM(Centroid). Table 3 shows the ROUGE scores and the reading easiness score of the EUSUM system with the graphbased method, which is denoted as EUSUM(Graph).",null,null
,,,
125,"Seen from the tables, the ROUGE scores of EUSUM(Centroid) and EUSUM(Graph) are decreased with the increase of the combination weight , and the reading easiness scores of them are increased with the increase of . And we can see that with the increase of , the summary's reading easiness can be more quickly becoming significantly different from that of the summary with ,""0, while the summary's content informativeness is not significantly affected when  is set to a small value. Moreover, For EUSUM(Graph), even the ROUGE scores with "",0.1 are better than that with ,""0. The results demonstrate that when  is set to a small value, the content informativeness aspect of the extracted summary are almost not affected, but the reading easiness aspect of the extracted summary can be significantly improved.""",null,null
,,,
126,"By comparing the performance values in the two tables, we can see that when  is fixed, the ROUGE-1, ROUGE-W and ROUGESU* scores of EUSUM(Graph) are higher than the corresponding scores of EUSUM(Centroid), which verifies the effectiveness of the graph-based summarization method. We can also see that when  is fixed, the reading easiness scores of EUSUM(Graph) are always higher than the corresponding scores of EUSUM(Centroid), which demonstrates that EUSUM(Graph) can extract more easy-to-understand summaries than EUSUM(Centroid). We explain the results by that the graph-based sentence extraction method tends to extract sentences with good feature values for indicating reading easiness. For example, sentence length is one of the important features for reading",null,null
,,,
127,495,null,null
,,,
128,"easiness prediction, and a shorter sentence is more likely to be easy to understand. We compare the average sentence length (average word number per sentence) in the summaries extracted by EUSUM(Graph) and EUSUM(Centroid) in Figure 1. We can see that EUSUM(Graph) usually extracts shorter sentences than EUSUM(Centroid), which verifies the results from one perspective. Overall, the results show that EUSUM(Graph) is more suitable than EUSUM(Centroid) for extracting easy-tounderstand summaries5.",null,null
,,,
129,Table 2. EUSUM (Centroid) results vs. ,null,null
,,,
130,ROUGE-1 Average_F,null,null
,,,
131,ROUGE-2 ROUGE-W ROUGE-SU*,null,null
,,,
132,Average_F Average_F,null,null
,,,
133,Average_F,null,null
,,,
134,Reading Easiness,null,null
,,,
135,score,null,null
,,,
136,0,null,null
,,,
137,0.31682,null,null
,,,
138,0.05896,null,null
,,,
139,0.13532,null,null
,,,
140,0.09405,null,null
,,,
141,3.45681,null,null
,,,
142,0.1,null,null
,,,
143,0.31468,null,null
,,,
144,0.05529,null,null
,,,
145,0.13346,null,null
,,,
146,0.09335,null,null
,,,
147,3.53835,null,null
,,,
148,0.2,null,null
,,,
149,0.31419,null,null
,,,
150,0.05488,null,null
,,,
151,0.13367,null,null
,,,
152,0.09254,null,null
,,,
153,3.58633,null,null
,,,
154,0.3,null,null
,,,
155,0.31153,null,null
,,,
156,0.05379,null,null
,,,
157,0.13311,null,null
,,,
158,0.09085,null,null
,,,
159,3.65514,null,null
,,,
160,0.4,null,null
,,,
161,0.30938,null,null
,,,
162,0.05255,null,null
,,,
163,0.13203,null,null
,,,
164,0.08878,null,null
,,,
165,3.74286,null,null
,,,
166,0.5,null,null
,,,
167,0.30754,null,null
,,,
168,0.6,null,null
,,,
169,0.30402,null,null
,,,
170,0.7,null,null
,,,
171,0.30228,null,null
,,,
172,0.8,null,null
,,,
173,0.30227,null,null
,,,
174,0.9,null,null
,,,
175,0.29344,null,null
,,,
176,0.05029 0.04920 0.04792 0.04700 0.04233,null,null
,,,
177,0.13113 0.13011 0.12955 0.12959 0.12579,null,null
,,,
178,0.08802 0.08648 0.08506 0.08353 0.07808,null,null
,,,
179,3.82912 3.85643 3.96930 4.09463 4.22541,null,null
,,,
180,1,null,null
,,,
181,0.29242,null,null
,,,
182,0.04033,null,null
,,,
183,0.12514,null,null
,,,
184,0.07762,null,null
,,,
185,4.33986,null,null
,,,
186,Table 3. EUSUM (Graph) results vs. ,null,null
,,,
187,ROUGE-1 Average_F,null,null
,,,
188,ROUGE-2 ROUGE-W ROUGE-SU*,null,null
,,,
189,Average_F Average_F,null,null
,,,
190,Average_F,null,null
,,,
191,Reading Easiness,null,null
,,,
192,score,null,null
,,,
193,0,null,null
,,,
194,0.3201,null,null
,,,
195,0.05205,null,null
,,,
196,0.13701,null,null
,,,
197,0.09488,null,null
,,,
198,3.91532,null,null
,,,
199,0.1,null,null
,,,
200,0.32286,null,null
,,,
201,0.05364,null,null
,,,
202,0.13928,null,null
,,,
203,0.09612,null,null
,,,
204,3.98131,null,null
,,,
205,0.2,null,null
,,,
206,0.31928,null,null
,,,
207,0.05155,null,null
,,,
208,0.13693,null,null
,,,
209,0.09414,null,null
,,,
210,4.07323,null,null
,,,
211,0.3,null,null
,,,
212,0.31751,null,null
,,,
213,0.0492,null,null
,,,
214,0.13587,null,null
,,,
215,0.09235,null,null
,,,
216,4.16455,null,null
,,,
217,0.4,null,null
,,,
218,0.31598,null,null
,,,
219,0.04677,null,null
,,,
220,0.13518,null,null
,,,
221,0.091,null,null
,,,
222,4.31125,null,null
,,,
223,0.5,null,null
,,,
224,0.30828,null,null
,,,
225,0.04544,null,null
,,,
226,0.13231,null,null
,,,
227,0.08773,null,null
,,,
228,4.40275,null,null
,,,
229,0.6,null,null
,,,
230,0.30723,null,null
,,,
231,0.04516,null,null
,,,
232,0.13308,null,null
,,,
233,0.08753,null,null
,,,
234,4.47232,null,null
,,,
235,0.7,null,null
,,,
236,0.30608,null,null
,,,
237,0.04525,null,null
,,,
238,0.13306,null,null
,,,
239,0.08656,null,null
,,,
240,4.56735,null,null
,,,
241,0.8,null,null
,,,
242,0.30197,null,null
,,,
243,0.04396,null,null
,,,
244,0.13165,null,null
,,,
245,0.0843,null,null
,,,
246,4.63991,null,null
,,,
247,0.9,null,null
,,,
248,0.29936,null,null
,,,
249,0.04343,null,null
,,,
250,0.13131,null,null
,,,
251,0.08324,null,null
,,,
252,4.67847,null,null
,,,
253,1,null,null
,,,
254,0.29498,null,null
,,,
255,0.04166,null,null
,,,
256,0.12993,null,null
,,,
257,0.07964,null,null
,,,
258,4.72482,null,null
,,,
259,"(The bolded scores indicate that the difference between the scores and the corresponding scores when ,0 is statistically significant by using t-test.)",null,null
,,,
260,"In the above experiments, the penalty weight  is fixed to 10. We now take EUSUM(Graph) as an example to show how the penalty weight  influences the two aspects of the proposed summarization system. Figures 2 and 3 show the reading easiness score curves and the ROUGE-SU* F-score curves of EUSUM(Graph) with different settings, respectively. We can see that the reading easiness scores of EUSUM(Graph) with different settings have a tendency to increase with the increase of . And after  is larger than 10, the reading easiness scores for most settings do not change any more, which shows that the penalty weight has no significant influences on the reading easiness of the summaries when the weight is set to a moderately large value. The ROUGE-SU* scores are firstly increasing with the increase of",null,null
,,,
261,"5 Actually, we can improve the centroid-based method by incorporating some useful features such as sentence length, but it is not the focus of this paper.",null,null
,,,
262," and then decreased with the increase of , which demonstrates that less or much penalty will lower the performance instead of content informativeness.",null,null
,,,
263,30,null,null
,,,
264,Average sentence length,null,null
,,,
265,25,null,null
,,,
266,20,null,null
,,,
267,15,null,null
,,,
268,10,null,null
,,,
269,EUSUM(Centroid),null,null
,,,
270,5,null,null
,,,
271,EUSUM(Graph),null,null
,,,
272,0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ,null,null
,,,
273,Figure 1. EUSUM average sentence length (sentence word number) comparison.,null,null
,,,
274,Reading easiness score,null,null
,,,
275,5 4.8 4.6 4.4 4.2,null,null
,,,
276,4 3.8,null,null
,,,
277,0,null,null
,,,
278,",0 ,0.8",null,null
,,,
279,24,null,null
,,,
280,",0.2 ,1",null,null
,,,
281,",0.4",null,null
,,,
282,",0.6",null,null
,,,
283,6 8 10 12 14 16 18 20 ,null,null
,,,
284,Figure 2. EUSUM(Graph) reading easiness vs. penalty weight.,null,null
,,,
285,ROUGE-SU* Average_F,null,null
,,,
286,0.1 0.095,null,null
,,,
287,0.09 0.085,null,null
,,,
288,0.08 0.075,null,null
,,,
289,0.07 0.065,null,null
,,,
290,0.06 0,null,null
,,,
291,",0 ,0.8",null,null
,,,
292,24,null,null
,,,
293,",0.2 ,1",null,null
,,,
294,",0.4",null,null
,,,
295,",0.6",null,null
,,,
296,6 8 10 12 14 16 18 20 ,null,null
,,,
297,Figure 3. EUSUM(Graph) content informativeness vs. penalty weight.,null,null
,,,
298,4.2.2.2 User Study Results,null,null
,,,
299,"In order to validate the effectiveness of the system by real nonnative readers, two user study procedures were performed:",null,null
,,,
300,"User study 1: The summaries extracted by EUSUM(Centroid) (,0) and EUSUM(Graph) (,""0.2) are compared and scored by subjects. Seen from Tables 2 and 3, most ROUGE scores of the two systems are very similar, and the reading easiness score of EUSUM(Graph) ("",0.2) is higher than that of EUSUM(Centroid) (,0). Table 4 gives the averaged subjective scores of the two systems. The user study results verify that the summaries by EUSUM(Graph) (,""0.2) are indeed significantly easy to understand by non-native readers, while the content informativeness of the two systems are not significantly different.""",null,null
,,,
301,"User study 2: The summaries extracted by EUSUM(Graph) (,0) and EUSUM(Graph) (,0.3) are compared and scored by subjects.",null,null
,,,
302,496,null,null
,,,
303,"Seen from Tables 2 and 3, most ROUGE scores of the two",null,null
,,,
304,"systems are not significantly different, and the reading easiness score of EUSUM(Graph) (,0.3) is higher than that of EUSUM(Graph) (,0). Table 5 gives the averaged subjective",null,null
,,,
305,"scores of the two systems. The user study results verify that the summaries by EUSUM(Graph) (,0.3) is indeed significantly",null,null
,,,
306,"easy to understand by non-native readers, while the content",null,null
,,,
307,informativeness of the two systems are not significantly different.,null,null
,,,
308,Table 4. Results for user study 1,null,null
,,,
309,"EUSUM(Centroid) (,0) EUSUM(Graph) (,0.2)",null,null
,,,
310,Content Informativeness,null,null
,,,
311,3.47 3.63,null,null
,,,
312,Reading Easiness,null,null
,,,
313,3.33 4.01*,null,null
,,,
314,Table 5. Results for user study 2,null,null
,,,
315,"EUSUM(Graph) (,0)",null,null
,,,
316,Content Informativeness,null,null
,,,
317,3.71,null,null
,,,
318,Reading Easiness,null,null
,,,
319,3.73,null,null
,,,
320,"EUSUM(Graph) (,0.3)",null,null
,,,
321,3.52,null,null
,,,
322,4.02*,null,null
,,,
323,(* indicates that the performance difference is statistically significant by using ttest.),null,null
,,,
324,4.2.2.3 Running Examples,null,null
,,,
325,"In order to better compare the results, we give several typically extracted summaries for two document sets D14 and D59. The predicted reading easiness score of each sentence is also given in brackets.",null,null
,,,
326,"EUSUM(Centroid)(,0) for D14:",null,null
,,,
327,"A U.S. Air Force F-111 fighter-bomber crashed today in Saudi Arabia, killing both crew members, U.S. military officials reported. (3.97397) A jet trainer crashed Sunday on the flight deck of the aircraft carrier Lexington in the Gulf of Mexico, killing five people, injuring at least two and damaging several aircraft (3.182) U.S. Air Force war planes participating in Operation Desert Shield are flying again after they were ordered grounded for 24 hours following a rash of crashes. (3.41654) A U.S. military jet crashed today in a remote, forested area in northern Japan, but the pilot bailed out safely and was taken by helicopter to an American military base, officials said. (3.42433)",null,null
,,,
328,"EUSUM(Graph)(,0) for D14:",null,null
,,,
329,"The U.S. military aircraft crashed about 800 meters northeast of a Kadena Air Base runway and the crash site is within the air base's facilities. (3.84771) Two U.S. Air Force F-16 fighter jets crashed in the air today and exploded, an air force spokeswoman said. (4.35604) West German police spokesman Hugo Lenxweiler told the AP in a telephone interview that one of the pilots was killed in the accident. (3.79754) Even before Thursday's fatal crash, 12 major accidents of military aircraft had killed 95 people this year alone. (3.92878) Air Force Spokesman 1st Lt. Al Sattler said the pilot in the Black Forest crash ejected safely before the crash and was taken to Ramstein Air Base to be examined. (3.70656)",null,null
,,,
330,"EUSUM(Graph)(,0.3) for D14:",null,null
,,,
331,"Two U.S. Air Force F-16 fighter jets crashed in the air today and exploded, an air force spokeswoman said. (4.35604) The U.S. military aircraft crashed about 800 meters northeast of a Kadena Air Base runway and the crash site is within the air base's facilities. (3.84771) West German police spokesman Hugo Lenxweiler told the AP in a telephone interview that one of the pilots was killed in the accident. (3.79754) Even before Thursday's fatal crash, 12 major accidents of military aircraft had killed 95 people this year alone. (3.92878) However, suspension of training flights indicated otherwise. (4.97415) Listed as dead from the 433rd were Maj. (4.99479)",null,null
,,,
332,"EUSUM(Centroid)(,0) for D59:",null,null
,,,
333,"The Northwest Airlines jet that crashed Sunday in Detroit, killing at least 154 people, was involved in two incidents of engine failure in the past two years. (3.54385) A French DC-10 jetliner with 171 people aboard experienced a powerful highaltitude explosion, possibly from a terrorist bomb, before crashing in a remote desert region of Niger in northern Africa, officials in France said Wednesday. (2.60945) Freshman congressman Larkin Smith (R-Miss). died in a light plane crash in Mississippi, authorities said Monday, making him the second member of the House killed in an aviation accident in a week. (3.18245) Local news reporters quoted witnesses as saying that the plane appeared to nosedive into the earth. (3.9432)",null,null
,,,
334,"EUSUM(Graph)(,0) for D59:",null,null
,,,
335,"Sunday's crash was the first time in 24 years that passengers were killed in an accident involving a Northwest plane. (3.98315) FAA officials at the crash scene wouldn't speculate on the reasons for the crash or comment on the plane's engines. (4.16897) There were reports from passengers and observers that the plane's right-wing engine also failed before the crash. (3.94093) In July 1988, a United DC-10 crashed in Sioux City, Iowa, after an engine broke apart in flight, killing 112 people. (4.2899) The worst airline accident ever in the U.S. was the 1979 crash of an American Airlines jet in Chicago. (4.38547) FAA records show that besides those incidents that involved the plane that crashed, problems with the turbine sections of JT8D-200 series engines occurred on three Republic flights in the past four years. (2.97711)",null,null
,,,
336,"EUSUM(Graph)(,0.3) for D59:",null,null
,,,
337,"Sunday's crash was the first time in 24 years that passengers were killed in an accident involving a Northwest plane. (3.98315) FAA officials at the crash scene wouldn't speculate on the reasons for the crash or comment on the plane's engines. (4.16897) There were reports from passengers and observers that the plane's right-wing engine also failed before the crash. (3.94093) A team of National Transportation Safety Board investigators left Washington Wednesday night for Sioux City. (4.06095) The DC-10 operated by the French airline UTA crashed Tuesday after taking off from N'Djamena, Chad, on a flight that originated in Brazzaville, Congo. (3.63393) It can smash an airplane into the ground. (4.86026)",null,null
,,,
338,5. DISCUSSION,null,null
,,,
339,"In this study, the experiments were performed by Chinese college students. Because college students in different countries may have different English reading levels, the experimental results may be slightly changed if we use non-native students in other countries for evaluation. Even for Chinese readers, college students and high school students may have different English reading levels, and thus the experimental results may be slightly changed if we use high school students for evaluation. That's to say, the reading easiness level of a summary should be adjusted with the particular reader. In practice, we can let readers to tune the combination weight  in the proposed EUSUM system, and they can select the best weight for extracting summaries best suitable for reading.",null,null
,,,
340,"Similarly, for native English readers, different persons may have different English reading levels, and thus the framework proposed in this paper is also applicable. However, the reading easiness score of each sentence may be different because of the differences between the English reading abilities and behaviors of Chinese readers and native English readers.",null,null
,,,
341,6. CONCLUSION AND FUTURE WORK,null,null
,,,
342,"In this study, we investigate the new factor of reading easiness for document summarization, and we propose a novel summarization system - EUSUM for producing easy-to-understand summaries for non-native readers. We performed automatic evaluation and user study to verify the effectiveness of the proposed system.",null,null
,,,
343,497,null,null
,,,
344,"In future work, we will further improve the summary's reading easiness in the following two ways: 1) The summary fluency (e.g. sentence ordering in a summary) has influences on the reading easiness of a summary, and we will consider the summary fluency factor in the summarization system. 2) More sophisticated sentence reduction and sentence simplification techniques will be investigated for improving the summary's readability.",null,null
,,,
345,7. ACKNOWLEDGMENTS,null,null
,,,
346,"This work was fully supported by NSFC (60873155), and partially supported by RFDP (20070001059), Beijing Nova Program (2008B03), NCET (NCET-08-0006) and National High-tech R&D Program (2008AA01Z421).",null,null
,,,
347,8. REFERENCES,null,null
,,,
348,"[1] M. R. Amini, P. Gallinari. The Use of Unlabeled Data to Improve Supervised Learning for Text Summarization. In Proceedings of SIGIR2002, 105-112.",null,null
,,,
349,"[2] R. Barzilay, N. Elhadad and K. McKeown, Inferring strategies for sentence ordering in multidocument news summarization, Journal of Artificial Intelligence Research 17, 2002.",null,null
,,,
350,"[3] D. Bollegala, N. Okazaki and M. Ishizuka. A bottom-up approach to sentence ordering for multi-document summarization. In Proceedings of ACL2006.",null,null
,,,
351,"[4] T. Brants, A. Franz. Web 1T 5-gram Version 1. Linguistic Data Consortium, Philadelphia, 2006.",null,null
,,,
352,"[5] J. S. Chall and E. Dale. Readability revisited: the new DaleChall readability formula. Brookline Books. Cambridge, MA, 1995.",null,null
,,,
353,"[6] C.-C. Chang and C.-J. Lin. LIBSVM : a library for support vector machines, 2001. Software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm",null,null
,,,
354,"[7] K. Collins-Thompson and J. Callan. Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science and Technology, 56(13), 2005.",null,null
,,,
355,"[8] G. ErKan, D. R. Radev. LexPageRank: Prestige in MultiDocument Text Summarization. In Proceedings of EMNLP2004.",null,null
,,,
356,"[9] T. L. François. Combining a statistical language model with logistic regression to predict the lexical and syntactic difficulty of texts for FFL. In Proceedings of the EACL2009 Student Research Workshop, 2009.",null,null
,,,
357,[10] S. Harabagiu and F. Lacatusu. Topic themes for multidocument summarization. In Proceedings of SIGIR-05.,null,null
,,,
358,"[11] M. Heilman, K. Collins-Thompson, J. Callan and M. Eskenazi. Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of HLT-2007.",null,null
,,,
359,"[12] M. Heilman, K. Collins-Thompson and M. Eskenazi. An analysis of statistical models and features for reading difficulty prediction. In Proceedings of the 3rd Workshop on Innovative Use of NLP for Building Educational Applications, 2008.",null,null
,,,
360,[13] T. Kanungo and D. Orr. Predicting the readability of short web summaries. In Proceedings of WSDM2009.,null,null
,,,
361,"[14] P. Kidwell, G. Lebanon and K. Collins-Thompson. Statistical estimation of word acquisition with application to readability prediction. In Proceedings of EMNLP2009.",null,null
,,,
362,"[15] J. Kincaid, R. Fishburne, R. Rodgers and B. Chissom. Derivation of new readability formulas for navy enlisted personnel. Branch Report 8-75. Chief of Naval Training, Millington, TN, 1975.",null,null
,,,
363,[16] D. Klein and C. D. Manning. Fast Exact Inference with a Factored Model for Natural Language Parsing. In Proceedings of NIPS-2002.,null,null
,,,
364,"[17] J. Kupiec, J. Pedersen, F. Chen. A.Trainable Document Summarizer. In Proceedings of SIGIR1995, 68-73.",null,null
,,,
365,"[18] W. Li, F. Wei, Q. Lu and Y. He. PNR2: ranking sentences with positive and negative reinforcement for query-oriented update summarization. In Proceedings of COLING-08.",null,null
,,,
366,"[19] C. Y. Lin, E. Hovy. The Automated Acquisition of Topic Signatures for Text Summarization. In Proceedings of the 17th Conference on Computational Linguistics, 495-501, 2000.",null,null
,,,
367,[20] C..-Y. Lin and E.. H. Hovy. From Single to Multi-document Summarization: A Prototype System and its Evaluation. In Proceedings of ACL-02.,null,null
,,,
368,[21] C.-Y. Lin and E.H. Hovy. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics. In Proceedings of HLT-NAACL -03.,null,null
,,,
369,"[22] H. P. Luhn. The Automatic Creation of literature Abstracts. IBM Journal of Research and Development, 2(2), 1969.",null,null
,,,
370,"[23] R. Mihalcea, P. Tarau. TextRank: Bringing Order into Texts. In Proceedings of EMNLP2004.",null,null
,,,
371,[24] R. Mihalcea and P. Tarau. A language independent algorithm for single and multiple document summarization. In Proceedings of IJCNLP-05.,null,null
,,,
372,[25] A. Nenkova and A. Louis. Can you summarize this? Identifying correlates of input difficulty for generic multidocument summarization. In Proceedings of ACL-08:HLT.,null,null
,,,
373,[26] E. Pitler and A. Nenkova. Revisiting readability: a unified framework for predicting text quality. In Proceedings of EMNLP2008.,null,null
,,,
374,"[27] D. R. Radev, H. Y. Jing, M. Stys and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40: 919-938, 2004.",null,null
,,,
375,[28] S. Schwarm and M. Ostendorf. Reading level assessment using support vector machines and statistical language models. In Proceedings of ACL2005.,null,null
,,,
376,"[29] A. J. Stenner. Measuring reading comprehension with the Lexile framework. Fourth North American Conference on Adolescent/Adult Literacy, 1996.",null,null
,,,
377,"[30] V. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.",null,null
,,,
378,"[31] X. Wan, J. Yang and J. Xiao. Using cross-document random walks for topic-focused multi-documetn summarization. In Proceedings of WI2006.",null,null
,,,
379,[32] X. Wan and J. Yang. Multi-document summarization using cluster-based link analysis. In Proceedings of SIGIR-08.,null,null
,,,
380,"[33] X. Wan, J. Yang and J. Xiao. Towards an Iterative Reinforcement Approach for Simultaneous Document Summarization and Keyword Extraction. In Proceedings of ACL2007.",null,null
,,,
381,"[34] K.-F. Wong, M. Wu and W. Li. Extractive summarization using supervised and semi-supervised learning. In Proceedings of COLING-08.",null,null
,,,
382,498,null,null
,,,
383,,null,null

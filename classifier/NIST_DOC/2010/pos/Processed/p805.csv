,sentence,label,data
0,Feature Subset Non-Negative Matrix Factorization and its Applications to Document Understanding,null,null
1,Dingding Wang,null,null
2,School of Computer Science Florida International University,null,null
3,"Miami, FL 33199",null,null
4,dwang003@cs.fiu.edu,null,null
5,Chris Ding,null,null
6,CSE Department University of Texas at Arlington,null,null
7,"Arlington, TX 76019",null,null
8,chqding@uta.edu,null,null
9,Tao Li,null,null
10,School of Computer Science Florida International University,null,null
11,"Miami, FL 33199",null,null
12,taoli@cs.fiu.edu,null,null
13,ABSTRACT,null,null
14,"In this paper, we propose feature subset non-negative matrix factorization (NMF), which is an unsupervised approach to simultaneously cluster data points and select important features. We apply our proposed approach to various document understanding tasks including document clustering, summarization, and visualization. Experimental results demonstrate the effectiveness of our approach for these tasks. Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval. General Terms: Algorithms, Experimentation. Keywords: Feature subset selection, NMF.",null,null
15,1. INTRODUCTION,null,null
16,"Keyword (Feature) selection enhances and improves many IR tasks such as document categorization, automatic topic discovery, etc. Many supervised keyword selection techniques have been developed for selecting keywords for classification problems. In this paper, we propose an unsupervised approach that combines keyword selection and document clustering (topic discovery) together.",null,null
17,The proposed approach extends non-negative matrix factorization (NMF) by incorporating a weight matrix to indicate the importance of the keywords. This work considers both theoretically and empirically feature subset selection for NMF and draws the connection between unsupervised feature selection and data clustering.,null,null
18,"The selected keywords are discriminant for different topics in a global perspective, unlike those obtained in co-clustering, which typically associate with one cluster strongly and are absent from other clusters. Also, the selected keywords are not linear combinations of words like those obtained in Latent Semantic Indexing (LSI): our selected words provide clear semantic meanings of the key features while LSI features combine different words together and are not easy to interpret. Experiments on various document understanding applications demonstrate the effectiveness of our proposed approach.",null,null
19,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
20,2. FEATURE SUBSET NON-NEGATIVE MA-,null,null
21,TRIX FACTORIZATION (FS-NMF),null,null
22,"Let X ,"" {x1, · · · , xn) contains n documents with m keywords (features). In general, NMF factorizes the input nonnegative data matrix X into two nonnegative matrices,""",null,null
23,"X  F GT ,",null,null
24,"where G  Rn+×k is the cluster indicator matrix for clustering columns of X and F ,"" (f1, · · · , fk)  R+m×k contains k cluster centroids. In this paper, we propose a new objective""",null,null
25,to simultaneously factorize X and rank the features in X as,null,null
26,follows:,null,null
27,W,null,null
28,min,null,null
29,"0,F 0,G0",null,null
30,||X,null,null
31,-,null,null
32,F GT,null,null
33,||2W,null,null
34,",",null,null
35,s.t.,null,null
36,"Wj , 1",null,null
37,(1),null,null
38,j,null,null
39,"where W  Rm + ×m which is a diagonal matrix indicating the weights of the rows (keywords or features) in X, and  is a",null,null
40,parameter (set to 0.7 empirically).,null,null
41,2.1 Optimization,null,null
42,We will optimize the objective with respect to one variable while fixing the other variables. This procedure repeats until convergence.,null,null
43,2.1.1 Computation of W,null,null
44,Optimizing Eq.(1) with respect to W is equivalent to op-,null,null
45,timizing,null,null
46,"J1 ,"" Wibi - ( Wi - 1), bi "", (X - F GT )2ij .",null,null
47,i,null,null
48,i,null,null
49,j,null,null
50,"Now,",null,null
51,setting,null,null
52,J1 Wi,null,null
53,", bi - Wi-1",null,null
54,","" 0,""",null,null
55,we,null,null
56,obtain,null,null
57,the,null,null
58,fol-,null,null
59,lowing updating formula,null,null
60,1,null,null
61,Wi,null,null
62,",",null,null
63,i,null,null
64,1,null,null
65,bi-1,null,null
66,1,null,null
67,bi-1,null,null
68,(2),null,null
69,2.1.2 Computation of G,null,null
70,Optimizing Eq.(1) with respect to G is equivalent to optimizing,null,null
71,"J2 , T r(XT W T X - 2GF T W T X + F T W T F GT G).",null,null
72,Setting,null,null
73,J2 G,null,null
74,",",null,null
75,-2XT W F,null,null
76,+ 2GF T W T F,null,null
77,",",null,null
78,"0,",null,null
79,we,null,null
80,obtain,null,null
81,the,null,null
82,following updating formula,null,null
83,Gik,null,null
84,Gik,null,null
85,(XT W F )ik (GF T W F )ik,null,null
86,(3),null,null
87,The correctness and convergence of this updating rule can,null,null
88,be rigorously proved. Details are skipped here.,null,null
89,805,null,null
90,2.1.3 Computation of F,null,null
91,Optimizing Eq.(1) with respect to F is equivalent to optimizing,null,null
92,"J3 , T r[W XXT - 2W XGF T + W F GT GF ].",null,null
93,Setting,null,null
94,J3 F,null,null
95,",",null,null
96,-2W XG + W F GT G + (GT GF T W )T,null,null
97,","" 0,""",null,null
98,we,null,null
99,obtain the following updating formula,null,null
100,Fik,null,null
101,Fik,null,null
102,(W XG)ik (W F GT G)ik,null,null
103,(4),null,null
104,3. EXPERIMENTS,null,null
105,3.1 Document Clustering,null,null
106,"First of all, we examine the clustering performance of FS-NMF using four text datasets as described in Table 1, and compare the results of FS-NMF with seven widely used document clustering methods: (1) K-means; (2) PCA-Km: PCA is firstly applied to reduce the data dimension followed by the K-means clustering; (3) LDA-Km [2]: an adaptive subspace clustering algorithm by integrating linear discriminant analysis (LDA) and K-means; (4)Euclidean coclustering (ECC) [1]; (5) minimum squared residueco-clustering (MSRC) [1]; (6) Non-negative matrix factorization (NMF) [5]; (7) Spectral Clustering with Normalized Cuts (Ncut) [9]. More description of the datasets can be found in [6]. The accuracy evaluation results are presented in Figure 2.",null,null
107,Datasets CSTR Log Reuters,null,null
108,WebACE,null,null
109,# Samples 475 1367 2900 2340,null,null
110,# Dimensions 1000 200 1000 1000,null,null
111,# Class 4 8 10 20,null,null
112,Table 1: Dataset descriptions.,null,null
113,K-means PCA-Km LDA-Km,null,null
114,ECC MSRC NMF Ncut FS-NMF,null,null
115,WebACE,null,null
116,0.4081 0.4432 0.4774 0.4081 0.4432 0.4774 0.4513 0.5577,null,null
117,Log,null,null
118,0.6979 0.6562 0.7198 0.7228 0.5655 0.7608 0.7574 0.7715,null,null
119,Reuters,null,null
120,0.4360 0.3925 0.5142 0.4968 0.4516 0.4047 0.4890 0.4697,null,null
121,CSTR,null,null
122,0.5210 0.5630 0.5630 0.5210 0.5630 0.5630 0.5435 0.6996,null,null
123,Table 2: Clustering accuracy on text datasets.,null,null
124,"From the results, we clearly observe that FS-NMF outperforms other document clustering algorithms in most of the cases, and the effectiveness of FS-NMF for document clustering is demonstrated.",null,null
125,3.2 Document Summarization,null,null
126,"In this set of experiments, we apply our FS-NMF algorithm on document summarization. Let X be the documentsentence matrix, which can be generated from the documentterm and sentence-term matrices, and now each feature (column) in X represents a sentence. Then the sentences can be ranked based on the weights in W . Top-ranked sentences are included into the final summary. We use the DUC benchmark dataset (DUC2004) for generic document summarization to compare our method with other state-ofart document summarization methods using ROUGE evaluation toolkit [7]. The results are demonstrated in Table 3.",null,null
127,Systems,null,null
128,DUC Best Random Centroid [8] LexPageRank [3] LSA [4] NMF [5] FS-NMF,null,null
129,R-1,null,null
130,0.382 0.318 0.367 0.378 0.341 0.367 0.388,null,null
131,R-2,null,null
132,0.092 0.063 0.073 0.085 0.065 0.072 0.101,null,null
133,R-L,null,null
134,0.386 0.345 0.361 0.375 0.349 0.367 0.381,null,null
135,R-W,null,null
136,0.133 0.117 0.124 0.131 0.120 0.129 0.139,null,null
137,R-SU,null,null
138,0.132 0.117 0.125 0.130 0.119 0.129 0.134,null,null
139,Table 3: Overall performance comparison on DUC2004 data using ROUGE evaluation methods.,null,null
140,"From the results, we observe that the summary generated by FS-NMF outperforms those created by other methods, and the scores are even higher than the best results in DUC competition. The good results benefit from good sentence feature selection in FS-NMF.",null,null
141,3.3 Visualization,null,null
142,"In this set of experiments, we calculate the pairwise document similarity using the top 20 word features selected by different methods, and Figure 1 demonstrates the document similarity matrix visually. Note that in the document dataset (CSTR dataset), we order the documents based on their class labels.",null,null
143,50,null,null
144,100,null,null
145,150,null,null
146,200,null,null
147,250,null,null
148,300,null,null
149,350,null,null
150,400,null,null
151,50,null,null
152,100,null,null
153,150,null,null
154,200,null,null
155,250,null,null
156,300,null,null
157,350,null,null
158,400,null,null
159,50,null,null
160,100,null,null
161,150,null,null
162,200,null,null
163,250,null,null
164,300,null,null
165,350,null,null
166,400,null,null
167,450,null,null
168,50,null,null
169,100 150 200 250 300 350 400 450,null,null
170,50,null,null
171,100,null,null
172,150,null,null
173,200,null,null
174,250,null,null
175,300,null,null
176,350,null,null
177,400,null,null
178,450,null,null
179,50,null,null
180,100 150 200 250 300 350 400 450,null,null
181,(a) FS-NMF,null,null
182,(b) NMF,null,null
183,(c) LSI,null,null
184,Figure 1: Visualization results on the CSTR Dataset. Note that CSTR has 4 clusters.,null,null
185,"From Figure 1, we have the following observations. (1) Word features selected by FS-NMF can effectively reflet the document distribution. (2) NMF tends to select some irrelevant or redundant words thus Figure 1(b) shows no obvious patterns at all. (3) LSI can also find meaningful words, however, the first two clusters are not clearly discovered in Figure 1(c).",null,null
186,Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants DMS-0915110 and DMS-0915228.,null,null
187,4. REFERENCES,null,null
188,"[1] H. Cho, I. Dhillon, Y. Guan, and S. Sra. Minimum sum squared residue co-clustering of gene expression data. In Proceedings of SDM 2004.",null,null
189,"[2] C. Ding and T. Li. Adaptive dimension reduction using discriminant analysis and k-means c lustering. In ICML, 2007.",null,null
190,"[3] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.",null,null
191,"[4] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.",null,null
192,"[5] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In NIPS, 2000.",null,null
193,"[6] T. Li and C. Ding. The relationships among various nonnegative matrix factorization methods for clustering. In ICDM, 2006.",null,null
194,"[7] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.",null,null
195,"[8] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.",null,null
196,"[9] S. X. Yu and J. Shi. Multiclass spectral clustering. In ICCV, 2003.",null,null
197,806,null,null
198,,null,null

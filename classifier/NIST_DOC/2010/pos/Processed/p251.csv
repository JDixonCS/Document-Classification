,sentence,label,data
0,Geometric Representations for Multiple Documents,null,null
1,Jangwon Seo jangwon@cs.umass.edu,null,null
2,W. Bruce Croft croft@cs.umass.edu,null,null
3,Center for Intelligent Information Retrieval Department of Computer Science,null,null
4,"University of Massachusetts, Amherst Amherst, MA 01003",null,null
5,ABSTRACT,null,null
6,"Combining multiple documents to represent an information object is well-known as an effective approach for many Information Retrieval tasks. For example, passages can be combined to represent a document for retrieval, document clusters are represented using combinations of the documents they contain, and feedback documents can be combined to represent a query model. Various techniques for combination have been introduced, and among them, representation techniques based on concatenation and the arithmetic mean are frequently used. Some recent work has shown the potential of a new representation technique using the geometric mean. However, these studies lack a theoretical foundation explaining why the geometric mean should have advantages for representing multiple documents. In this paper, we show that the arithmetic mean and the geometric mean are approximations to the center of mass in certain geometries, and show empirically that the geometric mean is closer to the center. Through experiments with two IR tasks, we show the potential benefits for geometric representations, including a geometry-based pseudo-relevance feedback method that outperforms state-of-the-art techniques.",null,null
7,Categories and Subject Descriptors,null,null
8,H.3.3 [Information Search and Retrieval]: Retrieval Models,null,null
9,General Terms,null,null
10,"Algorithms, Measurement, Experimentation",null,null
11,Keywords,null,null
12,"multiple documents, information geometry, geometric mean",null,null
13,1. INTRODUCTION,null,null
14,"A typical goal in Information Retrieval (IR) is to find relevant documents, where we rank the documents using a representation for a single document. Often, however, a",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",null,null
16,"representation for multiple documents is needed. For example, tasks such as relevance feedback, passage retrieval and resource selection in distributed information retrieval or in aggregated search, use representations for sets of multiple documents.",null,null
17,"One of standard approaches for relevance feedback is to estimate an underlying relevance model from given feedback documents and sample likely terms from the model for query expansion. That is, the estimated underlying model can be considered as a representation of the feedback documents. In passage retrieval, representations of text passages can be used to rank passages or documents. In the latter case, we represent a document using a combination of some or all of its passages. In resource selection tasks, the resource or collection is represented using the documents in the collection.",null,null
18,"As many tasks require representations for multiple documents, various approaches have been introduced. Among them, representation techniques based on the arithmetic mean and concatenation are frequently used. Representation techniques based on the arithmetic mean literally compute the arithmetic mean of multiple language models or vector representations. Representation techniques based on concatenation make a large document by concatenating multiple documents and use a language model or vector to represent the large document.",null,null
19,"In addition to traditional group representation techniques, some recent studies show the potential of a new representation technique, the geometric mean representation of language models [26, 30, 11, 31]. Liu and Croft [26] compared various representation techniques for cluster retrieval and demonstrated that representations using the geometric mean outperformed others via empirical evaluation. Seo and Croft [30] applied a resource selection technique based on the geometric mean to blog site search. Moreover, Elsas and Carbonell [11] and Seo et al. [31] showed that a thread representation using the geometric mean of postings in the thread can be a good choice for online forum search.",null,null
20,"The previous work which uses the geometric mean to represent a group of documents, however, did not theoretically analyze the geometric mean in the language modeling framework. In other words, although they have demonstrated the performance of representation techniques based on the geometric mean empirically, theoretical evidence or the assumptions behind the geometric mean have not been sufficiently addressed to justify its use in IR.",null,null
21,"Therefore, in this paper, we give a theoretically grounded explanation for geometric mean-based techniques for representing multiple documents. To do this, we consider Information Geometry as a tool and discuss how the arithmetic mean as well as the geometric mean can be inter-",null,null
22,251,null,null
23,"preted in certain geometries. More specifically, we show that the arithmetic mean and the geometric mean relate to the Fr´echet sample mean which minimizes the Fr´echet sample function. Furthermore, we empirically show that the geometric mean is closer to the Fr´echet mean.",null,null
24,"In addition, we address two applications considering the geometric interpretation: cluster retrieval and pseudo-relevance feedback. Particularly, for pseudo-relevance feedback, we introduce a variation of the relevance model [21], the geometric relevance model, and show that this new approach performs better than the relevance model.",null,null
25,"The remainder of this paper is organized as follows. Section 2 reviews previous work. In Section 3, we introduce the Fr´echet mean and geometric representations correspond to the Fr´echet mean in two different metric spaces using Information Geometry. In Section 4, we provide empirical evidence for the geometric representations through experiments for two IR tasks. Section 5 discusses other evidence for the geometric representations. Section 6 concludes this paper.",null,null
26,2. PREVIOUS WORK,null,null
27,"Combining multiple evidence is one of the most frequently addressed topics in Information Retrieval. Belkin et al. [2] showed that different representations of the same information object leads to different results and combinations of such representations can improve retrieval performance. Various combination heuristics suggested by Fox and Shaw [12] and analyzed by Lee [23] are still used in many IR tasks such as passage retrieval and resource selection. Using passage-level evidence [7, 25, 3] for document retrieval necessarily requires combination techniques. Resource selection where a collection is represented by its own documents [6, 32] actively uses combination techniques as well.",null,null
28,"Relevance feedback (and pseudo-relevance feedback) is another task using combination-based representation techniques. To estimate a query model for query expansion, the top ranked documents are combined. Rocchio [29] introduced a feedback technique to combine positive or negative feedback documents in vector spaces. Lavrenko and Croft [21] introduced a technique that estimates a underlying relevance model in the language modeling framework. In fact, these standard relevance feedback approaches implicitly use the arithmetic mean. Recently, Collins-Thompson and Callan [9] used a parametric approach using re-sampling to estimate a posterior Dirichlet distribution for the documents. That is, they use the mean and the variance of the Dirichlet distribution to get a feedback model.",null,null
29,"The geometric mean-based representation technique was relatively recently introduced. Liu and Croft [26] demonstrated that representation by the geometric mean works well for cluster retrieval via comparisons with vairous representation techniques. Seo and Croft [30] suggested a resource selection technique by the geometric mean for blog site retrieval. Furthermore, the technique was shown to work well for thread search in online forums [11, 31]. The geometric mean is often used in other fields. For example, Kogan et al. [18] used the geometric mean for k-means clustering. Veldhuis [34] showed that a centroid of the symmetrical Kullback-Leibler divergence is related to the arithmetic mean and the normalized geometric mean.",null,null
30,"In this paper, to justify the use of the geometric mean in IR, we find evidence from Information Geometry. Rao",null,null
31,"[28] and Jeffreys [14] are the first people who considered the Fisher information metric as a Riemannian metric. Later, Efron [10] focused on differential geometry in statistics considering the curvature of statistical models. Recently, Lebanon [22] applied the theory to many machine learning tasks. See Amari and Nagaoka [1] and Kass and Vos [16] for comprehensive introduction to Information Geometry.",null,null
32,3. GEOMETRY OF MULTIPLE DOCUMENTS,null,null
33,"We introduce the Fr´echet mean and derive the mean in two different metric spaces, i.e., the Euclidean metric space and the Riemannian manifold defined by the Fisher information metric.",null,null
34,3.1 Fréchet Mean,null,null
35,"Let us consider a Riemannian manifold M with a distance measure dist(x, y) where x and y are points on the manifold. Assume that we have a distribution Q on a convex set U  M. Now we define a function F : M  R as follows:",null,null
36,"(c) ,",null,null
37,"dist2(c, p)Q(dp)",null,null
38,pU,null,null
39,"This function is known as the Fr´echet function. A set of points which minimize the function is called the Fr´echet mean set of Q. If there is only a point in the set, the point is called the Fr´echet mean. This general notation for a center or centroid associated with a probability distribution was introduced by Fr´echet [13] and Karcher [15]. This mean is called by various names, e.g., the center of mass, barycenter, Karcher mean and Fr´echet mean. In this work, we refer to this mean as the Fr´echet mean1. The concept of the Fr´echet mean is general and not limited to any specific metric; accordingly, this can be applied to any metric space. Indeed, as we will see soon, it also generalizes the ordinary Euclidean mean.",null,null
40,"Kendall [17] proved that if the support of Q is in a geodesic ball of sufficiently small radius r, then one Fr´echet mean uniquely exists. As we see later, we consider a statistical manifold for multinomial distributions, and the distributions are mapped onto a simplex or a positive sphere. Since the mapped area is sufficiently small, a unique Fr´echet mean exists. For example, in case of a sphere, the radius of the geodesic ball is /4 and the positive sphere is contained in the ball.",null,null
41,"If we have n unique points p1, p2, · · · , pn in m i.i.d. samples from distribution Q, then we consider the sample Fr´echet mean which minimizes the Fr´echet sample function given by",null,null
42,n,null,null
43,"¯ (c) ,"" dist2(c, pi)Q^(pi)""",null,null
44,(1),null,null
45,"i,1",null,null
46,where Q^ is an empirical distribution estimated from the samples.,null,null
47,"Bhattacharya and Patrangenaru [5] showed that every measurable choice from the Fr´echet sample mean set of Q^ is a strongly consistent estimator of the Fr´echet mean of Q. In this paper, we consider multiple documents to represent as samples and the Fr´echet sample mean as a representation.",null,null
48,"1Strictly speaking, this is the intrinsic Fr´echet mean in that we use a geodesic distance. However, since we address only the intrinsic Fr´echet means in this paper, we omit term ""intrinsic"".",null,null
49,252,null,null
50,1,null,null
51,2,null,null
52,0,null,null
53,0,null,null
54,0,null,null
55,0,null,null
56,0,null,null
57,0,null,null
58,1,null,null
59,1,null,null
60,2,null,null
61,2,null,null
62,"Figure 1: Assuming the Euclidean metric space, a n + 1 dimensional multinomial distribution is mapped to a point in the n-simplex in Euclidean space (left). Assuming the Riemannian manifold defined by the Fisher information metric, the same point is mapped to a point in the positive n-sphere of radius 2 (right).",null,null
63,"Therefore, we address how to compute the sample Fr´echet mean from the multiple documents in the following sections.",null,null
64,3.2 Euclidean Metric space,null,null
65,"Let's begin with the Euclidean metric space. We assume that terms observed in a document are samples from a multinomial distribution and each document has a distinct distribution. Assuming a conjugate Dirichlet prior, we estimate the multinomial distribution, i.e. a language model, using Dirichlet smoothing [35] as follows:",null,null
66,Pr(w|D),null,null
67,",",null,null
68,"tfw,D",null,null
69,+  · cfw/|C| |D| + ,null,null
70,(2),null,null
71,"where tfw,D is the occurrence of term w in document D, cfw is the occurrence of w in a set of observations C considered for the prior distribution (typically, a corpus), |D| is the number of observations, i.e. the length of D, |C| is the length of C, and  is the Dirichlet smoothing parameter. Note that P (w|D) is a parameter which corresponds to outcome w in the multinomial distribution.",null,null
72,"The size of vocabulary of a language model is defined as the number of terms observed in C, which also determines the number of dimensions of the Euclidean metric space for a multinomial distributions. When the number of dimensions is n + 1, a multinomial distribution corresponds to a point in n-simplex Pn which is defined as follows:",null,null
73,"Pn ,",null,null
74,n+1,null,null
75,"x  Rn+1 : i, x(i) > 0, x(i) , 1",null,null
76,"i,1",null,null
77,An example of 2-simplex embedded in 3-dimensional Euclidean space is shown in Figure 1.,null,null
78,"Since a geodesic linking two points in n-simplex is a straight line, the distance between two multinomial distributions is calculated by the Euclidean distance as follows:",null,null
79,"dist(x, y) ,",null,null
80,n+1,null,null
81,(x(i) - y(i))2,null,null
82,"i,1",null,null
83,"Consider multinomial distributions of k given documents, p1, p2, · · · , pk as samples from distribution Q over the nsimplex. Then, the Fr´echet sample function is given by",null,null
84,k,null,null
85,n+1,null,null
86,"¯ (c) , Q^(pi) (c(j) - p(ij))2",null,null
87,"i,1",null,null
88,"j,1",null,null
89,"Therefore, we have the following optimization problem to",null,null
90,obtain the Fr´echet sample mean.,null,null
91,minimize,null,null
92,k,null,null
93,n+1,null,null
94,Q^(pi) (c(j) - p(ij))2,null,null
95,"i,1",null,null
96,"j,1",null,null
97,n+1,null,null
98,subject to,null,null
99,"c(j) ,"" 1,""",null,null
100,"j, c(j) > 0",null,null
101,(3),null,null
102,"j,1",null,null
103,"It is trivial to solve this problem using the method of Lagrange multipliers. Finally, we have a solution as follows:",null,null
104,k,null,null
105,"c(j) ,",null,null
106,p(ij ) Q^ (pi ),null,null
107,(4),null,null
108,"i,1",null,null
109,"This is the Fr´echet sample mean in the Euclidean metric space. Indeed, if Q^(pi) is uniform, i.e, 1/k, then this is the same as the ordinary Euclidean mean or the arithmetic mean. Therefore, the Fr´echet sample mean in the Euclidean metric space generalizes the arithmetic mean.",null,null
110,We use the Fr´echet sample mean as a representative multinomial distribution for the given group of multiple documents.,null,null
111,3.3 Riemannian manifold defined by the Fisher information metric,null,null
112,"Many IR approaches assume that data is embedded in the Euclidean geometry. However, assumptions of non-Euclidean geometries may lead to a better understanding of data. We here consider a Riemannian space where a Riemannian metric is the Fisher information metric. This metric space is used for investigating the geometric structures of statistical models in most of the Information Geometry literature [28, 1, 16]. Furthermore, a number of approaches assume this metric space for statistical inference and machine learning [20, 22, 1]. Particularly, for text classification, Lafferty and Lebanon [20] showed that techniques based on this metric space perform better than techniques based on the Euclidean metric.",null,null
113,The Fisher information metric is defined as follows:,null,null
114,"gi,j() ,",null,null
115,log p(x;  (i),null,null
116,),null,null
117,log p(x;  (j ),null,null
118,),null,null
119,p(x;,null,null
120,)dx,null,null
121, log p(x; )  log p(x; ),null,null
122,", E",null,null
123, (i),null,null
124, (j ),null,null
125,"where  is a point in a differential manifold and corresponds to a statistical model in a parametric familty p(x; ), i and j are indices for a coordinate system. In this work, it is easy",null,null
126,253,null,null
127,to think that  is a multinomial model for a document while i and j are indices for unique terms in vocabulary.,null,null
128,"This metric has some nice properties. By Cram´er-Rao inequality [28], the variance of unbiased estimators is bounded by the inverse of the metric. Particularly, an unbiased estimator achieving the bound is called an efficient estimator which is the best unbiased estimator because it minimizes the variance. Furthermore, by Chentsov's theorem [8], the Fisher information metric is the only Riemannian metric which is invariant under basic probabilistic transformations.",null,null
129,"We now look into the Riemannian geometry with the Fisher information metric as a Riemannian metric. First of all, let us consider the positive n-sphere of radius 2, S~n+ instead of n-simplex Pn.",null,null
130,"S~n+ ,",null,null
131,n+1,null,null
132,"x  Rn+1 : i, x(i) > 0, (x(i))2 , 22",null,null
133,"i,1",null,null
134,Figure 1 shows an example of the positive 2-sphere of radius,null,null
135,2. We can define transformation  : Pn  S~n+ by,null,null
136," z(j) , (x)(j) , 2 x(j)",null,null
137,"The inverse transformation -1 is well known to pull back the Fisher information metric on Pn to the Euclidean metric on S~n+ [16, 22]. Therefore, the transformation is an isometry, and we can compute the distance between two statistical models by the Fisher information metric using the geodesic distance between two corresponding points on the sphere. In other words, the distance is the length of the shortest curve linking two corresponding points on the sphere and is given by",null,null
138,n+1,null,null
139,"dist(x, y) , 2 arccos",null,null
140,"j,1",null,null
141,x(j ) y (j ),null,null
142,"This is called the information distance. With this distance, we have the following Fr´echet sample",null,null
143,function.,null,null
144,k,null,null
145,n+1,null,null
146,"¯ (c) , 4 arccos2",null,null
147,x(j)y(j) Q^(pi),null,null
148,"i,1",null,null
149,"j,1",null,null
150,"Unfortunately, there is no closed form solution for the Fr´echet sample mean which minimizes this function. Although we can use some convex optimization techniques, such approaches may be impractical in case that n is large. Indeed, in many IR tasks, n + 1 is the size of vocabulary and can be very large.",null,null
151,"Therefore, to find the Fr´echet sample mean, we try an approximation approach using the Kullback-Leibler (KL) divergence which is defined as follows:",null,null
152,D(x||y),null,null
153,",",null,null
154,n+1,null,null
155,x(j),null,null
156,log,null,null
157,x(j) y(j),null,null
158,"j,1",null,null
159,"As y  x, approximately by the Taylor expansion,",null,null
160,log x(j) - log y(j),null,null
161,",",null,null
162,-,null,null
163,(y,null,null
164,(j) - x(j)) x(j),null,null
165,+,null,null
166,(y(j) - x(j))2 2(x(j))2,null,null
167,+ O((y(j),null,null
168,- x(j))3),null,null
169,"From this,",null,null
170,D(x||y) + D(y||x),null,null
171,n+1,null,null
172,",",null,null
173,x(j) log x(j) - log y(j) + y(j) log y(j) - log x(j),null,null
174,"j,1",null,null
175,",",null,null
176,1 2,null,null
177,n+1,null,null
178,(y(j) - x(j))2 x(j),null,null
179,+,null,null
180,1 2,null,null
181,n+1,null,null
182,(x(j) - y(j))2 y(j),null,null
183,+,null,null
184,O(||y,null,null
185,-,null,null
186,x||3),null,null
187,"j,1",null,null
188,"j,1",null,null
189,(5),null,null
190,"Since y approaches x along geodesic c linking them, we can parameterize the path by arclength s so that c(s0) ,"" x, c(s1) "", y and s1 - s0 ,"" dist(x, y). The difference between two points is expressed by a product of the geodesic length and the tangent vector to the curve as follows:""",null,null
191,y(j) - x(j),null,null
192,",",null,null
193,(s1,null,null
194,-,null,null
195,s0),null,null
196, c(j ) s,null,null
197,","" dist(x, y) c(j) s""",null,null
198,"Then, the first term in Equation (5) can be rewritten as follows:",null,null
199,1 n+1 1,null,null
200,2,null,null
201,x(j),null,null
202,"j,1",null,null
203," c(j ) dist(x, y)",null,null
204,s,null,null
205,2,null,null
206,",",null,null
207,"1 dist2(x, y) n+1 2",null,null
208,1 c(j)(s),null,null
209,"j,1",null,null
210,c(j) 2 s,null,null
211,","" 1 dist2(x, y) n+1 c(j)(s)  log c(j) 2 "","" 1 dist2(x, y)I(s)""",null,null
212,2,null,null
213,s,null,null
214,2,null,null
215,"j,1",null,null
216,"where I(s) is the Fisher information for s. By definition of the length of the curve,",null,null
217,s1,null,null
218,"I(s)ds ,"" dist(x, y) "", s1 - s0",null,null
219,s0,null,null
220,"Hence, I(s) ,"" 1, and we finally have the following:""",null,null
221,1 2,null,null
222,n+1,null,null
223,(y(j) - x(j))2 x(j),null,null
224,",",null,null
225,"1 dist2(x, y) 2",null,null
226,(6),null,null
227,"j,1",null,null
228,"Similarly, the second term in Equation (5) can be also written as Equation (6). Therefore, we have an approximation of Equation (5) as follows:",null,null
229,"D(x||y) + D(y||x) ,"" dist2(x, y) + O(||y - x||3)  dist2(x, y)""",null,null
230,"Similar relationships between divergences and distances can be founded in various texts [1, 16].",null,null
231,"From this approximation, we can express the Fr´echet sample mean with the KL divergence as follows:",null,null
232,k,null,null
233,¯ (c)  (D(pi||c) + D(c||pi)) Q^(pi),null,null
234,(7),null,null
235,"i,1",null,null
236,This means that finding the Fr´echet sample mean is reduced to finding the symmetrized Bregman centroid cF [27] which,null,null
237,is defined as follows:,null,null
238,"cF , arg min c",null,null
239,k,null,null
240,1 2,null,null
241,(DF,null,null
242,(pi||c),null,null
243,+,null,null
244,DF,null,null
245,(c||pi)),null,null
246,Q^(pi),null,null
247,"i,1",null,null
248,"where DF (x||y) is the Bregman divergence defined by F (x)- F (y)- x-y, F (y) and F is a generator function. For example, if F is the negative Shannon entropy, i.e. j x(j) log x(j),",null,null
249,254,null,null
250,then the Bregman divergence is the same as the KL diver-,null,null
251,"gence. That is, the Bregman divergence is a generalized divergence. In addition, right-sided centroid cFR and left-sided centroid cFL are defined as follows:",null,null
252,k,null,null
253,cFR,null,null
254,",",null,null
255,arg,null,null
256,min c,null,null
257,DF (pi||c)Q^(pi),null,null
258,"i,1",null,null
259,k,null,null
260,cFL,null,null
261,",",null,null
262,arg min c,null,null
263,DF (c||pi)Q^(pi),null,null
264,"i,1",null,null
265,Nielsen and Nock [27] show that symmetrized Bregman centroid cF lies on a geodesic linking cFR and cFL via the Bregman Pythagoras' theorem. We can apply the result to,null,null
266,the KL divergence. We can easily compute cFR using the method of Lagrange,null,null
267,"multipliers with the same constraints as Equation (3), and",null,null
268,the solution coincides with the arithmetic mean as follows:,null,null
269,k,null,null
270,"cFR(j) ,",null,null
271,Q^ (pi )p(ij ),null,null
272,"i,1",null,null
273,"Similarly, using the method of Lagrange multipliers, we compute cFL as follows:",null,null
274,k,null,null
275,"cFL (j) ,",null,null
276,"i,1",null,null
277,p(ij ) Q^ (pi ),null,null
278,n+1 k,null,null
279,/,null,null
280,"j,1 i,1",null,null
281,p(ij ) Q^ (pi ),null,null
282,"If Q^ ,"" 1/k, then this is the ordinary normalized geometric mean.""",null,null
283,"Therefore, the symmetrized Bregman centroid when F is the negative Shannon entropy, or the approximated Fr´echet sample mean lies on the geodesic linking the arithmetic mean and the normalized geometric mean.",null,null
284,We consider the two means as approximations to the Fr´echet sample mean and take the following approach to decide a representation among them:,null,null
285,1. Compute the arithmetic mean cA and the normalized geometric mean cG from multinomial models of multiple documents.,null,null
286,2. Compute ¯ (cA) and ¯ (cG) by Equation (1),null,null
287,"3. As a representation, choose cG if ¯ (cA) > ¯ (cG), cA otherwise.",null,null
288,"That is, we choose a point which is closer to the Fr´echet sample mean as a representation. We call this approach ""geometric selection"".",null,null
289,4. EXPERIMENTS,null,null
290,"To evaluate representation techniques derived in the previous section, we conduct experiments for two different tasks: cluster retrieval and pseudo-relevance feedback.",null,null
291,"For the experiments, we use 3 standard collections from TREC. Table 1 shows the statistics of the collections. To estimate a language model from each document, we use the Dirichlet smoothing. For each task, the initial results are obtained by query-likelihood scores which are computed under an independence assumption as follows:",null,null
292,"P (Q|D) , P (q|D)",null,null
293,qQ,null,null
294,where P (q|D) is estimated by Equation (2).,null,null
295,AP,null,null
296,WSJ,null,null
297,GOV2,null,null
298,TREC topics 51-200 51-200 701-800,null,null
299,#docs,null,null
300,"242,918 173,252 25,205,179",null,null
301,Table 1: Test collections.,null,null
302,"For index building, we used the Indri system [33]. Each document was stemmed by the Krovetz stemmer and stopped by a standard stopword set. To test the significance of results, we performed a randomization test.",null,null
303,4.1 Cluster Retrieval,null,null
304,"Cluster retrieval involves finding the best document cluster [24, 26]. We first retrieve the top 100 documents for each query according to query-likelihood scores. Next, we perform kNN clustering [19]. That is, assuming that each returned document is a cluster centroid, a cluster is formed by its k - 1 nearest neighbors (k is set to 5). We use cosine similarity as a similarity measure. In fact, since cosine similarity assumes the Euclidean metric space, other similarity measures may perform better for our representation technique which assumes a different metric. However, since arbitrary clusters are assumed in cluster retrieval, we use the same similarity measure as used in previous work [26].",null,null
305,"Once we have clusters, we represent each cluster by the arithmetic mean of language models of documents in a cluster assuming the Euclidean metric. On the other hand, assuming the Fisher information metric, we can determine a representation via geometric selection between the arithmetic mean and the normalized geometric mean of the documents.",null,null
306,"Evaluation of various representation techniques such as concatenation or CombMax [12] for cluster retrieval has been already done by Liu and Croft [26]. They concluded that the geometric mean representation outperforms other techniques. Therefore, we do not intend to repeat the same work. Instead, we focus on geometric interpretations for experimental results.",null,null
307,"For a fair comparison, the same clusters are given to each representation technique. The only parameter to be tuned is the smoothing parameter for the initial results. We set the parameter so that Mean Average Precision (MAP) for the initial results by the query-likelihood P (Q|D) is maximized. Evaluation is performed using all topics. Since our goal is to find the best cluster, we use Precision at 5 (P@5) in order to evaluate the cluster first ranked by each representation technique, i.e. how many relevant documents the cluster has. Table 2 shows the results. In addition to the arithmetic mean and geometric selection, we present results using the geometric mean as well.",null,null
308,"For all collections, representations by the geometric mean and geometric selection show better performance than representations by the arithmetic mean. Except for GOV2, The improvements are statistically significant. These experiments indicate some interesting points. First, in geometric selection, the normalized geometric means were selected as representations which minimize the Fr´echet sample function for all queries across all collections. In other words, the normalized geometric means are better approximations to the Fr´echet sample mean. Second, since the normalized geometric means selected by geometric selection lead to consistently better retrieval results, we may say that the goodness of a representation for this task is related to how close the rep-",null,null
309,255,null,null
310,A-MEAN G-MEAN SELECT,null,null
311,AP,null,null
312,0.3053 0.3347 0.3347,null,null
313,WSJ,null,null
314,0.4747 0.5040 0.5027,null,null
315,GOV2 0.5374 0.5576 0.5556,null,null
316,"Table 2: Results for cluster retrieval. A-MEAN, GMEAN and SELECT mean representations by the arithmetic mean, by the geometric mean, and by geometric selection, respectively. The numbers are P@5 scores. A * indicates a statistically significant improvement over A-MEAN (p < 0.05).",null,null
317,"resentation is to the center of mass, i.e. the Fr´echet sample mean. Moreover, this justifies the assumption of the geometry defined by the Fisher information metric. Lastly, since geometric selection does not consider the geometric mean but the normalized geometric mean, the results in the `SELECT' row are exactly the same as those by the normalized geometric means. Therefore, the differences between the `G-MEAN' row and the `SELECT' row are caused by the normalization. As you see, since the differences are small, we suggest that the geometric mean without normalization can be a better choice in practice.",null,null
318,4.2 Pseudo-Relevance Feedback,null,null
319,"Lavrenko and Croft's relevance model [21] is one of the standard language modeling approaches for pseudo-relevance feedback. The model assumes that the top k retrieved documents for query q are sampled from an underlying relevance model for q. That is, a hidden multinomial model relevant to a user information need exists, and we estimate the model from the top k documents. Then, we sample terms which describe the information need better than the original query and use the terms for query expansion.",null,null
320,Estimation of the relevance model is done by the following formula:,null,null
321,"P (w|q) ,",null,null
322,"k i,1",null,null
323,p(w|Di,null,null
324,)P,null,null
325,(q|Di,null,null
326,)P,null,null
327,(Di),null,null
328,(8),null,null
329,p(q),null,null
330,"where q is a user query, w is a candidate for expansion terms, and Di is a document in the top k initial results, respectively.",null,null
331,"Although this is derived from a Bayesian model, we can see this as a representation for the top k documents by the arithmetic mean rewriting Equation (8) as follows:",null,null
332,k,null,null
333,p(w|Di,null,null
334,),null,null
335,P,null,null
336,(q|Di)P p(q),null,null
337,(Di,null,null
338,),null,null
339,",",null,null
340,k,null,null
341,p(w|Di)P (Di|q),null,null
342,"i,1",null,null
343,"i,1",null,null
344,"This has the same form as the weighted arithmetic mean of Equation (4). In other words, P (w|Di) is a multinomial parameter and P (Di|q) represents a distribution over a sample space limited by q, i.e, Q^. In the standard implementation",null,null
345,"of the relevance model by the Indri system [33], P (D) is",null,null
346,"assumed to be uniform. Hence,",null,null
347,"P (Di|q) ,",null,null
348,P (q|Di)P (D),null,null
349,"k i,1",null,null
350,P,null,null
351,(q|Di,null,null
352,)P,null,null
353,(D),null,null
354,",",null,null
355,P (q|Di),null,null
356,"k i,1",null,null
357,P,null,null
358,(q|Di,null,null
359,),null,null
360,"That is, the weight Q^ ,"" P (Di|q) is the normalized querylikelihood scores obtained in the initial retrieval phase. Therefore, we can say that the relevance model represents a group of the top k documents combining the language models by the arithmetic mean weighted by the initial search results.""",null,null
361,RM GRM,null,null
362,AP,null,null
363,0.2541 0.2769,null,null
364,WSJ,null,null
365,0.3531 0.3851,null,null
366,GOV2,null,null
367,0.3204 0.3300,null,null
368,"Table 3: Results for pseudo-relevance feedback. RM and GRM mean the relevance model and the geometric relevance model, respectively. The numbers are MAP scores. A * indicates a statistically significant improvement over RM (p < 0.01).",null,null
369,"In this sense, we can say that the relevance model implicitly assumes the Euclidean metric space.",null,null
370,We can replace the arithmetic mean by the normalized geometric mean to develop a new representation as follows:,null,null
371,k,null,null
372,"P (w|q) , p(w|Di)P (Di|q)/",null,null
373,k,null,null
374,p(w|Di)P (Di|q) (9),null,null
375,"i,1",null,null
376,"wV i,1",null,null
377,"We can consider the original relevance model and this model as two approximated representations in the Riemannian manifold defined by the Fisher information metric. To determine a representation, we use geometric selection and call the selected model the ""geometric relevance model"".",null,null
378,"We compare the geometric relevance model with the relevance model. For each query, we first retrieve the top k documents by query-likelihood scores and build a relevance model or geometric relevance model for the documents. Then, we choose the top M terms according to probabilities of the terms in the models. Finally, we expand the original query combining the expansion terms using an interpolation weight  in the Indri query language. The paremeters k, M and  are tuned so that MAP scores by the relevance model are maximized. The same parameters are used for the geometric relevance model. Topic 51-150 for AP and WSJ and topic 701-750 for GOV2 are used as training topics to learn the parameters. Topic 151-200 for AP and WSJ and topic 751800 for GOV2 are used as test topics. We retrieve up to 1000 results for each expanded query and use MAP as the evaluation metric.",null,null
379,"Table 3 shows the results. The geometric relevance model significantly outperforms the relevance model for all three collections. Similar to cluster retrieval, geometric selection selected models by Equation (9) rather than the original relevance model as representations for all queries except for three queries of GOV2. That is, the geometric mean is a better approximation to the center of mass for this task. This provides more empirical evidence that the geometric mean can be an appropriate choice for representation.",null,null
380,5. DISCUSSIONS,null,null
381,5.1 Visualization of geometries,null,null
382,"To show how multiple documents, the arithmetic mean and the normalized geometric mean are distributed in each geometry, we use the following visualization. First, we construct a weighted complete graph, where each node is a document or the mean and a weight is determined by a kernel reflecting each geometry.",null,null
383,"For the Euclidean metric, we use the following heat kernel:",null,null
384,"K(x1, x2) , exp",null,null
385,n+1,null,null
386,-,null,null
387,x(1j) - x(2j),null,null
388,2,null,null
389,/4t,null,null
390,"j,1",null,null
391,256,null,null
392,"Figure 2: Geometric visualization of the top 20 documents for Topic 770 (GOV2), the arithmetic mean (AM) and the normalized geometric mean (GM) for different metrics, i.e. the Euclidean metric (left) and the Fisher information metric (right).",null,null
393,TxM,null,null
394,x,null,null
395,V m' V,null,null
396,M,null,null
397,m,null,null
398,y',null,null
399,y,null,null
400,Figure 3: Determinination of a middle point m on a geodesic linking x and y,null,null
401,"where t is a time parameter. For the Fisher information metric, we use the following",null,null
402,information diffusion kernel [20]:,null,null
403,n+1,null,null
404,"K(x1, x2) , exp - arccos2",null,null
405,x(1j)x(2j) /4t,null,null
406,"j,1",null,null
407,"We visualize each geometry using CCVisu [4] which is a tool implementing energy models so that the higher weight between two points results in the smaller Euclidean distance between them. A visualization example is shown in Figure 2. As you see, the arithmetic mean appears closer to the center in the Euclidean metric space while the normalized geometric mean appears closer in the Riemannian manifold defined by the Fisher information metric. Since the visualization tool uses random seeds to initialize the layout, the results vary every time. However, the trend for the locations of the means was consistent.",null,null
408,5.2 More accurate estimation,null,null
409,"Geometric selection is a somewhat simple approach to determine the approximated Fr´echet sample mean. That is, we choose one among only two options: the normalized geometric mean and the arithmetic mean. We now consider a more accurate estimation technique for the Fr´echet sample mean.",null,null
410,"A point which minimizes the approximated Fr´echet sample function of Equation (7) lies on a geodesic linking the arithmetic mean and the normalized geometric mean. Let M , x, y and c be the statistical manifold defined by the Fisher information metric, the arithmetic mean, the normalized geometric mean and a geodesic linking the two points, respectively. First, we get vector V on tangent space TxM via log map logx : M  TxM . In case of a sphere, the log",null,null
411,"Figure 4: Relative locations of the more accurately estimated Fr´echet sample means. The x-axis corresponds to the relative locations, and the y-axis corresponds to queries for each collection. As a relative location is closer to 1.0, the estimated mean for the topic is located near the normalized geometric mean.",null,null
412,AP WSJ GOV2 GRM+ 0.2769 0.3852 0.3309,null,null
413,Table 4: Pseudo-relevance feedback results of the more accurately estimated Fr´echet sample mean in the Riemannian manifold defined by the Fisher information metric.,null,null
414,map is given by:,null,null
415,V (j),null,null
416,",",null,null
417,logx (y)(j ),null,null
418,",",null,null
419,"arccos( x, y ) 1 - x, y 2",null,null
420,"y(j) - x, y x(j)",null,null
421,"Then, V links x to y on TxM corresponding to y on M .",null,null
422,"m denotes a middle point between x and y on TxM , reached by V (0    1). We now get a middle point m on c via exponential map expx : TxM  M . The exponential map of a sphare is:",null,null
423,m(j),null,null
424,",",null,null
425,expx(V,null,null
426,)(j),null,null
427,",",null,null
428,cos (||V,null,null
429,||),null,null
430,+,null,null
431,sin,null,null
432,(||V ||V ||,null,null
433,||) V,null,null
434,(j),null,null
435,Figure 3 illustrates this procedure. Note that the arithmetic mean x and the geometric mean y are interchangeable in the above formulation because a sphere is symmetric.,null,null
436,"We apply this result to pseudo-relevance feedback experiments. We perform grid search on the geodesic varying  in [0,1] by step-size 0.1, and a point which minimizes the Fr´echet sample function of Equation (1) is selected as a representation. Figure 4 shows 's selected for test queries for each collection. For all test topics except for three topics of GOV2, the selected 's are equal to or greater than 0.5. That is, the more accurately estimated Fr´echet sample means are also closer to the normalized geometric mean than the arithmetic mean. Table 4 shows the results when the representations are used for pseudo-relevance feedback. All results are equal to or a little bit better than the results of the GRM in the Table 3, but not significantly. Therefore, we can say that the geometric relevance model is a reasonable approximation to the Fr´echet sample mean for this task.",null,null
437,5.3 Anoher reason for the geometric mean,null,null
438,We have addressed so far theoretical and empirical reasons explaining why the geometric mean should have advantages,null,null
439,257,null,null
440,"for many IR tasks. There can be many other explanations. One of them is the log-linearity of the geometric mean. As more documents contain a specific term, the geometric mean for the term increases exponentially while the arithmetic mean increases linearly. Accordingly, the arithmetic mean can be sensitive to a few dominant terms in a small number of documents. On the other hand, the geometric mean favors the common terms across a whole set of documents and is relatively insensitive to such a few dominant terms. This shows the robustness of the geometric mean which can lead to a good representation for multiple documents.",null,null
441,6. CONCLUSIONS,null,null
442,"Previous work which uses the geometric mean as a representation technique does not provide enough theoretical evidence explaining why the geometric mean should have advantages as a representation for IR. There are various explanations. In this work, we showed that using Information Geometry, the arithmetic mean and the normalized geometric mean are approximation points to the center of mass in the Euclidean space or in a statistical manifold. In particular, through empirical evidence, we demonstrated that the normalized geometric mean is closer to the center in the statistical manifold. In addition to this discovery, we introduced a new approach to pseudo-relevance feedback that outperformed the relevance model. For future work, we will investigate how geometric interpretations can be applied to other IR tasks. We expect that this effort will lead to not only the discovery of novel IR theories but also development of effective algorithms.",null,null
443,7. ACKNOWLEDGMENTS,null,null
444,"This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF grant #IIS0711348, and in part by NSF grant #IIS-0534383. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",null,null
445,8. REFERENCES,null,null
446,"[1] S. Amari and H. Nagaoka. Methods of Information Geometry. American Mathematical Society, 2000.",null,null
447,"[2] N. J. Belkin, C. Cool, W. B. Croft, and J. P. Callan. The effect multiple query representations on information retrieval system performance. In SIGIR '93, 1993.",null,null
448,"[3] M. Bendersky and O. Kurland. Utilizing passage-based language models for document retrieval. In ECIR '08, 2008.",null,null
449,"[4] D. Beyer. CCVisu: Automatic visual software decomposition. In Proc. Int'l Conf. on Software Engineering, 2008.",null,null
450,"[5] R. Bhattacharya and V. Patrangenaru. Nonparametic estimation of location and dispersion on riemannian manifolds. Journal of Statistical Planning and Inference, 108, 2002.",null,null
451,"[6] J. Callan. Distributed information retrieval. In W. B. Croft, editor, Advances in Information Retrieval. Kluwer Academic Publishers, 2000.",null,null
452,"[7] J. P. Callan. Passage-level evidence in document retrieval. In SIGIR '94, 1994.",null,null
453,"[8] N. N. Chentsov. Statistical Decision Rules and Optimal Inference. American Mathematical Society, 1982.",null,null
454,"[9] K. Collins-Thompson and J. Callan. Estimation and use of uncertainty in pseudo-relevance feedback. In SIGIR '07, 2007.",null,null
455,"[10] B. Efron. Defining the curvature of a statistical problem. The Annals of Statistics, 3(6).",null,null
456,"[11] J. L. Elsas and J. G. Carbonell. It pays to be picky: an evaluation of thread retrieval in online forums. In SIGIR '09, 2009.",null,null
457,"[12] E. A. Fox and J. A. Shaw. Combination of multiple searches. In TREC-2, 1994.",null,null
458,"[13] M. Fr´echet. Les ´el´ements al´eatoires de nature quelconque dans un espace distanci´e. Ann. Inst. H. Poincar´e, 10, 1948.",null,null
459,"[14] H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, 186(1007), 1946.",null,null
460,"[15] H. Karcher. Riemannian center of mass and mollifier smoothing. Communications on pure and applied mathematics, 30(5), 1977.",null,null
461,"[16] R. E. Kass and P. W. Vos. Geometrical Foundations of Asymptotic Inference. Wiley-Interscience, 1997.",null,null
462,"[17] W. Kendall. Probability, convexity, and harmonic maps with small image i: Uniqueness and fine existence. Proc. London Math. Soc., 61, 1990.",null,null
463,"[18] J. Kogan, M. Teboulle, and C. Nicholas. The entropic geometric means algorithm: An approach for building small clusters for large text datasets. In the Workshop on Clustering Large Data Sets, 2003.",null,null
464,"[19] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In SIGIR '04, 2004.",null,null
465,"[20] J. Lafferty and G. Lebanon. Diffusion kernels on statistical manifolds. The Journal of Machine Learning Research, 6, 2005.",null,null
466,"[21] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR' 01, 2001.",null,null
467,"[22] G. Lebanon. Riemannian Geometry and Statistical Machine Learning. PhD thesis, 2005.",null,null
468,"[23] J. H. Lee. Analyses of multiple evidence combination. In SIGIR '97, 1997.",null,null
469,"[24] A. Leuski. Evaluating document clustering for interactive information retrieval. In CIKM '01, 2001.",null,null
470,"[25] X. Liu and W. B. Croft. Passage retrieval based on language models. In CIKM '02, 2002.",null,null
471,"[26] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In ECIR '08, 2008.",null,null
472,"[27] F. Nielsen and R. Nock. Sided and symmetrized Bregman centroids. IEEE Transactions on Information Theory, 55(6), 2009.",null,null
473,"[28] C. Rao. Information and the accuracy attainable in the estimation of statistical parameters. Bulletin of the Calcutta Mathematical Society, 37, 1945.",null,null
474,"[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System Experiments in Automatic Document Processing. Prentice Hall, 1971.",null,null
475,"[30] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08, 2008.",null,null
476,"[31] J. Seo, W. B. Croft, and D. A. Smith. Online community search using thread structure. In CIKM '09, 2009.",null,null
477,"[32] L. Si and J. Callan. Unified utility maximization framework for resource selection. In CIKM '04, 2004.",null,null
478,"[33] T. Strohman, D. Metzler, H. Turtle, and W. B. Croft. Indri: A language model-based search engine for complex queries. In Proc. of the Intl. Conf. on Intelligence Analysis, 2005.",null,null
479,"[34] R. Veldhuis. The centroid of the symmetrical Kullback-Leibler distance. IEEE Signal Processing Letters, 9(3), 2002.",null,null
480,"[35] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In SIGIR '01, 2001.",null,null
481,258,null,null
482,,null,null

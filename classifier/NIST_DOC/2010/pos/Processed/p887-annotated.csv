,sentence,label,data
,,,
0,Author Interest Topic Model,null,null
,,,
1,Noriaki Kawamae,null,null
,,,
2,"NTT Comware 1-6 Nakase Mihama-ku Chiba-shi, Chiba 261-0023 Japan",null,null
,,,
3,kawamae@gmail.com,null,null
,,,
4,ABSTRACT,null,null
,,,
5,"This paper presents a hierarchical topic model that simultaneously captures topics and author's interests. Our proposed model, the Author Interest Topic model (AIT), introduces a latent variable with a probability distribution over topics into each document. Experiments on a research paper corpus show that AIT is very useful as a generative model.",null,null
,,,
6,Categories and Subject Descriptors,null,null
,,,
7,H.3.1 [Content Analysis and Indexing]:,null,null
,,,
8,General Terms,null,null
,,,
9,"Algorithms, experimentation",null,null
,,,
10,Keywords,null,null
,,,
11,"Topic Modeling, Latent Variable Modeling",null,null
,,,
12,1. INTRODUCTION,null,null
,,,
13,"Attention is being focused on how to model users' interests in several fields. A model of interest allows us to infer which topics each user prefers and to measure the similarity between them in terms of their interests. For example, the Author-Topic(AT) [3] groups all papers associated with a given author by using a single topic distribution associated with this author. Author-Persona-Topic(APT) [2] introduces a persona, which is also a latent variable, under a single given author. Thus, these models allow each author's documents to be divided into one or more clusters, each with its own separate topic distribution specific to that persona",null,null
,,,
14,"This paper presents the Author Interest Topic(AIT) model; it is a generalization of known author interest models such as AT and APT. AIT allows a number of possible latent variables to be associated with author's interest, while previous models limit this number. Therefore, AIT can describe a wider variety of authors' interests than other models, which reduces the perplexity. Moreover, AIT can infer the overall interest in the training data and so can assign probabilities to previously unseen documents.",null,null
,,,
15,2. AUTHOR INTEREST TOPIC MODEL,null,null
,,,
16,This section details our model. Table 1 shows the notations used in this paper. Figure 1 shows graphical models to,null,null
,,,
17,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
18,"Figure 1: Graphical models: In this figure, shaded and unshaded variables indicate observed and latent variables, respectively. An arrow indicates a conditional dependency between variables and the plates indicate a repeated sampling with the iteration number shown. This figure shows that each author produces words from a set of topics that are preferred by the author in (a), persona associated with the author in (b), each document class in (c). In learning a document written by multiple authors, AIT makes copies of the document and associates one copy with each author.",null,null
,,,
19,"describe the generative process. For modeling each author's interest, our proposal, AIT, incorporates document class cd; it provides an indicator variable that describes which mixture of topics each document d takes, into d. Accordingly, AIT represents documents of similar topics as the same document class in the same way that topic models represent cooccurrence words as the same topic variable. Therefore, the difference between AIT and AT, APT is that rather than representing author's interest as a mixture of topic variables a(AT) or Pa (APT) in each document layer, AIT represents each author's interest as a mixture of document classes a in each author layer. Although both a(AT) and Pa (APT) are associated with only authors, the document class can be shared among authors. This class allows AIT to represent documents having similar topics as the same document class by merging parameters; this reduces the number of possible parameters without losing generality. Accordingly, as the size of training data is increased, relatively fewer parameters are needed. On the contrary, the parameters of the other models track the order of authors and so experience linear growth with the size of the training data. Moreover we decide the number of latent variables following CRP [1]. Consequently, AIT increases the number of possible latent variables for explaining all authors' interests.",null,null
,,,
20,"AIT employs Gibbs sampling to perform inference approximation. In the Gibbs sampling procedure, we need to cal-",null,null
,,,
21,887,null,null
,,,
22,Table 1: Notations used in this paper,null,null
,,,
23,SYMBOL DESCRIPTION,null,null
,,,
24,A,null,null
,,,
25,number of authors,null,null
,,,
26,J,null,null
,,,
27,number of document classes,null,null
,,,
28,T,null,null
,,,
29,number of topics,null,null
,,,
30,D,null,null
,,,
31,number of documents,null,null
,,,
32,V,null,null
,,,
33,number of unique words,null,null
,,,
34,Ad,null,null
,,,
35,authors associated with document d,null,null
,,,
36,Da,null,null
,,,
37,number of documents written by author a,null,null
,,,
38,Nd,null,null
,,,
39,number of word tokens in document d,null,null
,,,
40,ai,null,null
,,,
41,author associated with ith token in document d,null,null
,,,
42,pd,null,null
,,,
43,persona associated with document d,null,null
,,,
44,cd,null,null
,,,
45,document class associated with document d,null,null
,,,
46,zdi,null,null
,,,
47,topic associated with the ith token in document,null,null
,,,
48,d,null,null
,,,
49,wdi,null,null
,,,
50,ith token in document d,null,null
,,,
51,a,null,null
,,,
52,multinomial distribution of document classes,null,null
,,,
53,specific to author a (a|  Dirichlet() ),null,null
,,,
54,j,null,null
,,,
55,multinomial distribution of topics specific to in-,null,null
,,,
56,terest j (j |  Dirichlet() ),null,null
,,,
57,t,null,null
,,,
58,multinomial distribution of words specific to,null,null
,,,
59,topic t (t|  Dirichlet() ),null,null
,,,
60,"culate the conditional distributions. The predictive distribution of adding interest class cd in documents written by author a to topic cd , j is given by",null,null
,,,
61,8 (Pt,null,null
,,,
62,njt\d +t),null,null
,,,
63,Q t,null,null
,,,
64,(njt +t ),null,null
,,,
65,">>< n , aj\d",null,null
,,,
66,Q t,null,null
,,,
67,(njt\d +t),null,null
,,,
68,(Pt,null,null
,,,
69,njt +t ),null,null
,,,
70,"P (j|c\d, a, z, , )  if j is an existing class class",null,null
,,,
71,> > :,null,null
,,,
72,j,null,null
,,,
73,(Pt njt\d +t),null,null
,,,
74,Q t,null,null
,,,
75,(njt\d +t),null,null
,,,
76,", Q t",null,null
,,,
77,(njt +t,null,null
,,,
78,),null,null
,,,
79,(Pt njt+t),null,null
,,,
80,otherwise,null,null
,,,
81,-1,null,null
,,,
82,"where naj\d represents the number of documents assigned to j in all documents written by author a, except d, and njt\di represents the total number of tokens assigned to topic t",null,null
,,,
83,"in the documents associated with document class j, except",null,null
,,,
84,token di.,null,null
,,,
85,The predictive distribution of adding word wdi in docu-,null,null
,,,
86,"ment d written by a to topic zd , t is given by",null,null
,,,
87,8,null,null
,,,
88,> > <,null,null
,,,
89,"n , ntw\di+w",null,null
,,,
90,jt\di,null,null
,,,
91,PV v,null,null
,,,
92,(ntv\di +v ),null,null
,,,
93,"P (t|j, z\di, w, , )  if t is an existing class",null,null
,,,
94,> > :,null,null
,,,
95," , ntw\di+w",null,null
,,,
96,t,null,null
,,,
97,PV v,null,null
,,,
98,(ntv\di +v,null,null
,,,
99,),null,null
,,,
100,otherwise,null,null
,,,
101,-2,null,null
,,,
102,"where ntw\di represents the total number of tokens assigned to word w in topic t, except token di, and njt\di represents the total number of tokens assigned to topic t in",null,null
,,,
103,"all tokens assigned to j, except token di.",null,null
,,,
104,3. EXPERIMENTS,null,null
,,,
105,"We focus here on the extraction of interests from given documents, and demonstrate AIT's performance as a generative model. The dataset used in our experiments consisted of research papers in the proceedings of ACM CIKM, SIGIR, KDD, and WWW gathered over the last 8 years (2001-2008). We removed stop words, numbers, and the words that appeared less than five times in the corpus. Accordingly, we obtained a total set of 3078 documents and 20286 unique words from 2204 authors. Additionally, we applied both AT and APT to this dataset for training and comparison.",Y,null
,,,
106,"In our evaluation, the smoothing parameters ,  and",null,null
,,,
107,"Table 2: Perplexity of AT, APT and AIT: This difference between AIT and APT is significant according to one-tailed t-test with the number of samples G ,"" 100. For fair comparison, the number of topic variables T was fixed at 200, the number of document classes J was fixed at 40(AIT). Results that differ significantly by t-test p < 0.01, p < 0.05 from APT are marked with '**', '*' respectively. The value of Avg means the average computing time for each iteration in gibbs sampling.""",null,null
,,,
108,Iteration AT APT AIT,null,null
,,,
109,2000 1529 1454 1321,null,null
,,,
110,4000 1488 1304 1217,null,null
,,,
111,6000 1343 1180 1103,null,null
,,,
112,8000 1339 1059 988,null,null
,,,
113,10000 1333 1027 964,null,null
,,,
114,Avg 3.2s 10.4s 11.7s,null,null
,,,
115," were set at 0.1, 10(APT),1(AIT) and 1, respectively. We ran single Gibbs sampling chains for 10000 iterations on machines with Dual Core 2.66 GHz Xeon processors.",null,null
,,,
116,"To measure the ability of a model to act as a generative model, we computed test-set perplexity under estimated parameters and compared the resulting values.",null,null
,,,
117,"Perplexity, which is widely used in the language modeling community to assess the predictive power of a model, is algebraically equivalent to the inverse of the geometric mean per-word likelihood (lower numbers are better). Table 2 shows the results of the perplexity comparison. This table shows that AIT yielded significantly lower perplexity on the test set than AT or APT, which shows that AIT is better as a topic model. This is due to the ability of AIT to allow the document class to be shared across authors and to group documents under the various topic distributions rather than grouping documents by a given author or persona under a few topic distributions. This implies that clustered documents contain less noise than otherwise. If the number of document classes is overly restricted, the difference between the observed data and the data generated by the model under test increases, which raises the perplexity.",null,null
,,,
118,4. CONCLUSION,null,null
,,,
119,"Our proposed model, AIT, supports the expression of topics in text documents and can identify the interests of authors in these documents. Future work includes extending AIT by taking other metadata such as time, references and link structure into account, for tracking the dynamics of interests and topics.",null,null
,,,
120,5. REFERENCES,null,null
,,,
121,"[1] D. J. D. Aldous. Exchangeability and related topics, volume 1117 of Lecture Notes in Math. Springer, Berlin, 1985.",null,null
,,,
122,"[2] D. Mimno and A. McCallum. Expertise modeling for matching papers with reviewers. In KDD, pages 500­509, 2007.",null,null
,,,
123,"[3] M. Steyvers, P. Smyth, M. Rosen-Zvi, and T. L. Griffiths. Probabilistic author-topic models for information discovery. In KDD, pages 306­315, 2004.",null,null
,,,
124,888,null,null
,,,
125,,null,null

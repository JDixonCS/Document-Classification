,sentence,label,data
0,Many are Better Than One: Improving Multi-Document Summarization via Weighted Consensus,null,null
1,Dingding Wang Tao Li,null,null
2,School of Computer Science Florida International University,null,null
3,"Miami, FL 33199",null,null
4,"{dwang003,taoli}@cs.fiu.edu",null,null
5,ABSTRACT,null,null
6,"Given a collection of documents, various multi-document summarization methods have been proposed to generate a short summary. However, few studies have been reported on aggregating different summarization methods to possibly generate better summarization results. We propose a weighted consensus summarization method to combine the results from single summarization systems. Experimental results on DUC2004 data sets demonstrate the performance improvement by aggregating multiple summarization systems, and our proposed weighted consensus summarization method outperforms other combination methods.",null,null
7,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval.,null,null
8,"General Terms: Algorithms, Experimentation.",null,null
9,"Keywords: Weighted consensus, summarization.",null,null
10,1. INTRODUCTION,null,null
11,"Various multi-document summarization methods base on different strategies and usually produce diverse outputs. A natural question arises: can we perform ensemble or consensus summarization by combining different summarization methods to improve summarization performance? In general, the terms of ""consensus methods"" or ""ensemble methods"" are commonly reserved for the aggregation of a number of different (input) systems. Previous research has shown that ensemble methods, by combining multiple input systems, are a popular way to overcome instability and increase performance in many machine learning tasks, such as classification, clustering and ranking. The success of ensemble methods in other learning tasks provides the main motivation for applying ensemble methods in summarization. To the best of our knowledge, so far there are only limited attempts on using ensemble methods in multi-document summarization.",null,null
12,"As a good ensemble requires the diversity of the individual members, here we study several widely used multi-document summarization systems based on a variety of strategies, and evaluate different baseline combination methods for obtaining a consensus summarizer to improve the summarization performance. Motivated from [5], we also propose a novel weighted consensus scheme to aggregate the results from",null,null
13,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
14,"individual summarization methods, in which, the relative contribution of an individual summarizer to the consensus is determined by its agreement with other members of the summarization systems. Note that usually a high degree of agreements does not automatically imply the correctness since the systems could agree on a faulty answer. However, each of the summarization systems has shown its effectiveness individually, so the agreement measure can be used in the consensus summarization.",null,null
15,2. WEIGHTED CONSENSUS SUMMARIZATION (WCS),null,null
16,2.1 Notations,null,null
17,"Suppose there are K single summarization methods, each of which produces a ranking for the sentences containing in the document collection. Then we have K ranking lists {r1, r2, · · · , rK} and ri  RN , i ,"" 1, · · · , K, where N is the total number of sentences in the documents. The task is to find a weighted consensus ranking of the sentences r with a set of weights {w1, w2, · · · , wK } assigning to each of the individual summarization methods.""",null,null
18,2.2 Formulation,null,null
19,"Our goal is to minimize the weighted distance between r and all the ri. Let w ,"" [w1, w2, . . . , wK ]T  RK . The problem can be formulated as follows.""",null,null
20,arg min,null,null
21,w,null,null
22,s.t.,null,null
23,K,null,null
24,(1 - ) wi r - ri 2 +  w 2,null,null
25,"i,1",null,null
26,K,null,null
27,"wi ,"" 1; wi  0 i,""",null,null
28,"i,1",null,null
29,where 0    1 is the regularization parameter which spec-,null,null
30,ifies the tradeoff between the minimization of the weighted,null,null
31,"distance and the smoothness enforced by w. In experiments,",null,null
32," is set to 0.3 empirically. For simplicity, we use Euclidean",null,null
33,distance to measure the discordance of the consensus rank-,null,null
34,ing r and each of individual sentence rankings ri.,null,null
35,We,null,null
36,initialize,null,null
37,wi,null,null
38,",",null,null
39,1 K,null,null
40,",",null,null
41,and,null,null
42,this,null,null
43,optimization,null,null
44,problem,null,null
45,can,null,null
46,be solved by iterating the following two steps:,null,null
47,Step 1: Solve for r while fixing w. The optimal solution,null,null
48,is,null,null
49,"the weighted average r , Step 2: Solve for w while",null,null
50,i wi fixing,null,null
51,ri. r,null,null
52,.,null,null
53,Let,null,null
54,"d ,"" [ r - r1 2, r - r2 2, · · · , r - rK 2]  RK .""",null,null
55,809,null,null
56,Note that,null,null
57,K,null,null
58,"(1 - ) wi r - ri 2 +  w 2 , (1 - )d w + w w",null,null
59,"i,1",null,null
60,",",null,null
61,w-,null,null
62,-1 2 d,null,null
63,2-,null,null
64,( - 1)2 4,null,null
65,d2,null,null
66,"For fixing r, the optimization problem becomes",null,null
67,arg min,null,null
68,w,null,null
69,w,null,null
70,-,null,null
71,-,null,null
72,1 d,null,null
73,2,null,null
74,s.t.,null,null
75,2,null,null
76,K,null,null
77,"wi , 1;",null,null
78,"i,1",null,null
79,"wi  0,",null,null
80,i,null,null
81,This is a quadratic function optimization problem with,null,null
82,linear constraints with K variables. This is a problem of,null,null
83,"just about tens of variables (i.e., weights for each input sum-",null,null
84,marization system) and thus can be computed quickly. It,null,null
85,can,null,null
86,also,null,null
87,be,null,null
88,solved,null,null
89,by,null,null
90,simply,null,null
91,projecting,null,null
92,vector,null,null
93,-1 2,null,null
94,d,null,null
95,onto,null,null
96,"(K - 1)-simplex. With step 1 and 2, we iteratively update",null,null
97,w and r until convergence. Then we sort r in ascending,null,null
98,order to get the consensus ranking.,null,null
99,3. EXPERIMENTS,null,null
100,"In the experiments, we use four typical multi-document summarization methods as individual summarizers and compare our WCS method with other eight aggregation methods. The four individual summarization methods are: (a) Centroid [7], (b) LexPageRank [1], (c) LSA [2], and (d) NMF [4]. And the baseline aggregation methods are: (1) average score (Ave Score), which normalizes and averages the raw scores from different summarization systems; (2) average rank (Ave Rank), which averages individual rankings; (3) median aggregation (Med Rank); (4) Round Robin (RR); (5) Borda Count (BC); (6) correlation-based weighting (CW), which weights individual systems by their average Kendall's Tau correlation between the ranking list they generated and all the other lists; (7) ULTRA [3], which aims to find a consensus ranking with the minimum average Spearman's distance [8] to all the individual ranking lists; (8) graph-based combination (Graph), the basic idea of which is similar to the work proposed in [9], however, we use cosine similarity so that we can compare this method with other combination methods fairly. We conduct experiments on DUC benchmark data for generic multi-document summarization and use ROUGE [6] toolkit (version 1.5.5) to measure the summarization performance.",null,null
101,3.1 Overall Summarization Performance,null,null
102,"Table 1 show Rouge-1, Rouge-2, and Rouge-SU scores of different individual and combination methods using DUC2004 data sets (intuitively, the higher the scores, the better the performance). From the results, we observe that (1) Most of the combination summarization systems outperform all the individual systems except the round robin combination. The results demonstrate that in general consensus methods can improve the summarization performance. (2) Weighted combinations (e.g., CW, ULTRA, and WCS) outperform average combination methods which treat each individual system equally. (3) Our WCS method outperforms other weighted combination methods because WCS optimizes the weighted distance between the consensus sentence ranking to individual rankings and updates the weights and consensus ranking iteratively, which is closer to the nature of consensus summarization than other approximation based weighted methods.",null,null
103,Systems,null,null
104,DUCBest Centroid LexPageRank,null,null
105,LSA NMF Ave Score Ave Rank Med Rank RR BC CW ULTRA Graph WCS,null,null
106,R-1,null,null
107,0.382 0.367 0.378 0.341 0.367 0.388 0.385 0.385 0.364 0.378 0.378 0.392 0.379 0.398,null,null
108,R-2,null,null
109,0.092 0.073 0.085 0.065 0.072 0.089 0.087 0.087 0.072 0.085 0.085 0.090 0.086 0.096,null,null
110,R-SU,null,null
111,0.132 0.125 0.130 0.119 0.129 0.132 0.131 0.131 0.126 0.129 0.131 0.133 0.132 0.135,null,null
112,Table 1: Overall performance comparison on DUC2004. Remark: DUCBest shows the best results from DUC 2004 competition.,null,null
113,3.2 Diversity of Individual Summarizers,null,null
114,"In this set of experiments, we further examine if the four individual summarization methods are complementary to each other. We use our WCS method to aggregate any three of the four summarization methods and compare the results with the aggregation utilizing all the four methods. Table 2 shows the comparison results. From the results, we observe that adding any of the four individual methods improves the summarization performance. This is because these individual summarization methods are diverse and their performance is data dependant.",null,null
115,Systems,null,null
116,Centroid+LexPageRank+LSA Centroid+LexPageRank+NMF,null,null
117,Centroid+LSA+NMF LexPageRank+LSA+NMF,null,null
118,All,null,null
119,R-1,null,null
120,0.383 0.385 0.376 0.382 0.398,null,null
121,R-2,null,null
122,0.088 0.090 0.082 0.087 0.096,null,null
123,R-SU,null,null
124,0.132 0.133 0.131 0.132 0.135,null,null
125,Table 2: WCS results on DUC2004.,null,null
126,Acknowledgements: The work is partially supported by an FIU Dissertation Year Fellowship and NSF grants IIS0546280 and DMS-0915110.,null,null
127,4. REFERENCES,null,null
128,"[1] G. Erkan and D. Radev. Lexpagerank: Prestige in multi-document text summarization. In EMNLP, 2004.",null,null
129,"[2] Y. Gong and X. Liu. Generic text summarization using relevance measure and latent semantic analysis. In SIGIR, 2001.",null,null
130,"[3] A. Klementiev, D. Roth, and K. Small. An unsupervised learning algorithm for rank aggregation. In ECML, 2007.",null,null
131,"[4] D. Lee and H. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, pages 788­791, 1999.",null,null
132,"[5] T. Li and C. Ding. Weighted consensus clustering. SIAM Data Mining, 2008.",null,null
133,"[6] C.-Y. Lin and E.Hovy. Automatic evaluation of summaries using n-gram co-occurrence statistics. In NLT-NAACL, 2003.",null,null
134,"[7] D. Radev, H. Jing, M. Stys, and D. Tam. Centroid-based summarization of multiple documents. Information Processing and Management, pages 919­938, 2004.",null,null
135,"[8] C. Spearman. The proof and measurement of association between two things. Amer. J. Psychol., 1904.",null,null
136,"[9] V. Thapar, A. A. Mohamed, and S. Rajasekaran. A consensus text summarizer based on meta-search algorithms. In Proceedings of 2006 IEEE International Symposium on Signal Processing and Information Technology, 2006.",null,null
137,810,null,null
138,,null,null

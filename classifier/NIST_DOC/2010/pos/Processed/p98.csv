,sentence,label,data
0,A Joint Probabilistic Classification Model for Resource Selection,null,null
1,"Dzung Hong , Luo Si",null,null
2,Department of Computer Science Purdue University,null,null
3,"250 N. University Street West Lafayette, IN 47907, USA",null,null
4,"{dthong, lsi}@cs.purdue.edu",null,null
5,"Paul Bracke, Michael Witt",null,null
6,Purdue University Libraries Purdue University,null,null
7,"504 West State Street West Lafayette, IN 47907, USA",null,null
8,"{pbracke,mwitt}@purdue.edu",null,null
9,Tim Juchcinski,null,null
10,Department of Computer Science Purdue University,null,null
11,"250 N. University Street West Lafayette, IN 47907, USA",null,null
12,tjuchcin@purdue.edu,null,null
13,ABSTRACT,null,null
14,"Resource selection is an important task in Federated Search to select a small number of most relevant information sources. Current resource selection algorithms such as GlOSS, CORI, ReDDE, Geometric Average and the recent classificationbased method focus on the evidence of individual information sources to determine the relevance of available sources. Current algorithms do not model the important relationship information among individual sources. For example, an information source tends to be relevant to a user query if it is similar to another source with high probability of being relevant. This paper proposes a joint probabilistic classification model for resource selection. The model estimates the probability of relevance of information sources in a joint manner by considering both the evidence of individual sources and their relationship. An extensive set of experiments have been conducted on several datasets to demonstrate the advantage of the proposed model.",null,null
15,Categories and Subject Descriptors,null,null
16,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
17,General Terms,null,null
18,"Algorithms, Design, Performance",null,null
19,Keywords,null,null
20,"Federated Search, Resource Selection, Joint Classification",null,null
21,1. INTRODUCTION,null,null
22,"Federated text search provides a unified search interface for multiple search engines of distributed text information sources. There are three major research problems in federated search as resource representation, resource selection",null,null
23,Vietnam Education Foundation Fellow,null,null
24,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",null,null
25,"and results merging. This paper focuses on resource selection, which selects a small number of most relevant information sources to search for any particular user query.",null,null
26,"Resource selection for federated search has been a popular research topic in the last two decades. Many methods treat each information source as a single big document and rank available sources either by using statistics from the sample documents (CORI [6]), or by building a language model for each source (Xu and Croft [27], Si and Callan [22]). Other methods such as GlOSS [12], Geometric Average [17], ReDDE [20], CRCS [18] and SUSHI [24] look further inside an information source by estimating the relevance of each document and calculate the source's score as an aggregate function of the documents that the source contains. More recent methods such as the classification-based method [1] and the work in [2] treat resource selection as a classification problem and build probabilistic models by combining multiple types of evidence of individual sources.",null,null
27,"Existing resource selection methods judge an information source by its own characteristics, but miss an important piece of evidence, which is the relationship between available sources. In practice, we notice that relationship can be meaningful and indicative. An information source that is ""similar"" to another highly relevant source has a better chance of being relevant. The evidence of source relationship can be very valuable for real world federated search solutions. In particular, the resource representation (e.g., sample documents) of each information source is often limited and prevents resource selection algorithms from identifying relevant sources, while the relationship between sources can help to alleviate the problem by providing more evidence from similar sources. Our study of a real world federated search application with digital libraries also suggests that it is difficult to obtain thorough resource representation from many sources (i.e., digital libraries). For example, some sources may only provide the abstracts of its documents instead of the full texts.",null,null
28,"This paper proposes a novel probabilistic discriminative model for resource selection that explicitly models the relationship between information sources. In particular, the new research combines both the evidence of individual sources and the relationship evidence between sources into a single probabilistic model for estimating the joint probability of relevance of a set of sources. Different similarity metrics have been studied to explore the relationship between information sources. An extensive set of experiments have been conducted on two TREC testbeds for federated search",null,null
29,98,null,null
30,research and one real world application for searching digital libraries. The experiment results demonstrate the effectiveness and robustness of the proposed resource selection algorithm with the joint classification model.,null,null
31,The rest of this paper is organized as follows: the next section discusses the related research work. Section 3 presents the classification model for resource selection. Section 4 proposes the joint classification model. Section 5 discusses experimental methodology. Section 6 presents experimental results and related discussions. Section 7 concludes and points out some future research work.,null,null
32,2. RELATED WORK,null,null
33,"There has been considerable research on all of the three subtasks of federated search as resource representation, resource selection and results merging. Since this paper focuses on the resource selection task, we mainly survey most related prior research work in resource selection and briefly talk about resource representation and results merging.",null,null
34,"The first step in federated search is to obtain representative resource descriptions from available information sources. The START protocol [11] provides accurate information in collaborative federated search environments, but it does not work for uncooperative environments. On other side, the query-based sampling technique [4] has been widely used in federated search to obtain sample documents from each source by issuing randomly generated queries. In particular, the query-based sampling approach is used in this work to acquire sample documents from available sources. After that, all sample documents are merged together as a centralized sample database.",null,null
35,"Resource selection selects a small set of most relevant sources for each user query [3][8][13]. Most early resource selection algorithms treat each individual source as a single big document which they extract summary statistics from. Those big document methods such as KL [27], CORI [6], and CVV [28] utilize different types of summary statistics of sources and finally rank available sources by matching the statistics with the user's query. These methods ignore the boundaries of individual documents within individual sources, which limits their performance of identifying sources with a large number of relevant documents.",null,null
36,"Some recent resource selection algorithms such as ReDDE [20], DTF [9][10], CRCS [18] and SUSHI [24] step away from treating each source as a single big document. Those algorithms often analyze individual sample documents within resource representation for ranking sources. For example, the ReDDE selection algorithm estimates the distribution of relevant documents by treating top-ranked sample documents as a representative subset of relevant documents in available sources. Related algorithms such as UUM [22], RUM [23] and CRCS [18] have been proposed, which use different methods to weight top-ranked documents and estimate the probability of relevance.",null,null
37,More recent resource selection algorithms such as the classification-based resource selection in federated search [1] or vertical search [2] treat resource selection as a classification problem. A classification model can be learned from a set of training queries and is used to predict the relevance of a source for test queries. It has been shown [1] that the classification approach can outperform state-of-the-art resource selection algorithms like ReDDE.,null,null
38,Existing resource selection methods utilize evidence within,null,null
39,"individual sources to judge their relevance but ignore the evidence of the relationship between available sources. However, the relationship evidence is a valuable piece of information, which promises to improve the accuracy of resource selection.",null,null
40,"Two other related research work in [25][7] learn from the results of past queries for resource selection. However, these two methods do not model the relationship between information sources and do not use formal models based on classification.",null,null
41,"The last step of federated search is results merging, which merges returned documents from selected sources into a single list. The most effective method is to download and recalculate scores for all returned documents within a centralized retrieval model, but this is often inefficient. More efficient methods such as the CORI merging formula, the SSL [21] and the SAFE merging algorithms [19] try to approximate the results of centralized retrieval in different ways.",null,null
42,3. CLASSIFICATION MODEL,null,null
43,3.1 Classification Approach,null,null
44,"Many resource selection algorithms are unsupervised and provide one source of evidence. To combine different evidence in a unified framework, one needs a training dataset, usually in the form of binary judgments on sources. Specifically, given a set of sources C and a set of training queries Q, the objective is to find a mapping F of the form",null,null
45,"F : Q × C  {+1, -1}",null,null
46,"where +1 indicates the relevance between the query and the source, and -1 indicates irrelevance.",null,null
47,"Arguello et al.[1] have proposed a method to construct those judgments. Each query q  Q will be issued to a fulldataset index for searching. A source Ci  C is considered to be relevant with q if more than  documents from Ci are present in top T of the full-dataset result. Otherwise, it is marked as irrelevant.",null,null
48,"While this method can produce a rank list that mimics the rank list produced by a full-dataset retrieval, it is difficult to apply in a real world environment because of the absence of a full-dataset. We propose an alternative method that could be more feasible. A query q is now issued to each remote source Ci and we only count their returned documents that are relevant. Top T documents from each source will be inspected, then a source is marked as relevant if it has more than  relevant documents presenting in that list. In our work, we set T ,"" 100. For dataset with a large average number of relevant documents per query (over 100), we set  "", 3; otherwise  is equal to 1.",null,null
49,3.2 Sources of Evidence,null,null
50,This section presents different types of evidence of individual sources for building our classification model.,null,null
51,3.2.1 Big Document,null,null
52,"Big Document (BIGDOC) approach treats each information source as a big document that contains all of its sample documents. A query is then issued to an index which contains a set of big documents, each representing one source. Sources are then ranked by how their merged sample documents match the query. The disadvantage of this method is that it does not take into account the variation of sources'",null,null
53,99,null,null
54,"sizes. Assuming that the sampling process is uniform, for a very big source, the sampling process only covers a small fraction of its documents. Therefore, it may present fewer relevant documents in the centralized sample database than a much smaller one, although the absolute number of relevant documents in the big source is higher. Without considering the sources' sizes, it would be misleading to conclude that the small source is the better choice. Nevertheless, when combined with other features, BIGDOC could have a good contribution, especially in the case that many sources contain roughly the same number of documents. While CORI (discussed in the next part) also treats each source as one document, BIGDOC approach is more flexible since it can be used with different retrieval algorithms. In our experiments, the algorithm is Indri [14]. For each pair of a query and a source, one BIGDOC feature is built from the sample documents.",null,null
55,3.2.2 CORI,null,null
56,"The CORI resource selection algorithm [6] uses Bayesian Inference Network model to rank sources. The belief P (q|Ci) that a source Ci satisfies query q is the combination of multiple P (rj|Ci), the belief corresponding to each term rj of query q. CORI applies a variant of tf.idf formula to determine each P (rj|Ci) and combine them together to calculate the final belief score of each source. CORI was proven to have robust performance for resource selection. In our experiments, one CORI feature is used for each pair of a query and a source.",null,null
57,3.2.3 Geometric Average,null,null
58,"In this method, a query is first issued to a centralized sample database, which was mentioned in section 2. Then, each source Ci is scored according to the geometric average query likelihood of its top K sample documents [17],",null,null
59,  K,null,null
60,1 K,null,null
61,"GAV Gq(Ci) ,",null,null
62,P (q|dij ),null,null
63,"j,1",null,null
64,"where dij is the j-th sample document in the rank list of source Ci. If Ci presents less than K documents in the rank list, the product above is padded with the minimum query likelihood score.",null,null
65,3.2.4 Modified ReDDE & ReDDE.top,null,null
66,Recall that ReDDE score [20] is calculated according to :,null,null
67,ReDDEq (Ci ),null,null
68,",",null,null
69,Niest Nisamp,null,null
70,×,null,null
71,I (d,null,null
72,dRsNamp,null,null
73,Ci) × Pq(rel|d),null,null
74,"where RsNamp is the top N documents returned from searching the centralized sample database. Niest is the estimated size of source Ci, Nisamp is the sample size of Ci, and I(.) is the indicator function. The number of top returned documents, N , is equal to  × Naelslt, where Naelslt is the estimated total number of documents of all sources and  is a constant,",null,null
75,which is usually in the range 0.002-0.005.,null,null
76,"ReDDE uses a step function to estimate Pq(rel|d), the probability that the document d is relevant to query q. For",null,null
77,"all top N documents, that probability is equal to a constant.",null,null
78,"In our experiment, we use a modified version of ReDDE,",null,null
79,"which replaces Pq(rel|d) by P (q|d), the retrieval score of document d with respect to query q. The Indri retrieval",null,null
80,algorithm [14] is used for searching the centralized sample database. The modified ReDDE feature has been shown empirically better than the original ReDDE feature. There is one modified ReDDE feature for each pair of a query and a source.,null,null
81,"ReDDE.top [1] is another variant of ReDDE. Unlike ReDDE, ReDDE.top set a specific number to N . In our experiment, we add another two ReDDE.top features with N , 100 and N , 1000 respectively.",null,null
82,4. JOINT CLASSIFICATION MODEL,null,null
83,4.1 Probabilistic Discriminative Model,null,null
84,"We propose a novel joint probabilistic model for the resource selection task. First of all, a logistic model is built to combine all the features of individual sources. We refer to this model as the independent model (Ind).",null,null
85,"Let v ,"" {v1, ..., vn} be the relevance vector. vi "","" 1 indicates that the i-th source is relevant, otherwise vi "", 0. The relevance probability of a source ci given its feature vector f(ci) is calculated as:",null,null
86,P (vi,null,null
87,",",null,null
88,1|ci),null,null
89,",",null,null
90,1,null,null
91,exp(f(ci) · ) + exp(f(ci) · ),null,null
92,"where  denotes the combination weight vector. For simplicity, the vector f(ci) contains the bias feature (which is 1 for every pair of a query and a source) and the weight vector ",null,null
93,contains the bias element 0. The conditional probability of v given n sources is:,null,null
94,P (v|c),null,null
95,",",null,null
96,1 Z,null,null
97,n exp,null,null
98, log P,null,null
99,(vi,null,null
100,",",null,null
101,1|ci)vi P (vi,null,null
102,",",null,null
103,0|ci)1-vi ,null,null
104,i,null,null
105,where Z is the normalizing constant. Our joint classification model (Jnt) expands the above,null,null
106,formula with a new term to model the relationship between sources. The conditional probability of v given n sources is now:,null,null
107,P (v|c),null,null
108,",",null,null
109,1 Z,null,null
110,n exp,null,null
111, log P (vi,null,null
112,",",null,null
113,1|ci)vi P (vi,null,null
114,",",null,null
115,0|ci)1-vi ,null,null
116,+,null,null
117, |v|,null,null
118,i,null,null
119," sim(ci, cj )vivj",null,null
120,"i,j(i<j)",null,null
121,which can be rewritten as:,null,null
122,P (v|c),null,null
123,",",null,null
124,1 Z,null,null
125,n exp,null,null
126,(1,null,null
127,-,null,null
128,vi,null,null
129,)(f(ci),null,null
130,·,null,null
131,),null,null
132,-,null,null
133,log(1,null,null
134,+,null,null
135,exp(f(ci),null,null
136,·,null,null
137,)),null,null
138,+,null,null
139, |v|,null,null
140,i,null,null
141," sim(ci, cj )vivj",null,null
142,"i,j(i<j)",null,null
143,"where sim(ci, cj) denotes the similarity between two sources ci and cj, and Z is another normalizing constant.",null,null
144,"The parameter  controls the influence of similarity. If || is high, the model tends to promote only similar (or dissimilar) sources. When  ,"" 0, we get back to the independent model.""",null,null
145,"In the learning step, we learn the feature weight vector  from the independent model by using logistic regression. This vector is then used in the joint model. Learning , however, is generally intractable. One can see that the space of vector v is 2n, and so inferencing and estimation become",null,null
146,100,null,null
147,"impossible when n is large. We resolve this issue by first ranking the sources using the independent model, and then apply the joint classification model only to the top K , 10 sources. This is equivalent to reranking the top K sources.",null,null
148,"From the set of training queries, we use maximum loglikelihood estimation to learn the parameter . Because there is no closed-form solution for the maximum of this loglikelihood function, gradient search method is used instead.",null,null
149,"In the prediction step, for a test query, the score of each source ci is assigned by its probability of being relevant:",null,null
150," R(ci) , P (vi , 1|c) ,"" P (v1, v2, ..., vi "","" 1, ..., vn|c)""",null,null
151,vi,null,null
152,"where vi denotes the set of variables in v with variable vi omitted. In practice, the summation is taken over K - 1 variables and so is feasible when K is small. After that, the top K sources will be reranked according to the new score.",null,null
153,4.2 Similarity Metrics,null,null
154,4.2.1 Similarity Metric based-on Evaluation,null,null
155,"Given a set of training queries, the similarity between two sources can be measured by looking at the set of queries for wich they are both relevant. The bigger that set is, the more related they are. Specifically, we apply a cross-product formula to measure this metric:",null,null
156," SM E(ci, cj) ,"" rel(ci, q)rel(cj, q)""",null,null
157,qQ,null,null
158,"where Q is the set of training queries, rel(ci, q) is equal to 1 if source ci is relevant to query q based on the classification approach described above, otherwise it is 0. This method is called Similarity Metric based-on Evaluation (SME).",null,null
159,4.2.2 Similarity Metric based-on Query-specific,null,null
160,Evaluation,null,null
161,"One issue with the SME is that it is independent of the query. A source may be highly related with another source with respect to a query but unrelated with that source with respect to another query. Therefore, it is better to incorporate the similarity between queries into this formula. By extending the above SME, we derive another metric called Similarity Metric based on Query-specific Evaluation (SMQE).",null,null
162,"SM QEq(ci, cj ) ,",null,null
163,"sim(q, q)rel(ci, q)rel(cj , q)",null,null
164,q Q,null,null
165,"where sim(q, q) denotes the correlation (or similarity) between the test query q and a training query q. There are many studies that explore the topicality or classification of queries, however, in this paper, we choose one simple approach. A query in consideration is issued to the centralized sample database, and the number of documents from each source that appear in top M documents of the result is recorded. In our work, M is equal to 100. The correlation between two queries is derived by a cosine-like formula:",null,null
166,"sim(q, q)",null,null
167,",",null,null
168,i,null,null
169,"numdoc(q,",null,null
170,ci)numdoc(q,null,null
171,",",null,null
172,ci,null,null
173,),null,null
174,"i numdoc(q, ci)2 i numdoc(q, ci)2",null,null
175,"where numdoc(q, ci) is the number of documents of source ci that appear in the top M documents returned from query q.",null,null
176,"Both the SME and SMQE metrics can be modified in many ways. First of all, the term rel(i, q) can be represented either by a binary number or the absolute number of relevant documents. Or we can set different thresholds to the searching on the centralized sample database. Another choice is to normalize the relevance vector. However, in our experiments, those changes do not have much effect on the results. In fact, SMQE provides the best result, proving that it better reflects the relationship between sources.",null,null
177,4.2.3 Similarity-Metric based-on Kullback-Leibler,null,null
178,divergence,null,null
179,"This method tries to reveal the similarity between sources by looking at their own vocabularies. Specifically, a language model [16] is built for each sample source. Then we calculate the Kullback-Leibler divergence between those two language models. Recall that the Kullback-Leibler divergence is actually the distance between two probabilistic models, which is the inverse of their similarity. However, because our model can adapt this change by inferring a negative similarity coefficient , we keep the KL-value as it is. This metric is referred to as SMKL.",null,null
180,5. EXPERIMENTAL METHODOLOGY,null,null
181,"We evaluate our proposed algorithms on 3 datasets. The first two datasets are well-known TREC testbeds, the last one comes from a real world application.",null,null
182,"· TREC123-100col-bysource (TREC123): 100 collections (information sources) were created from TREC CDs 1,2 and 3 [3]. They are organized by publication source and publication date. The size of each source varies from 7,000 to 39,700 documents (see Table 1 for more statistics). This testbed comes with 100 queries (TREC topics 51-150) with judgments.",null,null
183,"· TREC4-100col-bysource (TREC4): 100 collections were created according to the publication source of documents in TREC4 [26]. One actual publication source is distributed across a number of information sources depending on its total number of documents. Each information source has roughly 5,675 documents. This testbed comes with 50 queries (TREC topics 201250) with judgments. More statistics about both TREC123 and TREC4 are presented in Table 1.",null,null
184,"· Digital Library (DIGLIB): This real world dataset contains 80 sources (i.e., digital libraries) that are accessible from Purdue University Libraries1. This testbed presents a heterogeneous sources of information. Each document from those sources composes of many fields. Three fields that convey rich information are the abstract, subject heading (or document's category) and full text. Not all sources provide all those three fields. A document from one source may not be provided with its full text, while a document from another source may not have the subject heading. Table 2 shows that only 65% of 80 sources provide abstracts, 65% provide subject headings, and only 30% provide full texts. In our current work, we temporally merge all those available information into one document. Future research may",null,null
185,1We make the dataset available as feature file at http://www.cs.purdue.edu/homes/dthong/,null,null
186,101,null,null
187,Table 1: Summary Statistics of TREC123 and TREC4,null,null
188,Testbed,null,null
189,TREC123 TREC4,null,null
190,Size (GB),null,null
191,3.2 2.0,null,null
192,Number of Documents (x1000) Min Avg Max 0.7 10.8 39.7 5.6 5.6 5.6,null,null
193,Size(MB),null,null
194,Min Avg Max 28 32 42 4 20 138,null,null
195,Rk,null,null
196,",",null,null
197,"k ki,1",null,null
198,"i,1",null,null
199,Ei Bi,null,null
200,"Let Ei denote the number of relevant documents of the i-th source according to the ranking E, Bi denote the same thing with respect to ranking B. We also report the results at P @{1, 3, 5, 10} as in source level accuracy.",null,null
201,Table 2: Statistical Information about DIGLIB: Number of Sources Corresponding to their Available Information Fields,null,null
202,Abstract Subject Full Text Number of Sources 52(65%) 52(65%) 24(30%),null,null
203,"· Document Level, High Precision: To make it independent from result merging algorithm, we use fulldataset retrieval as our merging method. The chosen retrieval algorithm is Inquery in Lemur Toolkit [5]. For each query, top 5 sources from the joint classification rank list are selected for this step. Documents not from selected sources are filtered out from the full-dataset rank list. The remaining list is checked by their precision at P @{5, 10, 15, 20, 30} accordingly. We also report a full-dataset precision which includes all sources.",null,null
204,consider to treat each source differently according the type of information that is available.,null,null
205,"We also build a set of 100 queries, some of them are extracted from the library log, which are real queries. For each pair of a query and a source, we manually assign a binary value to indicate their relevance.",null,null
206,"On TREC123 and TREC4, all tests at different levels are presented. On DIGLIB dataset, we only report the results at source level because the document judgments are difficult to make as many sources do not provide their full text information. In most of the experiments, SMQE is used as our default similarity metric. However, in section 6.4, we also discuss the experimental results with different other metrics.",null,null
207,"A note on resource specific retrieval algorithm: DIGLIB is a real world application of digital libraries, each of its sources implements a different retrieval algorithm, which is not known. We can only access those sources through a unified interface. For TREC123 and TREC4, we assign one retrieval algorithm to each source in a round-robin manner. The set of assigned algorithms is Inquery, Language Model and Vector Space (tf.idf). These algorithms influence the query-based sampling process, as well as the classification process. A less effective retrieval algorithm like Vector Space model may reduce a source's chance of being marked as relevant.",null,null
208,"For each testbed, we repeat every experiment 5 times. In each trial, we randomly select 50% of the queries as training set, and test on the other 50%. All the results shown in the next section are averaged over 5 trials.",null,null
209,Each source is sampled with 300 documents. We also compare the main results with 100 sample documents. The experiments are measured on several levels:,null,null
210,"· Source Level (Resource Selection), Accuracy: This level measures the precision of the resource selection algorithms. Top sources (i.e., top 10) are judged by their precisions at different levels. The judgments come from the classification method, as described in section 3.1. We report the results at P @{1, 3, 5, 10} accordingly.",null,null
211,"· Source Level, Recall Metric (R-metric): This metric is widely used in comparing resource selection algorithms [3]. Let E denote the ranking produced by a resource selection algorithm, B denote the based line ranking, in this case, the Relevance-Based Ranking. At level k, the R-metric is defined as:",null,null
212,6. EXPERIMENTAL RESULTS,null,null
213,"In all of our experiments, we use paired t-test on queries to check significance. A  denotes a significance on p < 0.1 level;  corresponds to p < 0.05 level and  corresponds to p < 0.01 level.",null,null
214,6.1 TREC123 & TREC4,null,null
215,"First of all, we compare the joint classification model with the independent model on the two TREC testbeds. Table 3 represents the source level results in accuracy on TREC123 and TREC4. The second column of each dataset is the joint classification model. Numbers in parentheses show the relative improvement of the joint classification model (denoted as ""Jnt"") over the independent model (denoted as ""Ind"").",null,null
216,"Table 4 shows the R-metric comparison between the independent model and joint classification model. Table 5 shows the high precision at document level. We also report the full centralized retrieval, which includes all sources. This is denoted as the ""Full"" column in the table.",null,null
217,"It can be seen that joint classification model always leads to better results than independent model, as it shows in all three tables. Both models have the same source level accuracy and R-metric values at top 10 because of the fact that we rerank the top 10 sources. The results are more statistically significant on TREC123 than on TREC4. This can be explained as in TREC123, we have trained on 50 queries; whereas in TREC4, we use only 25 queries out of 50 for training.",null,null
218,6.2 Digital Library,null,null
219,"The result at source level of Digital Library is reported in Table 6. In this real world dataset, the joint classification model significantly outperforms the independent model. This accounts to the fact that many sources only provide",null,null
220,102,null,null
221,Table 3: Source Level Results in Accuracy on TREC123 & TREC4 with 300 Sample Documents,null,null
222,Table 5: Document Level Results in High Precision on TREC123 & TREC4 with 300 Sample Documents,null,null
223,Src Rank,null,null
224,@1 @3 @5 @10,null,null
225,TREC123,null,null
226,Ind 0.512 0.456 0.451 0.439,null,null
227,Jnt 0.524(2.3%) 0.499(9.4%) 0.484(7.3%),null,null
228,0.439(0%),null,null
229,TREC4,null,null
230,Ind 0.480 0.451 0.430 0.414,null,null
231,Jnt 0.536(11.7%) 0.475(5.3%) 0.446(3.7%),null,null
232,0.414(0%),null,null
233,Docs Rank,null,null
234,@5 @10 @15 @20 @30,null,null
235,Full 0.446 0.444 0.435 0.430 0.414,null,null
236,TREC123,null,null
237,Ind 0.392 0.355 0.332 0.309 0.280,null,null
238,Jnt 0.410(4.6%) 0.360(1.4%) 0.347(4.5%) 0.326(5.5%) 0.300(7.1%),null,null
239,Table 4: Source Level Results in R-metric on TREC123 & TREC4 with 300 Sample Documents,null,null
240,Src Rank,null,null
241,@1 @3 @5 @10,null,null
242,TREC123,null,null
243,Ind 0.262 0.309 0.354 0.426,null,null
244,Jnt 0.319(21.8%) 0.364(17.8%) 0.400(13.0%),null,null
245,0.426(0%),null,null
246,TREC4,null,null
247,Ind 0.287 0.324 0.343 0.414,null,null
248,Jnt 0.309(7.7%) 0.340(4.9%) 0.355(3.5%),null,null
249,0.414(0%),null,null
250,Docs Rank,null,null
251,@5 @10 @15 @20 @30,null,null
252,Full 0.549 0.459 0.422 0.384 0.354,null,null
253,TREC4,null,null
254,Ind 0.282 0.238 0.209 0.186 0.167,null,null
255,Jnt 0.290(2.8%) 0.254(6.7%) 0.224(7.2%) 0.200(7.5%) 0.170(1.8%),null,null
256,partial information about themselves. This also shows that the joint classification model can alleviate the problem of missing information.,null,null
257,6.3 Tests with Different Sample Sizes,null,null
258,"We conduct experiments on three datasets with only 100 documents sampled from each source. This test is to show the robustness of the model, as well as the effect of sampling size on the results. The results of source level (both in accuracy and R-metric) and document level are reported for TREC123 and TREC4 (Table 7, Table 8 and Table 9 respectively), while only source level is reported for DIGLIB (Table 10).",null,null
259,"The sample size clearly affects TREC123. Its performance of the independent model drops significantly. However, this also leaves room for joint classification model to show its effectiveness: the accuracy on source level is statistically more significant. On document level, the improvement is a bit weaker. This can be explained as the initial choice of top 10 sources from the independent model is less precise, so is the joint classification model, which uses the initial ranking list directly.",null,null
260,"Most results on TREC4 from Table 7 to Table 9 indicate the advantage of the joint classification model against independent model with a small number of sample documents, although the difference is smaller than TREC123 due to the limited amount of training information.",null,null
261,"The results on DIGLIB (Table 10) are also consistent. The performances of both resource selection algorithms drop with 100 sample documents. However, the results of the joint classification method are still significantly better than those of the independent method.",null,null
262,6.4 Test with Different Similarity Metrics,null,null
263,We conduct tests on three testbeds with different similarity metrics discussed in Section 4.2. Figure 1 shows the,null,null
264,Table 6: Source Level Results in Accuracy on DIGLIB with 300 Sample Documents,null,null
265,Src Rank,null,null
266,@1 @3 @5 @10,null,null
267,DIGLIB,null,null
268,Ind 0.552 0.460 0.419 0.356,null,null
269,Jnt 0.640(15.9%) 0.536(16.5%) 0.487(16.2%),null,null
270,0.356(0%),null,null
271,Table 7: Source Level Results in Accuracy on TREC123 & TREC4 with 100 Sample Documents,null,null
272,Src Rank,null,null
273,@1 @3 @5 @10,null,null
274,TREC123,null,null
275,Ind 0.320 0.299 0.318 0.319,null,null
276,Jnt 0.380(18.8%) 0.373(24.7%) 0.357(12.3%),null,null
277,0.319(0%),null,null
278,TREC4,null,null
279,Ind,null,null
280,0.496 0.405 0.379 0.367,null,null
281,Jnt 0.480(-3.2%) 0.411(1.5%) 0.403(6.3%),null,null
282,0.367(0%),null,null
283,Table 8: Source Level Results in R-metric on TREC123 & TREC4 with 100 Sample Documents,null,null
284,Src Rank,null,null
285,@1 @3 @5 @10,null,null
286,TREC123,null,null
287,Ind 0.183 0.214 0.244 0.311,null,null
288,Jnt 0.233(27.3%) 0.262(22.4%) 0.279(14.3%),null,null
289,0.311(0%),null,null
290,TREC4,null,null
291,Ind 0.278 0.264 0.293 0.341,null,null
292,Jnt 0.317(14%) 0.293(11%) 0.311(6.1%),null,null
293,0.341(0%),null,null
294,103,null,null
295,Table 9: Document Level Results in High Precision on TREC123 & TREC4 with 100 Sample Documents,null,null
296,TREC123,null,null
297,0.3,null,null
298,0.42,null,null
299,SMQE,null,null
300,SME,null,null
301,0.4,null,null
302,Independent,null,null
303,0.28,null,null
304,SMKL,null,null
305,TREC4,null,null
306,SMQE SME Independent SMKL,null,null
307,Docs Rank,null,null
308,@5 @10 @15 @20 @30,null,null
309,TREC123,null,null
310,Ind 0.328 0.302 0.288 0.277 0.253,null,null
311,Jnt 0.329(0.3%) 0.316(4.6%) 0.306(6.2%) 0.296(6.9%) 0.268(5.9%),null,null
312,TREC4,null,null
313,Ind 0.283 0.243 0.223 0.195 0.165,null,null
314,Jnt 0.301(6.4%) 0.254(4.5%) 0.227(1.8%) 0.204(4.6%) 0.166(0.6%),null,null
315,Table 10: Source Level Results in Accuracy of DIGLIB with 100 Sample Documents,null,null
316,Precision Precision,null,null
317,0.38 0.36 0.34 0.32,null,null
318,0.3 0.28 0.26,null,null
319,0,null,null
320,10,null,null
321,20,null,null
322,Document Rank,null,null
323,0.26,null,null
324,0.24,null,null
325,0.22,null,null
326,0.2,null,null
327,0.18,null,null
328,0.16,null,null
329,30,null,null
330,0,null,null
331,10,null,null
332,20,null,null
333,30,null,null
334,Document Rank,null,null
335,Src Rank,null,null
336,@1 @3 @5 @10,null,null
337,DIGLIB,null,null
338,Ind 0.436 0.383 0.375 0.318,null,null
339,Jnt 0.620(42.2%) 0.531(38.6%) 0.474(26.4%),null,null
340,0.318(0%),null,null
341,"results of TREC123 and TREC4 at document level. From this figure, we notice that the SMQE method outperforms all other metrics, due to the fact the it considers the similarity between queries. The SME produces a quite close-to-best result, but the SMKL tends not to be a good choice for the joint classification model. On TREC4, SMKL is comparable with independent model, but it is worse than SMQE and SME.",null,null
342,"Figure 2 shows the results of DIGLIB at source level. In this case, both SMQE and SME are comparable, except for the precision at top 1. Again SMKL is not a good choice.",null,null
343,Figure 1: Document Level High Precision on TREC123 & TREC4 with Different Similarity Metrics,null,null
344,Precision,null,null
345,DIGLIB,null,null
346,0.65,null,null
347,SMQE,null,null
348,SME,null,null
349,Independent,null,null
350,0.6,null,null
351,SMKL,null,null
352,0.55,null,null
353,0.5,null,null
354,0.45,null,null
355,0.4,null,null
356,0.35,null,null
357,0,null,null
358,2,null,null
359,4,null,null
360,6,null,null
361,8,null,null
362,10,null,null
363,Source Rank,null,null
364,7. CONCLUSION & FUTURE WORK,null,null
365,"This paper proposes a novel joint probabilistic classification model for the resource selection task in federated text search. Existing resource selection algorithms only utilize evidence of individual information sources to select relevant sources, but they do not model the valuable relationship information between the sources. The proposed algorithm estimates the probability of relevance of information sources in a joint manner by combining both the evidence of individual sources and the relationship between the sources. The importance of different types of evidence is determined in a discriminative manner for maximizing the accuracy of resource selection with some training queries. Different types of similarity metrics have been explored to model source similarity based on the performance of available sources on training queries and the Kullback-Leibler divergence on the contents of the sources. A set of experiments were conducted with two TREC datasets and one real world application with digital libraries. The empirical results in different configurations have demonstrated the effectiveness of the proposed joint classification model.",null,null
366,"There are several directions to extend the research work in the paper. First, one advantage of the proposed joint probabilistic model is to integrate different types of evidence of",null,null
367,Figure 2: Source Level Accuracy on DIGLIB with,null,null
368,Different Similarity Metrics,null,null
369,"individual sources and their relationship. We plan to explore more features for improving the performance of resource selection. For example, we can combine multiple types of similarity evidence in a single framework (with different  weights), which may better model sources' relationship for more accurate resource selection. Second, the joint model in this paper utilizes a reranking approach in resource selection with a small set of information sources (e.g., top 10) to avoid large computational complexity. It is possible to break this limit by utilizing some other approximate inference algorithms (e.g., the pseudo likelihood approach [15]) or making further assumptions on the sources' relationship. For example, one strategy is to first divide available sources into groups of closely related sources. Inference can be conducted by building a small model in each group and assuming independence of sources between different groups.",null,null
370,8. ACKNOWLEDGMENTS,null,null
371,This research was partially supported by the Vietnam Education Foundation (VEF) and the NSF grant IIS-0749462.,null,null
372,104,null,null
373,"The opinions, findings, and conclusions stated herein are those of the authors and do not necessarily reflect those of the sponsors.",null,null
374,9. REFERENCES,null,null
375,"[1] J. Arguello, J. Callan, and F. Diaz. Classification-based resource selection. Proceeding of",null,null
376,"the 18th ACM Conference on Information and Knowledge Management, 2009. [2] J. Arguello, F. D´iaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. In Proceedings of the",null,null
377,"32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [3] J. Callan. Distributed information retrieval. Advances in Information Retrieval, pages 127­150, 2000. [4] J. Callan and M. Connell. Query-based sampling of text databases. ACM Transactions on Information Systems, 19(2):97­130, 2001. [5] J. Callan, W. B. Croft, and S. M. Harding. The inquery retrieval system. In Proceedings of the Third",null,null
378,"International Conference on Database and Expert Systems Applications, 1992. [6] J. Callan, Z. Lu, and W. B. Croft. Searching distributed collection with inference networks. In",null,null
379,Proceedings of the 18th Annual International ACM,null,null
380,"SIGIR Conference on Research and Development in Information Retrieval, 1995. [7] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. In Proceeding of the 18th",null,null
381,"ACM Conference on Information and Knowledge Management. ACM, 2009. [8] N. Craswell, P. Bailey, and D. Hawking. Server selection on the world wide web. In Proceedings of the 5th ACM Conference on Digital Libraries. ACM, 2000. [9] N. Fuhr. A decision-theoretic approach to database selection in networked ir. ACM Transactions on Information Systems (TOIS), 17(3):229­249, 1999. [10] N. Fuhr. Resource discovery in distributed digital libraries. In In Digital Libraries '99: Advanced Methods and Technologies, Digital Collections, 1999. [11] L. Gravano, K. Chang, C-C., H. Garc´ia-Molina, and A. Paepcke. Starts: Stanford proposal for internet meta-searching. In Proceedings of the ACM-SIGMOD",null,null
382,"International Conference on Management of Data (SIGMOD). ACM, 1997. [12] L. Gravano, H. Garc´ia-Molina, and A. Tomasic. Gloss: Text-source discovery over the internet. ACM Transactions on Database Systems, 24(2):229­264, 1999. [13] W. Meng, C. Yu, and K. Liu. Building efficient and effective metasearch engines. ACM Computing Surveys (CSUR), 34(1):48­89, 2002. [14] D. Metzler and W. Croft. Combining the language model and inference network approaches to retrieval. Information Processing and Management, 40(5):735­750, 2004. [15] S. Parise and M. Welling. Learning in markov random fields: An empirical study. In Joint Statistical Meeting (JSM2005), volume 4, 2005. [16] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of",null,null
383,the 25th Annual International ACM SIGIR,null,null
384,"Conference on Research and Development in Information Retrieval. ACM, 1998. [17] J. Seo and W. B. Croft. Blog site search using resource selection. In CIKM '08: Proceeding of the",null,null
385,"17th ACM Conference on Information and Knowledge Management, pages 1053­1062, New York, NY, USA, 2008. ACM. [18] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. In",null,null
386,"Proceedings of the 29th European Conference on Information Retrieval, 2007. [19] M. Shokouhi and J. Zobel. Robust result merging using sample-based score estimates. ACM Transactions on Information Systems, 27(3):1­29, 2009. [20] L. Si and J. Callan. Relevant document distribution estimation method for resource selection. In",null,null
387,Proceedings of the 26th Annual International ACM,null,null
388,"SIGIR Conference on Research and Development in Information Retrieval. ACM, 2003. [21] L. Si and J. Callan. A semi-supervised learning method to merge search engine results. ACM Transactions on Information Systems, 21(4):457­491, 2003. [22] L. Si and J. Callan. Unified utility maximization framework for resource selection. In Proceedings of",null,null
389,"13th ACM International Conference on Information and Knowledge Management (CIKM), 2004. [23] L. Si and J. Callan. Modeling search engine effectiveness for federated search. In Proceedings of the",null,null
390,"28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2005. [24] P. Thomas and M. Shokouhi. Sushi: scoring scaled samples for server selection. In SIGIR '09: Proceedings",null,null
391,"of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2009. [25] E. Voorhees, N. K. Gupta, and B. Johnson-Laird. Learning collection fusion strategies. In Proceedings of",null,null
392,the 18th Annual International ACM SIGIR,null,null
393,"Conference on Research and Development in Information Retrieval. ACM, 1995. [26] J. Xu and J. Callan. Effective retrieval with distributed collections. In Proceedings of the 21st",null,null
394,"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1998. [27] J. Xu and W. B. Croft. Cluster-based language models for distributed retrieval. In Proceedings of the 22nd",null,null
395,"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 1999. [28] B. Yuwono and D. L. Lee. Server ranking for distributed text retrieval systems on the internet. In",null,null
396,Proceedings of the 5th Annual International,null,null
397,"Conference on Database Systems for Advanced Applications, 1997.",null,null
398,105,null,null
399,,null,null

,sentence,label,data
,,,
0,A Content based Approach for Discovering Missing Anchor Text for Web Search,null,null
,,,
1,Xing Yi and James Allan,null,null
,,,
2,Center for Intelligent Information Retrieval Computer Science Department,null,null
,,,
3,"University of Massachusetts, Amherst, MA, USA",null,null
,,,
4,"{yixing,allan}@cs.umass.edu",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"Although anchor text provides very useful information for web search, a large portion of web pages have few or no incoming hyperlinks (anchors), which is known as the anchor text sparsity problem. In this paper, we propose a language modeling based technique for overcoming anchor text sparsity by discovering a web page's plausible missing anchor text from its similar web pages' in-link anchor text. We design experiments with two publicly available TREC web corpora (GOV2 and ClueWeb09) to evaluate different approaches for discovering missing anchor text. Experimental results show that our approach can effectively discover plausible missing anchor terms. We then use the web named page finding task in the TREC Terabyte track to explore the utility of missing anchor text information discovered by our approach for helping retrieval. Experimental results show that our approach can statistically significantly improve retrieval performance, compared with several approaches that only use anchor text aggregated over the web graph.",null,null
,,,
7,"Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval­Search process,Retrieval models;H.3.5 [Information Storage and Retrieval]:Online Information Services­ Web-based services",null,null
,,,
8,"General Terms: Algorithms, Experimentation",null,null
,,,
9,"Keywords: anchor text, anchor text sparsity, language models, relevance models, content similarity, web search",null,null
,,,
10,1. INTRODUCTION,null,null
,,,
11,"There are rich dynamic human generated hyperlink structures on the web. Most web pages contain some hyperlinks, referred to as anchors, that point to other pages. Each anchor consists of a destination URL and a short piece of text, which is called anchor text. Anchors play an important role in helping web users conveniently navigate to their interested web information. Although some anchor text only functions as a navigational shortcut which does not have direct semantic relation to the destination URL (e.g.,""click",null,null
,,,
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",null,null
,,,
13,# of web pages # of pages having inlinks # of pages having original,null,null
,,,
14,or enriched inlinks[14],null,null
,,,
15,GOV2,Y,
,,,
16,"25,205,179 376,121 (1.5%) 977,538 (3.9%)",null,null
,,,
17,ClueWeb09-T09B,Y,
,,,
18,"50,220,423 7,640,585 (15.2%) 19,096,359 (38.0%)",null,null
,,,
19,Table 1: Summary of in-link statistics on two TREC web corpora used in our study.,null,null
,,,
20,"here"" and ""next""), many times anchor text provides succinct description of the destination URL's content, e.g. ""SIGIR 2010(Geneva, Switzerland)"" is from an anchor linked to http://www.sigir2010.org/. Anchor text instances are usually reasonable queries that web users may issue to search for the associated URL and have been used to simulate plausible web queries relevant to the associated web pages in some web search research [15]. Therefore, anchor text is highly useful for bridging the lexical gap between user issued web queries and the relevant web pages. It is arguably the most important piece of evidence used in web ranking functions[14].",null,null
,,,
21,"However, previous research has shown that the distribution of the number of inlinks on the web follows a power law [1], where a small portion of web pages have a large number of inlinks while most have few or no inlinks. Thus, most web pages do not have in-link associated anchor text, a problem originally referred to as the anchor text sparsity problem by Metzler et al. [14]. This problem presents a major obstacle for any web search algorithms that want to use anchor text to improve retrieval effectiveness. Table 1 shows the anchor text sparsity problem in two large TREC1 web corpora (GOV22 and ClueWeb09-T09B3). To address this problem, Metzler et al. [14] proposed aggregating, or propagating, anchor text across the web hyperlink graph so that web pages' lack of anchor text can be enriched with their linked web pages' associated anchor text. Table 1 shows that the number of URLs associated with some anchor text (original or propagated) in the two TREC web corpora is significantly increased by using their linked-based anchor text enrichment approach. Nevertheless, in Table 1 we notice that large portion of web pages still do not have any associated anchor text after having been enriched. This observation motivated us to consider another possible approach, which utilizes the content similarity between web pages, to alleviate anchor text sparsity.",Y,
,,,
22,1http://trec.nist.gov/ 2http://ir.dcs.gla.ac.uk/test_collections/ gov2-summary.htm 3http://boston.lti.cs.cmu.edu/Data/clueweb09/,null,null
,,,
23,427,null,null
,,,
24,"Specifically, we hypothesize that the anchor text associated with a web page's inlinks typically has close semantic relations to the web page so that web pages that are similar in content may be pointed to by anchors having similar anchor text. Under this assumption, in this paper we propose a language modeling based technique for discovering a web page's plausible missing in-link anchor text by using its most similar web pages' in-link anchor text. We then test the effectiveness of our approach by using the discovered missing anchor text information for some TREC web search tasks. We find that even on the GOV2 data where a serious anchor text sparsity problem exists as shown in Table 1, our approach can significantly improve retrieval performance. Our content based approach can be combined with the hyperlink based approach to further reduce anchor text sparsity and benefit web search. Our enriched document and anchor text representations can also be used for many other tasks beyond web search, including estimating better document models and extracting advanced textual features for content match and document classification.",Y,
,,,
25,"Our work has four chief contributions: 1) although content similarity has been used widely in other applications, we are the first to propose using web content similarity to address the anchor text sparsity problem. 2) We develop a language modeling based technique, which stems from ideas in one effective retrieval technique ­ relevance based language models [10], to effectively discover plausible missing anchor text information and use it for retrieval. 3) We empirically show that our approach performs better than Metzler et al.'s linked-based approach [14] in terms of discovering plausible missing anchor terms in two standard large TREC web corpora. 4) We show that our approach statistically significantly improves retrieval effectiveness, compared with several approaches that only use aggregated anchor text over the web graph, in the web named page finding task of the TREC Terabyte track [4].",Y,
,,,
26,"We begin by reviewing related work in §2. Next, we describe different approaches of discovering missing anchor text to enrich document representations in §3. Then we describe the experimental setup and results of evaluating different approaches for anchor text discovery in §4. After that, we present how to use discovered anchor text information for retrieval in a language modeling approach and report the experimental results in §5. We conclude in §6.",null,null
,,,
27,2. RELATED WORK,null,null
,,,
28,"Metzler et al. [14] first directly addressed the anchor text sparsity problem by using the web hyperlink graph and propagating anchor text over the web graph. Our work also addresses the same problem but using a different approach, which is based on the content similarity between web pages. Our approach is similar in nature to other similarity based techniques, such as cluster-based smoothing from the language modeling framework[8, 9, 11], except we focus on enriching web documents' anchor text representation by using their similar documents' associated anchor text.",null,null
,,,
29,"Anchor text can be modeled in many different ways. Westerveld et al. [20] and Nallapati et al. [15] model anchor text in the language modeling approach [17] and calculate an associated anchor language model to update the original document model for retrieval. Fujii [6] further considers differently weighting each line of anchor text associated with the same page thus obtaining a more robust anchor language model. Here, we also adopt the language modeling approach",null,null
,,,
30,"but focus on discovering a plausible associated anchor language model for web pages with no or few inlinks. Our approach can be easily used together with any language model based retrieval model (e.g., Ogilvie and Callan's model [16]) that takes document structure into account.",null,null
,,,
31,"Our approach of overcoming anchor text sparsity stems from ideas in the relevance based language models(RMs), proposed by Lavrenko and Croft [10]. Their original work introduces the RMs to find plausible useful terms missing in the original query for query expansion. Here we adapt the RMs to compute a web content dependent associated anchor language model for discovering missing anchor terms and using anchor text for retrieval. Thus, our approach, although similar in spirit to, differs from document expansion [18] and graph-based document smoothing[13].",null,null
,,,
32,3. DISCOVERING MISSING ANCHOR TEXT,null,null
,,,
33,We now describe three different approaches for discovering plausible missing anchor text for web pages with few or no inlinks. The goal of each is to produce a ranked list of plausible anchor text terms for a page.,null,null
,,,
34,3.1 Aggregating Anchor Text,null,null
,,,
35,"To overcome anchor text sparsity, Metzler et al.[14] originally proposed to augment web pages with auxiliary anchor text (denoted as ) that is derived by aggregating anchor text over the web graph. We first briefly review the procedure they have used to build , which is very important for our discussions and comparisons in this research. Given a web page 0's URL 0 , the procedure first collects all pages (0), within the same site (domain), that link to 0 . These links are known as 0 's internal inlinks. Then the procedure collects all anchor text  from pages (denoted as (0)) that are linked to any page in (0) from outside the site. The anchor text set  is known as external anchor text and is used as  for 0 .",null,null
,,,
36,"Figure 1 illustrates the procedure by using a real-world example from the TREC GOV2 collection. We collect the auxiliary anchor text  for the page 0. 0's original anchor text (denoted as ), which comes from all pages (denoted as (0)) that are directly linked to 0 from outside the site, consists of lines including ""Optima National Wildlife Refuge"" and ""Optima NWR"". 0's  consists of lines including ""Oklahoma Refuge Websites"" and ""Oklahoma National Wildlife Refuges"".",null,null
,,,
37,"Note that the above procedure does not use any anchor text associated with internal inlinks, because internal inlinks are typically generated by the owner of the site for navigational purposes and their associating anchor text tends to be navigational in nature (e.g., ""home"",""next page"", etc.; refer to [14] for more discussions on this issue). We emphasize that in the remainder part of this paper we follow Metzler et al. and do not use the anchor text associated with internal inlinks in any way.",null,null
,,,
38,"In this paper we are specifically interested in the effectiveness of using  to serve as a surrogate for possibly missing original anchor text. In other words, we consider how effectively we may use  to discover plausibly missing original anchor text of the URL of the interest so that anchor text sparsity can be effectively reduced. Therefore, we focus on the discovered anchor terms themselves in the . We use two typical methods to rank the relative importance of each anchor term . The first method, denoted as AUX-TF, is to use each term 's term frequency () in the .",null,null
,,,
39,428,null,null
,,,
40,http://saltplains.fws.gov/index.html,null,null
,,,
41,Oklahoma Refuge Websites,null,null
,,,
42,... P5,null,null
,,,
43,auxiliary anchortext (aggregated),null,null
,,,
44,Pages within,null,null
,,,
45,P2,null,null
,,,
46,the same site,null,null
,,,
47,P1 P0,null,null
,,,
48,http://saltplains.fws.gov/just4kid s.html,null,null
,,,
49,Oklahoma National Wildlife Refuges,null,null
,,,
50,P6,null,null
,,,
51,http://ifw2irm2.irm1.r2.fws.gov/texas.html,null,null
,,,
52,Buffalo Lake NWR,null,null
,,,
53,P7,null,null
,,,
54,Similar Pages:,null,null
,,,
55,P3,null,null
,,,
56,P4,null,null
,,,
57,Optima National Wildlife Refuge,null,null
,,,
58,P8,null,null
,,,
59,original anchortext,null,null
,,,
60,..... Optima NWR,null,null
,,,
61,..... P9,null,null
,,,
62,"PIn(P0),""{P1, P2} POrig (P0)"",""{P8, P9} PAux (P0)"",""{P5, P6} POrig (P4)"",{P7}",null,null
,,,
63,http://ifw2es.fws.gov/Oklahoma/refuges.htm l http://ifw2irm2.irm1.r2.fws.gov/toklahoma.html,null,null
,,,
64,P0: http://southwest.fws.gov/refuges/oklahoma/optima.ht ml. P1 :http://southwest.fws.gov/oklahoma.html . P2: http://southwest.fws.gov/refuges/okrefuges.html . P4 : http://southwest.fws.gov/refuges/texas/buffalo.html .,null,null
,,,
65,"Figure 1: Illustration of how to aggregate anchor text over the web graph or use similar web pages' anchor text for discovering more anchor text for a web page (0 in this example). The page 0 is a GOV2 web page, whose DocID is GX010-01-9459902 and URL is http://southwest.fws.gov/refuges/oklahoma/optima.html.",null,null
,,,
66,"The second method, denoted as AUX-TFIDF, is to use each term 's    () score, computed by multiplying () with 's  score in the web collection. The quality of the discovered anchor term rank lists produced from these two link based approaches implies the effectiveness of using auxiliary anchor text as a surrogate of missing original anchor text. We will compare these two approaches with our content based approach in §4.",null,null
,,,
67,3.2 Discovering Anchor Text through Finding Similar Web Pages,null,null
,,,
68,"Note that in the link based approach, a web page 0 still",null,null
,,,
69,cannot obtain the auxiliary anchor text if it has no internal,null,null
,,,
70,inlinks or if all pages in its (0) have no external anchor,null,null
,,,
71,"text. Indeed, Metzler et al. reported only 38% anchor text",null,null
,,,
72,sparsity reduction on a web sample with the link based ap-,null,null
,,,
73,"proach[14]. Therefore, we propose a content based approach,",null,null
,,,
74,which does not have specific link structure requirements on,null,null
,,,
75,"the target web page, to discover its plausible missing an-",null,null
,,,
76,"chor text. Intuitively, our approach assumes that web pages",null,null
,,,
77,that are similar in content may be described by similar as-,null,null
,,,
78,"sociated anchor text. For example, in Figure 1, the target",null,null
,,,
79,"page 0, which is about Optima national wildlife refuge, is",null,null
,,,
80,"similar in content with the page 4, which is about Buffalo",null,null
,,,
81,Lake national wildlife refuge. We observe that the anchor,null,null
,,,
82,"term ""NWR"", which appears in 0's and 4's  but not",null,null
,,,
83,"in 0's , can be used to partially describe both 0 and",null,null
,,,
84,4 although two pages are concerned about different places.,null,null
,,,
85,We consider a language modeling approach to better use,null,null
,,,
86,"document similarity and anchor text information, based on",null,null
,,,
87,ideas from the relevance-based language models (RM)[10].,null,null
,,,
88,"In brief, given a query , RM first calculates the posterior",null,null
,,,
89,() of each document  in the collection  generating,null,null
,,,
90,"the query , then calculates a query dependent language",null,null
,,,
91,model ():,null,null
,,,
92,"() ,",null,null
,,,
93,"() × (),",null,null
,,,
94,-1,null,null
,,,
95,"where  is a word from the vocabulary  of . Similarly, given an target page 0, our approach aims to calculate a relevant anchor text language model (RALM) (0) by:",null,null
,,,
96,"(0) ,",null,null
,,,
97,"() × (0),",null,null
,,,
98,-2,null,null
,,,
99,"where  denotes the complete original anchor text that should be associated with  but may be missing,  denotes the complete original anchor text space for all pages, () is a multinomial distribution over the anchor text vocabulary . To compute (0) in Equation 2 where 0 and  information may be missing, we view each page 's content as its anchor text 's context and use 's document language model  , {()} as 's contextual model. Then we can calculate a translation model (0) by using 0 and 's contextual models and use (0) to approximate (0). This contextual translation approach is also used in Wang and Zhai's work [19].",null,null
,,,
100,"When calculating a page 's document language model {()}, we employ Dirichlet smoothing on the maximum likelihood (ML) estimate of observing a word  in the page (()) with the word's collection probability ():",null,null
,,,
101,(),null,null
,,,
102,",",null,null
,,,
103,  +,null,null
,,,
104,(,null,null
,,,
105,),null,null
,,,
106,+,null,null
,,,
107," (),  + ",null,null
,,,
108,-3,null,null
,,,
109,"where  is the length of 's content and  is the Dirichlet smoothing parameter ( , 2500 in our experiments). Then",null,null
,,,
110,"given two pages 0 and , we use the Kullback-Leibler divergence (KL) () between their document models",null,null
,,,
111,0 and  to measure their similarity and view that as the contextual similarity between the associated anchor text 0 and . Then the contextually based translation probability (0) is calculated by:,null,null
,,,
112,0,null,null
,,,
113,",",null,null
,,,
114,. exp(-(0  )),null,null
,,,
115, exp(-(0)),null,null
,,,
116,-4,null,null
,,,
117,429,null,null
,,,
118,This (0) is then used to approximate (0) in Equation 2 to get:,null,null
,,,
119,0,null,null
,,,
120,() × (0).,null,null
,,,
121,-5,null,null
,,,
122,A few transformations of Equation 4 can obtain:,null,null
,,,
123,"(0)   ()(0),",null,null
,,,
124,-6,null,null
,,,
125,"which is the likelihood of generating 0's context 0 from 's context 's smoothed language model and being normalized by 0's context length. This likelihood can be easily obtained by issuing 0 as a long query to any language model based search engine. In addition, we use the observed incomplete original anchor text language model () associated with  to approximate () in Equation 5, and let () ,"" 0 if  has no . In this way, the RALM (0) can be computed.""",null,null
,,,
126,"In practice, for efficiency the RALM of the target page 0 is computed from 0's top- most similar pages'  (original anchor text) because (0) in Equation 4 is very small for the other pages. Due to the anchor text sparsity, we set  ,"" 2000 in our experiments. Because some of these similar pages do not have associated , we use another parameter  to denote the number of most similar pages whose associated original anchor text is not missing and contributes information in the RALM, and we tune  in the experiments. Intuitively, increasing  can increase the number of anchor text samples to better estimate RALM but may also introduce more noise when the sample size is large.""",null,null
,,,
127,"The probability (0) of an anchor term  in the RALM directly reflects the goodness of the term  used as original anchor text for the page 0, thus we use the anchor terms that have the largest probabilities (0) in the RALM to evaluate the effectiveness of our content based approach. Theoretically our approach can associate any web page with some anchor term distribution information if there is some anchor text in the corpus, thus it can further reduce the anchor text sparsity.",null,null
,,,
128,3.3 Using Keywords as Anchor Text,null,null
,,,
129,The keyword based approaches come from the intuition,null,null
,,,
130,that important keywords in a web page may be good de-,null,null
,,,
131,"scription terms for the page, thus may be arguably used as",null,null
,,,
132,anchor text. We use three typical term weighting schemes,null,null
,,,
133,to identify the keywords and rank the words in a web page's,null,null
,,,
134,"content. The first method, denoted as DOC-TF, uses each",null,null
,,,
135,"word 's term frequency 0 () in the page 0 for term weighting. The second method, denoted as DOC-TFIDF,",null,null
,,,
136,"uses each word 's 0   () score, computed by multiplying 0 () with 's  score in the web collection. The third method, denoted as DOC-OKAPI, uses each word",null,null
,,,
137,"'s Okapi BM25 score  250 (), computed by:",null,null
,,,
138," 25() ,",null,null
,,,
139,0 ()(1+1),null,null
,,,
140,0,null,null
,,,
141,()+1,null,null
,,,
142,(1-+,null,null
,,,
143,0,null,null
,,,
144,"  (),",null,null
,,,
145,-7,null,null
,,,
146,"where  is the average document length of the pages in the collection. We use the typical setting 1 ,"" 2,  "", 0.75 in Equation 7 in our experiments.",null,null
,,,
147,The top ranked terms in a page 0 by three methods are used as the possible missing original anchor terms for 0. We will use three keyword based methods as baselines in §4.,null,null
,,,
148,4. EVALUATING DISCOVERY,null,null
,,,
149,"We now compare the capability of discovering missing anchor text by different approaches described in §3, including two link based approaches (AUX-TF and AUX-TFIDF), our content based approach (RALM), and three keyword based approaches (DOC-TF, DOC-TFIDF and DOC-OKAPI).",null,null
,,,
150,4.1 Data and Methodology,null,null
,,,
151,"We use two publicly available large TREC web collections (GOV2 and ClueWeb09-T09B). GOV2 is a standard TREC web collection [4] crawled from government web sites during early 2004. The ClueWeb09 collection is a much larger and more recent web crawl, which contains over 1 billion pages. ClueWeb09-T09B is a subset of ClueWeb09 and contains about 50 million English web pages. Compared with GOV2 crawled only from the gov domain, ClueWeb09T09B is crawled from the general web thus is a less biased web sample; in another aspect, GOV2 contains relatively high quality government web pages thus having less noise than ClueWeb09-T09B. Thus we use both GOV2 and ClueWeb09-T09B in our experiments to show how different approaches perform in web collections that have different characteristics. The Indri Search Engine4 was used to index both collections by removing a standard list of 418 INQUERY [2] stopwords and applying Krovetz stemmer. In a separate process, we run Indri Search Engine's harvestlinks utility on the two collections to collect web page inlinks and raw anchor text information where we do not perform stopping or stemming.",null,null
,,,
152,"To evaluate the quality of discovered anchor text for a web page 0, we utilize the original anchor text  associated with all inlinks of 0. Specifically, we first hide the page 0's , apply different anchor text discovery approaches on 0, then compare the discovered anchor text with 0's . This procedure can be run automatically so that we can leverage large volumes of web pages to evaluate the performance of different approaches with no human labeling effort. More specifically, we consider each anchor term in a page 0's  as a good description term, or a relevant term, for 0 while terms not in  as non-relevant ones; in this way, we can generate term relevance judgments for 0. Then we employ each different approach to discover a ranked list of plausible missing anchor terms for 0 and then use the relevant judgments to evaluate the ranked anchor term list. Note that for fair comparison 0's  is not used in Equation 2 for calculating RALM in our approach. In the experiments, we perform slight stopping on the raw anchor text by removing a short list of 39 stopwords, which includes 25 common stopwords[12, pp.26] and 14 additional anchor terms5 that are either common navigational purposed words or part of URLs ­ it is common that anchor text contains some URL.",null,null
,,,
153,"We calculate some typical TREC style evaluation measurements including Mean Average Precision (MAP), Mean Reciprocal Rank(MRR), Precision at the number of relevant terms(R-Prec), Precision at  (P@) and also normalized discounted cumulative gain (NDCG) [7]. In the experiments, we are specifically interested in the quality of top ranked discovered anchor terms; thus, we only use the",null,null
,,,
154,"4http://www.lemurproject.org/indri/ 5The additional terms are: http, https, www, gov, com, org, edu, net, html, htm, click, here, next, home.",null,null
,,,
155,430,null,null
,,,
156,DOC-TF DOC-TFIDF DOC-OKAPI,null,null
,,,
157,AUX-TF AUX-TFIDF,null,null
,,,
158,RALM,null,null
,,,
159,MAP 0.3162 0.2936 0.2936 0.1969 0.1716 0.3183,null,null
,,,
160,NDCG 0.4585 0.4348 0.4348 0.2598 0.2423 0.4275,null,null
,,,
161,MRR 0.5441 0.5400 0.5400 0.3707 0.3442 0.5050,null,null
,,,
162,P@2 0.3833 0.3700 0.3700 0.2833 0.2433 0.3467,null,null
,,,
163,P@5 0.2800 0.2613 0.2613 0.1773 0.1720 0.2840,null,null
,,,
164,P@10 0.2060 0.1827 0.1827 0.1153 0.1140 0.1860,null,null
,,,
165,P@20 0.1333 0.1240 0.1240 0.0643 0.0647 0.1140,null,null
,,,
166,R-Prec 0.2716 0.2530 0.2530 0.1643 0.1428 0.3051,null,null
,,,
167,Discovered Rel. 400 372 372 193 194 342,null,null
,,,
168,"Table 2: Performances on the GOV2 collection. There are 708 relevant anchor terms overall. Column 10 shows overall relevant anchor terms discovered by each different approach. RALM performs statistically significantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.005). There exists no statistically significant difference between each pair of RALM, DOC-TF, DOC-TFIDF and DOC-OKAPI by each measurement according to the one-sided t-test ( < 0.05).",null,null
,,,
169,DOC-TF DOC-TFIDF DOC-OKAPI,null,null
,,,
170,AUX-TF AUX-TFIDF,null,null
,,,
171,RALM,null,null
,,,
172,MAP 0.3517 0.3107 0.3107 0.1840 0.1634 0.2612,null,null
,,,
173,NDCG 0.4891 0.4388 0.4388 0.2507 0.2347 0.3615,null,null
,,,
174,MRR 0.5588 0.5145 0.5145 0.3309 0.3116 0.4630,null,null
,,,
175,P@2 0.3467 0.3133 0.3133 0.2248 0.2047 0.2833,null,null
,,,
176,P@5 0.2373 0.2213 0.2213 0.1463 0.1383 0.1733,null,null
,,,
177,P@10 0.1360 0.1173 0.1173 0.0729 0.0676 0.0911,null,null
,,,
178,P@20 0.1090 0.0983 0.0983 0.0577 0.0560 0.0770,null,null
,,,
179,R-Prec 0.2990 0.2608 0.2608 0.1675 0.1402 0.2398,null,null
,,,
180,Discovered Rel. 327 295 295 172 167 231,null,null
,,,
181,Table 3: Performances on the ClueWeb09-T09B collection. There are 582 relevant anchor terms overall. Column 10 shows overall relevant anchor terms discovered by each different approach. DOC-TF performs statistically significantly better than both RALM and AUX-TF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.05). RALM performs statistically significantly better than AUX-TF and AUX-TFIDF by each measurement in columns 2­9 according to the one-sided t-test ( < 0.05).,null,null
,,,
182,top-20 terms in the discovered term rank lists by different approaches to calculate the measurements.,null,null
,,,
183,"Note that web pages that can be used in our evaluation procedure need to satisfy two requirements: (1) they need to have some associated  and (2) they can collect some auxiliary anchor text from the web graph as described in §3.1. Thus, for each of two collections, we randomly sample 150 pages satisfying the two requirements for training and another 150 pages for testing. On both training sets, RALM's parameter  , 15 described in §3.2 achieves the highest MAPs.",null,null
,,,
184,4.2 Results and Analysis,null,null
,,,
185,"The performance of discovering original anchor text by different approaches on the testing set of GOV2 and ClueWeb09-T09B are shown in Table 2 and Table 3, respectively. The results show that our approach (RALM) can effectively discover missing original anchor terms. On both collections RALM performs statistically significantly better than two link based approaches (AUX-TF and AUX-TFIDF). This indicates that, for discovering a page's missing anchor text, the anchor text associated with the similar pages provides more useful information than that associated with the linked web neighbors. The numbers of discovered relevant anchor terms by different approaches, shown in the last column of two tables, also indicate that only using auxiliary anchor text misses more original anchor text information than our content based approach.",null,null
,,,
186,Another observation is that RALM performs worse on ClueWeb09-T09B and not statistically significantly better on GOV2 than the keyword based approaches. This indicates that words having high IR utility like  or    scores are often good description terms for the page and used by human being as the anchor text. Removing a long list of stopwords from web page content has also helped the keyword based approaches to effectively select good descrip-,null,null
,,,
187,"(AUX-TF, DOC-TF) (AUX-TF, RALM) (RALM, DOC-TF)",null,null
,,,
188,GOV2 30.5% 47.6% 26.0%,null,null
,,,
189,ClueWeb09-T09B 26.0% 46.3% 22.3%,null,null
,,,
190,"Table 4: The average percentage (,  ) of the terms discovered by the  approach appearing in the ones discovered by the  approach.",null,null
,,,
191,"tion words from the web content. One plausible reason that RALM performs relatively poorly on ClueWeb09-T09B is that, compared with the high quality GOV2 pages, ClueWeb pages are crawled from the general web, where the inlinks and anchor text may be generated in a more noisy way (e.g. spam), degrading RALM's performance. To better understand the performance of different approaches, in Table 5 and Table 6 we show the top-10 words of the anchor term rank lists discovered by different approaches for one evaluation web page in GOV2 and ClueWeb09-T09B, respectively.",null,null
,,,
192,"Although using keyword information can discover some good anchor terms, the content-generated anchor terms do not help bridging the lexical gap between a web page and varied queries that attempt to search the page. Indeed, human generated anchor text is highly useful for reducing the word mismatch problem because the lexical gap between anchor text and queries is relatively small[14]. Here, we do some lexical gap analysis to show that our approach can also discover anchor terms similar in nature to human-generated ones but different from content-generated ones.",null,null
,,,
193,"For each web page  in the testing set, we calculate the percentage (,  ) of the terms discovered by the  approach also appearing in the ones discovered by the  approach, then compute the average percent (,  ) with all the pages. We use the outputs from the keyword based DOC-TF, the link based AUX-TF, and the RALM in this analysis. Table 4 shows three average percentages (,  )",null,null
,,,
194,431,null,null
,,,
195,"which we have specific interest in. We observe that AUXTF's discovered terms have much higher average per query overlap ratio with RALM's than with DOC-TF's. Moreover, RALM's discovered anchor terms have small overlap with DOC-TF's.",null,null
,,,
196,5. USING DISCOVERED ANCHOR TEXT FOR WEB SEARCH,null,null
,,,
197,"We now describe how we use the discovered anchor text by different approaches for retrieval in a language modeling approach [17]. We point out that our focus here is not to evaluate different schemes to aggregate or combine anchor text [14]; instead, we focus on comparing the utility of RALM and auxiliary anchor text for helping retrieval.",null,null
,,,
198,5.1 Retrieval Models,null,null
,,,
199,We follow the typical language modeling based retrieval,null,null
,,,
200,approach[17] and score each web page  for a query  by the,null,null
,,,
201,likelihood of the page  's document language model ( ),null,null
,,,
202,generating the query :,null,null
,,,
203,"( ) , ( ).",null,null
,,,
204,-8,null,null
,,,
205,"When using Dirichlet smoothing, the document language model ( ) can be calculated by Equation 3 and then used in Equation 8 for retrieval. We call this baseline QL. We only fix  ,"" 2500 in Equation 3 for the document models used to calculate RALM, but tune the  for QL to achieve the best retrieval performance in our experiments in §5.2.""",null,null
,,,
206,"We follow the mixture model approach [15, 16] to use the discovered anchor text information for helping retrieval. In this approach, a web page  's document language model is assumed to be a mixture of multiple component distributions where each component is associated with a prior probability, or a mixture weight. Therefore, we can estimate a language model () from anchor text discovered by each different approach for the page  and use () as a component of  's document model thus obtaining a better document language model ~( ):",null,null
,,,
207,"~( ) ,"" ( ) + (1 - )(),""",null,null
,,,
208,-9,null,null
,,,
209,where ( ) is the original smoothed document model in the QL baseline. Then we can plug ~( ) into equation 8 for retrieval. We compare the retrieval performance of document language models updated by different discovered anchor text information.,null,null
,,,
210,"We consider three different anchor text sources to update a web page  's document model: (1) the observed original anchor text  associated with  , (2) the auxiliary anchor text  of  , and (3) the RALM computed by our approach for  . We estimate the anchor text language model () and () by using the ML estimate of observing each word  in  and , respectively. Here, we design the following five retrieval methods that use the above three anchor text sources: 1. M-ORG, which only uses the observed original anchor text language (). 2. M-AUX, which only uses the auxiliary anchor text language (). 3. M-ORG-AUX, which uses both () and () to update the document model ( ) by:",null,null
,,,
211,"~( ) , (( ) + (1 - )()) +(1 - )().",null,null
,,,
212,-10,null,null
,,,
213,QL M-ORG M-AUX M-ORG-AUX M-RALM M-ORG-RALM,null,null
,,,
214,MRR 0.3132 0.3696 0.3187 0.3711 0.3388 0.3975,null,null
,,,
215,%Top10 49.7 57.5 50.8 57.5 53.6 59.7,null,null
,,,
216,Opt. Param.,null,null
,,,
217," , 0.95  , 0.99  ,"" 0.95,  "", 0.99  ,"" 20,  "","" 0.95 ,  "","" 0.95,  "", 20",null,null
,,,
218,Table 7: Retrieval performance of different approaches with TREC 2006 NP queries. The star indicates statistically significant improvement over MRRs of M-ORG and M-ORG-AUX by one-sided t-test ( < 0.05). The triangle indicates statistically significant improvement over MRRs of QL and MAUX by one-sided t-test ( < 0.05).,null,null
,,,
219,"4. M-RALM, which only uses the RALM (0) in Equation 2. The original anchor text of 0 is not used in Equation 2 for calculating RALM. 5. M-ORG-RALM, which uses both () and the RALM (0) in Equation 2 by:",null,null
,,,
220,"~( ) , (( ) + (1 - )()) +(1 - )(0).",null,null
,,,
221,-11,null,null
,,,
222,The original anchor text of 0 is not used in Equation 2 for calculating RALM.,null,null
,,,
223,"Note that we can update each page's document model offline, thus this computationally expensive procedure has little impact on the online query processing time. Moreover, different from experiments in §4.1, we use all anchor terms instead of the top-20 most important terms discovered by different approaches.",null,null
,,,
224,5.2 Experiments,null,null
,,,
225,"We use the TREC web named page finding tasks in Terabyte Track[4, 5] to evaluate the performance of different retrieval methods described in §5.1. The objective of the named page (NP) finding task is to find a particular page in the GOV2 collection, given a topic that describes it. We use the NP topics and their relevance judgments for our experiments. In this experiment, we used Porter stemmer and did not remove stopwords when indexing the GOV2 collection.",null,null
,,,
226,"For each NP query, we first run it against the GOV2 collection to obtain the QL baseline; then we use five retrieval methods described in §5.1 to rerank the top-100 web pages returned by QL. The reranked lists are evaluated by two TREC measurements previously used for the task [5]: MRR which is the mean reciprocal rank of the first correct answer and the %Top10 which is the proportion of queries for which a correct answer was found in the first 10 search results. We use the TREC 2005 NP topics (NP601-872) for training and the TREC 2006 NP topics (NP901-1081) for testing. We first tune the Dirichlet parameter  , 500 for QL to achieve the highest MRR on the training set and obtain QL's top-100 web pages for reranking. We then fix  ,"" 500 to calculate the smoothed document model component ( ) in the five retrieval methods but tune the mixture parameters  and  for them to achieve the highest MRRs with the training queries. For the two approaches that use RALM, the parameter  of RALM is also tuned. After that, we run different methods on the testing set.""",null,null
,,,
227,Table 7 shows the retrieval performance of different methods and the tuned parameters in each method. We observe: (1) M-ORG-RALM performs statistically significantly bet-,null,null
,,,
228,432,null,null
,,,
229,"""Optima National Wildlife Refuge"", ""Optima NWR"", ""Washita Optima National Wildlife Refuge near Butler OK""",null,null
,,,
230,DOC-TF,null,null
,,,
231,refuge wildlife oklahoma optima species hawk habitat,null,null
,,,
232,area prairie national,null,null
,,,
233,0 () 15 10 10 8 6 6 6 6 5 5,null,null
,,,
234,DOC-TFIDF,null,null
,,,
235,refuge optima hardesty hawk oklahoma wildlife guymon habitat species quail,null,null
,,,
236,0  () 79.69 74.30 47.48 36.20 36.03 31.98 29.35 26.42 23.70 21.74,null,null
,,,
237,DOC-OKAPI,null,null
,,,
238,refuge optima hardesty hawk oklahoma wildlife guymon habitat species quail,null,null
,,,
239, 250 () 153.76 143.37 91.63 69.86 69.53 61.71 56.63 50.98 45.73 41.95,null,null
,,,
240,AUX-TF,null,null
,,,
241,oklahoma wildlife refuge website u service s office national fish,null,null
,,,
242,() 6 2 2 1 1 1 1 1 1 1,null,null
,,,
243,AUX-TFIDF,null,null
,,,
244,oklahoma refuge wildlife fish u website office s national service,null,null
,,,
245, () 21.62 10.62 6.40 3.11 3.03 2.36 1.54 1.29 1.22 1.09,null,null
,,,
246,RALM,null,null
,,,
247,nwr wildlife refuge national general brochure kansas,null,null
,,,
248,lake tear sheet,null,null
,,,
249, (0) 0.1164 0.0834 0.0834 0.0834 0.0657 0.0657 0.0601 0.0522 0.0308 0.0308,null,null
,,,
250,Rel.,null,null
,,,
251,butler national,null,null
,,,
252,near nwr optima refuge washita wildlife,null,null
,,,
253,"Table 5: Discovered missing anchor terms and their term weights by applying different approaches on one GOV2 web page (TREC DocID in GOV2: GX010-01-9459902) . The first row shows the original three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments extracted from the first row. RALM can discover some term like ""NWR"", which may not appear in both the page and the auxiliary anchor text, thus may help to bridge the lexical gap between pages and web queries as using the original anchor text does.",null,null
,,,
254,"Weight Loss Resolutions, ""Weight Loss New Year's Resolution to Lose Weight"",""Resolve to Lose Weight""",null,null
,,,
255,DOC-TF,null,null
,,,
256,weight loss lose new year,null,null
,,,
257,resolution time make goal diet,null,null
,,,
258,0 () 46 26 20 17 15 13 12 10 9 9,null,null
,,,
259,DOC-TFIDF,null,null
,,,
260,weight loss lose,null,null
,,,
261,resolution diet goal eat year,null,null
,,,
262,calorie pound,null,null
,,,
263,0  () 96.38 78.65 64.47 46.57 34.27 26.01 25.61 23.90 15.73 15.34,null,null
,,,
264,DOC-OKAPI,null,null
,,,
265,weight loss lose,null,null
,,,
266,resolution diet goal eat year,null,null
,,,
267,calorie pound,null,null
,,,
268, 250 () 112.53 91.83 75.28 54.38 40.02 30.37 29.90 27.90 18.36 17.91,null,null
,,,
269,AUX-TF,null,null
,,,
270,weight loss diet,null,null
,,,
271,weightloss guide scott jennifer contact site s,null,null
,,,
272,() 709 705 32 21 20 8 8 8 6 4,null,null
,,,
273,AUX-TFIDF,null,null
,,,
274,loss weight weightloss,null,null
,,,
275,diet guide jennifer scott guidesite,null,null
,,,
276,em mlibrary,null,null
,,,
277, () 2132.63 1485.49 157.70 121.86 37.26 33.96 28.52 22.04 13.15 11.37,null,null
,,,
278,RALM,null,null
,,,
279,weight loss diet easy lose way myth warn ppa fda,null,null
,,,
280, (0) 0.2245 0.1737 0.0550 0.0436 0.0422 0.0412 0.0396 0.0232 0.0232 0.0232,null,null
,,,
281,Rel.,null,null
,,,
282,lose loss new resolution resolve,null,null
,,,
283,s weight,null,null
,,,
284,"Table 6: Discovered missing anchor terms and their term weights by applying different approaches on one ClueWeb09 web page (ClueWeb09 RecordID: clueweb09-en0004-60-01628). The first row shows the original three pieces of anchor text associated with the page. The Rel column in bold font shows the term relevance judgments extracted from the first row. The keyword approaches discovered ""new year resolution"", which may be hard to be discovered by using the page's web-graph neighbor pages' anchor text or using the page's similar pages' anchor text.",null,null
,,,
285,433,null,null
,,,
286,"ter than M-ORG. This indicates that missing anchor text discovered by RALM provides additional information not in the original anchor text so that combining them can further improve the retrieval performance. (2) M-ORG-RALM and M-RALM performs statistically significantly better than MORG-AUX and M-AUX, respectively. This indicates that in GOV2 missing anchor text information discovered by our content based approach helps retrieval more effectively than the auxiliary anchor text.6",null,null
,,,
287,"In Table 7, we observe that the auxiliary anchor text helps the performance very little in this task. There are two plausible reasons: first, TREC NP queries are short queries and Metzler et al. observed that auxiliary anchor text does not help or even hurts the performance of short navigational web queries[14]; second, the anchor text sparsity problem is serious on the GOV2, thus very small percentage of pages can collect some auxiliary anchor text as shown in Table 1 to benefit the search task. However, even when serious anchor text sparsity exists and queries are short, our content based approach still helps improving retrieval effectiveness.",null,null
,,,
288,"We expect our technique can enhance the retrieval performance of general web search engines where there are large portion of short navigational queries. As is well known, in the general web search environment there are many lowquality web pages and spam; thus, we need to address issues about web page quality and noise filtering for better benefitting general web search. We leave this as future work.",null,null
,,,
289,6. CONCLUSIONS AND FUTURE WORK,null,null
,,,
290,"In this paper, we proposed a language modeling based technique to overcome the anchor text sparsity problem by using web content similarity. Our approach computes a relevant anchor text language model, called RALM, from its similar web pages' associated anchor text to discover its plausible missing anchor text. Compared with a link based approach [14], our content based approach has no specific link structure requirements on the web page of interest and thus can further reduce anchor text sparsity.",null,null
,,,
291,"We designed experiments with two TREC web corpora to evaluate the effectiveness of discovering missing anchor terms by three different approaches: the link based approach, the RALM approach, and the keyword based approach. Experimental results show that the RALM approach can effectively discover missing original anchor text and performs statistically significantly better than the two link based approaches on both collections. Moreover, RALM's discovered anchor text is similar in nature to auxiliary anchor text while different from the keywords in the web page.",null,null
,,,
292,"By using the mixture model[15, 16], we used different discovered anchor text information within the language modeling framework for retrieval. We evaluated using different approaches for improving retrieval effectiveness with the TREC named page finding task. The results show that (1) RALM helps retrieval more than using the auxiliary anchor text collected over the web graph and (2) combining RALM and the original anchor text can statistically significantly improve the retrieval performance of only using the original anchor text. Furthermore, RALM can help improving retrieval effectiveness for short navigational queries even when serious anchor text sparsity exists. This makes RALM a promising technique for improving general web search engines.",null,null
,,,
293,"6Our goal is not to compare ranking schemes, but to show the utility of the discovered anchor text. However, we note that these scores match or beat top-performing approaches [4].",null,null
,,,
294,"There are several interesting directions of future work. Metzler et al. found that auxiliary anchor text can effectively help longer, informational queries [14]; we will explore how well RALM can help long informational queries. We also want to explore using RALM's discovered missing anchor text information beyond the language modeling based retrieval framework, e.g. using it to extract useful features for learning-to-rank retrieval approaches [3].",null,null
,,,
295,7. ACKNOWLEDGMENTS,null,null
,,,
296,"This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",null,null
,,,
297,8. REFERENCES,null,null
,,,
298,"[1] A. Broder et al. Graph structure in the web. Comput. Netw., 33(1-6):309­320, 2000.",null,null
,,,
299,"[2] J. Broglio, J. P. Callan, and W. B. Croft. An overview of the INQUERY system as used for the TIPSTER project. Technical report, Amherst, MA, USA, 1993.",null,null
,,,
300,"[3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. of ICML, pp. 89­96, 2005.",null,null
,,,
301,"[4] S. Bu¨ttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 Terabyte Track. In TREC, 2006.",null,null
,,,
302,"[5] C. L. A. Clarke, F. Scholer, and I. Soboroff. The TREC 2005 Terabyte Track. In TREC, 2005.",null,null
,,,
303,"[6] A. Fujii. Modeling anchor text and classifying queries to enhance web document retrieval. In Proc. of WWW, pp. 337­346, 2008.",null,null
,,,
304,"[7] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst., 20(4):422­446, 2002.",null,null
,,,
305,"[8] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In SIGIR, pp. 194­201, 2004.",null,null
,,,
306,"[9] O. Kurland and L. Lee. Respect my authority!: Hits without hyperlinks, utilizing cluster-based language models. In SIGIR, pp. 83­90, 2006.",null,null
,,,
307,"[10] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR, pp. 120­127, 2001.",null,null
,,,
308,"[11] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In SIGIR, pp. 186­193, 2004.",null,null
,,,
309,"[12] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge Univ. Press. 2008.",null,null
,,,
310,"[13] Q. Mei, D. Zhang, and C. Zhai. A general optimization framework for smoothing language models on graph structures. In SIGIR, pp. 611­618, 2008.",null,null
,,,
311,"[14] D. Metzler, J. Novak, H. Cui, and S. Reddy. Building enriched document representations using aggregated anchor text. In SIGIR, pp. 219­226, 2009.",null,null
,,,
312,"[15] R. Nallapati, B. Croft, and J. Allan. Relevant query feedback in statistical language modeling. In Proc. of CIKM, pp. 560­563, 2003.",null,null
,,,
313,"[16] P. Ogilvie and J. Callan. Combining document representations for known-item search. In SIGIR, pp. 143­150, 2003.",null,null
,,,
314,"[17] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In SIGIR, pp. 275­281, 1998.",null,null
,,,
315,"[18] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In Proc. of NAACL-HLT, pp. 407­414, 2006.",null,null
,,,
316,"[19] X. Wang and C. Zhai. Mining term association patterns from search logs for effective query reformulation. In Proc. of CIKM, pp. 479­488, 2008.",null,null
,,,
317,"[20] T. Westerveld, W. Kraaij, and D. Hiemstra. Retrieving web pages using content, links, urls and anchors. In Proc. of TREC, pp. 663­672, 2001.",null,null
,,,
318,434,null,null
,,,
319,,null,null

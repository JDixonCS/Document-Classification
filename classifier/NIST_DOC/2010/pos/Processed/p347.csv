,sentence,label,data
0,Estimating Probabilities for Effective Data Fusion,null,null
1,David Lillis,null,null
2,School of Computer Science and Informatics,null,null
3,University College Dublin,null,null
4,david.lillis@ucd.ie,null,null
5,Rem W. Collier,null,null
6,School of Computer Science and Informatics,null,null
7,University College Dublin,null,null
8,rem.collier@ucd.ie,null,null
9,Lusheng Zhang,null,null
10,School of Computer Science and Informatics,null,null
11,University College Dublin,null,null
12,lu-sheng.zhang @ucdconnect.ie,null,null
13,David Leonard,null,null
14,School of Computer Science and Informatics,null,null
15,University College Dublin,null,null
16,david.leonard@ucd.ie,null,null
17,Fergus Toolan,null,null
18,School of Computer Science and Informatics,null,null
19,University College Dublin,null,null
20,fergus.toolan@ucd.ie,null,null
21,John Dunnion,null,null
22,School of Computer Science and Informatics,null,null
23,University College Dublin,null,null
24,john.dunnion@ucd.ie,null,null
25,ABSTRACT,null,null
26,"Data Fusion is the combination of a number of independent search results, relating to the same document collection, into a single result to be presented to the user. A number of probabilistic data fusion models have been shown to be effective in empirical studies. These typically attempt to estimate the probability that particular documents will be relevant, based on training data. However, little attempt has been made to gauge how the accuracy of these estimations affect fusion performance. The focus of this paper is twofold: firstly, that accurate estimation of the probability of relevance results in effective data fusion; and secondly, that an effective approximation of this probability can be made based on less training data that has previously been employed. This is based on the observation that the distribution of relevant documents follows a similar pattern in most high-quality result sets. Curve fitting suggests that this can be modelled by a simple function that is less complex than other models that have been proposed. The use of existing IR evaluation metrics is proposed as a substitution for probability calculations. Mean Average Precision is used to demonstrate the effectiveness of this approach, with evaluation results demonstrating competitive performance when compared with related algorithms with more onerous requirements for training data.",null,null
27,Categories and Subject Descriptors,null,null
28,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
29,General Terms,null,null
30,"Algorithms, Experimentation",null,null
31,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",null,null
32,Keywords,null,null
33,"information retrieval, probabilistic data fusion, results merging",null,null
34,1. INTRODUCTION,null,null
35,"In the context of Information Retrieval (IR), many researchers have attempted to use data fusion to improve the quality of their results. This involves submitting a query to a number of distinct IR systems (known as ""input systems"", as they provide the inputs to the fusion process) that have access to the same document collection, and subsequently merging their outputs into a single result set to be presented to the user. This is related to, but distinct from, the concept of meta-search (or collection fusion), where the results being merged are from IR systems operating with disjoint (or partially overlapping) document collections [18].",null,null
36,"Many techniques to tackle the data fusion task are available that use only the result sets that are actually being fused. These approaches vary from purely rank-based algorithms such as interleaving [18] to score-based techniques such as linear combinations [3, 14, 17] and the popular CombSum and CombMNZ algorithms [5, 6, 15]. Algorithms based on voting have also been popular [1, 13].",null,null
37,"More recently, attempts have been made to take into account the past performance of input systems when performing fusion [1, 8, 10, 16]. These techniques make use of probabilities to calculate a score on which the final, fused result set will be ranked. Many, however, require detailed training data to be available, from which the probabilities are calculated. Typically, a number of training queries are run, with each of the input systems required to provide results for each. These results are then compared with relevance judgements so as to identify the positions in each result set where relevant documents have been returned. From this data, a model can be built up that predicts the probability of particular documents being relevant, based on the system that returned them and the positions in the respective result sets they occupy.",null,null
38,"This position-level granularity of training data is an onerous requirement to have on a fusion process. Probabilistic data fusion with a minimal requirement for training would be preferable. The aim of this paper is to attempt to perform fusion based on probability of relevance, but without such",null,null
39,347,null,null
40,"a reliance on detailed training data. In order to do this, we must firstly demonstrate that an accurate probability model is indeed beneficial to fusion performance. Following this we attempt to show that this probability of relevance can be approximated by a function of a documents position within a result set. We also outline one candidate function to achieve effective fusion.",null,null
41,"This paper is organised as follows: Firstly, Section 2 outlines a number of considerations that must be taken into account when developing a data fusion solution. Section 3 motivates the work by considering some pre-existing probabilistic fusion models and examines how an accuratelyconstructed probability model can result in effective data fusion. Following from this, Section 4 shows how such a probability model can be estimated by reference to a singlevalue measure of the quality of the inputs to the fusion process. Having chosen the Mean Average Precision evaluation metric as this single-value measure, a set of experiments is outlined in Section 5 that demonstrates the effectiveness of our approach when compared with others. Finally, we outline our conclusions and present some ideas for future work in Section 6.",null,null
42,2. CHARACTERISTICS OF DATA FUSION,null,null
43,"When performing effective data fusion, there are a number of ""effects"" that may be taken into account. These were initially outlined by Vogt and Cottrell in [17].",null,null
44,· The Skimming Effect is based on the observation that relevant documents are more likely to appear at the top of result sets (where an IR system would place those documents it estimates to be most relevant). Thus favouring early-ranked documents when compiling the final result set can result in improved fusion performance.,null,null
45,· The Chorus Effect argues that if multiple input systems agree on the relevance of a document (by including it in each of their result sets) then this is increased evidence of relevance. This is also consistent with Lee's observation that IR systems tend to return the same relevant documents but different nonrelevant ones [7]. Fusion algorithms that attach greater importance to documents that are returned by multiple input systems attempt to exploit this effect.,null,null
46,"· The Dark Horse Effect refers to a situation where an input system returns an unusually high- or low-quality result set. In this situation, if a fusion technique was able to identify a ""dark horse"", it may opt to return only the result set of that input system, rather than performing any fusion. This effect is very difficult to detect and we are not aware of any techniques that attempt to make use of it.",null,null
47,3. PROBABILITY AS A STRATEGY FOR DATA FUSION,null,null
48,A number of data fusion algorithms have been proposed that use the probability of relevance as a method of assigning scores to documents. Aslam and Montague make use of a Bayesian model that uses both the probability of relevance and the probability of non-relevance to rank documents [1]. The probabilities are calculated by examining the precision,null,null
49,"at a number of document levels. Result sets are divided into ranges between these document levels, with appropriate probability values being associated with each range. Manmatha et al. infer probabilities from the ranking scores given to documents by the various input systems [11].",null,null
50,"Another group of probability-based fusion algorithms use training data to calculate a set of probabilities for each of the systems providing result sets to be fused. In this context, training data consists of result sets produced by the same input systems in response to queries for which relevance judgements are available. Having analysed where relevant documents tend to be returned by each inputs system, a probability model is built. For each of the input systems, this model maps a probability score on to each position in which a document may potentially be returned. For instance, System A may have a probability of 0.4 associated with position 1. This would imply that for any given document returned by System A at the top of its result set, there is an estimated probability of 0.4 that the document is relevant. Algorithms utilising this type of probability model include Lillis et al.'s ProbFuse [8] and SlideFuse [10] and the SegFuse algorithm developed by Shokouhi [16].",null,null
51,"This approach to data fusion relies on two fundamental assumptions. Firstly, it is assumed that a system's performance in response to training queries is indicative of how it will perform when faced with different queries. It is also assumed that the construction of an accurate probability model will result in effective fusion. Although the empirical experiments presented in [9, 10, 16] demonstrate effective retrieval performance when compared against the baseline CombMNZ, the accuracy of the probability model is not tested.",null,null
52,The aim of this paper is to examine this second assumption in more detail. Establishing that an accurate model of the probabilities required results in effective fusion further motivates the examination of further methods of constructing such models.,null,null
53,"In order to test this, we evaluated the effectiveness of using a perfect probability model for fusion. This perfect probability model was constructed by using the same queries (and consequently the same result sets) for training as for fusion. The consequence of this is that the probability model perfectly reflects the positions of the relevant documents in the result sets being used for fusion. Clearly, such an approach is not feasible from a practical point of view, as the relevant documents are not known at query time. However, the aim of this experiment is to demonstrate how effective fusion would be if an accurate approximation of the real probability distribution could be constructed.",null,null
54,"The inputs for this experiment were taken from the TREC 2004 Web Track [4]. Five fusion runs were performed, using six input systems each time. The systems were chosen by their overall MAP score, with the six best systems being part of run1, the seventh to twelfth best systems in run2 etc. These inputs consisted of result sets relating to 225 distinct topics (queries).",null,null
55,"The specific inputs used for each run are the same for all experiments presented in this paper, and are as follows:",null,null
56,"· run1: MSRC04B2S, MSRC04C12, MSRC04B1S, MSRAx4, MSRAx2,MSRAmixed1",null,null
57,"· run2: MSRAmixed3, MSRC04B1S2, MSRAx5, UAmsT04MSind, UAmsT04MWScb, UAmsT04MSinu",null,null
58,348,null,null
59,"· run3: UAmsT04MWinu, uogWebSelAn, uogWebSelAnL, MSRC04B3S, THUIRmix045, THUIRmix041",null,null
60,"· run4: uogWebCA, ICT04MNZ3, THUIRmix043, ICT04CIIS1AT, ICT04RULE, THUIRmix042",null,null
61,"· run5: ICT04basic, ICT04CIILC, MeijiHILw1, uogWebSelL, UAmsT04LnuNG, MeijiHILw3",null,null
62,"By way of comparison, the result sets were fused using the SlideFuse and CombMNZ fusion algorithms, which are described in detail in [10] and [5] respectively. SlideFuse is a probabilistic data fusion algorithm that estimates the probability of relevance at each position using training queries. It is chosen as a representative from the family of probabilistic algorithms to which it belongs (also including ProbFuse [9] and SegFuse [16]). In order to compensate for incomplete relevance judgements, where judgements of relevance or nonrelevance are not available for every document in the collection, SlideFuse smooths these probabilities using a sliding window approach. This means that the probabilities associated with each position also depends on the occurrence of relevant documents in neighbouring positions. In contrast, CombMNZ is a much simpler algorithm and has been chosen because it is frequently used as a baseline in fusion experiments. This does not use any training data, but rather uses the scores given to each document by the input systems to rank the fused result set. The details of how these were implemented are given in the following subsections.",null,null
63,3.1 PosFuse,null,null
64,"The approach based on the perfect probability model is described here as ""PosFuse"" (as it is based on the probability at the position in which a document appears). Like SlideFuse, it is calculated in two stages: a training phase and a fusion phase.",null,null
65,"In the training phase, P (dp|s) is calculated. This is the probability that a document d returned in position p of a result set is relevant, given that is has been returned by input system s. It is calculated by",null,null
66,"P (dp|s) ,",null,null
67,"qQp Rdp,q Qp",null,null
68,(1),null,null
69,"where Qp is the set of all training queries for which at least p documents were returned by the input system and Rdp,q is the relevance of the document dp to query q (1 if the document is relevant, 0 if not). This is calculated for",null,null
70,"each input system to be used in the fusion phase. Following this, the fusion stage requires that a ranking",null,null
71,score be assigned to each document (Rd). This is given by,null,null
72,"Rd , P (dp|s)",null,null
73,(2),null,null
74,sS,null,null
75,"where S is the set of all input systems used and p is the position in which document d was returned by input system s. Although the use of probabilities would suggest that multiplication would be an obvious operator to use, the nature of data fusion makes addition more useful in this scenario. Adding the probability scores together results in a document's ranking score receiving a boost for every result set in which it appears (thus leveraging the Chorus Effect). Rd",null,null
76,"is intended as a score on which to rank documents, rather than an accurate estimation of the probability of a document's relevance.",null,null
77,3.2 SlideFuse,null,null
78,"SlideFuse is a probabilistic fusion algorithm that is also based on the probability of relevance in various positions in result sets [10]. For SlideFuse, this probability calculation is the same as described above in Equation 1.",null,null
79,"However, SlideFuse does not use this probability alone in order to calculate scores. It also employs a smoothing of these probabilities based on the notion of a sliding window. The argument in favour of this smoothing is that in certain situations, some positions may be ultimately given a probability of zero. This occurs whenever no relevant documents are returned by an input system at that exact position during the training phase. There are two principal reasons why this may happen:",null,null
80,"1. Few Training Queries: If the number of queries being used for training is very small, this reduces the overall number of relevant documents being returned by each input system. Because of this, it consequently increases the chance that a particular position may not contain a relevant document for any of the training queries.",null,null
81,"2. Incomplete Relevance Judgements: When relevance judgements are ""incomplete"", not all documents have been judged for relevance to all the queries. This means that there three types of document: relevant, nonrelevant and unjudged. The lack of judged relevant documents appearing at any position may merely be as a result of documents being unjudged.",null,null
82,"Whatever the reason, a probability of zero is undesirable. Firstly, it runs contrary to the Chorus Effect to neglect to take into account that a document was actually returned by an input system, regardless of its position. Secondly, it is counter-intuitive to give any document that was returned in a result set the same treatment as one that was not returned at all.",null,null
83,The sliding window is designed to reduce the likelihood of zero probabilities by also taking into account neighbouring positions. The start and end points (a and b respectively) of the sliding window surrounding each result set position p are given by,null,null
84,p-w,null,null
85,"p - w >, 0",null,null
86,"a,",null,null
87,(3),null,null
88,0,null,null
89,p-w <0,null,null
90,p+w,null,null
91,p+w < N,null,null
92,"b,",null,null
93,(4),null,null
94,N -1,null,null
95,"p + w >, N",null,null
96,"where w is a parameter that indicates how many positions on either side of p should be included in the window and N is the total number of documents in the result set. In effect, the above definitions of a and b ensure that the window cannot begin before the first document in the result set and also cannot extend beyond the last document.",null,null
97,"Once the boundaries of the window have been set, a probability must be associated with each. P (dp,w|s), the probability of relevance of document d in position p using a window",null,null
98,349,null,null
99,"size of w documents either side of p, given that it has been returned by input system s is given by",null,null
100,"P (dp,w|s) ,",null,null
101,"b i,a",null,null
102,P,null,null
103,(di,null,null
104,|s),null,null
105,b-a+1,null,null
106,(5),null,null
107,"Finally, a ranking score is given to each document using a formula very similar to Equation 2, except that the probability associated with the window is used instead of the probability at a particular rank.",null,null
108,"Rd ,"" P (dp,w|s)""",null,null
109,(6),null,null
110,sS,null,null
111,3.3 CombMNZ,null,null
112,"Although it is not a probabilistic model, we also include the CombMNZ fusion algorithm, as it has become a standard baseline against which other fusion algorithms are compared [2, 13, 19]. Originally proposed by Fox and Shaw in [5], CombMNZ is a score-based algorithm that does not rely on training. It has gained popularity as a baseline measure principally because it is easily implemented and its retrieval performance tends to be very strong, despite its simplicity. Our implementation of CombMNZ follows that of Lee [7], who carried out a number or experiments using a variety of techniques proposed by Fox and Shaw.",null,null
113,"CombMNZ is run in two phases. Unlike PosFuse and SlideFuse, both of these are done at fusion time, with no training required. Because CombMNZ is based on the scores attributed to each document by each of the input systems, the first requirement is that these be normalised. This is intended to scale all of the scores into the same range, so as to avoid a situation where one input system attaches greater weight to documents merely because it calculates scores from 0 to 100 rather than from 0 to 1.",null,null
114,"The normalisation formula used by Lee is known as ""standard normalisation"" [12] and is given by",null,null
115,normalised,null,null
116,sim,null,null
117,",",null,null
118,unnormalised sim - max sim - min,null,null
119,min sim,null,null
120,sim,null,null
121,(7),null,null
122,"where max sim and min sim are the maximum and minimum scores that are actually seen in the input result set. Once the scores have been normalised, CombM N Zd, the CombMNZ ranking score for any document d is given by",null,null
123,S,null,null
124,"CombM N Zd ,"" Ns,d × |Nd > 0|""",null,null
125,(8),null,null
126,"s,1",null,null
127,"where S is the number of result sets to be fused, Ns,d is the normalised score of document d in result set s and |Nd > 0| is the number of non-zero normalised scores given to d by any result set.",null,null
128,3.4 Initial Results,null,null
129,"Table 1 shows the MAP score for a number of data fusion algorithms. For comparison purposes, the column labelled ""MaxMAP"" shows the highest overall MAP score achieved by any individual input. It can be argued that this is the baseline that all fusion algorithms should aim to beat. If a fusion algorithm cannot achieve this level of performance, then a superior approach would simply be to identify which of the input systems performs best, discarding the others.",null,null
130,"In this table, the highest MAP score amongst the fusion algorithms is shown in bold.",null,null
131,MaxMAP PosFuse SlideFuse CombMNZ,null,null
132,run1 run2 run3 run4 run5,null,null
133,0.5389 0.5120 0.4589 0.4325 0.3976,null,null
134,0.5751 0.5679 0.5375 0.4791 0.4907,null,null
135,0.5697 0.5651 0.5223 0.4628 0.4640,null,null
136,0.3317 0.5249 0.1862 0.1740 0.4203,null,null
137,Table 1: MAP Scores when training on the actual result sets being fused. The highest MAP score for a fusion technique on each run is in bold.,null,null
138,"From Table 1 it can be seen that the highest MAP scores are achieved on all runs by PosFuse. Additionally, these MAP scores are greater than the best performing individual input for each run. This is an interesting result in that it adds motivation to the pursuit of a probability distribution for the purposes of fusion.",null,null
139,"Another interesting observation is that the PosFuse technique was able to achieve marginally greater MAP scores than SlideFuse. SlideFuse is based on probabilities that are initially calculated in the same way as for PosFuse, with the addition of the smoothing that is performed by the sliding window. It is, however, important to note that the principal motivation behind the use of sliding windows is to cater for situations where a small quantity of training queries combined with incomplete relevance judgements may cause some positions to be attributed a probability of zero. As the results shown in Table 1 are for fusion runs consisting of 225 training queries, this kind of situation does not arise to the same extent and so the motivation for this is lost. The performance of SlideFuse is still greater than the maximum individual MAP score, however.",null,null
140,4. MODELLING PROBABILITY,null,null
141,"Having demonstrated the effectiveness of using accurate probability figures, we now investigate how this may be modelled, preferably without the necessity for large quantities of training data.",null,null
142,4.1 Curve Fitting,null,null
143,"To do this, the probability distributions calculated for the",null,null
144,225-query run outlined in Section 3 were analysed. For each,null,null
145,"distribution (each one related to one input system), a curve was fitted using the gnuplot graphing utility 1. In each case,",null,null
146,"the probability of relevance was plotted on the y-axis, with",null,null
147,the result set position (starting at 1 for the top position in,null,null
148,"the result set) on the x-axis. In each case, gnuplot fit a curve",null,null
149,of,null,null
150,the,null,null
151,form,null,null
152,y,null,null
153,",",null,null
154,a x,null,null
155,",",null,null
156,meaning,null,null
157,that,null,null
158,the,null,null
159,probability,null,null
160,of,null,null
161,relevance,null,null
162,would become a function of the result set position.,null,null
163,Figure 1 illustrates the process of fitting a curve for the,null,null
164,MSRC04B32S input system. The first graph in that figure,null,null
165,shows the results of fitting a curve only to the first 100 posi-,null,null
166,"tions in each result set, whereas the second uses the entirety",null,null
167,of each result set (TREC results are truncated to at most,null,null
168,1http://gnuplot.info,null,null
169,350,null,null
170,"1000 documents in each result set). The fitted a-value is shown at the top-right of each graph. Despite the large difference in the lengths of the result sets being used, there is less than a 1% difference between the a-values generated.",null,null
171,"Figure 1: Curves fit to probability distribution for the MSRC04B2S input, showing 100 positions and 1000 positions respectively",null,null
172,"Using the latter a-value (relating to 1000-document result sets), this leads to a modification of Equation 1 for calculating the probability of relevance. For the specific result sets used, the probability that a document d returned in position p in a result set created by the MSRC04B25 input system is represented by P (dp|M SRC04B25). Its value is given by",null,null
173,P (dp|M SRC04B25),null,null
174,",",null,null
175,0.79929 p,null,null
176,(9),null,null
177,"This is shown for illustrative purposes: similar fitting was done for all of the other input systems available, with a variety of a-values being generated. Although interesting that such a function can be generated for a range of input systems, to do so requires even more training effort than what was needed for the PosFuse algorithm used in Section 3. In addition to the training data necessary to calculate the probability of relevance at each position, the curve fitting would also have to be performed.",null,null
178,The shape of the fitted curves is interesting in that it supports the reasoning behind the description of the Skimming Effect. The graph shown in Figure 1 shows that documents ranked in early positions in result sets are much more likely to be relevant than those further down the result set. It also supports the idea that probability scores (or approximations thereof) can be effectively used in the calculation of fusion scores.,null,null
179,4.2 Evalation of curve fitted approach,null,null
180,"To gauge how effective this is in terms of fusion performance, a comparison is made with the results obtained for the experiment outlined in Section 3. Table 3 reproduces the figures shown in Table 1, with the addition of an extra column (marked ""FitFuse""), which is based on the fitted curves.",null,null
181,"For FitFuse, rather than using the probabilities of relevance calculated on a per-position basis, we use the formula described in Equation 9. The fitted a-values used for each input system is given in Table 2.",null,null
182,Input,null,null
183,run1 MSRAmixed1 MSRAx4 MSRC04B2S MSRAx2 MSRC04B1S MSRC04C12 run2 MSRAmixed3 MSRC04B1S2 UAmsT04MSinu MSRAx5 UAmsT04MSind UAmsTo4MWScb run3 MSRC04B3S THUIRmix041 THUIRmix045 UAmsT04MWinu uogWebSelAn uogWebSelAnL run4 ICT04CIIS1AT ICT04MNZ3 ICT04RULE THUIRmix042 THUIRmix043 uogWebCA run5 ICT04basic ICT04CIILC MeijiHILw1 MeijiHILw3 UAmsT04LnuNG uogWebSelL,null,null
184,a-value,null,null
185,0.806350 0.803393 0.799290 0.798310 0.786756 0.802797,null,null
186,0.774764 0.701004 0.469070 0.787904 0.454965 0.466055,null,null
187,0.649945 0.641529 0.661245 0.458516 0.469182 0.451617,null,null
188,0.685360 0.690526 0.653053 0.649099 0.638313 0.401074,null,null
189,0.643282 0.660100 0.374485 0.371387 0.670690 0.424656,null,null
190,Table 2: a-values used for FitFuse the various input systems.,null,null
191,The results shown in Table 3 are promising. Having moved away from the perfectly accurate probability distribution,null,null
192,351,null,null
193,MaxMAP FitFuse PosFuse SlideFuse CombMNZ,null,null
194,run1 run2 run3 run4 run5,null,null
195,0.5389 0.5120 0.4589 0.4325 0.3976,null,null
196,0.5773 0.5640 0.5140 0.4704 0.4724,null,null
197,0.5751 0.5679 0.5375 0.4791 0.4907,null,null
198,0.5697 0.5651 0.5223 0.4628 0.4640,null,null
199,0.3317 0.5249 0.1862 0.1740 0.4203,null,null
200,Table 3: MAP Scores when training on the actual result sets being fused. The highest MAP score for a fusion technique on each run is in bold.,null,null
201,"used in PosFuse, FitFuse shows only a slight disimprovement in MAP score, despite it being merely an estimate of the probabilities involved. On the first run, it actually gains a marginally higher MAP score than PosFuse, which indicates that although estimating probability may not be expected to achieve the same quality results as a perfectlymodelled probability distribution, this is not necessarily the case.",null,null
202,4.3 Towards single-value training,null,null
203,"Because of the onerous training needs, we are interested",null,null
204,in finding other values that can be substituted for a fitted,null,null
205,a-value in,null,null
206,a,null,null
207,y,null,null
208,",",null,null
209,a x,null,null
210,style,null,null
211,probability model.,null,null
212,In order for,null,null
213,"a candidate value to be suitable for use in this way, it is",null,null
214,required to satisfy three criteria:,null,null
215,· Correlation: It must be shown to correlate to the fitted a-values.,null,null
216,· Training: It should require less exhaustive training calculations than methods such as PosFuse and SlideFuse.,null,null
217,· Results: It must be competitive in terms of the evaluation of fusion performance.,null,null
218,"The first of these criteria is particularly important for selecting what value to use. Logically, a high a-value indicates that an input system is more likely to return relevant documents than one with a lower a-value. Clearly, this a-value is linked to the overall performance of an input system. As such, this motivates the use of established IR evaluation metrics for fusion.",null,null
219,"Evaluation metrics have been a key focus of IR research for many years. Each is designed to measure the quality of a result set in some way, with different metrics having their own emphasis, strengths and weaknesses. The metric selected for investigation in this work is the widely-used Mean Average Precision (MAP) metric.",null,null
220,"Figure 2 shows a graph of the fitted a-values (on the xaxis) plotted against MAP (on the y-axis). Although the two values are not shown to be directly proportional, a clear upward trend can be seen. Input systems with higher fitted a-values tend to also have higher MAP scores. Intuition would dictate that this is not a surprising result: a system with a greater tendency to return relevant documents would typically achieve a higher MAP score on evaluation (although the position of the relevant documents is also important).",null,null
221,"A curve can be fitted for this graph also (and is indicated by the straight line in Figure 2). However, it important to",null,null
222,Figure 2: Correlation between MAP score and fitted a-value,null,null
223,"bear in mind that a direct mapping from a MAP score to an a-value is not necessarily required. The aim of the fusion task is to rank the documents, rather than calculate an accurate a-value. Given the formula used below in Section 5, it can be shown that multiplying the MAP scores by a constant to better approximate a-values does not affect the ranking of the documents in the fused result set.",null,null
224,The second criterion required above is that the amount of training effort should be less than that of alternative techniques such as SlideFuse. SlideFuse requires knowledge of the exact positions in which relevant documents were returned in response to training queries. In contrast the proposed approach requires only a single-value estimation of the quality of the input system.,null,null
225,"Finally, it is required to evaluate the effectiveness of using a function of MAP score and document position as a fusion strategy.",null,null
226,5. EVALUATION,null,null
227,"In order to evaluate the effectiveness of the approach outlined above, it is necessary to carry out a number of experiments. These experiments involved running this new technique (which we shall call ""MAPFuse"") alongside a number of alternatives.",null,null
228,5.1 Experiment Setup,null,null
229,"As with the initial experiments outlined in Section 3, five separate runs were performed. These use the same inputs as in the earlier experiments. Each input was divided into a set of training result sets and a set of fusion result sets. For this, an 20%/80% split was used (i.e. 45 training queries and 180 queries used for fusion).",null,null
230,"Dividing query sets in this way alone may cause unrepresentative results being obtained. For instance, early queries (i.e. those used for training) may be disproportionately straightforward (or indeed difficult) when compared with those used for fusion. This may mean that differences between fusion techniques' performance may be a consequence of the training data rather than the algorithms themselves.",null,null
231,"For this reason, each of the fusion runs was performed five separate times, with the queries being shuffled into a",null,null
232,352,null,null
233,MaxMAP MAPFuse PosFuse SlideFuse CombMNZ,null,null
234,different randomised order before each time. Thus the set of training queries was different each time. The evaluation results reported here for each run are the average of each of these five sets of shuffled inputs.,null,null
235,"The baseline MAP score for each run is that achieved by the best-performing individual input system (denoted by MaxMAP). Training queries are ignored in this calculation, so the figures presented relate to the same query set for each technique.",null,null
236,"Three data fusion algorithms were chosen for comparison. SlideFuse and CombMNZ are implemented as described in [10] and [5], respectively. Training queries are also ignored for CombMNZ, since that algorithm does not require a training phase. The implementation of PosFuse is as described in Section 3, with the exception that the probabilities are calculated on the training queries and then used to fuse the fusion queries at a later stage, rather than being calculated on the same result sets that are to be fused.",null,null
237,5.2 Defining MAPFuse,null,null
238,"For the MAPFuse fusion algorithm, the training phase requires only that the MAP score for each input system on the training queries be calculated. This is performed by trec eval, which is a tool provided by TREC to calculate evaluation metrics for IR systems. Unlike the proofof-concept results shown in Section 3, relevance information for the actual result sets being fused is not required, as the MAP scores used for fusion are calculated using only the training queries.",null,null
239,"Once the relevant MAP scores have been calculated, they are used in the fusion phase to calculate the scores on which the documents are ranked in the final, fused result set.",null,null
240,The ranking score Rd attributed to document d is given by,null,null
241,Rd,null,null
242,",",null,null
243,sS,null,null
244,M APs ps(d),null,null
245,(10),null,null
246,"where S is the set of the input systems that returned document d somewhere in their result sets, M APs is the MAP score associated with system s and ps(d) is the position in which document d was ranked by system s.",null,null
247,"The fact that the MAP score is divided by a document's position helps to leverage the Skimming Effect, whereas the fact that the scores are added to give the document's final ranking score boosts documents that have appeared in multiple result sets and so makes use of the Chorus Effect.",null,null
248,5.3 Results,null,null
249,The results of running these experiments are presented in Table 4. Values in bold face are the highest score achieved by a fusion algorithm on a particular run. Asterisks are used to indicate a statistically significant difference to the performance of MAPFuse when measured using the t-test.,null,null
250,"With the exception of CombMNZ, each of the fusion algorithms achieves a higher MAP score than that of the best individual input system (again shown as ""MaxMAP""). MAPFuse achieves comparable results to PosFuse and SlideFuse, with the highest MAP score for three of the five runs. It also shows a statistically significant improvement in MAP score over MaxMAP and CombMNZ on all runs. As an aside, it is of note that, unlike the situation in Section 3, the scores achieved by SlideFuse are consistently higher than those of PosFuse (with the sole exception of run5). This may pos-",null,null
251,run1 run2 run3 run4 run5,null,null
252,0.5468* 0.5120* 0.4555* 0.4357* 0.4084*,null,null
253,0.5767 0.5693 0.5132 0.4711 0.4858,null,null
254,0.5591* 0.5682* 0.5045*,null,null
255,0.4591 0.4777*,null,null
256,0.5728 0.5718 0.5171 0.4665 0.4681*,null,null
257,0.3361* 0.5404* 0.1842* 0.1785* 0.4277*,null,null
258,Table 4: MAP Scores From Fusion Runs. The highest MAP score for a fusion technique on each run is in bold. Asterisks indicate a statistically significant difference when compared to MAPFuse using the t-test.,null,null
259,"sibly be explained by the lower quantity of training queries being used, with the sliding window beginning to show its advantages over the strictly position-based PosFuse.",null,null
260,"The difference between the MAP scores of the three probabilistic techniques is quite small (MAPFuse's score is never more than 3% higher than PosFuse or 4% higher than that of SlideFuse). It is notable that despite this, the difference between MAPFuse and PosFuse is statistically significant in 4 of the 5 runs. Another observation is that for the two runs in which SlideFuse achieves a higher MAP score than MAPFuse, this difference is not statistically significant.",null,null
261,"Despite these observations, the aim of the experiment is not necessarily to achieve significantly higher MAP scores. According to the third success criterion in Section 4, we merely require comparable performance with competing techniques. The principal advantage is that comparable retrieval results can be achieved by using only a single figure to represent the effectiveness of an underlying input system, rather than the detailed information about relevant documents' positions that is required by the other algorithms.",null,null
262,6. CONCLUSIONS AND FUTURE WORK,null,null
263,"In this paper, we have examined the use of the probabil-",null,null
264,"ity of relevance in performing data fusion. In this context,",null,null
265,"we use ""probability of relevance"" to mean the probability",null,null
266,that a document returned by a particular input system in a,null,null
267,particular position in its result set is relevant.,null,null
268,Initially we showed that if a fully accurate model of the,null,null
269,"probability of relevance at each position is available, positive",null,null
270,fusion results can be achieved using these probabilities to,null,null
271,calculate the ranking scores for documents. Following from,null,null
272,"this, we have shown that these probabilities can be modelled",null,null
273,by,null,null
274,a,null,null
275,function,null,null
276,of,null,null
277,the,null,null
278,form,null,null
279,y,null,null
280,",",null,null
281,a x,null,null
282,.,null,null
283,Using the MAP score,null,null
284,of each input system on a number of training queries as,null,null
285,"an substitute for a, we have shown that comparable MAP",null,null
286,scores to alternative fusion algorithms can be achieved.,null,null
287,The benefits of this approach are principally in the level,null,null
288,of training data that is required. Whereas algorithms like,null,null
289,SlideFuse required detailed training data on the specific lo-,null,null
290,"cation of relevant documents within result sets, MAPFuse",null,null
291,requires only a single summary metric to represent the qual-,null,null
292,ity of each input system being used.,null,null
293,"For the purposes of this paper, the common Mean Average",null,null
294,Precision (MAP) evaluation metric was used as the single-,null,null
295,"value substitution for a. However, a range of alternative",null,null
296,353,null,null
297,metrics are available and so future work will concentrate on evaluating the impact of using alternative metrics.,null,null
298,"Additionally, a more exhaustive study on a greater number of document collections will be necessary to demonstrate the wider applicability of this work. Such a study would also include a separation of the training and fusion phases so that each is carried out on a different document collection (although the retrieval systems generating each result set would not change). This would be an important stage in demonstrating that this type of fusion could be employed in a real-world information retrieval system.",null,null
299,7. ACKNOWLEDGEMENTS,null,null
300,This material is based upon works supported by the Science Foundation Ireland under Grant No. 08/RFP/CMS1183.,null,null
301,8. REFERENCES,null,null
302,"[1] J. A. Aslam and M. Montague. Models for metasearch. In SIGIR '01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 276­284, New York, NY, USA, 2001.",null,null
303,"[2] S. M. Beitzel, E. C. Jensen, A. Chowdhury, D. Grossman, O. Frieder, and N. Goharian. Fusion of effective retrieval strategies in the same information retrieval system. J. Am. Soc. Inf. Sci. Technol., 55:859­868, 2004.",null,null
304,"[3] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In SIGIR '95: Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval, pages 21­28, New York, NY, USA, 1995.",null,null
305,"[4] N. Craswell and D. Hawking. Overview of the TREC-2004 web track. In Proceedings of the Thirteenth Text REtrieval Conference (TREC-2004), 2004.",null,null
306,"[5] E. A. Fox and J. A. Shaw. Combination of Multiple Searches. In Proceedings of the 2nd Text REtrieval Conference (TREC-2), National Institute of Standards and Technology Special Publication 500-215, pages 243­252, 1994.",null,null
307,"[6] A. E. Howe and D. Dreilinger. SavvySearch: A Metasearch Engine That Learns Which Search Engines to Query. AI Magazine, 18:19­25, 1997.",null,null
308,"[7] J. H. Lee. Analyses of multiple evidence combination. SIGIR Forum, 31:267­276, 1997.",null,null
309,"[8] D. Lillis. ProbFuse: Probabilistic Data Fusion. Msc, University College Dublin, UCD, February 2006.",null,null
310,"[9] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. ProbFuse: A Probabilistic Approach to Data Fusion. In Proceedings of the 29th annual international ACM SIGIR Conference on Research and Development in information retrieval, pages 139­146, New York, USA, 2006.",null,null
311,"[10] D. Lillis, F. Toolan, R. Collier, and J. Dunnion. Extending Probabilistic Data Fusion Using Sliding Windows. In Proceedings of the 30th European Conference on Information Retrieval (ECIR '08), volume 4956 of Lecture Notes in Computer Science, pages 358­369, Berlin, 2008. Springer.",null,null
312,"[11] R. Manmatha, T. Rath, and F. Feng. Modeling score distributions for combining the outputs of search engines. In SIGIR '01: Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 267­275, New York, NY, USA, 2001.",null,null
313,"[12] M. Montague and J. A. Aslam. Relevance score normalization for metasearch. In CIKM '01: Proceedings of the Tenth International Conference on Information and Knowledge Management, pages 427­433, New York, NY, USA, 2001.",null,null
314,"[13] M. Montague and J. A. Aslam. Condorcet fusion for improved retrieval. In CIKM '02: Proceedings of the eleventh international conference on Information and knowledge management, pages 538­548, New York, NY, USA, 2002.",null,null
315,"[14] A. L. Powell, J. C. French, J. Callan, M. Connell, and C. L. Viles. The impact of database selection on distributed searching. In SIGIR '00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 232­239, New York, NY, USA, 2000.",null,null
316,"[15] E. Selberg and O. Etzioni. The MetaCrawler Architecture for Resource Aggregation on the Web. IEEE Expert, pages 11­14, 1997.",null,null
317,"[16] M. Shokouhi. Segmentation of Search Engine Results for Effective Data-Fusion. Advances in Information Retrieval, 4425, April 2007.",null,null
318,"[17] C. C. Vogt and G. W. Cottrell. Fusion Via a Linear Combination of Scores. Information Retrieval, 1:151­173, 1999.",null,null
319,"[18] E. M. Voorhees, N. K. Gupta, and B. Johnson-Laird. The Collection Fusion Problem. In Proceedings of the Third Text REtrieval Conference (TREC-3), pages 95­104, 1994.",null,null
320,"[19] S. Wu and F. Crestani. Data fusion with estimated weights. In CIKM '02: Proceedings of the eleventh international conference on Information and knowledge management, pages 648­651, New York, NY, USA, 2002.",null,null
321,354,null,null
322,,null,null

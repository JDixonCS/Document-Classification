,sentence,label,data
,,,
0,Hierarchical Pitman-Yor Language Model for Information Retrieval,null,null
,,,
1,"Saeedeh Momtazi, Dietrich Klakow",null,null
,,,
2,"Spoken Language Systems Saarland University, Saarbrücken, Germany",null,null
,,,
3,"{saeedeh.momtazi,dietrich.klakow}@lsv.uni-saarland.de",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"In this paper, we propose a new application of Bayesian language model based on Pitman-Yor process for information retrieval. This model is a generalization of the Dirichlet distribution. The Pitman-Yor process creates a power-law distribution which is one of the statistical properties of word frequency in natural language. Our experiments on Robust04 indicate that this model improves the document retrieval performance compared to the commonly used Dirichlet prior and absolute discounting smoothing techniques.",null,null
,,,
6,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]:Information Search and Retrieval,null,null
,,,
7,"General Terms: Theory, Algorithm, Experimentation",null,null
,,,
8,"Keywords: information retrieval, language modeling, PitmanYor process, smoothing methods",null,null
,,,
9,1. INTRODUCTION,null,null
,,,
10,"Statistical language modeling has successfully been used in speech recognition and many natural language processing tasks. Language models for information retrieval have been the topics of intense research interest in recent years. The efficiency of this approach, its simplicity, the state-ofthe-art performance it provides, and straightforward probabilistic interpretation are the most important factors which contribute to its popularity [3].",null,null
,,,
11,"Smoothing plays an essential role when estimating a language model for retrieving relevant documents. A large number of smoothing methods have been proposed for language modeling; among them, three different techniques-- namely Jelinek-Mercer, Bayesian smoothing with Dirichlet priors, and absolute discounting--have shown significant improvements in information retrieval performance [6].",null,null
,,,
12,"A hierarchical Bayesian language model based on PitmanYor processes has been recently proposed by Teh [5]. This model which is a nonparametric generalization of the Dirichlet distribution [5] has been shown to produce results superior to the state-of-the-art smoothing methods. Hierarchical Pitman-Yor language model has also been applied in speech recognition task and improved the system performance significantly [1]. However, to the best knowledge of the authors this method has not been used for language model-based information retrieval.",null,null
,,,
13,"In this work, we propose using the hierarchical Pitman-",null,null
,,,
14,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
15,"Yor language model for the document retrieval task, and compare this approach with the state-of-the-art smoothing methods widely studied for language model-based information retrieval.",null,null
,,,
16,2. METHOD,null,null
,,,
17,"In language model-based document retrieval, P (Q|d) is estimated by the probability of generating each query term:",null,null
,,,
18,"P (Q|d) ,",null,null
,,,
19,P (qi|d),null,null
,,,
20,-1,null,null
,,,
21,"i,1...M",null,null
,,,
22,"where M is the number of terms in the query, qi denotes the ith term of query Q ,"" {q1, q2, ..., qM }, and d is the document model. Therefore, the goal is to estimate P (w|d) which can""",null,null
,,,
23,be simply calculated by the maximum likelihood estimation:,null,null
,,,
24,"Pml(w|d) ,",null,null
,,,
25,"c(w, d) w c(w, d)",null,null
,,,
26,-2,null,null
,,,
27,"However, having the problem of unseen words, we need to use a smoothing technique to give a non-zero probability to the unseen words. We hypothesized that Bayesian smoothing based on Pitman-Yor process can be used as a new approach to solve the zero probability problem in document retrieval.",null,null
,,,
28,"Pitman-Yor process is a nonparametric Bayesian model which recursively placed as prior for predicting probabilities in language model. Considering P (w|d) as the probability of word w given the observation of document d to be estimated, the Pitman-Yor process can be defined as:",null,null
,,,
29,"P (w|d)  P Y (, , PBG(w))",null,null
,,,
30,-3,null,null
,,,
31,"where  is a discount parameter,  is a strength parame-",null,null
,,,
32,"ter, and PBG is the prior/background probability of word w",null,null
,,,
33,before observing any document.,null,null
,,,
34,The procedure of drawing word probabilities from the,null,null
,,,
35,"Pitman-Yor process can be described using the ""Chines restau-",null,null
,,,
36,"rant"" analogy. Imagine a Chinese restaurant with an infinite",null,null
,,,
37,"number of tables, each with an infinite number of seats. Cus-",null,null
,,,
38,"tomers, which correspond to word tokens, enter the restau-",null,null
,,,
39,rant and seat themselves at a table. Each customer can sit at,null,null
,,,
40,an,null,null
,,,
41,occupied,null,null
,,,
42,table,null,null
,,,
43,k,null,null
,,,
44,with,null,null
,,,
45,probability,null,null
,,,
46,ck -   + c.,null,null
,,,
47,where,null,null
,,,
48,ck,null,null
,,,
49,is,null,null
,,,
50,the,null,null
,,,
51,"number of customers already sitting there and c. , k ck;",null,null
,,,
52,the customer can also sit at a new unoccupied table with,null,null
,,,
53,probability,null,null
,,,
54, + t.  + c.,null,null
,,,
55,where,null,null
,,,
56,t.,null,null
,,,
57,is,null,null
,,,
58,the,null,null
,,,
59,current,null,null
,,,
60,number,null,null
,,,
61,of,null,null
,,,
62,occu-,null,null
,,,
63,pied tables. It is necessary to mention that all customers,null,null
,,,
64,that correspond to the same word type w can sit at different,null,null
,,,
65,793,null,null
,,,
66,"tables, in which tw denotes the number of tables occupied by customers w.",null,null
,,,
67,"One of the advantages of Pitman-Yor process is improving the Dirichlet prior by using a discounting parameter  (0 <  < 1) deriving from absolute discounting method. Another key advantage of Pitman-Yor process is generating a power-law distribution in the language model, which is one of the statistical properties of word frequencies in natural language. This property, which is based on the scenario of rich-get-richer, implies that in the statistical property of word counts, words with low frequency have a high probability and words with high frequency occur with low probability. Benefiting from this idea in the document smoothing can help us to have different discounting value for each word based on the frequency of that word in the document.",null,null
,,,
68,"Given the seating arrangement of customers as described above, the estimated probability of word w having the observation of document d is given by:",null,null
,,,
69,P (w|d),null,null
,,,
70,",",null,null
,,,
71,"c(w, d) - tw + ( + t.)PBG(w) w c(w, d) + ",null,null
,,,
72,-4,null,null
,,,
73,"If we set the discounting parameter  ,"" 0, then the model reduces to the Dirichlet process. If we set the strength parameter  "", 0 and limit tw ,"" 1, then the model reverts to the absolute discounting method.""",null,null
,,,
74,"Although this formula is based on unigram model, the hierarchical behavior of the Pitman-Yor process allows us to use this model for higher level n-grams as well.",null,null
,,,
75,"The most important and computationally expensive part of the above formula is calculating tw for each word which should have a relation to the word count c(w, d). Towards this end, we use the power-law discounting model proposed by Huang and Renals [2]:",null,null
,,,
76,"tw , 0 tw ,"" f (c(w, d)) "","" c(w, d)""",null,null
,,,
77,"if c(w, d) ,"" 0 if c(w, d) > 0""",null,null
,,,
78,-5,null,null
,,,
79,"They showed that the above formula is a near optimum estimate for tw, which can be obtained without a computationally expensive training procedure.",null,null
,,,
80,3. EXPERIMENTAL RESULTS,null,null
,,,
81,"To evaluate our methods, we used TREC ad hoc testing collections from disk 4 and 5 minus CR which includes Financial Times (1991-1994) and Federal Register (1994) from disk 4 and Foreign Broadcast Information Service (1996) and Los Angeles Times (1989-1990) from disk 5. The total number of documents are 528,155.",Y,
,,,
82,"We used Robust04 topics for our experiment such that topics 301-450 have been used as development set and topics 601-700 for test set. For each of the topics, the set of top 1000 documents retrieved by Indri [4] was selected and then the documents are ranked with LSVLM, the language modeling toolkit developed by our chair, in the second step.",null,null
,,,
83,"Table 1 shows the results of our experiments in which Mean Average Precision (MAP) and Precision at 10 (P@10) serve as the primary metrics, and results are marked as significant* (p < 0.05), highly significant** (p < 0.01), or neither according to 2-tailed paired t-test. This table presents our main results evaluating the accuracy of Bayesian smoothing with Dirichlet prior, absolute discounting and our proposed Bayesian smoothing based on Pitman-Yor process.",null,null
,,,
84,"As shown by the tabulated results, the Pitman-Yor language model significantly outperforms both Dirichlet prior",null,null
,,,
85,Table 1: Retrieval results with different smoothings.,null,null
,,,
86,Significant differences with absolute discounting and,null,null
,,,
87,Dirichlet prior are marked by a and d respectively.,null,null
,,,
88,Model,null,null
,,,
89,MAP P@10,null,null
,,,
90,Absolute Discounting Dirichlet Prior,null,null
,,,
91,"Pitman-Yor Process Pitman-Yor Process ( , 0)",null,null
,,,
92,0.3138 0.3147 0.3271da 0.3222a,null,null
,,,
93,0.4484 0.4518 0.4657a 0.4566,null,null
,,,
94,"and absolute discounting. As mentioned, the major features of the Pitman-Yor process are generalizing Dirichlet prior and generating power-law distribution by having different discounting parameters for each word based on its frequency. We believe that the power-law distribution is the main contribution of the Pitman-Yor language model which causes such an improvement in retrieval performance. We also applied Pitman-Yor language model while setting  ,"" 0; i.e. the model became more similar to absolute discounting, but it still creates power-law distribution by benefiting from tw parameter. The results are presented in the last raw of the table. From the results we can see that although setting  "","" 0 decreases the performance, the reduction is not significant; and the simplified version of Pitman-Yor smoothing which only has one parameter still beat the other smoothing methods.""",null,null
,,,
95,4. CONCLUDING REMARKS,null,null
,,,
96,"We proposed a new smoothing method for language modelbased document retrieval, named Bayesian smoothing based on Pitman-Yor process, and verified that this language model provides better performance than other state-of-the-art smoothing techniques. The key advantage of Pitman-Yor language model is generating a power-law word distribution, which is the primary reason for its superior performance.",null,null
,,,
97,Acknowledgments,null,null
,,,
98,Saeedeh Momtazi is funded by the German research foundation DFG through the International Research Training Group (IRTG 715).,null,null
,,,
99,5. REFERENCES,null,null
,,,
100,"[1] S. Huang and S. Renals. Hierarchical Pitman-Yor language models for ASR in meetings. In Proceedings of IEEE ASRU International Conference, pages 124­129, 2007.",null,null
,,,
101,"[2] S. Huang and S. Renals. Power law discounting for n-gram language models. In Proceedings of IEEE ICASSP International Conference, 2010.",null,null
,,,
102,"[3] J. Ponte and W. Croft. A language modeling approach to information retrieval. In Proceedings of ACM SIGIR International Conference, pages 275­281, 1998.",null,null
,,,
103,"[4] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In Proceedings of International Conference on Intelligence Analysis, 2005.",null,null
,,,
104,"[5] Y. Teh. A hierarchical Bayesian language model based on Pitman-Yor process. In Proceedings of ACL International Conference, 2006.",null,null
,,,
105,"[6] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of ACM SIGIR International Conference, 2001.",null,null
,,,
106,794,null,null
,,,
107,,null,null

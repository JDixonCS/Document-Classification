,sentence,label,data
,,,
0,Information-Based Models for Ad Hoc IR,null,null
,,,
1,Stéphane Clinchant,null,null
,,,
2,"XRCE & LIG, Univ. Grenoble I Grenoble, France",null,null
,,,
3,stephane.clinchant@xrce.xerox.com,null,null
,,,
4,Eric Gaussier,null,null
,,,
5,"LIG, Univ. Grenoble I Grenoble, France",null,null
,,,
6,eric.gaussier@imag.fr,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"We introduce in this paper the family of information-based models for ad hoc information retrieval. These models draw their inspiration from a long-standing hypothesis in IR, namely the fact that the difference in the behaviors of a word at the document and collection levels brings information on the significance of the word for the document. This hypothesis has been exploited in the 2-Poisson mixture models, in the notion of eliteness in BM25, and more recently in DFR models. We show here that, combined with notions related to burstiness, it can lead to simpler and better models.",null,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
,,,
11,General Terms,null,null
,,,
12,"Theory, Algorithms, Experimentation",null,null
,,,
13,Keywords,null,null
,,,
14,"IR Theory, Probabilistic Models, Burstiness",null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"The purpose of this paper is to introduce the family of information based model for ad hoc information retrieval (IR). By information, we refer to Shannon information when observing a statistical event. The informativeness of a word in a document has a rich tradition in information retrieval since the influential indexing methods developed by Harter ([11]). The idea that the respective behaviors of words in documents and in the collection bring information on word type is, de facto, not a novel idea in IR. It has inspired the 2-Poisson mixture model, the concept of eliteness in BM25 models and is at the heart of DFR models. In this paper, we come back to this idea in order to present a new family of IR models: information models. To do so, we first present,",null,null
,,,
17,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",null,null
,,,
18,"in section 2, the conditions a retrieval function should satisfy, on the basis of the heuristic retrieval constraints proposed by Fang et al. [9]. Section 3 is then devoted to the presentation of information models, and their link with the retrieval conditions and the phenomenon known as burstiness. We present two instances of information models, based on two power law distributions, and show how to perform pseudo-relevance feedback for information models. Section 4 provides an experimental validation of our models. Our experiments show that the information models we introduce significantly outperform language models and Okapi BM25. They are on par with DFR models, while being conceptually simpler, when pseudo-relevance feedback is not used. When using pseudo-relevance feedback, they significantly outperform all models, including DFR ones.",null,null
,,,
19,2. PRELIMINARIES,null,null
,,,
20,"The notations we use throughout the paper are summarized in table 2 (w represents a term). They slightly differ from standard notations for convenience reasons, i.e. their easiness of use in the mathematical framework we deploy. We",null,null
,,,
21,Notation xqw xdw tdw yd m L N M Fw,null,null
,,,
22,Nw,null,null
,,,
23,zw,null,null
,,,
24,Description,null,null
,,,
25,Number of occurrences of w in query q,null,null
,,,
26,Number of occurrences of w in document d,null,null
,,,
27,Normalized version of xdw Length of document d,null,null
,,,
28,Average document length,null,null
,,,
29,Length of collection d,null,null
,,,
30,Number of documents in the collection,null,null
,,,
31,Number of terms in the collection,null,null
,,,
32,Number of occurrences of w in collection:,null,null
,,,
33,Fw,null,null
,,,
34,",",null,null
,,,
35,P,null,null
,,,
36,d,null,null
,,,
37,xdw,null,null
,,,
38,Number of documents containing w:,null,null
,,,
39,Nw,null,null
,,,
40,",",null,null
,,,
41,P,null,null
,,,
42,d,null,null
,,,
43,I (xdw,null,null
,,,
44,>,null,null
,,,
45,0),null,null
,,,
46,"zw , Fw or zw , Nw",null,null
,,,
47,Table 1: Notations,null,null
,,,
48,"consider here retrieval functions, denoted RSV , of the form:",null,null
,,,
49,"RSV (q, d) ,"" X a(xqw)h(xdw, yd, zw, )""",null,null
,,,
50,wq,null,null
,,,
51,"where  is a set of parameters and where h, the form of which depends on the IR model considered, is assumed to be of class1 C2 and defined over R+ ×R+ ×R+ ×, where 1A function of class C2 is a function for which second derivatives exist and are continuous.",null,null
,,,
52,234,null,null
,,,
53," represents the domain of the parameters in  and a is often the identity function. Language models [21], Okapi [15] and Divergence from Randomness [3] models as well as vector space models [16] all fit within the above form. For example, for the pivoted normalization retrieval formula [17],  ,"" (s, m, N ) and:""",null,null
,,,
54,"h(x,",null,null
,,,
55,"y,",null,null
,,,
56,"z,",null,null
,,,
57,),null,null
,,,
58,",",null,null
,,,
59,I (x,null,null
,,,
60,>,null,null
,,,
61,0),null,null
,,,
62,1,null,null
,,,
63,+,null,null
,,,
64,ln(1 + ln(x)I(x>0)),null,null
,,,
65,1,null,null
,,,
66,-,null,null
,,,
67,s,null,null
,,,
68,+,null,null
,,,
69,s,null,null
,,,
70,y m,null,null
,,,
71,ln(,null,null
,,,
72,N,null,null
,,,
73,#NAME?,null,null
,,,
74,1,null,null
,,,
75,),null,null
,,,
76,"where I is an indicator function which equals 1 when its argument is true and 0 otherwise. A certain number of hypotheses, experimentally validated, sustain the development of IR models. In particular, it is important that documents with more occurrences of query terms get higher scores than documents with less occurrences. However, the increase in the retrieval score should be smaller for larger term frequencies, inasmuch as the difference between say 110 and 111 is not as important as the one between 1 and 2 (the number of occurrences has doubled in the second case, whereas the increase is relatively marginal in the first case). In addition, longer documents, when compared to shorter ones with exactly the same number of occurrences of query terms, should be penalized as they are likely to cover additional topics than the ones present in the query. Lastly, it is important, when evaluating the retrieval score of a document, to weigh down terms occurring in many documents, i.e. which have a high document/collection frequency, as these terms have a lower discrimination power. These different considerations can be analytically formalized as a set of simple conditions the retrieval function h should satisfy:",null,null
,,,
77,"(y, z, ),",null,null
,,,
78,"h(x, y, z, x",null,null
,,,
79,),null,null
,,,
80,>,null,null
,,,
81,0,null,null
,,,
82,(condition 1),null,null
,,,
83,"(y, z, ),",null,null
,,,
84,"2h(x, y, z, ) x2",null,null
,,,
85,<,null,null
,,,
86,0,null,null
,,,
87,(condition 2),null,null
,,,
88,"(x, z,",null,null
,,,
89,"),",null,null
,,,
90,"h(x, y, z, y",null,null
,,,
91,),null,null
,,,
92,<,null,null
,,,
93,0,null,null
,,,
94,(condition 3),null,null
,,,
95,"(x, y, ), h(x, y, z, ) < 0 (condition 4) z",null,null
,,,
96,"Conditions 1, 3 and 4 directly state that h should be increasing with the term frequency, and decreasing with the document length and the document/collection frequency. Conditions 1 and 2, already mentioned in this form by Fang et al. [9], state that h should be an increasing, concave function of the term frequency, the concavity ensuring that the increase in the retrieval score will be smaller for larger term frequencies. We will refer to the above conditions as the form conditions inasmuch as they define the general shape the function h should have. They respectively correspond to the heuristic retrieval constraints TFC1, TFC2, LNC1 and TDC2 introduced by Fang et al. [9]. In addition to this form conditions, Fang et al. [9] used two additional constraints to regulate the interaction between frequency and document length, i.e. between the derivatives wrt to x and y. These conditions, which we will refer to as adjustment conditions, allow to adjust the functions h satisfying the form conditions 1, 2, 3 and 4. They correspond to:",null,null
,,,
97,"2Condition 4 is in fact a special case of TDC, but this is beyond the scope of the current paper.",null,null
,,,
98,"Condition 5 LNC2: Let q a query. k > 1, if d1 and d2 are two documents such that yd1 ,"" k × yd2 and for all words w, xdw1 "","" k × xdw2, then RSV (d1, q)  RSV (d2, q)""",null,null
,,,
99,"Condition 6 TF-LNC: Let q , w a query with only word w. if xdw1 > xdw2 et yd1 ,"" yd2 + xdw1 - xdw2, then RSV (d1, q) > RSV (d2, q).""",null,null
,,,
100,We are now ready to proceed to the presentation of information models.,null,null
,,,
101,3. INFORMATION MODELS,null,null
,,,
102,"In order to take into account the fact that one is comparing documents of different length, most IR models do not rely directly on the raw number of occurrences of words in documents, but rather on normalized versions of it. Language models for example use the relative frequency of words in the document and the collection. Other classical term normalization schemes include the well know Okapi normalization, as well as the pivoted length normalization [17]. More recently, [14] propose another formulation for the language model using the notion of verbosity. DFR models usually adopt one of the two following term frequency normalizations (c is a multiplying factor):",null,null
,,,
103,tdw,null,null
,,,
104,",",null,null
,,,
105,xdw,null,null
,,,
106,c,null,null
,,,
107,m yd,null,null
,,,
108,or,null,null
,,,
109,xdw,null,null
,,,
110,log(1,null,null
,,,
111,+,null,null
,,,
112,c,null,null
,,,
113,m yd,null,null
,,,
114,),null,null
,,,
115,-1,null,null
,,,
116,"The concept of the information brought by a term in a document has been considered in several IR models. Harter [11] observed that 'significant', 'specialty' words of a document do not behave as 'functional' words. Indeed, the more a word deviates in a document from its average behavior in the collection, the more likely it is 'significant' for this particular document. This can be easily captured in terms of information: If a word behaves in the document as expected on the collection, then it has a high probability of occurrence in the document p, according to the distribution collection, and the information it brings to the document, - log(p), is small. On the contrary, if it has a low probability of occurrence in the document, according to the distribution collection, then the amount of information it conveys is more important. Because of the above consideration, this idea, at the basis of DFR models, has to be applied to the normalized form of the term frequency. This leads to the general and simple retrieval function:",null,null
,,,
117,"RSV (q, d) , X -xqw log P rob(Xw  tdw|w) (2)",null,null
,,,
118,wq,null,null
,,,
119,"where tdw is the normalized form of xdw and w is a parameter for the probability distribution of w in the collection. We simply consider here that w is set to either the average number of occurrences of w in the collection, or to the average number of documents in which w occurs, that is:",null,null
,,,
120," , zw , Fw or Nw",null,null
,,,
121,-3,null,null
,,,
122,NNN,null,null
,,,
123,It is interesting to note that the retrieval function defined,null,null
,,,
124,"by equation 2, which is rank invariant by the change of the logarithmic base, satisfies the heuristic retrieval conditions 1 and 3. Indeed, P rob(Xw  tdw|w) is a decreasing function of tdw. So, as long as tdw is an increasing function of xdw and a decreasing function of yd, which is the case for all the normalization functions we are aware of, conditions 1 and 3",null,null
,,,
125,are satisfied for this family of models.,null,null
,,,
126,235,null,null
,,,
127,3.1 Burstiness (and condition 2),null,null
,,,
128,"Church and Gale [6] were the first to study, to our knowledge, the phenomenon of burstiness in texts. The term ""burstiness"" describes the behavior of words which tend to appear in bursts, i.e., once they appear in a document, they are much more likely to appear again. The notion of burstiness is similar to the one of aftereffect of future sampling ([10]), which describes the fact that the more we find a word in a document, the higher the expectation to find new occurrences. Burstiness has recently received a lot of attention from different communities. Madsen [13], for example, proposed to use the Dirichlet Compound Multinomial (DCM) distribution in order to model burstiness in the context of text categorization and clustering. Elkan [8] then approximated the DCM distribution by the EDCM distribution, which learning time is faster, and showed the good behavior of the model obtained on different text clustering experiments. A related notion is the one of preferential attachment ([4] and [5]) often used in large networks, such as the web or social networks. It conveys the same idea: the more we have, the more we will get. In the context of IR, Xu and Akella [19] studied the use of a DCM model within the Probability Ranking Principle for modeling the dependency of word repetitive occurrences (a notion directly related to burstiness), and argue that multinomial distributions alone are not appropriate for IR within this principle. More formally, Clinchant and Gaussier [7] introduced the following definition (slightly simplified here for clarity's sake) in order to characterize discrete distributions which can account for burstiness:",null,null
,,,
129,"Definition 1. [Discrete case] A discrete distribution P is bursty iff for all integers (n, n), n  n:",null,null
,,,
130,P (X  n + 1|X  n) > P (X  n + 1|X  n),null,null
,,,
131,We generalize this definition to the continuous case as follows:,null,null
,,,
132,Definition 2. [General case] A distribution P is bursty iff the function g defined by:,null,null
,,,
133," > 0, g(x) , P (X  x + |X  x)",null,null
,,,
134,is a strictly increasing function of x. A distribution which verifies this condition is said to be bursty.,null,null
,,,
135,"which translates the fact that, with a bursty distribution, it is easier to generate higher values of X once lower values have been observed. We now show that this notion is directly related to the heuristic retrieval condition 2.",null,null
,,,
136,"In the retrieval function defined by equation 2, the function h we have considered so far corresponds to:",null,null
,,,
137,#NAME?,null,null
,,,
138,"In this case, condition 2 can be re-expressed as:",null,null
,,,
139,"2h(x, y, z, ) x2",null,null
,,,
140,<,null,null
,,,
141,0,null,null
,,,
142,2,null,null
,,,
143,log(P rob(X  (xdw )2,null,null
,,,
144,tdw )),null,null
,,,
145,>,null,null
,,,
146,0,null,null
,,,
147,But:,null,null
,,,
148,2f t2,null,null
,,,
149,",",null,null
,,,
150,2f x2,null,null
,,,
151,(,null,null
,,,
152,x t,null,null
,,,
153,)2,null,null
,,,
154,+,null,null
,,,
155,f x,null,null
,,,
156,2x t2,null,null
,,,
157,.,null,null
,,,
158,"Furthermore,",null,null
,,,
159,f x,null,null
,,,
160,is,null,null
,,,
161,here,null,null
,,,
162,negative,null,null
,,,
163,as,null,null
,,,
164,f,null,null
,,,
165,is,null,null
,,,
166,log(P rob(X,null,null
,,,
167,tdw )).,null,null
,,,
168,"So,",null,null
,,,
169,as,null,null
,,,
170,long,null,null
,,,
171,as,null,null
,,,
172,2x t2,null,null
,,,
173,0,null,null
,,,
174,(which is the case for all the normalization functions we are,null,null
,,,
175,"aware of, in particular the ones provided by equation 1), a",null,null
,,,
176,sufficient condition for condition 2 is:,null,null
,,,
177,2 log(P rob(X  (tdw )2,null,null
,,,
178,tdw )),null,null
,,,
179,>,null,null
,,,
180,0,null,null
,,,
181,The following theorem (the proof of which is given in the appendix) shows that bursty distributions satisfy this condition.,null,null
,,,
182,"Theorem 3. Let P be a ""bursty"" probability distribution of class C2. Then:",null,null
,,,
183,2 log(P (X x2,null,null
,,,
184,x)),null,null
,,,
185,>,null,null
,,,
186,0,null,null
,,,
187,"We thus see that under certain assumptions, IR models de-",null,null
,,,
188,"fined by equation 2 satisfy the form conditions 1, 2 and 3.",null,null
,,,
189,We now summarize these assumptions which characterize,null,null
,,,
190,information models.,null,null
,,,
191,3.2 Characterization of Information Models,null,null
,,,
192,We characterize information models by the following three elements:,null,null
,,,
193,"1. Normalization function The normalization function tdw, function of xdw and yd (respectively the number of occurrences of the word in the document and the length of the document), satisfies:",null,null
,,,
194, tdw  xdw,null,null
,,,
195,> 0;,null,null
,,,
196, tdw yd,null,null
,,,
197,<,null,null
,,,
198,0;,null,null
,,,
199, 2 xdw  (tdw )2,null,null
,,,
200,0,null,null
,,,
201,2. Probability distribution The probability distribution at the basis of the model has to be:,null,null
,,,
202,"· Continuous, the random variable under consideration, tdw, being continuous;",null,null
,,,
203,"· Compatible with the domain of tdw, i.e. if tmin is the minimum value of tdw, then P rob(Xw  tmin|w) ,"" 1 (because of the first inequality above, tmin is obtained when xdw "", 0);",null,null
,,,
204,· Bursty according to definition 2 above.,null,null
,,,
205,"3. Retrieval function The retrieval function satisfies equation 2, i.e.:",null,null
,,,
206,"RSV (q, d) , X -xqw log P rob(Xw  tdw|w)",null,null
,,,
207,wq,null,null
,,,
208,", X -xqw log P rob(Xw  tdw|w)",null,null
,,,
209,wqd,null,null
,,,
210,"where the second equality derives from the fact that the probability function verifies P rob(Xw  tmin|w) ,"" 1, with tmin obtained when xdw "","" 0. The above ranking function corresponds to the mean information a document brings to a query (or, equivalently, to the average of the document information brought by each query term). Furthermore, the parameter w is set as in equation 3:""",null,null
,,,
211," , zw , Fw or Nw NNN",null,null
,,,
212,"The general form of the retrieval function and the first two inequalities on the normalization function ensure that the model satisfies conditions 1 and 3. Theorem 3, in conjunction with the last condition on the normalization function, additionally ensures that it satisfies condition 2. Hence, information models satisfy three (out of four) form conditions. The choice of the particular bursty distribution to be used has to be made in such a way that the last form condition and the two adjustment conditions are satisfied.",null,null
,,,
213,236,null,null
,,,
214,3.3 Two Power-law Instances,null,null
,,,
215,"We present here two power law distributions which are bursty and lead to information models satisfying all form and adjustment conditions. The use of power law distributions to model burstiness is not entirely novel, as other studies ([4, 5]) have used similar distributions to model preferential attachment, a notion equivalent to burstiness.",null,null
,,,
216,Log-Logistic Distribution,null,null
,,,
217,"The log-logistic (LL) distribution is defined by, for X  0:",null,null
,,,
218,PLL (X,null,null
,,,
219,<,null,null
,,,
220,"x|r, )",null,null
,,,
221,",",null,null
,,,
222,x x + r,null,null
,,,
223,"We consider here a restricted form of the log-logistic distribution where  ,"" 1, so that the the log-logistic information model takes the form:""",null,null
,,,
224,"RSV (q, d) , X -xqw log(PLL(X  tdw|w))",null,null
,,,
225,wqd,null,null
,,,
226,",",null,null
,,,
227,X,null,null
,,,
228,wqd,null,null
,,,
229,#NAME?,null,null
,,,
230,log,null,null
,,,
231,(,null,null
,,,
232,tdw,null,null
,,,
233,w + w,null,null
,,,
234,),null,null
,,,
235,-4,null,null
,,,
236,"The log-logistic motivation resorts to previous work on text modeling. Following Church and Gale [6] and Airoldi [1], Clinchant and Gaussier [7] studied the negative binomial distribution in the context of text modeling. They then assumed a uniform Beta prior distribution over one of the parameters, leading to a distribution they refer to as the Beta negative binomial distribution, or BNB for short. One problem with the BNB distribution is that it is a discrete distribution and cannot be used for modeling tdw. However, the log-logistic distribution, with its  parameter set to 1, is a continuous counterpart of the BNB distribution since PLL(x  X < x + 1; r) , PBNB (x).",null,null
,,,
237,A Smoothed Power-Law (SPL) Distribution,null,null
,,,
238,"We consider here the distribution, which we will refer to as SPL, defined, for x > 0, by:",null,null
,,,
239,x,null,null
,,,
240,f (x; ),null,null
,,,
241,",",null,null
,,,
242,- log  1-,null,null
,,,
243, x+1 (x + 1)2,null,null
,,,
244,(0,null,null
,,,
245,<,null,null
,,,
246,<,null,null
,,,
247,1),null,null
,,,
248,P (X > x|),null,null
,,,
249,",",null,null
,,,
250,Z f (x; ),null,null
,,,
251,x,null,null
,,,
252,",",null,null
,,,
253,x,null,null
,,,
254, x+1 -  1-,null,null
,,,
255,"where f denotes the probability density function. Based on this distribution, the SPL information model thus takes the form:",null,null
,,,
256,tdw,null,null
,,,
257,RSV,null,null
,,,
258,"(q,",null,null
,,,
259,d),null,null
,,,
260,",",null,null
,,,
261,X,null,null
,,,
262,wqd,null,null
,,,
263,#NAME?,null,null
,,,
264,log(,null,null
,,,
265, tdw +1,null,null
,,,
266,w,null,null
,,,
267,1-,null,null
,,,
268,#NAME?,null,null
,,,
269,),null,null
,,,
270,-5,null,null
,,,
271,"From equations 4 or 5, and using the normalization functions defined by equation 1, one can verify (a) that the log-logistic and SPL distributions are bursty, and (b) that their corresponding information models additionally satisfy conditions 4, 5 and 6 (the demonstration is purely technical, and is skipped here). The log-logistic and SPL information models thus satisfy all the form and adjustment conditions.",null,null
,,,
272,"Figure 1 illustrates the behavior of the log-logistic model, the SPL model and the InL2 DFR model (referred to as INL for short). To compare these models, we used a value of 0.005 for  and computed the term weight obtained for term frequencies varying from 0 to 15. For information models, the weight corresponds to the quantity - log P rob, whereas",null,null
,,,
273,"r , 0.005",null,null
,,,
274,8,null,null
,,,
275,6,null,null
,,,
276,4,null,null
,,,
277,2,null,null
,,,
278,0,null,null
,,,
279,loglogistic inl spl,null,null
,,,
280,0,null,null
,,,
281,5,null,null
,,,
282,10,null,null
,,,
283,15,null,null
,,,
284,Figure 1: Plot of Retrieval Functions,null,null
,,,
285,"in the case of DFR models, this quantity is corrected by the Inf2 part, leading to, with the underlying distributions retained:",null,null
,,,
286,8,null,null
,,,
287,> >,null,null
,,,
288,-,null,null
,,,
289,log(,null,null
,,,
290,w tdw +w,null,null
,,,
291,),null,null
,,,
292,> > <,null,null
,,,
293,tdw,null,null
,,,
294,"weight , >",null,null
,,,
295,-,null,null
,,,
296,log(,null,null
,,,
297,wtdw +1 -w 1-w,null,null
,,,
298,),null,null
,,,
299,> > > :,null,null
,,,
300,#NAME?,null,null
,,,
301,tdw +1,null,null
,,,
302,log(,null,null
,,,
303,Nw +0.5 N +1,null,null
,,,
304,),null,null
,,,
305,(log-logistic),null,null
,,,
306,(SPL) (InL2),null,null
,,,
307,"As one can note, the weight values obtained with the two information models are always above the ones obtained with the DFR model, the log-logistic model having a sharper increase than the other ones for low frequency terms.",null,null
,,,
308,3.4 PRF in Information Models,null,null
,,,
309,"Pseudo-relevance feedback (PRF) in information models can be performed following the same approach as the one used in other models: The weight of a term in the original query is updated on the basis of the information brought by the top retrieved documents on the term. Denoting by R the set of top n documents retrieved for a given query, R ,"" (d1, . . . , dn), the average information this set brings on a given term w can directly be computed as:""",null,null
,,,
310,InfoR(w),null,null
,,,
311,",",null,null
,,,
312,1 n,null,null
,,,
313,X - log(P (Xw,null,null
,,,
314,>,null,null
,,,
315,tdw |w )),null,null
,,,
316,-6,null,null
,,,
317,dR,null,null
,,,
318,"where the mean is taken over all the documents in R. This is a major difference with the approach in [2] where all documents in R are merged into a single document. Considering the documents in R as different documents allows one to take into account the differences in document lengths and number of occurrences. The original query is then modified, following standard approaches to PRF, to take into account the words appearing in R as:",null,null
,,,
319,xqw2,null,null
,,,
320,",",null,null
,,,
321,xqw maxw,null,null
,,,
322,xqw,null,null
,,,
323,+,null,null
,,,
324,InfoR(w) maxw InfoR(w),null,null
,,,
325,-7,null,null
,,,
326,where  is a parameter controlling the modification brought by R to the original query. xqw2 denotes the updated weight of w in the query.,null,null
,,,
327,4. EXPERIMENTAL VALIDATION,null,null
,,,
328,"To assess the validity of our models, we used standard IR collections, from two evaluation campaigns: TREC (trec.nist.gov) and CLEF (www.clef-campaign.org). Table 2 gives the number of documents (N ), number of unique terms (M ), aver-",null,null
,,,
329,237,null,null
,,,
330,"age document length and number of test queries for the collections we retained: ROBUST (TREC), TREC3, CLEF03 AdHoc Task, GIRT (CLEF Domain Specific Task, from the years 2004 to 2006). For the ROBUST and TREC3 collections, we used standard Porter stemming. For the CLEF03 and GIRT collections, we used lemmatization, and an additional decompounding step for the GIRT collection which is written in German.",Y,null
,,,
331,Table 2: Characteristics of the different collections,null,null
,,,
332,N,null,null
,,,
333,M Avg DL # Queries,null,null
,,,
334,ROBUST 490 779 992 462 289,Y,null
,,,
335,250,null,null
,,,
336,TREC-3 741 856 668 648 438,Y,null
,,,
337,50,null,null
,,,
338,CLEF03 166 754 80 000 247,Y,null
,,,
339,60,null,null
,,,
340,GIRT 151 319 179 283 109,Y,null
,,,
341,75,null,null
,,,
342,We evaluated the log-logistic and the SPL model against,null,null
,,,
343,"language models, with both Jelinek-Mercer and Dirichlet",null,null
,,,
344,"Prior smoothing, as well as against the standard DFR mod-",null,null
,,,
345,"els and Okapi BM25. For each dataset, we randomly split",null,null
,,,
346,queries in train and test (half of the queries are used for,null,null
,,,
347,"training, the other half for testing). We performed 10 such",null,null
,,,
348,splits on each collection. The results we provide for the,null,null
,,,
349,Mean Average Precision (MAP) and the precision at 10 doc-,null,null
,,,
350,uments (P10) is the average of the values obtained over the,null,null
,,,
351,10 splits. The parameters of the different models are opti-,null,null
,,,
352,mized (respectively for the MAP and the precision at 10) on,null,null
,,,
353,the training set. The performance is then measured on the,null,null
,,,
354,"test set. To compare the different methods, a two-sided t-",null,null
,,,
355,test (at the 0.05 level) is performed to assess the significance,null,null
,,,
356,of the difference measured between the methods. All our,null,null
,,,
357,experiments were carried out thanks to the Lemur Toolkit,null,null
,,,
358,"(www.lemurproject.org). In all the following tables, ROB-t",null,null
,,,
359,"represents the robust collection with query titles only, ROB-",Y,null
,,,
360,d the robust collection with query titles and description,Y,null
,,,
361,"fields,CL-t represent titles for the CLEF collection, CL-d",Y,null
,,,
362,queries with title and descriptions and T3-t query titles for,null,null
,,,
363,TREC-3 collection. The GIRT queries are just made up of,Y,null
,,,
364,a single sentence.,null,null
,,,
365,The version of the log-logistic model used in all our ex-,null,null
,,,
366,"periments is based on w ,",null,null
,,,
367,nw N,null,null
,,,
368,and the second length nor-,null,null
,,,
369,malization in equation 1 (called L2 in DFR). We refer to,null,null
,,,
370,this model as the LGD model. The same settings are cho-,null,null
,,,
371,sen for the SPL model. As the parameter c in equation 1,null,null
,,,
372,"is not bounded, we have to define a set of possible values",null,null
,,,
373,from which to select the best value on the training set. We,null,null
,,,
374,make use of the typical range proposed in works on DFR,null,null
,,,
375,"models, which also rely on equation 1 for document length",null,null
,,,
376,"normalization. The set of values we retained is: {0.5, 0.75,",null,null
,,,
377,"1, 2, 3, 4, 5, 6, 7, 8, 9}.",null,null
,,,
378,Comparison with Jelinek-Mercer and Dirichlet language models,null,null
,,,
379,"As the smoothing parameter of the Jelinek-Mercer language model is comprised between 0 and 1, we use a regular grid on [0, 1] with a step size of 0.05 in order to select, on the training set, the best value for this parameter. Table 3 shows the comparison of our models, LGD and SPL, with the JelinekMercer language model (LM). On all collections, on both short and long queries, the LGD model significantly outperforms the Jelinek-Mercer language model. This is an interesting finding as the complexity of the two models is the same (in a way, they are both conceptually simple). Further-",null,null
,,,
380,"more, as the results displayed are averaged over 10 different splits, this shows that the LGD model consistently outperforms the Jelinek-Mercer language model and thus yields a more robust approach to IR. Lastly, the SPL model is better than the Jelinek-Mercer model for most collections for MAP and P10.",null,null
,,,
381,Table 3: LGD and SPL versus LM-Jelinek-Mercer after 10 splits; bold indicates significant difference,null,null
,,,
382,MAP ROB-d ROB-t GIR T3-t CL-d CL-t JM 26.0 20.7 40.7 22.5 49.2 36.5 LGD 27.2 22.5 43.1 25.9 50.0 37.5 P10 ROB-d ROB-t GIR T3-t CL-d CL-t JM 43.8 35.5 67.5 40.7 33.0 26.2 LGD 46.0 38.9 69.4 52.4 33.6 26.6,null,null
,,,
383,MAP JM SPL P10 JM SPL,null,null
,,,
384,ROB-d 26.6 26.7,null,null
,,,
385,ROB-d 44.4 47.6,null,null
,,,
386,ROB-t 23.1 25.2,null,null
,,,
387,ROB-t 39.8 45.3,null,null
,,,
388,GIR 39.2 41.7 GIR 66.0 69.8,null,null
,,,
389,T3-t 22.3 26.6 T3-t 43.9 56.0,null,null
,,,
390,CL-d 47.2 44.1 CL-d 34.0 34.0,null,null
,,,
391,CL-t 37.2 37.7 CL-t 25.6 25.6,null,null
,,,
392,"For the Dirichlet prior language model, we optimized the smoothing parameter from a set of typical values, defined by: {10, 50, 100, 200, 500, 800, 1000, 1500, 2000, 5000, 10000}. Table 4 shows the results of the comparison between our models and the Dirichlet prior language model (DIR). These results parallel the ones obtained with the JelinekMercer language model on most collections, even though the difference is less marked. For the ROB collection with short queries, the Dirichlet prior language model outperforms in average the log-logistic model (the difference being significant for the precision at 10 only). On the other collections, with both short and long queries and on both the MAP and the precision at 10, the log-logistic model outperforms in average the Dirichlet prior language model, the difference being significant in most cases. The Dirichlet model has a slight advantage in MAP over the SPL model, but SPL is better for precision. Overall, the information-based models outperform in average language models.",null,null
,,,
393,Table 4: LGD and SPL versus LM-Dirichlet after 10 splits; bold indicates significant difference,null,null
,,,
394,MAP ROB-d ROB-t GIR T3-t CL-t CL-d DIR 27.1 25.1 41.1 25.6 36.2 48.5 LGD 27.4 25.0 42.1 24.8 36.8 49.7 P10 ROB-d ROB-t GIR T3-t CL-t CLF-d DIR 45.6 43.3 68.6 54.0 28.4 33.8 LGD 46.2 43.5 69.0 54.3 28.6 34.5,null,null
,,,
395,MAP DIR SPL P10 DIR SPL,null,null
,,,
396,ROB-d 26.7 25.6,null,null
,,,
397,ROB-d 45.2 46.6,null,null
,,,
398,ROB-t 25.0 24.9,null,null
,,,
399,ROB-t 43.8 44.7,null,null
,,,
400,GIR 40.9 42.1 GIR 68.2 70.8,null,null
,,,
401,T3-t 27.1 26.8 T3-t 52.8 55.3,null,null
,,,
402,CL-t 36.2 36.4 CL-t 27.3 27.1,null,null
,,,
403,CL-d 50.2 46.9 CL-d 32.8 32.9,null,null
,,,
404,Comparison with BM25,null,null
,,,
405,We adopt the same methodology to compare information models with BM25. We choose only to optimize the k1 pa-,null,null
,,,
406,238,null,null
,,,
407,"rameter of BM25 among the following values: {0.3, 0.5, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.2, 2.5}. The others parameters b and k3 take their default values implemented in Lemur (0.75 and 7). Table 5 shows the comparison of the log-logistic and SPL models with Okapi BM25. The log-logistic is either better (4 collections out of 6 for mean average precision, 3 collections out of 6 for P10) or on par with Okapi BM25. The same thing holds for the SPL model, which is 3 times better and 3 times on par for the MAP, and 4 times better, 1 time worse and 1 time on a par for the precision at 10 documents. Overall, information models outperform in average Okapi BM25.",null,null
,,,
408,Table 5: LGD and SPL versus BM25 after 10 splits; bold indicates best performance significant difference,null,null
,,,
409,MAP ROB-d ROB-t GIR T3-t CL-t CL-d BM25 26.8 22.4 39.8 25.4 34.9 46.8 LGD 28.2 23.5 41.4 26.1 34.8 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d BM25 45.9 42.6 62.6 50.6 28.5 33.7 LGD 46.5 44.3 66.6 53.8 28.7 34.4,null,null
,,,
410,MAP BM25 SPL P10 BM25 SPL,null,null
,,,
411,ROB-d 26.9 27.1,null,null
,,,
412,ROB-d 45.7 47.6,null,null
,,,
413,ROB-t 24.2 25.4,null,null
,,,
414,ROB-t 41.4 44.1,null,null
,,,
415,GIR 38.5 40.5 GIR 62.8 67.9,null,null
,,,
416,T3-t 25.3 26.8 T3-t 51.0 57.0,null,null
,,,
417,CL-t 35.1 34.5 CL-t 28.5 28.0,null,null
,,,
418,CL-d 47.3 47.0 CL-d 36.1 35.4,null,null
,,,
419,"Comparison with DFR models To compare our model with DFR ones, we chose, in this latter family, the InL2 model, based on the Geometric distribution and Laplace law of succession, and the PL2 model based on the Poisson distribution and Laplace law. These models have been used with success in different works ([3, 7, 18] for example). All the models considered here make use of the same set of possible values for c, namely: {0.5, 0.75, 1, 2, 3, 4, 5, 6, 7, 8, 9}. It is however interesting to note that both PL2 and InL2 make use of discrete distributions (Geometric and Poisson) over continuous variables (tdw) and are thus theoretically flawed. This is not the case of the information models which rely on a continuous distribution.",null,null
,,,
420,"The results obtained, presented in tables 6 and 7 are more contrasted than the ones obtained with language models and Okapi BM25. In particular, for the precision at 10, LGD and InL2 perform similarly (LGD being significantly better on GIRT whereas InL2 is significantly better on ROB with long queries, the models being on a par in the other cases). For the MAP, the LGD model outperforms the InL2 model as it is significantly better on ROB (for both sort and long queries) and GIRT, and on a par on CLEF. SPL is better than InL2 for precision but on a par for MAP. Moreover, LGD and PL2 are on a par for MAP, while PL2 is better for P10. Lastly, PL2 is better than SPL for MAP but not for the precision at 10 documents. Overall, DFR models and information models yield similar results. This is all the more so interesting that information models are simpler than DFR ones: They rely on a single information measure (see equation 2) without the re-normalization (Inf2 part) used in DFR models.",null,null
,,,
421,Table 6: LGD and SPL versus INL after 10 splits; bold indicates significant difference,null,null
,,,
422,MAP ROB-d ROB-t GIR T3-t CL-t CL-d INL2 27.7 24.8 42.5 27.3 37.5 47.7 LGD 28.5 25.0 43.1 27.3 37.4 48.0 P10 ROB-d ROB-t GIR T3-t CL-t CL-d INL2 47.7 43.3 67.0 52.4 27.3 33.4 LGD 47.0 43.5 69.4 53.2 27.2 33.3,null,null
,,,
423,MAP INL SPL P10 INL SPL,null,null
,,,
424,ROB-d 26.9 26.6,null,null
,,,
425,ROB-d 47.6 47.8,null,null
,,,
426,ROB-t 24.3 24.6,null,null
,,,
427,ROB-t 42.8 44.1,null,null
,,,
428,GIR 40.4 40.7 GIR 63.4 68.0,null,null
,,,
429,T3-t 24.8 25.4 T3-t 52.5 53.9,null,null
,,,
430,CL-t 35.5 34.6 CL-t 28.8 28.7,null,null
,,,
431,CL-d 49.4 48.1 CL-d 33.8 33.6,null,null
,,,
432,Table 7: LGD and SPL versus PL2 after 10 splits; bold indicates significant difference,null,null
,,,
433,MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.2 24.8 40.6 24.9 36.0 47.2 LGD 27.3 24.7 40.5 24.0 36.2 47.5 P10 ROB-d ROB-t GIR T3-t CL-t CL-d PL2 46.4 44.1 68.2 55.0 28.7 33.1 LGD 46.6 43.2 66.7 53.9 28.5 33.7,null,null
,,,
434,MAP ROB-d ROB-t GIR T3-t CL-t CL-d PL2 26.3 25.2 42.8 25.8 37.3 45.7 SPL 26.3 25.2 42.7 25.3 37.4 44.1 P10 ROB-d ROB-t GIR T3-t CL-t CL-d PL2 46.0 45.2 69.3 54.8 26.2 32.7 SPL 47.0 45.2 69.8 55.4 25.9 32.9,null,null
,,,
435,Pseudo-relevance feedback,null,null
,,,
436,"There are many parameters for pseudo-relevance feedback algorithms: The number of document to consider (N ), the number of terms to add the query (T C) and the weight to give to those new query terms (parameter  in equation 7). Optimizing all these parameters and smoothing ones at the same time would be very costly. We thus modify here our methodology. For each collection, we choose the optimal smoothing parameters for each model (c,µ,k1) on all queries. The results obtained in this case are given in table 8, where LM+MIX corresponds here to the Dirichlet language model. They show, for example, that on the ROBUST collection there is no difference between the baseline systems we will use for pseudo-relevance feedback in terms of MAP. Overall, the precision at 10 is very similar for the different systems, so that there is no bias, with the setting chosen, towards a particular system. We compare here the results obtained with the information models to two state-ofthe-art pseudo-relevance feedback models: Bo2, associated with DFR models ([2]), and the mixture model associated with language models ([20]). For each collection, we average the results obtained over 10 random splits, the variation of N and T C being made on each split so as to be able to compare the results of the different settings. For each setting, we optimize the weight to give to new terms:  (within {0.1, 0.25, 0.5, 0.75, 1, 1.5, 2}) in information and Bo2 models,  ( within {0.1, 0.2, . . . , 0.9}) in the mixture-model for",null,null
,,,
437,239,null,null
,,,
438,"feedback in language models. In this latter case, we set the feedback mixture noise to its default value (0.5). As before, we used Lemur to carry our experiments and optimize here only the mean average precision. Table 9 displays the results for the different models (as before, a two-sided t-test at the 0.05 level is used to assess whether the difference is statistically significant, which is indicated by a ). As one can note, the information models significantly outperform the pseudo-relevance feedback versions of both language models and DFR models. The SPL model is the best one for N , 5 and T C ,"" 5, while the LGD model yields the best performance in most other cases. Altough DFR and information models perform similarly when no feedback is used, their pseudo-relevance feedback versions do present differences, information models outperforming significantly both language and DFR models in this latter case.""",null,null
,,,
439,"Table 8: Performances of baseline setting for PRF (N ,"" 0, T C "", 0): bold indicates significant difference",null,null
,,,
440,MAP LM+MIX,null,null
,,,
441,LGD P10 LM+MIX LGD,null,null
,,,
442,ROB-t 25.4 25.4,null,null
,,,
443,ROB-t 44.6 44.1,null,null
,,,
444,GIRT 41.1 42.4 GIRT 68.3 68.7,null,null
,,,
445,T3-t 28.3 27.1 T3-t 56.3 55.3,null,null
,,,
446,CLEF-t 37.0 37.5,null,null
,,,
447,CLEF-t 27.5 27.2,null,null
,,,
448,"Table 9: Mean average precision of PRF experiments; bold indicates best performance,  significant",null,null
,,,
449,difference over LM and Bo2 models Model N TC ROB-t GIR T3-t,null,null
,,,
450,CL-t,null,null
,,,
451,LM+MIX 5 5 27.5 44.4 30.7 36.6,null,null
,,,
452,LGD,null,null
,,,
453,5 5 28.3 44.3 32.9 37.6,null,null
,,,
454,INL+Bo2 5 5 26.5 42.0 30.6 37.6,null,null
,,,
455,SPL,null,null
,,,
456,5 5 28.9 45.6 32.9 39.0,null,null
,,,
457,LM+MIX 5 10 28.3 45.7 33.6 37.4,null,null
,,,
458,LGD,null,null
,,,
459,5 10 29.4 44.9 35.0 40.2,null,null
,,,
460,INL+Bo2 5 10 27.5 42.7,null,null
,,,
461,SPL,null,null
,,,
462,5 10 29.6 47.0,null,null
,,,
463,32.6 34.6,null,null
,,,
464,37.5 39.5,null,null
,,,
465,LM+MIX 10 10 28.4 45.5 31.8 37.6 LGD 10 10 30.0 46.8 35.5 38.9,null,null
,,,
466,INL+Bo2 10 10 27.2 43.0 32.3 37.4,null,null
,,,
467,SPL,null,null
,,,
468,10 10 30.0 48.9 33.8 39.1,null,null
,,,
469,LM+MIX 10 20 29.0 46.2 33.7 38.2 LGD 10 20 30.3 47.6 37.4 38.6,null,null
,,,
470,INL+Bo2 10 20 27.7 43.5 33.8 37.7,null,null
,,,
471,SPL,null,null
,,,
472,10 20 29.9 50.2 34.3 39.7,null,null
,,,
473,LM+MIX 20 20 28.6 47.9 32.9 37.8 LGD 20 20 29.5 48.9 37.2 41.0,null,null
,,,
474,INL+Bo2 20 20 27.4 44.3 33.5 36.8,null,null
,,,
475,SPL,null,null
,,,
476,20 20 28.8 50.3 33.9 39.0,null,null
,,,
477,5. DISCUSSION,null,null
,,,
478,"The Divergence from Randomness (DFR) framework proposed by Amati and van Rijsbergen [3] is based on the informative content provided by the occurrences of terms in documents, a quantity which is then corrected by the risk of accepting a term as a descriptor in a document (first",null,null
,,,
479,"normalization principle) and by normalizing the raw occurrences by the length of a document (second normalization principle). The informative content Inf1(tdw) is based on a first probability distribution and is defined as: Inf1(tdw) , - log P rob1(tdw). The first normalization principle is associated with a second information defined from a second probability distribution through: Inf2(tdw) , 1 - P rob2(tdw). The overall IR model is then defined as a combination of Inf1 and Inf2:",null,null
,,,
480,"RSV (q, d) , X xqwInf2(tdw)Inf1(tdw)",null,null
,,,
481,wqd,null,null
,,,
482,", X -xqwInf2(tdw) log P rob1(tdw)",null,null
,,,
483,wqd,null,null
,,,
484,"The above form shows that DFR models can be seen as information models, as defined by equation 2, with a correction brought by the Inf2 term. If Inf2(tdw) was not used in DFR models, the models with Poisson, Geometric, Binomial distributions would not respect condition 2, i.e would not be concave. In contrast, the use of bursty distributions in information models, together with the conditions on the normalization functions, ensure that condition 2 is satisfied. Another important difference between the two models is that DFR models make use of discrete distributions for realvalued variables, a conceptual flaw that information models do not have. Lastly, if the log-logistic, SPL and INL models have very simple forms (see for example the formulas given above for the weight they generate), the PL2 DFR model, one of the top performing DFR models, has a much more complex form ([18]). Information models are thus not only conceptually simpler, they also lead to simpler formulas.",null,null
,,,
485,6. CONCLUSION,null,null
,,,
486,"We have presented in this paper the family of information models. These models draw their inspiration from a long standing idea in information retrieval, namely the one that a word in a document may not behave statistically as expected on the collection. Shannon information can be used to capture whenever a word deviates from its average behavior, and we showed how to design IR models based on this information. In particular, we showed that the choice of the distribution to be used in such models was crucial for obtaining good retrieval models, the notion of good retrieval models being formalized here on the basis of the heuristic retrieval constraints developed in [9]. Our theoretical development also emphasized the notion of ""burstiness"", which has been central to several studies. We showed how this notion relates to heuristic retrieval constraints, and how it can be captured through, e.g., power-law distributions. From these two distributions, we have proposed two effective IR models. The experiments we have conducted on four different collections illustrate the good behavior of these models. They outperform in average the Jelinek-Mercer and Dirichlet prior language models as well as the Okapi BM25 model. They yield results similar to state-of-the-art DFR models (InL2 and PL2) when no pseudo-relevance feedback is used. When using pseudo-relevance feedback, however, the information models we have considered significantly outperform all the other models.",null,null
,,,
487,240,null,null
,,,
488,Acknowledgements,null,null
,,,
489,This research was partly supported by the Pascal-2 Network of Excellence ICT-216886-NOE and the French project Fragrances ANR-08-CORD-008.,null,null
,,,
490,7. REFERENCES,null,null
,,,
491,"[1] E. M. Airoldi, W. W. Cohen, and S. E. Fienberg. Bayesian methods for frequent terms in text: Models of contagion and the 2 statistic.",null,null
,,,
492,"[2] G. Amati, C. Carpineto, G. Romano, and F. U. Bordoni. Fondazione Ugo Bordoni at TREC 2003: robust and web track, 2003.",null,null
,,,
493,"[3] G. Amati and C. J. V. Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.",null,null
,,,
494,"[4] A. L. Barabasi and R. Albert. Emergence of scaling in random networks. Science, 286(5439):509­512, October 1999.",null,null
,,,
495,"[5] D. Chakrabarti and C. Faloutsos. Graph mining: Laws, generators, and algorithms. ACM Comput. Surv., 38(1):2, 2006.",null,null
,,,
496,"[6] K. W. Church and W. A. Gale. Poisson mixtures. Natural Language Engineering, 1:163­190, 1995.",null,null
,,,
497,"[7] S. Clinchant and E´. Gaussier. The BNB distribution for text modeling. In Macdonald et al. [12], pages 150­161.",null,null
,,,
498,"[8] C. Elkan. Clustering documents with an exponential-family approximation of the dirichlet compound multinomial distribution. In W. W. Cohen and A. Moore, editors, ICML, volume 148 of ACM International Conference Proceeding Series, pages 289­296. ACM, 2006.",null,null
,,,
499,"[9] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, 2004.",null,null
,,,
500,"[10] W. Feller. An Introduction to Probability Theory and Its Applications, Vol. I. Wiley, New York, 1968.",null,null
,,,
501,"[11] S. P. Harter. A probabilistic approach to automatic keyword indexing. Journal of the American Society for Information Science, 26, 1975.",null,null
,,,
502,"[12] C. Macdonald, I. Ounis, V. Plachouras, I. Ruthven, and R. W. White, editors. Advances in Information Retrieval , 30th European Conference on IR Research, ECIR 2008, Glasgow, UK, March 30-April 3, 2008. Proceedings, volume 4956 of Lecture Notes in Computer Science. Springer, 2008.",null,null
,,,
503,"[13] R. E. Madsen, D. Kauchak, and C. Elkan. Modeling word burstiness using the dirichlet distribution. In L. D. Raedt and S. Wrobel, editors, ICML, volume 119 of ACM International Conference Proceeding Series, pages 545­552. ACM, 2005.",null,null
,,,
504,"[14] S.-H. Na, I.-S. Kang, and J.-H. Lee. Improving term frequency normalization for multi-topical documents and application to language modeling approaches. In Macdonald et al. [12], pages 382­393.",null,null
,,,
505,[15] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR '94:,null,null
,,,
506,"Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.",null,null
,,,
507,"[16] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA, 1983.",null,null
,,,
508,"[17] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In SIGIR '96: Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, pages 21­29, New York, NY, USA, 1996. ACM.",null,null
,,,
509,"[18] I. O. V. Plachouras, B. He. University of Glasgow at TREC 2004: Experiments in web, robust and terabyte tracks with terrier, 2004.",null,null
,,,
510,"[19] Z. Xu and R. Akella. A new probabilistic retrieval model based on the dirichlet compound multinomial distribution. In SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, pages 427­434, New York, NY, USA, 2008. ACM.",null,null
,,,
511,"[20] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In CIKM '01: Proceedings of the tenth international conference on Information and knowledge management, pages 403­410, New York, NY, USA, 2001. ACM.",null,null
,,,
512,"[21] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, 2004.",null,null
,,,
513,APPENDIX,null,null
,,,
514,A. PROOF OF THEOREM 3,null,null
,,,
515,Let us recall what property 3 states: Let P be a probability distribution of class C2. A necessary condition for P to be bursty is:,null,null
,,,
516,2 log(P (X x2,null,null
,,,
517,x)),null,null
,,,
518,>,null,null
,,,
519,0,null,null
,,,
520,"Proof Let P be a continuous probability distribution of class C2. y > 0, the function gy defined by:",null,null
,,,
521,"y > 0,",null,null
,,,
522,"gy(x) , P (X",null,null
,,,
523, x + y|X,null,null
,,,
524," x) ,",null,null
,,,
525,P (X  x + y) P (X  x),null,null
,,,
526,is increasing in x (by definition of a bursty distribution).,null,null
,,,
527,"Let F be the cumulative function of P . Then: gy(x) ,",null,null
,,,
528,F (x+y)-1 F (x)-1,null,null
,,,
529,.,null,null
,,,
530,For,null,null
,,,
531,y,null,null
,,,
532,sufficiently,null,null
,,,
533,"small,",null,null
,,,
534,using,null,null
,,,
535,a,null,null
,,,
536,Taylor,null,null
,,,
537,expansion,null,null
,,,
538,"of F (x + y), we have:",null,null
,,,
539,gy (x),null,null
,,,
540,F (x) + yF (x) - 1 F (x) - 1,null,null
,,,
541,",",null,null
,,,
542,g(x),null,null
,,,
543,where,null,null
,,,
544,F,null,null
,,,
545,denotes,null,null
,,,
546,F x,null,null
,,,
547,.,null,null
,,,
548,ering only the sign of,null,null
,,,
549,"Then, derivating g, we get:",null,null
,,,
550,g,null,null
,,,
551,wrt,null,null
,,,
552,x,null,null
,,,
553,and,null,null
,,,
554,consid-,null,null
,,,
555,sg[g],null,null
,,,
556,",",null,null
,,,
557,sg[F F,null,null
,,,
558,-,null,null
,,,
559,F ,null,null
,,,
560,-,null,null
,,,
561,F 2],null,null
,,,
562,",",null,null
,,,
563,sg[(,null,null
,,,
564,F,null,null
,,,
565,F -,null,null
,,,
566,1 )],null,null
,,,
567,", sg[(log(1 - F ))] , sg[(log P (X  x))]",null,null
,,,
568,As,null,null
,,,
569,gy,null,null
,,,
570,is,null,null
,,,
571,increasing,null,null
,,,
572,in,null,null
,,,
573,"x,",null,null
,,,
574,so,null,null
,,,
575,is,null,null
,,,
576,"g,",null,null
,,,
577,and,null,null
,,,
578,thus,null,null
,,,
579,2 log(P (Xx)) x2,null,null
,,,
580,>,null,null
,,,
581,"0,",null,null
,,,
582,which establishes the property.,null,null
,,,
583,241,null,null
,,,
584,,null,null

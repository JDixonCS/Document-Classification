,sentence,label,data
0,Interactive Retrieval Based on Faceted Feedback,null,null
1,"Lanbo Zhang, Yi Zhang",null,null
2,School of Engineering UC Santa Cruz,null,null
3,"Santa Cruz, CA, USA",null,null
4,"{lanbo, yiz}@soe.ucsc.edu",null,null
5,ABSTRACT,null,null
6,"Motivated by the commonly used faceted search interface in e-commerce, this paper investigates interactive relevance feedback mechanism based on faceted document metadata. In this mechanism, the system recommends a group of document facet-value pairs, and lets users select relevant ones to restrict the returned documents. We propose four facetvalue pair recommendation approaches and two retrieval models that incorporate user feedback on document facets. Evaluated based on user feedback collected through Amazon Mechanical Turk, our experimental results show that the Boolean filtering approach, which is widely used in faceted search in e-commerce, doesn't work well for text document retrieval, due to the incompleteness (low recall) of metadata assignment in semi-structured text documents. Instead, a soft model performs more effectively. The faceted feedback mechanism can also be combined with document-based relevance feedback and pseudo relevance feedback to further improve the retrieval performance.",null,null
7,Categories and Subject Descriptors,null,null
8,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
9,General Terms,null,null
10,"Algorithms, Experimentation",null,null
11,Keywords,null,null
12,"interactive retrieval, faceted feedback, relevance feedback, metadata-based retrieval",null,null
13,1. INTRODUCTION,null,null
14,"A personalized search or filtering system usually suffers from the ""cold start"" problem, where the system performs poorly when it has little training data about new users. Researchers have proposed some approaches trying to alleviate",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'10, July 19­23, 2010, Geneva, Switzerland. Copyright 2010 ACM 978-1-60558-896-4/10/07 ...$10.00.",null,null
16,"this problem. One direction is to borrow information from other users [20, 24]. For example, the idea in [24] is to learn a prior of user interests based on the behaviors (training data) of all users, and learn the user profile for a new user based on both the prior and the training data from this user. Another direction is to develop user interaction mechanisms to collect more information from users[18]. In this paper, we focus on the second direction. We aim to study a new interactive user feedback mechanism that helps retrieval systems learn more about user information needs with limited user interactions.",null,null
17,"Faceted search has gained great success in e-commerce domain over the past years, and most popular online retailers, such as Amazon and eBay, now provide faceted search interfaces. On faceted-search-enabled websites, buyers can narrow down the list of products by putting constraints on a group of merchandize facets, such as category, price, brand, size, etc. Well designed faceted search has been shown to be understood by the average user [11]. This motivates us to explore whether we can adapt the faceted search idea to the general purpose document retrieval. In each domain, documents have their own facets, which might be manually assigned or generated automatically. These facets are usually stored in the form of faceted document metadata. Each metadata field corresponds to a facet type, and the specific value assigned to a field for a particular document is a facet value.",null,null
18,"Users might have preferences for certain document facets. For example, Chinese readers prefer reading news written in Chinese; some students enjoy learning by reading slides which are usually in the ""ppt"" format rather than reading long papers which are usually in the ""pdf"" format; researchers are usually interested in papers within their own subjects; movie viewers might have preferences on movie genres, directors, or casts; online buyers might have preferences on brands, colors, etc. In all these cases, users have clear ideas about some facets of their interested documents, and this information might help the system learn users' preferences and interests. Ideally, users would provide structured queries to describe their information needs more accurately. However, INEX experiments on structured documents retrieval and previous research on log analysis found that people do not use structure in their queries frequently, or use them incorrectly and thus do not improve search effectiveness if they are forced to do so [8].",null,null
19,"In this paper, we explore a simple interactive user feedback mechanism based on document facets, called faceted feedback. In this mechanism, instead of letting users pro-",null,null
20,363,null,null
21,"vide relevance feedback on documents or create structured queries actively, the system suggests faceted constraints (in the form of facet-value pairs) and users can choose interesting facet-value pairs to improve the returned documents.",null,null
22,"We study two major problems of designing a faceted feedback based retrieval system. First, how to recommend facetvalue pairs to users. In e-commerce domain, the candidates of facets and possible values for products are usually manually designed. To make it applicable in general purpose document retrieval, automatic facet recommendation is needed. In this paper, we investigate four approaches to recommending good facet-value pairs. Secondly, we study how to use user faceted feedback in retrieval. Existing e-commerce websites often use a Boolean filtering strategy while retrieving products. However, this may not be a good approach for all domains, since the document metadata is usually imperfect, and the rigid Boolean model may miss relevant documents and hurts the system recall. Thus we also propose a soft retrieval model. In this model, a document that meets a users elected faceted constraint gets a certain number of credits.",null,null
23,"The proposed faceted feedback mechanism may have the following advantages. First, the suggested facet-value pairs are usually short and easy to understand. Compared with document-based feedback, this may reduce the cognitive overload of the user and thus is more likely to be adopted by the average user. Users can quickly select multiple facet-value pairs in a short time, so the system might get more user feedback. Second, it may help a user better understand the corpus, how the engine works, and train users in how to form better queries.",null,null
24,"The rest of this paper is organized as follows. In section 2, we talk about the related work. Section 3 is the focus of this paper, and describes the faceted feedback mechanism. We propose four facet-value pair recommendation methods and two retrieval models in this section. In section 4, we describe the methodology of our experiments. Section 5 gives the experimental results and the corresponding analysis. Section 6 concludes this paper.",null,null
25,2. RELATED WORK,null,null
26,"Many existing search engines equate user information needs with a keyword query, assuming that a user knows what words to use to best describe his or her information need. However, a user's information need is characterized by complex user criteria that are not included in a simple keyword query. Relevance feedback is a commonly used query refinement technique that can be traced back to 1960s. The basic idea is to rely on user interactions to better capture the user information need.",null,null
27,"Document-based relevance feedback is one of the most widely used explicit feedback mechanisms. In this scenario, users are asked to provide feedback on the relevance of delivered documents. Many approaches have been proposed to incorporate document relevance feedback into retrieval. For example, Rocchio proposed to combine the original query vector with the center of relevant documents and the center of non-relevant documents [18]. Zhai et al. proposed to estimate a feedback topic model based on user feedback using Maximum Likelihood Estimation (MLE) in the language modeling approach [23]. Zhang et al. proposed to use the Bayesian logistic regression model combined with Rocchio algorithm [24]. Also, several approaches have been proposed to actively select good documents for users to pro-",null,null
28,"vide relevance feedback. The simplest way is to choose the top ranked documents since they are most probably relevant. Others also tried other approaches, such as to choose documents with presumably good qualities (e.g., Wikipedia articles), or to choose a diversified set of documents based on document clustering or active learning [19].",null,null
29,"A special type of document-based relevance feedback is pseudo relevance feedback. In this case, the top ranked documents are assumed to be relevant and used to modify the query based on the document feedback algorithms described above. Though the assumption is not true, pseudo relevance feedback has been proven effective in improving retrieval performances for short queries [12]. Kelly et al. found that pseudo-relevance feedback performs better for recall-oriented measures [13].",null,null
30,"Term-based relevance feedback is to let users select relevant terms from a group of candidates suggested by query expansion techniques. However, research on term-based feedback have mixed results: some found it effectively improves retrieval performance [10, 22], while others found no obvious improvement [4].",null,null
31,Raghavan et al. proposed to use feedback on both instances and features and proposed a unified framework that can be used to combine document-based relevance feedback and feature-based relevance feedback [15].,null,null
32,"Our work is motivated by early work in relevance feedback, and differs by focusing on retrieving semi-structured documents with faceted metadata. Anick et al. proposed to extract faceted terminologies automatically from the document text and let users provide relevance feedback on these faceted terminologies [5]. However, the facets in this paper refer to faceted terminologies, usually noun phrases. [9] proposed to get user feedback about controlled indexing vocabulary and got promising results on OHSUMED data set. However, existing research didn't provide detailed description about the algorithms or any quantitative evaluation with real users.",null,null
33,3. FACETED FEEDBACK,null,null
34,"Unlike document-based relevance feedback mechanism which asks users to give feedback on the relevance of documents, faceted feedback allows users to give feedback on document metadata fields. In this paper, each metadata field is called a facet, and a facet (f ) with a specific value (v) is called a facet-value pair (f : v). Each facet-value pair represents a faceted constraint on returned documents, E.g., language:Chinese, format:ppt, subject:IR, genre:comedy.",null,null
35,3.1 Facet-value pair recommendation,null,null
36,"To avoid overwhelming users with many facet-value pair candidates, the system needs to recommend a small number of facet-value pairs that are most probably interesting to a user. A good recommendation approach is crucial in the faceted feedback mechanism. Intuitively, the recommended facet-value pairs should be good in two respects: 1) they have a high probability of being relevant and thus chosen by the user; 2) they maximize the learning benefits if known to be relevant. Based on the first respect, we propose four facet-value pair recommendation methods. We will investigate the second respect in our future work.",null,null
37,364,null,null
38,3.1.1 Top Document Frequency (TDF),null,null
39,normalization here:,null,null
40,"The first approach is to select the most frequent facetvalue pairs occurring in the top N ranked documents returned by a baseline retrieval algorithm using the initial query. We calculate the frequency of each facet-value pair in the top N documents, which is called ""Top N Document Frequency"" (TDF). The top K most frequent facet-value pairs are chosen as candidates to present to the user. The underlying assumption is that the more frequently a facet-value pair appears in the top ranked documents, the more likely the user will like it.",null,null
41,N Sl(s),null,null
42,",",null,null
43,s - minsiS (si) maxsiS (si) - minsiS (si),null,null
44,(5),null,null
45,where S is the set of scores of all considered facet-value pairs.,null,null
46,3.2 Incorporate faceted feedback into retrieval,null,null
47,We present two retrieval models to incorporate user faceted feedback in this section. Pu denotes the set of facet-value pairs chosen by the user.,null,null
48,3.2.1 Boolean Model,null,null
49,3.1.2 TDF-IDF,null,null
50,"In the term-based feedback literature, researchers have concerns about using the most frequent terms from the top ranked documents, because a lot of common noisy terms are likely to be selected [23]. To avoid similar problems in faceted feedback, we consider another feature of facet-value pairs: the Inverse Document Frequency (IDF), which has a similar definition to the IDF of terms. When scoring a facet-value pair, we use the product of its top N document frequency (TDF) and IDF:",null,null
51,"score(f : v, q) ,"" tdf (f : v, q, N )  idf (f : v) (1)""",null,null
52,"where f : v is a facet-value pair, q is the initial query, and tdf (f : v, q, N ) is the top N document frequency of f : v for query q.",null,null
53,The motivation of using IDF is twofold: 1) a facet-value pair that appears rarely in the whole corpus while frequently in top ranked documents has a high probability to be relevant; 2) the retrieval system gets more benefits by knowing a rare facet-value pair covering a small number of documents being relevant than a frequent one.,null,null
54,3.1.3 Query Likelihood (QL),null,null
55,Our third method is based on the language modeling approach. The query likelihood given each facet-value pair P (q|f : v) is estimated. The facet-value pairs with the largest query likelihoods are chosen as the candidates.,null,null
56,"P (q|f : v) ,",null,null
57,"P (wj |f : v)c(wj ,q)",null,null
58,(2),null,null
59,wj q,null,null
60,"where c(wj, q) is the frequency of wj in the query q, and",null,null
61,"P (wj|f : v) , P (wj|d)P (d|f : v)",null,null
62,(3),null,null
63,dC,null,null
64,"This is a ""translation"" model motivated by Berger et al. [6]. C is the whole corpus, P (wj|d) is the language model of document d, and P (d|f : v) is assumed to be uniform over all documents that contain f : v.",null,null
65,3.1.4 TDF-QL,null,null
66,"The Boolean model filters documents with user faceted feedback. We can use the AND operation to require the retrieved documents contain all of the user-selected facet-value pairs. In practice, the AND operation might be too strict. One alternative is to use the OR operation to allow any document that contains at least one user-selected facet-value pair to pass. Another alternative is to use AND across different facets and OR within each facet. The Boolean model itself returns a document set instead of a ranked list. We can use any ranking methods, such as TFIDF, BM25 [21], etc., to rank the passed documents. We score documents by the Boolean model as follows:",null,null
67," sm(d)  sbl(d) ,",null,null
68,-,null,null
69,if d contains all(AND) / any one(OR) facet-value pair f : v  Pu otherwise,null,null
70,(6) where sm(d) is the score of document d computed using a baseline ranking method m.,null,null
71,3.2.2 Soft Model,null,null
72,"Despite the fact that the Boolean model is commonly used in the e-commerce domain, it may not work well for semistructured text document retrieval. The Boolean model is based on two assumptions: 1) users are very clear about what they are looking for, and thus are able to select perfect facet-value pairs to restrict the returned documents; 2) document facets are accurate and complete so that no potentially relevant document is filtered out in retrieval due to meta data errors. These two assumptions may not hold in text document retrieval.",null,null
73,"In a specific domain, some facets might be more informative than others. For example, for news articles, the information of time, locations, persons, and topics may be more important than publishers; for research papers, the subjects and keywords may be more informative than the file formats; For movies, the genres, casts and directors may be more informative than producers.",null,null
74,"Based on the above motivations, we propose a soft retrieval model. In this model, we learn a weight for each type of facet, which is expected to reflect the quality of the facet. Here the quality may include user acquaintance, metadata accuracy, facet importance, etc. The soft model scores a document as follows:",null,null
75,TDF and QL capture the relationships between the user query and a facet-value pair from different aspects and may complement each other. We combine these two features to score a facet-value pair as follows:,null,null
76,"ssm(d) ,N Ss(sm(d))",null,null
77,+ f  N Sl(,null,null
78,"(d, f : v)  idf (f : v)) (7)",null,null
79,f F,null,null
80,f :vPu,null,null
81,where,null,null
82,"score(f : v, q) ,"" N Sl(P (q|f : v))+(1-)N Sl(tdf (f : v, q, N ))""",null,null
83,(4),null,null
84,"(d, f : v) , 1 if d contains f : v",null,null
85,(8),null,null
86,where N Sl() is to normalize the features. We use linear,null,null
87,0 otherwise,null,null
88,365,null,null
89,f is the weight of facet f and is learned automatically. sm(d) is the original score of document d.,null,null
90,"N Ss() is the standard normalization that converts the original document scores into a distribution with mean 0 and variance 1. The distributions of original document scores (sm(d)) across different queries and using different baseline retrieval models might be significantly different. We found the difference would badly hurt the retrieval performance in our experiment. So we chose to normalize the original document scores first. As defined in Equation 5, N Sl() is the linear normalization of the score part of a facet f .",null,null
91,4. EXPERIMENTAL METHODOLOGY,null,null
92,4.1 Datasets,null,null
93,"To evaluate the proposed faceted feedback mechanism, we use two TREC filtering track datasets: the medical article collection OHSUMED and the news story collection RCV1 [14]. We choose these two corpora because they contain metadata, user queries/profiles, and relevance judgments.",null,null
94,"OSHUMED dataset contains 348,566 medical articles selected from a subset of 270 medical journals covering years from 1987 to 1991. This dataset was used in the TREC 2000 filtering track [16], and we use the topics of this track to simulate user information needs in our experiment. The metadata field MeSH (Medical Subject Headline) is used as a document facet.",null,null
95,"RCV1(Reuters Corpus Volume 1) dataset contains about 810,000 Reuters news stories published from 1996-0820 to 1997-08-19. There are three types of codes assigned to documents in this collection: topic, geographical region, and industry. These codes are generated with a process involved a combination of auto-categorization, manual editing, and manual correction. We use the three codes as document facets. RCV1 was used in the TREC 2002 filtering track [17], and the first 50 topics of this track are used to simulate user information needs in our experiment1.",null,null
96,4.2 Evaluation Based on Mechanical Turk,null,null
97,"We use the Mechanical Turk [1] to collect user faceted feedback. Mechanical Turk is an online marketplace for work, where requesters can publish some tasks that require human intelligence, and workers can choose to work on the tasks to get paid. Comparing TREC assessors with Mechanical Turk workers, prior research shows Mechancial Turk workers are a good source for IR evaluation [3]. In our experiments, we ask workers to act as a real user to provide faceted feedback. For each query2, we design a question, in which the TREC topic statement (including the title and description) and a group of recommended facet-value pairs are shown (See figure 1). Mechanical Turk workers are asked to select good facet-value pairs to restrict the search results according to their understanding of the query. The topic statement helps them act as if they are the real search engine users with the information need. By configuring the Human Intelligence Task (HIT) properties, we make sure there are three workers work on each query to give faceted feedback. Note that those workers are all random workers on Mechanical Turk who happen to see our task and choose",null,null
98,1The prior research shows that the other topics do not match real user information needs well. 2Each query corresponds to a TREC topic.,null,null
99,to work on it. We also design some questions about their knowledge background related to the query topics in order to help us understand if there is a strong correlation between user knowledge levels and feedback quality.,null,null
100,4.3 Experimental settings,null,null
101,Our experiment is designed to answer the following questions:,null,null
102,· Is faceted feedback mechanism effective in improving retrieval performance?,null,null
103,· How does faceted feedback compare to other feedback mechanisms?,null,null
104,· Can faceted feedback be used together with other feedback mechanisms?,null,null
105,· Which facet-value pair recommendation methods are better?,null,null
106,"To answer the first question, we compare the retrieval performance of faceted feedback to the baseline method BM25 without user feedback. In the baseline retrieval, only the title parts of TREC topics are used in order to simulate the short queries in real scenarios. To answer the second question, we compare faceted feedback with pseudo relevance feedback (PRF) and real document relevance feedback (RRF). We use the relevance judgments provided by TREC as simulated user feedback for RRF. To answer the third question, we use PRF and RRF respectively to calculate the original document scores (sm(d) in equation 6 and 7). The final retrieval performances will tell us whether faceted feedback complements existing feedback mechanisms and can be combined with them to further improve the retrieval performance. To answer the fourth question, we compare the retrieval performances of different faceted-value pair recommendation methods. Standard IR evaluation measures Mean Average Precision (MAP), Precision@N (P@N) and Recall@N (R@N) are used to evaluate the retrieval performances.",null,null
107,"In our initial experiment, we found different users might choose different facet-value pairs given the same query and the same candidate set, which will lead to different retrieval performances. To avoid the influence of user difference, one possibility is to have the same user work on each of the candidate sets to be compared. However, a user's choice is influenced by his/her past experience and thus the order of how the candidate sets are presented will influence the results. To alleviate this problem, we combine the candidate sets recommended by four methods and present the large set to the user for feedback. When calculating the retrieval performance of a specific recommendation method, we only use the user-selected facet-value pairs that are included in the candidate set recommended by this method.",null,null
108,"We set the number of facet-value pairs each recommendation method recommends (K) to 10, the number of top ranked documents used in the recommendation of facetvalue pairs (N in equation 1) to 100, and the weight of query likelihood ( in equation 4) to 0.5.",null,null
109,366,null,null
110,Figure 1: User interface on Mechanical Turk,null,null
111,5. EXPERIMENTAL RESULTS,null,null
112,5.1 Overall performances of faceted feedback,null,null
113,"Table 1 shows the retrieval performances of the baseline (BM25), using faceted feedback (FF) from individual user (User1, 2 and 3 for OHSUMED dataset, User4, 5 and 6 for RCV1 dataset), and the average over three users (FF(Average)). P@10 is the precision of top 10 documents. All the performances reported here are obtained using the soft retrieval model3. The average MAP and P@10 of using faceted feedback on OHSUMED dataset are improved by 32.4% and 43.9% over the baseline (BM25) respectively. The average MAP and P@10 on RCV1 dataset are improved by 11.1% and 8.8% respectively. According to these results, we conclude that faceted feedback is effective in improving retrieval performance.",null,null
114,"Table 1: Performances of Faceted Feedback (FF). ""FF (User1|4)"" means to use faceted feedback from User1 (on OHSUMED dataset) and User4 (on RCV1 dataset).",null,null
115,"usually have disagreements about document relevance judgments. This is also consistent with our anticipation: users' feedback may be different due to their different backgrounds and different understandings about the same information need. For example, users majoring in medicine are very likely to give more accurate feedback than the average user, which will result in better performance on the OHSUMED dataset. However, this does not mean faceted feedback is only useful for smart or expert users. Table 1 shows that three users' feedback are all useful in improving retrieval performances.",null,null
116,Table 3: Performance comparison of different retrieval models on OHSUMED dataset. The feedback from User1 is used.,null,null
117,Retrieval model BM25 (baseline) Boolean model (AND) Boolean model (OR) Soft model,null,null
118,MAP 0.0921 0.0403 0.1120 0.1354,null,null
119,P@10 0.1397 0.1522 0.1758 0.2286,null,null
120,R@1000 0.4612 0.0935 0.4650 0.5301,null,null
121,Dataset Performance BM25 (baseline) FF (User1|4) FF (User2|5) FF (User3|6) FF (Average) Imprv over BM25,null,null
122,OHSUMED MAP P@10 0.0921 0.1397 0.1354 0.2286 0.1112 0.1873 0.1189 0.1873 0.1219 0.2010 32.4% 43.9%,null,null
123,RCV1 MAP P@10 0.2907 0.5680 0.3221 0.6180 0.3150 0.6120 0.3318 0.6240 0.3230 0.6180 11.1% 8.8%,null,null
124,5.2 User disagreement on faceted feedback,null,null
125,"Given the same query and the same group of facet-value pair candidates, users may select different facet-value pairs, which lead to different retrieval performances. In Table 1, the performance using feedback from User1 and User6 (in bold) are better than other users. Table 2 gives two query examples for which users' faceted feedback are different from each other. Further analysis shows that there are very few queries that users gave exactly the same feedback. This is common in IR evaluation, as well trained TREC assessors",null,null
126,3The Boolean model will be discussed in a later section,null,null
127,"Table 4: Performance comparison of different retrieval models on RCV1 dataset. The feedback from User6 is used. ""Boolean model (A+O)"" means to use AND operation across facets and OR operation within each facet.",null,null
128,Retrieval model BM25 (baseline) Boolean model (AND) Boolean model (A+O) Boolean model (OR) Soft model,null,null
129,MAP 0.2907 0.1046 0.2208 0.2912 0.3318,null,null
130,P@10 0.5680 0.3311 0.5102 0.5780 0.6240,null,null
131,R@1000 0.6658 0.1514 0.5062 0.6563 0.6954,null,null
132,5.3 Boolean model v.s. Soft model,null,null
133,"Table 3 and 4 compare the performances of the Boolean models and the soft model. R@1000 is the recall of top 1000 documents. In Table 4, ""Boolean (A+O)"" means to use AND across facets and OR within each facet (Table 3 doesn't have this since only one facet is used on the OHSUMED dataset).",null,null
134,367,null,null
135,Table 2: Examples of user-selected facet-value pairs,null,null
136,"Query ""58 yo with cancer and hypercalcemia""",null,null
137,Aborigine health,null,null
138,User1 MeSH:Hypercalcemia MeSH:Diphosphonates MeSH:Calcium,null,null
139,Industry:Hospitals & Healthcare Topic:Health,null,null
140,"User2 MeSH:Hypercalcemia MeSH:Carcinoma, Squamous Cell MeSH:Parathyroid Hormones",null,null
141,"Region:Australia Topic:Health Topic:Welfare, Social Services",null,null
142,User3 MeSH:Hypercalcemia MeSH:Paraneoplastic Syndromes MeSH:Bone Neoplasms MeSH:Bone Resorption Region:Australia Topic:Health Topic:Government/Social,null,null
143,"The Boolean model with AND operation works poorly on both datasets. It results in much lower Recall@1000 than other retrieval models. The Boolean OR operation works better than the baseline method on OHSUMED dataset and a little worse on RCV1 dataset. The Boolean A+O works better than Boolean AND while still worse than Boolean OR. This reveals that when we loosen the Boolean restriction, we are actually getting improved retrieval performances. In contrast to the general practice of using Boolean approach in faceted search, the Boolean model in our experiments doesn't work well for text document retrieval. We did some further analysis and figured out two reasons for that. First, document metadata assignments are not perfect. Many documents are not assigned with metadata that they should have (we call this case incompleteness of metadata assignment). Secondly, some users select ambiguous or inappropriate facet-value pairs, probably because they are not familiar with the current topic. When using the Boolean model, many potentially relevant documents are filtered out due to either incompleteness of metadata assignment or users' inappropriate feedback, and thus the system recall is hurt seriously.",null,null
144,"Soft model works well since it uses user feedback as preferences instead of rigid requirements. We proposed to use the parameter f to capture the quality of a facet previously. The motivation is that the values of some facets are easy to determine by either human beings or algorithms, while for some other facets this might be hard. For example, the facet ""Region"" might be easier for human, while ""topic"" might be harder. Someone may think a news article talking about ""resident health"" should be categorized into the topic ""Government/Social"" while some others may not think so. We found that the feedback on the ""Topic"" facet are different across three users, and feedback on the ""Region"" facet are more consistent across the users. Besides, for easier facets, the metadata assignment tends to be accurate and complete, and thus trustable. While for harder facets, the metadata assignment might be inaccurate and incomplete, and thus less trustable. These observations further justified our motivation for introducing parameter f .",null,null
145,"The proposed soft model requires training data to learn f for each facet, and we use the 3-fold cross validation in our experiment. The queries are randomly split into three equal-size sets. In each fold, two sets are used as training queries to learn the parameter (f ), and the last set is used for testing. The average performances over three folds are reported in Table 3 and 4. Table 5 shows the  values learnt",null,null
146,Table 5: The optimal f trained in each fold,null,null
147,Dataset,null,null
148,Fold,null,null
149,Facet Optimal ,null,null
150,Fold1,null,null
151,MeSH,null,null
152,8.5,null,null
153,OHSUMED Fold2,null,null
154,MeSH,null,null
155,7.5,null,null
156,Fold3,null,null
157,MeSH,null,null
158,9.5,null,null
159,Region,null,null
160,10,null,null
161,Fold1,null,null
162,Topic,null,null
163,2,null,null
164,Industry,null,null
165,1.5,null,null
166,Region,null,null
167,10,null,null
168,RCV1,null,null
169,Fold2,null,null
170,Topic,null,null
171,1.5,null,null
172,Industry,null,null
173,0.5,null,null
174,Region,null,null
175,2.5,null,null
176,Fold3,null,null
177,Topic,null,null
178,2,null,null
179,Industry,null,null
180,2,null,null
181,"in each fold4. On RCV1 dataset, Region are consistently larger than T opic and Industry, which suggests that the ""Region"" facet is more trustable or easier for users than the other two facets.",null,null
182,"It is worth mentioning that if a software such as those proposed by Herst et al.[11] is used to automatically generate facet values, we may prefer completeness/recall of metadata assignment instead of precision. Because metadata incompleteness hurts the system recall badly, while inappropriate facet value assignment hurts less, as the final ranking algorithm would rank a document low if it is non-relevant to the user information need.",null,null
183,5.4 Comparison of different facet-value pair recommendation approaches,null,null
184,"Figure 2 compares the retrieval performances corresponding to four facet-value pair recommendation methods: TDF (Top N Document Frequency), TDFIDF (Combine TDF with IDF), QL (Query Likelihood), and TDFQL (Combine QL with TDF). The horizontal axis shows the number of facet-value pair candidates used, and the vertical axis shows the corresponding MAP. The performances shown in the figures are the average across three users. TDFIDF and TDFQL methods perform better than the other two methods on OHSUMED dataset. This is consistent with our expectation, since both methods combine two features of facet-value pairs.",null,null
185,"Interestingly, on RCV1 dataset, TDFIDF and TDFQL perform worse than TDF. Further analysis shows that the",null,null
186,"4The smallest scale we tried for f is 0.5, since smaller scales have no significant influences on retrieval performances.",null,null
187,368,null,null
188,"facet-value pair ""Region:USA"" benefits retrieval performance a lot for several queries. Unfortunately, both TDFIDF and TDFQL rank it out of the top 10 since it appears frequently in the whole corpus, so users have no chance to see this candidate. One possible solution is to use facet weights (f ) as an extra feature in our facet-value pair recommendation methods. The motivation is that those facets more trustable for retrieval (with bigger f , such as the facet ""Region"" over the other two facets) should be boosted in recommendation. This is consistent with the second criterion of good facetvalue pairs we mentioned in section 3.1. We will evaluate this idea in our future work.",null,null
189,5.5 Comparison with other types of feedback,null,null
190,"The retrieval performances of faceted feedback (FF), Pseudo Relevance Feedback (PRF) and Real document-based Relevance Feedback (RRF) are compared and shown in Table 6. The performances of combining PRF with FF (PRF@5+FF) and combing RRF with FF(RRF@5+FF) are also reported. The performance of FF is the average over three users. The top 5 ranked documents in BM25 are used for pseudo relevance feedback (PRF@5). The relevance judgments of top 5 are used in the BM25 relevance feedback algorithm as implemented in Lemur [2] (RRF@5). Table 6 shows that FF performs better than PRF, and closely to RRF on OHSUMED dataset; FF performs worse than PRF and RRF on RCV1 dataset, and 10% better than BM25.",null,null
191,"Though FF might perform worse than RRF, FF is still very promising because of three major reasons. First, a retrieval system may get more faceted feedback than document feedback, as faceted search is commonly accepted by the average Internet user and faceted feedback seems very easy for a user to understand. Second, RRF and PRF often help little for hard queries when no relevant documents are retrieved in the top positions, while faceted feedback might help boost relevant documents in these cases. A major scenario where search engine fails is that an engine only focuses on one aspect of a query and ignores some other important aspects [7]. Faceted feedback provides the mechanism for users to put constraints on the important aspects to avoid this problem. Actually, in our experiment, we find there are a number of queries for which FF helps a lot when PRF and RRF help little or even hurt. These queries are mostly hard queries with poor initial retrieval performances. Take the query ""Nuclear plants U.S."" as an example, almost all the initially returned top documents in baseline ranking are about nuclear plants outside U.S., thus PRF and RRF hurt. FF helps since all users are able to identify the faceted restriction ""Region:USA"", which boosts those documents talking about events happening in U.S.. Third, different types of feedback are not exclusive and they could complement each other. We can easily combine FF with PRF or RRF and obtain better retrieval performances. This can be done by using PRF or RRF as the baseline method to calculate the original document scores (sm(d) in equation 6 and 7). Table 6 shows that the combininations of FF with PRF or RRF improve the performances further.",null,null
192,6. CONCLUSIONS,null,null
193,"We researched the user feedback mechanism based on faceted document metadata. The results on a medical dataset and a news dataset show that faceted feedback is useful, though different users may give different feedback for the same query.",null,null
194,Table 6: Performance comparison of different types of feedback. FF: faceted feedback; PRF@5: pseudo relevance feedback using top 5 docs; RRF@5: real document-based relevance feedback using top 5 docs.,null,null
195,Dataset Performance BM25 (baseline) FF PRF@5 RRF@5 PRF@5+FF RRF@5+FF,null,null
196,OHSUMED MAP P@10 0.0921 0.1397 0.1219 0.2010 0.1096 0.1746 0.1240 0.2048 0.1269 0.1937 0.1473 0.2481,null,null
197,RCV1 MAP P@10 0.2907 0.5680 0.3230 0.6180 0.3711 0.6280 0.3887 0.6940 0.3899 0.6320 0.4025 0.7007,null,null
198,"Directly using the Boolean model, which is commonly used in e-commerce, is inappropriate for metadata-based general purpose document retrieval, since the document metadata assignment is usually incomplete. The proposed soft model is shown consistently more effective on both datasets, as it automatically learns a weight for each facet, which captures the facet quality. The proposed facet-value pair recommendation methods are generally effective and can be improved in the future. Faceted feedback could be combined with pseudo relevance feedback and document relevance feedback. We tried one simple combining method and found better retrieval performance.",null,null
199,"In the future, more research is needed to explore different facet-value pair recommendation algorithms, for example, incorporating facet weights (f ), considering the interaction among facet-value pairs and how user choices are affected by context. We also want to explore different ways to combine various feedback mechanisms.",null,null
200,7. ACKNOWLEDGMENTS,null,null
201,"We thank Jessica Gronski and Yize Li for valuable discussions related to this research. The work was funded by National Science Foundation IIS-0713111, AFRL/AFOSR and UCSC/LANL Institute for Scalable Scientific Data Management. Any opinions, findings, conclusions or recommendations expressed in this paper are the authors', and do not necessarily reflect those of the sponsors.",null,null
202,8. REFERENCES,null,null
203,[1] Amazon mechanical turk. https://www.mturk.com. [2] The lemur toolkit for language modeling and,null,null
204,information retrieval. http://www.lemurproject.org/. [3] O. Alonso and S. Mizzaro. Can we get rid of trec,null,null
205,"assessors? using mechanical turk for relevance assessment. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [4] P. Anick. Using terminological feedback for web search refinement: a log-based study. In SIGIR '03: Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 88­95, New York, NY, USA, 2003. ACM. [5] P. Anick and S. Tipirneni. Interactive document retrieval using faceted terminological feedback. Hawaii",null,null
206,369,null,null
207,Figure 2: Performances of different facet-value pair recommendation approaches. The left figure: on OHSUMED dataset; the right figure: on RCV1 dataset,null,null
208,"International Conference on System Sciences, 2:2036, 1999.",null,null
209,"[6] A. Berger and J. Lafferty. Information retrieval as statistical translation. In SIGIR, 1998.",null,null
210,"[7] C. Buckley. Why current ir engines fail. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 584­585, New York, NY, USA, 2004. ACM.",null,null
211,"[8] B. Croft, D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practise, chapter 11 Beyond Bag of Words, page 463. Pearson, 2009.",null,null
212,"[9] J. C. French, A. L. Powell, F. Gey, and N. Perelman. Exploiting a controlled vocabulary to improve collection selection and retrieval effectiveness. In CIKM '01: Proceedings of the tenth international conference on Information and knowledge management, pages 199­206, New York, NY, USA, 2001. ACM.",null,null
213,"[10] D. Harman. Towards interactive query expansion. In SIGIR '88: Proceedings of the 11th annual international ACM SIGIR conference on Research and development in information retrieval, pages 321­331, New York, NY, USA, 1988. ACM.",null,null
214,"[11] M. A. Hearst and E. Stoica. Nlp support for faceted navigation in scholarly collection. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 62­70, Suntec City, Singapore, August 2009. Association for Computational Linguistics.",null,null
215,"[12] D. Kelly, V. D. Dollu, and X. Fu. The loquacious user: a document-independent source of terms for query expansion. In SIGIR '05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 457­464, New York, NY, USA, 2005. ACM.",null,null
216,"[13] D. Kelly and X. Fu. Elicitation of term relevance feedback: an investigation of term source and context. In SIGIR '06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 453­460, New York, NY, USA, 2006. ACM.",null,null
217,"[14] D. D. Lewis, Y. Yang, T. Rose, and F. Li. Rcv1: A",null,null
218,new benchmark collection for text categorization research. 2004.,null,null
219,"[15] H. Raghavan, O. Madani, and R. Jones. Active learning with feedback on features and instances. J. Mach. Learn. Res., 7:1655­1686, 2006.",null,null
220,"[16] S. Robertson and D. Hull. The TREC-9 filtering track report. In The Ninth Text REtrieval Conference (TREC-9), pages 25­40. National Institute of Standards and Technology, special publication 500-249, 2001.",null,null
221,"[17] S. Robertson and I. Soboroff. The TREC-10 filtering track final report. In Proceeding of the Tenth Text REtrieval Conference (TREC-10), pages 26­37. National Institute of Standards and Technology, special publication 500-250, 2002.",null,null
222,[18] J. J. Rocchio. Relevance feedback in information retrieval. 1971.,null,null
223,"[19] X. Shen and C. Zhai. Active feedback in ad hoc information retrieval. In SIGIR '05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, pages 59­66, New York, NY, USA, 2005. ACM.",null,null
224,"[20] L. Si and R. Jin. Flexible mixture model for collaborative filtering. In ICML '03: Proceedings of the Twentieth International Conference on Machine Learning, 2003.",null,null
225,"[21] S. J. M. H.-B. Stephen E. Robertson, Steve Walker and M. Gatford. Okapi at trec-3. 1994.",null,null
226,"[22] B. Tan, A. Velivelli, H. Fang, and C. Zhai. Term feedback for information retrieval with language models. In SIGIR '07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 263­270, New York, NY, USA, 2007. ACM.",null,null
227,"[23] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. pages 403­410, 2001.",null,null
228,"[24] Y. Zhang. Using bayesian priors to combine classifiers for adaptive filtering. In SIGIR '04: Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 345­352, New York, NY, USA, 2004. ACM.",null,null
229,370,null,null
230,,null,null

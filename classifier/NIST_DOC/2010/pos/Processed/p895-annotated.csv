,sentence,label,data
,,,
0,A Co-learning Framework for Learning User Search Intents from Rule-Generated Training Data,null,null
,,,
1,Jun Yan1 Zeyu Zheng1,null,null
,,,
2,"1Microsoft Research Asia Sigma Center, No.49, Zhichun Road Beijing, 100190, China",null,null
,,,
3,"{junyan, v-zeyu, zhengc}@microsoft.com",null,null
,,,
4,Li Jiang2 Yan Li2 Shuicheng Yan3 Zheng Chen1,null,null
,,,
5,2Microsoft Corporation,null,null
,,,
6,3Department of Electrical and,null,null
,,,
7,One Microsoft Way,null,null
,,,
8,Computer Engineering,null,null
,,,
9,"Redmond, WA 98004",null,null
,,,
10,National University of Singapore,null,null
,,,
11,"{lij, roli}@microsoft.com",null,null
,,,
12,"117576, Singapore",null,null
,,,
13,eleyans@nus.edu.sg,null,null
,,,
14,ABSTRACT,null,null
,,,
15,"Learning to understand user search intents from their online behaviors is crucial for both Web search and online advertising. However, it is a challenging task to collect and label a sufficient amount of high quality training data for various user intents such as ""compare products"", ""plan a travel"", etc. Motivated by this bottleneck, we start with some user common sense, i.e. a set of rules, to generate training data for learning to predict user intents. The rule-generated training data are however hard to be used since these data are generally imperfect due to the serious data bias and possible data noises. In this paper, we introduce a Co-learning Framework (CLF) to tackle the problem of learning from biased and noisy rule-generated training data. CLF firstly generates multiple sets of possibly biased and noisy training data using different rules, and then trains the individual user search intent classifiers over different training datasets independently. The intermediate classifiers are then used to categorize the training data themselves as well as the unlabeled data. The confidently classified data by one classifier are added to other training datasets and the incorrectly classified ones are instead filtered out from the training datasets. The algorithmic performance of this iterative learning procedure is theoretically guaranteed.",null,null
,,,
16,Categories and Subject Descriptors,null,null
,,,
17,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ search process.,null,null
,,,
18,"Though various popular machine learning techniques could be applied to learn the underlying search intents of the users, it is generally laborious or even impossible to collect sufficient and label high quality training data for such learning task [1]. Despite of the laborious human labeling efforts, many intuitive insights, which could be formulated as rules, can help generate small scale possibly biased and noisy training data. For example, to identify whether the users have intents to compare different products, several assumptions may help make the judgment. Generally, we may assume that if a user submits a query with explicit intent expression, such as ""Canon 5d compare with Nikon D300"", he/she may want to compare products. Though the rules satisfy the human common sense, there are two major limitations if we directly use them to infer ground truth. First, the coverage of each rule is often small and thus the training data may be seriously biased and insufficient. Second, the training data are usually not clean since no matter which rule we use, there may exist exceptions. In this paper, we propose a co-learning framework (CLF) for learning user intent from the rule-generated training data, which are possibly biased and noisy. The problem is,",null,null
,,,
19,"Without laborious human labeling work, is it possible to train",null,null
,,,
20,"user search intent classifier using the rule-generated training data,",null,null
,,,
21,which are generally noisy and biased? Given sets of rule-,null,null
,,,
22,"generated training datasets , 1,2, ... , how to train the",null,null
,,,
23,classifier :,null,null
,,,
24,on top of these biased and noisy training data,null,null
,,,
25,sets with good performance?,null,null
,,,
26,General Terms,null,null
,,,
27,"Algorithms, Experimentation",null,null
,,,
28,Keywords,null,null
,,,
29,"User intent, search engine, classification.",null,null
,,,
30,1. INTRODUCTION,null,null
,,,
31,"The classical relevance based search strategies may fail in satisfying the end users due to the lack of consideration on the real search intents of users. For example, when different users search with the same query ""Canon 5D"" under different contexts, they may have distinct intents such as to buy Canon 5D, to repair Canon 5D, etc. The search results about Canon 5D repairing obviously cannot satisfy the users who want to buy a Canon 5D camera. Learning to understand the true user intents behind the users' search queries is becoming a crucial problem for both Web search and behavior targeted online advertising.",null,null
,,,
32,"Copyright is held by the author/owner(s). SIGIR'10, July 19-23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
,,,
33,2. THE CO-LEARNING FRAMEWORK,null,null
,,,
34,"Suppose we have sets of rule-generated training data ,",null,null
,,,
35,"1,2, ... , which are possibly noisy and biased, and a set of",null,null
,,,
36,unlabeled user behavioral data . Each data sample in the,null,null
,,,
37,"training datasets is represented by a triple , ,",null,null
,,,
38,"1,",null,null
,,,
39,"1,2, ... | |, where stands for the feature vector of the data",null,null
,,,
40,"sample in the training data , is its class label and | | is the",null,null
,,,
41,"total number of training data in . On the other hand, each",null,null
,,,
42,"unlabeled data sample, i.e. the user search session that could not",null,null
,,,
43,"be covered by our rules, is represented as , ,",null,null
,,,
44,"0,",null,null
,,,
45,"1,2, ... | |. Suppose for any",null,null
,,,
46,", all the features constituting",null,null
,,,
47,the feature space are represented as a set,null,null
,,,
48,"| 1,2, ... .",null,null
,,,
49,"Suppose among all the features F, some have direct correlation to",null,null
,,,
50,"the rules, that is they are used to generate the training dataset .",null,null
,,,
51,These features are denoted by,null,null
,,,
52,", which constitute a subset",null,null
,,,
53,"of F. Let ,",null,null
,,,
54,be the subset of features having no direct,null,null
,,,
55,correlation to the rules used for generating training dataset .,null,null
,,,
56,Given a classifier :,null,null
,,,
57,", where",null,null
,,,
58,"is any subset of F, we",null,null
,,,
59,use to represent an untrained classifier and use to represent,null,null
,,,
60,the classifier trained by the training data . Suppose,null,null
,,,
61,|,null,null
,,,
62,895,null,null
,,,
63,means to train the classifier by training dataset using the,null,null
,,,
64,features,null,null
,,,
65,", we have",null,null
,,,
66,"trained classifier , let",null,null
,,,
67,"| , 1,2, ... . For the | stands for classifying",null,null
,,,
68,using features F. We assume for each output result of trained,null,null
,,,
69,"classifier , it can output a confidence score. Let",null,null
,,,
70,|,null,null
,,,
71,",",null,null
,,,
72,where is the class label of assigned by and the is the corresponding confidence score.,null,null
,,,
73,"After generating a set of training data , 1,2, ... , based on rules, we first train the classifier by , 1,2, ... ,",null,null
,,,
74,"independently. Then we can get a set of K classifiers,",null,null
,,,
75,"| , 1,2, ... .",null,null
,,,
76,Note that the reason why we use to train classifier on top of,null,null
,,,
77,instead of using the full set of features F is that is generated,null,null
,,,
78,"from some rules correlated to , which may overfit the classifier",null,null
,,,
79,if we do not exclude them. After each classifier is trained,null,null
,,,
80,"by , we use to classify the training dataset itself. A basic assumption of CLF is that the confidently classified instances by",null,null
,,,
81,"classifier , 1,2, ... , have high probability to be correctly",null,null
,,,
82,"classified. Based on this assumption, for any",null,null
,,,
83,", if the",null,null
,,,
84,"confidence score of the classification is larger than a threshold, i.e. > and the class label assigned by the classifier is different",null,null
,,,
85,"from the class label assigned by the rule, i.e.",null,null
,,,
86,", then",null,null
,,,
87,is considered as noise in the training data . Note that here,null,null
,,,
88,"is the label of assigned by classifier, is its observed",null,null
,,,
89,"class label in training data, and is the true class label, which is",null,null
,,,
90,not observed. We exclude it from and put it into the unlabeled dataset . Thus we update the training data by,null,null
,,,
91,",",null,null
,,,
92,.,null,null
,,,
93,"Then we use the classifier ,",null,null
,,,
94,"1,2, ... , to classify the",null,null
,,,
95,unlabeled data independently. Based on the same assumption,null,null
,,,
96,"that the confidently classified instances by classifier have high probability to be correctly classified, for any data belonging to ,",null,null
,,,
97,if the confidence score of the classification is larger than a,null,null
,,,
98,"threshold, i.e. > , where",null,null
,,,
99,|,null,null
,,,
100,", we",null,null
,,,
101,"include into the training dataset. In other words,",null,null
,,,
102,",",null,null
,,,
103,", 1,2 ... , .",null,null
,,,
104,"Through this way, we can gradually reduce the bias of the rulegenerated training data.",null,null
,,,
105,"On the other hand, some unlabeled data are added into the training",null,null
,,,
106,"datasets. Suppose the ,",null,null
,,,
107,1| is the probability of a,null,null
,,,
108,data sample to be involved in the training data at the iteration n,null,null
,,,
109,conditioned on this data sample is represented as a feature vector,null,null
,,,
110,and,null,null
,,,
111,1 is the probability of any data sample in D is,null,null
,,,
112,considered as a training data sample. It can be proved that after n,null,null
,,,
113,"iterations using CLF, for each training dataset, we have",null,null
,,,
114,",",null,null
,,,
115,1|,null,null
,,,
116,1,null,null
,,,
117,The remaining questions are when to stop the iteration and how to,null,null
,,,
118,"train the classifier after iteration stops. In this work, we define the",null,null
,,,
119,"iteration stopping criteria as ""if |{ |",null,null
,,,
120,",",null,null
,,,
121,}| < n,null,null
,,,
122,"or the number of iterations reaches N, then we stop the iteration"".",null,null
,,,
123,"After the iterations stop, we obtain K updated training datasets",null,null
,,,
124,"with both noise and bias reduced. Finally, we merge all these",null,null
,,,
125,training datasets into one. Thus we can train the final classifier as,null,null
,,,
126,.,null,null
,,,
127,3. EXPERIMENTS,null,null
,,,
128,"In this short paper, we utilize the real user search behavioral dataset, which comes from the search click-through log of a commonly used commercial search engine. It contains 3,420 user search sessions, in each of which, the user queries and clicked Web pages are all logged. Six labelers are asked to label the user intents according to the user behaviors as ground truth for results validation. We name this dataset as the ""Real User Behavioral Data"". The n-gram features are used for classification in the Bag of Words (BOW) model. One of the most classical evaluation metrics for classification problems, F1, which is a tradeoff between Precision (Pre) and recall (Rec) is used as the evaluation metric. For comparison purpose, we utilize several baselines to show the effectiveness of the proposed CLF. Firstly, since we can use different rules to initialize several sets of training data, directly utilizing one training dataset or the combination of all rule-generated training datasets to train the same classification model can give us a set of classifiers. Among them, we take the classifier with the best performance as the first baseline, referred to as ""Baseline"" in the remaining parts of this section. The second baseline is the DL-CoTrain algorithm, which is a variant of cotraining algorithm. It also starts from the rule-generated training data for classification and thus has the same experiments configuration as CLF. The classification method selected in CLF is the classical Support Vector Machine (SVM). In Table 3, we show the experimental results of CLF after 25 rounds of iterations compared with the baseline algorithms. From the results we can see that, in terms of F1, the CLF can improve the classification performance as high as 47% compared with the baseline.",Y,null
,,,
129,Table 3. Results of CLF after 25 iterations,null,null
,,,
130,Baseline,null,null
,,,
131,DL-CoTrain,null,null
,,,
132,Pre,null,null
,,,
133,0.78,null,null
,,,
134,0.78,null,null
,,,
135,Rec,null,null
,,,
136,0.24,null,null
,,,
137,0.12,null,null
,,,
138,F1,null,null
,,,
139,0.36,null,null
,,,
140,0.21,null,null
,,,
141,CLF 0.81 0.39 0.53,null,null
,,,
142,4. CONCLUSION,null,null
,,,
143,"One bottleneck of user search intent learning for Web search and online advertising is the laborious training data collection. In this paper, we proposed a co-learning framework (CLF), which aims to classify the users' search intents without laborious human labeling efforts. We firstly utilize a set of rules coming from the common sense of humans to automatically generate some initial training datasets. Since the rule-generated training data are generally noisy and biased, we propose to iteratively reduce the bias of the training data and control the noises in the training data. Experimental results on both real user search click data and public dataset show the good performance of the co-learning framework.",null,null
,,,
144,5. REFERENCE,null,null
,,,
145,"[1] Russell, D.M., Tang, D., Kellar, M. and Jeffries, R. 2009. Task behaviors during web search: the difficulty of assigning labels. Proceedings of the 42nd Hawaii International Conference on System Sciences (Hawaii, United States, January 05 - 08, 2009). HICSS '09. IEEE Press, 1-5. DOI, 10.1109/HICSS.2009.417.",null,null
,,,
146,896,null,null
,,,
147,,null,null

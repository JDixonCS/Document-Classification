,sentence,label,data
0,Retrieval System Evaluation: Automatic Evaluation versus Incomplete Judgments,null,null
1,Claudia Hauff,null,null
2,"University of Twente Enschede, The Netherlands",null,null
3,c.hauff@ewi.utwente.nl,null,null
4,Franciska de Jong,null,null
5,"University of Twente Enschede, The Netherlands",null,null
6,f.m.g.dejong@ewi.utwente.nl,null,null
7,ABSTRACT,null,null
8,"In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced (i.e. incomplete) amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The proposed methods in both areas are commonly evaluated by comparing their performance estimates for a set of systems to a ground truth (provided for instance by evaluating the set of systems according to mean average precision). In contrast, in this poster we compare an automatic system evaluation approach directly to two evaluations based on incomplete manual relevance assessments. For the particular case of TREC's Million Query track, we show that the automatic evaluation leads to results which are highly correlated to those achieved by approaches relying on incomplete manual judgments.",null,null
9,"Categories and Subject Descriptors: H.3.3 Information Storage and Retrieval: Information Search and Retrieval General Terms: Experimentation, Performance Keywords: Automatic System Evaluation",null,null
10,1. INTRODUCTION,null,null
11,"In information retrieval (IR), research aiming to reduce the cost of retrieval system evaluations has been conducted along two lines: (i) the evaluation of IR systems with reduced amounts of manual relevance assessments, and (ii) the fully automatic evaluation of IR systems, thus foregoing the need for manual assessments altogether. The two most important approaches in the first category are the determination of good documents to assess (the MTC approach) [6] and the proposal of alternative pooling methods (the statAP approach) [4]. Both, MTC and statAP, are now accepted system evaluation metrics at TREC1. They stand in contrast to the depth pooling methodology which has until recently been employed at TREC; due to the ever increasing size of test collections and query sets though, pooling the top 100 documents of each retrieval run participating in a benchmark and assessing those documents manually for their relevance, has become infeasible. The earliest method for a fully automatic evaluation was proposed by Soboroff et al.",null,null
12,1http://trec.nist.gov/,null,null
13,"Copyright is held by the author/owner(s). SIGIR'10, July 19­23, 2010, Geneva, Switzerland. ACM 978-1-60558-896-4/10/07.",null,null
14,(the RS approach) [7]. It relies on drawing random samples from the pool of top retrieved documents.,null,null
15,"The quality of statAP, MTC and RS is usually evaluated by comparing the performances of a set of retrieval runs for which sufficient relevance judgments are available according to a standard effectiveness metric (mean average precision) with the estimated system performances. Generally missing though is a direct comparison between statAP /MTC and an automatic method such as RS.",null,null
16,"In recent work [3], we found the commonly reported problem of automatic evaluation approaches (the severe misranking of the very best retrieval runs [5]) not to be inherent to automatic system evaluation methods. The extent of this problem is strongly related to the degree of human intervention in the best retrieval runs: the larger the amount of human intervention, the less able automatic approaches are to identify the best runs correctly.",null,null
17,"In this poster, we turn to investigating how closely the automatic evaluation of retrieval runs approximates the evaluation with incomplete manual relevance assessments. We perform this analysis in a setting which favors automatic evaluation: TREC's Million Query tracks of 2007 (MQ2007) [2] and 2008 (MQ-2008) [1]. Due to the size of the query sets, creating retrieval runs with a great amount of human intervention is virtually impossible. We thus expect the RS approach to lead to similar estimates of system performances as statAP and MTC respectively. If this would indeed be the case, it would bring into question the need for manual assessments in this type of setting.",null,null
18,2. EXPERIMENTS,null,null
19,"For our experiments, we relied on the twenty-nine retrieval runs submitted to MQ-2007 and the twenty-four2 runs submitted to MQ-2008. Both sets of retrieval runs as well as their retrieval effectiveness scores according to statAP and MTC are available from the TREC website. Specifically, for MQ-2007, TREC provides the statAP measures3, while for MQ-2008 both, MTC and statAP, are provided. Of the 10000 queries that were released for each year, 1153 (MQ2007) and 564 (MQ-2008) queries respectively have valid statAP measurements; 784 (MQ-2008) queries have valid MTC measurements. These are the queries we also rely on in the RS approach.",null,null
20,"2In total, twenty-five runs exist, though one is not accessible from the TREC website and thus had to be ignored. 3The MTC measures are not accessible from the TREC website.",null,null
21,863,null,null
22,Pool Depth p Avg. Sampled Documents,null,null
23,MQ-2007 statAP,null,null
24,10,null,null
25,4.6,null,null
26,50,null,null
27,22.2,null,null
28,100,null,null
29,43.0,null,null
30,250,null,null
31,102.6,null,null
32,MQ-2008 statAP,null,null
33,10,null,null
34,6.0,null,null
35,50,null,null
36,28.8,null,null
37,100,null,null
38,55.7,null,null
39,250,null,null
40,132.4,null,null
41,MQ-2008 MTC,null,null
42,10,null,null
43,6.1,null,null
44,50,null,null
45,29.4,null,null
46,100,null,null
47,56.8,null,null
48,250,null,null
49,135.0,null,null
50,Kendall's Tau,null,null
51,0.803 0.783 0.754 0.719,null,null
52,0.768 0.812 0.833 0.841,null,null
53,0.722 0.759 0.773 0.780,null,null
54,Table 1: Kendall's Tau rank correlation coefficient between the automatic RS approach and statAP/MTC respectively. All correlations are significant (p < 0.01). Column 2 contains the average number of sampled documents from the pool.,null,null
55,"For the automatic evaluation, we implemented the random sampling approach [7]: first, the top p retrieved documents of all retrieval runs for a particular query are pooled together such that a document that is retrieved by x runs, appears x times in the pool. Then, a number m of documents are drawn at random from the pool; those are now considered to be the pseudo relevant documents. This process is performed for each query and the subsequent evaluation of each system is performed with pseudo relevance judgments instead of relevance judgments. Due to the randomness of the sampling, we performed 20 trials per query and averaged the pseudo relevance based system performance. We fixed the number m of documents to sample 5% of the number of unique documents in the pool and evaluated pool depths of p ,"" {10, 50, 100, 250}.""",null,null
56,"In Table 1 (column 3) we report the rank correlation coefficient Kendall's Tau ( ) between the performance scores estimated by the automatic RS approach and the performance scores estimated by statAP /MTC which exploit manual relevance assessments. In the ideal case,  ,"" 1.0, that is, RS leads to the same rank estimate of system performances as statAP /MTC. It is apparent, that although the correlations are not perfect, the correlation coefficients are consistently high; in the worst instance the correlation reaches  "", 0.72 for MQ-2007 statAP and a pool depth of p , 250; at best the correlation reaches  , 0.84 for MQ-2008 statAP and p , 250.",null,null
57,Figures 1 and 2 show scatter plots of MQ-2007 statAP scores versus RS scores and of MQ-2008 MTC scores versus RS scores respectively. It is evident that the best retrieval runs as identified by statAP /MTC are also identified correctly by the automatic RS approach.,null,null
58,3. DISCUSSION AND CONCLUSION,null,null
59,"In this poster, we investigated the ability of an automatic system evaluation approach (RS [7]) to approximate the system performance estimates as derived by two evaluation methods that rely on manually derived incomplete relevance judgments: statAP and MTC. Experiments on TREC's Mil-",null,null
60,random sampling,null,null
61,0.18 0.16 0.14 0.12,null,null
62,0.1 0.08 0.06 0.04 0.02,null,null
63,0,null,null
64,",0.803",null,null
65,0.05,null,null
66,0.1,null,null
67,0.15,null,null
68,0.2,null,null
69,0.25,null,null
70,0.3,null,null
71,statAP,null,null
72,"Figure 1: MQ-2007 statAP scores (x-axis) versus RS scores (y-axis) for a pool depth of p , 10.",null,null
73,random sampling,null,null
74,0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01,null,null
75,0 0,null,null
76,",0.780",null,null
77,0.02,null,null
78,0.04,null,null
79,0.06,null,null
80,0.08,null,null
81,0.1,null,null
82,MTC,null,null
83,"Figure 2: MQ-2008 MTC scores (x-axis) versus RS scores (y-axis) for a pool depth of p , 250.",null,null
84,"lion Query tracks showed that RS is highly correlated to statAP and MTC, an outcome which implies that retrieval runs, which are automatic in nature, can be evaluated by an automatic approach such as RS which requires no manual assessments at all.",null,null
85,One direction of future work will be the adaptation of RS to further improve the method's correlation with statAP and MTC by for instance taking advantage of the relationship between queries of a query set (as is possible for larger sets of queries) in contrast to the current approach where each query is viewed in isolation.,null,null
86,4. REFERENCES,null,null
87,"[1] J. Allan, J. A. Aslam, V. Pavlu, E. Kanoulas, and B. Carterette. Million Query Track 2008 Overview. In TREC 2008, 2008.",null,null
88,"[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million Query Track 2007 Overview. In TREC 2007, 2007.",null,null
89,"[3] C. Hauff, D. Hiemstra, L. Azzopardi, and F. de Jong. A Case for Automatic System Evaluation. In ECIR '10, pages 153­165, 2010.",null,null
90,"[4] J. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR '06, pages 541­548, 2006.",null,null
91,"[5] J. A. Aslam and R. Savell. On the effectiveness of evaluating retrieval systems in the absence of relevance judgments. In SIGIR '03, pages 361­362, 2003.",null,null
92,"[6] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR '06, pages 268­275, 2006.",null,null
93,"[7] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In SIGIR '01, pages 66­73, 2001.",null,null
94,864,null,null
95,,null,null

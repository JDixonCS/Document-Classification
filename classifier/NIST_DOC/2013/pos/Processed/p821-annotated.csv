,sentence,label,data
,,,
0,Query Change as Relevance Feedback in Session Search,null,null
,,,
1,"Sicong Zhang, Dongyi Guan, Hui Yang",null,null
,,,
2,Department of Computer Science Georgetown University,null,null
,,,
3,"37th and O Street, NW, Washington, DC 20057 USA",null,null
,,,
4,"{sz303, dg372}@georgetown.edu, huiyang@cs.georgetown.edu",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"Session search retrieves documents for an entire session. During a session, users often change queries to explore and investigate the information needs. In this paper, we propose to use query change as a new form of relevance feedback for better session search. Evaluation conducted over the TREC 2012 Session Track shows that query change is a highly effective form of feedback as compared with existing relevance feedback methods. The proposed method outperforms the state-of-the-art relevance feedback methods for the TREC 2012 Session Track by a significant improvement of >25%.",Y,null
,,,
7,Categories and Subject Descriptors,null,null
,,,
8,H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval,null,null
,,,
9,Keywords,null,null
,,,
10,Relevance Feedback; Session Search; Query Change,null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"Session search retrieves documents for an entire session of queries. [3, 4]. It allows the user to constantly modify queries in order to find relevant documents. Session search involves many interactions between the search engine and the user. The challenge for session search is how to make use of these interactions and the user feedback to effectively improve search accuracy. In TREC (Text REtrieval Conference) 2012 Session tracks [6], the users (NIST assessors) clicked retrieved documents and interacted with a search engine to produce the queries and sessions. For each intermediate query, a retrieved document set containing the top 10 retrieval results ranked in decreasing relevance for the query are kept. The clicked data contains the documents clicked by users, their clicking orders, and dwell time. Figure 1 illustrates the interactions among the user and the search engine.",Y,null
,,,
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
14,Figure 1: Session search. (TREC 2012 session 85),Y,null
,,,
15,"Relevance feedback is a popular IR technique. By expanding queries with terms from relevant feedback documents, relevance feedback is able to generate better queries and uses them for better retrieval accuracy. Commonly used relevant feedback schemes include Rocchio with real user feedback [5], pseudo relevance feedback [1],and implicit relevant feedback [7]. Real user feedback is obtained from human assessors that indicate the relevance of a document retrieved for a query. Pseudo relevance feedback, also known as blind relevance feedback, that assumes that the top retrieved documents are relevant, and makes query expansion based on these pseudo relevant documents. Implicit relevance feedback is the form of feedback that is inferred from user behaviors, such as user clicks, clicking order, and dwell time.",null,null
,,,
16,"In this paper, we propose to use query change as a new form of relevance feedback in session search. Our method utilizes editing changes between two adjacent queries, and the relationship between query change and retrieved documents for the earlier query to enhance session search. Our experiments demonstrate that the proposed approach outperforms other relevance feedback methods.",null,null
,,,
17,2. DEFINING QUERY CHANGE,null,null
,,,
18,"We represent a search session S as a series of queries {q1, ..., qi, ..., qn}. For an individual query qi, we can write it as a combination of the common part and the changes between it and its previous query: qi , (qi  qi-1) + qi.",null,null
,,,
19,We define query change qi as the syntactic editing differ-,null,null
,,,
20,821,null,null
,,,
21,"ences between two adjacent queries qi-1 and qi. Considering the directions of editing, query change qi can be further decomposed into two parts: positive q and negative q. They are written as ""+q"" and ""-q"" respectively. The positive q are new terms that the user adds to the previous query; that is, they appear in qi, but did not appear in qi-1. The negative q are terms that the user deletes from the previous query; that is, they appeared in qi - 1, but not appear in qi.",null,null
,,,
22,We thus decompose an adjacent query pair into:,null,null
,,,
23,#NAME?,null,null
,,,
24,"-qi , qi-1 qi",null,null
,,,
25,"qtheme , qi (+qi) , qi-1 (-qi)",null,null
,,,
26,"where +qi and -qi represent added terms and removed terms respectively, qtheme is the theme terms, and the notation of represents set-theoretic difference. Table 1 demonstrates a few example TREC 2012 Session queries and their query changes.",Y,null
,,,
27,"The theme terms (qtheme) appear in both qi-1 and qi. Generally it implies a strong preference for those terms from the user. For example, in Table 1 session 32, q1 ,"" """"bollywood legislation"""", q2 "","" """"bollywood law"""". qtheme "","" """"bollywood"""".""",null,null
,,,
28,"The added terms (+q) may indicate a specification or drifting between qi-1 and qi. In session 32, (+q2) ,"" """"law"""".""",null,null
,,,
29,"The removed terms (-q) may indicate a generalization or a drifting. In session 32, (-q2) ,"" """"legislation"""".""",null,null
,,,
30,3. UTILIZING QUERY CHANGE,null,null
,,,
31,"Besides queries, a TREC session also contains retrieved document sets D (set of Di) for each query qi, and clicked information C (set of Ci) for each query qi.",Y,null
,,,
32,"Based on observation of session search and user intension, we propose an important assumption that the previous search result Di-1 influences the current query change qi:",null,null
,,,
33,qi  Di-1.,null,null
,,,
34,"In fact, this influence can be in quite a complex way. Figure 1 shows session 85 as an example, illustrating how the previous retrieved documents Di-1 influence the query changes.",null,null
,,,
35,"Based on our definition of query change, we utilize different cases of query change in the calculation of relevance score between the current query qi and a document d.",null,null
,,,
36,"Suppose P (t|d) is the original term weight for the retrieve model in our utilization, we increase and decrease term weights on top of it. In the following formulas, P (t|d) is calculated by the multinomial query generation language model with Dirichlet smoothing [9] while P (t|d) is calculated based on Maximum-Likelihood Estimation (MLE):",null,null
,,,
37,"T F (t, d) + µP (t|C)",null,null
,,,
38,"P (t|d) ,",null,null
,,,
39,",",null,null
,,,
40,Length(d) + µ,null,null
,,,
41,"where d is the document under evaluation, Length(d) is the length of the document, T F (t, d) is the term frequency of t in document d, P (t|C) calculates the probability that t appears in corpus C based on MLE. µ is set to 5000 in experiments.",null,null
,,,
42,We adjust the term weights for the three types of query changes as the following:,null,null
,,,
43,· Theme terms are the repeated common parts nearly appearing in the entire session. It implies their importance,null,null
,,,
44,Table 1: Examples of TREC 2012 Session queries.,null,null
,,,
45,Queries,null,null
,,,
46,Query Change,null,null
,,,
47,Session 32 Session 85,null,null
,,,
48,"query 1 , bollywood legislation query 2 , bollywood law query 1 , glass blowing query 2 , glass blowing science query 3 , scientific glass blowing",null,null
,,,
49,#NAME?,null,null
,,,
50,"+q2 , science -q2 ,  +q3 , scientif ic -q3 , science",null,null
,,,
51,"to the session and to the user. We therefore propose to increase their term weights. It is worth noting that theme terms are common terms within a session which show a similar effect of stop words. However, they may not be common terms in the entire corpus. We propose to use a measure that is similar to inverse document frequency (idf ) to capture this characteristic. We employ the negation of the number of occurrences of t in Di-1, 1 - P (t|Di-1). The weight increase for a theme term t  qtheme is formulated as follows:",null,null
,,,
52,"WT heme ,",null,null
,,,
53,[1 - P (t|Di-1)] log P (t|d) (1),null,null
,,,
54,tqtheme,null,null
,,,
55,"· For the added terms that occurred in the previous search result Di-1, which are terms t  +q and t  Di-1, we deduct their term weights. This is because the term appear both in documents for previous query and in the current query, it will bring back repeated information from the previous query to the current query in some degree. In addition, t  +q shows these added terms are not theme terms. Therefore, it has a high probability to deviate from the recent focus of the current query. We thus deduct more weights to reduce redundant information. The weight deduction is proportional to t's term frequency in Di-1.",null,null
,,,
56,"-WAdd,In , -",null,null
,,,
57,P (t|Di-1) log P (t|d),null,null
,,,
58,t+q,null,null
,,,
59,-2,null,null
,,,
60,tDi-1,null,null
,,,
61,"· For the added terms that did not occur in the search result of previous query Di-1, which are terms t  +q and t / Di-1, we increase the term weights because they demonstrate the novel interests of the user for the current query qi. We propose to raise the term weights based on inverse document frequency in order not to increase their weights too much if they are common terms in the corpus.",null,null
,,,
62,"WAdd,Out ,",null,null
,,,
63,idf (t) log P (t|d),null,null
,,,
64,t+q,null,null
,,,
65,-3,null,null
,,,
66,t/Di-1,null,null
,,,
67,"· For the terms that are from the previous query, which are terms t  -q. No matter t  Di-1 or t / Di-1, we should deduct their term weights. The reason is the following. If they appeared in Di-1, it means that the user observed them and disliked them. If they did not appear in Di-1, the user still dislikes the terms since they are not included in qi anyway. Just like terms that in added terms that appeared in previously retrieved documents (t  +q and t  Di-1), we deduct the term weight for the removed terms according to the following formula.",null,null
,,,
68,"-WRemove , -",null,null
,,,
69,P (t|Di-1) log P (t|d) (4),null,null
,,,
70,t-q,null,null
,,,
71,822,null,null
,,,
72,Table 2: Dataset statistics for TREC 2012 Session Track.,Y,null
,,,
73,"#topic ,48 #query/session ,3.03 #query , 297 #session ,98 #session/topic ,2.04 #docs ,""17,861""",null,null
,,,
74,"Table 3: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012. A statistical significant improvement on nDCG@10 over the baseline is indicated with a  at p < 0.05.",Y,null
,,,
75,Lemur PRF RF Di-1 Implicit Click Implicit SAT,null,null
,,,
76,QueryChg CLK,null,null
,,,
77,QueryChg SAT,null,null
,,,
78,nDCG@10 0.2622 0.2718 0.2122 0.2668 0.2655,null,null
,,,
79,0.3306,null,null
,,,
80,0.33,null,null
,,,
81,%chg 0.00% 3.66% -19.07% 1.75% 1.26% 26.09% 25.86%,null,null
,,,
82,MAP 0.1342 0.1309 0.1137 0.1355 0.1335,null,null
,,,
83,0.1533,null,null
,,,
84,0.1535,null,null
,,,
85,%chg 0.00% -2.46% -15.28% 0.97% -0.52%,null,null
,,,
86,14.23%,null,null
,,,
87,14.38%,null,null
,,,
88,"By considering all cases above, the relevance score between the current query qi and a document d can be represented as a linear combination of various term weight adjustments:",null,null
,,,
89,"Score(qi, d) , log P (qi|d)+",null,null
,,,
90,-5,null,null
,,,
91,#NAME?,null,null
,,,
92,"where d is the document under evaluation, log P (qi|d) is the original query-document relevance scoring function in log form, , , , and  are coefficients for each type of query changes. Empirically, we set the coefficients as  ,"" 2.2,  "","" 1.8, "","" 0.07, and  "", 0.4.",null,null
,,,
93,4. EXPERIMENTS,null,null
,,,
94,4.1 Search Accuracy Using the Last Query,null,null
,,,
95,"We evaluate our algorithm on the TREC 2012 Session Track [6]. According to how much prior information is used, the Track is divided into four phases: RL1 (using only the last query), RL2 (using all queries in the session), RL3 (using all session queries and ranked lists of URLs and the corresponding web pages), RL4 (using all session queries, the ranked lists of URLs and the corresponding web pages, the clicked URLs, and the time that the user spent on the corresponding web pages). Table 2 shows the statistics about the TREC 2012 Session Track.",Y,null
,,,
96,"The corpus used in our evaluation is ClueWeb09 CatB.1 CatB consists of 50 million English pages from the Web collected during two months in 2009. We removed documents whose Waterloo's ""GroupX"" spam raining score [2] are less than 70.",null,null
,,,
97,We compare the following algorithms in this paper:,null,null
,,,
98,· Baseline (Lemur without relevance feedback) Using the original Lemur system (language modeling + Dirichlet smoothing) to retrieve for the last query qn.,null,null
,,,
99,· PRF (Pseudo Relevance Feedback). We utilize pseudo relevance feedback algorithm that developed in Lemur. We use the top 20 documents as pseudo relevant documents. The retrieval is for the last query qn.,null,null
,,,
100,"· RF Di-1. Rocchio using the previously retrieved top documents proved by TREC. This method uses qn, qn-1, and Dn-1.",null,null
,,,
101,1http://lemurproject.org/clueweb09/,null,null
,,,
102,"Table 4: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012, after uniform aggregation. A statistical significant improvement on nDCG@10 over the baseline is indicated with a  at p < 0.05.",null,null
,,,
103,Lemur PRF RF Di-1 Implicit Click Implicit SAT QueryChg CLK QueryChg SAT,null,null
,,,
104,nDCG@10 0.3227 0.2986 0.2446 0.2916 0.2889 0.3258 0.3350,null,null
,,,
105,%chg 0.00% -7.46% -24.20% -9.64% -10.47% 0.96% 3.81%,null,null
,,,
106,MAP 0.1558 0.1413 0.1281 0.1449 0.1467 0.1532 0.1534,null,null
,,,
107,%chg 0.00% -9.31% -17.78% -7.00% -5.84% -1.67% -1.54%,null,null
,,,
108,"· Implicit Click. Implicit relevance feedback based on clicked documents of the previous search query qi-1. This method uses qn, qn-1, Dn-1, Cn-1.",null,null
,,,
109,"· Implicit SAT. Implicit relevance feedback based on SAT [8] clicked documents (the documents that the user clicked and stayed on for at least 30 seconds) from the previous query. This method uses qn, qn-1, Dn-1, Cn-1.",null,null
,,,
110,"· QueryChg CLK. (Our algorithm) Relevance feedback using query change based on Eq. 5. This method uses qn, qn-1, Dn-1, Cn-1. Di-1 include the clicked documents and all snippets for the previous query.",null,null
,,,
111,"· QueryChg SAT. (Our algorithm) Relevance feedback using query change based on Eq. 5. This method uses qn, qn-1, Dn-1, Cn-1. Di-1 are SAT clicks and all snippets for the previous query.",null,null
,,,
112,"Table 3 shows the search accuracy for these seven runs. We employ the official TREC Session evaluation metrics, nDCG@10 and mean average precision (MAP), for measuring search accuracy. We can see that the proposed methods (QueryChg CLK, QueryChg SAT) improve the baseline by 26.09% and 25.86% respectively in nDCG@10. The improvements are statistically significant (one sided t-test, p ,""0.05). They also outperforms all other relevance feedback runs. Among other runs, PRF and implicit relevance feedback both improve over the baseline. RF Di-1, however, decreases nDCG@10 by 19.07% than the baseline. This decrease is expected. RF Di-1 makes query expansion based on Di-1, which increases the weights of old terms in Di-1. An ideal relevance feedback model, however, should assign a lower weight to these terms since they are no longer novel or no longer satisfying the current information need.""",null,null
,,,
113,4.2 Search Accuracy Using All Queries,null,null
,,,
114,There are multiple queries in sessions search. Prior re-,null,null
,,,
115,search has demonstrated that using all queries can effec-,null,null
,,,
116,tively improve search accuracy for session search over just,null,null
,,,
117,using the last query [6]. This technique is called query aggre-,null,null
,,,
118,gation. We evaluate our algorithm with query aggregation,null,null
,,,
119,in this section.,null,null
,,,
120,"Let Scoresession(qn, d) denote the overall relevance score",null,null
,,,
121,"for a document d to the entire session, the aggregated ses-",null,null
,,,
122,"sion relevance score can be written as: Scoresession(qn, d) ,",null,null
,,,
123,"n i,1",null,null
,,,
124,i,null,null
,,,
125,"· Score(qi, d),",null,null
,,,
126,where,null,null
,,,
127,n,null,null
,,,
128,is,null,null
,,,
129,the,null,null
,,,
130,number,null,null
,,,
131,of,null,null
,,,
132,queries,null,null
,,,
133,in,null,null
,,,
134,"a session, Score(qi, d) is the relevance score between d and",null,null
,,,
135,"qi, and i is the query weight for qi. In this paper, we em-",null,null
,,,
136,ploy the uniform query aggregation by setting all queries are,null,null
,,,
137,"equally weighted (i , 1) for all systems under evaluation.",null,null
,,,
138,Table 4 shows the search accuracy with uniform aggrega-,null,null
,,,
139,"tion over all queries. Comparing Table 4 with Table 3, we",null,null
,,,
140,823,null,null
,,,
141,Table 5: nDCG@10 for different classes of sessions in TREC 2012 Session Track.,null,null
,,,
142,Lemur PRF RF Di-1 Implicit Click Implicit SAT QueryChg Click QueryChg SAT,null,null
,,,
143,Intellectual 0.2740 0.2814 0.2009 0.2742 0.2749 0.3746 0.3759,null,null
,,,
144,%chg 0.00% 2.70% -26.65% 0.10% 0.34% 36.73% 37.20%,null,null
,,,
145,Specific 0.2529 0.2721 0.1995 0.2508 0.2555 0.3041 0.3062,null,null
,,,
146,%chg 0.00% 7.60% -21.12% -0.83% 1.03% 20.23% 21.08%,null,null
,,,
147,Amorphous 0.2741 0.2713 0.2285 0.2873 0.2783 0.3646 0.3604,null,null
,,,
148,%chg 0.00% -1.02% -16.63% 4.81% 1.55% 33.03% 31.48%,null,null
,,,
149,Factual 0.2557 0.2664 0.2185 0.2627 0.2603 0.3062 0.3045,null,null
,,,
150,%chg 0.00% 4.20% -14.54% 2.74% 1.81% 19.77% 19.09%,null,null
,,,
151,Table 6: nDCG@10 for different classes of sessions in TREC 2012 Session Track. Uniform Aggregation,null,null
,,,
152,Lemur PRF RF Di-1 Implicit Click Implicit SAT QueryChg Click QueryChg SAT,null,null
,,,
153,Intellectual 0.3656 0.3634 0.2703 0.3235 0.3235 0.3575 0.3818,null,null
,,,
154,%chg 0.00% -0.60% -26.08% -11.51% -11.53% -2.22% 4.43%,null,null
,,,
155,Specific 0.2983 0.2654 0.2233 0.2743 0.2767 0.3089 0.3125,null,null
,,,
156,%chg 0.00% -11.05% -25.15% -8.07% -7.24% 3.55% 4.76%,null,null
,,,
157,Amorphous 0.3539 0.3412 0.2719 0.3138 0.3045 0.3474 0.3637,null,null
,,,
158,%chg 0.00% -3.58% -23.18% -11.33% -13.96% -1.84% 2.76%,null,null
,,,
159,Factual 0.2989 0.2626 0.2304 0.2739 0.2697 0.3082 0.3089,null,null
,,,
160,%chg 0.00% -12.12% -22.92% -8.36% -9.75% 3.11% 3.37%,null,null
,,,
161,"observe that all systems improve their search accuracy when using query aggregation. The proposed QueryChg SAT run achieves an nDCG@10 of 0.3350, which is a 3.81% improvement over Lemur after uniform query aggregation, and a 27.76% improvement over Lemur without query aggregation. The Lemur run after query aggregation performs well (nDCG@10,0.32) as compared with without query aggregation (nDCG@10,""0.26 in Table 3). However, the proposed query change runs (QueryChg Click, QueryChg SAT) do not benefit much from query aggregation. This may be because that uniform aggregation equally weights each query, which assumes query independence among the queries in a session; whereas the query change relevance feedback runs assume that previous query and current query are dependent. The difference in the assumptions between query change relevance feedback model and the uniform aggregation may be the reason that the former does not benefit much from the latter. Other aggregation methods may be able to improve the situation.""",null,null
,,,
162,4.3 Results On Different Session Types,null,null
,,,
163,"TREC 2012 sessions were created by considering two different dimensions: product type and goal quality. For product type, a session can be classified as searching for either factual or intellectual target. For search goal, a session can be classified as either specific or amorphous.",Y,null
,,,
164,"Both Table 5 and Table 6 show that the proposed method demonstrate difference effects on different session types. It achieves more improvement on Intellectual sessions (37.20%) and Amorphous sessions (31.48%) than on Factual sessions (19.09%) and Specific sessions (21.08%). This suggests that for more exploratory-style sessions, i.e., more difficult sessions, such as Intellectual and Amorphous sessions, our method is able to generate more performance gain. We believe that our method effectively captures query changes and well represents the dynamics in a search session.",null,null
,,,
165,5. CONCLUSION,null,null
,,,
166,"Based on the idea that query change is an important form of feedback, this paper presents a novel relevance feedback model by utilizing query change. Experiments show that our approach is highly effective and outperforms other feedback models for the TREC 2012 Session Track. Moreover, the",Y,null
,,,
167,"proposed relevance feedback method demonstrates different effects over sessions with different types of search targets and goals. It achieves more improvement on the more difficult sessions, such as Intellectual and Amorphous sessions, over the baseline system which does not use relevance feedback. We believe that our method better captures the exploratory nature of a search session by treating query changes as effective user feedback.",null,null
,,,
168,6. ACKNOWLEDGMENTS,null,null
,,,
169,"This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.",null,null
,,,
170,7. REFERENCES,null,null
,,,
171,"[1] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR '08, pages 243­250. ACM.",null,null
,,,
172,"[2] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.",null,null
,,,
173,"[3] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.",null,null
,,,
174,"[4] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.",null,null
,,,
175,[5] T. Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. In ICML '97.,null,null
,,,
176,"[6] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.",null,null
,,,
177,[7] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.,null,null
,,,
178,"[8] D. Sontag, K. Collins-Thompson, P. N. Bennett, R. W. White, S. Dumais, and B. Billerbeck. Probabilistic models for personalizing web search. In WSDM '12.",null,null
,,,
179,"[9] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.",null,null
,,,
180,824,null,null
,,,
181,,null,null

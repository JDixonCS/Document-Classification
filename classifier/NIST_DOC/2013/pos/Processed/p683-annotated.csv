,sentence,label,data
,,,
0,A Mutual Information-based Framework for the Analysis of Information Retrieval Systems,null,null
,,,
1,Peter B. Golbus Javed A. Aslam,null,null
,,,
2,"College of Computer and Information Science Northeastern University Boston, MA, USA",null,null
,,,
3,"{pgolbus,jaa}@ccs.neu.edu",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"We consider the problem of information retrieval evaluation and the methods and metrics used for such evaluations. We propose a probabilistic framework for evaluation which we use to develop new information-theoretic evaluation metrics. We demonstrate that these new metrics are powerful and generalizable, enabling evaluations heretofore not possible.",null,null
,,,
6,"We introduce four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  , a powerful meta-evaluation tool whose use we demonstrate on understanding novelty and diversity evaluation; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are in fact different.",null,null
,,,
7,Categories and Subject Descriptors,null,null
,,,
8,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation (efficiency and effectiveness),null,null
,,,
9,General Terms,null,null
,,,
10,Experimentation; Theory; Measurement,null,null
,,,
11,Keywords,null,null
,,,
12,"Information Retrieval, Search Evaluation",null,null
,,,
13,1. INTRODUCTION,null,null
,,,
14,"In order to improve search engines, it is necessary to accurately measure their current performance. If we cannot measure performance, how can we know whether a change",null,null
,,,
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
16,"was beneficial? In recent years, much of the work on information retrieval evaluation has focused on user models [7, 18] and diversity measures [1, 10, 24] which attempt to accurately reflect the experience of the user of a modern internet search engine. However, these measure are not easily generalized. In this work, we introduce a probabilistic framework for evaluation that encompasses and generalizes current evaluation methods. Our probabilistic framework allows us to view evaluation using the tools of information theory [11]. While our framework is not designed to coincide with user experience, it provides immediate access to a large number of powerful tools allowing for a deeper understanding of the performance of search engines.",null,null
,,,
17,"Our framework for evaluation is based on the observation that relevance judgments can also be interpreted as a preference between those documents with different relevance grades. This implies that relevance judgments can be treated as a retrieval system, and that evaluation can be considered as the ""rank"" correlation between systems and relevance judgments. To this end, we develop a probabilistic framework for rank correlation based on the expectation of random variables, which we demonstrate can also be used to compute existing evaluation metrics. However, the true value of our framework lies in its extension to new information-theoretic evaluation tools.",null,null
,,,
18,"After a discussion of related work (Section 2), we introduce our framework in Section 3. In Section 4, we demonstrate that our framework allows for an information theoretic understanding of Kendall's  [17], information  , which we use to define a conditional version of the rank correlation between two lists conditioned on a third. In Section 5, we define a new evaluation measure based on our framework: relevance information correlation. We validate our measure by showing that it is highly correlated with existing measures such as average precision (AP) and normalized discounted cumulative gain (nDCG). As a demonstration of the versatility of our framework when compared to, for example, user models, we show that our measure can be used to evaluate a collection of systems simultaneously (Section 6), creating an upper bound on the performance of metasearch algorithms. Finally, in Section 7, we introduce information difference, a powerful new tool for evaluating the similarity of retrieval systems beyond simply comparing their performance.",null,null
,,,
19,"This material is based upon work supported by the National Science Foundation under Grant No. IIS-1256172. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF).",null,null
,,,
20,683,null,null
,,,
21,2. RELATED WORK,null,null
,,,
22,"Search systems are typically evaluated against test collections which consist of a corpus of documents, a set of topics, and relevance assessments--whether a subset of those documents are relevant with respect to each topic.1 For example, the annual, NIST-sponsored Text REtrieval Conference (TREC) creates test collections commonly used in academic research. The performance of systems is assessed with regards to a specific task. A traditional search task is to attempt to rank all relevant documents above any nonrelevant documents. For this task, systems are evaluated in terms of the average trade-off between their precision and recall with respect to multiple topics. For a given topic, Let gi  {0, 1} be the relevance grade of the document at rank i, and let R be the number of relevant documents in the collection. At rank k,",null,null
,,,
23,k,null,null
,,,
24,gi,null,null
,,,
25,"precision@k , i,1",null,null
,,,
26,-1,null,null
,,,
27,k,null,null
,,,
28,k,null,null
,,,
29,gi,null,null
,,,
30,"recall@k , i,1",null,null
,,,
31,-2,null,null
,,,
32,R,null,null
,,,
33,"The trade-off between the two is measured by average precision, which can be interpreted as the area under the precisionrecall curve.",null,null
,,,
34,gi × precision@i,null,null
,,,
35,"AP , i,1",null,null
,,,
36,-3,null,null
,,,
37,R,null,null
,,,
38,"Average precision does not include information about document quality and degrees of relevance, and is an inherently recall-oriented measure. It is therefore not suitable for evaluating commercial web search engines.",null,null
,,,
39,"With the growth of the World Wide Web, test collections began to include graded, non-binary relevance judgments, e.g. G ,"" {non-relevant, relevant, highly relevant} or G "","" {0, . . . , 4}. To make use of these graded assessments, J¨arvelin and Kek¨al¨ainen developed normalized discounted cumulative gain (nDCG) [15]. nDCG also has the advantage that it can be evaluated at arbitrary ranks, and can therefore be used for precision-oriented tasks like web search.""",null,null
,,,
40,"Unlike average precision, which has a technical interpretation, nDCG can be best understood in terms of a model of a hypothetical user. In this model, a user will read the first k documents in a ranked list, deriving utility from each document. The amount of utility is proportional to the document's relevance grade and inversely proportional to the rank at which the document is encountered. We first define discounted cumulative gain (DCG).",null,null
,,,
41,k 2gi - 1,null,null
,,,
42,"DCG@k ,",null,null
,,,
43,-4,null,null
,,,
44,"i,1 log2(i + 1)",null,null
,,,
45,"Since the range of DCG will vary from topic to topic, it is necessary to normalize these scores so that an average can",null,null
,,,
46,"1For historical reasons, the set of relevance assessments is often referred to as a QREL.",null,null
,,,
47,be computed. Normalization is performed with regard to an ideal ranked list. If DCG @k is the maximum possible DCG of any ranked list of documents in the collection then,null,null
,,,
48,"nDCG@k , DCG@k",null,null
,,,
49,-5,null,null
,,,
50,DCG @k,null,null
,,,
51,"However, one does not always know how many documents are relevant at each level, and therefore the ideal list used for normalization is only an approximation. Moffat and Zobel [18] introduced a measure, rank-biased precision (RBP), that addresses this issue. In RBP, the probability that a user will read the document at rank k is drawn from a geometric distribution, whose parameter,   [0, 1), models the user's persistence. Given a utility function u : G  [0, 1], commonly defined as",null,null
,,,
52,2g - 1,null,null
,,,
53,"u(g) , 2d",null,null
,,,
54,-6,null,null
,,,
55,"where d is the maximum possible relevance grade, RBP is defined as the expected utility of a user who browses according to this model.",null,null
,,,
56,"RBP , (1 - ) u(gi) × i-1",null,null
,,,
57,-7,null,null
,,,
58,"i,1",null,null
,,,
59,"Since RBP is guaranteed to be in the range [0,1) for any topic and , it does not require normalization.",null,null
,,,
60,"Craswell et al. [12] introduced the Cascade model of user behavior. In this model, a user is still assumed to browse documents in order, but the probability that a user will view a particular document is no longer assumed to be independent of the documents that were viewed previously, i.e. a user is not assumed to stop at a particular rank, or at each rank with some probability. Instead, the user is assumed to stop after finding a relevant document. This implies that if a user reaches rank k, then all of the k - 1 documents ranked before it were non-relevant. Craswell et al. demonstrated empirically that this model corresponds well to observed user behavior in terms of predicting the clickthrough data of a commercial search engine.",null,null
,,,
61,"Chapelle et al. [7] developed an evaluation measure, expected reciprocal rank (ERR), based on the Cascade model. Let Ri denote the probability that a user will find the document at rank i to be relevant. Then in the Cascade model, the likelihood that a user will terminate his or her search at rank r is",null,null
,,,
62,r-1,null,null
,,,
63,Rr (1 - Ri).,null,null
,,,
64,-8,null,null
,,,
65,"i,1",null,null
,,,
66,"If we interpret the previously defined utility function (Equation 6) as the probability that a user will find a document relevant, i.e. Ri ,"" u(gi), then we can computed the expected reciprocal rank at which a user will terminate his or her search as""",null,null
,,,
67, 1 r-1,null,null
,,,
68,"ERR , r Rr (1 - Ri).",null,null
,,,
69,-9,null,null
,,,
70,"r,1",null,null
,,,
71,"i,1",null,null
,,,
72,"In this work, we propose an alternative, information-theoretic framework for evaluation. The first step is to reformulate these measures as the expected outcomes of random experiments. Computing evaluation measures in expectation is",null,null
,,,
73,684,null,null
,,,
74,"not uncommon in the literature, and we are not the first to suggest that reformulating an evaluation measure as an expectation allows for novel applications. For example, Yilmaz and Aslam [30] formulated average precision as the expectation of the following random experiment:",null,null
,,,
75,"1. Pick a random relevant document,",null,null
,,,
76,2. Pick a random document ranked at or above the rank of the document selected in step 1.,null,null
,,,
77,"3. Output 1 if the document from step 2 is relevant, otherwise output 0.",null,null
,,,
78,"Their intention was to accurately estimate average precision while collecting fewer relevance judgments (a process also applied to nDCG [32]). However, this formulation led to new uses, such as defining an information retrieval-specific rank correlation measure, AP [31], and a variation of average precision for graded relevance judgments, Graded Average Precision (GAP) [21].",null,null
,,,
79,"Our work uses pairwise document preferences rather than absolute relevance judgments. The use of preferences is somewhat common in IR. For example, many learning-torank algorithms, such as LambdaMart [3] and RankBoost [13], use pairwise document preferences in their objective functions. Carterette et al. [4, 5] explored the collection of preference judgments for evaluation, showing that they are faster to collect and have lower levels of inter-assessor disagreement. More recently, Chandar and Carterette [6] crowdsourced the collection of conditional document preferences to evaluate the standard assumptions underlying diversity evaluation, for example that users always prefer novel documents. Relative document preferences can also be inferred from the clickthrough data collected in the logs of commercial search engines [16]. These preferences can be used for evaluation without undertaking the expense of collecting relevance judgments from assessors.",null,null
,,,
80,3. A PROBABILISTIC FRAMEWORK FOR EVALUATION,null,null
,,,
81,"Mathematically, one can view the search system as providing a total ordering of the documents ranked and a partial ordering of the entire collection, where all ranked documents are preferred to unranked documents but the relative preference among the unranked documents is unknown. Similarly, one can view the relevance assessments as providing a partial ordering of the entire collection: in the case of binary relevance assessments, for example, all judged relevant documents are preferred to all judged non-relevant and unjudged documents, but the relative preferences among the relevant documents and among the non-relevant and unjudged documents is unknown. Thus, mathematically, one can view retrieval evaluation as comparing the partial ordering of the collection induced by the search system with the partial ordering of the collection induced by the relevance assessments.",null,null
,,,
82,"To formalize and instantiate a framework for comparing such partial orderings, consider the simplest case where we have two total orderings of objects, i.e., where the entire ""collection"" of objects is fully ranked in both ""orderings."" While such a situation does not typically arise in search system evaluation (since not all documents are ranked by the retrieval system nor are they fully ranked by relevance assessments), it does often arise when comparing the rankings",null,null
,,,
83,of systems induced by two (or more) evaluation metrics; here,null,null
,,,
84,Kendall's  is often the metric used to compare these (total,null,null
,,,
85,order) rankings.,null,null
,,,
86,"In what follows, we define a probabilistic framework within",null,null
,,,
87,"which to compare two total orderings, and we show how tra-",null,null
,,,
88,ditional metrics (such as Kendall's  ) are easily cast within,null,null
,,,
89,this framework. The real power of such a framework is,null,null
,,,
90,shown in subsequent sections: (1) the framework can be,null,null
,,,
91,easily generalized to handle the comparison of two partial,null,null
,,,
92,"orderings, such as arise in search system evaluation, and",null,null
,,,
93,"(2) well-studied, powerful, and general information-theoretic",null,null
,,,
94,metrics can be developed within this generalized framework.,null,null
,,,
95,Consider two total orderings of n objects. There are,null,null
,,,
96,n 2,null,null
,,,
97,"(unordered) pairs of such objects, and a pair is said to be",null,null
,,,
98,concordant if the two orderings agree on the relative rankings,null,null
,,,
99,of the objects and discordant if the two orderings disagree.,null,null
,,,
100,Let c and d be the number of concordant and discordant,null,null
,,,
101,"pairs, respectively. Then Kendall's  is defined as follows:",null,null
,,,
102,c-d,null,null
,,,
103,",",null,null
,,,
104,.,null,null
,,,
105,-10,null,null
,,,
106,c+d,null,null
,,,
107,If we let C and D denote the fraction of concordant and discordant pairs then Kendall's  is defined as,null,null
,,,
108," , C - D.",null,null
,,,
109,-11,null,null
,,,
110,"Note that c + d ,",null,null
,,,
111,n 2,null,null
,,,
112,if there are ties.2,null,null
,,,
113,"To define a probabilistic framework, we must specify three",null,null
,,,
114,"things: (1) a sample space of objects, (2) a distribution over",null,null
,,,
115,"this sample space, and (3) random variables over this sample",null,null
,,,
116,space. Let our sample space  be all possible 2 ·,null,null
,,,
117,n 2,null,null
,,,
118,ordered,null,null
,,,
119,"pairs of distinct objects, and consider a uniform distribution",null,null
,,,
120,"over this sample space. For a given ranking R, define a",null,null
,,,
121,"random variable XR :   {-1, +1} that outputs +1 for",null,null
,,,
122,any ordered pair concordant with R and -1 for any ordered,null,null
,,,
123,pair discordant with R.,null,null
,,,
124,"XR [(di, dj )] ,",null,null
,,,
125,1 if di appears before dj in R. -1 otherwise.,null,null
,,,
126,-12,null,null
,,,
127,"We thus have a well-defined random experiment: draw an ordered pair of objects at random and output +1 if that ordered pair agrees with R's ranking and -1 otherwise. Since all ordered pairs of objects are considered uniformly, the expected value E[XR] of this random variable is zero.",null,null
,,,
128,"Given a second ranked list S, one can similarly define an associated random variable XS. Now consider the random experiment of multiplying the two random variables: the product XR · XS will be +1 precisely when the pair is concordant--i.e. both lists agree that the ordering of the objects is correct (+1) or incorrect (-1), and the product will be -1 when the pair is discordant--i.e. the lists disagree. In this probabilistic framework, Kendall's  is the expected value",null,null
,,,
129,"2Kendall defined two means by which  can account for ties, depending on the desired behavior. Imagine comparing two ranked lists, one of which is almost completely composed of ties. A, defined above, approaches 1. B includes the number of ties in the denominator, and therefore approaches 0. We believe that the former approach is appropriate in this context. Since QRELs are almost exclusively composed of ties (recall that all pairs of unjudged documents in the corpus are considered to be tied), using the latter would mean that effect of the relatively rare meaningful comparisons would be negligible.",null,null
,,,
130,685,null,null
,,,
131,of the product of these random variables:,null,null
,,,
132," , E[XR · XS].",null,null
,,,
133,-13,null,null
,,,
134,"The real power of this framework is in the definition of these random variables: (1) the ability to generalize them to compare partial orderings as arise in system evaluation, and (2) the ability to measure the correlation of these random variables using information-theoretic techniques.",null,null
,,,
135,4. INFORMATION-THEORETIC RANK CORRELATION,null,null
,,,
136,"In Section 3, we defined Kendall's  as the expected product of random variables. The following theorem allows us to restate Kendall's  equivalently as the mutual information between the random variables.",null,null
,,,
137,Theorem,null,null
,,,
138,1,null,null
,,,
139,"I(XR; XS) ,",null,null
,,,
140,1+ 2,null,null
,,,
141,log(1+,null,null
,,,
142,)+,null,null
,,,
143,2-Jan,null,null
,,,
144,log(1- ).,null,null
,,,
145,"(For a proof of Theorem 1, see Appendix). Unlike Kendall's  , the mutual information between ranked lists ranges from 0 on lists that are completely uncorrelated to 1 on lists that are either perfectly correlated or perfectly anti-correlated.",null,null
,,,
146,"If we restrict our attention to pairs of lists that are not anti-correlated, then the relationship is bijective. Given this fact, we define a variant of Kendall's  , information  :",null,null
,,,
147,"I (R, S) , I(XR; XS)",null,null
,,,
148,-14,null,null
,,,
149,"where XR is the ranked list random variable defined in Equation 12 observed with respect to the uniform probability distribution over all pairs of distinct objects. By reframing Kendall's  equivalently in terms of mutual information, we immediately gain access to a large number of powerful theoretical tools. For example, we can define a conditional information  between two lists given a third. For lists R and S given T ,",null,null
,,,
150,"I (R, S | T ) , I(XR; XS | XT ).",null,null
,,,
151,-15,null,null
,,,
152,"Kendall's  can tell you whether two sets of rankings are similar, but it cannot tell you why. Information  can be used as a meta-evaluation tool to find the underlying cause of correlation between measures. We demonstrate the use of information  as a meta-evaluation tool by using it to analyze measures of the diversity of information retrieval systems. In recent years, several diversity measures (e.g. [1, 10, 24]) have been introduced to evaluate how well systems perform in response to ambiguous or underspecified queries that have multiple interpretations. These measures conflate several factors [14], including: a diversity model that rewards novelty and penalizes redundancy, and a measure of ad hoc performance that rewards systems for retrieving highly relevant documents. We wish to know not only whether two diversity measures are correlated, but also the similarity between their component diversity models. Using Kendall's  , we can observe whether the rankings of systems by each measure are correlated. But even if they are correlated, this could still be for one of two reasons: either both the diversity and the performance components evaluate systems similarly; or else one of the components is similar, and its effect on evaluation is dominant. However, if the measures are correlated when conditioned on their underlying performance components, then this must be due to similarities in their models of diversity.",null,null
,,,
153,Figure 1: Per-query information  (conditional rank correlation) between the TREC and NTCIR gold standard diversity measures conditioned on their underlying performance measures.,null,null
,,,
154,"We measured this effect on the the TREC 2011 and 2012 Web collections [8, 9]. Note that the performance measures are evaluated using graded relevance, while the diversity measures use binary judgments for each subtopic. All evaluations are performed at rank 20. Figure 1 shows the rank correlation between ERR-IA and D#-nDCG, the primary measures reported by TREC and NTCIR [26], when conditioned on their underlying performance models. Each query is computed separately, with each datapoint in the figure corresponding to a different query. Table 1 shows the results of conditioning additional pairs of diversity measures (now averaged over queries in the usual way) on their performance models. The results in Figure 1 are typical of all pairs of measures on a per-query basis.",null,null
,,,
155,"Our results confirm that while diversity measures are very highly correlated, most of this correlation disappears when one conditions on the underlying performance model. This indicates that most of the correlation is due to the similarity between the performance components and not the diversity components. For example, in TREC 2010, ERR-IA and -nDCG have an information  of almost 0.9. However, when conditioned on ERR, the similarity falls to only 0.25. This means that while these two measures are mostly ranking systems for the same reason, that reason is simply ERR. However, of the 0.9 bits that are the same, 0.25 are due to some factor other than ERR. This other factor must presumably be the similarity in their diversity models.",null,null
,,,
156,686,null,null
,,,
157,"I (ERR-IA ; -nDCG) I (ERR-IA ; -nDCG | nDCG) I (ERR-IA ; -nDCG | ERR) I (ERR-IA ; -nDCG | nDCG, ERR) I (ERR-IA ; D#-nDCG) I (ERR-IA ; D#-nDCG | nDCG) I (ERR-IA ; D#-nDCG | ERR) I (ERR-IA ; D#-nDCG | nDCG, ERR)",null,null
,,,
158,TREC 2010 0.8290 0.4860 0.2499 0.2451 0.6390 0.3026 0.1222 0.1239,null,null
,,,
159,TREC 2011 0.8375 0.4434 0.3263 0.2805 0.5545 0.1728 0.1442 0.1003,null,null
,,,
160,Table 1: TREC 2010 and 2011 information  (conditional rank correlation) between diversity measures conditioned on ad hoc performance measures.,Y,null
,,,
161,5. EVALUATION MEASURE,null,null
,,,
162,"In this section, we demonstrate an extension of our probabilistic framework for evaluation to measuring the correlation between a system and the incomplete ranking generated by a set of relevance judgments. This allows us to define an information-theoretic evaluation measure, relevance information correlation. While our measure has novel applications, we will demonstrate that the evaluations produced are consistent with those of existing measures.",null,null
,,,
163,"To compute mutual information, we must define a sample space, a probability distribution, and random variables. Let the sample space,  ,"" {(di, dj)}, be the set of all ordered pairs of judged documents. This means that we are ignoring unjudged documents, rather than considering them non-relevant. This is equivalent to computing an evaluation measure on the condensed list [23] created by removing all non-judged documents from the list. We define the probability distribution in terms of the QREL to ensure that all ranked lists will be evaluated using the same random experiment. Let P "", U |I(gi,""gj), where gi represents the relevance grade of document di, be the uniform probability distribution over all pairs of documents whose relevance grades are not equal. We define a QREL variable Q over ordered pairs of documents as""",null,null
,,,
164,"Q [(di, dj )] ,",null,null
,,,
165,1 0,null,null
,,,
166,if gi > gj otherwise.,null,null
,,,
167,-16,null,null
,,,
168,Note that this definition can be applied to both graded and binary relevance judgments.,null,null
,,,
169,"We now turn our attention to defining a ranked list random variable over ordered pairs of documents (di, dj). If both document di and dj appear in the ranked list, than our output can simply indicate whether di was ranked above dj. If document di appears in the ranked list and dj does not, then we will consider di as having been ranked above dj, and vice versa. If neither di nor dj is ranked, we will output a null value. If we were to instead restrict our attention only to judged document pairs where at least one document is ranked, then a ranked list consisting of a single relevant document followed by some number of non-relevant documents would have perfect mutual information with the QREL--all of the ranked relevant documents appear before all of the ranked non-relevant documents. However, this system must be penalized for preferring all of the ranked non-relevant documents to all of the unranked relevant documents. If we instead use a null value, our example ranked",null,null
,,,
170,list would almost always output null. This behavior would,null,null
,,,
171,"be independent of the QREL, meaning the two variables will",null,null
,,,
172,"have almost no mutual information. In effect, the null value",null,null
,,,
173,creates a recall component for our evaluation measure; no,null,null
,,,
174,system can have a large mutual information with the QREL,null,null
,,,
175,unless it retrieves most of the relevant documents.,null,null
,,,
176,Another problem we must consider is that mutual infor-,null,null
,,,
177,mation is maximized when two variables are completely cor-,null,null
,,,
178,related or completely anti-correlated. Consider an example,null,null
,,,
179,ranked list consisting of a few non-relevant documents fol-,null,null
,,,
180,lowed by several relevant documents and then many more,null,null
,,,
181,non-relevant documents. Since this example ranked list will,null,null
,,,
182,"disagree with the QREL on almost all document pairs, its",null,null
,,,
183,random variable will have a very high mutual information,null,null
,,,
184,with the QREL variable. The system is effectively being,null,null
,,,
185,rewarded for finding the subset of non-relevant documents,null,null
,,,
186,"that happen to be present in the QREL. To address this,",null,null
,,,
187,we truncate the list at the last retrieved relevant document,null,null
,,,
188,prior to evaluation.,null,null
,,,
189,Let ri represent the rank of document di in the list S. Then the ranked list variable RS is defined as,null,null
,,,
190," 1  RS [(di, dj )] , 0",null,null
,,,
191,-1,null,null
,,,
192,if ri < rj if neither di nor djwere retrieved otherwise.,null,null
,,,
193,-17,null,null
,,,
194,"We define our new measure, Relevance Information Correlation, as the mutual information between the QREL variable Q and the truncated ranked list variable R",null,null
,,,
195,"RIC(System) , I(RSystem; Q).",null,null
,,,
196,-18,null,null
,,,
197,"RIC is computed separately for each query, and then averaged, as with mean average precision.",null,null
,,,
198,"In order to compute RIC we must estimate the joint probability distribution of document preferences over Q and R. This could be done in various ways. In this work, we use the maximum likelihood estimate computed separately for each query. Since the MLE requires a large number of observations, RIC is only accurate for recall-oriented evaluation. In future work, we intend to explore other means of estimating P (Q, R) that will allow RIC to be used for precision-oriented evaluation as well.",null,null
,,,
199,"We also note that RIC has no explicit rank component, and would therefore seem to treat all relevant documents equally independent of the rank at which they were observed. However, there is an implicit rank component in that a relevant document that is not retrieved early in the list must be incorrectly ranked below many non-relevant documents. This argument is similar in spirit to Bpref [2].",null,null
,,,
200,"Our measure is quite novel in its formulation, and makes many non-standard assumptions about information retrieval evaluation. Therefore it is necessary to validate experimentally that our measure prefers the same retrieval systems as existing measures. Note that for two evaluation measures to be considered compatible, it is sufficient that they rank systems in the same relative order; it is not necessary that they always assign systems similar absolute scores. For example, a system's nDCG is often higher than its average precision.",null,null
,,,
201,"To show that RIC is consistent with AP and nDCG, we computed the RIC, AP, and nDCG of all systems submitted to TRECs 8 and 9. Figure 2 shows the output of RIC plotted against AP (top) and nDCG (bottom) on TRECs 8 (left) and 9 (right) [28, 29]. TREC 8 uses binary relevance",Y,null
,,,
202,687,null,null
,,,
203,Figure 2: Correlation between RIC and AP (top) and nDCG (bottom). TREC 8 (left) uses binary relevance judgments. TREC 9 (right) uses graded relevance judgments.,Y,null
,,,
204,(G)AP nDCG,null,null
,,,
205,MI,null,null
,,,
206,TREC 8 0.716 0.713 0.719,null,null
,,,
207,TREC9 0.648 0.757 0.744,null,null
,,,
208,Table 2: Discriminative power of (graded) AP and nDCG vs. RIC,null,null
,,,
209,"judgments. TREC 9 uses graded relevance judgments, requiring the use of graded average precision. Inset into each plot is the output of the measures on the top ten systems. For each experiment, we report the Kendall's  and Spearman's  [27] rank correlations for all systems, and for the top ten systems. With Kendall's  values of at least 0.799 on all systems and 0.644 on top ten systems, the ranking of systems by RIC is still highly correlated with those of both AP and nDCG. However, RIC is not as highly correlated with either AP or nDCG as AP and nDCG are with each other. Note that the correlation between RIC and GAP on TREC 9 is highly monotonic, even if is not particularly linear. This implies that the two measures do rank systems in a consistent relative order, even if RIC is a biased estimator of GAP.",Y,null
,,,
210,"To further validate our measure, we also compute the discriminative power [22] of the various measures. Discriminative power is a widely used tool for evaluating a measure's sensitivity i.e. how often differences between systems can be",null,null
,,,
211,"detected with high confidence. A high sensitivity can be seen as a necessary, though not sufficient, condition for a good evaluation measure. Discriminative power is defined as the percentage of pairs of runs that are found to be statistically significantly different by some significance test. As per Sakai, we use a two-tailed paired bootstrap test with 1000 bootstrap samples per pair of systems. Our results are displayed in Table 2. As measured by discriminatory power, we see that RIC is at least as sensitive, if not more so, than AP and nDCG.",null,null
,,,
212,6. UPPER BOUND ON METASEARCH,null,null
,,,
213,"In Section 5, we defined an evaluation measure in terms of mutual information. One advantage of this approach is that collections of systems can be evaluated directly by considering the output of their random variables jointly, without their needing to be combined. For a collection of systems, denoted S1 through Sn, the relevance information correlation can be defined as",null,null
,,,
214,"RIC(S1, . . . , Sn) ,"" I(RS1 , . . . , RSn ; Q)""",null,null
,,,
215,-19,null,null
,,,
216,"In this section, we will show that this produces a natural upper bound on metasearch performance that is consistent with other upper bounds appearing in the literature.",null,null
,,,
217,"We compare our upper bound against those of Montague [19]. Montague describes metasearch algorithms as sorting functions whose comparators, as well as the documents to be sorted, are defined in terms of collections of input systems.",null,null
,,,
218,688,null,null
,,,
219,"By also using the QREL as input, these algorithms can estimate upper bounds on metasearch performance. These bounds range from the ideal performance that cannot possibly be exceeded by any metasearch algorithm, to descriptions of reasonable metasearch behavior that should be similar to the performance of any quality metasearch algorithm.",null,null
,,,
220,Montague defines the following upper bounds on metasearch:,null,null
,,,
221,"1. Naive: Documents are sorted by comparison of relevance judgments, i.e. the naive upper bound is created by returning all relevant documents returned by any system in the collection above any non-relevant document. Relevant documents not retrieved by any system are not ranked.",null,null
,,,
222,"2. Pareto: If document A is ranked above document B by all systems, then document A is considered ""greater"" than document B. Otherwise, the documents are sorted by comparison of relevance judgments.",null,null
,,,
223,"3. Majoritarian: If document A is ranked above document B by at least half of the systems, then document A is considered ""greater"" than document B. Otherwise, the documents are sorted by comparison of relevance judgments.",null,null
,,,
224,"We will compare our direct joint evaluation with these upper bounds, and several metasearch algorithms commonly used as baselines in the IR literature: the CondorcetFuse metasearch algorithm [20], and the comb family of metasearch algorithms [25].",null,null
,,,
225,TREC 8 ANZ MNZ,Y,null
,,,
226,Condorcet Majoritarian,null,null
,,,
227,Pareto Naive,null,null
,,,
228, 0.221 0.587 0.519 0.552 0.657 0.788,null,null
,,,
229, 0.330 0.764 0.689 0.735 0.836 0.931,null,null
,,,
230,RMSE 0.481 0.351 0.362 0.340 0.044 0.039,null,null
,,,
231,"Table 3: Correlation between joint distribution and metasearch algorithms (Kendall's  , Spearman's , root mean square error).",null,null
,,,
232,"perform comparably to the Majoritarian bound, and the Naive bound is not appreciably better than the Pareto bound. If direct evaluation and the Naive bound are both reasonable estimates of the actual upper bound, then these results should be confirmed by Figure 3 and Table 3, as indeed they are. Note that there is almost no correlation between the joint evalution and the weakest metasearch algorithm, combANZ: combANZ does not approximate the upper bound on metasearch. The correlation improves as the quality of the metasearch algorithm improves, and it does so in a manner consistent with Montague. The correlations between the joint evaluation and the output of combMNZ, CondorcetFuse, and the Majoritarian bound are similar; while they are still biased as estimators, the correlation is beginning to approach monotonicity. Finally, with a root mean square error of 0.039, the joint evaluation estimation of the upper bound is essentially identical to that of the Naive upper bound. If the Naive upper bound is a reasonable estimate of the upper bound on metasearch performance, then so is the joint evaluation of the input systems.",null,null
,,,
233,"Figure 3: RIC of systems output by metasearch algorithms (Fusion System) versus RIC of systems computed directly (S1, . . . , S10) without combining.",null,null
,,,
234,"We examined the direct evaluation and metasearch performance of collections of ten randomly selected systems. Experiments were performed on TREC 8 and 9, with both binary and graded relevance judgments. To conserve space, we only show the results from TREC 8. The results from TREC 9 were highly similar, both when using binary and graded relevance judgments.",Y,null
,,,
235,"Figure 3 shows the RIC of the system output by a metasearch algorithm plotted against the joint RIC of the input systems, and Table 3 shows various measures of their correlation. Montague found that combANZ is inferior to CondorcetFuse and combMNZ, CondorcetFuse and combMNZ",null,null
,,,
236,7. INFORMATION DIFFERENCE,null,null
,,,
237,"In this section, we introduce a novel application of our probabilistic framework. Imagine that you are attempting to improve an existing ranker. On what basis do you decide whether or not your changes are beneficial? One typically evaluates both systems on a number of queries, and measures the difference in average performance. If one system outperforms the other, whether you have made an improvement is clear. But what happens when the systems perform similarly? It could be that your new system is essentially unchanged from your old system, but it is also possible that the two systems chose highly different document and just happened to have very similar evaluation scores. In the latter case, it may be possible to create a new, better system based on a combination of the two existing systems.",null,null
,,,
238,"We propose to measure the magnitude of the difference between systems in their ranking of documents for which we have relevance information, rather than the magnitude of the difference between their performance. We denote this new quantity as the information difference between systems. Our definition of information difference is inspired by the Boolean Algebra symmetric difference operator as applied to information space (see Figure 4).",null,null
,,,
239,"id(S1, S2) , I(S1; Q | S2) + I(S2; Q | S1)",null,null
,,,
240,-20,null,null
,,,
241,689,null,null
,,,
242,System 1,null,null
,,,
243,System 2,null,null
,,,
244,QREL,null,null
,,,
245,Figure 4: Information difference corresponds to the symmetric difference between the intersections of the systems with the QREL in information space (red portion of the Venn diagram).,null,null
,,,
246,"As a preliminary validation of information difference, we analyzed the change in AP and information difference between pairs of systems submitted to TREC 8, selected at random. We expect the two to be somewhat directly correlated, since, in general, if two systems rank documents similarly, we would expect them to have similar AP. However, we expect that they will not be highly correlated, since we believe that information difference is much more informative. Our intuition is supported by Figure 5, which shows the magnitude of the change in AP on the horizontal axis, and the information difference on the vertical axis.",Y,null
,,,
247,Figure 5: Scatter plot of information difference and the magnitude of change in AP of random pairs of TREC 8 systems.,null,null
,,,
248,"To demonstrate the utility of information difference, we sorted all the systems submitted to TREC 8 by AP and separated them into twenty equal-sized bins. By construction, each bin contained systems with small differences in performance. Our goal is to distinguish between similar and dissimilar systems within each bin. To this end, all systems within each bin were compared with one other (see Table 4). When the system pairs were sorted by their information difference, both systems in the first 27 pairs were submitted by the same group, whereas sorting by | AP| produced no discernible pattern. It is reasonable to assume that these systems were different instantiations of the same underlying",null,null
,,,
249,"technology. We can therefore conclude that information difference is able to determine whether systems with the same underlying performance are in fact similar, as desired.",null,null
,,,
250,Rank,null,null
,,,
251,1 2 3 4 5,null,null
,,,
252,28 29 30 31 32,null,null
,,,
253,System 1,null,null
,,,
254,UB99T unc8al32 fub99tt nttd8al ibmg99a,null,null
,,,
255,isa25t CL99SD ok8amxc tno8d4 uwmt8a2,null,null
,,,
256,System 2,null,null
,,,
257,UB99SW unc8al42 fub99tf nttd8alx ibmg99b,null,null
,,,
258,...,null,null
,,,
259,cirtrc82 CL99SDopt2 ok8alx MITSLStd uwmt8a1,null,null
,,,
260,id,null,null
,,,
261,0.010 0.012 0.017 0.023 0.027,null,null
,,,
262,0.084 0.086 0.086 0.088 0.089,null,null
,,,
263,| AP|,null,null
,,,
264,0.005 0.002 0.000 0.002 0.012,null,null
,,,
265,0.004 0.000 0.006 0.016 0.002,null,null
,,,
266,"Table 4: The systems from TREC 8 were binned by average precision. Information difference and  AP were computed for all system pairs within each bin. Sorting by information difference, both systems in the first 27 pairs were submitted by the same group.",Y,null
,,,
267,8. CONCLUSION,null,null
,,,
268,"In this work, we developed a probabilistic framework for the analysis of information retrieval systems based on the correlation between a ranked list and the preferences induced by relevance judgments. Using this framework, we developed powerful information theoretic tools for better understanding information retrieval systems. We introduced four preliminary uses of our framework: (1) a measure of conditional rank correlation, information  , which is a powerful meta-evaluation tool whose use we demonstrated on understanding novelty and diversity evalution; (2) a new evaluation measure, relevance information correlation, which is correlated with traditional evaluation measures and can be used to (3) evaluate a collection of systems simultaneously, which provides a natural upper bound on metasearch performance; and (4) a measure of the similarity between rankers on judged documents, information difference, which allows us to determine whether systems with similar performance are actually different.",null,null
,,,
269,"Our framework is based on the choice of sample space, probability distribution, and random variables. Throughout this work, we only used a uniform distribution on appropriate pairs of documents. However, not all document pairs are equal. The use of additional distributions is an immediate avenue for improvement that we intend to explore in future work. For example, a geometric distribution may be employed to force our evaluation tools to concentrate their attention at the top of a ranked list.",null,null
,,,
270,"The primary limitation of our evaluation measure as implemented in this work is that it is only applicable to recalloriented retrieval tasks. In future work, we intend to develop a precision-oriented version that is applicable to web search. Given such a measure, judgments can be combined in the way systems were in our upper bound on metasearch. In that way, a small number of expensive to produce nominal relevance judgments, a somewhat larger number of somewhat less expensive preference judgments, and a gold-stander ranker could all be used simultaneously to evaluate systems.",null,null
,,,
271,690,null,null
,,,
272,"Finally, we intend to explore the application of information difference to the understanding of information retrieval models. For example, BM25 and Language Models have long been used as baselines in information retrieval experiments. On the surface, these two models appear to be completely different. And yet, the two share deep theoretical connections [33]. Using information difference, we can determine whether their theoretical similarities outweigh their superficial differences in terms of how they rank documents.",null,null
,,,
273,9. REFERENCES,null,null
,,,
274,"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. Diversifying search results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining, WSDM '09, pages 5­14, New York, NY, USA, 2009. ACM.",null,null
,,,
275,"[2] Chris Buckley and Ellen M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, 2004.",null,null
,,,
276,"[3] Christopher J.C. Burges. From ranknet to lambdarank to lambdamart: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.",null,null
,,,
277,"[4] Ben Carterette and Paul N. Bennett. Evaluation measures for preference judgments. In SIGIR, 2008.",null,null
,,,
278,"[5] Ben Carterette, Paul N. Bennett, David Maxwell Chickering, and Susan T. Dumais. Here or there: preference judgments for relevance. In Proceedings of the IR research, 30th European conference on Advances in information retrieval, ECIR'08, 2008.",null,null
,,,
279,"[6] Praveen Chandar and Ben Carterette. Using preference judgments for novel document retrieval. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR '12, 2012.",null,null
,,,
280,"[7] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.",null,null
,,,
281,"[8] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Gordon V. Cormack. Overview of the TREC 2010 Web Track. In 19th Text REtrieval Conference, Gaithersburg, Maryland, 2010.",null,null
,,,
282,"[9] Charles L. A. Clarke, Nick Craswell, Ian Soboroff, and Ellen M. Voorhees. Overview of the TREC 2011 Web Track. In 20th Text REtrieval Conference, Gaithersburg, Maryland, 2011.",null,null
,,,
283,"[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ttcher, and Ian MacKinnon. Novelty and diversity in information retrieval evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '08, pages 659­666, New York, NY, USA, 2008. ACM.",null,null
,,,
284,"[11] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. Wiley-Interscience, 1991.",null,null
,,,
285,"[12] Nick Craswell, Onno Zoeter, Michael Taylor, and Bill Ramsey. An experimental comparison of click position-bias models. In Proceedings of the 2008",null,null
,,,
286,"International Conference on Web Search and Data Mining, WSDM '08, pages 87­94, New York, NY, USA, 2008. ACM.",null,null
,,,
287,"[13] Yoav Freund, Raj Iyer, Robert E. Schapire, and Yoram Singer. An efficient boosting algorithm for combining preferences. J. Mach. Learn. Res., 4, December 2003.",null,null
,,,
288,"[14] Peter B. Golbus, Javed A. Aslam, and Charles L.A. Clarke. Increasing evaluation sensitivity to diversity. In Journal of Information Retrieval, To Appear.",null,null
,,,
289,"[15] Kalervo J¨arvelin and Jaana Kek¨al¨ainen. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems, 20(4):422­446, October 2002.",null,null
,,,
290,"[16] Thorsten Joachims. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '02, 2002.",null,null
,,,
291,"[17] M. G. Kendall. A New Measure of Rank Correlation. Biometrika, 30(1/2):81­93, June 1938.",null,null
,,,
292,"[18] Alistair Moffat and Justin Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, December 2008.",null,null
,,,
293,"[19] Mark Montague. Metasearch: Data Fusion for Document Retrieval. PhD thesis, Dartmouth College. Dept. of Computer Science, 2002.",null,null
,,,
294,"[20] Mark Montague and Javed A. Aslam. Condorcet fusion for improved retrieval. In Proceedings of the eleventh international conference on Information and knowledge management, CIKM '02, 2002.",null,null
,,,
295,"[21] Stephen E. Robertson, Evangelos Kanoulas, and Emine Yilmaz. Extending average precision to graded relevance judgments. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, SIGIR '10, 2010.",null,null
,,,
296,"[22] Tetsuya Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, 2006.",null,null
,,,
297,"[23] Tetsuya Sakai. Alternatives to Bpref. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, 2007.",null,null
,,,
298,"[24] Tetsuya Sakai and Ruihua Song. Evaluating diversified search results using per-intent graded relevance. In SIGIR, pages 1043­1052, 2011.",null,null
,,,
299,"[25] Joseph A. Shaw and Edward A. Fox. Combination of multiple searches. In The Second Text REtrieval Conference (TREC-2), pages 243­252, 1994.",null,null
,,,
300,"[26] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. Kato, Yiqun Liu, Miho Sugimoto, Qinglei Wang, and Naoki Orii. Overview of the ntcir-9 intent task. In Proceedings of the 9th NTCIR Workshop, Tokyo, Japan, 2011.",null,null
,,,
301,"[27] C. Spearman. The proof and measurement of association between two things. The American Journal of Psychology, 1904.",null,null
,,,
302,"[28] E. M. Voorhees and D. Harman. Overview of the eighth text retrieval conference (TREC-8). In Proceedings of the Eighth Text REtrieval Conference (TREC-8), 2000.",null,null
,,,
303,691,null,null
,,,
304,"[29] E. M. Voorhees and D. Harman. Overview of the ninth text retrieval conference (TREC-9). In Proceedings of the Ninth Text REtrieval Conference (TREC-9), 2001.",null,null
,,,
305,"[30] Emine Yilmaz and Javed A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM '06, 2006.",null,null
,,,
306,"[31] Emine Yilmaz, Javed A. Aslam, and Stephen Robertson. A new rank correlation coefficient for information retrieval. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, 2008.",null,null
,,,
307,"[32] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. A simple and efficient sampling method for estimating AP and nDCG. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, 2008.",null,null
,,,
308,"[33] Chengxiang Zhai and John Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, April 2004.",null,null
,,,
309,10. APPENDIX,null,null
,,,
310,Theorem,null,null
,,,
311,1,null,null
,,,
312,"I(XR; XS) ,",null,null
,,,
313,1+ 2,null,null
,,,
314,log(1+,null,null
,,,
315,)+,null,null
,,,
316,2-Jan,null,null
,,,
317,log(1- ).,null,null
,,,
318,Proof. Denote XR and XS as X and Y . Consider the following joint probability distribution table.,null,null
,,,
319,Y -1 1 X -1 a b,null,null
,,,
320,1 cd,null,null
,,,
321,"Observe that: a + b + c + d , 1; C ,"" a + d, D "","" b + c, and therefore  "","" a + d - b - c; and since document pairs appear in both orders, a "", d and b , c.",null,null
,,,
322,The joint probability distribution can be rewritten as follows.,null,null
,,,
323,Y,null,null
,,,
324,-1 1,null,null
,,,
325,X -1,null,null
,,,
326,C 2,null,null
,,,
327,D 2,null,null
,,,
328,1,null,null
,,,
329,DC,null,null
,,,
330,2,null,null
,,,
331,2,null,null
,,,
332,"Observe that the marginal probability P (X) , P (Y ) ,",null,null
,,,
333,C 2,null,null
,,,
334,+,null,null
,,,
335,D 2,null,null
,,,
336,",",null,null
,,,
337,C 2,null,null
,,,
338,+,null,null
,,,
339,D 2,null,null
,,,
340,",",null,null
,,,
341,1 2,null,null
,,,
342,",",null,null
,,,
343,1 2,null,null
,,,
344,.,null,null
,,,
345,"I(X; Y ) ,"" KL(P (X, Y )||P (X)P (Y ))""",null,null
,,,
346,"p(x, y)",null,null
,,,
347,",",null,null
,,,
348,"p(x, y) lg",null,null
,,,
349,p(x)p(y),null,null
,,,
350,"x,y",null,null
,,,
351,1,null,null
,,,
352,",",null,null
,,,
353,"p(x, y) lg p(x, y) + p(x, y) lg",null,null
,,,
354,.,null,null
,,,
355,p(x)p(y),null,null
,,,
356,"x,y",null,null
,,,
357,"x,y",null,null
,,,
358,"Since P (X, Y ) ,",null,null
,,,
359,C 2,null,null
,,,
360,",",null,null
,,,
361,D 2,null,null
,,,
362,",",null,null
,,,
363,C 2,null,null
,,,
364,",",null,null
,,,
365,D 2,null,null
,,,
366,"and P (X)P (Y ) ,",null,null
,,,
367,1 4,null,null
,,,
368,",",null,null
,,,
369,1 4,null,null
,,,
370,",",null,null
,,,
371,1 4,null,null
,,,
372,",",null,null
,,,
373,1 4,null,null
,,,
374,",",null,null
,,,
375,"I(X, Y ) , 2 · C lg C + 2 · D lg D + 2 · C lg 4 + 2 · D lg 4",null,null
,,,
376,22,null,null
,,,
377,22,null,null
,,,
378,2,null,null
,,,
379,2,null,null
,,,
380,", C lg C + D lg D + 2C + 2D",null,null
,,,
381,2,null,null
,,,
382,2,null,null
,,,
383,", C lg C - C + D lg D - D + 2C + 2D",null,null
,,,
384,", C lg C + D lg D + 1",null,null
,,,
385,", C lg C + (1 - C) lg(1 - C) + 1",null,null
,,,
386,"Since C + D , 1 and  ,"" C - D, we have that  "","" 2C - 1,""",null,null
,,,
387,"C,",null,null
,,,
388,1+ 2,null,null
,,,
389,"and D , 1 - C ,",null,null
,,,
390,2-Jan,null,null
,,,
391,.,null,null
,,,
392,"In terms of C, if H2 represents the entropy of a Bernoulli",null,null
,,,
393,"random variable ,3",null,null
,,,
394,"I(X; Y ) , -H2(C) + 1",null,null
,,,
395,"1+ , -H2 2 + 1",null,null
,,,
396,1+ 1+ 1- 1-,null,null
,,,
397,",",null,null
,,,
398,lg,null,null
,,,
399,+,null,null
,,,
400,lg,null,null
,,,
401,1,null,null
,,,
402,2,null,null
,,,
403,2,null,null
,,,
404,2,null,null
,,,
405,2,null,null
,,,
406,", 1 +  lg(1 +  ) - 1 +  + 1 -  lg(1 -  )",null,null
,,,
407,2,null,null
,,,
408,2,null,null
,,,
409,2,null,null
,,,
410,-1- +1 2,null,null
,,,
411,1+,null,null
,,,
412,1-,null,null
,,,
413,",",null,null
,,,
414,lg(1 +  ) +,null,null
,,,
415,lg(1 -  ),null,null
,,,
416,2,null,null
,,,
417,2,null,null
,,,
418,"Corollary 1. For two ranked lists R and S, I(XR; XS) ,",null,null
,,,
419,1 - H2(K) where K,null,null
,,,
420,",",null,null
,,,
421,2-Jan,null,null
,,,
422,is the normalized Kendall's ,null,null
,,,
423,distance between R and S.,null,null
,,,
424,"3H2(p) , -p lg p - (1 - p) lg(1 - p). Note that H2(p) , H2(1 - p).",null,null
,,,
425,692,null,null
,,,
426,,null,null

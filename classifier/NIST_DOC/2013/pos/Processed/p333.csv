,sentence,label,data
0,Ranking Document Clusters Using Markov Random Fields,null,null
1,Fiana Raiber fiana@tx.technion.ac.il,null,null
2,Oren Kurland kurland@ie.technion.ac.il,null,null
3,"Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel",null,null
4,ABSTRACT,null,null
5,"An important challenge in cluster-based document retrieval is ranking document clusters by their relevance to the query. We present a novel cluster ranking approach that utilizes Markov Random Fields (MRFs). MRFs enable the integration of various types of cluster-relevance evidence; e.g., the query-similarity values of the cluster's documents and queryindependent measures of the cluster. We use our method to re-rank an initially retrieved document list by ranking clusters that are created from the documents most highly ranked in the list. The resultant retrieval effectiveness is substantially better than that of the initial list for several lists that are produced by effective retrieval methods. Furthermore, our cluster ranking approach significantly outperforms stateof-the-art cluster ranking methods. We also show that our method can be used to improve the performance of (stateof-the-art) results-diversification methods.",null,null
6,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
7,"General Terms: Algorithms, Experimentation",null,null
8,"Keywords: ad hoc retrieval, cluster ranking, query-specific clusters, markov random fields",null,null
9,1. INTRODUCTION,null,null
10,The cluster hypothesis [33] gave rise to a large body of work on using query-specific document clusters [35] for improving retrieval effectiveness. These clusters are created from documents that are the most highly ranked by an initial search performed in response to the query.,null,null
11,"For many queries there are query-specific clusters that contain a very high percentage of relevant documents [8, 32, 25, 14]. Furthermore, positioning the constituent documents of these clusters at the top of the result list yields highly effective retrieval performance; specifically, much better than that of state-of-the art retrieval methods that rank documents directly [8, 32, 25, 14, 10].",null,null
12,"As a result of these findings, there has been much work on ranking query-specific clusters by their presumed relevance",null,null
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
14,"to the query (e.g., [35, 22, 24, 25, 26, 14, 15]). Most previous approaches to cluster ranking compare a representation of the cluster with that of the query. A few methods integrate additional types of information such as inter-cluster and cluster-document similarities [18, 14, 15]. However, there are no reports of fundamental cluster ranking frameworks that enable to effectively integrate various information types that might attest to the relevance of a cluster to a query.",null,null
15,"We present a novel cluster ranking approach that uses Markov Random Fields. The approach is based on integrating various types of cluster-relevance evidence in a principled manner. These include the query-similarity values of the cluster's documents, inter-document similarities within the cluster, and measures of query-independent properties of the cluster, or more precisely, of its documents.",null,null
16,"A large array of experiments conducted with a variety of TREC datasets demonstrates the high effectiveness of using our cluster ranking method to re-rank an initially retrieved document list. The resultant retrieval performance is substantially better than that of the initial ranking for several effective rankings. Furthermore, our method significantly outperforms state-of-the-art cluster ranking methods. Although the method ranks clusters of similar documents, we show that using it to induce document ranking can help to substantially improve the effectiveness of (state-of-the-art) retrieval methods that diversify search results.",null,null
17,2. RETRIEVAL FRAMEWORK,null,null
18,Suppose that some search algorithm was employed over a corpus of documents in response to a query. Let Dinit be the list of the initially highest ranked documents. Our goal is to re-rank Dinit so as to improve retrieval effectiveness.,null,null
19,"To that end, we employ a standard cluster-based retrieval paradigm [34, 24, 18, 26, 15]. We first apply some clustering method upon the documents in Dinit; C l(Dinit) is the set of resultant clusters. Then, the clusters in C l(Dinit) are ranked by their presumed relevance to the query. Finally, the clusters' ranking is transformed to a ranking of the documents in Dinit by replacing each cluster with its constituent documents and omitting repeats in case the clusters overlap. Documents in a cluster are ordered by their query similarity.",null,null
20,"The motivation for employing the cluster-based approach just described follows the cluster hypothesis [33]. That is, letting similar documents provide relevance status support to each other by the virtue of being members of the same clusters. The challenge that we address here is devising a (novel) cluster ranking method -- i.e., we tackle the second step of the cluster-based retrieval paradigm.",null,null
21,333,null,null
22,"Figure 1: The three types of cliques considered for graph G. G is composed of a query node (Q) and three (for the sake of the example) nodes (d1, d2, and d3) that correspond to the documents in cluster C. (i) lQD contains the query and a single document from C; (ii) lQC contains all nodes in G; and, (iii) lC contains only the documents in C.",null,null
23,"Formally, let C and Q denote random variables that take as values document clusters and queries respectively. The cluster ranking task amounts to estimating the probability that a cluster is relevant to a query, p(C|Q):",null,null
24,p(C |Q),null,null
25,",",null,null
26,"p(C, Q) p(Q)",null,null
27,"ra,nk",null,null
28,"p(C, Q).",null,null
29,(1),null,null
30,The rank equivalence holds as clusters are ranked with respect to a fixed query.,null,null
31,"To estimate p(C, Q), we use Markov Random Fields (MRFs). As we discuss below, MRFs are a convenient framework for integrating various types of cluster-relevance evidence.",null,null
32,2.1 Using MRFs to rank document clusters,null,null
33,"An MRF is defined over a graph G. Nodes represent random variables and edges represent dependencies between these variables. Two nodes that are not connected with an edge correspond to random variables that are independent of each other given all other random variables. The set of nodes in the graph we construct is composed of a node representing the query and nodes representing the cluster's constituent documents. The joint probability over G's nodes, p(C, Q), can be expressed as follows:",null,null
34,"p(C, Q) , lL(G) l(l) ;",null,null
35,(2),null,null
36,Z,null,null
37,"L(G) is the set of cliques in G and l is a clique; l(l) is a potential (i.e., positive function) defined over l; Z ,",null,null
38,"C,Q lL(G) l(l) is the normalization factor that serves to ensure that p(C, Q) is a probability distribution. The normalizer need not be computed here as we rank clusters with respect to a fixed query.",null,null
39,A common instantiation of potential functions is [28]:,null,null
40,"l(l) d,""ef exp(lfl(l)),""",null,null
41,"where fl(l) is a feature function defined over the clique l and l is the weight associated with this function. Accordingly, omitting the normalizer from Equation 2, applying the rank-preserving log transformation, and substituting the potentials with the corresponding feature functions results in our ClustMRF cluster ranking method:",null,null
42,"p(C|Q) ra,nk",null,null
43,l fl (l).,null,null
44,(3),null,null
45,lL(G),null,null
46,"This is a generic linear (in feature functions) cluster ranking function that depends on the graph G. To instantiate a specific ranking method, we need to (i) determine G's structure,",null,null
47,"specifically, its clique set L(G); and, (ii) associate feature functions with the cliques. We next address these two tasks.",null,null
48,2.1.1 Cliques and feature functions,null,null
49,We consider three types of cliques in the graph G. These are depicted in Figure 1. In what follows we write d  C to indicate that document d is a member of cluster C.,null,null
50,"The first clique (type), lQD, contains the query and a single document in the cluster. This clique serves for making inferences based on the query similarities of the cluster's constituent documents when considered independently. The second clique, lQC , contains all nodes of the graph; that is, the query Q and all C's constituent documents. This clique is used for inducing information from the relations between the query-similarity values of the cluster's constituent documents. The third clique, lC, contains only the cluster's constituent documents. It is used to induce information based on query-independent properties of the cluster's documents.",null,null
51,"In what follows we describe the feature functions defined over the cliques. In some cases a few feature functions are defined for the same clique, and these are used in the summation in Equation 3. Note that the sum of feature functions is also a feature function. The weights associated with the feature functions are set using a train set of queries. (Details are provided in Section 4.1.)",null,null
52,The lQD clique. High query similarity exhibited by C's,null,null
53,constituent documents can potentially imply to C's rele-,null,null
54,"vance [26]. Accordingly, let d ( C) be the document in",null,null
55,lQD .,null,null
56,We,null,null
57,define fgeo-qsim;lQD (lQD),null,null
58,"d,ef",null,null
59,1,null,null
60,"log sim(Q, d) |C| ,",null,null
61,"where |C| is the number of documents in C, and sim(·, ·) is",null,null
62,"some inter-text similarity measure, details of which are pro-",null,null
63,vided in Section 4.1. Using this feature function in Equation,null,null
64,3 for all the lQD cliques of G amounts to using the geometric,null,null
65,mean of the query-similarity values of C's constituent docu-,null,null
66,ments. All feature functions that we consider use logs so as,null,null
67,to have a conjunction semantics for the integration of their,null,null
68,assigned values when using Equation 3.1,null,null
69,The lQC clique. Using the lQD clique from above results,null,null
70,"in considering the query-similarity values of the cluster's documents independently of each other. In contrast, the lQC clique provides grounds for utilizing the relations between these similarity values. Specifically, we use the log",null,null
71,"1Before applying the log function we employ add- (, 10-10) smoothing.",null,null
72,334,null,null
73,"of the minimal, maximal, and standard deviation2 of the {sim(Q, d)}dC values as feature functions for lQC, denoted min-qsim, max-qsim, and stdv-qsim, respectively.",null,null
74,"The lC clique. Heretofore, the lQD and lQC cliques served",null,null
75,for inducing information from the query similarity values of,null,null
76,C's documents. We now consider query-independent proper-,null,null
77,ties of C that can potentially attest to its relevance. Doing so,null,null
78,amounts to defining feature functions over the lC clique that,null,null
79,contains C's documents but not the query. All the feature,null,null
80,functions that we define for lC are constructed as follows.,null,null
81,"We first define a query-independent document measure, P,",null,null
82,and apply it to document d ( C) yielding the value P(d).,null,null
83,"Then, we use log A({P(d)}dC) where A is an aggregator function: minimum, maximum, and geometric mean. The",null,null
84,"resultant feature functions are referred to as min-P, max-",null,null
85,"P, and geo-P, respectively. We next describe the document",null,null
86,measures that serve as the basis for the feature functions.,null,null
87,The cluster hypothesis [33] implies that relevant docu-,null,null
88,"ments should be similar to each other. Accordingly, we mea-",null,null
89,sure for document d in C its similarity with all documents,null,null
90,in C:,null,null
91,"Pdsim(d) d,ef",null,null
92,1 |C|,null,null
93,"diC sim(d, di).",null,null
94,The next few query-independent document measures are,null,null
95,based on the following premise. The higher the breadth of,null,null
96,"content in a document, the higher the probability it is rel-",null,null
97,"evant to some query. Thus, a cluster containing documents",null,null
98,with broad content should be assigned with relatively high,null,null
99,probability of being relevant to some query.,null,null
100,High entropy of the term distribution in a document is a,null,null
101,"potential indicator for content breadth [17, 3]. This is be-",null,null
102,"cause the distribution is ""spread"" over many terms rather",null,null
103,"than focused over a few ones. Accordingly, we define",null,null
104,"Pentropy(d) d,""ef - wd p(w|d) log p(w|d), where w is a term and p(w|d) is the probability assigned to w by an unsmoothed""",null,null
105,"unigram language model (i.e., maximum likelihood estimate)",null,null
106,induced from d.,null,null
107,"Inspired by work on Web spam classification [9], we use",null,null
108,"the inverse compression ratio of document d, Picompress(d),",null,null
109,as an additional measure. (Gzip is used for compression.),null,null
110,High compression ratio presumably attests to reduced con-,null,null
111,tent breadth [9].,null,null
112,Two additional content-breadth measures that were pro-,null,null
113,posed in work on Web retrieval [3] are the ratio between the,null,null
114,"number of stopwords and non-stopwords in the document,",null,null
115,"Psw1(d); and, the fraction of stopwords in a stopword list",null,null
116,"that appear in the document, Psw2(d). We use INQUERY's",null,null
117,stopword list [2]. A document containing many stopwords,null,null
118,is presumably of richer language (and hence content) than,null,null
119,"a document that does not contain many of these; e.g., a",null,null
120,document containing a table composed only of keywords [3].,null,null
121,For some of the Web collections used for evaluation in,null,null
122,"Section 4, we also use the PageRank score [4] of the docu-",null,null
123,"ment, Ppr(d), and the confidence level that the document is",null,null
124,"not spam, Pspam(d). The details of the spam classifier are",null,null
125,provided in Section 4.1.,null,null
126,We note that using the feature functions that result from,null,null
127,applying the geometric mean aggregator upon the query-,null,null
128,"independent document measures just described, except for",null,null
129,"2It was recently argued that high variance of the querysimilarity values of the cluster's documents might be an indicator for the cluster's relevance, as it presumably attests to a low level of ""query drift"" [19].",null,null
130,"dsim, could have been described in an alternative way. That",null,null
131,1,null,null
132,"is, using log P(d) |C| as a feature function over a clique con-",null,null
133,"taining a single document. Then, using these feature functions in Equation 3 amounts to using the geometric mean.3",null,null
134,3. RELATED WORK,null,null
135,"The work most related to ours is that on devising cluster ranking methods. The standard approach is based on measuring the similarity between a cluster representation and that of the query [7, 34, 35, 16, 24, 25, 26]. Specifically, a geometric-mean-based cluster representation was shown to be highly effective [26, 30, 15]. Indeed, ranking clusters by the geometric mean of the query-similarity values of their constituent documents is a state-of-the-art cluster ranking approach [15]. This approach rose as an integration of feature functions used in ClustMRF, and is shown in Section 4 to substantially underperform ClustMRF.",null,null
136,"Clusters were also ranked by the highest query similarity exhibited by their constituent documents [22, 31] and by the variance of these similarities [25, 19]. ClustMRF incorporates these methods as feature functions and is shown to outperform each.",null,null
137,"Some cluster ranking methods use inter-cluster and clusterdocument similarities [14, 15]. While ClustMRF does not utilize such similarities, it is shown to substantially outperform one such state-of-the-art method [15].",null,null
138,"A different use of clusters in past work on cluster-based retrieval is for ""smoothing"" (enriching) the representation of documents [20, 16, 24, 13]. ClustMRF is shown to substantially outperform one such state-of-the-art method [13].",null,null
139,"To the best of our knowledge, our work is first to use MRFs for cluster ranking. In the context of retrieval tasks, MRFs were first introduced for ranking documents directly [28]. We show that using ClustMRF to produce document ranking substantially outperforms this retrieval approach; and, that which augments the standard MRF retrieval model with query-independent document measures [3]. MRFs were also used, for example, for query expansion, passage-based document retrieval, and weighted concept expansion [27].",null,null
140,4. EVALUATION 4.1 Experimental setup,null,null
141,corpus AP,null,null
142,ROBUST,null,null
143,WT10G GOV2 ClueA ClueAF ClueB ClueBF,null,null
144,"# of docs 242,918",null,null
145,"528,155",null,null
146,"1,692,096 25,205,179",null,null
147,"503,903,810",null,null
148,data Disks 1-3,null,null
149,Disks 4-5 (-CR),null,null
150,WT10g GOV2,null,null
151,ClueWeb09 (Category A),null,null
152,queries,null,null
153,"51-150 301-450, 600-700 451-550 701-850",null,null
154,1-150,null,null
155,"50,220,423 ClueWeb09 (Category B) 1-150",null,null
156,Table 1: Datasets used for experiments.,null,null
157,"The TREC datasets specified in Table 1 were used for experiments. AP and ROBUST are small collections, composed mostly of news articles. WT10G and GOV2 are Web",null,null
158,"3Similarly, we could have used the geometric mean of the query-similarity values of the cluster constituent documents as a feature function defined over the lQC clique rather than constructing it using the lQD cliques as we did above.",null,null
159,335,null,null
160,"collections; the latter is a crawl of the .gov domain. For the ClueWeb Web collection both the English part of Category A (ClueA) and the Category B subset (ClueB) were used. ClueAF and ClueBF are two additional experimental settings created from ClueWeb following previous work [6]. Specifically, documents assigned by Waterloo's spam classifier [6] with a score below 70 and 50 for ClueA and ClueB, respectively, were filtered out from the initial corpus ranking described below. The score indicates the percentage of all documents in ClueWeb Category A that are presumably ""spammier"" than the document at hand. The ranking of the residual corpus was used to create the document list upon which the various methods operate. Waterloo's spam score is also used for the Pspam(·) measure that was described in Section 2.1. The Pspam(·) and Ppr(·) (PageRank score) measures are used only for the ClueWeb-based settings as these information types are not available for the other settings.",null,null
161,The titles of TREC topics served for queries. All data was stemmed using the Krovetz stemmer. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.,null,null
162,Initial retrieval and clustering. As described in Section,null,null
163,"2, we use the ClustMRF cluster ranking method to re-rank an initially retrieved document list Dinit. Recall that after ClustMRF ranks the clusters created from Dinit, these are ""replaced"" by their constituent documents while omitting repeats. Documents within a cluster are ranked by their query similarity, the measure of which is detailed below. This cluster-based re-ranking approach is employed by all the reference comparison methods that we use and that rely on cluster ranking. Furthermore, ClustMRF and all reference comparison approaches re-rank a list Dinit that is composed of the 50 documents that are the most highly ranked by some retrieval method specified below. Dinit is relatively short following recommendations in previous work on cluster-based re-ranking [18, 25, 26, 13]. In Section 4.2.7 we study the effect of varying the list size on the performance of ClustMRF and the reference comparisons.",null,null
164,"We let all methods re-rank three different initial lists Dinit. The first, denoted MRF, is used unless otherwise specified. This list contains the documents in the corpus that are the most highly ranked in response to the query when using the state-of-the-art Markov Random Field approach with the sequential dependence model (SDM) [28]. The free parameters that control the use of term proximity information in SDM, T , O, and U , are set to 0.85, 0.1, and 0.05, respectively, following previous recommendations [28]. We also use MRF's SDM with its free parameters set using cross validation as one of the re-ranking reference comparisons. (Details provided below.) All methods operating on the MRF initial list use the exponent of the document score assigned by SDM -- which is a rank-equivalent estimate to that of log p(Q, d) -- as simMRF (Q, d), the document-query similarity measure. This measure was used to induce the initial ranking using which Dinit was created. More generally, for a fair performance comparison we maintain in all the experiments the invariant that the scoring function used to create an initially retrieved list is rank equivalent to the documentquery similarity measure used in methods operating on the list. Furthermore, the document-query similarity measure is",null,null
165,used in all methods that are based on cluster ranking (including ClustMRF) to order documents within the clusters.,null,null
166,"The second initial list used for re-ranking, DocMRF (discussed in Section 4.2.4), is created by enriching MRF's SDM with query-independent document measures [3].",null,null
167,"The third initial list, LM, is addressed in Section 4.2.5. The list is created using unigram language models. In contrast, the MRF and DocMRF lists were created using retrieval methods that use term proximity information. Let pDz ir[µ](·) be the Dirichlet-smoothed unigram language model induced from text z; µ is the smoothing parameter. The LM similarity between texts x and y is simLM (x, y) d,ef",null,null
168,"exp -CE pDx ir[0](·) pDy ir[µ](·) [37, 17], where CE is",null,null
169,"the cross entropy measure; µ is set to 1000.4 Accordingly, the LM initial list is created by using simLM (Q, d) to rank the entire corpus.5 This measure serves as the documentquery similarity measure for all methods operating over the LM list, and for the inter-document similarity measure used by the dsim feature function.",null,null
170,"Unless otherwise stated, to cluster any of the three initial lists Dinit, we use a simple nearest-neighbor clustering approach [18, 25, 14, 26, 13, 15]. For each document d ( Dinit), a cluster is created from d and the k - 1 documents di in Dinit (di ,"" d) with the highest simLM (d, di); k is set to a value in {5, 10, 20} using cross validation as described below. Using such small overlapping clusters (all of which contain k documents) was shown to be highly effective for cluster-based document retrieval [18, 25, 14, 26, 13, 15]. In Section 4.2.6 we also study the performance of ClustMRF when using hierarchical agglomerative clustering.""",null,null
171,Evaluation metrics and free parameters. We use MAP,null,null
172,"(computed at cutoff 50, the size of the list Dinit that is reranked) and the precision of the top 5 documents (p@5) and their NDCG (NDCG@5) for evaluation measures.6 The free parameters of our ClustMRF method, as well as those of all reference comparison methods, are set using 10-fold cross validation performed over the queries in an experimental setting. Query IDs are the basis for creating the folds. The two-tailed paired t-test with p  0.05 was used for testing statistical significance of performance differences.",null,null
173,"For our ClustMRF method, the free-parameter values are set in two steps. First, SVMrank [12] is used to learn the values of the l weights associated with the feature functions. The NDCG@k of the k constituent documents of a cluster serves as the cluster score used for ranking clusters in the learning phase7. (Recall from above that documents in a",null,null
174,"4The MRF SDM used above also uses Dirichlet-smoothed unigram language models with µ ,"" 1000. 5Queries for which there was not a single relevant document in the MRF or LM initial lists were removed from the evaluation. For the ClueWeb settings, the same query set was used for ClueX and ClueXF. 6We note that statAP, rather than AP, was the official TREC evaluation metric in 2009 for ClueWeb with queries 1­50. For consistency with the other queries for ClueWeb, and following previous work [3], we use AP for all ClueWeb queries by treating prel files as qrel files. We hasten to point out that evaluation using statAP for the ClueWeb collections with queries 1­50 yielded relative performance patterns that are highly similar to those attained when using AP. 7Using MAP@k as the cluster score resulted in a slightly less effective performance. We also note that learning-to-""",null,null
175,336,null,null
176,AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF,null,null
177,MAP p@5 NDCG@5,null,null
178,MAP p@5 NDCG@5,null,null
179,MAP p@5 NDCG@5,null,null
180,MAP p@5 NDCG@5,null,null
181,MAP p@5 NDCG@5 MAP p@5 NDCG@5,null,null
182,MAP p@5 NDCG@5,null,null
183,MAP p@5 NDCG@5,null,null
184,Init,null,null
185,10.1 50.7 50.6,null,null
186,19.9 51.0 52.5,null,null
187,15.8 37.5 37.2,null,null
188,12.7 59.3 48.6,null,null
189,4.5 19.1 12.6 8.6 46.3 32.4,null,null
190,12.5 33.1 24.4,null,null
191,15.8 44.8 33.2,null,null
192,TunedMRF,null,null
193,9.9 48.7 49.4,null,null
194,20.0 51.0 52.7,null,null
195,15.4 36.9 35.3i,null,null
196,12.7 60.8,null,null
197,49.5 4.9i,null,null
198,21.1 15.6i,null,null
199,8.7 47.8 33.1 13.5i,null,null
200,35.5,null,null
201,27.0 16.3i 46.8 34.3,null,null
202,ClustMRF,null,null
203,10.8,null,null
204,53.0,null,null
205,54.4t 21.0it 52.4,null,null
206,54.7 18.0it 44.9it 42.8it 14.2it 70.1it 56.2it 6.3it 44.6it 29.4it,null,null
207,8.9,null,null
208,50.2,null,null
209,33.9 16.1it 48.7it 37.4it 17.0,null,null
210,48.5,null,null
211,36.9,null,null
212,"Table 2: The performance of ClustMRF and a tuned MRF (TunedMRF) when re-ranking the MRF initial list (Init). Boldface: the best result in a row. 'i' and 't' mark statistically significant differences with Init and TunedMRF, respectively.",null,null
213,"cluster are ordered based on their query similarity.) A ranking of documents in Dinit is created from the cluster ranking, which is performed for each cluster size k ( {5, 10, 20}), using the approach described above; k is then also set using cross validation by optimizing the MAP performance of the resulting document ranking. The train/test split for the first and second steps are the same -- i.e., the same train set used for learning the l's is the one used for setting the cluster size. As is the case for ClustMRF, the final document ranking induced by any reference comparison method is based on using cross validation to set free-parameter values; and, MAP serves as the optimization criterion in the training (learning) phase.",null,null
214,"Finally, we note that the main computational overhead, on top of the initial ranking, incurred by using ClustMRF is the clustering. That is, the feature functions used are either query-independent, and therefore can be computed offline; or, use mainly document-query similarity values that have already been computed to create the initial ranking. Clustering of a few dozen documents can be computed efficiently; e.g., based on document snippets.",null,null
215,4.2 Experimental results,null,null
216,4.2.1 Main result,null,null
217,"Table 2 presents our main result. Namely, the performance of ClustMRF when used to re-rank the MRF initial list. Recall that the initial ranking was induced using MRF's SDM with free-parameter values set following previous recommendations [28]. Thus, we also present for reference the re-ranking performance of using MRF's SDM with its three free parameters set using cross validation as is the case for",null,null
218,"rank methods [23] other than SVMrank, which proved to result in highly effective performance as shown below, can also be used for setting the values of the l weights.",null,null
219,ClustMRF,null,null
220,AP ROBUST WT10G GOV2,null,null
221,MAP p@5 NDCG@5,null,null
222,MAP p@5 NDCG@5,null,null
223,MAP p@5 NDCG@5,null,null
224,MAP p@5 NDCG@5,null,null
225,10.8 53.0 54.4 21.0 52.4 54.7 18.0 44.9 42.8 14.2 70.1 56.2,null,null
226,ClustMRF,null,null
227,ClueA ClueAF ClueB ClueBF,null,null
228,MAP p@5 NDCG@5,null,null
229,MAP p@5 NDCG@5,null,null
230,MAP p@5 NDCG@5,null,null
231,MAP p@5 NDCG@5,null,null
232,6.3 44.6 29.4 8.9 50.2 33.9 16.1 48.7 37.4 17.0 48.5 36.9,null,null
233,stdvqsim,null,null
234,9.4 43.7c 45.0c 19.0c 50.7 52.4 15.4c 38.4c 37.8c 12.7c 59.3c 48.2c,null,null
235,maxsw2,null,null
236,5.4c 28.7c 20.3c 8.6 47.2 32.5 14.2c 41.9c 30.1c 16.3 45.0 35.5,null,null
237,maxsw2,null,null
238,9.7 44.6c 45.8c 17.7c 46.9c 49.1c 12.2c 31.7c 28.6c 12.9c 62.3c 48.8c,null,null
239,maxsw1,null,null
240,5.3c 29.3c 20.5c 7.8c 40.4c 28.9c,null,null
241,15.4 42.9c 32.5c 15.7c 42.3c 32.8,null,null
242,geoqsim,null,null
243,10.6 50.9 52.0 20.6 50.4 52.4 16.3c 39.3c 39.0c 13.2c 58.0c 46.6c,null,null
244,maxqsim,null,null
245,4.5c 18.7c 12.4c 8.3 49.3 34.3,null,null
246,12.8c 33.9c 25.5c 14.8c 42.9c 32.8,null,null
247,minsw2,null,null
248,9.6 49.1 50.4 16.8c 44.7c 45.9c 14.2c 33.9c 32.4c,null,null
249,14.2,null,null
250,66.3 52.3,null,null
251,geoqsim,null,null
252,4.8c 20.9c 14.0c 8.6 48.7 33.9 12.9c 34.2c 25.6c 15.9 43.2 33.6,null,null
253,Table 3: Using each of ClustMRF's top-4 feature functions by itself for ranking the clusters so as to re-rank the MRF initial list. Boldface: the best performance per row. 'c' marks a statistically significant difference with ClustMRF.,null,null
254,"the free parameters of ClustMRF; TunedMRF denotes this method. We found that using exhaustive search for finding SDM's optimal parameter values in the training phase yields better performance (on the test set) than using SVMrank [12] and SVMmap [36]. Specifically, T , O, and U were set to values in {0, 0.05, . . . , 1} with T + O + U , 1.",null,null
255,"We first see in Table 2 that while TunedMRF outperforms the initial MRF ranking in most relevant comparisons (experimental setting × evaluation measure), there are cases (e.g., for AP and WT10G) for which the reverse holds. The latter finding implies that optimal free-parameter values of MRF's SDM do not necessarily generalize across queries.",null,null
256,"More importantly, we see in Table 2 that ClustMRF outperforms both the initial ranking and TunedMRF in all relevant comparisons. Many of the improvements are substantial and statistically significant. These findings attest to the high effectiveness of using ClustMRF for re-ranking.",null,null
257,4.2.2 Analysis of feature functions,null,null
258,"We now turn to analyze the relative importance attributed to the different feature functions used in ClustMRF; i.e., the l weights assigned to these functions in the training phase by SVMrank. We first average, per experimental setting and cluster size, the weights assigned to a feature function using the different training folds. Then, the feature function is assigned with a score that is the reciprocal rank of its corresponding (average) weight. Finally, the feature functions are ordered by averaging their scores across experimental settings and cluster sizes. Two feature functions, pr and spam, are only used for the ClueWeb-based settings. Hence, we perform the analysis separately for the ClueWeb and nonClueWeb (AP, ROBUST, WT10G, and GOV2) settings.",null,null
259,337,null,null
260,MAP,null,null
261,AP,null,null
262,p@5,null,null
263,NDCG@5,null,null
264,MAP,null,null
265,ROBUST p@5,null,null
266,NDCG@5,null,null
267,MAP,null,null
268,WT10G p@5,null,null
269,NDCG@5,null,null
270,GOV2,null,null
271,MAP p@5,null,null
272,NDCG@5,null,null
273,MAP,null,null
274,ClueA p@5,null,null
275,NDCG@5,null,null
276,MAP,null,null
277,ClueAF p@5,null,null
278,NDCG@5,null,null
279,MAP,null,null
280,ClueB,null,null
281,p@5,null,null
282,NDCG@5,null,null
283,MAP,null,null
284,ClueBF p@5,null,null
285,NDCG@5,null,null
286,Init,null,null
287,10.1 50.7 50.6,null,null
288,19.9c 51.0 52.5,null,null
289,15.8c 37.5c 37.2c 12.7c 59.3c 48.6c 4.5c 19.1c 12.6c 8.6 46.3 32.4,null,null
290,12.5c 33.1c 24.4c 15.8 44.8 33.2,null,null
291,Inter,null,null
292,10.4 55.9i,null,null
293,56.0i,null,null
294,20.8i 52.2 53.9,null,null
295,15.1c 38.0c 36.8c 12.9c 62.9c 50.2c 5.3c 24.3c 17.8c 8.9,null,null
296,44.8 32.6 14.9i 44.5i 34.3i,null,null
297,16.7 48.2 36.4,null,null
298,AMean,null,null
299,10.6 51.1,null,null
300,52.2,null,null
301,20.3c 49.1c 51.2c 16.6ic 39.6ic 38.5c 13.1ic 58.8c 47.8c,null,null
302,4.6c 19.3c 13.2c 8.8 49.8i 35.0i,null,null
303,13.0ic 34.7c 26.1ic 15.9 45.6 34.4,null,null
304,GMean,null,null
305,10.6 50.9 52.0 20.6i 50.4 52.4,null,null
306,16.3c 39.3c 39.0c 13.2ic 58.0c 46.6c,null,null
307,4.8c 20.9c 14.0c 8.6 48.7 33.9,null,null
308,12.9c 34.2c 25.6c,null,null
309,15.9 43.2 33.6,null,null
310,CRank,null,null
311,10.0 50.0,null,null
312,50.5,null,null
313,19.7c 46.6ic 49.1ic 14.5c 34.2c 32.7ic 12.7c 62.3c 48.4c,null,null
314,5.2c 24.3c 18.5ic 8.3,null,null
315,41.5c 30.0 16.0i 46.6i 35.3i 17.7i,null,null
316,50.3,null,null
317,38.0i,null,null
318,CMRF,null,null
319,10.8,null,null
320,53.0 54.4 21.0i,null,null
321,52.4,null,null
322,54.7 18.0i 44.9i 42.8i 14.2i 70.1i 56.2i 6.3i 44.6i 29.4i,null,null
323,8.9,null,null
324,50.2,null,null
325,33.9 16.1i 48.7i 37.4i,null,null
326,17.0 48.5 36.9,null,null
327,"Table 4: Comparison with cluster-based retrieval methods used for re-ranking the MRF initial list. (CMRF is a shorthand for ClustMRF.) Boldface marks the best result in a row. 'i' and 'c' mark statistically significant differences with the initial ranking and ClustMRF, respectively.",null,null
328,"For the non-ClueWeb settings, the feature functions, in descending order of attributed importance, are: stdv-qsim, max-sw2, geo-qsim, min-sw2, max-sw1, max-qsim, min-dsim, geo-sw2, min-icompress, min-qsim, min-sw1, geo-icompress, max-dsim, geo-dsim, max-icompress, geo-entropy, min-entropy, geo-sw1, max-entropy. For the ClueWeb settings the feature functions are ordered as follows: max-sw2, max-sw1, maxqsim, geo-qsim, max-spam, geo-sw2, min-icompress, minsw2, geo-sw1, min-sw1, min-qsim, stdv-qsim, max-pr, mindsim, min-entropy, max-entropy, min-spam, geo-icompress, geo-entropy, max-icompress, geo-spam, geo-pr, geo-dsim, minpr, max-dsim.",null,null
329,"Two main observations rise. First, each of the three types of cliques used in Section 2.1 for defining the MRF has at least one associated feature function that is assigned with a relatively high weight. For example, the geo-qsim function defined over lQD, the max-qsim function defined over lQC , and the max-sw2 function defined over lC , are among the 4, 6 and 2 most important functions in both cases (nonClueWeb and ClueWeb settings). Second, for the ClueWeb settings, the feature functions defined over the lC clique and which are based on query-independent document measures (e.g., max-sw1, max-sw2, max-spam) are attributed with high importance. In fact, among the top-10 feature functions for the ClueWeb settings only two (max-qsim and geoqsim) are not based on a query-independent measure. This is not the case for the non-ClueWeb settings where different statistics of the query-similarity values are among the top10 feature functions. We note that using some of the queryindependent document measures utilized here was shown in work on Web retrieval to be effective for ranking documents directly [3]. We demonstrated the merits of using such measures for ranking document clusters.",null,null
330,"In Table 3 we present the performance of using each of the top-4 feature functions (for the non-ClueWeb and ClueWeb settings) by itself as a cluster ranking method. As in Section 4.2.1, we use the cluster ranking to re-rank the MRF initial list. We see in Table 3 that in almost all relevant comparisons ClustMRF is more effective -- often to a substantial and statistically significant degree -- than using one of its top-4 feature functions alone. Thus, we conclude that ClustMRF's effective performance cannot be attributed to a single feature function that it utilizes.",null,null
331,"We also performed ablation tests as follows. ClustMRF was trained each time without one of its top-10 feature functions. This resulted in a statistically significant performance decrease with respect to at least one of the three evaluation metrics of concern (MAP, p@5 and NDCG@5) for all top-10 feature functions for the ClueWeb settings. (Actual numbers are omitted as they convey no additional insight.) Yet, there was no statistically significant performance decrease for any of the top-10 feature functions for the non-ClueWeb settings. These findings attest to the redundancy of feature functions when employing ClustMRF for the non-ClueWeb settings and to the lack thereof in the ClueWeb settings.",null,null
332,"Finally, we computed the Pearson correlation of the learned l's values (averaged over the train folds and cluster sizes) between experimental settings. We found that for pairs of non-ClueWeb settings, excluding AP, the correlation was at least 0.5; however, the correlation with AP was much smaller. For the ClueWeb settings, the correlation between ClueB and ClueBF was high (0.83) while that for other pairs of settings was lower than 0.5. Thus, we conclude that the learned l values can be collection, and setting, dependent.",null,null
333,4.2.3 Comparison with cluster-based methods,null,null
334,We next compare the performance of ClustMRF with that,null,null
335,of highly effective cluster-based retrieval methods. All meth-,null,null
336,ods re-rank the MRF initial list.,null,null
337,The InterpolationF method (Inter in short) [13] ranks,null,null
338,documents directly using the score function:,null,null
339,"Score(d; Q) d,ef (1 - )",null,null
340,"+ sim(Q,d)",null,null
341,"dDinit sim(Q,d )",null,null
342,". CC l(Dinit) sim(Q,C)sim(C,d)",null,null
343,"dDinit CC l(Dinit) sim(Q,C)sim(C,d )",null,null
344,This state-of-the-,null,null
345,art re-ranking method represents the class of approaches,null,null
346,"that use clusters to ""smooth"" document representations [13].",null,null
347,"In contrast to Inter, ClustMRF belongs to a class of meth-",null,null
348,"ods that rely on cluster ranking. Accordingly, the next ref-",null,null
349,erence comparison methods represent this class. Section 4.1,null,null
350,provided a description of how the cluster ranking is trans-,null,null
351,formed to a ranking of the documents in Dinit. The AMean,null,null
352,"method [26, 15], for example, scores cluster C by the arith-",null,null
353,metic mean of the query similarity values of its constituent,null,null
354,documents.,null,null
355,"Formally,",null,null
356,"Score(C; Q) d,ef",null,null
357,1 |C|,null,null
358,"dC sim(Q, d).",null,null
359,Scoring C by the geometric mean of the query-similarity,null,null
360,"values of its constituent documents, Score(C; Q) d,ef",null,null
361,"|C| dC sim(Q, d), was shown to yield state-of-the-art clus-",null,null
362,"ter ranking performance [15]. This approach, henceforth",null,null
363,"referred to as GMean, results from aggregating several fea-",null,null
364,ture functions (geo-qsim) that are used in our ClustMRF,null,null
365,method. (See Section 2.1 for details.),null,null
366,An additional state-of-the-art cluster ranking method is,null,null
367,ClustRanker (CRank in short) [15]. Cluster C is scored by,null,null
368,"Score(C; Q) d,ef (1 - )",null,null
369,"+ sim(Q,C)p(C)",null,null
370,"C C l(Dinit) sim(Q,C )p(C )",null,null
371,338,null,null
372,AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF,null,null
373,MAP p@5 NDCG@5,null,null
374,MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP,null,null
375,p@5,null,null
376,NDCG@5 MAP p@5 NDCG@5,null,null
377,MAP,null,null
378,p@5,null,null
379,NDCG@5,null,null
380,MAP,null,null
381,p@5,null,null
382,NDCG@5,null,null
383,DocMRF,null,null
384,9.9 50.7 51.0 20.3 52.1 54.0 17.1 42.0 40.4,null,null
385,15.0 66.3 54.0 9.8 42.4 28.4 9.5,null,null
386,52.6,null,null
387,35.7,null,null
388,16.6 45.6 33.6 17.6 50.3 37.5,null,null
389,ClustMRF,null,null
390,11.0 53.5 53.5 21.2d 53.2 55.3,null,null
391,17.7 42.5 40.3 15.3 68.7 55.8,null,null
392,10.0 49.3d 33.4d,null,null
393,9.5 49.6 35.7 18.9d 52.9d 39.9d 19.4d 55.3d 41.9d,null,null
394,Table 5: Using ClustMRF to re-rank the DocMRF [3] list. Boldface: best result in a row. 'd' marks a statistically significant difference with DocMRF.,null,null
395,"; dC sim(Q,d)sim(C,d)p(d)",null,null
396,"C C l(Dinit) dC sim(Q,d)sim(C ,d)p(d)",null,null
397,p(C),null,null
398,and,null,null
399,p(d),null,null
400,are,null,null
401,estimated based on inter-cluster and inter-document (across,null,null
402,"clusters) similarities, respectively. These similarities, com-",null,null
403,"puted using the language-model-based measure simLM (·, ·),",null,null
404,are not utilized by ClustMRF that uses inter-document sim-,null,null
405,ilarities only within a cluster.,null,null
406,"Following the original reports of Inter [13] and CRank [15],",null,null
407,"we estimate sim(Q, C) and sim(C, d) in these methods using",null,null
408,"simLM (·, ·); C is represented by the concatenation of its con-",null,null
409,"stituent documents. For a fair comparison with ClustMRF,",null,null
410,"sim(Q, d) is set in all reference comparisons considered here",null,null
411,"to simMRF (·, ·), which was used to create the initial MRF",null,null
412,list that is re-ranked.,null,null
413,All free parameters of the methods are set using cross val-,null,null
414,"idation. Specifically,  which is used by Inter and CRank",null,null
415,"is set to values in {0, 0.1, . . . , 1}. The graph out degree",null,null
416,and the dumping factor used by CRank are set to values,null,null
417,"in {4, 9, 19, 29, 39, 49} and {0.05, 0.1, . . . , 0.9, 0.95}, respec-",null,null
418,tively. The cluster size used by each method is selected from,null,null
419,"{5, 10, 20} as is the case for ClustMRF. Table 4 presents the",null,null
420,performance numbers.,null,null
421,We can see in Table 4 that in a vast majority of the rele-,null,null
422,vant comparisons ClustMRF outperforms the reference com-,null,null
423,parison methods. Many of the improvements are substantial,null,null
424,and statistically significant. In the few cases that ClustMRF,null,null
425,"is outperformed by one of the other methods, the perfor-",null,null
426,mance differences are not statistically significant.,null,null
427,4.2.4 Using ClustMRF to re-rank the DocMRF list,null,null
428,"Heretofore, we studied the performance of ClustMRF when used to re-rank the MRF initial list. The analysis presented in Section 4.2.2 demonstrated the effectiveness -- especially for the ClueWeb settings -- of using feature functions that utilize query-independent document measures. Thus, we now turn to explore ClustMRF's performance when employed over a document ranking that is already based on using query-independent document measures.",null,null
429,"To that end, we follow some recent work [3]. We re-rank the 1000 documents that are the most highly ranked by MRF's SDM that was used above to create the MRF initial list. Re-ranking is performed using an MRF model that is enriched with query-independent document measures [3]. We use the same document measures utilized by ClustMRF, except for dsim which is based on inter-document similarities and which was not considered in this past work that ranked documents independently of each other [3]. The resultant ranking, induced using SVMrank for learning parameter values, is denoted DocMRF. (SVMrank yielded better performance than SVMmap.) We then let ClustMRF rerank the top-50 documents. In doing so, we use the exponent of the score assigned by DocMRF to document d, which is a rank equivalent estimate to that of log p(Q, d), as the sim(Q, d) value used by ClustMRF. Thus, we maintain the invariant mentioned above that the scoring function used to induce the ranking upon which ClustMRF operates is rank equivalent to the document-query similarity measure used in ClustMRF. We note that ClustMRF is different from DocMRF in two important respects. First, by the virtue of ranking clusters first and transforming the ranking to that of documents rather than ranking documents directly as is the case in DocMRF. Second, by the completely different ways that document-query similarities are used.",null,null
430,Comparing the performance of DocMRF in Table 5 with that of the MRF initial ranking in Table 2 attests to the merits of using DocMRF for re-ranking. We can also see in Table 5 that applying ClustMRF over the DocMRF list results in performance improvements in almost all relevant comparisons. Many of the improvements for the ClueWeb settings are substantial and statistically significant.,null,null
431,4.2.5 Using ClustMRF to re-rank the LM list,null,null
432,"The third list we re-rank using ClustMRF is LM, which was created using unigram language models. For reference comparison we use the cluster-based Inter method which was used in Section 4.2.3. Experiments show -- actual numbers are omitted due to space considerations -- that for reranking the LM list, the GMean cluster ranking method is more effective in most relevant comparisons than the other two cluster ranking methods used in Section 4.2.3 for reference comparison (AMean and CRank). Hence, GMean is used here as an additional reference comparison.",null,null
433,"ClustMRF, Inter and GMean use the simLM (·, ·) similarity measure, which was used for inducing the initial ranking, for sim(Q, d). All other implementation details are the same as those described above. As a result, ClustMRF, as well as Inter and GMean, use only unigram language models in the LM setting considered here. This is in contrast to the MRF-list setting considered above where term-proximities information was used.",null,null
434,"An additional reference comparison that uses unigram language models is relevance model number 3 [1], RM3, which is a state-of-the-art query expansion approach. RM3 is also used to re-rank the LM list. All (50) documents in the list are used for constructing RM3. Its free-parameter values are set using cross validation. Specifically, the number of expansion terms and the interpolation parameter that controls the reliance on the original query are set to values in {5, 10, 25, 50} and {0.1, 0.3, . . . , 0.9}, respectively. Dirichletsmoothed language models are used with µ , 1000.",null,null
435,339,null,null
436,MAP,null,null
437,AP,null,null
438,p@5,null,null
439,NDCG@5,null,null
440,MAP,null,null
441,ROBUST p@5,null,null
442,NDCG@5,null,null
443,MAP WT10G p@5,null,null
444,NDCG@5,null,null
445,MAP,null,null
446,GOV2 p@5,null,null
447,NDCG@5,null,null
448,MAP,null,null
449,ClueA p@5,null,null
450,NDCG@5,null,null
451,MAP ClueAF p@5,null,null
452,NDCG@5,null,null
453,MAP,null,null
454,ClueB,null,null
455,p@5,null,null
456,NDCG@5,null,null
457,MAP,null,null
458,ClueBF p@5,null,null
459,NDCG@5,null,null
460,Init,null,null
461,9.9 49.6 49.9 19.3c 49.5c 51.6c 15.0,null,null
462,36.4c 35.8 11.8c 56.6c 46.5c 3.3c 16.1c 10.7c 8.0c 47.4 32.3 11.4c 29.0c 21.2c 14.7c 42.9c 32.1c,null,null
463,Inter,null,null
464,10.6i 56.1ic 55.6i,null,null
465,20.1i 50.9,null,null
466,53.1 14.9 37.5 37.1 12.6ic 62.4ic 50.4i 5.0i 24.6ic 17.9ic 8.5i 46.7 32.6 13.8ic 40.5i 29.6i,null,null
467,15.6,null,null
468,46.3,null,null
469,34.6,null,null
470,GMean RM3,null,null
471,10.8i 9.9,null,null
472,50.7 49.1,null,null
473,51.8 20.6i,null,null
474,52.1 53.8 14.9,null,null
475,49.3 19.7ic 49.7c,null,null
476,52.1c 14.5,null,null
477,37.5 35.5 12.4ic 60.8ic 48.8c 3.7ic 17.2c,null,null
478,11.5c,null,null
479,8.2 45.7 32.3 12.0ic 31.6ic 23.4ic 15.5,null,null
480,43.4,null,null
481,33.4c,null,null
482,36.6c 35.9 12.7ic 60.4ic 49.1c 3.8ic 17.4c,null,null
483,11.0c 8.7i,null,null
484,47.6 34.3 13.9ic 40.2i 30.0i 16.4i 48.9i 36.6i,null,null
485,ClustMRF,null,null
486,10.5 51.3 51.7 20.5i 52.9i 55.6i,null,null
487,14.6 42.2i,null,null
488,39.3 13.5i 68.4i 54.3i 5.5i 43.3i 27.7i 8.7i,null,null
489,51.5,null,null
490,35.6 16.0i 46.0i 34.8i 16.8i 49.2i 38.7i,null,null
491,"Table 6: Re-ranking the LM initial list. Boldface: the best result in a row. 'i' and 'c' mark statistically significant differences with the initial ranking and ClustMRF, respectively.",null,null
492,"We see in Table 6 that ClustMRF outperforms the reference comparisons in a vast majority of the relevant comparisons. Many of the improvements are substantial and statistically significant. These results, along with those presented in Sections 4.2.1 and 4.2.4, attest to the effectiveness of using ClustMRF to re-rank different initial lists.",null,null
493,4.2.6 Varying the clustering algorithm,null,null
494,"Thus far, we used ClustMRF and the reference compar-",null,null
495,isons with nearest-neighbor (NN) clustering. In Table 7 we,null,null
496,present the retrieval performance of using hierarchical ag-,null,null
497,glomerative clustering (HAC) with the complete link mea-,null,null
498,sure. This clustering was shown to be among the most ef-,null,null
499,fective hard clustering methods for cluster-based retrieval,null,null
500,"[24, 13].",null,null
501,We,null,null
502,use,null,null
503,"1 simLM (d1 ,d2)",null,null
504,+,null,null
505,"1 simLM (d2,d1)",null,null
506,for an inter-,null,null
507,"document dissimilarity measure; and, cut the clustering den-",null,null
508,drogram so that the resultant average cluster size is the clos-,null,null
509,"est to a value k ( {5, 10, 20}). Doing so somewhat equates",null,null
510,the comparison terms with using the NN clusters whose size,null,null
511,"is in {5, 10, 20}. Cross validation is used in all cases for",null,null
512,setting the value of k.,null,null
513,The MRF initial list is clustered and serves as the ba-,null,null
514,sis for re-ranking. Experiments show (actual numbers are,null,null
515,omitted due to space considerations) that among the three,null,null
516,cluster ranking methods which were used above for refer-,null,null
517,"ence comparison (AMean, GMean, and CRank) CRank is",null,null
518,"the most effective when using HAC. Hence, CRank serves",null,null
519,as a reference comparison here.,null,null
520,We see in Table 7 that in the majority of relevant com-,null,null
521,"parisons, ClustMRF improves over the initial ranking when",null,null
522,"using HAC. In contrast, CRank is outperformed by the ini-",null,null
523,"tial ranking in most relevant comparisons for HAC. Indeed,",null,null
524,ClustMRF outperforms CRank in most cases for both NN,null,null
525,and HAC. We also see that ClustMRF is (much) more effec-,null,null
526,tive when using the overlapping NN clusters than the hard,null,null
527,AP ROBUST WT10G GOV2 ClueA ClueAF ClueB ClueBF,null,null
528,MAP p@5 NDCG@5,null,null
529,MAP p@5 NDCG@5,null,null
530,MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5 MAP p@5 NDCG@5,null,null
531,MAP p@5 NDCG@5,null,null
532,MAP p@5 NDCG@5,null,null
533,Init,null,null
534,10.1 50.7 50.6 19.9 51.0 52.5 15.8 37.5 37.2 12.7 59.3 48.6 4.5 19.1 12.6 8.6 46.3 32.4 12.5 33.1 24.4 15.8 44.8 33.2,null,null
535,HAC,null,null
536,NN,null,null
537,CRank ClustMRF CRank ClustMRF,null,null
538,9.9,null,null
539,49.8,null,null
540,50.5,null,null
541,19.1 50.1 51.7 14.8 36.6 34.4 13.2i 61.5 49.7 5.6i 23.7 16.9i 8.4 43.9 32.0 14.4i 39.5i 30.6i,null,null
542,15.3,null,null
543,43.9,null,null
544,32.7,null,null
545,9.6i 46.5i 46.8i 19.6,null,null
546,50.4,null,null
547,51.9,null,null
548,15.8,null,null
549,38.2,null,null
550,37.0 13.6i,null,null
551,63.9,null,null
552,51.5 5.8i 31.7ic 21.0i,null,null
553,9.2 48.9 33.4 14.5i 39.7i 30.3i 15.2 43.1 32.5,null,null
554,10.0 50.0 50.5,null,null
555,19.7 46.6i 49.1i,null,null
556,14.5 34.2 32.7i,null,null
557,12.7 62.3 48.4,null,null
558,5.2 24.3 18.5i 8.3 41.5 30.0 16.0i 46.6i 35.3i 17.7i,null,null
559,50.3,null,null
560,38.0i,null,null
561,10.8,null,null
562,53.0,null,null
563,54.4 21.0ic 52.4c 54.7c 18.0ic 44.9ic 42.8ic 14.2ic 70.1ic 56.2ic 6.3ic 44.6ic 29.4ic 8.9,null,null
564,50.2c 33.9 16.1i 48.7i 37.4i,null,null
565,17.0 48.5 36.9,null,null
566,"Table 7: Using nearest-neighbor clustering (NN) vs. (complete link) hierarchical agglomerative clustering (HAC). The MRF initial list is used. Boldface: the best result in a row per clustering algorithm; underline: the best result in a row. 'i' and 'c': statistically significant differences with the initial ranking and CRank, respectively.",null,null
567,"clusters created by HAC. The improved effectiveness of using NN in comparison to HAC echoes findings in previous work on cluster-based re-ranking [13]. For CRank, the performance of using neither NN nor HAC dominates that of using the other.",null,null
568,4.2.7 The effect of the size of the initial list,null,null
569,"Until now, ClustMRF and all reference comparison methods were used to re-rank an initial list of 50 documents. Using a short list follows common practice in work on clusterbased re-ranking [18, 25, 26, 13] as was mentioned in Section 4.1. We now turn to study ClustMRF's performance when re-ranking longer lists. To that end, we use for the initial list the n ( {50, 100, 250, 500}) documents that are the most highly ranked by MRF's SDM [28] which was used above for creating the MRF initial list. For reference comparisons we use TunedMRF (see Section 4.2.1); and, the AMean and GMean cluster ranking methods described in Section 4.2.3. Nearest-neighbor clustering is used.",null,null
570,"We see in Figure 2 that in almost all cases -- i.e., experimental settings and values of n -- ClustMRF outperforms both the initial ranking and TunedMRF; often, the performance differences are quite substantial. Furthermore, in most cases (with the notable exception of AP) ClustMRF outperforms AMean and GMean.",null,null
571,4.2.8 Diversifying search results,null,null
572,"We next explore how ClustMRF can be used to improve the performance of search-results diversification approaches. Specifically, we use the MMR [5] and the state-of-the-art xQuAD [29] diversification methods.",null,null
573,340,null,null
574,MAP,null,null
575,13.5 13.0 12.5 12.0 11.5 11.0 10.5 10.0 9.5,null,null
576,50 100,null,null
577,11.0 10.0 9.0 8.0 7.0 6.0 5.0 4.0,null,null
578,50 100,null,null
579,AP,null,null
580,Init TunedMRF,null,null
581,AMean GMean ClustMRF,null,null
582,250,null,null
583,500,null,null
584,n,null,null
585,ClueA,null,null
586,Init TunedMRF,null,null
587,AMean GMean ClustMRF,null,null
588,250,null,null
589,500,null,null
590,n,null,null
591,MAP,null,null
592,MAP,null,null
593,23.0 22.5 22.0 21.5 21.0 20.5 20.0 19.5,null,null
594,50 100,null,null
595,10.0,null,null
596,9.5,null,null
597,9.0,null,null
598,8.5,null,null
599,8.0,null,null
600,7.5 50 100,null,null
601,ROBUST,null,null
602,Init TunedMRF,null,null
603,AMean GMean ClustMRF,null,null
604,250,null,null
605,500,null,null
606,n,null,null
607,ClueAF,null,null
608,Init TunedMRF,null,null
609,AMean GMean ClustMRF,null,null
610,250,null,null
611,500,null,null
612,n,null,null
613,MAP,null,null
614,MAP,null,null
615,19.0 18.5 18.0 17.5 17.0 16.5 16.0 15.5 15.0,null,null
616,50 100,null,null
617,20.0 19.0 18.0 17.0 16.0 15.0 14.0 13.0 12.0 11.0,null,null
618,50 100,null,null
619,WT10G,null,null
620,Init TunedMRF,null,null
621,AMean GMean ClustMRF,null,null
622,250,null,null
623,500,null,null
624,n,null,null
625,ClueB,null,null
626,Init TunedMRF,null,null
627,AMean GMean ClustMRF,null,null
628,250,null,null
629,500,null,null
630,n,null,null
631,MAP,null,null
632,MAP,null,null
633,18.0 17.0 16.0 15.0 14.0 13.0 12.0,null,null
634,50 100,null,null
635,19.0 18.0 17.0 16.0 15.0 14.0,null,null
636,50 100,null,null
637,GOV2,null,null
638,Init TunedMRF,null,null
639,AMean GMean ClustMRF,null,null
640,250,null,null
641,500,null,null
642,n,null,null
643,ClueBF,null,null
644,Init TunedMRF,null,null
645,AMean GMean ClustMRF,null,null
646,250,null,null
647,500,null,null
648,n,null,null
649,MAP,null,null
650,Figure 2: The effect on MAP(@50) performance of the size n of the MRF initial list that is re-ranked.,null,null
651,ClueA ClueAF ClueB ClueBF,null,null
652,-NDCG ERR-IA P-IA -NDCG ERR-IA P-IA -NDCG ERR-IA P-IA -NDCG ERR-IA P-IA,null,null
653,Init,null,null
654,24.5 16.0 11.8 42.6 32.0 21.0 33.2 21.1 15.4 41.6 29.7 18.9,null,null
655,MRF,null,null
656,26.2c 17.3c 10.3c 42.9 32.3 20.2c 33.6c 21.3c 14.4ic 42.6ic 30.2ic 18.4,null,null
657,MMR QClust ClustMRF,null,null
658,25.4c,null,null
659,17.5c 9.6ic 39.0ic 29.8c 14.9ic,null,null
660,33.9c,null,null
661,21.5c 12.8ic 38.7ic 27.0ic 14.5ic,null,null
662,38.7i 30.5i 16.7i,null,null
663,43.8,null,null
664,34.2 17.6i 43.7i 32.0i 17.4i 45.4i 33.3i,null,null
665,17.8,null,null
666,MRF,null,null
667,27.4ic 17.9ic 13.3c 44.3i 33.4i,null,null
668,21.0,null,null
669,39.7ic 25.9ic 19.4ic 46.1ic 33.2i 21.4ic,null,null
670,xQuAD QClust ClustMRF,null,null
671,28.9ic 19.6ic 13.6ic 43.7,null,null
672,33.1,null,null
673,38.8i 30.6i 17.2i 45.5i 34.9i,null,null
674,20.0,null,null
675,39.3ic 25.3ic 19.2ic 44.2ic 31.2c 20.9ic,null,null
676,20.6 45.5i 32.9i 21.0i,null,null
677,48.1i 34.8i 22.0i,null,null
678,"Table 8: Diversifying search results. Underline and boldface mark the best result in a row, and per diversification method in a row, respectively. 'i' and 'c' mark statistically significant differences with the initial ranking (Init) and ClustMRF, respectively. The MRF initial list is used.",null,null
679,MMR and xQuAD iteratively re-rank an initial list Dinit. In each iteration the document in Dinit \ S assigned with the highest score is added to the set S; S is empty at the beginning. The final ranking is determined by the order of insertion to S.,null,null
680,"The score MMR assigns to document d ( Dinit \ S) is sim1(Q, d)-(1-) maxdiS sim2(d, di);  is a free parameter; sim1(·, ·) and sim2(·, ·) are discussed below. In contrast to MMR, xQuAD uses information about Q's subtopics, T (Q), and assigns d with the score p(d|Q)+",null,null
681,"(1 - ) tT (Q) p(t|Q)p(d|t) diS (1 - p(di|t)) ; p(t|Q) is the relative importance of subtopic t with respect to Q; p(d|Q) and p(d|t) are the estimates of d's relevance to Q and t, respectively.",null,null
682,The parameter  controls in both methods the tradeoff between using relevance estimation and applying diversification. Our focus is on improving the former and evaluating the resulting (diversification based) performance. This was also the case in previous work that used cluster ranking,null,null
683,"for results diversification [11]. Hence, this work serves for",null,null
684,reference comparison below.8,null,null
685,"We study three different estimates for sim1(Q, d) (used in MMR) which we also use for p(d|Q) (used in xQuAD).9",null,null
686,"The first, simMRF (Q, d), is that employed in the evalua-",null,null
687,tion above to create the MRF initial list that is also used,null,null
688,here for re-ranking. (Further details are provided below.),null,null
689,The next two estimates are based on applying cluster rank-,null,null
690,ing and transforming it to document ranking using the ap-,null,null
691,proach,null,null
692,described,null,null
693,in Section,null,null
694,4.1.,null,null
695,In these,null,null
696,"cases,",null,null
697,1 r(d),null,null
698,serves,null,null
699,"for sim1(Q, d), where r(d) is the rank of d in the document",null,null
700,result list produced by using the cluster ranking method.,null,null
701,"The first cluster ranking method is ClustMRF. The second,",null,null
702,"QClust, was used in the work mentioned above on utilizing",null,null
703,"cluster ranking for results diversification [11]. Specifically,",null,null
704,"cluster C is scored by simLM (Q, C) (see Section 4.1 for de-",null,null
705,"8There is work on using information induced from clusters for the diverisification itself (e.g., [21]). Using ClustMRF for cluster ranking in these approaches is future work. 9For scale compatibility, the two resultant quantities that are interpolated (using ) in MMR and xQuAD are sum normalized with respect to all documents in Dinit before the interpolation is performed.",null,null
706,341,null,null
707,"tails of simLM (·, ·)); C is represented by the concatenation",null,null
708,of its documents.,null,null
709,We use MMR and xQuAD to re-rank the MRF initial,null,null
710,"list that contains 50 documents. simLM (·, ·) serves for the",null,null
711,"sim2(·, ·) measure used in MMR and for p(d|t) that is used in",null,null
712,"xQuAD. The official TREC subtopics, which are available",null,null
713,"for the ClueWeb settings that we use here, were used for",null,null
714,"experiments. Following the findings in [29], we set p(t|Q) d,ef",null,null
715,|T,null,null
716,1 (Q)|,null,null
717,.,null,null
718,"The value of  is selected from {0.1, 0.2, . . . , 0.9}",null,null
719,using cross validation; -NDCG (@20) is the optimization,null,null
720,"metric. In addition to -NDCG (@20), ERR-IA (@20) and",null,null
721,P-IA (@20) are used for evaluation.,null,null
722,Table 8 presents the results. We see that using the MRF,null,null
723,similarity measure in MMR and xQuAD outperforms the ini-,null,null
724,"tial ranking, which was created using this measure, in most",null,null
725,relevant comparisons. This attests to the diversification ef-,null,null
726,fectiveness of MMR and xQuAD. Using QClust outperforms,null,null
727,"the initial ranking in most cases, but is consistently out-",null,null
728,performed by using the MRF measure and our ClustMRF,null,null
729,"method. More generally, the best performance for each di-",null,null
730,versification method (MMR and xQuAD) is almost always,null,null
731,"attained by ClustMRF, which often outperforms the other",null,null
732,methods in a substantial and statistically significant man-,null,null
733,"ner. Thus, although ClustMRF ranks clusters of similar",null,null
734,"documents, using the resultant document ranking can help",null,null
735,to much improve results-diversification performance.,null,null
736,5. CONCLUSIONS,null,null
737,We presented a novel approach to ranking (query specific) document clusters by their presumed relevance to the query. Our approach uses Markov Random Fields that enable the integration of various types of cluster-relevance evidence. Empirical evaluation demonstrated the effectiveness of using our approach to re-rank different initially retrieved lists. The approach also substantially outperforms state-of-the-art cluster ranking methods and can be used to substantially improve the performance of results diversification methods.,null,null
738,6. ACKNOWLEDGMENTS,null,null
739,We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center.,null,null
740,7. REFERENCES,null,null
741,"[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proc. of TREC-13, 2004.",null,null
742,"[2] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proc. of TREC-9, 2000.",null,null
743,"[3] M. Bendersky, W. B. Croft, and Y. Diao. Quality-biased ranking of web documents. In Proc. of WSDM, pages 95­104, 2011.",null,null
744,"[4] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. In Proc. of WWW, pages 107­117, 1998.",null,null
745,"[5] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, pages 335­336, 1998.",null,null
746,"[6] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.",null,null
747,"[7] W. B. Croft. A model of cluster searching based on classification. Information Systems, 5:189­195, 1980.",null,null
748,"[8] D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey. Scatter/Gather: A cluster-based approach to browsing large document collections. In Proc. of SIGIR, pages 318­329, 1992.",null,null
749,"[9] D. Fetterly, M. Manasse, and M. Najork. Spam, damn spam, and statistics: Using statistical analysis to locate spam web pages. In Proc. of WebDB, pages 1­6, 2004.",null,null
750,"[10] N. Fuhr, M. Lechtenfeld, B. Stein, and T. Gollub. The optimum clustering framework: implementing the cluster hypothesis. Information Retrieval Journal, 15(2):93­115, 2012.",null,null
751,"[11] J. He, E. Meij, and M. de Rijke. Result diversification based on query-specific cluster ranking. JASIST, 62(3):550­571, 2011.",null,null
752,"[12] T. Joachims. Training linear svms in linear time. In Proc. of KDD, pages 217­226, 2006.",null,null
753,"[13] O. Kurland. Re-ranking search results using language models of query-specific clusters. Journal of Information Retrieval, 12(4):437­460, August 2009.",null,null
754,"[14] O. Kurland and C. Domshlak. A rank-aggregation approach to searching for optimal query-specific clusters. In Proc. of SIGIR, pages 547­554, 2008.",null,null
755,"[15] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research (JAIR), 41:367­395, 2011.",null,null
756,"[16] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In Proc. of SIGIR, pages 194­201, 2004.",null,null
757,"[17] O. Kurland and L. Lee. PageRank without hyperlinks: Structural re-ranking using links induced by language models. In Proc. of SIGIR, pages 306­313, 2005.",null,null
758,"[18] O. Kurland and L. Lee. Respect my authority! HITS without hyperlinks utilizing cluster-based language models. In Proc. of SIGIR, pages 83­90, 2006.",null,null
759,"[19] O. Kurland, F. Raiber, and A. Shtok. Query-performance prediction and cluster ranking: Two sides of the same coin. In Proc. of CIKM, pages 2459­2462, 2012.",null,null
760,"[20] K.-S. Lee, Y.-C. Park, and K.-S. Choi. Re-ranking model based on document clusters. Inf. Process. Manage., 37(1):1­14, 2001.",null,null
761,"[21] T. Leelanupab, G. Zuccon, and J. M. Jose. When two is better than one: A study of ranking paradigms and their integrations for subtopic retrieval. In Proc. of AIRS, pages 162­172, 2010.",null,null
762,"[22] A. Leuski. Evaluating document clustering for interactive information retrieval. In Proc. of CIKM, pages 33­40, 2001.",null,null
763,"[23] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",null,null
764,"[24] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proc. of SIGIR, pages 186­193, 2004.",null,null
765,"[25] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",null,null
766,"[26] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proc. of ECIR, pages 454­462, 2008.",null,null
767,"[27] D. Metzler. A feature-centric view of information retrieval. Springer, 2011.",null,null
768,"[28] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.",null,null
769,"[29] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, pages 881­890, 2010.",null,null
770,"[30] J. Seo and W. B. Croft. Geometric representations for multiple documents. In Proc. of SIGIR, pages 251­258, 2010.",null,null
771,"[31] J. G. Shanahan, J. Bennett, D. A. Evans, D. A. Hull, and J. Montgomery. Clairvoyance Corporation experiments in the TREC 2003. High accuracy retrieval from documents (HARD) track. In Proc. of TREC-12, pages 152­160, 2003.",null,null
772,"[32] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Inf. Process. Manage., 38(4):559­582, 2002.",null,null
773,"[33] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.",null,null
774,"[34] E. M. Voorhees. The cluster hypothesis revisited. In Proc. of SIGIR, pages 188­196, 1985.",null,null
775,"[35] P. Willett. Query specific automatic document classification. International Forum on Information and Documentation, 10(2):28­32, 1985.",null,null
776,"[36] Y. Yue, T. Finley, F. Radlinski, and T. Joachims. A support vector method for optimizing average precision. In Proc. of SIGIR, pages 271­278, 2007.",null,null
777,"[37] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.",null,null
778,342,null,null
779,,null,null

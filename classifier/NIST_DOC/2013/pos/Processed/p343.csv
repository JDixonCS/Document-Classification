,sentence,label,data
0,A Novel TF-IDF Weighting Scheme for Effective Ranking,null,null
1,Jiaul H. Paik,null,null
2,"Indian Statistical Institute, Kolkata, India",null,null
3,jia.paik@gmail.com,null,null
4,ABSTRACT,null,null
5,"Term weighting schemes are central to the study of information retrieval systems. This article proposes a novel TF-IDF term weighting scheme that employs two different within document term frequency normalizations to capture two different aspects of term saliency. One component of the term frequency is effective for short queries, while the other performs better on long queries. The final weight is then measured by taking a weighted combination of these components, which is determined on the basis of the length of the corresponding query.",null,null
6,Experiments conducted on a large number of TREC news and web collections demonstrate that the proposed scheme almost always outperforms five state of the art retrieval models with remarkable significance and consistency. The experimental results also show that the proposed model achieves significantly better precision than the existing models.,null,null
7,Categories and Subject Descriptors,null,null
8,H.3.3 [Information Systems]: Information Search and Retrieval : Retrieval Models,null,null
9,General Terms,null,null
10,"Algorithm, Experimentation, Performance",null,null
11,Keywords,null,null
12,"Document ranking, Retrieval model, Term weighting",null,null
13,1. INTRODUCTION,null,null
14,"Term weighting schemes are the central part of an information retrieval system. Effectiveness of IR systems are thus crucially dependent on the underlying term weighting mechanism. Almost all retrieval models integrate three major variables to determine the degree of importance of a term for a document: (i) within document term frequency, (ii) document length and (iii) the specificity of the term in the",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
16,"collection. Term frequency and document length combination is used to infer the saliency of a term in a document, and when a query contains more than one term, term specificity is used to reward the documents that contain the terms rare in the collection.",null,null
17,"Retrieval models can be broadly classified into two major families based on their term weight estimation principle. Vector space model casts queries and documents as finite dimensional vectors, where the weight of an individual component is computed using numerous variations of tf-idf scores. On the other hand, probabilistic models [16, 17] primarily focus on estimating the probabilities of the terms in the documents, and the estimation techniques differ from one approach to the other. But in essence all of them use the same basic principles that we have outlined before.",null,null
18,"Most of the existing models (possibly all) employ a single term frequency normalization mechanism that does not take into account various aspects of a term's saliency in a document. For example, frequency of a term in a document relative to the frequency of the other terms in the same document gives us an important clue that can not be achieved by the commonly used document length based normalization scheme. On the contrary, length based normalization can restrict the likelihood of retrieval of extremely long documents which can not be taken care of by the relative frequency based term weighting.",null,null
19,"Another major limitation of the present models is that they do not balance well in preferring short and long documents. Such limitation makes a system to retrieve low quality documents at the top of the ranked list when they face queries of varying length. For example, in pivoted document length normalization scheme, if the parameter is set to a smaller value, it performs better for shorter queries, and when the parameter value is larger, longer queries are benefited more than the shorter queries [10]. Similar observation can be made for other models such as BM25, language model or relatively recent divergence from randomness based models [13, 10].",null,null
20,"The main reason is that when the parameter is set to a static value, most of the models prefer either short documents or long documents. If a weighting scheme prefers long documents, it pulls up extremely long documents when longer queries are encountered, since the longer documents have higher verbosity level it matches more query terms[28]. On the other direction, preference of short documents may degrade the overall retrieval performance, since it violates the likelihood of relevance versus retrieval pattern suggested by Singhal et al. [28].",null,null
21,343,null,null
22,"This article proposes a term weighting scheme that can address these weaknesses in an effective way. In particular, we argue that the two aspects of term frequencies, when combined appropriately, leads to significant performance benefit. In this article we make the following contributions.",null,null
23,"· It introduces a two aspect term frequency normalization scheme, that combines relative tf weighting and the tf normalization based on document length. One component of the term frequency tends to prefer long documents, while the other component prefers short documents and therefore, it maintains a good balance in preferring short and long documents.",null,null
24,"· It uses the query length information to emphasize the appropriate component. In particular, when the system faces a long query, it down-weights the part that prefers long documents in order to compensate the effect and vice versa.",null,null
25,· It modifies the usual term discrimination measure (idf) by integrating mean term frequency of a term in the set of documents the term is contained in.,null,null
26,"· Finally, we use asymptotically bounded function (similar to Robertson and Walker [22]) to transform the tf factors that better handles the term coverage issues in the documents and also helps to combine the two tf factors more easily. As a bi-product of such transformation, the ranking function easily produces the similarity values in the range of [0-1].",null,null
27,"In order to assess the effectiveness of the proposed model we carry out a set of experiments on a large number of standard test collections containing news and web data. The experimental results show that the proposed weighting function consistently and significantly outperforms five state of the art retrieval models (from vector space as well as probabilities families) measured in terms of the standard metrics such as MAP and NDCG. The experimental outcomes also attest that the model achieves significantly better precision than all the other models when measured in terms of a recently popularized metric, namely, expected reciprocal rank (ERR) [6].",null,null
28,"The remainder of the article is organized as follows. In Section 2 we review the state of the art. Section 3 describes the proposed weighing scheme. Description about the test collections, evaluation metrics and the details of the baselines are given in Section 4. Experimental results are presented in Section 5, where we compare the performance of the proposed model with the state of the art vector space models, followed by the comparison with the probabilistic models. Finally, we conclude in Section 6.",null,null
29,2. STATE OF THE ART,null,null
30,"Information retrieval systems, when encounter a query, tries to rank documents by their likelihood of relevance. Most IR systems assign a numeric score to the documents and then they are ranked based on these scores. Three widely used models in IR are the vector space model [26, 25], the probabilistic models [21] and the inference network based model [30]. In this section we review some of the state of the art models.",null,null
31,"In vector space model, queries and documents are represented as the vector of terms. To compute a score between a",null,null
32,document and a query the model measures the similarity be-,null,null
33,tween the query and document vector using cosine function.,null,null
34,The central part of the vector space model is to determine,null,null
35,the weight of the terms that are present in the query and,null,null
36,the documents. Three main factors that come into play to,null,null
37,compute the weight of a term are: (i) frequency of the term,null,null
38,"in the document, (ii) document frequency of the term in the",null,null
39,collection (first proposed in [29]) and (iii) the length of the,null,null
40,document that contains the term. Fang et al. [10] give a,null,null
41,comprehensive analysis of four retrieval models by defining,null,null
42,a set of constraints that needs to be satisfied for effective,null,null
43,retrieval. Using these constraints the strengths and weak-,null,null
44,nesses of some well known models are analyzed and some,null,null
45,of the models are modified. There are also a number of re-,null,null
46,cent works that focus on the constraint based analysis of the,null,null
47,"retrieval models [8, 9].",null,null
48,Salton and Buckley [24] summarize a number of term,null,null
49,weighting approaches which use various types of normalization. It is evident that document length is an important,null,null
50,component in effective term weighting. Singhal et al. [28],null,null
51,identify a number of weaknesses of cosine and maximum tf,null,null
52,normalization and they observe that a weighting formula,null,null
53,"that retrieves documents with chances similar to their probability of relevance performs better. Following this observation, they propose a pivoted normalization scheme that",null,null
54,acts as a correction factor of old normalization and is one,null,null
55,of the most effective term weighting schemes in the vector,null,null
56,space framework. The pivoted length normalization scheme,null,null
57,computes the term weight as follows [27]:,null,null
58,X,null,null
59,tQD,null,null
60,1,null,null
61,+,null,null
62,"ln(1 + ln(T F (t, D)))",null,null
63,1,null,null
64,-,null,null
65,s,null,null
66,+,null,null
67,s,null,null
68,len(D) ADL(C),null,null
69,·,null,null
70,"T F (t, Q)",null,null
71,·,null,null
72,ln N + 1 df (t),null,null
73,(1),null,null
74,"The parameter s controls the extent of normalization with respect to the document. Typically, the term weighting functions in vector space model are designed heuristically, which are based on the researchers experience. Several work tried to use the data to learn the patterns that satisfy the data. For example, Greiff [11] uses exploratory data analysis to uncover some important relationship between the document frequency and the relevance of a document.",null,null
75,The key part of the probabilistic models is to estimate the probability of relevance of the documents for a query. This is where most probabilistic model differs from one another. Binary independence model is perhaps the most widely accepted technique in the classical probabilistic model. A number of weighting formula have been developed and BM25 [20] has been the most effective among the formulae. The major differences between BM25 and the other commonly used TFIDF models are the slightly variant IDF formulation and the use of the query term frequency. The length normalization factor uses the average document length and a parameter has been introduced to control the relative length effect.,null,null
76,"Probabilistic language modeling technique [19, 14] is another effective ranking model that is widely used today. Typically, language modeling approaches compute the probability of generating a query from a document, assuming that the query terms are chosen independently. Unlike TFIDF models, language modeling approaches do not explicitly use document length factor and the idf component. It seems that the length of the document is an integral part of this formula and that automatically takes care of the length normalization issue. However, smoothing is crucial and it has very similar effect as the parameter that controls the",null,null
77,344,null,null
78,"length normalization components in pivoted normalization or BM25 model. Three major smoothing techniques (Dirichlet, Jelinek-Mercer and Two-stage) are commonly used in this model [31].",null,null
79,"Relatively recent, another probabilistic model is proposed in [3] that computes the weight of a term by measuring the informative content of a term by computing the amount of divergence of the term frequency distribution from the distribution based on a random process. Like most of the well known models, they also use the same basic components. However, the integration of various component are derived theoretically. This family of formula also uses the average document length as an ideal length of the documents and the term frequencies are normalized with respect to the average document length.",null,null
80,"In inference network, document retrieval is modeled as an inference process [30]. A document instantiates a term with a certain strength and given a query the credit from multiple terms is accumulated to compute a relevance that is very equivalent to the similarity score of vector space model. From an operational angle, the strength of instantiation of a term for a document can be considered as weight of the term in a document. The strength of instantiation of a term can be computed using any reasonable formula.",null,null
81,3. PROPOSED WORK,null,null
82,3.1 Preliminaries,null,null
83,"Given a query Q and a document D, the main task of a ranking function is to assign a score to D with respect to the query Q. The main objective of a term weighting scheme is to quantify the saliency of the query terms in the document. This section describes a novel TF-IDF term weighting scheme that serves above purpose. En-route to the development, we are guided by a number of hypotheses that are commonplace in quantifying the importance of a term. Thus, before we give the main motivation behind our work, let us first revisit the three key hypotheses.",null,null
84,"1. Term Frequency Hypothesis (TFH): The weight of term in a document should increase with the increase in term frequency (T F ). However, it seems unlikely that the importance of a term grows linearly with the increase in T F . Therefore, researchers have used dampened TF instead of the raw TF for ranking. The most widely used damping function has been log(T F ) and the basis of this damping can be best captured by the following advanced hypothesis.",null,null
85,"Advanced TF Hypothesis (AD-TFH): The modified term frequency hypothesis captures the intuition that the rate of change of weight of a term should decrease with the larger TF. For example, the change in the weight caused by increasing TF from 2 to 3 should be higher than that caused by increasing TF from 25 to 26 [10]. Thus, the raw T F has to be transformed to fulfill the above goal. Formally, we hypothesize that, a function Ft(T F ), that maps the original T F to the resultant value (which will be used for final weighting), should possess the following two properties.",null,null
86,(a) Ft (T F ) > 0,null,null
87,(b) Ft (T F ) < 0,null,null
88,"2. Document Length Hypothesis (DLH): This hypothesis captures the relationship between the term frequency and the document length. Long documents tend to use a term repeatedly, thus term frequency can be higher in a long document. Therefore, if T F is considered in isolation (disregarding the document length), long documents are given undue preference. Thus, it is necessary to regulate the T F value in accordance with the document length. The general principle is that if two documents have different lengths and the same T F values for a term t, then the contribution of T F (of t) should be higher for the shorter document.",null,null
89,"3. Term Discrimination Hypothesis (TDH): If a query contains more than one term, then a good weighting scheme should prefer a document that contains the rare term (in the collection).",null,null
90,3.2 Two Aspects of TF,null,null
91,"Most existing weighting schemes employ the above heuristics to quantify the term importance. However, they generally normalize the term frequency that captures a single aspect of the saliency of the terms and hence disregards another important aspect that we detail next. In particular, we consider the following two aspects:",null,null
92,"1. Relative Intra-document TF (RITF) : In this factor, the importance of a term is measured by considering its frequency relative to the average T F of the document. Thus, a natural formulation for this could be",null,null
93,"RIT F (t, D) ,"" T F (t, D)""",null,null
94,(2),null,null
95,Avg.T F (D),null,null
96,"where T F (t, D) and Avg.T F (t, D) denote the frequency of the term t in D and average term frequency of D respectively. However, Equation 2 may too much prefer excessively long documents, since the denominator is close to 1 for a very long document [28]. Hence, a sublinear damping of T F seems to be a better choice over the raw T F and thus we use the following function.",null,null
97,"RIT F (t, D) ,"" log2(1 + T F (t, D))""",null,null
98,(3),null,null
99,log2(1 + Avg.T F (D)),null,null
100,"Indeed, such a formula has been used by Singhal et al. [28] in the pivoted length normalization framework to normalize the tf values in accordance with the number of unique terms in the document.",null,null
101,2. Length Regularized TF (LRTF) : This factor nor-,null,null
102,malizes the term frequency by considering the number,null,null
103,of terms present in a document. Similar to Robert-,null,null
104,"son's [22] notion, we assume that the appropriate length",null,null
105,of a document should be the average document length,null,null
106,of the collection and the frequency of the terms of an,null,null
107,average length document should remain unchanged.,null,null
108,"Thus, a reasonable starting point could be T F (t, D) ×",null,null
109,ADL(C) len(D),null,null
110,",",null,null
111,where,null,null
112,ADL(C),null,null
113,is,null,null
114,the,null,null
115,average,null,null
116,document,null,null
117,length,null,null
118,of the collection and len(D) is the length of the doc-,null,null
119,"ument D. But once again, it seems unlikely that the",null,null
120,increase in term frequency follows a linear relationship,null,null
121,"with the document length, and thus the above formula",null,null
122,over-penalizes the long documents. To overcome this,null,null
123,345,null,null
124,"bias, we employ the following function (used in [3]) to",null,null
125,achieve the length dependent normalization.,null,null
126,",,",null,null
127,«,null,null
128,"LRT F (t, D) ,"" T F (t, D) × log2""",null,null
129,1 + ADL(C) len(D),null,null
130,(4),null,null
131,Equation 4 still punishes the long documents but with diminishing effect.,null,null
132,"However, we believe that any document length normalization can be used to achieve this purpose. Some of the possible alternatives might be the length normalization component of BM25 or that of the pivoted normalization scheme.",null,null
133,3.2.1 Motivation,null,null
134,"In order to motivate the use of two T F factors, let us consider the following two somewhat toy examples. We use these examples just to introduce the basic idea.",null,null
135,"Example 1 Let D1 and D2 be two documents of equal lengths, with the following statistics.",null,null
136,"1. len(D1) ,"" 20, # distinct term of D1 "","" 5, T F (t, D1)"",4",null,null
137,"2. len(D2) ,"" 20, # distinct term of D2 "","" 15, T F (t, D2)"",4",null,null
138,"In both of the cases, LRT F considers t equally important. A little thought will convince us that this is not appropriate, since the focus of the document D1 seems to be divided equally among 5 terms and therefore t should not be considered salient, while t seems to be important for D2. Thus, in the later case RIT F seems to be a better choice to LRT F .",null,null
139,Let us now turn to the other direction and consider the second example.,null,null
140,Example 2 Let D1 and D2 be two documents with the following statistics.,null,null
141,"1. len(D1) ,"" 20, # distinct term of D1 "","" 15, T F (t, D1)"",4",null,null
142,"2. len(D2) ,"" 200, # distinct term of D2 "","" 150, T F (t, D2)"",4",null,null
143,"For this instance however, RIT F considers the term t equally important for both D1 and D2, which is not right, since D2 contains more distinct terms and thus seems to cover many other topics (also possibly uses t repeatedly). Therefore, in this case, the use of LRT F seems to be a potential choice over RIT F .",null,null
144,"Motivated by the above examples, our main goal now is to integrate the above two factors into our weighting scheme. However, we do not use the T F factors as defined in the Equations 3 and 4. We transform these T F values for our final use that in some sense makes use of the hypothesis ADTFH. The next section details the transformation procedure and the underlying motivation.",null,null
145,3.2.2 Transforming TF Factors,null,null
146,"Recall that the main motivation behind the advanced hypothesis on term frequency (AD-TFH) is that a good weighting function, while emphasizing on term frequencies and term discrimination factors, should also pay attention to the term coverage issue (i. e number of match). For example, if a document D1 contains a query term 10 times and another document D2 contains two query terms (of the same query)",null,null
147,"5 times each, then the assigned score should be higher for D2",null,null
148,(assuming that both the query terms have equal term dis-,null,null
149,crimination values). That is probably the most important,null,null
150,"reason why raw T F does not work well in practice. Second,",null,null
151,another common trait of many weighting schemes (for exam-,null,null
152,"ple, pivoted normalization) is that they use the T F functions",null,null
153,that are not bounded above. We transform the T F factors,null,null
154,using a function f (x) that possesses the following proper-,null,null
155,"ties: (i) vanishes at 0, (ii) satisfies the two conditions of",null,null
156,"AD-TFH hypothesis (f (x) > 0 and f (x) < 0), and (iii)",null,null
157,asymptotically upper bounded to 1.,null,null
158,One of the simplest functions that satisfies the above prop-,null,null
159,"erties is f (x) ,",null,null
160,x 1+x,null,null
161,.,null,null
162,"Indeed, similar functions have been",null,null
163,"employed before in [22] and in [3]. Using this function, we",null,null
164,now transform the two T F factors as follows:,null,null
165,"RIT F (t, D)",null,null
166,"BRIT F (t, D) ,",null,null
167,(5),null,null
168,"1 + RIT F (t, D)",null,null
169,"BLRT F (t, D) ,"" LRT F (t, D)""",null,null
170,(6),null,null
171,"1 + LRT F (t, D)",null,null
172,3.2.3 Combining Two TF Factors,null,null
173,"Now the key question that we face: how should we combine BRIT F (t, D) and BLRT F (t, D)? A natural way to do this is as follows:",null,null
174,"T F F (t, D) ,"" w × BRIT F (t, D) + (1 - w) × BLRT F (t, D) (7)""",null,null
175,where 0 < w < 1. The next important issues that arise out of Equation 7 are the following:,null,null
176,"· Should we prefer BRIT F (t, D) (w > 0.5)?",null,null
177,"· Should we prefer BLRT F (t, D) (w < 0.5)?",null,null
178,"In order to answer these questions, we now analyze the properties of the two TF components. From Equation 5, it is clear that BRIT F (t, D) has a tendency to prefer long documents, since for long documents the denominator part of RIT F (t, D) is close to 1, and T F is usually larger. On the other hand, BLRT F (t, D) tends to prefer short documents, since LRT F (t, D)  0 as len(D)  . Therefore, when a query is long, BRIT F (t, D) heavily prefers extremely long documents, since the number of matches is more or less proportional to the length of the document [28]. On the contrary, since BLRT F (t, D) prefers short documents it can penalize extremely long documents when it faces longer queries, and thus it is preferable when longer queries are encountered. Another interesting property of BRIT F (t, D) is that it emphasizes on the number of matches, since the main component of this formula RIT F (t, D) heavily punishes the term frequency, and thus important for the short queries. Hence, the foregoing discussion suggests that, for short queries BRIT F (t, D) should be preferred, while for longer queries, BLRT F (t, D) should be given more weight",null,null
179,"Based on the discussion given in the previous section, we now turn to incorporate the query length information into our weighting formula. The value of w should decrease with the increase in query length, while it must lie between [0-1]. Specifically, we characterize the query length factor (QLF (Q)) by the following variables. (i) QLF (Q) , 1 for |Q| ,"" 1, (ii) QLF (Q) < 0 and (iii) 0 < QLF (Q) < 1. Numerous different functions can be constructed that satisfy the above conditions. We used the following three different""",null,null
180,346,null,null
181,functions.,null,null
182,1,null,null
183,"QLF1(Q) , log2(1 + |Q|)",null,null
184,(8),null,null
185,2,null,null
186,"QLF2(Q) , 1 + log2(1 + |Q|)",null,null
187,(9),null,null
188,3,null,null
189,"QLF3(Q) , 2 + log2(1 + |Q|)",null,null
190,(10),null,null
191,"The first function descends more rapidly than the second function, while the second function descends more rapidly than the third function. Our experiments suggest that function 9 performs consistently better than the other two functions on all the collections. Hence, we set w , QLF2(Q). We leave this issue for further investigation.",null,null
192,3.3 Term Discrimination Factor,null,null
193,The goal of the term discrimination factor in weighting,null,null
194,is to assign higher score to the documents that contain the,null,null
195,terms which are rare in the collection. Inverse document,null,null
196,frequency (IDF ) is a well known measure that serves the,null,null
197,above purpose. A number of IDF formulation are prevalent,null,null
198,"in the IR literature, all of which essentially quantify the",null,null
199,above intuition. We use the following standard idf measure.,null,null
200,",,",null,null
201,«,null,null
202,"IDF (t, C) , log",null,null
203,"CS(C) + 1 DF (t, C)",null,null
204,(11),null,null
205,The above IDF measure considers only the presence or,null,null
206,absence of a term in a document and does not take into ac-,null,null
207,count the document specific term occurrence. We hypoth-,null,null
208,esize that the term discrimination is a combination of the,null,null
209,"above two factors. In particular, we hypothesize that if two",null,null
210,"terms have equal document frequencies, then the term dis-",null,null
211,crimination should increase with the increase in average elite,null,null
212,set term frequency. The average elite set term frequency,null,null
213,(AEF ),null,null
214,is,null,null
215,defined,null,null
216,as,null,null
217,"CT F (t,C) DF (t,C)",null,null
218,",",null,null
219,where,null,null
220,"CT F (t, C)",null,null
221,denotes,null,null
222,the,null,null
223,total occurrence of the term t in the entire collection. In,null,null
224,"fact, Kwok [18] used AEF for term weighting, but the pur-",null,null
225,"pose was different. However, the combination of raw AEF",null,null
226,"with IDF may disturb the overall term discrimination value,",null,null
227,since the IDF values are obtained by dampening through,null,null
228,"log function. Hence, we employ a slowly increasing function",null,null
229,to transform the AEF values for this combination. Once,null,null
230,"again, we use the function f (x) , x/(1 + x) to transform",null,null
231,the AEF values for the final use. The final term discrimi-,null,null
232,nation value of term t is computed as,null,null
233,T,null,null
234,DF,null,null
235,"(t,",null,null
236,C),null,null
237,",",null,null
238,IDF,null,null
239,"(t,",null,null
240,C),null,null
241,×,null,null
242,1,null,null
243,"AEF (t, C) + AEF (t, C)",null,null
244,(12),null,null
245,"Our experiments reveal that the use of the above term discrimination has not very significant effect on the overall performance. However, it is observed that the improvements, although are small, consistent across the collections.",null,null
246,3.4 Final Formula,null,null
247,Integrating the above factors we now obtain the following final scoring formula.,null,null
248,X |Q|,null,null
249,"Sim(Q, D) ,"" T F F (qi, D) × T DF (qi, C)""",null,null
250,(13),null,null
251,"i,1",null,null
252,"Again, since T F F (qi, D) < 1, we obtain the following rela-",null,null
253,tionship.,null,null
254,X |Q|,null,null
255,"Sim(Q, D) < T DF (qi, C)",null,null
256,(14),null,null
257,"i,1",null,null
258,"Therefore, we can easily modify Equation 13 to get the normalized similarity scores (0 < Sim(Q, D) < 1) as follows:",null,null
259,"|PQ| T F F (qi, D) × T DF (qi, C)",null,null
260,"Simnorm(Q, D) , i,1",null,null
261,"|PQ| T DF (qi, C)",null,null
262,"i,1",null,null
263,(15),null,null
264,"Equations 13 and 15 are equivalent in the sense that they produce the same ranked lists. However, an application that requires normalized scores, Equation 15 can be used as a suitable alternative.",null,null
265,4. EXPERIMENTAL SETUP,null,null
266,"In this section we describe the details of our experimental setup. First, in Section 1 we give the details of the test collections used in our experiments. In Section 4.2 and Section 4.3 we describe the evaluation measures and the baseline retrieval models respectively.",null,null
267,4.1 Data,null,null
268,"Table 1 summarizes the statistics on test collections used in our experiments. The experiments are conducted on a large number of standard test collections, that vary both by type, the size of the document collections and the number of queries.",null,null
269,"TREC 6,7,8 and ROBUST are news collections containing 528,155 documents and supplemented by 150 (queries 301-450) and 100 (601-700) queries respectively. WT10G is a web collection of moderate size supplemented by 100 queries (451-550), while GOV2 is another web collection of larger size, which is crawled from .gov domain. There are 150 (queries 701-850) queries attached with GOV2 collection which were used in TREC terabyte [4] track for three years.",null,null
270,Table 1: Test Collection Statistics,null,null
271,Name,null,null
272,# of docs # of queries,null,null
273,"TREC 6,7,8 528,155",null,null
274,150,null,null
275,ROBUST,null,null
276,"528,155",null,null
277,100,null,null
278,WT10G,null,null
279,"1,692,096",null,null
280,100,null,null
281,GOV2,null,null
282,"25,205,179",null,null
283,150,null,null
284,MQ-07,null,null
285,"25,205,179",null,null
286,1778,null,null
287,MQ-08,null,null
288,"25,205,179",null,null
289,784,null,null
290,"The MQ-07 and MQ-08 set of queries are based on the Million Query Track 2007 [2] and 2008 [1] respectively. This track was designed to serve two purposes. First, it was an exploration of ad-hoc retrieval on a large collection of documents. Second, it investigated questions of system evaluation, particularly whether it is better to evaluate using many shallow judgments or fewer thorough judgments. Both million query track use GOV2 as document collection. Topics for this task were drawn from a large collection of queries that were collected by a large Internet search engine. The queries also vary by their length, with short (2-3 words) to long (6-10 words). Specifically, MQ-07 and MQ-08 collections contain 505 and 433 queries of length higher than 5",null,null
291,347,null,null
292,"respectively. Therefore, the test collections provide us a diverse experimental setup for assessing the effectiveness of the proposed weighting method.",null,null
293,"Except TREC-6,7,8, all the test collections have three scale graded relevance assessment. The grades are 0, 1 and 2- meaning non-relevant, relevant and highly relevant respectively. TREC-6,7,8 collection uses binary relevance assessment.",null,null
294,4.2 Evaluation Measures and IR System,null,null
295,"All our experiments are carried out using TERRIER1 retrieval system (version 3.5). Terrier is a flexible Information retrieval system which provides the implementation of many well known models. We use title field of the topics (note that two million query data contain more than 1000 queries that contain more than 5 terms). From all the collections we removed stopwords during indexing. Documents and queries are stemmed using Porter stemmer. Statistical significance tests are done using two sided paired t-test at 95% confidence level (i,e p < 0.05).",null,null
296,We use the following metrics to evaluate the systems.,null,null
297,· Mean Average Precision (MAP): This is a standard metric for binary relevance assessment.,null,null
298,"· Normalized DCG at k (NDCG@k) [15]: Discounted cumulative gain (DCG) is an evaluation measure that can leverage the relevance judgment in terms of multiple grades, and has an explicit position-wise discount factor. NDCG is the normalized version of DCG.",null,null
299,"· Expected Reciprocal Rank at k (ERR@k) [6]: To relax the additive nature and the underlying independent assumption in NDCG, another evaluation measure, namely, Expected Reciprocal Rank (ERR) is proposed in [6]. It discounts the documents which are shown below very relevant documents, and is defined as the expected reciprocal length of time that a user will take to find a relevant document. ERR@k is computed as follows:",null,null
300,ERR@k,null,null
301,",",null,null
302,Xk,null,null
303,R(gi) i,null,null
304,iY -1 (1,null,null
305,-,null,null
306,R(gi)),null,null
307,"i,1",null,null
308,"j,1",null,null
309,(16),null,null
310,where,null,null
311,R(g),null,null
312,",",null,null
313,", 2g -1",null,null
314,2hg,null,null
315,hg,null,null
316,"is the highest grade and g1, g2 . . . gk",null,null
317,are the relevance grades associated with the top k doc-,null,null
318,"uments. The value of mg is 2 for all the collections,",null,null
319,"except TREC 6,7&8 (for this mg , 1).",null,null
320,"The first two metrics are used to reflect the overall performance of the systems, while the last evaluation measure reflects better the precision of search results, thereby making more important for the precision oriented systems. ERR has been chosen as one of the official metrics for recent TREC web tracks [7].",null,null
321,"Note that, two million query collections (MQ-07 and MQ08) have incomplete relevance assessment. Therefore, for the sake of more reliable conclusions, we evaluate the million query sets in two different ways. First, we skip the unjudged documents from the ranked lists to compute the values of well known metrics following the recommendation made in [23]. Additionally, we also present the statistical",null,null
322,1http://terrier.org/,null,null
323,average precision2 [5] which was one of the official metrics for the million query tracks [2].,null,null
324,4.3 Baselines,null,null
325,"We have compared the performance of the proposed weighting scheme with five state of the art retrieval models. Since the proposed weighting function is a TF-IDF based formula, we have taken two well known state of the art TF-IDF models. We have also chosen BM25, language model with Dirichlet smoothing (LM), and relatively recent divergence from randomness based formula (PL2) as the other state of the art baselines. The choice of our baselines are primarily motivated by [10], which provides a thorough and detailed description of all the state of the art models along with the parameter sensitivity issues.",null,null
326,"The performances of all the baseline models are dependent on the parameters they contain. Therefore, for the sake of more reliable comparisons with the baselines, we carry out two experiments by taking first 50 judged queries from MQ07 and MQ-08 collections. We search parameters by optimizing NDCG@20. The parameters values are given in the description of the corresponding baselines. Our experiments mostly agree with the findings reported by Fang et al. [10]. In particular, we find that the performances of Pivoted TFIDF and PL2 are very sensitive with the variation of the parameters. For example, the parameter value (s ,"" 0.2) suggested for pivoted TF-IDF in the original paper [28] gives 12% poorer MAP than that we find here by training. Similarly, the default PL2 parameter (c "","" 1) is 14% poorer than the one we find. Therefore, for fair comparisons, we use these optimal parameter values for the baselines. The details of the baselines are given below.""",null,null
327,1. Pivoted length normalized TF-IDF model: This model is one of the best performing TF-IDF formula in the vector space model framework. The value of the parameter s is set to 0.05.,null,null
328,"2. Lemur TF-IDF model: This model is another TF-IDF model that uses Robertson's tf and the standard idf . The parameter of this model is set to its default value, 0.75.",null,null
329,"3. Classical Probabilistic model (BM25): BM25 is chosen as a state of the art representative of the classical probabilistic model. The main differences with this model and the previous model are that BM25 uses query term frequency in a different way and the idf also differs with the standard one. The parameters of this model is set to k1 ,"" 1.2, b "", 0.6 and k3 , 1000. Note that we found (on training data) slightly better results for b , 0.6 than the default 0.75.",null,null
330,4. Dirichlet smooth language model (LM): Language model is another probabilistic model that performs very effectively. For this model we set the value of Dirichlet smoothing (µ) to 1700.,null,null
331,"5. Divergence from Randomness model (PL2): Finally, PL2 [12] represents the recently proposed non-parametric probabilistic model from divergence from randomness",null,null
332,2the code available at TREC million query page is used to compute stat AP,null,null
333,348,null,null
334,"(DFR) family. Similar to the previous models, its performance also depends on a parameter value (c in the formula). We conduct experiments for this model by setting c , 13.",null,null
335,5. RESULTS,null,null
336,"In this section we present the experimental results of our proposed work and compare them with the state of the art retrieval models. In Section 5.1 we compare the performance of the proposed model (MATF for Multi Aspect TF) with the two TF-IDF models, followed by the comparison with three probabilistic models- BM25, language model (LM) and PL2. We use three evaluation measures to evaluate the performance of all the methods.",null,null
337,5.1 Comparison with TF-IDF Models,null,null
338,"In this section we focus on to compare the performance of the proposed model (MATF) with the Lemur TF-IDF and and Pivoted TF-IDF models. Table 2 presents the experimental results for six test collections measured in terms MAP, NDCG@20 and ERR@20.",null,null
339,"First, we describe the results in terms of MAP. Table 2 clearly shows that MATF gains significantly better MAP than both of the TF-IDF models on two news collections. MATF performs 12% and 15.7% better than Lemur TF-IDF model on TREC-678 and ROBUST respectively. MATF is also significantly surpasses the Pivoted TF-IDF model on these collections with a margin of 8.8% and 6.3% respectively.",null,null
340,"The behavior of MATF is similar when we see the results for two web collections, namely, WT10G and GOV2. Once again, MATF outperforms Lemur TF-IDF model by a margin of more than 20% in both of the occasions, which is clearly highly significant as confirmed by the paired t test. Like the previous two news collections, MATF maintains its superior behavior over Pivoted TF-IDF in these web collections also. In particular MATF gains more than 8% and 19% average precision than Pivoted TF-IDF for both of the collections and paired t test once again attests the significance.",null,null
341,"We now turn to describe the results on two million query data sets. These two collections are particularly interesting, since they contain real search queries collected from a commercial search engine and also because of their variations in length. Table 2 once again demonstrates that MATF unequivocally outperforms the two TF-IDF models with significantly large margin. The MAP achieved by MATF is nearly 11% and 7% better than that achieved by Lemur TF-IDF on MQ-07 and MQ-08 collections respectively. Similarly, MATF surpasses the Pivoted TF-IDF by more than 10% margin on both of the occasions. Significance tests show that the performance differences are always statistically significant.",null,null
342,"Among the two TF-IDF models, Lemur TF-IDF often seems to perform poorer than Pivoted (except MQ-08 where Lemur TF-IDF is nearly 4% better than pivoted). One potentially interesting outcome that we can see from Table 2 is that, when the document collection is larger MATF outperforms Pivoted TF-IDF with larger margin. In particular, MATF gains a MAP on GOV2 collection which is almost 20% better than the pivoted TF-IDF, which is a clear sign of effectiveness of MATF over the state of the TF-IDF models.",null,null
343,"So far our discussion of experimental outcomes primarily confined on the basis of the binary relevance assessment. Note that five out of six test collections used in our evaluation have graded assessment in three scales (0,1,2). Therefore, we now turn to describe the results measured in terms of NDCG, that leverages the graded assessment.",null,null
344,"The middle segment of Table 2 presents the results in terms of NDCG@20. It is once again clear that the performances are more or less consistent with MAP. Specifically, MATF surpasses the Lemur TF-IDF models with consistently and significantly large margin on all six collections and often the differences are higher or close to 10%, which once again clearly demonstrates the effectiveness of MATF. Performance of pivoted TF-IDF is once again very similar under the graded assessment and it achieves larger NDCG than Lemur TF-IDF except in one occasion. MATF once again is significantly better than the pivoted TF-IDF on all the collections and the differences are larger for larger web collections.",null,null
345,"Our final comparison between the proposed model and the TF-IDF models focus on precision enhancing capabilities measured in terms of a metric ERR, that consider three things simultaneously: rank of the document, quality conveyed by the assessor assigned grade (non-relevant, relevant and highly relevant) and the quality of the documents that have been seen before the document of our focus.",null,null
346,"The last segment of Table 2 reports the ERR@20 values achieved by the competing models on six collections. We can easily infer that MATF once again unanimously beats the two TF-IDF models. Only on WT10G, pivoted TF-IDF performs slightly better than MATF. Consistent with the previous measures, ERR@20 results demonstrate that on larger web collections the performance differences between MATF and the two TF-IDF models are larger.",null,null
347,Table 3: Comparison with TF-IDF models (statAP). Lemur means Lemur TF-IDF. Superscripts have their usual meaning.,null,null
348,"Lemur pivot MATF (% improv) MQ-07 29.0 29.7 34.4lp (18.2, 15.8) MQ-08 28.4 27.6 32.5lp (14.9, 17.8)",null,null
349,"The performances of MATF and the two TF-IDF models on two million query data, measured by statistical average precision, are shown in Table 3. MATF transcends Lemur TF-IDF by a margin of 18% and 15% on MQ-07 and MQ08 respectively, while it is better than pivoted TF-IDF with more than 15% on both of the collections.",null,null
350,"In summary, based on the results shown in Table 2 and Table 3 we can infer that MATF outperforms two state of the art TF-IDF models with remarkable significance and consistency, and the performance differences are often noticeably large. The performance measured by three evaluation metrics unequivocally demonstrate that MATF is highly effective in ranked retrieval. Moreover, the results also show that MATF is more effective for larger web collections.",null,null
351,5.2 Comparison with Probabilistic Models,null,null
352,In the last section we compare the performance of our model with two TF-IDF models. In this section we compare the performance of MATF with three well known state of the,null,null
353,349,null,null
354,"Table 2: Comparison with the TF-IDF models measured in terms of MAP, NDCG@20 and ERR@20. MATF denotes the proposed model. The best results are boldfaced. Superscripts l and p denote that the performance difference is statistically significant (p < 0.05) compared to Lemur TF-IDF and Pivot TF-IDF respectively.",null,null
355,Metric,null,null
356,Method,null,null
357,Lemur.TF-IDF,null,null
358,MAP,null,null
359,Pivot.TF-IDF,null,null
360,MATF,null,null
361,% better than Lemur.TF-IDF,null,null
362,% better than Pivot.TF-IDF,null,null
363,TREC-678 20.9 21.5 23.4lp 12.0 8.8,null,null
364,ROBUST 26.1 28.4 30.2lp 15.7 6.3,null,null
365,WT10G 18.4 20.5 22.2lp 20.7 8.3,null,null
366,GOV2 24.8 26.5 31.7lp 27.8 19.6,null,null
367,MQ-07 39.6 40.0 44.2lp 11.6 10.5,null,null
368,MQ-08 42.8 41.2 45.7lp 6.8 10.9,null,null
369,Lemur.TF-IDF NDCG@20 Pivot.TF-IDF,null,null
370,MATF % better than Lemur.TF-IDF % better than Pivot.TF-IDF,null,null
371,40.0 41.5 44.6lp,null,null
372,11.5 7.5,null,null
373,37.5 40.2 41.5lp,null,null
374,10.7 3.2,null,null
375,31.6 33.4 34.6l,null,null
376,9.5 3.6,null,null
377,43.8 46.8 51.0lp,null,null
378,16.4 9.0,null,null
379,46.8 48.3 51.1lp,null,null
380,9.2 5.8,null,null
381,50.1 48.7 52.6lp,null,null
382,5.0 8.0,null,null
383,ERR@20,null,null
384,Lemur.TF-IDF Pivot.TF-IDF MATF,null,null
385,40.7,null,null
386,41.9 43.9lp,null,null
387,45.7,null,null
388,46.3 48.5lp,null,null
389,34.7,null,null
390,37.9 37.1l,null,null
391,48.3,null,null
392,49.4 53.4lp,null,null
393,40.6,null,null
394,42.9 44.9lp,null,null
395,44.5,null,null
396,44.6 47.3lp,null,null
397,"art probabilistic retrieval models. Our evaluation strategy is once again similar to the previous section. We compare the performances of the models under MAP, NDCG@20 and ERR@20.",null,null
398,"First we compare the performance of MATF with the BM25 model. Table 4 shows the summary of the retrieval results on six test collections. It is clear from the table that MATF is superior to BM25 model. This result holds for all the collections and for all three evaluation measure. When the performance differences between them are measured in terms of MAP, we notice that MATF is significantly effective for news as well as web corpora compared to BM25. In fact, MATF is nearly 10% better than BM25 on two news data, while on two web collections (WT10G and GOV2), MATF achieves 17% and 12% more MAP than BM25. The differences on MQ-07 and MQ-08 are similarly significant with substantial margins. The performance differences between MATF and BM25 revealed by NDCG metric are consistent with that revealed by MAP and once again, all the differences are statistically significant. ERR@20 depicts that for all the collections, MATF remains consistently superior to BM25, which clearly confirms that MATF is very effective for precision oriented systems.",null,null
399,"We now compare the effectiveness of MATF and language model with Dirichlet prior language model. From Table 4 we clearly see that the performance differences between MATF and LM are larger in three out of six cases than that we had observed when comparing the performance of MATF and BM25. Specifically, MATF achieves close to or more than 10% MAP than LM on four out out six instances (except WT10G). The performance measured on graded relevance assessment also demonstrates that MATF unequivocally beats the Dirichlet prior language model based approach, and the differences are substantially large. On GOV2, MQ-07 and MQ-08 data, MATF surpasses LM with a margin of 14%, 8% and nearly 9% respectively. The comparison of precision enhancing abilities of MATF and LM also clearly indicates that MATF is always better than LM, which is very concordant with the experimental findings captured by MAP and NDCG.",null,null
400,"We now compare the performance of the proposed model with another probabilistic model from the divergence from randomness family, namely, PL2. This model is relatively recent compared to the previous two probabilistic models and was also found to be better than BM25 in the experiments reported in [3]. Table 4 reflects two major facts. First, it appears from the table that PL2 is most effective among the probabilistic models and in particular only on MQ-08 data it performs worse than BM25 as reflected by both MAP and NDCG. The second major observation that can be made from Table 4 is that MATF beats this model also with harmonious consistency and performance differences are statistically significant on TREC-678, GOV2, MQ07 and MQ-08 data. Similar to the previous outcomes, on web collections the performance differences between MATF and PL2 are larger than that for the news collections. Lastly, ERR metric depicts that MATF is better than PL2 across all six collection.",null,null
401,Table 5: Comparison with probabilistic models (statAP).,null,null
402,"BM25 LM PL2 MATF (% improvement) MQ-07 30.6 29.7 30.4 34.4blp (12.8, 15.8, 13.2) MQ-08 29.6 27.6 27.4 32.5blp (10.5, 17.8, 18.6)",null,null
403,Table 5 compares the performance of four models for million query collections measured in terms of statistical average precision. It is once again clearly evident that MATF is consistently better than all three models and all the differences are very large and it is very consistent with the performance measured in terms other metrics presented in Table 4.,null,null
404,"Overall, the comparative analysis clearly shows that MATF is the most effective retrieval model, which unequivocally outperforms all three probabilistic models, when the performances are measured in terms of MAP, NDCG and a precision biased metric, namely, ERR. Also, the relative per-",null,null
405,350,null,null
406,"Table 4: Comparison with probabilistic models measured in terms of MAP, NDCG@20 and ERR@20. MATF denotes the proposed model. The best results are boldfaced. Superscripts b, l and p denote that the performance differences are statistically significant compared to BM25, LM and PL2 respectively.",null,null
407,Metric,null,null
408,Method,null,null
409,BM25,null,null
410,MAP,null,null
411,LM,null,null
412,PL2,null,null
413,MATF,null,null
414,% better than BM25,null,null
415,% better than LM,null,null
416,% better than PL2,null,null
417,TREC-678 21.3 21.3 22.7 23.4blp 9.9 9.9 3.1,null,null
418,ROBUST 27.7 28.4 29.5 30.2bl 9.0 6.9 2.4,null,null
419,WT10G 18.9 21.3 21.3 22.2b 17.5 4.2 4.2,null,null
420,GOV2 28.3 29.1 29.7 31.7blp 12.1 8.9 6.7,null,null
421,MQ-07 41.2 40.1 40.9 44.2blp 7.3 10.2 8.1,null,null
422,MQ-08 43.6 41.0 41.5 45.7blp 4.8 11.5 10.1,null,null
423,BM25 NDCG@20 LM,null,null
424,PL2 MATF % better than BM25 % better than LM % better than PL2,null,null
425,41.2 40.2 42.9 44.6blp,null,null
426,8.3 10.9 4.0,null,null
427,39.3 39.3 41.1 41.5bl,null,null
428,5.6 5.6 1.0,null,null
429,32.4 32.5 33.1 34.6bl,null,null
430,6.8 6.5 4.5,null,null
431,45.2 44.6 46.1 51.0blp,null,null
432,12.8 14.3 10.6,null,null
433,48.1 47.2 48.0 51.1blp,null,null
434,6.2 8.3 6.5,null,null
435,50.9 48.3 49.0 52.6blp,null,null
436,3.3 8.9 7.3,null,null
437,ERR@20,null,null
438,BM25 LM PL2 MATF,null,null
439,41.1 41.2 43.0 43.9blp,null,null
440,45.6 46.5 47.0 48.5b,null,null
441,34.7 35.4 35.2 37.1b,null,null
442,48.2 47.6 47.7 53.4blp,null,null
443,41.3 39.9 40.7 44.9blp,null,null
444,44.8 42.7 42.7 47.3blp,null,null
445,Table 6: Performance of two TF factors on short,null,null
446,and long query. The values are MAP.,null,null
447,short,null,null
448,long,null,null
449,MQ-07 MQ-08 MQ-07 MQ-08,null,null
450,LRTF 43.5 43.3 39.9 44.3,null,null
451,RITF 45.4 45.0 37.7 41.8,null,null
452,"formance differences are often substantially large and the differences are even larger for the web collections that contain large number of queries. Among the three probabilistic models, PL2 and Dirichlet prior language model perform almost equally, with PL2 having a marginal edge over LM.",null,null
453,5.3 Analysis,null,null
454,"In this section we analyze the effect of the two TF factors on short and long queries. For this analysis we choose the two million query collections, primarily because the collections have large number of queries. We divide the queries in two sets. The queries having at least 5 terms are denoted as short, while the rest of the queries (longer than 5 words) are treated as long. The main goal of this section is to validate the hypothesis made in the proposed section that relative intra-document based TF (RITF) performs better on short queries, while length regularized TF (RLTF) performs better on long queries.",null,null
455,"Table 6 presents the experimental results on two million query data. The results seem to confirm our aforesaid assumption. LRTF always performs better than RITF on both of the collections, while RITF does better for short queries. However, the performance differences between the methods on longer queries are noticeably better than that for shorter queries.",null,null
456,6. CONCLUSION,null,null
457,"In this paper, we present a novel TF-IDF term weighting scheme. The proposed term weighting scheme employs two aspects of within document term frequency normalization to determine the importance of a term. One component of the term frequency tends to prefer short documents, while the other tends to prefer long documents. We then combine these two TF components using the query length information, that maintains a balanced trade-off in retrieving short and long documents, when the ranking function faces queries of varying lengths.",null,null
458,"Experiments carried out on a set of news and web collections show that the proposed model outperforms two well known state of the art TF-IDF baselines with significantly large margin, when measured in terms of MAP and NDCG. The model also surpasses three state of the art probabilistic models with remarkable significance almost always. Moreover, the proposed model is also significantly better than all of the five baselines in improving precision.",null,null
459,Acknowledgments,null,null
460,"I would like to thank Dipasree Pal, Mandar Mitra and Swapan Parui for their comments, suggestions and help.",null,null
461,7. REFERENCES,null,null
462,"[1] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million query track 2008 overview. In E. M. Voorhees and L. P. Buckland, editors, The Sixteenth Text REtrieval Conference Proceedings (TREC 2008). National Institute of Standards and Technology, December 2009.",null,null
463,"[2] J. Allan, B. Carterette, B. Dachev, J. A. Aslam, V. Pavlu, and E. Kanoulas. Million query track 2007 overview. In TREC, 2007.",null,null
464,351,null,null
465,"[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, Oct. 2002.",null,null
466,"[4] S. Bu¨ttcher, C. L. A. Clarke, and I. Soboroff. The trec 2006 terabyte track. In TREC, 2006.",null,null
467,"[5] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation over thousands of queries. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '08, pages 651­658, New York, NY, USA, 2008. ACM.",null,null
468,"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proceedings of the 18th ACM conference on Information and knowledge management, CIKM '09, pages 621­630, New York, NY, USA, 2009. ACM.",null,null
469,"[7] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In TREC, 2011.",null,null
470,"[8] S. Clinchant and E. Gaussier. Retrieval constraints and word frequency distributions a log-logistic model for ir. Inf. Retr., 14(1):5­25, Feb. 2011.",null,null
471,"[9] R. Cummins and C. O'Riordan. A constraint to automatically regulate document-length normalisation. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM '12, pages 2443­2446, New York, NY, USA, 2012. ACM.",null,null
472,"[10] H. Fang, T. Tao, and C. Zhai. Diagnostic evaluation of information retrieval models. ACM Trans. Inf. Syst., 29(2):7:1­7:42, Apr. 2011.",null,null
473,"[11] W. R. Greiff. A theory of term weighting based on exploratory data analysis. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 11­19, New York, NY, USA, 1998. ACM.",null,null
474,"[12] B. He and I. Ounis. A study of the dirichlet priors for term frequency normalisation. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 465­471, New York, NY, USA, 2005. ACM.",null,null
475,"[13] B. He and I. Ounis. On setting the hyper-parameters of term frequency normalization for information retrieval. ACM Trans. Inf. Syst., 25(3), July 2007.",null,null
476,"[14] D. Hiemstra, S. Robertson, and H. Zaragoza. Parsimonious language models for information retrieval. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, pages 178­185, New York, NY, USA, 2004. ACM.",null,null
477,"[15] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422­446, Oct. 2002.",null,null
478,"[16] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments - part 1. Inf. Process. Manage., 36(6):779­808, 2000.",null,null
479,"[17] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval:",null,null
480,"development and comparative experiments - part 2. Inf. Process. Manage., 36(6):809­840, 2000.",null,null
481,"[18] K. L. Kwok. A new method of weighting query terms for ad-hoc retrieval. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96, pages 187­195, New York, NY, USA, 1996. ACM.",null,null
482,"[19] J. M. Ponte and W. B. Croft. A language modeling approach to information retrieval. In Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '98, pages 275­281, New York, NY, USA, 1998. ACM.",null,null
483,"[20] S. Robertson and H. Zaragoza. The probabilistic relevance framework: BM25 and beyond. Found. Trends Inf. Retr., 3(4):333­389, Apr. 2009.",null,null
484,"[21] S. E. Robertson. Readings in information retrieval. chapter The probability ranking principle in IR, pages 281­286. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.",null,null
485,"[22] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '94, pages 232­241, New York, NY, USA, 1994. Springer-Verlag New York, Inc.",null,null
486,"[23] T. Sakai. Alternatives to bpref. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 71­78, New York, NY, USA, 2007. ACM.",null,null
487,"[24] G. Salton and C. Buckley. Term-weighting approaches in automatic text retrieval. Inf. Process. Manage., 24(5):513­523, Aug. 1988.",null,null
488,"[25] G. Salton and M. J. McGill. Introduction to Modern Information Retrieval. McGraw-Hill, Inc., New York, NY, USA, 1986.",null,null
489,"[26] G. Salton, A. Wong, and C. S. Yang. A vector space model for automatic indexing. Commun. ACM, 18(11):613­620, Nov. 1975.",null,null
490,"[27] A. Singhal. Modern information retrieval: A brief overview. IEEE Data Eng. Bull., 24(4):35­43, 2001.",null,null
491,"[28] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '96, pages 21­29, New York, NY, USA, 1996. ACM.",null,null
492,"[29] K. Sparck Jones. Document retrieval systems. chapter A statistical interpretation of term specificity and its application in retrieval, pages 132­142. Taylor Graham Publishing, London, UK, UK, 1988.",null,null
493,"[30] H. Turtle and W. B. Croft. Evaluation of an inference network-based retrieval model. ACM Trans. Inf. Syst., 9(3):187­222, July 1991.",null,null
494,"[31] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.",null,null
495,352,null,null
496,,null,null

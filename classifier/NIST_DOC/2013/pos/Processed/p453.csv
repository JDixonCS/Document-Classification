,sentence,label,data
0,Utilizing Query Change for Session Search,null,null
1,"Dongyi Guan, Sicong Zhang, Hui Yang",null,null
2,Department of Computer Science Georgetown University,null,null
3,"37th and O Street, NW, Washington, DC, 20057",null,null
4,"{dg372, sz303}@georgetown.edu, huiyang@cs.georgetown.edu",null,null
5,ABSTRACT,null,null
6,"Session search is the Information Retrieval (IR) task that performs document retrieval for a search session. During a session, a user constantly modifies queries in order to find relevant documents that fulfill the information need. This paper proposes a novel query change retrieval model (QCM), which utilizes syntactic editing changes between adjacent queries as well as the relationship between query change and previously retrieved documents to enhance session search. We propose to model session search as a Markov Decision Process (MDP). We consider two agents in this MDP: the user agent and the search engine agent. The user agent's actions are query changes that we observe and the search agent's actions are proposed in this paper. Experiments show that our approach is highly effective and outperforms top session search systems in TREC 2011 and 2012.",null,null
7,Categories and Subject Descriptors,null,null
8,H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval,null,null
9,Keywords,null,null
10,Session search; query change model; retrieval model,null,null
11,1. INTRODUCTION,null,null
12,"Session search is the Information Retrieval (IR) task that retrieves documents for a search session [4, 8, 13, 14, 15, 25, 32]. During a search session, a user keeps modifying queries in order to find relevant documents that fulfill his/her information needs. In session search, many factors, such as relevance feedback, clicked data, changes in queries, and user intentions, are intertwined together and make it a quite challenging IR task. TREC (Text REtrieval Conference) 20102012 Session tracks [18, 19, 20] studied session search with a focus on the ""current query"" task, which retrieves relevant documents for the current/last query in a session based on previous queries and interactions. Table 1 shows examples from the TREC 2012 Session track.1",null,null
13,"1All examples mentioned in this paper are from TREC 2012. For simplicity, we use `sx' to refer to a TREC 2012 session where x is the session identification number.",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM or the author must be honored. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
15,Table 1: Examples of TREC 2012 Session queries.,null,null
16,session 6 1.pocono mountains pennsylvania 2.pocono mountains pennsylvania hotels 3.pocono mountains pennsylvania things to do 4.pocono mountains pennsylvania hotels 5.pocono mountains camelbeach 6.pocono mountains camelbeach hotel 7.pocono mountains chateau resort 8.pocono mountains chateau resort attractions 9.pocono mountains chateau resort getting to 10.chateau resort getting to 11.pocono mountains chateau resort directions session 85 1.glass blowing 2.glass blowing science 3.scientific glass blowing,null,null
17,session 28 1.france world cup 98 reaction stock market 2.france world cup 98 reaction 3.france world cup 98 session 32 1.bollywood legislation 2.bollywood law session 37 1.Merck lobbists 2.Merck lobbying US policy,null,null
18,"From Table 1, we notice that queries change constantly in a session. The patterns of query changes include general to specific (pocono mountains  pocono mountains park), specific to general (france world cup 98 reaction  france world cup 98), drifting from one to another (pocono mountains park  pocono mountains shopping), or slightly different expressions for the same information need (glass blowing science  scientific glass blowing). These changes vary and sometimes even look random (gun homicides australia  martin bryant port arthur massacre), which increases the difficulty of understanding user intention. However, since query changes are made after the user examines search results, we believe that query change is an important form of feedback. We hence propose to study and utilize query changes to facilitate better session search.",null,null
19,"One approach to handle query change is to classify them based on various types of explorations [20], such as specification, generalization, drifting, or slight change, then perform retrieval. Another approach is mapping queries into semantic graphical representations, such as ontologies [7] or query flow graphs developed from query logs [2], then studying how queries move in the graphs. However, ontology mapping is challenging [17], which may introduce inaccurate intermediate results and hurt the search accuracy. Moreover, relying on large scale query logs may not be applicable due to lack of such data. Therefore, although these approaches have been applied to IR tasks such as query reformulation [3] and query suggestion [2, 30], they have yet to be directly applied to session search. It is therefore necessary to explore new solutions to utilize query change for session search.",null,null
20,"We propose to model session search as a Markov Decision Process (MDP) [16, 28], which is applicable to many human decision processes. MDP models a state space and an action space for all agents participating in the process. Actions from the agents influence the environment/states and the environment/states influence the agents' subsequent actions",null,null
21,453,null,null
22,Figure 1: Session search MDP. (example is from s32),null,null
23,"based on certain policies. A transition model between states indicates the dynamics of the entire system. In our MDP, queries are modeled as states. Previous queries that the user wrote influence the search results; the search results again influence the user's decision of the next query. This interaction continues until the search stops.",null,null
24,"As illustrated in Figure 1, we consider two agents in this entire process: the user agent and the search engine agent. The user agent's actions are mainly human actions that are able to change search results, such as adding and deleting query terms, i.e. query change. Clicking is a human action; however, it does not explicitly impact the retrieval. Therefore, it is not considered as a user action here. Query change is the only form of user action in this paper. Based on the user actions, we design corresponding policies for the search engine agent; which is the main focus of this paper.",null,null
25,"It is difficult to interpret the user intent [5, 31] behind query change. For instance, for a query change from Kurosawa to Kurosawa wife (s38), there is no indication about `wife' in the search results returned for the first query. However, Kurosawa's wife is actually among the information needs provided to the user by NIST's topic descriptions. Our experience with TREC Session tracks suggests that information needs and previous search results are two main factors that influence query change. However, knowing information needs before search could not easily be achieved. This paper focuses on utilizing evidence found in previous search results and the relationship between previous search results and query change to improve session search.",null,null
26,"In this paper, we summarize various types of query changes based on potential user intents into user agent policies. We further propose corresponding policies for the search engine agent and model them in the query change retrieval model (QCM), a novel reinforcement learning [16] inspired framework. The relevance of a document to the current query is recursively calculated as the reward beginning from the starting query and continuing until the current query. This research is perhaps the first to employ reinforcement learning to tackle session search. Our experiments demonstrate that the proposed approach is highly effective and outperforms the best performing TREC 2011 and 2012 session search systems.",null,null
27,"The remainder of this paper is organized as follows. Section 2 analyzes query change and summarizes policies for the user agent. Section 3 proposes policies for the search engine agent. Section 4 elaborates the query change retrieval model. Section 5 discusses how to handle duplicated queries. Section 6 evaluates our approach, followed by a discussion in Section 7. Section 8 presents the related work and Section 9 concludes the paper.",null,null
28,Table 2: Evidence that query change q appears in previous,null,null
29,search results Di-1.,null,null
30,qtheme +q,null,null
31,# in TREC'11,null,null
32, Di-1,null,null
33,/ Di-1,null,null
34,184,null,null
35,20,null,null
36,80,null,null
37,124,null,null
38,# in TREC'12,null,null
39, Di-1,null,null
40,/ Di-1,null,null
41,178,null,null
42,21,null,null
43,97,null,null
44,102,null,null
45,-q,null,null
46,141,null,null
47,63,null,null
48,112,null,null
49,87,null,null
50,Total,null,null
51,"204 adjacent query pairs, 76 sessions",null,null
52,"199 adjacent query pairs, 98 sessions",null,null
53,2. USER AGENT: QUERY CHANGE AS A,null,null
54,FORM OF FEEDBACK,null,null
55,"We define a search session S ,"" {Q, D, C} as a combination of a series of queries Q "","" {q1, ..., qi, ..., qn}, retrieved document sets D "","" {D1, ..., Di, ..., Dn}, and clicked information C "","" {C1, .., Ci, ..., Cn}, where n is the number of queries in the session (i.e., the session length) and i indexes the queries. In TREC 2010-2012 Session tracks, each retrieved document set Di contains the top 10 retrieval results di1, ..., di10 ranked in decreasing relevance for qi. Each clicked data Ci contains the user-clicked documents, clicking order, and dwell time. For instance, for s6 q6, pocono mountains camelbeach hotel (Table 1), C6 tells us that the user clicked the 4th ranked search result, followed by the 2nd, with dwell time 15 seconds and 17 seconds, respectively.""",null,null
56,"TREC 2010-2012 Session Tracks aim to retrieve a list of documents for the current query, i.e. the last query qn in a session, ordered in decreasing relevance. Without loss of specificity, we assume that any query between q1 to qn could be the last query. We therefore study the problem of retrieving relevant documents for qi, given all previous queries q1 to qi-1, previous retrieval results D1 to Di-1, and previous clicked data C1 to Ci-1.",null,null
57,We define query change qi as the syntactic editing changes between two adjacent queries qi-1 and qi:,null,null
58,"qi , qi - qi-1",null,null
59,"qi can be written as a combination of the shared portion between qi and qi-1 and query change: qi , (qi qi-1)+qi.",null,null
60,"The query change qi comes from two sources. First, the added terms, which we call positive q, are new terms that the user adds to the previous query. Second, the removed terms, which we call negative q, are terms that the user deletes from the previous query. For example, in Table 1 s37, `US' and `policy' are the added terms; while in s28, `stock' and `market' are the removed terms.",null,null
61,"We call the common terms shared by two adjacent queries theme terms since they often represent the main topic of a session. For example, in Table 1 s37 the theme terms are ""Merck lobby"".2",null,null
62,"We thus decompose a query into three parts as theme terms, added terms, and removed terms and write it as:",null,null
63,"qi , (qiqi-1)+(+qi)-(-qi) , qtheme+(+qi)-(-qi)",null,null
64,"where qtheme are the theme terms, +qi and -qi represent added terms and removed terms, respectively.",null,null
65,"Our observations suggest that documents that have been examined by the user factor in deciding the next query change. We therefore propose the following important assumption between qi, the query change between adjacent",null,null
66,"2We perform K-stemming to all query terms. For instance, `lobbists' and `lobbying' are both stemmed to `lobby'.",null,null
67,454,null,null
68,Table 3: User agent's policies and actions about a query term t  qi-1. (Refer to sessions shown in Table 1),null,null
69,t  Di-1,null,null
70,user intention,null,null
71,1. find more information about t 2. satisfied & move to the next information need 3. satisfied,null,null
72,user likes Di-1 user action example,null,null
73,add new terms s85,null,null
74,t about t,null,null
75,q1  q2,null,null
76,remove t & add new terms t as new focus keep t,null,null
77,s6 q8  q9,null,null
78,theme term,null,null
79,type specification,null,null
80,drift,null,null
81,no change,null,null
82,user intention,null,null
83,user dislikes Di-1 user action example,null,null
84,5. remove the wrong remove t,null,null
85,s28,null,null
86,terms,null,null
87,q1  q2,null,null
88,6. not satisfied & move to the next information need 7. try different expression for t,null,null
89,remove t & add new terms t slight change of t to t,null,null
90,s6 q6  q7,null,null
91,s85 q2  q3,null,null
92,type generalization,null,null
93,drift,null,null
94,slight change,null,null
95,4. inspired by add terms t t / Di-1 terms t in Di-1 about t,null,null
96,s37 q1  q2,null,null
97,specification 8. try different ex- slight,null,null
98,s32,null,null
99,pression for t to get change of t q1  q2,null,null
100,more documents for t to t,null,null
101,slight change,null,null
102,"queries qi and qi-1, and Di-1, the search results for qi-1:",null,null
103,qi  Di-1.,null,null
104,"The assumption basically says that previous search results decide query change. In fact, previous search results Di-1 could influence query change qi in quite complex ways. For instance, the added terms in s37 (Table 1) q1 to q2, are `US' and `policy'. D1 contains several mentions of `policy', such as ""A lobbyist who until 2004 worked as senior policy advisor to Canadian Prime Minister Stephen Harper was hired last month by Merck"". However, these `policy'-related mentions are about ""Canada policy"" whereas the user adds ""US policy"" in q2. This suggests that the user might have been inspired by `policy' in D1, however he preferred the policy in US, not in Canada. Therefore, instead of simply cutting and pasting identical terms from Di-1, the user creates related terms to add for the next search.",null,null
105,"In another example, s28 (Table 1) q1, `stock' and `market' are frequent terms that are similar to stopwords. Documents in D1 are hence all about them and totally ignore the theme terms ""france world cup 98."" In q2, the user removes ""stock market"" to boost rankings for documents about the theme terms. In this case, removing terms is not only about generalization, but also about document re-ranking.",null,null
106,"To provide a convincing foundation for our approach, we look for evidence to support our assumption. We investigate whether qi (at least) appears in Di-1. Table 2 shows how often theme terms, added terms, and removed terms are present in Di-1 for both TREC 2011 and 2012 datasets. Around 90% of the time theme terms occur in Di-1 and most removed terms (>60%) appear in Di-1.3 Added terms are new terms for the previous query qi-1; we thus expect to see few occurrences of added terms in Di-1. Surprisingly, however, more than a third of them appear in Di-1. It suggests that it is quite probable that previous search results motivate the subsequent query change.",null,null
107,"Table 3 summarizes various types of query changes into possible policies for the user agent. This table mainly serves as a guide for us to design the policies for the search engine agent. We do not perform a thorough user study to validate this table. However, we believe that it is a good representative of various search scenarios and can help design a good session search agent.",null,null
108,"Along two dimensions, Table 3 summarizes the user agent's actions and possible policies. The dimensions are whether a previous query term t  qi-1 appears in previous search",null,null
109,3A third of query terms that do not appear in Di-1 are removed by the user.,null,null
110,"results Di-1 (the left most column) and whether the user likes Di-1 and the occurrence of t in Di-1 (the top most row). Combinations of the two dimensions yield 4 main cases (as in a contingency table) and 8 sub-cases. For each case, we identify four items: a rough guess of user intention, the user's actual action, an example, and the semantic exploration type for this action. For example, query change in s6 q8  q9, pocono mountains chateau resort attractions  pocono mountains chateau resort getting to can be interpreted as the following. Previous query term `attractions' appears in Di-1 and the user likes the returned documents Di-1. One possibility is that he is satisfied with what he reads and moves to the next information need. Therefore, the user removes `attraction' and adds new terms ""getting to"" as the new query focus. This is a drift in search focus. (case 2 in Table 3)",null,null
111,"We further group the cases in Table 3 by types of user actions, i.e., query change, and summarize them into:",null,null
112,"· Theme terms (qtheme), terms that appear in both qi-1 and qi. In fact, they often appear in many queries in a session. It implies a strong preference for those terms from the user. If they appear in Di-1, it shows that the user favors them since the user issues them again in qi. If they do not appear in Di-1, the user still favors towards them and insists to include them in the new query. This corresponds to t in cases 1 and 3 in Table 3.",null,null
113,"· Added terms (+q), terms that appear only in qi, not in qi-1. They indicate specification, destination of drifting, or destination of slight change. If they appear in Di-1, for the sake of novelty [14], they will not be favored in Di. If they do not appear in Di-1, which means that they are novel and the user favors them now. This corresponds to t in cases 1, 2, 6, 7, and 8, and t in case 4 in Table 3.",null,null
114,"· Removed terms (-q), terms that appear only in qi-1, not in qi. They indicate generalization, source of drifting, and source of slight change. If they appear in Di-1, removing them means that the user observes them and dislikes them. If they do not appear in Di-1, the user still dislikes the terms since they are not in qi anyway. This corresponds to t in cases 2, 5, 6, 7, and 8 in Table 3.",null,null
115,3. SEARCH ENGINE AGENT: STRATEGIES TO IMPROVE SEARCH,null,null
116,"The search engine agent observes query change from the user agent and takes corresponding actions. For each type of query change, theme terms, added terms, and removed terms, we propose to adjust the term weights accordingly for better retrieval accuracy. The search engine agent's action include",null,null
117,455,null,null
118,"Table 4: Search engine agent's policy. Actions are adjustments on the term weights.  means increasing,  means decreasing, and  means keeping the original term weight.",null,null
119,qtheme,null,null
120, Di-1 Y,null,null
121,N,null,null
122,+q,null,null
123,Y N,null,null
124,-q,null,null
125,Y N,null,null
126,action Example,null,null
127," ""pocono mountain"" in s6",null,null
128,"france world cup 98 reaction in s28, q1  q2",null,null
129," `policy' in s37, q1  q2  `US' in s37, q1  q2",null,null
130," `reaction' in s28, q2  q3  `legislation' in s32, q2  q3",null,null
131,"increasing, decreasing, and maintaining the term weights. Based on the observed query change as well as whether the query terms appeared in the previous search results Di-1, we can sense whether the user will favor the query terms in the current run of search. Table 4 illustrates the policies that we propose for the search engine agent.",null,null
132,"As shown in Section 2, theme terms qtheme often appear in many queries in a session and there is a strong preference for them. Thus, we propose to increase the weights of theme terms no matter whether they appeared in Di-1 or not (rows 1 and 2 in Table 4). In the latter case, if a theme term was not found in Di-1 (top retrieval results), it is likely that the documents containing them were ranked low. Therefore, the weights of theme terms need to be raised to boost the rankings of those documents (row 2 in Table 4). However, since theme terms are topic words in a session, they could appear like stopwords within the session. To avoid biasing too much towards them, we lower their term weights proportionally to their numbers of occurrences in Di-1.",null,null
133,"For added terms +q, if they occurred in previous search results Di-1, we propose to decrease their term weights for the sake of novelty [14]. For example, in s5 q1  q2, ""pocono mountains""""pocono mountains park"", the added term `park' appeared in a document in D5. If we use the original weight of `park', this document might still be ranked high in D2 and the user may dislike it since he read it before. We hence decrease added terms' weights if they are in Di-1 (row 3 in Table 4). On the other hand, if the added terms did not occur in Di-1, they are the new search focus and we increase their term weights (row 4 in Table 4). In an interesting case (s37 q1  q2), part of +q, `policy', occurred in D1 whereas the other part, `US', did not. To respect the user's preference, we increase the weight of `US' while decreasing that of `policy' to penalize documents about other `polices' including ""Canada policy"".",null,null
134,"For removed terms -q, if they appeared in Di-1, their term weights are decreased since the user dislikes them by deleting them (row 5 in Table 4). For example, in s28 q2  q3, `reaction' existed in D2 and is removed in q3. However, if the removed terms are not in Di-1, we do not change their weights since they are already removed from qi by the user (row 6 in Table 4).",null,null
135,"In the sections below, we follow policies proposed for the search engine agent as shown in Table 4 and incorporate them into a novel query change retrieval model (QCM).",null,null
136,4. MODELING SESSION SEARCH,null,null
137,"Markov Decision Process (MDP) [16, 28] models a state space S and an action space A. Its states S ,"" {s1, s2, ...} change from one to another according to a transition model T "","" P (si+1|si, ai), which models the dynamics of the entire""",null,null
138,"system. A policy (s) ,"" a indicates that at a state s, what""",null,null
139,are the actions a can be taken by the agent. In session,null,null
140,"search, we employ queries as states. Particularly, we denote",null,null
141,"q as state, T as the transition model P (qi|qi-1, ai-1), D",null,null
142,"as documents, and A as actions. Actions include keeping,",null,null
143,"adding, and removing query terms for the user agent and",null,null
144,"increasing, decreasing, and maintaining the term weights for",null,null
145,the search engine agent.,null,null
146,"In a MDP, each state is associated with a reward function",null,null
147,R that indicates possible positive reward or negative loss,null,null
148,"that a state and an action may result. In session search, we",null,null
149,consider the reward function to be the relevance function.,null,null
150,Reinforcement learning [16] offers general solutions to MDP,null,null
151,and seeks for the best policy for an agent. Each policy has,null,null
152,"a value associated with the policy and denoted as V(s),",null,null
153,which is the expected long-term reward starting from state,null,null
154,"s and continuing with policy  from then on. In a MDP, it",null,null
155,is believed that a future reward is not worth quite as much,null,null
156,"as a current reward and thus a discount factor   (0, 1) is",null,null
157,applied to future rewards. By considering the discount fac-,null,null
158,"tor, the value function starting from s0 for a policy  can be",null,null
159,"written as: V(s0) , E[R(s0) + R(s1) + 2R(s2) + ...] ,",null,null
160,E [,null,null
161," t,0",null,null
162,tR(si,null,null
163,optimal value V,null,null
164,)]. The  for a,null,null
165,Bellman equation [16] describes the state s in the long run and is often,null,null
166,used to obtain the best value for a MDP:,null,null
167,"V (s) ,"" max R(s, a) +  P (s |s, a)V (s )""",null,null
168,a s,null,null
169,"where s is the next state after s, V (s) and V (s ) are the optimal values for s and s .",null,null
170,"For session search, we observe that the influence of previous queries and previous search results to the current queries, becomes weaker and weaker. The user's desire for novel documents also supports this argument. We hence propose to employ the reinforcement learning model backwards. That is, instead of discounting the future rewards, we discount the past rewards, i.e. the relevant documents that appeared in the previous search results.",null,null
171,"We propose the query change retrieval model (QCM) as the following. We consider the task of retrieving relevant documents for qi as ranking documents based on the reward, i.e., how relevant it is to qi. Inspired by the Bellman equation, we model the relevance of a document d to the current query qi as:",null,null
172,"Score(qi, d) ,"" P (qi|d)+ P (qi|qi-1, Di-1, a) max P (qi-1|Di-1)""",null,null
173,a,null,null
174,Di-1,null,null
175,(1),null,null
176,which recursively calculates the reward starting from q1 and,null,null
177,continues with the search engine agent's policy until qi.  ,null,null
178,"(0, 1) is the discount factor, maxDi-1 P (qi-1|Di-1) is the maximum of the past rewards, P (qi|d) is the current reward,",null,null
179,"and P (qi|qi-1, Di-1, a) is the query transition model.",null,null
180,"The first component in Eq.1, P (qi|d), measures the rel-",null,null
181,evance between qi and a document d that is under evalu-,null,null
182,ation. This component can be estimated by the Bayesian,null,null
183,"belief network model [27]: P (qi|d) ,"" 1 - tqi (1 - P (t|d)), where P (t|d) is calculated by the multinomial query genera-""",null,null
184,"tion language model with Dirichlet smoothing [33]: P (t|d) ,",null,null
185,"#(t,d)+µP |d|+µ",null,null
186,(t|C),null,null
187,",",null,null
188,where,null,null
189,"#(t, d)",null,null
190,denotes,null,null
191,the,null,null
192,number,null,null
193,of,null,null
194,occur-,null,null
195,"rences of term t in document d, P (t|C) calculates the prob-",null,null
196,ability that t appears in corpus C based on Maximum Like-,null,null
197,456,null,null
198,"lihood Estimation (MLE), |d| is the document length, and",null,null
199,ognize added terms +q and removed terms -q. Gener-,null,null
200,µ is the Dirichlet smoothing parameter (set to 5000).,null,null
201,"ally, the terms that occur in the current query but not in",null,null
202,The remaining challenges of calculating Eq.1 include max-,null,null
203,the previous query constitute +q; while the terms occur in,null,null
204,"imizing the reward function maxDi-1 P (qi-1|Di-1) and estimating the transition model P (qi|qi-1, Di-1, a). They are",null,null
205,"the previous query but not in the current query constitute -q. In the above example, -q7 ,"" """"camelbeach hotel"""",""",null,null
206,"described in Section 4.1 and Section 4.2, respectively.",null,null
207,"and +q7 ,"" """"chateau resort"""".""",null,null
208,4.1 Maximizing the Reward Function,null,null
209,"The search engine actions are decreasing, increasing, and maintaining term weights. According to Table 4 rows 3 and",null,null
210,"When considering the past/future rewards, MDP uses only 5, we decrease a term's weight if the query change, either",null,null
211,the optimal (the maximum possible) values from those past,null,null
212,"+q or -q, occurred in the effective previous search re-",null,null
213,/future rewards. This is reflected in maxDi-1 P (qi-1|Di-1) as part of Eq. 1.,null,null
214,"sults Di-1. We propose to deduct term t's weight by P (t|d), i.e. t's default contribution to the relevance score between",null,null
215,"Prior research [10, 22] suggests that Satisfying (SAT) clicks, i.e., clicked documents with dwell time longer than 30 seconds [10, 22], are probably the only ones that are effective",null,null
216,"qi and the document under evaluation (denoted as d). Furthermore, since t already occurred in Di-1, for the sake of novelty, we deduct more weight that is proportional to t's",null,null
217,at predicting user behaviors and relevance judgments. Since,null,null
218,"the user also skims snippets in search interactions, in this",null,null
219,"work, we consider both the top 10 returned snippets and",null,null
220,SAT clicks as effective previous search results and denote them as Die-1.,null,null
221,To obtain an maximum reward from all possible reward,null,null
222,"frequency in Di-1 such that the more frequently t occurred in Di-1, the more heavily t's weight is deducted from the current query qi and d. We formulate this weight deduction for a term t  +q or t  -q as:",null,null
223,"log Pnew(t|d) , (1 - P (t|di-1)) log P (t|d)",null,null
224,(2),null,null
225,"functions P (qi-1|di-1), i.e., the text relevance of previous query qi-1 and all previous search results di-1  Di-1, we",null,null
226,"where di-1 denotes the maximum rewarded document, d is the document under evaluation, and P (t|d) is calculated by",null,null
227,"propose to generate a maximum rewarding document, denoted as di-1. We further propose that the candidates for the di-1 should only be selected from the effective previous search results Die-1. We define di-1 as the document(s) that is the most relevant to qi-1. To discover di-1, we first",null,null
228,rank all the documents (either a snippet or a document),null,null
229,MLE. We apply the log function to avoid numeric underflow. We notice that Eq. 2 has an interesting connection with,null,null
230,the Kullback-Leibler divergence (KL divergence) [33]:,null,null
231,-,null,null
232,P,null,null
233,(t|di-1),null,null
234,log,null,null
235,P,null,null
236,(t|d),null,null
237,",",null,null
238,P,null,null
239,(t|di-1),null,null
240,log,null,null
241,P,null,null
242,1 (t|d),null,null
243,"di-1  Die-1 by measuring the relevance between qi-1 and di-1 as: P (qi-1|di-1) ,"" 1 - tqi-1 {1 - P (t|di-1)}, where""",null,null
244,"ra,nk",null,null
245,P,null,null
246,(t|di-1),null,null
247,log,null,null
248,P (t|di-1) P (t|d),null,null
249,(3),null,null
250,P (t|di-1),null,null
251,is,null,null
252,calculated by MLE: P (t|di-1),null,null
253,",",null,null
254,", #(t,di-1 )",null,null
255,|di-1 |,null,null
256,"#(t, di-1)",null,null
257,"is the number of occurrences of term t in document di-1, and",null,null
258,"ra,nk KLDt di-1 ||d",null,null
259,"|di-1| is the document length. We do not apply smoothing here since P (t|di-1) can be zero, i.e., t / Die-1. In fact, we rely on this property in later calculation.",null,null
260,"After ranking documents di-1 in Die-1, we generate di-1 by the following options: (1) using the document with the largest P (qi-1|di-1), (2) concatenating the top k documents in Die-1 with the largest P (qi-1|di-1), or (3) concatenating all documents in Die-1. Experiments show that option (1) works the best and we use this setting throughout the paper. For notation simplicity, we use Di-1 from now on to denote effective previous search results.",null,null
261,4.2 Estimating the Transition Model,null,null
262,"where KLDt di-1 ||d denotes the contribution of term t to the KL divergence between two documents' language models di-1 and d. In Eq. 3, the larger the divergence between di-1 and d, the more novel document d is compared to Di-1, and the less deduction to the relevance score. In this sense, Eq. 2 models novelty for the added terms and the removed terms during a query transition.",null,null
263,"According to Table 4 row 4, we increase a term's weight if it is an added term and did not occur in Di-1. We propose to raise the term weight proportional to its inverse document frequency (idf). This is to make sure that while increasing a preferred term's weight, we avoid increasing its weight",null,null
264,"The transition model indicated in Eq. 1 is a P (qi|qi-1, Di-1, a). It includes the probabilities of query transitions",null,null
265,under various actions. We incorporate polices designed in,null,null
266,too much if it is a common term in many documents. We formulate this weight increase for a novel added term t (t  +q and t / Di-1) as:,null,null
267,Table 4 to calculate it. Search engine agent performs actions based on user agent's,null,null
268,"log Pnew(t|d) , (1 + idf (t)) log P (t|d)",null,null
269,(4),null,null
270,"actions. We need to identify user's actions, i.e. query change",null,null
271,where idf (t) is the inverse document frequency of t in Corpus,null,null
272,"q before search engine takes actions. Particularly, we rec-",null,null
273,C and P (t|d) is calculated by MLE. Note that this term,null,null
274,"ognize q by the following procedure. First, we generate",null,null
275,weight adjustment is in a form of tf-idf.,null,null
276,qtheme based on the Longest Common Subsequence (LCS),null,null
277,"The increasing in term weights also applies to theme terms,",null,null
278,[11] in both qi-1 and qi. A subsequence is a sequence that,null,null
279,which corresponds to rows 1 and 2 in Table 4. Theme terms,null,null
280,appears in two strings in the same relative order but is not,null,null
281,"repeatedly appear in a session, which implies the impor-",null,null
282,necessarily continuous. The LCS can be the common prefix,null,null
283,"tance of them. Similar to the novel added terms, we should",null,null
284,or the common suffix of the two queries; it can also consist,null,null
285,avoid increasing their weights too much. We could discount,null,null
286,of several discontinuous common parts from the two queries.,null,null
287,"the increment proportional to idf. However, theme terms",null,null
288,"Take s6 q6  q7 as an example: q6,""pocono mountains",null,null
289,"are topical/common terms within a session, not necessarily",null,null
290,"camelbeach hotel, q7"",""pocono mountains chateau resort,""",null,null
291,"common terms in the entire corpus. Therefore, idf may not",null,null
292,"qtheme ,"" LCS(q6, q7) "","" """"pocono mountains"""". Next, we rec-""",null,null
293,be applicable here. We hence employ the negation of the,null,null
294,457,null,null
295,"number of occurrences of t in previous maximum rewarding document, 1 - P (t|di-1), to substitute idf. We formulate this weight increase for a theme term t  qtheme as:",null,null
296,"log Pnew(t|d) , (1 + (1 - P (t|di-1))) log P (t|d) (5)",null,null
297,where di-1 denotes the maximum rewarded document and P (t|d) is calculated by MLE.,null,null
298,Table 5: Dataset statistics for TREC 2011 and 2012 Session.,null,null
299,#topics #sessions #queries #dups,null,null
300,2011 62 76 280 16,null,null
301,2012 48 98 297 5,null,null
302,#queries/session #sessions/topic #pages judged #sessions w/o rel. docs,null,null
303,"2011 3.68 1.23 19,413",null,null
304,2,null,null
305,"2012 3.03 2.04 17,861",null,null
306,4,null,null
307,Next we determine exact string matches between every query,null,null
308,"For removed terms that did not appear in Di-1 (Table 4 row 6), the search agent does not change their term weights.",null,null
309,pair. The exactly matched query pairs are identified as duplicated queries.,null,null
310,By considering all possible cases for the transition model,null,null
311,Since the user may dislike the queries and their corre-,null,null
312,"as defined in Eq. 1, the relevance score between the current",null,null
313,"sponding search results between two duplicated queries, we",null,null
314,query qi and a document d is represented as below:,null,null
315,propose to eliminate from the MDP the undesired queries,null,null
316,"Score(qi, d) , log P (qi|d) + ",null,null
317,and their interactions. We achieve this by setting the dis[1 - P (t|di-1)] log P (t|d) count factor to zero for any interaction between two dupli-,null,null
318,tqtheme,null,null
319,cated queries as well as that for the earlier query in the two.,null,null
320,-,null,null
321,P (t|di-1) log P (t|d) +,null,null
322,idf (t) log P (t|d),null,null
323,The new discount factor  can be calculated as:,null,null
324,t+q tdi-1,null,null
325,-,null,null
326,P (t|di-1) log P (t|d),null,null
327,t+q t/di-1,null,null
328,"i ,",null,null
329,0 i,null,null
330,"{i|i  [j, k), qj ,"" qk, j < k)} otherwise""",null,null
331,(9),null,null
332,t-q,null,null
333,"(6) where , , , and  are parameters for each types of actions. Note that we apply different parameters  and  on +q and -q, since added terms and removed terms may affect the retrieval differently. We report the parameter selection in Section 6.",null,null
334,"where i is the original discount factor for the ith query, i is the updated discount factor for the ith query after deduplication.",null,null
335,"For the above example s6, the effects from q2 and q3 on the session are eliminated. The entire session is now equivalent to q1, q4, q5, ..., q11.",null,null
336,4.3 Scoring the Entire Session,null,null
337,"It is worth noting that Eq. 6 is valid only when i > 1. When i ,"" 1, there is no previous result for q1. We thus use""",null,null
338,"Score(q1, d) , log P (q1|d)",null,null
339,(7),null,null
340,as a base case. P (q1|d) is calculated by Eq. 4. Using Eq. 7 as the base case for the recursive function,null,null
341,"described in Eq. 1, we obtain the overall document relevance score Scoresession(qn, d) for a session that starts at q1 and ends at qn by considering all queries in the session:",null,null
342,"Scoresession(qn, d) ,"" Score(qn, d) + Scoresession(qn-1, d)""",null,null
343,","" Score(qn, d) +  [Score(qn-1, d) + Scoresession(qn-2, d)]""",null,null
344,n,null,null
345,","" n-iScore(qi, d)""",null,null
346,"i,1",null,null
347,(8),null,null
348,"where q1, q2, · · · , qn are in the same session, and   (0, 1) is the discount factor. Eq. 8 provides a form of aggregation over the relevance functions of all the queries in a session.",null,null
349,5. DUPLICATED QUERIES,null,null
350,"Duplicated queries sometimes occur in a search session. Prior work shows that removing duplicated queries could effectively boost the search accuracy [8, 19]. Duplicated queries often occur when a user is frustrated by irrelevant documents in search results and comes back to one of the previous queries for a fresh start. For example, in s6 (Table 1), q2 and q4 are duplicates and both search for pocono mountains pennsylvania hotels. The query between them is q3: pocono mountains pennsylvania things to do. It suggests that the user might dislike the search results for q3 and he returns to q2 to search again (q2 , q4).",null,null
351,"To detect query duplicates, we first remove punctuations and white spaces in queries, then apply stemming on them.",null,null
352,6. EVALUATION,null,null
353,"The evaluation datasets are from TREC 2011 and 2012 Session tracks [18, 19]. Table 5 lists the statistics about these two datasets. Each search session includes several queries and the corresponding search results. The users (NIST assessors) were given a topic description about information needs before they searched. For example, s85 (Table 1) are related to topic 43 ""When is scientific glass blowing used? What are the purposes? What organizations do scientific glass blowing?"" Multiple sessions can relate to the same topic. The search engine used to create the sessions was Yahoo! BOSS. The top 10 returned documents were shown to the users and they clicked documents that were interesting to them and interacted with the system. We use TREC's official ground truth and official evaluation metrics nDCG@10 and MAP.",null,null
354,"The corpus used in this evaluation is ClueWeb09 Category B collection (CatB).4 CatB contains the first 50 million English pages crawled from the Web during January to February 2009. We filter out the spam documents by removing documents whose WateQCMoo's ""GroupX"" spam ranking scores [6] are less than 70.",null,null
355,We compare the following systems in this evaluation:,null,null
356,· Lemur : Directly submitting the current query qn (with punctuations removed) to the Lemur search engine [21] (language modeling + Dirichlet smoothing) and obtain the returned documents.,null,null
357,"· TREC best : The top TREC system as reported by NIST [13, 14]. It adopts a query generation model with relevance feedback and handles document novelty. CatB was used in their TREC submissions. This system is used as the baseline system in this evaluation.",null,null
358,· Nugget: Another top TREC 2012 session search system groups semantically coherent query terms as nuggets and,null,null
359,4http://lemurproject.org/clueweb09/.,null,null
360,458,null,null
361,"Figure 2: nDCG@10 for TREC 2012 against the parameters. (a), (b), (c), and (d) are about , , , and  respectively.",null,null
362,"Table 6: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2012 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  at p < 0.05 level.",null,null
363,Approach Lemur TREC median Nugget TREC best,null,null
364,QCM,null,null
365,QCM+Dup,null,null
366,nDCG@10 0.2474 0.2608 0.3021 0.3221,null,null
367,0.3353,null,null
368,0.3368,null,null
369,%chg -21.54% -17.29% -4.19% 0.00% 4.10% 4.56%,null,null
370,MAP 0.1274 0.1440 0.1490 0.1559,null,null
371,0.1529,null,null
372,0.1537,null,null
373,%chg -18.28% -7.63% -4.43% 0.00%,null,null
374,-1.92%,null,null
375,-1.41%,null,null
376,"Table 7: nDCG@10, MAP, and their improvements over the baseline (%chg) for TREC 2011 sessions. The runs are sorted by nDCG@10. A statistical significant improvement over the baseline is indicated with a  at p < 0.05 level.",null,null
377,Approach Lemur TREC median TREC best,null,null
378,QCM,null,null
379,QCM+Dup Nugget,null,null
380,nDCG@10 0.3378 0.3544 0.4409,null,null
381,0.4728,null,null
382,0.4821 0.4836,null,null
383,%chg -23.38% -19.62% 0.00% 7.24% 9.34% 9.68%,null,null
384,MAP 0.1118 0.1143 0.1508,null,null
385,0.1713,null,null
386,0.1714 0.1724,null,null
387,%chg -25.86% -24.20% 0.00% 13.59% 13.66% 14.32%,null,null
388,"creates structured Lemur queries [8]. We re-implement and apply it on both TREC 2011 and 2012. · TREC median: The median TREC system as reported by NIST [18, 19]. · QCM : The proposed query change retrieval model. · QCM + De-Duplicate (Dup): The proposed query change retrieval model with duplicated queries removed.",null,null
389,6.1 Search Accuracy,null,null
390,"Table 6 and Table 7 demonstrate search accuracy for all systems under comparison for TREC 2012 and TREC 2011, respectively. The evaluation metrics are nDCG@10 and MAP, the same as in the official TREC evaluations. TREC best serves as the baseline.",null,null
391,"Table 6 shows that the proposed QCM approach outperforms the best TREC 2012 system on nDCG@10 by 4.1%, which is statistically significant (one sided t-test, p , 0.05). The search accuracy is further improved by 0.46% through removing the duplicated queries. The experimental results strongly suggest that our approach is highly effective.",null,null
392,"Table 7 shows that for TREC 2011, our approach again outperforms the baseline by a statistically significant 7.24% (one sided t-test, p ,"" 0.05) and achieves a further improvement of 9.34% by the QCM+Dup approach. For TREC 2011, the performance gain by performing de-dup is 2.1%, which is bigger than that for TREC 2012 (0.46%). The reason is probably because that TREC 2012 only has 5 duplicated queries while TREC 2011 has 16 (shown in Table""",null,null
393,"5). However, the best approach for TREC 2011 is the nugget approach, which is slightly better than QCM+Dup.",null,null
394,"Table 5 illustrates the dataset differences between TREC 2011 and 2012. These differences may affect search accuracy. The average number of sessions per topic is 2.04 in 2012, that is more than that in 2011 (1.23). Moreover, on average, TREC 2012 sessions contain less queries per session (3.03) than 2011 (3.68). As a result, the shorter sessions in 2012 may make the search task more difficult than 2011 since less information are provided by previous interactions. Another difference is that 2012 sessions have fewer (sometimes even none) relevant documents than 2011 sessions in CatB ground truth. It unavoidably hurts the performance for any retrieval system. Generally, we observe lower search accuracy in 2012 (Table 6) than in 2011 (Table 7).",null,null
395,6.2 Parameter Tuning,null,null
396,"We investigate good values for parameters in Eq. 6. A supervised learning-to-rank method should be able to find the optimal values for those parameters. However, in this paper, we take a step-by-step parameter tuning procedure and leave the supervised learning method as future work.",null,null
397,"We add each component, i.e., theme terms, added terms, and removed terms, one by one into Eq. 6. The tuning is performed for QCM only and the parameters are shared between QCM and QCM+Dup.",null,null
398,"First, we plot nDCG@10 against  while setting other parameters to 0 (Figure 2(a)).  represents the parameter for theme terms.  ranges over [1.1, 2.5] by an interval of 0.1. We notice that nDCG@10 reaches its maximum at  , 2.2. We find 2 other local maximums at 1.6 and 1.2 for .",null,null
399,"Next, we fix  to the above values and plot nDCG@10 against  (Figure 2(b)).  is the parameter for added terms that appeared in effective previous search results; we call them old added terms.  ranges over [1.0, 2.4] by an interval of 0.2. We choose the top 2 local values from each curve and pick 6 combinations for (, ) as indicated in Figure 2(c).",null,null
400,"Then, we fix (, ) and plot nDCG@10 against (Figure 2(c)). is the parameter for added terms that did not appear in effective previous search results; we call them novel added terms. ranges over [0.05, 0.1] by an interval of 0.01. All the curves show similar trends and reach the highest nDCG@10 at around 0.07. We hence fix to 0.07.",null,null
401,"Finally, we plot nDCG@10 against  (Figure 2(d)) with the parameter combinations that we discover eerlier. Eventually, nDCG@10 reaches its peak 0.3353 at  ,"" 2.2,  "","" 1.8, "","" 0.07, and  "", 0.4. We apply this set of parameters to both QCM and QCM+Dup.",null,null
402,"As we can see, , , and  are much larger than . This is because that in Eq. 4, idf (t) ,"" log(N/nd) falls in the range of [1, 10], while in Eq. 2 and Eq. 5, P (t|di-1) falls""",null,null
403,459,null,null
404,"by an interval of 0.02. Figure 3 illustrates the relationship between nDCG@10 and . nDCG@10 climbs to its peak 0.3368 when  ,"" 0.92. The result suggests that a good discount factor  is very close to 1, implying that previous queries contribute to the overall search accuracy nearly the same as the last query. It suggests that in QCM, a discount between two adjacent queries should be mild.""",null,null
405,7. DISCUSSION,null,null
406,Figure 3: Discount factor .,null,null
407,Figure 4: Error types.,null,null
408,7.1 Advantages of Our Approach,null,null
409,Table 8: Aggregation schemes.,null,null
410,A main contribution of our approach is that we treat a search session as a continuous process by studying changes,null,null
411,Approach,null,null
412,Query change model,null,null
413,Aggregation Scheme,null,null
414,Uniform PvC,null,null
415,Distance-based,null,null
416,"qn 1 n , 1 n , 1 - p",null,null
417,"n , 1 - p",null,null
418,"qi(i  [1, n - 1])  n-i",null,null
419,"i , 1",null,null
420,"i , p",null,null
421,i,null,null
422,",",null,null
423,p n-i,null,null
424,"among query transitions and modeling the dynamics in the entire session. Through the reinforcement learning style framework, our system provides the best aggregation scheme for all queries in a session (Table 9). This allows us to better handle sessions that demonstrate evolution and exploration",null,null
425,Table 9: nDCG@10 for various aggregation schemes. p is 0.4 in PvC.  is 0.92 in QCM and QCM+Dup. TREC 2012 best serves as the baseline. A significant improvement over the baseline is indicated with a  at p < 0.05 level.,null,null
426,Aggregation Scheme,null,null
427,Distance-based TREC best,null,null
428,Uniform,null,null
429,TREC 2011,null,null
430,nDCG@10 %chg,null,null
431,0.4431,null,null
432,-2.40%,null,null
433,0.4540,null,null
434,0.00%,null,null
435,0.4626,null,null
436,1.89%,null,null
437,TREC 2012,null,null
438,nDCG@10 %chg,null,null
439,0.3111 -3.42%,null,null
440,0.3221,null,null
441,0.00%,null,null
442,0.3316,null,null
443,2.95%,null,null
444,"in nature than most existing systems do. On the contrary, for sessions that are clear in search goals and lack of a exploratory nature, the advantage of our system over other systems looks less significant.",null,null
445,"This can be seen in Table 10, which illustrates the search accuracy for the TREC best, Nugget, and our system for various classes of sessions. The TREC best is used as the baseline and we also show the percentile improvement over it in Table 10. TREC 2012 sessions were created by consider-",null,null
446,PvC,null,null
447,0.4713,null,null
448,3.81%,null,null
449,0.3351,null,null
450,4.04%,null,null
451,ing and hence can be classified into two facets: search target,null,null
452,QCM,null,null
453,0.4728 4.14% 0.3353 4.10% (factual or intellectual) and goal quality (specific/good or,null,null
454,QCM+Dup,null,null
455,0.4821 6.19% 0.3368 4.56%,null,null
456,"amorphous/ill) [19]. Table 10 shows that QCM works very well for all classes of sessions. Specifically, QCM works even",null,null
457,"in the range of [0,0.1]. Therefore, the values of are two",null,null
458,"better, i.e. outperforms the TREC best even more signifi-",null,null
459,magnitudes less than that for the other parameters. Among,null,null
460,"cantly, for sessions that search for intellectual targets as well",null,null
461,", , and , we find that  and  are larger than , which",null,null
462,as sessions that search with amorphous goals. In our opin-,null,null
463,implies that theme terms and added terms may play more,null,null
464,"ion, this is due to that intellectual tasks produce new ideas",null,null
465,important roles in session search than removed terms.,null,null
466,or new findings (e.g. learn about a topic or make decision,null,null
467,6.3 Aggregation for the Entire Session,null,null
468,based on the information collected so far) while searching. Both intellectual and amorphous sessions rely more on pre-,null,null
469,QCM proposes an effective way to aggregate all queries in,null,null
470,"vious search results. Thus, users reformulate queries based",null,null
471,a session as in Eq.8. We compare how effective it is to prior,null,null
472,"more on what they have retrieved, not the vague informa-",null,null
473,query aggregation methods. A query aggregation scheme,null,null
474,tion need. This is a scenario where our approach is good at,null,null
475,"can be represented as: Score(session, d) ,",null,null
476,"n i,1",null,null
477,i,null,null
478,·,null,null
479,"Score(qi,",null,null
480,"d),",null,null
481,since,null,null
482,we,null,null
483,employ,null,null
484,previous,null,null
485,search,null,null
486,results,null,null
487,to,null,null
488,guide,null,null
489,the,null,null
490,search,null,null
491,"where Score(qi, d) is the relevance scoring function of d and",null,null
492,"engine's action. For specific and factual sessions, users are",null,null
493,qi and i is the query weight for qi.,null,null
494,"clearer in search goals, query changes may come less from the",null,null
495,[8] proposed several aggregation schemes for TREC 2012,null,null
496,"previous search results. In summary, our good performance",null,null
497,Session track. The schemes are: uniform (all queries are,null,null
498,on both intellectual task and amorphous task is consistent,null,null
499,"equally weighted), previous vs. current (known as PvC;",null,null
500,with our efforts of modeling query changes.,null,null
501,"all previous queries are discounted by p, while the current",null,null
502,"Moreover, we benefit from term-level manipulation in var-",null,null
503,"query uses a complementary and higher coefficient (1 - p),",null,null
504,ious aspects in our system. The first aspect is novelty. Both,null,null
505,and distance-based (previous queries are discounted based,null,null
506,the TREC best system and our system handle novelty in,null,null
507,on a reciprocal function of queries' positions in the session).,null,null
508,a session. The TREC best system only deals with novelty,null,null
509,We express various query aggregation schemes in terms,null,null
510,at the document level. They consider documents that have,null,null
511,of the discount factor  in order to compare them with our,null,null
512,been examined by the user in a previous interaction not,null,null
513,"approach. From Table 8, we find that QCM degenerates to",null,null
514,"novel and the rest are novel [14]. That is, they determine",null,null
515,"uniform when  , 1. Previous queries in PvC and Distance-",null,null
516,"novelty purely based on document identification number, not",null,null
517,"based schemes are also discounted as they are in QCM, but",null,null
518,the actual content. Through studying whether query terms,null,null
519,with different decay functions.,null,null
520,"appeared in previous search results, our approach evaluates",null,null
521,The search accuracy for different aggregation schemes are,null,null
522,"and models novelty at the term level (or concept level),",null,null
523,compared in Table 9. QCM performs the best for both,null,null
524,which we believe better represents the evolving informa-,null,null
525,TREC 2011 and 2012. The PvC scheme is the second best,null,null
526,tion needs in a session. The second aspect is query han-,null,null
527,"scheme, which confirms what is reported in [8]. The Distance- dling. The Nugget approach [8] treats queries at the phrase",null,null
528,based scheme gives the worst performance.,null,null
529,level and formulates structured queries based on phrase-like,null,null
530,"We explore the best discount factor  for QCM over (0, 1)",null,null
531,"nuggets. The approach achieves good performance, espe-",null,null
532,460,null,null
533,Table 10: nDCG@10 for different classes of sessions in TREC 2012.,null,null
534,TREC best Nugget QCM QCM+DUP,null,null
535,Intellectual 0.3369 0.3305 0.3870 0.3900,null,null
536,%chg 0.00% -1.90% 14.87% 15.76%,null,null
537,Amorphous 0.3495 0.3397 0.3689 0.3692,null,null
538,%chg 0.00% -2.80% 5.55% 5.64%,null,null
539,Specific 0.3007 0.2736 0.3091 0.3114,null,null
540,%chg 0.00% -9.01% 2.79% 3.56%,null,null
541,Factual 0.3138 0.2871 0.3066 0.3072,null,null
542,%chg 0.00% -8.51% -2.29% -2.10%,null,null
543,"cially for TREC 2011. However, due to complexity in natural language, nugget detection is sensitive to dataset and the approach's performance is not quite as stable as ours on different datasets.",null,null
544,"Lastly, our system benefits from trusting the user. Our approach does not use too much materials from other resources such as anchor texts, meta data, or click orders, as many other approaches do [8, 26]. We believe that the most direct and valuable feedback is the next query that the user enters. In this work, we manage to capture the query change and investigate the reasons behind it. We use ourselves as users to summarize possible human users' reasoning and actions. More detailed analysis about user intent might be useful for researchers to understand web users, however, it might be overwhelming (too fine-grained or too much semantics) for a search engine that essentially only counts words.",null,null
545,7.2 Error Analysis & Future Work,null,null
546,"Our system retrieves nothing for 22 out of 98 sessions in TREC 2012. To analyze the reason for the poor performance for those sessions, we study their topic descriptions, queries, and ground truth documents. We summarize the types of errors as ""two theme concepts"", ""ill query"", ""few relevant documents"", and others. Figure 4 shows how many sessions that we fail to retrieve under each error type.",null,null
547,"We call the first type of errors ""two theme concepts"". It comes from a type of session where the information need cover more than one concepts. For instance, s17 and s18 share the the same topic ""... To what extent can decisions and policies of the Indian government be credited with these wins?"". Queries in s17 and s18 ask about both concepts ""indian politics' and ""miss universe"". Unfortunately, very few relevant documents about both theme concepts exist in the corpus. The retrieved documents are about either concept, but none is about both. Eight sessions belong to this type. As future work, we can improve our system by incorporating structures in queries, and enable more sophisticated operators such as Boolean and proximity search.",null,null
548,"The second type of errors is ""ill query"", where in such sessions, queries themselves are ill formulated and do not well-represent the information needs indicated in the given topic. A common mistake is that the user misses some subinformation need. For example, the topic for s16 is: ""... you want to reduce the use of air conditioning in your house ... you could protect the roof being overly hot due to sun exposure... Find information of ... how it could be done."" A good query for this topic should include roof and air conditioning. However, the queries that the user issued for s60, ""reduce airconditioning"" and ""attic insulation air conditioning costs"", do not mention roof at all. Because of this ill query formulation, our system yields no relevant documents for s60. On the other hand, for s59, which shares the same information need with s60, our system achieves a nDCG@10 of 0.48 simply because s59 queries ""cool roof"". It suggests that ill queries mislead the search engine and yield poor retrieval performance. Four sessions belong to this type. As",null,null
549,"future work, we will explore effective query suggestion by studying sessions that share the same topic.",null,null
550,"The third type of errors is ""too few relevant documents"". For sessions with too few relevant documents in the ground truth, our system do not perform well. In total 2,573 relevant documents exist in CatB for all 48 TREC 2012 topics; on average 53.6 relevant documents per topic. However, topics 10, 45, 47 and 48, each has no more than 2 relevant documents and topic 47 (s92 to s95) has no relevant document in CatB (Table 5). This problem could be reduced if we index the entire ClubWeb09 CatA collection.",null,null
551,"Figure 4 also indicates in which classes of sessions these errors lie. We find that all ""two theme concept"" errors belong to sessions created with amorphous goals while all ""too few relevant documents"" errors belong to those with specific goals. Moreover, ""ill queries"" tend to occur more in sessions with amorphous goals. Note that ""ill query"" and ""few relevant documents"" are errors due to either the user or the data. There might not be much room for our system to improve over them. However, ""two theme concepts"" is where our system can certainly make further improvements.",null,null
552,8. RELATED WORK,null,null
553,"Session search is a challenging IR task [4, 8, 13, 14, 25, 32]. Existing approaches investigate session search from various aspects such as semantic meanings of search tasks [23], document novelty [14], and phrase structure in queries [8]. The best TREC system [13, 14] employs an adaptive browsing model by considering both relevance and novelty; however it does not demonstrate improvement by handling novelty. In this paper, we successfully model query and document novelty by investigating the relationship between query change and previous search results. Moreover, our analysis on query change does not require knowledge of semantic types for the sessions as [23] proposed.",null,null
554,"Our proposed work is perhaps the most similar to the problem of query formulation [1, 9, 12, 24] and query suggestion [29]. [12] showed that certain query changes such as adding/removing words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. The finding is generally consistent with our view of query change. However, their work only emphasized on understanding of query changes, without showing how to apply it to help session search. [24] examined the relationship between task types and how users change queries. They classified query changes by semantic types: Generalization, Specialization, Word Substitution, Repeat, and New. Similar to [12], however, [24] stopped at understanding query changes and didn't apply their findings to help session search. This probably makes us the first to utilize query changes in actual retrieval. [1] derived queryflow graph, a graph representation of user query behavior, from user query logs. The approach detected query chains in the graph and recommended queries based on maximum weights, random walk, or just the previous query. Other mining approaches [1, 29] identify the importance of query",null,null
555,461,null,null
556,"change in sessions; however, they require the luxury of large user query logs.",null,null
557,"This research is perhaps the first to employ reinforcement learning to solve the Markov Decision Process demonstrated in session search. Reinforcement learning is complex and difficult to solve. Its solutions include model-based approaches and model-free approaches [16]. The former learn the transition model and the reward function for every possible states and actions and mainly employ MLE to estimate the model parameters. Others also use matrix inversion or linear programming to solve the Bellman equation. It works well when state spaces are small. However, in our case, the state space is large since we use natural language queries as the states; hence we could not easily apply model-based approaches in practice. In this work, we effectively reduce the search space by summarizing users' and search engine's actions into a few types and employ a model-free approach to learn value functions directly.",null,null
558,9. CONCLUSION,null,null
559,"This paper presents a novel session search approach (QCM) by utilizing query change and modeling the dynamic of the entire session as a Markov Decision Process. We assume that query change is an important form of feedback. Based on this assumption, through studying editing changes between adjacent queries, and their relationship with previous retrieved documents, we propose corresponding search engine actions to handle individual term weights for both the query and the document. In a reinforcement learning inspired framework, we incorporate various ingredients present in session search, such as query changes, satisfactory clicks, desire for document novelty, and duplicated queries. The proposed framework provides a theoretically sound and general foundation that allows more novel features to be incorporated. Experiments on both TREC 2011 and 2012 Session tracks show that our approach is highly effective and outperforms the best session search systems in TREC. This research is perhaps the first to employ reinforcement learning in session search. Our MDP view of modeling session search can potentially benefit a wide range of IR tasks.",null,null
560,10. ACKNOWLEDGMENT,null,null
561,"This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.",null,null
562,11. REFERENCES,null,null
563,"[1] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, and S. Vigna. The query-flow graph: model and applications. In CIKM '08.",null,null
564,"[2] I. Bordino, C. Castillo, D. Donato, and A. Gionis. Query similarity by projecting the query-flow graph. In SIGIR '10.",null,null
565,"[3] P. Bruza, R. McArthur, and S. Dennis. Interactive internet search: keyword, directory and query reformulation mechanisms compared. In SIGIR '00.",null,null
566,"[4] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM '11.",null,null
567,"[5] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and attention in exploratory health search. In SIGIR'11.",null,null
568,"[6] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.",null,null
569,[7] D. Guan and H. Yang. Increasing stability of result organization for session search. In ECIR '13.,null,null
570,"[8] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.",null,null
571,"[9] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and discriminative model for query refinement. In SIGIR '08.",null,null
572,[10] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting web searcher goals from interaction data. In SIGIR '10.,null,null
573,"[11] D. S. Hirschberg. Algorithms for the longest common subsequence problem. J. ACM, 24(4), Oct. 1977.",null,null
574,[12] J. Huang and E. N. Efthimiadis. Analyzing and evaluating query reformulation strategies in web search logs. In CIKM '09.,null,null
575,"[13] J. Jiang, S. Han, J. Wu, and D. He. Pitt at trec 2011 session track. In TREC '11.",null,null
576,"[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.",null,null
577,[15] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08.,null,null
578,"[16] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May 1996.",null,null
579,"[17] Y. Kalfoglou and M. Schorlemmer. Ontology mapping: the state of the art. Knowl. Eng. Rev., 18(1), Jan. 2003.",null,null
580,"[18] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2011 session track. In TREC'11.",null,null
581,"[19] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.",null,null
582,"[20] E. Kanoulas, P. D. Clough, B. Carterette, and M. Sanderson. Session track at trec 2010. In TREC'10.",null,null
583,"[21] Lemur Search Engine. http://www.lemurproject.org/. [22] C. Liu, N. J. Belkin, and M. J. Cole. Personalization of",null,null
584,search results using interaction behaviors in search sessions. In SIGIR '12.,null,null
585,"[23] C. Liu, M. Cole, E. Baik, and J. N. Belkin. Rutgers at the trec 2012 session track. In TREC'12.",null,null
586,"[24] C. Liu, J. Gwizdka, J. Liu, T. Xu, and N. J. Belkin. Analysis and evaluation of query reformulations in different task types. In ASIST '10.",null,null
587,[25] J. Liu and N. J. Belkin. Personalizing information retrieval for multi-session tasks: the roles of task stage and task type. In SIGIR '10.,null,null
588,"[26] A. M-Dyaa, K. Udo, N. Nikolaos, N. Brendan, L. Deirdre, and F. Maria. University of essex at the trec 2011 session track. In TREC '11.",null,null
589,"[27] D. Metzler and W. B. Croft. Combining the language model and inference network approaches to retrieval. Inf. Process. Manage., 40(5), Sept. 2004.",null,null
590,"[28] S. P. Singh. Learning to solve markovian decision processes. Technical report, Amherst, MA, USA, 1993.",null,null
591,[29] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.,null,null
592,"[30] Y. Song, D. Zhou, and L.-w. He. Query suggestion by constructing term-transition graphs. In WSDM '12.",null,null
593,"[31] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: modeling queries with variation in user intent. In SIGIR '08.",null,null
594,"[32] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst., 23(3), July 2005.",null,null
595,"[33] C. Zhai and J. Lafferty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179­214, Apr. 2004.",null,null
596,462,null,null
597,,null,null

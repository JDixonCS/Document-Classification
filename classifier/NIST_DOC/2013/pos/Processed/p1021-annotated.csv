,sentence,label,data
,,,
0,Bias-Variance Decomposition of IR Evaluation,null,null
,,,
1,"Peng Zhang1, Dawei Song1,2, Jun Wang3, Yuexian Hou1",null,null
,,,
2,"1Tianjin Key Laboratory of Cognitive Computing and Application, Tianjin University, China 2The Computing Department, The Open University, United Kingdom",null,null
,,,
3,"3Department of Computer Science, University College London, United Kingdom",null,null
,,,
4,"{darcyzzj, dawei.song2010}@gmail.com, jun_wang@acm.org, yxhou@tju.edu.cn",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"It has been recognized that, when an information retrieval (IR) system achieves improvement in mean retrieval effectiveness (e.g. mean average precision (MAP)) over all the queries, the performance (e.g., average precision (AP)) of some individual queries could be hurt, resulting in retrieval instability. Some stability/robustness metrics have been proposed. However, they are often defined separately from the mean effectiveness metric. Consequently, there is a lack of a unified formulation of effectiveness, stability and overall retrieval quality (considering both). In this paper, we present a unified formulation based on the bias-variance decomposition. Correspondingly, a novel evaluation methodology is developed to evaluate the effectiveness and stability in an integrated manner. A case study applying the proposed methodology to evaluation of query language modeling illustrates the usefulness and analytical power of our approach.",null,null
,,,
7,Category and Subject Descriptors: H.3.3 [Information Search and Retrieval],null,null
,,,
8,"General Terms: Theory, Measurement, Performance",null,null
,,,
9,"Keywords: Bias-Variance, Decomposition, Effectiveness, Stability, Robustness, Evaluation",null,null
,,,
10,1. INTRODUCTION,null,null
,,,
11,"Recently, it has been noticed that when we are trying to improve the mean retrieval effectiveness (e.g., measured by MAP [3]) over all queries, the stability of performance across different individual queries could be hurt. For example, compared with a baseline (using the original query in the first round retrieval), query expansion based on pseudo relevance feedback can generally achieve better MAP, but it can hurt the performance for some individual queries [2].",null,null
,,,
12,"In the literature, various stability (or robustness) measures, e.g., R-Loss [2], Robustness Index [2], and < Init [9] have been proposed. R - Loss computes the averaged net loss of relevant documents (due to query expansion failure) in the retrieved documents. <Init calculates the percent-",null,null
,,,
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
14,"age of queries for which the retrieval performance of a query model is worse than that of the original query model. The robustness index is defined as RI(Q) ,"" (n+ - n-)/|Q| [2], where n+ is the number of queries for which the performance is improved, n- is the number of queries hurt, over the original model, and |Q| is the number of all queries.""",null,null
,,,
15,"These existing measures have some limitations, as shown by the example in Table 1. Let us consider model A as the original query model, as well as regard B and C as two query expansion models. The example shows that A is less effective (with a lower MAP), but more stable (with a lower < Init) than B. In this case, we can not judge which model (A or B) has the better overall performance (considering both effectiveness and stability). For comparison of B and C, both <Init and robustness index can not distinguish the stability between them. The MAPs of B and C are also the same. Intuitively, B would seem more stable due to the less variance of its AP values for different queries. However, the above stability metrics do not take into account such variance (denoted as V ar in Table 1). For example, one way of computing the variance of AP for model A can be [(0.3 - 0.2)2 + (0.1 - 0.2)2]/2 ,"" 0.01, which calculates the derivation of AP from its MAP (i.e., 0.2). In Table 1, V ar distinguishes the models A, B and C: the smaller V ar can indicate the better retrieval stability.""",null,null
,,,
16,"Now let us change the AP values of C to 0.32 and 0.11 (for q1 and q2 respectively), then its MAP, < Init and Robustness Index become 0.215, 0 and 1 respectively, suggesting C is more stable but less effective than B. We are not sure which one (B or C) is overall better when considering both effectiveness and stability. This is mainly because the existing stability metrics are often defined separately from the effectiveness metric (MAP). There is a lack of a unified formulation of the effectiveness and stability to allow them to be looked at in an integrated manner.",null,null
,,,
17,"In this paper, we present a formulation based on a fundamental concept in Statistics, namely the bias-variance decomposition, to tackle the problem. In a nutshell, we view the unsatisfactory overall performance as one total error, which can be decomposed into bias and variance of the AP values of different queries. The bias captures the expected difference of the APs from their upper bounds (the best AP obtained by model T in Table 1). The variance has been illustrated earlier. The detailed formulation will be given in the next section. Briefly speaking, the smaller bias or variance reflects the better retrieval effectiveness or stability, respectively. The total error (denoted as Bias2 +V ar) can reflect the overall quality of a model. As an illustration,",null,null
,,,
18,1021,null,null
,,,
19,Table 1: Examples of Bias-Variance (AP),null,null
,,,
20,Model AP,null,null
,,,
21,A q1 q2 0.3 0.1,null,null
,,,
22,B q1 q2 0.6 0.08,null,null
,,,
23,C,null,null
,,,
24,q1,null,null
,,,
25,q2,null,null
,,,
26,0.65 0.03,null,null
,,,
27,T q1 q2 0.7 0.2,null,null
,,,
28,MAP,null,null
,,,
29,0.2,null,null
,,,
30,0.34,null,null
,,,
31,0.34,null,null
,,,
32,0.45,null,null
,,,
33,Robust Index,Y,null
,,,
34,0,null,null
,,,
35,0,null,null
,,,
36,0,null,null
,,,
37,1,null,null
,,,
38,< Init,null,null
,,,
39,0,null,null
,,,
40,0.5,null,null
,,,
41,0.5,null,null
,,,
42,0,null,null
,,,
43,Bias,null,null
,,,
44,V ar Bias2 + V ar,null,null
,,,
45,0.25 0.01 0.0725,null,null
,,,
46,0.11 0.0646 0.0797,null,null
,,,
47,0.11 0.0961 0.182,null,null
,,,
48,0 0.0625 0.0625,null,null
,,,
49,"in Table 1, Bias2 +V ar indicates that (1) the target model T has the best overall retrieval quality; (2) the model B is more desirable than C; (3) in comparison with the baseline A, both query expansion models B and C reduce the bias but enlarge the variance as well as the total error.",null,null
,,,
50,"Recently, Wang and Zhu [7] studied the mean-variance analysis regarding the document relevance scores. The pertopic variance of AP (of each query) was investigated in [6]. In this paper, we explore the variance of AP (across queries) and the bias-variance decomposition.",null,null
,,,
51,2. BIAS-VARIANCE DECOMPOSITION OF,null,null
,,,
52,RETRIEVAL PERFORMANCE,null,null
,,,
53,"In Statistics [1], bias and variance, which are measurements of estimation quality in different aspects, are decomposed from the expected squared error of the estimated values with respect to the target value. In this paper, we formulate the bias-variance decomposition in the IR evaluation scenario. Let us first assume there exists a target model T that can have an upper-bound performance for each query 1.",null,null
,,,
54,2.1 Bias-Variance of AP,null,null
,,,
55,"We can let AP be a random variable over queries. Bias(AP) essentially calculates the average difference between the target AP (i.e. the AP of the target model T, denoted APT ) and the actual AP of a retrieval model over all different queries. Specifically,",null,null
,,,
56,"Bias(AP) , E(APT - AP) , E(APT ) - E(AP) (1)",null,null
,,,
57,where the expectation E(·) is over a set of queries that are,null,null
,,,
58,"assumed to be uniformly distributed. E(APT ) corresponds to the target MAP (i.e., the MAP of the target model T,",null,null
,,,
59,"denoted MAPT ), and E(AP) corresponds to the MAP of",null,null
,,,
60,the retrieval model under evaluation.,null,null
,,,
61,It turns out that the smaller bias indicates the better mean,null,null
,,,
62,"retrieval effectiveness over queries. In Table 1, the bias for",null,null
,,,
63,model,null,null
,,,
64,A,null,null
,,,
65,is,null,null
,,,
66,1 2,null,null
,,,
67,[(0.7,null,null
,,,
68,-,null,null
,,,
69,0.3),null,null
,,,
70,+,null,null
,,,
71,(0.2,null,null
,,,
72,-,null,null
,,,
73,0.1)],null,null
,,,
74,", 0.25.",null,null
,,,
75,According,null,null
,,,
76,to,null,null
,,,
77,"Eq. 1, it can be equivalently calculated as 0.45-0.2 ,"" 0.25,""",null,null
,,,
78,where 0.45 is the target MAP and 0.2 is the MAP of A.,null,null
,,,
79,"Similarly, the variance of the APs for different queries is",null,null
,,,
80,defined as:,null,null
,,,
81,"V ar(AP) , E(AP - E(AP))2",null,null
,,,
82,-2,null,null
,,,
83,The smaller V ar(AP) indicates the better retrieval stability of AP across queries.,null,null
,,,
84,1We will relax this constraint in Sections 2.4.,null,null
,,,
85,Table 2: Examples of Additional Bias-Variance (),null,null
,,,
86,Model ,null,null
,,,
87,A q1 q2 0.4 0.1,null,null
,,,
88,B q1 q2 0.1 0.12,null,null
,,,
89,C,null,null
,,,
90,q1,null,null
,,,
91,q2,null,null
,,,
92,0.05 0.17 0,null,null
,,,
93,T q1 q2 00,null,null
,,,
94,Bias,null,null
,,,
95,0.25,null,null
,,,
96,0.11,null,null
,,,
97,0.11,null,null
,,,
98,0,null,null
,,,
99,V ar,null,null
,,,
100,0.0225,null,null
,,,
101,0.0001,null,null
,,,
102,0.0036,null,null
,,,
103,0,null,null
,,,
104,Bias2 + V ar 0.0850,null,null
,,,
105,0.0122,null,null
,,,
106,0.0157,null,null
,,,
107,0,null,null
,,,
108,"We now add up the bias and variance, yielding:",null,null
,,,
109,"Bias2(AP) + V ar(AP) , [E(APT ) - E(AP)]2 + E(AP - E(AP))2 (3) , E(AP - MAPT )2",null,null
,,,
110,"It turns out E(AP - MAPT )2 is not exactly the expected squared error E(AP - APT )2. However, we will show later that E(AP - MAPT )2 is a simplified version of an expected squared error E(AP-1)2 in Section 2.3. This error formulation will help us understand the difference between different bias-variance decompositions (see Section 2.3).",null,null
,,,
111,2.2 Additional Bias-Variance,null,null
,,,
112,"To clarify the above observation, let us first look at the decomposition of the expected squared error E(AP-APT )2. We need to define another random variable",null,null
,,,
113," , APT - AP",null,null
,,,
114,-4,null,null
,,,
115,"As an illustration, for the model A in Table 2,  is 0.4 (0.70.3) and 0.1 (0.2-0.1), for q1 and q2 respectively.",null,null
,,,
116,"Let T be the target value of , which is indeed 0, since for model T,  , APT - APT ,""0. We need T in the following analysis, since the target value is an important component""",null,null
,,,
117,in the standard definition of bias. We define,null,null
,,,
118,"Bias() , E() - T",null,null
,,,
119,-5,null,null
,,,
120,"where E() is the averaged  over all queries. Since T is 0, Bias() equals to E() ,"" E(APT -AP), which is Bias(AP).""",null,null
,,,
121,"The variance based on , denoted V ar(), computes the variance of the difference between the AP (of the test model)",null,null
,,,
122,"and the AP target across all queries. Formally,",null,null
,,,
123,"V ar() , E( - E())2",null,null
,,,
124,-6,null,null
,,,
125,"As shown in Table 2, V ar() of B and C are smaller than V ar() of A, indicating that B and C are more stable than A. This observation is different from < Init and V ar(AP) which shows that B and C are less stable than A in Table 1.",null,null
,,,
126,"Now, we derive the decomposition of E(AP - APT )2:",null,null
,,,
127,"E(AP - APT )2 , E( - T )2",null,null
,,,
128,", E( - E())2 + (E() - T )2 (7)",null,null
,,,
129,", V ar() + Bias2()",null,null
,,,
130,"The above equations show the expected squared error E(AP - APT )2 can be exactly decomposed into bias and variance on . The expected squared error E(AP - APT )2 always computes the error of the target model as zero, no",null,null
,,,
131,matter whether or not the target model still has room for,null,null
,,,
132,improvement. It is likely that the current best performance,null,null
,,,
133,for some queries can be further advanced in the near future.,null,null
,,,
134,1022,null,null
,,,
135,2.3 Further Investigation on Decomposition of Expected Squared Error,null,null
,,,
136,"In order to further investigate two aforementioned biasvariance decompositions, we set 1 (the maximum AP) as the upper-bound AP for each query. We can have an expected squared error E(AP - 1)2 and its decomposition as:",null,null
,,,
137,E(AP - 1)2,null,null
,,,
138,", E(AP - APT + APT - 1)2",null,null
,,,
139,", E(AP - APT )2 + E(APT - 1)2 + 2E(AP - APT )(APT - 1) (8)",null,null
,,,
140,"which shows that E(AP-APT )2 is only one part of E(AP- 1)2, and the target model T still has an error E(APT - 1)2,",null,null
,,,
141,suggesting there is still a room for improvement. We can also decompose E(AP - 1)2 as:,null,null
,,,
142,E(AP - 1)2,null,null
,,,
143,", E[AP - E(AP) + E(AP) - 1]2",null,null
,,,
144,", E(AP - E(AP))2 + [E(AP) - 1]2",null,null
,,,
145,-9,null,null
,,,
146,", V ar(AP) + (MAP - 1)2",null,null
,,,
147,", (1 - MAP)2 + V ar(AP)",null,null
,,,
148,"It turns out the variance parts in Eq. 3 and Eq. 9 are the same (i.e., V ar(AP)). The term (1 - MAP)2 in Eq. 9 has the same trend as Bias2(AP) (i.e., (MAPT -MAP)2), where MAPT is the upper bound of the MAP. The above observations show that the decomposition in Eq. 3 can be considered",null,null
,,,
149,as a simplified version of the decomposition in Eq. 9.,null,null
,,,
150,2.4 Comparison between Two Bias-Variance,null,null
,,,
151,"First, the bias-variance of  requires a target AP for every query. On the other hand, the bias-variance of AP (in Eq. 3) can be used when only a target MAP (i.e., MAPT ) is given. Specifically, two biases (i.e., Bias(AP) and Bias()) are equivalent and both can be computed by MAPT -MAP. Regarding variance, the variance of  requires APT for each query (see Eq. 6 and Eq. 4), while the variance of AP can be calculated without knowing APT (see Eq. 2). Thus, the bias-variance based on AP is more flexible for practical use.",null,null
,,,
152,"Second, the bias-variance of  is an exact decomposition of E(AP - APT )2 and it always regards the target model as a zero-error model. However, the decomposition of E(AP-1)2 in Eq. 8 shows that the target model can still has error. On the other hand, under the bias-variance of AP, the variance for the target model still exists, indicating that although we can assume a target model as an upper-bound at a certain stage, it can be further improved.",null,null
,,,
153,2.5 Bias-Variance Evaluation,null,null
,,,
154,"To carry out IR evaluation based on the proposed the bias-variance decomposition methods, one needs to choose the upper-bound settings (i.e., the target model T). There can be a number of ways. First, the upper-bound AP for each query can be simply set as 1, which corresponds to a perfect target model. Second, the upper-bound AP for each query can be obtained based on the best AP among evaluated retrieval models in the historic TREC results. Third, one can simply set an upper-bound MAP (i.e., MAPT ).",Y,null
,,,
155,"The first and second upper-bound settings correspond to a virtual target model. For example, in the second setting, the target model collects the best AP among many retrieval",null,null
,,,
156,"models. The third setting can correspond to a real target model (see the model settings in the experiments.) With the first and second upper-bound settings, either of biasvariance metrics (based on AP or ) can be adopted. The third setting is suitable for bias-variance of AP (see Section 2.4). Once an upper-bound setting and a variable (AP or ) are chosen, we can calculate bias-variance figures for a series of models and then see which model can have the minimum bias, or minimum variance, or the sum of them (i.e., the total error). We can also get an overview on the trend of bias and variance over parameters.",null,null
,,,
157,3. EMPIRICAL EVALUATION,null,null
,,,
158,"We use the query modeling as an example of bias-variance evaluation. Due to the page limit, we only report the evaluation results on two standard TREC collections, i.e., WSJ (87-92) over queries 151-200 and ROBUST 2004 over queries 601-700. The title field of the queries is used. Lemur 4.7 [5] is used for indexing and retrieval. The top n , 30 ranked documents in the initial ranking by the original query model are selected as the pseudo-relevance feedback (PRF) documents. The number of expanded terms is fixed as 100. 1000 documents are retrieved by the KL-divergence model [5].",Y,null
,,,
159,"Model Settings We customize a number of query models according to three factors (i.e., model complexity, model combination, ground-truth data) that can generally influence the bias-variance tradeoff [1]. The first model is the original query model which is a maximum likelihood estimation of original query. We also evaluate an expanded query model RM (i.e., RM1 in [4]) which is generated from feedback documents D. Compared with the original query model, the expanded query model is more complex due to the fact that it has more terms and additional assumptions (e.g., assuming that all feedback documents are relevant). The above two models can be combined, leading to a combined query model (also called as RM3). Let  be the combination coefficient which is in the range (0,1). When  gets close to 0, the combined model is towards the RM, and otherwise towards the original model. In RM, we can gradually remove a percentage (denoted by rn) of nonrelevant documents DN along the initial ranking of feedback documents, based on the available relevance judgements (as ground-truth) [8]. We finally derive a target model which are generated by keeping only relevant documents DR in D:",null,null
,,,
160,p(w|q(t)) ,null,null
,,,
161,p(w|d),null,null
,,,
162,dDR,null,null
,,,
163,-10,null,null
,,,
164,"which gives the best MAP over the aforementioned query models. In Eq. 10, the document weights are set as uniform (different from the weights in RM), since the relevant judgements of relevant documents are the same (i.e., 1).",null,null
,,,
165,"Bias-Variance Metrics Setting We report the biasvariance on AP in our evaluation, since the target query model used in our evaluation only guarantees the upperbound MAP. According to the discussion in Sections 2.4 and 2.5, the bias-variance of AP is more suitable.",null,null
,,,
166,"Evaluation Results We first look at the bias-variance results in Figure 1. Recall that the smaller bias or variance (based on AP) corresponds to the better retrieval effectiveness or stability, respectively. Figure 1(a) shows that on WSJ8792, the original query model has the smallest variance (the highest stability), but the largest bias (the lowest MAP). This is a tradeoff between bias and variance. One",Y,null
,,,
167,1023,null,null
,,,
168,Var(AP),null,null
,,,
169,0.066 0.064 0.062,null,null
,,,
170,0.06,null,null
,,,
171,Bias-Variance (AP),null,null
,,,
172,Original Model Expanded Model RM Combinaed Model Removing DN Target Model,null,null
,,,
173,0.058,null,null
,,,
174,0.056,null,null
,,,
175,0.054 0,null,null
,,,
176,0.005 0.01 0.015 0.02 0.025 0.03 0.035 Bias2(AP),null,null
,,,
177,(a) WSJ8792,Y,null
,,,
178,Var(AP),null,null
,,,
179,0.06 0.058 0.056 0.054 0.052,null,null
,,,
180,0.05 0.048 0.046 0.044 0.042,null,null
,,,
181,0.04 0,null,null
,,,
182,Bias-Variance (AP),null,null
,,,
183,Original Model Expanded Model RM Combinaed Model Removing DN Target Model 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045,null,null
,,,
184,Bias2(AP),null,null
,,,
185,(b) ROBUST2004,Y,null
,,,
186,Figure 1: Results of Bias and Variance of AP of all the concerned query models. The x-axis shows the squared bias and the y-axis shows the variance.,null,null
,,,
187,"reason in Statistics language is that the expanded models are all complex than the original one. The bias of the expanded model RM is much smaller, while its variance is much larger, than the original model. The different variants of the combined model (with  in [0.1, 0.9] with increment 0.1) are lying between the expanded and original models. We can also see that 2 (out of 9) combined models (when ,""0.1 and 0.2) has smaller bias and smaller variance than the expanded query model RM. This suggests that the combined query model with good parameters can achieve better effectiveness and stability over RM. Among the combined query models, bias and variance are negatively correlated, indicating a bias-variance tradeoff occurs. On the other hand, if we remove the non-relevant documents DN (with rn in [0.1, 1] with increment 0.1) in RM, the bias and variance are positively correlated, and 9 (out of 10) models (when rn "","" 0.2 to 1) can reduce both the bias and variance over RM. The target model has zero bias, although there is a variance of its AP across different queries. On ROBUST2004, the results are similar. The differences are that: 1) by removing non-relevant documents, only 6 (out of 10) models (when rn "", 0.5 to 1) reduce both bias and variance over RM; 2) there are 3 (out of 9) (when  , 0.1 to 0.3) variants of the combined model for which both bias and variance can be smaller than those of RM.",Y,null
,,,
188,"Figure 2 shows Bias2 + V ar results for all the concerned models. On both collections, the target method achieves the smallest Bias2 + V ar and the original query model achieves the largest Bias2 +V ar. Recall that Bias2 +V ar represents the total error and the smaller error can reflect the better overall quality (or performance), by considering both the effectiveness and stability in one single criterion (Bias2 + V ar). The results also suggest that the model combination and the relevance judgements are helpful to reduce Bias2 + V ar over the original model and the expanded model RM.",null,null
,,,
189,4. CONCLUSIONS AND FUTURE WORK,null,null
,,,
190,"In this paper, a novel evaluation strategy based on the bias-variance decomposition has been presented. We formulate two forms of the bias-variance decomposition and theoretically compare them. We also use query expansion as an example to demonstrate the use of the bias-variance for IR evaluation and analysis, in terms of bias, variance or sum of them. In the future, we will further explore the other upper-bound settings and variables discussed in Section 2.5,",null,null
,,,
191,Bias2(AP)+Var(AP) Bias2(AP)+Var(AP),null,null
,,,
192,Bias-Variance (AP) 0.09,null,null
,,,
193,0.085,null,null
,,,
194,Bias-Variance (AP),null,null
,,,
195,0.085,null,null
,,,
196,0.08,null,null
,,,
197,0.08,null,null
,,,
198,0.075,null,null
,,,
199,0.075 0.07,null,null
,,,
200,0.065 0.06 0,null,null
,,,
201,Original Model Expanded Model RM Combinaed Model Removing DN Target Model,null,null
,,,
202,0.005 0.01 0.015 0.02 0.025 0.03 0.035 Bias2(AP),null,null
,,,
203,(a) WSJ8792,Y,null
,,,
204,0.07,null,null
,,,
205,0.065 0.06,null,null
,,,
206,0.055 0,null,null
,,,
207,Original Model Expanded Model RM Combinaed Model Removing DN Target Model,null,null
,,,
208,0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 Bias2(AP),null,null
,,,
209,(b) ROBUST2004,Y,null
,,,
210,Figure 2: Results of Bias2 and Bias2+V ar of AP of all the concerned query models. The x-axis shows the squared bias and the y-axis shows the Bias2 + V ar.,null,null
,,,
211,test with more retrieval models and explore the deep insights behind the bias-variance trends.,null,null
,,,
212,5. ACKNOWLEDGMENTS,null,null
,,,
213,"This work is supported in part by the Chinese National Program on Key Basic Research Project (973 Program, grant No. 2013CB329304), the Natural Science Foundation of China (grant No. 61272265, 61070044), and EU's FP7 QONTEXT project (grant No. 247590).",null,null
,,,
214,6. REFERENCES,null,null
,,,
215,"[1] C. M. Bishop. Pattern Recognition and Machine Learning. Springer-Verlag New York, Inc., 2006.",null,null
,,,
216,"[2] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In CIKM '09, pages 837­846, 2009. ACM.",Y,null
,,,
217,"[3] G. V. Cormack and T. R. Lynam. Statistical Precision of Information Retrieval Evaluation. In SIGIR '06, pages 533­540, 2006. ACM.",null,null
,,,
218,"[4] V. Lavrenko and W. B. Croft. Relevance-based language models. In SIGIR '01, pages 120­127, 2001.",null,null
,,,
219,"[5] P. Ogilvie and J. Callan. Experiments using the lemur toolkit. In TREC '02, pages 103­108, 2002.",Y,null
,,,
220,"[6] S. E. Robertson and E. Kanoulas. On per-topic variance in ir evaluation. In SIGIR '12, pages 891­900, 2012.",null,null
,,,
221,"[7] J. Wang and J. Zhu. Portfolio theory of information retrieval. In SIGIR '09, pages 115­122, 2009.",null,null
,,,
222,"[8] P. Zhang, Y. Hou and D. Song. Approximating true relevance distribution from a mixture model based on irrelevance data. In SIGIR '09, pages 107­114, 2009.",null,null
,,,
223,"[9] L. Zighelnic and O. Kurland. Query-drift prevention for robust query expansion. In SIGIR '08, pages 825­826, 2008.",Y,null
,,,
224,1024,null,null
,,,
225,,null,null

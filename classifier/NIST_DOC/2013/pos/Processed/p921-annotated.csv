,sentence,label,data
,,,
0,The Impact of Intent Selection on Diversified Search Evaluation,null,null
,,,
1,Tetsuya Sakai,null,null
,,,
2,"Microsoft Research Asia,",null,null
,,,
3,P.R.C.,null,null
,,,
4,tetsuyasakai@acm.org,null,null
,,,
5,Zhicheng Dou,null,null
,,,
6,Charles L. A. Clarke,null,null
,,,
7,"Microsoft Research Asia,",null,null
,,,
8,"University of Waterloo,",null,null
,,,
9,P.R.C.,null,null
,,,
10,Canada,null,null
,,,
11,zhichdou@microsoft.com claclark@plg.uwaterloo.ca,null,null
,,,
12,ABSTRACT,null,null
,,,
13,"To construct a diversified search test collection, a set of possible subtopics (or intents) needs to be determined for each topic, in one way or another, and per-intent relevance assessments need to be obtained. In the TREC Web Track Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis; in the NTCIR INTENT Task, intents are determined by manually clustering ""subtopics strings"" returned by participating systems. In this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may affect relative performances, and that this choice may be far more important than how many intents are selected for each topic.",Y,null
,,,
14,Categories and Subject Descriptors,null,null
,,,
15,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
,,,
16,General Terms,null,null
,,,
17,Experimentation,null,null
,,,
18,Keywords,null,null
,,,
19,"diversity, evaluation, intents, subtopics, test collections",null,null
,,,
20,1. INTRODUCTION,null,null
,,,
21,"Given an ambiguous or underspecified query, diversified search aims to cover different possible search intents with a single search engine result page, by balancing relevance and diversity. TREC1 started a Diversity Task in the Web Track2 in 2009, while NTCIR3 started a related task called INTENT4 in 2011. Unlike traditional retrieval evaluation where pooled documents are assessed in terms",Y,null
,,,
22,1http://trec.nist.gov/ 2http://plg.uwaterloo.ca/~trecweb/ 3http://research.nii.ac.jp/ntcir/ 4http://research.microsoft.com/INTENT/,null,null
,,,
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
24,"quit smoking (TREC topicID,182; NTCIR topicID,0432)",Y,null
,,,
25,"(a) TREC ""subtopics""",Y,null
,,,
26,1. What are the ways you can quit smoking?,null,null
,,,
27,2. What are the benefits of quitting,null,null
,,,
28,smoking?,null,null
,,,
29,3. Can you quit smoking using the cold turkey method?,null,null
,,,
30,4. How can hypnosis help someone,null,null
,,,
31,quitting smoking?,null,null
,,,
32,"(b) NTCIR ""intents"" 1. effects (0.15) 2. ways (0.15) 3. benefit (0.14) 4. reasons (0.14) 5. products (0.14)",null,null
,,,
33,6. public resource (0.10) 7. aids (0.10),null,null
,,,
34,8. people (0.08),null,null
,,,
35,"(c) NTCIR ""subtopic strings""",null,null
,,,
36,women weight gain quit smoking;what happens when you quit smoking;...,null,null
,,,
37,what to do to quit smoking;ways to quit smoking cigarettes;...,null,null
,,,
38,quit smoking health benefits;quit smoking health;...,null,null
,,,
39,why quit smoking;reasons to quit smoking,null,null
,,,
40,wellbutrin quit smoking;using guided imagery to quit smoking;...,null,null
,,,
41,women quit smoking forums;tobacco products scientific advisory committee;... Stop Smoking Aids;quit smoking tobacco,null,null
,,,
42,advertising;... the people behind quitsmoking;the man who,null,null
,,,
43,quit smoking;...,null,null
,,,
44,Figure 1: TREC subtopics vs. NTCIR intents and subtopic strings.,null,null
,,,
45,"of relevance with respect to each topic, diversity evaluation requires a set of subtopics (or intents) for each topic, and pooled documents are assessed with respect to each intent. In the TREC Diversity Task, subtopics are manually developed at NIST, based on results of automatic click log analysis [4]; in the NTCIR INTENT Task, intents are determined by manually clustering ""subtopics strings"" returned by participating systems [6, 7]. However, it is difficult to say exactly what the most appropriate intents are for a given topic for the purpose of evaluating diversified search.",Y,null
,,,
46,"One may be tempted to hypothesise that an effective diversified search system should be effective regardless of the particular choice of intents used to evaluate it. Thus, in this study, we address the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we use the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets [6, 7]. Figure 1 provides an actual example from the 50 topics we used in our experiments: for this topic (""quit smoking""), four subtopics were obtained for TREC at NIST as shown in Figure 1(a), by following the aforementioned click-based methodology; whereas, at NTCIR, subtopic strings were returned by the participating system of the INTENT2 Subtopic Mining Subtask as shown in Figure 1(c), which were then manually clustered and filtered to form a set of eight intents as shown in Figure 1(b)5. As (a) and (b) were obtained completely independently using different methods, they obviously differ, although they may partially overlap as indicated by the dotted lines in the figure. For example, the first TREC subtopic for this topic is ""What are the ways you can quit smoking?"" which is probably similar to the second NTCIR intent ""(quit smoking) ways.""",Y,null
,,,
47,"5As shown in the figure, the INTENT task also estimates the probability of each intent given the query based on assessor voting. However, following the practice at TREC, we assume that the probability distribution is uniform across all intents throughout this study.",Y,null
,,,
48,921,null,null
,,,
49,Table 1: Test collection and pseudo-qrels statistics. Eight,null,null
,,,
50,TREC intents with no relevant documents have been removed.,null,null
,,,
51,"In Part (b), statistics for the truncated pseudo-qrels are shown",null,null
,,,
52,in parentheses.,null,null
,,,
53,(a) TREC 2012,Y,null
,,,
54,(b) English INTENT-2,Y,null
,,,
55,Diversity and pseudo-qrels derived,null,null
,,,
56,topics,null,null
,,,
57,50 (provided from TREC to NTCIR),Y,null
,,,
58,intents/topic,null,null
,,,
59,3.7 all: 7.8; matched: 7.7 (3.7),null,null
,,,
60,subtopic strings/topic,null,null
,,,
61,­,null,null
,,,
62,82.7,null,null
,,,
63,pooled non-junk docs/topic,null,null
,,,
64,303.9,null,null
,,,
65,­,null,null
,,,
66,unique relevant/topic 4-relevant/topic 3-relevant/topic 2-relevant/topic 1-relevant/topic,null,null
,,,
67,111.2 49.7,null,null
,,,
68,2.6 23.5 111.6,null,null
,,,
69,287.0 (263.8) ­,null,null
,,,
70,11.8 (8.8) 289.7 (182.7) 881.7 (435.3),null,null
,,,
71,"foreach topic t do { foreach NTCIR intent i for t do { foreach TREC pooled document d for t do { matchcount , 0; foreach reduced subtopic s for i do { if d contains s by exact match then matchcount + +; } relevancelevel (d) ,"" max(0, trunc(log(matchount ) + 1)); //the function trunc takes the integer part of the argument. }""",Y,null
,,,
72,},null,null
,,,
73,Figure 2: Algorithm for automatically generating pseudo-qrels.,null,null
,,,
74,"This intent was devised at NTCIR based on a cluster of subtopic strings obtained from participating systems, including ""what to do quit smoking"" and ""ways to quit smoking cigarettes"" amongst others. Hereafter, we shall also refer to TREC subtopics as ""intents"" to avoid confusion.",Y,null
,,,
75,"To address the above research question, we replace the TREC intents with the NTCIR intents and then re-evaluate the runs submitted to the TREC 2012 diversity task. Unfortunately, while the NTCIR-10 INTENT-2 Task had Document Ranking (i.e., diversified search) subtasks for Chinese and Japanese, it only had a Subtopic Mining subtask for English [6, 7], and therefore the English intents from NTCIR lack document relevance assessments. While it would be ideal to actually conduct relevance assessments of the TREC pooled documents with respect to each NTCIR intent, we explore a cheaper alternative in this paper, namely, to automatically construct pseudo-qrels by simply matching the TREC pooled documents against the NTCIR subtopic strings6. While the lack of true relevance assessments for the NTCIR intents is a limitation of this study, our pseudo-qrels do provide partial answers to our research question: our initial results suggest that the choice of intents may in fact affect relative performances, and that this choice may be more important than how many intents are selected for each topic.",Y,null
,,,
76,2. EXPERIMENTAL SETTING,null,null
,,,
77,2.1 Data,null,null
,,,
78,"Table 1(a) shows some statistics of the TREC 2012 Web Diversity topics, after having removed eight intents from the original qrels.diversity file as they did not have any relevant documents. At TREC, there were six relevance levels: 4 (""navigational""), 3 (""key""), 2 (""highly relevant""), 1 (""relevant""), 0 (""nonrelevant"") and -2 (""junk"") [4]. The table refers to the first four as 4-, 3-, 2- and 1-relevant, respectively; we treat the other two",Y,null
,,,
79,"6In TREC parlance, qrels means relevance assessments. When qrels are obtained automatically without involving manual relevance assessments, they are often referred to as pseudo-qrels [9].",Y,null
,,,
80,"as nonrelevant. Also, as shown in the table, we had 303.9 pooled non-junk documents per topic on average, which we obtained from qrels.diversity7.",null,null
,,,
81,"To re-evaluate the TREC 2012 diversity runs after replacing the TREC intents with the NTCIR intents, we created pseudo-qrels as follows: The original NTCIR intents have an average of 7.8 intents and 82.7 subtopic strings per topic (recall Figure 1(b) and (c)). To obtain pseudo-relevant documents from the TREC pools for each NTCIR intents, we first removed the topic string from each NTCIR subtopic string automatically: for example, ""women weight gain quit smoking"" in Figure 1 was turned into ""women weight gain."" We call the resultant strings reduced subtopics. We then used the simple algorithm shown in Figure 2 to obtain pseudo-relevant documents for each NTCIR intent. Note that each pooled document is tested whether it matches with any of the reduced subtopic, under the assumption that pooled documents already contain the actual topic string (e.g., ""quit smoking"") or some related term. The algorithm also determines the relevance level of each document based on the number of matches with reduced subtopics: the actual number of matches (matchcount ) varied from 0 to 19; the (natural) log-based function in the algorithm maps them to 0-3. A total of 28 pooled documents were removed during the process, as they have been detected as containing a virus.",Y,null
,,,
82,"Table 1(b) shows some statistics of the NTCIR-10 INTENT-2 intents and the pseudo-qrels we derived from them. Note that, of the 303.9 TREC pooled documents per topic, as many as 293.6 matched with at least one reduced subtopic and are treated as relevant. This strongly suggests that our pseudo-qrels contain a lot of false matches. Moreover, because of this problem, note that the average number of intents per topic in the pseudo-qrels is 7.7, which is easily twice as large as the corresponding number for TREC, namely, 3.7. Thus, if the evaluation outcome with the pseudo-qrels is different from that with the true qrels, this may be because either (i) the two intent sets contain different intents; or (ii) the two intent sets differ in size (the NTCIR intents sets are larger so may require systems to diversify more aggresively); or both.",Y,null
,,,
83,"In order to separate the above two effects, we also created another version of pseudo-qrels, called truncated pseudo-qrels (or simply ""truncated"" for short). This was done by cutting down ""less popular"" intents from the original pseudo-qrels to ensure that the TREC and NTCIR intent sets are equal in size for each topic. For the example shown in Figure 1, although the original pseudo-qrels has eight intents, the truncated pseudo-qrels has only the first four intents with the highest intent probabilities. The statistics for the truncated pseudo-qrels are shown in parentheses in Table 1(b).",Y,null
,,,
84,2.2 Evaluation Metrics and Analysis Methods,null,null
,,,
85,"We primarily consider four diversity evaluation metrics: D-nDCG, D -nDCG [8], -nDCG and ERR-IA [3]. D-nDCG is a version of normalised Cumulative Discounted Gain (nDCG) [5] which combines per-intent graded relevance and intent probabilities to compute the gain value of each document. D -nDCG is a simple average of D-nDCG and intent recall (I-rec), a.k.a. subtopic recall [11]. D -nDCG summarises a graph that plots D-nDCG (i.e. overall relevance) against I-rec (pure diversity). -nDCG is a version of nDCG which defines graded relevance as the number of intents covered by a document, and discounts the value of a retrieved relevant document for each intent based on relevant documents already seen. This property is known as diminishing return [2]. ERR-IA first",null,null
,,,
86,"7At TREC 2012, a common pool was created across the ad hoc task and the diversity task for each topic. Hence, the pooled documents obtained from qrels.diversity are identical to those from qrels.adhoc.",Y,null
,,,
87,922,null,null
,,,
88,"computes an Expected Reciprocal Rank value for each intent and then combines them across intents. It also possesses the diminishing return property. Both -nDCG and ERR-IA may be expressed in terms of a common framework, differing primarily in the discounts they apply for document rank [3].",null,null
,,,
89,"The NTCIR INTENT task uses I-rec, D-nDCG and D -nDCG as the primary metrics for ranking the runs. Following the task's practice, we compute the values using NTCIREVAL8, by using the relevance levels as the gain values. However, it should be noted that I-rec is not a good stand-alone metric for our purpose: although we measure performance at document cutoffs of 10 and 20 (denoted by ""@10"" and ""@20""), recall that the average number of intents per topic with the true qrels is only 3.7: thus it should be fairly easy for systems to cover most intents, especially with 20 documents. Furthermore, I-rec does not work well with our pseudo-qrels: because of the aforementioned false match problem, I-rec is heavily overestimated for all of the TREC runs when the pseudo-qrels are used. Nevertheless, we include the results with I-rec for separating the effects of diversity and relevance in diversified search evaluation [8].",Y,null
,,,
90,"The TREC Web Track Diversity Task uses -nDCG and ERR-IA along with some other metrics. Following the practice at TREC, we computed these metrics using ndeval9. It should be noted that, while NTCIREVAL utilises the per-intent graded relevance data to compute D( )-nDCG, ndeval reduces the data to per-intent binary relevance data before computing -nDCG and ERR-IA. Thus the computation of relevance levels in Figure 2 does not affect these two metrics.",Y,null
,,,
91,"In order to compare the relative performances of the TREC 2012 diversity runs before and after replacing the original TREC intents with the NTCIR ones, we compare the run rankings in terms Kendall's  , and its variant called ap [10]. These measures count the number of pairwise system swaps; ap is more sensitive to the swaps near the top ranks than  is. However, what is perhaps more important is whether replacing the intent sets affects statistical significance testing, which is often used for forming research conclusions in the IR community. We therefore conduct a randomised version of the two-sided Tukey's Honestly Significantly Different (HSD) test [1] at  ,"" .05 for the entire set of runs before and after replacing the intent sets. Given the entire set of runs, this kind of test is more appropriate than those that test one run pair at a time while ignoring the others. We then compare the two sets of significantly different run pairs. For example, is a significantly different run pair obtained according to the TREC intents still significantly different according to the NTCIR intents with its pseudo-qrels?""",Y,null
,,,
92,3. RESULTS AND DISCUSSIONS,null,null
,,,
93,"Table 2 shows the  and ap between rankings produced by two different metrics based on the true qrels, to show how the diversity metrics behave differently. Table 3 is more important for our purpose: for each metric, the  and ap for the ranking with the true qrels and that with the pseudo-qrels are shown. It can be observed that the rankings with the pseudo-qrels (i.e., those based on the NTCIR intents) are quite different from those with the true qrels (i.e., those based on the TREC intents). This is true even for the truncated pseudo-qrels, as shown in Part (b) of the table, which suggests that the discrepancies between TREC and NTCIR may arise not from how many intents are used but from the actual choice of intents.",Y,null
,,,
94,8http://research.nii.ac.jp/ntcir/tools/ ntcireval-en.html 9http://trec.nist.gov/data/web2012.html,Y,null
,,,
95,Table 2:  /ap between two metrics using true qrels (20 TREC,Y,null
,,,
96,2012 diversity runs).,null,null
,,,
97,@10,null,null
,,,
98,D-nDCG D -nDCG -nDCG ERR-IA,null,null
,,,
99,I-rec,null,null
,,,
100,.347/.323 .642/.493 .547/.451 .568/.496,null,null
,,,
101,D-nDCG,null,null
,,,
102,- .705/.780 .695/.719 .674/.700,null,null
,,,
103,D -nDCG,null,null
,,,
104,-,null,null
,,,
105,- .779/.769 .695/.706,null,null
,,,
106,#NAME?,null,null
,,,
107,-,null,null
,,,
108,-,null,null
,,,
109,-1,null,null
,,,
110,@20,null,null
,,,
111,D-nDCG D -nDCG -nDCG ERR-IA,null,null
,,,
112,I-rec,null,null
,,,
113,.179/.319 .705/.661 .495/.537 .453/.522,null,null
,,,
114,D-nDCG,null,null
,,,
115,- .474/.580 .621/.656 .600/.637,null,null
,,,
116,D -nDCG,null,null
,,,
117,-,null,null
,,,
118,- .705/.745 .579/.635,null,null
,,,
119,#NAME?,null,null
,,,
120,-,null,null
,,,
121,-,null,null
,,,
122,-0.997660819,null,null
,,,
123,Table 3:  /ap between rankings by the same metric using true,null,null
,,,
124,and (truncated) pseudo-qrels (20 TREC 2012 diversity runs).,Y,null
,,,
125,@10,null,null
,,,
126,@20,null,null
,,,
127,(a) true,null,null
,,,
128,I-rec,null,null
,,,
129,.632/.392 .568/.302,null,null
,,,
130,vs.,null,null
,,,
131,D-nDCG .653/.698 .684/.692,null,null
,,,
132,pseudo,null,null
,,,
133,D -nDCG .611/.651 .632/.652 -nDCG .674/.673 .716/.704,null,null
,,,
134,ERR-IA .589/.611 .589/.614,null,null
,,,
135,(b) true,null,null
,,,
136,I-rec,null,null
,,,
137,.579/.317 .526/.271,null,null
,,,
138,vs.,null,null
,,,
139,D-nDCG .621/.660 .653/.666,null,null
,,,
140,truncated D -nDCG .684/.682 .695/.668 -nDCG .663/.668 .716/.706,null,null
,,,
141,ERR-IA .579/.614 .579/.616,null,null
,,,
142,1 0.9 0.8 0.7 0.6 0.5 0.4,null,null
,,,
143,0.3 (a) I-rec@20,null,null
,,,
144,0.8 0.7 0.6 0.5 0.4 0.3 0.2,null,null
,,,
145,0.1 (b) D-nDCG@20,null,null
,,,
146,true pseudo truncated,null,null
,,,
147,uogTrA44xu uogTrB44xu utw2012c1 uogTrA44xl UDInfoDivSt utw2012lda utw2012sc1 srchvrs12c00 srchvrs12c10 UDInfoDivC2 DFalah120D DFalah121D UDInfoDivC1 ICTNET12DVR3 ICTNET12DVR2 ICTNET12DVR1,null,null
,,,
148,lcm4res manualSTA,null,null
,,,
149,autoSTB autoSTA uogTrA44xu uogTrB44xu DFalah120D DFalah121D uogTrA44xl lcm4res srchvrs12c10 srchvrs12c00 autoSTA autoSTB utw2012c1 manualSTA utw2012lda utw2012sc1 UDInfoDivSt ICTNET12DVR1 ICTNET12DVR2 ICTNET12DVR3 UDInfoDivC1 UDInfoDivC2,null,null
,,,
150,0.9 0.8 0.7 0.6 0.5 0.4 0.3,null,null
,,,
151,(c) D#-nDCG@20,null,null
,,,
152,0.2,null,null
,,,
153,0.9 0.8 0.7 0.6 0.5 0.4,null,null
,,,
154,0.3 (d) ERR-IA@20,null,null
,,,
155,0.2,null,null
,,,
156,uogTrA44xu uogTrB44xu uogTrA44xl DFalah120D DFalah121D utw2012c1 srchvrs12c00 srchvrs12c10 utw2012lda utw2012sc1 UDInfoDivSt,null,null
,,,
157,lcm4res UDInfoDivC2 UDInfoDivC1 ICTNET12DVR3 ICTNET12DVR2 ICTNET12DVR1,null,null
,,,
158,manualSTA autoSTB autoSTA,null,null
,,,
159,uogTrA44xu uogTrB44xu DFalah121D DFalah120D utw2012c1 utw2012lda uogTrA44xl utw2012sc1 srchvrs12c00 srchvrs12c10 ICTNET12DVR1,null,null
,,,
160,autoSTA ICTNET12DVR3 ICTNET12DVR2,null,null
,,,
161,lcm4res autoSTB manualSTA UDInfoDivSt UDInfoDivC1 UDInfoDivC2,null,null
,,,
162,Figure 3: Run rankings: true vs. pseudo vs. truncated. The x axis represents runs sorted by a metric with true relevance data from TREC.,Y,null
,,,
163,"Figure 3 visualises the ""@20"" column of Table 3 for selected metrics. Recall that I-rec is a pure diversity metric; that D-nDCG is an overall relevance metric; and that D -nDCG and ERR-IA consider both aspects. It can be observed that I-rec with pseudo-qrels is almost completely useless for ranking runs. On the other hand, D-nDCG with pseudo-qrels does better: for example, the top two runs in terms of D-nDCG with the true qrels (uogTrA44xu and uogTrB44xu) are still the top two in terms of D-nDCG with the (truncated) pseudo-qrels. The same two runs are also top performers in terms of D -nDCG as well, regardless of the qrels being used. As for ERR-IA, while the same two runs are the top performer in terms of the true qrels, the second run uogTrB44xu is ranked third with the (truncated) pseudo-qrels. To sum up, while our pseudoqrels cannot properly estimate systems's intent recall, the top run at TREC, namely, uogTrA44xu, is still the top run when evaluated with D( )-nDCG and ERR-IA based on the NTCIR intents and the pseudo-qrels. However, the overall rankings do differ when the TREC intents are replaced with those from NTCIR. Again, since the graphs for the original and truncated pseudo-qrels behave very",Y,null
,,,
164,923,null,null
,,,
165,1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1,null,null
,,,
166,0,null,null
,,,
167,151 156 161 166 171 176 181 186 191 196,null,null
,,,
168,1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1,null,null
,,,
169,0,null,null
,,,
170,151 156 161 166 171 176 181 186 191 196,null,null
,,,
171,(a) D#-nDCG @20,null,null
,,,
172,true pseudo truncated,null,null
,,,
173,(b) ERR-IA @20,null,null
,,,
174,true pseudo truncated,null,null
,,,
175,Figure 4: Per-topic performance values for lcm4res.,null,null
,,,
176,"similarly, the discrepancies between TREC and NTCIR are probably due to the choice of intents.",Y,null
,,,
177,"In Figure 3, as indicated by the arrows, Run lcm4res is heavily overestimated with D( )-nDCG and ERR-IA based on the pseudoqrels. Figure 4 provides a per-topic diagnosis for this run with D nDCG and ERR-IA, which reveals that the pseudo-qrels overestimate the run's performance for almost all topics. Perhaps the worst case is Topic 170 (""scooters""), for which there are three TREC intents and eight NTCIR intents: even though the true D -nDCG and ERR-IA values are zero (as indicated by the arrows), the corresponding values with the pseudo-qrels are .8698 and .8750 (.9378 and 1 when truncated). As we have not conducted relevance assessments, we cannot rule out the possibility that this run actually retrieved many documents that are relevant to the NTCIR intents yet nonrelevant to the TREC intents for this topic. However, the overall trend across the topics strongly suggests that our pseudoqrels do not provide accurate estimates of intent recall. We leave the analysis of the TREC runs using true relevance assessments for the NTCIR intents for future work.",Y,null
,,,
178,"We now discuss the effect of replacing the TREC intent sets with the NTCIR ones on statistical significance testing: Table 4 summarises the results. Note that, if the significance test results with true and pseudo-qrels are identical, the number of significantly different pairs in the TR, PS and TRPS will be the same, and that the TR-PS and PS-TR will contain zeroes. Such is not the case. That is, conclusions drawn from an experiment based on the original TREC intents and those drawn from one based on the intents derived from NTCIR can be quite different. For example, in Table 4(b), ERR-IA@20 obtains 31 significantly different run pairs with the true qrels and 18 significantly different run pairs with the pseudo-qrels; but only 9 pairs overlap. Again, truncating the pseudo-qrels (Table 4(c)(d)) does not seem to solve any problems, which again suggests that the choice of intents do matter for the purpose of comparing diversified search systems.",Y,null
,,,
179,4. CONCLUSIONS AND FUTURE WORK,null,null
,,,
180,"We addressed the following research question: Does the choice of intents for a test collection affect relative performances of diversified search systems? To this end, we used the TREC 2012 Web Track Diversity Task data and the NTCIR-10 INTENT-2 Task data, which share a set of 50 topics but have different intent sets. Our initial results suggest that the choice of intents may in fact affect relative performances, and that this choice may be more important than how many intents are selected for each topic.",Y,null
,,,
181,One limitation of the present work is that we used automaticallygenerated pseudo-qrels for the NTCIR intents instead of conducting relevance assessments of TREC pooled documents for the NT-,Y,null
,,,
182,Table 4: Significance test concordances and discordances be-,null,null
,,,
183,tween true qrels and pseudo-qrels (190 TREC 2012 diver-,Y,null
,,,
184,sity run pairs; randomised two-sided Tukey's HSD test at,null,null
,,,
185," , .05). TR (PS): significant differences obtained with true",null,null
,,,
186,qrels (pseudo-qrels); TR-PS (PS-TR): pairs significant with,null,null
,,,
187,true qrels (pseudo-qrels) but not significant with pseudo-qrels,null,null
,,,
188,(true qrels); TRPS: pairs significant with both true qrels and,null,null
,,,
189,pseudo-qrels.,null,null
,,,
190,TR PS TR-PS TRPS PS-TR,null,null
,,,
191,(a) true,null,null
,,,
192,I-rec,null,null
,,,
193,12 21,null,null
,,,
194,6,null,null
,,,
195,6,null,null
,,,
196,15,null,null
,,,
197,vs.,null,null
,,,
198,D-nDCG 51 56,null,null
,,,
199,9,null,null
,,,
200,42,null,null
,,,
201,14,null,null
,,,
202,pseudo D -nDCG 25 29,null,null
,,,
203,9,null,null
,,,
204,16,null,null
,,,
205,13,null,null
,,,
206,@10,null,null
,,,
207,-nDCG 26 24,null,null
,,,
208,16,null,null
,,,
209,10,null,null
,,,
210,14,null,null
,,,
211,ERR-IA 29 19,null,null
,,,
212,20,null,null
,,,
213,9,null,null
,,,
214,10,null,null
,,,
215,(b) true,null,null
,,,
216,I-rec,null,null
,,,
217,9 11,null,null
,,,
218,7,null,null
,,,
219,2,null,null
,,,
220,9,null,null
,,,
221,vs.,null,null
,,,
222,D-nDCG 60 60,null,null
,,,
223,15,null,null
,,,
224,45,null,null
,,,
225,15,null,null
,,,
226,pseudo D -nDCG 35 40,null,null
,,,
227,9,null,null
,,,
228,26,null,null
,,,
229,14,null,null
,,,
230,@20,null,null
,,,
231,-nDCG 26 24,null,null
,,,
232,15,null,null
,,,
233,11,null,null
,,,
234,13,null,null
,,,
235,ERR-IA 31 18,null,null
,,,
236,22,null,null
,,,
237,9,null,null
,,,
238,9,null,null
,,,
239,(c) true,null,null
,,,
240,I-rec,null,null
,,,
241,12 11,null,null
,,,
242,8,null,null
,,,
243,4,null,null
,,,
244,7,null,null
,,,
245,vs.,null,null
,,,
246,D-nDCG 51 47,null,null
,,,
247,16,null,null
,,,
248,35,null,null
,,,
249,12,null,null
,,,
250,truncated D -nDCG 25 14,null,null
,,,
251,11,null,null
,,,
252,14,null,null
,,,
253,0,null,null
,,,
254,@10,null,null
,,,
255,-nDCG 26 9,null,null
,,,
256,24,null,null
,,,
257,2,null,null
,,,
258,7,null,null
,,,
259,ERR-IA 29 9,null,null
,,,
260,25,null,null
,,,
261,4,null,null
,,,
262,5,null,null
,,,
263,(d) true,null,null
,,,
264,I-rec,null,null
,,,
265,9 28,null,null
,,,
266,3,null,null
,,,
267,6,null,null
,,,
268,22,null,null
,,,
269,vs.,null,null
,,,
270,D-nDCG 60 55,null,null
,,,
271,20,null,null
,,,
272,40,null,null
,,,
273,15,null,null
,,,
274,truncated D -nDCG 35 35,null,null
,,,
275,12,null,null
,,,
276,23,null,null
,,,
277,12,null,null
,,,
278,@20,null,null
,,,
279,-nDCG 26 13,null,null
,,,
280,24,null,null
,,,
281,2,null,null
,,,
282,11,null,null
,,,
283,ERR-IA 31 10,null,null
,,,
284,27,null,null
,,,
285,4,null,null
,,,
286,6,null,null
,,,
287,"CIR intents. In particular, we found that the pseudo-qrels estimate intent recall very poorly. On the other hand, we have also found that the official top performer at the TREC 2012 diversity task is still the top performer even after the intent sets have been replaced with the ones from NTCIR. In order to obtain a more clear answer to our research question, we hope to come back to it with true relevance assessments for the NTCIR intents.",Y,null
,,,
288,5. REFERENCES,null,null
,,,
289,"[1] B. Carterette. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM TOIS, 30(1), 2012.",null,null
,,,
290,"[2] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results: Metrics and algorithms. Information Retrieval, 14(6):572­592, 2011.",null,null
,,,
291,"[3] C. L. A. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of ACM WSDM 2011, pages 75­84, 2011.",null,null
,,,
292,"[4] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. Overview of the TREC 2012 web track. In Proceedings of TREC 2012, 2013.",Y,null
,,,
293,"[5] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. ACM TOIS, 20(4):422­446, 2002.",null,null
,,,
294,"[6] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. P. Kato, R. Song, and M. Iwata. Overview of the NTCIR-10 INTENT-2 task. In Proceedings of NTCIR-10, 2013.",Y,null
,,,
295,"[7] T. Sakai, Z. Dou, T. Yamamoto, Y. Liu, M. Zhang, M. P. Kato, R. Song, and M. Iwata. Summary of the NTCIR-10 INTENT-2 task: Subtopic mining and search result diversification. In Proceedings of ACM SIGIR 2013, 2013.",Y,null
,,,
296,"[8] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of ACM SIGIR 2011, pages 1043­1042, 2011.",null,null
,,,
297,"[9] I. Soboroff, C. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proceedings of ACM SIGIR 2001, pages 66­73, 2001.",null,null
,,,
298,"[10] E. Yilmaz, J. Aslam, and S. Robertson. A new rank correlation coefficient for information retrieval. In Proceedings of ACM SIGIR 2008, pages 587­594, 2008.",null,null
,,,
299,"[11] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: Methods and evaluation metrics for subtopic retrieval. In Proceedings of ACM SIGIR 2003, pages 10­17, 2003.",null,null
,,,
300,924,null,null
,,,
301,,null,null

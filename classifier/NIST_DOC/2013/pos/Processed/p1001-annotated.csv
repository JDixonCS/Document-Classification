,sentence,label,data
,,,
0,Estimating Topical Context by Diverging from External Resources,null,null
,,,
1,Romain Deveaud,null,null
,,,
2,"University of Avignon - LIA Avignon, France",null,null
,,,
3,romain.deveaud@univavignon.fr,null,null
,,,
4,Eric SanJuan,null,null
,,,
5,"University of Avignon - LIA Avignon, France",null,null
,,,
6,eric.sanjuan@univavignon.fr,null,null
,,,
7,Patrice Bellot,null,null
,,,
8,"Aix-Marseille University - LSIS Marseille, France",null,null
,,,
9,patrice.bellot@lsis.org,null,null
,,,
10,ABSTRACT,null,null
,,,
11,"Improving query understanding is crucial for providing the user with information that suits her needs. To this end, the retrieval system must be able to deal with several sources of knowledge from which it could infer a topical context. The use of external sources of information for improving document retrieval has been extensively studied. Improvements with either structured or large sets of data have been reported. However, in these studies resources are often used separately and rarely combined together. We experiment in this paper a method that discounts documents based on their weighted divergence from a set of external resources. We present an evaluation of the combination of four resources on two standard TREC test collections. Our proposed method significantly outperforms a state-of-the-art Mixture of Relevance Models on one test collection, while no significant differences are detected on the other one.",Y,null
,,,
12,Categories and Subject Descriptors,null,null
,,,
13,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Relevance feedback,null,null
,,,
14,Keywords,null,null
,,,
15,"External resources, language models, topical context",null,null
,,,
16,1. INTRODUCTION,null,null
,,,
17,"When searching for specific information in a document collection, users submit a query to the retrieval system. The query is a representation or an interpretation of an underlying information need, and may not be accurate depending on the background knowledge of the user. Automatically retrieving documents that are relevant to this initial information need may thus be challenging without additional information about the topical context of the query. One common approach to tackle this problem is to extract evidences from query-related documents [8, 16]. The basic idea is to expand the query with words or multi-word terms extracted",null,null
,,,
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright © 2013 ACM 978-1-4503-2034-4/13/07...$15.00.",null,null
,,,
19,"from feedback documents. This feedback set is composed of documents that are relevant or pseudo-relevant to the initial query, and that are likely to carry important pieces of information. Words that convey the most information or that are the most relevant to the initial query are then used to reformulate the query. They can come from the target collection or from external sources and several sources can be combined [1, 3]. These words usually are synonyms or related concepts, and allow to infer the topical context of the user search. Documents are then ranked based, among others, on their similarity to the estimated topical context.",null,null
,,,
20,"We explore the opposite direction and choose to carry experiments with a method that discounts documents scores based on their divergences from pseudo-relevant subsets of external resources. We allow the method to take several resources into account and to weight the divergences in order to provide a comprehensive interpretation of the topical context. More, our method equally considers sequences of 1, 2 or 3 words and chooses which terms best describe the topical context without any supervision.",null,null
,,,
21,"The use of external data sets had been extensively studied in the pseudo-relevance feedback setting, and proved to be effective at improving search performance when choosing proper data. However studies mainly concentrated on demonstrating how the use of a single resource could improve performance. Data sources like Wikipedia [10, 15], WordNet [11, 15], news corpora or even the web itself [1, 3] were used separately for enhancing search performances. Combining several source of information was nonetheless studied in [1]. However the authors used web anchor and heading texts, which are very small units that are less likely to carry a complete context. They also used the entire Wikipedia but they did not report results of its contribution in the information sources combination. Diaz and Metzler [3] investigated the use of larger and more general external resources than those used in [1]. They present a Mixture of Relevance Models (MoRM) that estimates the query model using a news corpus and two web corpora as external sources, and achieves state-of-the-art retrieval performance. To our knowledge, this last approach is the closest one from the method we experiment in this paper.",null,null
,,,
22,2. DIVERGENCE FROM RESOURCES,null,null
,,,
23,"In this work, we use a language modeling approach to information retrieval. Our goal is to accurately model the topical context of a query by using external resources. We use the Kullback-Leibler divergence to measure the information gain (or drift) between a given resource R and a",null,null
,,,
24,1001,null,null
,,,
25,"document D. Formally, the KL divergence between two language models R and D is written as:",null,null
,,,
26,K L(R ||D ),null,null
,,,
27,",",null,null
,,,
28,tV,null,null
,,,
29,P (t|R) log,null,null
,,,
30,P (t|R) P (t|D),null,null
,,,
31,",",null,null
,,,
32,P (t|R) log P (t|R) - P (t|R) log P (t|D),null,null
,,,
33,tV,null,null
,,,
34,tV,null,null
,,,
35, - P (t|R) log P (t|D),null,null
,,,
36,-1,null,null
,,,
37,tV,null,null
,,,
38,"where t is a term belonging to vocabulary V . The first part is the resource entropy and does not affect ranking of documents, which allows us to simplify the KL divergence and to obtain equation (1). In order to capture the topical context from the resource, we estimate the R model through pseudo-relevance feedback. Given a ranked list RQ obtained by retrieving the top N documents of R using query likelihood, the feedback query model is estimated by:",null,null
,,,
39,P (t|^R) ,null,null
,,,
40,P (Q|DF ) - P (w|DF ) log P (w|DF ),null,null
,,,
41,DF RQ,null,null
,,,
42,wt,null,null
,,,
43,"The right-hand expression of this estimation is actually equivalent to computing the entropy of the term t in the pseudorelevant subset RQ. One advantage of doing so it that t may not be necessarily a single term, like in traditional relevance models approaches [3, 9], or a fixed-length term [12]. When forming the V set, we slide a window over the entire textual content of RQ and consider all sequences of 1, 2 or 3 words.",null,null
,,,
44,"Following equation (1), we compute the information divergence between a resource R and a document D as:",null,null
,,,
45,"D(^R||D) , - P (t|^R) log P (t|D)",null,null
,,,
46,tV,null,null
,,,
47,The final score of a document D with respect to a given user query Q is determined by the linear combination of query word matches (standard retrieval) and the weighted divergence from general resources. It is formally written as:,null,null
,,,
48,"s(Q, D) ,  log P (Q|D) - (1 - )",null,null
,,,
49,R · D(^R||D) (2),null,null
,,,
50,RS,null,null
,,,
51,"where S is a set of resources, P (Q|D) is standard query likelihood with Dirichlet smoothing and R represents the weight given to resource R. We use here the information divergence to reduce the score of a document: the greater the divergence, the lower the score of the document will be. Hence the combination of several resources intuitively acts as a generalization of the topical context, and increasing the number of resources will eventually improve the topical representation of the user information need. While we chose to use traditional query likelihood for practical and reproducibility reasons, it could entirely be substituted with other state-of-the-art retrieval models (e.g. MRF-IR [12], BM25 [13]...).",null,null
,,,
52,3. EXPERIMENTS,null,null
,,,
53,3.1 Experimental setup,null,null
,,,
54,"We performed our evaluation using two main TREC1 collections which represent two different search contexts. The first one is the WT10g web collection and consists of 1,692,096",Y,null
,,,
55,1http://trec.nist.gov,null,null
,,,
56,"web pages, as well as the associated TREC topics (451-550) and judgments. The second data set is the Robust04 collection, which is composed of news articles coming from various newspapers. It was used in the TREC 2004 Robust track and is composed of standard corpora: FT (Financial Times), FR (Federal Register 94), LA (Los Angeles Times) and FBIS (i.e. TREC disks 4 and 5, minus the Congressional Record). The test set contains 250 topics (301-450, 601-700) and relevance judgements of the Robust 2004 track. Along with the test collections, we used a set of external resources from which divergences are computed. This set is composed of four general resources: Wikipedia as an encyclopedic source, the New York Times and GigaWord corpora as sources of news data and the category B of the ClueWeb092 collection as a web source. The English GigaWord LDC corpus consists of 4,111,240 news-wire articles collected from four distinct international sources including the New York Times [4]. The New York Times LDC corpus contains 1,855,658 news articles published between 1987 and 2007 [14]. The Wikipedia collection is a recent dump from May 2012 of the online encyclopedia that contains 3,691,092 documents3. We removed the spammed documents from the category B of the ClueWeb09 according to a standard list of spams for this collection4. We followed authors recommendations [2] and set the ""spamminess"" threshold parameter to 70. The resulting corpus contains 29,038,220 web pages.",Y,null
,,,
57,"Indexing and retrieval were performed using Indri5. The two test collections and the four external resources were indexed with the exact same parameters. We use the standard INQUERY english stoplist along with the Krovetz stemmer. We employ a Dirichlet smoothing and set the  parameter to 1, 500. Documents are ranked using equation (2). We compare the performance of the approach presented in Section 2 (DfRes) with that of three baselines: Query Likelihood (QL), Relevance Models (RM3) [9] and Mixture of Relevance Models (MoRM) [3]. In the results reported in Table 1, the MoRM and DfRes approaches both perform feedback using all external resources as well as the target collection, while RM3 only performs feedback using the target collection. QL uses no additional information.",null,null
,,,
58,"RM3, MoRM and DfRes depend on three free-parameters:  which controls the weight given to the original query, k which is the number of terms and N which is the number of feedback documents from which terms are extracted. We performed leave-one-query-out cross-validation to find the best parameter setting for  and averaged the performance for all queries. Previous research by He and Ounis [5] showed that doing PRF with the top 10 pseudo-relevant feedback documents was as effective as doing PRF with only relevant documents present in the top 10, and that there are no statistical differences. Following these findings, we set N , 10 and also k ,"" 20, which was found to be a good PRF setting. DfRes depends on an additional parameter R which controls the weight given to each resource. We also perform leave-one-query-out cross-validation to learn the best setting for each resource. Although the results in Table 1 correspond to this parameter setting, we explore in the following section the influence of the N and k parameters. In""",null,null
,,,
59,2http://boston.lti.cs.cmu.edu/clueweb09/ 3http://dumps.wikimedia.org/enwiki/20110722/ 4http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/ 5http://www.lemurproject.org/,Y,null
,,,
60,1002,null,null
,,,
61,wt10g robust,Y,null
,,,
62,QL MAP P@20,null,null
,,,
63,0.2026 0.2461,null,null
,,,
64,0.2429 0.3528,null,null
,,,
65,RM3 MAP P@20,null,null
,,,
66,0.2035 0.2449 0.2727 0.3677,null,null
,,,
67,MoRM,null,null
,,,
68,MAP,null,null
,,,
69,P@20,null,null
,,,
70,"0.2339, 0.2869,",null,null
,,,
71,"0.2833, 0.3799,",null,null
,,,
72,DfRes,null,null
,,,
73,MAP,null,null
,,,
74,P@20,null,null
,,,
75,"0.2463, 0.3147,,",null,null
,,,
76,"0.2954, 0.4024,,",null,null
,,,
77,"Table 1: Document retrieval results reported in terms of Mean Average Precision and Precision at 20 documents. We use a two sided paired wise t-test to determine significant differences over baselines. ,  and  indicate statistical improvements over QL, RM3 and MoRM respectively, with p < 0.05.",null,null
,,,
78,"the following section, when discussing results obtained using single sources of expansion with DfRes, we use the notation DfRes-r where r  (Web,Wiki,NYT,Gigaword).",null,null
,,,
79,3.2 Results,null,null
,,,
80,"The main observation we can draw from the ad hoc retrieval results presented in Table 1 is that using a combination of external information sources performs always better than only using the target collection. The numbers we report vary from those presented in [3], however we could not replicate the exact same experiments since the authors do not detail indexing parameters. DfRes significantly outperforms RM3 on both collections, which confirms that state that combining external resources improves retrieval.",null,null
,,,
81,"We see from Figure 1 that DfRes-Gigaword is ineffective on the WT10g collection, which is not in line with the results reported in [3] where the Gigaword was found to be an interesting source of expansion. Another remarkable result is the ineffectiveness of the WT10g collection as a single source of expansion. However we see from Table 2 that the learned weight R of this resource is very low (,"" 0.101), which significantly reduces its influence compared to other best performing resources (such as NYT or Web).""",Y,null
,,,
82,wt10g robust,Y,null
,,,
83,nyt,null,null
,,,
84,0.303 0.309,null,null
,,,
85,wiki,null,null
,,,
86,0.162 0.076,null,null
,,,
87,gigaword,null,null
,,,
88,0.121 0.281,null,null
,,,
89,web,null,null
,,,
90,0.313 0.149,null,null
,,,
91,robust,Y,null
,,,
92,0.185,null,null
,,,
93,wt10g,Y,null
,,,
94,0.101 -,null,null
,,,
95,Table 2: R weights learned for resources on the two collections. We averaged weights over all queries.,null,null
,,,
96,"Results are more coherent on the Robust collection. DfResNYT and DfRes-Gigaword achieve very good results, while the combination of all resources consistently achieves the best results. The very high weights learned for these resources hence reflect these good performances. As previously noted, the Robust collection is composed of news articles coming from several newspapers (not including the NYT). In this specific setting, it seems that the nature of the good-performing resources is correlated with the nature of the target collection. We observed that NYT and Gigaword articles, which are focused contributions produced by professional writers, are smaller on average (in unique words) than Wikipedia or Web documents.",Y,null
,,,
97,"We explored the influence of the number of feedback documents used for the approximation of each resource. We omit the plots of retrieval performances for the sake of space, and also because they are not noteworthy. Performances indeed remain almost constant for all resources as N varies. Changes in MAP are about ± 2% from N , 1 to N , 20 depending on the resource. However we also explored the influence of the number of terms used to estimate each resource's model. While we could expect that increasing the number of terms would improve the granularity of the model",null,null
,,,
98,"and maybe capture more contextual evidences, we see from Figure 2 that using 100 terms is not really different than using 20 terms. We even see that using only 5 terms achieves the best results for DfRes on the WT10g collection.",Y,null
,,,
99,"Overall, these results show support for the principles of polyrepresentation [6] and intentional redundancy [7] which state that combining cognitively and structurally different representations of information needs and documents will increase the likelihood of finding relevant documents. Since we use several resources of very different natures ranging from news articles to web pages, DfRes takes advantage of this variety to improve the estimation of the topical context. Moreover, the most effective values of  tend to be low, which means that DfRes is more effective than the initial query. We even see on Figure 1 that only relying on the divergence from resources (i.e. setting  , 0) achieves better results than only relying on the user query (i.e. setting  ,"" 1). More, setting  "", 0 for DfRes also outperforms MoRM (significantly on the Robust collection). This suggests that DfRes is actually better as estimating the topical context of the information need than the user keyword query.",null,null
,,,
100,"We also observe from Figure 1 and 2 that the NYT is the resource that provides the best estimation of the topical context for the two collections, despite being the smallest one. This may be due to the fact that articles are well-written by professionals and contain lots of synonyms to avoid repetition. Likewise, the failure of Wikipedia may be due to the encyclopedic segmentation of articles. Since each Wikipedia article covers a specific concept, it is likely that only conceptrelated articles compose the pseudo-relevant set, which may limit a larger estimation of the topical context. One of the originality of the DfRes is that it can automatically take into account n-grams without any supervision (such as setting the size of the grams prior to retrieval). In practice, there is on average 1.19 words per term, but most of the time articles like ""the"" are added to words that already were selected (i.e. ""the nativity scene"", where ""nativity"" and ""scene"" were used before as single words).",null,null
,,,
101,4. CONCLUSION & FUTURE WORK,null,null
,,,
102,"Accurately estimating the topical context of a query is a challenging issue. We experimented a method that discounts documents based on their average divergence from a set of external resources. Results showed that, while reinforcing previous research, this method performs at least as good as a state-of-the-art resource combination approach, and sometimes achieves significantly higher results. Performances achieved by the NYT as a single resource are very promising and need further exploration, as well as the counter-performance of Wikipedia. More specifically, using nominal groups or sub-sentences that rely on the good quality of NYT articles could be interesting and in line with ongoing research in the Natural Language Processing field.",null,null
,,,
103,1003,null,null
,,,
104,wt10g,Y,null
,,,
105,robust,Y,null
,,,
106,MAP 0.20 0.22 0.24 0.26 0.28 0.30,null,null
,,,
107,MAP 0.12 0.14 0.16 0.18 0.20 0.22 0.24,null,null
,,,
108,wt10g web wiki nyt gigaword all,Y,null
,,,
109,robust web wiki nyt gigaword all,Y,null
,,,
110,0,null,null
,,,
111,0.2,null,null
,,,
112,0.4,null,null
,,,
113,0.6,null,null
,,,
114,0.8,null,null
,,,
115,1,null,null
,,,
116,0,null,null
,,,
117,0.2,null,null
,,,
118,0.4,null,null
,,,
119,0.6,null,null
,,,
120,0.8,null,null
,,,
121,1,null,null
,,,
122,lambda,null,null
,,,
123,lambda,null,null
,,,
124,"Figure 1: Retrieval performance (in MAP) as a function of the  parameter. The DfRes results reported in Table 1 are depicted by curve ""all"", while all other curves correspond to DfRes with a single resource. Baselines are shown for reference: dashed lines represent RM3 and dotted lines represent MoRM.",null,null
,,,
125,wt10g,Y,null
,,,
126,robust,Y,null
,,,
127,0.3,null,null
,,,
128,0.25,null,null
,,,
129,MAP,null,null
,,,
130,MAP 0.14 0.16 0.18 0.20 0.22 0.24,null,null
,,,
131,0.2,null,null
,,,
132,wt10g web wiki nyt gigaword all,Y,null
,,,
133,robust web wiki nyt gigaword all,Y,null
,,,
134,0.15,null,null
,,,
135,0,null,null
,,,
136,20,null,null
,,,
137,40,null,null
,,,
138,60,null,null
,,,
139,80,null,null
,,,
140,100,null,null
,,,
141,0,null,null
,,,
142,20,null,null
,,,
143,40,null,null
,,,
144,60,null,null
,,,
145,80,null,null
,,,
146,100,null,null
,,,
147,k,null,null
,,,
148,k,null,null
,,,
149,Figure 2: Retrieval performance (in MAP) as a function of the number of terms k used for estimating the,null,null
,,,
150,resource language model. Legend is the same as in Figure 1.,null,null
,,,
151,5. ACKNOWLEDGMENTS,null,null
,,,
152,This work was supported by the French Agency for Scientific Research (Agence Nationale de la Recherche) under CAAS project (ANR 2010 CORD 001 02).,null,null
,,,
153,6. REFERENCES,null,null
,,,
154,"[1] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proceedings of WSDM, 2012.",null,null
,,,
155,"[2] G. Cormack, M. Smucker, and C. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 2011.",null,null
,,,
156,"[3] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of SIGIR, 2006.",null,null
,,,
157,"[4] D. Graff and C. Cieri. English Gigaword. Philadelphia: Linguistic Data Consortium, LDC2003T05, 2003.",null,null
,,,
158,"[5] B. He and I. Ounis. Finding good feedback documents. In Proceedings of CIKM, 2009.",null,null
,,,
159,"[6] P. Ingwersen. Polyrepresentation of information needs and semantic entities: elements of a cognitive theory for information retrieval interaction. In Proc. of SIGIR, 1994.",null,null
,,,
160,"[7] K. Jones. Retrieving Information Or Answering Questions? British Library annual research lecture. British Library Research and Development Department, 1990.",null,null
,,,
161,"[8] R. Kaptein and J. Kamps. Explicit extraction of topical context. J. Am. Soc. Inf. Sci. Technol., 62(8):1548­1563, Aug. 2011.",null,null
,,,
162,"[9] V. Lavrenko and W. B. Croft. Relevance based language models. In Proceedings of SIGIR, 2001.",null,null
,,,
163,"[10] Y. Li, W. P. R. Luk, K. S. E. Ho, and F. L. K. Chung. Improving weak ad-hoc queries using Wikipedia as external corpus. In Proceedings of SIGIR, 2007.",null,null
,,,
164,"[11] S. Liu, F. Liu, C. Yu, and W. Meng. An effective approach to document retrieval via utilizing WordNet and recognizing phrases. In Proceedings of SIGIR, 2004.",null,null
,,,
165,"[12] D. Metzler and W. B. Croft. Latent Concept Expansion Using Markov Random Fields. In Proc. of SIGIR, 2007.",null,null
,,,
166,"[13] S. E. Robertson and S. Walker. Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval. In Proceedings of SIGIR, 1994.",null,null
,,,
167,"[14] E. Sandhaus. The New York Times Annotated Corpus. Philadelphia: Linguistic Data Consortium, LDC2008T19, 2008.",null,null
,,,
168,"[15] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a core of semantic knowledge. In Proceedings of WWW, 2007.",null,null
,,,
169,"[16] R. W. White, P. Bailey, and L. Chen. Predicting user interests from contextual information. In Proceedings of SIGIR, 2009.",null,null
,,,
170,1004,null,null
,,,
171,,null,null

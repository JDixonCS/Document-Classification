,sentence,label,data
,,,
0,Modeling Term Dependencies with Quantum Language Models for IR,null,null
,,,
1,Alessandro Sordoni,null,null
,,,
2,Jian-Yun Nie,null,null
,,,
3,sordonia@iro.umontreal.ca nie@iro.umontreal.ca,null,null
,,,
4,"DIRO, Université de Montréal Montréal, H3C 3J7, Québec",null,null
,,,
5,Yoshua Bengio bengioy@iro.umontreal.ca,null,null
,,,
6,ABSTRACT,null,null
,,,
7,"Traditional information retrieval (IR) models use bag-ofwords as the basic representation and assume that some form of independence holds between terms. Representing term dependencies and defining a scoring function capable of integrating such additional evidence is theoretically and practically challenging. Recently, Quantum Theory (QT) has been proposed as a possible, more general framework for IR. However, only a limited number of investigations have been made and the potential of QT has not been fully explored and tested. We develop a new, generalized Language Modeling approach for IR by adopting the probabilistic framework of QT. In particular, quantum probability could account for both single and compound terms at once without having to extend the term space artificially as in previous studies. This naturally allows us to avoid the weight-normalization problem, which arises in the current practice by mixing scores from matching compound terms and from matching single terms. Our model is the first practical application of quantum probability to show significant improvements over a robust bag-of-words baseline and achieves better performance on a stronger non bag-of-words baseline.",null,null
,,,
8,Categories and Subject Descriptors,null,null
,,,
9,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
,,,
10,Keywords,null,null
,,,
11,Density Matrices; Language Modeling; Retrieval Models,null,null
,,,
12,1. INTRODUCTION,null,null
,,,
13,"The quest for the effective modeling of term dependencies has been of central interest in the information retrieval (IR) community since the inception of first retrieval models. However, the gradual shift towards non bag-of-words models is strewn with modeling difficulties. One of the central",null,null
,,,
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
15,"problems is to find an effective way of representing and scoring documents based on such dependencies. As pointed out by Gao et al. [9], dependencies can be handled in two ways.",null,null
,,,
16,"The first approach is to extend the dimensionality of the representation space. In early geometrical retrieval models such as the Vector Space Model (VSM), dependencies arising from phrases (compound terms) are represented by defining additional dimensions in the space, i.e. both the phrase and its component single terms are regarded as representation features [8, 21, 28]. For example, computer architecture is considered as disjoint from computer and architecture, which is a strong modeling assumption, and does not take advantage of the semantic relation that generally exists between a compound phrase and its component terms.",null,null
,,,
17,"The second approach is more principled in such that simple terms are kept as representational units and term dependencies are modeled statistically as joint probabilities, i.e. p(computer, architecture). Proposed dependence models such as n-gram Language Model (LM) for IR [30], biterm LM [31] or the dependence LM [9] adopt such a representation. However, the gain from integrating dependencies was smaller than hoped [35] and it came with higher computational costs due to dependency parsing or n-gram models [13, 30], or unsupervised iterative methods for estimating the joint probability [9].",null,null
,,,
18,"Recently, non bag-of-words models such Markov random field (MRF) [19], quasi-synchronous dependence model [24] and the query hypergraph model [2] have been proposed. Most of these retrieval models take a log-linear form, which offers a very flexible way of taking into account term dependencies by integrating different sources of evidence, such as proximity heuristics and exact matching. However, the LM is used as a black box to estimate single-term and compoundterm influences separately and then the model combines them to compute the final score. We believe that, from a representational point of view, these models have implicitly made a turn back to the first VSM approach in the sense that the dependencies are assumed to represent additional concepts, i.e. atomic units for the purpose of document and query representation, thus disjoint from the component terms [2, 3]. This choice indeed allows for flexible scoring functions. However, the retrieval model boils down to a combination of scores obtained separately from matching single terms and from matching compound dependencies. This is the main cause of the weight-normalization problem [9, 11] which is that a dependency may be counted twice, as a compound and as component terms. In the context of phrases, Sparck Jones et al. note that ""the weight of the",null,null
,,,
19,653,null,null
,,,
20,"phrase should reflect not the increased odds of relevance implied by its presence as compared to its absence, as a whole unit, but the increased odds compared to the presence of its components words"" [11]. When integrating the evidence, the weights for the combination are usually estimated by optimizing a retrieval measure such as mean average precision (MAP). In this sense, a principled probabilistic interpretation of these models is difficult.",null,null
,,,
21,"The pioneering work by Van Rijsbergen [33] officially formalized the idea that Quantum Theory (QT) could be seen as a ""formal language that can be used to describe the objects and processes in information retrieval"". The idea of QT as a framework for manipulating vector spaces and probability is appealing. However, the methods that stem from this initial intuition provided only limited evidence about the usefulness and effectiveness of the framework for IR tasks. For example, Piwowarski et al. [25] test if acceptable performance for ad-hoc tasks could be achieved with a quantum approach to IR. The authors represent documents as subspaces and queries as density operators. However, both documents and queries representations are estimated through passage-retrieval like heuristics, i.e. a document is divided into passages and is associated to a subspace spanned by the vectors corresponding to document passages [25]. Different representations for the query density matrix are tested but none of them led to good retrieval performance. Successively, a number of works took inspiration from quantum phenomena in order to relax some common assumption in IR [37, 38]. Zuccon and Azzopardi [38] introduce interference effects into the Probability Ranking Principle (PRP) in order to rank interdependent documents. Although this method achieves good results, it does not make principled use of the quantum probability space and cannot be considered as evidence towards the usefulness of the enlarged probabilistic space. In general, these methods made heuristic use of the concepts of the theory and no clear probabilistic interpretation can be given.",null,null
,,,
22,"The intrinsic heuristic flavor in preceding approaches motivated some authors to provide evidence to the hypothesis that there exists an IR situation in which classical probabilistic IR fails, or it is severely limited, and it is thus necessary to switch to a more general probabilistic theory [16, 17, 34]. Although these works are theoretically grounded and heavily influenced our general vision of the theory, no clue is given on how to operationalize such results in real-world applications.",null,null
,,,
23,"In this paper, we propose a novel retrieval framework for modeling term dependencies based on the probabilistic calculus offered by QT. In our model, both single terms and compound dependencies are mathematically modeled as projectors in a vector space, i.e. elementary events in an enlarged probabilistic space. In particular, a compound dependency is represented as a superposition event which is a special kind of projector that is neither disjoint from its component terms, nor a joint event. Documents and queries are represented as a sequence of projectors associated to a quantum language model (QLM), encapsulated in a particular matrix. The scoring function is a divergence between query and document QLMs. We will show that our model is a generalization of classical unigram LMs. To our knowledge, this work can be seen as the first work to use the quantum probabilistic calculus in order to achieve improvements over state-of-the-art models.",null,null
,,,
24,Our contributions are as follows:,null,null
,,,
25,1. We propose a novel application of quantum probability to IR.,null,null
,,,
26,"2. Using this approach, we show significant improvements over a strong baseline bag-of-words model and a strong non bag-of-words model.",null,null
,,,
27,3. We propose a new way of representing dependencies without artificially extending the term space and without estimating expensive n-gram probabilities.,null,null
,,,
28,4. We show how the new representation of the dependency permits to specify how the dependency behaves with respect to its component terms.,null,null
,,,
29,"5. In our model, the dependency information is not integrated in the scoring phase, but in the estimation phase. Hence, our model does not suffer the weightnormalization problem.",null,null
,,,
30,2. A BROADER VIEW ON PROBABILITY,null,null
,,,
31,2.1 The Quantum Sample Space,null,null
,,,
32,"In quantum probability, the probabilistic space is natu-",null,null
,,,
33,"rally encapsulated in a vector space, specifically a Hilbert",null,null
,,,
34,"space, noted Hn, but for the sake of simplicity, in this pa-",null,null
,,,
35,"per we limit ourselves to finite real spaces, noted Rn. We",null,null
,,,
36,"will be using Dirac's notation restricted to the real field, for",null,null
,,,
37,"which a unit vector u  Rn, u 2 , 1 and its transpose u are respectively written as a ket |u and a bra u|. Using",null,null
,,,
38,"this notation, the projector onto the direction u writes as",null,null
,,,
39,|u u|. The inner product between two vectors writes as,null,null
,,,
40,"u|v . Moreover, we note by |ei the elements of the standard basis in Rn, i.e. |ei ,"" (1i, . . . , ni), where ij "", 1 iff",null,null
,,,
41,"i , j.",null,null
,,,
42,"Events are no more defined as subsets but as subspaces,",null,null
,,,
43,"more specifically as projectors onto subspaces [23, 34]. Given",null,null
,,,
44,"a 1-dimensional subspace spanned by a ket |u , the projector",null,null
,,,
45,"onto the unit norm vector |u , |u u|, is an elementary event",null,null
,,,
46,"of the quantum probability space, also called a dyad. A dyad",null,null
,,,
47,is always a projector onto a 1-dimensional space. Given the,null,null
,,,
48,"bijection between subspaces and projectors, it is correct to",null,null
,,,
49,"state that |u is itself an elementary event. For example,",null,null
,,,
50,"if n ,"" 2, the quantum elementary events |e1 "","" (1, 0),""",null,null
,,,
51,|f1,null,null
,,,
52,",",null,null
,,,
53,(,null,null
,,,
54,1 2,null,null
,,,
55,",",null,null
,,,
56,1 2,null,null
,,,
57,"),",null,null
,,,
58,can,null,null
,,,
59,be,null,null
,,,
60,represented,null,null
,,,
61,by,null,null
,,,
62,the,null,null
,,,
63,following,null,null
,,,
64,dyads:,null,null
,,,
65,"|e1 e1| ,",null,null
,,,
66,1 0,null,null
,,,
67,0 0,null,null
,,,
68,", |f1",null,null
,,,
69,"f1| ,",null,null
,,,
70,0.5 0.5,null,null
,,,
71,0.5 0.5,null,null
,,,
72,.,null,null
,,,
73,-1,null,null
,,,
74,"Generally, any ket |v ,"" i i|ui is called a superposition of the {|ui } where {|u1 , . . . , |un } form an orthonormal basis. In order to see the generalization that is taking place, one has to consider that in Rn there is an infinite number of""",null,null
,,,
75,"vectors even if the dimension n is finite. Hence, contrary to",null,null
,,,
76,"the classical case, an infinite number of elementary events",null,null
,,,
77,can be defined.,null,null
,,,
78,2.2 Density Matrices,null,null
,,,
79,"A quantum probability measure µ is the generalization of a classical probability measure such that (i) for every dyad |u u|, µ(|u u|)  [0, 1] and (ii) it reduces to a classical probability measure for any orthonormal basis {|u1 , . . . , |un }, i.e. i µ(|ui ui|) ,"" 1. Gleason's Theorem [10] states that,""",null,null
,,,
80,654,null,null
,,,
81,(a),null,null
,,,
82,",",null,null
,,,
83,(,null,null
,,,
84,0.75 0,null,null
,,,
85,0 0.25,null,null
,,,
86,),null,null
,,,
87,(b),null,null
,,,
88,",",null,null
,,,
89,(,null,null
,,,
90,0.5 0.5,null,null
,,,
91,0.5 0.5,null,null
,,,
92,),null,null
,,,
93,(c),null,null
,,,
94,",",null,null
,,,
95,(,null,null
,,,
96,0.5 0.25,null,null
,,,
97,0.25 0.5,null,null
,,,
98,),null,null
,,,
99,"Figure 1: The ellipses depict the set of points {|u : |u  R2}. The eigenvalues of  define how much each ellipse is stretched along the corresponding eigenvectors. To the left,  corresponds to a classical probability distribution. To the center,  is a pure state, thus the ellipse degenerates along the eigenvector corresponding to its unit eigenvalue. To the right, a general density matrix for which we vary both the eigenvalues and the eigensystem.",null,null
,,,
100,"for any real vector space with dimension greater than 2, there is a one-to-one correspondence between quantum probability measures µ and density matrices . The form of this correspondence is given by:",null,null
,,,
101,"µ(|v v|) , tr(|v v|).",null,null
,,,
102,-2,null,null
,,,
103,"A real density matrix is symmetric,  ,"" , positive semidefinite,   0, and of trace 1, tr  "","" 11. From now on, the set of n × n real density matrices would be noted Sn.""",null,null
,,,
104,"By Gleason's theorem, a density matrix can be seen as the proper quantum generalization of a classical probability distribution. It assigns a quantum probability to each one of the infinite dyads. For example, the density matrix:",null,null
,,,
105,",",null,null
,,,
106,0.5 0.5,null,null
,,,
107,0.5 0.5,null,null
,,,
108,",",null,null
,,,
109,-3,null,null
,,,
110,"assigns probabilities tr(|e1 e1|) , 0.5 and tr(|f1 f1|) ,"" 1. Hence, the event |f1 f1| is certain and still there is non-classical uncertainty on |e1 e1|. Only if {|u1 , . . . , |un } form an orthonormal system of Rn can the dyads |ui ui| be understood as disjoints events of a classical sample space, i.e.""",null,null
,,,
111,their probabilities sum to one. The relation that ties |e1 e1| and |f1 f1| is purely geometrical and cannot be expressed using set theoretic operations.,null,null
,,,
112,Any classical discrete probability distribution can be seen,null,null
,,,
113,"as a mixture over n elementary points, i.e. a parameter  ,",null,null
,,,
114,"(1, . . . , n), where i  0 and i i , 1. The density matrix is the straightforward generalization of this idea by con-",null,null
,,,
115,"sidering a mixture over orthogonal dyads  , i i|ui ui| where i  0 and i i ,"" 1. Given a density matrix , one can find the components dyads by taking its eigende-""",null,null
,,,
116,composition and building a dyad for each eigenvector. We,null,null
,,,
117,"note such decomposition by  , RR ,",null,null
,,,
118,"n i,1",null,null
,,,
119,i|ri,null,null
,,,
120,"ri|,",null,null
,,,
121,where |ri are the eigenvectors and i their corresponding,null,null
,,,
122,eigenvalues. This decomposition always exists for density,null,null
,,,
123,matrices [23].,null,null
,,,
124,Conventional probability distributions can be represented,null,null
,,,
125,"by diagonal density matrices. The sample space corresponds to the standard basis E , {|ei ei|}ni,""1. Hence, the density""",null,null
,,,
126,matrix corresponding to the parameter  above can be repre-,null,null
,,,
127,"sented as a mixture over E, i.e.  , diag() , i i|ei ei|.",null,null
,,,
128,1The trace is equal to the sum of the diagonal terms in a matrix.,null,null
,,,
129,"Consider a vocabulary of two terms V ,"" {a, b}. A unigram""",null,null
,,,
130,"language model  ,"" (0.75, 0.25) defined on V is represented by:""",null,null
,,,
131,",",null,null
,,,
132,3 4,null,null
,,,
133,|ea,null,null
,,,
134,ea|,null,null
,,,
135,+,null,null
,,,
136,1 4,null,null
,,,
137,|eb,null,null
,,,
138,"eb| ,",null,null
,,,
139,0.75 0,null,null
,,,
140,0 0.25,null,null
,,,
141,.,null,null
,,,
142,"Hence, term projectors are orthogonal, i.e. terms correspond",null,null
,,,
143,"to disjoint events. For example, the probability of the term",null,null
,,,
144,"a is computed by tr(|ea ea|) , 0.75. As conventional",null,null
,,,
145,probability distributions are restricted to the identity eigen-,null,null
,,,
146,"system, they differ in their eigenvalues, which correspond to",null,null
,,,
147,"diagonal entries. On the contrary, general density matrices",null,null
,,,
148,"can differ also in the eigensystem. For example, the density",null,null
,,,
149,matrix  of Eq. 3 has eigenvector |f1,null,null
,,,
150,",",null,null
,,,
151,(,null,null
,,,
152,1 2,null,null
,,,
153,",",null,null
,,,
154,1 2,null,null
,,,
155,),null,null
,,,
156,with,null,null
,,,
157,eigenvalue 1 and the eigenvector |f2,null,null
,,,
158,",",null,null
,,,
159,(,null,null
,,,
160,1 2,null,null
,,,
161,",",null,null
,,,
162,-,null,null
,,,
163,1 2,null,null
,,,
164,),null,null
,,,
165,with,null,null
,,,
166,"eigenvalue 0. Hence, it can be represented as a one-element",null,null
,,,
167,"mixture containing the projector  , |f1 f1|. When the",null,null
,,,
168,"mixture weights are concentrated into a single projector, the",null,null
,,,
169,"corresponding density matrix is called pure state. Otherwise,",null,null
,,,
170,it is called mixed state.,null,null
,,,
171,"When defined over Rn, density matrices can be seen as el-",null,null
,,,
172,"lipsoids, i.e. deformations of the unit sphere (Figure 1) [34].",null,null
,,,
173,"Classical probability distributions, i.e. diagonal density ma-",null,null
,,,
174,"trices, are ellipsoids stretched along the identity eigensys-",null,null
,,,
175,tem. As quantum probability has access to an infinite num-,null,null
,,,
176,"ber of eigensystems, the ellipsoid can be ""rotated"", i.e. de-",null,null
,,,
177,"fined on a different eigensystem. In this work, we will use",null,null
,,,
178,this additional feature in order to build a more reliable rep-,null,null
,,,
179,resentation of documents and queries taking into account,null,null
,,,
180,more complex information than single terms.,null,null
,,,
181,3. QUANTUM LANGUAGE MODELS,null,null
,,,
182,"The approach Quantum Language Modeling (QLM) retains the classical Language Modeling for IR as a special case. Hereafter, we will present in details the quantum counterpart of unigram language models. Although it is not explicitly developed in this paper, we argue that arbitrary n-gram models could be modeled as well.",null,null
,,,
183,3.1 Representation,null,null
,,,
184,"In classical bag-of-words language models, a document d is represented by a sequence of i.i.d. term events, i.e.",null,null
,,,
185,655,null,null
,,,
186,"Wd , {wi : i ,"" 1, . . . , N }, where N is the document length. Each wi belongs to a sample space V, corresponding to the vocabulary, of size n. It is assumed that such sequences cor-""",null,null
,,,
187,"respond to a sample from an unknown distribution  over the vocabulary V, for which we want to gain insight.",null,null
,,,
188,"A quantum language model assigns quantum probabilities to arbitrary subsets of the vocabulary. It is parametrized by an n × n density matrix ,   Sn, where n is the size of the vocabulary V. In QLM, a document d is considered as a sequence of M quantum events associated with a density matrix :",null,null
,,,
189,"Pd , {i : i ,"" 1, . . . , M },""",null,null
,,,
190,-4,null,null
,,,
191,where each i is a general dyad |u u| and represents a sub-,null,null
,,,
192,set of the vocabulary. Note that the number of dyads M can,null,null
,,,
193,"be different from N , the total number of terms in the document. The sequence Pd is constructed from the observed terms Wd: we have to define how to map subsets of terms to",null,null
,,,
194,projectors. Separating the observed text from the observed,null,null
,,,
195,projectors constitutes the main flexibility of our model. In,null,null
,,,
196,"what follows, we define a way of mapping single terms and",null,null
,,,
197,"arbitrary dependencies to quantum elementary events. Formally, we seek to define a mapping m : P(V)  L(Rn), where P(V) is the powerset of the vocabulary and L(Rn) is the set of dyads on Rn. As an initial assumption, we set m() ,"" O, where O is the projector onto the zero vector.""",null,null
,,,
198,3.1.1 Representing Single Terms,null,null
,,,
199,"In Section 2.2, we showed that unigram sample spaces can be represented as the set of projectors on the standard basis E , {|ei ei|}ni,""1 and unigram language models can be represented as mixtures over E, i.e. diagonal matrices. Therefore, a straightforward mapping from single terms to quantum events is:""",null,null
,,,
200,"m({w}) ,"" |ew ew|,""",null,null
,,,
201,-5,null,null
,,,
202,where w  V. This choice associates the occurrence of,null,null
,,,
203,"each term to a dyad |ew ew|, and these dyads form an or-",null,null
,,,
204,"thonormal basis. Hence, occurrences of single terms are still",null,null
,,,
205,"represented as disjoint events. Consider n , 3 and V ,",null,null
,,,
206,"{computer, architecture, games}. If Wd ,"" {computer, archi-""",null,null
,,,
207,"tecture} and one applies m to each of the terms, the sequence",null,null
,,,
208,"of corresponding projectors is Pd ,"" {Ecomputer, Earchitecture}""",null,null
,,,
209,"where Ew , |ew ew|:",null,null
,,,
210,100,null,null
,,,
211,0,null,null
,,,
212,"Ecomputer ,"" 0 0 0 , Earchitecture "", 0 1 0 . (6)",null,null
,,,
213,0,null,null
,,,
214,0,null,null
,,,
215,"Note that if we decide to observe only single terms, Pd turns out to be the quantum counterpart of classical observed terms Wd, i.e. M , N .",null,null
,,,
216,3.1.2 Representing Dependencies,null,null
,,,
217,"In this paper, by dependency, we mean a relationship linking two or more terms and we represent such an entity abstractly by a subset of the vocabulary, i.e.  ,"" {w1, . . . , wK }. We define the following mapping for an arbitrary dependency :""",null,null
,,,
218,K,null,null
,,,
219,"m() ,"" m({w1, . . . , wK }) "","" | |, | "","" i|ewi , (7)""",null,null
,,,
220,"i,1",null,null
,,,
221,"where the coefficients i  R must be chosen such that i i2 ,"" 1, in order to ensure the proper normalization of""",null,null
,,,
222,"Figure 2: The dependency ca is modeled as a projector onto |ca , i.e. as a superposition event.",null,null
,,,
223,| . The well-defined dyad | | is a superposition event.,null,null
,,,
224,"As we showed in Section 2.2, superposition events are justifi-",null,null
,,,
225,able only in the quantum probabilistic space. They are nei-,null,null
,,,
226,"ther disjoint from their constituents |ewi ewi | nor do they solely constitute joint events in the sense of n-grams: here,",null,null
,,,
227,the compound dependency is not considered as an additional,null,null
,,,
228,"entity, as done in previous models [2, 3, 19, 21]. The pro-",null,null
,,,
229,posed mapping allows for the representation of relationships,null,null
,,,
230,within a group of terms by creating a new quantum event,null,null
,,,
231,in the same n-dimensional space.,null,null
,,,
232,"In addition, superposition events come with a flexible way",null,null
,,,
233,in quantifying how much evidence the observation of depen-,null,null
,,,
234,dency  brings to its component terms. This is achieved by,null,null
,,,
235,changing the distribution of the i: if one wants to attempt,null,null
,,,
236,"a classical interpretation, the i can be viewed as relative",null,null
,,,
237,"pseudo-counts, i.e. observing | | adds fractional occur-",null,null
,,,
238,"rence to the events of its component terms |ewi ewi |. To our knowledge, until now this feature has been only mod-",null,null
,,,
239,"eled heuristically, or not modeled at all. In our framework,",null,null
,,,
240,it fits nicely in the quantum probabilistic space by specify-,null,null
,,,
241,ing how a compound dependency event and its constituent,null,null
,,,
242,single terms events are related.,null,null
,,,
243,"As an example, one could model the compound depen-",null,null
,,,
244,"dency between computer and architecture, ca ,"" {computer,""",null,null
,,,
245,"architecture}, by the dyad Kca ,"" |ca ca|, where |ca "",",null,null
,,,
246,2/3|ec + 1/3|ea (Figure 2). With respect to the exam-,null,null
,,,
247,"ple taken above, the event is represented by the matrix:",null,null
,,,
248,2,null,null
,,,
249,Kca,null,null
,,,
250,",",null,null
,,,
251,3 2,null,null
,,,
252,3,null,null
,,,
253, 2 3 1 3,null,null
,,,
254, 0 0 .,null,null
,,,
255,-8,null,null
,,,
256,0 00,null,null
,,,
257,The superposition coefficients entail that observing Kca adds more evidence to |ec ec| than to |ea ea|.,null,null
,,,
258,3.1.3 Choosing When and What to Observe,null,null
,,,
259,"Once we have defined the mapping m, one must ask three questions:",null,null
,,,
260,1. Which compound dependencies to consider?,null,null
,,,
261,2. When does such a compound dependency hold in a document?,null,null
,,,
262,"3. When the compound dependency is detected, should we also consider the projectors for its subsets as observed events?",null,null
,,,
263,"Regarding the first question, one may (a) use a dictionary of phrases or frequent n-grams, or (b) assume that any",null,null
,,,
264,656,null,null
,,,
265,w1,null,null
,,,
266,w2 w3,null,null
,,,
267,w4,null,null
,,,
268,w5 w6,null,null
,,,
269,w7,null,null
,,,
270,w8,null,null
,,,
271,Sakamura says he created Tron a computer architecture,null,null
,,,
272,1,null,null
,,,
273,2,null,null
,,,
274,3,null,null
,,,
275,4,null,null
,,,
276,5,null,null
,,,
277,6,null,null
,,,
278,7,null,null
,,,
279,8,null,null
,,,
280,ESakamura Esays Ehe Ecreated ETron Ea Ecomputer Earchitecture,null,null
,,,
281,9,null,null
,,,
282,Kca,null,null
,,,
283,1,null,null
,,,
284,2,null,null
,,,
285,3,null,null
,,,
286,4,null,null
,,,
287,5,null,null
,,,
288,6,null,null
,,,
289,ESakamura Esays Ehe Ecreated ETron Ea,null,null
,,,
290,7,null,null
,,,
291,Kca,null,null
,,,
292,"Figure 3: Two possible quantum sequences Pdi of an excerpt Wd from a TREC collection. The observation of computer architecture is associated to a superposition projector Kca , |ca ca| while Ew , |ew ew| are classical projectors. For Pd2 we observed only the compound while in Pd1 we also added its subsets.",Y,null
,,,
293,"subset of terms that appear in short queries are candidate compound dependencies to capture. In this paper, we want to make the approach as independent as possible of any linguistic resource. So the second approach (b) is used. This will also allow us to make a fair comparison with the previous approaches using the same strategy (such as the MRF model [19]).",null,null
,,,
294,"The second question regards whether such selected compound dependencies hold in a given document. In other words, one has to decide when to add the selected dependency projector into a document sequence Pd. This can be done for example by assuming that the components terms in the dependency appear as a bigram in a document, as biterm or in a unordered window of L terms. Convergent evidence from different works [1, 12, 14, 18, 31, 36] confirms that proximity is a strong indicator of dependence. Therefore, in this work we choose to detect a dependency if its component terms appear in a fixed-window of length L.",null,null
,,,
295,"The third question regards how to apply the mapping m and can be more easily understood by a practical example. Consider a document Wd ,"" {computer, architecture} and a query Wq "","" {computer, architecture}. Once the dependency ca "","" {computer, architecture} has been detected in the document, i.e. the component terms appear next to each other, one can further decide:""",null,null
,,,
296,"1. to map only the dependency, i.e. Pd ,"" {Kca},""",null,null
,,,
297,"2. to map both the dependency and the component terms, i.e. Pd ,"" {Ecomputer, Earchitecture, Kca}.""",null,null
,,,
298,"These two choices are illustrated in Figure 3. The first choice is a highly non-classical one because it completely steals the occurrence of its component terms. Nevertheless, it becomes a valid choice in our framework. Differently from classical approaches, the fact that we only consider a count for the compound computer architecture does not mean that we assume that the terms computer and architecture do not occur. The dependency event is not disjoint from the single term events, and its occurrence partially entails the occurrence of its component terms. However, this choice is more dangerous because it over-penalizes the component terms: we should know very precisely when such a strong dependency is observed and which coefficients to assign to it.",null,null
,,,
299,"The second choice is implicitly done in current dependency models and is at the basis of the weight-normalization problem. From this point of view, the sequence Pd could be seen as composed by concepts as recently formalized by Bendersky et al. [2, 3]. However, there are crucial differences from",null,null
,,,
300,"that work: (1) we give a clear probabilistic status to such concepts and (2) we do not assume that concepts are atomic units of information, completely unrelated from each other. In classical dependence models, single terms and compound dependencies are scored separately and then the scores are combined together [2, 19, 35]. A critical aspect of such models is that the occurrence of the phrase computer architecture will be counted twice - as single terms and as a compound. That is why the score on compound dependencies must be reweighed before integrating it with the independence score [9, 11, 19]. Contrary to classical models, our model does not suffer from such a problem because the evidences brought by the compound dependency as a whole and by its component terms are integrated in the estimation phase. Even if not reported explicitly in the experiments section, conducted experiments show that including projectors for both the dependency and its subsets is much more effective for the ad-hoc task evaluated here and thus this strategy will be preferred throughout this paper. In addition, an algorithm building the sequence of projectors from the document sequence will be presented in Section 4.3.1.",null,null
,,,
301,3.2 Estimation,null,null
,,,
302,3.2.1 Maximum Likelihood Estimation,null,null
,,,
303,"Given that a document is represented by a set of observed projectors, one has to find ways to learn a quantum language model  to associate with a document. In QT, a number of objective functions have been proposed to estimate an unknown density matrix from a set of projectors: Linear Inversion [23] and Hedged ML [4] are notorious examples. In this work, we use the Maximum Likelihood (ML) formulation proposed in [15], because (1) it can easily be seen as a quantum generalization of a classical likelihood function (2) contrary to linear inversion, ML generates a well-defined density matrix, i.e.   Sn, and (3) proposed estimation methods remain computationally affordable in high-dimensional spaces.",null,null
,,,
304,"Given the observed projectors Pd ,"" {1, . . . , M } for document d, we define as training criterion for the quantum language model  the maximization of the following product proposed in [15] and corresponding in the unigram case to a proper likelihood:""",null,null
,,,
305,M,null,null
,,,
306,"LPd () , tr(i).",null,null
,,,
307,-9,null,null
,,,
308,"i,1",null,null
,,,
309,657,null,null
,,,
310,The estimate  can be obtained by approximately solving the following maximization problem:,null,null
,,,
311,maximize,null,null
,,,
312,log LPd (),null,null
,,,
313,-10,null,null
,,,
314,subject to   Sn.,null,null
,,,
315,"This maximization is difficult and must be approximated by using iterative methods. In [15], the following iterative scheme is proposed, also called the ""RR algorithm"". One introduces the operator:",null,null
,,,
316,R(),null,null
,,,
317,",",null,null
,,,
318,"M i,1",null,null
,,,
319,1 tr(i),null,null
,,,
320,i,null,null
,,,
321,",",null,null
,,,
322,-11,null,null
,,,
323,and updates an initial density matrix (0) by applying repetitive iterations:,null,null
,,,
324,(k+1),null,null
,,,
325,",",null,null
,,,
326,1 Z,null,null
,,,
327,R((k),null,null
,,,
328,)(k),null,null
,,,
329,R((k),null,null
,,,
330,"),",null,null
,,,
331,-12,null,null
,,,
332,"where, Z ,"" tr(R((k))(k)R((k))) is a normalization factor in order to ensure that (k+1) respects the constraint of unitary trace [15]. Despite the RR algorithm being a quantum generalization of the well-behaving Expectation Maximization (EM) algorithm, the likelihood is not guaranteed to increase at each step because the nonlinear iteration may overshoot, similarly to a gradient descent algorithm with a too big step size. Characterizing such situations still remains an open problem [27]. In this work, in order to ensure convergence, if the likelihood is decreased at k + 1, we use the following damped update:""",null,null
,,,
333,"(k+1) ,"" (1 - )(k) + (k+1),""",null,null
,,,
334,-13,null,null
,,,
335,"where   [0, 1) controls the amount of damping and is optimized by linear search in order to ensure the maximum increase of the training objective2. As Sn is convex [23], (k+1) is a proper candidate density matrix. The process stops if the change in the likelihood is below a certain threshold or if a maximum number of iterations is attained.",null,null
,,,
336,"From an IR point of view, the metric divergence problem [22] tells us that the maximization of the likelihood does not mean that the evaluation metric under consideration, such as mean average precision, is also maximized. In the experiments section, we address the two following questions from a perspective closer to IR concerns:",null,null
,,,
337,1. Which initial matrix (0) to choose? 2. When to stop the update process?,null,null
,,,
338,"As the estimation of a quantum document model requires an iterative process, one may believe that the complexity will make the process intractable. In Section 4.5, we provide an analysis of the complexity of the proposed computation, which will show that the process is quite tractable.",null,null
,,,
339,3.2.2 Smoothing Density Matrices,null,null
,,,
340,"The ML estimation presented above suffers from a generalization of the usual zero-probability problem of classical ML, i.e. the estimator assigns zero probability to unseen data [35]. This is also called the zero eigenvalue problem [4]. Bayesian smoothing for density matrices has not yet been proposed. This may be because Bayesian inference",null,null
,,,
341,2Similar damped updates were successfully used in [26] to improve convergence and stability of the loopy belief propagation algorithm.,null,null
,,,
342,"in the quantum setting has just started to be the subject of intensive research [5, 34]. In this work, we propose to smooth density matrices by linear interpolation [35]. If d is a document quantum language model obtained by ML, its smoothed version is obtained by interpolation with the ML collection quantum language model c:",null,null
,,,
343,"d ,"" (1 - d) d + d c,""",null,null
,,,
344,-14,null,null
,,,
345,"where d  [0, 1] controls the amount of smoothing. As",null,null
,,,
346,"the set of density matrices Sn is convex, the resulting d",null,null
,,,
347,"is a proper density matrix. In this work, we assume that",null,null
,,,
348,d,null,null
,,,
349,",",null,null
,,,
350,µ (µ+M,null,null
,,,
351,),null,null
,,,
352,",",null,null
,,,
353,which,null,null
,,,
354,is,null,null
,,,
355,the,null,null
,,,
356,well-known,null,null
,,,
357,form,null,null
,,,
358,of,null,null
,,,
359,the,null,null
,,,
360,parameter,null,null
,,,
361,for Dirichlet smoothing [35].,null,null
,,,
362,3.3 Scoring,null,null
,,,
363,"The flexibility of the Kullback Liebler (KL) divergence approach in keeping distinct query and document representations makes it attractive for a candidate scoring function in our new framework. The direct generalization of classical KL divergence was introduced by Umegaki in [32] and is called quantum relative entropy or Von-Neumann (VN) divergence. Given two quantum language models q and d for the query and a document respectively, our scoring function is the negative query-to-document VN divergence:",null,null
,,,
364,"-V N (q d) , - tr(q(log q - log d))",null,null
,,,
365,"ra,nk",null,null
,,,
366,"tr(q log d),",null,null
,,,
367,-15,null,null
,,,
368,"where log applied to a matrix denotes the matrix logarithm, i.e. the classical logarithm applied to the matrix eigenvalues. Rank equivalence is obtained by noting that tr(q log q) does not depend on the particular document. Denote by q ,"" i qi |qi qi|, d "","" i di |di di| the eigendecompositions of the density matrices q and d respectively. By substituting into the above equation, the scoring function rewrites as:""",null,null
,,,
369,"-V N (q||d) ra,nk",null,null
,,,
370,qi,null,null
,,,
371,log dj qi|dj 2.,null,null
,,,
372,i,null,null
,,,
373,j,null,null
,,,
374,-16,null,null
,,,
375,"Compared to a classical KL divergence, the additional term qi|dj 2 quantifies the difference in the eigenvectors between the two models. Following the representation introduced in Section 2.2, the VN divergence compares two ellipsoids not only by differences in the ""shape"" but also by differences in the ""rotation"".",null,null
,,,
376,"If a VSM-like interpretation is attempted, one can think about {|qi }, {|dj } as semantic concepts for the query and the document respectively, whereas the vectors of eigenval-",null,null
,,,
377,"ues q, d denote the importance of the corresponding semantic concepts in the two models. The VN divergence offers a way of matching query concepts by analyzing how much such concepts are related to documents concepts, i.e. i, j, qi|dj 2. Particularly, j qi|dj 2 ,"" 1. Thus, qi|dj 2 can be interpreted as the quantum probability associated with the pure state |qi qi| for the elementary event |dj dj|, i.e. µqi (|dj dj |) "", tr(|qi qi|dj dj |) ,"" qi|dj 2. Hence, one could rewrite Eq. 16 as:""",null,null
,,,
378,"-V N (q||d) ra,nk",null,null
,,,
379,qi Eµqi log d .,null,null
,,,
380,i,null,null
,,,
381,-17,null,null
,,,
382,"Therefore, the VN divergence scores a document based on the expectation of how important concept |qi is in document d even if it does not appear in it explicitly.",null,null
,,,
383,658,null,null
,,,
384,(a) -V N (q d1 )  -.76,null,null
,,,
385,(b) -V N (q d2 )  -1.06,null,null
,,,
386,o,null,null
,,,
387,Wo,null,null
,,,
388,Po,null,null
,,,
389,q,null,null
,,,
390,"{computer, architecture}",null,null
,,,
391,"{Ec, Ea, Kca}",null,null
,,,
392,"d1 {computer, architecture, and, games} {Ec, Ea, Kca, Eg}",null,null
,,,
393,"d2 {computer, games, and, architecture} {Ec, Eg, Ea}",null,null
,,,
394,"Figure 4: A synthetic example of QLM with a vocabulary of n , 3 terms. The orthogonal rays are the eigenvectors of the ellipsoids. q is not smoothed thus degenerates onto a ray. d1 rotates towards the direction of observed query dependencies and is thus ranked higher.",null,null
,,,
395,3.4 Final Considerations,null,null
,,,
396,"The estimation and scoring process of quantum language models retains classical unigram LMs and KL divergence as special cases. The classical unigram LM is recovered by restricting the maximization in Eq. 10 to diagonal density matrices and including into the sequence of projectors Pd only an orthonormal basis, such as the elements of E. Classical KL divergence is recovered by noting that if q and d are diagonal density matrices, they share the same eigensystem. Hence, |qi , |di and qi ,"" qi, di "","" di, where q, d are the parameters of classical unigram LMs for the query and the document respectively. In this setting, qi|dj 2 "", 0 for i ,"" j and the VN divergence reduces to classical KL, i.e. -V N (q d) "", -KL(q d) ra,nk i qi log di.",null,null
,,,
397,"In Figure 4, we report a synthetic example of the application of the model. We plot the density matrices obtained by the MLE (Section 3.2.1) on the sequence of projectors reported in the table. As usual in ad-hoc tasks, we smooth only the QLMs of the documents. The model corresponding to the query is a projector, i.e. it has two zero eigenvalues, because we did not apply smoothing. If the dependencies are included in the sequence Po, the MLE rotates the corresponding QLM towards the direction spanned by the observed projector (i.e. Kca). This entails that the model d1 is considered more similar to the query than the model d2 which corresponds to a classical language model.",null,null
,,,
398,4. EVALUATION,null,null
,,,
399,4.1 Experimental Setup,null,null
,,,
400,All the experiments reported in this work were conducted using the open source Indri search engine (version 5.3)3. The test collections used are reported in Table 1. We choose the,null,null
,,,
401,3http://www.lemurproject.org,null,null
,,,
402,collections in order to vary (1) the collection size and (2) collection type. This will produce a comprehensive test set in order to verify the properties of our approach. All the,null,null
,,,
403,Name SJMN TREC7-8 WT10g ClueWeb-B,Y,null
,,,
404,Content Newswire Newswire Web Web,null,null
,,,
405,"# Docs 90,257 528,155 1,692,096 50,220,423",null,null
,,,
406,Topic Numbers 51-150 351-450 451-550 51-200,null,null
,,,
407,Table 1: Summary of the TREC collections used to support the experimental evaluation.,null,null
,,,
408,"collections have been stemmed with the Krovetz stemmer. Both documents and queries have been stopped using the standard INQUERY stopword list. For all the methods, the Dirichlet smoothing parameter µ is set to the default Indri value (µ ,"" 2500). The optimization of all the other free parameters for the proposed model and the baselines is done using five-fold cross validation using coordinate ascent [18] with mean average precision (MAP) as the target metric. The performance is measured on the top 1000 ranked documents. In addition to MAP, for newswire collections we report the early precision metric @10 (precision at 10) and for web collections with graded relevance judgements we report the recent ERR@10, which correlates better with click metrics than other editorial metrics [6]. The statistical significance of differences in the performance of tested methods is determined using a two-sided Fisher's randomization test [29] with 25,000 permutations evaluated at  < 0.05.""",null,null
,,,
409,4.2 Methodology,null,null
,,,
410,"Our experimental methodology goes as follows. In a first step, we compare our QLM approach to a unigram Language Modeling baseline (denoted LM) based on Dirichlet smoothing [35], which is a strong bag-of-words baseline. This comparison is done by assigning uniform superposition weights to each dependency , i.e. i ,"" 1/ ||, where || is the cardinality of  (denoted QLM-UNI). This step has two main objectives: (1) to test if quantum probability can bring better performance than a standard bag-of-words model and (2) to test if uniform superposition weights are a reasonable baseline setting.""",null,null
,,,
411,"As a second step, we test the proposed model against the strong non bag-of-words MRF model, which has shown to be highly effective especially for large scale web collections [19, 20]. We test the full dependence version of the model (denoted MRF-FD) which captures dependencies between all the query terms and thus is the most natural choice for a comparison with our model. However, MRF-FD exploits both proximity (#uw) and exact matching (#1). As our model only exploits proximity as an indicator of dependence, we also propose to test the variant MRF-FD-U, which is a MRF using only the proximity feature. This could provide interesting insights on how the models score based upon the same evidence.",null,null
,,,
412,"Finally, we propose a slightly more elaborate version of our model (denoted QLM-IDF) in which the superposition weights are no more assumed to be uniform. Instead, we assign to each i the normalized idf weight of the corresponding term wi. The objective is to test if a more reasonable parametrization of superposition weights can improve the retrieval effectiveness.",null,null
,,,
413,659,null,null
,,,
414,"Figure 5: Plots of MAP (QLM-UNI and LM) and MLE objective against the number of updates of the density matrix for SJMN, TREC7-8 and WT10g (left, center and right).",Y,null
,,,
415,All the results exposed in this paper have been obtained by reranking. We rerank a pool of 20000 documents retrieved using LM in order to make a fair comparison between our method and the baselines.,null,null
,,,
416,4.3 Setting up QLM,null,null
,,,
417,4.3.1 Building the Sequence of Projectors,null,null
,,,
418,"Very similarly to MRF-FD, given a query Q ,"" {q1, . . . , qn}, we assume that the interesting dependencies to consider correspond to the power set P(Q)4. In order to build the set of projectors for the given document we apply Algorithm 1.""",null,null
,,,
419,"Algorithm 1 Builds the sequence Pd given Wd, Q",null,null
,,,
420,"Require: Wd, Q",null,null
,,,
421,1: Pd  ,null,null
,,,
422,2: for   P(Q) do,null,null
,,,
423,"3: for #(, Wd) do",null,null
,,,
424,4:00,null,null
,,,
425,Pd  Pd  m() %Adds the projector to the sequence,null,null
,,,
426,5: end for,null,null
,,,
427,6: end for,null,null
,,,
428,7: return Pd,null,null
,,,
429,"For each dependency  in P(Q), the algorithm scans the document sequence Wd. For each occurrence of , it adds a projector m() to the sequence Pd. The function #(, Wd) returns how many times the dependency  is observed in Wd. Therefore, the algorithm adds as many projectors as the number of detected compound dependencies. Note that by looping on P(Q), we are actually implementing the strategy exposed in Section 3.1.3, i.e. adding both the dependence and all of its subsets. Following Section 3.1.3, we choose to parametrize # as the unordered window operator in Indri (#uwL). Therefore, a given dependency  will be detected if the component terms appear in any order in a fixed-window of length L ,"" l||. This kind of adaptive parametrization of the window length is state-of-the-art for dependence models such as MRF-FD [2, 19]. For all the dependence models, the coordinate ascent for l spans {1, 2, 4, 8, 16, 32}, which is a robust pool covering different window lengths, including the standard value (l "", 4) for MRF-FD.",null,null
,,,
430,"4In order to keep the retrieval complexity reasonable both for MRF and QLM, we limit ourselves to query term subsets with at most three terms.",null,null
,,,
431,4.3.2 MLE Convergence Analysis,null,null
,,,
432,"Before doing any comparisons, we answer the questions",null,null
,,,
433,"related to the construction of a quantum language model,",null,null
,,,
434,i.e. (1) how to initialize (0)? (2) when to stop the update,null,null
,,,
435,process? In order to help the maximum likelihood process to,null,null
,,,
436,"converge faster, we initialize the matrix (0) to the density",null,null
,,,
437,matrix corresponding to the classical maximum likelihood,null,null
,,,
438,language model ML of the document or query under con-,null,null
,,,
439,"sideration. This is a diagonal matrix (0) , diag(ML). We",null,null
,,,
440,"also tested with the uniform density matrix, as suggested",null,null
,,,
441,"in [15], but we found that the MAP was severely harmed.",null,null
,,,
442,"In order to address the second question, we analyze the",null,null
,,,
443,variation of MAP with respect to the maximum number of,null,null
,,,
444,"iterations nit  [1, 50]. The damping factor  is optimized",null,null
,,,
445,"over the set of values  ,"" {0, 0.1, ..., 0.9}. The iterative""",null,null
,,,
446,process stops before nit if the change in the likelihood is below 10-4. In order to check for possible variations due,null,null
,,,
447,"to the collection type, we plot the iteration-MAP curve for",null,null
,,,
448,"two similar collections, i.e. SJMN and TREC7-8, and a web",Y,null
,,,
449,"collection, WT10g. We also plot the training objective in",null,null
,,,
450,Eq. 10 over the set of topics:,null,null
,,,
451,1 |R|,null,null
,,,
452,"dR log LPd (d), where",null,null
,,,
453,R is the multiset of retrieved documents. The trend is shown,null,null
,,,
454,"in Figure 5. Generally, at any number of iterations, the",null,null
,,,
455,MAP stays significantly above the baseline. It seems that,null,null
,,,
456,there is a good correlation between likelihood maximization,null,null
,,,
457,"and MAP, although one can note some overfitting at high",null,null
,,,
458,number of iterations. Capping by 10  nit  20 seems a,null,null
,,,
459,good trade-off between likelihood maximization and MAP.,null,null
,,,
460,"However, to provide a fair comparison with the baselines,",null,null
,,,
461,we choose to include nit as a free parameter to train by,null,null
,,,
462,coordinate ascent.,null,null
,,,
463,4.4 Results,null,null
,,,
464,The results discussed in this section are compactly reported in Table 2.,null,null
,,,
465,4.4.1 Language Modeling Baseline,null,null
,,,
466,"From the comparisons with the LM baseline, one can see that QLM-UNI outperforms LM significantly, with relative improvements in MAP going up to 12.1% in the case of WT10g collection and 19.2% for the ClueWeb-B collection. This seems to be in line with the hypothesis formulated in [19], for which dependence models may yield larger improvements for large collections.",null,null
,,,
467,The weight-normalization problem seems to be addressed automatically: our model does not need for any combina-,null,null
,,,
468,660,null,null
,,,
469,LM MRF-FD-U MRF-FD QLM-UNI,null,null
,,,
470,QLM-IDF,null,null
,,,
471,SJMN,null,null
,,,
472,P@10,null,null
,,,
473,MAP,null,null
,,,
474,0.3064,null,null
,,,
475,0.1995,null,null
,,,
476,0.3138,null,null
,,,
477,0.2071,null,null
,,,
478,0.3074,null,null
,,,
479,0.2061,null,null
,,,
480,0.3181,null,null
,,,
481,(+1.4/+3.5),null,null
,,,
482,0.317,null,null
,,,
483,(+1.0/+3.1),null,null
,,,
484,0.2077,null,null
,,,
485,(+0.3/+0.8),null,null
,,,
486,0.2093,null,null
,,,
487,(+1.1/+1.6),null,null
,,,
488,TREC7-8,Y,null
,,,
489,P@10,null,null
,,,
490,MAP,null,null
,,,
491,0.423,null,null
,,,
492,0.212,null,null
,,,
493,0.435,null,null
,,,
494,0.2228,null,null
,,,
495,0.446,null,null
,,,
496,0.2243,null,null
,,,
497,0.448,null,null
,,,
498,(+3.0/+0.4),null,null
,,,
499,0.445,null,null
,,,
500,(+2.3/-0.2),null,null
,,,
501,0.224,null,null
,,,
502,(+0.5/-0.1),null,null
,,,
503,0.2254,null,null
,,,
504,(+1.2/+0.5),null,null
,,,
505,WT10g,Y,null
,,,
506,ERR@10,null,null
,,,
507,MAP,null,null
,,,
508,0.1068,null,null
,,,
509,0.1975,null,null
,,,
510,0.1136,null,null
,,,
511,0.2097,null,null
,,,
512,0.1147,null,null
,,,
513,0.2146,null,null
,,,
514,0.1162,null,null
,,,
515,0.2215,null,null
,,,
516,(+2.2/+1.3) (+5.6/+3.2),null,null
,,,
517,0.1176,null,null
,,,
518,0.2264,null,null
,,,
519,(+3.5/+2.6) (+7.9/+5.5),null,null
,,,
520,ClueWeb-B,Y,null
,,,
521,ERR@10,null,null
,,,
522,MAP,null,null
,,,
523,0.0718,null,null
,,,
524,0.1003,null,null
,,,
525,0.0828,null,null
,,,
526,0.1103,null,null
,,,
527,0.0881,null,null
,,,
528,0.1137,null,null
,,,
529,0.1015,null,null
,,,
530,0.1196,null,null
,,,
531,(+22.6/+15.2) (+8.4/+5.2),null,null
,,,
532,0.0997,null,null
,,,
533,0.1189,null,null
,,,
534,(+20.4/+13.1) (+7.8/+4.5),null,null
,,,
535,Table 2: Evaluation of the performance for the five methods tested. Best results are highlighted in boldface.,null,null
,,,
536,"Numbers in parentheses indicate relative improvement (%) in MAP over MRF-FD-U/MRF-FD. All the results for dependence models are significant with respect to the baseline LM. The symbols , means statistical",null,null
,,,
537,"significance over MRF-FD-U, MRF-FD respectively.",null,null
,,,
538,"tion weights. Moreover, it is robust across the folds. From an analysis of the optimal values of the parameters obtained across the different folds, we found that optimal window sizes were l  {1, 2}. This can be explained by considering that in the current version of QLM, it is possible to decide if the dependency is detected or not, but the model cannot discriminate its ""importance"". If one decides to increase l, more inaccurate dependencies will be detected and the performance will be deteriorated. However, even with a larger window size, statistical significance over LM is maintained. From these considerations, we suggest l ,"" 2 as a default setting for our model. Finally, the results endorse that our QLM does not need an engineered estimation of superposition weights to perform well.""",null,null
,,,
539,4.4.2 Markov Random Fields Baseline,null,null
,,,
540,"As a second test, we report the results obtained for the MRF-FD and MRF-FD-U baselines. These have proved to be very robust non bag-of-words baselines [2, 19, 20]. Contrary to our model, MRF does not handle dependency information in the estimation phase. One has to specify the coefficients (T , O, U ) for the combination of dependence and independence scores. To limit per-fold overfitting, for the dependence models, we first train combination parameters (f  {0, 0.01, ..., 1}) then l for each fold. For MRF-FD-U, we set O , 0.",null,null
,,,
541,"Results show that for SJMN and TREC7-8, QLM-UNI, MRFFD and MRF-FD-U are essentially equivalent. However, for the two Web collections, our model significantly outperforms both MRF variants. On ClueWeb-B, statistical significance is attained for the two reported measures. As conjectured in [19], noisy web collections could be a more discriminative testbed for dependence models. Optimal l values for MRFFD were very small for SJMN (l  {1, 2}) in contrast to the optimal setting for ClueWeb-B (l  {16, 32}). In [19], the authors suggest that for homogenous newswire collections a small window is enough to capture useful dependencies, while for large, noisy web collections, a larger span must be set. However, the performances obtained by our model seem to suggest that it can greatly benefit from term dependencies, on a variety of collections, even when a small window size is used. This elucidates the fact that even short range information can be extremely useful if integrated in the estimation phase. In order to get a more comprehensive view on such issues, we trained on the entire set of ClueWeb-B topics three versions of MRF-FD-U, each obtained by clamp-",Y,null
,,,
542,"ing a different value of l  {1, 2, 4}. The best performing model obtained a MAP of 10.91. It seems that our model can exploit this short range information in a better way than MRF models.",null,null
,,,
543,4.4.3 Setting Superposition Weights,null,null
,,,
544,"Our last test aimed at verifying if a more reasonable setting of the superposition weights could further improve retrieval performance. For a dependency {w1, . . . , wK }, we",null,null
,,,
545,"set i ,"" idfwi / i idfwi . This has the effect of attributing a larger count to the more """"important"""" term in the dependency. QLM-IDF generally increases MAP. However, this is not the case for ClueWeb-B. From a query-by-query analysis, we noticed that QLM-IDF increases the performance for noisy queries by promoting the most """"important"""" terms in unnecessary subsets. For multiword expressions such as ClueWebB topics continental plates and rock art, weighting by idf may be misleading by assigning more weight to one of the terms. In this cases, a uniform parametrization is far more effective. This demonstrates that there is still room for improvement by a clever tuning of superposition parameters, for example by leveraging feature functions [2, 3].""",null,null
,,,
546,4.5 Complexity Analysis,null,null
,,,
547,"Complexity issues can be tackled by noting that it is not necessary to manipulate n × n matrices. We associate a dimension for each query term and an additional dimension for a ""don't care"" term that will store the probability mass for the other terms in the vocabulary. Therefore, a multinomial over n points is reduced to a multinomial over |Q| + 1 points, where |Q| is the number of unique terms in the query and the additional dimension is simply a relabeling of the other term events. In this way, the QLM to manipulate is k × k, where k , |Q| + 1. The eigendecomposition generally requires O(k3). The iterative process requires at most |P(Q)| ,"" 2|Q| matrix multiplications for the expectation step, where 2|Q| is the maximum number of unique projectors in Pd and 2 matrix multiplications for the maximization step. In the case the likelihood is decreased, || more iterations are done giving a worst-case complexity of O(nit||2k + k3), i.e. if each iteration needs damping. We showed that 10  nit  20 is enough; we use || "","" 10 and k is very small for title queries, which make the process computationally tractable. In practice, we observed that the damping process is very effective and dramatically improves convergence speed. As an example, the mean number of iter-""",null,null
,,,
548,661,null,null
,,,
549,"ations for ClueWeb-B when nit , 15 is 7.02 which is orders of magnitude less than nit|| ,"" 150. Finally, we conjecture that such process could be executed at indexing time, thus eliminating any additional on-line costs.""",Y,null
,,,
550,5. CONCLUSION,null,null
,,,
551,"In this paper, we presented a principled application of quantum probability for IR. We showed how the flexibility of vector spaces joined with the powerful tools of probabilistic calculus can be mixed together for a flexible, yet principled account of term dependencies for IR. In our model, dependencies are neither represented as additional dimensions, nor stochastically as joint probabilities. They assume a new status as superposition events. The relationship of such an event to the traditional term events are encoded by the off-diagonal values in the corresponding projection matrix. Both documents and queries are associated to density matrices estimated through the maximization of a product, which in the classical case reduces to a likelihood. As our model integrates the dependencies in the estimation phase, it has no need for combination parameters. Experiments showed that it performs equivalently to the existing dependence models on newswire test collections and outperforms the latter on web data.",null,null
,,,
552,"To our knowledge, this work provides the first experimental result showing the usefulness of this kind of probabilistic calculus for IR. The marriage between vector spaces and probability can be endlessly improved in the future. One straightforward direction is to relax the assumption that single terms represent orthogonal projectors. This could lead to a new way of integrating latent directions as estimated by purely geometric methods such as Latent Semantic Indexing (LSI) [7] into a probabilistic model. In this work, we did not exploit the full machinery of complex vector spaces. We do not have a practical justification for the use of the complex field for IR tasks. However, we speculate that this could bring improved representational power and thus remains an interesting direction to explore. At last, we believe that our model could be potentially applied to other fields of natural language processing only by means of a principled Bayesian calculus capable of manipulating density matrices. We hope that this work will foster future research in this direction.",null,null
,,,
553,6. ACKNOWLEDGMENTS,null,null
,,,
554,We would like to thank the anonymous reviewers for their valuable comments and suggestions.,null,null
,,,
555,7. REFERENCES,null,null
,,,
556,"[1] J. Bai, Y. Chang, H. Cui, Z. Zheng, G. Sun, and X. Li. Investigation of partial query proximity in web search. In Proc. of WWW, pages 1183­1184, 2008.",null,null
,,,
557,[2] M. Bendersky and W. B. Croft. Modeling higher-order term,null,null
,,,
558,"dependencies in information retrieval using query hypergraphs. In Proc. of SIGIR, pages 941­950, 2012. [3] M. Bendersky, D. Metzler, and W. B. Croft. Parametrized concept weighting in verbose queries. In Proc. of SIGIR, pages 605­614, 2011.",null,null
,,,
559,"[4] R. Blume-Kohout. Hedged maximum likelihood estimation. Phys. Rev. Lett., 105:200504, 2010.",null,null
,,,
560,"[5] R. Blume-Kohout. Optimal, reliable estimation of quantum states. New J. Phys., 12:043034, 2010.",null,null
,,,
561,"[6] O. Chapelle, D. Metzler, Y. Zhang, P. Grinspan. Expected reciprocal rank for graded relevance In Proc. of CIKM, 2009.",null,null
,,,
562,"[7] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer,",null,null
,,,
563,"and R. Harshman. Indexing by latent semantic analysis. JASIST, 41:391­407, 1990.",null,null
,,,
564,"[8] J. L. Fagan. Automatic phrase indexing for document retrieval. In Proc. of SIGIR, pages 91­101, 1987.",null,null
,,,
565,"[9] J. Gao, J. Y. Nie, G. Wu, and G. Cao. Dependence language model for information retrieval. In Proc. of SIGIR, pages 170­177, 2004.",null,null
,,,
566,"[10] A. Gleason. Measures on the closed subspaces of a hilbert space. Journ. Math. Mech., 6:885­893, 1957.",null,null
,,,
567,"[11] K. S. Jones, S. Walker, and S. E. Robertson. A probabilistic model of information retrieval: development and comparative experiments. Inf. Proc. Manag., pages 779­840, 2000.",null,null
,,,
568,"[12] M. Lease. An improved markov random field model for supporting verbose queries. In Proc. of SIGIR, pages 476­483, 2009.",null,null
,,,
569,"[13] C. Lee, G. G. Lee, and M. G. Jang. Dependency structure applied to language modeling for information retrieval. ETRI, 28(3):337­346, 2006.",null,null
,,,
570,"[14] Y. Lv and C. Zhai. Positional language models for information retrieval. In Proc. of SIGIR, pages 299­306, 2009.",null,null
,,,
571,"[15] A. I. Lvovsky. Iterative maximum-likelihood reconstruction in quantum homodyne tomography. Journ. Opt. B6, pages S556­S559, 2004.",null,null
,,,
572,"[16] M. Melucci. Deriving a quantum information retrieval basis. The Computer Journal, 2012.",null,null
,,,
573,"[17] M. Melucci and K. Rijsbergen. Quantum mechanics and information retrieval. Advanced Topics in Information Retrieval, 33:125­155, 2011.",null,null
,,,
574,"[18] D. Metzler and W. Bruce Croft. Linear feature-based models for information retrieval. Inf. Retr., 10(3):257­274, 2007.",null,null
,,,
575,"[19] D. Metzler and W. B. Croft. A markov random field model for term dependencies. In Proc. of SIGIR, pages 472­479, 2005.",null,null
,,,
576,"[20] D. Metzler, T. Strohman, Y. Zhou, and W. B. Croft. Indri at TREC 2005: Terabyte Track. In Proc. of TREC, 2005.",null,null
,,,
577,"[21] M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An analysis of statistical and syntactic phrases. In Proc of RIAO, pages 200­217, 1997.",null,null
,,,
578,"[22] W. Morgan, W. Greiff, and J. Henderson. Direct maximization of average precision by hill-climbing, with a comparison to a maximum entropy approach. In Proc. of HLT-NAACL, pages 93­96, 2004.",null,null
,,,
579,"[23] M. A. Nielsen and I. L. Chuang. Quantum Computation and Quantum Information. Cambridge University Press, 2004.",null,null
,,,
580,"[24] J. H. Park, W. B. Croft, and D. A. Smith. A quasi-synchronous dependence model for information retrieval. In Proc. of CIKM, pages 17­26, 2011.",null,null
,,,
581,"[25] B. Piwowarski, I. Frommholz, M. Lalmas, and K. van Rijsbergen. What can quantum theory bring to information retrieval. In Proc. of CIKM, pages 59­68, 2010.",null,null
,,,
582,"[26] M. Pretti. A message-passing algorithm with damping. J. Stat. Mech., page P11008, 2005.",null,null
,,,
583,"[27] J. R eh´acek, Z. Hradil, E. Knill, A. I. Lvovsky. Diluted maximum-likelihood algorithm for quantum tomography. Phys. Rev. A, 75:042108, 2007.",null,null
,,,
584,"[28] G. Salton, C. S. Yang, and C. T. Yu. A Theory of Term Importance in Automatic Text Analysis. JASIST, 26(1):33­44, 1975.",null,null
,,,
585,"[29] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. of CIKM, pages 623­632, 2007.",null,null
,,,
586,"[30] F. Song and W. B. Croft. A general language model for information retrieval. In Proc. of SIGIR, pages 279­280, 1999.",null,null
,,,
587,"[31] M. Srikanth and R. Srihari. Biterm language models for document retrieval. In Proc. of SIGIR, pages 425­426, 2002.",null,null
,,,
588,"[32] H. Umegaki. Conditional expectation in an operator algebra. Kodai Mathematical Seminar Reports, 14(2):59­85, 1962.",null,null
,,,
589,"[33] K. van Rijsbergen. The Geometry of Information Retrieval. Cambridge University Press, 2004.",null,null
,,,
590,"[34] M. K. Warmuth and D. Kuzmin. Bayesian generalized probability calculus for density matrices. Machine Learning, 78(1-2):63­101, 2009.",null,null
,,,
591,"[35] C. Zhai. Statistical language models for information retrieval a critical review. Found. Trends Inf. Retr., 2(3):137­213, 2008.",null,null
,,,
592,"[36] J. Zhao and Y. Yun. A proximity language model for information retrieval. In Proc. of SIGIR, pages 291­298, 2009.",null,null
,,,
593,"[37] X. Zhao, P. Zhang, D. Song, and Y. Hou. A novel re-ranking approach inspired by quantum measurement. In Proc. of ECIR, pages 721­724, 2011.",null,null
,,,
594,"[38] G. Zuccon and L. Azzopardi. Using the quantum probability ranking principle to rank interdependent documents. In Proc. of ECIR, page 357­369, 2010.",null,null
,,,
595,662,null,null
,,,
596,,null,null

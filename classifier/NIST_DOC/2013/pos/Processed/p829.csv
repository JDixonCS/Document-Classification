,sentence,label,data
0,Boosting Novelty for Biomedical Information Retrieval through Probabilistic Latent Semantic Analysis,null,null
1,"Xiangdong An, Jimmy Xiangji Huang",null,null
2,Information Retrieval and Knowledge Management Research Lab School of Information Technology,null,null
3,"York University, Toronto, ON M3J 1P3, Canada",null,null
4,"{xan, jhuang}@yorku.ca",null,null
5,ABSTRACT,null,null
6,"In information retrieval, we are interested in the information that is not only relevant but also novel. In this paper, we study how to boost novelty for biomedical information retrieval through probabilistic latent semantic analysis. We conduct the study based on TREC Genomics Track data. In TREC Genomics Track, each topic is considered to have an arbitrary number of aspects, and the novelty of a piece of information retrieved, called a passage, is assessed based on the amount of new aspects it contains. In particular, the aspect performance of a ranked list is rewarded by the number of new aspects reached at each rank and penalized by the amount of irrelevant passages that are rated higher than the novel ones. Therefore, to improve aspect performance, we should reach as many aspects as possible and as early as possible. In this paper, we make a preliminary study on how probabilistic latent semantic analysis can help capture different aspects of a ranked list, and improve its performance by re-ranking. Experiments indicate that the proposed approach can greatly improve the aspect-level performance over baseline algorithm Okapi BM25.",null,null
7,Categories and Subject Descriptors,null,null
8,H.3.3 [Information Systems]: Information Search and Retrieval,null,null
9,General Terms,null,null
10,"Algorithms, Experimentation",null,null
11,Keywords,null,null
12,"Genomics IR, passage retrieval, aspect search",null,null
13,1. INTRODUCTION,null,null
14,"Information retrieval (IR) in the context of biomedical databases is characterized by the frequent use of abundant acronyms, homonyms and synonyms. How to deal with the tremendous variants of the same term has been a challenging task in biomedical IR. The Genomics track of Text REtrieval Conference (TREC) provided a common platform to evaluate the methods and techniques proposed by various research groups for biomedical IR. In its last two years",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28 ­ August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
16,"(2006 & 2007), the Genomics track focused on the passage retrieval for question answering, where a passage is a piece of continuous text ranging from a phrase up to a paragraph of a document. One of the performances concerned for passage retrieval was the aspect-based mean average precision (MAP) [8]. To evaluate the performance of a ranked list, in 2006, the judges of the competition first identified all relevant passages for each topic from all submissions, and then, based on the content of such relevant passages, assigned a set of Medical Subject Headings (MeSH) terms to each topic as their representative ""aspects"". In 2007, instead of MeSH terms, the judges picked and assigned terms from the pool of nominated passages deemed relevant to each topic as their ""aspects"". That is, the ""aspects"" of a topic in Genomics Track are represented by a set of terms. A passage for a topic is novel if it contains aspect terms assigned to the topic which has not appeared in the passages ranked higher. The novelty of a ranked list is rewarded by the amount of relevant aspects reached at each rank and penalized by the amount of irrelevant passages ranked higher than novel ones. The aspect-based MAP is an average reflection of novelty retrieval performance on all topics.",null,null
17,"For the aspect-level evaluation, the search should reach as many relevant aspects as possible and rank their containing passages as high as possible. ""Aspects"" are assigned to each topic by the judges only after the submission by all groups, and such aspects are picked only from the nominated passages. At competition, nobody knows how many aspects there exist for each topic in the literature, and what they are. Therefore, ""aspects"" of each topic in this problem are latent, and it is also not an easy problem to figure out the aspects covered by a passage from its ""bag-of-words"" representation. However, it is well known that a topic model can represent a document as a mixture of latent aspects. That is, a topic model can convert a document from its ""bag-of-words"" space to its latent semantic space of a reduced dimensionality. In this paper, we study whether the latent semantic representation would help capture different ""aspects"" of a passage and further improve the performance of a ranked list by re-ranking. There exist a list of topic models such as Latent Semantic Analysis (LSA)[5], Probabilistic Latent Semantic Analysis (PLSA) [9], and Latent Dirichlet Allocation (LDA) [2]. In this preliminary study, we focus on PLSA. In the future, we would study the problem with both LSA and LDA included.",null,null
18,"To the best of our knowledge, this is the first investigation about how well a topic model such as PLSA can help capture hidden aspects in novelty information retrieval. In the investigation, we also examine the hyperparameter settings for PLSA such as initial conditional probabilities and zero estimate smoothing in the context of our problem. Besides standard PLSA model [9], we also examine its variants, e.g. instead of word frequencies, tf-idf weighting is used.",null,null
19,829,null,null
20,2. RELATED WORK,null,null
21,"In information retrieval, ranking based on pure relevance may not be sufficient when the potential relevant documents are huge and highly redundant with each other. In [3, 16, 14, 15, 18], different ways representing and optimizing novelty and diversity of the retrieved documents are studied. The objective is to find the documents that cover as many different aspects (subtopics) as possible while maintaining minimal redundancy. One problem with the novelty (diversity, aspect, subtopic)-based retrieval is how to evaluate the ranking quality. In [14], 3 metrics are introduced: the subtopic recall measures the percentage of subtopics covered as a function of rank, the subtopic precision measures the precision of the retrieved documents as a function of the minimal rank for a certain subtopic recall, and the weighted subtopic precision measures the precision with redundancy penalized based on ranking cost. In [4], a cumulative gain-based metric is proposed to measure the novelty and redundancy, which is also a function of rank.",null,null
22,"Most existing methods [3, 16, 14, 15] improve novelty in IR by penalizing redundancy, but they seem not to work well in Genomics aspect search. In the 2006 TREC Genomics track, University of Wisconsin at Madison failed to promote novelty by penalizing redundancy based on a clustering-based approach [7]. In the 2007 TREC Genomics track, most teams simply submitted their relevant passage retrieval results for aspect evaluation such as National Library of Medicine (NLM) [6] and University of Illinois at Chicago [17]. In [10] and [12], a Bayesian learning approach is proposed to find potential aspects for different topics. In [13], a survival model approach is applied to biomedical search diversification with Wikipedia.",null,null
23,3. HIDDEN ASPECT-BASED RE-RANKING,null,null
24,"It is well known that PLSA can help to reveal semantic relations between entities of interest in a principled way [9]. In this paper, we consider each retrieved passage di (1  i  N ) in D ,"" {d1, ..., dN } as being generated under the influence of a number of hidden aspect factors Z "","" {z1, ..., zK } with words from a vocabulary W "","" {w1, ..., wM }. Therefore, all passages retrieved initially can be described as an N × M matrix T "","" ((c(di, wj))ij, where c(di, wj) is the number of times wj appears in passage di. Each row in D is then a frequency vector that corresponds to a passage. Assume given a hidden aspect factor z, a passage d is independent of the word w. Then by Bayes' rule, the joint probability P (d, w) can be obtained as follows:""",null,null
25,"P (d, w) , P (z)P (d|z)P (w|z).",null,null
26,zZ,null,null
27,"To explain the observed frequencies in matrix T , we need to find P (z), P (d|z), and P (w|z) that maximize the following likelihood function:",null,null
28,"L(D, W ) ,",null,null
29,"c(d, w)logP (d, w).",null,null
30,dD wW,null,null
31,It can be shown that the solution can be achieved by EM algorithm iteratively through the following two alternating steps.,null,null
32,"1. By E-step, we calculate the posterior probabilities of the hidden aspect factors:",null,null
33,"P (z|d, w)",null,null
34,",",null,null
35,P (z)P (d|z)P (w|z) P (z )P (d|z )P (w|z ),null,null
36,.,null,null
37,"2. By M-step, we update parameters to maximize the complete",null,null
38,data likelihood:,null,null
39,"P (w|z) ,",null,null
40,dD,null,null
41,"dD c(d, w)P (z|d, w) w W c(d, w )P (z|d, w",null,null
42,"),",null,null
43,"P (d|z) ,",null,null
44,d,null,null
45,"wW c(d, w)P (z|d, w) D wW c(d , w)P (z|d",null,null
46,",",null,null
47,w),null,null
48,",",null,null
49,"P (z) ,",null,null
50,dD,null,null
51,wW dD,null,null
52,"c(d,",null,null
53,wW,null,null
54,"w)P c(d,",null,null
55,"(z|d, w)",null,null
56,w),null,null
57,.,null,null
58,"After its convergence, we can calculate the probability of hidden aspect factor z given passage d by",null,null
59,"P (z|d) ,",null,null
60,P (d|z)P (z) zZ P (d|z)P (z),null,null
61,"P (d|z)P (z) ,"" P (d, z).""",null,null
62,"Hence, we can summarize the aspect trend of each passage d by a normalized factor vector (P (zi|d))K i,""1. By this way, we transform the passage representation from the """"bag-of-words"""" space to""",null,null
63,a lower latent semantic space. We expect this representation would,null,null
64,capture the aspect trend of each passage in a better way. All pas-,null,null
65,sages can then be clustered based on this vector representation or,null,null
66,simply based on their most probable hidden aspect factor,null,null
67,"zd , argmax P (zi|d).",null,null
68,zi Z,null,null
69,"With latter, we may sort all passages in each group based on the probability P (zd|d) in descending order. By either way, we can always re-rank retrieved passages by repetitively picking one passage from the top of each group until none is left.",null,null
70,4. EXPERIMENTAL RESULTS,null,null
71,"We test our method on a set of runs obtained by the improved Okapi retrieval system [11] for TREC Genomics Track 2007 topics. The set of runs are acquired under different conditions as shown in Tables 1 to 4, where k1 and b are tuning constants of the weighting function BM25. Indexing on database could be paragraph-based (where each piece of indexed information is a paragraph from documents) or word-based (where each piece of indexed information has a limited number of words), and topic expansion is applied once based on unified medical language system (UMLS). To enhance the performance of these runs, feedback analysis is performed by the Okapi retrieval system. In feedback analysis, the system retrieves ten passages that are deemed most relevant for a particular topic, and forms a list of the most recurring words from those passages. Each topic is expanded by these words, and then relevant passages for the extended topic is retrieved. Each feedback term is assigned a weight by Okapi. In our experiments, feedback weight is set to 0.25.",null,null
72,"To get their vector representation, we apply both Porter stemming and a stoplist with general stopwords to passages. After Porter stemming and stoplist application, around 4000 words are left for each topic. All passages nominated for each topic are then represented with these words weighted by tf-idf (Better performance is observed with tf-idf instead of frequency used in the standard PLSA model as described in Section 3). We try to use principal component analysis (PCA) to reduce vector dimensionality. It seems PCA is not very helpful in reducing vector dimensionality without hurting performance in this problem. It might be because of the sparsity of data, no obvious dimensions are much more important than others, and every word has some contribution in representing passages nominated for a topic.",null,null
73,"Topic models like PLSA typically operate in extremely high dimensional spaces. As a consequence, the ""curse of dimensionality"" is lurking around the corner, and thus the hyperparameters (such as initial conditional probabilities and smoothing parameters) settings have the potential to significantly affect the results [1]. In the experiments, we find that we cannot start PLSA model with a",null,null
74,830,null,null
75,"Table 1: Run1: k1,""1.4, b"",""0.55, word-based indexing, no topic expansion, aspect-level MAP 0.1017.""",null,null
76,# of aspects (K) 1,null,null
77,2,null,null
78,3,null,null
79,4,null,null
80,5,null,null
81,6,null,null
82,7,null,null
83,8,null,null
84,9,null,null
85,10,null,null
86,Rerank,null,null
87,0.1017 0.1124 0.1157 0.1430 0.1263 0.1295 0.1243 0.1355 0.1373 0.1279,null,null
88,Improvement 0.00% 10.48% 13.78% 40.67% 24.18% 27.38% 22.24% 33.25% 35.02% 25.75%,null,null
89,"Table 2: Run2: k1,""1.4, b"",""0.55, word-based indexing, with topic expansion, aspect-level MAP 0.0611.""",null,null
90,# of aspects (K) 1,null,null
91,2,null,null
92,3,null,null
93,4,null,null
94,5,null,null
95,6,null,null
96,7,null,null
97,8,null,null
98,9,null,null
99,10,null,null
100,Rerank,null,null
101,0.0611 0.0639 0.0721 0.0779 0.0886 0.0627 0.0726 0.0739 0.0815 0.0852,null,null
102,Improvement 0.00% 4.50% 17.91% 27.45% 44.90% 2.62% 18.72% 20.92% 33.41% 39.32%,null,null
103,"Table 3: Run3: k1,""2.0, b"",""0.4, paragraph-based indexing, no topic expansion, aspect-level MAP 0.0596.""",null,null
104,# of aspects (K) 1,null,null
105,2,null,null
106,3,null,null
107,4,null,null
108,5,null,null
109,6,null,null
110,7,null,null
111,8,null,null
112,9,null,null
113,10,null,null
114,Rerank,null,null
115,0.0596 0.0672 0.0650 0.0875 0.0774 0.0832 0.0726 0.0616 0.0660 0.0723,null,null
116,Improvement 0.00% 12.74% 9.10% 46.97% 30.05% 39.76% 21.83% 3.43% 10.88% 21.46%,null,null
117,"Table 4: Run4: k1,""2.0, b"",""0.4, word-based indexing, no topic expansion, aspect-level MAP 0.08237957.""",null,null
118,# of aspects (K) 1,null,null
119,2,null,null
120,3,null,null
121,4,null,null
122,5,null,null
123,6,null,null
124,7,null,null
125,8,null,null
126,9,null,null
127,10,null,null
128,Rerank,null,null
129,0.0824 0.0886 0.0942 0.0919 0.0846 0.0888 0.0836 0.0930 0.0953 0.0902,null,null
130,Improvement 0.00% 7.54% 14.34% 11.56% 2.57% 7.83% 1.47% 12.85% 15.68% 9.45%,null,null
131,MAP MAP,null,null
132,"uniform distribution for P (z), P (d|z), and P (w|z); otherwise, the convergence will happen immediately in the first iteration due to the sparsity of data. Instead, we start with a normalized random distribution for all these conditional probabilities (the results reported in this paper are the average of a few runs). Due to the large dimensionality, there are a lot of zero probabilities in each passage vector representation. Zero estimates could cause significant problems such as zeroing-out the impact of some other useful parameters in multiplication. Zero estimates could also cause computation problems such as ""division by zero"". In our experiments, we apply Laplace smoothing to avoid zero probability estimates. We add a small value 2-52 to all probabilities before normalization. In the future, more smoothing techniques would be studied.",null,null
133,0.15 0.145,null,null
134,Aspect-level performance comparison - run1,null,null
135,rerank original,null,null
136,0.14,null,null
137,0.135 0.13,null,null
138,0.125,null,null
139,0.12,null,null
140,0.115 0.11,null,null
141,0.105,null,null
142,0.1,null,null
143,1,null,null
144,2,null,null
145,3,null,null
146,4,null,null
147,5,null,null
148,6,null,null
149,7,null,null
150,8,null,null
151,9,null,null
152,10,null,null
153,Number of hidden factors,null,null
154,Figure 1: Performance improvement for run1.,null,null
155,"In the experiment, we examine two ways of clustering passages in latent semantic space: one is centroid-based clustering with different distance functions (squared Euclidean, cosine, and cityblock) and the other is based on their most probable aspect factor. It is found that our problem is not so sensitive to either way of clustering, and for the former, not so much sensitive to the change of distance functions. We believe that this is also caused by the sparsity",null,null
156,"of data. Our experiment results reported here are from centroidbased clustering with cityblock distance function. In the future, we would explore other clustering algorithms that might be more suitable to our problem such as hierarchical clustering and densitybased clustering.",null,null
157,0.09 0.085,null,null
158,Aspect-level performance comparison - run2,null,null
159,rerank original,null,null
160,0.08,null,null
161,0.075,null,null
162,0.07,null,null
163,0.065,null,null
164,0.06,null,null
165,1,null,null
166,2,null,null
167,3,null,null
168,4,null,null
169,5,null,null
170,6,null,null
171,7,null,null
172,8,null,null
173,9,null,null
174,10,null,null
175,Number of hidden factors,null,null
176,Figure 2: Performance improvement for run2.,null,null
177,"In the experiments, we change the number of hidden aspects K from 1 to 10 continuously for all runs. When the number of hidden aspects is set to 1, there is no re-ranking and hence the performances are the same as the original runs. It turns out for all other 9 different hidden aspect numbers, all runs get positive performance improvements by re-ranking as shown in Tables 1 to 4. To illustrate the re-ranking performance graphically, we plot the data in Figures 1 to 4, respectively, where y-axis stands for the aspect-level performance MAP. It can be observed that on all 9 different number of hidden factors, the re-ranked results are all better than the original ones. Over all runs, the maximum improvement is 46.97% when K ,"" 5 for run2, the minimum improvement is 1.47% when K "","" 7 for run4, and the average improvement is 20.06%. This is illustrated in Figure 5.""",null,null
178,It should be noted that the hidden aspect factors in PLSA mod-,null,null
179,831,null,null
180,"els are not necessarily the same as the aspects of Genomics Track. In PLSA models, the number of hidden aspect factors is a tuning variable, while the aspects of Genomics Track topics are constants once the corpus and topics are determined. The hidden aspect factors in PLSA models are statistically identified from data while the aspects of Genomics Track topics are assigned by the judges but not results of statistical analyses. Since PLSA models are good in semantic analysis and synonym and concept recognition [9], we use the hidden aspect factors identified by PLSA models to classify passages and then use this classification information to re-rank ranked lists in the hope that the hidden aspect factors do have some correlation with topic aspects in some way. Our experiment results highly support the hope.",null,null
181,0.09 0.085,null,null
182,Aspect-level performance comparison - run3,null,null
183,rerank original,null,null
184,0.08,null,null
185,0.075,null,null
186,MAP,null,null
187,0.07,null,null
188,0.065,null,null
189,0.06,null,null
190,0.055,null,null
191,1,null,null
192,2,null,null
193,3,null,null
194,4,null,null
195,5,null,null
196,6,null,null
197,7,null,null
198,8,null,null
199,9,null,null
200,10,null,null
201,Number of hidden factors,null,null
202,Figure 3: Performance improvement for run3.,null,null
203,0.096 0.094,null,null
204,Aspect-level performance comparison - run4,null,null
205,rerank original,null,null
206,0.092,null,null
207,0.09,null,null
208,MAP,null,null
209,0.088,null,null
210,0.086,null,null
211,0.084,null,null
212,0.082,null,null
213,1,null,null
214,2,null,null
215,3,null,null
216,4,null,null
217,5,null,null
218,6,null,null
219,7,null,null
220,8,null,null
221,9,null,null
222,10,null,null
223,Number of hidden factors,null,null
224,Figure 4: Performance improvement for run4.,null,null
225,5. CONCLUSIONS AND FUTURE WORK,null,null
226,"In this paper, we conducted a preliminary study on using PLSA models to capture hidden aspects of retrieved passages. The hidden aspects caught are used to improve the performance of a ranked list by re-ranking. It turned out all runs on all 9 continuous hidden aspect numbers got positive improvements. This indicates PLSA models are very promising in finding diverse aspects in retrieved passages. By contrast, it was indicated [7] a clustering-based method always failed to improve the aspect performance over baseline algorithms.",null,null
227,"In the future, more experiments will be conducted to further investigate the proposed method. We will extend the method to more runs, and will study whether there exist a range of hidden aspect numbers that can always be safely used in re-ranking to improve",null,null
228,"performance. In addition, we will investigate how to set different hidden aspect numbers for different topics. We will also examine other topic models such as LDA and LSA on this matter.",null,null
229,0.5 0.45,null,null
230,0.4 0.35,null,null
231,0.3 0.25,null,null
232,0.2 0.15,null,null
233,0.1 0.05,null,null
234,1.47% 0,null,null
235,Min,null,null
236,46.97%,null,null
237,20.06%,null,null
238,Max,null,null
239,Average,null,null
240,Figure 5: Performance improvement summary.,null,null
241,6. ACKNOWLEDGMENT,null,null
242,This research is supported by the research grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada. We thank anonymous reviewers for their thorough review comments on this paper.,null,null
243,7. REFERENCES,null,null
244,"[1] A. Asuncion and et al. On smoothing and inference for topic models. In UAI'09, pages 27­34, 2009.",null,null
245,"[2] D. M. Blei and et al. Latent dirichlet allocation. JMLR, 3(4-5):993­1022, 2003.",null,null
246,"[3] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR'98, pages 335­336.",null,null
247,"[4] C. Clarke and et al. Novelty and diversity in information retrieval evaluation. In SIGIR'08, pages 659­666.",null,null
248,"[5] S. Deerwester and et al. Indexing by latent semantic analysis. JASIST, 41, 1990.",null,null
249,"[6] D. Demner-Fushman and et al. Combining resources to find answers to biomedical questions. In TREC-2007, pages 205­214.",null,null
250,"[7] A. B. Goldberg and et al. Ranking biomedical passages for relevance and diversity: University of Wisconsin, Madison at TREC genomics 2006. In TREC-2006, pages 129­136.",null,null
251,"[8] W. Hersh, A. Cohen, and P. Roberts. TREC 2007 genomics track overview. In TREC-2007, pages 98­115.",null,null
252,"[9] T. Hofmann. Probabilistic latent semantic analysis. In UAI'99, pages 289­296.",null,null
253,"[10] Q. Hu and X. Huang. A reranking model for genomics aspect search. In SIGIR'08, pages 783­784.",null,null
254,"[11] X. Huang and et al. A platform for okapi-based contextual information retrieval. In SIGIR'06, pages 728­728, 2006.",null,null
255,"[12] X. Huang and Q. Hu. A bayesian learning approach to promoting diversity in ranking for biomedical informaiton retrieval. In SIGIR'09, pages 307­314.",null,null
256,"[13] X. Yin and et al. Survival modeling approach to biomedical search result diversification using wikipedia. TKDE, 25(6):1201­1212, 2013.",null,null
257,"[14] C. Zhai, W. W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In SIGIR'03, pages 10­17.",null,null
258,"[15] B. Zhang and et al. Improving web search results using affinity graph. In SIGIR'05, pages 504­511.",null,null
259,"[16] Y. Zhang, J. Callan, and T. Minka. Novelty and redundancy detection in adaptive filtering. In SIGIR'02, pages 81­88.",null,null
260,"[17] W. Zhou and C. Yu. TREC genomics track at UIC. In TREC-2007, pages 221­226.",null,null
261,"[18] X. Zhu and et al. Improving diversity in ranking using absorbing random walks. In NAACL-HLT 2007, pages 97­104.",null,null
262,832,null,null
263,,null,null

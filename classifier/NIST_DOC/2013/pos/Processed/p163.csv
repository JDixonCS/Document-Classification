,sentence,label,data
0,An Information-Theoretic Account of Static Index Pruning,null,null
1,Ruey-Cheng Chen,null,null
2,"National Taiwan University 1 Roosevelt Rd. Sec. 4 Taipei 106, Taiwan",null,null
3,rueycheng@turing.csie.ntu.edu.tw,null,null
4,Chia-Jung Lee,null,null
5,University of Massachusetts 140 Governors Drive,null,null
6,"Amherst, MA 01003-9264",null,null
7,cjlee@cs.umass.edu,null,null
8,ABSTRACT,null,null
9,"In this paper, we recast static index pruning as a model induction problem under the framework of Kullback's principle of minimum cross-entropy. We show that static index pruning has an approximate analytical solution in the form of convex integer program. Further analysis on computation feasibility suggests that one of its surrogate model can be solved efficiently. This result has led to the rediscovery of uniform pruning, a simple yet powerful pruning method proposed in 2001 and later easily ignored by many of us. To empirically verify this result, we conducted experiments under a new design in which prune ratio is strictly controlled. Our result on standard ad-hoc retrieval benchmarks has confirmed that uniform pruning is robust to high prune ratio and its performance is currently state of the art.",null,null
10,Categories and Subject Descriptors,null,null
11,H.1.1 [Systems and Information Theory]: Information theory; H.3.1 [Content Analysis and Indexing]: Indexing methods; H.3.4 [Systems and Software]: Performance evaluation (efficiency and effectiveness),null,null
12,Keywords,null,null
13,Static index pruning; principle of minimum cross-entropy; model induction; uniform pruning,null,null
14,1. INTRODUCTION,null,null
15,"Kullback discussed one famous problem in his seminal work [14] about inducing a probability measure based on some previous measurement. When one has some initial hypothesis about a system and seeks to update this measurement incrementally, she needs to choose a new hypothesis from a set of feasible measures that best approximates her current belief. Here, the difficulty lies in defining the notion of closeness in the probability space. While at the time this was an important issue in everyday probabilistic modeling, a genuine solution had yet to come.",null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
17,"To answer the question he had raised, Kullback introduced a method called ""minimum discrimination information,"" or in the recent literature known as the principle of minimum cross-entropy. This approach has later become one of the most influential inductive principles in statistics, and also has benefited numerous fields, including some subareas in information retrieval [15,20]. Kullback's solution was simple and elegant: One shall choose a measure that most closely resembles the previous measurement in terms of KullbackLeibler divergence. Specifically, this is equivalent to solving the following optimization problem, given some prior measure p and a set of feasible measures F:",null,null
18,minimize subject to,null,null
19,D(q||p) q  F.,null,null
20,(1),null,null
21,"In this paper, we apply this induction framework to a classic problem in information retrieval, called static index pruning. Static index pruning is a task that reduces the index size for improving disk usage and query throughput [1]. Size reduction is done by removing index entries. Generally, the aim in static index pruning is to find a subset of index entries that best approximates the full index in terms of retrieval performance. This aspect, as we will show later, is closely related to model induction.",null,null
22,"One key assumption in this paper is that an inverted index is a nonparametric, conditional distribution of document D given term T , i.e., p(D|T ). This follows directly from Chen et al.'s definition [10], which allows us to measure the resemblance between two versions of inverted indexes the way we do probability distributions. Here, the following definitions put static index pruning into the framework of Equation (1):",null,null
23,· The prior distribution p is defined as the full (unpruned) inverted index.,null,null
24,"· The set of feasible hypotheses F contains all the possible pruned indexes of p that have reached some given prune ratio . In other words, each element q  F is a pruned version of the original inverted index p.",null,null
25,"This conception marks the very beginning of our quest for developing an efficient solution of static index pruning. Through analysis, we first show that static index pruning is essentially a combinatorial optimization problem. Nevertheless, in Section 3, we manage to obtain a weaker analytical solution that is practically operable by trading off some mathematical rigor. We found that, under appropriate assumptions, static index pruning reduces to a convex integer program. But this is not a good solution in general, since the number of variables in the convex program is linear to the",null,null
26,163,null,null
27,"number of postings in the inverted index, which may easily exceed a few millions on any medium-sized text collection. That means this solution does not scale at all, even with the latest super-efficient convex solver.",null,null
28,"We further attacked this problem using an alternative approach, called surrogate modeling. We created a surrogate problem that is easier to solve. As we will show in later sections, this analytical solution has pointed us to a general version of a simple pruning method called uniform pruning. Sharp-eyed readers might notice that uniform pruning is by no means a new invention. Uniform pruning was originally introduced to static index pruning in Carmel et al.'s paper as a baseline approach [9]. In a preliminary experiment, Carmel et al. compared this method with their termbased pruning method. Using TF-IDF as the score function, they found that, even though term-based method performed slightly better, in general the performance for both approaches was roughly comparable. While this was indeed a very interesting finding, the exploration was discontinued as they went ahead to study other important issues.",null,null
29,"To the best of our knowledge, since then uniform pruning has not been studied in any follow-up work. It is easy to see why this has been the case. The lack of control on one experiment variable, prune ratio, has made the performance result difficult to interpret. When we make comparisons between methods, this variable needs to be strictly controlled so that the comparisons make sense. Nevertheless, very few in the previous work adopted this design. As a result, there was no obvious way to conduct any form of significance testing to static index pruning. Without serious scrutiny--by which we mean significance assessment--it is only reasonable to dismiss uniform pruning, for that it seemed like an ad-hoc and maybe inferior approach.",null,null
30,"In our study, the rediscovery of uniform pruning has gained us a second chance to rethink this issue. Our answer was a redesigned empirical study, in which prune ratio for each experimental method is strictly controlled to minimize the experimental error, and the performance is analyzed using multi-way repeated-measure analysis of variance. As we will shortly cover, the experiment result suggests that uniform pruning with Dirichlet smoothing significantly outperformed the other term-based methods under diverse settings.",null,null
31,"The rest of the paper is structured as follows. Section 2 covers an overview to static index pruning and the relevant research. In Section 3, we motivate static index pruning in the minimum cross-entropy framework and show that the analytical solution leads to the uniform pruning method. An empirical study is given in Section 4. We put the theoretical and empirical evidence together and discuss the implication in Section 5. Section 6 delivers the concluding remarks.",null,null
32,2. RELATED WORK,null,null
33,"In the coming subsections, we briefly review the literature and discuss the recent development of static index pruning. Following an overview, some notable pruning methods will be treated in slightly more details. Note that this is only aimed at providing enough background knowledge for the reader. A complete coverage is not attempted here.",null,null
34,2.1 Overview,null,null
35,The idea of static index pruning first appeared in the groundbreaking work of Carmel et al. [9] and has since garnered much attention for its implication to Web-scale re-,null,null
36,"trieval [8, 11]. Static index pruning is all about reducing index size--by removing index entries from the inverted index. This technique was proposed to mitigate the efficiency issue caused by operating a large index, for that a smaller index loads faster, occupies less disk space, and has better query throughput. But since only partial term-document mapping is preserved, a loss in retrieval performance is inevitable.",null,null
37,"Much effort has been driven towards developing importance measures of individual index entries, so that one can easily prioritize index entries on their way out of the index. Many such measures have been proposed and tested in various retrieval settings. One simple example is impact, the contribution of a term-document pair to the final retrieval score [8, 9]. Other approaches in this line include probability ranking principle (PRP) [7], two-sample two proportion (2P2N) [19], and information preservation (IP) [10]. Some measures assess only term importance [6], so the corresponding pruning algorithms can only choose between keeping the entire term posting list or not at all. Some others assess only documents importance [21].",null,null
38,2.2 Methodologies,null,null
39,"Term-based pruning (or term-centric pruning) is proposed by Carmel et al. [9]. It was so named because it attempts to reduce the posting list for each term in the index. The basic idea is to compute a cutting threshold for each term, and throw away those entries with smaller impact values. Since the cutting threshold depends on some order statistics (i.e., the k-th largest impact value) about the posting list, term-based pruning is less efficient than the other methods.",null,null
40,"In contrast to the aforementioned term-centric approach, document-centric pruning seeks to reduce the posting list for each document. Bu¨ttcher and Clarke [8] considered the contribution for term t to the Kullback-Leibler divergence D(d||C) between document d and the collection model C. This quantity is used to measure the importance of a posting. Analogously, for each document, a cutting threshold has to be determined based on some order statistics.",null,null
41,"There are also other pruning strategies that focus on removing an entire term posting list (whole-term) or an entire document (whole-document) all at once. Blanco and Barreiro [6] presented four term-importance measures, including inverse document frequency (idf), residual inverse document frequency (ridf), and two others based on term discriminative value (TDM). They adopted a whole-term pruning strategy. Analogously, Zheng and Cox [21] proposed an entropy-based measure in a whole-document pruning setting. Both parties have reported comparable performance to term-based pruning on some standard benchmark.",null,null
42,"Blanco and Barreiro [7] developed a decision criterion based on the probability ranking principle [18]. The idea is to take every term in the index as a single-word query and calculate the odd-ratio of relevance p(r|t, d)/p(r|t, d). This quantity is used in prioritizing all the term-document pair. Since there is only one cutting threshold determined globally, the implementation is relatively easy and efficient.",null,null
43,"Thota and Carterette [19] used a statistical procedure, called two-sample two-proportion (2P2N), to determine if the occurrence of term t within document d is significantly different from its occurrence within the whole collection. Chen et al. [10] developed a method called information preservation. They suggest using the innermost summand of the conditional entropy H(D|T ) to measure predictive power",null,null
44,164,null,null
45,contributed by individual term-document pairs to the index model. This quantity is claimed easier to compute than the probability ranking principle.,null,null
46,"Altingovde et al. [2] proposed an interesting query-view technique that works orthogonally with the aforementioned measure-based approaches. The general idea is to count the number of time a document falls within the top-k window of any given query collected from the query log. The count collected from a query is then evenly distributed to individual query terms. Thus the larger this number, the greater importance the posting is. The query view algorithm would later use this information to prune the entries.",null,null
47,"Our work in this paper departs from the previous effort in three major ways. First, our approach is model-based, meaning that we infer a pruned model as a whole rather than partially. This is a novel approach in contrast to all the previous methods. Second, other information-theoretic approaches, such as Zheng and Cox [21] and Chen et al. [10], focused on minimizing the loss of information, while ours focused on minimizing the divergence from the full index. These are entirely different concepts in information theory. Three, our result on the uniform pruning method is more general than Carmel et al.'s description because we considered the query model p(t). Our take of uniform pruning is a weighted version, which may be useful when such a query model (e.g., session logs) is available.",null,null
48,3. MINIMUM CROSS-ENTROPY AND STATIC INDEX PRUNING,null,null
49,3.1 Problem Definition,null,null
50,"Let us first develop some notation for describing an inverted index. Let T denote the set of terms and D denote the set of documents. We define an index entry (posting) as a 3-tuple of the form (t, d, n), where t  T , d  D, and n  N+ (i.e., n is a positive integer.) This means that ""term t appears n times in document d."" We further consider an inverted index as a probabilistic model p(D|T ; ) that takes a set of index entries  as parameters. This model is therefore nonparametric because the number of its parameters is not fixed. For brevity, in this paper we sometimes abuse the notation and use one symbol, e.g., , to represent both a distribution and its parameters.",null,null
51,"In static index pruning, one seeks to induce a pruned model  from a full model 0 such that the following two constraints are satisfied: (i)  is a subset of the full model 0, and (ii) the size of  is 1 -  times the size of 0. Here, 0 <  < 1 denotes the prune ratio. Note that these constraints only specify what we need as the output from static index pruning, not how pruning shall be done. As there are exponentially many ways to prune an index down to a given ratio, it is natural to ask how does one engineer this decision to avoid excessive performance loss.",null,null
52,"Now, to illustrate this point, let us assume the existence of a function g() that measures the retrieval performance of model . With this hypothetical construct, we formally define static index pruning as follows.",null,null
53,maximize g(),null,null
54,subject to   0,null,null
55,(2),null,null
56,||/|0| reaches 1 - .,null,null
57,"It is not difficult to envision static index pruning being formulated this way, as a constrained optimization problem.",null,null
58,"For now, we shall focus on estimation of this hypothetical function. The conventional approach, as discussed in Section 2.2, is to devise an importance measure to take the role of g(·), which is expected to capture certain properties of an index relevant to retrieval performance. Yet one caveat is that sometimes we risk being arbitrary: The importance measure may only be empirically tested and does not necessarily come with any theoretical guarantee.",null,null
59,"One simple idea that we had failed to see casted away all these doubts. We noticed the similarity between this formulation and Kullback's famous induction framework. As we replace g(·) in Equation (2) with the negative KullbackLeibler divergence (KL divergence), the static index pruning problem reduces to a model induction problem, written in a minimization form:",null,null
60,minimize D(||0),null,null
61,subject to   0,null,null
62,(3),null,null
63,||/|0| reaches 1 - .,null,null
64,"In the following subsections, we shall develop a procedure to practically solve this optimization problem. For brevity, we write p(·) and p0(·), respectively, to denote the models parametrized by inverted indexes  and 0. The probability measures that we consider here are conditional distributions of D given T . To make this explicit, we define:",null,null
65,D(||0)  D(p(D|T )||p0(D|T )).,null,null
66,(4),null,null
67,3.2 Assumptions,null,null
68,"Before diving into the full analysis, we need to make explicit two important assumptions.",null,null
69,"Assumption 1 (Query and Index Models). We can separate a joint distribution of D and T into a product of two models: (1) a distribution of T , called the query model, and (2) a conditional distribution of D given T , called the index model. We assume there is only one query model q(t) and it is independent of the index model in use. In other words, we have:",null,null
70,"p(d, t) ,"" p(d|t)q(t), p0(d, t) "", p0(d|t)q(t).",null,null
71,"Sometimes, we simply write p(t) or p0(t) to denote the query model when the meaning is clear in the context.",null,null
72,Assumption 1 simply states that the query model q(t) (or p(t)) has to be estimated from somewhere else. It makes little sense to infer a query model from the index.,null,null
73,"Assumption 2 (Normalization Factor). Let p(t|d) and p0(t|d) be the conditional distributions of T given D for the induced and the original models, respectively. Let It,d be a binary variable that indicates whether an index entry (t, d, n) (for some n  N+) in the original model is retained in the induced model. We have p(t|d)  It,dp0(t|d)/Zd, where Zd is the normalization factor for document d.",null,null
74,"In Assumption 2, we introduce a normalization factor Zd for each document d. As we shall address later, setting an appropriate value for Zd is the key step in the subsequent analysis. To correctly normalize p(t|d), we need to set:",null,null
75,"Zd ,"" It,dp0(t|d).""",null,null
76,t,null,null
77,165,null,null
78,"But this would make the resulting formula intractable, since the value of It,d depends on other variables in the same document, i.e., I·,d. To deal with this issue, we suggest setting Zd ,"" k for all d  D, where k > 0 is some constant. Using this normalization trick results in weak inference and inevitably sacrifices mathematical rigors. We want to emphasize that this is a necessary compromise, without which the following analysis would not have been possible.""",null,null
79,3.3 Analysis,null,null
80,"Now, we shall go ahead and analyze the objective function. First of all, let us write out the objective in full:",null,null
81,D(p(D|T )||p0(D|T )) ,null,null
82,"t,d",null,null
83,"p(d,",null,null
84,t),null,null
85,log,null,null
86,p(d|t) p0(d|t),null,null
87,.,null,null
88,(5),null,null
89,"We use Assumption 1 to dissect the joint distribution p(d, t) into the product of the query model p(t) and the index model p(d|t). Applying Bayes Theorem to p(d|t) and p0(d|t) and assuming uniform p(d) and p0(d), we have the objective organized as follows:",null,null
90,p(t),null,null
91,t,null,null
92,d,null,null
93,p(t|d) d p(t|d),null,null
94,log,null,null
95,p(t|d) p0(t|d),null,null
96,d d,null,null
97,p0(t|d) p(t|d),null,null
98,.,null,null
99,(6),null,null
100,"Observe that, since in this optimization framework we look for a subset of 0, we are essentially dealing with a combinatorial problem (""assignment problem""). Each index entry (t, d, n)  0 either stays within the induced model  or gets removed. This combinatorial nature is best characterized via the indicator variables I·,· in Assumption 2.",null,null
101,"Let us now replace all the occurrences of p(t|d). Note that, under the setting Zd ,"" k (suggested), all the normalization factors cancel out. We have:""",null,null
102,p(t),null,null
103,t,null,null
104,d,null,null
105,"It,dp0(t|d) d It,d p0(t|d)",null,null
106,log,null,null
107,"It,d",null,null
108,"d p0(t|d) . (7) d It,d p0(t|d)",null,null
109,"As we separate the support of the inner summation over d into two subsets according to whether It,d is switched on, i.e., one over {d|It,d ,"" 1} and the other over {d|It,d "","" 0}, the latter sub-summation disappears since 0 log 0 "", 0. The resulting equation becomes:",null,null
110,p(t),null,null
111,t,null,null
112,"d:It,d ,1",null,null
113,d,null,null
114,"p0(t|d) It,d p0(t|d)",null,null
115,log,null,null
116,d,null,null
117,"d p0(t|d) It,d p0(t|d)",null,null
118,.,null,null
119,(8),null,null
120,"Notice that the innermost logarithm does not depend on d anymore. We can therefore move that entire term out of the inner summation. From there, we have the inner summation over d canceled out. The equation is now written as:",null,null
121,p(t) log,null,null
122,t,null,null
123,d,null,null
124,"d p0(t|d) It,d p0(t|d",null,null
125,),null,null
126,.,null,null
127,(9),null,null
128,"We can get rid of the numerator, i.e., d p0(t|d), in the logarithm when minimizing this equation, because the numerator does not depend any combinatorial choice we make. Once again, we rewrite it as a maximization problem by taking the negation. The final form of static index pruning is expressed as the following:",null,null
129,maximize,null,null
130,"t p0(t) log d It,dp0(t|d)",null,null
131,"subject to It,d is binary, for all (t, d, ·)  0, (10)",null,null
132,"t,d It,d , (1 - )|0|.",null,null
133,Input: a global threshold  begin,null,null
134,"for t  T do for d  postings(t) do compute A(t, d) ,"" p(t)p(t|d) if A(t, d) <  then remove d from postings(t) end end""",null,null
135,end end,null,null
136,Algorithm 1: The weighted uniform pruning algorithm.,null,null
137,"Equation (10) is in general ill-posed even though it can be solved with a convex integer program solver. This is because the number of index entries can easily exceed a few millions in any production retrieval system. Solving this exactly is only possible for very small test collections. To tackle this issue, we resort to a technique called surrogate modeling (or optimization transfer), which approximates the original objective using a majorization/minorization function that is analytically or numerically efficient to compute. See Lange et al. [16] for a comprehensive treatment.",null,null
138,"To the best of our knowledge, there are two major approaches for inducing such surrogate models: taking the first-order Taylor approximation, or using the Jensen's inequality. In this paper, we stick with the second approach1. Recall that Jensen's inequality states that the following properties hold for any convex (or concave) function f :",null,null
139,Ef (X)  f (EX) (f is convex),null,null
140,Ef (X)  f (EX) (f is concave).,null,null
141,"Let f be the logarithmic function. The original objective in our problem (Equation 10) now corresponds to the lefthand side Ef (X). Since the logarithmic function is concave, we have the surrogate model f (EX) an upper bound of the original objective:",null,null
142,"maximize log It,dp0(t)p0(t|d),",null,null
143,(11),null,null
144,"t,d",null,null
145,or equivalently:,null,null
146,maximize,null,null
147,"It,dp0(t)p0(t|d).",null,null
148,(12),null,null
149,"t,d",null,null
150,"This surrogate model has a simple analytical solution: Sort the index entries according to weighted query likelihood, i.e., p(t)p(t|d), and keep only the top (1 - )N entries. It can be shown that a simple maneuver such as Algorithm 1, called weighted uniform pruning, guarantees to maximize the objective. This corresponds to a weighted version of Carmel et al.'s uniform pruning method. This algorithm would fall back to the unweighted form when we supply a uniform p(t) and a plug-in estimate of the query likelihood. Note that the plug-in approach is valid only when the score function is proportional to the true likelihood.",null,null
151,"For simplicity, we take a very loose definition of query likelihood in this paper so as to cover the well-known BM25 function. As we shall present shortly, the empirical result",null,null
152,"1In our case, the first-order Taylor expansion leads to an even more sophisticated objective.",null,null
153,166,null,null
154,shows that the performance of BM25 is no worse than that of a rigorously defined language model (with Jelinek-Mercer smoothing). What is left unsettled is how to estimate  given a target prune ratio . This issue is treated in Section 4.2.,null,null
155,4. EXPERIMENT,null,null
156,"Thus far, we have established the theoretical ground for uniform pruning. Our next quest is to find empirical evidence that supports this result. In the coming subsections, we shall briefly describe the experiment settings and present the experimental result in greater detail.",null,null
157,4.1 Setup,null,null
158,"We used three test collections in this experiment: TREC disks 4 & 5, WT2G, and WT10G. The first two collections are tested against topics 401-450 and the latter against topics 451-550. For each topic, we tested both short (title) and long (title + description) queries. Details about the benchmark are summarized in Table 1. All three collections were indexed using the Indri toolkit2. To preprocess the documents, we applied the porter stemmer and used the standard 411 InQuery stoplist. No additional text processing is done to the test collections.",null,null
159,"According to how index traversal is preferred, a pruning method can be either term-centric or document-centric. Since different traversal strategies rely on different index creation procedures, it is difficult to have both sets implemented in one place. For simplicity, in this experiment we focused only on term-centric methods. Specifically, we tested the following methods:",null,null
160,"1. Uniform pruning (UP) [9]: This method is the subject of this experiment. In this experiment, we tested three variations of uniform pruning, each using a different score function. These functions are BM25 (UP-bm25), language model using Dirichlet smoothing (UP-dir), and language model using Jelinek-Mercer smoothing (UP-jm). For BM25, we used the standard setting provided by Indri. For language models, we set µ , 2500 in Dirichlet smoothing and  , 0.6 for the JelinekMercer smoother.",null,null
161,"2. Top-k term-centric pruning (TCP) [9]: We set k ,"" 10 as suggested to maximize the top-10 precision and used BM25 as the score function. Note that other score functions such as language models may also apply to this pruning method. Here, we simply comply with the previous work.""",null,null
162,3. Probability ranking principle (PRP) [7]:,null,null
163,"p(r|t, d) p(r|t, d)",null,null
164,p(t|D)p(r|D) p(t|r)(1 - p(r|D)),null,null
165,.,null,null
166,"As suggested, we use the following equations to estimate these probabilities:",null,null
167,"p(t|D) ,"" (1 - )pML(t|D) + p(t|C), (13)""",null,null
168,p(r|D),null,null
169,",",null,null
170,1 2,null,null
171,+,null,null
172,1 10,null,null
173,tanh,null,null
174,dl,null,null
175,- Xd Sd,null,null
176,",",null,null
177,(14),null,null
178,"p(t|r) , p(t|C).",null,null
179,(15),null,null
180,2http://www.lemurproject.org/indri.php,null,null
181,Collection Disks 4 & 5 WT2G WT10G,null,null
182,# Documents 528k 247k 1692k,null,null
183,Query Topics 401-450 401-450 451-550,null,null
184,Table 1: Test collections and the corresponding query topics.,null,null
185,"Note that dl is the document length. Xd and Sd respectively are the sample mean and sample standard deviation of document length. For query likelihood, we set  , 0.6.",null,null
186,"4. Information preservation, with uniform document prior (IP-u) [10]:",null,null
187,-,null,null
188,p(t|d)p(d) d p(t|d)p(d),null,null
189,log,null,null
190,p(t|d)p(d) d p(t|d)p(d,null,null
191,),null,null
192,.,null,null
193,"In this formula, the query likelihood p(t|d) is estimated using Jelinek-Mercer smoothing. Here, We set  , 0.6 and assumed uniform document prior p(d).",null,null
194,"We are aware that document-length update may improve the TCP and PRP retrieval performance [5,7]. Nevertheless, in this study we did not implement this feature. This shall be addressed in the future work.",null,null
195,4.2 Prune Ratio,null,null
196,"In this experiment, we settled on 9 fixed prune levels at  ,"" 0.1, 0.2, . . . , 0.9. To control the prune ratio, comparison is only allowed between experimental runs at the same prune level. In each reference method, the true prune ratio depends on some parameter (e.g.,  in TCP and PRP), which we called the threshold parameter. To reduce the index down to the right size, we employed two different approaches to determine this cutting threshold:""",null,null
197,"1. Sample percentile: Collect the prune scores on top of a sample of index entries and use the percentile estimates to determine the right cutting threshold. This is mostly useful when the prune score is globally determined. Here, we used Definition 8 from Hyndman and Fan [13] to estimate percentiles.",null,null
198,"2. Bisection: Take an interval of feasible parameter values [a, b], and test-prune using the median value (a + b)/2. Return the current median if the test-prune reached the expected ratio; otherwise shrink the interval in half and repeat. This method is useful when the prune score for each index entry depends on the others in the same posting list, as in TCP.",null,null
199,"Bisection requires several test-prune runs into the entire index and is therefore more time-consuming. Sample percentile needs only one pass through the index, but the resulting prune ratio can be less precise than that with the values learned using bisection. In this paper, we applied bisection to TCP to learn the parameter , and applied sample percentile to the rest of methods. Specifically, we used a sample size of 10% of the entire index. For either case, the prune ratio error is controlled to within ±0.2%.",null,null
200,167,null,null
201,MAP (t) 0.10,null,null
202,MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
203,.1 .2 .3 .4 .5 .6 .7 .8 .9 160 157 152 145 139 134 127 118 095 159 158 154 148 144 140 135 127 102 160 157 153 149 145 143 142 137 120 160 158 153 146 140 133 126 110 085 156 152 148 142 133 123 110 088 045 156 153 148 140 135 123 107 092 046,null,null
204,MAP (td) 0.00 0.10 0.20 0.00,null,null
205,MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
206,.1 .2 .3 .4 .5 .6 .7 .8 .9 203 197 187 177 163 147 142 130 091 204 189 177 170 160 158 141 141 116 204 199 191 183 177 175 174 164 136 204 199 185 179 164 150 137 103 074 186 174 160 150 136 123 112 090 050 189 171 165 149 143 124 116 095 051,null,null
207,P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
208,.1 .2 .3 .4 .5 .6 .7 .8 .9 260 256 252 246 245 236 209 201 174 259 258 254 239 226 225 204 192 168 261 257 254 248 249 242 241 236 222 261 259 253 249 243 237 225 190 168 256 250 239 224 201 182 157 125 107 257 248 243 214 202 193 164 133 106,null,null
209,P@10 (t),null,null
210,0.20,null,null
211,Unpruned TCP UP-bm25 UP-dir,null,null
212,0.10,null,null
213,P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
214,.1 .2 .3 .4 .5 .6 .7 .8 .9 347 347 339 332 315 290 279 267 231 351 339 336 309 295 287 264 248 209 347 350 341 338 327 317 316 311 291 347 350 341 335 319 299 267 229 191 344 324 300 281 249 222 181 169 127 340 329 310 280 253 233 186 175 122,null,null
215,P@10 (td) 0.10 0.20 0.30,null,null
216,UP-jm PRP IP-u,null,null
217,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
218,"Figure 1: The overall performance result on WT10g. All the measurements are rounded to the 3rd decimal place, with preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has achieved 0.160/0.204 (t/td) in MAP and 0.261/0.347 (t/td) in P@10.",null,null
219,4.3 Retrieval Performance,null,null
220,"We followed Blanco and Barreiro [7] for using BM25 as the post-pruning retrieval method. Retrieval performance is measured in mean average precision (MAP) and precisionat-10 (P@10). The result on the largest set WT10G is summarized in Figure 1 (see Figures 2 and 3 at the very end of this paper for results on the smaller sets). Each figure has four sets of measure-to-query-type combinations, and the result for each combination is given both as a table on the left and a plot on the right. These combinations from top to bottom, respectively, are MAP-t, MAP-td, P@10-t, and P@10-td. Table columns and x-axes in the plots indicate prune levels, from 0.1 to 0.9 (10% to 90%). Rows and curves indicate pruning methods.",null,null
221,"Our result shows that, at small prune levels ( 0.5), all these methods differ little in performance, and the difference at larger prune levels seems more evident. Both PRP and IP-u, whose performance was nearly identical, have consistently achieved the bottom performance in all settings. In general, the performance for the UP family and TCP is comparable, though UP-dir performed slightly better than the other. We noticed that the performance of UP-dir is also robust to high prune ratio. On WT10G, when tested under an extreme setting with 90% prune ratio, UP-dir still managed to retain 75% of the baseline MAP for short queries,",null,null
222,"and 66.7% for long queries. Of the baseline P@10, UP-dir retained 85.1% for short queries and 83.9% for long queries. Under a less aggressive setting such as 50% prune ratio, UPdir have done even better by retaining 90.6% and 86.8% of baseline MAP, and 95.4% and 94.2% of baseline P@10, respectively for short and long queries.",null,null
223,4.4 Significance Tests,null,null
224,"We further conducted an analysis of variance (ANOVA) to check if the performance difference is significant. Due to the unbalanced size of measurement, we tested each corpus independently. Here, we assume a fixed-effect, 4-way no interaction, repeated measure design, expressed as:",null,null
225,"Yi,j,k,l ,"" ai + bj + ck + dl + i,j,k,l,""",null,null
226,"where Yi,j,k,l is the measured performance, ai is the querytype effect, bj the prune-level effect, ck the method effect, and dl the topic effect, and i,j,k,l denotes the error.",null,null
227,"The result is covered in Table 2. Each row indicates a measure-effect combination and each column a test corpus. Test statistics, such as degrees of freedom (DF) and F-values (F), are given for every test case. We used partial eta-square (p2) to measure the effect size [17]. We first ran an omnibus test to see if any main effect is significant. Of all three collections, all the main effects were tested significant for",null,null
228,168,null,null
229,Disks 4 & 5,null,null
230,WT2G,null,null
231,WT10G,null,null
232,Response Main Effect DF,null,null
233,MAP,null,null
234,"Query Type F(1, 5336)",null,null
235,"F p2 DF 74.10 .01 F(1, 5336)",null,null
236,F p2 DF,null,null
237,F p2,null,null
238,"42.57 .01 F(1, 10686) 192.25 .02",null,null
239,"Prune Ratio F(8, 5336) 240.30 .26 F(8, 5336) 306.17 .31 F(8, 10686) 193.26 .13",null,null
240,Method,null,null
241,"F(5, 5336) 11.00 .01 F(5, 5336) 40.20 .04 F(5, 10686) 61.47 .03",null,null
242,"Query Topic F(49, 5336) 885.35 .89 F(49, 5336) 335.89 .76 F(49, 10686) 422.46 .80",null,null
243,P@10,null,null
244,"Query Type F(1, 5336) 66.16 .01 F(1, 5336) 10.89 .00 F(1, 10686) 622.34 .06",null,null
245,"Prune Ratio F(8, 5336) 105.00 .14 F(8, 5336) 133.98 .17 F(8, 10686) 122.43 .08",null,null
246,Method,null,null
247,"F(5, 5336) 20.34 .02 F(5, 5336) 44.06 .04 F(5, 10686) 71.01 .03",null,null
248,"Query Topic F(49, 5336) 484.06 .82 F(49, 5336) 296.88 .73 F(49, 10686) 226.31 .68",null,null
249,Table 2: The 4-way no-interaction ANOVA result. Each cell indicates a combination of performance measure,null,null
250,"(row) and test collection (column). Degrees of freedom and F-values are given for testing all the main effects. Effect size is given in p2. In our experiment, all the main effects are significant for p < 0.001.",null,null
251,MAP P@10,null,null
252,Disks 4 & 5,null,null
253,Method Mean Group,null,null
254,UP-bm25 .204 a..,null,null
255,UP-dir,null,null
256,.200 a..,null,null
257,TCP,null,null
258,.196 ab.,null,null
259,UP-jm,null,null
260,.191 .bc,null,null
261,PRP,null,null
262,.187 ..c,null,null
263,IP-u,null,null
264,.187 ..c,null,null
265,UP-dir,null,null
266,.433 a..,null,null
267,TCP,null,null
268,.433 a..,null,null
269,UP-jm,null,null
270,.424 a..,null,null
271,UP-bm25 .417 a..,null,null
272,PRP,null,null
273,.392 .b.,null,null
274,IP-u,null,null
275,.389 .b.,null,null
276,WT2G,null,null
277,Method Mean,null,null
278,UP-dir,null,null
279,.223,null,null
280,UP-bm25 .211,null,null
281,TCP,null,null
282,.204,null,null
283,UP-jm,null,null
284,.192,null,null
285,IP-u,null,null
286,.181,null,null
287,PRP,null,null
288,.179,null,null
289,UP-dir,null,null
290,.404,null,null
291,TCP,null,null
292,.385,null,null
293,UP-jm,null,null
294,.367,null,null
295,UP-bm25 .359,null,null
296,IP-u,null,null
297,.322,null,null
298,PRP,null,null
299,.319,null,null
300,Group a... .b.. .b.. ..c. ...d ...d a... ab.. .bc. ..c. ...d ...d,null,null
301,WT10G,null,null
302,Method Mean,null,null
303,UP-dir,null,null
304,.162,null,null
305,UP-bm25 .151,null,null
306,TCP,null,null
307,.148,null,null
308,UP-jm,null,null
309,.145,null,null
310,IP-u,null,null
311,.129,null,null
312,PRP,null,null
313,.127,null,null
314,UP-dir,null,null
315,.286,null,null
316,TCP,null,null
317,.268,null,null
318,UP-jm,null,null
319,.265,null,null
320,UP-bm25 .259,null,null
321,IP-u,null,null
322,.222,null,null
323,PRP,null,null
324,.219,null,null
325,Group a.. .b. .b. .b. ..c ..c a.. .b. .b. .b. ..c ..c,null,null
326,"Table 3: The overall result for Tukey's HSD test. For each combination of performance measure (row) and test collection (column), pruning methods are ordered in descending mean and tested for group difference. Methods that differ significantly do not share the same group label.",null,null
327,"p < 0.001. Further analysis shows that query type and prune method have relatively small effect sizes, suggesting that query topic and prune ratio have much greater influence on the retrieval performance than the others do.",null,null
328,"Post-hoc tests are then called for to examine the difference caused by different factor values. Since our experimental setting involves multiple comparison, we employed Tukey's honest significance difference (HSD) to control the overall Type I error [12]. Note that since Tukey's HSD is a one-way test, only one effect is tested in each run. In the following paragraphs, we summarize the HSD results for all the main effects. Here, since our focus is on the method effect, we shall briefly cover the other three for the sake of completeness.",null,null
329,Method Effect. Table 3 summarizes this HSD result for the,null,null
330,"method effect. For each measure-corpus combination, we assigned group labels, e.g., ""a"" to ""d"", to individual methods based on the pairwise differences in their means. The difference between two methods is significant if and only if they share no common group label.",null,null
331,"The result is briefly summarized as follows. First, the UP family and TCP consistently achieved top performance in both MAP and P@10 across different test settings. In the leading group, UP-dir delivers slightly better performance than the others. This is even more pronounced under the Web settings, in which UP-dir significantly outperformed the rest of methods in MAP (on both corpora) and in P@10 (on WT10G only). Second, the performance for the rest",null,null
332,"of UP family and TCP is in general comparable. Take UPbm25 and TCP. The performance difference between the two is subtle: UP-bm25 was shown superior in MAP on Disks 4 & 5 but inferior in P@10 on WT2G. Third, PRP and IP-u are inferior to all the other methods. This result is consistent with our analysis on the raw performance measurements.",null,null
333,Query Type Effect. Long queries (td) achieve better per-,null,null
334,"formance than short queries (t), which is expected because short queries are less precise than longer ones. This difference is confirmed on all three test collections, and appears more evident in the largest set WT10G.",null,null
335,Prune Ratio Effect. Small prune levels do better than large,null,null
336,"ones in both metrics, which is also expected since more aggressive pruning results in less information in the index. According to the pairwise comparison made within the HSD test, this result is generally true except for a few small pairs such as 0.1-against-0.2. Specifically, WT10G has many such insignificant small pairs, suggesting that retrieval on larger Web collections is less sensitive to information loss.",null,null
337,Topic Effect. The result is difficult to interpret due to the,null,null
338,"size of topic pairs, e.g., topics 451-551 on WT10G has produced 4950 such pairwise comparisons. In general, only a small number of queries have significantly deviated from the average performance, meaning that most queries are designed to be about equally difficult.",null,null
339,169,null,null
340,5. DISCUSSION,null,null
341,"The experiment result for uniform pruning is generally in line with our understanding to impact, much of this was contributed by the previous work in index compression and dynamic pruning. Since many ideas come from the same outlet in the indexing pruning community, it is no surprise that uniform pruning is related to many existing impact-based methods. For example, Anh et al. [3] concluded that impactsorted indexes combined with early termination heuristics can best optimize retrieval system performance. This technique is conceptually equivalent to uniform pruning. Further work in this line investigated impact-based pruning, an application of impact-sorting to dynamic query pruning [4]. And again, this is a dynamic version of uniform pruning. Adding to these results, our analysis shows that impactbased methods are good approximate solutions to the proposed model induction problem.",null,null
342,"One further question that invites curious eyes is why Dirichlet smoothing worked so well with uniform pruning that it significantly outperformed all the other variations on our Web benchmark WT2G and WT10G. So far the answer is still unclear to us. Here, let us discuss a few possibilities:",null,null
343,"· BM25 might be a poor approximation to the probability p(t|d) since the framework presented in this paper was tailored specifically for language models. While this may explain why BM25 was inferior to Dirichlet smoothing in our experiments, it does not tell us why the performance for Jelinek-Mercer smoother and for BM25 were comparable.",null,null
344,"· Another possibility is that, since parameter optimization is lacking in our experiment, we might have failed in producing the most competitive result for BM25 and Jelinek-Mercer smoother. If this theory is true, score functions will need task-specific fine-tuning in their further use. But for that to make sense, one needs to point out in what major way the role of a score function in index pruning departs from that in ordinary ad-hoc retrieval. This may point to an interesting direction for future work, but based on the evidence collected so far this claim is difficult to verify.",null,null
345,"With the argument given in Section 3 about the convex integer program, one may argue that it is important to prevent depleting any term posting since doing so would take the objective in Equation 10 to minus infinity. In other words, an additional constraint, called ""no depletion"", shall be added into the index pruning guideline. This is because, even though we do not attempt to solve the convex program, the constraint still needs to be enforced to guarantee that information loss is bounded. In this respect, it is necessary to adopt a top-k preserving strategy (i.e., skip any term posting that has less than k entries), such as the one in TCP, to avoid depleting term postings.",null,null
346,6. CONCLUSION,null,null
347,"In this paper, we review the problem of static index pruning from a brand new perspective. Given the appropriate assumptions, we show that this problem can essentially be tackled within a model induction framework, using the principle of minimum cross-entropy. The theory guarantees that the induced model best approximates the full model in terms",null,null
348,"of probability divergence. We show that static index pruning can be written as a convex integer program. Yet exact inference, though possible as it might be, is generally computationally infeasible for large collections. So we further propose a surrogate model to address the computation issue, and show that uniform pruning is indeed an optimal solution to the formalism. To verify the correctness of our result, we conducted an extensive empirical study. The experiment was redesigned to take two factors, variable control and significance testing, into consideration. This setup has helped us reduce possible experimental bias or error.",null,null
349,"Our result confirms that, when paired with the Dirichlet smoother, the performance of uniform pruning is state of the art. Significant improvement over the other methods were observed across diverse retrieval settings. Uniform pruning also exhibits an advantage in robustness with respect to large prune ratio. Specifically, our result on WT10G for short queries suggests that uniform pruning with the Dirichlet smoother retains at least 90% of the baseline performance at 50% prune ratio and 85% at 80% prune ratio. To the best of our knowledge, this is by far the best performance ever reported for static index pruning on the standard benchmark.",null,null
350,"This research work has given rise to many technical issues, some have been addressed in Section 5 and some remain unsettled. It shall be interesting to see how uniform pruning responds to other test environments, such as different retrieval engines, corpora, or tasks. Document-length update and pseudo relevance feedback have been two landmark issues that we are ready to explore. Since we did not fine-tune the baseline performance, testing pruning methods against optimized, strong baseline shall provide more insight about this art. Besides all these possibilities, one promising direction is to extend the model induction idea to other type of structured data, such as lexicons or language models. Further investigation into the theory may shed us some light in the role that impact plays in different IR tasks.",null,null
351,7. ACKNOWLEDGMENT,null,null
352,"We would like to thank Wei-Yen Day, Ting-Chu Lin, and the anonymous reviewers for their useful comments.",null,null
353,8. REFERENCES,null,null
354,"[1] I. S. Altingovde, R. Ozcan, and O. Ulusoy. A practitioner's guide for static index pruning. In M. Boughanem, C. Berrut, J. Mothe, and C. Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, chapter 65, pages 675­679. Springer Berlin / Heidelberg, Berlin, Heidelberg, 2009.",null,null
355,"[2] I. S. Altingovde, R. Ozcan, and O. Ulusoy. Static index pruning in web search engines: Combining term and document popularities with query views. ACM Transactions on Information Systems, 30(1), Mar. 2012.",null,null
356,"[3] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space ranking with effective early termination. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 35­42, New York, NY, USA, 2001. ACM.",null,null
357,[4] V. N. Anh and A. Moffat. Pruned query evaluation using pre-computed impacts. In Proceedings of the,null,null
358,170,null,null
359,MAP (t) 0.00 0.10 0.20,null,null
360,MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
361,.1 .2 .3 .4 .5 .6 .7 .8 .9 225 221 213 206 198 186 172 149 109 227 226 223 218 208 194 168 166 143 225 222 213 206 197 190 178 157 117 224 221 211 202 192 180 163 140 103 227 224 219 209 194 168 148 149 108 227 225 220 211 195 173 147 147 108,null,null
362,MAP (td) 0.00 0.10 0.20,null,null
363,MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
364,.1 .2 .3 .4 .5 .6 .7 .8 .9 251 246 239 232 219 201 184 162 117 250 246 235 229 222 202 179 176 158 251 247 239 232 220 210 196 173 133 250 246 238 228 216 194 173 148 110 253 246 228 212 195 168 148 146 129 252 240 230 215 195 173 145 138 125,null,null
365,P@10 (t) 0.10 0.25 0.40,null,null
366,P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
367,.1 .2 .3 .4 .5 .6 .7 .8 .9 438 434 434 430 428 426 424 384 326 438 440 436 438 438 412 370 358 294 438 434 432 428 426 434 416 398 344 438 434 432 428 424 416 410 370 318 436 442 444 452 428 400 340 284 204 438 440 440 450 430 406 338 278 190,null,null
368,Unpruned TCP UP-bm25 UP-dir,null,null
369,P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
370,.1 .2 .3 .4 .5 .6 .7 .8 .9 476 468 478 474 468 460 462 432 358 470 486 482 470 444 420 400 390 324 474 468 472 462 458 450 440 442 386 476 470 470 468 466 442 430 412 336 486 486 468 470 428 388 350 316 236 474 478 474 464 436 400 340 292 226,null,null
371,P@10 (td) 0.10 0.25 0.40,null,null
372,UP-jm PRP IP-u,null,null
373,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
374,"Figure 2: The performance result on Disk 4 & 5, with all measurements rounded to the 3rd decimal place, preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has has achieved 0.228/0.256 (t/td) in MAP and 0.436/0.478 (t/td) in P@10.",null,null
375,"29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, pages 372­379, New York, NY, USA, 2006. ACM.",null,null
376,"[5] R. Blanco and A. Barreiro. Boosting static pruning of inverted files. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 777­778, New York, NY, USA, 2007. ACM.",null,null
377,"[6] R. Blanco and A. Barreiro. Static pruning of terms in inverted files. In G. Amati, C. Carpineto, and G. Romano, editors, Advances in Information Retrieval, volume 4425 of Lecture Notes in Computer Science, chapter 9, pages 64­75. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.",null,null
378,"[7] R. Blanco and A. Barreiro. Probabilistic static pruning of inverted files. ACM Transactions on Information Systems, 28(1), Jan. 2010.",null,null
379,"[8] S. Bu¨ttcher and C. L. A. Clarke. A document-centric approach to static index pruning in text retrieval systems. In Proceedings of the 15th ACM international conference on Information and knowledge management, CIKM '06, pages 182­189, New York, NY, USA, 2006. ACM.",null,null
380,"[9] D. Carmel, D. Cohen, R. Fagin, E. Farchi, M. Herscovici, Y. S. Maarek, and A. Soffer. Static index pruning for information retrieval systems. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '01, pages 43­50, New York, NY, USA, 2001. ACM.",null,null
381,"[10] R.-C. Chen, C.-J. Lee, C.-M. Tsai, and J. Hsiang. Information preservation in static index pruning. In Proceedings of the 21st ACM international conference on Information and knowledge management, CIKM '12, pages 2487­2490, New York, NY, USA, 2012. ACM.",null,null
382,"[11] E. S. de Moura, C. F. dos Santos, D. R. Fernandes, A. S. Silva, P. Calado, and M. A. Nascimento. Improving web search efficiency via a locality based static pruning method. In Proceedings of the 14th international conference on World Wide Web, WWW '05, pages 235­244, New York, NY, USA, 2005. ACM.",null,null
383,"[12] D. Hull. Using statistical testing in the evaluation of retrieval experiments. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '93, pages 329­338, New York, NY, USA, 1993. ACM.",null,null
384,171,null,null
385,0.00 0.10 0.20 0.30 0.00 0.10 0.20,null,null
386,MAP (t),null,null
387,MAP (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
388,.1 .2 .3 .4 .5 .6 .7 .8 .9 248 242 233 222 208 195 176 143 090 249 250 249 236 225 213 188 133 108 247 240 234 226 207 195 198 181 139 247 239 229 220 199 173 143 107 080 253 249 237 226 196 166 114 092 063 250 247 240 225 205 164 126 081 074,null,null
389,MAP (td),null,null
390,MAP (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
391,.1 .2 .3 .4 .5 .6 .7 .8 .9 288 278 258 241 222 206 183 147 096 288 276 260 237 226 209 193 151 122 290 279 268 258 238 225 224 209 161 289 281 262 245 217 187 149 113 078 284 262 233 214 189 166 119 093 067 284 263 236 216 190 161 124 087 079,null,null
392,P@10 (t) 0.10 0.25 0.40,null,null
393,P10 (t) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
394,.1 .2 .3 .4 .5 .6 .7 .8 .9 416 410 408 398 400 380 374 338 268 414 420 418 386 380 384 340 238 210 414 402 402 404 380 386 392 374 332 414 404 402 402 378 348 336 306 248 418 418 408 394 362 342 250 126 144 410 408 400 390 380 338 268 138 126,null,null
395,Unpruned TCP UP-bm25 UP-dir,null,null
396,P10 (td) TCP UP-bm25 UP-dir UP-jm PRP IP-u,null,null
397,.1 .2 .3 .4 .5 .6 .7 .8 .9 464 446 428 410 410 388 356 328 302 464 446 412 370 378 384 326 268 218 460 446 432 436 422 428 422 408 346 460 446 430 420 388 354 340 276 260 442 434 392 382 340 334 260 142 146 450 430 402 392 348 324 258 158 170,null,null
398,P@10 (td) 0.10 0.25 0.40,null,null
399,UP-jm PRP IP-u,null,null
400,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
401,"Figure 3: The performance result on WT2G. All measurements were rounded to the 3rd decimal place, and preceding zero and decimal point removed. The best raw performance at each prune level is underlined. The unpruned baseline has has achieved 0.249/0.293 (t/td) in MAP and 0.414/0.460 (t/td) in P@10.",null,null
402,"[13] R. J. Hyndman and Y. Fan. Sample quantiles in statistical packages. The American Statistician, 50(4):361­365, Nov. 1996.",null,null
403,"[14] S. Kullback. Information Theory and Statistics. Wiley, New York, 1959.",null,null
404,"[15] J. D. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In W. B. Croft, D. J. Harper, D. H. Kraft, J. Zobel, W. B. Croft, D. J. Harper, D. H. Kraft, and J. Zobel, editors, SIGIR, SIGIR '01, pages 111­119, New York, NY, USA, 2001. ACM.",null,null
405,"[16] K. Lange, D. R. Hunter, and I. Yang. Optimization transfer using surrogate objective functions. Journal of Computational and Graphical Statistics, 9(1), 2000.",null,null
406,"[17] D. C. Montgomery. Design and analysis of experiments (6th ed.). Wiley, 2004.",null,null
407,"[18] S. Robertson. The probability ranking principle in IR. In K. S. Jones and P. Willett, editors, Reading in Information Retrieval, chapter The probability ranking principle in IR, pages 281­286. Morgan",null,null
408,"Kaufmann Publishers Inc., San Francisco, CA, USA, 1997.",null,null
409,[19] S. Thota and B. Carterette. Within-Document Term-Based index pruning with statistical hypothesis,null,null
410,"testing. In P. Clough, C. Foley, C. Gurrin, G. Jones, W. Kraaij, H. Lee, and V. Mudoch, editors, Advances in Information Retrieval, volume 6611 of Lecture Notes in Computer Science, chapter 54, pages 543­554. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011.",null,null
411,"[20] C. Zhai and J. Lafferty. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of the tenth international conference on Information and knowledge management, CIKM '01, pages 403­410, New York, NY, USA, 2001. ACM.",null,null
412,"[21] L. Zheng and I. J. Cox. Entropy-Based static index pruning. In M. Boughanem, C. Berrut, J. Mothe, and C. Soule-Dupuy, editors, Advances in Information Retrieval, volume 5478 of Lecture Notes in Computer Science, chapter 72, pages 713­718. Springer Berlin / Heidelberg, Berlin, Heidelberg, 2009.",null,null
413,172,null,null
414,,null,null

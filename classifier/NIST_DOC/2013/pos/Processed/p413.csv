,sentence,label,data
0,Preference Based Evaluation Measures for Novelty and Diversity,null,null
1,"Praveen Chandar and Ben Carterette {pcr,carteret}@udel.edu",null,null
2,Department of Computer and Information Sciences,null,null
3,University of Delaware,null,null
4,"Newark, DE, USA 19716",null,null
5,ABSTRACT,null,null
6,"Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to maximize the amount of novel and relevant information available to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be disambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a preference for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depending on user. A user profile contains information about the extent to which each factor, including subtopic relevance, plays a role in the user's preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users.",null,null
7,"In this work, we propose an evaluation framework that not only can consider implicit factors but also handles differences in user preference due to varying underlying information need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as -nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the potential to capture various other factors when real data is used. We also show that the proposed measures can easily handle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments.",null,null
8,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
9,Categories and Subject Descriptors: H.3 [Information Storage and Retrieval]; H.3.4 [Systems and Software]: Performance Evaluation,null,null
10,"Keywords: Novelty and Diversity, Evaluation",null,null
11,1. INTRODUCTION,null,null
12,"The concept of relevance is the probably the most critical aspect of theoretical and practical information retrieval (IR) models. But which documents are relevant can differ from user to user depending on their exact information need, even if they start with the same keyword query. Queries can be ambiguous and/or underspecified, and the retrieval systems are required to handle these diverse information needs while providing novel information. Traditional IR evaluation also works under the assumption that documents are independently relevant separate from any user context The major drawback with this approach is that it does not penalize redundancy in rankings, potentially reducing the amount of novel information available to the user. Recently a subtopic based approach was introduced, to handle the redundancy problem and account for diverse information needs. The underlying information need for a query is decomposed into set of subtopics, and the number of novel subtopics that a document is relevant to (i.e. not seen earlier in the ranking) provides a measure of novelty. Various evaluation measures have been defined based on this approach [1, 9, 23, 27].",null,null
13,"While subtopics are used to account for the diverse information needs of a query, the relation between them varies from user to user. For example, consider the query living in India. A person planning to visit India could be interested in information for visitors and immigrants & how people live in India whereas a student writing an essay would be more interested in the history about life and culture in India. Even though all of these subtopics seem relevant to the query, the importance of a subtopic is dependent on the user and the scenario in which the search was performed. It is well-known that user preferences are influenced not only by topical relevance but also by other factors such as readability, subtopic importance, completeness, etc. User profiles can be used to represent the combination of relevant subtopics and the above mentioned factors that precisely reflects the user's information need. Currently, there is no evaluation measure that (a) takes into account various factors affecting user preference, (b) handles multiple user profiles for a given query.",null,null
14,"In this work, we propose an evaluation framework and metrics based on user preference for the novelty and diversity task. The framework revolves around the idea of assigning",null,null
15,413,null,null
16,"utility scores that reflect each set of user`s preference towards each document. The document utilities are estimated using a series of preference judgments collected conditional on previously ranked documents. Document utility at a given rank implicitly accounts for the subtopic coverage, novelty, topical relevance and the other factors as well. As pointed out earlier, the utility of document could differ for each user, thus user preference are obtained across a pool of users to account for diverse information need of a query. Evaluation metrics defined based on this framework directly models a user traversing a ranking from top to bottom seeking relevant and novel information for the issued query. Therefore, our proposed measures estimate the total utility of a ranked list available to the user for a given query.",null,null
17,"The rest of the paper is organized as follows: a detail explanation of the existing evaluation framework and the existing metrics for novelty and diversity is provided in Section 2. We point out issues with the current method and propose preference-based evaluation measures in Section 3. A description of the datasets along with the experimental design employed in our work can be found in Section 4. We analyze in detail the performance of our metrics and compare it to various existing ones in Section 5. Finally, Section 6 summarizes our findings and sketches our future directions.",null,null
18,2. NOVELTY/DIVERSITY EVALUATION,null,null
19,"Search result diversification is an effective strategy to deal with the diverse information needs of the user while reducing redundancy in the ranked list [19, 28, 25]. Several methods have been proposed to produce a ranking that maximizes relevance with respect to multiple information needs for a given query, starting with the maximum marginal relevance model of Carbonell et al. [4]. In addition to new models, the task demands new evaluation metrics, as traditional IR measures are focused on relevance with respect to a single user and do not penalize redundancy in results. Zhai et al. studied the subtopic retrieval task in the context of the TREC Interactive track [17], and defined simple evaluation measures such as subtopic recall and subtopic precision based on the relevance of documents to pre-defined subtopics. Clarke et al. proposed an evaluation strategy that decomposes underlying information needs of a query into information nuggets; document utility is determined by the number of novel nuggets covered by the document. NRBP, also introduced by Clarke et al. combines ideas from -nDCG and Rank-Biased Precision [12]. Agarwal et al. focused on the diversity problem in the web domain by taking into account the importance of user intents via a probability distribution. Each of these measures will be described in more detail below.",null,null
20,"Almost all of the existing measures are based on the idea of explicit subtopics: decompositions of a given query into several pieces of information (such as facets, intents, or nuggets) that account for various underlying information needs. In this framework, novelty is solely dependent on the document's relevance to a subtopic. System effectiveness is estimated by iterating over the ranked list, penalizing relevant documents relevant to subtopic(s) seen earlier in the ranking, and rewarding documents relevant to unseen subtopic(s).",null,null
21,2.1 Test Collection,null,null
22,Test collections such as those produced for the TREC Interactive tracks [17] and the TREC Question Answering tracks [26] consist of subtopic-level judgments in documents.,null,null
23,"The TREC Web track diversity datasets created to study the problem of novelty and diversity are most suitable to our work. These datasets comprise a set of topics, and for each topic a set of subtopics that were identified semiautomatically with the help of a tool that clusters reformulations of the given query. The tool combined evidences from clicks and reformulations to obtain clusters of queries; the track organizers used these clusters to manually pick the set of subtopics for a given target query.",null,null
24,"Binary judgments of relevance were made by NIST assessors for each subtopic to each document. Note that the use of this method means that only subtopics evidenced by a large number of users will be present in the data; interpretations that are equally ""real"" yet less popular will not be represented when this method is used.",null,null
25,2.2 Evaluation Measures,null,null
26,"Evaluation measures for novelty and diversity must account for both relevance and novelty in rankings. It is important that redundancy caused by documents containing previously retrieved information are penalized while documents containing novel information are rewarded; as described above, this is achieved using subtopic relevance judgments. A brief description of the commonly used metrics that employ a subtopic based approach is given below:",null,null
27,"Subtopic recall measures the proportion of unique subtopics retrieved at a given rank [27]. Given that a query q has m subtopics, the subtopic recall at rank k is given by the ratio of number of unique subtopics contained by the subset of document up to rank k to the total number of subtopics m.",null,null
28,"Sk  i,1",null,null
29, subtopics(di),null,null
30,"S-recall@k ,",null,null
31,(1),null,null
32,m,null,null
33,-nDCG scores a result set by rewarding newly found subtopics and penalizing redundant subtopics [13]. Computation of the gain vector and a rank discount are key to -nDCG. The gain vector is computed by summing over subtopics appearing in the document at rank i:,null,null
34,m,null,null
35,"G[i] ,"" X(1 - )cj,i-1""",null,null
36,(2),null,null
37,"j,1",null,null
38,"where cj,i is the number of times subtopic j has appeared",null,null
39,in documents up to (and including) rank i.,null,null
40,"The most commonly used discount function is log2(1 + i), although other discount functions are possible. Summing",null,null
41,gains over discounts gives discounted cumulative gain:,null,null
42,k,null,null
43,"X -DCG@k ,",null,null
44,G[i],null,null
45,(3),null,null
46,"i,1 log2(1 + i)",null,null
47,"-DCG must be normalized to compare the scores against various topics. This is done by finding an ""ideal"" ranking that maximizes -DCG, which can be done using a greedy algorithm. The ideal ranking computation is an NPComplete problem [5]. The ratio of -DCG to that ideal gives -nDCG.",null,null
48,"Intent-aware family Agrawal et al. [1] studied the problem of evaluating ambiguous web queries. They proposed evaluating a ranking against each subtopic (or ""intent"") by",null,null
49,414,null,null
50,"any traditional IR measure, and then combining the results based on importance of subtopic. This gave rise to a family of measures that are known as intent-aware. Most traditional measures such as precision@k, average precision (AP), nDCG, etc. can be cast as intent-aware versions; for instance, intent-aware AP would be expressed as:",null,null
51,m,null,null
52,"AP -IA , X P (i|q)APi",null,null
53,(4),null,null
54,"i,1",null,null
55,"where m is the number of intents/subtopics, P (i|q) is the probability that the user is interested in intent i for query q, and APi is average precision computed only with the documents relevant to intent i.",null,null
56,"ERR-IA Expected Reciprocal Rank (ERR) is a measure based on ""diminishing returns"" for relevant documents [10]. According to this measure, the contribution of each document is based on the relevance of documents ranked above it. The discount function is therefore not just dependent on the rank but also on relevance of previously ranked documents.",null,null
57,ERR,null,null
58,",",null,null
59,X,null,null
60,1 i,null,null
61,Ri,null,null
62,i-1,null,null
63,Y(1,null,null
64,-,null,null
65,Rj ),null,null
66,(5),null,null
67,"i,1",null,null
68,"j,1",null,null
69,where Ri is a function of the relevance grade of the document at rank i (typically defined to be (2g - 1)/2gmax). ERR-IA is defined exactly as other intent-aware measures: a weighted average of ERR computed separately for each subtopic/intent [9]. We mention it separately because it has some appealing mathematical properties and it is one of the official measures of the TREC Web track [9].,null,null
70,D-Measure The D and the D# measures described by Sakai et al. [22] aim to combine two properties into a single evaluation measure. The first property is to retrieval documents covering as many intents as possible and second is to rank documents relevant to more popular intents higher than documents relevant to less popular intents.,null,null
71,3. PREFERENCE BASED FRAMEWORK,null,null
72,"The subtopic-based evaluation framework focuses on estimating the effectiveness of a system based on topical and sub-topical relevance. In practice, there may be many other factors such as reading level, presentation, completeness, etc. that influence user preferences for one document over another in the context of novelty and diversity [8]. We could describe the information needs of a user that consists of various details, including specifics of pieces of information the user is interested in, reading level of the user, and so on in a user profile. Then we could view the goal of an evaluation measure as determining how well a ranking of documents satisfies a variety of user profiles.",null,null
73,"In order to understand the concept of user profiles, let us consider an example query from the TREC Web track: air travel information. Table 1 shows the subtopics defined for the Web track's diversity task and provides the information needs of three different possible users for the given query (assuming we restrict ourselves to the TREC paradigm and represent the user's information need using only subtopics). We can think of user A as a first time air traveler looking for information on air travel tips and guidelines, user B as a journalist writing an article on the current quality of air travel and looking for statistics and reports to accomplish",null,null
74,"the task, and user C as an infrequent traveler looking restrictions and rules for check-in and carry-on luggages. Therefore, user A's profile for the above example query consists of subtopics d and e, user B's of c, and user C's of a and b. (In practice, the profiles would typically take into account other factors such as presentation, readability, and other factors as well, but none of this need be made explicit.)",null,null
75,"Even if we restrict ourselves to modeling only subtopics, there are some issues with existing measures based on subtopics:",null,null
76,"(a) subtopic identification is challenging and tricky as it is not easy to enumerate all possible information needs for a given query,",null,null
77,"(b) measures often require many parameters to be set before use,",null,null
78,(c) measures assume subtopics to be independent of each other but in reality this is not true.,null,null
79,"Let us refer to Table 1 to consider these issues. First, given the granularity of these subtopics, it would not be difficult to come up with additional subtopics that are not in the data. Top-ranked results from a major search engine suggest subtopics such as ""Are airports currently experiencing a high level of delays and cancellations?"", ""I am disabled and require special consideration for air travel; help me find tips."", and ""My children are flying alone, I am looking for tips on how to help them feel comfortable and safe."" Are users with these needs going to be satisfied by a system that optimizes for the limited set provided?",null,null
80,"Second, measures like -nDCG and ERR-IA have a substantial number of parameters that must be decided on. Some are explicit, such as  (the penalization for redundancy) [15] or P (i|q) (the probability of an intent/subtopic given a query1). Others are implicit, hidden in plain sight because they have ""standard"" settings: the log discount of nDCG or the grade value Ri of ERR-IA, for instance. Each of these parameters requires some value; it is all too easy to fall back on defaults even when they are not appropriate.",null,null
81,"Third, some subtopics are clearly more related to each other than others (in fact, we used this similarity to create the profiles). Documents that are relevant to subtopic c are highly unlikely to also be relevant to any of the other subtopics, but it is more likely that there are pages relevant to both subtopics a and b.",null,null
82,"In this work, we sidestep these issues by proposing an evaluation framework that simply allows users to express preferences between documents. Their preferences may be based on topical or subtopic relevance, but they may also be based on any other factors that are important to them. Preferences can be obtained over many users to capture the varying importance of topics and factors, and when a sufficiently large set of preferences has been obtained, systems can be evaluated according to how well they satisfy those users. Preference judgments have only scantly been used in IR evaluation, having been introduced by Rorvig [20] but not subject to empirical study until recently [7, 2]. Comparison studies between absolute and preference judgments show that preference judgments can often be made faster than graded judgments, with better agreement between assessors (and more consistency with individual assessors) [7] while making much finer distinctions between documents.",null,null
83,1The original definition of -nDCG has parameters for subtopic weights as well.,null,null
84,415,null,null
85,subtopic a. What restrictions are there for checked baggage during air travel?,null,null
86,user A user B user C ,null,null
87,b. What are the rules for liquids in carry-on luggage?,null,null
88,c. Find sites that collect statistics and reports about airports,null,null
89,d. Find the AAA's website with air travel tips.,null,null
90,e. Find the website at the Transportation Security Administration (TSA)  that offers air travel tips.,null,null
91,Table 1: An example topic (air travel information) along with its subtopics from the TREC Diversity dataset and three possible user profiles indicating the interests of different users.,null,null
92,"Chandar and Carterette [8] introduced a preference-based framework similar to ours, but there exists no evaluation measure that incorporates preference judgments directly for novelty and diversity. Moreover, that work focused only on ranking novel documents, without considering the more general question of diversity--that different users will have different preferences depending on their profile.",null,null
93,3.1 Test Collection,null,null
94,"Chandar and Carterette's preference-based framework is based on so-called levels of preference judgments. We use a similar idea; in this work, a test collection of preferences for novelty and diversity consists of two different types of preference judgments:",null,null
95,"1. simple pairwise preference judgments, in which a user is shown two documents and asked which they prefer.",null,null
96,"2. conditional preference judgments, in which a user is shown three or more documents and asked to express a preference between two of them, supposing they had read the others.",null,null
97,"Simple pairwise preferences produce a relevance ranking: given a pair of documents, assessors select the preferred document based on some criteria. We expect topical relevance to be the primary criteria, although many criteria (such as ease of reading, completeness of information, salience of article, etc.) could factor into an assessor's choice. Since different users may have different needs and different preferences for the same query, pairs can be shown to multiple assessors to get multiple preferences. Over a large space of assessors, we would expect that documents are preferred proportionally according to the relative importance of the subtopics they are relevant to, with various other factors influencing finer-grained orderings.",null,null
98,"Simple pairwise preferences cannot capture novelty; in fact, two identical documents should be equally preferred in all pairs in which they appear and therefore end up tied in the final ordering. Conditional preference judgments attempt to resolve this by asking for a preference for a given pair of document conditional on the information in other documents shown to the assessor at the same time. The assessor is asked to read those documents, then select which of the remaining two they would like to see next.",null,null
99,"Figure 1 illustrates conditional preferences with a triplet of documents: the assessor would read document X, then select which of A or B they would like to see next2 We",null,null
100,2Note that any document may be placed at the top of a triplet; it need not be the most preferred document among the simple pairwise preferences.,null,null
101,Figure 1: Left: a simple pairwise preference for which an assessor chooses A or B. Right: a triplet of documents for conditional preference judgments. An assessor would be asked to choose A or B conditional on having read X.,null,null
102,"expect the assessor's choice to be based not only on topical relevance, but also on the amount of new information given what is provided in the top document. Again, they can use other factors in their preferences, but novelty should be a primary consideration: if X is identical to A, we expect them to choose B, and then a system that ranks X and A adjacent would be penalized for failing to rank B after X.",null,null
103,"Similarly, we could obtain preferences with quadruplets of documents, quintuplets of documents, and so on. In practice it becomes increasingly difficult for assessors to make such fine distinctions, so we limit to only obtaining judgments on triplets. A triplet in our framework corresponds to Chandar and Carterette's ""level 2"" judgments; as they showed, these judgments capture most of the necessary information about novelty. Preferences conditional on greater numbers of other documents contribute less and less [8].",null,null
104,3.2 Preference-Based Evaluation Measure,null,null
105,"We propose a model-based measure using preferences to assess the effectiveness of systems for the novelty and diversity task. Model based measures can be composed from three underlying models: browsing model, document utility, and utility accumulation [6]. The way users interact with the ranked list is defined by the browsing model; we rely on the most accepted model in which the user scans documents down a ranked list one-by-one and stops at some rank k. The document utility model defines the amount of utility provided by a single document, and utility accumulation models the total utility derived during browsing.",null,null
106,"We define our utility based model for novel and diversity ranking task as follows: a user scanning documents down a ranked list derives some utility U (d) from each document and stops at some rank k. We hypothesize that the utility of a document at rank i is dependent on previously ranked document (i.e. d1 to di-1). Given a probability distribution for a user stoping at rank k, the utility accumulation model",null,null
107,416,null,null
108,can be defined as:,null,null
109,n,null,null
110,X,null,null
111,"P rf ,"" P (k)U (d1, ..., dk)""",null,null
112,(6),null,null
113,"k,1",null,null
114,"where P (k) is the probability that a user stops at rank k and U (d1, ..., dk) is the total utility of the documents from ranks 1 through k.",null,null
115,"We simplify this by formulating U (d1, ..., dk) as a sum of individual document utilities conditional on documents ranked before:",null,null
116,n,null,null
117,k,null,null
118,"P rf , X P (k) X U (di|S)",null,null
119,(7),null,null
120,"k,1",null,null
121,"i,1",null,null
122,"where P (k) is the probability that a user stops at rank k, U (di|S) gives the utility of the document at rank i conditional on a set of previously ranked document S, and the sum from i ,"" 1 to k gives the total utility of all documents from ranks 1 through k. There are two main components in the above equation: the probability that a user stops at a given rank (P (k)) and the utility of a document conditioned of previously ranked documents (U (di|S)). Carterette demonstrated different ways to model the stopping rank from the various ad-hoc measure such as Rank Biased Precision [16], nDCG, and Reciprocal Rank [6].""",null,null
123,"1. PRBP (k) , (1 - )k-1",null,null
124,2.,null,null
125,"PDCG(k) ,",null,null
126,1 log(k+1),null,null
127,-,null,null
128,1 log(k+2),null,null
129,3.,null,null
130,"PRR(k) ,",null,null
131,1 k(k+1),null,null
132,"Finally, we define the document utility model in which the document utility at a given rank is conditioned on previously ranked documents. The utility of the document at rank i is given by U (di) for i ,"" 1 since at rank 1 the user would not have seen any other documents and therefore would not be conditioning on any other documents. For subsequent ranks, utility is U (di|di-1, ...d1), indicating that the utility depends on documents already viewed.""",null,null
133,"Now our goal is to estimate these utilities using preference judgments. Since we have simple pairwise preferences and conditional preferences in triplets, we decompose the document utility model as follows:",null,null
134,"8 >U (di), <",null,null
135,"U (di|S) ,"" U (di|di-1),""",null,null
136,if i is 1,null,null
137,if i is 2,null,null
138,(8),null,null
139,">:F ({U (di|dj)}ij-,""11), if i > 2""",null,null
140,where the function F () takes an array of conditional utilities (U (di|dj )).,null,null
141,"The utility U (di) can be directly obtained using the pairwise judgments; we simply compute it as the ratio of number of times a document was preferred to the number of times it appeared in a pair. The utilities U (di|di-1) can similarly be obtained from the conditional preferences, computed as the ratio of the number of times di was preferred conditional on di-1 appearing as the ""given"" document to the number of times it appear with di-1 as the ""given"" document. Note that these utilities can be computed regardless of how many times a document has been seen, how many different assessors have seen it, how much disagreement there is between assessors, and so on. Although, a document must be shown",null,null
142,at least few time in order to determine its relevance estimate. An estimate of the document's utility is obtain using the ratio of number of times the document was preferred to the number of time it was shown.,null,null
143,"We experiment with two functions for F (): average and minimum. The intuition behind these functions can be explained with the help of an example. Consider a ranking R ,"" {d1, d2, d3}. According to equation 8 the utility of d3 depends on U (d3|d1) and U (d3|d2). The minimum function assumes that d3 cannot be any more useful conditional on both d1 and d2 than it is on either one separately, thus giving a sort of worst-case scenario. The average function assumes that the utility of d3 conditional on both d1 and d2 is somewhere in between its utility conditioned on each separately, giving d3 some benefit of the doubt that it may contribute something more when appearing after both d1 and d2 than it does when appearing after either one on its own.""",null,null
144,"Our measure as defined is computed over the entire ranked list. In practice, measures are often computed only to rank 5, 10, or 20 (partially because relevance judgments may not be available deeper than that). When we compute the measure to a shallower depth, we must normalize it so that it will average over a set of queries. As a final step in the computation of nP rf , we normalize equation 7 cut off at rank K by the ideal utility score.",null,null
145,nP rf [K],null,null
146,",",null,null
147,P rf [K] I-P rf [K],null,null
148,(9),null,null
149,where I-P rf [K] is the ideal utility score that could be obtained at rank K. This can be obtained by selecting the document with the highest utility value conditioned on previously ranked documents. Document (d1) with the highest utility value takes rank 1 and the document with highest utility when conditioned on d1 takes rank 2 and so on.,null,null
150,"Table 2 provides an example showing the distinction between our preference based measure and -nDCG based on the user profiles in Table 1. The document utilities are estimated by obtaining the preference judgements for all documents from all three users. We would expect the users' preferences to be consistent with their information need, for example user A would prefer d1 and d2 consistently to other documents that are not relevant to their needs (but relevant to other needs). Notice that -nDCG weighs all subtopics equally but the preference measure takes into account the dependency between the subtopics.",null,null
151,4. EXPERIMENT DESIGN,null,null
152,"In Section 3.2, we proposed various evaluation measures based on a user model for novelty and diversity. Evaluation of the proposed metrics is challenging since there is no ground truth to compare to; there are only other measures. Approaches used in the past to validate newly introduced metrics include comparing the proposed measure to existing measures or click metrics [18, 11]; using user preferences to compare the metrics [24]; and evaluating the metric on various properties such as discriminative power [21]. While each of these approaches have their own advantages, we argue that comparison of existing measures to our measures using simulated data is suitable for this work.",null,null
153,"Remember, our goal is to build evaluation measures for our preference based framework that assigns utility scores to a document based on user preferences. In reality, user preferences are based on various implicit factors that include",null,null
154,417,null,null
155,documents,null,null
156,a,null,null
157,subtopics bcd,null,null
158,e,null,null
159,user A,null,null
160,d1 d2,null,null
161,user B,null,null
162,d3 d4,null,null
163,user C,null,null
164,d5 d6,null,null
165,List1 d1 d2 d3 1.0 0.9,null,null
166,List2 d1 d3 d5 1.0 1.0,null,null
167,-nDCG Preference Measure,null,null
168,"Table 2: Synthetic example with 6 documents and 5 subtopics. The first ranked list does not satisfy all users where as the second one does but both rankings are scored by equally by -nDCG, while the preference metrics are able to distinguish the difference.",null,null
169,"subtopic relevance as well as many other properties. Since prior work [8] has suggested that presence of subtopics in a document plays a major role in user preferences, we believe it is important to validate our measures when user preferences are based solely on subtopic information. We therefore rely on the existing data with subtopic information to simulate user preferences.",null,null
170,4.1 Data,null,null
171,"In our experiments, we used the ClueWeb09 dataset3 consisting of one billion web pages (5 TB compressed, 25 TB uncompressed), in ten languages, crawled in January and February 2009. A subset of this collection with only English documents was used for the diversity task at TREC in 2009/10/11 [14]. A total of 150 queries have been developed and judged for the TREC Web track; the number of subtopics for each ranges from 3 to 8. For the diversity task, subtopic level judgments are available for each subtopic indicating the relevance of a document to each subtopic along with the general topical relevance. We also acquired the experimental runs submitted to TREC each year by Web track participants. A total of 48 systems were submitted by 18 groups in 2009, 32 system by 12 groups in 2010, and 62 systems by 16 groups in 2011.",null,null
172,4.2 Simulation of Users and Preferences,null,null
173,"In order to verify and compare our metrics against existing measures, we acquire preferences by simulating them from subtopic relevance information. These will be based on the preferences of simulated users that are modeled by groupings of subtopics (as in Table 1). In this way we use only data that is provided as part of the TREC collection, and therefore achieve the fairest and most reproducible possible comparison between evaluation measures. In reality, our measure is well-suited for crowd-sourced assessments in a way that other measures are not, but we save that experiment for future work.",null,null
174,"We created our user profiles by generating search scenarios for each query and marking subtopics relevant to the scenario. In Section 3, we explained our reasoning behind the user profiles in Table 1 for the query air travel information; we use the same approach to obtain the user profiles for all TREC queries. The user profiles were created by the authors of this paper and have been made available for public download at http://ir.cis.udel.edu/~ravichan/ data/profiles.tar. In addition, there is a mega-user that we refer to as the ""TREC profile""; this user is equally interested in all subtopics.",null,null
175,3http://lemurproject.org/clueweb09.php,null,null
176,"These profiles are used to determine the outcome of preferences. For simple pairwise preferences, we always prefer the document with greater number of subtopics relevant to the user profile. In the case of a tie, we make a random choice between the left or right document. For conditional preferences, we have three documents (left, right, and top); between the left and the right, we prefer the document that contains the greater number of subtopics relevant to the user profile and not present in the top document. Preference judgments obtained this way are used to compute our preference measure. Finally, using the ""TREC profile"" to simulate preferences for our measure offers the most direct comparison to other measures.",null,null
177,5. ANALYSIS,null,null
178,"We have presented a family of preference-based measures for evaluating systems based on novelty and diversity, and outlined the advantages of our metrics over existing subtopicbased measures. In this section, we demonstrate how our metrics take into account the presence of subtopics implicitly by comparing them with -nDCG, ERR-IA, and s-recall.",null,null
179,5.1 System Ranking Comparisons,null,null
180,5.1.1 System Performance,null,null
181,"We evaluated all experimental runs submitted to TREC in 2009, 2010, and 2011 using our proposed measure with three different stopping probabilities P (k) and two different utility aggregation functions F (). Figure 2 shows the performance of systems with respect to both -nDCG and our preference measure computed with PRBP (k) and Favg() functions and preferences simulated using the ""TREC profile"". Each point represents a TREC participant system; they are ordered on the x-axis by -nDCG. Black circles give -nDCG values as computed by the ndeval utility used for the Web track; blue x's indicate the preference measure score for the same system. In these figures we can see that the preference measure is roughly on the same scale as -nDCG, though typically 0.1 - 0.2 lower in an absolute sense.",null,null
182,Each increase or drop in the position of x's indicates disagreement with -nDCG. The increasing trend of the curves in Figure 2 indicates that the correlation between the preference measure and -nDCG is high. A similar trend was observed while using different P (k) and F () functions as well (not shown). Both -nDCG and our preference measure agree on the top ranked system in 2009 and 2010.,null,null
183,We analyzed the reason behind disagreement by carefully looking at the actual ranked lists. We investigated how nDCG and our proposed measures reward diversified sys-,null,null
184,418,null,null
185,ERR-IA@20 s-recall@20,null,null
186,-nDCG@20,null,null
187,0.893,null,null
188,0.828,null,null
189,ERRIA@20,null,null
190,-,null,null
191,0.739,null,null
192,Table 3: Kendall's  correlation values between the existing evaluation measures. Values were computed using 48 submitted runs in TREC 2009 dataset.,null,null
193,"tems on a per topic basis. Based on our analysis, the major reason for disagreement is that -nDCG penalizes systems that miss documents containing many unique subtopics more harshly than the preference measure does. Much of the variance in -nDCG scores is due to differences in rank position of the documents with the greatest number of unique subtopics. In practice, this explains the lower scores returned by the preference measure as well.",null,null
194,5.1.2 Rank Correlation Between Measures,null,null
195,"We measure the stability of our metrics using Kendall's  by ranking the experimental runs under different effectiveness measures. Kendall's  ranges from -1 (lists are reversed) to 1 (lists are exactly the same), with 0 indicating essentially a random reordering. Prior work suggest that a  value of 0.9 or higher between a pair of rankings indicates high similarity between rankings while a value of 0.8 or lower indicates significant difference [3].",null,null
196,"Figure 3 summarizes the rank correlations between existing subtopic-based metrics and our proposed preference metric using all three P (k) (plus using no P (k) at all-- equivalent to a uniform stopping probability) and both F () functions, simulating preferences with the ""TREC profile"". The correlations are fairly high across TREC datasets, P (k) functions, and F () functions. The PDCG(k) rank function fares worst, with correlations dipping quite a bit for the 2010 data in particular. Subtopic recall is a very simple non-rank based metric for diversity and thus the Kendall's  values are expected to be slightly lower.",null,null
197,"For comparison, Table 3 shows the Kendall's  correlation values between -nDCG, ERR-IA and s-recall. These correlations are similar to those in Figure 3, suggesting that the ranking of systems given by our preference measure varies no more than the rankings of systems given by any two standard measures.",null,null
198,"There is almost no difference between the correlations for Favg() and Fmin() functions for aggregating utility. In fact, the correlation between preference measures computed with those two is nearly 1. Thus we can conclude that the choice of F () (between those two options) does not matter. There is a great deal of difference depending on choice of P (k), however, and thus this is a decision that should be made carefully based on the observed behavior of users.",null,null
199,5.2 Evaluating Multiple User Profiles,null,null
200,"The experiments above are based on the ""TREC profile"", a user profile that considers every subtopic to be equally relevant. In this experiment, we demonstrate the ability of our methods to handle multiple, more realistic user profiles and show the stability of our metrics. Measures based on absolute subtopic judgments cannot naturally incorporate multiply-judged documents. One must average judgments, or take a majority vote, or use some other scheme. In contrast, judgments from multiple users can be incorporated",null,null
201,"easily into our preference framework in the estimation of document utilities, as the document utility is simply the ratio of number of times a document was preferred to the number of times it appeared in a pair, regardless of which user or assessor happened to see it.",null,null
202,"We simulate preferences for each of our user profiles for each topic in the TREC set. We compute the preference measure using each profile's preferences separately (giving at least three separate values for each system: one for each user profile), and then use the full set of preferences obtained to compute a single value of the measure. Note that the latter case is not the same as computing the preference measure with the ""TREC profile"": the TREC profile user uses all subtopics to determine the outcome of a preference, while individual users would never use a subtopic that is not relevant to them to determine the outcome of a preference.",null,null
203,"We can also compute subtopic-based measures such as -nDCG against our profiles. To do this, we simply assume that only the subtopics that are relevant to the profile ""count"" in the measure computation. We will compare values of measures computed this way to our preference measures.",null,null
204,"Our hypothesis for this experiment is twofold: 1) that the preference measure computed for a single profile will correlate well to subtopic-based measures computed against the same profile; 2) that the preference measure computed with preferences from all profiles will not be the same as an average of the individual profile measures, and also not the same as subtopic-based measures computed as usual. In other words, that the preference measure based on preferences from many different users is measuring something different than the preference measure based on preferences from one user, and also different from the subtopic measures.",null,null
205,"Figure 4 shows the results of evaluating systems using user profile 1, 2, and 3 for each topic and averaging over topics (note that the user profile number is arbitrary; there is nothing connecting user profile 1 for topic 100 to user profile 1 for topic 110). We can see that the system ranking changes for both -nDCG and the preference measure, as expected. The correlation between the two remains high: 0.83, 0.88, and 0.82 for user profile 1, 2, and 3 respectively. This is in the same range of correlation values that we saw in Figure 3, and supports the first part of our hypothesis.",null,null
206,"Figure 5 shows the results of evaluating systems with all user profiles, comparing to the evaluation with the TREC profile and with -nDCG computed with all subtopics. Note here that all three rankings are different, as evidenced by the  correlations reported in the inset tables. This supports the second part of our hypothesis: that allowing many different users the opportunity to express their preferences can result in a different ranking of systems than treating all assessors as equivalent, as the TREC profile and -nDCG do.",null,null
207,5.3 Incomplete Judgments,null,null
208,The test collection procedure discussed in Section 3.1 requires two sets of judgments: pairwise and conditional preferences. The number of pairwise judgments increases quadratically with increase in number of documents in the pool; it is not feasible to collect a complete set of preferences. We envision that our measure would always be computed with incomplete judgments. For this experiment we test the stability of our measures by comparing the system rankings obtained by using all preference judgments against a set of incomplete judgments.,null,null
209,419,null,null
210,Figure 2: TREC 09/10/11 diversity runs evaluated with our preference based metric at rank 20 (nPrf@20) with PRBP and FAverage. Compare to -nDCG scores.,null,null
211,"Figure 3: Kendall's  correlation values between our proposed measures and -nDCG, ERR-IA, s-recall. Values were computed using the submitted runs in the TREC 2009/10/11 dataset. The scores for various P (k) and F() are shown.",null,null
212,"To do this, we randomly select N triplets of documents for each query. For each triplet, one document is randomly selected to be the ""top"" document that the other two would be judged conditional on. Though we do not explicitly obtain simple pairwise preferences, we expect that there will",null,null
213,be enough cases in which the top document is not relevant to the user profile that they must fall back on a simple pairwise comparison. We then sample 5 user profiles (with replacement) from those defined for the topic and simulate their preferences for the triplet. In this way we obtain 5N prefer-,null,null
214,420,null,null
215,"Figure 5: Comparison between -nDCG, our preference measure computed using the TREC profile, and our preference measure computed using a mix of user profiles. Note that all three rankings, while similar, have substantial differences as well.",null,null
216,0.0 0.1 0.2 0.3 0.4 0.5,null,null
217,Performance Scores 0.0 0.1 0.2 0.3 0.4 0.5,null,null
218,TREC 2009,null,null
219,nPrf@20 G -nDCG@20,null,null
220,"Kendall's Tau , 0.775",null,null
221,G,null,null
222,G,null,null
223,G,null,null
224,G,null,null
225,G,null,null
226,G,null,null
227,G,null,null
228,GG,null,null
229,G G,null,null
230,G,null,null
231,G,null,null
232,GGGG,null,null
233,G,null,null
234,G,null,null
235,G,null,null
236,G,null,null
237,GG,null,null
238,G,null,null
239,GGGGGGG,null,null
240,GG,null,null
241,GGG,null,null
242,G,null,null
243,G,null,null
244,G,null,null
245,G,null,null
246,G,null,null
247,GG,null,null
248,G,null,null
249,G,null,null
250,G G,null,null
251,G,null,null
252,0,null,null
253,10,null,null
254,20,null,null
255,30,null,null
256,40,null,null
257,nPrf@20 G -nDCG@20,null,null
258,"Kendall's Tau , 0.828",null,null
259,G G GGGG,null,null
260,G,null,null
261,GGG,null,null
262,G,null,null
263,G,null,null
264,G,null,null
265,G,null,null
266,G,null,null
267,G G,null,null
268,GG,null,null
269,GGG,null,null
270,G,null,null
271,G,null,null
272,G,null,null
273,GGGGGGG,null,null
274,G,null,null
275,G,null,null
276,G,null,null
277,G,null,null
278,G,null,null
279,G,null,null
280,G,null,null
281,G,null,null
282,G,null,null
283,G,null,null
284,G,null,null
285,G,null,null
286,G,null,null
287,GG,null,null
288,G,null,null
289,0,null,null
290,10,null,null
291,20,null,null
292,30,null,null
293,40,null,null
294,nPrf@20 G -nDCG@20,null,null
295,"Kendall's Tau , 0.816",null,null
296,G GG,null,null
297,G,null,null
298,G,null,null
299,GG,null,null
300,G,null,null
301,G,null,null
302,G,null,null
303,G,null,null
304,G,null,null
305,G,null,null
306,G,null,null
307,G,null,null
308,G G,null,null
309,G,null,null
310,G,null,null
311,G,null,null
312,G,null,null
313,G,null,null
314,G,null,null
315,G,null,null
316,G,null,null
317,GGGGGG,null,null
318,G,null,null
319,G,null,null
320,G,null,null
321,G,null,null
322,G,null,null
323,G,null,null
324,G,null,null
325,G,null,null
326,G,null,null
327,G,null,null
328,G,null,null
329,G,null,null
330,G,null,null
331,G,null,null
332,G,null,null
333,G,null,null
334,G,null,null
335,0,null,null
336,10,null,null
337,20,null,null
338,30,null,null
339,40,null,null
340,Systems ordered by -nDCG@20 using TREC QRELS,null,null
341,0.0 0.1 0.2 0.3 0.4 0.5,null,null
342,"Figure 4: Comparison between -nDCG and our preference measure computed against user profiles 1 (top), 2 (middle), and 3 (bottom) for TREC 2009 systems.",null,null
343,"ences for each topic in a similar way as would be done in a real crowd-sourced assessment. We use those preferences to compute our measure, then compute the correlation to the measure computed with all available preferences. We repeat this 10 times for each topic, measure the correlation each time, and average the correlations.",null,null
344,"Figure 6 shows the correlation between the system rankings when evaluated using complete judgements and increasing numbers of preferences. Correlation tends to increase as the number of preferences increases, though it does not reach 0.9. This may be partly because user profiles are not evenly represented in the preferences (which is in fact more realistic than when they are, as in the full-preference case), and",null,null
345,Kendall's Tau,null,null
346,0.4,null,null
347,0.5,null,null
348,0.6,null,null
349,0.7,null,null
350,0.8,null,null
351,0.9,null,null
352,TREC 09/10/11,null,null
353,TREC 09 TREC 10 TREC 11 500 1000 1500 2000 3000 3000 4000 5000 6000 7000 Number of Sampled Triplets,null,null
354,Figure 6: TREC 09/10/11 diversity runs evaluated with our preference based metric at rank 20 (nPrf@20) with PRR and FMinimum using single assessor with complete judgments and multiple assessor with incomplete judgments.,null,null
355,"partly because our max number of preferences is still a fairly small fraction of the total number possible: even selecting triplets from only 100 documents, there are over 161,000 possible triplets, of which we have only obtained less than 5%! Thus we expect that continuing to increase the number of triplets would continue to push the correlations higher, even though we see dips in the trend (due to variance).",null,null
356,6. CONCLUSION AND FUTURE WORK,null,null
357,"In this work, we proposed a novel evaluation framework and a family of measures for IR evaluation. Our measure incorporates novelty and diversity, but can also incorporate any property that influences user preferences for one document over another. Our measure is motivated directly by",null,null
358,421,null,null
359,"a user model and has several advantage over the existing measures based on explicit subtopic judgments: it captures subtopics implicitly and at finer-grained levels, it accounts for subtopic importance and dependence as expressed by user preferences, and it requires few parameters--only a stopping probability function, for which there are several well-accepted options that can be chosen from by comparing to user log data. It correlates well with existing measures, but also clearly measures something different (which is a positive for a new measure).",null,null
360,"This framework and measure is most well-suited for assessments done by crowd-sourcing. In a crowd-sourced assessment, we would naturally have a large user base with a wide range of preferences. Over a large number of preferences, the most important subtopics and intents would naturally emerge; documents relevant to those would become the documents with the highest utility scores. Yet the conditional judgments would prevent too many documents with those subtopics from reaching the top of the ranking. The measure is designed to handle multiple judgments, disagreements in preferences, and novelty of information, and as such it is novel to the information retrieval literature.",null,null
361,"The clearest direction for future work is to perform an actual crowd-sourced assessment and determine whether our preference measure correlates better with human judgments of system performance than other measures. We plan to start this immediately. Another direction for future work is using triplets in a learning-to-rank algorithm to learn a novelty ranker. Since many learning algorithms are based on pairwise preferences, it seems a natural extension to triplets.",null,null
362,"Acknowledgments: This work was supported in part by the National Science Foundation (NSF) under grant number IIS-1017026. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsor.",null,null
363,7. REFERENCES,null,null
364,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. Proceedings of WSDM '09, page 5, 2009.",null,null
365,"[2] J. Arguello, F. Diaz, and J. Callan. Learning to aggregate vertical results into web search results. In Proceedings of CIKM '11, page 201, New York, USA, 2011. ACM Press.",null,null
366,"[3] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of SIGIR '04, page 25, New York, USA, 2004. ACM Press.",null,null
367,"[4] J. Carbonell and J. Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. Proceedings of SIGIR '98, pages 335­336, 1998.",null,null
368,"[5] B. Carterette. An analysis of np-completeness in novelty and diversity ranking. Information Retrieval, 14(1):89­106, Dec. 2010.",null,null
369,"[6] B. Carterette. System effectiveness, user models, and user utility. In Proceedings of SIGIR '11, page 903, New York, USA, 2011. ACM Press.",null,null
370,"[7] B. Carterette, P. N. Bennett, D. M. Chickering, and T. Susan. Here or there preference judgments for relevance. In Proceedings of ECIR '08, pages 16­27, 2008.",null,null
371,"[8] P. Chandar and B. Carterette. Using preference judgments for novel document retrieval. Proceedings of SIGIR '12, page 861, 2012.",null,null
372,"[9] O. Chapelle, S. Ji, C. Liao, E. Velipasaoglu, L. Lai, and S.-L. Wu. Intent-based diversification of web search results: metrics and algorithms. Information Retrieval, 14(6):572­592, May 2011.",null,null
373,"[10] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. Proceeding of CIKM '09, page 621, 2009.",null,null
374,"[11] C. Clarke, N. Craswell, I. Soboroff, and A. Ashkan. A comparative analysis of cascade measures for novelty and diversity. In Proceedings of WSDM '11, pages 75­84. ACM, 2011.",null,null
375,"[12] C. Clarke, M. Kolla, and O. Vechtomova. An effectiveness measure for ambiguous and underspecified queries. Advances in Information Retrieval Theory, pages 188­199, 2010.",null,null
376,"[13] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. Proceedings of SIGIR '08, page 659, 2008.",null,null
377,"[14] C. L. A. Clarke, N. Craswell, I. Soboroff, and E. M. Voorhees. Overview of the trec 2011 web track. In Proceedings of The Eighteenth Text REtrieval Conference TREC, pages 1­9, Gaithersburg, Maryland, 2011. NIST.",null,null
378,"[15] T. Leelanupab, G. Zuccon, and J. M. Jose. A query-basis approach to parametrizing novelty-biased cumulative gain. In Proceedings of the Third international conference on Advances in information retrieval theory, ICTIR '11, pages 327­331. Springer-Verlag, 2011.",null,null
379,"[16] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Transactions on Information Systems, 27(1):1­27, Dec. 2008.",null,null
380,"[17] P. Over. Trec-6 interactive track report. In The Sixth Text Retrieval Conference (TREC-6), pages 57­64, 1998.",null,null
381,"[18] F. Radlinski, M. Kurup, and T. Joachims. How does clickthrough data reflect retrieval quality? In Proceedings of CIKM '08, pages 43­52, New York, USA, 2008. ACM.",null,null
382,"[19] D. Rafiei, K. Bharat, and A. Shukla. Diversifying web search results. In Proceedings of WWW O~10, pages 781­790, New York, USA, Apr. 2010. ACM.",null,null
383,"[20] M. E. Rorvig. The simple scalability of documents. Journal of the American Society for Information Science, 41(8):590­598, Dec. 1990.",null,null
384,"[21] T. Sakai. Evaluating evaluation metrics based on the bootstrap. In Proceedings of SIGIR '06, pages 525­532, New York, USA, 2006. ACM.",null,null
385,"[22] T. Sakai, N. Craswell, R. Song, S. Robertson, Z. Dou, and C. Lin. Simple evaluation metrics for diversified search results. In Proceedings of the 3rd International Workshop on Evaluating Information Access (EVIA), 2010.",null,null
386,"[23] T. Sakai and R. Song. Evaluating diversified search results using per-intent graded relevance. In Proceedings of SIGIR '11, pages 1043­1052. ACM, 2011.",null,null
387,"[24] M. Sanderson, M. Paramita, P. Clough, and E. Kanoulas. Do user preferences and evaluation measures line up? In Proceeding of SIGIR '10, pages 555­562. ACM, 2010.",null,null
388,"[25] R. L. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. Proceedings of WWW '10, page 881, 2010.",null,null
389,"[26] E. M. Voorhees and H. T. Dang. Overview of the trec 2005 question answering track. In In TREC 2005, 1999.",null,null
390,"[27] C. Zhai, W. Cohen, and J. Lafferty. Beyond independent relevance: methods and evaluation metrics for subtopic retrieval. In Proceedings of SIGIR '03, pages 10­17. ACM, 2003.",null,null
391,"[28] W. Zheng, X. Wang, H. Fang, and H. Cheng. Coverage-based search result diversification. Information Retrieval, 2011.",null,null
392,422,null,null
393,,null,null

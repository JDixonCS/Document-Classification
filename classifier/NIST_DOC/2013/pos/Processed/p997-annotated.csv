,sentence,label,data
,,,
0,Effectiveness/Efficiency Tradeoffs for Candidate Generation in Multi-Stage Retrieval Architectures,null,null
,,,
1,"Nima Asadi1,2, Jimmy Lin3,2,1",null,null
,,,
2,"1Dept. of Computer Science, 2Institute for Advanced Computer Studies, 3The iSchool University of Maryland, College Park",null,null
,,,
3,"nima@cs.umd.edu, jimmylin@umd.edu",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"This paper examines a multi-stage retrieval architecture consisting of a candidate generation stage, a feature extraction stage, and a reranking stage using machine-learned models. Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs with three candidate generation approaches: postings intersection with SvS, conjunctive query evaluation with Wand, and disjunctive query evaluation with Wand. We find no significant differences in end-to-end effectiveness as measured by NDCG between conjunctive and disjunctive Wand, but conjunctive query evaluation is substantially faster. Postings intersection with SvS, while fast, yields substantially lower end-to-end effectiveness, suggesting that document and term frequencies remain important in the initial ranking stage. These findings show that conjunctive Wand is the best overall candidate generation strategy of those we examined.",null,null
,,,
6,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval,null,null
,,,
7,"General Terms: Algorithms, Experimentation",null,null
,,,
8,Keywords: query evaluation; postings intersection,null,null
,,,
9,1. INTRODUCTION,null,null
,,,
10,"One possible architecture for web retrieval breaks document ranking into three stages: candidate generation, feature extraction, and document reranking. There are two main reasons for this multi-stage design. First, there is a general consensus that learning to rank provides the best solution to document ranking [13, 12]. As it is difficult to apply machine-learned models over the entire collection, in practice a candidate list of potentially-relevant documents is first generated. Thus, learning to rank is actually a reranking problem (hence the first and third stages). Second, separating candidate generation from feature extraction has the advantage of providing better control over cost/quality tradeoffs. For example, term proximity features are significantly more costly to compute than unigram features; therefore, by",null,null
,,,
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
12,"decoupling the first two stages in the architecture, systems can exploit ""cheap"" features to generate candidates quickly and only compute ""expensive"" term proximity features when necessary--thus decreasing overall query evaluation latency.",null,null
,,,
13,"Given a fixed set of features and a learning-to-rank model, we explore effectiveness/efficiency tradeoffs in the candidate generation stage, comparing postings intersection with SvS, conjunctive query evaluation with Wand, and disjunctive query evaluation with Wand. Previous work has suggested that conjunctive query evaluation yields early precision results that are at least as good as disjunctive query evaluation, but is much faster. We experimentally confirm this observation, and additionally show that postings intersection (with results sorted by spam scores) yields substantially worse output within the same framework.",null,null
,,,
14,"The contribution of this work is an empirical evaluation of common candidate generation algorithms in a multi-stage retrieval architecture. By fixing the feature generation and reranking stages, we are able to isolate the end-to-end effectiveness and efficiency implications of different algorithms.",null,null
,,,
15,2. BACKGROUND AND RELATED WORK,null,null
,,,
16,"We begin with a more precise specification of our threestage architecture, illustrated in Figure 1. The input to the candidate generation stage is a query Q and the output is a list of k document ids {d1, d2, ...dk}. In principle, this can be considered a sorted list, but that detail is unimportant here. These document ids serve as input to the feature extraction stage, which returns a list of k feature vectors {f1, f2, ...fk}, each corresponding to a candidate document. These serve as input to the third document reranking stage, which typically applies a machine-learned model to produce a final ranking.",null,null
,,,
17,"This multi-stage retrieval architecture has recently been explored by many researchers. Some have examined the entire pipeline; for example, Macdonald et al. [14] assessed the impact of variables such as the number of candidate documents and the objective metric to use when training the learning-to-rank model (however, they did not explore effectiveness/efficiency tradeoffs with candidate generation). Others have specifically looked at candidate generation, e.g., postings intersection on multi-core architectures [17], dynamic pruning [18], and approximate techniques [3]. There has been some work focused on the feature extraction stage, exploring how to best represent positional information; two studies have independently come to the conclusion that it is advantageous to store document positions in a document vector representation distinct from the inverted index [1, 2]. Finally, most work on learning to rank [13, 12] assumes such",null,null
,,,
18,997,null,null
,,,
19,Relevance,null,null
,,,
20,d1,null,null
,,,
21,d2,null,null
,,,
22,d3,null,null
,,,
23,Q,null,null
,,,
24,Candidate Generation,null,null
,,,
25,d4 d5 d6,null,null
,,,
26,Feature Extraction,null,null
,,,
27,d7,null,null
,,,
28,...,null,null
,,,
29,dk,null,null
,,,
30,f1,null,null
,,,
31,dr1,null,null
,,,
32,f2,null,null
,,,
33,dr2,null,null
,,,
34,f3,null,null
,,,
35,dr3,null,null
,,,
36,f4 f5 f6,null,null
,,,
37,Document Reranking,null,null
,,,
38,dr4 dr5 dr6,null,null
,,,
39,f7,null,null
,,,
40,dr7,null,null
,,,
41,...,null,null
,,,
42,...,null,null
,,,
43,fk,null,null
,,,
44,drk,null,null
,,,
45,Candidates,null,null
,,,
46,Features,null,null
,,,
47,Final Results,null,null
,,,
48,"Figure 1: Illustration of a multi-stage retrieval architecture with distinct candidate generation, feature extraction, and document reranking stages.",null,null
,,,
49,"an architecture, because the input to the machine-learned model is a set of feature vectors. Occasionally, this fact is made explicit: for example, Cambazoglu et al. [6] experimented with reranking 200 candidate documents to produce the final ranked list of 20 results.",null,null
,,,
50,"There are two general approaches to candidate generation: conjunctive and disjunctive query processing. In the first, only documents that contain all query terms are considered, whereas in the second, any document with at least one query term is potentially retrievable. One popular algorithm is Wand [4], which can operate in either conjunctive or disjunctive mode with respect to a particular scoring model (e.g., BM25). Wand uses a pivot-based pointer movement strategy to avoid needlessly evaluating documents that cannot be in the final top k ranking. An alternative approach to candidate generation involves simple postings intersection, without reference to a particular scoring model. Culpepper and Moffat [8] demonstrated SvS to be the best algorithm for accomplishing this. Note that since SvS works by iteratively intersecting the current result set with the next shortest postings list, it must exhaustively compute the intersection set. In this paper we explore both Wand and SvS for candidate generation.",null,null
,,,
51,"It is a well-known fact that disjunctive query processing is much slower than conjunctive processing. However, it is unclear what impact output quality at the candidate generation stage has on end-to-end effectiveness. Although Broder et al. [4] found conjunctive processing to yield higher early precision, the results were not in the context of learningto-rank experiments. We hypothesize that end-to-end effectiveness is relatively insensitive to candidate generation quality, due to an emphasis on early precision in most web search tasks today--we simply need to make sure there are enough relevant documents for the machine-learned model to identify. Thus, there is an interesting possibility that conjunctive query processing might lead to both good and fast results. Our paper explores this hypothesis.",null,null
,,,
52,3. EXPERIMENTAL SETUP,null,null
,,,
53,3.1 Candidate Generation Algorithms,null,null
,,,
54,"We examined four approaches to candidate generation, outlined below. For each, we varied the number of candidates generated k, where k  {100, 250, 500, 1000}.",null,null
,,,
55,"SvSSpam. Since SvS must compute the entire intersection set, to obtain k documents, we sort the intersection results by a static prior and return the top k.1",null,null
,,,
56,1One caveat worth mentioning: the common trick to renumber docids based on the static prior doesn't help much here since we need to compute the complete intersection set regardless; it would save the final sort by score but time spent on that is negligible.,null,null
,,,
57,"SvSBM25. In this approach, we first compute the full intersection set, and then in a second pass we compute BM25 scores for every document in the intersection set. After the second pass, the top k documents are returned.",null,null
,,,
58,WandCon. We evaluate the query in conjunctive mode and return the top k documents based on BM25.,null,null
,,,
59,WandDis. We evaluate the query in disjunctive mode and return the top k documents based on BM25.,null,null
,,,
60,"All algorithms were implemented in C and an equal amount of time was spent optimizing each to ensure a fair comparison. All conditions used the same (non-positional) inverted index (even though SvSSpam does not need access to tf's) and for SvSBM25 we modified the accumulators to hold the tf's extracted from the postings for the second-pass scoring. We assumed that all index structures are retained in memory, so query evaluation never involves hitting disk. Experiments were performed on a server running Red Hat Linux, with dual Intel Xeon ""Westmere"" quad-core processors (E5620 2.4GHz) and 128GB RAM.",null,null
,,,
61,3.2 Test Collections and Metrics,null,null
,,,
62,"We performed experiments on the ClueWeb09 collection, a best-first web crawl from early 2009. Our experiments used only the first English segment, which has 50 million documents (247GB compressed). The Waterloo spam scores [7] were used as the static priors for SvS reranking.",Y,null
,,,
63,"For evaluation, we used three different sets of queries: first, the TREC 2005 terabyte track ""efficiency"" queries (50,000 queries total).2 Since there are no relevance judgments for these queries, they were used solely for efficiency experiments. Second, a set of 100,000 queries sampled randomly from the AOL query log [16]. Our sample retains the query length distribution of the original dataset. Similar to the TREC 2005 terabyte track queries, we used these queries only to evaluate efficiency.",Y,null
,,,
64,"Finally, we used the TREC web track topics from 2009­ 2011, 150 in total. These topics comprise a complete test collection in that we have relevance judgments, but there are too few queries for meaningful efficiency experiments. We performed five fold cross validation, using three folds for training, one for validation, and one for testing.",Y,null
,,,
65,"We collected two figures of merit: the first was end-to-end effectiveness in terms of NDCG. For efficiency, we measured query latency of the candidate generation stage, defined as the elapsed time between the moment a query is presented to the system and the time when top k candidates are retrieved and forwarded to the feature extraction stage. To capture variance, we repeated runs five times.",null,null
,,,
66,3.3 Features and Ranking Model,null,null
,,,
67,"We used a standard suite of features very similar to those described in previous work [15, 18]. They consist of basic information retrieval scores (e.g., language modeling and BM25 scores), term proximity features (exact phrases, ordered windows, unordered windows), query-independent features (e.g., PageRank, content quality score [7], etc.). Relevant features were computed across multiple fields: the entire document, anchor text, as well as title fields. In total, there are 91 features.3",null,null
,,,
68,"2http://www-nlpir.nist.gov/projects/terabyte/ 3The complete test collection, including feature descriptions, can be found at https://github.com/lintool/ClueWeb09-TREC-LTR.",Y,null
,,,
69,998,null,null
,,,
70,Dis. Con. Spam Number of documents,null,null
,,,
71,Model,null,null
,,,
72,BM25 Linear -MART,null,null
,,,
73,BM25 Linear -MART,null,null
,,,
74,BM25 Linear -MART,null,null
,,,
75,NDCG@1 100 250 500 1000,null,null
,,,
76,0.06 0.09 0.09 0.11 0.06 0.08 0.10 0.14 0.05 0.09 0.10 0.15,null,null
,,,
77,0.25 0.25 0.25 0.25 0.26 0.25 0.25 0.24 0.26 0.25 0.24 0.25,null,null
,,,
78,0.25 0.25 0.25 0.25 0.26 0.25 0.25 0.23 0.26 0.25 0.25 0.24,null,null
,,,
79,100,null,null
,,,
80,0.05 0.05 0.05,null,null
,,,
81,0.22 0.25 0.28,null,null
,,,
82,0.22 0.25 0.28,null,null
,,,
83,NDCG@5 250 500,null,null
,,,
84,0.06 0.08 0.06 0.08 0.07 0.09,null,null
,,,
85,0.22 0.22 0.24 0.25 0.27 0.25,null,null
,,,
86,0.22 0.22 0.24 0.24 0.27 0.26,null,null
,,,
87,1000,null,null
,,,
88,0.10 0.10 0.11,null,null
,,,
89,0.22 0.23 0.25,null,null
,,,
90,0.22 0.23 0.25,null,null
,,,
91,100,null,null
,,,
92,0.04 0.03 0.03,null,null
,,,
93,0.21 0.25 0.26,null,null
,,,
94,0.21 0.25 0.26,null,null
,,,
95,NDCG@20 250 500,null,null
,,,
96,0.05 0.06 0.05 0.07 0.05 0.06,null,null
,,,
97,0.21 0.21 0.24 0.23 0.25 0.24,null,null
,,,
98,0.21 0.21 0.24 0.23 0.25 0.24,null,null
,,,
99,1000,null,null
,,,
100,0.08 0.08 0.08,null,null
,,,
101,0.21 0.23 0.23,null,null
,,,
102,0.21 0.23 0.23,null,null
,,,
103,100,null,null
,,,
104,0.027 0.027 0.027,null,null
,,,
105,0.230 0.248 0.250,null,null
,,,
106,0.230 0.248 0.250,null,null
,,,
107,NDCG 250 500,null,null
,,,
108,0.044 0.045 0.045,null,null
,,,
109,0.060 0.060 0.060,null,null
,,,
110,0.280 0.304 0.297 0.317 0.300 0.318,null,null
,,,
111,0.281 0.308 0.298 0.320 0.301 0.321,null,null
,,,
112,1000,null,null
,,,
113,0.077 0.079 0.079,null,null
,,,
114,0.327 0.340 0.340,null,null
,,,
115,0.331 0.344 0.344,null,null
,,,
116,"Table 1: NDCG at different cutoffs using different candidate generation algorithms: SvSSpam (Spam), SvSBM25",null,null
,,,
117,"and WANDCon (Con.), and WANDDis (Dis.). Vertical blocks show each metric, and columns show the number of candidate documents retrieved.  indicates statistical significance vs. BM25 using the t-test (p<0.05).",null,null
,,,
118,"For the third stage reranking model, we used two different learning-to-rank techniques. In the first, we trained a linear model using the greedy feature selection approach [15]. The model is iteratively constructed by adding features, one at a time, according to a greedy selection criterion. During each iteration, the feature that provides the biggest gain in effectiveness (as measured by NDCG [11]) after being added to the existing model is selected. This yields a sequence of one-dimensional optimizations that can easily be solved using line search techniques. The algorithm stops when the difference in NDCG between successive iterations drops below a given threshold (10-4). This training procedure is simple, fast, and fairly effective.",null,null
,,,
119,"Separately, we learned a LambdaMART model [5] using the open-source jforests implementation4 [10]. To tune parameters, we used grid search as suggested by the authors and selected the parameter setting that results in the largest gain in NDCG on the validation set: max number of leaves (9), feature and data sub-sampling (0.3), minimum observations per leaf (0.75), and the learning rate (0.05).",null,null
,,,
120,4. RESULTS,null,null
,,,
121,"Table 1 summarizes NDCG at different rank cutoffs and no cutoff for all combinations of candidate generation and reranking models. The first horizontal block of the table labeled ""Spam"" refers to SvSSpam, ""Con."" indicates SvSBM25 or WandCon (as both algorithms produce the same results), and ""Dis."" shows the use of WandDis. Within each block of the table, ""BM25"" indicates reranking candidates with BM25, which serves as the baseline (note this only alters the postings intersection candidate results; in the other cases, this is equivalent to a reranker that does nothing); ""Linear"" is the linear model learned using Metzler's greedy-feature selection method; and ""-MART"" is LambdaMART. Each column shows the number of candidate documents retrieved.",null,null
,,,
122,"Candidates generated using SvSSpam yield very low endto-end effectiveness. This shows that combining postings intersection and with a static prior does not provide a sufficiently discriminative signal to include relevant documents in the top k, where k is small relative to the size of the collection. The quality of the static prior is not to blame here, as the Waterloo spam scores have been demonstrated to be very helpful in reranking [7]. Of course, as we increase the size of k, the results of SvSSpam will approach that of the other candidate generation algorithms, but at the cost of more time spent performing feature extraction. It is clear",null,null
,,,
123,4http://code.google.com/p/jforests/,null,null
,,,
124,300 No judgments,null,null
,,,
125,Non-relevant,null,null
,,,
126,250,null,null
,,,
127,Relevant,null,null
,,,
128,200,null,null
,,,
129,150,null,null
,,,
130,100,null,null
,,,
131,50,null,null
,,,
132,0,null,null
,,,
133,1000 500 250 100,null,null
,,,
134,1000 500 250 100,null,null
,,,
135,Linear,null,null
,,,
136,K,null,null
,,,
137,#NAME?,null,null
,,,
138,"Figure 2: Number of rel, non-rel, and unjudged documents in the top 20 for different candidate sizes using WANDDis (averaged across folds).",null,null
,,,
139,that both term frequencies and document frequencies cannot be ignored in candidate generation.,null,null
,,,
140,"On the other hand, WandCon (and SvSBM25, which produces exactly the same results) yields NDCG scores that are statistically indistinguishable from those produced by WandDis (with no cutoff, WandDis yields slightly higher scores, but the differences are not statistically significant). In other words, with BM25 scoring, conjunctive candidate generation and disjunctive candidate generation are equally good from the end-to-end effectiveness perspective. This finding is consistent with the results of Broder et al. [4] (but in a learning-to-rank context).",null,null
,,,
141,"Increasing k improves NDCG at all rank cutoffs for candidates obtained by SvSSpam. We observe the same trend for NDCG without cutoff in all settings. This is expected since larger k translates into higher recall at the candidate generation stage, i.e., more relevant documents are available to the reranker. However, with conjunctive and disjunctive query processing we see a decline in NDCG at rank cutoffs 5 and 20 as we increase the number of candidate documents; this trend is consistent for both the linear model and LambdaMART. To examine this effect in a bit more detail, we computed the number of relevant, non-relevant, and unjudged documents in the top 20 obtained by reranking WandDis. Figure 2 shows these statistics for different values of k. We see that as k increases, more unjudged documents find their way to the top 20, while both the number of relevant and non-relevant documents decreases. Thus, this trend appears to be an artifact of the test collection and not a property of the candidate generation algorithms.",null,null
,,,
142,999,null,null
,,,
143,Time (ms) Time (ms),null,null
,,,
144,140,null,null
,,,
145,SvS (Spam) WAND-Conjunctive,null,null
,,,
146,SvS (BM25) WAND-Disjunctive,null,null
,,,
147,120,null,null
,,,
148,100,null,null
,,,
149,80,null,null
,,,
150,60,null,null
,,,
151,40,null,null
,,,
152,20,null,null
,,,
153,400 SvS (Spam) WAND-Conjunctive,null,null
,,,
154,350,null,null
,,,
155,SvS (BM25) WAND-Disjunctive,null,null
,,,
156,300,null,null
,,,
157,250,null,null
,,,
158,200,null,null
,,,
159,150,null,null
,,,
160,100,null,null
,,,
161,50,null,null
,,,
162,100,null,null
,,,
163,250,null,null
,,,
164,500,null,null
,,,
165,1000,null,null
,,,
166,100,null,null
,,,
167,250,null,null
,,,
168,500,null,null
,,,
169,1000,null,null
,,,
170,Number of Candidates,null,null
,,,
171,Number of Candidates,null,null
,,,
172,(a) Terabyte,null,null
,,,
173,(b) AOL,null,null
,,,
174,Figure 3: Average candidate generation per-query latency across five trials.,null,null
,,,
175,"Figure 3 shows per-query latency of the candidate generation stage for different values of k. On average, SvSSpam is the fastest since the ranking function is query independent: score computation boils down to table lookups of spam scores. We see that SvSBM25 is on par with WandCon for larger k values. SvSBM25 retrieves the full intersection set and computes scores for every document in the set regardless of k; therefore, increasing k has no impact on latency. On the other hand, WandCon slows with increasing k (but is faster with a small k). Conjunctive query evaluation with Wand can only terminate when a postings list is fully consumed. However, this termination is mostly independent of k; the only factor affected by k is the set of heap operations performed during query evaluation. Finally, WandDis is not only the slowest overall, but latency grows with larger values of k faster than with conjunctive evaluation.",null,null
,,,
176,5. CONCLUSIONS,null,null
,,,
177,"Our experiments show that conjunctive Wand is the best candidate generation strategy of those examined: in terms of end-to-end effectiveness, it is statistically distinguishable from disjunctive query evaluation using Wand, but much faster in query evaluation. Note that the ""block-max"" optimization to Wand proposed by Ding and Suel [9] does not affect this conclusion--although the optimization increases disjunctive query evaluation speed, it remains slower than conjunctive processing. In addition, we also show that postings intersection with static priors yields very poor end-toend effectiveness, at least with the size of the candidate sets we examined. This suggests that postings intersection, while an interesting algorithmic challenge since it represents the more general problem of intersecting two lists of sorted integers, is not particularly useful by itself if one's goal is to build effective and efficient search systems.",null,null
,,,
178,6. ACKNOWLEDGMENTS,null,null
,,,
179,"This work has been supported by NSF under awards IIS0916043, IIS-1144034, and IIS-1218043. Any opinions, findings, or conclusions are the authors' and do not necessarily reflect those of the sponsor. The first author's deepest gratitude goes to Katherine, for her invaluable encouragement and wholehearted support. The second author is grateful to Esther and Kiri for their loving support and dedicates this work to Joshua and Jacob.",null,null
,,,
180,7. REFERENCES,null,null
,,,
181,"[1] D. Arroyuelo, S. Gonz´alez, M. Marin, M. Oyarzu´n, and T. Suel. To index or not to index: Time-space trade-offs in search engines with positional ranking functions. SIGIR, 2012.",null,null
,,,
182,"[2] N. Asadi and J. Lin. Document vector representations for feature extraction in multi-stage document ranking. IRJ, in press, 2012.",null,null
,,,
183,"[3] N. Asadi and J. Lin. Fast candidate generation for two-phase document ranking: Postings list intersection with Bloom filters. CIKM, 2012.",null,null
,,,
184,"[4] A. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. Efficient query evaluation using a two-level retrieval process. CIKM, 2003.",null,null
,,,
185,"[5] C. Burges. From RankNet to LambdaRank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, Microsoft Research, 2010.",null,null
,,,
186,"[6] B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. Early exit optimizations for additive machine learned ranking systems. WSDM, 2010.",null,null
,,,
187,"[7] G. Cormack, M. Smucker, and C. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. arXiv:1004.5168v1, 2010.",null,null
,,,
188,"[8] J. Culpepper and A. Moffat. Efficient set intersection for inverted indexing. TOIS, 29(1), 2010.",null,null
,,,
189,"[9] S. Ding and T. Suel. Faster top-k document retrieval using block-max indexes. SIGIR, 2011.",null,null
,,,
190,"[10] Y. Ganjisaffar, R. Caruana, and C. Lopes. Bagging gradient-boosted trees for high precision, low variance ranking models. SIGIR, 2011.",null,null
,,,
191,"[11] K. J¨arvelin and J. Kek¨al¨ainen. Cumulative gain-based evaluation of IR techniques. TOIS, 20(4):422­446, 2002.",null,null
,,,
192,"[12] H. Li. Learning to Rank for Information Retrieval and Natural Language Processing. Morgan & Claypool, 2011.",null,null
,,,
193,"[13] T.-Y. Liu. Learning to rank for information retrieval. FnTIR, 3(3):225­331, 2009.",null,null
,,,
194,"[14] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of learning to rank for web search. IRJ, in press, 2012.",null,null
,,,
195,"[15] D. Metzler. Automatic feature selection in the Markov random field model for information retrieval. CIKM, 2007.",null,null
,,,
196,"[16] G. Pass, A. Chowdhury, and C. Torgeson. A picture of search. InfoScale, 2006.",null,null
,,,
197,"[17] S. Tatikonda, B. Cambazoglu, and F. Junqueira. Posting list intersection on multicore architectures. SIGIR, 2011.",null,null
,,,
198,"[18] N. Tonellotto, C. Macdonald, and I. Ounis. Efficient and effective retrieval using selective pruning. WSDM, 2013.",null,null
,,,
199,1000,null,null
,,,
200,,null,null

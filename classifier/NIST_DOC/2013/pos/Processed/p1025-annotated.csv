,sentence,label,data
,,,
0,An Adaptive Evidence Weighting Method for Medical Record Search,null,null
,,,
1,Dongqing Zhu and Ben Carterette,null,null
,,,
2,Department of Computer & Information Sciences University of Delaware,null,null
,,,
3,"Newark, DE, USA 19716",null,null
,,,
4,[zhu | carteret]@cis.udel.edu,null,null
,,,
5,ABSTRACT,null,null
,,,
6,"In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence in multiple medical reports (from different hospital departments or from different tests within the same department) of a patient. Furthermore, we explore several informative features for learning our retrieval model.",null,null
,,,
7,Categories and Subject Descriptors,null,null
,,,
8,H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
,,,
9,Keywords,null,null
,,,
10,medical record search; EMR; information retrieval; cohort identification; language models,null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"The rich health information contained in electronic medical records (EMR) is useful for improving quality of care. One important application is to search EMR to identify cohorts for clinical studies, which requires retrieval systems specifically designed with medical domain knowledge.",null,null
,,,
13,"To promote research on medical information retrieval, particularly for EMR retrieval, the Text REtrieval Conference (TREC) organized a Medical Records track in 2011 and 2012 [11, 10]. The task is an ad hoc search task for patient visits based on unstructured text in EMR. One particular problem in EMR search is how to aggregate and score evidence that distributes across multiple documents. This is because a patient can have multiple medical reports generated from several hospital departments or even from different tests within a single department.",Y,null
,,,
14,"In this paper, we propose a novel weighting method that can adaptively weight evidence with respect to different queries. We evaluate our algorithm on TREC test collections. The",Y,null
,,,
15,"cross-validation results show that our weighting method is better than a fixed-weighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved when more test collections are available.",null,null
,,,
16,"Our work makes the following contributions: 1) we propose a novel adaptive weighting method for aggregating and scoring evidence in medical records, 2) we propose and explore several features that are based on semantic similarity between medical concepts for predicting the weights of our adaptive weighting method.",null,null
,,,
17,2. RETRIEVAL TASK AND DATA,null,null
,,,
18,"We use the official test collection of the TREC 2011 & 2012 Medical Records Track [11, 10] for our experiments. The test collection contains 100,866 de-identified medical reports, mainly containing clinical narratives, from the University of Pittsburgh NLP Repository.",null,null
,,,
19,"The retrieval task1 is an ad hoc search task for patient visits. A patient visit to the hospital usually results in multiple medical reports, meaning there is a 1-to-n relationship between visits and reports.",null,null
,,,
20,ID Topic 107 Patients with ductal carcinoma in situ (DCIS) 118 Adults who received a coronary stent during an admission 109 Women with osteopenia 112 Female patients with breast cancer with mastectomies during admission,null,null
,,,
21,Table 1: Example topics of medical records track.,null,null
,,,
22,"NIST released 81 information needs (or ""topics"" in TREC terminology) which were designed to require information mainly from the free-text fields (i.e., topics are not answerable solely by the diagnostic codes). Topics are meant to reflect the types of queries that might be used to identify cohorts for comparative effectiveness research [11]. Table 1 lists several TREC topics as examples. The topic specifies the patient's condition, disease, treatment, etc. Relevance judgments for the topics were also developed by TREC assessors based on the pooled results from TREC participants.",Y,null
,,,
23,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
,,,
24,3. ADAPTIVE EVIDENCE AGGREGATION,null,null
,,,
25,"Evidence in a visit can have different forms of distribution. Generally, there are two extreme cases: 1) Strong evidence exists in only one report of the visit; 2) Evidence spreads almost evenly across the majority of reports associated with the visit.",null,null
,,,
26,1 http://www- nlpir.nist.gov/projects/trecmed/2011/tm2011.html,Y,null
,,,
27,1025,null,null
,,,
28,"Report-based Retrieval For the first case, we can estimate the relevance of a visit based on its most relevant report. Thus, we use reports as the initial retrieval units (i.e., building an index for reports and applying the retrieval model to each report), and then transform a report ranking into a visit ranking based on the strongest report-level evidence, which is equivalent to using the following report score merging method for ranking visits:",null,null
,,,
29,"scoreR(V, Q) ,"" MAX(score(R1V , Q), score(R2V , Q), ...), (1)""",null,null
,,,
30,"where RjV is a report associated with visit V based on the report-to-visit mapping, score(RjV , Q) is the relevance score of the report with respect to query Q.",null,null
,,,
31,"Visit-based Retrieval Eq.1 cannot handle case 2 well. For example, if visit V1 has strong evidence in multiple reports and visit V2 has strong evidence in only one report, V1 and V2 will have the same relevance score by using Eq.1. To deal with this problem,",null,null
,,,
32,"we aggregate evidence by merging reports from a single visit field by field into a single visit document V , and then performing retrieval from an index of visits.",null,null
,,,
33,"Like report-based retrieval, this visit-based retrieval has",null,null
,,,
34,"its own disadvantages since it cannot handle case 1 well. For example, if visits V1 and V2 both have strong evidence in only one of their reports but V1 has three times more reports than V2, the strong evidence in V1 will be weakened after merging, resulting in V1 receiving a lower relevance score than V2.",null,null
,,,
35,3.1 A Novel Scoring Function,null,null
,,,
36,"The comparison of the report-based and visit-based retrievals shows that these two strategies complement each other. Thus, we propose a new query-adaptive scoring function as shown below:",null,null
,,,
37,"score(V, Q) ,"" Q ·scoreR(V, Q)+(1-Q)·scoreV(V, Q), (2)""",null,null
,,,
38,"where scoreR(V, Q) and scoreV(V, Q) are the relevance scores of document V from report-based and visit-based retrievals respectively, and Q is the query-adaptive coefficient for scoring merging. If we can adjust Q appropriately, Eq.2 should be able to deal with all the evidence distribution",null,null
,,,
39,cases mentioned above.,null,null
,,,
40,3.2 Learning Algorithm,null,null
,,,
41,"In this paper, we propose to adaptively set Q with respect to different queries by learning the weight Q based on a set of features.",null,null
,,,
42,"In particular, we can view Q as a mixing probability: the probability that the evidence clusters in only one report rather than spreads across multiple reports. Then, assuming the log-odds of that probability can be expressed as a linear combination of feature values, we may write:",null,null
,,,
43,log,null,null
,,,
44,1,null,null
,,,
45,Q - Q,null,null
,,,
46,m,null,null
,,,
47,", 0 + ixi +",null,null
,,,
48,"i,1",null,null
,,,
49,Q,null,null
,,,
50,"where 0 is a model intercept (or bias term), xi is the value of feature number i, i is the weight coefficient of that feature, and Q is a slack variable.",null,null
,,,
51,This is essentially a logistic regression model2. Logistic,null,null
,,,
52,regression is fit using iteratively reweighted least squares to,null,null
,,,
53,"2While logistic regression is often used for 0/1 classification problems,",null,null
,,,
54,"find the values of the  coefficients that are the best fit to training data. Given feature values and their  coefficients, we can then predict the mixing probability Q for new queries.",null,null
,,,
55,3.3 Features,null,null
,,,
56,"We propose 14 features that are possibly related to the evidence distribution in visits, and can be used to predict the weight Q in Eq.2. All these features are based on characteristics of the medical concepts contained in the query. We detect these medical concepts using MetaMap [1], a medical NLP tool developed at the National Library of Medicine (NLM) to map biomedical text to concepts in the Unified Medical Language System (UMLS) Metathesaurus. The concepts are represented by the Concept Unique Identifier (CUI) in UMLS Metathesaurus. Thus, we use QC to represent a concept query that is converted from the original text query Q and contains only CUIs. Next, we describe these 14 features in detail:",null,null
,,,
57,"1. Length of the query Intuitively, evidence is more likely to resides across reports for long queries. Thus, we use the length of query |Q| as the feature to estimate the evidence distribution. It is defined formally as |Q| ,"" wQ cnt(w, Q), where c(w, Q) is the count of term w in Q.""",null,null
,,,
58,2. Number of concepts in the query,null,null
,,,
59,"Similarly, if a query contains more medical concepts, it is",null,null
,,,
60,more likely to find that the evidence distributes across mul-,null,null
,,,
61,"tiple reports. We define this feature formally as |QC | ,",null,null
,,,
62,w,null,null
,,,
63,"winQQCCc.nQt(Cw,aQbCe)t,tewrhfeeraetucrnet(twh,aQn CQ)",null,null
,,,
64,is the count of term because if the query,null,null
,,,
65,contains a medical concept whose name is very long then Q,null,null
,,,
66,might not be a good indicator of the evidence distribution.,null,null
,,,
67,3. Broad/narrow query concepts,null,null
,,,
68,"A text query can contain several medical concepts, for each",null,null
,,,
69,of which the MetaMap program will return 1 to 10 candi-,null,null
,,,
70,dates. We hypothesize that a concept with more candidates,null,null
,,,
71,"is less specific, and thus more likely to be a broad concept",null,null
,,,
72,"and appears in multiple reports. Thus, the average number",null,null
,,,
73,of returned MetaMap candidates for concepts in a query,null,null
,,,
74,may be a good indicator of evidence distribution. We define,null,null
,,,
75,"this feature as RC ,",null,null
,,,
76,wQC |Meta(w)|,null,null
,,,
77,|QC |,null,null
,,,
78,",",null,null
,,,
79,where,null,null
,,,
80,|QC |,null,null
,,,
81,the,null,null
,,,
82,orig-,null,null
,,,
83,"inal concept query length (i.e., the length before expansion),",null,null
,,,
84,|Meta(w)| is the number of concept candidates returned by,null,null
,,,
85,MetaMap for term w in concept query QC .,null,null
,,,
86,"4. Semantic similarity among query concepts Intuitively, if QC contains concepts that are semantically close, the associated evidence in a visit may also co-occur in a single report. However, if the concepts are semantically distant, the corresponding evidence may tend to distribute across reports. Thus, we use the semantic distance among query concepts to estimate how the evidence distributes.",null,null
,,,
87,"We use YTEX3 to measure semantic similarity. Given a pair of UMLS concepts, YTEX can produce knowledge based and distributional based similarity measures. The former uses knowledge sources such as dictionaries, taxonomies,",null,null
,,,
88,it can also be used when the target variable is a real number between,null,null
,,,
89,"0 and 1. In this case it is sometimes called a ""quasibinomial"" model. 3 http://code.google.com/p/ytex/wiki/SemanticSim_V06",null,null
,,,
90,1026,null,null
,,,
91,Type Knowledge-based Distributional-based,null,null
,,,
92,Method Path-Finding,null,null
,,,
93,Intrinsic IC based Corpus IC based,null,null
,,,
94,Notation WUPALMER,null,null
,,,
95,LCH PATH RADA IC LIN IC LCH IC PATH IC RADA JACCARD SOKAL CIC LIN,null,null
,,,
96,Name Wu & Palmer Leacock & Chodorow Path Rada Lin Leacock & Chodorow Jiang & Conrath Rada Jaccard Sokal & Sneath Lin,null,null
,,,
97,Table 2: Semantic similarity measures.,null,null
,,,
98,"and semantic networks, while the latter mainly uses the distribution of concepts within some domain-specific corpus [3].",null,null
,,,
99,"We use the 11 measures listed in Table 2 as our features. Due to the limited space, we will not describe these features; Garla and Brandt provide a detailed overview [3].",null,null
,,,
100,"For each query and each specific measure, we take the mean of the semantic similarity scores for all UMLS concept pairs in the query. This averaged semantic similarity score will be the feature score.",null,null
,,,
101,4. EXPERIMENTAL SETUP,null,null
,,,
102,"We use the Indri4 retrieval system for indexing and retrieving. In particular, we use the Porter stemmer to stem words in both text documents and queries, and use a standard medical stoplist [4] for stopping words in queries only.",null,null
,,,
103,"Our retrieval model is a linear combination of the Markov random field model (MRF) [8] and a mixture of external collection-based relevance models (MRM) [2] for query expansion. Our collections for expansion are the ClueWeb09 Category B (excluding the Wikipedia pages) corpus, the 2009 Genomics Track corpus, 2012 Medical Subject Headings (MeSH), and the medical records corpus itself. Both report and visit-based retrievals use this system.",Y,null
,,,
104,"Because the focus of this work is to evaluate the adaptive scoring function as shown in 2, we will set the parameters of the MRF and MRM models to some default values. We use the same set of parameter values for both the report and visit-based retrievals. We set the Dirichlet smoothing parameter  to 2500. For MRF model, we follow Metzler and Croft [8] and set the feature weights (T , O, U ) to (0.8, 0.1, 0.1). For MRM model, we take take the top-weighted 10 terms from the top-ranked 50 documents for each expansion collection. More detail about our model is presented in recent work [13].",null,null
,,,
105,"To evaluate our learning algorithm as described in Section 3, we first obtain the optimal coefficient Q-opt for each topic Q by sweeping [0, 1] at a step size of 0.1. Then we conduct leave-one-out cross-validation (LOOCV), in each iteration of which the system predicts Q for one new topic based on Q-opt's for the other 80 topics. With limited topics available for learning a relatively complex prediction model, using LOOCV can maximize the size of training data we can use in each iteration of the cross-validation, and lead to a better estimate for each feature weight.",null,null
,,,
106,We train our systems on MAP. This is because: 1) training on MAP is most commonly used in IR to improve retrieval performance; 2) we find that training on MAP improves the retrieval performance on other evaluation metrics as well while training on other evaluation measures does not,null,null
,,,
107,4 http://www.lemurproject.org/indri/,null,null
,,,
108,Feature IC RADA WUPALMER,null,null
,,,
109,RADA JACCARD,null,null
,,,
110,Significance 0.0112 0.0299 0.0368 0.0647,null,null
,,,
111,Feature RC,null,null
,,,
112,SOKAL IC LIN IC PATH,null,null
,,,
113,Significance 0.0654 0.0671 0.0824 0.0876,null,null
,,,
114,"Table 3: Features in the pruned set using LOOCV, sorted by their statistical significance scores.",null,null
,,,
115,"improve the overall performance. Thus, MAP will be the primary evaluation measure in this work. In fact, MAP correlates well with other evaluation measures as we will show in the Section 5.",null,null
,,,
116,"To access the statistical significance of differences in the performance of two systems, we perform one-tailed paired t-test for MAP (since we train systems on MAP). We report scores for MAP, R-precision (Rprec), bpref, and precision at rank 10 (P10).",null,null
,,,
117,5. RESULTS AND ANALYSIS,null,null
,,,
118,5.1 Feature Selection,null,null
,,,
119,"To choose a good feature combination, we use a greedy feature elimination approach in which we start with a full set of features and iteratively eliminate exactly one feature at a time that has the greatest negative impact on the retrieval performance until when further removing any feature will degrade the performance.",null,null
,,,
120,"After the above feature set pruning step, there are 8 features left as shown in Table 3. We further study the importance of each feature by analyzing the prediction model trained in a randomly selected iteration of LOOCV using these 8 features. Based on the statistical significance of each feature as shown in Table 3, we can infer that:",null,null
,,,
121,"1) All the intrinsic IC based features except IC LCH are in the pruned feature set, indicating that these types of similarity measures are generally more effective for predicting Q than other measures. In fact, the intrinsic IC similarity measure incorporates taxonomical evidence explicitly modeled in ontologies (such as the number of leaves/hyponyms and subsumers), which are not captured by the path-finding based measure. Furthermore, the intrinsic IC similarity measure avoids dependence on the availability of domain corpora, thus is considered more scalable and easily applicable than the distributional-based measure [9].",null,null
,,,
122,2) RC is a good feature though it only uses similarity information about each query concept and its neighbors (rather than other query concepts) in the semantic network.,null,null
,,,
123,"3) Neither |Q| nor QC is in the pruned set, indicating that non-semantic-similarity-related features are generally not useful for estimating the evidence distribution.",null,null
,,,
124,4) RADA is a feature that might worth further exploration because both the Path-finding based and the intrinsic IC based RADA features are in the pruned set.,null,null
,,,
125,5.2 Adaptive Weighting,null,null
,,,
126,"Fixed Weighting We first evaluate the performance of Eq. 2 when  is fixed (i.e., not adaptive). In each iteration of the LOOCV, we obtain the best value setting for  on the 80 training topics by sweeping [0, 1] at a step size of 0.1, and then apply the trained  value to the single testing topic. We show the results in the `Fixed-weighting' row of Table 4. Note that",null,null
,,,
127,1027,null,null
,,,
128,System Visit-based Report-based Fixed-weighting Adaptive-weighting Optimal-weighting,null,null
,,,
129,MAP,null,null
,,,
130,"0.4122 0.4354V 0.4472V,R 0.4485V,R 0.4639V,R,F,A",null,null
,,,
131,R-prec 0.422 0.435 0.443 0.447 0.457,null,null
,,,
132,bpref 0.499 0.511 0.520 0.523 0.539,null,null
,,,
133,P10 0.619 0.607 0.631 0.642 0.656,null,null
,,,
134,Pred. MSE ­ ­,null,null
,,,
135,0.128 0.125 0.000,null,null
,,,
136,"Table 4: Performance comparison. A superscript on the MAP score of system X corresponds to the initial of system Y, and indicates statistical significance (p < 0.05) in the MAP difference between X and Y. The last column is the mean square error of the predicted weights. `Fixed-weighting' corresponds to one of the top-ranked TREC systems as mentioned in Sections 4 and 5.2.",Y,null
,,,
137,this system is a better version of system udelSUM [13] which is one of the top-ranked 2012 Medical Records track systems.,null,null
,,,
138,"Optimal Weighting We also obtain the optimal Q-opt for each topic separately by sweeping  from 0 to 1 with a step size of 0.1. Then, we use the Q-opt's to compute the best retrieval performance (i.e., an upper-bound) Eq. 2 can possibly achieve, as shown in the `Optimal-weighting' row of Table 4.",null,null
,,,
139,"Performance Comparison Table 4 shows performance comparison of our adaptive merging method with fixed-weighting, optimal-weighting, and two other baselines (report-based retrieval and visit-based retrieval). Our adaptive merging method is better than the fixed weighting method on all the evaluation metrics. The improvement is not statistically significant (p ,"" 0.191), possibly because 81 topics may not be enough to train a good prediction model for our adaptive weighting method. In addition, the data are slightly skewed as Figure 1 showing that Q-opt "", 1 or 0.9 on about one third of the topics.",null,null
,,,
140,Figure 1: Distribution of topics against Q-opt.,null,null
,,,
141,6. RELATED WORK,null,null
,,,
142,"Due to the sensitivity of patient data, methods emerging from research on information retrieval for EMR retrieval have not been well explored by academic researchers. Fortunately, the Text REtrieval Conference (TREC) organized the Medical Records track in 2011 & 2012 making a set of real medical records and human judgments of relevance to search queries available to the research community.",Y,null
,,,
143,Some interesting work have been done using the TREC collection. Limsopatham et al. [5] proposed an effective term representation to handle negated phrases in clinical text. They also incorporated dependence information of the negated terms into the term representation and achieved significant improvement over a baseline system that had no negation handling mechanism.,Y,null
,,,
144,"More recently, Limsopatham et al. [7] proposed an effective representation for EMR retrieval, in which medical",null,null
,,,
145,"records and queries are represented by medical concepts that directly relate to symptom, diagnostic, test, diagnosis, and treatment. We have built on their work, combining a concept representation with text-based retrieval to improve on both and provide a base in which additional medical knowledge can be incorporated easily.",null,null
,,,
146,"Among more relevant works, Limsopatham et al. [6] explored using the type of medical records for enhancing retrieval performance. They demonstrated that incorporating department level evidence of the medical reports in their extended voting model and federated search model could improve the retrieval effectiveness. Their work opens another interesting direction for exploring evidence distribution and score merging. Zhu and Carterette's system [12] aggregated report-level evidence and visit-level evidence, and achieved significant improvement over a strong baseline.",null,null
,,,
147,7. CONCLUSION AND FUTURE WORK,null,null
,,,
148,"In this paper, we present a medical record search system which is useful for identifying cohorts required in clinical studies. In particular, we propose a query-adaptive weighting method that can dynamically aggregate and score evidence within multiple medical reports. We show by crossvalidation that our weighting method is better than a fixedweighting method across several evaluation metrics. Though the improvement is not statistically significant, we believe that our method has the potential to be further improved by incorporating other useful features or by using advanced prediction models. Furthermore, we explore several informative features for weight prediction. We believe these features might be useful for improving medical IR systems.",null,null
,,,
149,"8. REFERENCES [1] A. R. Aronson. Effective mapping of biomedical text to the UMLS metathesaurus: The MetaMap program. Proceedings of AMIA Symposium, pages 17­21, 2001. [2] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of SIGIR, pages 154­161, 2006. [3] V. Garla and C. Brandt. Semantic similarity in the biomedical domain: an evaluation across knowledge sources. BMC Bioinformatics, 13:261, 2012. [4] W. Hersh. Information Retrieval: A Health and Biomedical Perspective. Health Informatics. Springer, 3rd edition, 2009. [5] N. Limsopatham, C. Macdonald, R. McCreadie, and I. Ounis. Exploiting term dependence while handling negation in medical search. In Proceedings of SIGIR, pages 1065­1066, 2012. [6] N. Limsopatham, C. Macdonald, and I. Ounis. Aggregating evidence from hospital departments to improve medical records search. In Proceedings of ECIR, 2013. [7] N. Limsopatham, C. Macdonald, and I. Ounis. A task-specific query and document representation for medical records search. In Proceedings of ECIR, 2013. [8] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, page 472, 2005. [9] D. Sa´nchez and M. Batet. Semantic similarity estimation in the biomedical domain: An ontology-based information-theoretic perspective. Journal of Biomedical Informatics, 44(5):749 ­ 759, 2011. [10] E. M. Voorhees. DRAFT: Overview of the TREC 2012 medical",Y,null
,,,
150,"records track. In TREC, 2012. [11] E. M. Voorhees and R. M. Tong. DRAFT: Overview of the",Y,null
,,,
151,"TREC 2011 medical records track. In TREC, 2011. [12] D. Zhu and B. Carterette. Combining multi-level evidence for",Y,null
,,,
152,"medical record retrieval. In Proceedings of SHB, 2012. [13] D. Zhu and B. Carterette. Exploring evidence aggregation",null,null
,,,
153,"methods and external expansion sources for medical record search. In Proceedings of TREC, 2012.",Y,null
,,,
154,1028,null,null
,,,
155,,null,null

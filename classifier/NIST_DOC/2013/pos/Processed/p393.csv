,sentence,label,data
0,On the Measurement of Test Collection Reliability,null,null
1,Julián Urbano jurbano@inf.uc3m.es,null,null
2,Mónica Marrero mmarrero@inf.uc3m.es,null,null
3,Diego Martín dmartin@dit.upm.es,null,null
4,University Carlos III of Madrid Department of Computer Science,null,null
5,"Leganés, Spain",null,null
6,Technical University of Madrid Department of Telematics Engineering,null,null
7,"Madrid, Spain",null,null
8,ABSTRACT,null,null
9,"The reliability of a test collection is proportional to the number of queries it contains. But building a collection with many queries is expensive, so researchers have to find a balance between reliability and cost. Previous work on the measurement of test collection reliability relied on databased approaches that contemplated random what if scenarios, and provided indicators such as swap rates and Kendall tau correlations. Generalizability Theory was proposed as an alternative founded on analysis of variance that provides reliability indicators based on statistical theory. However, these reliability indicators are hard to interpret in practice, because they do not correspond to well known indicators like Kendall tau correlation. We empirically established these relationships based on data from over 40 TREC collections, thus filling the gap in the practical interpretation of Generalizability Theory. We also review the computation of these indicators, and show that they are extremely dependent on the sample of systems and queries used, so much that the required number of queries to achieve a certain level of reliability can vary in orders of magnitude. We discuss the computation of confidence intervals for these statistics, providing a much more reliable tool to measure test collection reliability. Reflecting upon all these results, we review a wealth of TREC test collections, arguing that they are possibly not as reliable as generally accepted and that the common choice of 50 queries is insufficient even for stable rankings.",null,null
10,Categories and Subject Descriptors,null,null
11,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance evaluation.,null,null
12,General Terms,null,null
13,"Experimentation, Measurement, Reliability.",null,null
14,Keywords,null,null
15,"Test Collection, Evaluation, Reliability, Generalizability Theory, TREC.",null,null
16,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
17,1. INTRODUCTION,null,null
18,The purpose of evaluating the effectiveness of an Informa-,null,null
19,tion Retrieval (IR) system is to assess how well it would sat-,null,null
20,isfy real users. The main tool used in these evaluations are,null,null
21,"test collections, which comprise a collection of documents",null,null
22,"to search, a set of queries Q, and a set of relevance judg-",null,null
23,ments that contains information as to what documents are,null,null
24,"relevant, and to which degree, to the queries [16]. Given",null,null
25,the results returned by a system A for one of the queries,null,null
26,"q  Q, an effectiveness measure uses the information in the",null,null
27,"relevance judgments to compute a score q,A that represents",null,null
28,the effectiveness of the system for that query. After run-,null,null
29,"ning the system for all queries in the collection, the average",null,null
30,"Q,A",null,null
31,",",null,null
32,1 |Q|,null,null
33,"i,A is usually reported as the main measure",null,null
34,"of system effectiveness, representing the expected behavior",null,null
35,of the system for an arbitrary new query. When comparing,null,null
36,"two systems A and B, the main measure reported is the av-",null,null
37,"erage effectiveness difference Q,AB ,"" Q,A - Q,B. Based""",null,null
38,"on this difference, we conclude which system is better.",null,null
39,The immediate question to ask is: how reliable are those,null,null
40,"conclusions about system effectiveness? Ideally, researchers",null,null
41,would evaluate the system with the set of all possible queries,null,null
42,"that a user might request. In such a case, we could be sure",null,null
43,that the true average performance of the system corresponds,null,null
44,to the score we computed with the collection. The prob-,null,null
45,lem is that building such a collection is either impractical,null,null
46,for requiring an enormous amount of queries and relevance,null,null
47,"judgments, or just impossible if the potential query set is",null,null
48,"not defined, which use to be the case because we can not",null,null
49,"account for future queries that do not yet exist. Therefore,",null,null
50,the query set Q in a test collection must be regarded as,null,null
51,"a sample from the universe of all queries, and the sample",null,null
52,"mean Q,A as an estimate of the true effectiveness mean A.",null,null
53,But because we are estimating this score with a sample of,null,null
54,"queries, our estimates are erroneous to some degree. The",null,null
55,"results may change drastically with a different query set Q,",null,null
56,so much that differences between systems could be reversed.,null,null
57,An evaluation result is reliable if it can be replicated with,null,null
58,another collection: if the set of queries Q suggests that sys-,null,null
59,"tem A outperforms system B, we can be very sure that the conclusion would hold for a different set of queries Q, and in",null,null
60,"the end, for the universe of all queries. A simple way to make",null,null
61,a collection reliable is to include many queries; the more we,null,null
62,employ the smaller the variance of the estimates and thus,null,null
63,the more reliable the conclusion. The problem is that more,null,null
64,"queries also means more cost to create the collection, so re-",null,null
65,searchers have to find a balance between the reliability of,null,null
66,"the results and the cost of the collection. To this end, it is",null,null
67,necessary to develop indicators of test collection reliability.,null,null
68,393,null,null
69,"Several works in the last fifteen years have studied the problem of reliability in IR evaluation experiments. The basic methodology consisted in evaluating a series of systems with two different and random sets of queries, computing several reliability indicators that measured how similar those evaluations were. Using different query sample sizes and randomizing query selection, researchers were able to map query set size to reliability and extrapolate results to larger query sets. The data used consisted in runs submitted to several TREC tracks (mostly the Ad Hoc tracks), and the sets of queries employed in each edition. While these approaches are clearly faithful to the data, they are limited in that the full query set had to be partitioned in two disjoint sets to comply with the assumption that they were independent.",null,null
70,"In 2007 Bodoff and Li [6] proposed Generalizability Theory (GT) as an alternative [7, 18]. GT is grounded on analysis of variance components, which allows to dissect the variability in effectiveness scores and figure out how much of it is due to system differences, query difficulty, assessors, etc. In an ideal evaluation setting, we would like all variance to be due to actual differences between systems and not due to query variability; if the queries in the collection are too varied, or differences between systems too small, then we need many queries to ensure that our estimates are reliable. From these variance components GT allows researchers to estimate the reliability of a test collection even before it has been created. Based on some previous data, GT can estimate the reliability of a collection with a larger number of queries, more than one assessor providing judgments for each query, etc. GT provides indicators for the stability of both the absolute scores and the relative differences by computing different variance ratios.",null,null
71,"The main advantages of GT against the traditional databased approaches are that 1) it is based on statistical theory, 2) it is easy to employ because it does not require tedious and repetitive what if scenarios, and 3) it allows us to estimate the reliability of a collection or experimental design that does not exist yet. But it has disadvantages too: 1) it is unknown the extent to which reliability indicators are affected by the data used to estimate variance components, and 2) it is very hard to interpret them in practical terms.",null,null
72,"We address these two problems of GT applied to the measurement of test collection reliability. In the next section we review past work following data-based approaches and the reliability indicators used. We then review the use of GT and discuss the motivation for this work. In Section 3 we show how the initial data used in GT studies has a very large effect on the results, discussing minimum sample sizes and interval estimators. Section 4 reports a study to provide an empirical mapping between GT-based indicators of reliability and the well known data-based ones. Next we discuss the reliability of several TREC collections based on the results from previous sections, presenting conclusions in Section 6.",null,null
73,2. INDICATORS OF RELIABILITY,null,null
74,Several indicators of test collection reliability have been proposed in the literature. This section reviews traditional indicators found in the early data-based studies and the GTbased indicators more recently proposed.,null,null
75,2.1 Data-based Indicators,null,null
76,"Given a query set Q and a similar set Q of the same size, we can define the following data-based reliability indicators:",null,null
77,"· Kendall correlation ( ), compares the order in which systems are ranked according to Q and Q, regardless of the magnitude of the differences AB. It ranges from 1 (same rankings) to -1 (reversed rankings), counting the number of system pairs that are swapped between the two rankings. For Q to be reliable,  must therefore tend to 1.",null,null
78,"· AP correlation (AP ), adds a top-heaviness component to Kendall  , such that swaps between systems towards the top of the rankings are more penalized than swaps towards the bottom [23].",null,null
79,"· Power ratio (), is the fraction of pairwise system differences that result statistically significant according to query set Q. If the difference Q,AB between two systems is deemed as statistically significant, it serves as further evidence that the true difference AB has the same sign. For Q to be reliable,  must therefore tend to 100%. In this paper we compute standard 2-tailed t-tests at the 0.05 level [19].",null,null
80,"· Minor Conflict ratio (-), is the fraction of statistically significant differences with Q that have a sign swap with Q but are not statistically significant there. - is therefore the fraction of uncertain conclusions when measuring statistical significance, so for Q to be reliable - must therefore tend to 0%.",null,null
81,"· Major Conflict ratio (+), is the fraction of statistically significant differences with Q that are also significant with Q but have a sign swap. + is therefore the fraction of incorrect conclusions when measuring statistical significance, so for Q to be reliable + must therefore tend to 0% as well.",null,null
82,"· Absolute Sensitivity (a), is the minimum absolute difference Q,AB that need be observed between any two systems such that the differences with Q have the same sign at least 95% of the times. For Q to be reliable, a must therefore tend to 0, meaning that even small differences can be trusted.",null,null
83,"· Relative Sensitivity (r), is the minimum relative difference Q,AB/ max Q,A, Q,B that need be observed with Q such that the differences with Q have the same sign at least 95% of the times. For Q to be reliable, r must therefore tend to 0% too.",null,null
84,"· Root Mean Squared Error (), measures the difference between the absolute scores with Q and with Q. Thus, for Q to be reliable  must tend to 0 too.",null,null
85,"One of the first reliability studies was conducted in 1998 by Voorhees [20], who analyzed the effect of having different assessors provide relevance judgments. Employing a methodology based on randomization, she concluded that the absolute scores could suffer wide variations between assessors, but that the ranking of systems was seldom altered, establishing  ,"" 0.9 as the de facto minimum on ranking similarity. She also studied swap rates as a function of  and suggested a minimum of 25 queries to have a somewhat stable ranking. Also in 1998, Zobel [24] studied the effect of pool depth on absolute system scores, extrapolating trends to larger pool depths. He also compared different statistical procedures in terms of power and conflict ratios.""",null,null
86,"Buckley and Voorhees [8] compared in 2000 the reliability of various effectiveness measures by mapping effectiveness differences to error rates. Extrapolating to 50 queries, they concluded that   0.05 produced less than 1.5% system swaps when computing Average Precision (AP), while",null,null
87,394,null,null
88,"other measures such as Precision at cutoff 10 (P@10) produced 3.6% of swaps. In 2002, Voorhees and Buckley [22] extended their work with other collections and methods, but again extrapolating trends. They concluded that with 50 queries the sensitivity of AP was a ,"" 0.05, while increasing the query set size to 100 would yield a "","" 0.03. They also reported large differences across collections and effectiveness measures. Lin and Hauptmann [13] showed that the empirical model used by Voorhees and Buckley can be derived theoretically, and that the three factors affecting reliability are query set size, mean effectiveness scores, and variability of scores. Sanderson and Zobel [17] also revisited this work by computing relative sensitivity and incorporating statistical procedures to account for score variability. They concluded r "","" 10% with AP if coupled with statistical significance, and r "","" 25% if not. They observed very similar relative sensitivity between AP and P@10, arguing the use of more queries with fewer judgments as previous work suggested that much of the score variability is due to queries [4].""",null,null
89,"In 2007 Sakai [15] used similar methods to compare the reliability of several effectiveness measures, though he did not extrapolate to larger query sets. He computed  correlations, absolute sensitivity a and a variation of r, and observed that these indicators were not very correlated with statistical significance, arguing the importance of considering score variability rather than just means. Voorhees revisited in 2009 [21] the use of statistical procedures with the TREC Robust 2004 collection, computing reliability indicators with an unprecedented set of 100 queries, therefore avoiding the need to extrapolate to the usual size 50. When using AP, she observed power  , 47% and conflict ratios - , 2.7% and + ,"" 0.04%. She showed again that P@10 is less reliable than AP also in these terms; and that nDCG showed higher reliability (agreeing with Sakai [15]). She also found that minor conflicts were usually coupled with large relative differences, thus suggesting that researchers employ several large collections to draw general conclusions.""",null,null
90,2.2 GT-based Indicators,null,null
91,"Bodoff and Li [6] proposed Generalizability Theory [7, 18] as an alternative to measure test collection reliability that directly addresses variability of scores rather than just the mean as was common before. GT has two stages: a Generalizability study (G-study) to estimate variance components based on previous data, and a Decision study (D-study) that subsequently computes reliability indicators for a different experimental design. We consider a fully crossed design and decompose variability of scores into three components: variance due to actual differences among systems (s2), variance due to differences in difficulty among queries (q2), and variance due to the system-query interaction effect whereby some systems are particularly good (or bad) for some queries (s2:q). The variance due to other effects, such as assessors, is in our case confounded with the interaction effect.",null,null
92,"Using Analysis of Variance (ANOVA) procedures, these variance components can be estimated from previous data:",null,null
93,"^s2:q , ^e2 , EMresidual",null,null
94,(1),null,null
95,^s2,null,null
96,",",null,null
97,EMs - nq,null,null
98,^e2,null,null
99,(2),null,null
100,^q2,null,null
101,",",null,null
102,EMq - ^e2 ns,null,null
103,(3),null,null
104,"where EM is the expected Mean Square of component , and ns and nq are the number of systems and queries [7, 18]. These estimates can be used to compute the proportion of total variance that is due to each of the effects, such as how much of it is due to actual differences between systems.",null,null
105,"In the D-study, we can use the variance estimates from the G-study to compute the reliability of a larger query set. To this end, two reliability indicators are usually employed:",null,null
106,"· Generalizability Coefficient (E2), is the ratio of system variance to itself plus relative error variance:",null,null
107,E2,null,null
108,nq,null,null
109,",",null,null
110,s2,null,null
111,s2,null,null
112,+,null,null
113,2 e,null,null
114,n,null,null
115,(4),null,null
116,q,null,null
117,"and it provides a measure of the stability of relative differences between systems . By extension, it measures the reliability of the ranking. For a collection to be reliable, E2 must therefore tend to 1.",null,null
118,"· Index of Dependability (), is the ratio of system variance to itself plus absolute error variance:",null,null
119, nq,null,null
120,",",null,null
121,s2,null,null
122,s2,null,null
123,+ 2+2 qe nq,null,null
124,(5),null,null
125,"and it provides a measure of the stability of absolute effectiveness scores . For a collection to be reliable,  must therefore tend to 1 as well.",null,null
126,"The main advantage of these indicators is that they allow us to estimate the reliability of an arbitrary query set size nq, so there is no need to follow the traditional methodologies based on random what if scenarios and extrapolation. From equations (4) and (5) it can be seen that the reliability of the collection increases as nq increases, because the estimates of query difficulty (i.e. average system performance per query) are more precise. These indicators were used by Kanoulas and Aslam [12] to derive the gain and discount functions of nDCG that yield optimal reliability when nq is constant.",null,null
127,"With simple algebraic manipulation, we can calculate the minimum number of queries needed to reach some level of relative or absolute stability :",null,null
128,"nE2 () ,",null,null
129, · e2 s2 (1 - ),null,null
130,(6),null,null
131,"n () ,",null,null
132, q2 + e2 s2 (1 - ),null,null
133,(7),null,null
134,"which can be used to estimate how many more queries we need to add to our collection for it to be reliable. The main use of this approach can be found in the TREC Million Query Track [2, 1], which set out to study whether many queries with a few judgments yield more reliable results than a few queries with many judgments. The conclusion was that nq  80 queries are sufficient for a reliable ranking, while nq  130 are needed for reliable absolute scores.",null,null
135,2.3 Motivation,null,null
136,"The two problems of GT can be clearly spotted at this point. First, equations (1) to (3) show that we do not compute the true 2 variance components, but just estimates ^2 based on some previous data. If we use a different, yet similar set of systems or queries to estimate these variance components, the resulting E^2 and ^ scores might be very",null,null
137,395,null,null
138,Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011,null,null
139,Span covering 95% of ^ observations 0.0 0.2 0.4 0.6 0.8 1.0,null,null
140,Span covering 95% of E^2 observations 0.0 0.2 0.4 0.6 0.8 1.0,null,null
141,Variability due to queries,null,null
142,20,null,null
143,40,null,null
144,60,null,null
145,80,null,null
146,100,null,null
147,Initial number of queries in the G-study,null,null
148,20,null,null
149,40,null,null
150,60,null,null
151,80,null,null
152,100,null,null
153,Initial number of queries in the G-study,null,null
154,Span covering 95% of ^ observations 0.0 0.2 0.4 0.6 0.8 1.0,null,null
155,Span covering 95% of E^2 observations 0.0 0.2 0.4 0.6 0.8 1.0,null,null
156,Variability due to systems,null,null
157,20,null,null
158,40,null,null
159,60,null,null
160,80,null,null
161,100,null,null
162,Initial number of systems in the G-study,null,null
163,20,null,null
164,40,null,null
165,60,null,null
166,80,null,null
167,100,null,null
168,Initial number of systems in the G-study,null,null
169,Figure 1: Variability in E^2 (top) and ^ (bottom) scores as a function of the initial number of queries (left) and number of systems (right) used in the G-study to estimate variance components.,null,null
170,"different. In a revised paper, Bodoff [5, §4.6] briefly discussed this issue and argued that differences are marginal. However, he reports the results when randomly selecting only one system per research group instead of all of them, and only one trial of such experiment. We argue that this situation is not representative because the full set of systems and the reduced set after removing runs by the same groups are actually very similar to begin with, so it is expected that reliability scores do not change much. Also, only one such randomly reduced set is compared, so there is really no evidence to support that claim. Likewise, he further suggests that as few as five queries or systems are often enough to provide stable estimates of the variance components in the G-study [5, §3.1]. We further analyze this issue in Section 3.",null,null
171,"Second, equations (6) and (7) allow us to estimate the minimum number of queries nq to reach some stability level , but the greater question is: how much is stable enough? Bodoff [5] mentions that in most Social Science applications a stability coefficient of 0.8 is acceptable, but there is no similar standard for Engineering applications. Kanoulas and Aslam [12] set  ,"" 0.95 as the target in their experiments, but this choice is arbitrary. In their analysis of the Million Query Track 2007 [2] and 2008 [1], Allan et al. [1] and Carterette et al. [9, 10] also set E2 "", 0.95 as the target. They mention in a footnote that in their experiments E2 , 0.95 approximately corresponded to  ,"" 0.9, but details are omitted. We study this issue in Section 4 by empirically mapping GT-based indicators onto data-based indicators that are easier to understand and use in practice.""",null,null
172,3. VARIABILITY OF GT INDICATORS,null,null
173,"To measure the effect of the number of queries and number of systems used in the G-study to estimate variance components, we use data from 43 TREC collections covering 12 tasks across 10 tracks, from TREC 3 to TREC 2011 (see Table 1). As in previous studies [22, 17, 6, 21], we remove",null,null
174,"the bottom 25% of systems so that our results are not obscured by possibly buggy implementations. For each collection, we randomly selected nq ,"" 5 queries and computed the variance components using the full set of systems. We then calculated E2 and  for the full query set size, and the required number of queries to reach 0.95 stability. This was repeated with increments in nq of 5 queries, up the maximum permitted by the collection or 100. For each query set size, we ran 200 random trials, each of which can be considered as the possible data available for a G-study when analyzing a test collection design. The same process was followed by varying the initial number of systems ns and using the full set of queries instead.""",null,null
175,"Figure 1 shows the variability in G-study results1. For each collection and initial number of queries used, the y-axis plots the length of the span covering 95% of the E^2 and ^ observations in the 200 random trials. The right hand side plots show the same span lengths, but for different number of systems used in the G-study. As expected, the queries have a larger effect. Most importantly, we see that the average span length with just 5 queries is about 0.5 across collections. That is, the stability estimates could be as low as 0.3 or as high 0.8, for example, just depending on the particular set of queries we use in the G-study. In fact, estimates of the minimum number of queries required can vary in orders of magnitude if not using enough data. For example, with as many as 30 initial queries and all 184 systems from the Microblog 2011 collection, GT may suggest from 63 to 133 queries to reach E2 ,"" 0.95. Similarly, from 40 initial systems and all 34 queries from the Medical 2011 collection, GT may suggest from 109 to 566 queries. In general, at least 50 queries and 50 systems seem necessary for 95% of estimates to be within a 0.1 span. This means that GT may be trusted to measure the reliability of an existing collection, but that""",null,null
176,"1Given the amount of datapoints displayed in this paper, we recommend to access the full-color version available online.",null,null
177,396,null,null
178,L - nq,null,null
179,"1,",null,null
180,U - nq,null,null
181,1,null,null
182,", where",null,null
183,(8),null,null
184,L,null,null
185,",",null,null
186,"Ms Me F:dfs ,dfe",null,null
187,U,null,null
188,",",null,null
189,"Ms Me F1-:dfs ,dfe",null,null
190,nsL nsL +,null,null
191,nq,null,null
192,",",null,null
193,nsU nsU +,null,null
194,nq,null,null
195,", where",null,null
196,(9),null,null
197,L,null,null
198,",",null,null
199,Ms2,null,null
200,"- F:dfs,MsMe + (F:dfs, - F:dfs,dfe ) F,dfs,dfe Me2 (ns - 1)F:dfs,MsMe + F:dfs,dfq MsMq",null,null
201,U,null,null
202,",",null,null
203,Ms2,null,null
204,"- F1-:dfs,MsMe + (F1-:dfs, - F1-:dfs,dfe ) F1-,dfs,dfe Me2 (ns - 1)F1-:dfs,MsMe + F1-:dfs,dfq MsMq",null,null
205,researchers should be cautious when planning a collection,null,null
206,based on the results of a handful of systems and queries.,null,null
207,These results clearly evidence the need for a measure of,null,null
208,confidence on GT indicators. Bodoff [5] suggests the use of,null,null
209,"confidence intervals to account for this variability, but only",null,null
210,computes them for the variance components in the G-study.,null,null
211,Confidence intervals for the ultimately more useful D-study,null,null
212,"can be worked out from various variance ratios (see equations (8) and (9)2). Feldt [11] derived exact 100(1 - 2)% confidence intervals for the ratio  , s2/e2 under the assumption of normally distributed scores. The confidence interval on E2(nq) is computed using the endpoints in (8):",null,null
213,E2,null,null
214,nq,null,null
215,",",null,null
216,nq  1 + nq,null,null
217,(10),null,null
218,"Arteaga et al. [3] derived approximate 100(1 - 2)% confidence intervals for the ratio  ,"" s2/ s2 + q2 + e2 , again""",null,null
219,assuming a normal distribution of scores. The confidence interval on  nq is computed using the endpoints in (9):,null,null
220,nq,null,null
221,", 1+",null,null
222,nq  nq - 1 ,null,null
223,(11),null,null
224,"Brennan [7, §6] discusses different methods to compute confidence intervals in both G-studies and D-studies, showing that the above intervals work reasonably well even when the normality assumption is violated. The right hand side of Table 1 reports the point and 95% interval estimates of the stability of the 43 TREC collections we consider in this paper. These intervals provide a more suitable estimate of test collection reliability because they account for variability in the G-study. For example, researchers could use these intervals to infer the required number of queries to reach the lower endpoint of the interval instead of the point estimate:",null,null
225,"nE2 () ,",null,null
226,  (1 - ),null,null
227,(12),null,null
228,"n () ,",null,null
229, (1 - )  (1 - ),null,null
230,(13),null,null
231,4. INTERPRETING GT INDICATORS,null,null
232,"To empirically derive a mapping between GT-based and data-based reliability indicators, we again used the 43 TREC collections in Table 1. For each collection we proceeded as follows. Two random and disjoint query subsets of size nq ,"" 10 were selected from the full set of queries; let these subsets be Q and Q. The full set of systems was evaluated with both query subsets, and all data-based reliability indicators in Section 2.1 were computed, along with the two GTbased indicators according to Q and Q. This was repeated""",null,null
233,"2F:df1,df2 is the quantile function of the F distribution with df1 and df2 degrees of freedom. In our fully crossed design, dfs ,"" ns - 1, dfq "","" nq - 1, and dfe "", (ns - 1)(nq - 1).",null,null
234,"with increments in nq of 10 queries, up to the maximum permitted by the collection. For query subset size we ran",null,null
235,"50 random trials, each trial providing us with 32 datapoints (E^2 and ^ according to Q and to Q, mapped to ^,^AP , ^, ^-, ^+, ^a, ^r and ^). Theoretically though, E2 is better related to  , AP , , -, + and a because it measures the stability of relative differences, while  is better related to",null,null
236,r and  because it measures the stability of absolute scores.,null,null
237,We thus mapped only these combinations.,null,null
238,Figure 2 shows the mappings. For each collection we fitted,null,null
239,"a model with all available datapoints. However, we dropped points for which E^2 < 0.8 and ^ < 0.5 so that the trends",null,null
240,were not affected by mappings with such small stability to,null,null
241,be even practical. These thresholds were chosen based on,null,null
242,the observed stability of the 43 TREC collections; about,null,null
243,85% of them show larger stability scores (see Table 1). This,null,null
244,"resulted in over 28,000 points for each plot. In the top three plots ( , AP and ) we fitted the model y ,"" xa, where a is""",null,null
245,the parameter to fit. This resulted in the desired theoretical,null,null
246,"behavior that limx1 y , 1 and limx0 y ,"" 0, that is, when all variability is due to system differences  should be 1""",null,null
247,"because the ranking cannot be altered, and if all variance is",null,null
248,due to queries then  should be 0 because the rankings are,null,null
249,"completely random. Similarly, in the bottom four plots we fitted the model y ,"" (1 - x)a, such that limx1 y "", 0 and limx0 y ,"" 1, that is,  should for example be 0 if there is no variability due to queries.""",null,null
250,"As the first plot shows, all 43 collections do actually need E2 > 0.95 to reach  ,"" 0.9. In general, E2 "","" 0.95 corresponds to   0.85, and on average E2  0.97 is needed""",null,null
251,"across collections to reach  , 0.9. The two clear exceptions",null,null
252,are found in the Million Query Track. The 2008 collection,null,null
253,"is the one that reaches the target  ,"" 0.9 with the lowest stability (E2  0.93), while the 2007 collection needs the largest (E2  0.98). Note that these were the two collections for which the E2 "", 0.95   , 0.9 correspondence",null,null
254,"was established [1, 9, 10]. It should be noted here that these",null,null
255,"fits have an exponential-like shape, meaning that it is hard to achieve a mid level of  , but once E2 is large enough",null,null
256,"small improvements in stability translate into large improvements in  . However, the relation between nq and E2 has a logarithmic-like shape, meaning that it is increasingly more expensive to improve E2 to begin with. Thus, it should be",null,null
257,considered the required effort for slight improvements in  .,null,null
258,"The second plot shows quite high AP scores at these levels of relative stability, but generally below  . This suggests",null,null
259,that the swaps in the rankings are still happening between,null,null
260,systems at the top of the rankings [23]. The third plot shows,null,null
261,that at these stability levels it is expected to observe statis-,null,null
262,tical significance in about 80% of system comparisons. In,null,null
263,the middle right plot we can see that the proportion of con-,null,null
264,"flicting results is generally below the  , 0.05 significance level when E2  0.9.",null,null
265,397,null,null
266,Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011,null,null
267,0.3,null,null
268,0.4,null,null
269,0.5,null,null
270,0.6,null,null
271,0.7,null,null
272,0.8,null,null
273,0.9,null,null
274,1.0,null,null
275,Kendall correlation,null,null
276,AP correlation,null,null
277,Power ratio,null,null
278,1.0,null,null
279,1.0,null,null
280,0.9,null,null
281,0.9,null,null
282,0.8,null,null
283,0.8,null,null
284,0.7,null,null
285,0.7,null,null
286,AP,null,null
287,0.6,null,null
288,0.6,null,null
289,0.5,null,null
290,0.5,null,null
291,0.4,null,null
292,0.4,null,null
293,0.3,null,null
294,0.3,null,null
295,0.80,null,null
296,0.85,null,null
297,0.90,null,null
298,0.95,null,null
299,1.00,null,null
300,E2,null,null
301,Absolute Sensitivity,null,null
302,0.80,null,null
303,0.85,null,null
304,0.90,null,null
305,0.95,null,null
306,1.00,null,null
307,E2,null,null
308,0.80,null,null
309,0.85,null,null
310,0.90,null,null
311,0.95,null,null
312,1.00,null,null
313,E2,null,null
314,Minor Conflict ratio,null,null
315,0.20,null,null
316,0.12,null,null
317,0.15,null,null
318,0.09,null,null
319,0.10,null,null
320,-,null,null
321,0.06,null,null
322,a,null,null
323,0.05,null,null
324,0.03,null,null
325,0.00,null,null
326,0.00,null,null
327,0.80,null,null
328,0.85,null,null
329,0.90,null,null
330,0.95,null,null
331,1.00,null,null
332,0.80,null,null
333,0.85,null,null
334,0.90,null,null
335,0.95,null,null
336,1.00,null,null
337,E2,null,null
338,E2,null,null
339,Relative Sensitivity,null,null
340,RMS Error,null,null
341,0.15,null,null
342,0.10,null,null
343,r 0.0 0.1 0.2 0.3 0.4 0.5 0.6,null,null
344,0.05,null,null
345,0.00,null,null
346,0.5,null,null
347,0.6,null,null
348,0.7,null,null
349,0.8,null,null
350,0.9,null,null
351,1.0,null,null
352,0.5,null,null
353,0.6,null,null
354,0.7,null,null
355,0.8,null,null
356,0.9,null,null
357,1.0,null,null
358,Figure 2: Mapping from GT-based to data-based reliability indicators on a per-collection basis.,null,null
359,"Researchers interested in the particular mapping for one of these collections may use the estimates in Table 1 and the plots in Figure 2 to get a better understanding of the evaluation results and draw more informed conclusions. To assess the reliability of future collections and guide in their development process, we fitted a single model using all available data instead of one model per collection. Figure 3 shows these fits, along with 95% and 90% prediction intervals that theoretically cover 95% and 90% of all future observations. In terms of sensitivity, the middle left plots show that a  0.03 for E2  0.9, which is about 60% of what Voorhees and Buckley reported for the Ad Hoc tracks [22]; although the intervals cover their values well. In the bottom",null,null
360,"left plot we see that r  20% for   0.75, generally agreeing with Sanderson and Zobel [17]. As to statistical significance, we replicated Voorhees's [21] study with random sets of 50 queries from the Ad Hoc 7-8 topics and Robust 2004 systems. The average relative stability is E^2  [0.81, 0.88], which corresponds to   [37%, 54%], -  [3.9%, 7.8%] and +  [0.38%, 1.3%]. These are again larger than she reported, but the intervals cover her values well.",null,null
361,"Overall, these models produce a decent fit on the data, and they fill the gap between data-based methodologies and Generalizability Theory. They provide a valuable tool to rapidly assess and easily understand the reliability of a test collection design.",null,null
362,398,null,null
363,Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8 Web adhoc 8 Web adhoc 9 Web adhoc 2001 Web adhoc 2009 Web adhoc 2010 Web adhoc 2011 Web distillation 2002 Web distillation 2003 Web distillation 2004 Web diversity 2009 Web diversity 2010 Web diversity 2011 Novelty 2002 Novelty 2003 Novelty 2004 Genomics 2003 Genomics 2004 Genomics 2005 Robust 2003 Robust 2004 Robust 2005 Terabyte 2004 Terabyte 2005 Terabyte 2006 Terabyte all 2006 Enterprise 2005 Enterprise 2006 Enterprise 2007 Enterprise 2008 1MQ MTC 2007 1MQ MTC 2008 1MQ MTC 2009 1MQ statAP 2007 1MQ statAP 2008 1MQ statAP 2009 Medical 2011 Microblog 2011,null,null
364,0.3,null,null
365,0.4,null,null
366,0.5,null,null
367,0.6,null,null
368,0.7,null,null
369,0.8,null,null
370,0.9,null,null
371,1.0,null,null
372,Kendall correlation,null,null
373,AP correlation,null,null
374,Power ratio,null,null
375,1.0,null,null
376,1.0,null,null
377,0.9,null,null
378,0.9,null,null
379,0.8,null,null
380,0.8,null,null
381,0.7,null,null
382,0.7,null,null
383,AP,null,null
384,0.6,null,null
385,0.6,null,null
386,0.5,null,null
387,0.5,null,null
388,0.4,null,null
389,0.4,null,null
390,0.3,null,null
391,0.3,null,null
392,0.80,null,null
393,0.85,null,null
394,0.90,null,null
395,0.95,null,null
396,1.00,null,null
397,E2,null,null
398,Absolute Sensitivity,null,null
399,0.80,null,null
400,0.85,null,null
401,0.90,null,null
402,0.95,null,null
403,1.00,null,null
404,E2,null,null
405,0.80,null,null
406,0.85,null,null
407,0.90,null,null
408,0.95,null,null
409,1.00,null,null
410,E2,null,null
411,Minor Conflict ratio,null,null
412,0.20,null,null
413,0.12,null,null
414,0.15,null,null
415,0.09,null,null
416,0.10,null,null
417,-,null,null
418,0.06,null,null
419,a,null,null
420,0.05,null,null
421,0.03,null,null
422,0.00,null,null
423,0.00,null,null
424,0.80,null,null
425,0.85,null,null
426,0.90,null,null
427,0.95,null,null
428,1.00,null,null
429,0.80,null,null
430,0.85,null,null
431,0.90,null,null
432,0.95,null,null
433,1.00,null,null
434,E2,null,null
435,E2,null,null
436,Relative Sensitivity,null,null
437,RMS Error,null,null
438,0.15,null,null
439,0.10,null,null
440,r 0.0 0.1 0.2 0.3 0.4 0.5 0.6,null,null
441,0.05,null,null
442,0.00,null,null
443,0.5,null,null
444,0.6,null,null
445,0.7,null,null
446,0.8,null,null
447,0.9,null,null
448,1.0,null,null
449,0.5,null,null
450,0.6,null,null
451,0.7,null,null
452,0.8,null,null
453,0.9,null,null
454,1.0,null,null
455,"Figure 3: General mapping from GT-based to data-based reliability indicators, with 95% (dark grey) and 90% (light grey) prediction intervals.",null,null
456,5. DISCUSSION,null,null
457,"The last columns in Table 1 report point and 95% interval estimates of the stability of the 43 TREC collections we considered. Collections in the same group correspond to the same tasks, providing a historical perspective on the reliability of the collections used so far since 1994 and for a variety of tasks. For example, the average relative stability in the Ad Hoc collections was E2  [0.86, 0.93], which according to Figure 3 corresponds to   [0.65, 0.81]. For the Web Ad Hoc collections we find E2  [0.8, 0.93], which would correspond to   [0.53, 0.81]. There are large differences within some tasks, such as Web Distillation, Genomics, Terabyte",null,null
458,"and Enterprise. This is further evidence of the variability in D-study results due to the data used in the G-study. Except for a few particular cases though, the computation of confidence intervals smooths the problem. Across collections the averages are E2 , 0.88 and  ,"" 0.74, with some tasks having very low scores. According to Figure 3 the expected  correlation is 0.69 with variations from 0.49 to 0.95, that is, much lower than desired.""",null,null
459,"Figure 4 plots the historical trend of test collection reliability. The left plot shows that relative stability has varied in the (0.8,1) interval for the most part, but most importantly it suggests that the stability of collections has decreased very",null,null
460,399,null,null
461,Track Documents,null,null
462,Query Set,null,null
463,Measure ns nq,null,null
464,Ad Hoc 3 Disks 1 & 2,null,null
465,151-200,null,null
466,AP 40 50,null,null
467,Ad Hoc 4 Disks 2 & 3,null,null
468,201-250,null,null
469,AP 33 49,null,null
470,Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8,null,null
471,Disks 2 & 4 Disks 4 & 5 Disks 4 & 5 Disks 4 & 5,null,null
472,251-300 301-350  351-400  401-450 ,null,null
473,AP 94 50 AP 74 50 AP 103 50 AP 129 50,null,null
474,WebAdHoc 8 WebAdHoc 9 WebAdHoc 2001 WebAdHoc 2009 WebAdHoc 2010 WebAdHoc 2011,null,null
475,WebDistillation 2002 WebDistillation 2003 WebDistillation 2004,null,null
476,WebDiversity 2009 WebDiversity 2010 WebDiversity 2011,null,null
477,Novelty 2002,null,null
478,WT2g WT10g WT10g ClueWeb09 ClueWeb09 ClueWeb09,null,null
479,.GOV .GOV .GOV,null,null
480,ClueWeb09 ClueWeb09 ClueWeb09,null,null
481,Disks 4 & 5,null,null
482,401-450 ,null,null
483,AP 44 50,null,null
484,451-500,null,null
485,AP 104 50,null,null
486,501-550,null,null
487,AP 97 50,null,null
488,W1-W50  AP (MTC) 71 50,null,null
489,W51-W100 ,null,null
490,AP 56 48,null,null
491,W101-W150 ,null,null
492,AP 37 50,null,null
493,551-600,null,null
494,AP 71 49,null,null
495,TD1-TD50,null,null
496,AP 93 50,null,null
497,WT04,null,null
498,AP 74 75,null,null
499,W1-W50  -nDCG@20 48 50,null,null
500,W51-W100  -nDCG@20 32 50,null,null
501,W101-W150  -nDCG@20 25 50,null,null
502,50 from 300-450 ,null,null
503,F 42 49,null,null
504,Novelty 2003 AQUAINT,null,null
505,N1-N50,null,null
506,F 55 50,null,null
507,Novelty 2004 AQUAINT,null,null
508,N51-N100,null,null
509,F 60 50,null,null
510,GenomicsAdHoc 2003 GenomicsAdHoc 2004 GenomicsAdHoc 2005,null,null
511,Robust 2003,null,null
512,Robust 2004,null,null
513,Robust 2005,null,null
514,MEDLINE MEDLINE MEDLINE,null,null
515,Disks 4 & 5 Disks 4 & 5 AQUAINT,null,null
516,G1-G50,null,null
517,G51-G100,null,null
518,G101-150 50 from 301-450 & 601-650 ,null,null
519,301-450 & 601-700  50 from 301-700 ,null,null
520,AP 49 50 AP 43 50 AP 62 49,null,null
521,AP 78 100 AP 110 249 AP 74 50,null,null
522,Terabyte 2004,null,null
523,Terabyte 2005,null,null
524,Terabyte 2006,null,null
525,TerabyteAll 2006 EnterpriseExpert 2005 EnterpriseExpert 2006 EnterpriseExpert 2007 EnterpriseExpert 2008,null,null
526,GOV2 GOV2 GOV2 GOV2,null,null
527,W3C W3C CERC CERC,null,null
528,701-750  751-800  801-850  701-850 ,null,null
529,EX01-EX50 EX51-EX105 CE001-CE050 CE051-CE127,null,null
530,bpref 70 49 bpref 58 50 bpref 80 50 bpref 61 149,null,null
531,AP 37 50 AP 91 49 AP 55 50 AP 42 55,null,null
532,1MQ 2007,null,null
533,GOV2,null,null
534,MQ1-MQ10000 AP (MTC) 29 1692,null,null
535,1MQ 2008,null,null
536,GOV2,null,null
537,MQ10001-MQ20000 AP (MTC) 25 784,null,null
538,1MQ 2009 ClueWeb09,null,null
539,MQ20001-MQ60000 AP (MTC) 35 542,null,null
540,1MQ 2007,null,null
541,GOV2,null,null
542,MQ1-MQ10000,null,null
543,statAP 29 1153,null,null
544,1MQ 2008,null,null
545,GOV2,null,null
546,MQ10001-MQ20000,null,null
547,statAP 25 564,null,null
548,1MQ 2009 ClueWeb09,null,null
549,MQ20001-MQ60000,null,null
550,statAP 35 475,null,null
551,Medical 2011,null,null
552,NLP,null,null
553,M101-M135,null,null
554,bpref 127 34,null,null
555,Microblog 2011 Tweets2011,null,null
556,MB1-MB50,null,null
557,P@30 184 49,null,null
558,E^2 (nq ) 0.933 0.893-0.963 0.907 0.847-0.952 0.856 0.804-0.9 0.898 0.855-0.933 0.919 0.891-0.943 0.908 0.88-0.932,null,null
559,0.929 0.89-0.96 0.876 0.833-0.912 0.862 0.813-0.904 0.81 0.729-0.876 0.829 0.746-0.895 0.804 0.685-0.895,null,null
560,0.901 0.858-0.935 0.45 0.249-0.619 0.89 0.844-0.927,null,null
561,0.903 0.852-0.943 0.882 0.803-0.94 0.844 0.725-0.929,null,null
562,0.919 0.873-0.955 0.966 0.949-0.979 0.801 0.708-0.876,null,null
563,0.94 0.909-0.965 0.903 0.848-0.945 0.77 0.664-0.855,null,null
564,0.846 0.784-0.897 0.95 0.934-0.964 0.864 0.807-0.911,null,null
565,0.953 0.933-0.97 0.875 0.815-0.923 0.762 0.668-0.841 0.94 0.913-0.962,null,null
566,0.916 0.864-0.955 0.965 0.952-0.976 0.884 0.827-0.929 0.565 0.315-0.757,null,null
567,0.999 0.999-1 0.998 0.996-0.999 0.96 0.936-0.979 0.992 0.986-0.996 0.978 0.962-0.99 0.96 0.935-0.979,null,null
568,0.774 0.704-0.835,null,null
569,0.92 0.899-0.938,null,null
570,^ (nq) 0.786 0.661-0.88 0.79 0.658-0.89 0.62 0.488-0.732 0.806 0.714-0.875 0.799 0.71-0.864 0.701 0.59-0.787,null,null
571,0.83 0.728-0.904 0.76 0.662-0.835 0.711 0.598-0.801 0.619 0.473-0.744 0.662 0.513-0.787 0.702 0.537-0.835,null,null
572,0.84 0.762-0.898 0.315 0.144-0.492 0.747 0.643-0.832,null,null
573,0.847 0.759-0.911 0.804 0.676-0.899 0.719 0.535-0.865,null,null
574,0.792 0.671-0.883 0.944 0.91-0.967 0.181 0.1-0.301,null,null
575,0.87 0.792-0.925 0.768 0.64-0.868 0.422 0.269-0.586,null,null
576,0.509 0.384-0.636 0.824 0.768-0.872 0.693 0.564-0.797,null,null
577,0.877 0.809-0.924 0.648 0.501-0.774 0.427 0.283-0.575 0.719 0.617-0.812,null,null
578,0.824 0.713-0.905 0.939 0.909-0.96 0.785 0.674-0.87 0.28 0.11-0.498,null,null
579,0.998 0.997-0.999 0.988 0.979-0.995 0.908 0.854-0.951 0.982 0.97-0.991 0.969 0.946-0.986 0.929 0.886-0.963,null,null
580,0.497 0.348-0.628,null,null
581,0.818 0.747-0.869,null,null
582,"Table 1: Summary of all 43 TREC collections analyzed. Query sets with  are used in more than one collection. Query numbers in quotes are not official, but arbitrarily named for this paper. The last two columns report the point and 95% interval estimates of the GT-based reliability indicators.",null,null
583,"slightly with the years. The clear exceptions are again the Million Query Track collections, which specifically aimed at increasing the number of queries. Within each task it appears that stability tended to decrease as the tasks got older despite that query set sizes were normally unaltered. The second plot shows that this decrease in stability could be due to system variance getting smaller with the years. That is, systems perform more similarly as the tasks get older, indicating that retrieval techniques are generally improved. The right plot shows that query difficulty also varied within tasks. Sudden peaks may be explained by changes in the document set or in the task definition. The general trend suggests that queries are getting more alike with the years, further contributing to the decrease in reliability.",null,null
584,"Bodoff [5, §5] discusses the incorporation of the document set as another facet in Generalizability Theory, much like queries and systems, to measure variability due to documents [14]. He argues that it does not make sense in general, because we do no assign performance scores for indi-",null,null
585,"vidual documents but for sets of documents (e.g. the first k retrieved when computing P @k). In our case we could compare different editions of the same task but with different document sets to get a (weak) clue of the variability due to documents. For example, the Ad Hoc task of the Web Track shows quite different stability scores in the first three editions (WT2g and WT10g collections) compared to the last three editions (ClueWeb09), given that they all used the standard query set size of 50. Similarly, the Expert Search task in the Enterprise Track shows very different stability levels when using the W3C collection or the CERC collection. We must bear in mind though that these differences might actually be due to the systems and queries used, which varied from year to year.",null,null
586,"From the confidence intervals in Table 1, we used the models fitted in Section 4 to provide in Table 2 the estimated data-based reliability scores for all 43 collections. It is evident that expected  correlations are well below the desired 0.9 in most cases. In that line, some collections are clearly",null,null
587,400,null,null
588,Track,null,null
589,Ad Hoc 3 Ad Hoc 4 Ad Hoc 5 Ad Hoc 6 Ad Hoc 7 Ad Hoc 8,null,null
590,WebAdHoc 8 WebAdHoc 9 WebAdHoc 2001 WebAdHoc 2009 WebAdHoc 2010 WebAdHoc 2011 WebDistillation 2002 WebDistillation 2003 WebDistillation 2004 WebDiversity 2009 WebDiversity 2010 WebDiversity 2011 Novelty 2002 Novelty 2003 Novelty 2004,null,null
591,GenomicsAdHoc 2003 GenomicsAdHoc 2004 GenomicsAdHoc 2005,null,null
592,Robust 2003 Robust 2004 Robust 2005,null,null
593,Terabyte 2004 Terabyte 2005 Terabyte 2006 TerabyteAll 2006 EnterpriseExpert 2005 EnterpriseExpert 2006 EnterpriseExpert 2007 EnterpriseExpert 2008,null,null
594,1MQ 2007 1MQ 2008 1MQ 2009 1MQ 2007 1MQ 2008 1MQ 2009,null,null
595,Medical 2011,null,null
596,Microblog 2011,null,null
597,^,null,null
598,0.725-0.898 0.622-0.87,null,null
599,0.537-0.741 0.641-0.821,null,null
600,0.72-0.846 0.695-0.819,null,null
601,0.718-0.89 0.595-0.77 0.554-0.749 0.406-0.686 0.434-0.729 0.34-0.728,null,null
602,0.647-0.827 0.019-0.255 0.617-0.807,null,null
603,0.633-0.847 0.535-0.839 0.401-0.811,null,null
604,0.679-0.877 0.86-0.941,null,null
605,0.374-0.685,null,null
606,0.762-0.903 0.624-0.852 0.311-0.641,null,null
607,0.5-0.734 0.823-0.902 0.544-0.766,null,null
608,0.82-0.916 0.558-0.795,null,null
609,0.316-0.61 0.772-0.897,null,null
610,0.661-0.877 0.868-0.932 0.582-0.812 0.037-0.453,null,null
611,0.997-0.999 0.989-0.997 0.827-0.942 0.962-0.989 0.896-0.972 0.826-0.941,null,null
612,0.368-0.598,null,null
613,0.74-0.833,null,null
614,^AP,null,null
615,0.637-0.86 0.515-0.823 0.418-0.657 0.537-0.758 0.631-0.791,null,null
616,0.6-0.756,null,null
617,0.629-0.849 0.484-0.694 0.437-0.668,null,null
618,0.283-0.59 0.311-0.643 0.221-0.642,null,null
619,0.544-0.766 0.004-0.148 0.508-0.741,null,null
620,0.528-0.792 0.416-0.782 0.278-0.746,null,null
621,0.582-0.833 0.81-0.919,null,null
622,0.252-0.589,null,null
623,0.684-0.867 0.517-0.799 0.195-0.537,null,null
624,0.379-0.649 0.761-0.865 0.426-0.689,null,null
625,0.758-0.884 0.442-0.725,null,null
626,0.2-0.5 0.696-0.859,null,null
627,0.56-0.831 0.821-0.907 0.468-0.746,null,null
628,0.01-0.33,null,null
629,0.995-0.999 0.985-0.996,null,null
630,0.767-0.92 0.947-0.984 0.858-0.961 0.765-0.919,null,null
631,0.246-0.486,null,null
632,0.656-0.774,null,null
633,^ (%),null,null
634,58-83 45-79 35-60 47-72 58-76 54-72,null,null
635,57-82 42-65 37-62 22-53 25-59 16-59,null,null
636,48-73 0-10,null,null
637,44-70,null,null
638,46-76 35-74 22-70,null,null
639,52-80 78-90 19-53,null,null
640,63-84 45-76 14-47,null,null
641,31-60 72-84 36-64,null,null
642,72-86 38-68 14-44 65-83,null,null
643,50-80 79-89 40-70,null,null
644,0-26,null,null
645,99-100 98-100,null,null
646,73-90 94-98 83-95 73-90,null,null
647,19-42,null,null
648,60-74,null,null
649,^- (%),null,null
650,0.6-3.2 0.9-5.6 2.9-8.2 1.6-5.2 1.2-3.3 1.6-3.9,null,null
651,0.7-3.4 2.4-6.4 2.8-7.7 4.1-13.5 3.1-12.2 3.2-17,null,null
652,1.5-5 22.8-64.4,null,null
653,1.8-5.8,null,null
654,1.2-5.4 1.3-8.3 1.7-13.8,null,null
655,0.9-4.2 0.3-1.1 4.1-15.1,null,null
656,0.6-2.5 1.2-5.6 5.2-18.8,null,null
657,3.1-9.6 0.6-1.6,null,null
658,2.5-8,null,null
659,0.5-1.6 2-7.5,null,null
660,6-18.5 0.7-2.4,null,null
661,0.9-4.7 0.3-1,null,null
662,1.7-6.8 11.4-56,null,null
663,0-0 0-0 0.3-1.5 0-0.1 0.1-0.7 0.3-1.5,null,null
664,6.3-15.5,null,null
665,1.4-3,null,null
666,^+ (%),null,null
667,0.02-0.28 0.03-0.72 0.23-1.38 0.08-0.62 0.05-0.29 0.08-0.38,null,null
668,0.02-0.3 0.17-0.9 0.21-1.22 0.41-3.24 0.27-2.73 0.27-4.81,null,null
669,0.07-0.59 7.89-47.06,null,null
670,0.1-0.76,null,null
671,0.05-0.66 0.06-1.4,null,null
672,0.09-3.35,null,null
673,0.03-0.44 0-0.04,null,null
674,0.42-3.93,null,null
675,0.02-0.18 0.05-0.71 0.62-5.69,null,null
676,0.25-1.78 0.02-0.08 0.17-1.32,null,null
677,0.01-0.08 0.12-1.19,null,null
678,0.8-5.52 0.02-0.16,null,null
679,0.03-0.52 0.01-0.03,null,null
680,0.09-1 2.41-37.02,null,null
681,0-0 0-0 0-0.07 0-0 0-0.02 0-0.08,null,null
682,0.88-4.08,null,null
683,0.07-0.24,null,null
684,^a,null,null
685,0.01-0.03 0.01-0.06 0.03-0.08 0.02-0.05 0.01-0.03 0.02-0.04,null,null
686,0.01-0.03 0.02-0.06 0.03-0.08 0.04-0.13 0.03-0.12 0.03-0.17,null,null
687,0.01-0.05 0.23-0.64 0.02-0.06,null,null
688,0.01-0.05 0.01-0.08 0.02-0.14,null,null
689,0.01-0.04 0-0.01,null,null
690,0.04-0.15,null,null
691,0.01-0.02 0.01-0.05 0.05-0.19,null,null
692,0.03-0.09 0.01-0.02 0.02-0.08,null,null
693,0-0.02 0.02-0.07 0.06-0.18 0.01-0.02,null,null
694,0.01-0.05 0-0.01,null,null
695,0.02-0.07 0.11-0.56,null,null
696,0-0 0-0 0-0.01 0-0 0-0.01 0-0.01,null,null
697,0.06-0.15,null,null
698,0.01-0.03,null,null
699,^r (%),null,null
700,6-25 6-25 18-42 7-20 8-20 13-31,null,null
701,5-18 10-24 12-31 17-44 13-39 10-37,null,null
702,5-16 41-82 10-26,null,null
703,4-16 5-23 7-37,null,null
704,6-24 1-4,null,null
705,63-87,null,null
706,3-13 7-27 32-67,null,null
707,27-53 7-15,null,null
708,13-34,null,null
709,4-12 15-41 33-65 11-29,null,null
710,5-20 2-4,null,null
711,7-23 41-86,null,null
712,0-0 0-1 2-8 0-1 0-2 1-6,null,null
713,28-57,null,null
714,7-17,null,null
715,^,null,null
716,0.001-0.029 0.001-0.03,null,null
717,0.013-0.112 0.001-0.017 0.001-0.017 0.006-0.054,null,null
718,0-0.014 0.003-0.028 0.005-0.051 0.011-0.122 0.006-0.095,null,null
719,0.003-0.08,null,null
720,0.001-0.009 0.108-0.6,null,null
721,0.003-0.034,null,null
722,0-0.009 0.001-0.025 0.001-0.081,null,null
723,0.001-0.026 0-0,null,null
724,0.309-0.709,null,null
725,0-0.006 0.001-0.035 0.055-0.358,null,null
726,0.036-0.204 0.001-0.008 0.005-0.066,null,null
727,0-0.004 0.008-0.103,null,null
728,0.06-0.336 0.004-0.043,null,null
729,0-0.017 0-0,null,null
730,0.001-0.025 0.104-0.683,null,null
731,0-0 0-0 0-0.002 0-0 0-0 0-0.001,null,null
732,0.039-0.246,null,null
733,0.001-0.011,null,null
734,n^E2(.95) 37-114 47-169,null,null
735,106-233 69-161 58-117 69-130,null,null
736,40-118 92-190 102-220 135-354 107-311 112-438,null,null
737,65-154 585-2862,null,null
738,112-264,null,null
739,58-166 61-234 73-360,null,null
740,44-136 21-52,null,null
741,135-392,null,null
742,35-95 56-171 158-472,null,null
743,218-525 175-336,null,null
744,94-227,null,null
745,30-68 80-217 181-474 111-269,null,null
746,46-149 24-48,null,null
747,73-200 335-2277,null,null
748,11-38 16-59 219-710 88-304 107-421 194-628,null,null
749,129-273,null,null
750,62-105,null,null
751,n^(.95),null,null
752,130-487 116-484 348-999 136-381 150-389 257-662,null,null
753,102-355 189-484 236-640 327-1058 247-868 188-819,null,null
754,106-292 980-5631,null,null
755,288-791,null,null
756,93-301 107-457 149-826,null,null
757,124-457 33-94,null,null
758,2203-8579,null,null
759,78-250 146-536 657-2528,null,null
760,1087-3043 693-1428 242-733,null,null
761,77-220 279-947 702-2406 657-1761,null,null
762,100-383 39-93,null,null
763,143-459 1053-8458,null,null
764,30-104 81-313 534-1756 196-685 156-616 352-1156,null,null
765,383-1208,null,null
766,141-315,null,null
767,Table 2: Predicted reliability of all 43 TREC collections analyzed. All confidence intervals are based on the fits from Figure 3 at the endpoints of the 95% confidence intervals computed with equations (10) and (11).,null,null
768,"not reliable, such as the Web Distillation 2003, Genomics Ad Hoc 2005, Terabyte 2006, Enterprise Expert Search 2008, or the very recent Medical 2011 and Web Ad Hoc 2011. Regarding the expected RMS Error of absolute scores, we can see that collections are somewhat stable, but with clear exceptions such as Web Distillation 2003, Novelty 2004 and Enterprise Expert Search 2008.",null,null
769,"The last two columns in Table 2 report intervals on the number of queries, as per equations (12) and (13), required to achieve 0.95 stability. In general the number of queries needs to be at least doubled, and in many cases a few hundred queries seem to be needed. This is particularly interesting for the most recent collections, such as Web Ad Hoc 2010 and 2011, Medical 2011 and Microblog 2011, which stick to the traditional size of 50 queries but need about 200. What becomes clear from these figures is that the ideal size of a collection depends greatly on the task it will be used for, and thus it is not appropriate to fix some acceptable size such as 50 or 100 throughout tasks. Each task has different characteristics and should be analyzed accordingly.",null,null
770,6. CONCLUSIONS,null,null
771,"In this paper we discussed the measurement of test collection reliability from the perspective of traditional databased methodologies and of Generalizability Theory. GT is regarded as a more appropriate, easy to use, and powerful method to assess reliability, but it has two drawbacks. First, we showed that GT is very sensitive to the particular sample of systems and queries used to estimate reliability of a larger query set. We showed that about 50 systems and 50 queries are needed for robust estimates of collection reliability. Therefore, researchers should be cautious in using GT when building new collections from scratch. To account for all this variability we discussed a more robust approach based on interval estimates of the stability indicators, which helps in making more appropriate decisions regarding number of queries or different structure in the experimental design. Second, we empirically established a mapping between GT-based and traditional data-based indicators to help interpreting results from GT which, otherwise, do not have a",null,null
772,401,null,null
773,Ad Hoc Web adhoc Web distillation Web diversity Novelty Genomics Robust Terabyte Enterprise 1MQ MTC 1MQ statAP Medical Microblog,null,null
774,Linear trend,null,null
775,E^2 0.5 0.6 0.7 0.8 0.9 1.0,null,null
776,Relative Stability,null,null
777,1995,null,null
778,2000,null,null
779,Year,null,null
780,2005,null,null
781,2010,null,null
782,^s2 (% of total),null,null
783,0,null,null
784,5 10 15 20 25,null,null
785,Variability due to Systems,null,null
786,1995,null,null
787,2000,null,null
788,Year,null,null
789,2005,null,null
790,2010,null,null
791,^2q (% of total) 30 40 50 60 70 80 90,null,null
792,Variability due to Queries,null,null
793,1995,null,null
794,2000,null,null
795,Year,null,null
796,2005,null,null
797,2010,null,null
798,"Figure 4: Historical trend of relative stability (left), variability due to systems (middle) and to queries (right).",null,null
799,"clear and easily understandable meaning. Based on these results, we reviewed the reliability of 43 TREC test collections, evidencing that some of them are very little reliable. We show that the traditional choice of 50 queries is clearly not enough even for stable rankings, and in most cases a couple hundred queries are needed. Our results also show that the ideal query set size varies significantly across tasks, suggesting that we avoid the use of some fixed size such as 50 or 100 and that we analyze tasks and collections separately.",null,null
800,"There are two clear lines for future research. First, we completely ignored the assessor facet in our study. It is evident that different assessors provide different results, so it would be interesting to include them in the analysis. Second, although we fitted the theoretically correct models, it is clear that they can be improved (see for instance Power and RMS Error in Figure 3). IR evaluation experiments generally violate assumptions of GT, such as normality of distributions and random sampling, so different models and features to better fit the actual data should be investigated.",null,null
801,We created some scripts for the statistical software R that can help researchers perform all these computations to easily assess the reliability of custom test collection designs. They can be downloaded from http://julian-urbano.info.,null,null
802,7. REFERENCES,null,null
803,"[1] J. Allan, J. A. Aslam, B. Carterette, V. Pavlu, and E. Kanoulas. Million Query Track 2008 Overview. In Text REtrieval Conference, 2008.",null,null
804,"[2] J. Allan, B. Carterette, J. A. Aslam, V. Pavlu, B. Dachev, and E. Kanoulas. Million Query Track 2007 Overview. In Text REtrieval Conference, 2007.",null,null
805,"[3] C. Arteaga, S. Jeyaratnam, and G. A. Franklin. Confidence Intervals for Proportions of Total Variance in the Two-Way Cross Component of Variance Model. Communications in Statistics: Theory and Methods, 11(15):1643­1658, 1982.",null,null
806,"[4] D. Banks, P. Over, and N.-F. Zhang. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval, 1(1-2):7­34, 1999.",null,null
807,"[5] D. Bodoff. Test Theory for Evaluating Reliability of IR Test Collections. Information Processing and Management, 44(3):1117­1145, 2008.",null,null
808,"[6] D. Bodoff and P. Li. Test Theory for Assessing IR Test Collections. In ACM SIGIR, pages 367­374, 2007.",null,null
809,"[7] R. L. Brennan. Generalizability Theory. Springer, 2001.",null,null
810,"[8] C. Buckley and E. M. Voorhees. Evaluating Evaluation Measure Stability. In ACM SIGIR, pages 33­34, 2000.",null,null
811,"[9] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. Evaluation Over Thousands of Queries. In ACM SIGIR, pages 651­658, 2008.",null,null
812,"[10] B. Carterette, V. Pavlu, E. Kanoulas, J. A. Aslam, and J. Allan. If I Had a Million Queries. In ECIR, pages 288­300, 2009.",null,null
813,"[11] L. S. Feldt. The Approximate Sampling Distribution of Kuder-Richardson Reliability Coefficient Twenty. Psychometrika, 30(3):357­370, 1965.",null,null
814,"[12] E. Kanoulas and J. A. Aslam. Empirical Justification of the Gain and Discount Function for nDCG. In ACM CIKM, pages 611­620, 2009.",null,null
815,"[13] W.-H. Lin and A. Hauptmann. Revisiting the Effect of Topic Set Size on Retrieval Error. In ACM SIGIR, pages 637­638, 2005.",null,null
816,"[14] S. Robertson and E. Kanoulas. On Per-Topic Variance in IR Evaluation. In ACM SIGIR, pages 891­900, 2012.",null,null
817,"[15] T. Sakai. On the Reliability of Information Retrieval Metrics Based on Graded Relevance. Information Processing and Management, 43(2):531­548, 2007.",null,null
818,"[16] M. Sanderson. Test Collection Based Evaluation of Information Retrieval Systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.",null,null
819,"[17] M. Sanderson and J. Zobel. Information Retrieval System Evaluation: Effort, Sensitivity, and Reliability. In ACM SIGIR, pages 162­169, 2005.",null,null
820,"[18] R. J. Shavelson and N. M. Webb. Generalizability Theory: A Primer. Sage Publications, 1991.",null,null
821,"[19] J. Urbano, M. Marrero, and D. Mart´in. A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation. In ACM SIGIR, 2013.",null,null
822,"[20] E. M. Voorhees. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In ACM SIGIR, pages 315­323, 1998.",null,null
823,"[21] E. M. Voorhees. Topic Set Size Redux. In ACM SIGIR, pages 806­807, 2009.",null,null
824,"[22] E. M. Voorhees and C. Buckley. The Effect of Topic Set Size on Retrieval Experiment Error. In ACM SIGIR, pages 316­323, 2002.",null,null
825,"[23] E. Yilmaz, J. A. Aslam, and S. Robertson. A New Rank Correlation Coefficient for Information Retrieval. In ACM SIGIR, pages 587­594, 2008.",null,null
826,"[24] J. Zobel. How Reliable are the Results of Large-Scale Information Retrieval Experiments? In ACM SIGIR, pages 307­314, 1998.",null,null
827,402,null,null
828,,null,null

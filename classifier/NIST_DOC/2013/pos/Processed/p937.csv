,sentence,label,data
0,"Relating Retrievability, Performance and Length",null,null
1,Colin Wilkie and Leif Azzopardi,null,null
2,"School of Computing Science, University of Glasgow",null,null
3,"Glasgow, United Kingdom",null,null
4,"{Colin.Wilkie,Leif.Azzopardi}@glasgow.ac.uk",null,null
5,ABSTRACT,null,null
6,"Retrievability provides a different way to evaluate an Information Retrieval (IR) system as it focuses on how easily documents can be found. It is intrinsically related to retrieval performance because a document needs to be retrieved before it can be judged relevant. In this paper, we undertake an empirical investigation into the relationship between the retrievability of documents, the retrieval bias imposed by a retrieval system, and the retrieval performance, across different amounts of document length normalization. To this end, two standard IR models are used on three TREC test collections to show that there is a useful and practical link between retrievability and performance. Our findings show that minimizing the bias across the document collection leads to good performance (though not the best performance possible). We also show that past a certain amount of document length normalization the retrieval bias increases, and the retrieval performance significantly and rapidly decreases. These findings suggest that the relationship between retrievability and effectiveness may offer a way to automatically tune systems.",null,null
7,Categories and Subject Descriptors: H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval - Performance Evaluation,null,null
8,"Terms Theory, Experimentation Keywords Retrievability, Simulation",null,null
9,1. INTRODUCTION,null,null
10,"Traditionally IR systems are evaluated in terms of efficiency and performance [13]. However IR systems can also be evaluated in terms of retrievability [3], which assesses how often documents are retrieved (as opposed to the speed of retrieval and the quality of retrieval respectively). Retrievability is fundamental to IR because it precedes relevancy [3]. In this paper, we investigate the relationship between retrievability and retrieval performance in the context of ad-hoc topic retrieval for two standard best match retrieval models. To this end, we shall first formally define retrievability be-",null,null
11,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
12,"fore discussing how it relates to performance. Next, we perform an empirical analysis where we hypothesize that lower retrievability bias will lead to better retrieval performance.",null,null
13,2. BACKGROUND,null,null
14,"In [3], retrievability was defined as a measure that provides an indication of how easily a document could be retrieved under a particular configuration of an IR system. Put formally the retrievability r(d) of a document d with respect to an IR system is:",null,null
15,"r(d)  f (kdq, {c, g})",null,null
16,(1),null,null
17,qQ,null,null
18,"where q is a query in a very large query set Q, kdq is the rank at which d is retrieved given q, and f (kdq, c) is an access function which denotes how retrievable the document d is for the query q given the rank cutoff c. This implies that retrievability of a document is obtained by summing over all possible queries Q. The more queries that retrieve the document, the more retrievable the document is.",null,null
19,"A simple measure of retrievability is a cumulative-based approach, which employs an access function f (kdq, c), such that if d is retrieved in the top c documents given q, then f (kdq, c) ,"" 1 else f (kdq, c) "","" 0. Essentially, this measure provides an intuitive value for each document as it is simply the number of times that the document is retrieved in the top c documents. A gravity based approach instead weights the rank at which the document is retrieved, as follows: f (kdq, g) "","" 1/kdqg. This introduces a discount g, such that documents lower down the list are assumed to be less retrievable. Retrievability Bias: To quantify the retrievability bias across the collection, the Gini coefficient [8] is often used [3, 5, 6] as it measures the inequality within a population. For example, if all documents were equally retrievable according to r(d), then the Gini coefficient would be zero (denoting equality), but if all but one document had r(d) "","" 0 then the Gini coefficient would be one (denoting total inequality). Usually, most documents have some level of retrievability and the Gini coefficient is somewhere between zero and one. Essentially, the Gini coefficient provides an indication of the level of inequality between documents given how easily they can be retrieved using a particular retrieval system and configuration. Uses: Retrievability - and the theory of - has been used in numerous contexts, for example, in the creation of reverted indexes that improve the efficiency and performance of retrieval systems by capitalizing on knowing what terms""",null,null
20,937,null,null
21,"within a document makes that document more retrievable [10]. Retrievability has also been used to study search engine and retrieval system bias on the web [2] and within patent collections [4] to improve the efficiency of systems when pruning [14]. It has also been related to navigability when tagging information to improve how easily users browsing through the collection could find documents [12]. While these show that retrievability is useful in a number of respects, here, we are interested in how retrievability relates to performance. There have been a few works exploring this research direction in different ways [3, 1, 5, 6]. Relating Retrievability and Performance: In [1], Azzopardi discusses the relationship with respect to the definition of retrievability and claim that a purely random IR system would ensure equal retrievability (resulting in a Gini ,"" 0). However, the author argues that this would also result in very poor retrieval performance. Conversely, if an oracle IR system retrieved only the set of known relevant documents, and only these documents, then there would be a very high inequality in terms of retrievability across the collection. They suggest that neither extreme is desirable and instead suggest that there is likely to be a trade-off between retrievability and retrieval performance. In [3], it is acknowledged that some level of bias is necessary because a retrieval system must try and discriminate relevant from non-relevant. The preliminary study conducted in [1] investigating this relationship on two small TREC test collections showed that as retrieval performance increased, the retrievability bias tended to decrease: suggesting a positive and useful relationship.""",null,null
22,"In [5], Bashir and Rauber studied the effect of Pseudo Relevance Feedback (PRF) on performance and retrievability. They found that standard Query Expansion methods, while increasing performance, also increased the retrievability bias. To combat the increase in bias, they devised a method of PRF that used clustering; this resulted in a reduction in bias, as well as an increase in performance over other QE techniques. When employing their PRF technique to patent retrieval, they showed that the decrease in bias led to improved recall for prior art search [6]. This later work suggested that there may be a positive correlation between recall and retrievability. In [7], they compared a number of retrieval models in terms of retrievability bias and performance as a way to rank different retrieval systems. They found that there was a positive correlation between the bias and performance for standard best match retrieval models. It is important to note that since retrievability can be estimated without recourse to relevance judgements it provides an attractive alternative for evaluation.",null,null
23,3. EXPERIMENTAL METHOD,null,null
24,"Previous work suggests that the relationship between retrievability bias and retrieval performance is much more complicated than previously thought and the relationship is somewhat unclear. In this paper, we shall perform a more detailed analysis across larger TREC collections and explicitly plot the relationship between the retrievability bias and retrieval performance in order to form a deeper understanding of how they relate to each other.",null,null
25,"To this end, three TREC test collections were used AP, Aquaint (AQ) and DotGov (DG) along with their corresponding ad-hoc retrieval topics (see Table 1 for details). These collections had stop words removed and were Porter",null,null
26,"stemmed. For the purposes of this study, we will focus on the effect of document length normalization within two standard retrieval models (Okapi BM25 and DFRs PL2). This is due to the fact that retrieval models will often favour longer documents which introduces an undesirable retrieval bias that if accounted for, has been shown to improve performance [11]. For each of these retrieval models we shall investigate how retrievability bias and performance changes as we manipulate the normalization parameter b and c respectively. In our experiments for BM25's b parameter we used values between 0 and 1 with steps of 0.11, and for PL2's c parameter we used values 0.1, 1, . . . 10, 100 with steps of 1 between 1 and 10.",null,null
27,"For each collection, model and parameter value, we recorded the retrieval performance (Mean Average Precision (MAP) and Precision @ 10 (P@10)) and calculated the retrievability bias using the approach employed in prior work [3, 1, 5, 7]. This was performed as follows; we extracted all terms that co-occurred more than once with each other from the collection and used these as bigram queries (Table 1 shows the number of queries per collection issued). These queries were issued to the retrieval system given its parametrization and the results were used to compute retrievability measures. We computed both cumulative scores with cut-offs of 5, 10, 20, 50, 70, 100, and gravity based scores with cut-off of 100 and  (where  is the discount factor) values of 0.5, 1, 1.5 and 2. For brevity, we only report on a subset of these. These were used to compute the corresponding Gini coefficient for each measure, model, normalization parameter and collection.",null,null
28,Collections Docs,null,null
29,Collection Type Trec Topics,null,null
30,# Bigrams/Queries,null,null
31,"AP 164,597",null,null
32,News 1-200 81964,null,null
33,"AQ 1,033,461",null,null
34,News 303-369 273245,null,null
35,"DG 1,247,753",null,null
36,Web 551 - 600,null,null
37,337275,null,null
38,Table 1: Collection Statistics,null,null
39,4. RESULTS,null,null
40,During the course of our analysis we examined the following research questions:,null,null
41,"How does the retrievability bias change across length normalization parameters? In Figure 1, the left hand plots show the relationship between the retrievability bias (denoted by the Gini coefficient for different retrievability measures) and the parameter settings for BM25 and PL2. Immediately we can see that as the parameter setting is manipulated, the retrievability bias changes. For BM25, the minimum bias was when b,""0.7 on AP and AQ, and b"",""0.9 on DG, whereas on PL2 the minimum bias was was between c"",1 and c,""3 (see Table 2). This was regardless of the Gini measure used i.e. the cumulative and gravity based measures resulted in the same minimum. Subsequently, for the remainder of this paper, we will use the gravity based retrievability measure when g "","" 1. What was different between Gini measures was the magnitude of the bias. For example, when the cut-off is low then the bias observed was higher than when the cutoff was increased. This finding is consistent with prior work [3]. When the parameter value of the retrieval was increased or decreased, then we observed that retrieval biased increased, sometimes quite dramatically. This suggests that either longer or shorter documents""",null,null
42,"1When b ,"" 0, this is equivalent to BM15 and when b "", 1 this is equivalent to BM11",null,null
43,938,null,null
44,Gini Coefficient,null,null
45,1 0.95,null,null
46,0.9 0.85,null,null
47,0.8 0.75,null,null
48,0.7 0.65,null,null
49,0.6 0.55,null,null
50,0.5 0,null,null
51,1 0.95,null,null
52,0.9 0.85,null,null
53,0.8 0.75,null,null
54,0.7 0.65,null,null
55,0,null,null
56,Aquaint BM25 gini vs b,null,null
57,C10 C100 G0.5 G1.0 G1.5,null,null
58,0.2,null,null
59,0.4,null,null
60,0.6,null,null
61,0.8,null,null
62,BM25 b Parameter Values,null,null
63,.Gov BM25 gini vs b,null,null
64,C10 C100 G0.5 G1.0 G1.5,null,null
65,0.2,null,null
66,0.4,null,null
67,0.6,null,null
68,0.8,null,null
69,BM25 b Parameter Values,null,null
70,Gini Coefficient,null,null
71,1 0.95,null,null
72,0.9 0.85,null,null
73,0.8 0.75,null,null
74,0.7 0.65,null,null
75,1,null,null
76,0,null,null
77,0.98,null,null
78,0.96,null,null
79,0.94,null,null
80,Gini Coefficient,null,null
81,0.92,null,null
82,0.9,null,null
83,0.88,null,null
84,0.86,null,null
85,0.84,null,null
86,0.82,null,null
87,0.8,null,null
88,1,null,null
89,0,null,null
90,Aquaint PL2 gini vs. c,null,null
91,C10 C100 G0.5 G1.0 G1.5,null,null
92,2,null,null
93,4,null,null
94,6,null,null
95,8,null,null
96,10,null,null
97,PL2 c Parameter Values,null,null
98,.Gov PL2 gini vs. c,null,null
99,2,null,null
100,4,null,null
101,6,null,null
102,8,null,null
103,PL2 c Parameter Values,null,null
104,C10 C100 G0.5 G1.0 G1.5,null,null
105,10,null,null
106,Average R(d),null,null
107,Average R(d),null,null
108,Aquaint BM25 (G1.0) R(d) vs. Length,null,null
109,10,null,null
110,0.1,null,null
111,9,null,null
112,0.7,null,null
113,8 0.9,null,null
114,7,null,null
115,6,null,null
116,5,null,null
117,4,null,null
118,3,null,null
119,2,null,null
120,1,null,null
121,0,null,null
122,0,null,null
123,500,null,null
124,1000,null,null
125,1500,null,null
126,Average Length in Bucket,null,null
127,.Gov BM25 (G1.0) R(d) vs. Length,null,null
128,20,null,null
129,0.1,null,null
130,18,null,null
131,0.7,null,null
132,16 0.9,null,null
133,14,null,null
134,12,null,null
135,10,null,null
136,8,null,null
137,6,null,null
138,4,null,null
139,2,null,null
140,0,null,null
141,0,null,null
142,5000,null,null
143,10000,null,null
144,15000,null,null
145,Average Length in Bucket,null,null
146,Average R(d),null,null
147,Average R(d),null,null
148,Aquaint PL2 (G1.0) R(d) vs. Length,null,null
149,10,null,null
150,1,null,null
151,9,null,null
152,3,null,null
153,8 10,null,null
154,7,null,null
155,6,null,null
156,5,null,null
157,4,null,null
158,3,null,null
159,2,null,null
160,1,null,null
161,0,null,null
162,0,null,null
163,500,null,null
164,1000,null,null
165,1500,null,null
166,Average Length in Bucket,null,null
167,.Gov PL2 (G1.0) R(d) vs. Length,null,null
168,20,null,null
169,1,null,null
170,18,null,null
171,3,null,null
172,16,null,null
173,10,null,null
174,14,null,null
175,12,null,null
176,10,null,null
177,8,null,null
178,6,null,null
179,4,null,null
180,2,null,null
181,0,null,null
182,0,null,null
183,5000,null,null
184,10000,null,null
185,Average Length in Bucket,null,null
186,15000,null,null
187,Gini Coefficient,null,null
188,Figure 1: Left: Retrieval Bias versus b/c. Right: Retrievability versus Document Length,null,null
189,Collections AP,null,null
190,Aquaint .Gov,null,null
191,Model,null,null
192,BM25 PL2,null,null
193,BM25 PL2,null,null
194,BM25 PL2,null,null
195,Minimum Gini (G1.0) b/c Gini MAP P10,null,null
196,0.7 0.643 0.232 0.354 3 0.701 0.235 0.374,null,null
197,0.7 0.701 0.162 0.316 2 0.746 0.169 0.331,null,null
198,0.9 0.749 0.167 0.222 1 0.872 0.182 0.220,null,null
199,Maximum Map (G1.0) b/c Gini MAP P10,null,null
200,0.3 0.708 0.239* 0.378* 3 0.701 0.235 0.374,null,null
201,0.3 0.785 0.185* 0.390* 6 0.774 0.178 0.374*,null,null
202,0.6 0.707 0.179 0.216 2 0.883 0.184 0.222,null,null
203,Min. Gini (G1.0) Corr. Gini-MAP Gini-P10,null,null
204,-0.075 -0.978*,null,null
205,0.184 -0.925,null,null
206,0.186 -0.844*,null,null
207,0.451 -0.523,null,null
208,-0.646 -0.887,null,null
209,-0.775 -0.916*,null,null
210,Table 2: Minimum Gini/Maximum MAP and corresponding parameter settings/values.  represent statistical significance. Far right columns show the Pearson's Correlation between Retrievability Bias and performance.,null,null
211,"were being favoured depending on the setting of b and c. In BM25, as b tends to 1, longer documents are penalized, while when b tends to 0, longer documents are favoured. For PL2, as c tends to 0 then longer documents are penalized, and as c tends to infinity, longer documents are favoured.",null,null
212,"How does the retrievability of documents change across length normalization parameters? To examine this intuition, we sorted the documents in each collection by length and placed them into buckets. We then computed the mean length within each bucket, and the mean retrievability r(d) given the documents in the bucket. The plots on the right in Figure 1 show how the retrievability changes when the length normalization parameter changes on BM25/PL2 and AQ/DG for a subset of parameters with a retrievability measure of g ,"" 1. The plots clearly indicate that when the retrieval bias is minimized (for example, when b "", 0.7 on AQ) the retrievability across length is about as equal/fair as it gets across the collection. This shows that the measures of retrievability bias (i.e. the Gini coefficients) capture the bias towards longer and shorter documents as the document length parameter is manipulated.",null,null
213,"What is the relationship between Performance and Retrievability? In Figure 2, we plotted the Gini coefficient against MAP for each collection using the results from BM25. Similar plots can be found in Figure 3 for other retrieval models and other performance measures. On these plots, a large diamond has been added to the lines to indicate the length normalization parameter that favours longer documents (ie. b , 0 and c , 100). As the parameter value is increased/decreased for BM25/PL2 the retrieval model",null,null
214,setting favours shorter documents (as shown in the plots in Figure 1).,null,null
215,"The plots provide a number of key insights into the relationship between retrievability bias and performance. Firstly, the relationship is non-linear with a trend that suggests that minimizing bias leads to better performance. If we consider the relationship in Figure 2 for DotGov, we can see that a reduction in bias leads to successive improvements in terms of performance for MAP (and P@10). With this collection, the effect is the most pronounced and minimizing retrieval bias does tend to the best retrieval performance. Once too much length normalization has occurred, such that shorter documents are overly favoured, then the retrievability bias increases and performance begins to decrease.",null,null
216,"When we examine the relationship for AP and AQ, we see that as shorter documents become more favoured, a trade-off between bias and performance quickly develops, but again once the minimum bias point has been reached, the kick back (i.e. loss in performance and increase in bias) is much more pronounced. These findings suggest that as document length normalization is applied in order to penalize longer documents, going past the point of minimum bias will degrade performance and retrievability.",null,null
217,"Table 2 shows the performance of the retrieval models when bias is minimized and when performance is maximized. Included in the table are the results of t-tests conducted to determine whether there was a statistical difference in performance between the two settings (assuming p < 0.05,  indicates a significant difference). For AP and AQ, we found there was a significant difference for BM25 (both MAP and",null,null
218,939,null,null
219,"P@10), however the differences were less pronounced for PL2 and on DotGov. These findings suggest that minimizing bias may not lead to the best performance. It does however, tend to give reasonably good retrieval performance that on many occasions, is not significantly different from the best possible retrieval performance. We speculate that the mis-match between may be due to the length bias with the TREC relevance pools as discovered in [9]. Thus, in the absence of relevance judgements tuning a retrieval system such that it minimizes retrieval bias is likely to be a good starting point.",null,null
220,"AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini",null,null
221,1,null,null
222,AP,null,null
223,0.95,null,null
224,Aquaint,null,null
225,.Gov,null,null
226,0.9,null,null
227,0.85,null,null
228,Gini,null,null
229,0.8,null,null
230,0.75,null,null
231,0.7,null,null
232,0.65,null,null
233,0.05,null,null
234,0.1,null,null
235,0.15,null,null
236,0.2,null,null
237,0.25,null,null
238,MAP,null,null
239,Figure 2: MAP vs. Gini for all collections on BM25.,null,null
240,5. SUMMARY,null,null
241,"In this paper we have analysed the relationship between retrievability bias and performance in the context of adhoc retrieval across length normalization parameters for two standard IR models. Empirically, we have shown that the relationship is much more complex that previously thought and in fact non-linear. Nonetheless, we have shown that reducing the bias within the collection leads to reasonably good performance, and crucially, if too much document length normalization is performed then this will invariably result in a degradation of performance and an increase in bias. This is a useful finding suggesting that retrievability could be used to tune retrieval systems without recourse to relevance judgements. Future work will be directed to studying the relationship between retrievability and performance (in terms of MAP and P@10) in more detail across other collections and across different parameter settings (i.e. query parameters, smoothing parameters) and with different retrieval models (language models, query expansion, link/click evidence, etc).",null,null
242,"Acknowledgments This work is supported by the EPSRC Project, Models and Measures of Findability (EP/K000330/1).",null,null
243,6. REFERENCES,null,null
244,"[1] L. Azzopardi and R. Bache. On the relationship between effectiveness and accessibility. In Proc. of the 33rd international ACM SIGIR, pages 889­890, 2010.",null,null
245,"[2] L. Azzopardi and C. Owens. Search engine predilection towards news media providers. In Proc. of the 32nd ACM SIGIR, pages 774­775, 2009.",null,null
246,Gini,null,null
247,Gini,null,null
248,"AP, Aquaint & .Gov BM25 (G1.0) MAP vs. gini",null,null
249,1 AP,null,null
250,0.95,null,null
251,Aquaint,null,null
252,.Gov 0.9,null,null
253,0.85,null,null
254,0.8,null,null
255,0.75,null,null
256,0.7,null,null
257,0.65,null,null
258,0.05,null,null
259,0.1,null,null
260,0.15,null,null
261,0.2,null,null
262,0.25,null,null
263,MAP,null,null
264,"AP, Aquaint & .Gov BM25 (G1.0) P@10 vs. gini",null,null
265,1 AP,null,null
266,0.95,null,null
267,Aquaint,null,null
268,.Gov 0.9,null,null
269,0.85,null,null
270,0.8,null,null
271,0.75,null,null
272,0.7,null,null
273,0.65,null,null
274,0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 P@10,null,null
275,Gini,null,null
276,Gini,null,null
277,"AP, Aquaint & .Gov PL2 (G1.0) MAP vs. gini",null,null
278,1 AP,null,null
279,0.95,null,null
280,Aquaint,null,null
281,.Gov,null,null
282,0.9,null,null
283,0.85,null,null
284,0.8,null,null
285,0.75,null,null
286,0.7,null,null
287,0.65,null,null
288,0.05,null,null
289,0.1,null,null
290,0.15,null,null
291,0.2,null,null
292,0.25,null,null
293,MAP,null,null
294,"AP, Aquaint & .Gov PL2 (G1.0) P@10 vs. gini",null,null
295,1 AP,null,null
296,0.95,null,null
297,Aquaint,null,null
298,.Gov,null,null
299,0.9,null,null
300,0.85,null,null
301,0.8,null,null
302,0.75,null,null
303,0.7,null,null
304,0.65 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 P@10,null,null
305,Figure 3: MAP (Top plots) and P@10 (Bottom plots) vs. Gini for all collections on BM25 (right plots) and PL2 (left plots).,null,null
306,"[3] L. Azzopardi and V. Vinay. Retrievability: An evaluation measure for higher order information access tasks. In Proc. of the 17th ACM CIKM, pages 561­570, 2008.",null,null
307,"[4] R. Bache. Measuring and improving access to the corpus. In Current Challenges in Patent Information Retrieval, volume 29 of The Information Retrieval Series, pages 147­165. 2011.",null,null
308,"[5] S. Bashir and A. Rauber. Improving retrievability of patents with cluster-based pseudo-relevance feedback documents selection. In Proc. of the 18th ACM CIKM, pages 1863­1866, 2009.",null,null
309,"[6] S. Bashir and A. Rauber. Improving retrievability of patents in prior-art search. In Proc. of the 32nd ECIR, pages 457­470, 2010.",null,null
310,"[7] S. Bashir and A. Rauber. Improving retrievability and recall by automatic corpus partitioning Transactions on large-scale data- and knowledge-centered systems ii. chapter I, pages 122­140. 2010.",null,null
311,"[8] J. Gastwirth. The estimation of the lorenz curve and gini index. The Review of Economics and Statistics, 54:306­316, 1972.",null,null
312,"[9] D. E. Losada, L. Azzopardi, and M. Baillie. Revisiting the relationship between document length and relevance. In Proc. of the 17th ACM CIKM'08, pages 419­428, 2008.",null,null
313,"[10] J. Pickens, M. Cooper, and G. Golovchinsky. Reverted indexing for feedback and expansion. In Proc. of the 19th ACM CIKM, pages 1049­1058, 2010.",null,null
314,"[11] A. Singhal, C. Buckley, and M. Mitra. Pivoted document length normalization. In Proc. of the 19th ACM SIGIR, pages 21­29, 1996.",null,null
315,"[12] A. Singla and I. Weber. Tagging and navigability. In Proc. of the 19th WWW, pages 1185­1186, 2010.",null,null
316,[13] C. J. van Rijsbergen. Information Retrieval. 1979. [14] L. Zheng and I. J. Cox. Document-oriented pruning of,null,null
317,"the inverted index in information retrieval systems. In Proc. of the 2009 WIANA, pages 697­702, 2009.",null,null
318,940,null,null
319,,null,null

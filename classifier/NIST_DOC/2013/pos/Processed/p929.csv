,sentence,label,data
0,Assessor Disagreement and Text Classifier Accuracy,null,null
1,William Webber,null,null
2,College of Information Studies University of Maryland,null,null
3,United States of America wew@umd.edu,null,null
4,Jeremy Pickens,null,null
5,"Catalyst Repository Systems Denver, CO",null,null
6,United States of America jpickens@catalystsecure.com,null,null
7,ABSTRACT,null,null
8,"Text classifiers are frequently used for high-yield retrieval from large corpora, such as in e-discovery. The classifier is trained by annotating example documents for relevance. These examples may, however, be assessed by people other than those whose conception of relevance is authoritative. In this paper, we examine the impact that disagreement between actual and authoritative assessor has upon classifier effectiveness, when evaluated against the authoritative conception. We find that using alternative assessors leads to a significant decrease in binary classification quality, though less so ranking quality. A ranking consumer would have to go on average 25% deeper in the ranking produced by alternative-assessor training to achieve the same yield as for authoritative-assessor training.",null,null
9,Categories and Subject Descriptors,null,null
10,H.3.4 [Information Storage and Retrieval]: Systems and software--performance evaluation.,null,null
11,General Terms,null,null
12,Evaluation,null,null
13,Keywords,null,null
14,"Text classification, evaluation, assessor disagreement",null,null
15,1. INTRODUCTION,null,null
16,"Text classification based upon machine learning is a useful tool for text retrieval tasks on corpora with many relevant documents, where high recall is required, and where the searcher is willing to devote significant effort to the task. One such environment is that of e-discovery--the retrieval of responsive documents in civil law-- and classification technologies have been widely deployed there.",null,null
17,"To learn a relevance model, a machine learner is provided with example documents, annotated by a human assessor. The assessor making the relevance judgments may not, however, be the person whose conception of relevance is authoritative. In e-discovery, for instance, senior lawyers commonly delegate assessment to junior lawyers or contract paralegals, due to time and cost constraints.",null,null
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
19,"Human assessors frequently disagree on document relevance [8], which questions the use of non-authoritative assessors to train text classifiers. How reliable are classifiers trained by non-authoritative assessors when evaluated by the authoritative conception of relevance? Does the classifier compensate for the disagreement between assessors, or does it amplify it?",null,null
20,2. PREVIOUS WORK,null,null
21,"In an experiment reported by Voorhees [8], TREC AdHoc documents were assessed by two alternative assessors, and high levels of assessor disagreement were observed. Based on simulation experiments, Carterette and Soboroff [2] find that overly-conservative assessors (those who find fewer documents relevant) distort retrieval effectiveness evaluation less than liberal ones do.",null,null
22,"In the e-discovery domain, Grossman and Cormack [5] compare non-authoritative assessors with automated techniques guided by authoritative feedback, finding the latter to be at least as reliable as the former when evaluated against the authoritative conception of relevance. Webber [9] analyses assessor agreement levels on the same dataset, finding considerable variability in assessor reliability.",null,null
23,"Brodley and Friedl [1] present methods for automatically identifying mislabeled training data by using ensemble classifiers to detect outliers. Ramakrishnan et al. [7] similarly use a Bayesian network to detect outliers in textual data. Such methods do not work, however, if the annotator is consistently incorrect.",null,null
24,3. MATERIALS AND METHODS,null,null
25,"We distinguish two assessors: the training assessor, who makes the annotations of the training examples; and the testing assessor, whose conception of relevance the output classifier is intended to represent. Where training and test assessor are the same, we refer to the task as self-classification. Where the assessors are different, we refer to the task as cross-classification.",null,null
26,3.1 Metrics,null,null
27,"We use F1 score--the harmonic mean of precision and recall-- as our measure of effectiveness for binary classification. To measure ranking quality, we calculate the maximum F1 score achievable across all possible cutoff points in the ranking (termed hypothetical F1 by Cormack et al. [4]). Area under the ROC curve gives similar trends to those reported here. Significance testing is by paired two-tailed t tests.",null,null
28,3.2 Dataset,null,null
29,"Our dataset is taken from the TREC 4 AdHoc track. In that year, the organizers arranged for selected documents to be triplyassessed, first by the author of the TREC topic, and then by two additional assessors, who were authors of other TREC topics [8].",null,null
30,929,null,null
31,1.0,null,null
32,1.0,null,null
33,F1 (cross-classification),null,null
34,Max-F1 (cross-classification),null,null
35,0.8,null,null
36,0.8,null,null
37,0.6,null,null
38,0.6,null,null
39,0.4,null,null
40,0.4,null,null
41,0.2,null,null
42,0.2,null,null
43,0.0,null,null
44,0.0 0.2 0.4 0.6 0.8 1.0,null,null
45,Max-F1 (self-classification),null,null
46,"Figure 1: (Ranking) Maximum-F1 score for cross classification versus self-classification, with the original assessor as target assessor. An origin-anchored regression line is drawn.",null,null
47,"We treat the original assessor as the authoritative, testing assessor, and separately treat each additional assessor as a training assessor for cross-classification.",null,null
48,"We restrict our document set to the Associated Press (AP) subcollection, in order to avoid certain biases in the original (nonrandom) selection of documents for multiple assessment. We include only those 39 (of the original 49) topics for which all three assessors found at least 8 AP documents relevant. The mean number of relevant documents per topic is 73 (standard deviation 60), and of irrelevant documents 191 (sd 31). The mean F1 between the original and alternative assessors is 0.63 (sd 0.21).",null,null
49,3.3 Classifier,null,null
50,"We use LibSVM as our classifier [3], with a linear kernel and default settings. Features are term TF*IDF scores, using length normalization, the Lovins stemmer, case folding, and stop word removal. Inverse document frequency was calculated only on AP documents multiply-assessed for at least one topic.",null,null
51,"As the dataset is small, classification is approximated by classifying each tenth of the collection using a model trained on the other nine-tenths. The tested tenths are then amalgamated to form a single, margin-based ranking. Holdout experiments showed a mean Kendall's  of 0.88 between document rankings produced by different fold models, indicating high stability between models.",null,null
52,"LibSVM optimizes its binary classification for accuracy, but this proved to give poor results for the F1 measure. Instead, we create binary rankings by fitting probabilities using the method of Platt [6], then choosing the cutoff point that optimizes F1 in expectation.",null,null
53,4. RESULTS,null,null
54,4.1 Self- versus cross-classification,null,null
55,We begin by comparing the ranking effectiveness of the classifier trained by the authoritative assessor (self-classification) with that trained by an alternative assessor (cross-classification). Figure 1 compares the max-F1 scores achieved by the two approaches.,null,null
56,Max-F1 (liberal assessor),null,null
57,0.0 0.0 0.2 0.4 0.6 0.8 1.0 F1 (self-classification),null,null
58,Figure 2: (Binary) F1 score for cross classification versus selfclassification.,null,null
59,1.0,null,null
60,0.8,null,null
61,0.6,null,null
62,0.4,null,null
63,0.2,null,null
64,0.0 0.0 0.2 0.4 0.6 0.8 1.0 Max-F1 (conservative assessor),null,null
65,"Figure 3: Cross-classification effectiveness of conservative versus liberal alternative assessor, with original assessor as target, as measured by maximum-F1 score.",null,null
66,Mean max-F1 is 0.738 for self- and 0.637 for cross-classification; the difference is highly significant (p < 0.0001). Cross-classification leads to an average max-F1 score 14% than self-classification.,null,null
67,"Next we consider binary classification, as shown in Figure 2. Mean binary F1 is 0.629 for self- and 0.456 for cross-classification, again a highly significant difference. Cross-classification leads to a 28% lower F1 score than self-classification, a greater fall than for max-F1. Cross-classification seems to harm selection of a binary cutoff even more than it does ranking of the documents.",null,null
68,4.2 Comparing different assessor types,null,null
69,"An interesting question is whether, given an assessor disagrees",null,null
70,930,null,null
71,Max-F1 (union of assessors) Maximum F1,null,null
72,1.0,null,null
73,0.8,null,null
74,0.6,null,null
75,0.4,null,null
76,0.2,null,null
77,0.0,null,null
78,0.0 0.2 0.4 0.6 0.8 1.0,null,null
79,Max-F1 (intersection of assessors),null,null
80,"Figure 4: Cross-classification effectiveness of the union of alternative assessors' relevant documents versus the intersection, measured using maximum-F1 score.",null,null
81,"with the authoritative conception, it is better that the assessors tends to assign more documents as relevant (the assessor is liberal), or fewer (the assessor is conservative). We explore this question by denoting the alternative assessor with the lower prevalence for each topic the conservative assessor, and the assessor with the higher prevalence the liberal assessor. Figure 3 compares the max-F1 scores on the rankings produced via cross-classification using conservative versus liberal assessors. Mean max-F1 is 0.629 for the conservative assessors, 0.646 for the liberal ones. The difference, however, is not significant (p > 0.1).",null,null
82,"A related question is how to combine multiple assessments, where available, when creating training data. Should the union of the documents found relevant by either assessor be marked relevant in the training data, or the intersection (that is, only documents both assessors find relevant)? Figure 4 compares two alternatives: marking as relevant documents found relevant by either assessor (union), versus only those found relevant by both (intersection). The intersection of the assessors gives a mean max-F1 of 0.623, the union one of 0.657, with the difference being statistically significant (p < 0.05). It seems on balance better to give more, if noisier, examples of relevant documents than fewer, if cleaner, examples. (Only retaining examples on which both assessors agreed was also tried; the mean max-F1 score is intermediate between that for the intersection and that for the union.)",null,null
83,4.3 Random disagreement,null,null
84,"The previous sections have examined the absolute loss of effectiveness from using non-authoritative assessors to train the classifier. Is this loss greater or less than one would expected, given inter-assessor agreement? One way of answering this is to compare cross-classification effectiveness of the actual alternative assessor, with that of other randomly simulated alternative assessors having the same agreement level. We do this by starting with the original assessments and the false positive and false negative counts, FP and FN, of the alternative assessor (we arbitrarily choose the first alternative assessor for this experiment). We then random select FP of the originally irrelevant documents and mark them relevant, and",null,null
85,1.0,null,null
86,0.8 0.6 0.4,null,null
87,_x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x__x,null,null
88,0.2 _,null,null
89,Actual,null,null
90,0.0,null,null
91,Mean Random,null,null
92,0,null,null
93,10,null,null
94,20,null,null
95,30,null,null
96,40,null,null
97,Topics (ordered by median random Maximum F1),null,null
98,"Figure 5: Actual cross-classification effectiveness versus range of effectivenesses of randomly-degraded cross-classification, measured using maximum-F1 score. The mean and 95% intervals on the random cross-classifications are shown. Topics are sorted by mean random cross-classifier effectiveness. There are 81 random simulations of alternative assessors for each topic.",null,null
99,"FN of the originally relevant documents, and mark them irrelevant, creating a simulated alternative assessor training set. We then train a cross-classifier on this simulated set, and compare its effectiveness with the actual alternative assessor.",null,null
100,"Figure 5 compares simulated cross-classification effectiveness (across 81 simulations per topic) with that of the actual alternative assessor, measured using max-F1. The mean of the actual max-F1 scores is 0.633, that of the median random 0.615; the difference is statistically significant (p < 0.05). On average, the actual alternative assessor gives slightly better ranking quality than inter-assessor agreement would predict, though the difference is small. There is considerable variability between topics (or assessors): actual is outside the empirical 95% interval for 7 of the 39 topics (above for 3, below for 4).",null,null
101,4.4 User effort,null,null
102,"Differences in effectiveness have been expressed in previous sections in terms of the system evaluation metrics of F1 and max-F1. These results can be difficult to interpret in terms of the actual cost to the user of poorer performance. One way of measuring this cost is how much further down the ranking one must go in order to achieve a certain level--say 75%--of recall. In e-discovery, productions are often finalized by manually reviewing the ranking from the top down to the point where it is estimated that a certain threshold of recall (and 75% is one such threshold1) has been achieved, so depth to achieve 75% recall is a reasonable measure of one component of expense in e-discovery.",null,null
103,Figure 6 compares the proportion of the ranking that must be processed to achieve 75% recall for cross-classification with that,null,null
104,"1See, for instance, Global Aerospace Inc., et al., v. Landow Aviation, L.P., et al., No. CL 61040 (Va. Cir. Ct. Apr. 9, 2012) (""Memorandum in support of motion for protective order approving the use of predictive coding"").",null,null
105,931,null,null
106,Cross-classification,null,null
107,1.0,null,null
108,0.8,null,null
109,0.6,null,null
110,0.4,null,null
111,0.2,null,null
112,0.0,null,null
113,0.0 0.2 0.4 0.6 0.8 1.0,null,null
114,Self-classification,null,null
115,"Figure 6: Proportion of ranking that must be processed in order to achieve 75% recall, under cross-classification and selfclassification.",null,null
116,"for self-classification. In the median case, using cross-classification requires that 24% more of the ranking must be processed than using self-classification, but around one in eight cases, processing must go to twice the depth or more.",null,null
117,5. CONCLUSION,null,null
118,"In this paper, we have examined the loss of effectiveness that occurs when a text classifier is trained using annotations made by an assessor other than the authoritative assessor, whose conception of relevance is to be used to evaluate the classifier's effectiveness. We have found that using a non-authoritative assessor leads to a significant decrease in classifier reliability, of around 14% for ranking quality measured using maximum F1, and twice that for binary classification measured using F1 score. In terms of user effort, this means that around 24% more of the ranking must be processed to achieve recall of 75%. The liberality or conservativeness of the assessor does not make a significant difference to cross-classification reliability, though where multiple assessments are available, it seems slightly better to take the union of their relevance sets rather than their intersection as training data. Cross-classification leads to slightly better average performance than might be expected given the degree of inter-assessor disagreement (as measured via a random simulation experiment). However, for all of these findings, there is considerable variability between tasks and between assessors.",null,null
119,"Considerable future work remains to be done. Though the training sets employed here have been sufficient to achieve creditable accuracy (mean F1 of 0.629) on the low-yield ad-hoc tasks, larger training sets are used in many text-classification tasks, such as ediscovery, where a few thousand training examples are more common. Larger training sets may contain more redundancy, reducing the impact of assessor disagreement; though to the extent that disagreement is systematic rather than random, the reduction may be slight. Similarly, the relative desirability of liberal or conservative assessors, or of the union or intersection of multiple assessment sets, will likely be affected by the amount of training data. We have",null,null
120,"explored the question of user cost in terms of additional processing of the output ranking; another dimension of cost that a larger experimental training set would allow us to explore is the additional number of annotations required under cross-classification to achieve the same effectiveness as self-classification, and also what the (near-) maximum effectiveness achievable with both assessor types is. We intend to explore this question using TREC Legal Track data.",null,null
121,"Finally, the evaluation metrics used in this paper include F1 and user effort. However, user effort does not always have uniform cost. One of the primary motivations for this work is that nonauthoritative assessors (e.g. junior attorneys in an e-discovery matter) have a lower hourly cost than authoritative assessors (e.g. senior attorneys). One of the next phases of this research is integrating economic cost models with retrieval effectiveness metrics, to paint an overall picture of the cost of using non-authoritative, less accurate assessors.",null,null
122,Acknowledgments,null,null
123,"This material is based in part upon work supported by the National Science Foundation under Grant No. 1065250. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",null,null
124,References,null,null
125,"[1] Carla E. Brodley and Mark A. Friedl. Identifying mislabeled training data. Journal of Artificial Intelligence Research, 11:131­167, 1999.",null,null
126,"[2] Ben Carterette and Ian Soboroff. The effect of assessor errors on IR system evaluation. In Proc. 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 539­546, Geneva, Switzerland, July 2010.",null,null
127,"[3] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1­27:27, 2011. Software available at http://www. csie.ntu.edu.tw/~cjlin/libsvm.",null,null
128,"[4] Gordon V. Cormack, Maura R. Grossman, Bruce Hedin, and Douglas W. Oard. Overview of the TREC 2010 legal track. In Ellen Voorhees and Lori P. Buckland, editors, Proc. 19th Text REtrieval Conference, pages 1:2:1­45, Gaithersburg, Maryland, USA, November 2010.",null,null
129,"[5] Maura R. Grossman and Gordon V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law and Technology, 17 (3):11:1­48, 2011.",null,null
130,"[6] John C. Platt. Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. In Alexander J. Smola, Peter Bartlett, Bernhard Schölkopf, and Dale Schuurmans, editors, Advances in Large Margin Classifiers, pages 61­74. MIT Press, 1999.",null,null
131,"[7] Ganesh Ramakrishnan, Krishna Prasad Chitrapura, Raghu Krishnapuram, and Pushpak Bhattarcharyy. A model for handling approximate, noisy or incomplete labeling in text classification. In Proc. 22nd International Conference on Machine Learning, pages 681­688, Bonn, Germany, August 2005.",null,null
132,"[8] Ellen Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5):697­716, September 2000.",null,null
133,"[9] William Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, pages 2:1­8, Beijing, China, July 2011.",null,null
134,932,null,null
135,,null,null

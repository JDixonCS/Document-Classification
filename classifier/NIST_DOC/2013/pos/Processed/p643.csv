,sentence,label,data
0,A General Evaluation Measure for Document Organization Tasks,null,null
1,Enrique Amigó,null,null
2,"E.T.S.I. Informática UNED Juan del Rosal, 16 Madrid, Spain",null,null
3,enrique@lsi.uned.es,null,null
4,Julio Gonzalo,null,null
5,"E.T.S.I. Informática UNED Juan del Rosal, 16 Madrid, Spain",null,null
6,julio@lsi.uned.es,null,null
7,Felisa Verdejo,null,null
8,"E.T.S.I. Informática UNED Juan del Rosal, 16 Madrid, Spain",null,null
9,felisa@lsi.uned.es,null,null
10,ABSTRACT,null,null
11,"A number of key Information Access tasks ­ Document Retrieval, Clustering, Filtering, and their combinations ­ can be seen as instances of a generic document organization problem that establishes priority and relatedness relationships between documents (in other words, a problem of forming and ranking clusters). As far as we know, no analysis has been made yet on the evaluation of these tasks from a global perspective. In this paper we propose two complementary evaluation measures ­ Reliability and Sensitivity ­ for the generic Document Organization task which are derived from a proposed set of formal constraints (properties that any suitable measure must satisfy).",null,null
12,"In addition to be the first measures that can be applied to any mixture of ranking, clustering and filtering tasks, Reliability and Sensitivity satisfy more formal constraints than previously existing evaluation metrics for each of the subsumed tasks. Besides their formal properties, its most salient feature from an empirical point of view is their strictness: a high score according to the harmonic mean of Reliability and Sensitivity ensures a high score with any of the most popular evaluation metrics in all the Document Retrieval, Clustering and Filtering datasets used in our experiments.",null,null
13,Categories and Subject Descriptors,null,null
14,B.8 [Performance and Reliability]: General,null,null
15,General Terms,null,null
16,"Measurement, Performance",null,null
17,Keywords,null,null
18,IR effectiveness measures,null,null
19,1. INTRODUCTION,null,null
20,Some key Information Access tasks can be seen as instances of a generic document organization problem that es-,null,null
21,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
22,"tablishes priority (ranking) and relatedness (clustering) relationships between documents. Let us think of a generic document organization system as a function from document pairs d, d into one of these possible relationships: , which means that d has more priority than d , and , which means that d and d have some kind of topical equivalence. We will use the notation for the cases in which the other two relations do not hold, and the notation {d1, d2 . . . dn} to indicate that d1 . . . dn are all related via the topical equivalence relation .",null,null
23,"This general problems subsumes: Document Ranking. This case is illustrated in Table 1. The gold standard establishes (at least) two priority levels (relevant versus non-relevant), and the system output returns an ordered list with one priority level per document. For clarity, in the table we use a bold font for relevant documents. Note that, in this problem, it is assumed that there is an unlimited amount of irrelevant documents, while the set of relevant documents is limited. Both the gold standard and the system output contain, implicitly, an unlimited set of documents in the last level. The gold standard contains two priority levels (with the documents manually judged) while the system output contains as many levels as returned documents. Document Filtering. Table 2 illustrates this case; now, both the gold standard and the system output consist of two priority levels (relevant and irrelevant). Document Clustering is exemplified in Table 3. Now there is only one priority level, and a set of clusters which contain related documents. Both the gold standard and the system output have the same form. Table 4 illustrates the variant of overlapping clustering, where a document may simultaneously appear in more than one cluster. Evaluation metrics have been extensively discussed for each of these tasks. There are, however, many practical problems where the system must be able both to detect topical relationships (clustering documents) and relative priorities (some clusters are more relevant than others). Let us give a couple of examples: Alert detection. A number of practical information access problems involve detecting, in an incoming stream of documents, new information that is both novel and of high priority. Online Reputation Management, for instance, involves monitoring online information about an entity (a company, brand, product, person, etc.), clustering texts into the main topics, and establishing which of them have higher priority (for instance, those that may potentially damage the reputation of the entity).",null,null
24,643,null,null
25,"Gold Standard d1, d2, d3",null,null
26,"d4, d5, d6, d7 ..dn",null,null
27,System Output,null,null
28,"d1 d2 d6 d3 d14 d4, d5, d7, ..dn",null,null
29,Table 1: Example of Document Ranking task. Vertical ordering indicates relative priorities,null,null
30,"Goldstandard d1, d2 , d3",null,null
31,"d4, d5, d6, d7, d8 System output d1 , d2 , d4",null,null
32,"d3, d5, d6, d7, d8",null,null
33,Table 2: Example of filtering task,null,null
34,"Search Results Organization. Given the set of top ranked documents retrieved for a query, a mechanism to group related documents and assign priorities between clusters can be used to improve search results in many ways: (i) to enhance search results diversity by maximizing the number of highly relevant topics represented in the top results; (ii) to provide keyword suggestions to refine the query, sampled from the most relevant clusters; (iii) to directly display search results as a ranked list of relevant topics corresponding to alternative query interpretations, subtopics or facets in the retrieved documents.",null,null
35,"This type of tasks (and others such as composite retrieval) ­ which are more complex than the standard ranking and clustering problems ­ still match the generic Document Organization task as we have defined it. An example is shown in Table 5. In the gold standard there is, for instance, one topic with two documents (d1  d2) at the top priority level, and two topics at the second level (d3  d4 and d5  d6). In the next three priority levels there are two single documents and another topic. The example includes overlapping clusters: for instance, d5 is relevant for two topics, one in the second priority level and another one in the fourth.",null,null
36,"Finally, the (potentially long) list of documents at the bottom of the gold standard ranking (d11, d12, d13, ... dn) represents irrelevant documents which are not judged in terms of topical similarity, and are therefore represented via the empty relationship .",null,null
37,"Evaluation measures have been proposed to evaluate clustering outputs in the context of document retrieval [14, 8, 18] and to include the notion of diversity in search results[15, 24, 9]. But, to the best of our knowledge, no measure has been previously designed to evaluate the general document organization problem and all the tasks subsumed by this one.",null,null
38,"In order to find appropriate evaluation measures for the generic Document Organization problem, we will focus on the specification of the formal constraints that they should satisfy in each of the subsumed tasks (filtering, clustering and ranking). Our goal is finding an evaluation measure",null,null
39,"Gold standard {d1, d2, d3}, {d4,d5,d6}, d7",null,null
40,"System output {d1,d2}, d3, {d4, d5, d6, d7}",null,null
41,Table 3: Example of Clustering task.,null,null
42,"Gold standard {d1, d2, d3}, {d4, d5, d6}, {d4, d7}",null,null
43,"System output {d1, d2}, d3, {d4, d5, d6, d7}, {d6, d7}",null,null
44,Table 4: Example of overlapping clustering,null,null
45,for the general document organization task that (i) satisfies formal constraints for all the tasks; (ii) turns into suitable existing measures when mapped into each of the subsumed tasks.,null,null
46,"Our research leads to propose Reliability and Sensitivity which are, in short, precision and recall over document pair relationships established in the gold standard, with a suitable weighting scheme. Comparing them with state of the art measures for each particular scenario, we find that Reliability and Sensitivity satisfy more formal constraints than previously existing measures in most cases. In addition, its harmonic mean is stricter than previously existing measures ­ a high score implies high scores with respect to all other standard measures ­, which is an interesting property when the application scenario does not clearly prescribe a more specific evaluation measure.",null,null
47,"We will start by establishing the set of formal constraints that any suitable metric should satisfy (Section 2); then we present our proposed measures (Section 3) and discuss its application to the different tasks (Section 4). We end by showing how the measures apply to the more complex document organization task, and discussing the main implications of our results.",null,null
48,2. INFORMATION ACCESS MEASURES AND FORMAL CONSTRAINTS,null,null
49,"Our approach to measure design is to start defining a set of formal constraints, i.e. a set of formal, verifiable properties that any suitable measure should satisfy. In addition, they explain the nature of different evaluation measure families. We will start by reviewing (or proposing) formal constraints for each of the subsumed tasks, and then merge all collected constraints into a single list of properties for the generalized measures. Table 6 summarize the results of this analysis.",null,null
50,Gold standard,null,null
51,"{d1, d2} {d3, d4}, {d5, d6}",null,null
52,"d7 {d5, d8, d9}",null,null
53,"d10 d11, d12, d13, ... , dn",null,null
54,System output,null,null
55,"d1 d2 {d3, d4}, {d5, d6} {d5, d7, d9} d8 d12 d10, d11, d13, ... dn",null,null
56,Table 5: An example of gold standard and system output for the generic information retrieval task.,null,null
57,644,null,null
58,Measure,null,null
59,Homo. Comp. Rag Size vs Priority Deepness Deepness Closeness Confidence,null,null
60,Bag Quantity,null,null
61,Threshold Threshold,null,null
62,Set matching,null,null
63,Entropy based,null,null
64,Edit distance,null,null
65,Counting pairs,null,null
66,Bcubed,null,null
67, (overlap),null,null
68,"MAP, DCG, Q measure",null,null
69,P10,null,null
70,MRR,null,null
71,RBP,null,null
72,"UTILITY, F, LAM%",null,null
73,R*S,null,null
74,Table 6: Formal constraints satisfied by standard measures and R*S,null,null
75,2.1 Document Clustering,null,null
76,"[2] provides a detailed analysis of clustering evaluation measures (grouped by families) and the constraints they should satisfy. We briefly describe here these constraints and refer to [2] for its justification and formal description. We will say that the system output produce clusters while the gold standard is composed by classes. Q represents the quality of a clustering distribution and a, b, c, d.. are documents from different classes:",null,null
77,"Cluster Homogeneity: This restriction was firstly proposed in [22]. Given a certain system output document distribution, splitting documents that do not belong to the same class, must increase the output quality:",null,null
78,"Q({a, a, a}, {b, b}...) > Q({a, a, a, b, b}...)",null,null
79,"Although it seems a very basic constraint, in [2] it is shown that measures based in editing distance do not satisfy it.",null,null
80,"Cluster Completeness: The counterpart to the first constraint is that documents belonging to the same assessed class should be grouped in the same cluster [2, 22]:",null,null
81,"Q({a, a, a, a, a, }...) > Q({a, a, a}, {a, a}...)",null,null
82,"Measures based on set matching, such as Purity and Inverse Purity do not satisfy this contraint.",null,null
83,Rag Bag: This constraint states that introducing disorder into a disordered cluster (rag bag) is less harmful than introducing disorder into a clean cluster. That is:,null,null
84,"Q({a, a, a, a}, {b, c, d, e}..) > Q({a, a, a, a, b}, {c, d, e}..)",null,null
85,"In general, all traditional measures fail to comply with this constraint.",null,null
86,"Cluster size vs. quantity: A small error in a big cluster is preferable to a large number of small errors in small clusters [2, 19, 22]. This constraint prevents the problem that measures based on counting pairs [19, 13], overweight big clusters. That is:",null,null
87,"Q({a, a, a, a}, {a}, {b, b}{c, c}{d, d}{e, e}...) >",null,null
88,"Q({a, a, a, a, a}, {b}, {b}, {c}, {c}, {d}, {d}, {e}, {e}..)",null,null
89,"Measures based on counting pairs are sensitive to the combinatory explosion of pairs in big clusters, failing on this constraint.",null,null
90,In [2] it is shown how the above contraints discriminate between four families of evaluation measures for the clustering problem: measures based on set matching (e.g. F,null,null
91,"measure, Purity and Inverse Purity), entropy-based measures (Entropy, class Entropy, Mutual Information), measures based on counting pairs and edit distance measures. Interestingly, there is only one pair of measures, BCubed Precision and Recall, that satisfies all constraints simultaneously, and forms an independent family. However, we will show in this paper that the extended version of Bcubed for overlapping clustering does not satisfy the last constraint.",null,null
92,2.2 Document Filtering,null,null
93,"Filtering is a binary classification problem where there is a relative priority between the two classes: the system must classify each document as positive or negative, being the positive class the one that stores the relevant information. In the filtering scenario all existing measures satisfy a basic constraint:",null,null
94,"Priority Constraint Moving a positive (relevant) document from the predicted negative set to the predicted positive set, or moving a negative (irrelevant) document from the predicted positive to the predicted negative set must increase the system output quality. Being dr and d¬r a judged relevant/irrelevant document respectively:",null,null
95,"Q({dr..}, {..}) > Q({..}, {dr..})",null,null
96,"Q({..}, {d¬r..}) > Q({d¬r..}, {..})",null,null
97,"This constraint is satisfied by every standard measure. Filtering is a basic binary classification task that can be evaluated in multiple ways, but it is difficult to define objectively desirable boundary constraint that discriminate measures. However, there are descriptive properties which are mutually exclusive and explain the nature of different measure families [3]. These properties focus on how the metrics score non-informative outputs D¬inf depending on the size of the positive set S(D¬inf ). A non-informative output is a random distribution of the documents that is independent on the content of the input documents. An ""all positive"" baseline, for instance, is a non-informative output where the size of the positive set is equivalent to the size of the input set.",null,null
98,"For instance, there is a large set of filtering measures that assign a fixed score to every non-informative output: Lam [11], the Macro Average Accuracy [20] and, in general, measures based on correlation such as the Kappa statistic [10]. (Q(D¬inf ) ,"" constant). Other family of measures assumes that, if the filtering system does not know any-""",null,null
99,645,null,null
100,"thing, returning all is better than removing documents randomly. In this case, the score for a non-informative system is correlated with the size of the positive output set: (Q(D¬inf )  S(D¬inf )) This is the case of the harmonic mean of Precision and Recall over the positive class. Finally, measures such as Accuracy or Utility assign relative weights to documents in each of the cases in the confusion matrix; Depending on how these parameters are set and on the distribution of classes in the input, the optimal positive class size for a non-informative output varies.",null,null
101,2.3 Document Ranking,null,null
102,"In order to define a set of constraints for the document ranking (or document retrieval) task, we have to take into account some aspects. First, documents at the top of the ranking must have more weight in the evaluation process: even if the system is able to sort all documents, the user will not be able to explore all of them. The sizes of the relevant and the irrelevant set are expected to be heavily unbalanced: potentially, the amount of irrelevant documents for a given query is (in practice) unlimited. Second, traditional document retrieval is a mixture of filtering and ranking tasks: an optimal system should not only rank the documents, but also decide on the size of the output set, depending on the amount of relevant documents in the collection and the selfassessed quality of the system output. These two features ­ which are related the fact that the gold standard and the system output take different forms, unlike the filtering and clustering problems ­ make document retrieval evaluation harder than it seems a priori.",null,null
103,"There is a large number of proposed measures in the state of the art. Some of the most popular are: precision at certain recall levels or ranking positions, AUC [12], MAP (Mean Average Precision), Discounted Cumulative Gain [16], Expected Reciprocal Rank (ERR)[25], Q-measure, Binary Preference [4], or Rank Biased Precision [21], among many others. Let us analyze them in terms of formal constraints.",null,null
104,"First, the Priority Constraint from the filtering problem also applies here (see previous subsection) and it is satisfied by most measures. A notable exception is P@10 (precision at the top ten documents retrieved), given that it is not sensitive to relationships after the tenth position in the system output ranking, and to internal reorderings in the top ten setWe can express this constraint in the context of Document Retrieval evaluation as, being r and ¬r relevant and irrelevant documents respectively, and being {d1, d2, d3..dn} an output ranking:",null,null
105,"Q({..ri+1, ¬ri+2..}) > Q({..¬ri+1, ri+2..})",null,null
106,"Deepness Constraint: The more we go to a deeper point in the output ranking, the less the probability of documents being explored by the user. Therefore, the effect of a document priority relationship in the system quality depends on the depth of the documents in the ranking. Being i<j:",null,null
107,"Q({..ri, ¬ri+1..}) - Q({..¬ri, ri+1..}) <",null,null
108,"Q({..rj , ¬rj+1..}) - Q({..¬rj , rj+1..})",null,null
109,"Measures based on traditional correlation, such as AUC or Kendall, do not satisfy this contraint, because they give the same weight to all elements in the ranking. Also, P@10 obviously does not satisfy this contraint.",null,null
110,"Deepness Threshold Constraint: Although P@10 does not satisfy the previous two constraints, one motivation for",null,null
111,"using it is that in some cases it is advisable to assume a practical deepness threshold. In other words, there is a ranking area which will never be explored by the user. We can express this as a constraint by saying that there exists a value n large enough such that retrieving one relevant document at the top of the rank is better than retrieving n relevant documents after n irrelevant documents:",null,null
112,"Q({r1, ¬r2, ¬r3..¬r2n}) > Q({¬r1, ¬r2..¬rn-1, rn..r2n})",null,null
113,We will not include all the formal proofs on how measures,null,null
114,"satisfy this constraint, due to space availability; we will only",null,null
115,discuss the proof that MAP does not satisfy it.,null,null
116,The score for the leftmost distribution in the constraint,null,null
117,is,null,null
118,1 Nr,null,null
119,",",null,null
120,assuming,null,null
121,that,null,null
122,there,null,null
123,are,null,null
124,Nr,null,null
125,relevant,null,null
126,documents,null,null
127,in,null,null
128,the collectionWe can prove that the score for the rightmost,null,null
129,distribution is always higher:,null,null
130,"1 i,n i",null,null
131,"1 i,n i",null,null
132,"M AP ,",null,null
133,>,null,null
134,",",null,null
135,"Nr i,1 n + i Nr i,1 2n",null,null
136,"1 i,n",null,null
137,1 (n - 1)n (n - 1),null,null
138,"i,",null,null
139,",",null,null
140,"2nNr i,1",null,null
141,2nNr,null,null
142,2,null,null
143,4Nr,null,null
144,which,null,null
145,is,null,null
146,bigger than,null,null
147,1 Nr,null,null
148,for,null,null
149,any,null,null
150,n > 4.,null,null
151,DCG does,null,null
152,not,null,null
153,com-,null,null
154,ply with this constraint either: DCG for the first distribution,null,null
155,"is 1, before normalization. And for the second distribution",null,null
156,we have:,null,null
157,"i,n",null,null
158,1,null,null
159,"i,n",null,null
160,1,null,null
161,n,null,null
162,"DCG ,",null,null
163,>,null,null
164,",",null,null
165,>1,null,null
166,"i,1 log2(i + n) i,1 log2(2n) log2(2n)",null,null
167,"For instance, according to MAP or DCG, finding 1,000 relevant documents after 1,000 irrelevant documents is better than having only one relevant document, but at the top of the rank. This is counterintuitive in many practical settings.",null,null
168,"The Q-measure is an extension of MAP for multigraded relevance, having a similar behavior. However, the measure ERR does satisfy this constraint (as well as P@10), due to the strong relevance discount for positions deeper in the rank. Measures with similar weighting schemes also satisfy this constraint, but at the cost of failing to satisfy the next one.",null,null
169,"Closeness Threshold Constraint: There exists a (short) ranking area which is always explored by the user. For instance, we can assume that the top three documents returned by a search engine for informational queries are always inspected. We formalize this constraint as the counter part of the previous constraint: There exists a value n small enough such that retrieving one relevant document in the first position is worse than n relevant documents after n irrelevant documents:",null,null
170,"Q({r1, ¬r2, ¬r3..¬r2n}) < Q({¬r1, ¬r2..¬rn-1, rn..r2n})",null,null
171,"In the case of P@10, n is 9 (i.e. for any n lower than 9, the constraint is satisfied). ERR, on the other hand, does not satisfy the constraint: given its strong discount with ranking depth, one relevant document at the first position has always more weight than n relevant documents after the position n.",null,null
172,The measures RBP and the discounting function proposed by Smucker and Clarke [23] satisfy all the previous constraints. The common characteristic of both measures is that they are based on a probabilistic user behavior model.,null,null
173,646,null,null
174,"However, all proposed measures fail on the following constraint.",null,null
175,"Confidence Constraint. The output ranking does not necessarily include all documents in the collection and, therefore, the amount of documents returned is also an aspect of the system quality. The classical TREC ad-hoc evaluation does not consider this aspect, given that the length of the rank is fixed; nevertheless, there is research focused on the prediction of ranking quality, in order to determine when an output rank must be shown to the user. We include this aspect in our constraints by stating that extending the rank with irrelevant documents should decrease the output score:",null,null
176,"Q({d1, d2..dn}) > Q({d1, d2..dn, ¬rn+1})",null,null
177,"Given that most measures are based on accumulative relevance weighted by the location in the ranking [5], we can conclude that irrelevant documents at the bottom of the ranking do not affect the score and the constraint is not satisfied. As far as we know, current evaluation measures do not consider this aspect.",null,null
178,"In summary, for the tasks subsumed under our document organization problem, the result of our analysis is that (i) in clustering, only the Bcubed measure satisfies all constraints, with the exception of one of them in the case of overlapping clustering; (ii) in the filtering scenario, all measures satisfy the priority constraint, but behave in very different manners with respect to how they score non-informative outputs; and (iii) Finally, in the case of document retrieval, we have detected a few measures that satisfy all constraints except the last one, but none that satisfies all constraints.",null,null
179,"Our goal is finding a measure that can be applied to all of the subsumed tasks and to any combination of them (i.e. to the general document organization problem), and that satisfies all constraints coming from each of the subsumed tasks. In the next section we introduce our proposal.",null,null
180,3. PROPOSAL: RELIABILITY AND SENSITIVITY,null,null
181,"Our proposal consists of two complementary measures, Reliability and Sensitivity, with a straightforward initial definition. Let us consider a system output X and a gold standard G, which are both a set of document relationships r(d, d )  { , , }. The Reliability (R) of relationships in the system output is the probability of finding them in the gold standard. Reversely, the Sensitivity (S) of predicted relationships is the probability of finding them in the system output when they appear in the gold standard. In other words, R and S are precision and recall of the predicted set of relationships with respect to the true set of relationships:",null,null
182,"R(X )  P (r(d, d )  G|r(d, d )  X )",null,null
183,"S(X )  P (r(d, d )  X |r(d, d )  G)",null,null
184,We can express Reliability as a sum of probabilities pondered by the weight of each relationship in X :,null,null
185,R(X ) ,null,null
186,"P (r(d, d )  G)wX (r(d, d ))",null,null
187,"r(d,d )X",null,null
188,"We want to observe three restrictions on relationship weights: (i) wX (r(d, d )) ,"" f (wX (d), wX (d )), i.e., the weight of a relationship is a function of the weights of the documents involved. In Document Retrieval, the weight of a document""",null,null
189,"will be related to its probability of being inspected by the user, which is related at least to its position in the ranking; (ii) w(d) ,"" d w(r(d, d )), i.e., the weight of all relations starting from a document d determines the weight of document d; this restriction prevents the quadratic effect of counting binary relationships and is related to the """"size vs quantity"""" restriction for the clustering problem that we want to satisfy; and finally (iii) the contribution w(d, d ) of each d related to d should be proportional to the weight of d .""",null,null
190,"Then, we can express R and S in terms of weights of single documents. Being wX (d) the weight of d in X and being Wx,d the sum weight of documents related with d:",null,null
191,"WX ,d ,",null,null
192,wx(d ),null,null
193,"d /r(d,d )X",null,null
194,we can compute R and S as:,null,null
195,"R(X ) ,",null,null
196,P,null,null
197,"(r(d,",null,null
198,d,null,null
199,),null,null
200,G),null,null
201,"wx(d ) Wx,d",null,null
202,wx(d),null,null
203,"r(d,d )X",null,null
204,S(v),null,null
205,",",null,null
206,"r(d,d",null,null
207,)G,null,null
208,P,null,null
209,"(r(d,",null,null
210,d,null,null
211,),null,null
212,X,null,null
213,),null,null
214,"wg(d ) Wg,d",null,null
215,wg (d),null,null
216,"If all documents have the same weight in the distributions, R and S simply turn into the average R(d) and S(d) associated to each document:",null,null
217,"R(X ) ,"" AvgdP (r(d, d )  G|r(d, d )  X ) S(X ) "","" AvgdP (r(d, d )  X |r(d, d )  G)""",null,null
218,3.1 Estimating Document Weight Discounting,null,null
219,In the generic document organization task we must as-,null,null
220,sume that there exists a virtually unlimited amount of doc-,null,null
221,"uments in the collection. Therefore, we must weight docu-",null,null
222,ments according to their priority in the system output or in,null,null
223,the gold standard. There exist several studies on predict-,null,null
224,ing the weight of a document in the ranking in terms of the,null,null
225,probability to be explored by the user. Most of them are,null,null
226,"based on assumptions over user behavior [6, 7, 23]. How-",null,null
227,"ever, there is no clear consensus yet on how to model the",null,null
228,"empirical user behavior. Rather than this, we focus on ba-",null,null
229,sic constraints and interpretability as the main criteria to,null,null
230,choose a weighting scheme.,null,null
231,We model the weight of the document in the i position,null,null
232,as the weight integration between i - 1 and i. The first,null,null
233,constraint is that the sum weight for all documents must be,null,null
234,"finite. Therefore, we must employ a function with a converg-",null,null
235,ing,null,null
236,integral.,null,null
237,We,null,null
238,select,null,null
239,1 i2,null,null
240,because,null,null
241,it,null,null
242,is,null,null
243,a,null,null
244,"simple,",null,null
245,soft,null,null
246,"decay,",null,null
247,integrable and convergent function. We leave the refinement,null,null
248,of the discounting curve for a latter parameterization step;,null,null
249,"ideally, our evaluation measure should be as general as pos-",null,null
250,"sible, and therefore it must have parameters to establish how",null,null
251,much of the ranking (v.g. the top 10 vs the top 100) carries,null,null
252,on how much of the weight (v.g. 50% or 99% of the overall,null,null
253,score) in the evaluation.,null,null
254,According,null,null
255,to,null,null
256,1 i2,null,null
257,",",null,null
258,the,null,null
259,weight,null,null
260,of,null,null
261,the,null,null
262,document,null,null
263,in,null,null
264,position,null,null
265,i,null,null
266,in the priority ordering is:,null,null
267,c+i 1,null,null
268,1,null,null
269,1,null,null
270,"wX (d) , c1",null,null
271,c+i-1,null,null
272,dx x2,null,null
273,",",null,null
274,c1,null,null
275,- c+i-1 c+i,null,null
276,(1),null,null
277,where c1 is a normalization factor to ensure that the sum is 1. c is another parameter that moves the function in order,null,null
278,647,null,null
279,to give more or less weight to the high priority documents. Stating that the total sum weight is one:,null,null
280,1,null,null
281,1,null,null
282,c1,null,null
283,c,null,null
284,dx x2,null,null
285,",",null,null
286,c1 c,null,null
287,",",null,null
288,1,null,null
289,"Therefore, c ,"" c1. The next constraint is that we should be able to parameterize the weighting curve in an interpretable way. Ideally, we want to be able to set two parameters n and Wn which mean that the first n documents must cover a Wn weight ratio of the overall evaluation score. For instance, we may want to state that the first 30 positions in the ranking (n "", 30) will have an 80% of the total weight in the evaluation measure (Wn , 0.8). Therefore:",null,null
290,c,null,null
291,c+n c,null,null
292,1 dx,null,null
293,x2,null,null
294,",",null,null
295,Wn,null,null
296,",",null,null
297,c,null,null
298,",",null,null
299,(1 - Wn)n Wn,null,null
300,"Now, we can estimate the weight of each document in the system output or in the gold-standard according to Formula 1. Documents at the same priority level share the interval weight. Being n and n, the amount of documents with more and equal relevance than d respectively we can estimate the weight of each document as:",null,null
301,wX (d),null,null
302,",",null,null
303,"c1 n,",null,null
304,"c+n +n, 1",null,null
305,dx,null,null
306,c+n,null,null
307,x2,null,null
308,(2),null,null
309,"Smucker and Clarke proposed a discounting model based on exploration time calibration[23] which considers additional aspects such as the relevance and length of documents. Actually, this model is compatible with our proposal: we can incorporate this by replacing the i position of documents with a time function. We leave this analysis for future work.",null,null
310,3.2 Overlapping Clusters,null,null
311,"As we mentioned earlier, a document may appear in multiple clusters (corresponding, for instance, to different information nuggets in the document), and therefore it may appear at multiple priority levels. If overlapping between clusters is allowed, a document has potentially a different number of occurrences in the gold and system output distribution. If there exists only one instance of d and d in both the gold standard and the system output, the probability of coocurrence is 1 when the relationships match. Otherwise, following the extended Bcubed measure proposed in [2], we assume the best possible correspondence between relationships in X and G. For instance, if two documents are related in the system output less times than in the gold standard, then all the predicted relationships are assumed to be correct. Otherwise, the probability is the ratio of gold relationships per system relationships. Formally, being |rG(d, d )| and |rX (d, d )| the number of occurrences of r(d, d ) in G and X respectively:",null,null
312,"P (r(d, d )  G) ,"" min(|rG(d, d )|, |rX (d, d )|) (3) |rX (d, d )|""",null,null
313,"P (r(d, d )  X ) ,"" min(|rG(d, d )|, |rX (d, d )|) (4) |rG(d, d )|""",null,null
314,3.3 Measure Computation,null,null
315,"Here we state the method to compute R and S over the general document organization task. We assume that there is a set of prioritized documents Xr organized by levels and clusters and a special, bottom level containing an unlimited",null,null
316,amount of irrelevant/discarded documents X¬r (see Table 5). The weight of a single document in the set of prioritized documents Xr is computed as in Equation 2. The weight of prioritized documents Xr in the system output is:,null,null
317,c+|Xr | 1,null,null
318,c,null,null
319,"W (Xr) , c",null,null
320,c,null,null
321,"x2 , 1 - c + |Xr|",null,null
322,The weight of the long tail of non prioritized documents X¬r in the system output is:,null,null
323,1,null,null
324,c,null,null
325,"WX (X¬r) , c",null,null
326,", c+|Xr| x2 c + |Xr|",null,null
327,The sum weight of documents priority related with d is 1 minus the sum weight of documents in the same priority level:,null,null
328,"WX ,d, , 1 -",null,null
329,wx(d ),null,null
330,d /¬(d  X d),null,null
331,"The probability P (r(dij, dkl)  G) for a document relationship between two document occurrences in Xr is computed as in Equation 3 for both the clustering and priority relationships.",null,null
332,"As for the relationships between the unlimited tail X¬r and documents in Xr, we must consider that all documents in the infinite set X¬r have the same weight. Therefore, the finite amount of relevant documents in the long tail has no effect. Then, any relevant document in Xr is correctly related with all documents in X¬r if it appears between the prioritized documents in Gr. According to Equation 3, being dij the j occurrence of document di in the system output and being Di its set of occurrences1:",null,null
333,P (dij,null,null
334,G,null,null
335,X¬r ),null,null
336,",",null,null
337,"min(|Di  Gr|, |Di |Di  Xr|",null,null
338, Xr|),null,null
339,"According to all of this, Reliability over priority relationships can be computed as follows2",null,null
340,R,null,null
341,",",null,null
342,dij dklXr,null,null
343,"P (r(dij , dkl)  G)wx(di,j )wx(dk,l) + Wx,dij ,",null,null
344,"r(dij ,dkl)X",null,null
345,P (dij,null,null
346,dij Xr,null,null
347,g X¬r)wx(dij )W (X¬r),null,null
348,"1 Wx,dij ,",null,null
349,1 +,null,null
350,Wx (Xr ),null,null
351,"Assuming that the documents in the long tail do not have clustering relationships with each other, Reliability over clustering relationships can be computed as follows:",null,null
352,"R ,",null,null
353,dij Xr,null,null
354,P (dij,null,null
355,g,null,null
356,"dkl)wx(dij )wx(dkl) Wx,dij ,",null,null
357,+ Wx(X¬r),null,null
358,dkl Xr,null,null
359,dij xdkl,null,null
360,"Sensitivity is computed in the same way, but exchanging",null,null
361,"X by G and x by g. The complexity of this computation is O(n2), being n the amount of ranked documents Xr or prioritized documents Gr.",null,null
362,"1For the sake of simplicity, we notate P (dij X¬r  G) as P (dij G X¬r) 2The first component covers the relationships within documents in Xr. The second and third components cover the relationship Xr  X¬r and X¬r  Xr respectively.",null,null
363,648,null,null
364,Figure 1: Comparing Evaluation Measures over Document Filtering Task.,null,null
365,4. RELIABILITY AND SENSITIVITY: METAEVALUATION,null,null
366,"As far as we know, Reliability and Sensitivity are the first measures which are applicable to the general document organization task. For this reason, our comparison will focus on how R and S behave in each of the subsumed tasks (filtering, clustering, ranking) with respect to previously existing measures.",null,null
367,4.1 Meta-evaluation Criteria,null,null
368,"There are many ways of meta-evaluating a new measure. The most direct one consists of comparing measure scores vs. human assessments of quality, or system results in some extrinsic task. Other meta-evaluation criteria focus on the hability to capture information from limited data sets. Some examples are discriminativeness [23], statistical significant differences between systems[21], stability method, swap method, robustness against noisy data, correlation between rankings over different data sets [5], etc. The main drawback of these methods is that a measure can be, for instance, perfectly discriminative under limited data sets without giving information about quality. As an extreme example, the ranking length can be perfectly discriminative but not useful for evaluation purposes.",null,null
369,"in this study we want to investigate how useful is to evaluate measures in terms of a basic, intuitive set of formal constraints. According to this, our first meta-evaluation criterion is the ability to satisfy the stated formal constraints. After that formal analysis, and in order to compare measures empirically over data sets, we will assume that current standard measures used by the community are, to a certain extent, reliable: we assume that all of them give some useful information about system quality in certain scenarios. The problem is that, in most cases, we do not know exactly the real scenarios in which the system will be employed. In previous experiments, particularly in the case of clustering and filtering, it has been shown that there can be a very low correlation between measure results [3]. Therefore, we cannot expect to find a measure which is correlated with all of them. However, we can at least ensure that a high score according to the measure implies a high score according to all measures. This is strictness. In other words, a good score in a reliable measure should ensure a good system regardless of the environmental conditions. Note that strictness itself is not enough as a meta-evaluation criterion (a measure that always scores zero is the strictest of all). Note also that strictness could easily be achieved by computing a harmonic mean of the most popular measures, but we would have to solve scale-normalization issues and we would end",null,null
370,"up with a measure that would be hard to interpret, and would not cover all quality aspects in an homogeneous manner. Therefore, and ad-hoc combination of metrics is not the best solution to have a strict measure.",null,null
371,"Let us quantify Strictness in the following way: we compute, for each measure, all the rank positions obtained by each system output o  O for each topic and measure. Then we define strictness as the largest difference between a high rank assigned by our measure and a low rank assigned by a traditional measure:",null,null
372,"Strictness(m) ,"" - M axo,m (Rankm(o) - Rankm (o)) |O|""",null,null
373,"In order to maintain a correspondence with standard metaevaluation criteria, we will also compute the robustness of measures in terms of the average Spearman correlation of system scores across topics. Being Rnk(m, ti) the ranking of systems produced by metric m for topic ti:",null,null
374,"Robustness(m) ,"" Avgi,j(Spearman(Rnk(m, ti), Rnk(m, tj)))""",null,null
375,We now discuss each of the subsumed tasks.,null,null
376,4.2 Clustering Scenario,null,null
377,"In the non overlapped clustering scenario where all documents has the same weight, R and S turn into Bcubed Precision and Bcubed Recall:",null,null
378,"RX ,G  AvgdP (r(d, d )  G|r(d, d )  X ) , BR(X)",null,null
379,"SX ,G  AvgdP (r(d, d )  X |r(d, d )  G) , BR(X)",null,null
380,"Bcubed measures are the only ones that satisfy all the clustering constraints [2]. However, in the extended version for overlapping clustering, the Cluster Size vs Quantity restriction is no longer satisfied. This is due to the fact that, in the extended Bcubed version, all the relationships from one document are computed as a single unit, even when it belongs to several clusters at the same time. The result is that if two documents d and d are related to each other several times (e.g. they share more than one information nugget), breaking all these relationships is penalized only once. Therefore, splitting n clusters can have less effect than splitting one document from a cluster with size n. The solution provided by R and S consists of considering the document in each different cluster as a separate document. BCubed measures are well-known and have already been studied formally and empirically and compared with other measures in previous work [2]. Therefore, we will focus on the other subsumed tasks for our meta evaluation.",null,null
381,649,null,null
382,Accuracy Utility Lam% F Measure R*S,null,null
383,Rob.,null,null
384,0.3,null,null
385,0.39 0.38,null,null
386,0.49,null,null
387,0.70,null,null
388,Strict. -0.91 -0.91 -0.96,null,null
389,-0.92 -0.78,null,null
390,"Table 7: Robustness and Strictness achieved by Filtering Evaluation Measures, WEPS2 test set",null,null
391,"The measures F, Utility and Accuracy achieve -0.92 of Strictness. There exists a clear difference in robustness between R*S and other measures. In this scenario, robustness seems to be correlated with strictness.",null,null
392,4.4 Document Retrieval,null,null
393,4.3 Document Filtering Task,null,null
394,4.3.1 Formal Constraints,null,null
395,"The Filtering scenario consists of distributing a finite set of documents into two priority levels (binary classification). Being Xr and Gr the sets of prioritized documents in the system output and gold standard respectively, the correct relationships in the system output are priority relationships between relevant documents in Xr and irrelevant documents in ¬Xr. Therefore, R corresponds with:",null,null
396,"R(X) ,"" AvgdP (rg(d, d )|rx(d, d )) "", P (Gr|Xr)P (Xr)P (¬Gr|¬Xr) + P (¬Gr|¬Xr)P (¬Xr)P (Gr|Xr) ,",null,null
397,"P (¬Gr|¬Xr)P (Gr|Xr)(P (Xr) + P (¬Xr)) , P (¬Gr|¬Xr)P (Gr|Xr)",null,null
398,"This corresponds with the product of precisions over positive and negative sets in the output. Analogously, Sensitivity corresponds with the product of Recalls over the positive and negative sets.",null,null
399,"S(X) , P (¬Xr|¬Gr)P (Xr|Gr)",null,null
400,"Just like any other filtering measure, the combination of R and S (using the F measure) satisfies the priority constraint. Let us denotate the harmonic mean of R and S as R*S. With respect to how R*S scores non-informative outputs, its behavior is a mixture of the other measure families: it assigns a zero-score to the all-relevant and all-irrelevant baselines, because they are not able to distinguish any useful priority relationship between documents. Other arbitrary partitions receive scores that depend on how the ground truth partitions the test set.",null,null
401,4.3.2 Empirical meta-evaluation,null,null
402,"For the filtering scenario we employ the evaluation corpus and system results from the second task in the WePS3 competition [1]. The task consisted of classifying Twitter entries [17] that contain a company name as relevant when they do refer to the company and irrelevant otherwise. The test set includes tweets for 47 companies and the training set includes 52 company names. For each company, around 400 tweets were retrieved using the company name as query. The ratio of related tweets per company name is variable, covering both extremely low and high ratios. We will refer to a company tweet set retrieved by a query in a time slot as an input stream or topic. Five research teams participated in the competition, and sixteen runs were evaluated. The organizers included two baseline systems: the placebo system (all true) and its opposite (all false).",null,null
403,"Figure 1 shows the correspondence between R*S and standard measures employed in different evaluation campaigns; F represents the harmonic mean of precision and recall over the positive class. The most relevant fact is that a high score in all standard measures is necessary to achieve a high score according to R*S. Table 7 shows the Robustness and strictness of measures (see Section 4.1) in this test set. The strictest measure is R*S (-0.78), followed by Lam% (-0.89).",null,null
404,4.4.1 Formal Constraints,null,null
405,"Just like MAP and DCG, Reliability and Sensitivity satisfy the first two constraints, Priority and Deepness. Adding an incorrect relationship produces a score decrease, and the effect of an incorrect relationship depends on the deepness of the related documents in the system output ranking. However, unlike MAP or DCG, Reliability and Sensitivity also satisfy the third constraint, Deepness Threshold. And, unlike MRR, they also satisfy the fourth constraint, Closeness Threshold. Due to space constraints, we do not include here the formal proofs.",null,null
406,"Note that the parameters n and Wn offer great flexibility, and permit to accomodate scenarios where only the top documents in the rank matter (as in Web search) as well as recall-oriented scenarios, simply adjusting the parameters accordingly. The formal constraints are satisfied for any paratemer setting. For example, with the setting W30 ,"" 0.8, ranking 30 relevant documents after 30 irrelevant documents is worse than retrieving one document in the first position, but retrieving five relevant documents in the first 10 positions is better than retrieving only one document at the top 1. The cut point is n"",20.",null,null
407,"As for the confidence constraint, note that the more we include irrelevant documents in the ranking, the more we include incorrect priority relationships between the priorized documents and the long tail. We could also satisfy this property with measures like RBP by using a discounting score for each irrelevant document, but then the nature of RBP would change and its formal properties would not hold any longer. Table 6 summarizes the results of the formal constraint analysis.",null,null
408,4.4.2 Empirical Meta-evaluation,null,null
409,"We have used queries 701 to 750 in the GOV-2 collection, which were employed in the TREC 2004 Terabyte Track. The GOV-2 corpus consists of approximately 25 million documents. Therefore, we can assume that the amount of documents in the collection is unlimited for practical purposes. Relevant documents were manually annotated for each query. We consider the results of 60 retrieval systems developed by the participants in the track. Two relevance levels (high and medium) were considered in the human annotation.",null,null
410,"Table 8 shows the strictness and robustness3 of measures. We consider the strictness of all measures against the standard measures MAP, DCP, P@10, MRR, RBPp,0.8 and RBPp,0.95. The strictest measure is R*S with W80 ,"" 30 (-0.58) followed by MRR (-0.63), MAP (-0.74) and P@10 (0.75). However, R*S with W80 "","" 30 achieves low robustness, while R*S with n80 "", 800 and DCG have higher robustness at the cost of strictness. It seems that there is a trade-off between strictness and robustness. A possible explanation is that the robustness of measures across test cases depends on the amount of data that is considered for the evalua-",null,null
411,"3Discriminating systems that return short rankings is easy. In order to avoid this noise, we only consider in this test those systems that return at least 1000 documents",null,null
412,650,null,null
413,Measure,null,null
414,Robustness Strictness,null,null
415,MAP,null,null
416,0.55 -0.89,null,null
417,DCG,null,null
418,0.60 -0.68,null,null
419,P@10,null,null
420,0.3 -0.76,null,null
421,MRR,null,null
422,0.4 -0.63,null,null
423,"RBP p,0.8 0.28 -0.92",null,null
424,"RBP p,0.95",null,null
425,0.39 -0.65,null,null
426,R*S W30 0.35 -0.58,null,null
427,R*S W800 0.57 -0.73,null,null
428,"Table 8: Robustness and strictness of Document Retrieval Evaluation Measures, GOV2 test set.",null,null
429,Gold Standard,null,null
430,d1 {d2 d3 d4} {d4 d5} {d6 d7},null,null
431,"{d6 d7} R ,1 S ,1",null,null
432,"R,1 S,1",null,null
433,System Output 2,null,null
434,d1 {d2 d3}{ d4} {d4 d5} {d6 d7},null,null
435,"{d6 d7} R ,1 S ,1 R,1 S,0.8",null,null
436,System Output 4,null,null
437,d1 {d3 d4 } {d4 d5} {d6 d7},null,null
438,"{d6 d7 d8} R ,0.95 S ,0.85",null,null
439,"R,0.96 S,0.74",null,null
440,System Output 1,null,null
441,d1{d2 d3 d4} {d4 d5}{d6 d7},null,null
442,"d6 d7 R ,1 S ,1 R,1 S,0.97",null,null
443,System Output 3,null,null
444,d1 {d3 d4 } {d4 d5} {d6 d7},null,null
445,"{d6 d7} R ,1 S ,0.86",null,null
446,"R,1 S,0.74",null,null
447,System Output 5,null,null
448,{d6 d7} {d4 d5} {d6 d7},null,null
449,"d1 {d2 d3 d4} R ,0.64 S ,0.59",null,null
450,"R,1 S,1",null,null
451,Table 9: Sensitivity and Reliability examples for the generic Document Organization scenario.,null,null
452,"tion. Therefore, measures focused on the top of the ranking are less robust. Given that the max function in the strictness definition is very sensitive to outlier systems, we have also computed strictness considering the 10 maximum differences; results were the same.",null,null
453,5. GENERIC DOCUMENT ORGANIZATION SCENARIO,null,null
454,"As far as we know, there is no measure that can be directly compared with R*S in the generic document organization scenario as we have defined it. In this section, we illustrate the behavior of R*S across several instances of system outputs. See Table 9. The gold standard consists of seven relevant documents distributed along three priority levels. Each priority level contains two clusters (or information nuggets) and documents 4,6 and 7 appear in two clusters and priority levels simultaneously. The Reliability and Sensitivity over priority and clustering relationships have been computed with W10 ,"" 0.8, i.e., we require that the first 10 occurrences represent 80% of the score.""",null,null
455,"Of course, Reliability and Sensitivity are maximal for the gold standard. Starting from this, we can identify in the table the following behaviors: (System 1) breaking clusters at low priority levels decreases slightly S, (System 2) breaking clusters at high priority levels decreases S to a greater extent; (System 3) Removing one document (d2) decreases priority and clustering sensitivity; (System 4) removing and adding noisy documents (d2 and d8) decreases both sensitivity and reliability scores and in (System 5) we swap all priority levels. Then, clustering is perfect and the priority R and S decrease, but not to zero. Notice that the 10 first documents have only a 80% of weight in the evaluation. There exists a long tail of documents from which this set is priorized.",null,null
456,6. CONCLUSIONS,null,null
457,"In this paper we have discussed how some prominent Information Access tasks ­ Document Retrieval, Clustering and Filtering ­ can be subsumed under a generic Document Organization task that establishes two kinds of binary relationships between documents: relatedness (which forms clusters) and priority (ranking). We have then introduced two evaluation measures for the document organization problem: Reliability and Sensitivity, which are precision and recall of the predicted set of relationships with respect to the true relationships, with a specific relative weighting scheme between relations. The main contribution of this paper is that R and S can be applied to complex tasks which involve ranking, clustering and filtering at the same time. An example task is online reputation monitoring, where systems have to (i) filter out irrelevant information, (ii) organize relevant information in topics, and (iii) decide which topics have more priority from the point of view of reputation management. R and S are able to provide a unique evaluation measure for this combined problem.",null,null
458,"In addition, R and S satisfy all formal constraints in each of the subsumed tasks; in particular, they satisfy more formal constraints than any previous measure in the Document Retrieval task, and they are able to accomodate several retrieval scenarios (from precision-oriented to recall-oriented) via two parameters that establish that the first n levels in the rank carry on a fraction Wn of the overall quality score. Our empirical study indicates that R and S are stricter than standard measures, i.e., a high result with R and S ensures a high result with any other standard measure in all the subsumed tasks. That makes R and S a preferable choice in cases where the application scenario does not clearly point towards any of the previously existing measures, because it guarantees that a good result will still hold according to any other standard measure.",null,null
459,"For its application to combined tasks, future work involves extending the set of binary relations ­ the general principle of R and S can be applied to any kind of relations ­ together with a a careful analysis on how to assign relative weights to different types of relations; for instance, in certain application scenarios one type of relation (priority or relatedness) may dominate and obscure what is going on with the other, making R and S less transparent and/or useful.",null,null
460,Code to use R and S is available at http://nlp.uned.es.,null,null
461,Acknowledgments,null,null
462,"This work has been partially funded by EU FP7 project Limosine (grant number 288024), a Google Faculty Research Award (Axiometrics, project Holopedia (grant from the Spanish goverment) and project MA2VICMR (grant from the government of Comunidad de Madrid).",null,null
463,651,null,null
464,7. REFERENCES,null,null
465,"[1] E. Amigo´, J. Artiles, J. Gonzalo, D. Spina, B. Liu, and A. Corujo. WePS3 Evaluation Campaign: Overview of the On-line Reputation Management Task. In 2nd Web People Search Evaluation Workshop (WePS 2010), CLEF 2010 Conference, Padova Italy, 2010.",null,null
466,"[2] E. Amig´o, J. Gonzalo, J. Artiles, and F. Verdejo. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Inf. Retr., 12:461­486, August 2009.",null,null
467,"[3] E. Amig´o, J. Gonzalo, and F. Verdejo. A comparison of evaluation metrics for document filtering. In Proceedings of CLEF'11, CLEF'11, pages 38­49, Berlin, Heidelberg, 2011. Springer-Verlag.",null,null
468,"[4] C. Buckley and E. M. Voorhees. Retrieval evaluation with incomplete information. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '04, pages 25­32, New York, NY, USA, 2004. ACM.",null,null
469,"[5] B. Carterette. System effectiveness, user models, and user utility: a conceptual framework for investigation. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, SIGIR '11, pages 903­912, New York, NY, USA, 2011. ACM.",null,null
470,"[6] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM, pages 611­620, 2011.",null,null
471,"[7] O. Chapelle and Y. Zhang. A dynamic bayesian network click model for web search ranking. In WWW, pages 1­10, 2009.",null,null
472,"[8] J. M. Cigarra´n, A. Pen~as, J. Gonzalo, and F. Verdejo. Automatic selection of noun phrases as document descriptors in an fca-based information retrieval system. In ICFCA, pages 49­63, 2005.",null,null
473,"[9] C. L. A. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR, pages 659­666, 2008.",null,null
474,"[10] J. Cohen. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37, 1960.",null,null
475,"[11] G. Cormack and T. Lynam. Trec 2005 spam track overview. In Proceedings of the fourteenth Text Retrieval Conference 8TREC 2005), 2005.",null,null
476,"[12] G. V. Cormack and T. R. Lynam. TREC 2005 Spam Track Overview. In Proceedings of the fourteenth Text REtrieval Conference (TREC-2005), 2005.",null,null
477,"[13] M. Halkidi, Y. Batistakis, and M. Vazirgiannis. On Clustering Validation Techniques. Journal of Intelligent Information Systems, 17(2-3):107­145, 2001.",null,null
478,"[14] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/gather on retrieval results. pages 76­84, 1996.",null,null
479,"[15] B. Hu, Y. Zhang, W. Chen, G. Wang, and Q. Yang. Characterizing search intent diversity into click models. In Proceedings of the 20th international conference on World wide web, WWW '11, pages 17­26, New York, NY, USA, 2011. ACM.",null,null
480,[16] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated gain-based,null,null
481,"evaluation of ir techniques. ACM Trans. Inf. Syst., 20:422­446, October 2002.",null,null
482,"[17] B. Krishnamurthy, P. Gill, and M. Arlitt. A few chirps about twitter. In WOSP '08: Proceedings of the first workshop on Online social networks, pages 19­24, New York, NY, USA, 2008. ACM.",null,null
483,"[18] A. Leuski. Evaluating document clustering for interactive information retrieval. In CIKM, pages 33­40, 2001.",null,null
484,"[19] M. Meila. Comparing clusterings. In Proceedings of COLT 03, 2003.",null,null
485,"[20] T. M. Mitchell. Machine learning. McGraw Hill, New York, 1997.",null,null
486,"[21] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst., 27(1):2:1­2:27, Dec. 2008.",null,null
487,"[22] A. Rosenberg and J. Hirschberg. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of EMNLP-CoNLL 2007, pages 410­420, 2007.",null,null
488,"[23] M. D. Smucker and C. L. Clarke. Time-based calibration of effectiveness measures. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, SIGIR '12, pages 95­104, New York, NY, USA, 2012. ACM.",null,null
489,"[24] S. Vargas and P. Castells. Rank and relevance in novelty and diversity metrics for recommender systems. In 5th ACM Conference on Recommender Systems (RecSys 2011), pages 109­116, Chicago, Illinois, October 2011.",null,null
490,"[25] E. M. Voorhees. The trec-8 question answering track report. In In Proceedings of TREC-8, pages 77­82, 1999.",null,null
491,652,null,null
492,,null,null

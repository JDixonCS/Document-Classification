,sentence,label,data
0,The Cluster Hypothesis for Entity Oriented Search,null,null
1,"Hadas Raviv, Oren Kurland",null,null
2,"Faculty of Industrial Engineering and Management, Technion, Haifa 32000, Israel hadasrv@tx.technion.ac.il,kurland@ie.technion.ac.il",null,null
3,David Carmel,null,null
4,"Yahoo! Research, Haifa 31905, Israel",null,null
5,david.carmel@ymail.com,null,null
6,ABSTRACT,null,null
7,"In this work we study the cluster hypothesis for entity oriented search (EOS). Specifically, we show that the hypothesis can hold to a substantial extent for several entity similarity measures. We also demonstrate the retrieval effectiveness merits of using clusters of similar entities for EOS.",null,null
8,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
9,"Keywords: cluster hypothesis, entity oriented search",null,null
10,1. INTRODUCTION,null,null
11,"The entity oriented search (EOS) task has attracted much research attention lately. The main goal is to rank entities in response to a query by their presumed relevance to the information need that the query expresses. For example, the goal in the TREC's expert search task was to rank employees in the enterprise by their expertise in a topic [6]. The goal in INEX entity ranking track was to retrieve entities that pertain to a topic in the English Wikipedia [7, 8, 9]. The goal in TREC's entity track was to rank Web entities with respect to a given entity by their relationships [2, 3, 4].",null,null
12,"The EOS task is different than the standard ad-hoc document retrieval task as entities are somewhat more complex than (flat) documents. That is, entities are characterized by different properties such as name, type (e.g., place or person), and potentially, an associated document (e.g., a homepage or a Wikipedia page). Despite the fundamental difference between the two tasks, we set as a goal to study whether an important principle in ad-hoc document retrieval also holds for the EOS task; namely, the cluster hypothesis [22]. We present the first study of the cluster hypothesis for EOS, where the hypothesis is that ""closely associated entities tend to be relevant to the same requests"".",null,null
13,"We use several inter-entity similarity measures to quantify the association between entities, which is a key point in the hypothesis. These measures are based on the entity type which is a highly important source of information [18, 14]. We then show that the cluster hypothesis, tested using",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'13, July 28­August 1, 2013, Dublin, Ireland. Copyright 2013 ACM 978-1-4503-2034-4/13/07 ...$15.00.",null,null
15,"Voorhees' nearest neighbor test [23], can hold to a substantial extent for EOS for several of the similarity measures.",null,null
16,"Motivated by the findings about the cluster hypothesis, we explore the merits of using clusters of similar entities for entity ranking. We show that ranking entity clusters by the percentage of relevant entities that they contain can be used to produce extremely effective entity ranking. We also demonstrate the effectiveness of using cluster ranking techniques that are based on estimating the percentage of relevant entities in the clusters for entity ranking.",null,null
17,"Our main contributions are three fold: (i) showing that for several inter-entity similarity measures the cluster hypothesis holds for EOS to a substantial extent as determined by the nearest neighbor test; (ii) demonstrating the considerable potential of using clusters of similar entities for EOS; and, (iii) showing that using simple cluster ranking methods can help to improve retrieval performance with respect to that of an effective initial search.",null,null
18,2. RELATED WORK,null,null
19,"Any entity in the INEX entity ranking track, which we use for our experiments, has a Wikipedia page. Hence, some previously proposed estimates for the entity-query similarity are based on Wikipedia's category information [20, 1, 18, 14]. Similarly, we measure the similarity between two entities based on several measures of the similarity between the sets of categories of the two entity pages.",null,null
20,"In Cao et al.'s work on expert search [5] the retrieval score assigned to an entity was smoothed with the retrieval score assigned to a cluster constructed from the entity. In contrast, we explore a retrieval paradigm that ranks entity clusters and transforms the ranking to entity ranking. Work on document retrieval showed that cluster-based (score) smoothing and cluster ranking are complementary [16]. We leave the exploration of this finding for the EOS task for future work.",null,null
21,"There are several tests of the cluster hypothesis for document retrieval [13, 10, 23, 19]. We use Voorhees' nearestneighbor test for the EOS task as it is directly connected with the nearest neighbor clusters we use for ranking.",null,null
22,"The findings we present for the EOS task echo those reported for document retrieval. Namely, the extent to which the cluster hypothesis holds [23], and the (potential) merits of using cluster ranking [12, 21, 15, 17].",null,null
23,3. THE CLUSTER HYPOTHESIS,null,null
24,"Our first goal is to explore the extent to which the cluster hypothesis holds for EOS. To this end, we use the nearest",null,null
25,841,null,null
26,"neighbor test [23]. Let L[qn] be the list of n entities that are the highest ranked by an initial search performed in response to query q. For each relevant entity in L[qn], we record the percentage of relevant entities among its K nearest neighbors in L[qn]. The nearest neighbors are determined using one of the inter-entity similarity measures specified in Section 5.1. The test result is the average of the recorded percentages over all relevant entities in L[qn], averaged over all test queries.",null,null
27,"Some of the inter-entity similarity measures assign discrete values including 0. Hence, for some relevant entities there could be less than K neighbors as we do not consider neighbors with a 0 similarity value. In addition, a relevant entity might be assigned with more than K nearest neighbors due to ties in the similarity measure. That is, we keep collecting all entities having the same similarity value as that of the last one in the K neighbors list.",null,null
28,4. CLUSTER-BASED ENTITY RANKING,null,null
29,Our second goal is studying the potential merits of us-,null,null
30,ing entity clusters to induce entity ranking. We re-rank the,null,null
31,initial entity list L[qn] using a cluster-based paradigm which,null,null
32,is very common in work on document retrieval [17]. Let,null,null
33,Cl(L[qn]) be the set of clusters created from L[qn] using some,null,null
34,clustering method. The inter-entity similarity measures used,null,null
35,for creating clusters are those used for testing the cluster hy-,null,null
36,pothesis. (See Section 5.1 for further technical details.) The,null,null
37,clusters in Cl(L[qn]) are ranked by the presumed percentage,null,null
38,of relevant entities that they contain. Below we describe,null,null
39,"two cluster ranking methods. Then, each cluster is replaced",null,null
40,with its constituent entities while omitting repeats. Within,null,null
41,cluster entity ranking is based on the initial entity retrieval,null,null
42,scores which were used to create the list L[qn].,null,null
43,The MeanScore cluster ranking method scores cluster,null,null
44,c by the mean retrieval score of its constituent entities:,null,null
45,1 |c|,null,null
46,ec Sinit(e; q); Sinit(e; q) is the initial retrieval score,null,null
47,of entity e; |c| is the number of entities in c.,null,null
48,When Sinit(e; q) is a rank equivalent estimate to that of,null,null
49,"log(P r(q, e)) [18], the cluster score assigned by MeanScore",null,null
50,is rank equivalent to the geometric mean of the joint query-,null,null
51,entity probabilities' estimates in the cluster. Using a geometric-,null,null
52,mean-based representation for document clusters was shown,null,null
53,to be highly effective for ranking document clusters [17].,null,null
54,"The regularized mean score method, RegMeanScore in",null,null
55,"short, which is novel to this study, smoothes c's score:",null,null
56,. ec,null,null
57,Sinit (e;q)+,null,null
58,1 n,null,null
59,eL[qn] Sinit(e;q),null,null
60,|c|+1,null,null
61,The cluster score is the,null,null
62,mean retrieval score of a cluster composed of c's entities,null,null
63,"and an additional ""pseudo"" entity whose score is the mean",null,null
64,"score in the initial list. This method helps to address, among",null,null
65,"others, cluster-size bias issues.",null,null
66,5. EVALUATION,null,null
67,5.1 Experimental setup,null,null
68,"We conducted experiments with the datasets of the INEX entity ranking track of 2007 [7], 2008 [8], and 2009 [9]. Table 1 provides a summary of the datasets. The tracks for 2007 and 2008 used the English Wikipedia dataset from 2006, while the 2009 track used the English Wikipedia from 2008. The set of test topics for 2007 is composed of 21 topics that",null,null
69,Data set,null,null
70,2007 2008 2009,null,null
71,Collection size,null,null
72,4.4 GB 4.4 GB 50.7 GB,null,null
73,# of documents,null,null
74,"659, 388 659, 388 2, 666, 190",null,null
75,# of test topics,null,null
76,46 35 55,null,null
77,Table 1: INEX entity ranking datasets.,null,null
78,"were derived from the ad hoc 2007 assessments, and additional 25 topics that were created by the participants specifically for the track. In 2008, 35 topics were created and used for testing. The topics used for testing in 2009 were 55 topics out of the 60 test topics used in 2007 and 2008.",null,null
79,"We used Lucene (http://lucene.apache.org/core/) for experiments. The data was pre-processed using Lucene, including tokenization, stopword removal, and Porter stemming.",null,null
80,Inter-entity similarity measures. The inter-entity simi-,null,null
81,"larity measures that we use utilize Wikipedia categories. Specifically, the categories associated with the Wikipedia page of the entity, henceforth referred to as its category set, serve as the entity type.",null,null
82,"The Tree similarity between two entities e1 and e2 is exp(-d(e1, e2)) where d(e1, e2) is the minimum distance over Wikipedia's categories graph between a category in e1's category set and a category in e2's category set;  is a decay constant determined as in [18]. The SharedCat measure is the cosine similarity between the binary vectors representing two entities. An entity vector is defined over the categories space. An entry in the vector is 1 if the corresponding category is associated with the entity and 0 otherwise. Thus, SharedCat measures the (normalized) number of categories shared by the two entities [20]. The CE measure is based on measuring the language-model-based similarity between the documents associated with the category sets of two entities [14]. More specifically, each category is represented in this case by the text that results from concatenating all Wikipedia pages associated with the category. The similarity between the texts x and y that represent two categories is exp(-CE(p[x0](·)||p[yµ](·))); CE is the cross entropy measure; p[zµ](·) is the Dirichlet-smoothed unigram language model induced from z with the smoothing parameter µ (,""1000). The CE similarity between two entities is defined as the maximal similarity, over all pairs of categories, one in the first entity's category set and the other in the second entity's category set, of the texts representing the categories. Finally, the ESA (Explicit Semantic Analysis) [11] similarity measure is the cosine between two vectors, each represents the category set of an entity. The vectors representing the category sets are defined over the entities space. The value of an entry in the vector is the number of the categories in the given category set that are associated with the corresponding entity. Using ESA to measure inter-entity similarity is novel to this study.""",null,null
83,"Three different initially retrieved entity lists, L[qn], are used for both the cluster hypothesis test and cluster-based ranking. The lists are created in response to the query using highly effective entity retrieval methods [18]. The first list, LDoc, is created by representing an entity with its Wikipedia document (page). The documents are ranked in response to the query using the standard language-model-based approach with Dirichlet-smoothed unigram language models and the cross entropy similarity measure. The second list,",null,null
84,842,null,null
85,Similarity measure Tree,null,null
86,SharedCat CE ESA,null,null
87,Initial list LDoc LDoc;T ype LDoc;T ype;N ame,null,null
88,LDoc LDoc;T ype LDoc;T ype;N ame,null,null
89,LDoc LDoc;T ype LDoc;T ype;N ame,null,null
90,LDoc LDoc;T ype LDoc;T ype;N ame,null,null
91,2007 30.0 29.8 32.7,null,null
92,35.7 33.5 37.9,null,null
93,33.4 34.5 37.5,null,null
94,34.3 33.7 37.3,null,null
95,2008 32.0 35.5 37.7,null,null
96,41.0 45.5 44.3,null,null
97,36.2 38.6 41.7,null,null
98,36.2 41.0 39.1,null,null
99,2009 42.7 44.9 44.9,null,null
100,45.4 52.2 52.7,null,null
101,46.0 50.3 49.7,null,null
102,46.2 49.5 49.0,null,null
103,Table 2: The cluster hypothesis test: the average percentage of relevant entities among the 5 nearest neighbors of a relevant entity.,null,null
104,"LDoc;T ype, is created by scoring entities with an interpolation of two scores. The first is that used to create the list LDoc. The second is the similarity between the category set of the entity and the query target type (the set of categories that are relevant to the query, as defined by INEX topics). The Tree estimate described above is used for measuring similarity between the two category sets. The third list, LDoc;T ype;Name, is created by scoring an entity with an interpolation of the score used to create LDoc;T ype, and an estimate for the proximity-based association [18] between the query terms and the entity name (i.e., the title of its Wikipedia page) in the corpus. We employ the same traintest approach as in [18] to set the free-parameter values of the ranking methods used to create the initial lists. The number of entities in each initial list L[qn] is n , 50.",null,null
105,"We use a simple nearest neighbor clustering method to cluster entities in the initial list L[qn]. Specifically, each entity in L[qn] and the K (,"" 5) entities in L[qn] that are the most similar to it, according to the inter-entity similarity measures described above, form a cluster. Using such small overlapping clusters was shown to be highly effective for cluster-based document retrieval [16, 15, 17]. We note that not all clusters necessarily contain K + 1 documents due to the reasons specified in Section 3. For consistency, we also use K "", 5 in the cluster hypothesis test.",null,null
106,"Following the INEX guidelines, the evaluation metric for INEX 2007 is mean average precision (MAP) while that for INEX 2008 and 2009 is infAP. We also report the precision of the top 5 entities (p@5). Statistically significant differences of retrieval performance are determined using the two tailed paired t-test with a 95% confidence level.",null,null
107,5.2 Experimental results,null,null
108,5.2.1 The cluster hypothesis,null,null
109,"Table 2 presents the results of the nearest neighbor cluster hypothesis test that was described in Section 3. The test is performed on the different initially retrieved entity lists using the various inter-entity similarity measures. We see that the average percentage of relevant entities among the nearest neighbors of a relevant entity ranges between 30% and 53% across the various experimental settings. We also found out that, on average, the percentage of relevant entities in a list is often lower than 25% and can be as low as 10%. Thus, due to the relatively high percentage of relevant enti-",null,null
110,"ties among the nearest neighbors of relevant entities, we can conclude that the cluster hypothesis holds to a substantial extent, according to the nearest neighbor test, with various inter-entity similarity measures.",null,null
111,"Table 2 also shows that for most of the data sets and similarity measures the test results for the LDoc;T ype and LDoc;T ype;Name lists are higher than for LDoc. This finding is not surprising as LDoc;T ype and LDoc;T ype;Name were created using entity-query similarity measures that account for category information, while the similarity measure used to create LDoc does not use this information. The highest test results are obtained for the SharedCat similarity measure which, as noted above, measures the (normalized) number of shared categories between two entities.",null,null
112,5.2.2 Cluster-based entity ranking,null,null
113,"Table 3 presents the results of employing cluster-based entity re-ranking, as described in Section 4, upon the three initial entity lists. The various inter-entity similarity measures are used for creating the clusters. 'Initial' refers to the initial ranking of a list. 'Oracle' is the ranking of entities that results from employing the cluster-based re-ranking paradigm described in Section 4; the clusters are ranked by the true percentage of relevant entities that they contain.",null,null
114,"The high performance numbers for Oracle, which are substantially and statistically significantly better than those for Initial, attest to the existence of clusters that contain a very high percentage of relevant entities. More generally, these numbers attest to the incredible potential of employing effective cluster ranking methods to rank entities.",null,null
115,"RegMeanScore, which outperforms MeanScore due to the regularization discussed in Section 4, is in quite a few cases more effective than Initial; specifically, using the Tree and SharedCat inter-entity similarity measures. While the improvements for LDoc are often statistically significant, this is not the case for LDoc;T ype and LDoc;T ype;Name. Naturally, the more effective the initial ranking (Initial), the more challenging the re-ranking task. Yet, the very high Oracle numbers for LDoc;T ype and LDoc;T ype;Name imply that effective cluster ranking methods can yield performance that is much better than that of the initial ranking. Finally, for both LDoc;T ype and LDoc;T ype;Name the best performance is in most cases attained by using RegMeanScore.",null,null
116,6. CONCLUSIONS AND FUTURE WORK,null,null
117,We showed that the cluster hypothesis can hold to a substantial extent for the entity oriented search (EOS) task with several inter-entity similarity measures. We also demonstrated the potential merits of using a cluster-based retrieval paradigm for EOS that relies on ranking entity clusters. Devising improved cluster ranking techniques is a future venue we intend to explore.,null,null
118,7. ACKNOWLEDGMENTS,null,null
119,"We thank the reviewers for their comments. Part of the work reported here was done while David Carmel was at IBM. This paper is based on work that has been supported in part by the Israel Science Foundation under grant no. 433/12 and by Google's faculty research award. Any opinions, findings and conclusions or recommendations expressed in this material are the authors' and do not necessarily reflect those of the sponsors.",null,null
120,843,null,null
121,8. REFERENCES,null,null
122,"[1] K. Balog, M. Bron, and M. De Rijke. Query modeling for entity search based on terms, categories, and examples. ACM Trans. Inf. Syst., 29(4), 2011.",null,null
123,"[2] K. Balog, A. P. de Vries, P. Serdyukov, P. Thomas, and T. Westerveld. Overview of the trec 2009 entity track. In Proceedings of TREC, 2009.",null,null
124,"[3] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the trec 2010 entity track. In Proceedings of TREC, 2010.",null,null
125,"[4] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the trec 2011 entity track. In Proceedings of TREC, 2011.",null,null
126,"[5] Y. Cao, J. Liu, S. Bao, and H. Li. Research on expert search at enterprise track of TREC 2005. In Proceedings of TREC, volume 14, 2005.",null,null
127,"[6] N. Craswell, A. P. de Vries, and I. Soboroff. Overview of the trec 2005 enterprise track. In Proceedings of TREC, 2005.",null,null
128,"[7] A. P. de Vries, A.-M. Vercoustre, J. A. Thom, N. Craswell, and M. Lalmas. Overview of the INEX 2007 entity ranking track. In Proceedings of INEX, pages 245­251, 2007.",null,null
129,"[8] G. Demartini, A. P. de Vries, T. Iofciu, and J. Zhu. Overview of the INEX 2008 entity ranking track. In Proceedings of INEX, pages 243­252, 2008.",null,null
130,"[9] G. Demartini, T. Iofciu, and A. P. de Vries. Overview of the INEX 2009 entity ranking track. In Proceedings of INEX, pages 254­264, 2009.",null,null
131,"[10] A. El-Hamdouchi and P. Willett. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science, 13:361­365, 1987.",null,null
132,"[11] E. Gabrilovich and S. Markovitch. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In Proceedings of IJCAI, pages 1606­1611, 2007.",null,null
133,"[12] M. A. Hearst and J. O. Pedersen. Reexamining the cluster hypothesis: Scatter/Gather on retrieval results. In Proceedings of SIGIR, pages 76­84, 1996.",null,null
134,"[13] N. Jardine and C. J. van Rijsbergen. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7(5):217­240, 1971.",null,null
135,"[14] R. Kaptein and J. Kamps. Exploiting the category structure of wikipedia for entity ranking. Artificial Intelligence, 0:111 ­ 129, 2013.",null,null
136,"[15] O. Kurland and C. Domshlak. A rank-aggregation approach to searching for optimal query-specific clusters. In Proceedings of SIGIR, pages 547­554, 2008.",null,null
137,"[16] O. Kurland and L. Lee. Corpus structure, language models, and ad hoc information retrieval. In Proceedings of SIGIR, pages 194­201, 2004.",null,null
138,"[17] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proceedings of ECIR, pages 454­462, 2008.",null,null
139,"[18] H. Raviv, D. Carmel, and O. Kurland. A ranking framework for entity oriented search using markov random fields. In Proceedings of the 1st Joint International Workshop on Entity-Oriented and Semantic Search, 2012.",null,null
140,"[19] M. D. Smucker and J. Allan. A new measure of the cluster hypothesis. In Proceedings of ICTIR, pages 281­288, 2009.",null,null
141,"[20] J. A. Thom, J. Pehcevski, and A.-M. Vercoustre. Use of wikipedia categories in entity ranking. CoRR, abs/0711.2917, 2007.",null,null
142,"[21] A. Tombros, R. Villa, and C. Van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Information Processing & management, 38(4):559­582, 2002.",null,null
143,"[22] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.",null,null
144,"[23] E. M. Voorhees. The cluster hypothesis revisited. In Proceedings of SIGIR, pages 188­196, 1985.",null,null
145,Initial,null,null
146,Oracle MeanScore RegMeanScore,null,null
147,Oracle MeanScore RegMeanScore,null,null
148,Oracle MeanScore RegMeanScore,null,null
149,Oracle MeanScore RegMeanScore,null,null
150,Initial,null,null
151,Oracle MeanScore RegMeanScore,null,null
152,Oracle MeanScore RegMeanScore,null,null
153,Oracle MeanScore RegMeanScore,null,null
154,Oracle MeanScore RegMeanScore,null,null
155,Initial,null,null
156,Oracle MeanScore RegMeanScore,null,null
157,Oracle MeanScore RegMeanScore,null,null
158,Oracle MeanScore RegMeanScore,null,null
159,Oracle MeanScore RegMeanScore,null,null
160,2007,null,null
161,LDoc,null,null
162,2008,null,null
163,MAP p@5 infAP p@5,null,null
164,20.2 26.1 12.6 19.4,null,null
165,Tree,null,null
166,31.8i 51.3i 22.3i 49.7i,null,null
167,21.6 26.1 13.3 17.1,null,null
168,21.6 26.0 13.4 18.9,null,null
169,SharedCat,null,null
170,36.2i 60.0i 26.8i 66.3i,null,null
171,22.5 23.9 12.6 18.9,null,null
172,23.1i 27.8i 13.4i 20.6i,null,null
173,CE,null,null
174,32.3i 53.5i 23.3i 53.7i,null,null
175,22.6 27.4 13.5 20.6,null,null
176,22.0 26.5 13.5 20.6,null,null
177,33.7i,null,null
178,ESA 57.0i 26.0i,null,null
179,60.6i,null,null
180,20.9 22.6 12.8 14.9,null,null
181,21.9 23.9 12.9 14.9,null,null
182,2009 infAP p@5 19.1 35.3,null,null
183,25.5i 20.2i 20.2i,null,null
184,68.0i 36.7 36.7,null,null
185,30.5i 19.7 19.9,null,null
186,83.3i 37.1,null,null
187,38.5,null,null
188,28.0i 19.1 19.1,null,null
189,74.2i 36.7 36.7,null,null
190,28.8i 19.2 19.2,null,null
191,80.0i 35.6 35.3,null,null
192,LDoc;Type,null,null
193,2007,null,null
194,2008,null,null
195,MAP p@5 infAP p@5,null,null
196,30.8 37.4 28.2 44.0,null,null
197,Tree,null,null
198,37.7i 58.7i 32.5i 50.9i,null,null
199,31.6 40.0 27.7 37.7,null,null
200,31.6 40.0 28.1 40.6,null,null
201,SharedCat,null,null
202,43.8i 65.7i 38.3i 65.1i,null,null
203,30.8 36.1 28.7 42.3,null,null
204,31.1 37.0 28.9 42.3,null,null
205,CE,null,null
206,39.0i 60.0i 34.3i 58.3i,null,null
207,31.3 38.3 28.5 40.6,null,null
208,31.0 37.4 28.7 40.0,null,null
209,ESA,null,null
210,42.1i 64.3i 37.7i 66.9i,null,null
211,28.6 34.3 28.4 42.3,null,null
212,29.1 34.8 28.9 44.0,null,null
213,2009 infAP p@5 23.8 43.6,null,null
214,29.5i 23.6 23.4,null,null
215,70.2i 39.6 39.3,null,null
216,34.1i 23.2 23.6,null,null
217,87.6i 44.0 45.1,null,null
218,31.3i 23.7 23.7,null,null
219,75.6i 42.2 42.2,null,null
220,32.9i 22.8 22.7,null,null
221,83.6i 42.5 41.5,null,null
222,LDoc;Type;Name,null,null
223,2007,null,null
224,2008,null,null
225,MAP p@5 infAP p@5,null,null
226,33.3 40.4 35.4 46.9,null,null
227,Tree,null,null
228,39.6i 57.4i 42.3i 62.3i,null,null
229,34.1 43.5 35.7 42.3,null,null
230,34.0 42.6 35.7 43.4,null,null
231,SharedCat,null,null
232,47.4i 70.4i 47.8i 74.9i,null,null
233,32.9 38.7 33.5 42.9,null,null
234,33.1 39.1 34.8 44.0,null,null
235,CE,null,null
236,40.7i 59.1i 43.1i 63.4i,null,null
237,33.6 38.7 34.5 45.1,null,null
238,33.6 38.7 34.5 45.1,null,null
239,ESA,null,null
240,44.7i 66.5i 45.7i 70.3i,null,null
241,33.9 41.3 33.7 43.4,null,null
242,34.0 42.2 34.2 44.6,null,null
243,2009 infAP p@5 24.4 44.0,null,null
244,30.3i 24.7 24.6,null,null
245,72.0i 42.9 42.2,null,null
246,34.3i 24.6 24.9,null,null
247,87.i 3 41.5 42.9,null,null
248,31.6i 24.6 24.8,null,null
249,76.4i 43.3 43.6,null,null
250,32.9i 23.0 23.3,null,null
251,81.5i 35.3 35.6,null,null
252,Table 3: Retrieval performance. The best result in a column (excluding that of Oracle) per an initial list is boldfaced. 'i' marks statistically significant differences with Initial.,null,null
253,844,null,null
254,,null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Word Embedding Causes Topic Shi ing; Exploit Global Context!,null,null",null,null
4,"3,""Navid Rekabsaz, Mihai Lupu, Allan Hanbury"",null,null",null,null
5,"4,Information & So ware Engineering Group TU WIEN,null,null",null,null
6,"5,rekabsaz/lupu/hanbury@ifs.tuwien.ac.at,null,null",null,null
7,"6,ABSTRACT,null,null",null,null
8,"7,""Exploitation of term relatedness provided by word embedding has gained considerable a ention in recent IR literature. However, an emerging question is whether this sort of relatedness ts to the needs of IR with respect to retrieval e ectiveness. While we observe a high potential of word embedding as a resource for related terms, the incidence of several cases of topic shi ing deteriorates the nal performance of the applied retrieval models. To address this issue, we revisit the use of global context (i.e. the term co-occurrence in documents) to measure the term relatedness. We hypothesize that in order to avoid topic shi ing among the terms with high word embedding similarity, they should o en share similar global contexts as well. We therefore study the e ectiveness of post ltering of related terms by various global context relatedness measures. Experimental results show signi cant improvements in two out of three test collections, and support our initial hypothesis regarding the importance of considering global context in retrieval."",null,null",null,null
9,"8,KEYWORDS,null,null",null,null
10,"9,Word embedding; term relatedness; global context; word2vec; LSI,null,null",null,null
11,"10,""ACM Reference format: Navid Rekabsaz, Mihai Lupu, Allan Hanbury and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080733"",null,null",null,null
12,"11,1 INTRODUCTION,null,null",null,null
13,"12,""e e ective choice of related terms to enrich queries has been explored for decades in information retrieval literature and approached using a variety of data resources. Early studies explore the use of collection statistics. ey identify the global context of two terms either by directly measuring term co-occurrence in a context (i.e. document) [9] or a er applying matrix factorization [3]. Later studies show the higher e ectiveness of local approaches (i.e. using pseudo-relevant documents) [15]. More recently, the approaches to exploit the advancement in word embedding for IR"",null,null",null,null
14,"13,""Funded by: Self-Optimizer (FFG 852624) in the EUROSTARS programme, funded by EUREKA, BMWFW and European Union, and ADMIRE (P 25905-N23) by FWF. anks to Joni Sayeler and Linus Wretblad for their contributions in SelfOptimizer. was supported in part by the Center for Intelligent Information Retrieval."",null,null",null,null
15,"14,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080733"",null,null",null,null
16,"15,Hamed Zamani,null,null",null,null
17,"16,Center for Intelligent Information Retrieval University of Massachuse s Amherst zamani@cs.umass.edu,null,null",null,null
18,"17,""has shown not only to be competitive to the local approaches but also that combining the approaches brings further improvements in comparison to each of them alone [12, 16, 17]."",null,null",null,null
19,"18,""Word embedding methods provide vector representations of terms by capturing the co-occurrence relations between the terms, based on an approximation on the likelihood of their appearances in similar window-contexts. Word embedding is used in various IR tasks e.g. document retrieval [11, 12, 18], neural network-based retrieval models [4, 6, 8], and query expansion [16]."",null,null",null,null
20,"19,""In all of these studies, the concept of """"term similarity"""" is dened as the geometric proximity between their vector representations. However, since this closeness is still a mathematical approximation of meaning, some related terms might not t to the retrieval needs and eventually deteriorate the results. For instance, antonyms (cheap and expensive) or co-hyponyms (schizophrenia and alzheimer, mathematics and physics, countries, months) share common window-context and are therefore considered as related in the word embedding space, but can potentially bias the query to other topics."",null,null",null,null
21,"20,""Some recent studies aim to be er adapt word embedding methods to the needs of IR. Diaz et al. [5] suggest training separate word embedding models on the top retrieved documents per query, while Rekabsaz et al. [13] explore the similarity space and suggest a general threshold to lter the most e ective related terms. While the mentioned studies rely on the context around the terms, in this work we focus on the e ect of similarity achieved from global context as a complementary to the window-context based similarity."",null,null",null,null
22,"21,""In fact, similar to the earlier studies [9, 14], we assume each document to be a coherent information unit and consider the cooccurrence of terms in documents as a means of measuring their topical relatedness. Based on this assumption, we hypothesize that to mitigate the problem of topic shi ing, the terms with high word embedding similarities also need to share similar global contexts. In other words, if two terms appear in many similar window-contexts, but they share li le global contexts (documents), they probably re ect di erent topics and should be removed from the related terms."",null,null",null,null
23,"22,""To examine this hypothesis, we analyze the e ectiveness of each related term, when added to the query. Our approach is similar to that of Cao et al. [2] on pseudo-relevance feedback. Our analysis shows that the set of related terms from word embedding has a high potential to improve state-of-the-art retrieval models. Based on this motivating observation, we explore the e ectiveness of using word embedding's similar term when ltered by global context similarity on two state-of-the-art IR models. Our evaluation on three test collections shows the importance of using global context, as combining both the similarities signi cantly improves the results."",null,null",null,null
24,"23,1105,null,null",null,null
25,"24,Short Research Paper,null,null",null,null
26,"25,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
27,"26,2 BACKGROUND,null,null",null,null
28,"27,""To use word embedding in document retrieval, recent studies extend the idea of translation models in IR [1] using word embedding similarities. Zuccon et al. [18] use the similarities in the language modeling framework [10] and Rekabsaz et al. [12] extend the concept of translation models to probabilistic relevance framework. In the following, we brie y explain the translation models when combined with word embedding similarity."",null,null",null,null
29,"28,""In principle, a translation model introduces in the estimation of the relevance of the query term t a translation probability PT , de ned on the set of (extended) terms R(t), always used in its conditional form PT (t |t ) and interpreted as the probability of observing term t, having observed term t . Zuccon et al. [18] integrate word embedding with the translation language modeling by using the set of extended terms from word embedding:"",null,null",null,null
30,"29,""LM(q, d) , P(q|Md ) ,"",null,null",null,null
31,"30,PT (t |t )P(t |Md ) (1),null,null",null,null
32,"31,t q t R(t ),null,null",null,null
33,"32,Rekabsaz et al. [12] extend the idea into four probabilistic relevance,null,null",null,null
34,"33,frameworks. eir approach revisits the idea of computing docu-,null,null",null,null
35,"34,""ment relevance based on the occurrence of concepts. Traditionally,"",null,null",null,null
36,"35,""concepts are represented by the words appear in the text, quanti ed"",null,null",null,null
37,"36,by term frequency (t f ). Rekabsaz et al. posit that we can have a,null,null",null,null
38,"37,""t f value lower than 1 when the term itself is not actually appear,"",null,null",null,null
39,"38,""but another, conceptually similar term occurs in the text. Based on"",null,null",null,null
40,"39,""it, they de ne the extended t f of a query word t in a document as"",null,null",null,null
41,"40,follows:,null,null",null,null
42,"41,""t fd , t fd +"",null,null",null,null
43,"42,PT (t |t )t fd (t ),null,null",null,null
44,"43,(2),null,null",null,null
45,"44,t R(t ),null,null",null,null
46,"45,""However, in the probabilistic models, a series of other factors are"",null,null",null,null
47,"46,computed based on t f (e.g. document length). ey therefore,null,null",null,null
48,"47,propagate the above changes to all the other statistics and refer to,null,null",null,null
49,"48,the nal scoring formulas as Extended Translation model. Among,null,null",null,null
50,"49,""the extended models, as BM25 is a widely used and established"",null,null",null,null
51,"50,""model in IR, we use the extended BM25 translation model (BM25)"",null,null",null,null
52,"51,""in our experiments. Similar to the original papers in both models,"",null,null",null,null
53,"52,the estimation of PT is based on the Cosine similarity between two embedding vectors.,null,null",null,null
54,"53,3 EXPERIMENT SETUP,null,null",null,null
55,"54,""We conduct our experiments on three test collections, shown in Table 1. For word embedding vectors, we train the word2vec skipgram model [7] with 300 dimensions and the tool's default parameters on the Wikipedia dump le for August 2015. We use the Porter stemmer for the Wikipedia corpus as well as retrieval. As suggested by Rekabsaz et al. [13], the extended terms set R(t) is selected from the terms with similarity values of greater than a speci c threshold. Previous studies suggest the threshold value of around 0.7 as an optimum for retrieval [12, 13]. To explore the e ectiveness of less similar terms, we try the threshold values of {0.60, 0.65..., 0.80}."",null,null",null,null
56,"55,""Since the parameter µ for Dirichlet prior of the translation language model and also b, k1, and k3 for BM25 are shared between the methods, the choice of these parameters is not explored as part of this study and we use the same set of values as in Rekabsaz et al. [12]. e statistical signi cance tests are done using the two sided paired t-test and signi cance is reported for p < 0.05. e evaluation of retrieval e ectiveness is done with respect to Mean Average Precision (MAP) as a standard measure in ad-hoc IR."",null,null",null,null
57,"56,Table 1: Test collections used in this paper,null,null",null,null
58,"57,Name TREC Adhoc 1&2&3 TREC Adhoc 6&7&8 Robust 2005,null,null",null,null
59,"58,Collection Disc1&2 Disc4&5 AQUAINT,null,null",null,null
60,"59,# eries 150 150 50,null,null",null,null
61,"60,# Documents 740449 556028 1033461,null,null",null,null
62,"61,""Table 2: e percentage of the good, bad and neutral terms."",null,null",null,null
63,"62,#Rel averages the number of related terms per query term.,null,null",null,null
64,"63,Collection,null,null",null,null
65,"64,reshold 0.60 #Rel Good Neutral Bad,null,null",null,null
66,"65,TREC 123,null,null",null,null
67,"66,8.2 7%,null,null",null,null
68,"67,84%,null,null",null,null
69,"68,9%,null,null",null,null
70,"69,TREC 678,null,null",null,null
71,"70,8.8 9%,null,null",null,null
72,"71,78% 14%,null,null",null,null
73,"72,Robust 2005 10.3 8%,null,null",null,null
74,"73,77% 15%,null,null",null,null
75,"74,ALL,null,null",null,null
76,"75,8.1 8%,null,null",null,null
77,"76,81% 11%,null,null",null,null
78,"77,reshold 0.80,null,null",null,null
79,"78,#Rel Good Neutral Bad,null,null",null,null
80,"79,1.3 19%,null,null",null,null
81,"80,68% 13%,null,null",null,null
82,"81,1.2 34%,null,null",null,null
83,"82,48% 18%,null,null",null,null
84,"83,1.1 39%,null,null",null,null
85,"84,44% 17%,null,null",null,null
86,"85,1.2 27%,null,null",null,null
87,"86,58% 15%,null,null",null,null
88,"87,4 PRELIMINARY ANALYSIS,null,null",null,null
89,"88,""We start with an observation on the e ectiveness of each individual related term. To measure it, we use the LM model as it has shown slightly be er results than the BM25 model [12]. Similar to Cao et al. [2], given each query, for all its corresponding related terms, we repeat the evaluation of the IR models where each time R(t) consists of only one of the related terms. For each term, we calculate the di erences between its Average Precision (AP) evaluation result and the result of the original query and refer to this value as the retrieval gain or retrieval loss of the related term."",null,null",null,null
90,"89,""Similar to Cao et al. [2], we de ne good/bad groups as the terms with retrieval gain/loss of more than 0.005, and assume the rest with smaller gain or loss values than 0.005 as neutral terms. Table 2 summarizes the percentage of each group. Due to the lack of space, we only show the statistics for the lowest (0.6) and highest (0.8) threshold. e average number of related terms per query term is shown in the #Rel eld. As expected, the percentage of the good terms is higher for the larger threshold, however--similar to the observation on pseudo-relevance feedback [2]--most of the expanded terms (58% to 81%) have no signi cant e ect on performance."",null,null",null,null
91,"90,Let us imagine that we had a priori knowledge about the e ectiveness of each related term and were able to lter terms with negative e ect on retrieval. We call this approach Oracle post-,null,null",null,null
92,"91,""ltering as it shows us the maximum performance of each retrieval model. Based on the achieved results, we provide an approximation of this approach by ltering the terms with retrieval loss."",null,null",null,null
93,"92,Figures 1a and 1b show the percentage of relative MAP improve-,null,null",null,null
94,"93,""ment of the LM and BM25 models with and without post- ltering with respect to the original LM and BM25 models. In the plot, ignore the Gen and Col results as we return to them in Section 6. e results are aggregated over the three collections. In each threshold the statistical signi cance of the improvement with respect to two baselines are computed: (1) against the basic models (BM25 and LM), shown with the b sign and (2) against the translation models without post ltering, shown with the  sign."",null,null",null,null
95,"94,""As reported by Rekabsaz et al. [13], for the thresholds less than 0.7 the retrieval performance of the translation models (without post ltering) decreases as the added terms introduce more noise. However, the models with the Oracle post ltering continue to improve the baselines further for the lower thresholds with high margin. ese demonstrate the high potential of using related terms from word embedding but also show the need to customize the set of terms for IR. We propose an approach to this customization using the global-context of the terms in the following."",null,null",null,null
96,"95,1106,null,null",null,null
97,"96,Short Research Paper,null,null",null,null
98,"97,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
99,"98,% Improvement (MAP) % Improvement (MAP) Word2Vec Similarity,null,null",null,null
100,"99,20 ½b,null,null",null,null
101,"100,15,null,null",null,null
102,"101,10,null,null",null,null
103,"102,5½ ½,null,null",null,null
104,"103,0,null,null",null,null
105,"104,-5,null,null",null,null
106,"105,-10 0.6,null,null",null,null
107,"106,½b,null,null",null,null
108,"107,½b,null,null",null,null
109,"108,½b,null,null",null,null
110,"109,b½b,null,null",null,null
111,"110,LM,null,null",null,null
112,"111,Ld M Gen,null,null",null,null
113,"112,Ld M,null,null",null,null
114,"113,Ld M Col,null,null",null,null
115,"114,0.65,null,null",null,null
116,"115,0.7,null,null",null,null
117,"116,Threshold,null,null",null,null
118,"117,Ld M Oracle 0.75,null,null",null,null
119,"118,½b,null,null",null,null
120,"119,20,null,null",null,null
121,"120,½b,null,null",null,null
122,"121,15,null,null",null,null
123,"122,½b,null,null",null,null
124,"123,10,null,null",null,null
125,"124,½ 5½,null,null",null,null
126,"125,½b,null,null",null,null
127,"126,½b,null,null",null,null
128,"127,0,null,null",null,null
129,"128,-5,null,null",null,null
130,"129,-10 0.6,null,null",null,null
131,"130,BM25,null,null",null,null
132,"131,d BM25,null,null",null,null
133,"132,d BM25 Gen d BM25 Col,null,null",null,null
134,"133,0.65,null,null",null,null
135,"134,0.7,null,null",null,null
136,"135,Threshold,null,null",null,null
137,"136,d BM25 Oracle,null,null",null,null
138,"137,0.75,null,null",null,null
139,"138,0.90,null,null",null,null
140,"139,0.85,null,null",null,null
141,"140,0.80,null,null",null,null
142,"141,0.75,null,null",null,null
143,"142,0.70,null,null",null,null
144,"143,0.65,null,null",null,null
145,"144,0.60 0.5 0.6 0.7 0.8 0.9 1.0 LSI Similarity,null,null",null,null
146,"145,(a) LM,null,null",null,null
147,"146,(b) BM25,null,null",null,null
148,"147,(c) Retrieval Gain/Loss,null,null",null,null
149,"148,""Figure 1: (a,b) e percentage of relative MAP improvement to the basic models, aggregated on all the collections. e b and  signs show the signi cance of the improvement to the basic models and the extended models without post ltering respectively (c) Retrieval gain or loss of the related terms for all the collection. e red (light) color indicate retrieval loss and the green (dark) retrieval gain."",null,null",null,null
150,"149,5 GLOBAL-CONTEXT POST FILTERING,null,null",null,null
151,"150,""Looking at some samples of retrieval loss, we can observe many cases of topic shi ing: e.g. Latvia as query term is expanded with Estonia, Ammoniac with Hydrogen, Boeing with Airbus, and Alzheimer with Parkinson. As mentioned before, our hypothesis is that for the terms with high window-context similarity (i.e. word2vec similarity) when they have high global context similarity (i.e. co-occurrence in common documents), they more probably refer to a similar topic (e.g. USSR and Soviet) and with low global context similarity to di erent topics (e.g. Argentina and Nicaragua)."",null,null",null,null
152,"151,""To capture the global context similarities, some older studies use measures like Dice, Tanimoto, and PMI [9]. Cosine similarity has been used as well, considering each term a vector with dimensionality of the number of documents in the collection, with weights given either as simple incidence (i.e. 0/1), or by some variant of TFIDF. Cosine can also be used a er rst applying Singular Value Decomposition on the TFIDF weighted term-document matrix, resulting in the well known Latent Semantic Indexing (LSI) method [3] (300 dimensions in our experiments). To compute these measures, we consider both the collection statistics and Wikipedia statistics, resulting in 12 sets of similarities (Dice, Tanimoto, PMI, Incidence Vectors, TFIDF Vectors, LSI Vectors)×(collection, Wikipedia). We refer to these similarity value lists as global context features."",null,null",null,null
153,"152,""Let rst observe the relationship between LSI and word2vec similarities of the terms. Figure 1c plots the retrieval gain/loss of the terms of all the collections based on their word2vec similarities as well as LSI (when using test collection statistics). e size of the circles shows their gain/loss as the red color (the lighter one) show retrieval loss and green (the darker one) retrieval gain. For clarity, we only show the terms with the retrieval gain/loss of more than 0.01. e area with high word2vec and LSI similarity (top-right) contains most of the terms with retrieval gain. On the other hand, regardless of the word2vec similarity, the area with lower LSI tend to contain relatively more cases of retrieval loss. is observation encourages the exploration of a set of thresholds for global context features to post lter the terms retrieved by embedding."",null,null",null,null
154,"153,""To nd the thresholds for global context features, we explore the highest amount of total retrieval gain a er ltering the related terms with similarities higher than the thresholds. We formulate it"",null,null",null,null
155,"154,by the following optimization problem:,null,null",null,null
156,"155,N,null,null",null,null
157,"156,F,null,null",null,null
158,"157,argmin 1 xj > j i,null,null",null,null
159,"158,(3),null,null",null,null
160,"159,"" i,1 j,1"",null,null",null,null
161,"160,""where 1 is the indicator function, N and F are the number of terms"",null,null",null,null
162,"161,""and features respectively,  indicates the set of thresholds j , xj the value of the features, and nally refers to the retrieval gain/loss."",null,null",null,null
163,"162,We consider two approaches to selecting the datasets used to,null,null",null,null
164,"163,""nd the optimum thresholds: per collection, and general. In the"",null,null",null,null
165,"164,""per collection scenario (Col), for each collection we nd di erent"",null,null",null,null
166,"165,thresholds for the features. We apply 5-fold cross validation by rst,null,null",null,null
167,"166,using the terms of the training topics to nd the thresholds (solving,null,null",null,null
168,"167,Eq. 3) and then applying the thresholds to post lter the terms of,null,null",null,null
169,"168,""the test topics. To avoid over ing, we use the bagging method by"",null,null",null,null
170,"169,40 times bootstrap sampling (random sampling with replacement),null,null",null,null
171,"170,and aggregate the achieved thresholds.,null,null",null,null
172,"171,""In the general approach (Gen), we are interested in nding a"",null,null",null,null
173,"172,""`global' threshold for each feature, which is fairly independent of"",null,null",null,null
174,"173,the collections. As in this approach the thresholds are not speci c,null,null",null,null
175,"174,""for each individual collection, we use all the topics of all the test"",null,null",null,null
176,"175,collections to solve the optimization problem.,null,null",null,null
177,"176,6 RESULTS AND DISCUSSION,null,null",null,null
178,"177,""To nd the most e ective set of features, we test all combinations of features using the per collection (Col) post- ltering approach. Given the post- ltered terms with each feature set, we evaluate the"",null,null",null,null
179,"178,""LM and BM25 models. Our results show the superior e ectiveness of the LSI feature when using the test collections as resource in comparison with the other features as well as the ones based on Wikipedia. e results with the LSI feature can be further improved by combining it with the TFIDF feature. However, adding any of the other features does not bring any improvement and therefore, in the following, we only use the combination of LSI and TFIDF features with both using the test collections statistics."",null,null",null,null
180,"179,e evaluation results of the original LM and LM with post ltering with the general (Gen) and per collection (Col) approaches are,null,null",null,null
181,"180,""shown in Figure 2. e general behavior of BM25 is very similar and therefore no longer shown here. As before, statistical significance against the basic models is indicated by b and against the translation models without post ltering, by ."",null,null",null,null
182,"181,1107,null,null",null,null
183,"182,Short Research Paper,null,null",null,null
184,"183,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
185,"184,0.33 b½,null,null",null,null
186,"185,0.32,null,null",null,null
187,"186,b½,null,null",null,null
188,"187,0.31,null,null",null,null
189,"188,0.30,null,null",null,null
190,"189,b½,null,null",null,null
191,"190,b,null,null",null,null
192,"191,b½,null,null",null,null
193,"192,0.29 ½½,null,null",null,null
194,"193,0.29 b½,null,null",null,null
195,"194,0.28 0.27,null,null",null,null
196,"195,b½,null,null",null,null
197,"196,b½,null,null",null,null
198,"197,0.26,null,null",null,null
199,"198,½ 0.25 ½,null,null",null,null
200,"199,0.24,null,null",null,null
201,"200,0.23 b½,null,null",null,null
202,"201,0.22,null,null",null,null
203,"202,b½,null,null",null,null
204,"203,b½,null,null",null,null
205,"204,0.21,null,null",null,null
206,"205,½,null,null",null,null
207,"206,½,null,null",null,null
208,"207,½,null,null",null,null
209,"208,0.20,null,null",null,null
210,"209,MAP MAP MAP,null,null",null,null
211,"210,0.28,null,null",null,null
212,"211,0.24,null,null",null,null
213,"212,0.19,null,null",null,null
214,"213,0.27,null,null",null,null
215,"214,0.23,null,null",null,null
216,"215,0.18,null,null",null,null
217,"216,0.26,null,null",null,null
218,"217,0.22,null,null",null,null
219,"218,0.17,null,null",null,null
220,"219,0.6,null,null",null,null
221,"220,0.65,null,null",null,null
222,"221,0.7,null,null",null,null
223,"222,0.75,null,null",null,null
224,"223,0.6,null,null",null,null
225,"224,0.65,null,null",null,null
226,"225,0.7,null,null",null,null
227,"226,0.75,null,null",null,null
228,"227,0.6,null,null",null,null
229,"228,0.65,null,null",null,null
230,"229,0.7,null,null",null,null
231,"230,0.75,null,null",null,null
232,"231,TREC 123,null,null",null,null
233,"232,TREC 678,null,null",null,null
234,"233,Robust 2005,null,null",null,null
235,"234,LM,null,null",null,null
236,"235,Ld M,null,null",null,null
237,"236,Ld M Gen,null,null",null,null
238,"237,Ld M Col,null,null",null,null
239,"238,Ld M Oracle,null,null",null,null
240,"239,Figure 2: Evaluation results of LM with/without post ltering. LM and LM without post ltering respectively.,null,null",null,null
241,"240,""e results show the improvement of the LM models with postltering in comparison with the original LM. e models with postltering approaches speci cally improve in lower word embedding thresholds, however similar to the original translation models, the best performance is achieved on word embedding threshold of 0.7."",null,null",null,null
242,"241,e results for both LM and BM25 models with word embedding threshold of 0.7 are summarized in Table 3. Comparing the post-,null,null",null,null
243,"242,""ltering approaches, Col shows be er performance than Gen as with the optimum word embedding threshold, it achieves signi cant improvements over both the baselines in two of the collections."",null,null",null,null
244,"243,""Let us look back to the percentage of relative improvements, aggregated over the collections in Figures 1a and 1b. In both IR models, while the Col approach has be er results than Gen, their results are very similar to the optimum word embedding threshold (0.7). is result suggests to use the Gen approach as a more straightforward and general approach for post ltering. In our experiments, the optimum threshold value for the LSI similarities (as the main feature) is around 0.62 (vertical line in Figure 1c)."",null,null",null,null
245,"244,""As a nal point, comparing the two IR models shows that despite the generally be er performance of the LM models, the BM25 models gain more. We speculate that it is due to the additional modi cation of other statistics (i.e. document length and IDF) in the BM25 model and therefore it is more sensitive to the quality of the related terms. However an in-depth comparison between the models is le for future work."",null,null",null,null
246,"245,7 CONCLUSION,null,null",null,null
247,"246,""Word embedding methods use (small) window-context of the terms to provide dense vector representations, used to approximate term relatedness. In this paper, we study the e ectiveness of related terms, identi ed by both window-based and global contexts, in document retrieval. We use two state-of-the-art translation models to integrate word embedding information for retrieval. Our analysis shows a great potential to improve retrieval performance, damaged however by topic shi ing. To address it, we propose the use of global context similarity, i.e. the co-occurrence of terms in larger contexts such as entire documents. Among various methods to measure global context, we identify LSI and TFIDF as the most e ective in eliminating related terms that lead to topic shi ing. Evaluating the IR models using the post- ltered set shows a signi cant improvement in comparison with the basic models as well as the translation models with no post- ltering. e results"",null,null",null,null
248,"247,e b and  signs show the signi cance of the improvement to,null,null",null,null
249,"248,Table 3: MAP of the translation models when terms ltered,null,null",null,null
250,"249,with word embedding threshold of 0.7 and post ltered with,null,null",null,null
251,"250,the Gen and Col approach.,null,null",null,null
252,"251,Collection Model Basic Tran. Tran.+Gen Tran.+Col,null,null",null,null
253,"252,TREC 123,null,null",null,null
254,"253,LM 0.275 0.283 BM25 0.273 0.285,null,null",null,null
255,"254,0.290 0.288,null,null",null,null
256,"255,0.295 b 0.290 b,null,null",null,null
257,"256,TREC 678,null,null",null,null
258,"257,LM 0.252 0.259 BM25 0.243 0.255,null,null",null,null
259,"258,0.262 0.257,null,null",null,null
260,"259,0.261 0.256 b,null,null",null,null
261,"260,Robust 2005,null,null",null,null
262,"261,LM BM25,null,null",null,null
263,"262,0.183 0.181,null,null",null,null
264,"263,0.204 0.203,null,null",null,null
265,"264,0.208 0.207,null,null",null,null
266,"265,0.209 b 0.209 b,null,null",null,null
267,"266,demonstrate the importance of global context as a complementary to the window-context similarities.,null,null",null,null
268,"267,REFERENCES,null,null",null,null
269,"268,[1] Adam Berger and John La erty. 1999. Information Retrieval As Statistical Translation. In Proc. of SIGIR.,null,null",null,null
270,"269,""[2] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting good expansion terms for pseudo-relevance feedback. In Proc. of SIGIR."",null,null",null,null
271,"270,""[3] Sco Deerwester, Susan T Dumais, George W Furnas, omas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science (1990)."",null,null",null,null
272,"271,""[4] Mostafa Dehghani, Hamed Zamani, A. Severyn, J. Kamps, and W Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In Proc. of SIGIR."",null,null",null,null
273,"272,""[5] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery expansion with locally-trained word embeddings. Proc. of ACL (2016)."",null,null",null,null
274,"273,""[6] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A deep relevance matching model for ad-hoc retrieval. In Proc. of CIKM."",null,null",null,null
275,"274,""[7] Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013. E cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013)."",null,null",null,null
276,"275,""[8] Bhaskar Mitra, F. Diaz, and N. Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In Proc. of WWW."",null,null",null,null
277,"276,[9] Helen J Peat and Peter Wille . 1991. e limitations of term co-occurrence data for query expansion in document retrieval systems. Journal of the American society for information science (1991).,null,null",null,null
278,"277,[10] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In Proc. of SIGIR.,null,null",null,null
279,"278,""[11] Navid Rekabsaz, Ralf Bierig, Bogdan Ionescu, Allan Hanbury, and Mihai Lupu. 2015. On the use of statistical semantics for metadata-based social image retrieval. In Proc. of CBMI Conference."",null,null",null,null
280,"279,""[12] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In Proc. of CIKM."",null,null",null,null
281,"280,""[13] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2017. Exploration of a threshold for similarity based on uncertainty in word embedding. In Proc. of ECIR."",null,null",null,null
282,"281,[14] G Salton and MJ MacGill. 1983. Introduction to modern information retrieval. McGraw-Hill (1983).,null,null",null,null
283,"282,[15] Jinxi Xu and W Bruce Cro . 1996. ery expansion using local and global document analysis. In Proc. of SIGIR.,null,null",null,null
284,"283,[16] Hamed Zamani and W Bruce Cro . 2016. Embedding-based query language models. In Proc. of ICTIR.,null,null",null,null
285,"284,[17] Hamed Zamani and W Bruce Cro . 2017. Relevance-based Word Embedding. In Proc. of SIGIR.,null,null",null,null
286,"285,""[18] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Integrating and evaluating neural word embeddings in information retrieval. In Proc. of Australasian Document Computing Symposium."",null,null",null,null
287,"286,1108,null,null",null,null
288,"287,,null,null",null,null

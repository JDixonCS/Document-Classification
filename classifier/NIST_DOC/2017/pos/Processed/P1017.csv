,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,On the Benefit of Incorporating External Features in a Neural Architecture for Answer Sentence Selection,null,null",null,null
4,"3,""Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, W. Bruce Cro "",null,null",null,null
5,"4,""RMIT University, Melbourne, Australia University of Massachuse s, Amherst, MA, USA"",null,null",null,null
6,"5,""{ruey-cheng.chen,evi.yulianti,mark.sanderson}@rmit.edu.au,cro @cs.umass.edu"",null,null",null,null
7,"6,ABSTRACT,null,null",null,null
8,"7,""Incorporating conventional, unsupervised features into a neural architecture has the potential to improve modeling e ectiveness, but this aspect is o en overlooked in the research of deep learning models for information retrieval. We investigate this incorporation in the context of answer sentence selection, and show that combining a set of query matching, readability, and query focus features into a simple convolutional neural network can lead to markedly increased e ectiveness. Our results on two standard question-answering datasets show the e ectiveness of the combined model."",null,null",null,null
9,"8,CCS CONCEPTS,null,null",null,null
10,"9,·Information systems  Information retrieval; estion answering; ·Computing methodologies  Neural networks;,null,null",null,null
11,"10,KEYWORDS,null,null",null,null
12,"11,""Answer sentence selection, external features, convolutional neural networks"",null,null",null,null
13,"12,""ACM Reference format: Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, W. Bruce Cro . 2017. On the Bene t of Incorporating External Features in a Neural Architecture for Answer Sentence Selection. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080705"",null,null",null,null
14,"13,1 INTRODUCTION,null,null",null,null
15,"14,""Deep learning approaches have recently become a central methodology in the research of question answering (QA). Many recent a empts in this area have focused on utilizing neural architectures, such as convolutional neural networks (CNN) [8, 19], long shortterm memory networks [12], or a ention mechanisms [15, 18], to explicitly model high-level question-answer structures. ese advances outperform conventional approaches, which are based on engineered heuristics. However, whether deep learning will completely remove the need for such features remains an open question."",null,null",null,null
16,"15,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080705"",null,null",null,null
17,"16,""e lack of understanding as to whether the features are needed in a neural architecture is what we address in this paper. While the development of new models for question answering has been moving rapidly ahead in the past few years, a wealth of proven useful results from prior art [2, 9, 10, 16] were usually ignored. Recently, there has been some evidence pointing a value in using features [11, 19] in neural network models. Commonly used neural network substructures, such as multilayer perceptrons [3], have the capability to combine a large number of external features, so incorporating all available signals in a neural network could improve e ectiveness and provide robust measurement of any e ect [1]."",null,null",null,null
18,"17,""In this paper, we expand on past work using an extensive set of experiments. We demonstrate that, by incorporating a list of 21 common text features into a state-of-the-art CNN model, one can achieve an e ectiveness comparable to the currently best reported results on the TREC QA dataset and the WikiQA data."",null,null",null,null
19,"18,2 EXTERNAL FEATURES,null,null",null,null
20,"19,""Our hypothesis is that one can assist relevance modeling with a set of external features to capture aspects of the data that are different to those captured in a neural network model. e chosen set of features should also cover basic signals that can be easily reproduced and implemented. In our experiments, we se le on a set of simple features known to be useful for question answering. We focus on retrieval and readability features. ere are two motivations for this approach: 1) such features are be er understood by information retrieval practitioners, and 2) they are relatively cheap to implement. us, we precluded the use of some sophisticated NLP features in our experiments, such as convolutional tree kernel [11] or syntactic similarity [10]."",null,null",null,null
21,"20,""e full list of features is given in Table 1, they are divided into three categories."",null,null",null,null
22,"21,""Lexical and semantic matching. e rst group of features address non-factoid question answering, described Yang et al. [16], were rst selected, which cover topical relevance and semantic relatedness measures. e reference package released by Yang et al. was used in our implementation. For computing the language model and BM25 scores, we empirically set both µ in the language model and the average document length in BM25 to a xed value of ten."",null,null",null,null
23,"22,""Readability. e second group of features focus on text readability [5], which is a set of seven common surface readability features, such as number of words/syllables per sentence or the complexword ratio, plus one feature representing the notable Dale-Chall readability formula. We did not include other readability indices as"",null,null",null,null
24,"23,1017,null,null",null,null
25,"24,Short Research Paper,null,null",null,null
26,"25,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
27,"26,Table 1: List of unsupervised features used in this study,null,null",null,null
28,"27,Lexical and semantic matching,null,null",null,null
29,"28,Length,null,null",null,null
30,"29,Number of terms in the sentence,null,null",null,null
31,"30,ExactMatch Whether query is a substring,null,null",null,null
32,"31,Overlap,null,null",null,null
33,"32,Fraction of query terms covered,null,null",null,null
34,"33,OverlapSyn Fraction of query synonyms covered,null,null",null,null
35,"34,LM,null,null",null,null
36,"35,Language model score,null,null",null,null
37,"36,BM25,null,null",null,null
38,"37,BM25 score,null,null",null,null
39,"38,ESA,null,null",null,null
40,"39,Cosine similarity with the query ESA vector,null,null",null,null
41,"40,TAGME,null,null",null,null
42,"41,Overlap between query/sentence entities,null,null",null,null
43,"42,Word2Vec,null,null",null,null
44,"43,Cosine similarity with query word vectors,null,null",null,null
45,"44,Readability CPW SPW WPS CWPS CWR LWPS LWR DaleChall,null,null",null,null
46,"45,Number of characters per word Number of syllables per word Number of words per sentence Number of complex words per sentence Fraction of complex words Number of long words per sentence Fraction of long words (> 7 chars),null,null",null,null
47,"46,e Dale-Chall readability index,null,null",null,null
48,"47,Focus MatchedNGram,null,null",null,null
49,"48,""Maximum semantic relatedness between head question k-gram and any answer n-gram. See (1); 4 variants of (k, n) were used"",null,null",null,null
50,"49,most of these indices can be represented as a linear combination of the surface features.,null,null",null,null
51,"50,""Focus. e third group of features used four parameterized variants of a newly proposed feature MatchedNGram to account for the matching between question head words and the potential answer n-gram. e feature takes the maximum of the semantic similarity between the rst k question words and any n-grams in the answer, using the cosine similarity between question/answer word vectors as the similarity measure. Given k and n, the feature is de ned as follows:"",null,null",null,null
52,"51,""MatchedNGram(Q, A) , max cos"",null,null",null,null
53,"52,l,null,null",null,null
54,"53,""k i ,1"",null,null",null,null
55,"54,qi,null,null",null,null
56,"55,"","",null,null",null,null
57,"56,l +n-1,null,null",null,null
58,"57,""j,l aj"",null,null",null,null
59,"58,.,null,null",null,null
60,"59,(1),null,null",null,null
61,"60,""is simple feature explicitly looks for best matching answer n-grams with respect to question head phrases, such as """"who invented"""", """"how many"""", or """"what year did"""". Word embeddings are leveraged in the computation of the similarity measure. Also, not all combinations of (k, n) are found e ective. In our experiments, we empirically chose four best con gurations of (k, n), which are {(k, n) | k  {2, 3}, n  {2, 3}}, based on their e ectiveness within a learning-to-rank model."",null,null",null,null
62,"61,3 EXPERIMENTS,null,null",null,null
63,"62,""Our evaluation was based on two widely used question answering benchmarks: the TREC QA and the WikiQA datasets. e rst benchmark was originally developed for the task of identifying correct answer factoids in retrieved passages. We used the version prepared by Wang et al. [13], with 1,229 questions in the larger"",null,null",null,null
64,"63,""training set, 82 in the dev set, and 100 in the test set. No further ltering was performed on the data (the """"raw"""" se ing [7]).1 e second benchmark, WikiQA, was created by Yang et al."",null,null",null,null
65,"64,""over the English Wikipedia summary passages and the Bing query logs [17], with crowdsourced annotations. is new benchmark is developed to counter biases introduced in the creation TREC QA: the reliance on using lexical overlap with the question as the sole indication of a candidate answer. Hence, this dataset is by design made more challenging for retrieval based methods. Some major follow-up works [7, 18] used a split that includes questions with all positive labels, a version slightly di erent from the split distributed in the original data. We used the same split as in Rao et al. and used 873 questions in the training set, 126 in the dev set, and 243 in the test set [7]."",null,null",null,null
66,"65,3.1 Neural Network Con guration,null,null",null,null
67,"66,""For the choice of a base system, which would serve as the experimental control, we chose to implement a bi-CNN architecture as proposed in Severyn and Moschi i [8]. is state-of-the-art model is preferred over other candidates, i.e., a ention-based CNN [18], for the ease of implementation and parameter optimization. is architecture is fairly robust and in most cases overly excessive parameter tuning is not required."",null,null",null,null
68,"67,""Convolutional Neural Networks. Our implementation follows closely to the experimental se ing in the original paper. Two sets of word embeddings were used: one with 50 dimensions, developed on top of English Wikipedia and the AQUAINT corpus [8], and the other a 300-dimension pre-trained model released by the word2vec project, using 100-billion words from Google News. e sparse word overlapping indicator features were also used in the convolutional layer [11]. e proposed 21 features were incorporated in the fully-connected layer which also combines pooled representations for the question and the answer sentences. e size of the kernel is set to 100 throughout the experiments. We used hyperbolic tangent tanh as the activation function and max pooling in the pooling layer. e network is trained by using stochastic gradient descent with mini batches. e batch size is set to 50 and AdaDelta update [20] was used with  ,"""" 0.95. Early stopping was deployed tracking the change of dev set e ectiveness, and as a result the training almost always stopped in 5 to 10 epochs. We also experimented with dropout in two experimental runs by sweeping through a small set of dropout rates {0.1, 0.2, . . . , 0.9}."""""",null,null",null,null
69,"68,""e a ention mechanism can also a ect the e ectiveness of the neural network model. To control for this variable, we implemented a simple a ention layer in the base CNN model to approximate the ABCNN-1 model, which is the simplest form of a ention mechanism proposed in Yin et al [18]. In mathematical terms, an a ention layer takes a question-side feature map Fq  Rnq ×d and an answerside feature map Fa  Rna ×d as input. Here, nq and na denote the maximum question/answer sentence length, respectively, and d denotes the dimension of the word embeddings."",null,null",null,null
70,"69,""1Some previous work chose to remove questions that contain no answers and led to two inconsistent data splits """"raw"""" an """"clean"""", so results on one split are not directly comparable to those on the other [7]."",null,null",null,null
71,"70,1018,null,null",null,null
72,"71,Short Research Paper,null,null",null,null
73,"72,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
74,"73,Table 2: E ectiveness results on TREC QA and WikiQA datasets. Best-performing runs in each word-embedding group are un-,null,null",null,null
75,"74,derlined and the overall best result on individual benchmarks printed in boldface. Relative improvements (+/-%) are measured,null,null",null,null
76,"75,""against the group control (base system). Signi cant di erences with respect to bagged LambdaMART and the group control are indicated by /and /, respectively, for p < 0.05/p < 0.01 using the paired t-test."",null,null",null,null
77,"76,TREC QA,null,null",null,null
78,"77,WikiQA,null,null",null,null
79,"78,System,null,null",null,null
80,"79,A n? Drop? MAP,null,null",null,null
81,"80,MRR,null,null",null,null
82,"81,S@1,null,null",null,null
83,"82,MAP,null,null",null,null
84,"83,MRR,null,null",null,null
85,"84,S@1,null,null",null,null
86,"85,Runs (AQUAINT/Wikipedia),null,null",null,null
87,"86,CNN,null,null",null,null
88,"87,×,null,null",null,null
89,"88,× 76.2,null,null",null,null
90,"89,80.9,null,null",null,null
91,"90,73.7,null,null",null,null
92,"91,66.0,null,null",null,null
93,"92,67.4,null,null",null,null
94,"93,52.3,null,null",null,null
95,"94,Combined Model,null,null",null,null
96,"95,×,null,null",null,null
97,"96,× 77.9 (+2.2%) 82.2 (+1.6%) 74.7 (+1.4%) 67.2 (+1.8%) 68.5 (+1.6%) 53.9 (+3.1%),null,null",null,null
98,"97,Combined Model,null,null",null,null
99,"98,×,null,null",null,null
100,"99,78.2 (+2.6%) 83.7 (+3.5%) 76.8 (+4.2%) 64.7 (-2.0%) 65.7 (-2.5%) 48.6 (-7.1%),null,null",null,null
101,"100,CNN Combined Model Combined Model,null,null",null,null
102,"101,× 75.4,null,null",null,null
103,"102,79.9,null,null",null,null
104,"103,71.6,null,null",null,null
105,"104,65.3,null,null",null,null
106,"105,66.8,null,null",null,null
107,"106,52.7,null,null",null,null
108,"107,× 77.2 (+2.4%) 81.1 (+1.5%) 72.6 (+1.4%) 70.0 (+7.2%) 71.4 (+6.9%) 58.4 (+10.8%),null,null",null,null
109,"108,77.3 (+2.5%) 82.0 (+2.6%) 74.7 (+4.3%) 69.0 (+5.7%) 70.9 (+6.1%) 58.4 (+10.8%),null,null",null,null
110,"109,Runs (Google News),null,null",null,null
111,"110,CNN,null,null",null,null
112,"111,×,null,null",null,null
113,"112,Combined Model,null,null",null,null
114,"113,×,null,null",null,null
115,"114,Combined Model,null,null",null,null
116,"115,×,null,null",null,null
117,"116,CNN Combined Model Combined Model,null,null",null,null
118,"117,× 76.1,null,null",null,null
119,"118,82.3,null,null",null,null
120,"119,75.8,null,null",null,null
121,"120,67.3,null,null",null,null
122,"121,69.1,null,null",null,null
123,"122,57.2,null,null",null,null
124,"123,× 73.8 (-3.0%) 79.2 (-3.8%) 70.5 (-7.0%) 69.2 (+2.8%) 70.2 (+1.6%) 56.0 (-2.1%),null,null",null,null
125,"124,74.8 (-1.7%) 80.1 (-2.7%) 71.6 (-5.5%) 69.2 (+2.8%) 70.7 (+2.3%) 56.4 (-1.4%),null,null",null,null
126,"125,× 75.0,null,null",null,null
127,"126,81.1,null,null",null,null
128,"127,73.7,null,null",null,null
129,"128,66.3,null,null",null,null
130,"129,68.3,null,null",null,null
131,"130,54.7,null,null",null,null
132,"131,× 76.5 (+2.0%) 82.5 (+1.7%) 74.7 (+1.4%) 69.4 (+4.7%) 71.2 (+4.2%) 57.6 (+5.3%),null,null",null,null
133,"132,76.3 (+1.7%) 82.5 (+1.7%) 74.7 (+1.4%) 67.9 (+2.4%) 69.7 (+2.0%) 56.0 (+2.4%),null,null",null,null
134,"133,Reference methods Bagged LambdaMART LSTM [12] CNN [8] aNMM [15] ABCNN-3 [18] PairwiseRank + SentLevel [7],null,null",null,null
135,"134,75.7,null,null",null,null
136,"135,81.3,null,null",null,null
137,"136,72.6,null,null",null,null
138,"137,63.0,null,null",null,null
139,"138,63.8,null,null",null,null
140,"139,46.5,null,null",null,null
141,"140,71.3,null,null",null,null
142,"141,79.1,null,null",null,null
143,"142,--,null,null",null,null
144,"143,--,null,null",null,null
145,"144,74.6,null,null",null,null
146,"145,80.8,null,null",null,null
147,"146,--,null,null",null,null
148,"147,--,null,null",null,null
149,"148,75.0,null,null",null,null
150,"149,81.1,null,null",null,null
151,"150,--,null,null",null,null
152,"151,--,null,null",null,null
153,"152,--,null,null",null,null
154,"153,--,null,null",null,null
155,"154,69.2,null,null",null,null
156,"155,71.1,null,null",null,null
157,"156,78.0,null,null",null,null
158,"157,83.4,null,null",null,null
159,"158,70.1,null,null",null,null
160,"159,71.8,null,null",null,null
161,"160,""e matrix A  Rnq ×na representing the """"a ention"""" is computed internally to the layer as follows:"",null,null",null,null
162,"161,""Ai, j , 1 +"",null,null",null,null
163,"162,""1 Fq [i, :] - Fa [j, :]"",null,null",null,null
164,"163,"","",null,null",null,null
165,"164,(2),null,null",null,null
166,"165,""with · being the euclidean distance function. en, the layer"",null,null",null,null
167,"166,""generates two new a ention-based feature maps, Fq and Fa , which are to be combined in the follow-up convolutional layers:"",null,null",null,null
168,"167,""Fq , AWq Fa ,"""" AT Wa ,"""""",null,null",null,null
169,"168,(3),null,null",null,null
170,"169,where Wq  Rna ×d and Wa  Rnq ×d denote model weights which are to be learned from the data.,null,null",null,null
171,"170,3.2 Baselines,null,null",null,null
172,"171,""A number of published results were included as reference runs in the experiments [8, 12, 15, 18], including a recently proposed neural model PairwiseRank [7]. For comparing with learning-to-rank systems, a Bagged LambdaMART model trained using RankLib2 over the 21 sentence features is included. e bagged LambdaMART was trained by optimizing NDCG@20 with subsampling rate 0.7, feature sampling rate 0.3, using 300 bags."",null,null",null,null
173,"172,2h ps://www.lemurproject.org/ranklib.php,null,null",null,null
174,"173,3.3 Results,null,null",null,null
175,"174,""Table 2 gives the e ectiveness results for the TREC QA and the WikiQA datasets. e e ectiveness of answer selection is measured by Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Success at 1 (S@1), using the trec eval package following Severyn and Moschi i [8]. e experimental runs are divided into four groups, according to the word embeddings in use (AQUAINT/Wikipedia or Google News) and whether the a ention mechanism is enabled. In each group, the original CNN model is the experimental control, and runs with external features combined (denoted as Combined Model) are the treatments."",null,null",null,null
176,"175,""On both sets of data, the PairwiseRank model gives the best results. e baseline bagged LambdaMART model appears to be strong on the TREC QA data, beating a number of neural network models, but it does not appear particularly e ective on the more challenging WikiQA dataset."",null,null",null,null
177,"176,""Our base CNN model is found to be superior to the learning-torank model, and it gave be er results than the original implementation [8] on the TREC QA benchmark. In general, the Combined Model reliably improves the base CNN model. All experimental con gurations appear to bene t from the inclusion of the 21 extra features, with two exceptions: runs using GoogleNews embeddings on the TREC QA benchmark, and one of the dropout runs on the"",null,null",null,null
178,"177,1019,null,null",null,null
179,"178,Short Research Paper,null,null",null,null
180,"179,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
181,"180,""WikiQA data. On the TREC QA benchmark, we saw an increase of 1.3%­4.3% in the three evaluation metrics. On WikiQA, the increase is around 1.6%­7.2% in MAP/MRR and 2.4%­10.8% in S@1."",null,null",null,null
182,"181,""ese increases are in most cases consistent with each other, except that in one particular con guration a decreased S@1 is observed alongside improved MAP and MRR scores."",null,null",null,null
183,"182,""e AQUAINT/Wikipedia embeddings appear to have a slight advantage over the Google News embeddings. e best performing runs on both benchmarks using this embedding achieved a marked increased in e ectiveness compared to the best known results. On the TREC QA benchmark, however, the Combined Model with dropout surpassed the PairwiseRank model [7] in both MAP and MRR. On WikiQA, the Combined Model with the a ention mechanism outperformed ABCNN-3 (i.e., a stronger variant of ABCNN-1), with the achieved e ectiveness only marginally below the e ectiveness of the PairwiseRank model. In most cases, we found that the Combined Model works the best when the a ention mechanism is used together without dropout. We conjecture that in this case the a entional CNN model works di erently to the data, as external features on their own tend to t certain aspects well enough."",null,null",null,null
184,"183,""Based on these results, we conclude that, for answer sentence selection, combining the proposed external features into a convolutional neural architecture has a bene t of improving overall modeling e ectiveness. is improvement is evident even when the neural architecture is slightly altered to perform advanced neural functions such as a ention mechanism or dropout. e evidence is that the highly tuned convolutional neural architecture failed to model certain aspects in the data, which can be captured with a set of simple features. is points to a limitation of neural network methodology previously not mentioned in research on question answering."",null,null",null,null
185,"184,4 RELATED WORK,null,null",null,null
186,"185,""ere is a rich body of work in question answering focused on answer sentence selection [7, 8, 11, 12, 14, 15, 17­19]. Most of these e orts address the architectural issues in neural network models. Yu et al. [19] utilize a CNN architecture to model question-answer pairs, and this approach was taken by Yang et al [17] and Severyn and Moschi i [8], who later expanded the network architecture into a bi-CNN model. Wang et al. [14] decomposed vectors into similar/dissimilar components and used a two-channel CNN to capture the signals. e a ention mechanism was investigated in Yin et al. [18] and Yang et al. [15]. He and Lin [4] used a Bidirectional LSTM to model the context of input sentences. Rao et al. [7] proposed a pairwise ranking method that uses two bi-CNN architectures to perform sentence-level pairwise ranking."",null,null",null,null
187,"186,""Feature engineering has been a popular methodology for modeling question-answer structure, and is still actively used in nonfactoid question answering or answer re-ranking [10, 16]. One commonality between these specialized tasks is a focus on retrieving or ranking passage-level answers [6]. Surdeanu et al. [10] proposed using a translation model to capture the mapping between the high-level linguistic representations of the question and the answer. Yang et al. [16] proposed using query matching, semantic, and context features to select answer sentences for non-factoid questions."",null,null",null,null
188,"187,5 CONCLUSIONS,null,null",null,null
189,"188,We provide empirical evidence to support the use of conventional,null,null",null,null
190,"189,features in deep learning models on the task of answer sentence,null,null",null,null
191,"190,selection. We show that a convolutional neural network model ben-,null,null",null,null
192,"191,e ts from a group of commonly used text features and outperforms,null,null",null,null
193,"192,the best published result on a commonly used question answering,null,null",null,null
194,"193,benchmark. e fact that neural networks can still bene t from,null,null",null,null
195,"194,these conventional features may point to new possibilities in the,null,null",null,null
196,"195,""evolution of new neural architectures. In future work, we will seek"",null,null",null,null
197,"196,""to expand this analysis to other neural architectures, such as LSTM-"",null,null",null,null
198,"197,""CNN or recurrent neural networks, and other question answering"",null,null",null,null
199,"198,benchmarks that are more recent and larger.,null,null",null,null
200,"199,REFERENCES,null,null",null,null
201,"200,""[1] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don'T Add Up: Ad-hoc Retrieval Results Since 1998. In Proceedings of CIKM '09. ACM, New York, NY, USA, 601­610."",null,null",null,null
202,"201,""[2] Ma hew W. Bilo i, Jonathan Elsas, Jaime Carbonell, and Eric Nyberg. 2010. Rank Learning for Factoid estion Answering with Linguistic and Semantic Constraints. In Proceedings of CIKM '10. ACM, New York, NY, USA, 459­468."",null,null",null,null
203,"202,""[3] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Ma Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to Rank Using Gradient Descent. In Proceedings of ICML '05. ACM, New York, NY, USA, 89­96."",null,null",null,null
204,"203,[4] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement. In Proceedings of NAACL '16. 937­948.,null,null",null,null
205,"204,""[5] Tapas Kanungo and David Orr. 2009. Predicting the Readability of Short Web Summaries. In Proceedings of WSDM '09. ACM, New York, NY, USA, 202­211."",null,null",null,null
206,"205,""[6] Mostafa Keikha, Jae Hyun Park, W. Bruce Cro , and Mark Sanderson. 2014. Retrieving Passages and Finding Answers. In Proceedings of ADCS '14. ACM, New York, NY, USA, Article 81, 4 pages."",null,null",null,null
207,"206,""[7] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In Proceedings of CIKM '16. ACM, 1913­1916."",null,null",null,null
208,"207,""[8] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of SIGIR '15. ACM, 373­382."",null,null",null,null
209,"208,""[9] Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai, Jingjing Liu, and Ming-Wei Chang. 2015. Open Domain estion Answering via Semantic Enrichment. In Proceedings of WWW '15. 1045­1055."",null,null",null,null
210,"209,""[10] Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid estions from Web Collections. Computational Linguistics 37, 2 (April 2011), 351­383."",null,null",null,null
211,"210,""[11] Kateryna Tymoshenko, Daniele Bonadiman, and Alessandro Moschi i. 2016. Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking. In Proceedings of NAACL '16. 1268­1278."",null,null",null,null
212,"211,[12] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In Proceedings of ACL '15. 707­712.,null,null",null,null
213,"212,""[13] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In Proceedings of EMNLP '07. 22­32."",null,null",null,null
214,"213,""[14] Zhiguo Wang, Haitao Mi, and Abraham I ycheriah. 2016. Sentence Similarity Learning by Lexical Decomposition and Composition. In Proceedings of COLING 2016. Osaka, Japan, 1340­1349."",null,null",null,null
215,"214,""[15] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Cro . 2016. aNMM: Ranking Short Answer Texts with A ention-Based Neural Matching Model. In Proceedings of CIKM '16. ACM, 287­296."",null,null",null,null
216,"215,""[16] Liu Yang, Qingyao Ai, Damiano Spina, Ruey-Cheng Chen, Liang Pang, W. Bruce Cro , Jiafeng Guo, and Falk Scholer. 2016. Beyond Factoid QA: E ective Methods for Non-factoid Answer Sentence Retrieval. In Proceedings of ECIR '16. Springer International Publishing, 115­128."",null,null",null,null
217,"216,""[17] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain estion Answering. In Proceedings of EMNLP '15. Lisbon, Portugal, 2013­2018."",null,null",null,null
218,"217,""[18] Wenpeng Yin, Hinrich Schtze, Bing Xiang, and Bowen Zhou. 2015. ABCNN: A ention-Based Convolutional Neural Network for Modeling Sentence Pairs. arXiv:1512.05193 [cs] (Dec. 2015)."",null,null",null,null
219,"218,""[19] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. arXiv:1412.1632 [cs] (Dec. 2014)."",null,null",null,null
220,"219,[20] Ma hew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012).,null,null",null,null
221,"220,1020,null,null",null,null
222,"221,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,The Impact of Linkage Methods in Hierarchical Clustering for Active Learning to Rank,null,null",null,null
4,"3,Ziming Li,null,null",null,null
5,"4,""University of Amsterdam Amsterdam, e Netherlands"",null,null",null,null
6,"5,z.li@uva.nl,null,null",null,null
7,"6,ABSTRACT,null,null",null,null
8,"7,""Document ranking is a central problem in many areas, including information retrieval and recommendation. e goal of learning to rank is to automatically create ranking models from training data. e performance of ranking models is strongly a ected by the quality and quantity of training data. Collecting large scale training samples with relevance labels involves human labor which is timeconsuming and expensive. Selective sampling and active learning techniques have been developed and proven e ective in addressing this problem. However, most active methods do not scale well and need to rebuild the model a er selected samples are added to the previous training set. We propose a sampling method which selects a set of instances and labels the full set only once before training the ranking model. Our method is based on hierarchical agglomerative clustering (average linkage) and we also report the performance of other linkage criteria that measure the distance between two clusters of query-document pairs. Another di erence from previous hierarchical clustering is that we cluster the instances belonging to the same query, which usually outperforms the baselines."",null,null",null,null
9,"8,CCS CONCEPTS,null,null",null,null
10,"9,·Information systems Learning to rank;,null,null",null,null
11,"10,1 INTRODUCTION,null,null",null,null
12,"11,""Document ranking is an essential feature in many information retrieval applications. How to sort the returned results according to their degree of relevance has given birth to the area of learning to rank [6], which aims to automatically create ranking models from a training dataset; the learned models are then used to rank the results of new queries. Many learning to rank algorithms have been proposed; they can be categorized into three types of approach: pointwise [2], pairwise [5] and listwise [1]."",null,null",null,null
13,"12,""Like other supervised machine learning methods, the performance of a ranking model highly depends on the quantity and quality of training datasets. To create training datasets, experts are hired to manually provide relevance labels for training data, which is expensive and time-consuming. Human labeling, whether by"",null,null",null,null
14,"13,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080684"",null,null",null,null
15,"14,Maarten de Rijke,null,null",null,null
16,"15,""University of Amsterdam Amsterdam, e Netherlands"",null,null",null,null
17,"16,derijke@uva.nl,null,null",null,null
18,"17,""experts or crowds, can be noisy and biased [11]. Active learning is a paradigm to reduce the labeling e ort [12]; it has mostly been studied in the context of classi cation tasks [9, 10, 14]."",null,null",null,null
19,"18,""In this paper, we present an active learning method to select the most informative query-document pairs to be labeled for learning to rank. Our method relies on hierarchical clustering. Unlike traditional active learning methods, our method is unsupervised and the selected training sets can be used to train di erent learning to rank models. We build on the hypothesis that the information contained in an instance is highly correlated to the instance position in the feature space. Hierarchical clustering has the ability to group instances with similar information into the same cluster and each cluster can be represented by its centroid. While most active learning methods need to rebuild the training models each time new labeled documents are added to the training set, our method labels the instances only once before the training process. We rst evaluate our method on three datasets from Letor 3.0 and nd that the performance of our method is similar or superior to the baselines while we can achieve full training performance with fewer instances. We also analyze the limitations of our method and nd that the e ectiveness of our sampling method is closely related to the features and structures each dataset has."",null,null",null,null
20,"19,2 RELATED WORK,null,null",null,null
21,"20,""In order to address the lack of labeled data, active learning has proven to be a promising direction that aims to achieve high accuracy using as few labeled instances as possible [12]. A number of active learning methods have been proposed for classi cation. Most methods start with only a small set of labeled instances and sequentially select the most informative instances to be labeled by an oracle. e trained model will be updated when new labeled instances are added to the training set. Di erent strategies are proposed to choose the most informative instances that can maximize the information value to the current model [9, 10, 14]."",null,null",null,null
22,"21,""It is not straightforward to extend these methods to ranking problems. On the one hand, these methods try to minimize the classi cation error and do not take into account the rank order while position-based measures are usually non-continuous and non-di erentiable. On the other, each instance in most supervised classi cation tasks can be treated as independent of each other while the instances in learning to rank are conditionally independent. Compared with traditional classi cation problems, there is limited work about active learning in ranking. Long et al. [8] adopt a two-stage active learning strategy schema to integrate query level and document level data selection. ey select samples minimizing the expected loss as the most informative ones and achieves good results in the case of web search ranking. But this method requires"",null,null",null,null
23,"22,941,null,null",null,null
24,"23,Short Research Paper,null,null",null,null
25,"24,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
26,"25,""a relative big seed set and the ranking models are restricted to pointwise models. Donmez and Carbonell [3] select the documents that woluld impart the greatest change to the current model. While all these methods sequentially select instances to be labeled, Silva et al. [13] adopt hierarchical clustering to """"compress"""" the original training set, which is the state-of-the-art selection method in overall performance and e ciency. e method we propose can be viewed as an extension of [13]. e di erence are that we cluster the query-document pairs belonging to the same query separately and average linkage is used, which achieves be er performance."",null,null",null,null
27,"26,3 METHOD,null,null",null,null
28,"27,3.1 Hierarchical agglomerative clustering,null,null",null,null
29,"28,""In hierarchical agglomerative clustering, each instance is regarded as a singleton cluster and then merged based on the distance or similarity between clusters until there is only one cluster that contains all instances. e structure of the nal cluster is a tree or dendrogram and each level of the resulting tree is a segmentation of the data. For two given clusters, C1 and C2, and a non-negative real value , if distance f (C1, C2) < , then C1 and C2 will be merged."",null,null",null,null
30,"29,""According to the merging rule, the number of clusters is associated with the value of , which is the indistinguishability threshold [13]. Di erent linkage criterions have been proposed to measure the distance or dissimilarity between two clusters. We use average as our linkage criterion and we also report the performance of minimum linkage, maximum linkage and ward linkage."",null,null",null,null
31,"30,""3.1.1 Minimum linkage clustering. In minimum linkage clustering (also called single linkage clustering), one of the simplest agglomerative hierarchical clustering methods, the value of the shortest link from any member of one cluster to the member of another cluster denotes the distance of these two clusters. For two clusters C1 and C2, the distance between C1 and C2 is:"",null,null",null,null
32,"31,f,null,null",null,null
33,"32,""(C1, C2)"",null,null",null,null
34,"33,"","",null,null",null,null
35,"34,u,null,null",null,null
36,"35,min,null,null",null,null
37,"36,""C1, C2"",null,null",null,null
38,"37,""d (u,"",null,null",null,null
39,"38,""),"",null,null",null,null
40,"39,""In this paper, d is the Euclidean distance. is method is also known as the nearest technique and used in [13]. In this case, minimum linkage clustering can group the instances in """"stringy"""" clusters and be converted to nd the Minimum Spanning Tree (MST) of querydocument pairs [4]. By deleting all edges longer than a speci ed indistinguishability threshold in the MST, the remaining connected instances form a hierarchical cluster."",null,null",null,null
41,"40,3.1.2 Average linkage clustering. Average linkage clustering uses,null,null",null,null
42,"41,""the average of distances between all pairs of instances, where each"",null,null",null,null
43,"42,""pair consists of two points from two di erent clusters, as the dis-"",null,null",null,null
44,"43,tance between two clusters:,null,null",null,null
45,"44,f,null,null",null,null
46,"45,""(C1, C2)"",null,null",null,null
47,"46,"","",null,null",null,null
48,"47,N1,null,null",null,null
49,"48,1  N2,null,null",null,null
50,"49,""u C1,"",null,null",null,null
51,"50,""d (u,"",null,null",null,null
52,"51,C2,null,null",null,null
53,"52,""),"",null,null",null,null
54,"53,""where N1 and N2 are the sizes of clusters C1 and C2, respectively. We use average linkage as our linkage criterion to measure the"",null,null",null,null
55,"54,cluster distance and we perform clustering on each subset which,null,null",null,null
56,"55,contains all the query-document pairs belonging to the same query.,null,null",null,null
57,"56,""3.1.3 Maximum linkage clustering. Di erent from single linkage clustering, the distance of two clusters in maximum linkage clustering is the value of the largest link from one cluster to another"",null,null",null,null
58,"57,""cluster. For two clusters C1 and C2, the distance is computed as follows:"",null,null",null,null
59,"58,f,null,null",null,null
60,"59,""(C1, C2)"",null,null",null,null
61,"60,"","",null,null",null,null
62,"61,u,null,null",null,null
63,"62,max,null,null",null,null
64,"63,""C1, C2"",null,null",null,null
65,"64,""d (u,"",null,null",null,null
66,"65,""),"",null,null",null,null
67,"66,where d is the Euclidean distance.,null,null",null,null
68,"67,""3.1.4 Ward linkage clustering. In Ward's linkage method, the"",null,null",null,null
69,"68,distance between two clusters is the sum of the squares of the,null,null",null,null
70,"69,distances between all objects in the cluster and the centroid of the,null,null",null,null
71,"70,""cluster. For two clusters C1 and C2, the distance is computed as"",null,null",null,null
72,"71,follows:,null,null",null,null
73,"72,""f (C1, C2) ,"",null,null",null,null
74,"73,""d (x, µC1C2 )2,"",null,null",null,null
75,"74,x C1C2,null,null",null,null
76,"75,where µ is the centroid of the new cluster merged from C1 and C2.,null,null",null,null
77,"76,3.2 Hierarchical clustering for learning to rank,null,null",null,null
78,"77,""In this paper, we apply di erent linkage criteria to hierarchical clustering. As shown in Algorithm 1, we rst cut all the querydocuments pairs into di erent subsets which have di erent query ids and then adopt hierarchical clustering on each subset. A er the last two steps, we can get the clusters on each subset. In our sampling strategy, we use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster. In fact, the clustering distribution shows that there is only one single point in most clusters. e nal dataset to be labeled is made up of all the selected instances."",null,null",null,null
79,"78,Algorithm 1 Hierarchical Clustering on Each ery (HCEQ),null,null",null,null
80,"79,""Require: Unlabeled dataset D with m queries, desired sampling"",null,null",null,null
81,"80,""size n, linkage criterion linka e"",null,null",null,null
82,"81,Ensure: e subset S to be labeled,null,null",null,null
83,"82,""1: D1, D2, . . . , Di , . . . , Dm  Di idin Dataset (D). 2: S  "",null,null",null,null
84,"83,""3: for all i  {1, 2, . . . , m} do"",null,null",null,null
85,"84,4:,null,null",null,null
86,"85,ni,null,null",null,null
87,"86,n,null,null",null,null
88,"87,|Di | |D |,null,null",null,null
89,"88,""5: C1, C2, . . . , Cj , . . . , Cni  A Clusterin (Di , ni , linka e )"",null,null",null,null
90,"89,""6: for all j  {1, 2, . . . , ni } do"",null,null",null,null
91,"90,7:,null,null",null,null
92,"91,j  GeometricCentroid (Cj ),null,null",null,null
93,"92,8:,null,null",null,null
94,"93,""pj  N earest N ei hbour (Cj , j )"",null,null",null,null
95,"94,9:,null,null",null,null
96,"95,S  S  {pj },null,null",null,null
97,"96,10: end for,null,null",null,null
98,"97,11: end for,null,null",null,null
99,"98,12: return S,null,null",null,null
100,"99,4 EXPERIMENTAL SETUP,null,null",null,null
101,"100,4.1 Data Sets and Evaluation Measure,null,null",null,null
102,"101,""To compare the performance of di erent linkage criteria, we apply hierarchical clustering to a well-known L2R benchmarking collection, Letor 3.0 [7]. In Letor 3.0, there are 7 datasets, based on two document collections: Gov and OHSUMED. We focus on topic distillation (TD2003, TD2004) from GOV and OHSUMED from OHSUMED. In the sets from GOV, each instance is represented by 64 features extracted from the corresponding query-document pairs and has a label indicating its relevance level. e datasets from GOV have binary relevance labels (relevant, not relevant) while"",null,null",null,null
103,"102,942,null,null",null,null
104,"103,Short Research Paper,null,null",null,null
105,"104,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
106,"105,""OHSUMED has 3 levels (de nitely relevant, possibly relevant, not relevant) and each instance is represented by a 45-dimensional feature vector. Di erent datasets have di erent numbers of instances; there are 50, 75 and 106 queries, each with 50K, 74K and 16K instances in TD2003, TD2004 and OHSUMED, respectively."",null,null",null,null
107,"106,e metric we use is Normalized Discounted Cumulative Gain (NDCG). We run 5-fold cross-validation on all datasets which are query-level normalized and report the average NDCG@10 on 5 folds as nal performance.,null,null",null,null
108,"107,4.2 Baselines,null,null",null,null
109,"108,""We compare the results obtained using our methods with the approach in [13], which is also based on hierarchical clustering; the results of random sampling and the full training set are also reported for reference:"",null,null",null,null
110,"109,""Cover. e method proposed in [13] is an unsupervised and compression-based selection mechanism that tries to create a small and highly informative set to represent the original training dataset. Hierarchical clustering (single linkage) is employed to group querydocument pairs into di erent clusters with required number of clusters. e authors also use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster and form the nal training set to be labeled. Unlike our method, they perform clustering globally and instances belonging to di erent queries could be grouped into the same cluster."",null,null",null,null
111,"110,""Random. e instances to be labeled are selected randomly and no active learning methods are used here. For the same dataset, we run random sampling 10 times and report the average performance."",null,null",null,null
112,"111,Full Training. We use the labeled original training datasets to train the learning to rank models.,null,null",null,null
113,"112,""To be able to show the di erence between our methods and [13], we select SVMRank and Random Forests as our ranking models which are also used in [13]. e parameters of SVMRank and Random Forests are tuned using a small validation set."",null,null",null,null
114,"113,We run all sampling methods until the fraction of selected instances reaches 50% of the original set.,null,null",null,null
115,"114,5 RESULTS AND ANALYSIS,null,null",null,null
116,"115,5.1 Experimental Results,null,null",null,null
117,"116,""Fig. 1 shows the NDCG@10 of di erent linkage criteria and baselines (denoted by Average, Max, Min, Ward, Cover, Random and Full respectively) on the TD2003, TD2004 and OHSUMED datasets. As we use the instance closest to the centroid to represent the corresponding cluster, the instance selected before may not be selected in later sampling rounds; accuracy is not monotonically increasing."",null,null",null,null
118,"117,""On TD2003, in terms of SVMRank (Fig. 1(a)), the accuracy of Average rst exceeds the accuracy of Full at size 2%. Before 24% of the original training set have been selected, all the curves uctuate around 0.35 except Random and Ward. When more and more instances are added to the training set, the performance of Cover goes down and becomes worse than Full. For Random Forests (Fig. 1(d)), Average achieves the highest accuracy before 4% of the original training set has been selected and is the rst one to reach the same accuracy as Full. e accuracies of Ward, Max and Cover start from relatively low points. All methods achieve the performance of Full"",null,null",null,null
119,"118,""at around 12% except Random. A er 18%, Cover stays below Full (close to 0.36) and uctuates around 0.35."",null,null",null,null
120,"119,""Fig. 1(b) and 1(e) describe the performance of di erent methods on the TD2004 dataset. In Fig. 1(b) we see that Average has the highest starting point when 2% of the original training set have been selected. However, a er one more percent has been added to the training set, all methods have a very similar performance at around 0.295. Average and Cover reach the performance of Full with about 5% and continue to rise before they reach their peaks with 13% and 8%, respectively. Except Random, all methods reach the performance of Full with about 11% selected; their accuracy stays higher than that of Full. In terms of Random Forests (Fig. 1(e)), Average still has the highest starting accuracy and a er the uctuations before 11% of the training set has been selected, Average always performs be er than Full and peaks around 19% of the training set, which is also the highest accuracy in all methods. Cover has a relatively high accuracy when the selected size is 7­ 11%."",null,null",null,null
121,"120,""e OHSUMED dataset is based on another document collection, di erent from TD2003 and TD2004. e performance of SVMRank and Random Forests on OHSUMED are shown in Fig. 1(c) and 1(f), respectively. In terms of SVMRank, an interesting thing is that Random has similar performance as hierarchical clustering, which means that hierarchical clustering plays a small role when selecting informative instances in this case. With respect to Random Forests, Average, Min, Ward and Max have very similar performance a er 16% of the training data has been selected and they reach the accuracy of Full at around 30%. Although Cover is the rst method to achieve full training set performance with around 7% and reaches the peak at the same time, its accuracy is not stable and dramatically decreases until 17% of the training set has been selected. A er 30% of the original training set has been selected, all methods achieve the same performance as Full."",null,null",null,null
122,"121,5.2 Analysis,null,null",null,null
123,"122,""As we can see from the results presented above, most of the time, Average outperforms other methods on the TD2003 and TD2004 datasets. Although Min and Cover both use the shortest link to measure the distance between two clusters, they have di erent performance and Min performs be er. One possible reason is that our selection mechanism can guarantee that the proportions of selected instances from each query are same and every query contributes to the nal performance. Another reason might be Cover clusters query-document pairs globally, which causes that query-document pairs from di erent queries will be represented by the instances from one speci c query. And this will limit the number of applicable instance pairs for pairwise ranking models."",null,null",null,null
124,"123,""On OHSUMED, Average, Min and Max have similar and stable performance while Cover uctuates dramatically when relatively few instances are selected, especially with the Random Forests learner. e di erence between the proposed methods and Random is not signi cant. How datasets are constructed and what structures datasets have will in uence the performance of clusteringbased active learning methods. For example, an instance from the OHSUMED dataset is represented by 45 features which is fewer than for TD2003 and 2004, and some speci c features could have greater impact on the clustering results. In addition, a query has"",null,null",null,null
125,"124,943,null,null",null,null
126,"125,Short Research Paper,null,null",null,null
127,"126,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
128,"127,NDCG@10,null,null",null,null
129,"128,0.37 0.35,null,null",null,null
130,"129,0.34 0.32,null,null",null,null
131,"130,0.45 0.43 0.41,null,null",null,null
132,"131,0.33 0.31,null,null",null,null
133,"132,0.30,null,null",null,null
134,"133,0.39 0.37 0.35,null,null",null,null
135,"134,NDCG@10,null,null",null,null
136,"135,NDCG@10,null,null",null,null
137,"136,0.29,null,null",null,null
138,"137,Average,null,null",null,null
139,"138,0.27,null,null",null,null
140,"139,Cover Max,null,null",null,null
141,"140,Min,null,null",null,null
142,"141,0.25,null,null",null,null
143,"142,Ward Random,null,null",null,null
144,"143,Full,null,null",null,null
145,"144,0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,null,null",null,null
146,"145,(a) SVMRank on TD2003,null,null",null,null
147,"146,% selected,null,null",null,null
148,"147,0.28,null,null",null,null
149,"148,Average,null,null",null,null
150,"149,Cover,null,null",null,null
151,"150,Max,null,null",null,null
152,"151,0.26,null,null",null,null
153,"152,Min,null,null",null,null
154,"153,Ward,null,null",null,null
155,"154,Random,null,null",null,null
156,"155,Full,null,null",null,null
157,"156,0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,null,null",null,null
158,"157,(b) SVMRank on TD2004,null,null",null,null
159,"158,% selected,null,null",null,null
160,"159,0.33,null,null",null,null
161,"160,0.31,null,null",null,null
162,"161,Average,null,null",null,null
163,"162,0.29,null,null",null,null
164,"163,Cover Max,null,null",null,null
165,"164,0.27,null,null",null,null
166,"165,Min Ward,null,null",null,null
167,"166,0.25,null,null",null,null
168,"167,Random Full,null,null",null,null
169,"168,0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,null,null",null,null
170,"169,(c) SVMRank on OHSUMED,null,null",null,null
171,"170,% selected,null,null",null,null
172,"171,NDCG@10,null,null",null,null
173,"172,0.38,null,null",null,null
174,"173,0.36,null,null",null,null
175,"174,0.34,null,null",null,null
176,"175,0.32,null,null",null,null
177,"176,0.30,null,null",null,null
178,"177,Average,null,null",null,null
179,"178,0.28,null,null",null,null
180,"179,Cover Max,null,null",null,null
181,"180,Min,null,null",null,null
182,"181,0.26,null,null",null,null
183,"182,Ward Random,null,null",null,null
184,"183,Full,null,null",null,null
185,"184,0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,null,null",null,null
186,"185,%selected,null,null",null,null
187,"186,(d) Random Forests on TD2003,null,null",null,null
188,"187,NDCG@10,null,null",null,null
189,"188,0.35,null,null",null,null
190,"189,0.33,null,null",null,null
191,"190,0.31,null,null",null,null
192,"191,0.29,null,null",null,null
193,"192,Average Cover,null,null",null,null
194,"193,Max,null,null",null,null
195,"194,0.27,null,null",null,null
196,"195,Min Ward,null,null",null,null
197,"196,Random,null,null",null,null
198,"197,Full,null,null",null,null
199,"198,0.25 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,null,null",null,null
200,"199,%selected,null,null",null,null
201,"200,(e) Random Forests on TD2004,null,null",null,null
202,"201,NDCG@10,null,null",null,null
203,"202,0.45,null,null",null,null
204,"203,0.43,null,null",null,null
205,"204,0.41,null,null",null,null
206,"205,0.39,null,null",null,null
207,"206,0.37,null,null",null,null
208,"207,0.35,null,null",null,null
209,"208,0.33,null,null",null,null
210,"209,0.31,null,null",null,null
211,"210,0.29,null,null",null,null
212,"211,Average Cover,null,null",null,null
213,"212,0.27 0.25,null,null",null,null
214,"213,Max Min Ward,null,null",null,null
215,"214,0.23,null,null",null,null
216,"215,Random Full,null,null",null,null
217,"216,0.21 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,null,null",null,null
218,"217,%selected,null,null",null,null
219,"218,(f) Random Forests on OHSUMED,null,null",null,null
220,"219,""Figure 1: Performance on TD2003, TD2004, and OHSUMED. e x-axis displays the percentage of selected instances."",null,null",null,null
221,"220,""only about 152 associated documents in OHSUMED. When we cluster query-document pairs with respect to each query, the number of selected instances from each query is small and every individual query will have very few associated documents which can in uence the performance of ranking models."",null,null",null,null
222,"221,6 CONCLUSIONS AND FUTURE WORK,null,null",null,null
223,"222,""In this paper, we adopt hierarchical clustering to select the most informative instances for learning to rank and report the performance of di erent linkage criteria and baselines. On the Letor 3.0 dataset, the performance of average linkage is similar or superior to the baselines while fewer instances are needed. In the future, we will investigate how to make our method more stronger and robust on di erent datasets. One possible direction is to detect correlations between speci c features and clustering performance. How to choose an optimal fraction of instances for each query while the total number of selected instances is xed is another future direction worth exploring."",null,null",null,null
224,"223,""Acknowledgments. is research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Microso Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scienti c Research (NWO) under project nrs 612.001.116, HOR-11-10, CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors."",null,null",null,null
225,"224,REFERENCES,null,null",null,null
226,"225,""[1] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML '07, pages 129­136, 2007."",null,null",null,null
227,"226,""[2] K. Crammer and Y. Singer. Pranking with ranking. In NIPS '01, pages 641­647, 2001."",null,null",null,null
228,"227,""[3] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In ICML '08, pages 248­255, 2008."",null,null",null,null
229,"228,""[4] J. C. Gower and G. Ross. Minimum spanning trees and single linkage cluster analysis. Applied statistics, pages 54­64, 1969."",null,null",null,null
230,"229,""[5] R. Herbrich, T. Graepel, and K. Obermayer. Support vector learning for ordinal regression. In ICANN '99, pages 97­102, 1999."",null,null",null,null
231,"230,""[6] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009."",null,null",null,null
232,"231,""[7] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pages 3­10, 2007."",null,null",null,null
233,"232,""[8] B. Long, J. Bian, O. Chapelle, Y. Zhang, Y. Inagaki, and Y. Chang. Active learning for ranking through expected loss optimization. IEEE Transactions on Knowledge and Data Engineering, 27(5):1180­1191, 2015."",null,null",null,null
234,"233,""[9] A. K. McCallumzy and K. Nigamy. Employing em and pool-based active learning for text classi cation. In ICML '98, pages 359­367, 1998."",null,null",null,null
235,"234,""[10] H. T. Nguyen and A. Smeulders. Active learning using pre-clustering. In ICML '04, page 79, 2004."",null,null",null,null
236,"235,""[11] F. Radlinski and T. Joachims. Active exploration for learning rankings from clickthrough data. In KDD '07, pages 570­579, 2007."",null,null",null,null
237,"236,""[12] B. Se les. Active learning literature survey. Technical report, University of Wisconsin­Madison, 2009."",null,null",null,null
238,"237,""[13] R. M. Silva, G. Gomes, M. S. Alvim, and M. A. Gon¸calves. Compression-based selective sampling for learning to rank. In CIKM '16, pages 247­256, 2016."",null,null",null,null
239,"238,""[14] S. Tong and D. Koller. Support vector machine active learning with applications to text classi cation. Journal of Machine Learning Research, 2(Nov):45­66, 2001."",null,null",null,null
240,"239,944,null,null",null,null
241,"240,,null,null",null,null

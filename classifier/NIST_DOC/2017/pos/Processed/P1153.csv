,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Generating ery Suggestions to Support Task-Based Search,null,null",null,null
4,"3,Dar´io Gariglio i,null,null",null,null
5,"4,University of Stavanger dario.gariglio i@uis.no,null,null",null,null
6,"5,ABSTRACT,null,null",null,null
7,"6,""We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the rst place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model."",null,null",null,null
8,"7,CCS CONCEPTS,null,null",null,null
9,"8,·Information systems  ery suggestion;,null,null",null,null
10,"9,KEYWORDS,null,null",null,null
11,"10,""ery suggestions, task-based search, supporting search tasks"",null,null",null,null
12,"11,""ACM Reference format: Dar´io Gariglio i and Krisztian Balog. 2017. Generating ery Suggestions to Support Task-Based Search. In Proceedings of SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080745"",null,null",null,null
13,"12,1 INTRODUCTION,null,null",null,null
14,"13,""Search is o en performed in the context of some larger underlying task [11]. ere is a growing stream of research aimed at making search engines more task-aware (i.e., recognizing what task the user is trying to accomplish) and customizing the search experience accordingly (see §2). In this paper, we focus our a ention on one particular tool for supporting task-based search: query suggestions. ery suggestions are an integral part of modern search engines [16]. We envisage an user interface where these suggestions are presented once the user has issued an initial query; see Figure 1. Note that this is di erent from query autocompletion, which tries to recommend various possible completions while the user is still typing the query. e task-aware query suggestions we propose are intended for exploring various aspects (subtasks) of the given task a er inspecting the initial search results. Selecting them would allow the user to narrow down the scope of the search."",null,null",null,null
15,"14,""e Tasks track at the Text REtrieval Conference (TREC) has introduced an evaluation platform for this very problem, referred to as task understanding [20]. Speci cally, given an initial query,"",null,null",null,null
16,"15,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080745"",null,null",null,null
17,"16,Krisztian Balog,null,null",null,null
18,"17,University of Stavanger krisztian.balog@uis.no,null,null",null,null
19,"18,choose bathroom,null,null",null,null
20,"19,choose bathroom cabinets lightning choose bathroom decoration style bathroom get ideas renew floor bathroom changing furniture bathroom,null,null",null,null
21,"20,Figure 1: ery suggestions to support task-based search.,null,null",null,null
22,"21,""the system should return a ranked list of keyphrases """"that represent the set of all tasks a user who submi ed the query may be looking for"""" [20]. e goal is to provide a complete coverage of subtasks for an initial query, while avoiding redundancy. We use these keyphrases as query suggestions."",null,null",null,null
23,"22,Our aim is to generate such suggestions in a se ing where past usage data and query logs are not available or cannot be utilized.,null,null",null,null
24,"23,""is would be typical for systems that have a smaller user base (e.g., in the enterprise domain) or when a search engine has been newly deployed [4]. One possibility is to use query suggestion APIs, which are o ered by all major web search engines. ese are indeed one main source type we consider. Additionally, we use the initial query to search for relevant documents, using web search engines, and extract keyphrases from search snippets and from full text documents. Finally, given the task-based focus of our work, we lend special treatment to the WikiHow site,1 which is an extensive database of how-to guides."",null,null",null,null
25,"24,""e main contribution of this paper is twofold. First, we propose a transparent architecture, using generative probabilistic modeling, for extracting keyphrases from a variety of sources and generating query suggestions from them. Second, we provide a detailed analysis of the components of our framework using di erent estimation methods. Many systems that participated in the TREC Tasks track have relied on strategic combinations of di erent sources to produce query suggestions, see, e.g., [7­9]. However, no systematic comparison of the di erent source types has been performed yet--we ll this gap. Additional components include estimating a document's importance within a given source, extracting keyphrases from documents, and forming query suggestions from these keyphrases. Finally, we check whether our ndings are consistent across the 2015 and 2016 editions of the TREC Tasks track."",null,null",null,null
26,"25,2 RELATED WORK,null,null",null,null
27,"26,""ere is a large body of work on understanding and supporting users in carrying out complex search tasks. Log-based studies have been one main area of focus, including the identi cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query"",null,null",null,null
28,"27,1h p://www.wikihow.com/,null,null",null,null
29,"28,1153,null,null",null,null
30,"29,Short Research Paper,null,null",null,null
31,"30,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
32,"31,q0,null,null",null,null
33,"32,QS,null,null",null,null
34,"33,WS,null,null",null,null
35,"34,WD,null,null",null,null
36,"35,WH,null,null",null,null
37,"36,Keyphrases,null,null",null,null
38,"37,Query suggestions,null,null",null,null
39,"38,Figure 2: High-level overview of our approach.,null,null",null,null
40,"39,""reformulations [10] or search trails [19]. Another theme is supporting exploratory search, where users pursue an information goal to learn or discover more about a given topic. Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17]. Our main interest is in query suggestions, as a distinguished support mechanism. Most of the related work utilizes large-scale query logs. For example, Craswell and Szummer [6] perform a random walk on a query-click graph. Boldi et al. [5] model the query ow in user search sessions via chains of queries. Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus. However, their focus is on query autocompletion, representing the completed and partial terms in a query. Kelly et al. [12] have shown that users prefer query suggestions, rather than term suggestions. We undertake the task of suggesting queries to users, related to the task they are performing, as we shall explain in the next section."",null,null",null,null
41,"40,3 PROBLEM STATEMENT,null,null",null,null
42,"41,""We adhere to the problem de nition of the task understanding task of the TREC Tasks track. Given an initial query q0, the goal is to return a ranked list of query suggestions q1, . . . qn that cover all the possible subtasks related to the task the user is trying to achieve. In addition to the initial query string, the entities mentioned in it are also made available (identi ed by their Freebase IDs)."",null,null",null,null
43,"42,""For example, for the query """"low wedding budget,"""" subtasks include (but are not limited to) """"buy a used wedding gown,"""" """"cheap wedding cake,"""" and """"make your own invitations."""" ese subtasks have been manually identi ed by the track organizers based on information extracted from the logs of a commercial search engine."",null,null",null,null
44,"43,""e suggested keyphrases are judged with respect to each subtask on a three point scale (non-relevant, relevant, and highly relevant). Note that subtasks are only used in the evaluation, these are not available when generating the keyphrases."",null,null",null,null
45,"44,4 APPROACH,null,null",null,null
46,"45,""We now present our approach for generating query suggestions. As Figure 2 illustrates, we obtain keyphrases from a variety of sources, and then construct a ranked list of query suggestions from these."",null,null",null,null
47,"46,4.1 Generative Modeling Framework,null,null",null,null
48,"47,""We introduce a generative probabilistic model for scoring the candidate query suggestions according to P (q|q0), i.e., the probability that a query suggestion q was generated by the initial query q0."",null,null",null,null
49,"48,Formally:,null,null",null,null
50,"49,""P (q|q0) ,"""" P (q|q0, s)P (s |q0)"""""",null,null",null,null
51,"50,s,null,null",null,null
52,"51,"","",null,null",null,null
53,"52,""P (q|q0, s, d )P (d |q0, s) P (s |q0)"",null,null",null,null
54,"53,sd,null,null",null,null
55,"54,"","",null,null",null,null
56,"55,""P (q|q0, s, k )P (k |s, d ) P (d |q0, s) P (s |q0) ."",null,null",null,null
57,"56,sdk,null,null",null,null
58,"57,""is model has four components: (i) P (s |q0) expresses the importance of a particular information source s for the initial query q0; (ii) P (d |q0, s) represents the importance of a document d originating from source s, with respect to the initial query; (iii) P (k |d, s) is"",null,null",null,null
59,"58,the relevance of a keyphrase k extracted from a document d from,null,null",null,null
60,"59,""source s; and (iv) P (q|q0, s, k ) is the probability of generating query suggestion q, given keyphrase k, source s, and the initial query q0. Below, we detail the estimation of each of these components."",null,null",null,null
61,"60,4.2 Source Importance,null,null",null,null
62,"61,""We collect relevant information from four kinds of sources: query suggestions (QS), web search snippets (WS), web search documents (WD), and WikiHow (WH). For the rst three source types, we use three di erent web search engines (Google, Bing, and DuckDuckGo), thereby having a total of 10 individual sources. In this work, we assume conditional independence between a source s and the initial query q0, i.e., set P (s |q0) , P (s)."",null,null",null,null
63,"62,4.3 Document Importance,null,null",null,null
64,"63,""From each source s, we obtain the top-K (K ,"""" 10) documents for the query q0. We propose two ways of modeling the importance of a document d originating from s: (i) uniform and (ii) inversely proportional to the rank of d among the top-K documents, that is:"""""",null,null",null,null
65,"64,""P (d |q0, s) ,"",null,null",null,null
66,"65,K -r +1,null,null",null,null
67,"66,""K i ,1"",null,null",null,null
68,"67,K,null,null",null,null
69,"68,-,null,null",null,null
70,"69,i,null,null",null,null
71,"70,+,null,null",null,null
72,"71,1,null,null",null,null
73,"72,"","",null,null",null,null
74,"73,K -r +1 K (K + 1)/2,null,null",null,null
75,"74,"","",null,null",null,null
76,"75,where r is the rank position of d (r  [1..K]).,null,null",null,null
77,"76,4.4 Keyphrase Relevance,null,null",null,null
78,"77,""We obtain keyphrases from each document, using an automatic keyphrase extraction algorithm. Speci cally, we use the RAKE keyword extraction system [15]. For each keyphrase k, extracted from document d, the associated con dence score is denoted by c (k, d ). Upon a manual inspection of the extraction output, we introduce some data cleansing steps. We only retain keyphrases that: (i) have an extraction con dence above an empirically set threshold of 2; (ii) are at most 5 terms long; (iii) each of the terms has a length between 4 and 15 characters, and is either a meaningful number (i.e., max. 4 digits) or a term (excluding noisy substrings and reserved keywords from mark-up languages). Finally, we set the relevance of k as P (k |d, s) ,"""" c (k, d )/ k c (k , d )."""""",null,null",null,null
79,"78,""In case s is of type QS, each returned document actually corresponds to a query suggestion. us, we treat each of these documents d as a single keyphrase k, for which we set P (k |d, s) , 1."",null,null",null,null
80,"79,1154,null,null",null,null
81,"80,Short Research Paper,null,null",null,null
82,"81,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
83,"82,Table 1: Comparison of query suggestion generators across the di erent types of sources. Statistical signi cance is tested against the corresponding line in the top block.,null,null",null,null
84,"83,""P (q|q0, s, k ) S"",null,null",null,null
85,"84,2015,null,null",null,null
86,"85,2016,null,null",null,null
87,"86,ERR-IA -NDCG ERR-IA -NDCG,null,null",null,null
88,"87,Using raw keyphrases,null,null",null,null
89,"88,QS 0.0755 WS 0.2011 WD 0.1716 WH 0.0744,null,null",null,null
90,"89,0.1186 0.2426 0.2154 0.1044,null,null",null,null
91,"90,0.4114 0.3492 0.2339 0.1377,null,null",null,null
92,"91,0.5289 0.4038 0.2886 0.1723,null,null",null,null
93,"92,Using expanded keyphrases,null,null",null,null
94,"93,QS 0.0751 WS 0.1901 WD 0.1551 WH 0.0849,null,null",null,null
95,"94,0.1182 0.2274 0.2097 0.1090,null,null",null,null
96,"95,0.4046 0.2927 0.1045 0.0789,null,null",null,null
97,"96,0.5233 0.3467 0.1667 0.0932,null,null",null,null
98,"97,Table 2: Comparison of document importance estimators across the di erent types of sources. Statistical signi cance is tested against the corresponding line in the top block.,null,null",null,null
99,"98,""P (d |q0, s) S"",null,null",null,null
100,"99,2015,null,null",null,null
101,"100,2016,null,null",null,null
102,"101,ERR-IA -NDCG ERR-IA -NDCG,null,null",null,null
103,"102,Uniform,null,null",null,null
104,"103,QS 0.0755 WS 0.2011 WD 0.1716 WH 0.0849,null,null",null,null
105,"104,0.1186 0.2426 0.2154 0.1090,null,null",null,null
106,"105,0.4114 0.3492 0.2339 0.1377,null,null",null,null
107,"106,0.5289 0.4038 0.2886 0.1723,null,null",null,null
108,"107,Rank-based QS 0.0891 0.1307,null,null",null,null
109,"108,decay,null,null",null,null
110,"109,WS 0.1906 0.2315,null,null",null,null
111,"110,WD 0.1688 0.2119,null,null",null,null
112,"111,WH 0.0935 0.1225,null,null",null,null
113,"112,0.4288 0.3386 0.1964 0.1195,null,null",null,null
114,"113,0.5455 0.4011 0.2608 0.1495,null,null",null,null
115,"114,4.5 ery Suggestions,null,null",null,null
116,"115,""As a nal step, we need to generate query suggestions from the extracted keyphrases. As a baseline option, we take each raw keyphrase k as-is, i.e., with q ,"""" k we set P (q|q0, s, k ) """", 1."",null,null",null,null
117,"116,""Alternatively, we can form query suggestions by expanding keyphrases. Here, k is combined with the initial query q0 using a set of expansion rules proposed in [7]: (i) adding k as a suf-"",null,null",null,null
118,"117,""x to q0; (ii) adding k as a su x to an entity mentioned in q0; and (iii) using k as-is. Rules (i) and (ii) further involve a custom string concatenation operator; we refer to [7] for details. Each query suggestion q, that is generated from keyword k, has an associated con dence score c (q, q0, s, k ). We then set P (q|q0, s, k ) ,"""" c (q, q0, s, k )/ q c (q , q0, s, k ). By conditioning the suggestion probability on s, it is possible to apply a di erent approach for each source. Like in the previous subsection, we treat sources of type QS distinctly, by simply taking q """","""" k and se ing P (q|q0, s, k ) """", 1."",null,null",null,null
119,"118,""We note that it is possible that multiple query suggestions have the same nal probability P (q|q0). We resolve ties using a deterministic algorithm, which orders query suggestions by length (favoring short queries) and then sorts them alphabetically."",null,null",null,null
120,"119,5 RESULTS,null,null",null,null
121,"120,In this section we present our experimental setup and results.,null,null",null,null
122,"121,5.1 Experimental Setup,null,null",null,null
123,"122,""We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20]. ese contain 34 and 50 queries with relevance judgments, respectively. We report on the o cial evaluation metrics used at the TREC Tasks track, which are ERR-IA@20 and -NDCG@20. In accordance with the track's se ings, we use ERR-IA@20 as our primary metric. (For simplicity, we omit mentioning the cut-o rank of 20 in all the table headers.) We noticed that in the ground truth the initial query itself has been judged as a highly relevant suggestion in numerous cases. We removed these cases, as they make li le sense for the envisioned scenario; we note that this leads to a drop in absolute terms of performance. We report on statistical signi cance using a two-tailed paired t-test at p < 0.05 and p < 0.001, denoted by  and , respectively."",null,null",null,null
124,"123,""In a series of experiments, we evaluate each component of our approach, in a bo om-up fashion. For each query set, we pick the"",null,null",null,null
125,"124,""con guration that performed best on that query set, which is an idealized scenario. Note that our focus is not on absolute performance"",null,null",null,null
126,"125,""gures, but on answering the following research questions:"",null,null",null,null
127,"126,RQ1 What are the most useful information sources? RQ2 What are e ective ways of (i) estimating the importance,null,null",null,null
128,"127,of documents and (ii) generating query suggestions from keyphrases? RQ3 Are our ndings consistent across the two query sets?,null,null",null,null
129,"128,5.2 ery Suggestion Generation,null,null",null,null
130,"129,""We start our experiments by focusing on the generation of query suggestions and compare the two methods described in §4.5. e document importance is set to be uniform. We report performance separately for each of the four source types S (that is, we set P (s) uniformly among sources s  S and set P (s) ,"""" 0 for s S). Table 1 presents the results. It is clear that, with a single exception (2015 WH), it is be er to use the raw keyphrases, without any expansion. e di erences are signi cant on the 2016 query set for all source types but QS. Regarding the comparison of di erent source types, we nd that QS > WS > WD > WH on the 2016 query set, meanwhile for 2015, the order is WS > WD > QS, WH."""""",null,null",null,null
131,"130,5.3 Document Importance,null,null",null,null
132,"131,""Next, we compare the two document importance estimator methods, uniform and rank-based decay (cf. §4.3), for each source type. Table 2 reports the results. We nd that rank-based document importance is bene cial for the query suggestion (QS) source types, for both years, and for WikiHow (WH) on the 2015 topics. However, the di erences are only signi cant for QS 2015. For all other source types, the uniform se ing performs be er."",null,null",null,null
133,"132,""We also compare performance across the 10 individual sources. Figure 3 shows the results, in terms of ERR-IA@20, using the uniform estimator. We observe a very similar pa ern using the rankbased estimator (which is not included due to space constraints). On the 2016 query set, the individual sources follow the exact same pa erns as their respective types (i.e., QS > WS > WD > WH), with one exception. e Bing API returned an empty set of search suggestions for many queries, hence the low performance of QSBin . We can observe a similar pa ern on the 2015 topics, with the exception of sources of type QS, which are the least e ective here."",null,null",null,null
134,"133,1155,null,null",null,null
135,"134,Short Research Paper,null,null",null,null
136,"135,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
137,"136,ERR-IA@20,null,null",null,null
138,"137,0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00,null,null",null,null
139,"138,2015 Query set 2016 Query set,null,null",null,null
140,"139,QS,null,null",null,null
141,"140,Google,null,null",null,null
142,"141,QS,null,null",null,null
143,"142,DDG,null,null",null,null
144,"143,WS,null,null",null,null
145,"144,Bing,null,null",null,null
146,"145,WS,null,null",null,null
147,"146,Google,null,null",null,null
148,"147,WS,null,null",null,null
149,"148,DDG,null,null",null,null
150,"149,WD,null,null",null,null
151,"150,Bing,null,null",null,null
152,"151,WD,null,null",null,null
153,"152,Google,null,null",null,null
154,"153,WD,null,null",null,null
155,"154,DDG,null,null",null,null
156,"155,WH QS,null,null",null,null
157,"156,Bing,null,null",null,null
158,"157,""Figure 3: Performance of individual sources, sorted by performance on the 2016 query set."",null,null",null,null
159,"158,5.4 Source Importance,null,null",null,null
160,"159,""Finally, we combine query suggestions across di erent sources; for that, we need to set the importance of each source. We consider three di erent strategies for se ing P (s): (i) uniformly; (ii) proportional to the importance of the corresponding source type (QS, WS, WD, and WH) from the previous step (cf. Table 2); (iii) proportional to the importance of the individual source (cf. Figure 3). e results are presented in Table 3. Firstly, we observe that the combination of sources performs be er than any individual source type on its own. As for se ing source importance, on the 2015 query set we"",null,null",null,null
161,"160,""nd that (iii) delivers the best results, which is in line with our expectations. On the 2016 query set, only minor di erences are observed between the three methods, none of which are signi cant."",null,null",null,null
162,"161,5.5 Summary of Findings,null,null",null,null
163,"162,""(RQ1) ery suggestions provided by major web search engines are unequivocally the most useful information source on the 2016 queries. We presume that these search engine suggestions are already diversi ed, which we can directly bene t from for our task."",null,null",null,null
164,"163,""ese are followed, in order, by keyphrases extracted from (i) web search snippets, (ii) web search results, i.e., full documents, and (iii) WikiHow articles. On the 2015 query set, query suggestions proved much less e ective; see RQ3 below. (RQ2) With a single exception, using the raw keyphrases, as-is, performs be er than expanding them by taking the original query into account. For web query suggestions it is bene cial to consider the rank order of suggestions, while for web search snippets and documents the uniform se ing performs be er. For WikiHow, it varies across query sets. (RQ3) Our main observations are consistent across the 2015 and 2016 query sets, regarding documents importance estimation and suggestions generation methods. It is worth noting that some of our methods were o cially submi ed to TREC 2016 [7] and were included in the assessment pools. is is not the case for 2015, where many of our query suggestions are missing relevance assessments (and, thus, are considered irrelevant). is might explain the low performance of QS sources on the 2015 queries."",null,null",null,null
165,"164,6 CONCLUSIONS,null,null",null,null
166,"165,""In this paper, we have addressed the task of generating query suggestions that can assist users in completing their tasks. We have proposed a probabilistic generative framework with four components:"",null,null",null,null
167,"166,Table 3: Combination of all sources using di erent source importance estimators. Signi cance is tested against the uniform setting (line 1).,null,null",null,null
168,"167,P (s),null,null",null,null
169,"168,Uniform Source-type Individual,null,null",null,null
170,"169,2015 ERR-IA -NDCG,null,null",null,null
171,"170,0.2219 0.2835 0.2381 0.2905 0.2518 0.3064,null,null",null,null
172,"171,2016 ERR-IA -NDCG,null,null",null,null
173,"172,0.4561 0.4570 0.4562,null,null",null,null
174,"173,0.5793 0.5832 0.5832,null,null",null,null
175,"174,""source importance, document importance, keyphrase relevance,"",null,null",null,null
176,"175,and query suggestions. We have proposed and experimentally,null,null",null,null
177,"176,compared various alternatives for these components.,null,null",null,null
178,"177,""One important element, missing from our current model, is the"",null,null",null,null
179,"178,""representation of speci c subtasks. As a next step, we plan to cluster"",null,null",null,null
180,"179,query suggestions together that belong to the same subtask. is,null,null",null,null
181,"180,would naturally enable us to provide diversi ed query suggestions.,null,null",null,null
182,"181,REFERENCES,null,null",null,null
183,"182,""[1] Salvatore Andolina, Khalil Klouche, Jaakko Peltonen, Mohammad E. Hoque, Tuukka Ruotsalo, Diogo Cabral, Arto Klami, Dorota Glowacka, Patrik Flore´en, and Giulio Jacucci. 2015. IntentStreams: Smart Parallel Search Streams for Branching Exploratory Search. In Proc. of IUI. 300­305."",null,null",null,null
184,"183,""[2] Ahmed H. Awadallah, Ryen W. White, Patrick Pantel, Susan T. Dumais, and YiMin Wang. 2014. Supporting Complex Search Tasks. In Proc. of CIKM. 829­838."",null,null",null,null
185,"184,[3] Krisztian Balog. 2015. Task-completion Engines: A Vision with a Plan. In Proc. of the 1st International Workshop on Supporting Complex Search Tasks.,null,null",null,null
186,"185,""[4] Sumit Bhatia, Debapriyo Majumdar, and Prasenjit Mitra. 2011. ery Suggestions in the Absence of ery Logs. In Proc. of SIGIR. 795­804."",null,null",null,null
187,"186,""[5] Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. e ery- ow Graph: Model and Applications. In Proc. of CIKM. 609­618."",null,null",null,null
188,"187,[6] Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In Proc. of SIGIR. 239­246.,null,null",null,null
189,"188,[7] Dar´io Gariglio i and Krisztian Balog. 2016. e University of Stavanger at the TREC 2016 Tasks Track. In Proc. of TREC.,null,null",null,null
190,"189,""[8] Ma hias Hagen, Steve Go¨ring, Magdalena Keil, Olaoluwa Anifowose, Amir Othman, and Benno Stein. 2015. Webis at TREC 2015: Tasks and Total Recall Tracks. In Proc. of TREC."",null,null",null,null
191,"190,""[9] Ma hias Hagen, Johannes Kiesel, Payam Adineh, Masoud Alahyari, Ehsan Fatehifar, Arafeh Bahrami, Pia Fichtl, and Benno Stein. 2016. Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks. In Proc. of TREC."",null,null",null,null
192,"191,""[10] Jiepu Jiang, Daqing He, Shuguang Han, Zhen Yue, and Chaoqun Ni. 2012. Contextual Evaluation of ery Reformulations in a Search Session by User Simulation. In Proc. of CIKM. 2635­2638."",null,null",null,null
193,"192,""[11] Diane Kelly, Jaime Arguello, and Robert Capra. 2013. NSF Workshop on Taskbased Information Search Systems. SIGIR Forum 47, 2 (2013), 116­127."",null,null",null,null
194,"193,""[12] Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009. A Comparison of ery and Term Suggestion Features for Interactive Searching. In Proc. of SIGIR. 371­378."",null,null",null,null
195,"194,""[13] Udo Kruschwitz, Deirdre Lungley, M-Dyaa Albakour, and Dawei Song. 2013. Deriving query suggestions for site search. JASIST 64, 10 (2013), 1975­1994."",null,null",null,null
196,"195,""[14] Claudio Lucchese, Salvatore Orlando, Ra aele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2013. Discovering Tasks from Search Engine ery Logs. ACM Trans. Inf. Syst. 31, 3, Article 14 (2013), 43 pages."",null,null",null,null
197,"196,[15] Alyona Medelyan. 2015. Modi ed RAKE algorithm. h ps://github.com/zelandiya/ RAKE-tutorial. (2015). Accessed: 2017-01-23.,null,null",null,null
198,"197,""[16] Umut Ozertem, Olivier Chapelle, Pinar Donmez, and Emre Velipasaoglu. 2012. Learning to Suggest: A Machine Learning Framework for Ranking ery Suggestions. In Proc. of SIGIR. 25­34."",null,null",null,null
199,"198,""[17] Tuan A. Tran, Sven Schwarz, Claudia Niedere´e, Heiko Maus, and Na iya Kanhabua. 2016. e Forgo en Needle in My Collections: Task-Aware Ranking of Documents in Semantic Information Space. In Proc. of CHIIR. 13­22."",null,null",null,null
200,"199,""[18] Manisha Verma, Evangelos Kanoulas, Emine Yilmaz, Rishabh Mehrotra, Ben Cartere e, Nick Craswell, and Peter Bailey. 2016. Overview of the TREC Tasks Track 2016. In Proc. of TREC."",null,null",null,null
201,"200,[19] Ryen W. White and Je Huang. 2010. Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs. In Proc. of SIGIR. 587­594.,null,null",null,null
202,"201,""[20] Emine Yilmaz, Manisha Verma, Rishabh Mehrotra, Evangelos Kanoulas, Ben Cartere e, and Nick Craswell. 2015. Overview of the TREC 2015 Tasks Track. In Proc. of TREC."",null,null",null,null
203,"202,1156,null,null",null,null
204,"203,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Neural Network based Reinforcement Learning for Real-time Pushing on Text Stream,null,null",null,null
4,"3,Haihui Tan,null,null",null,null
5,"4,The Hong Kong Polytechnic University,null,null",null,null
6,"5,""Kowloon, Hong Kong tanhaihui92@gmail.com"",null,null",null,null
7,"6,Ziyu Lu,null,null",null,null
8,"7,The Hong Kong Polytechnic University,null,null",null,null
9,"8,""Kowloon, Hong Kong luziyuhku@gmail.com"",null,null",null,null
10,"9,Wenjie Li,null,null",null,null
11,"10,The Hong Kong Polytechnic University,null,null",null,null
12,"11,""Kowloon, Hong Kong cswjli@comp.polyu.edu.hk"",null,null",null,null
13,"12,ABSTRACT,null,null",null,null
14,"13,""The massive amount of noisy and redundant information in text streams makes it a challenge for users to acquire timely and relevant information in social media. Real-time noti cation pushing on text stream is of practical importance. In this paper, we formulate the real-time pushing on text stream as a sequential decision making problem and propose a Neural Network based Reinforcement Learning (NNRL) algorithm for real-time decision making, e.g., push or skip the incoming text, with considering both history dependencies and future uncertainty. A novel Q-Network which contains a Long Short Term Memory (LSTM) layer and three fully connected neural network layers is designed to maximize the long-term rewards. Experiment results on the real data from TREC 2016 Real-time Summarization track show that our algorithm signi cantly outperforms state-of-the-art methods."",null,null",null,null
15,"14,CCS CONCEPTS,null,null",null,null
16,"15,· Information systems  Document ltering;,null,null",null,null
17,"16,KEYWORDS,null,null",null,null
18,"17,""Text Stream, Real-Time Pushing, Deep Reinforcement Learning"",null,null",null,null
19,"18,1 INTRODUCTION,null,null",null,null
20,"19,""With the development of social media, text streams such as Twitter posts, news articles or user reviews are being generated in fastgrowing volumes. The explosive amount of noisy and redundant streaming texts are overwhelming and makes it di cult to nd useful information. Pushing or Summarizing the real-time streaming text for users is of practical importance. In recent decades, the real-time pushing on text streams has attracted increasing attention at Text Retrieve Conferences (TREC), e.g., the Microblog track in 2015 [4] and the Real-time Summarization track in 2016 [6]. For example, [9] which has achieved the best performance for the real-time ltering task at TREC 2015 Microblog track, explored dynamic emission strategies to maintain appropriate thresholds for pushing relevant tweets. Fan et al. [2] proposed an adaptive evolutionary ltering framework to lter interesting tweets from"",null,null",null,null
21,"20,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: http://dx.doi.org/10.1145/3077136.3080677"",null,null",null,null
22,"21,""the Twitter stream with respect to user interest pro les. However, pushing real-time text streams is not a short-term process. It is a dynamic forward decision process in which the current action will a ect further decisions (dependency) and further streaming texts generate uncertainty on the current decision. [5] treated real-time stream summarization as a sequential decision making problem and adopted a locally optimal learning to search algorithm to learn a policy for selecting or skipping a sentence in the text stream, by imitating a heuristic reference policy. However, the heuristic reference policy is di cult to construct for real-time streaming text and it needs massive human interventions. The real-time decision process needs to maximize the long-term rewards by taking both history dependencies and future uncertainty into consideration."",null,null",null,null
23,"22,""In this paper, we de ne the real-time pushing on text stream as a long-term optimization problem and propose a neural network based reinforcement learning algorithm as the solution. A novel Q-Network is de ned to learn a long-term policy for sequential decision making. When receiving text streams, the Q-network predicts the values for taking each action under the current state and chooses the action with the higher value. A long short-term memory (LSTM) layer is integrated into the Q-Network to represent the high-level abstraction of streaming text by considering both semantic and temporal dependencies of previously pushed texts. The Q-Network is continuously updated according to the observation of interrelationship between actions and rewards, and a long-term policy is explored and exploited to make real-time decisions on text stream. The main contributions of our paper is as follows:"",null,null",null,null
24,"23,""· We formulate real-time pushing on text streams as a longterm optimization problem, by considering both history dependencies and future uncertainty of text stream."",null,null",null,null
25,"24,· A Neural Network based Reinforcement learning (NNRL) algorithm is proposed to maximize long-term rewards and obtain the long-term policy for real-time decision making.,null,null",null,null
26,"25,· Experimental evaluations on the real tweet stream show that our method has superior performance over the stateof-the-art methods.,null,null",null,null
27,"26,2 METHOD,null,null",null,null
28,"27,""Problem De nition: Given a text stream S ,"""" {t1, t2, · · · , tn, · · · }, Our model makes real-time decisions from an action set A """","""" {ap , as } for each incoming text t. ap is to push t while as is to skip it. As our target is to maximize the long-term rewards, we adopt a reinforcement learning framework to de ne and solve this problem."""""",null,null",null,null
29,"28,""In reinforcement learning methods, the agent model selects an action from the action set A and passes it to the environment  which changes its inner state s and the reward score r . Given the"",null,null",null,null
30,"29,913,null,null",null,null
31,"30,Short Research Paper,null,null",null,null
32,"31,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
33,"32,""state st ,"""" [x1, a1, x2, a2, · · · , at -1, xt ] which is a sequence of observations and actions a, the model arrives at an optimal action-value function Q(s, a) which is the maximum expected return with a"""""",null,null",null,null
34,"33,policy  .,null,null",null,null
35,"34,2.1 Neural Network-based Reinforcement Learning (NNRL),null,null",null,null
36,"35,""In our problem, x is the feature representation of a text t, the action set A ,"""" {ap , as }. Similar to Deep Q-Network [8], we propose a"""""",null,null",null,null
37,"36,neural network-based reinforcement learning (NNRL) algorithm,null,null",null,null
38,"37,""where a neural network approximation function as Q-Network is used to estimate the action-value function Q(s, a,  ) ,"""" Q(s, a).  is"""""",null,null",null,null
39,"38,the weight parameters in Q-Network. The Q-Network is trained by minimizing the loss function L( ) as Equation 1.,null,null",null,null
40,"39,""L( ) ,"""" Es,a(s,a)[ - Q(s, a;  )]2"""""",null,null",null,null
41,"40,(1),null,null",null,null
42,"41,""in which ,"""" Es [r + maxa Q(s , a ;  )] is the target and (s, a) is the behavior distribution over s and a."""""",null,null",null,null
43,"42,Algorithm 1 outlines our proposed neural network-based rein-,null,null",null,null
44,"43,forcement learning (NNRL) algorithm. N is the number of iterations,null,null",null,null
45,"44,Algorithm 1 N,null,null",null,null
46,"45,N,null,null",null,null
47,"46,R,null,null",null,null
48,"47,L,null,null",null,null
49,"48,1: Initialize  in the action-value function Q,null,null",null,null
50,"49,""2: for n ,"""" 1, 2, · · · , N do"""""",null,null",null,null
51,"50,""3: for i ,"""" 1, 2, · · · , M do"""""",null,null",null,null
52,"51,4:,null,null",null,null
53,"52,""Si ,"""" [t1, t2, · · · , tn, · · · ]"""""",null,null",null,null
54,"53,5:,null,null",null,null
55,"54,""initialize s1 ,"""" {X^1 }, X^1 is appended representation for x1"""""",null,null",null,null
56,"55,6:,null,null",null,null
57,"56,""for j ,"""" 1, 2, · · · , |S | do"""""",null,null",null,null
58,"57,7:,null,null",null,null
59,"58,""Compute: Q (sj, ap ;  ) and Q (sj, as ;  )"",null,null",null,null
60,"59,8:,null,null",null,null
61,"60,Action: Randomly select an action for aj with  probability;,null,null",null,null
62,"61,""Otherwise aj ,"""" maxa A(Q (sj, a ;  )). Execute aj ."""""",null,null",null,null
63,"62,9:,null,null",null,null
64,"63,Observe: reward rj and xj+1; generate the appended,null,null",null,null
65,"64,representation X^ j+1,null,null",null,null
66,"65,"","",null,null",null,null
67,"66,[X^,null,null",null,null
68,"67,(k j,null,null",null,null
69,"68,""),"",null,null",null,null
70,"69,x,null,null",null,null
71,"70,j,null,null",null,null
72,"71,+1,null,null",null,null
73,"72,];,null,null",null,null
74,"73,set,null,null",null,null
75,"74,state,null,null",null,null
76,"75,s j +1,null,null",null,null
77,"76,"","",null,null",null,null
78,"77,""[sj, aj, X^ j+1]."",null,null",null,null
79,"78,10:,null,null",null,null
80,"79,Update Q-Network by performing a gradient descent step,null,null",null,null
81,"80,on the loss function in Equation 1.,null,null",null,null
82,"81,11:,null,null",null,null
83,"82,end for,null,null",null,null
84,"83,12: end for,null,null",null,null
85,"84,13: end for,null,null",null,null
86,"85,and M is the number of text streams (text episodes). Each text stream,null,null",null,null
87,"86,""for one topic, e.g. S ,"""" {t1, t2, · · · , tn, · · · }, has one episode training. |S | is the number of texts in each text episode S. Firstly, it initializes the start state s1 and the (high-level) representation X^1. For each text t in the episode, it executes four steps. In the rst step, compute Q function values for two actions in A (push or skip) (Line 7). They are Q(sj , ap ;  ) and Q(sj , as ;  ) respectively. At the action step (line 8): with  probability, a random action is chosen for aj ; otherwise, aj """","""" maxa A(Q(sj , a ;  )). Then execute aj . In the third step (line 9), observe the reward rj 1and xj+1 ; generate the representation X^j+1 """","""" [X^j(k), xj+1] by appending the text feature vector xj+1 and the last k text feature vector from previously selected texts in X^j ; set the next state sj+1 """","""" [sj , aj , X^j+1]. Finally, update the Q-Network by performing a gradient descent step [1]"""""",null,null",null,null
88,"87,""1we use expected gain (EG) as the reward. Only when the current text is the terminal of this episode, r equal to the EG value. Otherwise r is zero."",null,null",null,null
89,"88,on the loss function de ned in Equation 1 (line 10). The target j is set as Equation 2:,null,null",null,null
90,"89,""j,"",null,null",null,null
91,"90,""rj rj +  maxa Q(sj+1, aj ; i )"",null,null",null,null
92,"91,terminal non - terminal,null,null",null,null
93,"92,(2),null,null",null,null
94,"93,""If it is the terminal of the episode, j equals to the reward r . Oth-"",null,null",null,null
95,"94,""erwise, j equals to rj +  maxa Q(sj+1, aj ; i ).  is the discounted"",null,null",null,null
96,"95,""factor. After training, the Q-network is a neural network approxi-"",null,null",null,null
97,"96,mation function for real-time decision making.,null,null",null,null
98,"97,2.2 Q-Network,null,null",null,null
99,"98,Q value for push Q value for skip fully connected fully connected fully connected,null,null",null,null
100,"99,A sequence of vectors,null,null",null,null
101,"100,LSTM Q-Network,null,null",null,null
102,"101,Tweet Skip,null,null",null,null
103,"102,Tweet Push,null,null",null,null
104,"103,Tweet ... ... Skip Timeline,null,null",null,null
105,"104,Tweet Push,null,null",null,null
106,"105,Tweet Push,null,null",null,null
107,"106,Tweet,null,null",null,null
108,"107,?,null,null",null,null
109,"108,Now,null,null",null,null
110,"109,... ...,null,null",null,null
111,"110,Figure 1: Q-Network,null,null",null,null
112,"111,""In this section, we demonstrate the design of our proposed Q-"",null,null",null,null
113,"112,Network. Figure 1 shows the architecture of our Q-Network which,null,null",null,null
114,"113,consists of four layers. The rst layer is a single Long Short Term,null,null",null,null
115,"114,Memory (LSTM) [3] layer which generates the high-level abstrac-,null,null",null,null
116,"115,tion of the input sequence. Both semantic and temporal dependen-,null,null",null,null
117,"116,cies of the text stream are considered into LSTM by taking the last k,null,null",null,null
118,"117,previously selected text feature representation. Therefore the input,null,null",null,null
119,"118,""for LSTM is the representation X^ ,"""" [x1, x2, · · · , xT ]. T is the length of X^ . LSTM computes the hidden vector H """","""" [h1, h2, · · · , hT ] by iterating the following equations:"""""",null,null",null,null
120,"119,""it ,  (Wxi xt + Whiht -1 + Wcict -1 + bi ) ft ,  (Wx f xt + Whf ht -1 + Wcf ct -1 + bf ) ct , ft ct -1 + it tanh(Wxc xt + Whcht -1 + bc ) ot ,  (Wxoxt + Whoht -1 + Wco + bo ) ht , ot tanh(ct )"",null,null",null,null
121,"120,""in which  is the logistic sigmoid function, and i, f , o and c are"",null,null",null,null
122,"121,""respectively the input gate, forget gate, output gate and cell activa-"",null,null",null,null
123,"122,""tion vectors, all of which are the same size as the hidden vector h. W terms denote weight matrices, e.g. Wxi is the input-input gate weight matrix and Whi is the hidden-input gate matrix. The b terms denote bias vectors, e.g. bh is the hidden bias vector."",null,null",null,null
124,"123,LSTM is connected with three fully connected neural networks.,null,null",null,null
125,"124,The output of LSTM hT (the last item in the hidden vector H ) acts,null,null",null,null
126,"125,as the input for the later fully connected neural network layers.,null,null",null,null
127,"126,Each fully connected neural network can be formated as follows:,null,null",null,null
128,"127,""Zi , ^iWi ;"",null,null",null,null
129,"128,""^i+1 , F (Zi )"",null,null",null,null
130,"129,914,null,null",null,null
131,"130,Short Research Paper,null,null",null,null
132,"131,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
133,"132,""where each output of previous (the i layer) layer ^ will be the input of the later layer (the i + 1 layer). Here, i indexes from 1 to 3. ^1 is the output of LSTM hT . W are the weight matrix of each fully connected neural network. F is the activation function and for the previous two fully connected neural networks, the activation function is ReLU and we use a linear activation function for the last fully connected neural network."",null,null",null,null
134,"133,3 EXPERIMENTS 3.1 Dataset,null,null",null,null
135,"134,""We use TREC 2016 Microblog Track 2 as data set to evaluate our proposed approach. The dataset consists of the Twitter's sample tweet streams ( approximately 1% of public tweets) during the o cial evaluation period from August 2, 2016 to August 11, 2016. 56 judged interest pro les (topics) of tweets have been given. Each interest pro le (topic) consists of title, description and narrative description. Each sample tweet has a given label for its corresponding topic: highly relevant, relevant and non-relevant. We pre-process the dataset by removing the non-English tweets and tweets which have fewer than 5 words. Also we lter out very irrelevant tweets, e.g. tweets which have no interest pro le keywords. After preprocessing, there is a total of 57,419 tweets for 56 topics. Each topic has a tweet stream. The average number of tweets per topic is about 96.40. We randomly choose 85% of topics (48 topics) of tweets as training set and the remaining as test set."",null,null",null,null
136,"135,Table 1: Features used for basic representation,null,null",null,null
137,"136,Features Name,null,null",null,null
138,"137,Description,null,null",null,null
139,"138,Statistic Temporal Semantic,null,null",null,null
140,"139,N_{title} N_{Narr} N_{exp} N_{word} N_{hashtag} Time,null,null",null,null
141,"140,Time_interval,null,null",null,null
142,"141,Is_redundant is_link Relevance score Cosine Score Text vector,null,null",null,null
143,"142,""Number of terms in """"Title"""" Number of terms in """"Narrative"""" Number of terms in """"Description"""" Number of words in tweet text Number of hashtag in tweet text the current time in one day (ms) time interval between the incoming tweet and the last selected tweet Flag about whether it is redundant Flag about whether a URL exists. SVM regression score Cosine similarity score Word embedding representation"",null,null",null,null
144,"143,3.2 Features,null,null",null,null
145,"144,""Before we input the tweet stream into the Q-Network, we rstly extract some features as the basic presentation x for each tweet. The extracted features are listed in Table 1. There are three groups: respectively statistic features, temporal features and semantic features. For each tweet, we compute some statistics as features, including number of hashtag, number of terms and number of terms appearing in the title, narrative description and description. Also, two temporal features are extracted. They are the time in one day (milliseconds since 00:00 of one day) and the time interval between this tweet and the last selected tweet. As semantic features, we"",null,null",null,null
146,"145,2 http://trecrts.github.io/,null,null",null,null
147,"146,""generate a text vector (dimension,""""300) for a tweet by averaging the word embedding vectors in a tweet text. The word embedding representation is obtained from a pre-trained Google News corpus word vector model [7]. And we compute the relevance score from a SVM regression for each topic and the cosine similarity score. Also we use two boolean ags (is_link and is_redudant) about whether URL exists or it is redundant. We normalize all features, and concatenate them into one single vector as the basic presentation x for each tweet. The dimension of the basic presentation is 311."""""",null,null",null,null
148,"147,3.3 Evaluation Metric,null,null",null,null
149,"148,We use two TREC 2016 Real-time Summarization Track o cial evaluation metrics. They are expected gain (EG) and Normalized Cumulative Gain (nCG). The expected gain (EG) is de ned:,null,null",null,null
150,"149,EG,null,null",null,null
151,"150,"","",null,null",null,null
152,"151,1 N,null,null",null,null
153,"152,G (t ),null,null",null,null
154,"153,where N is the number of tweets returned and G(t) is the gain of,null,null",null,null
155,"154,each tweet: Not relevant tweets receive a gain of 0; relevant tweets,null,null",null,null
156,"155,receive a gain of 0.5; Highly-relevant tweets receive a gain of 1.0.,null,null",null,null
157,"156,Normalized Cumulative Gain (nCG) is de ned:,null,null",null,null
158,"157,nCG,null,null",null,null
159,"158,"","",null,null",null,null
160,"159,1 Z,null,null",null,null
161,"160,G (t ),null,null",null,null
162,"161,""where Z is the maximum possible gain (given the ten tweet per day limit). In order to di erentiate the performances in salient days when there are no relevant tweets, there are two variants for each metric, respectively EG-0, EG-1, nCG-0 and nCG-1. For salient days, the EG-1 and nCG-1 gives a score of one if not pushing any tweets, or zero otherwise. In the EG-0 and nCG-0 variants, for a silent day, the gain is zero [9]."",null,null",null,null
163,"162,3.4 Compared Methods,null,null",null,null
164,"163,""We compared the following methods: · Query similarity (QS): it pushes the tweet whose relevance score is higher than a xed threshold . Cosine similarity is used to measure the relevance score between the topic title and the tweet text. · YoGosling [9]: implements simple dynamic emission strategies to maintain appropriate dynamic thresholds for pushing updates. It achieved the best performance at the TREC 2015 Microblog Track Real-time Filtering task [4]. · Features+StaticThreshold (FST) [6]:It develops a relevance estimation model based on both lexical and non-lexical features, and set a static threshold to push tweets with their manual observation. It achieved the best performance at the TREC 2016 Real-time Summarization track [6]. · NNRL: This is our proposed algorithm in Section 2."",null,null",null,null
165,"164,3.5 Implementation Setting,null,null",null,null
166,"165,""In our algorithm, the learning rate for gradient descent is 0.001 and the discount factor is 1. The  is annealed linearly from 1 to 0, then x it at 0 until converging to a suboptimal policy. Then  is annealed linearly from 0.1 to 0; x at 0 until converging to another suboptimal. Repeat the  exploration step thereafter. In our Q-network, the hidden state size of LSTM is 512. The size of appended sequence from the previously selected text sequence k is"",null,null",null,null
167,"166,915,null,null",null,null
168,"167,Short Research Paper,null,null",null,null
169,"168,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
170,"169,up to 10 due to LSTM training e ciency. Both the rst and second fully-connected neural network layer have 256 hidden units. The output layer is a fully-connected linear layer with a single output for each valid action.,null,null",null,null
171,"170,EG-0 EG-1,null,null",null,null
172,"171,0.05 EG-0 performance over different thresholds,null,null",null,null
173,"172,0.04 0.03,null,null",null,null
174,"173,Max: 0.0362,null,null",null,null
175,"174,0.02,null,null",null,null
176,"175,0.01,null,null",null,null
177,"176,0.000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Threshold (®),null,null",null,null
178,"177,(a) EG-0,null,null",null,null
179,"178,0.30 EG-1 performance over different thresholds,null,null",null,null
180,"179,0.25,null,null",null,null
181,"180,Max: 0.2483,null,null",null,null
182,"181,0.20,null,null",null,null
183,"182,0.15,null,null",null,null
184,"183,0.10,null,null",null,null
185,"184,0.05,null,null",null,null
186,"185,0.000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Threshold (®),null,null",null,null
187,"186,(b) EG-1,null,null",null,null
188,"187,Figure 2: EG results with varying QS threshold ,null,null",null,null
189,"188,3.6 Experimental Results,null,null",null,null
190,"189,Table 2: Evaluation results for all compared methods,null,null",null,null
191,"190,""Method QS ( , 0.45) YoGosling FST NNRL"",null,null",null,null
192,"191,EG-1 0.2483 0.2289 0.2698 0.2816,null,null",null,null
193,"192,EG-0 0.0322 0.0253 0.0483 0.0691,null,null",null,null
194,"193,nCG-1 0.2325 0.0253 0.2909 0.2971,null,null",null,null
195,"194,nCG-0 0.0164 0.0253 0.0695 0.0846,null,null",null,null
196,"195,""Table 2 shows the evaluation results. We can see that our proposed NNRL outperforms the other competitors for all evaluation metrics. Our superior performances lie on that we format this problem as a sequential decision making process and deal with both history dependencies and future uncertainty while the other three compared methods mainly adopt the static or dynamic threshold to lter out the incoming tweet. Figure 2 shows the EG evaluation results (similar results with nCG; we omit it due to space limitation) with varying the threshold  of the QS method. There are big di erences in EG performance when using di erent thresholds. Therefore, static methods like QS and FST are inappropriate for the real-time environment which requires a dynamic and adaptive mechanism to consider future uncertainty. Although YoGosling proposed some strategies for obtaining dynamic threshold, it ignores future uncertainty. We use an exemplary case for topic """"Hiroshima atomic bomb"""" to demonstrate the adaptive ability of our method (NNLR) to address the potential future uncertainty. Table 3 shows a snippet of the tweet sequence of 7 tweets for topic """"Hiroshima atomic bomb"""" with each tweet's time, raw text and the selected action in our method. At the start of the tweet sequence, our method pushes the second tweet rather than the rst one because the second one is more speci c and relevant to the topic. Our method decides to skip the rst tweet and waits for the potential better tweet (e.g. the second one) in the future. After pushing the second one, it skips the following relevant but redundant tweets. When it has pushed some highly-relevant tweets and time elapses, the pushing condition might change as very few relevant tweets come. Therefore, it"",null,null",null,null
197,"196,""pushes the relevant one while the same text was skipped previously. For example, the rst tweet was skipped but the fourth tweet with the same content is pushed."",null,null",null,null
198,"197,""Table 3: An exemplary case for """"Hiroshima atomic bomb"""""",null,null",null,null
199,"198,Time Tweet Text,null,null",null,null
200,"199,Action,null,null",null,null
201,"200,08-02 04:21:18 08-09 17:10:15 08-09 17:15:11 08-09 21:59:15,null,null",null,null
202,"201,08-10 00:36:05,null,null",null,null
203,"202,08-10 02:14:43 08-11 03:41:12,null,null",null,null
204,"203,""Obama At Hiroshima: A World Without Nuclear Weapons ­ Ours - American Thinker RT @HistoryToLearn: Hiroshima, one year after the atomic bomb blast, 1946. RT @HistoryToLearn: Hiroshima, one year after the atomic bomb blast, 1946. Obama At #Hiroshima: A World Without Nuclear Weapons ­ Ours - American Thinker I dropped the bomb but now I have the proof so now I can drop the atomic bomb. I've never been so excited RT @AJENews: Nagasaki marks 71st anniversary of atomic bombing Nagasaki Marks 71st Anniversary of Atomic Bombing."",null,null",null,null
205,"204,Skip Push Skip Push Skip Skip Push,null,null",null,null
206,"205,4 CONCLUSIONS,null,null",null,null
207,"206,""In this paper, we propose a neural network based reinforcement"",null,null",null,null
208,"207,learning algorithm to address real-time pushing on text stream. A,null,null",null,null
209,"208,novel Q-Network is designed to approximate the maximum long-,null,null",null,null
210,"209,term rewards. Experiment results on real data from TREC 2016,null,null",null,null
211,"210,Real-time Summarization track demonstrate that our algorithm,null,null",null,null
212,"211,is superior to all compared methods for all the o cial evaluation,null,null",null,null
213,"212,metrics and has the ability to make real-time decisions. In future,null,null",null,null
214,"213,""work, we plan to study the case without speci c query topic."",null,null",null,null
215,"214,5 ACKNOWLEDGMENTS,null,null",null,null
216,"215,The work in this paper was supported by Research Grants Coun-,null,null",null,null
217,"216,""cil of Hong Kong (PolyU 152094/14E), National Natural Science"",null,null",null,null
218,"217,Foundation of China (61272291) and The Hong Kong Polytechnic,null,null",null,null
219,"218,University (4-BCB5).,null,null",null,null
220,"219,REFERENCES,null,null",null,null
221,"220,[1] Leemon Baird and Andrew Moore. Gradient Descent for General Reinforcement Learning. In NIPS'98.,null,null",null,null
222,"221,""[2] Feifan Fan, Yansong Feng, Lili Yao, and Dongyan Zhao. Adaptive Evolutionary Filtering in Real-Time Twitter Stream. In CIKM '16."",null,null",null,null
223,"222,""[3] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (1997), 1735­1780."",null,null",null,null
224,"223,""[4] Y. Wang G. Sherman J. Lin, M. Efron and E. Voorhees. Overview of the TREC-2015 Microblog Track. In TREC 2015."",null,null",null,null
225,"224,""[5] Chris Kedzie, Fernando Diaz, and Kathleen McKeown. Real-Time Web Scale Event Summarization Using Sequential Decision Making. In IJCAI '16."",null,null",null,null
226,"225,""[6] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees, and Fernando Diaz. Overview of the TREC-2016 Microblog Track. In TREC 2016."",null,null",null,null
227,"226,""[7] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Je rey Dean. Distributed Representations of Words and Phrases and Their Compositionality. In NIPS'13."",null,null",null,null
228,"227,""[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. CoRR (2013)."",null,null",null,null
229,"228,""[9] Luchen Tan, Adam Roegiest, Charles L.A. Clarke, and Jimmy Lin. Simple Dynamic Emission Strategies for Microblog Filtering. In SIGIR '16."",null,null",null,null
230,"229,916,null,null",null,null
231,"230,,null,null",null,null

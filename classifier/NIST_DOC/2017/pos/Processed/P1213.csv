,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Embedding-based ery Expansion for Weighted Sequential Dependence Retrieval Model,null,null",null,null
4,"3,Saeid Balaneshin-kordan,null,null",null,null
5,"4,Wayne State University saeid@wayne.edu,null,null",null,null
6,"5,ABSTRACT,null,null",null,null
7,"6,""Although information retrieval models based on Markov Random Fields (MRF), such as Sequential Dependence Model and Weighted Sequential Dependence Model (WSDM), have been shown to outperform bag-of-words probabilistic and language modeling retrieval models by taking into account term dependencies, it is not known how to e ectively account for term dependencies in query expansion methods based on pseudo-relevance feedback (PRF) for retrieval models of this type. In this paper, we propose Semantic Weighted Dependence Model (SWDM), a PRF based query expansion method for WSDM, which utilizes distributed low-dimensional word representations (i.e., word embeddings). Our method nds the closest unigrams to each query term in the embedding space and top retrieved documents and directly incorporates them into the retrieval function of WSDM. Experiments on TREC datasets indicate statistically signi cant improvement of SWDM over stateof-the-art MRF retrieval models, PRF methods for MRF retrieval models and embedding based query expansion methods for bagof-words retrieval models."",null,null",null,null
8,"7,CCS CONCEPTS,null,null",null,null
9,"8,·Information systems  ery reformulation;,null,null",null,null
10,"9,KEYWORDS,null,null",null,null
11,"10,Weighted Sequential Dependence Model; Term Dependencies; Word Embeddings; ery Expansion; Pseudo-Relevance Feedback,null,null",null,null
12,"11,1 INTRODUCTION,null,null",null,null
13,"12,""Designing retrieval models and addressing the problem of vocabulary mismatch via query and document expansion have traditionally been two orthogonal directions of information retrieval (IR) research. In particular, separate methods for query [4] or document [3] expansion are typically employed in conjunction with bag-of-words probabilistic, such as BM25 [15], and language modeling based, such as ery Likelihood [14], retrieval models. ese methods typically identify query expansion terms in the collection itself using statistical measures of semantic similarity between pairs of terms pre-computed in advance [2, 9], top-retrieved documents [11], external resources [10] or their combination [1]."",null,null",null,null
14,"13,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR 17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978­1­4503­5022­8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080764"",null,null",null,null
15,"14,Alexander Kotov,null,null",null,null
16,"15,Wayne State University kotov@wayne.edu,null,null",null,null
17,"16,""Markov Random Fields (MRF) retrieval models [12], such as Sequential Dependence Model (SDM) and Weighted Sequential Dependence Model (WSDM), consider retrieval function as a graph of dependencies between the query terms and a document and calculate document retrieval score as a linear combination of the potential functions de ned on the cliques in this graph. Although these retrieval models have been shown to outperform probabilistic and language modeling retrieval models by going beyond bagof-words assumption and taking into account term dependencies, it is not known how to e ectively incorporate term dependencies into query expansion methods based on pseudo-relevance feedback (PRF) for this type of retrieval models. Due to sparsity of n-grams, accounting for term dependencies in query expansion based only on term co-occurrence statistics within the collection itself is quite challenging. For this reason, only unigrams have been utilized for query expansion in Latent Concept Expansion (LCE) [13] and Parameterized ery Expansion (PQE) [6], state-ofthe-art PRF methods for SDM and WSDM, respectively."",null,null",null,null
18,"17,""Word embeddings are distributed low-dimensional vector representations, which have been successfully utilized in di erent IR contexts, such as estimation of translation models [3, 21] and query expansion for bag-of-words retrieval models [19], as well as in retrieval models based on neural architectures [7, 16] and proximity search [8]. Since n-grams typically appear in a limited number of contexts in a collection, the utility of n-gram embeddings for IR is limited. For example, based on the word embeddings trained on a Google News corpus with 100 billion words1, the most semantically similar words to """"human"""" are """"humankind"""", """"mankind"""" and """"humanity"""", all of which can be good query expansion terms."",null,null",null,null
19,"18,""e most semantically similar n-grams to """"human"""", however, are """"human beings"""", """"impertinent amboyant endearingly"""", """"employee Laura Althouse"""", and """"Christine Gaugler head""""2. It is obvious that these n-grams would only cause topic dri and degrade the retrieval results, if used for query expansion. Furthermore, due to sparsity, bigram embeddings are also ine ective for computing their importance weight in SDM [20]."",null,null",null,null
20,"19,""Our proposed model, Semantic Weighted Dependence Model (SWDM), mitigates the potential vocabulary mismatch between queries and documents in WSDM via query expansion. Similar to SDM and WSDM, the retrieval score of a document, according to SWDM, depends on the matching query unigrams, ordered and unordered bigrams. However, unlike SDM, SWDM nds the closest unigrams and bigrams to query terms in embedding space and directly incorporates them into the retrieval function of WSDM."",null,null",null,null
21,"20,""1h ps://drive.google.com/ le/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/ 2 e similarity of trigrams """"employee Laura Althouse"""" and """"Christine Gaugler head"""" was deduced from the fragments """". . . said Christine Gaugler, head of human resources . . . """" and """". . . and human resources employee Laura Althouse . . . """", which appear in multiple places within this corpus."",null,null",null,null
22,"21,1213,null,null",null,null
23,"22,Short Research Paper,null,null",null,null
24,"23,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
25,"24,""To overcome the n-gram sparsity problem, SWDM takes into account only the dependencies between those pairs of terms, in which at least one term is semantically similar to one of the query terms and both terms have appeared within multiple windows in the collection and top retrieved documents. For example, for the query """"human smuggling"""", SWDM identi es """"tra cking"""" as one of the expansion unigrams and """"human tra cking"""" as one of the expansion bigrams, since the unigram """"tra cking"""" is semantically similar to """"smuggling"""". Proximity to query terms in the embedding space as well as their frequency in the collection and top-retrieved documents are used as features for weighting the importance of original and expansion concepts (unigrams or bigrams)."",null,null",null,null
26,"25,2 RELATED WORK,null,null",null,null
27,"26,""Word embeddings are typically utilized in retrieval models to calculate distributional similarity between terms, quantify relevance of documents to queries or as an input to neural architectures for relevance matching."",null,null",null,null
28,"27,""Calculation of distributional similarity: cosine similarity between word embeddings is used in two scenarios. In the rst scenario, distributional similarity between word vectors is used to"",null,null",null,null
29,"28,""nd semantically similar words to expand queries [19] or documents [3, 21]. Components of a di erence vector between embeddings of query unigrams and embeddings of the entire query have been utilized as features to estimate the weights of query unigrams in SDM [20]. Relevance of documents to queries can be quanti ed by aggregating cosine similarity scores between pairs of the most similar query and document term embedding vectors [8]. An alternative approach to quantifying relevance of documents to queries involves aggregating the embeddings of all document and query words into a single embedding vector for the entire document and a single embedding vector for the entire query and calculating their relevance score as a cosine similarity between these vectors [17]."",null,null",null,null
30,"29,""e proposed method also falls into this category, since it relies on cosine similarity between word embeddings to nd the most semantically similar unigrams to query terms and uses these unigrams for query expansion."",null,null",null,null
31,"30,""Input to neural architectures: distributed representations of query and document terms have also been used as input to neural architectures based on Convolutional Neural Network [16] or Recurrent Neural Network [18], which directly calculate the relevance score of documents to queries. In [7], a histogram of cosine similarities between embeddings of a query and documents terms is used an input to a feed-forward neural network with term gating, which directly computes the relevance score."",null,null",null,null
32,"31,3 METHOD,null,null",null,null
33,"32,Retrieval function of SDM calculates the relevance score of document D to query Q as follows:,null,null",null,null
34,"33,P,null,null",null,null
35,"34,(D,null,null",null,null
36,"35,|Q,null,null",null,null
37,"36,),null,null",null,null
38,"37,r,null,null",null,null
39,"38,ank,null,null",null,null
40,"39,"","",null,null",null,null
41,"40,""T fT (qi , D) +"",null,null",null,null
42,"41,""B fB (qiqi+1, D)+"",null,null",null,null
43,"42,qi Q,null,null",null,null
44,"43,""qi ,qi+1 Q"",null,null",null,null
45,"44,""U fU (qiqi+1, D)"",null,null",null,null
46,"45,(1),null,null",null,null
47,"46,""qi ,qi+1 Q"",null,null",null,null
48,"47,""where qi is a query unigram and qiqi+1 is a query bigram, and fT (qi , D), fB (qiqi+1, D) and fU (qiqi+1, D) are the potential (i.e.,"",null,null",null,null
49,"48,""Figure 1: Graphical representation of SWDM. q1, q2 and q3 are the query terms and D is a collection document. e words in dashed circles are the nearest neighbors of the query terms in the embedding space (only two most semantically similar words to each query term are shown for illustration)."",null,null",null,null
50,"49,""matching) functions for query concept types (unigrams, ordered"",null,null",null,null
51,"50,""and unordered bigrams), respectively, and T , B and U are the weights of these potential functions, which determine the relative"",null,null",null,null
52,"51,""importance of query concept types. e potential function fT (qi , D) for unigrams is de ned as:"",null,null",null,null
53,"52,fT,null,null",null,null
54,"53,""(qi ,"",null,null",null,null
55,"54,D),null,null",null,null
56,"55,"","",null,null",null,null
57,"56,n(qi,null,null",null,null
58,"57,"","",null,null",null,null
59,"58,D),null,null",null,null
60,"59,+,null,null",null,null
61,"60,µ,null,null",null,null
62,"61,""n (qi , C |C |"",null,null",null,null
63,"62,),null,null",null,null
64,"63,|D| + µ,null,null",null,null
65,"64,(2),null,null",null,null
66,"65,""where n(qi , D) and n(qi , C) are the counts of unigram qi in D and collection C, |D| and |C | are the numbers of words in document and"",null,null",null,null
67,"66,""collection, and µ is the Dirichlet smoothing prior. fB (qiqi+1, D) and fU (qiqi+1, D) are obtained in a similar way by counting cooccurrences of qi and qi+1 in D and C in sequential order or within a window of a given size."",null,null",null,null
68,"67,WSDM provides a more exible parametrization of relevance,null,null",null,null
69,"68,than SDM by calculating the importance weight of each individual,null,null",null,null
70,"69,query concept rather than a concept type. e importance weight,null,null",null,null
71,"70,of each unigram and bigram is calculated as a linear combination,null,null",null,null
72,"71,of ku unigram feature functions,null,null",null,null
73,"72,u j,null,null",null,null
74,"73,(qi,null,null",null,null
75,"74,),null,null",null,null
76,"75,and,null,null",null,null
77,"76,kb,null,null",null,null
78,"77,bigram,null,null",null,null
79,"78,feature,null,null",null,null
80,"79,func-,null,null",null,null
81,"80,tions,null,null",null,null
82,"81,b j,null,null",null,null
83,"82,(qi,null,null",null,null
84,"83,"","",null,null",null,null
85,"84,qi,null,null",null,null
86,"85,+1,null,null",null,null
87,"86,),null,null",null,null
88,"87,as,null,null",null,null
89,"88,follows:,null,null",null,null
90,"89,P,null,null",null,null
91,"90,(D,null,null",null,null
92,"91,|Q,null,null",null,null
93,"92,),null,null",null,null
94,"93,r,null,null",null,null
95,"94,ank,null,null",null,null
96,"95,"","",null,null",null,null
97,"96,ku,null,null",null,null
98,"97,wuj,null,null",null,null
99,"98,u j,null,null",null,null
100,"99,(qi,null,null",null,null
101,"100,),null,null",null,null
102,"101,fT,null,null",null,null
103,"102,(qi,null,null",null,null
104,"103,"","",null,null",null,null
105,"104,D,null,null",null,null
106,"105,)+,null,null",null,null
107,"106,""qi Q j,1"",null,null",null,null
108,"107,kb,null,null",null,null
109,"108,w,null,null",null,null
110,"109,b j,null,null",null,null
111,"110,b j,null,null",null,null
112,"111,(qi,null,null",null,null
113,"112,"","",null,null",null,null
114,"113,qi,null,null",null,null
115,"114,+1),null,null",null,null
116,"115,fB,null,null",null,null
117,"116,(qi,null,null",null,null
118,"117,qi,null,null",null,null
119,"118,""+1,"",null,null",null,null
120,"119,D,null,null",null,null
121,"120,)+,null,null",null,null
122,"121,""qi ,qi+1 Q j,1"",null,null",null,null
123,"122,kb,null,null",null,null
124,"123,w,null,null",null,null
125,"124,b j,null,null",null,null
126,"125,b j,null,null",null,null
127,"126,(qi,null,null",null,null
128,"127,"","",null,null",null,null
129,"128,qi,null,null",null,null
130,"129,+1),null,null",null,null
131,"130,fU,null,null",null,null
132,"131,(qi,null,null",null,null
133,"132,qi +1,null,null",null,null
134,"133,"","",null,null",null,null
135,"134,D),null,null",null,null
136,"135,(3),null,null",null,null
137,"136,""qi ,qi+1 Q j,1"",null,null",null,null
138,"137,3.1 Semantic Weighted Dependence Model,null,null",null,null
139,"138,""In SWDM, the relevance score of D to Q also takes into account the words that are semantically similar to query terms in the embedding space. We de ne qij as the jth most similar term to qi in the embedding space, according to cosine similarity. We also de ne Eqi ,"""" {qi0, qi1, qi2, · · · } as a set of words, whose cosine similarity with qi in the embedding space exceeds a threshold s (including qi0 """","""" qi itself). Unlike SDM and WSDM, the potential functions"""""",null,null",null,null
140,"139,1214,null,null",null,null
141,"140,Short Research Paper,null,null",null,null
142,"141,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
143,"142,""Table 1: Performance of the proposed method with and without unigrams from the top retrieved documents and the baselines. 1 and 2 indicate statistically signi cant improvements of SWDM over WSDM and EQE1, respectively, while 3, 4 and 5 indicate statistically signi cant improvements of SWDM+ over EQE1+RM1, PQE, and SWDM+RM1, respectively, according to the Fisher's randomization test (p < 0.05). Percentage improvements of SWDM over WSDM and EQE1 as well as percentage improvements of SWDM+ over PQE and SWM+RM1 are shown in parenthesis."",null,null",null,null
144,"143,Method,null,null",null,null
145,"144,SDM WSDM EQE1,null,null",null,null
146,"145,SWDM,null,null",null,null
147,"146,LCE PQE EQE1+RM1 SWDM+RM1,null,null",null,null
148,"147,SWDM+,null,null",null,null
149,"148,ROBUST04,null,null",null,null
150,"149,MAP,null,null",null,null
151,"150,P@10,null,null",null,null
152,"151,0.2583,null,null",null,null
153,"152,0.4278,null,null",null,null
154,"153,0.2689,null,null",null,null
155,"154,0.4563,null,null",null,null
156,"155,""0.2597 0.28021, 2"",null,null",null,null
157,"156,""0.4336 0.46761, 2"",null,null",null,null
158,"157,(+4.20%/+7.89%) (+2.48%/+7.84%),null,null",null,null
159,"158,0.2886,null,null",null,null
160,"159,0.4697,null,null",null,null
161,"160,0.2921,null,null",null,null
162,"161,0.4726,null,null",null,null
163,"162,0.2872,null,null",null,null
164,"163,0.4672,null,null",null,null
165,"164,""0.2991 0.30343, 4, 5"",null,null",null,null
166,"165,""0.4828 0.49093, 4, 5"",null,null",null,null
167,"166,(+3.87%/+1.44%) (+3.87%/+1.68%),null,null",null,null
168,"167,GOV2,null,null",null,null
169,"168,MAP,null,null",null,null
170,"169,P@10,null,null",null,null
171,"170,0.3156,null,null",null,null
172,"171,0.5457,null,null",null,null
173,"172,0.3232,null,null",null,null
174,"173,0.5533,null,null",null,null
175,"174,""0.3172 0.33191, 2"",null,null",null,null
176,"175,""0.5466 0.55981, 2"",null,null",null,null
177,"176,(+2.69%/+4.63%) (+1.17%/+2.41%),null,null",null,null
178,"177,0.3408,null,null",null,null
179,"178,0.5667,null,null",null,null
180,"179,0.3526,null,null",null,null
181,"180,0.5858,null,null",null,null
182,"181,0.3315,null,null",null,null
183,"182,0.5459,null,null",null,null
184,"183,""0.3557 0.36863, 4"",null,null",null,null
185,"184,""0.5872 0.59973, 4"",null,null",null,null
186,"185,(+4.54%/+3.63%) (+2.37%/+2.13%),null,null",null,null
187,"186,ClueWeb09B,null,null",null,null
188,"187,MAP,null,null",null,null
189,"188,P@10,null,null",null,null
190,"189,0.0783,null,null",null,null
191,"190,0.2777,null,null",null,null
192,"191,0.0762,null,null",null,null
193,"192,0.2797,null,null",null,null
194,"193,""0.0742 0.08271, 2"",null,null",null,null
195,"194,""0.2778 0.28121, 2"",null,null",null,null
196,"195,(+8.53%/+11.46%) (+0.54%/+1.22%),null,null",null,null
197,"196,0.0738,null,null",null,null
198,"197,0.2693,null,null",null,null
199,"198,0.0749,null,null",null,null
200,"199,0.2751,null,null",null,null
201,"200,0.0731,null,null",null,null
202,"201,0.2695,null,null",null,null
203,"202,0.0756,null,null",null,null
204,"203,0.2716,null,null",null,null
205,"204,0.0787,null,null",null,null
206,"205,0.2778,null,null",null,null
207,"206,(+5.07%/+4.10%) (+0.98%/+2.28%),null,null",null,null
208,"207,""fT (qi , D), fB (qiqi+1, D) and fU (qiqi+1, D) in SWDM are calculated using all the terms in E and not just the query terms:"",null,null",null,null
209,"208,ku,null,null",null,null
210,"209,P (D |Q ),null,null",null,null
211,"210,r ank,null,null",null,null
212,"211,"","",null,null",null,null
213,"212,w,null,null",null,null
214,"213,u j,null,null",null,null
215,"214,u j,null,null",null,null
216,"215,(qmi,null,null",null,null
217,"216,),null,null",null,null
218,"217,fT,null,null",null,null
219,"218,(qmi,null,null",null,null
220,"219,"","",null,null",null,null
221,"220,D)+,null,null",null,null
222,"221,""qim  Eqi j,1"",null,null",null,null
223,"222,kb,null,null",null,null
224,"223,w,null,null",null,null
225,"224,b j,null,null",null,null
226,"225,b j,null,null",null,null
227,"226,(qmi,null,null",null,null
228,"227,"","",null,null",null,null
229,"228,qmi +1 ),null,null",null,null
230,"229,fB,null,null",null,null
231,"230,""(qmi qmi+1,"",null,null",null,null
232,"231,D,null,null",null,null
233,"232,)+,null,null",null,null
234,"233,""qim,qim+1  Eqi , Eqi+1 j ,1"",null,null",null,null
235,"234,kb,null,null",null,null
236,"235,w,null,null",null,null
237,"236,b j,null,null",null,null
238,"237,b j,null,null",null,null
239,"238,(qmi,null,null",null,null
240,"239,"","",null,null",null,null
241,"240,qmi +1 ),null,null",null,null
242,"241,fU,null,null",null,null
243,"242,""(qmi qmi+1,"",null,null",null,null
244,"243,D,null,null",null,null
245,"244,),null,null",null,null
246,"245,""qim,qim+1  Eqi , Eqi+1 j ,1"",null,null",null,null
247,"246,(4),null,null",null,null
248,"247,""erefore, besides pair-wise dependencies between adjacent query"",null,null",null,null
249,"248,""terms, SWDM also captures pair-wise dependencies between query"",null,null",null,null
250,"249,terms and the words semantically similar to them in the embed-,null,null",null,null
251,"250,""ding space. For an example query in Figure 1, besides the query"",null,null",null,null
252,"251,""unigram q1, retrieval score of D, according to SWDM, also includes the weighted matching scores for unigrams q11 and q12 that are semantically similar to q1. Similarly, besides the query bigram q1q2, SWDM also includes the weighted matching scores for: (1) the bigrams q1q21, q1q22, q11q2, and q12q2, which have only one of their constituent terms not from the original query (2) the bigrams q11q21, q11q22, q12q21, and q12q22, in which both constituent terms are not from the original query. If Eqi , {qi0} ,"""" {qi } (i.e., in the case when no semantically similar unigrams are in the neighborhood of orig-"""""",null,null",null,null
253,"252,""inal query terms), SWDM only takes into account the unigrams and"",null,null",null,null
254,"253,bigrams in the original query (i.e. is identical to WSDM).,null,null",null,null
255,"254,""e importance weight of each query concept is computed based on several features. For an expansion unigram qij , we use its cosine similarity with qi , frequency in the collection and top retrieved documents, document frequency in the collection and top documents as features. For an expansion bigram qijqij+1, we use an average cosine similarity of the terms qij and qij+1, sequential and window based co-occurrence frequency in the collection and top"",null,null",null,null
256,"255,""Figure 2: Graphical representation of SWDM+, a variant of SWDM, which besides the neighbors of the query terms in the embedding space (q11, . . . , q32), also incorporates the unigrams from the top retrieved documents ranked by RM1 [11] (q^11, . . . , q31)."",null,null",null,null
257,"256,documents as well as sequential and window based document frequency in the collection and top documents.,null,null",null,null
258,"257,4 EXPERIMENTS,null,null",null,null
259,"258,""Experimental results reported below were obtained using word2vec word embeddings with 300 dimensions that were pre-trained on the Google News corpus3, Indri-5.10 IR toolkit4 and GenSim library5 for all word embedding computations."",null,null",null,null
260,"259,""Retrieval accuracy of SWDM was evaluated with respect to Mean Average Precision (MAP) and Precision@10 (P@10) on ROBUST04, GOV2 and ClueWeb09B (excluding the documents for which Waterloo Fusion spam score was greater than 70) collections and compared with the state-of-the-art MRF retrieval models (SDM [12] and WSDM [5]), PRF methods for MRF retrieval models (LCE [13] and"",null,null",null,null
261,"260,3h ps://drive.google.com/ le/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/ 4h ps://www.lemurproject.org/indri/ 5h ps://radimrehurek.com/gensim/,null,null",null,null
262,"261,1215,null,null",null,null
263,"262,Short Research Paper,null,null",null,null
264,"263,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
265,"264,PQE [6]) and embedding based query expansion method for bag-ofwords retrieval models (EEQ1 [19]). Collection and document frequencies were used as features to calculate the weights of query concepts in WSDM and PQE.,null,null",null,null
266,"265,""We also experimented with SWDM+, a variant of SWDM, which, besides the neighbors of query terms in the embedding space, also incorporates E^Q ,"""" {q^1, q^2, . . . , q^k }, top k unigrams from the top retrieved documents according to the relevance model (RM1) [11] scores, as illustrated in Figure 2. SWDM+ uses the same set of features for weighting query concepts as SWDM. e similarity features to determine the importance of query concepts involving unigrams from top retrieved documents are calculated with respect to the closest query term in the embedding space. SWDM+RM1 and EEQ1+RM1 are a linear interpolation of RM1 with SWDM and EEQ1, respectively. Unigrams from the top retrieved documents in SWDM+RM1, EEQ1+RM1 and SWDM+ are ranked using RM1 and the top k are selected to be incorporated into the query."""""",null,null",null,null
267,"266,""e parameters of all models have been optimized using threefold cross-validation and coordinate ascent to maximize MAP. e range of continuous and discrete model parameters has been examined with the step sizes of 0.02 and 1, respectively."",null,null",null,null
268,"267,4.1 Results,null,null",null,null
269,"268,""Table 1 provides a summary of retrieval accuracy for SWDM, its variants and the baselines6. As follows from the rst half of Table 1, SWDM outperforms SDM, WSDM and EQE1 in terms of both MAP and P@10, which indicates the utility of incorporating semantically related terms into WSDM. It also follows from Table 1 that the retrieval accuracy of EQE1 is close to that of SDM, which indicates that utilizing word embeddings for query expansion in conjunction with bag-of-words retrieval model results in similar improvements of retrieval accuracy as accounting for sequential dependencies between query terms. Our results also indicate that SWDM has be er retrieval accuracy than EQE1, since besides incorporating similar words from the embedding space into a query, it also takes into account the dependencies between the expansion terms as well as between the expansion terms and the query terms."",null,null",null,null
270,"269,""e results in the second half of Table 1 indicate that incorporating unigrams from the top retrieved documents translates into a signi cant increase in retrieval accuracy of SWDM on ROBUST04 and GOV2 collections. In particular, SWDM+ outperforms both LCE and PQE, state-of-the-art PRF methods for MRF retrieval models, which include a separate potential function for expansion terms, but do not take into account neither dependencies between the expansion terms nor between the expansion terms and the original query terms, and EQE1+RM1, which is designed for bag-of-words retrieval models. SWDM+, however, has inferior performance to both SWDM and SWDM+RM1 on ClueWeb09B, which is due to relatively low accuracy of all retrieval models on this collection and, as a result, noisy unigrams from the top retrieved documents that are used for query expansion. is result suggests that the relative in uence of query term neighbors and the expansion terms from the top retrieved documents on retrieval accuracy depends on a collection and the quality of the initial retrieval results. SWDM+ also demonstrated a signi cantly statistical improvement in retrieval"",null,null",null,null
271,"270,6code and runs are available at h p://github.com/teanalab/SWDM,null,null",null,null
272,"271,""accuracy over SWDM+RM1 on ROBUST04 and GOV2 collections, in-"",null,null",null,null
273,"272,dicating that the features based on similarity of expansion and the,null,null",null,null
274,"273,original query terms in the embedding space have a positive e ect,null,null",null,null
275,"274,on retrieval accuracy.,null,null",null,null
276,"275,5 CONCLUSION,null,null",null,null
277,"276,""In this paper, we proposed Semantic Weighted Dependence Model,"",null,null",null,null
278,"277,which allows to address the vocabulary gap in Weighted Sequen-,null,null",null,null
279,"278,""tial Dependence Model, by leveraging distributed word represen-"",null,null",null,null
280,"279,""tations (i.e. word embeddings) in two di erent ways. On one hand,"",null,null",null,null
281,"280,word embeddings are used for calculating distributional similarity,null,null",null,null
282,"281,to nd the terms that are semantically similar to query terms for,null,null",null,null
283,"282,""query expansion. On the other hand, they are used as features"",null,null",null,null
284,"283,to calculate the importance of query concepts. We also proposed,null,null",null,null
285,"284,""an extension of SWDM, which besides semantically similar terms,"",null,null",null,null
286,"285,also incorporates the terms from the top retrieved documents.,null,null",null,null
287,"286,REFERENCES,null,null",null,null
288,"287,[1] Saeid Balaneshin-kordan and Alexander Kotov. 2016. Optimization method for weighting explicit and latent concepts in clinical decision support queries. In Proceedings of ACM ICTIR. 241­250.,null,null",null,null
289,"288,[2] Saeid Balaneshin-kordan and Alexander Kotov. 2016. Sequential query expansion using concept graph. In Proceedings of ACM CIKM. 155­164.,null,null",null,null
290,"289,[3] Saeid Balaneshin-kordan and Alexander Kotov. 2016. A study of document expansion using translation models and dimensionality reduction methods. In Proceedings of ACM ICTIR. 233­236.,null,null",null,null
291,"290,[4] Saeid Balaneshinkordan and Alexander Kotov. 2016. An Empirical comparison of term association and knowledge graphs for query expansion. In Proceedings of ECIR. 761­767.,null,null",null,null
292,"291,""[5] Michael Bendersky, Donald Metzler, and W Bruce Cro . 2010. Learning concept importance using a weighted dependence model. In Proceedings of ACM WSDM. 31­40."",null,null",null,null
293,"292,""[6] Michael Bendersky, Donald Metzler, and W Bruce Cro . 2011. Parameterized concept weighting in verbose queries. In Proceedings of ACM SIGIR. 605­614."",null,null",null,null
294,"293,""[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In Proceedings of ACM CIKM. 55­ 64."",null,null",null,null
295,"294,""[8] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. Semantic Matching by Non-Linear Word Transportation for Information Retrieval. In Proceedings of ACM CIKM. 701­710."",null,null",null,null
296,"295,[9] Alexander Kotov and ChengXiang Zhai. 2011. Interactive sense feedback for di cult queries. In Proceedings of ACM CIKM. 163­172.,null,null",null,null
297,"296,[10] Alexander Kotov and ChengXiang Zhai. 2012. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for di cult queries. In Proceedings of ACM WSDM. 403­412.,null,null",null,null
298,"297,[11] Victor Lavrenko and W Bruce Cro . 2001. Relevance based language models. In Proceedings of ACM SIGIR. 120­127.,null,null",null,null
299,"298,[12] Donald Metzler and W Bruce Cro . 2005. A Markov random eld model for term dependencies. In Proceedings of ACM SIGIR. 472­479.,null,null",null,null
300,"299,[13] Donald Metzler and W Bruce Cro . 2007. Latent concept expansion using markov random elds. In Proceedings of ACM SIGIR. 311­318.,null,null",null,null
301,"300,[14] Jay M Ponte and W Bruce Cro . 1998. A language modeling approach to information retrieval. In Proceedings of ACM SIGIR. 275­281.,null,null",null,null
302,"301,""[15] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M HancockBeaulieu, Mike Gatford, and others. 1995. Okapi at TREC-3. NIST 109 (1995)."",null,null",null,null
303,"302,[16] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of ACM SIGIR. 373­382.,null,null",null,null
304,"303,[17] Ivan Vulic´ and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In Proceedings of ACM SIGIR. 363­372.,null,null",null,null
305,"304,""[18] Di Wang and Eric Nyberg. 2015. A long short-term memory model for answer sentence selection in question answering. Proceedings of ACL (2015), 707­712."",null,null",null,null
306,"305,[19] Hamed Zamani and W Bruce Cro . 2016. Embedding-based ery Language Models. In Proceedings of ACM ICTIR. 147­156.,null,null",null,null
307,"306,[20] Guoqing Zheng and Jamie Callan. 2015. Learning to reweight terms with distributed representations. In Proceedings of ACM SIGIR. 575­584.,null,null",null,null
308,"307,""[21] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Integrating and evaluating neural word embeddings in information retrieval. In Proceedings of ADCS. 12."",null,null",null,null
309,"308,1216,null,null",null,null
310,"309,,null,null",null,null

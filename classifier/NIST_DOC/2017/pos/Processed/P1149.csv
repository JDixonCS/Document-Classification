,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues,null,null",null,null
4,"3,Amina Kadry,null,null",null,null
5,"4,""Dymatrix Consulting Stu gart, Germany a.kadry@dymatrix.de"",null,null",null,null
6,"5,ABSTRACT,null,null",null,null
7,"6,Our goal is to complement an entity ranking with human-readable explanations of how those retrieved entities are connected to the information need. Relation extraction technology should aid in,null,null",null,null
8,"7,""nding such support passages, especially in combination with entities and query terms. is work explores how the current state of the art in unsupervised relation extraction (OpenIE) contributes to a solution for the task, assessing potential, limitations, and avenues for further investigation."",null,null",null,null
9,"8,""ACM Reference format: Amina Kadry and Laura Dietz. 2017. Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080744"",null,null",null,null
10,"9,1 INTRODUCTION,null,null",null,null
11,"10,""It seems obvious that technology for extracting the meaning of text, such as relation extraction, should lead to be er text retrieval methods. Yet, so far successes have been rare. is paper studies di erent ways of exploiting open relation extraction technology, assesses the potential for merit as well as open issues that inhibit further success for text-centric information retrieval."",null,null",null,null
12,"11,""Given sentences as input, open relation extraction (OpenIE) algorithms extract information on how knowledge base entities are related by analyzing the grammatical structure of each sentence."",null,null",null,null
13,"12,""To assess opportunities for future merit, we choose a text ranking task that operates on the sentence level and for which information about entities and relations is clearly pertinent: Retrieving explanations for how/why a knowledge base entity is relevant for an information need. is task is useful whenever entities are displayed along with web search results, such as entity cards [3]."",null,null",null,null
14,"13,""Task (support passage ranking): A user enters information need Q; an external system predicts a ranking of relevant entities E. Our task is to, for every relevant entity ei  E, retrieve and rank K passages sik that explain why this entity ei is relevant for Q."",null,null",null,null
15,"14,""We postulate and study the following hypothesis. For a given entity ei , passages sik that explain a relevant relationship involving the"",null,null",null,null
16,"15,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080744"",null,null",null,null
17,"16,Laura Dietz,null,null",null,null
18,"17,""University of New Hampshire Durham, NH, USA dietz@cs.unh.edu"",null,null",null,null
19,"18,""entity ei , are also good human-readable descriptions of why the entity is relevant for the information need Q."",null,null",null,null
20,"19,""Of course, conventional OpenIE algorithms have no knowledge of the information need Q. erefore, we study outcomes of relation extraction in superposition with retrieval models such as query likelihood. is paper studies how much OpenIE contributes to accomplishing this task. While there are many suggested approaches to OpenIE, we focus on the ClausIE system, which has been shown to be one of the best OpenIE methods on three established benchmark datasets [5]."",null,null",null,null
21,"20,""Contributions. is paper features an in-depth study of the utility of a state-of-the-art OpenIE extraction system. We study how relation extraction can help, what are promising avenues for further research, and what are limitation of current relation extraction approaches that need to be overcome."",null,null",null,null
22,"21,""We demonstrate that OpenIE methods provide signi cantly better indicators for entity-centric passage ranking tasks, in contrast to low-level NLP methods such as part-of-speech tagging, named entity recognition, or dependency parsing. Despite these signi cant improvements, we quantify how limitations of current OpenIE systems are a ecting the quality of downstream information retrieval tasks."",null,null",null,null
23,"22,Outline. e state-of-the-art is summarized in Section 2. A short introduction to the relation extraction system ClausIE is given in Section 3. Section 4 details the feature-based learning-to-rank approach through which we evaluate the merit of OpenIE technology.,null,null",null,null
24,"23,antitative experimental results are provided in Section 5.,null,null",null,null
25,"24,2 RELATED WORK,null,null",null,null
26,"25,""Relations and retrieval. Given a relationship in a knowledge graph, Voskarides et al. [14] study the problem of nding human readable descriptions of that relationship. e relationship is given in the form ei , r , ej , where ei and ej are given entities, i.e., nodes in the knowledge graph and r is a type of a relationship, such as works for. Given this relationship, the task is to rank text passages sijk by how well they describe the relationship in human-readable form. is is the inverse problem to relation extraction [5, 11] where the task is to, given a textual description sijk , extract relational facts in the form ei , r , ej . None of these approaches take a further information need Q into consideration."",null,null",null,null
27,"26,""In the context of web queries, Schuhmacher et al. [12] apply supervised relation extraction to documents that are relevant for the information need Q and study how many of the extracted relations ei , r , ej are indeed relevant for Q. ey also analyze sentences, such as sijk , from which the relevant relation were extracted."",null,null",null,null
28,"27,Sentence retrieval. Previous work on retrieving entities and support sentences addresses the sentence retrieval problem. For,null,null",null,null
29,"28,1149,null,null",null,null
30,"29,Short Research Paper,null,null",null,null
31,"30,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
32,"31,""example Blanco et al. [4] present a model that ranks entity support sentences with learning-to-rank. eir work focuses on features based on named entity recognition (NER) in combination termbased retrieval models. Many features based on using knowledge graph entities for text retrieval could also be applied here, such as the latent entity space model of Liu et al. [9]."",null,null",null,null
33,"32,""Temporal event summarization. Temporal summarization is the task of identifying short and relevant sentences about a developing news event such as disasters, accidents, etc. [1] in a realtime se ing. Each event can be seen as a textual query that describes the event. For example, Kedzie et al. [8] propose to cluster sentences with salience predictions in the context of a named event within a multi-document summarization system. In line with many featurebased approaches, their system exploits term-based retrieval, query expansion, geographical and temporal relevance features."",null,null",null,null
34,"33,""estion answering. Given a question in natural language, estion Answering methods focus on providing correct and precise answers [13]. QA systems rst use IR techniques are used to retrieve passages that contain the answer. Next these are analyzed to extract a concise answer. Whenever the question includes an entity, a solution to our task is also applicable to the rst stage of question answering."",null,null",null,null
35,"34,3 FOUNDATION: CLAUSIE,null,null",null,null
36,"35,""ClausIE [5] is an OpenIE (unsupervised relation extraction) system designed for high-precision extractions. In contrast to previous OpenIE approaches, such as TextRunner [2] and Reverb [7], ClausIE distinguishes between the discovery of useful information from a given sentence and the representation of this information through multiple propositions. e system identi es di erent types of clauses, such as adverbial, complement, indirect object, and direct object. In contrast to many earlier approaches, ClausIE does not require labeled or unlabeled training data or global post-processing, making it applicable to open-domain retrieval tasks."",null,null",null,null
37,"36,""Example. Given the following sentence with token indices:1 """" e1 rules2 of3 golf4 are5 a6 standard7 set8 of9 regulations10 and11 procedures12 by13 which14 the15 sport16 of17 golf18 should19 be20 played21 ."""""",null,null",null,null
38,"37,""Phase 1. Clause types are extracted, representing constituents by their head word with token o set. For example:"",null,null",null,null
39,"38,""Complementary clause SVC(C:set8, V:are5, S:rules2 , A?:of9 )"",null,null",null,null
40,"39,Adverbial clause,null,null",null,null
41,"40,""SVA(V:played21, S:sport16, S:by13)"",null,null",null,null
42,"41,Phase 2. Propositions of relation tuples are derived. For example:,null,null",null,null
43,"42,e rules2 of golf e rules2 of golf e rules2 of golf,null,null",null,null
44,"43,are5,null,null",null,null
45,"44,a standard set8 of9 regulations,null,null",null,null
46,"45,are5,null,null",null,null
47,"46,a standard set8 of9 procedures,null,null",null,null
48,"47,are5,null,null",null,null
49,"48,a standard set8,null,null",null,null
50,"49,the sport16 of golf should be played21 by13 a standard set8 of regulations the sport16 of golf should be played21 by13 a standard set8 of procedures,null,null",null,null
51,"50,""Whenever entity and query terms are contained in the same proposition, this sentence is likely to explain the connection between query and entity."",null,null",null,null
52,"51,1See demo at h ps://gate.d5.mpi-inf.mpg.de/ClausIEGate/ClausIEGate.,null,null",null,null
53,"52,4 APPROACH: RANKING SENTENCES FOR EXPLAINING ENTITY RELEVANCE,null,null",null,null
54,"53,""To study the utility of ClausIE for the support passage ranking task, we make use of a common two-step approach of 1) extracting candidate sentences and 2) using learning to rank (LTR) with a rich set of features, some of which are based on ClausIE's extractions."",null,null",null,null
55,"54,4.1 Extracting Candidate Sentences,null,null",null,null
56,"55,""In order to create a set of candidate sentences for a given query Q and entity ei , a corpus of documents that is pertinent to the entity is required. Any corpus could be used here, such as the ClueWeb corpus with entity links, as used by Schuhmacher et al. [12]. Assuming that OpenIE works best on grammatically wellformed sentences, we instead follow Voskarides et al. [14] and base this study on sentences from the Wikipedia article of the entity ei ."",null,null",null,null
57,"56,4.2 Machine Learning (LTR),null,null",null,null
58,"57,Sentences are ranked with a list-wise learning-to-rank (LTR) approach implemented in RankLib.2 e weight parameter is learned by optimizing for the Mean-Average Precision metric (MAP) using coordinate ascent and 20 restarts. e LTR will learn a weighted feature combination to achieve the best possible ranking on the training set. Features of di erent categories are discussed below. We study feature sets for their merit by applying LTR on hold-out test data using cross-validation.,null,null",null,null
59,"58,4.3 Sentence Ranking Features,null,null",null,null
60,"59,""Table 1 details the features which fall into these categories: Text features and quality features (Text) (1­8) capture the relevance and quality of the sentence at the term level. NLP features (9­16) are derived from part-of-speech (POS) and named entity recognition (NER) tags. ese have been speculated to not help IR. Dependency parse tree (DP) features (17­19) capture the grammatical structure of the sentence. We use the Stanford dependency parser [10] which is also used by the ClausIE system. Earlier works on relation extraction use the direct path between two entities in the dependency parse tree [15]. ClausIE features (20­43) capture the sentence's relation information about entity and query terms. Features are divided by positions of the relation proposition, i.e., subject, verb, and object. Relation quality indicators are included, such as the proposition length measured in tokens or the maximum constituent length (number of tokens in dependency subtree)--both averaged across all propositions extracted from this sentence."",null,null",null,null
61,"60,5 EXPERIMENTAL EVALUATION,null,null",null,null
62,"61,We conduct a series of experiments to determine the utility and issues of an available state-of-the-art OpenIE system. We focus on the task of ranking support sentences by how well they explain the relevance of a given entity ei for a given information need Q.,null,null",null,null
63,"62,""e study is divided according to three questions: 1) Under ideal conditions, could relation extractions help rank relevant passages? 2) What quality is achieved by a fully-automatic learning-to-rank"",null,null",null,null
64,"63,2 lemurproject.org/ranklib.php,null,null",null,null
65,"64,1150,null,null",null,null
66,"65,Short Research Paper,null,null",null,null
67,"66,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
68,"67,Table 1: Features used for support passage ranking.,null,null",null,null
69,"68,Feat. Description,null,null",null,null
70,"69,Text 1 sentence length measured in number of words 2 sentence position measured as a fraction of the document 3 fraction words that are stop words 4 fraction of query terms covered by sentence 5 sum of ISF of query terms (ISF is inverse sentence frequency) 6 average of ISF of query terms 7 sum of TF-ISF of query terms 8 number of entities mentioned,null,null",null,null
71,"70,NLP 9-12 13 14-16,null,null",null,null
72,"71,for nouns/verbs/adjectives/adverbs: fraction of words with POS tag whether sentence contains a named entity for NER types PER/LOC/ORG: whether NER of type is contained,null,null",null,null
73,"72,DP 17 number of edges on the path between two entities in dependency tree 18 indicator whether path goes through root node 19 indicator whether path goes through query term,null,null",null,null
74,"73,ClausIE 20 whether ClausIE generated an extraction from this sentence,null,null",null,null
75,"74,21-27 for all seven clause types: whether clause of this type is extracted 28 proposition length measured in tokens 29 maximum constituent length (size of dependency tree) in proposition,null,null",null,null
76,"75,30-32 for subject/object/both: if another entity is in subject and/or object position of the proposition,null,null",null,null
77,"76,33-34 for subject/object position: if given entity is in position of proposition 35-36 for subject/object position: if any entity is in position of proposition 37-38 for subject/object position: if an entity link is in position of prop. 39-41 for subject/verb/object position: if a query term (ignoring stopwords),null,null",null,null
78,"77,is in position of proposition 42-43 for subject/object position: if a named entity (NER) is in position of,null,null",null,null
79,"78,proposition,null,null",null,null
80,"79,approach with OpenIE features (cf. Section 4)? 3) Which open issues of OpenIE systems inhibit the application to text ranking tasks?,null,null",null,null
81,"80,5.1 Test collection,null,null",null,null
82,"81,""For this study we build a test collection3 for 95 support passage rankings (one per query and entity). We use a subset of ten 2013/2014 TREC Web track queries and (up to) ten relevant entities E for these topics, which are taken from the REWQ gold standard.4 To focus this study on grammatically sound and well-wri en documents, we use Wikipedia articles of each relevant entity as a basis for candidate sentences. ese are taken from the 2012 Wikipedia Wex dump. To obtain a base set for assessment, these sentences are processed by the ClausIE extraction system."",null,null",null,null
83,"82,""We ask assessors to imagine they were to write a knowledge article on the topic Q, on which they were to include information about the given entity ei . Assessors are asked to mark passages that would be suitable support passages for the article by answering the following question:"",null,null",null,null
84,"83,AQ1) Explanation: Does the sentence explain the relevance of entity ei ?,null,null",null,null
85,"84,""is way we obtain candidate sentences for 95 query-entity pairs as input topics. We arrive at a total of 31,397 assessed sentences with 2,906 relevant support passages of entity relevance. O en, the relevant aspects of a relevant entity are not noteworthy enough to be described in the entity's article [6]. is leads to 20 query-entity pairs that don't contain any explanations of entity-relevance. ese"",null,null",null,null
86,"85,3data set available: www.cs.unh.edu/dietz/appendix/openie4ir 4h p://mschuhma.github.io/rewq/,null,null",null,null
87,"86,Table 2: Performance of AQ1­5 as predictors for explanations and Pearson correlation .  signi cance over Qterm,null,null",null,null
88,"87,Prec() Recall ,null,null",null,null
89,"88,Count,null,null",null,null
90,"89,Relation,null,null",null,null
91,"90,0.46 ±0.05 0.28 ±0.03 0.27,null,null",null,null
92,"91,1767 (8%),null,null",null,null
93,"92,Rel rel,null,null",null,null
94,"93,0.52 ±0.05  0.21 ±0.03 0.52,null,null",null,null
95,"94,935 (4%),null,null",null,null
96,"95,ClausIE,null,null",null,null
97,"96,0.45 ±0.05 0.20 ±0.02 0.33,null,null",null,null
98,"97,1172 (5%),null,null",null,null
99,"98,ClausIE rel,null,null",null,null
100,"99,0.49 ±0.05  0.14 ±0.02 0.49,null,null",null,null
101,"100,636 (3%),null,null",null,null
102,"101,Qterm (),null,null",null,null
103,"102,0.38 ±0.04 0.49 ±0.04 0.47,null,null",null,null
104,"103,4476 (20%),null,null",null,null
105,"104,Name,null,null",null,null
106,"105,0.33 ±0.05 0.43 ±0.04 0.35,null,null",null,null
107,"106,6173 (27%),null,null",null,null
108,"107,Table 3: Results on ranking of sentences explaining entity relevance with LTR.,null,null",null,null
109,"108,Method,null,null",null,null
110,"109,Full Text NLP DP ClausIE,null,null",null,null
111,"110,MAP (),null,null",null,null
112,"111,0.44 ±0.03 0.42* ±0.03 0.31* ±0.03 0.33* ±0.03 0.41* ±0.03,null,null",null,null
113,"112,Hurt,null,null",null,null
114,"113,­ 23 39 43 25,null,null",null,null
115,"114,Helped,null,null",null,null
116,"115,­ 9 11 5 11,null,null",null,null
117,"116,Ablation,null,null",null,null
118,"117,MAP,null,null",null,null
119,"118,Full-TEXT Full-NLP Full-DP Full-ClausIE,null,null",null,null
120,"119,0.41 ±0.03 0.43 ±0.04 0.43 ±0.04 0.43 ±0.03,null,null",null,null
121,"120,""are excluded from this study, leaving 75 query-entity pairs and 22,731 support passage annotations of which 2,906 are marked as relevant according to AQ1."",null,null",null,null
122,"121,""In order to study characteristics of sentences in relation to AQ1, we further ask annotators to assess the following questions for each sentence sik , per query Q and entity ei : AQ2) Relation: Does the sentence mention any relationship involving ei ? AQ3) Rel rel: Is this relationship relevant for the explanation? AQ4) ClausIE: Does ClausIE extract a valid relationship from sentence? AQ5) ClausIE rel: Is ClausIE's extraction relevant for the explanation?"",null,null",null,null
123,"122,We study these annotations in combination with two heuristics: Qterm: Does the sentence include query terms (stopwords ignored)? Name: Does the sentence include the entity's name?,null,null",null,null
124,"123,5.2 Experiment 1: Relations and Relevance,null,null",null,null
125,"124,""By casting the result of every annotation question (Relation, Rel rel, ClausIE, ClausIE rel) as well as heuristics (Qterm, Name) as a random variable, we study both the Pearson correlation  of these predictors and the ground truth (Explanation / AQ1) as well as their predictive power as measured by set precision and recall in Table 2."",null,null",null,null
126,"125,""ese demonstrate that good explanations are found in sentences that express a relevant relation of the entity (Rel rel / AQ3), re ected in the highest Pearson correlation of 0.52, as well as the highest precision of 0.52. Using Rel rel as a predictor is signi cantly5 be er in terms of precision than using the Qterm heuristic (which achieves precision of 0.38). However, the Qterm heuristic achieves a much higher recall of 0.49. is suggests that combining query terms and relation extractions is a worthwhile avenue for investigation."",null,null",null,null
127,"126,""Of course, this requires an automatic approach for distinguishing relevant from non-relevant relation expressions. On the pessimistic side, only half of all extracted relations are indeed relevant. On optimistic side, macro-avg precision drops only mildly from 0.52 for relevant relations (Rel rel / AQ3) to 0.46 for any relation (AQ2) and 0.45 for ClausIE extractions (AQ4). We speculate that an OpenIE relation extractor can also serve as a quality indicator for passages as it is sensitive towards well-formed sentences."",null,null",null,null
128,"127,""5Paired-t-test with  , 5%."",null,null",null,null
129,"128,1151,null,null",null,null
130,"129,Short Research Paper,null,null",null,null
131,"130,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
132,"131,MAP,null,null",null,null
133,"132,0.5,null,null",null,null
134,"133,0.4,null,null",null,null
135,"134,0.3,null,null",null,null
136,"135,0.2,null,null",null,null
137,"136,0.1,null,null",null,null
138,"137,0,null,null",null,null
139,"138,Full,null,null",null,null
140,"139,Text,null,null",null,null
141,"140,NLP,null,null",null,null
142,"141,DP,null,null",null,null
143,"142,ClausIE,null,null",null,null
144,"143,Figure 1: Results on ranking of sentences explaining entity relevance: full vs. subsets,null,null",null,null
145,"144,5.3 Experiment 2: Evaluation through LTR,null,null",null,null
146,"145,Next we demonstrate that features derived from ClausIE's extractions can be e ectively used to train a learning-to-rank (LTR) method for ranking support passages as detailed in Section 4.,null,null",null,null
147,"146,""e Full feature set, given in Table 1, is divided into four feature sets by category: Text, NLP, DP, and ClausIE. We compare our approach using all features (Full) versus each feature set individually. Statistically signi cant improvements of the Full using a paired t-test ( ,"""" 5%) are marked with *. We additionally perform an ablation study by removing one feature subset at a time (Full-category) from the Full feature set, to study redundancy in the feature space."""""",null,null",null,null
148,"147,""For learning to rank, approaches are evaluated with 5-fold crossvalidation, where all rankings associated with the same query (but di erent entities) are assigned to the same fold. e ranking performance is measured in mean-average precision (MAP) with respect to the ground truth of a sentence explaining the relevance of the entity for the query (AQ1). Results are presented in Table 3 and Figure 1. Unjudged sentences are considered non-relevant. Results given in Table 3 show that the Full method outperforms all other methods signi cantly with a MAP of 0.36. Individually, the strongest feature subsets are ClausIE and Text, and the ablation study con rms that they provide complementary merit."",null,null",null,null
149,"148,""Despite issues due to precision-orientation of OpenIE systems (more details about this in the next section), we obtain signi cant improvements with respect to the recall-oriented evaluation metric MAP. is demonstrates that there is merit in further investigating high-level NLP extractions based on OpenIE. is is in contrast to other kinds of NLP extractions such as POS tags, NER tags, and dependency parse information which are signi cantly worse indicator for support passages."",null,null",null,null
150,"149,5.4 Experiment 3: Open Issues,null,null",null,null
151,"150,""Many NLP-oriented systems are tuned for high precision at the expense of recall. While this is a desirable property in the context of knowledge base population, it may impose limitations for information retrieval tasks."",null,null",null,null
152,"151,""Among all sentences that express a relation, ClausIE is missing this relation in 32% of the cases. Additionally, only half of the sentences with relation expressions actually actually contain a relation that is relevant for the query-entity pair (con rming ndings of Schuhmacher et al. [12]). Together this results in only 636 sentences with relevant ClausIE extractions (3%) of all 22731 annotated sentences. In contrast, our data set contains 2906 sentences (13%) with explanations of relevance."",null,null",null,null
153,"152,""While there are ClausIE extractions for 9951 sentences, only"",null,null",null,null
154,"153,1172 constitute a correct extraction. Comparing this to the 2906,null,null",null,null
155,"154,true relevant sentences demonstrates that a perfect recall is not,null,null",null,null
156,"155,obtainable. Let us consider an optimistic thought experiment where,null,null",null,null
157,"156,all sentences with correct ClausIE extractions are relevant. An ideal,null,null",null,null
158,"157,""ranking, which places all relevant sentences rst, would obtain a"",null,null",null,null
159,"158,MAP,null,null",null,null
160,"159,value,null,null",null,null
161,"160,of,null,null",null,null
162,"161,1172 2906,null,null",null,null
163,"162,"","",null,null",null,null
164,"163,0.41,null,null",null,null
165,"164,(theoretical,null,null",null,null
166,"165,upper,null,null",null,null
167,"166,bound).,null,null",null,null
168,"167,is upper,null,null",null,null
169,"168,bound happens to coincide with the actual MAP achieved by the,null,null",null,null
170,"169,""ClausIE feature set alone, MAP 0.41, cf. Table 3. We conclude that"",null,null",null,null
171,"170,our approach obtains an optimal ranking under limitations imposed,null,null",null,null
172,"171,by the o -the-shelf OpenIE system. Improving coverage of OpenIE,null,null",null,null
173,"172,systems is likely to translate to immediate quality improvements,null,null",null,null
174,"173,for text-ranking tasks.,null,null",null,null
175,"174,6 CONCLUSION,null,null",null,null
176,"175,""We study the utility of OpenIE technology ranking sentences by how well they explain the relevance of a given entity for a query. Based on manual assessments and evaluation through a learning-torank framework, the study demonstrates that signi cant improvements are achieved by combining relation features with query and entity matches. While we demonstrate the merit of an OpenIE extraction system, we also quantify losses through limitations of current OpenIE systems. we hope this study stimulates work on relation extraction systems that are designed of information retrieval tasks."",null,null",null,null
177,"176,Acknowledgements,null,null",null,null
178,"177,""is research was performed as part of a Master's thesis at Mannheim University, Germany. We are grateful to Rainer Gemulla for providing access to the ClausIE system. is work was funded in part by a scholarship of the Eliteprogramm for Postdocs of the Baden-Wu¨r emberg Sti ung."",null,null",null,null
179,"178,REFERENCES,null,null",null,null
180,"179,""[1] J. Aslam, F. Diaz, M. Ekstrand-Abueg, R. McCreadie, V. Pavlu, and T. Sakai. Trec 2014 temporal summarization track overview. Technical report, 2015."",null,null",null,null
181,"180,""[2] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In Proc. of IJCAI, 2007."",null,null",null,null
182,"181,""[3] A. Berntson et al. Providing entity-speci c content in response to a search query, Mar. 8 2012. US Patent App. 12/876,638."",null,null",null,null
183,"182,""[4] R. Blanco and H. Zaragoza. Finding support sentences for entities. In Proc. of SIGIR, 2010."",null,null",null,null
184,"183,""[5] L. Del Corro and R. Gemulla. Clausie: clause-based open information extraction. In Proc. of WWW, 2013."",null,null",null,null
185,"184,""[6] L. Dietz, A. Kotov, and E. Meij. Tutorial on utilizing knowledge graphs in text-centric information retrieval. In Proc. of WSDM, 2017."",null,null",null,null
186,"185,""[7] A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In Proc. of EMNLP, 2011."",null,null",null,null
187,"186,""[8] C. Kedzie, K. McKeown, and F. Diaz. Predicting salient updates for disaster summarization. In Prof. of ACL, 2015."",null,null",null,null
188,"187,""[9] X. Liu and H. Fang. Latent entity space: a novel retrieval approach for entitybearing queries. Information Retrieval Journal, 18(6):473­503, 2015."",null,null",null,null
189,"188,""[10] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. e Stanford CoreNLP natural language processing toolkit. In Proc. of ACL, 2014."",null,null",null,null
190,"189,""[11] B. Roth, T. Barth, G. Chrupa la, M. Gropp, and D. Klakow. Relationfactory: A fast, modular and e ective system for knowledge base population. In Proc. of EACL, 2014."",null,null",null,null
191,"190,""[12] M. Schuhmacher, B. Roth, S. P. Ponze o, and L. Dietz. Finding relevant relations in relevant documents. In Proc. of ECIR, 2016."",null,null",null,null
192,"191,""[13] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351­383, 2011."",null,null",null,null
193,"192,""[14] N. Voskarides, E. Meij, M. Tsagkias, M. de Rijke, and W. Weerkamp. Learning to explain entity relationships in knowledge graphs. In Proc. of ACL, 2015."",null,null",null,null
194,"193,""[15] L. Yao, A. Haghighi, S. Riedel, and A. McCallum. Structured relation discovery using generative models. In Proc. of EMNLP, 2011."",null,null",null,null
195,"194,1152,null,null",null,null
196,"195,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 4C: Queries and Query Analysis,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,E icient & E ective Selective ery Rewriting with E iciency Predictions,null,null",null,null
4,"3,Craig Macdonald,null,null",null,null
5,"4,""University of Glasgow Glasgow, Scotland, UK craig.macdonald@glasgow.ac.uk"",null,null",null,null
6,"5,Nicola Tonello o,null,null",null,null
7,"6,""ISTI-CNR Pisa, Italy nicola.tonello o@isti.cnr.it"",null,null",null,null
8,"7,Iadh Ounis,null,null",null,null
9,"8,""University of Glasgow Glasgow, Scotland, UK iadh.ounis@glasgow.ac.uk"",null,null",null,null
10,"9,ABSTRACT,null,null",null,null
11,"10,""To enhance e ectiveness, a user's query can be rewri en internally by the search engine in many ways, for example by applying proximity, or by expanding the query with related terms. However, approaches that bene t e ectiveness o en have a negative impact on e ciency, which has impacts upon the user satisfaction, if the query is excessively slow. In this paper, we propose a novel framework for using the predicted execution time of various query rewritings to select between alternatives on a per-query basis, in a manner that ensures both e ectiveness and e ciency. In particular, we propose the prediction of the execution time of ephemeral (e.g., proximity) posting lists generated from uni-gram inverted index posting lists, which are used in establishing the permissible query rewriting alternatives that may execute in the allowed time. Experiments examining both the e ectiveness and e ciency of the proposed approach demonstrate that a 49% decrease in mean response time (and 62% decrease in 95th-percentile response time) can be a ained without signi cantly hindering the e ectiveness of the search engine."",null,null",null,null
12,"11,""ACM Reference format: Craig Macdonald, Nicola Tonello o, and Iadh Ounis. 2017. E cient & E ective Selective ery Rewriting with E ciency Predictions. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080827"",null,null",null,null
13,"12,1 INTRODUCTION,null,null",null,null
14,"13,""Search engines, such as those for the Web, are required to be e ective at answering users' queries but yet also e cient. In particular, while the relevance of the results are important for users' satisfaction, users are in general not willing to wait long for the results to arrive [36]. While search engines are operated in distributed retrieval se ings that can be scaled horizontally to reduce response times, the rami cation of this is increased cost (in terms of both capital outlay and running, e.g., for power) for the search engine infrastructure. is being the case, the e ciency of the search engine is key to providing e ective results without excessive nancial burden. Typically, the infrastructure is designed to maintain a service level, where high percentile response time (the so-called """"tail latencies"""" [16, 19]) should not exceed a given target. Indeed, for the Bing search engine, the target is reported to be that 99% percentile response time should not exceed 100 ms [19]."",null,null",null,null
15,"14,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080827"",null,null",null,null
16,"15,Table 1: Example rewrites for the query `poker tournament'.,null,null",null,null
17,"16,Original query: poker tournament Stemming: poker #syn(tournaments tournament) Proximity: poker tournament #1(poker tournament)0.1,null,null",null,null
18,"17,#uw8(poker tournament)0.1 Stemming and poker #syn(tournaments tournament),null,null",null,null
19,"18,Proximity: #1(poker #syn(tournaments tournament))0.1 #uw8(poker #syn(tournaments tournament))0.1,null,null",null,null
20,"19,""On the other hand, techniques that bene t the e ectiveness of a search engine may hinder e ciency [41], due to their complex nature. For example, in a modern search engine deploying learningto-rank approaches, the number of features to be computed and the learned models both contribute complexity, and have been the subject of recent studies (e.g., [25]). However, the time to traverse the inverted index's posting lists for the query terms, to identify the top K documents -- which are then re-ranked by the learned approach -- takes signi cant time [14]."",null,null",null,null
21,"20,""O en the query submi ed by the user is internally rewri en by the search engine to improve the quality of the search results [20, 33]. For instance, traditional pseudo-relevance feedback approaches typically results in a much larger query, with signi cant negative impact on e ciency. More recently, less aggressive query rewriting approaches such as term proximity [30], query substitutions [20] and query-time stemming [34] have been deployed by search engines. Each query rewriting approaches can lead to a query with additional terms, resulting in prolonged execution times."",null,null",null,null
22,"21,""Table 1 shows three possible rewritings of the query `poker tournament', based on application of combinations of stemming [34] and sequential dependence (proximity) [30]. In the rewri en examples using an Indri-like query language, complex query operators [37] denote additional postings lists that must be traversed during retrieval, namely: #syn, which combines the constituent terms into a single posting list; #1 creates a posting list representing an exact occurrence of an n-gram; and #uw creates a posting list that provides the number of times a n-gram appears in an unordered window of size  [37]. While the search engine may have indexed posting lists for some n-grams, not all possible query operators may have existing posting lists, and hence ephemeral posting lists are required, which need to be created on-the- y from the constituent terms. Moreover, statistics such as the total number of postings in an ephemeral posting list are unknown, and hence the number of postings to be processed and the resulting execution time of a query containing complex operators cannot be known in advance."",null,null",null,null
23,"22,""Indeed, while recent work in query e ciency prediction has shown the possibility of estimating the execution time of a query prior to its processing [19, 21, 29, 38], none of the existing work"",null,null",null,null
24,"23,495,null,null",null,null
25,"24,Session 4C: Queries and Query Analysis,null,null",null,null
26,"25,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
27,"26,Query,null,null",null,null
28,"27,Query Rewriting,null,null",null,null
29,"28,Rewritten Query,null,null",null,null
30,"29,Top K Processing,null,null",null,null
31,"30,Features Lookup and Calculation,null,null",null,null
32,"31,Documents,null,null",null,null
33,"32,Learned Ranking Function,null,null",null,null
34,"33,Unigram Inverted,null,null",null,null
35,"34,Index,null,null",null,null
36,"35,Top K Retrieval,null,null",null,null
37,"36,Features Repository,null,null",null,null
38,"37,Feature Extraction,null,null",null,null
39,"38,Learning to Rank Technique,null,null",null,null
40,"39,Learned Model Application,null,null",null,null
41,"40,Training Data,null,null",null,null
42,"41,Figure 1: A pictorial representation of the reference web search engine architecture that we consider in this work.,null,null",null,null
43,"42,""has considered the execution time of queries containing query operators that generate ephemeral posting lists, such as #syn, or #1."",null,null",null,null
44,"43,""is makes it di cult to select among query rewriting strategies that use such operators, as their likely execution time is unknown. Hence, in this work, we study the cost of scoring ephemeral posting lists, and use these observations to de ne accurate query e ciency predictions for advanced query operators. Furthermore, we use these query e ciency predictions to instantiate a novel mechanism that selects the best strategy among alternative query rewritings, to improve e ciency, while minimising impact on e ectiveness."",null,null",null,null
45,"44,""Our conducted experiments to measure the e ciency of our proposed selective mechanism upon TREC Web track test collections show that a 49% decrease in mean response time, and 62% decrease in tail (95th-percentile) response time, can be a ained without signi cantly hindering the e ectiveness of the search engine. e contributions of this work are as follows: we show how to make query e ciency predictions for ephemeral posting lists created by complex operators such as #syn and #1; we use these advanced query e ciency predictions to propose a selector mechanism that permits the query to be rewri en in an e ective manner while considering a target response time that the search engine should aim to meet."",null,null",null,null
46,"45,""e remainder of this paper is structured as follows: Section 2 provides an overview of a reference search engine architecture, describing the necessary background; Section 3 positions our contributions with respect to existing work; Section 4 describes our mechanism for selecting among query rewrites; Section 5 proposes new query e ciency predictors suitable for application to complex query operators. Our experimental setup and results follow in Sections 6 & 7. Finally, we provide concluding remarks in Section 8."",null,null",null,null
47,"46,2 PRELIMINARIES,null,null",null,null
48,"47,""In this section, we provide some essential background on index organisation and query processing in search engines. In doing so, we follow the reference architecture for a search engine depicted by Figure 1. e following section summarises and discusses the stateof-the-art query rewriting techniques and approaches addressing e cient but e ective retrieval."",null,null",null,null
49,"48,""Index Organisation. Given a collection D of documents, each document is identi ed by a non-negative integer called document identi er, or docid di . A posting list It is associated to each term t appearing in the collection, containing the list of the docids of all the documents in which the term occurs at least once. e collection of the posting lists for all of the terms is called the inverted index of D,"",null,null",null,null
50,"49,""while the set of the terms is usually referred to as the lexicon. For each term t, the lexicon stores a pointer to its posting list as well as additional information on the statistics of the term in the collection, such as its document frequency Nt , i.e., the length of its posting list, and the total number of occurrences of the term in the collection Ft . Each posting in a posting list typically contains additional information about the term's occurrences in the document, such as the number of occurrences ft,d , and the set of positions, pt,d , where the term t occurs [13]. is position information facilitates phrasal retrieval without resort to large n-gram index data structures."",null,null",null,null
51,"50,""e docids in a posting list can be sorted in increasing order enabling the use of e cient compression algorithms and query processing [31]; or the posting lists can be frequency-sorted [39] or impact-sorted [2], allowing for good compression rates, but also presenting practical disadvantages such as their di culty of use for phrasal queries [22, 37]. As such, in this paper, we focus on the more common search scenario of docid-sorted index layouts [15]."",null,null",null,null
52,"51,""ery Processing. e top K ranked retrieval stage identi es the K highest scored documents in the collection, where the relevance score is a function of the query-document pair. Multi-stage retrieval systems have become the dominant model for e cient and e ective web search engines [14]. In such systems (see Figure 1), a rst """"top K"""" stage retrieves from the inverted index a relatively small set of K possibly-relevant documents matching the user query, focusing on optimising recall. Subsequent stages compute additional query dependent (e.g., elds [28], proximity) and query independent features, before applying a learning-to-rank technique to re-rank the K documents coming from the rst stage, aiming to maximise measures like NDCG [25]. e inverted index posting lists are processed in the rst stage only, to produce a small set of candidate documents, that will be re-ranked in the subsequent stage(s)."",null,null",null,null
53,"52,""Documents are typically scored in the rst stage retrieval by the (weighted) linear combinations of weighting model (e.g., BM25, language models) functions computed for each term-document pair. Such weighting models are usually monotonically increasing in the number of occurrences of the term in the document ft,d . An obvious way to compute the top K scored documents is to exhaustively apply the weighting model to all the documents that match at least one query term in the inverted index. As such an exhaustive method is very expensive for large collections, several dynamic pruning techniques have been proposed in the last few years. Dynamic pruning makes use of the inverted index, augmented with additional data structures, to skip documents that cannot reach a su cient score to enter the top K. us, the nal result is the same as an exhaustive evaluation, but obtained with signi cantly less work. ese techniques include MaxScore [39], WAND [6], and BMW [17]. In this paper, we focus our a ention on the WAND strategy, since we deal with (rewri en) queries that can have a large number of terms (i.e., long queries). Indeed, several previous studies have con rmed that MaxScore performs be er than WAND for short queries while the opposite happens for long queries [18, 31] while, as we will discuss in Sec. 5, the BMW techniques are not suitable for processing rewri en queries."",null,null",null,null
54,"53,""WAND augments the posting list of each term t with an upper bound t on the maximum score of that term among all documents in the list. While processing the query by iterating on the posting lists of its terms, it records the top K scores among the documents"",null,null",null,null
55,"54,496,null,null",null,null
56,"55,Session 4C: Queries and Query Analysis,null,null",null,null
57,"56,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
58,"57,""evaluated thus far. To enter the top K, a new document needs to have a larger score than the current K-th score, which we call the min score. WAND maintains the posting list iterators sorted by increasing docid; at every step, it sums up the maximum scores of the lists in increasing order, until the min score is reached. It can be seen that the current docid of the rst list that exceeds the min score is the rst docid that can reach a score higher than the min score, so the other iterators can safely skip all the documents up to that docid. e alignment of the posting lists during WAND processing is achieved by means of a nextt (d) method upon the posting list iterators, which returns the smallest docid in the posting list It that is greater than or equal to d. is functionality signi cantly enhances the retrieval speed exhibited by WAND, by skipping docids that would never be retrieved in the top K, and hence avoiding their decompression and scoring. Indeed, smaller values of K allow for more skipping, since the threshold is in general larger for small values of K than for large values, resulting in smaller query processing times, as reported, for example, in [38]."",null,null",null,null
59,"58,3 RELATED WORK,null,null",null,null
60,"59,""In the following, we survey existing work in query rewriting and in query e ciency predictions, and position our work accordingly."",null,null",null,null
61,"60,ery Rewriting. ere are a number of related works across the areas of query rewriting and e cient yet e ective retrieval.,null,null",null,null
62,"61,""e internal rewriting of a user's query within an IR system has a long history, including pseudo-relevance feedback in the form of automatic query expansion, rst deployed by the SMART system in TREC-3 [7], while others considered the correction of spelling errors or the application of ontologies to identify related concepts [13, Ch. 6]. Indeed, query expansion, which adds additional terms to the query based upon their appearance in the top ranked documents, has been shown to be e ective for adhoc retrieval tasks in evaluation forums such as TREC [42]. For web search, the signi cantly longer generated queries, as well as the need to conduct two retrieval phases, make pseudo-relevance feedback approaches infeasible for e cient retrieval. A further risk is the possibility that the topic of the expanded query can dri from the intent of the initial query."",null,null",null,null
63,"62,""Instead, some of the techniques widely deployed in web search have focused on rewriting the query based on the large amounts of user interaction data available to a web search engine. For instance Jones et al. [20] describe a way to mine common query reformulation pa erns, based on log likelihood ratio, that can be automatically applied to re ne a new query. Random walks on the query-click graph [12] o er similar possibilities for identifying common paraphrasing queries."",null,null",null,null
64,"63,""Rewriting the query to include common variants of the original query terms can have an important e ectiveness bene t in addressing the word mismatch problem. Indeed, a query-side approach to stemming has a marked advantage of index-time stemming, in that the other words within the query can be taken into account to decide if the stemming is appropriate for a given word. For instance, Peng et al. [34] describe a context-sensitive stemming approach where query segments1 are carefully considered for stemming, by comparing the language model generation probability of both the original and the replacement segments. Naturally, adding additional terms"",null,null",null,null
65,"64,""1 N-gram subsequences of queries that demonstrate the underlying grammatical structure, usually determined by dividing longer sequences to maximise n-gram language model probabilities."",null,null",null,null
66,"65,""to the query can have a marked negative impact on e ciency, hence, as noted by Peng et al., it is not desirable to rewrite queries unnecessarily. In our work in this paper, query rewriting by application of stemming is one of the query rewriting techniques that we consider."",null,null",null,null
67,"66,""One method of query rewriting that has gained signi cant bene ts in e ectiveness is the application of term dependence (proximity) operators, to boost the retrieval of documents where the query terms occur close together [30, 35]. In particular, Metzler & Cro 's Markov Random Field sequential dependence model makes use of the Indri complex query operators #1 and #uw formed from adjacent pairs of query terms, added to the original query terms with low weights (typically [0.05,0.1]). However, such rewri en queries have a negative e ciency impact, in that more posting lists must be traversed, while if the index only has unigram posting lists, ephemeral posting lists must be created to handle the #1 and #uw operators2. Another variant, the full dependence model ­ which adds complex query operators for each pair of query terms ­ is generally considered too ine cient for common retrieval use [4, 30]. Hence, for large-scale environments, there has to be a perceived bene t in deploying such a term dependence model, due to its inherent negative e ciency impact. For this reason, we note various works that extract term dependence proximity features at the re-ranking stage [28, 38] ­ an approach that we deploy within our baseline retrieval system in this paper."",null,null",null,null
68,"67,""On the other hand, motivated by the ine ciencies in deploying sequential dependence, Wang et al. [41] proposed an e cient variant where the weights for bi-gram operators were adjusted to jointly optimise combinations of e ectiveness and e ciency, and bi-grams predicted not to be useful were eliminated. In this way, the work of Wang et al. is one of the closest to our work. However, their features for estimating the cost of the #1 and #uw bi-gram complex operators assumed the existence of bi-gram index statistics, something that our approach does not require; Moreover, their experiments did not consider deployment under a dynamic pruning strategy, where the e ciency cost of additional complex proximity operators, ­ which have low weighting in the query (see Table 1) ­ may be markedly reduced by the pruning. Finally, as we consider more than just proximity rewriting, our work is more general than that of Wang et al. [41]."",null,null",null,null
69,"68,""E ciency Predictions. Using a docid-sorted index layout, the time taken for a query to execute is correlated with the length of the posting lists of the query's constituent terms, as these posting lists must be traversed during execution. Dynamic pruning techniques such as WAND and BMW o er some relief as they o er the potential to safely skip the decompression of postings and the scoring of documents that cannot make the current top K. is makes the exact response time of a query di cult to predict, as not every posting in the postings lists will be decompressed and scored. Nevertheless recent work has considered making accurate predictions on the e ciency of a query, either in terms of absolute response time [29], or in terms of those queries with response times exceeding a threshold [19, 21]."",null,null",null,null
70,"69,""E ciency predictions facilitate a number of applications for ensuring e cient yet e ective retrieval - for instance, routing queries"",null,null",null,null
71,"70,""2 For instance, according to the recent IR system reproducibility e ort [23], Indri's average response time is decreased by a factor of 6 on deploying sequential dependence on the Gov2 corpus."",null,null",null,null
72,"71,497,null,null",null,null
73,"72,Session 4C: Queries and Query Analysis,null,null",null,null
74,"73,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
75,"74,""among busy replicated query shard servers [29]; selectively deploying multiple CPU cores for slow queries [19, 21]; or adjusting the pruning aggressiveness or size of K for di erent queries [5, 14, 38]. Of these, the work of Tonello o et al. [38] is among the most similar to ours, in that they vary the number of documents to be retrieved, K, as well as the pruning aggressiveness, before passing to a learning-to-rank re-ranking phase, based on the predicted execution time of the query. Similarly, in a very recently published work, Culpepper et al. [14] de ned an approach for training the rank cuto in a multi-stage ranking system based on closeness in overlap to a """"reference"""" system. However, their approach has a key disadvantage in that they use a simple reference system, and hence would not demonstrate the bene t in going beyond that system, for instance in deploying advanced query rewrites."",null,null",null,null
76,"75,""Indeed, di erently from [14] & [38], in this paper, we go further by considering a prediction of the execution time of possible rewritings of the users' original queries. is is made possible by the novel prediction of the execution time of complex query operators such as #syn and #uw. In the following, we rstly de ne the problem and introduce our selection mechanism (Section 4), before de ning how to obtain query e ciency predictions for queries involving complex operators in Section 5."",null,null",null,null
77,"76,4 SELECTING AMONG QUERY REWRITINGS,null,null",null,null
78,"77,4.1 Problem Statement,null,null",null,null
79,"78,""is work considers the e cient yet e ective rewriting of a given query q. Indeed, the search engine may consider several possible ways to reformulate the query into a re ned instance q, for instance, by spli ing compound words, adding alternative words identi ed using stemming algorithms or common query reformulations approaches, or adding proximity terms such as #1. Some such query rewritings can result in a longer query formulation, hindering its e ciency compared to the original query [33]."",null,null",null,null
80,"79,""Reformulating the query in a multi-stage ranking system ­ such as one deploying learning to rank ­ can be seen as aiming to improve the recall of the K documents retrieved in the Top K Retrieval stage. is ensures that when re-ranking the documents by the application of the learned model, the search engine has a high chance of identifying the most relevant documents for promotion to the top of the ranked list for presentation to the user. Naturally, improving the formulation of the query may also increase the high precision of these K documents ­ i.e., by retrieving more relevant documents towards the top of that initial list. is suggests that some rewri en queries only need a smaller K  < K. Moreover, as mentioned above, the e ciency of dynamic pruning techniques like WAND is bene ted by smaller K."",null,null",null,null
81,"80,""Hence, with various di erent rewriting techniques available, a natural question arises: for a given query q, which possible rewritings are appropriate to be applied to the query, q1 . . . qn , such that e ectiveness may be improved, and/or, K reduced to improve e ciency, without signi cantly damaging the overall e ectiveness."",null,null",null,null
82,"81,4.2 Selection Mechanism,null,null",null,null
83,"82,""To achieve this, we make use of query e ciency predictions that estimate the execution time of di erent rewritings of the query, before one is selected and executed. A particular challenge in doing so is making accurate estimations of the execution times of"",null,null",null,null
84,"83,""rewri en queries that use operators such as #syn, #1 and #uw. We discuss this further in Section 5."",null,null",null,null
85,"84,""Firstly, we generate all possible rewritings {q1 . . . qn } of the original query q. We note that the size of this set varies for each query q, as not all rewritings are applicable to each query ­ for instance, no term dependence can be applied to a query with only a single term, or no stemming may be applicable for each query term. At the very least, the original query will always be present."",null,null",null,null
86,"85,""We also consider m di erent K values for the number of documents to retrieve in the rst retrieval phase, which will then be re-ranked by application of the learned model, namely K1 . . . Km . In doing so, our intuition is that for some queries, simply identifying K ,"""" 20 documents will be su cient to identify enough relevant documents, leading to marked e ciency bene ts, particularly if the rewri en query permits higher recall of relevant documents within the top K. A possible plan for executing a query can be denoted as the tuple qi, Kj . All possible query plans, denoted P (q), for executing the query can be generated from the Cartesian product:"""""",null,null",null,null
87,"86,""P (q) ,"""" qi, Kj """", {q1 . . . qn } × {K1 . . . Km }"",null,null",null,null
88,"87,(1),null,null",null,null
89,"88,""e aim is then to rank and eliminate plans qi, Kj based on their predicted e ciency and expected e ectiveness. While the number of possible rewrites for each query varies, |P (q)|  m."",null,null",null,null
90,"89,""Consider that the search engine has a service level agreement in place [19], which aims to maximise the number of queries answered in time  . Some plans for queries may exceed  . We use query e ciency predictions, denoted t ( qi, Kj ) to eliminate such plans:"",null,null",null,null
91,"90,""EP (q,  ) ,"""" qi, Kj  P (q) | t ( qi, Kj )  """""",null,null",null,null
92,"91,(2),null,null",null,null
93,"92,""Naturally, plans will vary in e ectiveness. One possible approach to select among the feasible plans EP (q,  ) would be to try to predict the e ectiveness of a given rewriting of a query. However, Tonello o et al. [38] examined the usefulness of query performance (e ectiveness) predictors within their selective pruning mechanism, and found them to have li le correlation with maintaining e ectiveness while enhancing e ciency. Moreover, we are not aware of any existing works that make e ectiveness predictions in the presence of complex operators used by some rewritings."",null,null",null,null
94,"93,""Instead, to determine the likely e ectiveness of a plan qi, Kj , we measure the expectation of the e ectiveness for some measure µ (e.g., NDCG) of that given rewriting upon a set of training queries Qtr. Denoting with Qitr, Kj the set of query plans for the rewri en queries according to the i-th rewriting, we compute the expected e ectiveness of measure µ over all such query plans, Eµ ( Qitr, Kj ). However, due to excessively long posting lists, some queries may not have any plans that can be executed in time  , i.e., |EP (q,  )| ,"""" 0. For this reason, for such queries, we resort to a best a empt, by selecting the plan with the fastest predicted execution time. e"""""",null,null",null,null
95,"94,""nal selection mechanism to identify the best plan F P (q,  ) for executing the (possibly rewri en) query q in time  is as follows:"",null,null",null,null
96,"95,F,null,null",null,null
97,"96,""P (q, "",null,null",null,null
98,"97,),null,null",null,null
99,"98,"","",null,null",null,null
100,"99,argmax Eµ,null,null",null,null
101,"100,""qi, Kj  E P (q, )"",null,null",null,null
102,"101,(,null,null",null,null
103,"102,""Qitr, Kj"",null,null",null,null
104,"103,argmin t (,null,null",null,null
105,"104,""qi, Kj  E P (q, )"",null,null",null,null
106,"105,""qi, Kj"",null,null",null,null
107,"106,),null,null",null,null
108,"107,),null,null",null,null
109,"108,""if |EP (q,  )| > 0 otherwise"",null,null",null,null
110,"109,(3),null,null",null,null
111,"110,e usefulness of this mechanism is driven by the need to have ac-,null,null",null,null
112,"111,""curate estimation of the time to execute a given query plan, namely t ( qi, Kj ). Moreover, as mentioned above, the estimation of the"",null,null",null,null
113,"112,498,null,null",null,null
114,"113,Session 4C: Queries and Query Analysis,null,null",null,null
115,"114,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
116,"115,Table 2: Complex operators summary table.,null,null",null,null
117,"116,Complex operator,null,null",null,null
118,"117,#syn #1 #uw,null,null",null,null
119,"118,Complex term,null,null",null,null
120,"119,""#syn(car, cars) #1(new, york) #uw8(divx, codec)"",null,null",null,null
121,"120,Ephemeral posting list,null,null",null,null
122,"121,Disjunctive/OR Conjunctive/AND Conjunctive/AND,null,null",null,null
123,"122,""execution time of complex query operators, particularly under dynamic pruning strategies such as WAND, have not previously been addressed. In the next section, we propose a novel method to address this problem, by using machine-learned models to predict the execution times of query plans that use complex operators."",null,null",null,null
124,"123,5 EFFICIENCY PREDICTION FOR COMPLEX OPERATORS,null,null",null,null
125,"124,""e complex operators involved in stemming and proximity rewritings are summarised in Table 2. Such complex operators can be applied to two or more uni-gram terms, as well as other complex operators, i.e., complex operators can be nested. Complex operators, such as #1, generate complex terms once instantiated, such as #1(new, york). In the following, we discuss how the posting lists for such complex terms are generated (Section 5.1) and how they can be processed together with the original terms in the WAND strategy (Section 5.2). Section 5.3 presents our approach for predicting the query processing time of queries containing complex terms."",null,null",null,null
126,"125,5.1 Complex terms and ephemeral posting lists,null,null",null,null
127,"126,""While posting lists for simple (e.g., uni-gram) terms are stored in the inverted index, it is not feasible to pre-compute statistics and posting lists for complex terms, since their space occupancy will quickly become unmanageable, particularly if complex operators can be nested. Hence, for a complex term #op(t1, . . . , #op1, . . .), its posting list and statistics must be generated on-the- y, such that it can be processed together with other simple and complex terms. Such ephemeral posting lists are materialised as required, depending on the complex operator and the involved terms. We consider two types of ephemeral posting lists: disjunctive/OR-based lists and conjunctive/AND-based lists (see Table 2). OR-based posting lists are used with #syn operators, where the posting lists of two or more terms are merged into a single posting list containing all docids appearing in at least one of the terms' posting lists. ANDbased posting lists are used with #1 and #uw operators, where two or more terms' posting lists must be intersected, i.e., a posting appears in the ephemeral posting list only if it is present in all of the involved posting lists."",null,null",null,null
128,"127,""e involved complex operator de nes how to merge postings into ephemeral posting lists. In OR-based ephemeral posting lists, when the postings of two di erent terms that refer to the same docid are merged, their term frequencies in a document must be added, and the positions arrays must be merged. Similarly, while the terms' frequencies in the collection (Ft ) must be summed, we cannot know in advance the document frequencies (Nt ) of the resulting ephemeral posting list, as the number of docids in common between the two posting lists is unknown. In AND-based ephemeral posting lists, there is no general way to merge term frequencies in documents, while term positions must be stored separately and processed depending on the semantics of the complex"",null,null",null,null
129,"128,""operator [24]. Again, the document frequency Nt of the resulting ephemeral posting list, i.e., its length, cannot be known in advance."",null,null",null,null
130,"129,5.2 ery Processing,null,null",null,null
131,"130,""Given a user query q composed of simple terms, let us assume it"",null,null",null,null
132,"131,""is rewri en into a query composed of simple terms and complex terms, collectively denoted by q. Complex terms are obtained by"",null,null",null,null
133,"132,applying complex operators ­ summarised in Table 2 ­ to simple or,null,null",null,null
134,"133,""other complex terms, in a nested fashion. e score of a document d w.r.t. the rewri en query q can be expressed as follows:"",null,null",null,null
135,"134,""s (q, d ) ,"",null,null",null,null
136,"135,""wt st,d +"",null,null",null,null
137,"136,wt,null,null",null,null
138,"137,st,null,null",null,null
139,"138,"","",null,null",null,null
140,"139,d,null,null",null,null
141,"140,"","",null,null",null,null
142,"141,(4),null,null",null,null
143,"142,t qd,null,null",null,null
144,"143,t  (q\q)d,null,null",null,null
145,"144,where wt (resp. wt) denotes the simple (resp. complex) terms,null,null",null,null
146,"145,weights,null,null",null,null
147,"146,(see,null,null",null,null
148,"147,Table,null,null",null,null
149,"148,""1),"",null,null",null,null
150,"149,while,null,null",null,null
151,"150,""st,d"",null,null",null,null
152,"151,and,null,null",null,null
153,"152,st,null,null",null,null
154,"153,"","",null,null",null,null
155,"154,d,null,null",null,null
156,"155,are,null,null",null,null
157,"156,weighting,null,null",null,null
158,"157,models,null,null",null,null
159,"158,for simple and complex terms respectively. e linearity of Eq. (4),null,null",null,null
160,"159,""makes it easily usable in any exhaustive query processing algorithm,"",null,null",null,null
161,"160,by exploiting ephemeral posting lists as normal posting lists.,null,null",null,null
162,"161,""Conversely, dynamic pruning techniques are not directly usable,"",null,null",null,null
163,"162,""since they rely on the maximum score t of each query term, cal-"",null,null",null,null
164,"163,""culated among all documents in the respective posting lists, i.e.,"",null,null",null,null
165,"164,""t ,"""" wt · maxd It st,d . ese maximum scores can be computed o ine by taking the score value of the top document of a single"""""",null,null",null,null
166,"165,""term query stored in the lexicon. However, since ephemeral post-"",null,null",null,null
167,"166,""ing lists are materialised on-the- y, such a computation cannot be"",null,null",null,null
168,"167,""performed o ine, hence we must resort to a runtime estimation for"",null,null",null,null
169,"168,""upper bounds on these maximum scores. In [26], the authors pro-"",null,null",null,null
170,"169,posed a general framework to approximate the term upper bounds,null,null",null,null
171,"170,for proximity weighting models that monotonically increase with,null,null",null,null
172,"171,""respect to the frequency variable, called MaxTF. In that framework,"",null,null",null,null
173,"172,the upper bound t for a term t is computed by using the maximum,null,null",null,null
174,"173,term frequency fmax (t ) that appears in the term's posting list as,null,null",null,null
175,"174,""input for the scoring model, i.e.,"",null,null",null,null
176,"175,""t ,"""" wt st,d"""""",null,null",null,null
177,"176,""fmax (t ) ,"",null,null",null,null
178,"177,where,null,null",null,null
179,"178,fmax (t ),null,null",null,null
180,"179,"","",null,null",null,null
181,"180,max,null,null",null,null
182,"181,d It,null,null",null,null
183,"182,""ft,d ."",null,null",null,null
184,"183,(5),null,null",null,null
185,"184,""In our experiments, we exploit the DLH13 weighting model [1]"",null,null",null,null
186,"185,""for #syn operators and simple terms, and the pBiL dependence weighting model [35] for #uw and #1 operators3. Both models"",null,null",null,null
187,"186,are a generalisation of the parameter-free hypergeometric DFR,null,null",null,null
188,"187,""model in a binomial case, DLH13 for simple terms while pBiL for"",null,null",null,null
189,"188,""n-grams, and both are monotonically increasing with respect to"",null,null",null,null
190,"189,""the frequency variable ft,d . Hence, given a complex term, we can compute a term upper bound by using the maximum term/n-gram"",null,null",null,null
191,"190,frequency computed from the corresponding ephemeral posting,null,null",null,null
192,"191,""list, as in the MaxTF framework. Unfortunately, even computing"",null,null",null,null
193,"192,these maximum frequencies is impossible without a complete view,null,null",null,null
194,"193,""of the posting lists, hence we must resort to a further upper bound"",null,null",null,null
195,"194,""on those frequencies, easily computed as follows:"",null,null",null,null
196,"195,""fmax #syn(t1, t2) ,"""" fmax (t1) + fmax (t2),"""""",null,null",null,null
197,"196,""fmax #1(t1, t2) ,"""" min fmax (t1), fmax (t2) ,"""""",null,null",null,null
198,"197,(6),null,null",null,null
199,"198,""fmax #uw(t1, t2) ,"""" min fmax (t1), fmax (t2) ."""""",null,null",null,null
200,"199,""3 We use these models as they are parameter free, and their MaxTF upper-bound approximations were proven in [26]; However, the method we describe here is equally applicable for scoring simple and complex terms using Dirichlet language modelling. BM25 cannot be applied, as Nt is not available while scoring complex terms."",null,null",null,null
201,"200,499,null,null",null,null
202,"201,Session 4C: Queries and Query Analysis,null,null",null,null
203,"202,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
204,"203,""For instance, consider an AND-based ephemeral posting list (such as #1): no document can have more occurrences of the n-gram than the minimum of the maximum frequencies of the involved terms that has been observed in their posting lists. For the OR-based #syn operator, the worst-case maximum frequency would occur if a single document had the maximum frequencies of the constituent terms. Hence, the maximum frequency within a #syn ephemeral posting list cannot be greater than the sum of the maximum observed frequencies of the respective constituent posting lists."",null,null",null,null
205,"204,""Such term upper bounds, computed by substituting Equations (6) into Equation (5) as required, can be used in dynamic pruning strategies such as MaxScore and WAND, which leverage global upper bounds on each term, both simple and complex. Conversely, query processing strategies such as BMW leverage local term upper bounds: each posting list is split into consecutive blocks of constant size, e.g., 128 postings per block, and, for each block, a score upper bound is computed and stored, together with largest docid of each block. is cannot be done with ephemeral posting lists, since their sequences of postings is not known in advance. Hence, no block score upper bounds can be computed or stored."",null,null",null,null
206,"205,5.3 Predicting Complex Operator E ciency,null,null",null,null
207,"206,""e time spent processing a query in dynamic pruning strategies depends on several factors, the most important being: (i) the number of terms to be processed, (ii) the total number of postings to be processed and (iii) the relative importance of terms, i.e., their score contribution to the overall relevance of a document [29]. As shown in [19, 21, 29, 38], by using statistics derived from terms and queries it is possible to estimate the query processing time. However, since we must deal with ephemeral posting lists, most of the statistics used in prior research are not applicable. For example, the mean scores of postings (used in [29]) cannot be precomputed for an ephemeral posting list, since the postings and their statistics for all possible n-grams cannot feasibly be computed o ine. Hence, we must resort to """"estimators"""" of the quantities of interest, in particular for the total number to be processed, i.e., the simple and ephemeral posting list lengths, and the corresponding term upper bounds."",null,null",null,null
208,"207,""To provide an upper bound approximation to the number of postings in an ephemeral posting list, consider a generic complex term #op(t1, t2), where t1 and t2 can be simple or (nested) complex terms. If #op corresponds to an OR-based posting list such as #syn, then the size of its posting list cannot be greater than the sum of the document frequencies of its constituent terms. If #op corresponds to an AND-based posting list, then the total size of its postings list cannot be greater than the minimum of the document frequencies of its constituent terms, i.e., the smallest constituent posting list."",null,null",null,null
209,"208,""For term scores upper bound approximations, we leverage the MaxTF framework, adapted to complex operators, as summarised in Eq. (6). Upper bound approximations are scaled according to the term-speci c weight resulting from query rewriting (see Table 1)."",null,null",null,null
210,"209,""We adopt a machine-learned approach to predict the processing times of complex queries, i.e., t ( qi, Kj ), for di erent values of K. As we use the WAND dynamic pruning strategy (as justi ed in Section 2), the response times heavily depend on the K number of documents to be retrieved. Hence, we train a di erent model for each value of K, but all models share the same set of statistics. All previous works on query processing time prediction use of query statistics and aggregations (max, min, mean etc.) of term-based"",null,null",null,null
211,"210,""Table 3: Prediction statistics, projectors and aggregators."",null,null",null,null
212,"211,Statistics,null,null",null,null
213,"212,1. Number of terms (query-based) 2. Document frequency (term-based) 3. Score upper bound (term-based),null,null",null,null
214,"213,Projectors,null,null",null,null
215,"214,""1. Global 2. Original-only terms 3. #syn-only terms 4. #1-only terms 5-... #uw-only terms, one per di erent "",null,null",null,null
216,"215,Aggregators,null,null",null,null
217,"216,""1­2. Minimum, Maximum 3­5. Arithmetic, Harmonic, Geometric Means"",null,null",null,null
218,"217,""statistics across the query terms, as reported in Table 3. However, di erent complex operators acting on the same set of terms can have very di erent impacts on the running time of a query. Hence, we propose an additional, intermediate step between statistics generation and aggregation, namely projection. In this step, all query- and term-based statistics are divided into subsets, whose elements are grouped depending on the nature of the term. us we have several statistics projections, one for every complex operator, one for simple terms and one considering all terms globally. Term-based aggregators are then applied to these subsets of statistics. Note that, due to the di erent nature of AND- and OR-based posting lists, we do not consider the sum and variance operators, while we include the minimum operator, as suggested by [19]. In our experiments, we use #uw8 and #uw12 operators, hence we have 6 query-based features and 2×6×5 term-based features, for a total of 66 features. Section 7.1 provides the details and parameters of the learning algorithm using these feature to predict the processing times of complex queries."",null,null",null,null
219,"218,6 EXPERIMENTAL SETUP,null,null",null,null
220,"219,6.1 Research estions,null,null",null,null
221,"220,""In the following, we experiment to address two research questions: RQ1: How accurate are our query e ciency predictions for queries with complex operators? (Section 7.1) RQ2: Can we maintain e ectiveness while reducing response time when selectively rewriting queries? (Section 7.2)"",null,null",null,null
222,"221,""In the remainder of this section, we de ne the experimental setup under which our experiments are conducted."",null,null",null,null
223,"222,6.2 Datasets & Retrieval System,null,null",null,null
224,"223,""Our experiments address both e ciency and e ectiveness, and hence require diverse setups to ensure accurate conclusions can be drawn for both types of measures. All of our experiments are conducted on the TREC ClueWeb09 category B corpus4, which consists of 50M Web documents. For testing e ectiveness, we use the 197 queries from the TREC Web tracks 2009-2012 that have corresponding relevance assessments on a 4-point scale [9], and denote this as WT. For testing e ciency, we follow best practices in"",null,null",null,null
225,"224,4 h p://lemurproject.org/clueweb09/,null,null",null,null
226,"225,500,null,null",null,null
227,"226,Session 4C: Queries and Query Analysis,null,null",null,null
228,"227,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
229,"228,""sampling a signi cant number of queries from a real search engine, namely 1,956 successive queries from the MSN 2006 query log5 [11]."",null,null",null,null
230,"229,""We index all 50M documents of the ClueWeb09 corpus using the Terrier IR platform [32], including also the anchor text of incoming hyperlinks to each document. Position information is recorded for each term. Our index is compressed using Elias-Fano encoding provided in [40], widely considered to be the state-of-the-art in terms of fast decompression."",null,null",null,null
231,"230,""For retrieval, we conduct e ciency timings using a machine equipped with an AMD Opteron Processor 6276 with 6 MB L3 cache and 128 GB RAM. e entire index is loaded in memory. All experiments are performed on a single core. While the resulting retrieval times using a single machine for retrieval are marginally higher than would be expected for interactive retrieval in a deployed Web search engine, following previous work [41], this does not detract from the generality of the ndings, and avoids the complexities of performing experiments in a distributed retrieval environment."",null,null",null,null
232,"231,""Finally, in our timing experiments, we do not include the time to rewrite the query, calculate e ciency predictions, nor to apply the learned model. Each of these stages is comparatively cheap: for instance, the rewriting approaches discussed below are commonly deployed in search engines; query e ciency predictions can be calculated quickly using only term statistics from the lexicon [19, 29]; and the application of a learned model is also relatively less expensive than the top-K retrieval [38]."",null,null",null,null
233,"232,6.3 ery Plans,null,null",null,null
234,"233,""Besides the None strategy, where the original queries are not rewritten, we deploy the following rewriting approaches:"",null,null",null,null
235,"234,""MRF. To encapsulate proximity in rewriting a query, we deploy sequential dependence [30] proximity, by considering #1, #uw8 and #uw12 query operators for adjacent query terms."",null,null",null,null
236,"235,""Na¨ive. To address word mismatch, we rewrite the query by adding alternatives to query terms within a #syn query operator6. Inspired by Peng et al. [34], who noted corpus analysis as being a suitable method to determine similar words (based on which words they co-occur with). We select alternative terms for a given term t that each have the same stem as t based on Porter's stemmer, and which are among the M most similar terms to t within a word embedding space. We use Deeplearning4j's word2vec tool and word embeddings vectors trained on Wikipedia and the Gigaword corpus7 and select words with common stems in the top M , 20 for each query term t. is results in a less aggressive stemming than either index-time stemming or the equivalent query-side stemming using all alternatives identi ed by a Porter stemmer."",null,null",null,null
237,"236,""Na¨iveMRF. Finally, we mix Na¨ive and MRF rewritings to create a nal strategy of generating time-expensive query plans."",null,null",null,null
238,"237,""Table 4 reports the statistics of each query set, incl. the number of simple and complex terms generated by each rewriting approach. Finally, with regards to the value of K, i.e., the number of documents retrieved by WAND during the top K retrieval, we select 4 values, namely 20, 100, 1000 and 5000. is gives us a total of 16 query"",null,null",null,null
239,"238,""5 From a sample of 2000 queries, 44 queries had no matching terms in our collection, so were removed. 6 Initial experiments using the context-sensitive approach of [34] showed no e ectiveness bene t over this simpler Na¨ive stemming. 7 Available from h p://nlp.stanford.edu/projects/glove/"",null,null",null,null
240,"239,Table 4: ery sets statistics per rewriting.,null,null",null,null
241,"240,Dataset Train Test WT,null,null",null,null
242,"241,eries Rewriting simple #syn #1 #uw8 #uw12,null,null",null,null
243,"242,Na¨iveMRF 1458 1125 1133 1133,null,null",null,null
244,"243,0,null,null",null,null
245,"244,978,null,null",null,null
246,"245,MRF Na¨ive,null,null",null,null
247,"246,2556 0 1578 1578 781,null,null",null,null
248,"247,1458 1125 0 0,null,null",null,null
249,"248,0,null,null",null,null
250,"249,None,null,null",null,null
251,"250,2556 0 0 0,null,null",null,null
252,"251,0,null,null",null,null
253,"252,Na¨iveMRF 1452 1028 1185 1185,null,null",null,null
254,"253,0,null,null",null,null
255,"254,978,null,null",null,null
256,"255,MRF Na¨ive,null,null",null,null
257,"256,2468 0 1491 1491 768,null,null",null,null
258,"257,1452 1028 0 0,null,null",null,null
259,"258,0,null,null",null,null
260,"259,None,null,null",null,null
261,"260,2468 0 0 0,null,null",null,null
262,"261,0,null,null",null,null
263,"262,Na¨iveMRF 121 126 113 113,null,null",null,null
264,"263,0,null,null",null,null
265,"264,197,null,null",null,null
266,"265,MRF Na¨ive,null,null",null,null
267,"266,244 0 146 146 82,null,null",null,null
268,"267,121 126 0 0,null,null",null,null
269,"268,0,null,null",null,null
270,"269,None,null,null",null,null
271,"270,244 0 0 0,null,null",null,null
272,"271,0,null,null",null,null
273,"272,Table 5: ery-dependent (QD) & -independent (QI) ranking features within our experiments.,null,null",null,null
274,"273,""QD DLH13, Coordinate Level 2"",null,null",null,null
275,"274,QD pBiL (term dependence),null,null",null,null
276,"275,2,null,null",null,null
277,"276,""QI Inlinks, Outlinks, PageRank 3"",null,null",null,null
278,"277,QI URL features,null,null",null,null
279,"278,6,null,null",null,null
280,"279,QI Content quality [3],null,null",null,null
281,"280,4,null,null",null,null
282,"281,QI Spam score,null,null",null,null
283,"282,1,null,null",null,null
284,"283,Total,null,null",null,null
285,"284,18,null,null",null,null
286,"285,""plans to be plugged into our selective mechanism, and for which we need to train and test our e ciency predictors."",null,null",null,null
287,"286,6.4 Ranking features,null,null",null,null
288,"287,""As mentioned in Section 5, in the top K retrieval phase, we use the DLH13 term weighting model [1] from the Divergence from Randomness framework for weighting simple and #syn terms; For #1 and #uw terms, we use the DFR pBiL model [35]. Next, Table 5 lists the 18 features used for re-ranking the top K results during the application of the learned model. Note, that regardless of whether term dependency complex operators are deployed as a rewri en query, we include the score for the term dependency operators in the feature set. is allows the learner to consider the term dependence features separately from the score used in the initial ranking phase."",null,null",null,null
289,"288,""For learning and ranking, we use the Jforests implementation8 of LambdaMART [8], a gradient-boosted decision tree learning-torank technique. E ectiveness experiments on the 197 TREC Web track queries (denoted WT in Table 4) use a 5-fold cross validation, where each fold has 60% training, 20% validation and 20% test queries. We report NDCG@20."",null,null",null,null
290,"289,7 RESULTS,null,null",null,null
291,"290,""In the following, we report the results and analysis addressing our two research questions concerning the accuracy of our e ciency predictions for queries with complex operators (Section 7.1), and the application of the selective rewriting mechanism based upon those e ciency predictions (Section 7.2)."",null,null",null,null
292,"291,8 h ps://github.com/yasserg/jforests/,null,null",null,null
293,"292,501,null,null",null,null
294,"293,Session 4C: Queries and Query Analysis,null,null",null,null
295,"294,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
296,"295,""Table 6: Pearson correlation on the test query set, per rewriting and K value, of the baseline (Base) and the proposed predictors (Pred). Statistically signi cant improvements over the baseline are denoted in bold."",null,null",null,null
297,"296,Rewriting,null,null",null,null
298,"297,20,null,null",null,null
299,"298,K,null,null",null,null
300,"299,100,null,null",null,null
301,"300,1000,null,null",null,null
302,"301,5000,null,null",null,null
303,"302,Base Pred Base Pred Base Pred Base Pred,null,null",null,null
304,"303,Na¨iveMRF 0.639 0.833 0.722 0.840 0.827 0.881 0.861 0.868,null,null",null,null
305,"304,MRF,null,null",null,null
306,"305,0.612 0.803 0.697 0.813 0.788 0.870 0.790 0.884,null,null",null,null
307,"306,Na¨ive 0.620 0.848 0.735 0.878 0.828 0.915 0.886 0.935,null,null",null,null
308,"307,None,null,null",null,null
309,"308,0.548 0.738 0.691 0.842 0.802 0.907 0.884 0.952,null,null",null,null
310,"309,7.1 RQ1: E ciency Prediction,null,null",null,null
311,"310,""We use the 66 e ciency prediction features derived from statistics, projectors and aggregators summarised in Table 3 to train a machine-learned model on the 978 training queries (see Table 4), for every combination of rewriting and K value. e regression algorithm employed is gradient boosted regression trees, as provided by the scikit-learn toolbox. Denoting the 978 train queries as Qtr, we perform a 5-fold cross validation to train each Qitr, Kj model. Each model was trained with 20 trees, learning rate of 0.1, max depth of 5, and the least square loss function. To evaluate the accuracy of each model, we then used the 978 test queries Qte to compute the Pearson correlation, reported in Table 6. We compare each of our models (denoted Pred) with a linear regressor trained using the sum of number of postings per each simple term in the original terms and complex operators (Base). Our models perform very well, with correlations always higher than 0.8, and almost always signi cantly improving over the Base baseline predictor (according to a Fisher Z-transform, p < 0.05). To further assess the quality of our predictors, in Table 7, we compare the mean actual query times and the mean predicted query times for each model. Our models incur a mean prediction error no greater than 13%, even if they tend to underestimate the actual processing time, in particular for the models using the original queries (None)."",null,null",null,null
312,"311,""Moreover, since we use the trained model to compare the predicted execution times of query plans versus a given time  , in Table 8 we report the precision/recall measures of our models when classifying queries with a predicted execution time greater than 0.750 seconds. As can be observed from the table, the recall of our models is close or above 0.9 for Na¨iveMRF, MRF and Na¨ive rewritings. e smaller recall value for None is not problematic, since very few queries exhibit an execution time greater than 0.750 seconds."",null,null",null,null
313,"312,""Finally, we use the feature importance metric within gradient boosted trees to assess the contribution of the 66 prediction features. To combine the feature importances across various models, for each model we rank features by decreasing importance, and measure mean reciprocal ranks (Mean RR) across models. Table 9 shows the top features across all trained models (reported features a ained Mean RR greater than 0.1). All proposed statistics and aggregators appear in the top features, with the exception of geometric mean (which appears, but it is not reported, as next in the list). Notably, also the minimum document frequency across all terms is in the list, validating the assumption that the minimum aggregator is useful"",null,null",null,null
314,"313,""Table 7: Average actual and predicted query processing times on the test query set (in ms), per rewriting and K value, with relative errors (in %)."",null,null",null,null
315,"314,Rewriting 20,null,null",null,null
316,"315,K,null,null",null,null
317,"316,100,null,null",null,null
318,"317,1000,null,null",null,null
319,"318,5000,null,null",null,null
320,"319,Actual times,null,null",null,null
321,"320,Na¨iveMRF 2718,null,null",null,null
322,"321,3272,null,null",null,null
323,"322,MRF,null,null",null,null
324,"323,1438,null,null",null,null
325,"324,1757,null,null",null,null
326,"325,Na¨ive,null,null",null,null
327,"326,992,null,null",null,null
328,"327,1373,null,null",null,null
329,"328,None,null,null",null,null
330,"329,374,null,null",null,null
331,"330,551,null,null",null,null
332,"331,4244,null,null",null,null
333,"332,5090,null,null",null,null
334,"333,2618,null,null",null,null
335,"334,3310,null,null",null,null
336,"335,1893,null,null",null,null
337,"336,2381,null,null",null,null
338,"337,802,null,null",null,null
339,"338,1042,null,null",null,null
340,"339,Predicted times,null,null",null,null
341,"340,Na¨iveMRF 2618 (-3.65) 3087 (-5.63) 4109 (-3.18) 4608 (-9.46),null,null",null,null
342,"341,MRF,null,null",null,null
343,"342,1511 (5.12) 1762 (0.29) 2655 (1.44) 3101 (-6.31),null,null",null,null
344,"343,Na¨ive,null,null",null,null
345,"344,953 (-3.92) 1261 (-8.16) 1866 (-1.42) 2339 (-1.78),null,null",null,null
346,"345,None,null,null",null,null
347,"346,369 (-1.19) 485 (-12.13) 829 (-3.38) 1029 (-1.29),null,null",null,null
348,"347,Table 8: Precision/Recall accuracy to classify queries taking more than 750 milliseconds to process.,null,null",null,null
349,"348,Rewriting,null,null",null,null
350,"349,20,null,null",null,null
351,"350,K,null,null",null,null
352,"351,100,null,null",null,null
353,"352,1000,null,null",null,null
354,"353,5000,null,null",null,null
355,"354,PRPRPRPR,null,null",null,null
356,"355,Na¨iveMRF 0.752 0.978 0.790 0.991 0.767 1.000 0.828 1.000,null,null",null,null
357,"356,MRF,null,null",null,null
358,"357,0.831 0.936 0.843 0.984 0.819 0.996 0.858 1.000,null,null",null,null
359,"358,Na¨ive 0.769 0.908 0.813 0.935 0.843 0.979 0.867 0.998,null,null",null,null
360,"359,None,null,null",null,null
361,"360,0.800 0.520 0.839 0.719 0.836 0.881 0.783 0.963,null,null",null,null
362,"361,Table 9: Top features ranked by the mean reciprocal rank of importance across all trained models.,null,null",null,null
363,"362,Aggregator,null,null",null,null
364,"363,Arithmetic mean Maximum Arithmetic mean Maximum Maximum Maximum Harmonic mean ­ Maximum Arithmetic mean Minimum,null,null",null,null
365,"364,Feature,null,null",null,null
366,"365,Document frequency Document frequency Score upper bound Document frequency Document frequency Document frequency Document frequency Number of terms Document frequency Document frequency Document frequency,null,null",null,null
367,"366,Projector,null,null",null,null
368,"367,global #syn global original #uw8 global global global #1 original global,null,null",null,null
369,"368,Mean RR,null,null",null,null
370,"369,0.404 0.350 0.306 0.257 0.243 0.215 0.185 0.171 0.152 0.129 0.109,null,null",null,null
371,"370,""when processing AND-based posting lists. Moreover, all projectors appear in the top features list, with the notable exception of #uw12. We explain this by observing that, according to Table 4, the corresponding complex operator occurs infrequently in the processed queries compared with the frequency of other complex operators."",null,null",null,null
372,"371,""Hence, in addressing RQ1, we have experimentally evaluated the accuracy of our predictions at estimating the execution times of rewri en queries processed using WAND for di erent K. Overall, with correlations > 0.8 for all tested rewritings and values of K, we conclude that our e ciency predictions are indeed accurate."",null,null",null,null
373,"372,502,null,null",null,null
374,"373,Session 4C: Queries and Query Analysis,null,null",null,null
375,"374,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
376,"375,7.2 RQ2: Selective Rewriting,null,null",null,null
377,"376,""In this section, we experiment to determine the levels of e ciency and e ectiveness obtainable when using the selective mechanism proposed in Section 4, and using the predictors for complex operators evaluated in the preceding section. In terms of setup, our selective mechanism ranks the query plans by measuring µ , NDCG based on the validation set for each fold of the WT queries."",null,null",null,null
378,"377,""We consider the uniform application of None, K ,"""" 5000 to all queries as the baseline that we compare to in terms of e ectiveness. In particular, the use of K """","""" 5000 for retrieving on ClueWeb09 has been used by various previous work on the same test collection [10, 28] and empirically veri ed in [27]. We denote the uniform application of this query plan as """"""""Default"""""""". In our experiments, we aim to be more e cient than Default, while not experiencing a signi cant decrease in e ectiveness. In addition to Default, we also report the e ciency and e ectiveness of the uniform application of the Fastest, Slowest and Most E ective plans."""""",null,null",null,null
379,"378,""Table 10 provides the main ndings of the e ciency and e ectiveness of the uniform query plans. Note that, as discussed in Section 6, we use di erent query sets for measuring e ciency and e ectiveness; in particular, we report mean and 95th percentile (or """"tail"""") response times in milliseconds (MRT & TRT, respectively) on the 978 test queries from the MSN 2006 query log; for e ectiveness, we report NDCG@20 for the 197 TREC Web track queries."",null,null",null,null
380,"379,""e rst group of rows reports e ciency and e ectiveness for the Uniform plans, while the lower group reports the results for the Selection mechanism as threshold  is varied. Finally, for each row, Rw denotes the percentage of queries rewri en from None."",null,null",null,null
381,"380,""On inspection of the uniform plan in the top half of Table 10, we note that the MRF and Na¨iveMRF uniform plans result in very high response times (up to 4.86 times slower than Default). In terms of e ectiveness, deploying MRF to increase the recall in the sample results in a marked (but not statistically signi cant) increase in NDCG e ectiveness of the system (0.18770.2001), however, this comes at the cost of retrieval that is 3.2 times slower than Default."",null,null",null,null
382,"381,""Next, we consider the selective mechanism in the bo om half of Table 10. As expected, as  is varied we note changes in both e ectiveness and e ciency. In particular, for  ,"""" 500, we nd that a mean response time of 537 ms can be achieved (49% decrease in mean response time, and 62% decrease in tail response time, compared to Default), without signi cantly degrading e ectiveness (0.1877  0.1751). For this se ing, 8-10% of queries are being rewri en. With higher levels of  , we observe similar increases in e ectiveness and observed response times, and with more queries being rewri en. Finally, we note that the selection mechanism does not strictly observe the  threshold, as can be observed by the tail response times ­ this is expected, as the selection mechanism expressed in Equation (3) will default to the predicted fastest plan available (usually None, K """", 20 ) if no plans can be executed within  ."",null,null",null,null
383,"382,""To provide a graphical illustration of the e ciency/e ectiveness tradeo , Figure 2 presents mean response times and mean NDCG. Lines are provided for both our selective mechanism, Full, denoted by a solid line, as well as the same selective mechanism where the candidate plans are restricted only to those involving None (i.e., with K ,"""" {20, 100, 1000, 5000}), denoted None-only, and indicated by a dashed line. For most thresholds, the Full selective mechanism can be observed to o er the best tradeo , with points closer to the upper le hand corner. For low mean response times (e.g., < 400"""""",null,null",null,null
384,"383,""Table 10: E ciency/e ectiveness results using the selective mechanism. * denotes NDCG values that signi cantly di er from that of None, K ,"""" 5000 (paired t-test, p < 0.05). TRT denotes the tail response time (95%-th percentile), Rw denotes the % of queries rewritten from None. Times are in ms."""""",null,null",null,null
385,"384,Strategy,null,null",null,null
386,"385,E iciency (Test) MRT TRT Rw,null,null",null,null
387,"386,Uniform Plans,null,null",null,null
388,"387,""Default ( None, K ,"""" 5000 ) Fastest ( None, K """","""" 20 ) Slowest ( Na¨iveMRF, K """","""" 5000 ) Most E ective ( MRF, K """", 5000 )"",null,null",null,null
389,"388,1037 5281 0 376 1779 0 5045 17994 100 3281 12099 100,null,null",null,null
390,"389,Selective Mechanism,null,null",null,null
391,"390,"" , 200  , 300  , 400  , 500  , 600  , 700  , 750  , 800  , 900  , 1000"",null,null",null,null
392,"391,449 1986 1 472 1986 1 495 1986 4 537 2004 8 577 2023 13 606 2060 19 617 2060 21 639 2128 22 691 2226 27 733 2256 30,null,null",null,null
393,"392,E ectiveness (WT) NDCG@20 Rw,null,null",null,null
394,"393,0.1877,null,null",null,null
395,"394,0,null,null",null,null
396,"395,0.1375*,null,null",null,null
397,"396,0,null,null",null,null
398,"397,0.1755 100,null,null",null,null
399,"398,0.2001 100,null,null",null,null
400,"399,0.1642*,null,null",null,null
401,"400,0,null,null",null,null
402,"401,0.1670*,null,null",null,null
403,"402,1,null,null",null,null
404,"403,0.1711*,null,null",null,null
405,"404,6,null,null",null,null
406,"405,0.1751 10,null,null",null,null
407,"406,0.1762 14,null,null",null,null
408,"407,0.1782 18,null,null",null,null
409,"408,0.1779 19,null,null",null,null
410,"409,0.1807 19,null,null",null,null
411,"410,0.1825 21,null,null",null,null
412,"411,0.1828 21,null,null",null,null
413,"412,0.180 0.175,null,null",null,null
414,"413,NDCG@20,null,null",null,null
415,"414,0.170,null,null",null,null
416,"415,0.165,null,null",null,null
417,"416,500,null,null",null,null
418,"417,600,null,null",null,null
419,"418,700,null,null",null,null
420,"419,MRT,null,null",null,null
421,"420,Sig. Diff. vs. Default  False True,null,null",null,null
422,"421,Selection Candidates Full None-only,null,null",null,null
423,"422,""Figure 2: E ciency/e ectiveness tradeo . e best tradeo occurs for points closest to the upper le corner. Points denoted with  are signi cantly less e ective than None, K ,"""" 5000 (paired t-test, p < 0.05)."""""",null,null",null,null
424,"423,""ms), the None-only line rises above Full, as only None plans can be deployed to achieve such low response times. On the other hand, for larger mean response times (e.g., > 650 ms), the Full line is more e ective; this supports a central tenet of our work, i.e., when time allows, appropriate rewriting of queries results in increased e ectiveness. Moreover, as indicated by the  points of the None-only"",null,null",null,null
425,"424,503,null,null",null,null
426,"425,Session 4C: Queries and Query Analysis,null,null",null,null
427,"426,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
428,"427,""line in Figure 2, to achieve the response times savings without selectively rewriting the query, we would be forced to accept signi cant degradations in e ectiveness compared to the Default baseline."",null,null",null,null
429,"428,""Finally, to give a avour of the impact of our selective mechanism, we inspect the queries rewri en for  ,"""" 500 and select the query `disneyland hotel'. is query was rewri en to use the plan MRF, K """","""" 100 (as per the proximity example in Table 1), which had a predicted execution time of 486 ms, due to the relatively informative term disneyland (which only appears in Nt """","""" 78422 documents). Hence, the more e ective MRF rewrite was applied, which resulted in a 14% increase in NDCG@20 compared the Default plan (which had a predicted execution time of 542 ms)."""""",null,null",null,null
430,"429,""Overall, in addressing RQ2 we have determined that our selective mechanism can achieve a 49% decrease in mean response time, and 62% decrease in tail (95th-percentile) response time without signi cant degradations in e ectiveness."",null,null",null,null
431,"430,8 CONCLUSIONS,null,null",null,null
432,"431,""is work has considered the selective rewriting of web search queries, with the aim of a aining e cient retrieval without hindering the system's e ectiveness. In particular, we showed that it is possible to accurately measure the response time of a search engine in answering a query with complex operators such as #syn and #1, even when using the WAND dynamic pruning strategy. Our detailed experimental setup involved experiments upon the open TREC ClueWeb09 dataset, using TREC Web track queries for measuring e ectiveness in terms of NDCG@20, and real search engine user queries for measuring e ciency. Moreover, we deployed three strategies to rewrite each query. Our experiments showed not only the accuracy of the newly proposed query e ciency predictions for queries involving complex operators, but also the ability of our proposed selective mechanism to enhance e ciency bene ts (a 49% decrease in mean response time, and 62% decrease in tail, i.e., 95th-percentile, response time) without any signi cant degradation in mean NDCG@20. Overall, our results demonstrate that by selectively rewriting the query (when there is the time to execute the rewri en query), e ectiveness can be at least maintained while markedly bene ting response times. Our proposed selective rewriting mechanism can be further extended, for instance to more query rewriting techniques, such as those based on query reformulation and query-click pa erns [12, 20], and modelling the e ectiveness of rewriting plans through risk rather than mean e ectiveness alone."",null,null",null,null
433,"432,REFERENCES,null,null",null,null
434,"433,""[1] Gianni Amati. 2006. Frequentist and Bayesian Approach to IR. In ECIR. 13­24. [2] Vo Ngoc Anh, Owen de Kretser, and Alistair Mo at. 2001. Vector-space ranking"",null,null",null,null
435,"434,""with e ective early termination. In SIGIR. 35­42. [3] Michael Bendersky, W. Bruce Cro , and Yanlei Diao. 2011. ality-biased ranking"",null,null",null,null
436,"435,""of web documents. In WSDM. 95­104. [4] Michael Bendersky, Donald Metzler, and W. Bruce Cro . 2010. Learning Concept"",null,null",null,null
437,"436,""Importance Using a Weighted Dependence Model. In WSDM. 31­40. [5] Daniele Broccolo, Craig Macdonald, Salvatore Orlando, Iadh Ounis, Ra aele"",null,null",null,null
438,"437,""Perego, Fabrizio Silvestri, and Nicola Tonello o. 2013. Load-sensitive Selective Pruning for Distributed Search. In CIKM. 379­388. [6] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya So er, and Jason Y. Zien. 2003. E cient query evaluation using a two-level retrieval process. In CIKM. 426­434. [7] Chris Buckley, Gerard Salton, James Allan, and Amit Singhal. 1995. Automatic query expansion using SMART: TREC 3. In TREC 3. 69­80. [8] Christopher J.C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview. Technical Report MSR-TR-2010-82."",null,null",null,null
439,"438,""[9] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In TREC."",null,null",null,null
440,"439,""[10] Nick Craswell, Dennis Fe erly, Marc Najork, Stephen Robertson, and Emine Yilmaz. 2010. Microso Research at TREC 2009. In TREC."",null,null",null,null
441,"440,""[11] Nick Craswell, Rosie Jones, Georges Dupret, and Evelyne Viegas (Eds.). 2009. Proceedings of the Web Search Click Data Workshop at WSDM 2009."",null,null",null,null
442,"441,[12] Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In SIGIR. 239­246.,null,null",null,null
443,"442,""[13] W. Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice. Addison-Wesley Publishing Company."",null,null",null,null
444,"443,""[14] J. Shane Culpepper, Charles L. A. Clarke, and Jimmy Lin. 2016. Dynamic Cuto Prediction in Multi-Stage Retrieval Systems. In ADCS. 17­24."",null,null",null,null
445,"444,[15] Je rey Dean. 2009. Challenges in building large-scale information retrieval systems: invited talk. In WSDM.,null,null",null,null
446,"445,""[16] Je rey Dean and Luiz Andr Barroso. 2013. e Tail at Scale. Commun. ACM 56 (2013), 74­80."",null,null",null,null
447,"446,[17] Shuai Ding and Torsten Suel. 2011. Faster top-k document retrieval using blockmax indexes. In SIGIR. 993­1002.,null,null",null,null
448,"447,""[18] Marcus Fontoura, Vanja Josifovski, Jinhui Liu, Srihari Venkatesan, Xiangfei Zhu, and Jason Y. Zien. 2011. Evaluation Strategies for Top-k eries over Memory-Resident Inverted Indexes. PVLDB 4, 12 (2011), 1213­1224."",null,null",null,null
449,"448,""[19] Myeongjae Jeon, Saehoon Kim, Seung-won Hwang, Yuxiong He, Sameh Elnikety, Alan L. Cox, and Sco Rixner. 2014. Predictive Parallelization: Taming Tail Latencies in Web Search. In SIGIR. 253­262."",null,null",null,null
450,"449,""[20] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating ery Substitutions. In WWW. 387­396."",null,null",null,null
451,"450,""[21] Saehoon Kim, Yuxiong He, Seung-won Hwang, Sameh Elnikety, and Seungjin Choi. 2015. Delayed-Dynamic-Selective (DDS) Prediction for Reducing Extreme Tail Latency in Web Search. In WSDM. 7­16."",null,null",null,null
452,"451,""[22] Nicholas Lester, Alistair Mo at, William Webber, and Justin Zobel. 2005. SpaceLimited Ranked ery Evaluation Using Adaptive Pruning. In WISE. 470­477."",null,null",null,null
453,"452,""[23] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420."",null,null",null,null
454,"453,""[24] Xiaolu Lu, Alistair Mo at, and J. Shane Culpepper. 2015. On the Cost of Extracting Proximity Features for Term-Dependency Models. In CIKM. 293­302."",null,null",null,null
455,"454,""[25] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego, Nicola Tonello o, and Rossano Venturini. 2015. ickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees. In SIGIR. 73­82."",null,null",null,null
456,"455,""[26] Craig Macdonald, Iadh Ounis, and Nicola Tonello o. 2011. Upper-bound Approximations for Dynamic Pruning. ACM Trans. Inf. Syst. 29, 4 (2011), 17:1­17:28."",null,null",null,null
457,"456,""[27] Craig Macdonald, Rodrygo LT Santos, and Iadh Ounis. 2013. e whens and hows of learning to rank for web search. Information Retrieval 16, 5 (2013), 584­628."",null,null",null,null
458,"457,""[28] Craig Macdonald, Rodrygo L.T. Santos, Iadh Ounis, and Ben He. 2013. About Learning Models with Multiple ery-dependent Features . ACM Trans. Inf. Syst. 31, 3 (2013), 11:1­11:39."",null,null",null,null
459,"458,""[29] Craig Macdonald, Nicola Tonello o, and Iadh Ounis. 2012. Learning to predict response times for online query scheduling. In SIGIR. 621­630."",null,null",null,null
460,"459,[30] Donald Metzler and W. Bruce Cro . 2005. A Markov Random Field Model for Term Dependencies. In SIGIR. 472­479.,null,null",null,null
461,"460,""[31] Giuseppe O aviano, Nicola Tonello o, and Rossano Venturini. 2015. Optimal Space-time Tradeo s for Inverted Indexes. In WSDM. 47­56."",null,null",null,null
462,"461,""[32] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Christina Lioma. 2006. Terrier: A High Performance & Scalable IR Platform. In OSIR."",null,null",null,null
463,"462,""[33] Jan Pederson. 2010. ery Understanding at Bing. In Invited Talk, SIGIR 2010 Industry Day."",null,null",null,null
464,"463,""[34] Fuchun Peng, Nawaaz Ahmed, Xin Li, and Yumao Lu. 2007. Context Sensitive Stemming for Web Search. In SIGIR. 639­646."",null,null",null,null
465,"464,""[35] Jie Peng, Craig Macdonald, Ben He, Vassilis Plachouras, and Iadh Ounis. 2007. Incorporating Term Dependency in the DFR Framework. In SIGIR. 843­844."",null,null",null,null
466,"465,[36] Eric Shurman and Jake Brutlag. 2009. Performance related changes and their user impacts. In Velocity: Web Performance and Operations Conference.,null,null",null,null
467,"466,""[37] Trevor Strohman, Howard Turtle, and W. Bruce Cro . 2005. Optimization Strategies for Complex eries. In SIGIR. 219­225."",null,null",null,null
468,"467,""[38] Nicola Tonello o, Craig Macdonald, and Iadh Ounis. 2013. E cient and E ective Retrieval Using Selective Pruning. In WSDM. 63­72."",null,null",null,null
469,"468,""[39] Howard Turtle and James Flood. 1995. ery evaluation: Strategies and optimizations. Information Processing & Management 31, 6 (1995), 831 ­ 850."",null,null",null,null
470,"469,""[40] Sebastiano Vigna. 2013. asi-succinct indices. In WSDM. 83­92. [41] Lidan Wang, Jimmy Lin, and Donald Metzler. 2010. Learning to E ciently Rank."",null,null",null,null
471,"470,In SIGIR. 138­145. [42] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global,null,null",null,null
472,"471,Document Analysis. In SIGIR. 4­11.,null,null",null,null
473,"472,504,null,null",null,null
474,"473,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 4C: Queries and Query Analysis,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Intent-Aware Semantic ery Annotation,null,null",null,null
4,"3,Rafael Glater,null,null",null,null
5,"4,""CS Dept, UFMG Belo Horizonte, MG, Brazil rafaelglater@dcc.ufmg.br"",null,null",null,null
6,"5,Rodrygo L. T. Santos,null,null",null,null
7,"6,""CS Dept, UFMG Belo Horizonte, MG, Brazil"",null,null",null,null
8,"7,rodrygo@dcc.ufmg.br,null,null",null,null
9,"8,Nivio Ziviani,null,null",null,null
10,"9,""CS Dept, UFMG & Kunumi Belo Horizonte, MG, Brazil"",null,null",null,null
11,"10,nivio@dcc.ufmg.br,null,null",null,null
12,"11,ABSTRACT,null,null",null,null
13,"12,""ery understanding is a challenging task primarily due to the inherent ambiguity of natural language. A common strategy for improving the understanding of natural language queries is to annotate them with semantic information mined from a knowledge base. Nevertheless, queries with di erent intents may arguably bene t from specialized annotation strategies. For instance, some queries could be e ectively annotated with a single entity or an entity a ribute, others could be be er represented by a list of entities of a single type or by entities of multiple distinct types, and others may be simply ambiguous. In this paper, we propose a framework for learning semantic query annotations suitable to the target intent of each individual query. orough experiments on a publicly available benchmark show that our proposed approach can signi cantly improve state-of-the-art intent-agnostic approaches based on Markov random elds and learning to rank. Our results further demonstrate the consistent e ectiveness of our approach for queries of various target intents, lengths, and di culty levels, as well as its robustness to noise in intent detection."",null,null",null,null
14,"13,CCS CONCEPTS,null,null",null,null
15,"14,·Information systems Retrieval e ectiveness;,null,null",null,null
16,"15,KEYWORDS,null,null",null,null
17,"16,Semantic query annotation; Learning to rank; Intent-aware;,null,null",null,null
18,"17,""ACM Reference format: Rafael Glater, Rodrygo L. T. Santos, and Nivio Ziviani. 2017. Intent-Aware Semantic ery Annotation. In Proceedings of SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080825"",null,null",null,null
19,"18,1 INTRODUCTION,null,null",null,null
20,"19,""A user's search query has traditionally been treated a short, underspeci ed representation of his or her information need [22]. Despite the trend towards verbosity brought by the popularization of voice queries in modern mobile search and personal assistant interfaces [20], query understanding remains a challenging yet crucial task for the success of search systems. One particularly e ective strategy for improving the understanding of a query is to annotate it with semantic information mined from a knowledge"",null,null",null,null
21,"20,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080825"",null,null",null,null
22,"21,""base, such as DBPedia.1 In particular, previous analysis has shown that over 70% of all queries contain a semantic resource (a named entity, an entity type, relation, or a ribute), whereas almost 60% have a semantic resource as their primary target [32]."",null,null",null,null
23,"22,""State-of-the-art semantic query annotation approaches leverage features extracted from the descriptive content of candidate semantic resources (e.g., the various textual elds in the description of an entity [28, 44]) or their structural properties (e.g., related semantic resources [39]) in a knowledge base. In common, these approaches treat every query uniformly, regardless of its target intent.2 In contrast, we hypothesize that queries with di erent intents may bene t from specialized annotation strategies. For instance, some queries could be e ectively annotated with a single entity (e.g., """"us president"""") or an entity a ribute (e.g., """"us president salary""""). Other queries could be be er represented by a list of entities of a single type (e.g., """"us presidents"""") or of mixed types (e.g., """"us foreign affairs""""). Finally, some queries may be simply ambiguous and demand annotations suitable for disambiguation (e.g., """"us"""")."",null,null",null,null
24,"23,""In this paper, we propose a framework for learning semantic annotations suitable to the target intent of each individual query. Our framework comprises three main components: (i) intent-speci c learning to rank, aimed to produce ranking models optimized for di erent intents; (ii) query intent classi cation, aimed to estimate the probability of each query conveying each possible intent; and (iii) intent-aware ranking adaptation, aimed to promote the most relevant annotations given the detected intents. To demonstrate the applicability of our framework, we experiment with a state-of-theart learning to rank algorithm for intent-speci c learning, multiple classi cation approaches for intent classi cation, and two adaptive strategies for annotation ranking. orough experiments using a publicly available semantic annotation test collection comprising queries with di erent intents show that our proposed framework is e ective and signi cantly improves state-of-the-art intent-agnostic approaches from the literature. Moreover, a breakdown analysis further reveals the consistency of the observed gains for queries of various target intents, lengths, and di culty levels, as well as the robustness of the framework to noise in intent detection."",null,null",null,null
25,"24,""In summary, our main contributions are three-fold:"",null,null",null,null
26,"25,· An intent-aware framework for learning semantic query annotations from structured knowledge bases.,null,null",null,null
27,"26,· An analysis of the speci city of several content and structural features for di erent query intents.,null,null",null,null
28,"27,· A thorough validation of the proposed framework in terms of annotation e ectiveness and robustness.,null,null",null,null
29,"28,""1h p://wiki.dbpedia.org/ 2While the word """"intent"""" has been used as a synonym of """"information need"""" in some contexts, here we adopt the more traditional de nition of intent as the type (or class) of an information need, such as informational or navigational [9]."",null,null",null,null
30,"29,485,null,null",null,null
31,"30,Session 4C: Queries and Query Analysis,null,null",null,null
32,"31,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
33,"32,""In the remainder of this paper, Section 2 discusses related work on semantic query annotation and intent-aware information retrieval. Section 3 describes the various components of our proposed framework and their instantiation. Sections 4 and 5 describe the setup and the results of our empirical evaluation. Finally, Section 6 provides our conclusions and directions for future research."",null,null",null,null
34,"33,2 RELATED WORK,null,null",null,null
35,"34,""In this section, we provide an overview of related work on semantic query annotation using knowledge bases. In addition, we discuss related a empts to exploit query intents in di erent search tasks."",null,null",null,null
36,"35,2.1 Semantic ery Annotation,null,null",null,null
37,"36,""Semantic search approaches [4] have been extensively researched in recent years, motivated by a series of related workshops and evaluation campaigns [1, 2, 15]. While some research has been devoted to semantic search on the open Web [2, 10, 37], particularly relevant to this paper are approaches focused on ranking semantic resources (e.g., named entities) mined from a structured domain, such as a knowledge base. e top ranked resources can be used directly to enrich a search engine's results page with structured semantic information [5] or indirectly to annotate the user's query for further processing for improved search quality."",null,null",null,null
38,"37,""Search in knowledge bases is typically performed using structured query languages such as SPARQL.3 However, producing structured queries requires some expertise from the user, which limits the applicability of this approach in a broader scenario. To support unstructured querying, most previous semantic search approaches adapt traditional IR techniques to nd, in the knowledge base, resources that match the user's query. For instance, some related works have used standard bag-of-words models, like BM25 [3, 31, 39] and language models (LM) [16, 17, 21, 27, 43]. Extending traditional bag-of-words models, multi- elded approaches have been proposed to appropriately weight information present in di erent elds describing a semantic resource. For instance, approaches based on BM25F [6, 7, 12, 18, 31, 39] permit the combination of the BM25 scores of di erent elds into the nal retrieval score. Multi- elded approaches based on a mixture of language models have also been proposed [11, 29], which linearly combine query likelihood estimates obtained from multiple elds."",null,null",null,null
39,"38,""Also contrasting with bag-of-words models, recent approaches have exploited dependencies among query term occurrences in the descriptive content of a semantic resource. Building upon the framework of Markov random elds (MRF) [26], these approaches construct a graph of dependencies among the query terms, which is used to estimate the relevance of each retrieved semantic resource. In particular, Zhiltsov et al. [44] introduced a multi- elded extension of MRF, called FSDM, which estimates the weight of each eld with respect to three types of query concept: unigram, ordered bigram, and unordered bigram. FSDM was later extended by Nikolaev et al. [28], who proposed to estimate eld weights with respect to individual query concepts. To cope with the explosive number of concepts (i.e., every possible unigram, ordered, and unordered bigram), they instead learn eld weights with respect to a xed set of concept features (e.g., the probability of occurrence of the concept"",null,null",null,null
40,"39,3h ps://www.w3.org/TR/rdf-sparql-query/,null,null",null,null
41,"40,""in a eld). In contrast to both of these approaches, we propose to learn the appropriateness of intent-speci c feature-based ranking models for each individual query, by automatically predicting the target intent of this query. In Section 5, we compare our approach to FSDM as a representative of the current state-of-the-art."",null,null",null,null
42,"41,""In addition to exploiting the descriptive content of semantic resources, other researchers have adopted a hybrid approach [11, 17, 21, 34, 39], leveraging structural properties of the knowledge base. In these approaches, an initial ranking of semantic resources is either re-ranked or expanded using the knowledge base structure to nd related resources, which can be done through structured graph traversals [39] or random walks [34]. For instance, Tonon et al. [39] exploited entities initially retrieved using BM25 as seeds in the graph from which related entities could be reached. Bron et al. [11] proposed a method that makes a linear combination of the scores of a content-based approach using language models and a structure-based approach, which captures statistics from candidate entities represented according to their relations with other entities, expressed in RDF triples. Relatedly, Elbassouni et al. [17] proposed a language modeling approach to rank the results of exact, relaxed, and keyword-augmented graph-pa ern queries over RDF triples into multiple subgraphs. e Kullback-Leibler divergence between the query language model and the language models induced by the resulting subgraphs was then used to produce the nal ranking. While our main focus is on learning strategies rather than on speci c features, to demonstrate the exibility of our proposed framework, we exploit multiple structural properties of each semantic resource as additional features. In particular, these features are used for both detecting the intent of a query as well as for ranking semantic resources in response to this query."",null,null",null,null
43,"42,2.2 Exploiting ery Intents,null,null",null,null
44,"43,""e intent underlying a user's search query has been subject of intense research in the context of web search. Broder [9] proposed a well-known intent taxonomy, classifying web search queries into informational, navigational and transactional. Rose and Levinson [35] later extended this taxonomy to consider more ne-grained classes. In the context of semantic search, Pound et al. [32] categorized queries into four major intents: entity queries, which target a single entity; type queries, which target multiple entities of a single type; a ribute queries, which target values of a particular entity a ribute; and relation queries, which aim to nd how two or more entities or types are related. Entity queries and type queries accounted for more than 50% of a query log sampled in their study, whereas a ribute and relation queries accounted for just over 5%. Other works focused on more speci c intents, such as a question intent [40], which targets answers to the question expressed in the query. In our experiments, we use an intent taxonomy comprising the three major classes described in these studies, namely, entity, type, and question queries, as well as an additional class including less represented intents, such as a ribute and relation queries."",null,null",null,null
45,"44,""In addition to detecting query intents, several approaches have a empted to adapt the ranking produced for a query in light of some identi ed query property, such as its intent. For instance, Yom-Tov et al. [42] proposed to adaptively expand a query depending on its predicted di culty. Kang and Kim [23] proposed to apply"",null,null",null,null
46,"45,486,null,null",null,null
47,"46,Session 4C: Queries and Query Analysis,null,null",null,null
48,"47,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
49,"48,""di erent hand-cra ed ranking models for queries with a predicted informational, navigational, or transactional intent. However, such a hard intent classi cation may eventually harm the e ectiveness of an adaptive approach, when queries of di erent intents bene t from a single ranking model [14]. To mitigate this e ect, instancebased classi cation approaches have been used to identify similar queries (as opposed to queries with the same predicted intent) for training a ranking model. For example, Geng et al. [19] resorted to nearest neighbor classi cation for building training sets for a given test query. Relatedly, Peng et al. [30] proposed to estimate the bene t of multiple candidate ranking models for a given query by examining training queries that are a ected by these models in a similar manner. In the context of search result diversi cation, Santos et al. proposed adaptive approaches for estimating the coverage of di erent query aspects given their predicted intent [38] as well as for estimating when to diversify given the predicted ambiguity of the query [36]. Our proposed approach resembles these adaptive ranking approaches as we also resort to query intent classi cation as a trigger for ranking adaptation. Nonetheless, to the best of our knowledge, our approach is the rst a empt to produce adaptive learning to rank models for a semantic search task."",null,null",null,null
50,"49,3 INTENT-AWARE RANKING ADAPTATION,null,null",null,null
51,"50,FOR SEMANTIC QUERY ANNOTATION,null,null",null,null
52,"51,""Annotating queries with semantic information is an important step towards an improved query understanding [1]. Given a query, our goal is to automatically annotate it with semantic resources mined from a knowledge base, including named entities, a ributes, relations, etc. For instance, the query """"us president"""" could be annotated with arguably relevant semantic resources including """"Donald Trump"""", """"Federal Government"""", """"White House."""" In this paper, we hypothesize that the relevance of a semantic resource given a query depends on the intent underlying this query. For the previous example, knowing that the query """"us president"""" targets information around a single entity could promote alternative semantic resources including """"Inauguration"""", """"First 100 days"""", and """"Controversies""""."",null,null",null,null
53,"52,""In this section, we propose an intent-aware framework for learning to rank semantic query annotations. In particular, we posit that the probability P(r |q) that a given semantic resource r satis es the user's query q should be estimated in light of the possible intents i  I underlying this query. Formally, we de ne:"",null,null",null,null
54,"53,""P(r |q) ,"""" P(i |q) P(r |q, i),"""""",null,null",null,null
55,"54,(1),null,null",null,null
56,"55,i I,null,null",null,null
57,"56,""where P(i |q) is the probability that query q conveys an intent i, with i I P(i |q) ,"""" 1, and P(r |q, i) is the probability of observing semantic resource r given the query and this particular intent."""""",null,null",null,null
58,"57,""In Figure 1, we describe the three core components of our framework. In particular, the query intent classi cation and the intentspeci c learning to rank components rely on supervised learning approaches to estimate P(i |q) and P(r |q, i), respectively, for each intent i  I. In turn, the intent-aware ranking adaptation component implements two alternative policies to suit the nal ranking to the detected intents of each individual query."",null,null",null,null
59,"58,i1,null,null",null,null
60,"59,Li1,null,null",null,null
61,"60,i1 i2 i3 i4,null,null",null,null
62,"61,i2,null,null",null,null
63,"62,Li2,null,null",null,null
64,"63,query intent classification,null,null",null,null
65,"64,C,null,null",null,null
66,"65,q,null,null",null,null
67,"66,i3,null,null",null,null
68,"67,Li3,null,null",null,null
69,"68,i4,null,null",null,null
70,"69,Li4,null,null",null,null
71,"70,A,null,null",null,null
72,"71,intent-specific,null,null",null,null
73,"72,intent-aware,null,null",null,null
74,"73,learning to rank ranking adaptation,null,null",null,null
75,"74,Figure 1: Intent-aware semantic query annotation. Each intent-speci c ranking model Li is learned on a query set comprising only queries with intent i. e query intent classi cation model C is learned on a set comprising queries of,null,null",null,null
76,"75,various intents. e intent-aware ranking adaptation strategy A uses the query intent classi cation outcome to decide,null,null",null,null
77,"76,on how to leverage the intent-speci c ranking models.,null,null",null,null
78,"77,3.1 ery Intent Classi cation,null,null",null,null
79,"78,""e rst component of our framework is responsible for predicting the possible intents underlying a query [8]. For this task, we adopt a standard multi-class classi cation approach. In particular, we aim to learn a query classi cation model C : X  Y mapping the input space X into the output space Y. Our input space X comprises m learning instances {xj }mj,""""1, where xj """","""" (qj ) is a feature vector representation of query qj as produced by a feature extractor . In turn, our output space Y comprises m labels { j }mj"""",""""1, where j corresponds to one of the target intents i  I assigned to query qj by a human annotator. To learn an e ective classi er C, we experiment with several classi cation algorithms in Section 5.2."""""",null,null",null,null
80,"79,""Table 1 presents the features we use to represent a query for intent classi cation. We use a total of 31 simple features, including both lexical as well as semantic ones. Lexical features like number of query terms and mean query term size can help detect, for example, natural language queries, which are usually longer than others. In addition, part-of-speech tags can help identify question queries, indicating the presence of wh-pronouns (e.g., what, where, why, when). Lastly, semantic features include the number of categories and number of ontology classes returned when using the query to search a knowledge base. Our intuition is that queries seeking for a speci c entity will probably return fewer categories or ontology classes than queries seeking for a list of entities. For instance, the query """"ei el"""" returns only 5 categories, while the query """"list of lms from the surrealist category"""" returns more than 103,000."",null,null",null,null
81,"80,3.2 Intent-Speci c Learning to Rank,null,null",null,null
82,"81,""e second component of our framework aims to produce multiple ranking models, each one optimized for a speci c query intent i  I. To this end, we resort to learning to rank [24]. Analogously to our query intent classi cation models in Section 3.1, our goal is to learn an intent-speci c ranking model Li : V  W mapping the input space V into the output space W. Our input space"",null,null",null,null
83,"82,487,null,null",null,null
84,"83,Session 4C: Queries and Query Analysis,null,null",null,null
85,"84,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
86,"85,Table 1: ery features for intent classi cation.,null,null",null,null
87,"86,# Feature,null,null",null,null
88,"87,Qty,null,null",null,null
89,"88,1 No. of query terms,null,null",null,null
90,"89,1,null,null",null,null
91,"90,2 Avg. query term size (in characters),null,null",null,null
92,"91,1,null,null",null,null
93,"92,3 No. of matched categories in DBPedia,null,null",null,null
94,"93,1,null,null",null,null
95,"94,4 No. of matched ontology classes in DBPedia,null,null",null,null
96,"95,1,null,null",null,null
97,"96,5 No. of POS tags of di erent types,null,null",null,null
98,"97,27,null,null",null,null
99,"98,TOTAL,null,null",null,null
100,"99,31,null,null",null,null
101,"100,""includes n learning instances {Vj }nj,""""1, where Vj """","""" (qj , Rj ) is a feature matrix representation (produced by some feature extractor ) of a sample of semantic resources r  Rj retrieved for query qj annotated with intent i. In our experiments, Rj is produced using BM25 [33], although any unsupervised ranking technique could have been used for this purpose. Our output space W comprises n label vectors {Wj }nj"""",""""1, where Wj provides relevance labels for each semantic resource r  Rj . To learn an e ective ranking model Li for each intent i  I, we use LambdaMART [41], which represents the current state-of-the-art in learning to rank [13]."""""",null,null",null,null
102,"101,""Table 2 lists all 216 features used to represent each semantic resource r  Rj . Features #1-#6 are content-based features commonly used in the learning to rank literature [24], such as number of tokens, BM25, coordination level matching (CLM), TF, and IDF scores."",null,null",null,null
103,"102,""ese are computed in a total of 8 descriptive elds of r , such as name, a ributes, categories (see Section 4.1 for a full description). Since TF and IDF are de ned on a term-level, query-level scores are computed using multiple summary statistics (sum, min, max, avg, var). Finally, CLM, TF, IDF, and TF-IDF are computed for both unigrams and bigrams. Next, features #7-#14 are semantic features derived from a knowledge base. For instance, feature #7 indicates whether r is an entity directly mentioned in the query, while feature #8 considers the number of direct connections between r and all entities mentioned in the query. As an example of the la er feature, in the query """"songs composed by michael jackson"""", the candidate resource """" riller"""" will be directly related to the entity """"Michael Jackson"""" (present in the query). For both features, we use DBPedia Spotlight4 for entity recognition in queries. Features #9-#14 are query-independent features quantifying the connectivity of each candidate resource r with respect to other resources in the knowledge base (e.g., entities, categories, ontology classes)."",null,null",null,null
104,"103,""To keep our approach general, instead of handpicking features more likely to be useful for a particular intent, we use the same 216 available features when learning every intent-speci c model Li . To ensure that the learned model Li is indeed optimized to its target intent i, intent-speci c learning is achieved by using one training query set per intent, as illustrated in Figure 1."",null,null",null,null
105,"104,3.3 Intent-Aware Ranking Adaptation,null,null",null,null
106,"105,""Sections 3.1 and 3.2 described supervised approaches for learning a query intent classi cation model C as well as multiple intentspeci c ranking models Li for all i  I. Importantly, all of these models are learned o ine. When an unseen query q is submi ed online, we must be able to return a ranking of semantic resources"",null,null",null,null
107,"106,4h p://spotlight.dbpedia.org/,null,null",null,null
108,"107,Table 2: Semantic resource features for learning to rank. Features marked as `Bi' are computed also for bigrams.,null,null",null,null
109,"108,# Feature,null,null",null,null
110,"109,""1 No. of tokens (per- eld) 2 BM25 (per- eld) 3 CLM (per- eld) 4 TF (per- eld sum, min, max, avg, var) 5 IDF (per- eld sum) 6 TF-IDF (per- eld sum, min, max, avg, var) 7 Matching entity 8 No. of direct relations with query entities 9 No. of matched relations with query terms 10 No. of inlinks 11 No. of outlinks 12 No. of linked ontology classes 13 No. of linked categories 14 No. of linked entities"",null,null",null,null
111,"110,TOTAL,null,null",null,null
112,"111,Bi Qty,null,null",null,null
113,"112,8 8  16  80  16  80 1 1 1 1 1 1 1 1,null,null",null,null
114,"113,216,null,null",null,null
115,"114,""well suited to the target intent of q. Because we tackle query intent classi cation as a multi-class problem, we can actually estimate the probability P(i |q) of di erent intents i  I given the query q."",null,null",null,null
116,"115,""To exploit this possibility, we devise two strategies to adapt the ranking produced for a query q to the target intent(s) of this query. Our rst strategy, called intent-aware switching, assigns each query a single intent, namely, the most likely one as predicted by the intent classi cation model C. For instance, for a target set of intents I ,"""" {i1, i2, i3} of which i1 is predicted as the most likely for q, we could instantiate Equation (1) with P(i1|q) """","""" 1, P(i2|q) """","""" 0, and P(i3|q) """","""" 0. As a result, only P(r |q, i1) (estimated via ranking model L1) would have an impact on the nal ranking, such that:"""""",null,null",null,null
117,"116,""P(r |q) ,"""" P(r |q, i1)."""""",null,null",null,null
118,"117,""Some queries may have no clear winning intent. Other queries may prove simply di cult to classify correctly. To cope with uncertainty in intent classi cation, we propose a second ranking adaptation strategy, called intent-aware mixing. In this strategy, we use the full probability distribution over intents predicted by the classi cation model C to produce the nal ranking for q. In the aforementioned example, suppose the predicted intent distribution is P(i1|q) ,"""" 0.7, P(i2|q) """","""" 0.2, and P(i3|q) """","""" 0.1. Leveraging this distribution directly in Equation (1), we have a mixture of intent-speci c ranking models contributing to the nal ranking:"""""",null,null",null,null
119,"118,""P(r |q) ,"""" 0.7 × P(r |q, i1)"""""",null,null",null,null
120,"119,""+ 0.2 × P(r |q, i2)"",null,null",null,null
121,"120,""+ 0.1 × P(r |q, i3)."",null,null",null,null
122,"121,""To assess the e ectiveness of our proposed intent-aware ranking adaptation strategies for semantic query annotation, in the next section, we compare these strategies to each other as well as to state-of-the-art intent-agnostic approaches from the literature."",null,null",null,null
123,"122,4 EXPERIMENTAL SETUP,null,null",null,null
124,"123,""In this section, we detail the experimental setup that supports the evaluation of our proposed intent-aware semantic query annotation"",null,null",null,null
125,"124,488,null,null",null,null
126,"125,Session 4C: Queries and Query Analysis,null,null",null,null
127,"126,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
128,"127,""approach introduced in Section 3. In particular, our experiments aim to answer the following research questions:"",null,null",null,null
129,"128,Q1. Do di erent intents bene t from di erent ranking models? Q2. How accurately can we predict the intent of each query? Q3. How e ective is our semantic query annotation approach? Q4. What queries are improved the most and the least?,null,null",null,null
130,"129,""In the following, we describe the knowledge base, queries, relevance judgments, and intent taxonomy used in our experiments. We also describe the baselines used for comparison and the procedure undertaken to train and test them as well as our own models."",null,null",null,null
131,"130,4.1 Knowledge Base,null,null",null,null
132,"131,""e knowledge base used in our experiments is the English portion of DBPedia 3.7,5 which comprises RDF triples extracted from Wikipedia dumps generated in late July 2011. is version of DBPedia contains information on more than 3.6 million entities organized in over 170,000 categories and 320 ontology classes in a 6-level deep hierarchy. We indexed this knowledge base using Elasticsearch 1.7.56 for textual content and Titan 0.5.47 for the underlying graph structure. RDF triples were parsed to created a elded content representation. Following Zhiltsov et al. [44], we indexed the Names, A ributes, Categories, Similar entity names and Related entity names"",null,null",null,null
133,"132,""elds. In addition, we included three other elds: Ontology classes, URL and a special eld All, concatenating the available content from all elds. Indexed terms were lower-cased, stemmed using Krovetz stemmer and standard stopwords were removed."",null,null",null,null
134,"133,""4.2 eries, Relevance Judgments, and Intents"",null,null",null,null
135,"134,""We use a publicly available benchmark8 built on top of DBPedia 3.7, which comprises a total of 485 queries from past semantic search evaluation campaigns [3]. In total, there are 13,090 positive relevance judgments available. While some of these include graded labels, for a homogeneous treatment of all queries, we consider relevance as binary. e benchmark covers a wide variety of query intents, including entity, type, relation and a ribute queries, as well as queries with a question intent. Following past research [3, 28, 44], we organize these queries into four intent-speci c query sets, the salient statistics of which are described in Table 3:"",null,null",null,null
136,"135,""· E: entity queries (e.g., """"orlando orida""""); · T : type queries (e.g., """"continents in the world""""); · Q: question queries (e.g., """"who created wikipedia?""""); · O: queries with other intents, including less represented"",null,null",null,null
137,"136,""ones, such as relation queries and a ribute queries."",null,null",null,null
138,"137,4.3 Retrieval Baselines,null,null",null,null
139,"138,""We compare our approach to multiple intent-agnostic baselines from the literature. As a vanilla ad-hoc search baseline, we consider BM25 with standard parameter se ings (k1 ,"""" 1.2, b """","""" 0.8). To assess the e ectiveness of our intent-aware ranking adaptation strategies introduced in Section 3.3, we further contrast them to two intent-agnostic strategies, which consistently apply a single ranking model for all queries, regardless of their target intent. As"""""",null,null",null,null
140,"139,5h p://wiki.dbpedia.org/data-set-37 6h ps://www.elastic.co/products/elasticsearch 7h p://titan.thinkaurelius.com 8h p://bit.ly/dbpedia-entity,null,null",null,null
141,"140,Table 3: Statistics of the intent-speci c query sets used in our evaluation. Length and qrels denote per-query averages of query length and positive judgements in each set.,null,null",null,null
142,"141,Set Campaign [3],null,null",null,null
143,"142,""E SemSearch ES T INEX-XER, SemSearch LS,"",null,null",null,null
144,"143,TREC Entity Q QALD-2 O INEX-LD,null,null",null,null
145,"144,TOTAL,null,null",null,null
146,"145,eries,null,null",null,null
147,"146,130 115,null,null",null,null
148,"147,Length,null,null",null,null
149,"148,2.7 5.8,null,null",null,null
150,"149,Qrels,null,null",null,null
151,"150,8.7 18.4,null,null",null,null
152,"151,140,null,null",null,null
153,"152,7.9,null,null",null,null
154,"153,41.5,null,null",null,null
155,"154,100,null,null",null,null
156,"155,4.8,null,null",null,null
157,"156,37.6,null,null",null,null
158,"157,485,null,null",null,null
159,"158,5.3 26.55,null,null",null,null
160,"159,""illustrated in Table 4, the xed strategy applies a model Li learned on one intent-speci c query set, whereas the oblivious strategy applies a model LR learned on a set of random queries. For a fair comparison, both of these baseline strategies as well as our own intent-aware switching and mixing strategies use the same learning algorithm (LambdaMART) and ranking features (all 216 features in Table 2). Lastly, we further contrast our approach to FSDM [44] (see Section 2.1) as a representative of the current state-of-the-art."",null,null",null,null
161,"160,4.4 Training and Test Procedure,null,null",null,null
162,"161,""For a fair comparison between our intent-aware semantic query annotation approach and the intent-agnostic baselines described in Section 4.3, we randomly downsample all query sets in Table 3 until they reach 100 queries each (i.e., the number of queries in the smallest query set, namely, O). is procedure ensures the learning process is not biased towards any particular intent. To learn an intent-speci c model Li for each intent i  I ,"""" {E,T , Q, O }, we perform a 5-fold cross validation in the corresponding query set from Table 3. For the oblivious strategy, the intent-agnostic model LR is also learned via 5-fold cross validation on a set of 100 queries sampled uniformly at random from the four intent-speci c query sets a er downsampling. is multi-intent query set is also used to tune the parameters of FSDM [44] for di erent concepts (unigrams, ordered, an unordered bigrams) and each of the elds listed in Section 4.1. In each cross-validation round, we use three partitions (60 queries) for training, one partition (20 queries) for validation, and one partition (20 queries) for testing."""""",null,null",null,null
163,"162,""Learning to rank is performed using the LambdaMART implementation in RankLib 2.7,9 optimizing for normalized discounted cumulative gain at the top 100 results (nDCG@100). LambdaMART is deployed with default hyperparameter se ings,10 with 1,000 trees with 10 leaves each, minimum leaf support 1, unlimited threshold candidates for tree spli ing, learning rate 0.1, and early stopping a er 100 non-improving iterations. All results are reported as averages of all test queries across the ve cross-validation rounds. In particular, we report nDCG@10, precision at 10 (P@10), and mean average precision (MAP). All evaluation metrics are calculated on the top 100 results returned by each approach. To check for statistically signi cant di erences among them, we use a two-tailed paired"",null,null",null,null
164,"163,9h ps://sourceforge.net/p/lemur/wiki/RankLib%20How%20to%20use/ 10Hyperparameter tuning on validation data showed no signi cant improvements in our preliminary tests.,null,null",null,null
165,"164,489,null,null",null,null
166,"165,Session 4C: Queries and Query Analysis,null,null",null,null
167,"166,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
168,"167,Table 4: Example application of intent-agnostic (baseline) and intent-aware ranking adaptation strategies.,null,null",null,null
169,"168,intent-agnostic,null,null",null,null
170,"169,intent-aware,null,null",null,null
171,"170,i xed-E xed-T xed-Q xed-O oblivious switching mixing,null,null",null,null
172,"171,E LE,null,null",null,null
173,"172,LT,null,null",null,null
174,"173,LQ,null,null",null,null
175,"174,LO,null,null",null,null
176,"175,LR,null,null",null,null
177,"176,T LE,null,null",null,null
178,"177,LT,null,null",null,null
179,"178,LQ,null,null",null,null
180,"179,LO,null,null",null,null
181,"180,LR,null,null",null,null
182,"181,Q LE,null,null",null,null
183,"182,LT,null,null",null,null
184,"183,LQ,null,null",null,null
185,"184,LO,null,null",null,null
186,"185,LR,null,null",null,null
187,"186,O LE,null,null",null,null
188,"187,LT,null,null",null,null
189,"188,LQ,null,null",null,null
190,"189,LO,null,null",null,null
191,"190,LR,null,null",null,null
192,"191,LE,null,null",null,null
193,"192,i wi Li,null,null",null,null
194,"193,LT,null,null",null,null
195,"194,i wi Li,null,null",null,null
196,"195,LQ,null,null",null,null
197,"196,i wi Li,null,null",null,null
198,"197,LO,null,null",null,null
199,"198,i wi Li,null,null",null,null
200,"199,LO 1.0 0.42 0.7 0.38,null,null",null,null
201,"200,0.96,null,null",null,null
202,"201,0.88,null,null",null,null
203,"202,LQ 0.42 1.0 0.46 0.44,null,null",null,null
204,"203,0.80,null,null",null,null
205,"204,0.72,null,null",null,null
206,"205,LE 0.7 0.46 1.0 0.39,null,null",null,null
207,"206,0.64,null,null",null,null
208,"207,0.56,null,null",null,null
209,"208,LT 0.38 0.44 0.39 1.0,null,null",null,null
210,"209,0.48,null,null",null,null
211,"210,0.40,null,null",null,null
212,"211,LO,null,null",null,null
213,"212,LQ,null,null",null,null
214,"213,LE,null,null",null,null
215,"214,LT,null,null",null,null
216,"215,Figure 2: Spearman's correlation coe cient for feature importance across pairs of intent-speci c ranking models.,null,null",null,null
217,"216,""t-test and write ( ) and ( ) to denote signi cant increases (decreases) at the 0.05 and 0.01 levels, respectively. A further symbol  is used to denote no signi cant di erence."",null,null",null,null
218,"217,5 EXPERIMENTAL EVALUATION,null,null",null,null
219,"218,""In this section, we empirically evaluate our approach in order to answer the four research questions stated in Section 4. In the following, we address each of these questions in turn."",null,null",null,null
220,"219,5.1 Intent Speci city,null,null",null,null
221,"220,""e core hypothesis of our proposal is that di erent queries may bene t from a ranking model optimized to their intent. To verify this hypothesis, we address Q1, by assessing the speci city of ranking models optimized to the four intents described in Table 3. To this end, Figure 2 correlates the importance assigned to all 216 features by each intent-speci c ranking model Li , for i  I ,"""" {E,T , Q, O }. Feature importance is quanti ed using the least square improvement criterion proposed by Lucchese et al. [25] for gradient boosted regression tree learners, such as LambdaMART. From Figure 2, we observe a generally low correlation ( < 0.5) between models, except for the LE and LO models, with   0.7."""""",null,null",null,null
222,"221,""Table 5 lists the ve most important features for each intentspeci c model. e entity-oriented LE model gives importance to features related to the occurrence of bigrams in the name and similar entities elds. For instance, the query """"martin luther king"""" expects semantic resources named """"Martin Luther King III"""" and """"Martin Luther King High School."""" While the type-oriented LT model considers a variety of distinct features, two features related to the categories eld are present in the top 5, which are useful for queries like """"state capitals of the united states of america."""" e question-oriented LQ model gives importance to features describing the relation between entities and ontology classes, derived from"",null,null",null,null
223,"222,""both content elds as well as the graph structure underlying the knowledge base. ese can help identify relevant resources linked to an entity in the query through quali ed relations, as in the query """"who was the successor of john f. kennedy?"""" Lastly, the LO model, which is optimized on a set comprising queries of various intents, strongly favors content-based features, which are arguably e ective for broad queries such as """"einstein relativity theory."""" Recalling question Q1, these results provide a strong indication of the speci city of di erent models to queries of di erent intents."",null,null",null,null
224,"223,Table 5: Top 5 features per ranking model.,null,null",null,null
225,"224,# Feature,null,null",null,null
226,"225,1 TF-IDF sum of bigrams in similar entities 2 Matching entity LE 3 TF sum of bigrams in similar entities 4 TF avg of bigrams in similar entities 5 TF-IDF max of bigrams in similar entities,null,null",null,null
227,"226,1 CLM in categories 2 CLM in all content LT 3 No. of inlinks 4 No. of tokens in similar entities 5 TF-IDF sum of bigrams in categories,null,null",null,null
228,"227,1 BM25 in ontology classes 2 No. of matched relations with query terms LQ 3 No. of direct relations with query entities 4 No. of inlinks 5 TF-IDF max of unigrams in ontology classes,null,null",null,null
229,"228,1 TF sum of bigrams in name 2 BM25 in name LO 3 TF-IDF max of unigrams in categories 4 TF-IDF max of bigrams in name 5 TF-IDF var of bigrams in all content,null,null",null,null
230,"229,5.2 Intent Classi cation Accuracy,null,null",null,null
231,"230,""e results in the previous experiment suggest that exploiting the speci city of di erent query intents may result in more e ective ranking models. Before investigating whether this is indeed the case, in this section, we address Q2, with the aim of establishing what level of query intent detection accuracy can be a ained in practice. To this end, we experiment with a range of traditional classi cation algorithms implemented in Scikit-learn 0.17.1,11 optimized via 5-fold cross validation using the same partitions leveraged for learning to rank, as detailed in Section 4.4. Table 6 reports"",null,null",null,null
232,"231,11h p://scikit-learn.org/,null,null",null,null
233,"232,490,null,null",null,null
234,"233,Session 4C: Queries and Query Analysis,null,null",null,null
235,"234,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
236,"235,nDCG@100,null,null",null,null
237,"236,-MART (oblivious),null,null",null,null
238,"237,-MART (switching),null,null",null,null
239,"238,0.40 0.38 0.36 0.34 0.32 0.30 0.28,null,null",null,null
240,"239,0,null,null",null,null
241,"240,20,null,null",null,null
242,"241,40,null,null",null,null
243,"242,60,null,null",null,null
244,"243,80,null,null",null,null
245,"244,100,null,null",null,null
246,"245,% Noise amount,null,null",null,null
247,"246,Figure 3: Semantic query annotation robustness for simulated intent classi ers of a range of accuracy levels.,null,null",null,null
248,"247,""intent classi cation accuracy averaged across test queries in all cross-validation rounds. As shown in the table, the most accurate intent classi er is learned via stochastic gradient descent with a log loss, which performs an incremental logistic regression. As a result, we will use this classi er in the remainder of our experiments. Other e ective classi cation algorithms include random forest and bagging, while AdaBoost has the lowest accuracy."",null,null",null,null
249,"248,Table 6: ery intent classi cation accuracy.,null,null",null,null
250,"249,Algorithm,null,null",null,null
251,"250,AdaBoost Support Vector Machines Gradient Boosting Bagging Random Forest Logistic Regression,null,null",null,null
252,"251,Accuracy,null,null",null,null
253,"252,67.00% 74.00% 75.75% 76.00% 76.50% 77.00%,null,null",null,null
254,"253,""e top performing classi er in Table 6 still leaves room for further improvement in intent classi cation accuracy. An interesting question here is whether this level of accuracy is enough for an e ective deployment of our proposed intent-aware semantic query annotation approach. To further investigate the role of the intent classi cation component in our approach, we measure the impact of a range of simulated intent classi ers on the e ectiveness of the produced ranking of semantic annotations. In particular, starting from a perfect intent classi er (i.e., an oracle), we gradually introduce noise in the classi cation outcome by replacing the correct intent with a random one, up to the point where the classi cation itself becomes a random guess of the four available intents (i.e., E, T, Q, and O). As shown in Figure 3, our intent-aware switching strategy can outperform the intent-agnostic oblivious strategy with up to 50% of random noise in intent classi cation, which is a remarkable result. Recalling Q2, the experiments in this section demonstrate that accurate intent classi cation is feasible, and that the overall ranking annotation performance is robust to a considerable amount of noise in the predicted intents."",null,null",null,null
255,"254,5.3 Annotation E ectiveness,null,null",null,null
256,"255,""Section 5.1 showed the promise of leveraging intent-speci c ranking models, while Section 5.2 demonstrated that achieving this promise is feasible with reasonably accurate query intent classi ers. In this section, we address Q3, by assessing the e ectiveness of our"",null,null",null,null
257,"256,""intent-aware semantic query annotation approach in contrast to the various baselines described in Section 4.3. ese include BM25 as a vanilla ad-hoc search baseline, FSDM as a representative of the current state-of-the-art, and multiple deployments of LambaMART using baseline intent-agnostic ranking adaptation strategies ( xed and oblivious) as well as our proposed intent-aware strategies (switching and mixing). Table 7 summarizes the results of this investigation in terms of P@10, nDCG@10, and MAP averaged across all 400 test queries from the four query sets in Table 3.12 In each row describing baseline results (the top half of the table), a rst of the symbols introduced in Section 4.4 denotes a statistically signi cant di erence (or lack thereof) with respect to LambdaMART (switching), whereas a second symbol denotes potential di erences with respect to LambdaMART (mixing). A further symbol is shown alongside LambdaMART (switching) to denote a signi cant di erence (or lack thereof) with respect to LambdaMART (mixing)."",null,null",null,null
258,"257,""Table 7: Comparison of intent-agnostic (BM25, FSDM, LambdaMART xed and oblivious) and intent-aware (LambdaMART switching and mixing) semantic query annotation."",null,null",null,null
259,"258,BM25 FSDM LambdaMART,null,null",null,null
260,"259,( xed-E) ( xed-T) ( xed-Q) ( xed-O) (oblivious),null,null",null,null
261,"260,(switching) (mixing),null,null",null,null
262,"261,P@10,null,null",null,null
263,"262,0.181 0.204,null,null",null,null
264,"263,0.178 0.202 0.152 0.188 0.192,null,null",null,null
265,"264,0.227 0.243,null,null",null,null
266,"265,nDCG@10 MAP,null,null",null,null
267,"266,0.250 0.289,null,null",null,null
268,"267,0.163 0.195,null,null",null,null
269,"268,0.244 0.275 0.215 0.260 0.276,null,null",null,null
270,"269,0.329 0.346,null,null",null,null
271,"270,0.162 0.172 0.139 0.163 0.178,null,null",null,null
272,"271,0.219 0.229,null,null",null,null
273,"272,""From Table 7, we rst observe that FSDM performs strongly, outperforming all intent-agnostic variants deployed with LambdaMART, which con rms its e ectiveness as a representative of the state-of-the-art. Also of note is the fact that a single model trained on a set of multiple intents using the oblivious strategy cannot consistently improve upon the best performing intent-speci c model, produced by the xed-T strategy. In contrast, both of our intent-aware ranking adaptation strategies are able to consistently leverage the best characteristics of each individual intent, signi cantly outperforming all intent-agnostic baselines in all se ings. In particular, compared to FSDM, our switching strategy improves by up to 11% in P@10, 14% in nDCG@10, and 12% in MAP. Compared to the best performing intent-agnostic strategy under LambdaMART ( xed-T), gains are as high as 12% in P@10, 20% in nDCG@10, and 27% in MAP. Lastly, we also note that our mixing strategy further signi cantly improves upon the switching strategy. is result suggests that merging multiple intent-speci c models (the mixing strategy) can be safer than applying a single model associated with the most likely query intent (the switching strategy). Recalling Q3, these results a est the e ectiveness of our intent-aware ranking adaptation for semantic query annotation."",null,null",null,null
274,"273,12E ectiveness breakdown analyses per query intent and various other query characteristics are presented in Section 5.4.,null,null",null,null
275,"274,491,null,null",null,null
276,"275,Session 4C: Queries and Query Analysis,null,null",null,null
277,"276,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
278,"277,5.4 Breakdown Analyses,null,null",null,null
279,"278,""e previous analysis demonstrated the e ectiveness of our approach on the entire set of 400 queries. To further shed light on the reasons behind such an e ective performance, we address question Q4, by analyzing the improvements brought by our approach for queries with di erent intents, lengths, and di culty."",null,null",null,null
280,"279,""5.4.1 Analysis by ery Intent. Table 8 breaks down the results in Table 7 according to the target intent of each query. For brevity, only the best among the xed strategy variants is shown. Note that while our approach aims to predict the correct intent of each query, there is no guarantee that a perfect intent classi cation will be achieved, as discussed in Section 5.2. Hence, it is important to understand how well our approach performs on queries of each target intent. From Table 7, as expected, the best xed strategy for each group of queries is that optimized for the group itself (e.g., xed-E is the best xed strategy for entity queries--the E group). Nonetheless, our intent-aware mixing strategy is the most consistent across all groups, with e ectiveness on a par with the best xed strategy for each group. Compared to our switching strategy, the mixing strategy is particularly e ective for type queries (the T group), with statistical ties for all other groups. Regarding performance di erences across the target intents, we note that all approaches achieve their best absolute performance on E queries followed by queries with other intents (the O group), which also includes entity queries. e e ective results a ained even by the simple BM25 baseline suggest that queries with these intents are well handled by content-based approaches."",null,null",null,null
281,"280,""Compared to the intent-agnostic FSDM baseline, our largest improvements are observed for type queries (the T group) and question queries (the Q group). For T queries, the structure-based features exploited by our learning to rank approach bring only small improvements, as observed by contrasting LambdaMART (oblivious) with FSDM. However, with our proposed intent-aware ranking adaptation strategies, further marked improvements are observed, with the mixing strategy signi cantly improving upon the oblivious strategy by up to 25% in P@10, 35% in nDCG@10, and 44% in MAP. For Q queries, both the extra features exploited via learning to rank as well as our ranking adaptation strategies help, with the switching strategy improving even further compared to the oblivious one by up to 56% in P@10, 51% in nDCG@10, and 50% in MAP. Figure 4 further illustrates the consistent improvements in terms of nDCG@100 a ained by our intent-aware strategies (here represented by the mixing strategy) compared to the intentagnostic oblivious baseline. Indeed, not only does mixing improve more queries than it hurts compared to oblivious, but it also shows larger increases and smaller decreases throughout queries of all four intents. Analyzing each intent separately, the most noticeable di erence can be observed for Q queries, with mixing performing be er for 50% of the queries and losing in only 10%. For E and T queries, the di erences in nDCG are not as high, but mixing is still superior for 60% of the queries. e smallest gap between the two strategies appears in O queries, although once again mixing performs be er for 60% the queries."",null,null",null,null
282,"281,""5.4.2 Analysis by ery Length. Continuing our detailed analysis, Table 9 breaks down the results from Table 7 according to"",null,null",null,null
283,"282,Table 8: E ectiveness breakdown by query intent.,null,null",null,null
284,"283,P@10,null,null",null,null
285,"284,nDCG@10 MAP,null,null",null,null
286,"285,E queries (100 queries),null,null",null,null
287,"286,BM25 FSDM LambdaMART,null,null",null,null
288,"287,( xed-E) (oblivious),null,null",null,null
289,"288,(switching) (mixing),null,null",null,null
290,"289,0.240 0.286,null,null",null,null
291,"290,0.293  0.239 0.282 0.297,null,null",null,null
292,"291,0.416 0.499,null,null",null,null
293,"292,0.498 0.434 0.486 0.502,null,null",null,null
294,"293,0.320 0.396,null,null",null,null
295,"294,0.387 0.329 0.377 0.390,null,null",null,null
296,"295,T queries (100 queries),null,null",null,null
297,"296,BM25 FSDM LambdaMART,null,null",null,null
298,"297,( xed-T) (oblivious),null,null",null,null
299,"298,0.190 0.211,null,null",null,null
300,"299,0.289  0.216,null,null",null,null
301,"300,0.193 0.223,null,null",null,null
302,"301,0.327  0.225,null,null",null,null
303,"302,0.141 0.167,null,null",null,null
304,"303,0.219  0.146,null,null",null,null
305,"304,(switching) 0.232 (mixing) 0.271,null,null",null,null
306,"305,0.260 0.303,null,null",null,null
307,"306,0.185 0.210,null,null",null,null
308,"307,Q queries (100 queries),null,null",null,null
309,"308,BM25 FSDM LambdaMART,null,null",null,null
310,"309,( xed-Q) (oblivious),null,null",null,null
311,"310,(switching) (mixing),null,null",null,null
312,"311,0.060 0.061,null,null",null,null
313,"312,0.143 0.091 0.142 0.141,null,null",null,null
314,"313,0.108 0.127,null,null",null,null
315,"314,0.273 0.177 0.267 0.266,null,null",null,null
316,"315,0.077 0.098,null,null",null,null
317,"316,0.203 0.132 0.198 0.194,null,null",null,null
318,"317,BM25 FSDM LambdaMART,null,null",null,null
319,"318,( xed-O) (oblivious),null,null",null,null
320,"319,(switching) (mixing),null,null",null,null
321,"320,O queries (100 queries),null,null",null,null
322,"321,0.235 0.258,null,null",null,null
323,"322,0.282 0.308,null,null",null,null
324,"323,0.259 0.221,null,null",null,null
325,"324,0.254 0.264,null,null",null,null
326,"325,0.304 0.267,null,null",null,null
327,"326,0.305 0.312,null,null",null,null
328,"327,0.113 0.119,null,null",null,null
329,"328,0.113 0.105,null,null",null,null
330,"329,0.116 0.123,null,null",null,null
331,"330,""the length of each query. In particular, we consider three groups of queries: short queries, with 1 or 2 terms (74 queries); medium queries, with 3 or 4 terms (193 queries); and long queries, with 5 or more terms (133 queries). From Table 9, we observe relatively higher performances of all approaches on short queries compared to those of other lengths. FSDM delivers a particularly strong performance on this group, with only a small gap from our mixing strategy, which is the overall best. is can be explained by FSDM's previously discussed e ectiveness on E queries, which have only 2.7 terms on average. Compared to the oblivious strategy, mixing brings substantial and signi cant improvements, once again demonstrating the bene ts of an intent-aware ranking adaptation. For medium and long queries (5 or more terms), both of our intent-aware strategies bring even more pronounced improvements compared to all intent-agnostic baselines, with the top performing mixing strategy outperforming the oblivious strategy by up to 32% in P@10, 30% in nDCG@10, and 36% in MAP. is tendency is somewhat expected given the e ective performance observed"",null,null",null,null
332,"331,492,null,null",null,null
333,"332,Session 4C: Queries and Query Analysis,null,null",null,null
334,"333,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
335,"334,(a) All queries,null,null",null,null
336,"335,(b) E queries,null,null",null,null
337,"336,(c) T queries,null,null",null,null
338,"337,(d) Q queries,null,null",null,null
339,"338,(e) O queries,null,null",null,null
340,"339,Figure 4: Di erences in nDCG@100 between LambdaMART (mixing) and LambdaMART (oblivious) across: (a) all queries; (b) E queries; (c) T queries; (d) Q queries; (e) O queries. Positive values indicate mixing is better.,null,null",null,null
341,"340,""in Table 9 for the proposed intent-aware strategies on Q queries, which are typically longer (8 terms on average)."",null,null",null,null
342,"341,Table 9: E ectiveness breakdown by query length.,null,null",null,null
343,"342,P@10,null,null",null,null
344,"343,nDCG@10 MAP,null,null",null,null
345,"344,1 or 2 terms (74 queries),null,null",null,null
346,"345,BM25 FSDM LambdaMART,null,null",null,null
347,"346,( xed-E) (oblivious),null,null",null,null
348,"347,(switching) (mixing),null,null",null,null
349,"348,0.253 0.323,null,null",null,null
350,"349,0.324 0.276 0.324 0.337,null,null",null,null
351,"350,0.363 0.456,null,null",null,null
352,"351,0.453 0.394 0.455 0.465,null,null",null,null
353,"352,0.268 0.331,null,null",null,null
354,"353,0.327 0.278 0.324 0.332,null,null",null,null
355,"354,3 or 4 terms (193 queries),null,null",null,null
356,"355,BM25 FSDM LambdaMART,null,null",null,null
357,"356,( xed-T) (oblivious),null,null",null,null
358,"357,(switching) (mixing),null,null",null,null
359,"358,0.196 0.221,null,null",null,null
360,"359,0.234 0.208,null,null",null,null
361,"360,0.240 0.265,null,null",null,null
362,"361,0.264 0.308,null,null",null,null
363,"362,0.307 0.296,null,null",null,null
364,"363,0.349 0.376,null,null",null,null
365,"364,0.161 0.198,null,null",null,null
366,"365,0.182 0.183 0.227 0.239,null,null",null,null
367,"366,5 or more terms (133 queries),null,null",null,null
368,"367,BM25 FSDM LambdaMART,null,null",null,null
369,"368,( xed-T) (oblivious),null,null",null,null
370,"369,(switching) (mixing),null,null",null,null
371,"370,0.119 0.114,null,null",null,null
372,"371,0.141 0.121 0.156 0.160,null,null",null,null
373,"372,0.167 0.171,null,null",null,null
374,"373,0.206 0.181 0.230 0.236,null,null",null,null
375,"374,0.106 0.115,null,null",null,null
376,"375,0.134 0.116,null,null",null,null
377,"376,0.150 0.158,null,null",null,null
378,"377,""5.4.3 Analysis by ery Di iculty. To complete our breakdown analysis, we regroup all 400 queries in our investigation according to their di culty. In particular, we consider three groups: di cult queries, with 3 or less relevant results in the ground-truth (108 queries); moderate queries, with 4 to 20 relevant results (184 queries); and easy queries, with more than 20 relevant results (108 queries). e results of this investigation are shown in Table 10. From the table, we note as expected that di cult queries generally incur in reduced precision at early ranks (as measured by both P@10 and nDCG@10), while easy queries tend to penalize recall at"",null,null",null,null
379,"378,""lower ranks (as measured by MAP). Nevertheless, our intent-aware adaptation strategies are once again the most e ective across all groups of queries, with the mixing strategy consistently providing the overall best results. For di cult queries (3 or less relevant results), compared to the oblivious strategy, mixing improves by up to 19% in P@10, 24% in nDCG@10, and 26% in MAP. For easy queries (21 or more relevant results), improvements are as high as 24% in P@10, 27% in nDCG@10, and 39% in MAP."",null,null",null,null
380,"379,Table 10: E ectiveness breakdown by query di culty.,null,null",null,null
381,"380,P@10,null,null",null,null
382,"381,nDCG@10 MAP,null,null",null,null
383,"382,Di cult: 3 or less relevant results (108 queries),null,null",null,null
384,"383,BM25 FSDM LambdaMART,null,null",null,null
385,"384,( xed-T) (oblivious),null,null",null,null
386,"385,0.059 0.064,null,null",null,null
387,"386,0.062 0.063,null,null",null,null
388,"387,0.224 0.256,null,null",null,null
389,"388,0.230 0.259,null,null",null,null
390,"389,0.179 0.214,null,null",null,null
391,"390,0.180 0.213,null,null",null,null
392,"391,(switching) 0.069 (mixing) 0.075,null,null",null,null
393,"392,0.308 0.322,null,null",null,null
394,"393,0.260 0.268,null,null",null,null
395,"394,Moderate: 4 to 20 relevant results (184 queries),null,null",null,null
396,"395,BM25 FSDM LambdaMART,null,null",null,null
397,"396,( xed-O) (oblivious),null,null",null,null
398,"397,0.210 0.245,null,null",null,null
399,"398,0.218 0.214,null,null",null,null
400,"399,0.260 0.308,null,null",null,null
401,"400,0.274 0.276,null,null",null,null
402,"401,0.194 0.235,null,null",null,null
403,"402,0.195 0.202,null,null",null,null
404,"403,(switching) 0.261 (mixing) 0.279,null,null",null,null
405,"404,0.329 0.345,null,null",null,null
406,"405,0.245 0.257,null,null",null,null
407,"406,Easy: 21 or more relevant results (108 queries),null,null",null,null
408,"407,BM25 FSDM LambdaMART,null,null",null,null
409,"408,( xed-T) (oblivious),null,null",null,null
410,"409,0.255 0.275,null,null",null,null
411,"410,0.328 0.283,null,null",null,null
412,"411,0.259 0.292,null,null",null,null
413,"412,0.338 0.292,null,null",null,null
414,"413,0.094 0.107,null,null",null,null
415,"414,0.125 0.103,null,null",null,null
416,"415,(switching) 0.328 (mixing) 0.350,null,null",null,null
417,"416,0.352 0.371,null,null",null,null
418,"417,0.134 0.143,null,null",null,null
419,"418,""Recalling Q4, the results in this section demonstrate the consistency of our intent-aware ranking adaptation strategies for semantic query annotation. Overall, both the switching and the mixing strategies achieve generally improved results for queries"",null,null",null,null
420,"419,493,null,null",null,null
421,"420,Session 4C: Queries and Query Analysis,null,null",null,null
422,"421,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
423,"422,""of di erent target intents, lengths, and di culty levels, o en signi cantly. Particularly, question-oriented queries (the Q intent), long queries (queries with 5 or more terms), and moderate to easy queries (queries with 4 or more relevant results) are the ones that bene t the most from our intent-aware approach."",null,null",null,null
424,"423,6 CONCLUSIONS,null,null",null,null
425,"424,""We presented a framework for learning to rank semantic annotations suitable to the intent of each individual query. Our approach predicts the intent of a target query and adapts the ranking produced for this query using one of two strategies: switching, which applies a ranking model trained on queries of the same intent as predicted for the target query, or mixing, which combines the results of multiple intent-speci c ranking models according to their predicted likelihood for the target query. Extensive experiments on a publicly available benchmark demonstrated the e ectiveness of our approach for semantic query annotation, with signi cant improvements compared to state-of-the-art intent-agnostic approaches. e results also a ested the consistency of the observed improvements for queries of di erent intents, lengths, and di culty levels."",null,null",null,null
426,"425,""In the future, we plan to assess the impact of intent-aware learning on frameworks other than learning to rank. Preliminary results in this direction show that the FSDM baseline, which is based on the Markov random elds framework, can be improved with an intent-aware approach to hyperparameter tuning, although with less marked gains compared to the ones observed in our experiments with feature-based models using learning to rank. Another direction for future investigation includes evaluating our approach with a larger intent taxonomy, including more queries with less common intents such as a ribute and relation queries."",null,null",null,null
427,"426,ACKNOWLEDGMENTS,null,null",null,null
428,"427,""is work was partially funded by projects InWeb (MCT/ CNPq 573871/2008-6) and MASWeb (FAPEMIG/PRONEX APQ-01400-14), and by the authors' individual grants from CNPq and FAPEMIG."",null,null",null,null
429,"428,REFERENCES,null,null",null,null
430,"429,""[1] Omar Alonso and Hugo Zaragoza. 2008. Exploiting semantic annotations in information retrieval: ESAIR '08. SIGIR Forum 42, 1 (2008), 55­58."",null,null",null,null
431,"430,""[2] Kriztian Balog, Arien P. de Vries, Pavel Serdyukov, Paul omas, and ijs Westerveld. 2009. Overview of the TREC 2009 Entity track. In TREC."",null,null",null,null
432,"431,[3] Krisztian Balog and Robert Neumayer. 2013. A Test Collection for Entity Search in DBpedia. In Proc. of SIGIR. 737­740.,null,null",null,null
433,"432,""[4] Hannah Bast, Bjo¨rn Buchhold, Elmar Haussmann, and others. 2016. Semantic Search on Text and Knowledge Bases. Foundations and Trends in Information Retrieval 10, 2-3 (2016), 119­271."",null,null",null,null
434,"433,""[5] Bin Bi, Hao Ma, Bo-June (Paul) Hsu, Wei Chu, Kuansan Wang, and Junghoo Cho. 2015. Learning to Recommend Related Entities to Search Users. In Proc. of WSDM. 139­148."",null,null",null,null
435,"434,""[6] Roi Blanco, Peter Mika, and Sebastian Vigna. 2011. E ective and E cient Entity Search in RDF Data. In Proc. of ISWC. 83­97."",null,null",null,null
436,"435,""[7] Roi Blanco, Peter Mika, and Hugo Zaragoza. 2010. Entity Search Track Submission by Yahoo! Research Barcelona. In Proc. of Entity Search Track."",null,null",null,null
437,"436,""[8] David J. Brenes, Daniel Gayo-Avello, and Kilian Pe´rez-Gonza´lez. 2009. Survey and Evaluation of ery Intent Detection Methods. In Proc. of WSCD. 1­7."",null,null",null,null
438,"437,""[9] Andrei Broder. 2002. A Taxonomy of Web Search. SIGIR Forum 36, 2 (2002), 3­10."",null,null",null,null
439,"438,""[10] Marc Bron, Krisztian Balog, and Maarten de Rijke. 2010. Ranking related entities: components and analyses. In Proc. of CIKM. 1079­1088."",null,null",null,null
440,"439,""[11] Marc Bron, Krisztian Balog, and Maarten De Rijke. 2013. Example Based Entity Search in the Web of Data. In Proc. of ECIR. 392­403."",null,null",null,null
441,"440,""[12] Ste´phane Campinas, Renaud Delbru, Nur Aini Rakhmawati, Diego Ceccarelli, and Giovanni Tummarello. 2011. Sindice BM25F at SemSearch 2011. In Proc. of"",null,null",null,null
442,"441,SemSearch. [13] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge,null,null",null,null
443,"442,overview.. In Yahoo! Learning to Rank Challenge. 1­24. [14] Nick Craswell and David Hawking. 2004. Overview of the TREC 2004 Web track.,null,null",null,null
444,"443,""In Proc. of TREC. [15] Arjen P. de Vries, Anne-Marie Vercoustre, James A. om, Nick Craswell, and"",null,null",null,null
445,"444,""Mounia Lalmas. 2007. Overview of the INEX 2007 Entity Ranking track. In Proc. of INEX. 245­251. [16] Shady Elbassuoni and Roi Blanco. 2011. Keyword Search over RDF Graphs. In Proc. of CIKM. 237­242. [17] Shady Elbassuoni, Maya Ramanath, Ralf Schenkel, Marcin Sydow, and Gerhard Weikum. 2009. Language-Model-Based Ranking for eries on RDF-Graphs. In Proc. of CIKM. 977­986. [18] Besnik Fetahu, Ujwal Gadiraju, and Stefan Dietze. 2015. Improving entity retrieval on structured data. In Proc. of ISWC. 474­491. [19] Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold, Hang Li, and Heung-Yeung Shum. 2008. ery dependent ranking using k-nearest neighbor. In Proc. of SIGIR. 115­122. [20] Ido Guy. 2016. Searching by Talking: Analysis of Voice eries on Mobile Web Search. In Proc. of SIGIR. 35­44. [21] Daniel M Herzig, Peter Mika, Roi Blanco, and anh Tran. 2013. Federated Entity Search Using On-the- y Consolidation. In Proc. of ISWC. 167­183. [22] Bernard J. Jansen, Amanda Spink, and Te o Saracevic. 2000. Real Life, Real Users, and Real Needs: A Study and Analysis of User eries on the Web. Information Processing and Management 36, 2 (2000), 207­227. [23] In-Ho Kang and GilChang Kim. 2003. ery type classi cation for Web document retrieval. In Proc. of SIGIR. 64­71. [24] Tie-Yan Liu and others. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331. [25] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego, and Nicola Tonello o. 2015. Speeding Up Document Ranking with Rank-based Features. In Proc. of SIGIR. 895­898. [26] Donald Metzler and W. Bruce Cro . 2005. A Markov Random Field Model for Term Dependencies. In Proc. of SIGIR. 472­479. [27] Robert Neumayer, Krisztian Balog, and Kjetil Nørva°g. 2012. On the Modeling of Entities for Ad-Hoc Entity Search in the Web of Data. In Proc. of ECIR. 133­145. [28] Fedor Nikolaev, Alexander Kotov, and Nikita Zhiltsov. 2016. Parameterized Fielded Term Dependence Models for Ad-hoc Entity Retrieval from Knowledge Graph. In Proc. of SIGIR. 435­444. [29] Paul Ogilvie and Jamie Callan. 2003. Combining Document Representations for Known-item Search. In Proc. of SIGIR. 143­150. [30] Jie Peng, Craig Macdonald, and Iadh Ounis. 2010. Learning to select a ranking function. In Proc. of ECIR. 114­126. [31] Jose´ R Pe´rez-Agu¨era, Javier Arroyo, Jane Greenberg, Joaquin Perez Iglesias, and Victor Fresno. 2010. Using BM25F for semantic search. In Proc. of SemSearch. 2. [32] Je rey Pound, Peter Mika, and Hugo Zaragoza. 2010. Ad-hoc Object Retrieval in the Web of Data. In Proc. of WWW. 771­780. [33] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Mike Gatford, and A. Payne. 1995. Okapi at TREC-4. In Proc. of TREC. [34] Cristiano Rocha, Daniel Schwabe, and Marcus Poggi Aragao. 2004. A Hybrid Approach for Searching in the Semantic Web. In Proc. of WWW. 374­383. [35] Daniel E. Rose and Danny Levinson. 2004. Understanding User Goals in Web Search. In Proc. of WWW. 13­19. [36] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Selectively diversifying web search results. In Proc. of CIKM. 1179­1188. [37] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Voting for related entities. In Proc. of RIAO. 1­8. [38] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2011. Intent-aware Search Result Diversi cation. In Proc. of SIGIR. 595­604. [39] Alberto Tonon, Gianluca Demartini, and Philippe Cudre´-Mauroux. 2012. Combining Inverted Indices and Structured Search for Ad-hoc Object Retrieval. In Proc. of SIGIR. 125­134. [40] Gilad Tsur, Yuval Pinter, Idan Szpektor, and David Carmel. 2016. Identifying Web eries with estion Intent. In Proc. of WWW. 783­793. [41] Qiang Wu, Chris JC Burges, Krysta M Svore, and Jianfeng Gao. 2008. Ranking, boosting, and model adaptation. Technical Report. Technical report, Microso Research. [42] Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow. 2005. Learning to estimate query di culty: including applications to missing content detection and distributed information retrieval. In Proc. of SIGIR. 512­519. [43] Nikita Zhiltsov and Eugene Agichtein. 2013. Improving Entity Search over Linked Data by Modeling Latent Semantics. In Proc. of CIKM. 1253­1256. [44] Nikita Zhiltsov, Alexander Kotov, and Fedor Nikolaev. 2015. Fielded Sequential Dependence Model for Ad-Hoc Entity Retrieval in the Web of Data. In Proc. of SIGIR. 253­262."",null,null",null,null
446,"445,494,null,null",null,null
447,"446,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Resource Papers,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Luandri: A Clean Lua Interface to the Indri Search Engine,null,null",null,null
4,"3,Bhaskar Mitra,null,null",null,null
5,"4,""Microso , University College London Cambridge, UK"",null,null",null,null
6,"5,bmitra@microso .com,null,null",null,null
7,"6,Fernando Diaz,null,null",null,null
8,"7,""Spotify New York, USA diazf@acm.org"",null,null",null,null
9,"8,Nick Craswell,null,null",null,null
10,"9,""Microso Bellevue, USA nickcr@microso .com"",null,null",null,null
11,"10,ABSTRACT,null,null",null,null
12,"11,""In recent years, the information retrieval (IR) community has witnessed the rst successful applications of deep neural network models to short-text matching and ad-hoc retrieval tasks. However, the two communities--focused on deep neural networks and on IR-- have less in common when it comes to the choice of programming languages. Indri, an indexing framework popularly used by the IR community, is wri en in C++, while Torch, a popular machine learning library for deep learning, is wri en in the light-weight scripting language Lua. To bridge this gap, we introduce Luandri (pronounced """"laundry""""), a simple interface for exposing the search capabilities of Indri to Torch models implemented in Lua."",null,null",null,null
13,"12,CCS CONCEPTS,null,null",null,null
14,"13,·Information systems Information retrieval; Web searching and information discovery; ·Computing methodologies Neural networks;,null,null",null,null
15,"14,KEYWORDS,null,null",null,null
16,"15,Information retrieval; application programming interface; neural networks,null,null",null,null
17,"16,1 INTRODUCTION,null,null",null,null
18,"17,""In recent years, deep neural networks (DNNs) have demonstrated early positive results on a variety of standard information retrieval (IR) tasks, including on short-text matching [10, 11, 14, 19, 21, 22] and ad-hoc retrieval [9, 17], and shown promising performances on other emerging retrieval tasks such as multi-modal retrieval [15] and conversational IR [28, 30]. is work occurs at the intersection of the machine learning and information retrieval communities, who have di erent research tools that are implemented in di erent programming languages. Popular neural network toolkits are o en implemented in (or have bindings for) scripting languages, such as Python1 (e.g., TensorFlow [1], eano [2], CNTK [29], Ca e [13], MXNet [3], Chainer [25], and PyTorch2) or Lua [12] (e.g., Torch [4])"",null,null",null,null
19,"18, e author is a part-time PhD student at UCL. Work done while at Microso . 1h ps://www.python.org/ 2h ps://github.com/pytorch/pytorch,null,null",null,null
20,"19,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080650"",null,null",null,null
21,"20,""because of their rapid prototyping capabilities. In contrast, many popular indexing frameworks for IR are implemented in C++ (e.g., Indri [24]) or Java (e.g., Terrier [18] and Apache Lucene [16]). e open-source community has developed Python wrappers over the Indri [26] and the Apache Lucene [20] programming interfaces to expose the functionalities of these rich IR libraries to the programming language. However, there is still a gap that remains to be bridged for non-Python based deep learning toolkits, such as Torch."",null,null",null,null
22,"21,""Torch3 is a numeric computing framework popular among the deep neural network community. It has been shown to be significantly faster compared to other toolkits such as TensorFlow on convolutional neural networks in multi-GPU environment [23]. It is implemented using the light-weight scripting language Lua.4 In this paper, we introduce Luandri (pronounced """"laundry"""") ­ a Lua wrapper over the Indri search engine. In particular, Luandri exposes parts of the Indri query environment application programming interface (API) for document retrieval including support for the rich Indri query language."",null,null",null,null
23,"22,2 MOTIVATION,null,null",null,null
24,"23,""ere are a variety of scenarios in which a DNN model can bene t from having access to a search engine during training and/or evaluation. Existing DNN models for ad-hoc retrieval [9, 17], for example, operate on query-document pairs to predict relevance. Running these models on the full corpus is prohibitively costly ­ therefore the evaluation of these models is o en limited to re-ranking topN candidate documents retrieved by a traditional IR model or a search engine. Typically, these candidate sets are retrieved o ine in a process separate from the one in which the DNN is evaluated. However, if the search engine is accessible in the same language as the one in which the DNN is implemented, then the candidate generation step and the DNN-based re-ranking step can follow each other within the same process ­ removing the requirement to store large quantity of intermediate datasets containing the candidates to be ranked."",null,null",null,null
25,"24,""DNN models train on labelled data, although in some cases labels can be inferred rather than explicit. For example, many DNN models for IR [9­11, 14, 22] use negative training examples that are sampled uniformly from the corpus. Recently Mitra et al. [17] reported that training with judged negative documents can yield be er NDCG performance than training with uniform random negatives. Having access to a search engine during training could enable additional methods for generating negative samples, such as using documents that are retrieved by the engine but at lower ranks."",null,null",null,null
26,"25,3h ps://github.com/torch/torch7 4h ps://www.lua.org,null,null",null,null
27,"26,1221,null,null",null,null
28,"27,Short Resource Papers,null,null",null,null
29,"28,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
30,"29,""Code snippet 1: Sample Lua code for searching an Indri index using the Luandri API. e Indri index builder application is used for generating the index beforehand. e search query is written using the popular INQUERY structured operators that are supported natively by Indri for specifying matching constraints. e run ery method in the Luandri API accepts the request as a Lua table and automatically converts it into the appropriate C++ request object that Indri natively expects. Similarly, the result object returned by Indri in C++ is automatically converted to a Lua table."",null,null",null,null
31,"30,1,null,null",null,null
32,"31,""local luandri , paths.dofile( luandri.lua )"",null,null",null,null
33,"32,2,null,null",null,null
34,"33,""local query_environment , QueryEnvironment()"",null,null",null,null
35,"34,3,null,null",null,null
36,"35,query_environment:addIndex( path_to_index_file ),null,null",null,null
37,"36,4,null,null",null,null
38,"37,5,null,null",null,null
39,"38,""local request , {"",null,null",null,null
40,"39,6,null,null",null,null
41,"40,""query ,"""" #syn( #od1(neural networks) #od1(deep learning)) #greater(year 2009) ,"""""",null,null",null,null
42,"41,7,null,null",null,null
43,"42,""resultsRequested , 10"",null,null",null,null
44,"43,8,null,null",null,null
45,"44,},null,null",null,null
46,"45,9,null,null",null,null
47,"46,""local results , query_environment:runQuery(request).results"",null,null",null,null
48,"47,10,null,null",null,null
49,"48,11,null,null",null,null
50,"49,""for k, v in pairs(results) do"",null,null",null,null
51,"50,12,null,null",null,null
52,"51,print(v.docid .. ,null,null",null,null
53,"52, .. v.documentName .. ,null,null",null,null
54,"53, .. v.snippet .. ,null,null",null,null
55,"54, ),null,null",null,null
56,"55,13,null,null",null,null
57,"56,end,null,null",null,null
58,"57,e lack of adequate labelled data available for training DNN models for ad-hoc retrieval has been a focus for the neural IR community [5]. It is possible that alternate strategies for supervision may be considered for training these deep models ­ including reinforcement learning [27] and training under adversarial se ings [8] ­ which could also make use of retrieval from a full corpus during the model training.,null,null",null,null
59,"58,""Diaz et al. [6] demonstrated a di erent application of the traditional retrieval step in the neural IR model. Given a query, they retrieve a set of documents using Indri and use that to train a brand new distributed representation of words speci c to that query at run time. Such models, with query-speci c representation learning, can be implemented and deployed more easily if the machine learning framework has access to a search engine."",null,null",null,null
60,"59,""Finally, Ghazvininejad et al. [7] proposed to """"lookup"""" external repositories of facts as part of solving larger tasks using neural network models. Empowering DNN models with access to a search engine may be an exciting area for future exploration."",null,null",null,null
61,"60,""In all these scenarios, it is useful for a search engine, such as Indri, to be accessible from the same programming language used to implement the DNN. erefore, we are optimistic that by publicly releasing the Luandri API we will stimulate novel explorations from IR researchers already familiar with Torch."",null,null",null,null
62,"61,3 QUERYING INDRI FROM LUA,null,null",null,null
63,"62,Indri is an open-source search engine available with the Lemur toolkit.5 Indri consists of two primary components ­ an application that builds an index from a raw document collection and another application that can perform searches using this index. e Indri index builder can deal with several di erent document formats for,null,null",null,null
64,"63,5h p://www.lemurproject.org/indri/,null,null",null,null
65,"64,""indexing. is includes TREC (text and Web), HTML, XML, PDF, and plain text among many others."",null,null",null,null
66,"65,""Searching using Indri involves specifying one or more indices and querying them by either interactively calling the API or by running an application in batch-mode. e Indri query language supports a rich set of operators for specifying phrasal matching conditions, synonymy relationships, document ltering criteria, and other complex constraints. e full query language grammar is available online for reference.6"",null,null",null,null
67,"66,Invoking a search on an Indri index using the Luandri API is like how one may use the native C++ Indri API. Code snippet 1 shows a minimal example of a typical Indri-based search using the Luandri API. We observe that the search is performed by invoking very few lines of Lua code.,null,null",null,null
68,"67,""e example also demonstrates the use of Indri structured queries. A search is performed using a structured query that constraints the matching to either of the two ordered phrases ­ """"neural networks"""" or """"deep learning"""". e query directs Indri to treat both phrases as synonyms. In addition, a numeric lter is speci ed to limit matches to only documents whose value corresponding to the year eld is greater than 2009."",null,null",null,null
69,"68,""is example shows searching on the full document index. However, Luandri also allows users to specify a list of document identi ers in the request object to limit the search to only those set of documents. A xed list of stop words can also be speci ed for retrieval using the Luandri API."",null,null",null,null
70,"69,e full Luandri implementation is available on GitHub7 under the MIT license. We direct interested readers to the source code for exact API speci cations.,null,null",null,null
71,"70,6h ps://www.lemurproject.org/lemur/Indri eryLanguage.php 7h ps://github.com/bmitra-ms /Luandri,null,null",null,null
72,"71,1222,null,null",null,null
73,"72,Short Resource Papers,null,null",null,null
74,"73,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
75,"74,4 UNDER THE HOOD,null,null",null,null
76,"75,""e implementation of Lua as a programming language puts a strong emphasis on extensibility [12]. Lua is an extension language because any Lua code can be relatively easily embedded as libraries into code wri en in other languages. It is also an extensible language because of its ability to call functions wri en in other languages, such as C. e implementation of the Luandri API bene ts from the la er property of the language."",null,null",null,null
77,"76,Lua comes with a fast Just In Time (JIT) compiler called LuaJIT.8 LuaJIT exposes a foreign-function interface9 (FFI) that makes it easy to call external C functions and manipulate C data structures from Lua. e Luandri API is wri en using the LuaJIT FFI library.,null,null",null,null
78,"77,""Luandri API wraps Indri's query environment data types and methods by extern C functions. en using the LuaJIT's FFI library these C methods are exposed to any code wri en in Lua. Luandri automatically handles any conversions necessary between Lua tables and Indri's C++ objects, and vice versa. e """"Luandri.cpp"""" and """"luandri.lua"""" les contain all the wrapper logic on the C++ and the Lua side of our API code, respectively."",null,null",null,null
79,"78,""e current Luandri API exposes only some of the data structures and methods from Indri's query environment. In future, we hope to expose more of Indri's retrieval functionalities prioritizing based on the need of the broader research community."",null,null",null,null
80,"79,5 CONCLUSIONS,null,null",null,null
81,"80,""We introduced Luandri, a Lua API to the Indri search engine. Luandri brings to DNN models, implemented on Torch, the retrieval capabilities of Indri, including its powerful query language grammar. We posit that the capabilities of a search engine may be useful for training future DNN models for IR ­ for sampling negative examples, or for training under reinforcement or adversarial se ings. We hope that the release of Luandri will not only help researchers working on Torch models for IR, but also stimulate new research in novel DNN models that incorporate retrieval from an external knowledge base as an intermediate step towards solving larger tasks."",null,null",null,null
82,"81,REFERENCES,null,null",null,null
83,"82,""[1] Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Je rey Dean, Ma hieu Devin, and others. 2016. Tensor ow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016)."",null,null",null,null
84,"83,""[2] James Bergstra, Olivier Breuleux, Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. eano: A CPU and GPU math compiler in Python. In Proc. 9th Python in Science Conf. 1­7."",null,null",null,null
85,"84,""[3] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A exible and e cient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274 (2015)."",null,null",null,null
86,"85,""[4] Ronan Collobert, Koray Kavukcuoglu, and Cle´ment Farabet. 2011. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop."",null,null",null,null
87,"86,""[5] Nick Craswell, W Bruce Cro , Jiafeng Guo, Bhaskar Mitra, and Maarten de Rijke. 2016. Report on the SIGIR 2016 Workshop on Neural Information Retrieval (Neu-IR). 50, 2 (2016), 96­103."",null,null",null,null
88,"87,""[6] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with Locally-Trained Word Embeddings. arXiv preprint arXiv:1605.07891 (2016)."",null,null",null,null
89,"88,""[7] Marjan Ghazvininejad, Chris Brocke , Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2017. A Knowledge-Grounded Neural Conversation Model. arXiv preprint arXiv:1702.01932 (2017)."",null,null",null,null
90,"89,8h p://luajit.org 9h p://luajit.org/ext .html,null,null",null,null
91,"90,""[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. 2672­2680."",null,null",null,null
92,"91,""[9] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In Proc. CIKM. ACM, 55­64."",null,null",null,null
93,"92,""[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Proc. NIPS. 2042­2050."",null,null",null,null
94,"93,""[11] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proc. CIKM. ACM, 2333­2338."",null,null",null,null
95,"94,""[12] Roberto Ierusalimschy, Luiz Henrique De Figueiredo, and Waldemar Celes Filho. 1996. Lua-an extensible extension language. So w., Pract. Exper. 26, 6 (1996), 635­652."",null,null",null,null
96,"95,""[13] Yangqing Jia, Evan Shelhamer, Je Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Ca e: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia. ACM, 675­678."",null,null",null,null
97,"96,[14] Zhengdong Lu and Hang Li. 2013. A deep architecture for matching short texts. In Advances in Neural Information Processing Systems. 1367­1375.,null,null",null,null
98,"97,""[15] Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li. 2015. Multimodal convolutional neural networks for matching image and sentence. In Proceedings of the IEEE International Conference on Computer Vision. 2623­2631."",null,null",null,null
99,"98,""[16] Michael McCandless, Erik Hatcher, and Otis Gospodnetic. 2010. Lucene in Action: Covers Apache Lucene 3.0. Manning Publications Co."",null,null",null,null
100,"99,""[17] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In Proc. WWW. 1291­1299."",null,null",null,null
101,"100,""[18] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Christina Lioma. 2006. Terrier: A high performance and scalable information retrieval platform. In Proceedings of the OSIR Workshop. 18­25."",null,null",null,null
102,"101,""[19] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching as Image Recognition. In Proc. AAAI."",null,null",null,null
103,"102,[20] Andreas Schreiber. 2009. Mixing Python and Java. (2009). [21] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to rank short text,null,null",null,null
104,"103,""pairs with convolutional deep neural networks. In Proc. SIGIR. ACM, 373­382. [22] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014."",null,null",null,null
105,"104,""A latent semantic model with convolutional-pooling structure for information retrieval. In Proc. CIKM. ACM, 101­110. [23] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016. Benchmarking State-of-the-Art Deep Learning So ware Tools. arXiv preprint arXiv:1608.07249 (2016). [24] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Cro . 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. Citeseer, 2­6. [25] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for deep learning. In Proceedings of"",null,null",null,null
106,"105,""workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS). [26] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. 2017. Pyndri: a Python Interface to the Indri Search Engine. arXiv preprint arXiv:1701.00749 (2017). [27] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229­256. [28] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural networks for retrieval-based human-computer conversation system. In"",null,null",null,null
107,"106,""Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 55­64. [29] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, and others. 2014. An introduction to computational networks and the computational network toolkit. Technical Report. Tech. Rep. MSR, Microso Research, 2014, h p://codebox/cntk. [30] Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao, R Yan, D Yu, Xuan Liu, and H Tian. 2016. Multi-view response selection for human-computer conversation. EMNLP 16 (2016)."",null,null",null,null
108,"107,1223,null,null",null,null
109,"108,,null,null",null,null

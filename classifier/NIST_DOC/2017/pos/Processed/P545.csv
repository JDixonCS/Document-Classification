,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Learning to Diversify Search Results via Subtopic Attention,null,null",null,null
4,"3,""Zhengbao Jiang1,2, Ji-Rong Wen1,2,3, Zhicheng Dou1,2, Wayne Xin Zhao1,2, Jian-Yun Nie4, Ming Yue1,2"",null,null",null,null
5,"4,""1School of Information, Renmin University of China 2Beijing Key Laboratory of Big Data Management and Analysis Methods, China 3Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China"",null,null",null,null
6,"5,""4DIRO, Universite´ de Montre´al, Que´bec"",null,null",null,null
7,"6,""rucjzb@163.com, jirong.wen@gmail.com, dou@ruc.edu.cn,"",null,null",null,null
8,"7,""batmanfly@gmail.com, nie@iro.umontreal.ca, yomin@ruc.edu.cn"",null,null",null,null
9,"8,ABSTRACT,null,null",null,null
10,"9,""Search result diversification aims to retrieve diverse results to satisfy as many different information needs as possible. Supervised methods have been proposed recently to learn ranking functions and they have been shown to produce superior results to unsupervised methods. However, these methods use implicit approaches based on the principle of Maximal Marginal Relevance (MMR). In this paper, we propose a learning framework for explicit result diversification where subtopics are explicitly modeled. Based on the information contained in the sequence of selected documents, we use attention mechanism to capture the subtopics to be focused on while selecting the next document, which naturally fits our task of document selection for diversification. The framework is implemented using recurrent neural networks and max-pooling which combine distributed representations and traditional relevance features. Our experiments show that the proposed method significantly outperforms all the existing methods."",null,null",null,null
11,"10,KEYWORDS,null,null",null,null
12,"11,search result diversification; subtopics; attention,null,null",null,null
13,"12,1 INTRODUCTION,null,null",null,null
14,"13,""In real search scenario, queries issued by users are usually ambiguous or multi-faceted. In addition to being relevant to the query, the retrieved documents are expected to be as diverse as possible in order to cover different information needs. For example, when users issue """"apple"""", the underlying intents could be the IT company or the fruit. The retrieved documents should cover both topics to increase the chance to satisfy users with different information needs."",null,null",null,null
15,"14,""Traditional approaches to search result diversification are usually unsupervised and adopt manually defined functions with empirically tuned parameters. Depending on whether the underlying intents (or subtopics) are explicitly modeled, they can be categorized into implicit and explicit approaches [28]. Implicit approaches [6] do not model intents explicitly. They emphasize novelty,"",null,null",null,null
16,"15,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080805"",null,null",null,null
17,"16,""i.e. the following document should be """"different"""" from the former ones based on some similarity measures. Instead, explicit approaches [1, 12, 13, 16, 27, 35] model intents (or subtopics) explicitly. They aim to improve intent coverage, i.e. the following document should cover the intents not satisfied by previous ones. Intents or subtopics can be determined by techniques such as query reformulation [2, 14, 34, 38] and query clustering based on query logs and other types of information. Existing studies showed that explicit approaches have better performance [12, 13, 16, 27, 35] than implicit approaches due to several reasons: on the one hand, they provide a more natural way to handle subtopics than implicit approaches; on the other hand, their ranking functions are closer to the diversity evaluation metrics which are mostly based on explicit subtopics. Furthermore, most similarity measures used in the implicit approaches, e.g., those based on language model or vector space model, are determined globally on the whole documents, regardless of possible search intents. This might be problematic for search result diversification: two documents could contain similar words and considered globally similar, but this similar part may be unrelated to underlying search intents."",null,null",null,null
18,"17,""To avoid heuristic and handcrafted functions and parameters, a new family of research work using supervised learning is proposed. They try to learn a ranking function automatically. Their major focus lies in the modeling of diversity, including structural prediction [36], rewarding functions for novel contents [39], measurebased direct optimization [32], and neural network based method [33]. Regardless of diversity modeling and optimization methods, all these solutions inherit the spirit of MMR which is an implicit approach and do not take intents into consideration. Although the learning methods may result in a better similarity measure, they are hindered by the gap between reducing document redundancy and improving intent coverage. They suffer from similar problems with implicit unsupervised approaches. Without modeling subtopics explicitly, they can't directly improve intent coverage. Hence, there is a need to incorporate explicit subtopic modeling into supervised diversification methods."",null,null",null,null
19,"18,""To address the above issue, we propose to model subtopics in a general supervised learning framework. Our framework combines the strengths of both explicit unsupervised approaches and (implicit) supervised approaches. First, subtopics are explicitly modeled, allowing us to improve intent coverage in a proactive way. Second, it automatically learns the diversification ranking function, and is able to capture complex interaction among documents and"",null,null",null,null
20,"19,545,null,null",null,null
21,"20,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
22,"21,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
23,"22,Table 1: Subtopic relevance example.,null,null",null,null
24,"23,doc\subtopic i1 i2 i3,null,null",null,null
25,"24,d1 d2 d3,null,null",null,null
26,"25,× × × ×,null,null",null,null
27,"26,d4,null,null",null,null
28,"27,×,null,null",null,null
29,"28,×,null,null",null,null
30,"29,""subtopics. We call this framework Document Sequence with Subtopic Attention (DSSA). More specifically, to select the next document, we first model the sequence of selected documents in order to capture their contents as well as their relationship with the subtopics. Then based on the information contained by previous documents, attention mechanism is used to determine the undercovered subtopics to which we have to pay attention in selecting the next document. Attention mechanism has been successfully used to deal with various problems in image understanding [24] and NLP [3, 21]. This mechanism corresponds well to the document selection problem in search result diversification: attention on subtopics changes along with the addition of a document in the result list. For example. Assume that we have 3 subtopics and 4 documents whose relevance judgments are shown in Table 1. Given that we have selected d1 and d2, which cover subtopics i1 and i2, the attention for next choice should incline to i3 which is not covered, thus d3 is a better choice than d4 at this position. We will show that the DSSA framework is general enough to cover the ideas of previous unsupervised explicit methods."",null,null",null,null
31,"30,""We then propose a specific implementation of DSSA using recurrent neural networks (RNN) and max-pooling to leverage both distributed representations and traditional relevance features, which we call DSSA-RNNMP. Experimental results on TREC Web Track data show that our method outperforms the existing methods significantly. To our knowledge, this is the first time that a supervised learning framework with attention mechanism is used to model subtopics explicitly for search result diversification."",null,null",null,null
32,"31,2 RELATED WORK,null,null",null,null
33,"32,2.1 Implicit Diversification Approaches,null,null",null,null
34,"33,The basic assumption of implicit diversification approaches is that dissimilar documents are more likely to satisfy different information needs. The most representative approach is MMR [6]:,null,null",null,null
35,"34,""SMMR(q, d,"",null,null",null,null
36,"35,C),null,null",null,null
37,"36,"","",null,null",null,null
38,"37,(1,null,null",null,null
39,"38,-,null,null",null,null
40,"39,"")S rel (d ,"",null,null",null,null
41,"40,q),null,null",null,null
42,"41,-,null,null",null,null
43,"42,max,null,null",null,null
44,"43,dj C,null,null",null,null
45,"44,""S div (d ,"",null,null",null,null
46,"45,dj,null,null",null,null
47,"46,""),"",null,null",null,null
48,"47,(1),null,null",null,null
49,"48,""where Srel and Sdiv model document d's relevance to the query q and its similarity to a selected documents dj respectively. To gain high ranking score, a document should not only be relevant, but also be dissimilar from the selected documents. The definition of measures for relevance and document similarity is crucial, which is done manually in this approach."",null,null",null,null
50,"49,""Recently, machine learning methods have been leveraged to learn score functions. Yue and Joachims [36] proposed SVM-DIV which uses structural SVM to learn to identify a document subset with maximum word coverage. However, word coverage may be different from intent coverage. Optimizing the former may not necessarily lead to optimizing the latter. Similar to MMR, Zhu et"",null,null",null,null
51,"50,Table 2: Categorization of diversification approaches.,null,null",null,null
52,"51,unsupervised,null,null",null,null
53,"52,supervised,null,null",null,null
54,"53,implicit MMR,null,null",null,null
55,"54,""SVM-DIV, R-LTR, PAMM, NTN"",null,null",null,null
56,"55,explicit,null,null",null,null
57,"56,""IA-Select, xQuAD, PM2, TxQuAD, TPM2, HxQuAD, HPM2, 0-1 MSKP"",null,null",null,null
58,"57,DSSA (our approach),null,null",null,null
59,"58,""al. [39] proposed relational learning-to-rank model (R-LTR) which learns to score a document based on both relevance and novelty automatically, in order to maximize the probability of optimal rankings. Based on R-LTR score function, Xia et al. [32] proposed a perceptron algorithm using measures as margins (PAMM) to directly optimize evaluation metrics by enlarging the score margin of positive and negative rankings. They further proposed to use a neural tensor network (NTN) [33] to measure document similarity automatically from document representations, which avoids the burden to define handcrafted diversity features."",null,null",null,null
60,"59,""The above supervised approaches are shown to outperform the unsupervised counterparts. However, they are all implicit approaches without using subtopics. In this paper, we propose a learningbased explicit approach which models subtopics explicitly."",null,null",null,null
61,"60,2.2 Explicit Diversification Approaches,null,null",null,null
62,"61,""Explicit approaches model subtopics underlying a query, aiming at returning documents covering as many subtopics as possible. These approaches leverage external resources to explicitly represent information needs in subtopics. IA-Select [1] uses classified topical categories based on ODP taxonomy. xQuAD [27] is a probabilistic framework that uses query reformulations as intent representations. PM2 [13] tackles search result diversification problem from the perspective of proportionality. TxQuAD and TPM2 [12] represent intents by terms and transform intent coverage to term coverage. Hu et al. [16] proposed to use a hierarchical structure for subtopics instead of a flat list, which copes with the inherent interaction among subtopics. Two specific models, namely HxQuAD and HPM2, were proposed using hierarchical structure. Yu et al. [35] formulated diversification task as a 0-1 multiple subtopic knapsacks (0-1 MSKP) problem where documents are chosen like filling up multiple subtopic knapsacks. To tackle this NP-hard problem, max-sum belief propagation is used."",null,null",null,null
63,"62,""As summarized in Table 2, all existing explicit approaches are unsupervised and the functions and parameters are defined heuristically. In this paper, we use supervised learning to model the interaction among documents and subtopics simultaneously."",null,null",null,null
64,"63,2.3 RNN with Attention Mechanism,null,null",null,null
65,"64,""RNN can capture the interdependency between elements in a sequence. Attention mechanism, which is usually built on RNN, mimics human attention behavior focusing on different local region of the object (an image, a sentence, etc) at different times. In computer vision, Google DeepMind [24] used RNN with attention to extract information from an image by adaptively selecting a sequence of the most informative regions instead of the whole image. In NLP, attention mechanism is typically used in neural machine translation (NMT). Traditional encoder-decoder models encode the source"",null,null",null,null
66,"65,546,null,null",null,null
67,"66,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
68,"67,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
69,"68,""sentence into a fixed-length vector from which the target sentence is decoded. Such fixed-length vector may not be powerful enough to reflect all the information of the source sentence. An attentionbased model [3] was proposed to automatically pay unequal and varied attention to source words during decoding process. In particular, to decide the next target word, not only the fixed-length vector, but also the hidden states corresponding to source words relevant to the target word are used. Luong et al. [21] generalized the idea and proposed two classes of attention mechanism, namely global and local approaches. In this paper, attention mechanism is used on subtopics, which guides the model to emphasize different intents at different positions."",null,null",null,null
70,"69,""In the following section, we will first propose a general framework, then instantiate it with a specific implementation."",null,null",null,null
71,"70,3 DOCUMENT SEQUENCE WITH SUBTOPIC ATTENTION FRAMEWORK,null,null",null,null
72,"71,""Given a query set Q, a document set Dq and a subtopic set Iq for"",null,null",null,null
73,"72,""each query q  Q, the goal of explicit methods is to learn a ranking"",null,null",null,null
74,"73,""function f (q, Dq, Iq ) which is expected to output a ranking of do-"",null,null",null,null
75,"74,cuments in Dq that is both relevant and diverse. The loss function,null,null",null,null
76,"75,could be written in the following general form:,null,null",null,null
77,"76,""L(f (q, Dq , Iq ), Yq ),"",null,null",null,null
78,"77,(2),null,null",null,null
79,"78,qQ,null,null",null,null
80,"79,""where L measures the quality gap between the ranking outputted by f and the best ranking Yq . Different from traditional retrieval tasks, diversity has to be considered in the ranking and evaluation"",null,null",null,null
81,"80,""process. Theoretically, diversity ranking is NP-hard [1, 7]. Hence,"",null,null",null,null
82,"81,""a common strategy is to make greedy selections [6, 27]: at the t-th position, we assume that t - 1 documents have been selected and formed a document sequence Ct-1. The task is to select a locally optimal document dt from the remaining candidate documents based on a score function S(q, dt , Ct-1, Iq ). Note that implicit supervised methods correspond to the case where Iq is an empty set."",null,null",null,null
83,"82,""To motivate our approach, we start with the ideas of the unsu-"",null,null",null,null
84,"83,""pervised explicit approaches, which can be formulated as the fol-"",null,null",null,null
85,"84,lowing general form:,null,null",null,null
86,"85,""Sunsupervised(q, dt , Ct -1, Iq ) ,"",null,null",null,null
87,"86,(1,null,null",null,null
88,"87,- )S ,null,null",null,null
89,"88,rel(dt,null,null",null,null
90,"89,"","",null,null",null,null
91,"90,q)+,null,null",null,null
92,"91, relevance,null,null",null,null
93,"92,""Sdiv(dt , ik ) A(Ct -1, Iq )k ,  diversity"",null,null",null,null
94,"93,(3),null,null",null,null
95,"94,ik Iq,null,null",null,null
96,"95,subtopic weights,null,null",null,null
97,"96,where ik  Iq is the k-th subtopic of q and Srel and Sdiv calculate document dt 's relevance to a query and to a subtopic respectively.,null,null",null,null
98,"97,The essence of diversity lies in the function A which calculates the,null,null",null,null
99,"98,""weights for subtopics Iq based Ct -1. For xQuAD, A(Ct -1, Iq )k"",null,null",null,null
100,"99,"",onPp(irkev|qi)ousddj oCcut-m1 (e1nt-sPe(qduje|inkc)e)"",null,null",null,null
101,"100,""where P(ik |q) is the initial importance of subtopic ik , P(dj |ik ) is"",null,null",null,null
102,"101,the probability that dj is relevant to ik . The weight of a subtopic,null,null",null,null
103,"102,is determined by the likelihood that previous documents are not,null,null",null,null
104,"103,relevant to this subtopic. PM2 mimics seats allocation of compe-,null,null",null,null
105,"104,""ting political parties to adjust subtopic weights after each selection,"",null,null",null,null
106,"105,""i.e. A(Ct-1, Iq ) is estimated according to the difference between the subtopic's distributions in Ct-1 and in Iq . All these methods"",null,null",null,null
107,"106,sequence of selected,null,null",null,null
108,"107,documents d1 d2 d3,null,null",null,null
109,"108,candidate documents d4 d5 d6,null,null",null,null
110,"109,hidden state,null,null",null,null
111,"110,document sequence representation,null,null",null,null
112,"111,subtopic attention,null,null",null,null
113,"112,subtopic attention distribution,null,null",null,null
114,"113,diversity scoring,null,null",null,null
115,"114,score,null,null",null,null
116,"115,relevance scoring,null,null",null,null
117,"116,subtopics i1 i2,null,null",null,null
118,"117,query q,null,null",null,null
119,"118,Figure 1: Illustration of DSSA framework.,null,null",null,null
120,"119,Table 3: Notations in DSSA.,null,null",null,null
121,"120,Notation Definition,null,null",null,null
122,"121,""r , dt a ranking, the t-th document."",null,null",null,null
123,"122,""q, ik"",null,null",null,null
124,"123,""the query, the k-th subtopic."",null,null",null,null
125,"124,vdt,null,null",null,null
126,"125,representation of the document at the t-th position.,null,null",null,null
127,"126,vq,null,null",null,null
128,"127,representation of the query.,null,null",null,null
129,"128,vik,null,null",null,null
130,"129,representation of the k-th subtopic.,null,null",null,null
131,"130,ht,null,null",null,null
132,"131,hidden state of previous t documents.,null,null",null,null
133,"132,""at,k"",null,null",null,null
134,"133,""attkKe,""""n1tiaotn,k"""""",null,null",null,null
135,"134,""on ,"",null,null",null,null
136,"135,""the k-th 1, at,k "",null,null",null,null
137,"136,""subtopic at the t-th position. [0, 1] where K is the number"",null,null",null,null
138,"137,of subtopics. A large value means that this subtopic,null,null",null,null
139,"138,is less satisfied by previous t - 1 documents and thus,null,null",null,null
140,"139,needs more attention at the t-th position.,null,null",null,null
141,"140,sdt,null,null",null,null
142,"141,the final score of the document at the t-th position.,null,null",null,null
143,"142,""don't model the selected documents as a sequence. In addition, the functions and parameters are heuristically defined, which may not best fit the final goal."",null,null",null,null
144,"143,""To tackle the above problems, we extend Equation (3) to the following general learning framework:"",null,null",null,null
145,"144,""SDSSA(q, dt , Ct -1, Iq ) , sdt ,"",null,null",null,null
146,"145,(1,null,null",null,null
147,"146,-,null,null",null,null
148,"147,)Srel (,null,null",null,null
149,"148,(vdt,null,null",null,null
150,"149,"","",null,null",null,null
151,"150,vq,null,null",null,null
152,"151,)+ (,null,null",null,null
153,"152,)),null,null",null,null
154,"153,""Sdiv vdt , vi(·) , A H ([vd1 , ..., vdt-1 ]), vi(·) ,"",null,null",null,null
155,"154, relevance  diversity,null,null",null,null
156,"155,subtopic attention,null,null",null,null
157,"156,""(4) where documents, queries, and subtopics are denoted by their representations, as explained in Table 3. In this paper, we focus on learning a ranking function only and assume that these representations are given and will not be modified. There are three main components, namely (1) document sequence representation component H , (2) subtopic attention component A, and (3) scoring component Srel and Sdiv, which are also illustrated in Figure 1. This framework is inspired from the attention models"",null,null",null,null
158,"157,547,null,null",null,null
159,"158,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
160,"159,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
161,"160,""used in image understanding [24] and neural machine translation [3, 21], however adapted to our diversification task."",null,null",null,null
162,"161,""Next, we briefly describe the three components. The document sequence representation component H encodes the information contained in document sequence Ct-1 into a fixed-length hidden state ht-1, which could consider the interaction and dependency among these documents. ht-1 could be viewed as a comprehensive and high-level representation of Ct-1. The subtopic attention at,(·) is calculated by the subtopic attention component A using ht-1 and subtopic representations vi(·) . The attention evolves from the first to the last ranking position, driving the model to emphasize different subtopics based on previous document sequence. Finally, the scoring components Srel and Sdiv calculate relevance and diversity scores respectively. Notice that Sdiv is not limited to be a weighted sum over all subtopics as Equation (3). It can incorporate more complex interaction among subtopics."",null,null",null,null
163,"162,""The essence of this framework can be summarized as follows. Along with the selection of more documents, we encode the information of previous document sequence, and the attention mechanism will monitor the degree of satisfaction for each subtopic. High scores are assigned to the documents relevant to less covered subtopics. Finally, multiple subtopics would be well covered by adaptively learning the attention. In this way, our framework builds an intuitive approach to explicitly model subtopics. We name the framework Document Sequence with Subtopic Attention (DSSA). DSSA is a unified architecture that takes both relevance and diversity into consideration, and diversity is achieved by modeling the interaction among documents and subtopics."",null,null",null,null
164,"163,4 RESULT DIVERSIFICATION USING DSSA,null,null",null,null
165,"164,""In this section, we instantiate DSSA to a concrete form and articulate the training and prediction algorithms. The main idea of DSSA is to dynamically capture accumulative relevance information of previous document sequence, so as to calculate subtopic attention. Inspired by the recent progress on sequence data modeling, we adapt RNN to capture the information of previous document sequence based on distributed representations of documents. However, the effectiveness of distributed representation heavily depends on a large amount of training data. Typically, the representation is built automatically using the data to optimize an objective function [17]. We do not have such large data and we can only use unsupervised methods (e.g. doc2vec) to create representation, of which the effectiveness could be suboptimal. Indeed, our preliminary experiments using only the distributed representation created by unsupervised methods yield low effectiveness. To compensate this weakness, we also use traditional relevance features such as BM25 score, which are proven useful, to calculate subtopic attention and final score. Such a combination of distributed representations and features has been used in several previous works [29, 33]. In addition to RNN, we also adopt the way using max-pooling [33], which has been shown effective, to implement subtopic attention mechanism. We call this model DSSA-RNNMP (DSSA model using RNN and Max-Pooling), as illustrated in Figure 2. In addition, we also propose a list-pairwise approach for optimization, which is different from the existing studies."",null,null",null,null
166,"165,query/subtopic representation,null,null",null,null
167,"166,document representation,null,null",null,null
168,"167,max-pooling signals,null,null",null,null
169,"168,hidden state,null,null",null,null
170,"169,RNN,null,null",null,null
171,"170,h1,null,null",null,null
172,"171,ht -1,null,null",null,null
173,"172,ed1,null,null",null,null
174,"173,edt-1,null,null",null,null
175,"174,max-pooling,null,null",null,null
176,"175,""at ,(·)"",null,null",null,null
177,"176,subtopic attention,null,null",null,null
178,"177,edt,null,null",null,null
179,"178,s div dt,null,null",null,null
180,"179,e i1 ei2 eq,null,null",null,null
181,"180,s rel dt,null,null",null,null
182,"181,diversity score,null,null",null,null
183,"182,relevance score,null,null",null,null
184,"183,""xd1 ,i1 xd1 ,i2"",null,null",null,null
185,"184,""xd1 , q"",null,null",null,null
186,"185,""xdt-1 ,i1 xdt-1 ,i2"",null,null",null,null
187,"186,""xdt-1 ,q"",null,null",null,null
188,"187,""xdt ,i1 xdt ,i2 xdt ,q"",null,null",null,null
189,"188,Figure 2: Architecture of DSSA-RNNMP. Previous t - 1 do-,null,null",null,null
190,"189,""cuments are encoded into ht-1 from distributed representations ed1 , ..., edt-1 . Attention on the k-th subtopic at,k is then calculated based on (1) hidden state ht-1 and subto-"",null,null",null,null
191,"190,""pic representation eik (2) max-pooling on relevance features xd1,ik , ..., xdt-1,ik ."",null,null",null,null
192,"191,Table 4: Parameters in DSSA-RNNMP.,null,null",null,null
193,"192,""Notation W n, bn W a, wp W s, wr"",null,null",null,null
194,"193,Definition parameters of RNN with vanilla cell. parameters used in subtopic attention. parameters used in scoring.,null,null",null,null
195,"194,4.1 A Neural Network Implementation,null,null",null,null
196,"195,""We first describe the constitution of representations, namely vdt , vq , and vik , then elaborate how we implement document sequence representation, subtopic attention, and scoring com-"",null,null",null,null
197,"196,ponents. The parameters to be learned are listed in Table 4. vdt : the representation of a document is composed of two parts:,null,null",null,null
198,"197,distributed representations and relevance features. Distributed re-,null,null",null,null
199,"198,""presentation can be constructed in different ways. In this paper,"",null,null",null,null
200,"199,""we consider three methods: SVD, LDA [4], and doc2vec [18]. Rele-"",null,null",null,null
201,"200,""vance features are those used in traditional IR, such as BM25 score"",null,null",null,null
202,"201,""etc. Suppose that we have a distributed representation of size Ed , K subtopics, and R relevance features, the total size of vdt would be Ed + R + KR. We use edt  REd , xdt,q and xdt,ik  RR to denote distributed representation, relevance features for a query and"",null,null",null,null
203,"202,""a subtopic respectively. vq , vik : we first retrieve top Z documents using some basic re-"",null,null",null,null
204,"203,trieval model (such as BM25). These documents are concatenated,null,null",null,null
205,"204,""as a pseudo document, then similar to edt , a distributed representation of size Eq is generated. For consistency, we also use eq and eik  REq to represent these representations."",null,null",null,null
206,"205,548,null,null",null,null
207,"206,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
208,"207,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
209,"208,""4.1.1 Document Sequence Representation. H is instantiated using RNN to encode the information of previous document sequence. Several types of RNN cell can be used, ranging from the simple vanilla cell, GRU cell [9], to LSTM cell [15]. For simplicity, we only show the vanilla cell here. At the t-th position, we derive the (accumulative) document sequence representation as follows:"",null,null",null,null
210,"209,""ht ,"""" tanh(W n [ht -1; edt ] + bn ),"""""",null,null",null,null
211,"210,(5),null,null",null,null
212,"211,""where W n  RU ×(U +Ed ) (U is the size of the hidden state), bn  RU and [; ] is a concatenation. The cell transforms previous hidden"",null,null",null,null
213,"212,""layer ht -1 and another space,"",null,null",null,null
214,"213,current where a,null,null",null,null
215,"214,document distributed bias bn is added and a,null,null",null,null
216,"215,representation edt to non-linear activation,null,null",null,null
217,"216,""(i.e. tanh) then happens, producing the next hidden layer ht . h0"",null,null",null,null
218,"217,is initialized as a vector of zeros. The vanilla cell can be easily,null,null",null,null
219,"218,""replaced by GRU and LSTM cells, whose results will be report in"",null,null",null,null
220,"219,Section 6.2.,null,null",null,null
221,"220,""4.1.2 Subtopic Attention. By looking at ht-1 which stores the information of previous t - 1 documents and ei(·) which represents the meaning of each subtopic, we are capable of discovering which"",null,null",null,null
222,"221,""intents are not satisfied and thus need to be emphasized at the tth position. To capture this idea, we use A(ht -1, eik ) to measure the (unnormalized) importance of the k-th subtopic at the t-th po-"",null,null",null,null
223,"222,""sition, which could be implemented in many ways. We consider"",null,null",null,null
224,"223,the following two ways similar to [21]:,null,null",null,null
225,"224,{,null,null",null,null
226,"225,""A (ht -1, eik ) ,"",null,null",null,null
227,"226,""ht -1W aeik , -ht -1 · eik ,"",null,null",null,null
228,"227,(general) (dot),null,null",null,null
229,"228,(6),null,null",null,null
230,"229,""where W a  RU ×Eq . The """"general"""" operation uses bilinear tensor product to relate two vectors multiplicatively through its nonlinearity [30]. The """"dot"""" product requires both vectors to be in the same space. Similar ht-1 and eik mean that previous documents are likely to satisfy this subtopic, and thus a lower attention score will be attributed to it. The above way mainly relies on distributed representations, which may not always be effective, especially under limited data."",null,null",null,null
231,"230,""Hence, we further leverage relevance features to enhance the subtopic attention. xdt,ik directly reflects the degree of satisfaction for a subtopic-document pair and is combined linearly using wp to form an explicit signal. To derive the accumulative information of the document sequence, we adopt commonly used max-pooling to select the most significant signal from previous documents:"",null,null",null,null
232,"231,""A (xd1,ik , ..., xdt-1,ik ) ,"""" max([xd1,ik · wp , ..., xdt-1,ik · wp ]), (7)"""""",null,null",null,null
233,"232,""where A(xd1,ik , ..., xdt-1,ik ) measures the degree of satisfaction of the k-th subtopic based on relevance features through max-pooling. Lower value indicates that the previous documents are more likely to be relevant to this subtopic. Note that if we view the signals produced by max-pooling (i.e. the vectors in """"max-pooling"""" section of Figure 2) as a part of the general hidden states, our concrete implementation fit in DSSA framework."",null,null",null,null
234,"233,We adopt an addictive way to integrate both parts and then use softmax to produce (normalized) attention distribution:,null,null",null,null
235,"234,""at,k"",null,null",null,null
236,"235,"","",null,null",null,null
237,"236,""A (ht -1, eik ) +"",null,null",null,null
238,"237,""A (xd1,ik , ..., xdt-1,ik ),"",null,null",null,null
239,"238,""at,k , Kjw,""""i1kweixj pe(xapt(,kat), j )"""""",null,null",null,null
240,"239,""(wij  0, j)."",null,null",null,null
241,"240,(8),null,null",null,null
242,"241,""softmax is modified to include the initial subtopic importance wik , which encodes our intuition that an important subtopic is more"",null,null",null,null
243,"242,likely to gain attention than unimportant ones.,null,null",null,null
244,"243,""4.1.3 Scoring. The final score consists of relevance score sdretl and diversity score sddtiv, which are combined by a coefficient :"",null,null",null,null
245,"244,""sdt , (1 - )sdretl + sddtiv (0    1)."",null,null",null,null
246,"245,(9),null,null",null,null
247,"246,The relevance score and diversity score are calculated as follows:,null,null",null,null
248,"247,""sdretl ,"""" S (edt , eq ) + xdt ,q · wr ,"""""",null,null",null,null
249,"248,sddtiv,null,null",null,null
250,"249,"","",null,null",null,null
251,"250,""at , ( ·)"",null,null",null,null
252,"251,·,null,null",null,null
253,"252,""SS((eeddtt,,eeiiK1"",null,null",null,null
254,"253,) ),null,null",null,null
255,"254,+ ... +,null,null",null,null
256,"255,""xdt,i1 · wr xdt,iK · wr"",null,null",null,null
257,"256,"","",null,null",null,null
258,"257,(10),null,null",null,null
259,"258,""where wr  RR and at,(·) is the attention derived from subtopic attention component. The diversity score is calculated as a weighted"",null,null",null,null
260,"259,combination of the document's relevance to each subtopic by atten-,null,null",null,null
261,"260,tion distribution. We use the same way to calculate document's re-,null,null",null,null
262,"261,levance to a query and to its subtopics using both distributional re-,null,null",null,null
263,"262,""presentations and relevance features, although different ways can"",null,null",null,null
264,"263,""be used. Specifically, dt 's relevance to a query q (or a subtopic ik ) is calculated based on both the similarity between two distributed xrtwedpto,rqerse(eponrretxastdeitno,tniakst)iS.oSn (seidantnt,deenwqd)rs(otliornSpear o(reldydutcc,oeemiakbm)i)naaetnscdhfeirnaeglteuvsrcaeonsr.ceSeibfmeeatiwltauerreetnos A, S could also be implemented as:"",null,null",null,null
265,"264,{,null,null",null,null
266,"265,S (edt,null,null",null,null
267,"266,"","",null,null",null,null
268,"267,eik,null,null",null,null
269,"268,),null,null",null,null
270,"269,"","",null,null",null,null
271,"270,""edt W seik , edt · eik ,"",null,null",null,null
272,"271,(general) (dot),null,null",null,null
273,"272,(11),null,null",null,null
274,"273,where W s  REd ×Eq . Then the score of a ranking r is calculated by summing up all the |r | documents' scores:,null,null",null,null
275,"274, |r |,null,null",null,null
276,"275,""sr , sdt ."",null,null",null,null
277,"276,(12),null,null",null,null
278,"277,""t ,1"",null,null",null,null
279,"278,""Vector interaction operations A and S could be implemented using more complex models, such as multilayer perceptron (MLP), to model the interaction between two vectors more accurately. We could also use convolutional neural network (CNN) instead of RNN to model the interaction among a sequence of documents and encode their information. We deliberately choose to use simple mechanisms in this implementation in order to show that the general framework is capable of capturing the essence of diversification even without complex operations. More complex implementations will be examined in future work."",null,null",null,null
280,"279,549,null,null",null,null
281,"280,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
282,"281,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
283,"282,Algorithm 1 A List-pairwise Approach For Optimization,null,null",null,null
284,"283,d 3,null,null",null,null
285,"284,d 1,null,null",null,null
286,"285,d 3,null,null",null,null
287,"286,d 2,null,null",null,null
288,"287,d 4,null,null",null,null
289,"288,rank1,null,null",null,null
290,"289,1: procedure List-pairwise Training,null,null",null,null
291,"290,""input: loss function L, learning rate r , epochs V , query set Q,"",null,null",null,null
292,"291,dd,null,null",null,null
293,"292,1,null,null",null,null
294,"293,2,null,null",null,null
295,"294,d 4,null,null",null,null
296,"295,d 1,null,null",null,null
297,"296,d 2,null,null",null,null
298,"297,d 4,null,null",null,null
299,"298,d rank2 3,null,null",null,null
300,"299,""document set D, evaluation metric M, random permutation count N output: DSSA with trained parameters  2: initialize "",null,null",null,null
301,"300,(a) list-pairwise,null,null",null,null
302,"301,(b) PAMM,null,null",null,null
303,"302,3: for i from 1 to V do,null,null",null,null
304,"303,4:,null,null",null,null
305,"304,""for batch b  GetSamples(Q, D, M, N ) do"",null,null",null,null
306,"305,Figure 3: Pair sample examples of (a) list-pairwise and (b),null,null",null,null
307,"306,5:,null,null",null,null
308,"307,""  GetGradient(L(b,  ))"",null,null",null,null
309,"308,PAMM. Both samples are positive.,null,null",null,null
310,"309,6:,null,null",null,null
311,"310,   - r,null,null",null,null
312,"311,return DSSA,null,null",null,null
313,"312,4.2 A List-pairwise Approach for Optimization,null,null",null,null
314,"313,""Liu [19] classifies LTR approaches into three categories: pointwise, pairwise, and listwise. Search result diversification is naturally a listwise problem because the score of a document depends on the previous documents. Take Table 1 as an example, under no previous documents, d2 is better than d3 because d2 covers one more subtopic (subtopics are of equal weight). However, given that we have selected d1, which is similar to d2 while dissimilar to d3, d3 becomes superior because it provides additional information."",null,null",null,null
315,"314,4.2.1 List-pairwise Training. We propose a list-pairwise training approach. We call it list-pairwise because a sample in our algo-,null,null",null,null
316,"315,7: procedure GetSamples,null,null",null,null
317,"316,""input: query set Q, document set Dq for each query q, evaluation metric M, random permutation count N"",null,null",null,null
318,"317,""output: a set of ranking pairs with weight and preference {(q(1), C(1), d1(1), d2(1), w(1), y(1)), (q(2), C(2), d1(2), d2(2), w(2), y(2)), ...} include: GetPerms(Dq, l, N , M) return a best ranking (under metric M) and N random permutations of length l. GetPairs(q, Dq, C, M) samples pairs of documents (d1, d2) from Dq \ C under context C if and only if they lead to different metric scores. Let r1  [C, d1], r2  [C, d2], w , |M(r1) - M(r2)| and y , M(r1) > M(r2) . 8: R  "",null,null",null,null
319,"318,""rithm consists of a pair of rankings (r1, r2): r1 and r2 are totally identical except the last document. The sample can be written as (C, d1, d2), where C is the shared previous document sequence."",null,null",null,null
320,"319,""The pairwise preference ground-truth is generated based on an evaluation metric M, such as -nDCG. If M(r1) > M(r2), it is positive,"",null,null",null,null
321,"320,9: for query q in Q do,null,null",null,null
322,"321,10:,null,null",null,null
323,"322,for l from 0 to |Dq | - 1 do,null,null",null,null
324,"323,11:,null,null",null,null
325,"324,""for perm C in GetPerms(Dq, l, N , M) do"",null,null",null,null
326,"325,12:,null,null",null,null
327,"326,""R  R  GetPairs(q, Dq, C, M)"",null,null",null,null
328,"327,return R,null,null",null,null
329,"328,""otherwise it is negative. Our approach is similar to pairwise approaches because it aims to compare a pair of documents, but this is done within some context. Similarly to pairwise, the loss function"",null,null",null,null
330,"329,""LPAMM ,"",null,null",null,null
331,"330,""P (rq+) - P (rq-)  M(rq+) - M(rq-) ,"",null,null",null,null
332,"331,""q  Q rq+,rq-"",null,null",null,null
333,"332,(16),null,null",null,null
334,"333,can be defined as binary classification logarithmic loss:,null,null",null,null
335,"334,""Llist-pairwise , "",null,null",null,null
336,"335,w (o ),null,null",null,null
337,"336,q  Q o  Oq,null,null",null,null
338,"337,( y(o),null,null",null,null
339,"338,log,null,null",null,null
340,"339,""( P (r1(o),"",null,null",null,null
341,"340,) r2(o)),null,null",null,null
342,"341,+,null,null",null,null
343,"342,(1,null,null",null,null
344,"343,-,null,null",null,null
345,"344,y(o)),null,null",null,null
346,"345,log,null,null",null,null
347,"346,( 1,null,null",null,null
348,"347,-,null,null",null,null
349,"348,"")) P (r1(o), r2(o)) ,"",null,null",null,null
350,"349,(13),null,null",null,null
351,"350,""where condition is 1 if the condition is satisfied, 0 otherwise, MLE maximizes the probability of positive rankings, and PAMM enlarges the probability margin between positive and negative rankings according to an evaluation metric. For MLE, the number of best rankings is usually small if we only have hundreds of que-"",null,null",null,null
352,"351,""where Oq is all the pair samples of query q, y(o) ,"""" 1 indicates positive and 0 for negative, and P(r1(o), r2(o)) is the probability of being"""""",null,null",null,null
353,"352,""ries, which may not be enough to train adequately the parameters. PAMM uses preferences between very different rankings that are not comparable (see Figure 3(b)). In contrast, list-pairwise method"",null,null",null,null
354,"353,positive,null,null",null,null
355,"354,calculated,null,null",null,null
356,"355,by,null,null",null,null
357,"356,1 1+exp(sr2(o),null,null",null,null
358,"357,-sr1(o,null,null",null,null
359,"358,),null,null",null,null
360,"359,),null,null",null,null
361,"360,.,null,null",null,null
362,"361,To,null,null",null,null
363,"362,enhance,null,null",null,null
364,"363,""effectiveness,"",null,null",null,null
365,"364,we,null,null",null,null
366,"365,weight,null,null",null,null
367,"366,pairs,null,null",null,null
368,"367,with,null,null",null,null
369,"368,w (o ),null,null",null,null
370,"369,"","",null,null",null,null
371,"370,|M,null,null",null,null
372,"371,(r,null,null",null,null
373,"372,(o) 1,null,null",null,null
374,"373,),null,null",null,null
375,"374,-,null,null",null,null
376,"375,M,null,null",null,null
377,"376,""(r2(o))|,"",null,null",null,null
378,"377,which,null,null",null,null
379,"378,means,null,null",null,null
380,"379,that,null,null",null,null
381,"380,only allows the last document to be different (Figure 3(a)). This corresponds better to the decision-making situation in which we have to choose a document under a given context. It is expected that,null,null",null,null
382,"381,""the bigger the metric score gap, the more important the pair."",null,null",null,null
383,"382,such a pair sample allows us to better train the ranking function.,null,null",null,null
384,"383,Because DSSA calculates document d's score sdC based on previ-,null,null",null,null
385,"384,Experiments will show that our approach works better.,null,null",null,null
386,"385,""ous document C, we could also use Maximum Likelihood Estima-"",null,null",null,null
387,"386,""As shown in Figure 2, our architecture is a unified neural net-"",null,null",null,null
388,"387,tion (MLE) or PAMM to optimize our model. We use Plackett-Luce,null,null",null,null
389,"388,""work and the attention function is continuous, so the gradient of"",null,null",null,null
390,"389,model [22] to estimate the probability of a ranking r :,null,null",null,null
391,"390,the loss function can be backpropagated directly to train the model.,null,null",null,null
392,"391,P(r ),null,null",null,null
393,"392,"","",null,null",null,null
394,"393, |r |,null,null",null,null
395,"394,""i ,1"",null,null",null,null
396,"395,exp(sdr i[:i-1]),null,null",null,null
397,"396,|r |,null,null",null,null
398,"397,""j ,i"",null,null",null,null
399,"398,exp(sdr [j:i-1],null,null",null,null
400,"399,),null,null",null,null
401,"400,"","",null,null",null,null
402,"401,(14),null,null",null,null
403,"402,where r [: i - 1] means the top i - 1 documents of ranking r . Then,null,null",null,null
404,"403,the loss functions could be w ritten as:,null,null",null,null
405,"404,""LMLE ,"""" - log(P (rq+)),"""""",null,null",null,null
406,"405,(15),null,null",null,null
407,"406,qQ,null,null",null,null
408,"407,""We use mini-batch gradient descent to facilitate training process. Unfortunately, it is impossible to acquire all the list-pairwise"",null,null",null,null
409,"408,""samples, which has in total |Dq |! (|Dq | is the number of candidate documents) different permutations. So we develop a sampling strategy similar to negative sampling [23] as described in Algorithm 1: for each query q, we sample a large number of pairs of rankings, whose length ranges from 1 to |Dq |. We first obtain some contexts C from both best rankings and randomly sampled negative"",null,null",null,null
410,"409,550,null,null",null,null
411,"410,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
412,"411,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
413,"412,""rankings (rankings that are not optimal). Then under each C, a pair of documents (d1, d2) are sampled from the remaining documents Dq \ C if and only if they lead to different metric scores."",null,null",null,null
414,"413,""4.2.2 Prediction. In prediction stage, for each query, we sequentially and greedily choose the document with the highest score and append it to the ranking list. Specifically, the first document is selected under initial subtopic importance from the whole candidate set Dq . Once the top t - 1 documents have been selected (i.e. |C| ,"""" t - 1), we feed each document in Dq \ C into DSSA at the t-th position one by one and choose the one with the highest sdt . This process continues until all the documents in Dq are ranked."""""",null,null",null,null
415,"414,""4.2.3 Time Complexities. The training time complexity with vanilla cell and """"general"""" operation is O(V · |Q| ·  · |Dq | · ) where V is the number of iterations, |Q| is the number of training queries,  ,"""" N · |Dq |2 is the number of sampled pairs where N is the number of random permutations, |Dq | is the number of candidate documents, and  is the complexity for each position:"""""",null,null",null,null
416,"415,"" ,"""" U (U + Ed ) + KU Eq + KR + KEd Eq + KR, (17)"""""",null,null",null,null
417,"416,document sequence representation,null,null",null,null
418,"417,subtopic attention,null,null",null,null
419,"418,scoring,null,null",null,null
420,"419,where the dominating terms are KU Eq and KEd Eq which are proportional to the number of subtopics K. How to efficiently handle,null,null",null,null
421,"420,""a large number of subtopics is our future work. The prediction complexity is O(|Dq |2) for each query. We can limit |Dq | to a small number (say 50), so the prediction time can be reasonable."",null,null",null,null
422,"421,5 EXPERIMENTAL SETTINGS,null,null",null,null
423,"422,5.1 Data Collections,null,null",null,null
424,"423,""We use the same dataset as [16] which consists of Web Track dataset from TREC 2009 to 2012. There are 198 queries (query #95 and #100 are dropped because no diversity judgments are made for them), each of which includes 3 to 8 subtopics identified by TREC assessors. The relevance rating is given in a binary form at subtopic level. All experiments are conducted on ClueWeb09 [5] collection."",null,null",null,null
425,"424,""We use query suggestions of Google search engine as subtopics, which are released by Hu et al. [16] on their website1. For DSSA, we only use the first level subtopics and leave the exploration of hierarchical subtopics to future work. Following the existing work [16], we simply use uniform weights for these subtopics."",null,null",null,null
426,"425,5.2 Evaluation Metrics,null,null",null,null
427,"426,""We use ERR-IA [8], -nDCG [10], and NRBP [11], which are official diversity evaluation metrics used in Web Track. They measure the diversity by explicitly rewarding novelty and penalizing redundancy. D-measures [26], the primary metric used in NTCIR Intent [25] and IMine task [20], is also included. In addition, we also use traditional diversity measures Precision-IA (denoted as Pre-IA) [1] and Subtopic Recall (denoted as S-rec) [37]. Consistent with existing works [32, 33, 39] and TREC Web Track, all these metrics are computed on top 20 results of a ranking. We use two-tailed paired t-test to conduct significance testing with p-value < 0.05."",null,null",null,null
428,"427,1hierarchical search result diversification: http://www.playbigdata.com/dou/hdiv,null,null",null,null
429,"428,""Table 5: Relevance features. Each of the first 3 features is applied to body, anchor, title, URL, and the whole documents."",null,null",null,null
430,"429,Name,null,null",null,null
431,"430,TF-IDF BM25 LMIR,null,null",null,null
432,"431,PageRank #inlinks #outlinks,null,null",null,null
433,"432,Description,null,null",null,null
434,"433,the TF-IDF model BM25 with default parameters LMIR with Dirichlet smoothing,null,null",null,null
435,"434,PageRank score number of inlinks number of outlinks,null,null",null,null
436,"435,#Features,null,null",null,null
437,"436,5 5 5,null,null",null,null
438,"437,1 1 1,null,null",null,null
439,"438,Table 6: Diversity features. Each feature is extracted over a pair of documents.,null,null",null,null
440,"439,Name,null,null",null,null
441,"440,subtopic diversity text diversity title diversity anchor text diversity link-based diversity URL-based diversity,null,null",null,null
442,"441,Description,null,null",null,null
443,"442,euclidean distance based on SVD cosine-based distance on term vector text diversity on title text diversity on anchor link similarity of document pair URL similarity of document pair,null,null",null,null
444,"443,5.3 Baseline Models,null,null",null,null
445,"444,""We compare DSSA2 to various unsupervised and supervised diversification methods. The non-diversified baseline is denoted as Lemur. We use xQuAD [27], PM2 [13], TxQuAD, TPM2 [12], HxQuAD, and HPM2 [16] as our unsupervised baselines. We use ListMLE [31], R-LTR [39], PAMM [32], and NTN [33] as our supervised baselines. Top 20 results of Lemur are used to train supervised methods. Top 50 (i.e. |Dq |) results of Lemur are used for diversity re-ranking. To construct the representation of a query or a subtopic, we use the top 20 (Z ) documents. We use 5-fold cross validation to tune the parameters in all experiments based on nDCG@20, which is one of the most widely used metrics. A brief introduction to these baselines is as follows:"",null,null",null,null
446,"445,Lemur. We use the same non-diversified results as [16] for fair comparison. They are produced by language model and retrieved using the Lemur service3 of which the spams are filtered. These results are released by Hu et al. [16] on the website1.,null,null",null,null
447,"446,ListMLE. ListMLE is a representative listwise LTR method without considering diversity.,null,null",null,null
448,"447,""xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2. These are competitive unsupervised explicit diversification methods, as introduced in Section 2.2. All these methods use  to control the importance of relevance and diversity. HxQuAD and HPM2 use an additional parameter  to control the weight of each layer of the hierarchical structure. Both  and  are tuned using cross validation. They all require a prior relevance function to fulfill diversification re-ranking. Following [39], we use ListMLE."",null,null",null,null
449,"448,""R-LTR, PAMM, and NTN. For PAMM, we use -nDCG@20 as the optimization metric. We optimize NTN based on both R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN respectively."",null,null",null,null
450,"449,2data and code available at: http://www.playbigdata.com/dou/DSSA/ 3Lemur service: http://boston.lti.cs.cmu.edu/Services/clueweb09 batch/,null,null",null,null
451,"450,551,null,null",null,null
452,"451,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
453,"452,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
454,"453,""To achieve optimal results, for R-LTR and PAMM, we tune the relational function hS (R) from minimal, maximal, and average. For PAMM, we tune the number of positive rankings  + and negative rankings  - per query. For NTN, the number of tensor slices is tuned from 1 to 10. LDA is used to generate distributed representations of size 100 for NTN and DSSA. For all these supervised methods, the learning rate r is tuned from 10-7 to 10-1. For DSSA, we have different settings possible. In our first set of results, we will use """"general"""" as the implementation of vector interaction operations A and S, LSTM with hidden size of 50 as the cell of RNN. We set random permutation count as 10 in list-pairwise sampling. Similarly,  of DSSA is tuned by cross validation. We also test the impact of different model settings and permutation counts on performance in Section 6.2 and Section 6.3 respectively."",null,null",null,null
455,"454,""Similar to [39], we implement 18 relevance features and 6 diversity features, as listed in Table 5 and 6 respectively. We collect the candidate and retrieved documents of all queries and subtopics to generate the distributed representations."",null,null",null,null
456,"455,6 EXPERIMENTAL RESULTS,null,null",null,null
457,"456,6.1 Overall Results,null,null",null,null
458,"457,""The overall results are shown in Table 7. We find that DSSA significantly outperforms all implicit and explicit baselines, including both unsupervised and supervised. The improvements are statistically significant (two-tailed paired t-test) for all metrics, except S-rec. The results clearly show the superiority of DSSA."",null,null",null,null
459,"458,""(1) DSSA vs. unsupervised explicit methods. DSSA outperforms unsupervised explicit methods (xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2) on all the measures. The relative improvement over HxQuAD and HPM2, the best unsupervised explicit approaches, is up to 8.3% and 8.6% respectively in terms of -nDCG. This comparison shows the great advantage of using supervised method for learning the ranking function."",null,null",null,null
460,"459,""(2) DSSA vs. supervised implicit methods. DSSA also outperforms supervised implicit methods (R-LTR, PAMM, R-LTRNTN, and PAMM-NTN) by quite large margins. The improvement over R-LTR-NTN and PAMM-NTN, the best supervised implicit approaches is up to 9.9% and 9.4% respectively on -nDCG. This result demonstrates the utility of taking into account subtopics explicitly in supervised approaches. The improvements are similar to those observed between explicit approaches and implicit approaches in unsupervised framework [12, 13, 16, 27]. The combination of the two observations suggests that explicit modeling of subtopics can improve result diversification, whether it is in a supervised or unsupervised framework."",null,null",null,null
461,"460,6.2 Effects of Different Settings,null,null",null,null
462,"461,""We conduct experiments with different settings of DSSA to investigate whether the performance is sensitive to these settings. Different aspects of settings are listed follow. For simplicity, when investigating the impact of each aspect, we keep other aspects the same as the settings specified in Section 5.3."",null,null",null,null
463,"462,""(1) Representation generation methods: SVD, LDA, and doc2vec with window size of 5."",null,null",null,null
464,"463,""(2) Implementation of vector interaction operations A and S: """"general"""" and """"dot""""."",null,null",null,null
465,"464,Table 7: Performance comparison of all methods. The best result is in bold. Statistically significant differences between DSSA and baselines are marked with various symbols. # indicates significant improvement over all baselines.,null,null",null,null
466,"465,Methods,null,null",null,null
467,"466,ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,null,null",null,null
468,"467,Lemur ListMLE,null,null",null,null
469,"468,.271 .369 .287 .387,null,null",null,null
470,"469,.232 .424 .249 .430,null,null",null,null
471,"470,.153 .621 .157 .619,null,null",null,null
472,"471,xQuAD .317 .413,null,null",null,null
473,"472,TxQuAD .308 .410,null,null",null,null
474,"473,HxQuAD .326 .421,null,null",null,null
475,"474,PM2,null,null",null,null
476,"475,.306 .411,null,null",null,null
477,"476,TPM2,null,null",null,null
478,"477,.291 .399,null,null",null,null
479,"478,HPM2,null,null",null,null
480,"479,.317 .420,null,null",null,null
481,"480,.284 .437 .272 .441 .294 .441 .267 .450 .250 .443 .279 .455,null,null",null,null
482,"481,.161 .622 .155 .634 .158 .629 .169 .643 .161 .639 .172 .645,null,null",null,null
483,"482,R-LTR,null,null",null,null
484,"483,.303,null,null",null,null
485,"484,PAMM,null,null",null,null
486,"485,.309,null,null",null,null
487,"486,R-LTR-NTN .312,null,null",null,null
488,"487,PAMM-NTN .311,null,null",null,null
489,"488,DSSA,null,null",null,null
490,"489,.356#,null,null",null,null
491,"490,.403 .411 .415 .417 .456#,null,null",null,null
492,"491,.267 .441 .271 .450 .275 .451 .272 .457 .326# .473#,null,null",null,null
493,"492,.164 .631 .168 .643 .166 .644 .170 .648 .185# .649,null,null",null,null
494,"493,Table 8: Effects of different settings.,null,null",null,null
495,"494,Methods,null,null",null,null
496,"495,ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,null,null",null,null
497,"496,SVD LDA doc2vec,null,null",null,null
498,"497,.348 .450 .315 .356 .456 .326 .351 .452 .318,null,null",null,null
499,"498,.470 .184 .646 .473 .185 .649 .471 .184 .646,null,null",null,null
500,"499,general dot,null,null",null,null
501,"500,.356 .456 .326 .347 .450 .314,null,null",null,null
502,"501,.473 .185 .649 .470 .184 .647,null,null",null,null
503,"502,vanilla GRU LSTM,null,null",null,null
504,"503,.354 .454 .322 .357 .457 .326 .356 .456 .326,null,null",null,null
505,"504,.471 .184 .649 .473 .185 .649 .473 .185 .649,null,null",null,null
506,"505,DSSA-RNN,null,null",null,null
507,"506,.342,null,null",null,null
508,"507,DSSA-RNNMP .356,null,null",null,null
509,"508,.445 .306 .456 .326,null,null",null,null
510,"509,.466 .172 .657 .473 .185 .649,null,null",null,null
511,"510,""(3) RNN cell: vanilla, GRU, and LSTM cell. (4) Dimensionality: we test several representative settings on"",null,null",null,null
512,"511,""the size of distributed representations Ed and Eq , the size of hidden state U as (25, 10), (50, 25), (100, 50), (200, 100). (5) Max-pooling: we experiment without using max-pooling (denoted as DSSA-RNN) in subtopic attention component."",null,null",null,null
513,"512,""The results are reported in Table 8. We can observe that DSSA does not heavily rely on specific settings. As for different representation generation methods, LDA has slightly better results. doc2vec could have been more appropriate if we had large datasets with more queries. The """"general"""" operation yields slightly better results. A possible reason is that it is bilinear and thus is more powerful than """"dot"""" to model the interaction. GRU and LSTM cells yield slightly better results than vanilla cell because of their ability of modeling long-term dependency. The difference is however small. This may be due to that with a limited number of training data, a model is unable to take advantage of its higher complexity to capture the fine-grained subtlety. Results with different size of distributed representation and hidden state shown in Figure 4(a) also"",null,null",null,null
514,"513,552,null,null",null,null
515,"514,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
516,"515,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
517,"516,-nDCG -nDCG,null,null",null,null
518,"517,0.47,null,null",null,null
519,"518,0.47,null,null",null,null
520,"519,0.46,null,null",null,null
521,"520,0.46,null,null",null,null
522,"521,0.45,null,null",null,null
523,"522,0.45,null,null",null,null
524,"523,0.44,null,null",null,null
525,"524,0.44,null,null",null,null
526,"525,""0.43 (25,10) (50,25) (100,50) (200,100)"",null,null",null,null
527,"526,dimensionality,null,null",null,null
528,"527,0.43 0,null,null",null,null
529,"528,5,null,null",null,null
530,"529,10 15 20,null,null",null,null
531,"530,#random permutation,null,null",null,null
532,"531,(a)  -nDCG w.r.t. different size (b)  -nDCG w.r.t. different random permutation count,null,null",null,null
533,"532,Figure 4: Performance tendency of different settings.,null,null",null,null
534,"533,DSSA i1 i2 i3 i4 i5 d1 d2 d3 d4 d5,null,null",null,null
535,"534,PAMM-NTN i1 i2 i3 i4 i5,null,null",null,null
536,"535,DSSA,null,null",null,null
537,"536,PAMM-NTN,null,null",null,null
538,"537,i1 i2 i3 i4 i1 i2 i3 i4,null,null",null,null
539,"538,(a) ranking of query #58,null,null",null,null
540,"539,(b) ranking of query #182,null,null",null,null
541,"540,Figure 5: Case study for DSSA and PAMM-NTN. White means relevant and black means irrelevant.,null,null",null,null
542,"541,Table 9: Effects of different optimization methods.,null,null",null,null
543,"542,Methods ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,null,null",null,null
544,"543,MLE,null,null",null,null
545,"544,.349,null,null",null,null
546,"545,PAMM,null,null",null,null
547,"546,.348,null,null",null,null
548,"547,list-pairwise .356,null,null",null,null
549,"548,.446 .315 .445 .315 .456 .326,null,null",null,null
550,"549,.462 .176 .644 .463 .175 .644 .473 .185 .649,null,null",null,null
551,"550,""indicate no strong correlation between performance and settings. -nDCG remains above 0.45 using different sizes. The best performance is achieved using 100-dimensional representation and 50dimensional hidden state. This suggests that high dimensionality may result in overfitting. Without using max-pooling, -nDCG drops to 0.445, which demonstrates the usefulness of using maxpooling to enhance subtopic attention. The small differences between different settings suggest that DSSA is a stable and robust framework. Note that we use both distributed representations and relevance features, which are complementary to each other. This may be one of the reasons of the stability."",null,null",null,null
552,"551,6.3 Effects of Different Optimization Methods,null,null",null,null
553,"552,""Results in Table 9 shows that list-pairwise is more effective than MLE and PAMM. This confirms our earlier intuition that list-pairwise optimization corresponds better to the situation of diversification ranking than the two other methods. Note that even using MLE or PAMM as optimization methods, DSSA could also achieve stateof-the-art performances, which confirms the effectiveness of our explicit learning framework from another perspective."",null,null",null,null
554,"553,""We vary the number of random permutations used in list-pairwise sampling from 0 to 20 to investigate its effect. As depicted in Figure 4(b), the performance does not heavily rely it. The best performance is achieved around 10. More permutations lead to lower effectiveness, which could be explained by model overfitting."",null,null",null,null
555,"554,6.4 Visualization and Discussion,null,null",null,null
556,"555,We visualize the ranking results of DSSA and the variation of subtopic attention to better understand why DSSA performs well.,null,null",null,null
557,"556,""We show the top 5 ranking results of query #58 and #182 in Figure 5 to illustrate why DSSA outperforms implicit learning methods. We choose PAMM-NTN as comparison method, which is the best existing learning method. In Figure 5, white means relevant and black means irrelevant. For query #58, DSSA ranks a document relevant to subtopics i3 and i4 first and a document relevant to i1 and i2 at the second position, while the first two documents of PAMM-NTN cover the same subtopics. Note that there is no"",null,null",null,null
558,"557,d1 d2 d3 d4 d5,null,null",null,null
559,"558,z1. quit smoking tips (i1),null,null",null,null
560,"559,z2. quit smoking app (i1),null,null",null,null
561,"560,z3. quit smoking calculator (i1) subtopics from Google z4. quit smoking help (i1),null,null",null,null
562,"561,z5. quit smoking benefits (i2),null,null",null,null
563,"562,z6. quit smoking cold turkey (i3),null,null",null,null
564,"563,official subtopics,null,null",null,null
565,"564,z7. quit smoking hypnosis (i4) i1. What are the ways you can quit smoking? i2. What are the benefits of quitting smoking? i3. Can you quit smoking using the cold turkey method? i4. How can hypnosis help someone quit smoking?,null,null",null,null
566,"565,Figure 6: Subtopic attention variation of query #182. The top part is attention and the bottom part is relevance judgment.,null,null",null,null
567,"566,""document covering i5 in the candidate set. For query #182, DSSA successively chooses documents that cover i1, i3, i2, and i4. One additional intent is satisfied at every position. PAMM-NTN, however, just covers i1 and i2 by top 5 documents, which is obviously not optimal. We see that the unequal and varied subtopic attention is capable of discovering unsatisfied subtopics at different positions, which eventually leads to more subtopic coverage."",null,null",null,null
568,"567,""To study attention mechanism, we further visualize the variation of subtopic attention of top 5 documents of query #182, namely """"quit smoking"""", which has 4 official subtopics (i1 to i4), as shown in Figure 6. The top part is subtopic attention variation and the bottom part is relevance judgment. For attention part, the darker the cell is, the lower the attention (weight) on this subtopic is. Note that we actually leverage query suggestions of Google (z1 to z7) to serve as subtopics, which do not match official ones exactly. We manually align subtopics mined from Google to official ones. At the beginning, all the subtopics have equal attention. The first selected document d1 is relevant to i1, i.e. to the Goggle subtopics z1, z2, z3 and z4. We see that the attention to these latter decreases at second position. Then the document d2 is selected, which is relevant to uncovered i3. We see that the attention to the corresponding z6 begins to diminish from the third position. d3 and d4 satisfy additional i2 and i4 respectively, which leads to the reduction"",null,null",null,null
569,"568,553,null,null",null,null
570,"569,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
571,"570,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
572,"571,""of attention on z5 and z7 at the following position. The subtopic attention, initialized as uniform distribution, ends up with more emphasis on z4, z6, and z7. This example illustrates how the unequal and varied attention drives the model to emphasize different subtopics at different positions, which is crucial in explicit diversification. This example also shows a potential problem inherent for any method using automatically discovered subtopics: those topics may be different from the ones defined by human assessors. Equal distribution is assumed on all the subtopics zi . However, this implies an unequal distribution among the manually defined subtopics (more emphasis is put on i1). Assuming an equal distribution at the beginning may not necessarily be the best approach. We will deal with this problem in our future work."",null,null",null,null
573,"572,7 CONCLUSIONS,null,null",null,null
574,"573,""In this paper, we propose a general learning framework DSSA to model subtopics explicitly for search result diversification. Based on the sequence of selected documents, unequal and varied subtopic attention is calculated, driving the model to emphasize different subtopics at different positions. This is the first time that attention mechanism is used to model the process. We further instantiate DSSA using RNN and max-pooling to handle both distributed representations and relevance features, which outperforms significantly the existing approaches. The results confirm that modeling subtopics explicitly in a learning framework is beneficial and effective and this also avoids heuristically defined functions and parameters. However, accurately modeling the interaction among documents and subtopics is still challenging. There are many other more complex implementations besides our particular way, which will be investigated in future work. The proposed model contains a number of parameters to be learned. This requires a large number of training data. Collecting more training data to fully unlock the potential of the model is another direction. Finally, this work only deals with the learning of a ranking function, assuming that document and query representations have already been created. In practice, learning these representation is another interesting aspect, which could be incorporated into our framework, provided with sufficient training data."",null,null",null,null
575,"574,ACKNOWLEDGMENTS,null,null",null,null
576,"575,""Zhicheng Dou is the corresponding author. This work was funded by the National Natural Science Foundation of China under Grant No. 61502501 and 61502502, the National Key Basic Research Program (973 Program) of China under Grant No. 2014CB340403, and the Beijing Natural Science Foundation under Grant No. 4162032."",null,null",null,null
577,"576,REFERENCES,null,null",null,null
578,"577,""[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In WSDM."",null,null",null,null
579,"578,""[2] Ricardo A. Baeza-Yates, Carlos A. Hurtado, and Marcelo Mendoza. 2004. Query Recommendation Using Query Logs in Search Engines. In Current Trends in Database Technology EDBT 2004 Workshops."",null,null",null,null
580,"579,""[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473 (2014)."",null,null",null,null
581,"580,""[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003)."",null,null",null,null
582,"581,""[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/. (2009)."",null,null",null,null
583,"582,""[6] Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In SIGIR."",null,null",null,null
584,"583,[7] Ben Carterette. 2009. An Analysis of NP-Completeness in Novelty and Diversity Ranking. In ICTIR.,null,null",null,null
585,"584,""[8] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In CIKM."",null,null",null,null
586,"585,""[9] Junyoung Chung, C¸aglar Gu¨l¸cehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. CoRR abs/1412.3555 (2014)."",null,null",null,null
587,"586,""[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In SIGIR."",null,null",null,null
588,"587,""[11] Charles L. A. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. In ICTIR."",null,null",null,null
589,"588,[12] Van Dang and Bruce W. Croft. 2013. Term Level Search Result Diversification. In SIGIR.,null,null",null,null
590,"589,[13] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversification. In SIGIR.,null,null",null,null
591,"590,""[14] Amac Herdagdelen, Massimiliano Ciaramita, Daniel Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler, and Enrique Alfonseca. 2010. Generalized Syntactic and Semantic Models of Query Reformulation. In SIGIR."",null,null",null,null
592,"591,""[15] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997)."",null,null",null,null
593,"592,""[16] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversification Based on Hierarchical Intents. In CIKM."",null,null",null,null
594,"593,""[17] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry P. Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM."",null,null",null,null
595,"594,[18] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML.,null,null",null,null
596,"595,""[19] Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009)."",null,null",null,null
597,"596,""[20] Yiqun Liu, Ruihua Song, Min Zhang, Zhicheng Dou, Takehiro Yamamoto, Makoto P Kato, Hiroaki Ohshima, and Ke Zhou. 2014. Overview of the NTCIR-11 IMine Task.. In NTCIR. Citeseer."",null,null",null,null
598,"597,""[21] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In EMNLP."",null,null",null,null
599,"598,""[22] John I Marden. 1996. Analyzing and modeling rank data. CRC Press. [23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean."",null,null",null,null
600,"599,""2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS. [24] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent Models of Visual Attention. In NIPS. [25] Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto, Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and M Iwata. 2013. Overview of the NTCIR-10 INTENT-2 Task.. In NTCIR. [26] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results Using Per-intent Graded Relevance. In SIGIR. [27] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting Query Reformulations for Web Search Result Diversification. In WWW. [28] Rodrygo L.T. Santos, Craig Macdonald, Iadh Ounis, and others. 2015. Search result diversification. Foundations and Trends® in Information Retrieval (2015). [29] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. [30] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In NIPS. [31] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In ICML. [32] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In SIGIR. [33] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversification. In SIGIR. [34] Jeonghee Yi and Farzin Maghoul. 2009. Query Clustering Using Click-through Graph. In WWW. [35] Hai-Tao Yu and Fuji Ren. 2014. Search Result Diversification via Filling Up Multiple Knapsacks. In CIKM. [36] Yisong Yue and Thorsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In ICML. [37] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR. [38] Zhiyong Zhang and Olfa Nasraoui. 2006. Mining Search Engine Query Logs for Query Recommendation. In WWW. [39] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversification. In SIGIR."",null,null",null,null
601,"600,554,null,null",null,null
602,"601,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 4C: Queries and Query Analysis,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Relevance-based Word Embedding,null,null",null,null
4,"3,Hamed Zamani,null,null",null,null
5,"4,Center for Intelligent Information Retrieval College of Information and Computer Sciences,null,null",null,null
6,"5,""University of Massachuse s Amherst Amherst, MA 01003 zamani@cs.umass.edu"",null,null",null,null
7,"6,ABSTRACT,null,null",null,null
8,"7,""Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently a racted much a ention in natural language processing and information retrieval tasks. e embedding vectors are typically learned based on term proximity in a large corpus. is means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks. e primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity. is is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. In this paper, we propose two learning models with di erent objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classi es each term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classi cation. Both query expansion experiments on four TREC collections and query classi cation experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models signi cantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe."",null,null",null,null
9,"8,KEYWORDS,null,null",null,null
10,"9,""Word representation, neural network, embedding vector, query expansion, query classi cation"",null,null",null,null
11,"10,""ACM Reference format: Hamed Zamani and W. Bruce Cro . 2017. Relevance-based Word Embedding. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080831"",null,null",null,null
12,"11,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080831"",null,null",null,null
13,"12,W. Bruce Cro,null,null",null,null
14,"13,Center for Intelligent Information Retrieval College of Information and Computer Sciences,null,null",null,null
15,"14,""University of Massachuse s Amherst Amherst, MA 01003 cro @cs.umass.edu"",null,null",null,null
16,"15,1 INTRODUCTION,null,null",null,null
17,"16,""Representation learning is a long-standing problem in natural language processing (NLP) and information retrieval (IR). e main motivation is to abstract away from the surface forms of a piece of text, e.g., words, sentences, and documents, in order to alleviate sparsity and learn meaningful similarities, e.g., semantic or syntactic similarities, between two di erent pieces of text. Learning representations for words as the atomic components of a language, also known as word embedding, has recently a racted much a ention in the NLP and IR communities."",null,null",null,null
18,"17,""A popular model for learning word representation is neural network-based language models. For instance, the word2vec model proposed by Mikolov et al. [24] is an embedding model that learns word vectors via a neural network with a single hidden layer. Continuous bag of words (CBOW) and skip-gram are two implementations of the word2vec model. Another successful trend in learning semantic word representations is employing global matrix factorization over word-word matrices. GloVe [28] is an example of such methods. A theoretical relation has been discovered between embedding models based on neural network and matrix factorization in [21]. ese models have been demonstrated to be e ective in a number of IR tasks, including query expansion [11, 17, 40], query classi cation [23, 41], short text similarity [15], and document model estimation [2, 31]."",null,null",null,null
19,"18,""e aforementioned embedding models are typically trained based on term proximity in a large corpus. For instance, the word2vec model's objective is to predict adjacent word(s) given a word or context, i.e., a context window around the target word. is idea aims to capture semantic and syntactic similarities between terms, since semantically/syntactically similar words o en share similar contexts. However, this objective is not necessarily equivalent to the main objective of many IR tasks. e primary objective in many IR methods is to model the notion of relevance [20, 34, 43]. In this paper, we revisit the underlying assumption of typical word embedding methods, as follows:"",null,null",null,null
20,"19,e objective is to predict the words observed in the documents relevant to a particular information need.,null,null",null,null
21,"20,""is objective has been previously considered for developing relevance models [20], a state-of-the-art (pseudo-) relevance feedback approach. Relevance models try to optimize this objective given a set of relevant documents for a given query as the indicator of user's information need. In the absence of relevance information, the top ranked documents retrieved in response to the query are assumed to be relevant. erefore, relevance models, and in general all pseudo-relevance feedback models, use an online se ing to obtain training data: retrieving documents for the query and"",null,null",null,null
22,"21,505,null,null",null,null
23,"22,Session 4C: Queries and Query Analysis,null,null",null,null
24,"23,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
25,"24,""then using the top retrieved documents in order to estimate the relevance distribution. Although relevance models have been proved to be e ective in many IR tasks [19, 20], having a retrieval run for each query to obtain the training data for estimating the relevance distribution is not always practical in real-world search engines. We, in this paper, optimize a similar objective in an o ine se ing, which enables us to predict the relevance distribution without any retrieval runs during the test time. To do so, we consider the top retrieved documents for millions of training queries as a training set and learn embedding vectors for each term in order to predict the words observed in the top retrieved documents for each query. We develop two relevance-based word embedding models. e rst one, the relevance likelihood maximization model (RLM), aims to model the relevance distribution over the vocabulary terms for each query, while the second one, the relevance posterior estimation model (RPE), classi es each term as relevant or non-relevant to each query. We provide e cient learning algorithms to train these models on large amounts of training data. Note that our models are unsupervised and the training data is generated automatically."",null,null",null,null
26,"25,""To evaluate our models, we performed two sets of extrinsic evaluations. In the rst set, we focus on the query expansion task for ad-hoc retrieval. In this set of experiments, we consider four TREC collections, including two newswire collections (AP and Robust) and two large-scale web collections (GOV2 and ClueWeb09 - Cat. B). Our results suggest that the relevance-based embedding models outperform state-of-the-art word embedding algorithms. e RLM model shows be er performance compared to RPE in the context of query expansion, since the goal is to estimate the probability of each term given a query and this distribution is not directly learned by the RPE model. In the second set of experiments, we focus on the query classi cation task using the KDD Cup 2005 [22] dataset. In this extrinsic evaluation, the relevance-based embedding models again perform be er than the baselines. Interestingly, the query classi cation results demonstrate that the RPE model outperforms the RLM model, for the reason that in this task, unlike the query expansion task, the goal is to compute the similarity between two query vectors, and RPE can learn more accurate embedding vectors with less training data."",null,null",null,null
27,"26,2 RELATED WORK,null,null",null,null
28,"27,""Learning a semantic representation for text has been studied for many years. Latent semantic indexing (LSI) [8] can be considered as early work in this area that tries to map each text to a semantic space using singular value decomposition (SVD), a well-known matrix factorization algorithm. Subsequently, Clinchant and Perronnin [5] proposed Fisher Vector (FV), a document representation framework based on continuous word embeddings, which aggregates a non-linear mapping of word vectors into a document-level representation. However, a number of popular IR models, such as BM25 and language models, o en signi cantly outperform the models that are based on semantic similarities. Recently, extremely e cient word embedding algorithms have been proposed to model semantic similarly between words."",null,null",null,null
29,"28,""Word embedding, also known as distributed representation of words, refers to a set of machine learning algorithms that learn high-dimensional real-valued dense vector representation w  Rd"",null,null",null,null
30,"29,""for each vocabulary term w, where d denotes the embedding dimensionality. GloVe [28] and word2vec [24] are two well-known word embedding algorithms that learn embedding vectors based on the same idea, but using di erent machine learning techniques."",null,null",null,null
31,"30,""e idea is that the words that o en appear in similar contexts are similar to each other. To do so, these algorithms try to accurately predict the adjacent word(s) given a word or a context (i.e., a few words appeared in the same context window). Recently, Rekabsaz et al. [30] proposed to exploit global context in word embeddings in order to avoid topic shi ing."",null,null",null,null
32,"31,""Word embedding representations can be also learned as a set of parameters in an end-to-end neural network model. For instance, Zamani et al. [39] trained a context-aware ranking model in which the embedding vectors of frequent n-grams are learned using click data. More recently, Dehghani et al. [9] trained neural ranking models with weak supervision data (i.e., a set of noisy training data automatically generated by an existing unsupervised model) that learn word representations in an end-to-end ranking scenario."",null,null",null,null
33,"32,""Word embedding vectors have been successfully employed in several NLP and IR tasks. Kusner et al. [16] proposed word mover's distance (WMD), a function for calculating semantic distance between two documents, which measures the minimum traveling distance from the embedded vectors of individual words in one document to the other one. Zhou et al. [47] introduced an embeddingbased method for question retrieval in the context of community question answering. Vulic´ and Moens [37] proposed a model to learn bilingual word embedding vectors from document-aligned comparable corpora. Zheng and Callan [46] presented a supervised embedding-based technique to re-weight terms in the existing IR models, e.g., BM25. Based on the well-de ned structure of language modeling framework in information retrieval, a number of methods have been introduced to employ word embedding vectors within this framework in order to improve the performance in IR tasks. For instance, Zamani and Cro [40] presented a set of embedding-based query language models using the query expansion and pseudo-relevance feedback techniques that bene t from the word embedding vectors. ery expansion using word embedding has been also studied in [11, 17, 35]. All of these approaches are based on word embeddings learned based on term proximity information. PhraseFinder [14] is an early work using term proximity information for query expansion. Mapping vocabulary terms to HAL space, a low-dimensional space compared to vocabulary size, has been used in [4] for query modeling."",null,null",null,null
34,"33,""As is widely known in the information retrieval literature [11, 38], there is a big di erence between the unigram distribution of words on sub-topics of a collection and the unigram distribution estimated from the whole collection. Given this phenomenon, Diaz et al. [11] recently proposed to train word embedding vectors on the top retrieved documents for each query. However, this model, called local embedding, is not always practical in real-word applications, since the embedding vectors need to be trained during the query time. Furthermore, the objective function in local embedding is based on term proximity in pseudo-relevant documents."",null,null",null,null
35,"34,""In this paper, we propose two models for learning word embedding vectors, that are speci cally designed for information retrieval needs. All the aforementioned tasks in this section can potentially bene t from the vectors learned by the proposed models."",null,null",null,null
36,"35,506,null,null",null,null
37,"36,Session 4C: Queries and Query Analysis,null,null",null,null
38,"37,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
39,"38,3 RELEVANCE-BASED EMBEDDING,null,null",null,null
40,"39,""Typical word embedding algorithms, such as word2vec [24] and GloVe [28], learn high-dimensional real-valued embedding vectors based on the proximity of terms in a training corpus, i.e., cooccurrence of terms in the same context window. Although these approaches could be useful for learning the embedding vectors that can capture semantic and syntactic similarities between vocabulary terms and have shown to be useful in many NLP and IR tasks, there is a large gap between their learning objective (i.e., term proximity) and what is needed in many information retrieval tasks. For example, consider the query expansion task and assume that a user submi ed the query """"dangerous vehicles"""". One of the most similar terms to this query based on the typical word embedding algorithms (e.g., word2vec and GloVe) is """"safe"""", and thus it would get a high weight in the expanded query model. e reason is that the words """"dangerous"""" and """"safe"""" o en share similar contexts. However, expanding the query with the word """"safe"""" could lead to poor retrieval performance, since it changes the meaning and the intent of the query."",null,null",null,null
41,"40,""is example together with many others have motivated us to revisit the objective used in the learning process of word embedding algorithms in order to obtain the word vectors that be er match with the needs in IR tasks. e primary objective in many IR tasks is to model the notion of relevance. Several approaches, such as the relevance models proposed by Lavrenko and Cro [20], have been proposed to model relevance. Given the successes achieved by these models, we propose to learn word embedding vectors based on an objective that ma ers in information retrieval. e objective is to accurately predict the terms that are observed in a set of relevant documents to a particular information need."",null,null",null,null
42,"41,""In the following subsections, we rst describe our neural network architecture, and then explain how to build a training set for learning relevance-based word embeddings. We further introduce two models, relevance likelihood maximization (RLM) and relevance posterior estimation (RPE), with di erent objectives using the described neural network."",null,null",null,null
43,"42,3.1 Neural Network Architecture,null,null",null,null
44,"43,We use a simple yet e ective feed-forward neural network with a,null,null",null,null
45,"44,single linear hidden layer. e architecture of our neural network,null,null",null,null
46,"45,is shown in Figure 1. e input of the model is a sparse query,null,null",null,null
47,"46,""vector qs with the length of N , where N denotes the total number of vocabulary terms. is vector can be obtained by a projection"",null,null",null,null
48,"47,function given the vectors corresponding to individual query terms.,null,null",null,null
49,"48,""In this paper, we simply consider average as the projection function."",null,null",null,null
50,"49,""Hence, qs"",null,null",null,null
51,"50,"","",null,null",null,null
52,"51,1 |q |,null,null",null,null
53,"52,""w q ew , where ew and |q| denote the one-hot"",null,null",null,null
54,"53,""vector representation of term w and the query length, respectively."",null,null",null,null
55,"54,e hidden layer in this network maps the given query sparse vector,null,null",null,null
56,"55,""to a query embedding vector q, as follows:"",null,null",null,null
57,"56,""q , qs × WQ"",null,null",null,null
58,"57,(1),null,null",null,null
59,"58,where WQ  RN ×d is a weight matrix for estimating query embedding vectors and d denotes the embedding dimensionality. e,null,null",null,null
60,"59,output layer of the network is a fully-connected layer given by:,null,null",null,null
61,"60, (q × Ww + bw ),null,null",null,null
62,"61,(2),null,null",null,null
63,"62,query sparse vector,null,null",null,null
64,"63,hidden layer,null,null",null,null
65,"64,qs,null,null",null,null
66,"65,output layer,null,null",null,null
67,"66,W1 W2 W3,null,null",null,null
68,"67,......... .........,null,null",null,null
69,"68,d neurons,null,null",null,null
70,"69,WN,null,null",null,null
71,"70,N neurons,null,null",null,null
72,"71,Figure 1: e relevance-based word embedding architecture. e objective is to learn d-dimensional distributed represen-,null,null",null,null
73,"72,""tation for words based on the notion of relevance, instead of term proximity. N denotes the total number of vocabulary terms."",null,null",null,null
74,"73,where Ww  Rd×N and bw  R1×N are the weight and the bias matrices for estimating the probability of each term.  is the activation function which is discussed in Sections 3.3 and 3.4.,null,null",null,null
75,"74,""To summarize, our network contains two sets of embedding parameters, WQ and Ww . e former aims to map the query into the """"query embedding space"""", while the la er is used to estimate the weights of individual terms."",null,null",null,null
76,"75,3.2 Modeling Relevance for Training,null,null",null,null
77,"76,""Relevance feedback has been shown to be highly e ective in improving retrieval performance [7, 32]. In relevance feedback, a set of relevant documents to a given query is considered for estimating accurate query models. Since explicit relevance signals for a given query are not always available, pseudo-relevance feedback (PRF) assumes that the top retrieved documents in response to the given query are relevant to the query and uses these documents in order to estimate be er query models. e e ectiveness of PRF in various retrieval scenarios indicates that useful information can be captured from the top retrieved documents [19, 20, 44]. In this paper, we make use of this well-known assumption to train our model. It should be noted that there is a signi cant di erence between PRF and the proposed models: In PRF, the feedback model is estimated from the top retrieved documents of the given query in an online se ing. In other words, PRF retrieves the documents for the initial query and then estimates the feedback model using the top retrieved documents. In this paper, we propose to train the model in an o ine se ing. Moving from the online to the o ine se ing would lead to substantial improvements in e ciency, because an extra retrieval run is not needed in the o ine se ing. To learn a model in an o ine se ing, we consider a xed-length dense vector for each vocabulary term and estimate these vectors based on the information extracted from the top retrieved documents for large numbers of training queries. Note that our models are"",null,null",null,null
78,"77,507,null,null",null,null
79,"78,Session 4C: Queries and Query Analysis,null,null",null,null
80,"79,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
81,"80,""unsupervised. However, if explicit relevance data is available, such as click data, without loss of generality, both the explicit or implicit relevant documents can be considered for training our models. We leave studying the vectors learned based on supervised signals for future work."",null,null",null,null
82,"81,""To formally describe our training data, letT ,"""" {(q1, R1), (q2, R2), · · · , (qm, Rm )} be a training set with m training queries. e ith element of this set is a pair of query qi and the corresponding pseudo-relevance feedback distribution. ese distributions are estimated based on the top k retrieved documents (in our experiments, we set k to 10) for each query. e distributions can be estimated using any PRF model, such as those proposed in [20, 36, 42, 44]. In this paper, we only focus on the relevance model [20], a state-of-the-art PRF model, that estimates the relevance distribution as:"""""",null,null",null,null
83,"82,p(w |Ri )  p(w |d ),null,null",null,null
84,"83,p(w |d ),null,null",null,null
85,"84,(3),null,null",null,null
86,"85,d Fi,null,null",null,null
87,"86,w qi,null,null",null,null
88,"87,where Fi denotes a set of top retrieved documents for query qi . Note that the probability of terms that do not appear in the top,null,null",null,null
89,"88,retrieved documents is equal to zero.,null,null",null,null
90,"89,3.3 Relevance Likelihood Maximization Model,null,null",null,null
91,"90,""In this model, the goal is to learn the relevance distribution R."",null,null",null,null
92,"91,""Given a set of training data, we aim to nd a set of parameters  R"",null,null",null,null
93,"92,in order to maximize the likelihood of generating relevance model,null,null",null,null
94,"93,probabilities for the whole training set. e likelihood function is,null,null",null,null
95,"94,de ned as follows:,null,null",null,null
96,"95,m,null,null",null,null
97,"96,p (w |qi ;  R )p (w | Ri ),null,null",null,null
98,"97,(4),null,null",null,null
99,"98,""i,1 w Vi"",null,null",null,null
100,"99,where p is the relevance distribution that can be obtained given,null,null",null,null
101,"100,the learning parameters  R and p(w |Ri ) denotes the relevance model distribution estimated for the ith query in the training set,null,null",null,null
102,"101,(see Section 3.2 for more detail). Vi denotes a subset of vocabulary terms that appeared in the top ranked documents retrieved for the,null,null",null,null
103,"102,query qi . e reason for iterating over the terms that appeared in this set instead of the whole vocabulary set V is that the probability,null,null",null,null
104,"103,""p(w |Ri ) is equal to zero for all terms w  V - Vi . In this method, we model the probability distribution p using the"",null,null",null,null
105,"104,""so max function (i.e., the function  in Equation (2)) as follows:1"",null,null",null,null
106,"105,""p(w |q;  R ) ,"",null,null",null,null
107,"106,exp (wT q) w V exp (w T q),null,null",null,null
108,"107,(5),null,null",null,null
109,"108,""where w denotes the learned embedding vector for term w and q is the query vector came from the output of the hidden layer in our network (see Section 3.1). According to the so max modeling and the log-likelihood function, we have the following objective:"",null,null",null,null
110,"109,m,null,null",null,null
111,"110,arg max,null,null",null,null
112,"111,p(w |Ri ) log exp (wT qi ) - log,null,null",null,null
113,"112,exp (w T qi ),null,null",null,null
114,"113,""R i,1 w Vi"",null,null",null,null
115,"114,w V,null,null",null,null
116,"115,(6),null,null",null,null
117,"116,Computing this objective function and its derivatives would,null,null",null,null
118,"117,be computationally expensive (due to the presence of the normal-,null,null",null,null
119,"118,ization factor w V exp (w T q) in the objective function). Since all the word embedding vectors as well as the query vector are,null,null",null,null
120,"119,""1For simplicity, we drop the bias term in these equations."",null,null",null,null
121,"120,""changed during the optimization process, we cannot simply omit the normalization term as is done in [41] for estimating query embedding vectors based on pre-trained word embedding vectors. To make the computations more tractable, we consider a hierarchical approximation of the so max function, which was introduced by Morin and Bengio [26] in the context of neural network language models and then successfully employed by Mikolov et al. [24] in the word2vec model."",null,null",null,null
122,"121,""e hierarchical so max approximation uses a binary tree structure to represent the vocabulary terms, where each leaf corresponds to a unique word. ere exists a unique path from the root to each leaf, and this path is used for estimating the probability of the word representing by the leaf. erefore, the complexity of calculating so max probabilities goes down from O (|V |) to O (log(|V |)) which is the height of the tree. is leads to a huge improvement in computational complexity. We refer the reader to [25, 26] for the details of calculating the hierarchical so max approximation."",null,null",null,null
123,"122,3.4 Relevance Posterior Estimation Model,null,null",null,null
124,"123,""As an alternative to maximum likelihood estimation, we can estimate the relevance posterior probability. In the context of pseudorelevance feedback, Zhai and La ery [44] assumed that the language model of the top retrieved documents is estimated based on a mixture model. In other words, it is assumed that there are two language models for the feedback set: the relevance language model2 and a background noisy language model. ey used an expectationmaximization algorithm to estimate the relevance language model. In this model, we make use of this assumption in order to cast the problem of estimating the relevance distribution R as a classi cation task: Given a pair of word w and query q, does w come from the relevance distribution of the query q? Instead of p(w |R), this model estimates p(R ,"""" 1|w, q;  R ) where R is a Boolean variable and R """","""" 1 means that the given term-query pair (w, q) comes from the relevance distribution R.  R is a set of parameters that is going to be learned during the training phase."""""",null,null",null,null
125,"124,""erefore, the problem is cast as a binary classi cation task that can be modeled by logistic regression (which means the function  in Equation (2) is the sigmoid function):"",null,null",null,null
126,"125,1,null,null",null,null
127,"126,""p (R ,"""" 1|w, q;  R ) """", 1 + e (-wT q)"",null,null",null,null
128,"127,(7),null,null",null,null
129,"128,where w is the relevance-based word embedding vector for term w.,null,null",null,null
130,"129,""Similar to the previous model, q is the output of the hidden layer"",null,null",null,null
131,"130,""of the network, representing the query embedding vector."",null,null",null,null
132,"131,""In order to address this binary classi cation problem, we consider"",null,null",null,null
133,"132,""a cross-entropy loss function. In theory, for each training query,"",null,null",null,null
134,"133,our model should learn to model relevance for the terms appearing,null,null",null,null
135,"134,in the corresponding pseudo-relevant set and non-relevance for all,null,null",null,null
136,"135,""the other vocabulary terms, which could be impractical, due to the"",null,null",null,null
137,"136,""large number of vocabulary terms. Similar to [24], we propose to"",null,null",null,null
138,"137,use the noise contrastive estimation (NCE) [12] which hypothesizes,null,null",null,null
139,"138,that we can achieve a good model by only di erentiating the data,null,null",null,null
140,"139,from noise via a logistic regression model. e main concept in NCE,null,null",null,null
141,"140,is similar to those proposed in the divergence from randomness,null,null",null,null
142,"141,model [3] and the divergence minimization feedback model [44].,null,null",null,null
143,"142,""2 e phrase """"topical language model"""" was used in the original work [44]. We call it """"relevance language model"""" to have consistent de nitions in our both models."",null,null",null,null
144,"143,508,null,null",null,null
145,"144,Session 4C: Queries and Query Analysis,null,null",null,null
146,"145,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
147,"146,""Based on the NCE hypothesis, we de ne the following negative cross-entropy objective function for training our model:"",null,null",null,null
148,"147,arg,null,null",null,null
149,"148,max,null,null",null,null
150,"149,R,null,null",null,null
151,"150,""m i ,1"",null,null",null,null
152,"151,""+ j ,1"",null,null",null,null
153,"152,Ewj p (w,null,null",null,null
154,"153,| Ri,null,null",null,null
155,"154,),null,null",null,null
156,"155,""log p(R ,"""" 1|wj , qi ;  R )"""""",null,null",null,null
157,"156,-,null,null",null,null
158,"157,+ Ewj pn (w ),null,null",null,null
159,"158,""j ,1"",null,null",null,null
160,"159,""log p(R ,"""" 0|wj , qi ;  R ) """""",null,null",null,null
161,"160,(8),null,null",null,null
162,"161,""where pn (w ) denotes a noise distribution and  ,"""" (+, -) is a pair of hyper-parameters to control the number of positive and negative instances per query, respectively. We can easily calculate p(R """","""" 0|wj , qi ) """", 1 - p(R ,"""" 1|wj , qi ). e noise distribution pn (w ) can be estimated using a function of unigram distribution U (w ) in the whole training set. Similar to [24], we use pn (w )  U (w )3/4 which has been empirically shown to work e ectively for negative sampling."""""",null,null",null,null
163,"162,""It is notable that although this model learns embedding vectors for both queries and words, it is not obvious how to calculate the probability of each term given a query; because Equation 7 only gives us a classi cation probability and we cannot simply use the Bayes rule here (since, not all probability components are known). is model can perform well when computing the similarity between two terms or two queries, but not a query and a term. However, we can use the model presented in [41] to estimate the query model using the word embedding vectors (not the ones learned for query vectors) and then calculate the similarity between a query and a term."",null,null",null,null
164,"163,4 EXPERIMENTS,null,null",null,null
165,"164,""In this section, we rst describe how we train the relevance-based word embedding models. We further extrinsically evaluate the learned embeddings using two IR tasks: query expansion and query classi cation. Note that the main aim here is to compare the proposed models with the existing word embedding algorithms, not with the state-of-the-art query expansion and query classi cation models."",null,null",null,null
166,"165,4.1 Training,null,null",null,null
167,"166,""In order to train relevance-based word embeddings, we obtained millions of unique queries from the publicly available AOL query logs [27]. is dataset contains a sample of web search queries from real users submi ed to the AOL search engine within a three-month period from March 1, 2006 to May 31, 2006. We only used query strings and no session and click information was obtained from this dataset. We ltered out the navigational queries containing URL substrings, i.e., """"h p"""", """"www."""", """".com"""", """".net"""", """".org"""", """".edu"""". All nonalphanumeric characters were removed from all queries. Applying all these constraints leads to over 6 millions unique queries as our training query set. To estimate the relevance model distributions in the training set, we considered top 10 retrieved documents in a target collection in response to each query using the Galago3 implementation of the query likelihood retrieval model [29] with Dirichlet prior smoothing (µ , 1500) [45]."",null,null",null,null
168,"167,3h p://www.lemurproject.org/galago.php,null,null",null,null
169,"168,""We implemented and trained our models using TensorFlow4. e networks are trained based on the stochastic gradient descent optimizer using the back-propagation algorithm [33] to compute the gradients. All model hyper-parameters were tuned on the training set (the hyper-parameters with the smallest training loss value were selected). For each model, the learning rate and the batch size were selected from [0.001, 0.01, 0.1, 1] and [64, 128, 256], respectively. For RPE , we also tuned the number of positive and negative instances (i.e., + and -). e value of + was swept between [20, 50, 100, 200] and the parameter - was selected from [5+, 10+, 20+]. As suggested in [40], in all the experiments (unless otherwise stated) the embedding dimensionality was set to 300, for all models including the baselines."",null,null",null,null
170,"169,4.2 Evaluation via ery Expansion,null,null",null,null
171,"170,""In this subsection, we evaluate the embedding models in the context of query expansion for the ad-hoc retrieval task. In the following, we rst describe the retrieval collections used in our experiments. We further explain our experimental setup as well as the evaluation metrics. We nally report and discuss the query expansion results."",null,null",null,null
172,"171,""4.2.1 Data. We use four standard test collections in our experiments. e rst two collections (AP and Robust) consist of thousands of news articles and are considered as homogeneous collections. AP and Robust were previously used in TREC 1-3 Ad-Hoc Track and TREC 2004 Robust Track, respectively. e second two collections (GOV2 and ClueWeb) are large-scale web collections containing heterogeneous documents. GOV2 consists of the """".gov"""" domain web pages, crawled in 2004. ClueWeb (i.e., ClueWeb09Category B) is a common web crawl collection that only contains English web pages. GOV2 and ClueWeb were previously used in TREC 2004-2006 Terabyte Track and TREC 2009-2012 Web Track, respectively. e statistics of these collections as well as the corresponding TREC topics are reported in Table 1. We only used the title of topics as queries."",null,null",null,null
173,"172,4.2.2 Experimental Setup. We cleaned the ClueWeb collection,null,null",null,null
174,"173,by ltering out the spam documents. e spam ltering phase was done using the Waterloo spam scorer5 [6] with the threshold of 60%.,null,null",null,null
175,"174,Stopwords were removed from all collections using the standard,null,null",null,null
176,"175,INQUERY stopword list and no stemming were performed.,null,null",null,null
177,"176,""For the purpose of query expansion, we consider the language"",null,null",null,null
178,"177,modeling framework [29] and estimate a query language model,null,null",null,null
179,"178,based on a given set of word embedding vectors. e expanded query language model p(w |q ) is estimated as:,null,null",null,null
180,"179,""p(w |q ) , pML (w |q) + (1 -  )p(w |q)"",null,null",null,null
181,"180,(9),null,null",null,null
182,"181,""where pML (w |q) denotes maximum likelihood estimation of the original query and  is a free hyper-parameter that controls the weight of original query model in the expanded model. e probability p(w |q) is calculated based on the trained word embedding vectors. In our rst model, this probability can be estimated using Equation (5); while in the second model, we should simply use the Bayes rule given Equation (7) to estimate this probability. However, since we do not have any information about the probability of each"",null,null",null,null
183,"182,4h p://tensor ow.org/ 5h p://plg.uwaterloo.ca/gvcormac/clueweb09spam/,null,null",null,null
184,"183,509,null,null",null,null
185,"184,Session 4C: Queries and Query Analysis,null,null",null,null
186,"185,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
187,"186,Table 1: Collections statistics.,null,null",null,null
188,"187,ID AP Robust,null,null",null,null
189,"188,GOV2,null,null",null,null
190,"189,ClueWeb,null,null",null,null
191,"190,collection Associated Press 88-89 TREC Disks 4 & 5 minus Congressional Record,null,null",null,null
192,"191,2004 crawl of .gov domains,null,null",null,null
193,"192,ClueWeb 09 - Category B,null,null",null,null
194,"193,""queries (title only) TREC 1-3 Ad-Hoc Track, topics 51-200"",null,null",null,null
195,"194,""TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 2004-2006 Terabyte Track,"",null,null",null,null
196,"195,topics 701-850 TREC 2009-2012 Web Track,null,null",null,null
197,"196,topics 1-200,null,null",null,null
198,"197,#docs 165k 528k,null,null",null,null
199,"198,25m,null,null",null,null
200,"199,50m,null,null",null,null
201,"200,avg doc length 287 254,null,null",null,null
202,"201,648,null,null",null,null
203,"202,1506,null,null",null,null
204,"203,""#qrels 15,838 17,412"",null,null",null,null
205,"204,""26,917"",null,null",null,null
206,"205,""18,771"",null,null",null,null
207,"206,Table 2: Evaluating relevance-based word embeddings in the context of query expansion. e superscripts 0/1/2/3/4 denote that the MAP improvements over MLE/word2vec-external/word2vec-target/GloVe-external/GloVe-target are statistically signi cant. e highest value in each row is marked in bold.,null,null",null,null
208,"207,Collection AP,null,null",null,null
209,"208,Robust GOV2 ClueWeb,null,null",null,null
210,"209,Metric,null,null",null,null
211,"210,MAP P@20 NDCG@20,null,null",null,null
212,"211,MAP P@20 NDCG@20,null,null",null,null
213,"212,MAP P@20 NDCG@20,null,null",null,null
214,"213,MAP P@20 NDCG@20,null,null",null,null
215,"214,MLE,null,null",null,null
216,"215,0.2197 0.3503 0.3924,null,null",null,null
217,"216,0.2149 0.3319 0.3863,null,null",null,null
218,"217,0.2702 0.5132 0.4482,null,null",null,null
219,"218,0.1028 0.3025 0.2237,null,null",null,null
220,"219,word2vec,null,null",null,null
221,"220,external target,null,null",null,null
222,"221,0.2399 0.3688 0.4030,null,null",null,null
223,"222,0.2420 0.3738 0.4181,null,null",null,null
224,"223,0.2218 0.3357 0.3918,null,null",null,null
225,"224,0.2215 0.3337 0.3881,null,null",null,null
226,"225,0.2740 0.5257 0.4571,null,null",null,null
227,"226,0.2723 0.5172 0.4509,null,null",null,null
228,"227,0.1033 0.3040 0.2235,null,null",null,null
229,"228,0.1033 0.3053 0.2252,null,null",null,null
230,"229,GloVe,null,null",null,null
231,"230,external target,null,null",null,null
232,"231,0.2319 0.3581 0.4025,null,null",null,null
233,"232,0.2389 0.3631 0.4098,null,null",null,null
234,"233,0.2209 0.3345 0.3918,null,null",null,null
235,"234,0.2172 0.3281 0.3844,null,null",null,null
236,"235,0.2718 0.5186 0.4539,null,null",null,null
237,"236,0.2709 0.5128 0.4485,null,null",null,null
238,"237,0.1029 0.3033 0.2244,null,null",null,null
239,"238,0.1026 0.3048 0.2244,null,null",null,null
240,"239,Rel.-based Embedding,null,null",null,null
241,"240,RLM,null,null",null,null
242,"241,RPE,null,null",null,null
243,"242,0.258001234 0.388601234 0.424201234,null,null",null,null
244,"243,0.245001234 0.347601234 0.398201234,null,null",null,null
245,"244,0.286701234 0.536701234 0.45760234,null,null",null,null
246,"245,0.106601234,null,null",null,null
247,"246,0.3073 0.227301,null,null",null,null
248,"247,0.254301234 0.3812034 0.422601234,null,null",null,null
249,"248,0.237201234 0.3409024 0.39550,null,null",null,null
250,"249,0.285501234 0.535801234 0.4557024,null,null",null,null
251,"250,0.1031,null,null",null,null
252,"251,0.3030,null,null",null,null
253,"252,0.2241,null,null",null,null
254,"253,""term given a query, we use the uniform distribution. For other word embedding models (i.e., word2vec and GloVe), we use the standard method described in [11]. For all the models, we ignore the terms whose embedding vectors are not available."",null,null",null,null
255,"254,""We retrieve the documents for the expanded query language model using the KL-divergence formula [18] with Dirichlet prior smoothing (µ , 1500) [45]. All the retrieval experiments were carried out using the Galago toolkit [7]."",null,null",null,null
256,"255,""In all the experiments, the parameters  (the linear interpolation coe cient) and m (the number of expansion terms) were set using 2-fold cross-validation over the queries in each collection. We selected the parameter  from {0.1, . . . , 0.9} and the parameter m from {10, 20, ..., 100}."",null,null",null,null
257,"256,""4.2.3 Evaluation Metrics. To evaluate the e ectiveness of query expansion models, we report three standard evaluation metrics: mean average precision (MAP) of the top ranked 1000 documents, precision of the top 20 retrieved documents (P@20), and normalized discounted cumulative gain [13] calculated for the top 20 retrieved documents (nDCG@20). Statistically signi cant di erences of MAP, P@20, and nDCG@20 values based on the two-tailed paired t-test are computed at a 95% con dence level (i.e., p alue < 0.05)."",null,null",null,null
258,"257,""4.2.4 Results and Discussion. To evaluate our models, we consider the following baselines: (i) the standard maximum likelihood estimation (MLE) of the query model without query expansion, (ii) two sets of embedding vectors (one trained on Google News as a"",null,null",null,null
259,"258,""large external corpus and one trained on the target retrieval collection) learned by the word2vec model6 [24], and (iii) two sets of embedding vectors (one trained on Wikipedia 2004 plus Gigawords 5 as a large external corpus7 and the other on the target retrieval collection) learned by the GloVe model [28]."",null,null",null,null
260,"259,""Table 2 reports the results achieved by the proposed models and the baselines. According to this table, all the query expansion models outperform the MLE baseline in nearly all cases, which indicates the e ectiveness of employing high-dimensional word representations for query expansion. Similar observations have been made in [11, 17, 40, 41]. According to the results, although word2vec performs slightly be er than GloVe, no signi cant di erences can be observed between their performances. According to Table 2, both relevance-based embedding models outperform all the baselines in all the collections, which shows the importance of taking relevance into account for training embedding vectors. ese improvements are o en statistically signi cant compared to all the baselines. e relevance likelihood maximization model (RLM) performs be er than the relevance posterior estimation model (RPE) in all cases and the reason is related to their objective function. RLM learns the relevance distribution for all terms, while RPE learns the classi cation probability of being relevance for vocabulary terms (see Equations (5) and (7))."",null,null",null,null
261,"260,6We use the CBOW implementation of the word2vec model.,null,null",null,null
262,"261,also performs similarly. 7Available at h p://nlp.stanford.edu/projects/glove/.,null,null",null,null
263,"262,e skip-gram model,null,null",null,null
264,"263,510,null,null",null,null
265,"264,Session 4C: Queries and Query Analysis,null,null",null,null
266,"265,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
267,"266,""Table 3: Top 10 expansion terms obtained by the word2vec and the relevance-based word embedding models for two sample queries """"indian american museum"""" and """"tibet protesters""""."",null,null",null,null
268,"267,""query: """"indian american museum"""""",null,null",null,null
269,"268,word2vec,null,null",null,null
270,"269,Rel.-based Embedding,null,null",null,null
271,"270,external,null,null",null,null
272,"271,target,null,null",null,null
273,"272,RLM,null,null",null,null
274,"273,RPE,null,null",null,null
275,"274,history,null,null",null,null
276,"275,powwows,null,null",null,null
277,"276,chumash,null,null",null,null
278,"277,heye,null,null",null,null
279,"278,art,null,null",null,null
280,"279,smithsonian heye,null,null",null,null
281,"280,collection,null,null",null,null
282,"281,culture,null,null",null,null
283,"282,afro,null,null",null,null
284,"283,artifacts,null,null",null,null
285,"284,chumash,null,null",null,null
286,"285,british,null,null",null,null
287,"286,mesoamerica smithsonian smithsonian,null,null",null,null
288,"287,heritage,null,null",null,null
289,"288,smithsonians collection,null,null",null,null
290,"289,york,null,null",null,null
291,"290,society,null,null",null,null
292,"291,native,null,null",null,null
293,"292,washington new,null,null",null,null
294,"293,states,null,null",null,null
295,"294,heye,null,null",null,null
296,"295,institution,null,null",null,null
297,"296,apa,null,null",null,null
298,"297,contemporary hopi,null,null",null,null
299,"298,york,null,null",null,null
300,"299,native,null,null",null,null
301,"300,part,null,null",null,null
302,"301,mayas,null,null",null,null
303,"302,native,null,null",null,null
304,"303,americans,null,null",null,null
305,"304,united,null,null",null,null
306,"305,cimam,null,null",null,null
307,"306,apa,null,null",null,null
308,"307,history,null,null",null,null
309,"308,""To get a sense of what is learned by each of the embedding models8, in Table 3 we report the top 10 expansion terms for two sample queries from the Robust collection. According to this table, the terms added to the query by the word2vec model are syntactically or semantically related to individual query terms, which is expected. For the query """"indian american museum"""" as an example, the terms """"history"""", """"art"""", and """"culture"""" are related to the query term """"museum"""", while the terms """"united"""" and """"states"""" are related to the query term """"american"""". In contrast, looking at the expansion terms obtained by the relevance-based word embeddings, we can see that some relevant terms to the whole query were selected. For instance, """"chumash"""" (a group of native americans)9, """"heye"""" (the national museum of the American Indian in New York), """"smithsonian"""" (the national museum of the American Indian in Washington DC), and """"apa"""" (the American Psychological Association that actively promotes American Indian museums). A similar observation can be made for the other sample query (i.e., """"tibet protesters""""). For example, the word """"independence"""" is related to the whole query that was only selected by the relevance-based word embedding models, while the terms """"protestors"""", """"protests"""", """"protest"""", and """"protesting"""" that are syntactically similar to the query term """"protesters"""" were considered by the word2vec model. We believe that these di erences are due to the learning objective of the models. Interestingly, the expansion terms added to each query by the two relevance-based models look very similar, but according to Table 2, their performances are quite di erent. e reason is related to the weights given to each term by the two models. e weights given to the expansion terms by RPE are very close to each other because its objective is to just classify each term and all of these terms are classi ed with a high probability as """"relevant""""."",null,null",null,null
310,"309,""In the next set of experiments, we consider the methods that use the top retrieved documents for query expansion: the relevance model (RM3) [1, 20] as a state-of-the-art pseudo-relevance feedback model, and the local embedding approach recently proposed by Diaz et al. [11] with the general idea of training word embedding models on the top ranked documents retrieved in response to a given query. Similar to [11], we use the word2vec model to train"",null,null",null,null
311,"310,""8For the sake of space, we only report the expanded terms estimated by the word2vec model and the proposed models. 9see h ps://en.wikipedia.org/wiki/Chumash people"",null,null",null,null
312,"311,""query: """"tibet protesters"""""",null,null",null,null
313,"312,word2vec,null,null",null,null
314,"313,Rel.-based Embedding,null,null",null,null
315,"314,external,null,null",null,null
316,"315,target,null,null",null,null
317,"316,RLM,null,null",null,null
318,"317,RPE,null,null",null,null
319,"318,demonstrators tibetan,null,null",null,null
320,"319,tibetan,null,null",null,null
321,"320,tibetan,null,null",null,null
322,"321,protestors,null,null",null,null
323,"322,lhasa,null,null",null,null
324,"323,lama,null,null",null,null
325,"324,tibetans,null,null",null,null
326,"325,tibetan,null,null",null,null
327,"326,demonstrators tibetans,null,null",null,null
328,"327,lama,null,null",null,null
329,"328,protests,null,null",null,null
330,"329,tibetans,null,null",null,null
331,"330,lhasa,null,null",null,null
332,"331,independence,null,null",null,null
333,"332,tibetans,null,null",null,null
334,"333,marchers,null,null",null,null
335,"334,dalai,null,null",null,null
336,"335,lhasa,null,null",null,null
337,"336,protest,null,null",null,null
338,"337,lhasas,null,null",null,null
339,"338,independence dalai,null,null",null,null
340,"339,activists,null,null",null,null
341,"340,jokhang,null,null",null,null
342,"341,protest,null,null",null,null
343,"342,open,null,null",null,null
344,"343,protesting,null,null",null,null
345,"344,demonstrations open,null,null",null,null
346,"345,protest,null,null",null,null
347,"346,lhasa,null,null",null,null
348,"347,dissidents,null,null",null,null
349,"348,zone,null,null",null,null
350,"349,zone,null,null",null,null
351,"350,demonstrations barkhor,null,null",null,null
352,"351,followers,null,null",null,null
353,"352,jokhang,null,null",null,null
354,"353,Table 4: Evaluating relevance-based word embedding in pseudo-relevance feedback scenario. e superscripts 1/2/3 denote that the MAP improvements over RM3/Local Embedding/ERM with Local Embedding are statistically signi cant.,null,null",null,null
355,"354,e highest value in each row is marked in bold.,null,null",null,null
356,"355,Collection AP,null,null",null,null
357,"356,Robust GOV2 ClueWeb,null,null",null,null
358,"357,Metric,null,null",null,null
359,"358,MAP P@20 NDCG@20,null,null",null,null
360,"359,MAP P@20 NDCG@20,null,null",null,null
361,"360,MAP P@20 NDCG@20,null,null",null,null
362,"361,MAP P@20 NDCG@20,null,null",null,null
363,"362,RM3,null,null",null,null
364,"363,0.2927 0.4034 0.4368,null,null",null,null
365,"364,0.2593 0.3486 0.4011,null,null",null,null
366,"365,0.2863 0.5318 0.4503,null,null",null,null
367,"366,0.1079 0.3111 0.2309,null,null",null,null
368,"367,Local,null,null",null,null
369,"368,Emb.,null,null",null,null
370,"369,0.2412 0.3742 0.4173,null,null",null,null
371,"370,0.2235 0.3366 0.3868,null,null",null,null
372,"371,0.2748 0.5271 0.4576,null,null",null,null
373,"372,0.1041 0.3062 0.2261,null,null",null,null
374,"373,ERM,null,null",null,null
375,"374,Local RLM,null,null",null,null
376,"375,0.3047 0.4105 0.4411,null,null",null,null
377,"376,0.2643 0.3498 0.4080,null,null",null,null
378,"377,0.2924 0.5379 0.4584,null,null",null,null
379,"378,0.311912 0.423312 0.4495123,null,null",null,null
380,"379,0.2761123 0.3605123 0.4173123,null,null",null,null
381,"380,0.2986123 0.541712 0.4603123,null,null",null,null
382,"381,0.1094 0.112112,null,null",null,null
383,"382,0.3145 0.3168 0.2328 0.23602,null,null",null,null
384,"383,""word embedding vectors on top 1000 documents. e results are reported in Table 4. In this table, ERM refers to the embedding-based relevance model recently proposed by Zamani and Cro [40] in order to make use of semantic similarities estimated based on the word embedding vectors in a pseudo-relevance feedback scenario. According to Table 4, the ERM model that uses the relevance-based word embedding (RLM10) outperforms all the other methods. ese improvements are statistically signi cant in most cases. By comparing the results obtained by local embedding and those reported in Table 2, it can be observed that there are no substantial di erences between the results for local embedding and word2vec. is is similar to what is reported by Diaz et al. [11] when the embedding vectors are trained on the top documents in the target collection, similar to our se ing. Note that the relevance-based model was also trained on the target collection."",null,null",null,null
385,"384,""10For the sake of space, we only consider RLM which shows be er performance compared to RPE in query expansion."",null,null",null,null
386,"385,511,null,null",null,null
387,"386,Session 4C: Queries and Query Analysis,null,null",null,null
388,"387,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
389,"388,MAP,null,null",null,null
390,"389,0.06 0.07 0.08 0.09 0.1 0.23 0.24 0.25 0.26,null,null",null,null
391,"390,0.30,null,null",null,null
392,"391,0.25 0.20,null,null",null,null
393,"392,0.15 ,null,null",null,null
394,"393,MAP,null,null",null,null
395,"394, AP Robust GOV2 ClueWeb,null,null",null,null
396,"395,5,null,null",null,null
397,"396,10,null,null",null,null
398,"397,15,null,null",null,null
399,"398,20,null,null",null,null
400,"399,25,null,null",null,null
401,"400,# expansion terms,null,null",null,null
402,"401,0.10 0.05 0.00,null,null",null,null
403,"402,0.0,null,null",null,null
404,"403, AP Robust GOV2 ClueWeb,null,null",null,null
405,"404,0.2,null,null",null,null
406,"405,0.4,null,null",null,null
407,"406,0.6,null,null",null,null
408,"407,0.8,null,null",null,null
409,"408,1.0,null,null",null,null
410,"409,interpolation coefficient,null,null",null,null
411,"410,(a) # expansion terms,null,null",null,null
412,"411,(b) interpolation coe cient,null,null",null,null
413,"412,""Figure 2: Sensitivity of RLM to the number of expansion terms and the interpolation coe cient (), in terms of MAP."",null,null",null,null
414,"413,0.29,null,null",null,null
415,"414,MAP,null,null",null,null
416,"415,0.06 0.08 0.1 0.18 0.2 0.22 0.24 0.26 0.28,null,null",null,null
417,"416,0.27,null,null",null,null
418,"417,0.25,null,null",null,null
419,"418,MAP,null,null",null,null
420,"419,0.09 0.1 0.11,null,null",null,null
421,"420,0.07,null,null",null,null
422,"421, AP Robust GOV2 ClueWeb,null,null",null,null
423,"422,100,null,null",null,null
424,"423,200,null,null",null,null
425,"424,300,null,null",null,null
426,"425,400,null,null",null,null
427,"426,500,null,null",null,null
428,"427,embedding dimension,null,null",null,null
429,"428,""Figure 3: Sensitivity of RLM to the dimension of embedding vectors, in terms of MAP."",null,null",null,null
430,"429,""An interesting observation from Tables 2 and 4 is that the RLM performance (without using pseudo-relevant documents) in Robust and GOV2 is very close to the RM3 performance, and is slightly be er in the GOV2 collection. Note that RM3 needs two retrieval runs11 and uses top retrieved documents, while RLM only needs one retrieval run. is is an important issue in many real-world applications, since the e ciency constraints do not always allow them to have two retrieval runs per query."",null,null",null,null
431,"430,""Parameter Sensitivity. In the next set of experiments, we study the sensitivity of RLM as the best performing word embedding model in Table 2 to the expansion parameters. Figure 2a plots the sensitivity of RLM to the number of expansion terms where the parameter  is set to 0.5. According to this gure, in both newswire collections, the method shows its best performance when the queries are expanded with only 10 words. In the GOV2 collection, 15 words are needed for the method to show its best performance."",null,null",null,null
432,"431,""Figure 2b plots the sensitivity of the methods to the interpolation coe cient  (see Equation 9) where the number of expansion terms is set to 10. According to the curves correspond to AP and Robust, the original query language model needs to be interpolated with the model estimated using relevance-based word embeddings"",null,null",null,null
433,"432,""11Diaz [10] showed that for precision-oriented tasks, the second retrieval run can be restricted to the initial rank list for improving the e ciency of PRF models. However, for recall-oriented metrics, e.g., MAP, the second retrieval helps a lot."",null,null",null,null
434,"433, AP Robust GOV2 ClueWeb,null,null",null,null
435,"434,1,null,null",null,null
436,"435,2,null,null",null,null
437,"436,3,null,null",null,null
438,"437,4,null,null",null,null
439,"438,5,null,null",null,null
440,"439,million queries,null,null",null,null
441,"440,""Figure 4: e Performance of RLM with respect to di erent amount of training data (training queries), in terms of MAP."",null,null",null,null
442,"441,""with equal weights (i.e.,  ,"""" 0.5). is shows the quality of the estimated distribution via the learned embedding vectors. In the GOV2 collection, a higher weight should be given to the original query model, which indicates that the original query plays a key role in achieving good retrieval performance in this collection."""""",null,null",null,null
443,"442,""We also study the performance of RLM as the best performing word embedding model for query expansion with respect to the embedding dimensionality. e results are shown in Figure 3, where the query expansion performance generally improves as we increase the embedding dimensionality. e performances become stable when the dimension is larger than 300. is experiment suggests that 400 dimensions would be enough for the relevance-based embedding model."",null,null",null,null
444,"443,""Due to the large number of parameters in the neural networks, they can require large amounts of training data to achieve good performance. In the next set of experiments, we study how much training data is needed for training our best model. e results are plo ed in Figure 4. According to this gure, by increasing the number of training queries from one million to four million queries, the performance signi cantly increases, and becomes more stable a er four million queries."",null,null",null,null
445,"444,4.3 Evaluation via ery Classi cation,null,null",null,null
446,"445,""In this subsection, we evaluate the proposed embedding models in the context of query classi cation. In this task, each query is"",null,null",null,null
447,"446,512,null,null",null,null
448,"447,Session 4C: Queries and Query Analysis,null,null",null,null
449,"448,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
450,"449,Table 5: Evaluating embedding algorithms via query classication. e superscripts 1/2 denote that the improvements,null,null",null,null
451,"450,over word2vec/GloVe are signi cant. e highest value in each column is marked in bold.,null,null",null,null
452,"451,Method word2vec GloVe Rel.-based Embedding - RLM Rel.-based Embedding - RPE,null,null",null,null
453,"452,Precision,null,null",null,null
454,"453,0.3712,null,null",null,null
455,"454,0.3643 0.394312 0.396112,null,null",null,null
456,"455,F1-measure,null,null",null,null
457,"456,0.4008,null,null",null,null
458,"457,0.3912 0.426712 0.429412,null,null",null,null
459,"458,assigned to a number of labels (categories) which are pre-de ned and a few training queries are available for each label. is is a supervised multi-label classi cation task with li le training data.,null,null",null,null
460,"459,""4.3.1 Data. We consider the dataset that was introduced in KDD Cup 2005 [22] for the internet user search query categorization task and was previously used in [41] for evaluating query embedding vectors. is dataset contains 800 web queries submi ed by real users randomly collected from the MSN search logs. e queries do not contain """"junk"""" text or non-English terms. e queries were labelled by three human editors. 67 categories were pre-de ned and up to 5 labels were selected for each query by each editor."",null,null",null,null
461,"460,""4.3.2 Experimental Setup. In our experiments, we performed 5-fold cross-validation over the queries and the reported results are the average of those obtained over the test folds. In all experiments, the spelling errors in queries were corrected in a pre-processing phase, the stopwords were removed from queries (using the INQUERY stopword list), and no stemming was performed."",null,null",null,null
462,"461,""To classify each query, we consider a very simple kNN-based approach proposed in [41]. We rst compute the probability of each category/label given each query q and then select the top t categories with the highest probabilities. e probability p(Ci |q) is computed as follows:"",null,null",null,null
463,"462,""p(Ci |q) ,""""  (Ci , q)   (Ci , q)"""""",null,null",null,null
464,"463,(10),null,null",null,null
465,"464,""j  (Cj , q)"",null,null",null,null
466,"465,""where Ci denotes the ith category. Ci is the centroid vector of all query embedding vectors with the label of Ci in the training set. We ignore the query terms whose embedding vectors are not available. e number of labels assigned to each query was tuned on the training set from {1, 2, 3, 4, 5}. In the query classi cation experiments, we trained relevance-based word embedding using Robust as the collection."",null,null",null,null
467,"466,""4.3.3 Evaluation Metrics. We consider two evaluation metrics that were also used in KDD Cup 2005 [22]: precision and F1measure. Since the labels assigned by the three human editors di er in some cases, all the label sets should be taken into account."",null,null",null,null
468,"467,ese metrics are computed in the same way as what is described in [22] for evaluating the KDD Cup 2005 submi ed runs. Statistically signi cant di erences are determined using the two-tailed paired t-test computed at a 95% con dence level (p - alue < 0.05).,null,null",null,null
469,"468,""4.3.4 Results and Discussion. We compare our models against the word2vec and GloVe methods trained on the external collections that are described in the query expansion experiments. e results are reported in Table 5, where the relevance-based embedding"",null,null",null,null
470,"469,0.430,null,null",null,null
471,"470,F1-measure,null,null",null,null
472,"471,0.428,null,null",null,null
473,"472,0.426,null,null",null,null
474,"473,0.424,null,null",null,null
475,"474,0.422,null,null",null,null
476,"475,0.420,null,null",null,null
477,"476, RLM RPE,null,null",null,null
478,"477,100,null,null",null,null
479,"478,200,null,null",null,null
480,"479,300,null,null",null,null
481,"480,400,null,null",null,null
482,"481,500,null,null",null,null
483,"482,embedding dimension,null,null",null,null
484,"483,""Figure 5: Sensitivity of the relevance-based embedding models to the embedding dimensionality, in terms of F1measure."",null,null",null,null
485,"484,0.42 0.40,null,null",null,null
486,"485,F1-measure,null,null",null,null
487,"486,0.38 ,null,null",null,null
488,"487,0.36,null,null",null,null
489,"488,0.34,null,null",null,null
490,"489,1,null,null",null,null
491,"490, RLM RPE,null,null",null,null
492,"491,2,null,null",null,null
493,"492,3,null,null",null,null
494,"493,4,null,null",null,null
495,"494,5,null,null",null,null
496,"495,million queries,null,null",null,null
497,"496,""Figure 6: e Performance of relevance-based embedding models with respect to di erent amount of training data (training queries), in terms of F1-measure."",null,null",null,null
498,"497,""models signi cantly outperform the baselines in terms of both metrics. An interesting observation here is that contrary to the query expansion experiments, RPE performs be er than RLM in query classi cation. e reason is that in query expansion the weight of each term is considered in order to generate the expanded query language model. erefore, in addition to the order of terms, their weights should be also e ective for improving the retrieval performance with query expansion. In query classi cation, we only assign a few categories to each query, and thus as long as the order of categories is correct, the similarity values between the queries and the categories do not ma er."",null,null",null,null
499,"498,""In the next set of experiments, we study the performance of our relevance-based word embedding models with respect to the embedding dimensionality. e results are plo ed in Figure 5. According to this gure, the performance is generally improved by increasing the embedding dimensionality, and becomes stable when the dimension is greater than 400. is is similar to our observation in the query expansion experiments. We also study the amount of data needed for training our models in Figure 6. According to this gure, at least 4 million queries are needed in order to learn accurate relevance-based word embeddings. It can be seen from Figure 6 that RLM needs more training data compared to RPE in order to perform well, because by increasing the amount of training data the learning curves of these two models get closer."",null,null",null,null
500,"499,513,null,null",null,null
501,"500,Session 4C: Queries and Query Analysis,null,null",null,null
502,"501,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
503,"502,5 CONCLUSIONS AND FUTURE WORK,null,null",null,null
504,"503,""In this paper, we revisited the underlying assumption in typical word embedding models, such as word2vec and GloVe. Instead of learning embedding vectors based on term proximity, we proposed learning embeddings based on the notion of relevance, which is the primary objective in many IR tasks. We developed two neural network-based models for learning relevance-based word embeddings. e rst model, the relevance likelihood maximization model, aims to estimate the probability of each word in a relevance distribution for each query, while the second one, the relevance posterior estimation model, classi es each term as belonging to relevant or non-relevant class for each query. We evaluated our models using two sets of extrinsic evaluation: query expansion and query classi cation. e query expansion experiments using four standard TREC collections, two newswire and two large-scale web collections, suggested that the relevance-based word embedding models outperform state-of-the-art word embedding algorithms. We showed that the expansion terms chosen by our models are related to the whole query, while those chosen by typical word embedding models are related to individual query terms. e query classi cation experiments also validated these ndings and investigated the e ectiveness of our models."",null,null",null,null
505,"504,""In the future, we intend to evaluate the learned embedding models in other IR tasks, such as query reformulation, query intent prediction, etc. We can also achieve more accurate relevance-based embedding vectors by considering the clicked documents for training query, instead of or in addition to the top retrieved documents."",null,null",null,null
506,"505,""Acknowledgements. e authors thank Daniel Cohen, Mostafa Dehghani, and Qingyao Ai for their invaluable comments. is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions,"",null,null",null,null
507,"506,ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.,null,null",null,null
508,"507,REFERENCES,null,null",null,null
509,"508,""[1] Nasreen Abdul-jaleel, James Allan, W. Bruce Cro , Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In TREC '04."",null,null",null,null
510,"509,""[2] Qingyao Ai, Liu Yang, Jiafeng Guo, and W. Bruce Cro . 2016. Analysis of the Paragraph Vector Model for Information Retrieval. In ICTIR '16. 133­142."",null,null",null,null
511,"510,""[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357­389."",null,null",null,null
512,"511,[4] P. D. Bruza and D. Song. 2002. Inferring ery Models by Computing Information Flow. In CIKM '02. 260­269.,null,null",null,null
513,"512,[5] Stephane Clinchant and Florent Perronnin. 2013. Aggregating Continuous Word Embeddings for Information Retrieval. In CVSC@ACL '13. 100­109.,null,null",null,null
514,"513,""[6] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. E cient and E ective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (2011), 441­465."",null,null",null,null
515,"514,""[7] Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company."",null,null",null,null
516,"515,""[8] Sco Deerwester, Susan T. Dumais, George W. Furnas, omas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. 41, 6 (1990), 391­407."",null,null",null,null
517,"516,""[9] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In SIGIR '17."",null,null",null,null
518,"517,""[10] Fernando Diaz. 2015. Condensed List Relevance Models. In ICTIR '15. 313­316. [11] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with"",null,null",null,null
519,"518,Locally-Trained Word Embeddings. In ACL '16. [12] Michael U. Gutmann and Aapo Hyva¨rinen. 2012. Noise-contrastive Estimation of,null,null",null,null
520,"519,""Unnormalized Statistical Models, with Applications to Natural Image Statistics. J. Mach. Learn. Res. 13, 1 (2012), 307­361. [13] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446."",null,null",null,null
521,"520,[14] Yufeng Jing and W. Bruce Cro . 1994. An Association esaurus for Information Retrieval. In RIAO '94. 146­160.,null,null",null,null
522,"521,[15] Tom Kenter and Maarten de Rijke. 2015. Short Text Similarity with Word Embeddings. In CIKM '15. 1411­1420.,null,null",null,null
523,"522,""[16] Ma J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings to Document Distances. In ICML '15. 957­966."",null,null",null,null
524,"523,""[17] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. ery Expansion Using Word Embeddings. In CIKM '16. 1929­1932."",null,null",null,null
525,"524,""[18] John La erty and Chengxiang Zhai. 2001. Document Language Models, ery Models, and Risk Minimization for Information Retrieval. In SIGIR '01. 111­119."",null,null",null,null
526,"525,""[19] Victor Lavrenko, Martin Choque e, and W. Bruce Cro . 2002. Cross-lingual Relevance Models. In SIGIR '02. 175­182."",null,null",null,null
527,"526,[20] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR '01. 120­127.,null,null",null,null
528,"527,[21] Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. In NIPS '14. 2177­2185.,null,null",null,null
529,"528,""[22] Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005. KDD CUP-2005 Report: Facing a Great Challenge. SIGKDD Explor. Newsl. 7, 2 (2005), 91­99."",null,null",null,null
530,"529,""[23] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang. 2015. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classi cation and Information Retrieval. In NAACL '15. 912­921."",null,null",null,null
531,"530,""[24] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS '13. 3111­3119."",null,null",null,null
532,"531,[25] Andriy Mnih and Geo rey E Hinton. 2009. A Scalable Hierarchical Distributed Language Model. In NIPS '09. 1081­1088.,null,null",null,null
533,"532,[26] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model. In AISTATS '05. 246­252.,null,null",null,null
534,"533,""[27] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In InfoScale '06."",null,null",null,null
535,"534,""[28] Je rey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP '14. 1532­1543."",null,null",null,null
536,"535,[29] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In SIGIR '98. 275­281.,null,null",null,null
537,"536,""[30] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In SIGIR '17."",null,null",null,null
538,"537,""[31] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In CIKM '16. 711­720."",null,null",null,null
539,"538,[32] J. J. Rocchio. 1971. Relevance Feedback in Information Retrieval. In e SMART Retrieval System: Experiments in Automatic Document Processing. 313­323.,null,null",null,null
540,"539,""[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning representations by back-propagating errors. Nature 323 (Oct. 1986), 533­536."",null,null",null,null
541,"540,""[34] T. Saracevic. 2016. e Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Morgan & Claypool Publishers."",null,null",null,null
542,"541,""[35] Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. 2014. Learning Concept Embeddings for ery Expansion by antum Entropy Minimization. In AAAI '14. 1586­1592."",null,null",null,null
543,"542,[36] Tao Tao and ChengXiang Zhai. 2006. Regularized Estimation of Mixture Models for Robust Pseudo-relevance Feedback. In SIGIR '06. 162­169.,null,null",null,null
544,"543,[37] Ivan Vulic´ and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings. In SIGIR '15. 363­372.,null,null",null,null
545,"544,[38] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global Document Analysis. In SIGIR '96. 4­11.,null,null",null,null
546,"545,""[39] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017. Situational Context for Ranking in Personal Search. In WWW '17. 1531­1540."",null,null",null,null
547,"546,[40] Hamed Zamani and W. Bruce Cro . 2016. Embedding-based ery Language Models. In ICTIR '16. 147­156.,null,null",null,null
548,"547,[41] Hamed Zamani and W. Bruce Cro . 2016. Estimating Embedding Vectors for eries. In ICTIR '16. 123­132.,null,null",null,null
549,"548,""[42] Hamed Zamani, Javid Dadashkarimi, Azadeh Shakery, and W. Bruce Cro . 2016. Pseudo-Relevance Feedback Based on Matrix Factorization. In CIKM '16. 1483­ 1492."",null,null",null,null
550,"549,""[43] ChengXiang Zhai, William W. Cohen, and John La erty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR '03. 10­17."",null,null",null,null
551,"550,[44] Chengxiang Zhai and John La erty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01. 403­410.,null,null",null,null
552,"551,""[45] Chengxiang Zhai and John La erty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (2004), 179­214."",null,null",null,null
553,"552,[46] Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. In SIGIR '15. 575­584.,null,null",null,null
554,"553,""[47] Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for estion Retrieval in Community estion Answering. In ACL '15. 250­259."",null,null",null,null
555,"554,514,null,null",null,null
556,"555,,null,null",null,null

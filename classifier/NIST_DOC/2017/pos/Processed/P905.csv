,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Automatic and Semi-Automatic Document Selection for Technology-Assisted Review,null,null",null,null
4,"3,Maura R. Grossman,null,null",null,null
5,"4,University of Waterloo,null,null",null,null
6,"5,Gordon V. Cormack,null,null",null,null
7,"6,University of Waterloo,null,null",null,null
8,"7,Adam Roegiest,null,null",null,null
9,"8,Kira Systems Inc.,null,null",null,null
10,"9,ABSTRACT,null,null",null,null
11,"10,""In the TREC Total Recall Track (2015-2016), participating teams could employ either fully automatic or human-assisted (""""semi-automatic"""") methods to select documents for relevance assessment by a simulated human reviewer. According to the TREC 2016 evaluation, the fully automatic baseline method achieved a recall-precision breakeven (""""R-precision"""") score of 0.71, while the two semi-automatic efforts achieved scores of 0.67 and 0.51. In this work, we investigate the extent to which the observed effectiveness of the different methods may be confounded by chance, by inconsistent adherence to the Track guidelines, by selection bias in the evaluation method, or by discordant relevance assessments. We find no evidence that any of these factors could yield relative effectiveness scores inconsistent with the official TREC 2016 ranking."",null,null",null,null
12,"11,""ACM Reference format: Maura R. Grossman, Gordon V. Cormack, and Adam Roegiest. 2017. Automatic and Semi-Automatic Document Selection for Technology-Assisted Review. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. http://dx.doi.org/10.1145/3077136.3080675"",null,null",null,null
13,"12,1 INTRODUCTION,null,null",null,null
14,"13,""The purpose of the TREC Total Recall Track [1, 5] was to evaluate, through controlled simulation, technology-assisted review (""""TAR"""") methods to achieve very high recall with a human-in-the-loop. Towards this end, the Track provided a web-based server that simulated a human-in-the-loop, by providing (pre-coded) relevance assessments, on a documentby-document basis, in response to requests from participating teams during their completion of the task. The objective of each team was to request assessments for as many relevant documents as possible, while requesting assessments for as few non-relevant documents as possible."",null,null",null,null
15,"14,""Participants' methods were evaluated using the traditional set-based measures of recall and precision, as well as gain curves and a novel family of rank-based measures denoted by the Track coordinators as """"recall@aR+b."""" For brevity, we limit our consideration to the special case of """"recall@R,"""" which is equivalent to the recall-precision breakeven point, or R-precision. The recall-precision breakeven point is the"",null,null",null,null
16,"15,""Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5022-8/17/08. http://dx.doi.org/10.1145/3077136.3080675"",null,null",null,null
17,"16,""proportion of documents for which relevant assessments are returned among the first R requests, where R is the number of relevant documents in the collection."",null,null",null,null
18,"17,""At the outset, participants retrieved the document collection from the assessment server, as well as a short topic description. Each run was declared by the participant to be either """"Automatic"""" or """"Manual."""" Automatic runs were permitted no manual intervention whatsoever; the only information available to an Automatic run was the collection, the topic statement, and the results of any assessments requested from the server. Manual runs were permitted unrestricted manual intervention, including, but not limited to, independent research, search of the collection, and human review of documents in the collection. As stated in the Track guidelines, """"[i]f documents are manually reviewed, the same documents must also be submitted to the assessment server, at the time they are reviewed.""""1 One of the two Manual runs submitted to the TREC 2016 Total Recall Track conformed to this requirement; the other did not, in effect availing itself of pre-training not available to the other runs."",null,null",null,null
19,"18,""The pre-coded assessments that were used to simulate human feedback, as well as to evaluate the participating runs, were derived using a process similar to a Manual run, but with real human assessors-in-the-loop. Interactive search and judging [6], as well as two machine-learning methods, were used to identify potentially relevant documents, which were labeled as """"relevant"""" or """"non-relevant,"""" by a team of six assessors supervised by the National Institute of Standards and Technology (""""NIST""""). In total, the assessors reviewed 61,985 documents for relevance to 34 different topics, labeling 36,021 as """"relevant"""" and 25,964 as """"non-relevant."""" For the purposes of feedback and evaluation, the 36,021 documents were deemed """"relevant""""; all others, whether reviewed or not, were deemed """"non-relevant."""""",null,null",null,null
20,"19,""To provide an evaluation standard independent of the primary assessments, a non-uniform statistical sample [2] of 50 documents was drawn from the entire collection for each of the 34 topics; each sample was reviewed by three separate assessors from the same NIST team. The Track coordinators reported separate gain curves using each of these three alternate assessments, as well the majority vote among the three assessments, as a gold standard [1]."",null,null",null,null
21,"20,""Figures 1 and 2, reproduce the gain curves from the TREC 2016 proceedings [1] for the three runs of interest, evaluated using the primary assessments and the majority vote of the three alternate assessments, respectively. The runs of interest for this study are BMI-Desc (the Automatic baseline that achieved recall@R of 0.71), eDiscoveryTeam (the Manual"",null,null",null,null
22,"21,1 http://cormack.uwaterloo.ca/total-recall/2016/guidelines.html.,null,null",null,null
23,"22,905,null,null",null,null
24,"23,Short Research Paper,null,null",null,null
25,"24,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
26,"25,1.0,null,null",null,null
27,"26,1.0,null,null",null,null
28,"27,0.8,null,null",null,null
29,"28,0.8,null,null",null,null
30,"29,0.6,null,null",null,null
31,"30,0.6,null,null",null,null
32,"31,Recall,null,null",null,null
33,"32,Recall,null,null",null,null
34,"33,0.4,null,null",null,null
35,"34,0.4,null,null",null,null
36,"35,0.2 0.0,null,null",null,null
37,"36,0,null,null",null,null
38,"37,5000,null,null",null,null
39,"38,10000,null,null",null,null
40,"39,15000 Effort,null,null",null,null
41,"40,BMI-Desc catres-manual1 eDiscoveryTeam-Run1,null,null",null,null
42,"41,20000,null,null",null,null
43,"42,25000,null,null",null,null
44,"43,30000,null,null",null,null
45,"44,""Figure 1: Gain Curves Showing Recall (Averaged Over 34 Topics) as a Function of the Number of Documents Submitted, for the Athome4 (Jeb Bush) Test Collection."",null,null",null,null
46,"45,1.0,null,null",null,null
47,"46,0.2 0.0,null,null",null,null
48,"47,0,null,null",null,null
49,"48,5000,null,null",null,null
50,"49,10000,null,null",null,null
51,"50,15000 Effort,null,null",null,null
52,"51,BMI-Desc catres-manual1 eDiscoveryTeam-Run1,null,null",null,null
53,"52,20000,null,null",null,null
54,"53,25000,null,null",null,null
55,"54,30000,null,null",null,null
56,"55,""Figure 3: Gain Curves Showing Recall of Documents That Were Unjudged in the Primary Assessment, According to the Majority Vote of the Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Documents Submitted, for the Athome4 (Jeb Bush) Test Collection."",null,null",null,null
57,"56,0.8,null,null",null,null
58,"57,0.6,null,null",null,null
59,"58,Recall,null,null",null,null
60,"59,0.4,null,null",null,null
61,"60,0.2 0.0,null,null",null,null
62,"61,0,null,null",null,null
63,"62,5000,null,null",null,null
64,"63,10000,null,null",null,null
65,"64,15000 Effort,null,null",null,null
66,"65,BMI-Desc catres-manual1 eDiscoveryTeam-Run1,null,null",null,null
67,"66,20000,null,null",null,null
68,"67,25000,null,null",null,null
69,"68,30000,null,null",null,null
70,"69,""Figure 2: Gain Curves Showing Recall According to the Majority Vote of the Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Documents Submitted, for the Athome4 (Jeb Bush) Test Collection."",null,null",null,null
71,"70,""run that achieved recall@R of 0.67), and catres (the Manual run that achieved recall@R of 0.51). A paired t-test indicates that the 95% confidence interval of the difference between the BMI-Desc score and the e-DiscoveryTeam score is between -0.012 and +0.095; in other words, the difference is neither large nor significant. In contrast, the differences between these two scores and the catres score are both significant (  0.0001)."",null,null",null,null
72,"71,""Taken at face value, these results might suggest that there is little to choose between the the Automatic baseline method and one Manual method, and that both are superior to the second Manual method. Such a conclusion, we argue, would be premature without first considering the confounding factors that are investigated in this work. One such factor has already been noted ­ the eDiscoveryTeam run was primed with the result of assessing more than one hundred documents from"",null,null",null,null
73,"72,""the test collection, on average, per topic . For many topics, their """"recall@R"""" results were derived from assessments of many more than R documents [3]. Another factor, suggested by the catres team [4], was that the catres run returned disproportionately fewer documents that had been reviewed by the NIST assessors, raising the question of whether or not these documents might have been coded as """"relevant"""" had they been reviewed, thereby raising catres' score. Finally, discordant relevance assessments may have enured to the benefit of one run, at the expense of others."",null,null",null,null
74,"73,2 EQUALIZING PRIOR REVIEW,null,null",null,null
75,"74,We know of no way to exclude or to account for the effect,null,null",null,null
76,"75,""of the prior review for the eDiscoveryTeam runs. However,"",null,null",null,null
77,"76,we were able to devise a way to afford the other runs an,null,null",null,null
78,"77,advantage equal in magnitude to the prior review conducted,null,null",null,null
79,"78,""by eDiscoveryTeam. Consider, for example Topic 434. eDis-"",null,null",null,null
80,"79,coveryTeam reported having reviewed 83 documents for this,null,null",null,null
81,"80,""topic, and requested labels for only the 38 documents they"",null,null",null,null
82,"81,""believed to be relevant, of which 33 were deemed relevant"",null,null",null,null
83,"82,""by the simulated human assessor. Coincidentally, the offi-"",null,null",null,null
84,"83,""cial value of  was also 38. Accordingly, this run scored"",null,null",null,null
85,"84,recall@R,null,null",null,null
86,"85,"","",null,null",null,null
87,"86,33 38,null,null",null,null
88,"87,"","",null,null",null,null
89,"88,0.87.,null,null",null,null
90,"89,According,null,null",null,null
91,"90,to,null,null",null,null
92,"91,the,null,null",null,null
93,"92,Track,null,null",null,null
94,"93,""guidelines,"",null,null",null,null
95,"94,we,null,null",null,null
96,"95,posit that the score should more properly be interpreted as,null,null",null,null
97,"96,""recall@83,""""0.87. In contrast, if we consider the first 83 docu-"""""",null,null",null,null
98,"97,ments,null,null",null,null
99,"98,in,null,null",null,null
100,"99,the,null,null",null,null
101,"100,catres,null,null",null,null
102,"101,""run,"",null,null",null,null
103,"102,we,null,null",null,null
104,"103,find,null,null",null,null
105,"104,that,null,null",null,null
106,"105,""recall@83,"",null,null",null,null
107,"106,37 38,null,null",null,null
108,"107,"","",null,null",null,null
109,"108,0.97.,null,null",null,null
110,"109,""The Automatic baseline method (""""BMI-Desc"""") achieved an"",null,null",null,null
111,"110,""identical recall@83,0.97."",null,null",null,null
112,"111,""More generally, let us denote by  the number of docu-"",null,null",null,null
113,"112,ments reviewed prior to a Manual run. Some smaller number,null,null",null,null
114,"113,""   of these documents will be deemed """"relevant"""" and, we"",null,null",null,null
115,"114,""assume, submitted to the assessment server. In general, it will"",null,null",null,null
116,"115,""be the case that   , and the next  -  documents will"",null,null",null,null
117,"116,""be considered in computing recall@R, when in fact, R+H-h"",null,null",null,null
118,"117,906,null,null",null,null
119,"118,Short Research Paper,null,null",null,null
120,"119,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
121,"120,Run BMI-Desc eDiscoveryTeam,null,null",null,null
122,"121,catres,null,null",null,null
123,"122,Recall 0.79 0.67 0.61,null,null",null,null
124,"123,Std. Dev. 0.12 0.21 0.22,null,null",null,null
125,"124,p (vs. next) 0.0004 0.05,null,null",null,null
126,"125,""Table 1: Prior-Review Adjusted Recall@R+H-h, Averaged Over 34 Topics."",null,null",null,null
127,"126,Run eDiscoveryTeam,null,null",null,null
128,"127,BMI-Desc catres,null,null",null,null
129,"128,Recall 0.67 0.63 0.51,null,null",null,null
130,"129,Std. Dev. 0.34 0.34 0.34,null,null",null,null
131,"130,p (vs. Next) 0.4 0.01,null,null",null,null
132,"131,""Table 3: Recall@R as Evaluated by the Majority Vote of the Three Alternate Assessments, Averaged Over 34 Topics."",null,null",null,null
133,"132,Run BMI-Desc eDiscoveryTeam,null,null",null,null
134,"133,catres,null,null",null,null
135,"134,Judged 0.88 0.88 0.59,null,null",null,null
136,"135,Precision 0.80 0.83 0.80,null,null",null,null
137,"136,Std. Dev. 0.17 0.17 0.13,null,null",null,null
138,"137,Table 2: Precision of the Top R Results That Were Judged by NIST Assessors. None of the differences are significant ( > 0.05).,null,null",null,null
139,"138,Precision Std. Dev. p (vs. Next),null,null",null,null
140,"139,Judged 0.79,null,null",null,null
141,"140,0.22,null,null",null,null
142,"141,0.0001,null,null",null,null
143,"142,Unjudged 0.10,null,null",null,null
144,"143,0.18,null,null",null,null
145,"144,""Table 4: catres' Precision Among Judged and Unjudged Documents, According to a Sample Reviewed by the Second Author."",null,null",null,null
146,"145,""documents were assessed. We should compare any runs that received a """"head start"""" using the measure recall@ +  - , where  is the number of relevant documents among the first  submitted. In both cases, we are replacing the threshold value of  by a somewhat greater value, affording a controlled comparison, at the expense of altering somewhat the objective function."",null,null",null,null
147,"146,""Table 1 shows the average recall@R+H-h over 34 topics, as well as the standard deviation, and p-values versus the next-ranked run. Recall of both BMI-Desc and catres increase substantially, from 0.51 and 0.71, to 0.61 to 0.79, respectively, when given the benefit of  documents of prior review. With this adjustment, we see that BMI-Desc achieves substantially and significantly higher recall than eDiscoveryTeam, for the same number of documents reviewed, according to the NIST primary assessments. eDiscoveryTeam also achieves significantly higher recall than catres, but the margin is substantially reduced."",null,null",null,null
148,"147,3 SELECTION BIAS,null,null",null,null
149,"148,""In the first R documents, BMI-Desc and eDiscoveryTeam submitted a substantially higher proportion of documents that had been judged by the NIST assessors, as compared to catres. Table 2 shows the proportion of judged documents for each run, and the precision ­ the proportion of relevant documents ­ among that set. It is clear from this table that the catres run achieves similar precision on the judged documents that it returns, but returns fewer judged documents overall, and hence achieves a lower recall@R score. If a substantial number of the unjudged documents were in fact relevant, the catres result would be under-reported."",null,null",null,null
150,"149,""Our first avenue of investigation was to consider the secondary assessments reported by the Track coordinators. If there were a substantial number of relevant unassessed documents returned by some runs and not others, this effect should manifest itself in the results reported with respect to these alternate assessments. Figure 2 shows the gain curve"",null,null",null,null
151,"150,""achieved by the three runs, when evaluated by the majority vote of the three alternate assessors. We see very little difference compared to Figure 1, which is calculated with respect to the primary NIST assessments. It may be the case that eDiscoveryTeam's curve is closer to BMI-Desc, but the overall ordering remains intact."",null,null",null,null
152,"151,""To isolate the effect of unjudged documents, we obtained the raw scoring data from NIST, and calculated the recall of each run, as assessed by the majority vote of the three alternate assessors, considering only documents that were unjudged in the primary assessment. The result is shown as a gain curve in Figure 3. We had no prior hypothesis regarding what this gain curve would show. Somewhat to our surprise, we observed that the relative heights of the curves for the three runs were the same ­ there is no indication that the catres system was better at finding relevant, unjudged, documents than the others."",null,null",null,null
153,"152,""We also computed recall@R based on the majority-vote alternate assessments, the results of which are shown in Table 3. The results are generally consistent with those for the primary assessments, with larger variances, as expected. In this evaluation, eDiscoveryTeam achieves a higher score, but, as for the primary assessments, the difference is not significant. catres achieves lower recall, by a significant margin."",null,null",null,null
154,"153,""While the sampling and alternative assessments were independent of the primary method, the small sample size could miss small but important populations of unjudged relevant documents. For example, for Topic 434, an additional 38 relevant documents, over and above the 38 that were found by NIST, could escape sampling if they were dissimilar to the judged-relevant documents. We would expect, however, over 34 topics, that if this were a systematic issue, at least some such relevant documents would have come to light."",null,null",null,null
155,"154,""To avoid reliance on small samples, we sought further, direct confirmation or refutation of the hypothesis that the catres run included a substantial number of relevant unjudged documents. Towards this end, we reviewed a stratified random sample of ten documents for each topic: five judged"",null,null",null,null
156,"155,907,null,null",null,null
157,"156,Short Research Paper,null,null",null,null
158,"157,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
159,"158,Method BMI-Desc BMI-eDT eDiscoveryTeam,null,null",null,null
160,"159,catres,null,null",null,null
161,"160,Recall 0.74 0.79 0.73 0.62,null,null",null,null
162,"161,Std. Dev. 0.17 0.14 0.22 0.22,null,null",null,null
163,"162,p (vs. eDT) 0.8 0.08 -,null,null",null,null
164,"163,0.002,null,null",null,null
165,"164,""Table 5: Recall@R+H-h, Averaged Over 34 Topics, Evaluated According to the """"Corrected"""" Gold Standard. BMI-Desc is the official TREC baseline run, trained on feedback from the NIST gold standard; BMI-eDT is the same method, trained on feedback from the """"corrected"""" gold standard."",null,null",null,null
166,"165,""documents that were among the first R documents returned by catres, and five unjudged documents that were among the first R documents returned by catres. The review was conducted blind by the second author, who was familiar with the subject matter.2 The results, shown in Table 4 are consistent with the results in Table 2 with regard to precision among judged documents (0.80 vs. 0.79), and show eight times lower precision (0.10) among unjudged documents, leading us to conclude that unjudged relevant documents were not a major factor in the TREC 2016 Total Recall evaluation results."",null,null",null,null
167,"166,4 ASSESSOR DISCORD,null,null",null,null
168,"167,""Since the earliest days of IR evaluation, disputes over rele-"",null,null",null,null
169,"168,vance have raised concerns [7]. The eDiscoveryTeam report,null,null",null,null
170,"169,""[3] suggested that the primary NIST assessments were flawed,"",null,null",null,null
171,"170,""and that, instead, their results should be evaluated according"",null,null",null,null
172,"171,""to their own """"corrected"""" gold standard."",null,null",null,null
173,"172,Confusion matrices reported by eDiscoveryTeam suggest,null,null",null,null
174,"173,that the magnitude of discord between its assessments and,null,null",null,null
175,"174,the primary NIST assessments is well within the bounds of,null,null",null,null
176,"175,what would be expected for independent assessments [8]. Con-,null,null",null,null
177,"176,""sidering our running example of Topic 434, eDiscoveryTeam"",null,null",null,null
178,"177,""reports that, according to their """"corrected"""" gold standard,"",null,null",null,null
179,"178,the NIST assessments contain five false positives and five,null,null",null,null
180,"179,""false negatives. In other words, the overlap (i.e., Jaccard co-"",null,null",null,null
181,"180,efficient),null,null",null,null
182,"181,between,null,null",null,null
183,"182,the,null,null",null,null
184,"183,two,null,null",null,null
185,"184,gold,null,null",null,null
186,"185,standards,null,null",null,null
187,"186,is,null,null",null,null
188,"187,33 43,null,null",null,null
189,"188,"","",null,null",null,null
190,"189,0.77,null,null",null,null
191,"190,­,null,null",null,null
192,"191,much,null,null",null,null
193,"192,higher than the overlap values reported in the literature for,null,null",null,null
194,"193,""informed expert assessors, which have not proven to affect"",null,null",null,null
195,"194,the reliability of ad hoc system evaluation [8].,null,null",null,null
196,"195,""eDiscoveryTeam provided us with a copy of its """"corrected"""" gold standard.3 The average per-topic overlap between the"",null,null",null,null
197,"196,eDiscoveryTeam and primary NIST assessments was calcu-,null,null",null,null
198,"197,lated to be 0.75. Table 5 shows recall@R+H-r for the three,null,null",null,null
199,"198,""methods, evaluated by the eDiscoveryTeam's """"corrected"""" gold"",null,null",null,null
200,"199,standard. Even when BMI-Desc is trained using the pri-,null,null",null,null
201,"200,""mary NIST assessments, it achieves recall comparable to"",null,null",null,null
202,"201,""2The 340 documents and their corresponding assessments are available from the Authors. 3Whether or not eDiscoveryTeam's relevance assessments are more """"correct"""" than the primary NIST assessments is a subjective question that is beyond the scope of this work, and has no bearing on our results."",null,null",null,null
203,"202,""the eDiscoveryTeam run. When trained and evaluated using eDiscoveryTeam's """"corrected"""" assessments, BMI-Desc achieves exactly the same recall ­ 0.79 ­ as when trained and evaluated using the primary NIST assessments. When trained on the primary NIST assessments and evaluated with respect to the eDiscoveryTeam assessments, BMI-Desc yields slightly, but not significantly, higher recall than the eDiscoveryTeam submission: 0.74 vs. 0.73. catres yields insubstantially different results, whether evaluated with respect to the eDiscoveryTeam """"corrected"""" assessments or the primary NIST assessments: 0.62 vs. 0.61."",null,null",null,null
204,"203,""The impact of assessor discord on the reliability of Total Recall system evaluation has not previously been studied; the results above, and the agreement between Figures 1 and 2, suggest that, as with ad hoc retrieval, reliable evaluation of the relative effectiveness of Total Recall systems does not hinge on precise relevance assessments, either for feedback or for evaluation."",null,null",null,null
205,"204,5 CONCLUSIONS,null,null",null,null
206,"205,""Prior to the TREC Total Recall Track, we assumed that the best Manual runs would substantially outperform the best Automatic runs, as they did in previous TREC ad hoc tasks. We were surprised that they did not. For some topics, Manual runs achieved higher recall scores with less effort (discounting prior review), but no Manual method consistently improved on the fully Automatic TREC baseline method. In this study, we sought to determine whether the apparent superiority of the Automatic baseline method at TREC 2016 was real, or attributable to chance, failure to follow the Track guidelines, selection bias, or assessor discord. We found no evidence to suggest that the Manual runs were superior to the Automatic baseline, and we found evidence to suggest that, when review effort was properly controlled, the Automatic baseline method found more relevant documents with less effort than any Manual run in the TREC 2016 Total Recall Track."",null,null",null,null
207,"206,""Based on all currently available evidence, the TREC Automatic baseline method remains the method to beat."",null,null",null,null
208,"207,REFERENCES,null,null",null,null
209,"208,""[1] M. R. Grossman, G. V. Cormack, and A. Roegiest. TREC 2016 Total Recall Track Overview. In TREC 2016."",null,null",null,null
210,"209,""[2] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663­685, 1952."",null,null",null,null
211,"210,""[3] R. C. Losey, J. Sullivan, T. Reichenberger, L. Kuehn, and J. Grant. e-Discovery Team at TREC 2016 Total Recall Track. In TREC 2016."",null,null",null,null
212,"211,""[4] J. Pickens, T. Gricks, B. Hardi, M. Noel, and J. Tredennick. An exploration of Total Recall with multiple manual seedings. In TREC 2016."",null,null",null,null
213,"212,""[5] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. TREC 2015 Total Recall Track Overview. In TREC 2015."",null,null",null,null
214,"213,[6] M. Sanderson and H. Joho. Forming test collections with no system pooling. In SIGIR 2004.,null,null",null,null
215,"214,[7] T. Saracevic. Why is relevance still the basic notion in information science? In ISI 2015.,null,null",null,null
216,"215,""[8] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Informantion Processing & Management, 36(5), 2000."",null,null",null,null
217,"216,908,null,null",null,null
218,"217,,null,null",null,null

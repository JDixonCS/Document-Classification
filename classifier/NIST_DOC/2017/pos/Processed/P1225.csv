,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Resource Papers,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,""Finally, a Downloadable Test Collection of Tweets"",null,null",null,null
4,"3,Royal Sequiera and Jimmy Lin,null,null",null,null
5,"4,""David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada {rdsequie,jimmylin}@uwaterloo.ca"",null,null",null,null
6,"5,ABSTRACT,null,null",null,null
7,"6,""Due to Twi er's terms of service that forbid redistribution of content, creating publicly downloadable collections of tweets for research purposes has been a perpetual problem for the research community. Some collections are distributed by making available the ids of the tweets that comprise the collection and providing tools to fetch the actual content; this approach has scalability limitations. In other cases, evaluation organizers have set up APIs that provide access to collections for speci c tasks, without exposing the underlying content. is is a workable solution, but di cult to sustain over the long term since someone has to maintain the APIs. We have noticed that the non-pro t Internet Archive has been making available for public download captures of the so-called Twi er """"spritzer"""" stream, which is the same source as the Tweets2013 collection used in the TREC 2013 and 2014 Microblog Tracks. We analyzed both datasets in terms of content overlap and retrieval baselines to show that the Internet Archive data can serve as a dropin replacement for the Tweets2013 collection, thereby providing the research community with, nally, a downloadable collection of tweets. Beyond this nding, we also study the impact of tweet deletions over time and how they a ect the test collections."",null,null",null,null
8,"7,1 INTRODUCTION,null,null",null,null
9,"8,""Test collections--comprised of a corpus of documents, a set of information needs, and associated relevance judgments--lie at the heart of the Cran eld Paradigm [2] for information retrieval research. In most cases, researchers can acquire the document collection under study: in the 1990s, these were on physical CD-ROMs or DVDs delivered via postal mail; today, hard drives are shipped instead. What if it were not possible to distribute document collections for research use? One example is a collection of tweets: Twi er's terms of service forbid redistribution of such data. is is not a Twi erspeci c problem, as similar challenges exist with electronic medical records, emails, and a host of other sensitive collections researchers may wish to study."",null,null",null,null
10,"9,""Over the past several years, the community has experimented with and developed alternative evaluation approaches for cases where the distribution of documents is challenging, collectively known as """"Evaluation as a Service"""" (EaaS) [1, 3]. Speci cally for"",null,null",null,null
11,"10,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080667"",null,null",null,null
12,"11,""tweets, TREC organizers have built a search API for researchers to perform evaluation tasks without bulk access to the raw collection [4]; this approach was deployed in both the TREC 2013 and TREC 2014 Microblog Track evaluations."",null,null",null,null
13,"12,""e Internet Archive1--a nonpro t digital library with the mission of providing """"universal access to all knowledge""""--has been making available captures of the so-called Twi er """"spritzer"""" stream (an approximately 1% sample of public posts) for download. Putatively, this is the same source that was used to construct the Tweets2013 collection used in the TREC 2013 and 2014 Microblog Tracks. A natural question, therefore, is how this dataset compares to the o cial Tweets2013 collection and if it can be used as a dropin replacement for evaluation purposes. e main contribution of this paper is in answering these questions: we nd that, yes, the publicly downloadable Internet Archive data is substantially similar to the o cial Tweets2013 collection. We observe around 95% overlap in terms of tweet content, and retrieval baselines on the Internet Archive data yield e ectiveness that is statistically indistinguishable from the o cial API. us, the information retrieval community nally has access to a downloadable collection of tweets for research, obviating the need for the service API."",null,null",null,null
14,"13,""Beyond the contribution of validating a downloadable Twi er test collection, this paper also takes a closer look at deleted tweets."",null,null",null,null
15,"14,""e fact that users can delete their tweets means that any collection is constantly changing, and the size of the collection monotonically decreases over time (since there is no """"undelete"""" option). We present an analysis of deleted tweets in the Tweets2013 collection over the past several years to quantitatively characterize the delete process and to examine e ects on retrieval e ectiveness. We nd that although the collection indeed degrades over time, and almost a h of tweets from the raw Tweets2013 collection have been deleted as of December 31, 2016, these deletes appear to have minimal impact on the integrity of the test collections built on the tweets."",null,null",null,null
16,"15,2 BACKGROUND AND RELATED WORK,null,null",null,null
17,"16,""Restrictions on the redistribution of tweets have long been a hurdle to building test collections for information seeking on social media streams. e TREC Microblog Tracks, which ran from 2011 to 2015, have wrestled with this issue and experimented with two di erent solutions. e track organizers built the Tweets2011 collection that was used in TREC 2011 and 2012 [6]. To circumvent the no-redistribution limitation, the organizers devised a process whereby NIST distributed the ids of the tweets (rather than the tweets themselves). Given these ids and a downloading program developed by the organizers (essentially, a crawler), a participant could """"recreate"""" the collection [5]. Since the downloading program"",null,null",null,null
18,"17,1h ps://archive.org/,null,null",null,null
19,"18,1225,null,null",null,null
20,"19,Short Resource Papers,null,null",null,null
21,"20,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
22,"21,""accessed the twi er.com site directly, the tweets were delivered in accordance with Twi er's terms of service."",null,null",null,null
23,"22,""e """"download it yourself"""" approach successfully addressed the no-redistribution issue for the purposes of a shared evaluation, as evidenced by 59 participating groups in the TREC 2011 Microblog Track (one of the largest ever in the history of TREC). Beyond TREC, this approach has been adopted by other communities for sharing collections of tweets. However, distribution via re-downloading exhibits scalability limitations. In particular, the speed of the downloading program, which has built-in rate limiting (imposed voluntarily for robotic """"politeness""""), sets a practical upper bound on collection size. e Tweets2011 collection originally contained 16 million tweets, which is small by modern standards, especially considering that tweets are short."",null,null",null,null
24,"23,""e Tweets2011 collection also identi ed another issue with tweet collections in general, explored by McCreadie et al. [5]: they degrade over time due to deletes. Based on recrawls of the collection several months a er its original release, the authors concluded that the deletes did not impact the relative e ectiveness of runs submi ed to TREC 2011. However, to our knowledge, the e ects of deletes over much longer periods of time have not be studied."",null,null",null,null
25,"24,""In order to tackle the scalability challenges associated with the """"download it yourself"""" approach, for the TREC 2013 Microblog Track the organizers implemented an evaluation-as-a-service solution. ey gathered a collection of tweets centrally, but instead of distributing the tweets, the organizers provided a service API through which participants could access the tweets to complete the evaluation task. To build the o cial collection, organizers developed an open-source crawler using the twi er4j Java library2 to gather tweets from Twi er's public sample stream,3 colloquially known as the """"spritzer"""" stream. is level of access is available to anyone with a Twi er account and does not require special authorization. e organizers crawled all tweets between February 1 and March 31, 2013, UTC (inclusive). According to the TREC 2013 Microblog Track overview: e collection was gathered from two separate virtual machine instances on Amazon's EC2 service, one on the east coast of the US, and the other on the west coast of the US. e redundant setup guarded against network outages and other operational issues during the collection period. Fortunately, no downtime was experienced, so one of the copies was simply designated as the o cial collection. In total, the organizers reported gathering 259 million tweets, although at the time of the evaluation, the collection behind the API was reduced to 243 million tweets a er the removal of deleted tweets."",null,null",null,null
26,"25,""e API for accessing the Tweets2013 collection provided basic search capabilities using the open-source Lucene search engine. In addition to returning the text of the retrieved tweets, the API returned associated metadata about the time of the post, the user making the post, and other properties such as the number of retweets, whether the tweet was a reply, etc. Although the setup essentially limited participants to reranking tweets, this is not unlike multistage ranking architectures that are common today [9]. Additional meta-evaluations have shown that using the API does not appear to a ect the diversity of the submi ed runs [8] and a retrievability"",null,null",null,null
27,"26,2h p://twi er4j.org/en/index.html 3h ps://dev.twi er.com/docs/streaming-apis,null,null",null,null
28,"27,Source,null,null",null,null
29,"28,|T | |A| |T  A| |T  A| |T - A| |A - T |,null,null",null,null
30,"29,Count,null,null",null,null
31,"30,""259,035,603 246,615,368 260,382,756 245,268,215 13,767,388"",null,null",null,null
32,"31,""1,347,153"",null,null",null,null
33,"32,""Table 1: Collection statistics, where T represents the raw Tweets2013 collection and A represents the Tweets2013-IA collection from the Internet Archive."",null,null",null,null
34,"33,analysis does not reveal any substantive issues that arise from not having access to the entire collection [7].,null,null",null,null
35,"34,""Although the evaluation-as-a-service approach is a workable solution for some tasks, the biggest challenge of the approach is sustainability over the long term, since someone must ultimately devote resources to the service, manage access, troubleshoot issues, etc. is is an open-ended commitment for the life of the collection: as a point of comparison, TREC test collections from the 1990s are still being used today. It is di cult to imagine anyone supporting the API for two decades. For one, the so ware behind the service will have long become obsolete."",null,null",null,null
36,"35,3 COLLECTION STATISTICS,null,null",null,null
37,"36,""In this paper, we examine two tweet datasets available from the Internet Archive for public download:"",null,null",null,null
38,"37,· ArchiveTeam JSON Download of Twi er Stream 2013-02: h ps://archive.org/details/archiveteam-twi er-stream-2013-02,null,null",null,null
39,"38,· ArchiveTeam JSON Download of Twi er Stream 2013-03: h ps://archive.org/details/archiveteam-twi er-stream-2013-03,null,null",null,null
40,"39,""According to the Internet Archive, the above datasets are:"",null,null",null,null
41,"40,""A simple collection of JSON grabbed from the general Twi er stream, for the purposes of research, history, testing and memory. is is the """"Spritzer"""" version, the most light and shallow of Twi er grabs."",null,null",null,null
42,"41,""Putatively, this is the same source that the Tweets2013 collection was created from. We downloaded these tweets and compared them against the o cial Tweets2013 collection (collected by the organizers). In Table 1 we present some basic collection statistics for the raw Tweets2013 collection, which we denote as T , and the above datasets downloaded from the Internet Archive, which we refer to as Tweets2013-IA and denote as A for short. Note that T is not the collection exposed via the o cial API, since deletes were applied to it before the TREC evaluations."",null,null",null,null
43,"42,""Twi er's streaming API is forma ed in JSON and comprises messages of two types: actual tweet content and delete messages. ese statistics consider tweet JSON messages only. Due to transient network issues, some messages are delivered more than once, and therefore all reported statistics in this paper are on unique counts. For all experiments in this paper, data manipulation is performed using Spark on our Hadoop cluster since the datasets are large; for reference, the raw Tweets2013 collection (including all tweets and deletes) is 107 GB compressed."",null,null",null,null
44,"43,""Overlap statistics between T and A are shown in Table 2. Most importantly, we see that there is approximately 95% overlap in tweet content between the publicly downloadable datasets from"",null,null",null,null
45,"44,1226,null,null",null,null
46,"45,Short Resource Papers,null,null",null,null
47,"46,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
48,"47,Collection,null,null",null,null
49,"48,Overlap,null,null",null,null
50,"49,1 - |(T - A)|/|T | 1 - |(A - T )|/|A|,null,null",null,null
51,"50,94.69% 99.45%,null,null",null,null
52,"51,Table 2: Overlap analysis between the Tweets2013 (T ) and Tweets2013-IA (A) collections.,null,null",null,null
53,"52,""the Internet Archive and the raw Tweets2013 collection. It seems that the la er is nearly a superset of the former, as there are very few tweets in A that are not in T ."",null,null",null,null
54,"53,4 DELETION ANALYSIS,null,null",null,null
55,"54,""Per the Twi er Developer Agreement, one must """"delete content that Twi er reports as deleted or expired"""".4 Alongside the tweet content, Twi er's streaming API also delivers delete messages. is means that, in order to precisely follow the agreement, gathering any Twi er content from the API also incurs an open-ended liability to monitor the stream inde nitely for delete messages."",null,null",null,null
56,"55,""From the perspective of IR evaluation, this means that any collection of tweets is unstable and will degrade over time--the collection size will monotonically decrease, since there is no """"undelete"""" feature. Although McCreadie et al. [5] have previously examined this issue, their analysis was over a much smaller collection and a much shorter time span. Here, we characterize deletes on the raw Tweets2013 collection over a much longer period of time and examine its impact on associated test collections."",null,null",null,null
57,"56,""e deletion data in our analysis come from two long-term crawls of the Twi er spritzer stream from April 2013 through December 2016 (inclusive). Due to occasional crawler failures, we take the union of delete messages observed across both crawls as the """"ground truth"""". e notation D (YY/MM-YY/MM) refers to deletes observed between the speci ed years and months, inclusive. D (13/02-13/03) is observed directly in the raw Tweets2013 collection, while all other deletes come from the sources described above."",null,null",null,null
58,"57,""Deletion statistics are shown in Table 3, where we provide numbers for a few noteworthy periods: We show the count of deletes that are directly observed as part of the collection (in February and March of 2013). e period from February to June 2013 (inclusive) captures the deletes that were applied for the service API made available for TREC 2013 and TREC 2014. Also of interest are the delete aggregates at yearly intervals, i.e., the counts of deletes through the end of 2013, 2014, 2015, and 2016."",null,null",null,null
59,"58,""In Table 3 we also show the e ects of removing the deleted tweets from the raw Tweets2013 collection T and also the Tweets2013-IA collection A. From the table, we see that by the end of 2016, deletes have reduced the collection to 211m for T and 199m for A, down from the original sizes of 259m and 247m, respectively. Figure 1 plots the number of deletes by month on both the raw Tweets2013 collection and the Internet Archive data. Although we do see that the number of deletes drops o a er the initial few months, there is still a substantial number of deletes even years a er the tweets were originally posted. e total size a er applying all deletes is shown in Figure 2; as expected, we see a steady degradation of the collection over time."",null,null",null,null
60,"59,e next obvious question is how these deletes a ect test collections from the TREC 2013 and 2014 Microblog Tracks that have,null,null",null,null
61,"60,4h ps://dev.twi er.com/overview/terms/agreement-and-policy,null,null",null,null
62,"61,Source,null,null",null,null
63,"62,|T |,null,null",null,null
64,"63,|A| |D (13/02-13/03)| |D (13/04-13/06)| |D (13/07-13/12)| |D (14/01-14/12)| |D (15/01-15/12)| |D (16/01-16/12)| |T - D (13/02-13/03)| |A - D (13/02-13/03)| |T - D (13/02-13/06)| |A - D (13/02-13/06)| |T - D (13/02-13/12)| |A - D (13/02-13/12)| |T - D (13/02-14/12)| |A - D (13/02-14/12)| |T - D (13/02-15/12)| |A - D (13/02-15/12)| |T - D (13/02-16/12)| |A - D (13/02-16/12)|,null,null",null,null
65,"64,Count,null,null",null,null
66,"65,""259,035,603 246,615,368 10,631,099"",null,null",null,null
67,"66,""5,091,183 7,197,460 96,98,613 7,928,857 7,496,871 248,404,504 234,337,730 243,313,321 230,893,086 236,115,861 223,695,626 226,417,248 213,997,013 218,488,391 206,068,156 210,991,520 198,571,285"",null,null",null,null
68,"67,""Table 3: Deletion statistics, applying deletes observed in the Twitter """"spritzer"""" stream over time."",null,null",null,null
69,"68,Figure 1: Number of tweets deleted from Tweets2013 and Tweets2013-IA over time.,null,null",null,null
70,"69,""been built on the Tweets2013 data. e answer is shown in Table 4, which lists for various conditions the number of relevant documents and qrels (all judgments in the pool, regardless of relevance) that would have disappeared as a result of the deletes. We see that by the end of 2016, a li le over 5% of the relevant documents would have been deleted. is is a smaller value than the fraction of the entire collection that is deleted, which means that deletes are more likely to a ect non-relevant documents."",null,null",null,null
71,"70,5 RETRIEVAL EXPERIMENTS,null,null",null,null
72,"71,""In our nal set of experiments, we examined the e ectiveness of baseline retrieval techniques on some of the variant collections"",null,null",null,null
73,"72,1227,null,null",null,null
74,"73,Short Resource Papers,null,null",null,null
75,"74,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
76,"75,Figure 2: Size of the Tweets2013 and Tweets2013-IA collection over time a er applying observed deletes.,null,null",null,null
77,"76,Source,null,null",null,null
78,"77,|T - D (13/02-13/12)| |A - D (13/02-13/12)| |T - D (13/02-14/12)| |A - D (13/02-14/12)| |T - D (13/02-15/12)| |A - D (13/02-15/12)| |T - D (13/02-16/12)| |A - D (13/02-16/12)|,null,null",null,null
79,"78,missing reldocs,null,null",null,null
80,"79,""220 (1.12%) 209 (1.06%) 539 (2.74%) 513 (2.61%) 816 (4.15%) 776 (3.95%) 1,095 (5.57%) 1,042 (5.30%)"",null,null",null,null
81,"80,missing qrels,null,null",null,null
82,"81,""1,820 (1.41%) 1,707 (1.32%) 4,456 (3.45%) 4,190 (3.24%) 6,576 (5.09%) 6,193 (4.79%) 8,500 (6.58%) 7,997 (6.19%)"",null,null",null,null
83,"82,Table 4: Deletion statistics over relevance judgments; percentage of total is shown in parentheses.,null,null",null,null
84,"83,""explored above. e reference point for comparison is the o cial ri API, which served a collection of 243 million tweets (taking"",null,null",null,null
85,"84,""into account deletions up until the time the API was deployed for the evaluation). For our experiments, we used exactly the same code base5 as the API, which was built on top of the open-source Lucene search engine (although in our case, we had direct access to the Lucene indexes)."",null,null",null,null
86,"85,""For evaluation, we used 60 topics from TREC 2013 and 55 topics from TREC 2014. Ranking was performed using Lucene's implementation of query-likelihood, just as with the API. Following standard practice, we retrieved up to 1000 hits per topic and measured e ectiveness in terms of average precision (AP) and precision at 30 (P30), the two o cial metrics used in the evaluations. We report results with the o cial original NIST qrels. However, it would certainly be reasonable to remove deleted tweets from the judgments. We do so and report results under the modi ed qrels condition."",null,null",null,null
87,"86,Experimental results are shown in Table 5 for both the original and modi ed qrels. e condition denoted T - D (13/02-13/06) attempts to replicate the data conditions of the o cial API; our results are very close but not exactly the same because we only consider deletes at monthly increments. e condition A - D (13/02-13/06),null,null",null,null
88,"87,5h p://twi ertools.cc/,null,null",null,null
89,"88,Track,null,null",null,null
90,"89,O cial ri API T - D (13/02-13/06) A - D (13/02-13/06) T - D (13/02-16/12) A - D (13/02-16/12),null,null",null,null
91,"90,Original AP P30,null,null",null,null
92,"91,0.3198 0.5278 0.3120 0.5278 0.2951 0.5130 0.2996 0.5220 0.2864 0.5130,null,null",null,null
93,"92,Modi ed,null,null",null,null
94,"93,AP P30,null,null",null,null
95,"94,-,null,null",null,null
96,"95,-,null,null",null,null
97,"96,0.3120 0.5278 0.2951 0.5130,null,null",null,null
98,"97,0.3158 0.5220 0.3013 0.5130,null,null",null,null
99,"98,Table 5: E ectiveness measures on TREC 2013 and 2014 Microblog Track topics over di erent data conditions.,null,null",null,null
100,"99,""represents the best that a researcher can obtain in replicating the ofcial API using publicly available resources. Based on paired t-tests,"",null,null",null,null
101,"100,""we do not nd any signi cant di erences (at p < 0.01) between the o cial ri API and these two data conditions, for both metrics (AP and P30), with either the original qrels or the modi ed qrels."",null,null",null,null
102,"101,""e last two rows in Table 5 show the state of the collection as of December 31, 2016 if all deletes were applied. We also do not nd any signi cant di erences between these two data conditions and the o cial ri API, for both metrics and both qrel conditions."",null,null",null,null
103,"102,6 CONCLUSIONS,null,null",null,null
104,"103,""e Tweets2013 collection, used in the TREC 2013 and TREC 2014 Microblog Tracks, serves as the basis of the most comprehensive evaluation resource for ad hoc retrieval on social media to date. Hampering its availability, however, is the API-based access mechanism. However, courtesy of the Internet Archive, researchers now can directly download tweets from the same period and same source as the o cial Tweets2013 collection. Our analyses con rm that, indeed, the Internet Archive data can serve as a drop-in replacement for evaluation purposes. We share with the community all code and data associated with analyses in this paper as well as instructions for replicating reported data conditions to serve as the basis of future work.6 Finally, researchers now have a downloadable test collection of tweets!"",null,null",null,null
105,"104,REFERENCES,null,null",null,null
106,"105,""[1] Allan Hanbury, Henning Mu¨ller, Krisztian Balog, Torben Brodt, Gordon V. Cormack, Ivan Eggel, Tim Gollub, Frank Hopfgartner, Jayashree Kalpathy-Cramer, Noriko Kando, Anastasia Krithara, Jimmy Lin, Simon Mercer, and Martin Potthast. 2015. Evaluation-as-a-Service: Overview and Outlook. arXiv:1512.07454."",null,null",null,null
107,"106,[2] Donna Harman. 2011. Information Retrieval Evaluation. Morgan & Claypool Publishers.,null,null",null,null
108,"107,""[3] Jimmy Lin and Miles Efron. 2013. Evaluation as a Service for Information Retrieval. SIGIR Forum 47, 2 (2013), 8­14."",null,null",null,null
109,"108,[4] Jimmy Lin and Miles Efron. 2013. Overview of the TREC-2013 Microblog Track. In TREC.,null,null",null,null
110,"109,""[5] Richard McCreadie, Ian Soboro , Jimmy Lin, Craig Macdonald, Iadh Ounis, and Dean McCullough. 2012. On Building a Reusable Twi er Corpus. In SIGIR. 1113­1114."",null,null",null,null
111,"110,""[6] Iadh Ounis, Craig Macdonald, Jimmy Lin, and Ian Soboro . 2011. Overview of the TREC-2011 Microblog Track. In TREC."",null,null",null,null
112,"111,""[7] Jiaul H. Paik and Jimmy Lin. 2016. Retrievability in API-Based """"Evaluation as a Service"""". In ICTIR. 91­94."",null,null",null,null
113,"112,""[8] Ellen M. Voorhees, Jimmy Lin, and Miles Efron. 2014. On Run Diversity in """"Evaluation as a Service"""". In SIGIR. 959­962."",null,null",null,null
114,"113,""[9] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A Cascade Ranking Model for E cient Ranked Retrieval. In SIGIR. 105­114."",null,null",null,null
115,"114,Acknowledgments. is work was supported by the Natural Sciences,null,null",null,null
116,"115,""and Engineering Research Council (NSERC) of Canada, with additional"",null,null",null,null
117,"116,contributions from the U.S. National Science Foundation under IIS-1218043,null,null",null,null
118,"117,and CNS-1405688.,null,null",null,null
119,"118,6h ps://github.com/castorini/Tweets2013-IA,null,null",null,null
120,"119,1228,null,null",null,null
121,"120,,null,null",null,null

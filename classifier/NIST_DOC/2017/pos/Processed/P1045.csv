,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Document Expansion Using External Collections,null,null",null,null
4,"3,Garrick Sherman,null,null",null,null
5,"4,School of Information Sciences University of Illinois at Urbana-Champaign,null,null",null,null
6,"5,gsherma2@illinois.edu,null,null",null,null
7,"6,ABSTRACT,null,null",null,null
8,"7,""Document expansion has been shown to improve the effectiveness of information retrieval systems by augmenting documents' term probability estimates with those of similar documents, producing higher quality document representations. We propose a method to further improve document models by utilizing external collections as part of the document expansion process. Our approach is based on relevance modeling, a popular form of pseudo-relevance feedback; however, where relevance modeling is concerned with query expansion, we are concerned with document expansion. Our experiments demonstrate that the proposed model improves ad-hoc document retrieval effectiveness on a variety of corpus types, with a particular benefit on more heterogeneous collections of documents."",null,null",null,null
9,"8,1 INTRODUCTION,null,null",null,null
10,"9,""Relevance modeling is an extremely influential pseudo-relevance feedback technique in which we assume that both queries and documents are observations sampled from a relevance model (RM) [8], which is a probability distribution over terms in relevant documents. Because we do not have true relevance feedback, relevance modeling makes use of the query likelihood, P (Q |D), to quantify the degree to which words in each document should contribute to the final model R. However, since no document is perfectly representative of its underlying generative model, we may be reasonably concerned that our estimate of P (Q |D) is the result of chance. That is, there is no guarantee that D is a representative sample from R. The quality of our RM, therefore, may benefit from a higher quality document representation than that which is estimated from the text of D."",null,null",null,null
11,"10,""We employ two techniques to attempt to improve our document language models: document expansion and the use of external document collections. Expandeded documents are expected to exhibit less random variation in term frequencies, improving probability estimates. We hope that estimates may be further refined by expanding documents using external collections, thereby avoiding any term bias exhibited by relevant documents in an individual collection."",null,null",null,null
12,"11,Our study differs from prior work in a few important ways. Previous investigations into document expansion have tended to use only,null,null",null,null
13,"12,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080716"",null,null",null,null
14,"13,Miles Efron,null,null",null,null
15,"14,School of Information Sciences University of Illinois at Urbana-Champaign,null,null",null,null
16,"15,mefron@illinois.edu,null,null",null,null
17,"16,""the target collection to expand documents, while our work explores the use of one or more distinct collections. Conversely, most existing work involving external corpora in ad-hoc information retrieval has focused on query expansion; we are interested in incorporating external collections for purposes of document expansion."",null,null",null,null
18,"17,2 RELATED WORK 2.1 Document Expansion in IR,null,null",null,null
19,"18,""Document expansion has been well studied in information retrieval literature, e.g. [10, 11, 13, 16]. For example, Liu & Croft propose a method of retrieval that uses document clusters to smooth document language models [10]. Tao et al. propose a similar approach but place each document at the center of its own cluster; this helps to ensure that the expansion documents are as closely related to the target document as possible [13]."",null,null",null,null
20,"19,""Our approach takes as its starting point that of Efron, Organisciak & Fenlon [4], who issue very short microblog documents as pseudo-queries. They employ a procedure closely related to relevance modeling [8] to expand the original document using those microblog documents retrieved for the pseudo-query. We explore the application and adaptation of their work to different scenarios. First, Efron, Organisciak & Fenlon are concerned with microblog retrieval, in which documents are extremely short--perhaps as small as a keyword query. In contrast, we are interested in performing document expansion with more typical full-length documents, such as those found in news and web corpora. Second, while their work used only the target document collection, we propose an extension of their method that allows for multiple expansion corpora. Finally, we investigate pairing document expansion with query expansion, which their work suggests may be problematic in the microblog domain."",null,null",null,null
21,"20,2.2 Incorporating External Collections,null,null",null,null
22,"21,""The incorporation of external collections into document retrieval is a similarly common theme in the ad-hoc IR literature, particularly with respect to query expansion [2, 3, 9, 15, 17]. Of particular relevance to our work is that of Diaz & Metzler, whose mixture of relevance models is the basis of our Eq. 5 [3]. Their model simply interpolates RMs built on different collections, weighting each by a query-independent quantity P (c). Though our work bears similarities, Diaz & Metzler are interested in query expansion, whereas we apply the technique as one piece in a document expansion model."",null,null",null,null
23,"22,1045,null,null",null,null
24,"23,Short Research Paper,null,null",null,null
25,"24,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
26,"25,3 DOCUMENT EXPANSION PROCEDURE,null,null",null,null
27,"26,3.1 Underlying Retrieval Model,null,null",null,null
28,"27,""Throughout this paper we rely on the language modeling retrieval framework [7]. More specifically, we employ query likelihood (QL) and relevance modeling for ranking."",null,null",null,null
29,"28,""3.1.1 Query Likelihood. Given a query Q and a document D, we rank documents on P (Q |D ), where D is the language model (typically a multinomial over the vocabulary V ) that generated the text of document D. Assuming independence among terms and a uniform distribution over documents, each document is scored by"",null,null",null,null
30,"29,""P (Q |D) ,"",null,null",null,null
31,"30,""P (w |D )c (w,Q )"",null,null",null,null
32,"31,(1),null,null",null,null
33,"32,w Q,null,null",null,null
34,"33,""where c (w,Q ) is the frequency of word w in Q. We follow standard procedures for estimating P (w |D ) in Eq. 1, estimating a smoothed language model by assuming that document language models in a"",null,null",null,null
35,"34,given collection have a Dirichlet prior distribution:,null,null",null,null
36,"35,P^(w |D ),null,null",null,null
37,"36,"","",null,null",null,null
38,"37,""c (w,D) + µP^(w |C) |D| + µ"",null,null",null,null
39,"38,(2),null,null",null,null
40,"39,""where P^(w |C) is the maximum likelihood estimate of the probability of seeing word w in a """"background"""" collection C (typically C is the corpus from which D is drawn), and µ  0 is the smoothing hyperparameter."",null,null",null,null
41,"40,3.1.2 Relevance Modeling. Relevance modeling is a form of pseudo-relevance feedback that uses top ranked documents to estimate a language model representing documents relevant to a query [8].,null,null",null,null
42,"41,""Assuming uniform document prior probabilities, relevance models take the form of"",null,null",null,null
43,"42,""P (w |R) , P (w |D)P (Q |D)"",null,null",null,null
44,"43,(3),null,null",null,null
45,"44,D C,null,null",null,null
46,"45,where P (Q |D) is calculated as in Eq. 1 and essentially weights word w in D by the query likelihood of the document. Relevance models,null,null",null,null
47,"46,are most efficient and robust when calculated over only the top ranked documents and limited to the top terms. These parameters are referred to as f bDocs and f bT erms respectively in Table 1,null,null",null,null
48,"47,below.,null,null",null,null
49,"48,""Because relevance models are prone to query drift, it is often"",null,null",null,null
50,"49,desirable to linearly interpolate an RM with the original query,null,null",null,null
51,"50,model to improve effectiveness:,null,null",null,null
52,"51,""P (w |Q ) , (1 -  )P (w |R) + P (w |Q )."",null,null",null,null
53,"52,(4),null,null",null,null
54,"53,"" is a mixing parameter controlling the influence of the original query. This form of relevance model is known as """"RM3."""""",null,null",null,null
55,"54,3.2 Expanding with Document Pseudo-Queries,null,null",null,null
56,"55,""To expand a document D, we begin by treating the text of D as a pseudo-query which we pose against a collection of documents CE . To transform a document into a pseudo-query we apply two transformations. First we remove all terms from D that appear"",null,null",null,null
57,"56,""in the standard Indri stoplist1. Next, we prune our pseudo-query by retaining only the 0 < k  K most frequent words in the stopped text of D, where K is the total number of unique terms in D. The integer variable k is a parameter that we choose empirically."",null,null",null,null
58,"57,These are the non-stop words with the highest probabilities in a maximum likelihood estimate of D's language model and are,null,null",null,null
59,"58,therefore a reasonable representation of the topic of the document.,null,null",null,null
60,"59,""Though some information may be lost with stopping, with a large enough k we hope to nevertheless capture the general topic of a"",null,null",null,null
61,"60,""document; for example, a document about Hamlet's famous speech"",null,null",null,null
62,"61,""may not be represented by the terms """"to be or not to be,"""" but the"",null,null",null,null
63,"62,""terms """"Shakespeare,"""" """"Hamlet,"""" """"speech,"""" etc. will likely represent the document's subject sufficiently. Let QD be the pseudo-query for D, consisting of the text of D after our two transformations are"",null,null",null,null
64,"63,applied.,null,null",null,null
65,"64,""We rank related documents, called expansion documents, by running QD over a collection CE . More formally, we rank the documents in CE against D using Eq. 1, substituting QD for the query and Ei --the ith expansion document--for the document. Let i be the log-probability for expansion document Ei with respect to D given by Eq. 1."",null,null",null,null
66,"65,""We now have a ranked list of tuples {(E1,1), (E2,2), ..., (EN ,N )} relating expansion document Ei to D with log-probability i . We take the top n documents where 0  n  N . We call these top documents ED and designate them as our expansion documents for D. Finally, we exponentiate each i and normalize our retrieval scores so they sum to 1 over the n retained documents. Assum-"",null,null",null,null
67,"66,""ing a uniform prior over documents, we now have a probability distribution over our n retained documents: P (E|D)."",null,null",null,null
68,"67,""Since this procedure does not depend on the query, we may compute ED once at indexing time and reuse our expansion documents across queries."",null,null",null,null
69,"68,4 DOCUMENT EXPANSION RETRIEVAL MODEL,null,null",null,null
70,"69,We would now like to incorporate our expansion documents into,null,null",null,null
71,"70,a retrieval model over documents. We assume that a query is gen-,null,null",null,null
72,"71,""erated by a mixture of the original document language model D and language models E j representing the expansion documents in each corpus Cj  {C1,C2, ...,Cn }. We assume that E j can be estimated using the text of the expansion documents ED j in corpus Cj . This mixture model may be expressed as:"",null,null",null,null
73,"72,|Q |,null,null",null,null
74,"73,n,null,null",null,null
75,"74,n,null,null",null,null
76,"75,""P^ (Q |D) , (1 -  ED j )P (qi |D) +  ED j P (qi |ED j ) (5)"",null,null",null,null
77,"76,""i ,1"",null,null",null,null
78,"77,""j ,1"",null,null",null,null
79,"78,""j ,1"",null,null",null,null
80,"79,where 0 ,null,null",null,null
81,"80,""n j ,1"",null,null",null,null
82,"81,ED,null,null",null,null
83,"82,j,null,null",null,null
84,"83,1. We estimate P (qi |ED j ),null,null",null,null
85,"84,in expectation:,null,null",null,null
86,"85,""P (qi |ED j ) ,"",null,null",null,null
87,"86,P (qi |E)P (E|D).,null,null",null,null
88,"87,(6),null,null",null,null
89,"88,E ED j,null,null",null,null
90,"89,""Like P (qi |D), we estimate the probability of qi given expansion document E, P (qi |E), as a Dirichlet-smoothed query likelihood. By virtue of our expansion document scoring and normalization, we also have P (E|D). This general model may be used with any number"",null,null",null,null
91,"90,of expansion corpora.,null,null",null,null
92,"91,1 http://www.lemurproject.org/stopwords/stoplist.dft,null,null",null,null
93,"92,1046,null,null",null,null
94,"93,Short Research Paper,null,null",null,null
95,"94,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
96,"95,4.1 Relevance Modeling with Expanded Documents,null,null",null,null
97,"96,""Given our motivating intuition that document expansion allows for the more accurate estimation of document language models, we would expect that an RM computed using expanded documents should be more accurate than a standard RM. We therefore compute an RM3 as in Eqs. 3 and 4, substituting the expanded document for the original."",null,null",null,null
98,"97,5 EVALUATION,null,null",null,null
99,"98,5.1 Data,null,null",null,null
100,"99,""Although Eq. 5 allows for an arbitrary number of collections, for now we limit ourselves to two: the collection that the document appears in (the """"target"""" collection) and Wikipedia2. We expect the latter, as a general encyclopedia, to yield relatively unbiased probability estimates. We build an Indri [12] index over the Wikipedia page text."",null,null",null,null
101,"100,We test our approach using TREC datasets:,null,null",null,null
102,"101,· The AP newswire collection [5] from TREC disks 1 and 2 with topics 101-200.,null,null",null,null
103,"102,""· The robust 2004 [14] topics, numbering 250, from TREC disks 4 and 5."",null,null",null,null
104,"103,· The wt10g collection [1] with the 100 topics from the 2000 and 2001 TREC Web tracks.,null,null",null,null
105,"104,""These datasets provide a good range of collection types, from relatively homogeneous with well-formed documents (AP) to heterogeneous with varied document quality (wt10g)."",null,null",null,null
106,"105,5.2 Runs,null,null",null,null
107,"106,""For each collection, we produce eight runs representing a combination of expansion source and query expansion model. Expansion source refers to the collection(s) used for document expansion, while the query expansion model refers to unexpanded queries (QL) or expanded queries (RM3)."",null,null",null,null
108,"107,We produce runs with expansion documents from:,null,null",null,null
109,"108,""· no expansion, called baseline; · the target collection itself, called self ; · Wikipedia, called wiki; or · a mixture of the previous two, called combined."",null,null",null,null
110,"109,""For each source, both the QL and RM3 variations are compared. Stop words are removed from the query. For efficiency, we re-"",null,null",null,null
111,"110,trieve the top 1000 documents using the default Indri QL implementation and re-rank these documents based on their expanded representations as described in Section 4.,null,null",null,null
112,"111,5.3 Parameters,null,null",null,null
113,"112,""The parameters required for our approach, their meanings, and the values used in our experiments are shown in Table 1."",null,null",null,null
114,"113,""For this work, we set k heuristically. In principle, k may equal the length of the document; however, this would increase computation time significantly, so we have set it to a smaller value for efficiency. The parameter n is also set heuristically; see Section 6.1 for a discussion of the sensitivity of our model to the setting of n."",null,null",null,null
115,"114,2 http://en.wikipedia.org,null,null",null,null
116,"115,Table 1: Parameter settings for the document expansion procedure and retrieval model,null,null",null,null
117,"116,Param. k n,null,null",null,null
118,"117, ED,null,null",null,null
119,"118,µ f bDocs f bT erms,null,null",null,null
120,"119,Meaning,null,null",null,null
121,"120,The maximum number of document terms to use in constructing QD . The maximum number of expansion documents in ED . One of several related mixing parameters controlling the weights of P (q|D) and P (q|ED ) Used for Dirichlet smoothing of both P (q|D) and P (q|E). The number of feedback documents to use for RM3 runs.,null,null",null,null
122,"121,The number of terms per document to use for RM3 runs.,null,null",null,null
123,"122,Mixing parameter controlling the weights of the original query and relevance model for RM3 runs.,null,null",null,null
124,"123,Value 20 10,null,null",null,null
125,"124,0.0-1.0,null,null",null,null
126,"125,2500 20 20 0.0-1.0,null,null",null,null
127,"126,""The values of  ED and  , are determined using 10-fold crossvalidation. In the training stage, we sweep over parameter values in intervals of 0.1. The concatenated results of each test fold form a complete set of topics."",null,null",null,null
128,"127,6 RESULTS,null,null",null,null
129,"128,Retrieval effectiveness of each run is shown in Table 2. We measure effectiveness with mean average precision (MAP) and normalized discounted cumulative gain at 20 (nDCG@20) [6]. Each metric is optimized with 10-fold cross-validation.,null,null",null,null
130,"129,""The results confirm that document expansion provides benefit over a baseline query likelihood run--no run performs significantly worse than the baseline, and most runs improve over the baseline QL run."",null,null",null,null
131,"130,""Performance of RM3 runs is more surprising with improvement over the baseline RM3 occurring more rarely compared to improvement over the baseline QL. The data suggest that RM3 runs may be more effective in more heterogeneous collections: there are three RM3 improvements in robust and six in wt10g, compared to only one in AP. This makes intuitive sense since a homogeneous collection would be expected to receive less benefit from query expansion. We can also see that an RM3 run typically improves over its QL counterpart, demonstrating that relevance modeling continues to operate effectively with the introduction of document expansion."",null,null",null,null
132,"131,""In general, wiki runs perform similarly to combined runs. However, the strong performance of combined runs is visible when query expansion is ignored: five out of six combined QL runs show statistically significant improvement over wiki QL runs. In one case (wt10g measuring nDCG@20) the combined QL run even outperforms the wiki RM3 run with statistical significance."",null,null",null,null
133,"132,6.1 Sensitivity to n,null,null",null,null
134,"133,""Figure 1 shows sweeps over several values of n, the number of expansion documents, for the self and wiki QL runs using our"",null,null",null,null
135,"134,1047,null,null",null,null
136,"135,Short Research Paper,null,null",null,null
137,"136,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
138,"137,MAP nDCG@20,null,null",null,null
139,"138,0.26 0.24 0.22 0.20 0.18,null,null",null,null
140,"139,0,null,null",null,null
141,"140,Expansion Source self wiki Collection AP wt10g robust 0.45,null,null",null,null
142,"141,0.40,null,null",null,null
143,"142,0.35,null,null",null,null
144,"143,10,null,null",null,null
145,"144,20,null,null",null,null
146,"145,30,null,null",null,null
147,"146,40,null,null",null,null
148,"147,Number of expansion documents,null,null",null,null
149,"148,0.30,null,null",null,null
150,"149,50,null,null",null,null
151,"150,0,null,null",null,null
152,"151,10,null,null",null,null
153,"152,20,null,null",null,null
154,"153,30,null,null",null,null
155,"154,40,null,null",null,null
156,"155,50,null,null",null,null
157,"156,Number of expansion documents,null,null",null,null
158,"157,""Figure 1: Sweeps over the number of expansion documents, n ,"""" {0, 1, 5, 10, 50}, for the self and wiki QL runs."""""",null,null",null,null
159,"158,""established cross-validation procedure with identical folds. The sensitivity to n is not pronounced at n  5, and what little variation exists is not consistent across collections. We therefore set n to 10, an apparently safe value, for all other runs. This is a convenient result since it allows for more efficient document expansion."",null,null",null,null
160,"159,7 CONCLUSIONS,null,null",null,null
161,"160,""The results indicate that our approach for document expansion works well in general and especially in concert with traditional relevance modeling. We find that we can improve on traditional document expansion by incorporating external collections into the expansion process. In the future, we plan to investigate how important the choice of external collection is to the retrieval effectiveness of our model."",null,null",null,null
162,"161,REFERENCES,null,null",null,null
163,"162,""[1] P. Bailey, N. Craswell, and D. Hawking. Engineering a multi-purpose test collection for web retrieval experiments. Information Processing and Management, 39(6):853­871, 2003."",null,null",null,null
164,"163,""[2] M. Bendersky, D. Metzler, and B. W. Croft. Effective query formulation with multiple information sources. WSDM '12, 2012."",null,null",null,null
165,"164,""[3] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In SIGIR '06, pages 154­161, 2006."",null,null",null,null
166,"165,""[4] M. Efron, P. Organisciak, and K. Fenlon. Improving retrieval of short texts through document expansion. In SIGIR '12, pages 911­920, 2012."",null,null",null,null
167,"166,""[5] D. Harman. Overview of the first text retrieval conference (TREC-1). In TREC '92, pages 1­20, 1992."",null,null",null,null
168,"167,""[6] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. TOIS, 20(4):422­446, 2002."",null,null",null,null
169,"168,""[7] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR '01, pages 111­119, 2001."",null,null",null,null
170,"169,""[8] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR '01, pages 120­127, 2001."",null,null",null,null
171,"170,""[9] Y. Li, W. Luk, K. Ho, and F. Chung. Improving weak ad-hoc queries using Wikipedia as external corpus. SIGIR '07, pages 797­798, 2007."",null,null",null,null
172,"171,""[10] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In SIGIR '04, pages 186­193, 2004."",null,null",null,null
173,"172,""[11] A. Singhal and F. Pereira. Document expansion for speech retrieval. In SIGIR '99, pages 34­41, 1999."",null,null",null,null
174,"173,""[12] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, pages 2­6, 2005."",null,null",null,null
175,"174,""[13] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In NAACL '06, pages 407­414, 2006."",null,null",null,null
176,"175,""[14] E. M. Voorhees. Overview of the TREC 2004 robust track. In TREC '132, 2013. [15] W. Weerkamp, K. Balog, and M. de Rijke. A generative blog post retrieval model"",null,null",null,null
177,"176,""that uses query expansion based on external collections. In ACL '09, pages 1057­1065, 2009. [16] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR '06, pages 178­185, 2006. [17] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on Wikipedia. SIGIR '09, 2009."",null,null",null,null
178,"177,Table 2: Performance of runs using various expansion,null,null",null,null
179,"178,sources with (RM3) and without (QL) query expansion. Statistical significance at p  0.05 is marked with a variety of symbols; underlining further designates p  0.01:  indicates improvement over the baseline QL run;  and  indicate im-,null,null",null,null
180,"179,provement and decline respectively with respect to the baseline RM3 run;  indicates improvement over the QL run with the same expansion source; S and W indicate improvement,null,null",null,null
181,"180,""over the self and wiki sources, respectively, of the same run"",null,null",null,null
182,"181,type. Bolded runs are the highest raw score for an evaluation,null,null",null,null
183,"182,metric in a given collection.,null,null",null,null
184,"183,Corpus AP,null,null",null,null
185,"184,Robust wt10g,null,null",null,null
186,"185,Exp. Source Baseline,null,null",null,null
187,"186,Self Wiki Combined Baseline Self Wiki Combined Baseline Self Wiki Combined,null,null",null,null
188,"187,Run,null,null",null,null
189,"188,QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3,null,null",null,null
190,"189,MAP,null,null",null,null
191,"190,0.2337 0.3310 0.2694 0.3295 0.2644 0.3334 0.2774W S 0.3342S,null,null",null,null
192,"191,0.2183 0.2639 0.2369 0.2591 0.2326 0.2674S 0.2417W 0.2672S,null,null",null,null
193,"192,0.1683,null,null",null,null
194,"193,0.1651,null,null",null,null
195,"194,0.1660 0.1694 0.1780S 0.2089S 0.1759S 0.2061S,null,null",null,null
196,"195,nDCG@20,null,null",null,null
197,"196,0.4170 0.4855 0.4519 0.4876W 0.4582 0.4811 0.4734SW 0.4789,null,null",null,null
198,"197,0.3867 0.3908 0.4036 0.3894 0.4040 0.4201S  0.4156W S 0.4205S,null,null",null,null
199,"198,0.2816 0.2834,null,null",null,null
200,"199,0.2936 0.2758 0.3029 0.3085S  0.3148SW 0.3082S,null,null",null,null
201,"200,1048,null,null",null,null
202,"201,,null,null",null,null

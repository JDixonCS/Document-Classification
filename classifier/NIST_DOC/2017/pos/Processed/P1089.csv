,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Gauging the ality of Relevance Assessments using Inter-Rater Agreement,null,null",null,null
4,"3,Tadele T. Damessie,null,null",null,null
5,"4,""RMIT University Melbourne, Australia"",null,null",null,null
6,"5,Falk Scholer,null,null",null,null
7,"6,""RMIT University Melbourne, Australia"",null,null",null,null
8,"7,ABSTRACT,null,null",null,null
9,"8,""In recent years, gathering relevance judgments through non-topic originators has become an increasingly important problem in Information Retrieval. Relevance judgments can be used to measure the eectiveness of a system, and are oen needed to build supervised learning models in learning-to-rank retrieval systems. e two most popular approaches to gathering bronze level judgments ­ where the judge is not the originator of the information need for which relevance is being assessed, and is not a topic expert ­ is through a controlled user study, or through crowdsourcing. However, judging comes at a cost (in time, and usually money) and the quality of the judgments can vary widely. In this work, we directly compare the reliability of judgments using three dierent types of bronze assessor groups. Our rst group is a controlled Lab group; the second and third are two dierent crowdsourcing groups, CF-Document where assessors were free to judge any number of documents for a topic, and CF-Topic where judges were required to judge all of the documents from a single topic, in a manner similar to the Lab group. Our study shows that Lab assessors exhibit a higher level of agreement with a set of ground truth judgments than CF-Topic and CF-Document assessors. Inter-rater agreement rates show analogous trends. ese nding suggests that in the absence of ground truth data, agreement between assessors can be used to reliably gauge the quality of relevance judgments gathered from secondary assessors, and that controlled user studies are more likely to produce reliable judgments despite being more costly."",null,null",null,null
10,"9,1 INTRODUCTION,null,null",null,null
11,"10,""Gathering relevance judgments using humans is a key component in building Information Retrieval test collections. However, human interpretation of """"relevance"""" is an inherently subjective process [11]. According to Tang and Solomon [16], judging relevance is a dynamic, multidimensional process likely to vary between assessors, and sometimes even with a single assessor at dierent stages of the process. For example, Scholer et al. [13] found that 19% of duplicate"",null,null",null,null
12,"11,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: hp://dx.doi.org/10.1145/3077136.3080729"",null,null",null,null
13,"12,ao P. Nghiem,null,null",null,null
14,"13,""RMIT University Melbourne, Australia"",null,null",null,null
15,"14,J. Shane Culpepper,null,null",null,null
16,"15,""RMIT University Melbourne, Australia"",null,null",null,null
17,"16,document pairings were judged inconsistently in the TREC-7 and TREC-8 test collections. Understanding the factors that lead to such variation in relevance assessments is crucial to reliable test collection development.,null,null",null,null
18,"17,""To address this issue, Bailey et al. [3] proposed three classes of judges ­ gold, silver and bronze ­ based on the expertise of the assessor. Gold judges are topic originators as well as subject experts; whereas silver judges are subject experts but not topic originators. Bronze judges are neither topic originators nor subject experts. But are all judges in a single class really the same? Secondary assessors who are neither topic creators nor experts are all bronze assessors, but there are in fact many dierent types of assessors who fall into this class. As assessment at the bronze level is now becoming a common practice in IR, in particular with the growing popularity of crowdsourcing, we set up an experiment to investigate the homogeneity of assessment quality using three dierent variants of bronze judges. e classes used in this study are:"",null,null",null,null
19,"18,""· Lab: is group of assessors carried out a relevance assessment task in a monitored lab environment, with a requirement to assess a pre-determined number of 30 documents in relation to a single search topic."",null,null",null,null
20,"19,· CF-Topic: is group of assessors are an exact replica of the Lab group task except that the task was administered using the CrowdFlower crowdsourcing platform.,null,null",null,null
21,"20,""· CF-Document: is group of assessors performed the task using CrowdFlower just as the CF-Topic group, but unlike the other two groups, each participant could judge as few (minimum 1) or as many (maximum 30) documents as they liked for a topic."",null,null",null,null
22,"21,Our main research question can formally be stated as:,null,null",null,null
23,"22,Research estion: Are there dierences in the quality of relevance judgments gathered from dierent sub-classes of bronze-level judges?,null,null",null,null
24,"23,2 RELATED WORK,null,null",null,null
25,"24,""e subjective nature of relevance is likely to result in disagreement between judges [11, 15]. Voorhees [18] was among the rst to study this phenomenon, and quantied agreement in relevance assessment using overlap between primary TREC assessors and two secondary assessors on 48 topics. A total of 30% of the documents judged relevant by the primary assessor were judged non-relevant, and less than 3% of the documents initially judged as non-relevant by the primary assessor were judged relevant by the secondary assessors."",null,null",null,null
26,"25,1089,null,null",null,null
27,"26,Short Research Paper,null,null",null,null
28,"27,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
29,"28,Number of documents judged,null,null",null,null
30,"29,0,null,null",null,null
31,"30,20,null,null",null,null
32,"31,40,null,null",null,null
33,"32,60,null,null",null,null
34,"33,80,null,null",null,null
35,"34,100,null,null",null,null
36,"35,Assessors,null,null",null,null
37,"36,Figure 1: Distribution of number of documents judged per assessor by the CF-Document group.,null,null",null,null
38,"37,""Sormunen [14] also compared judgments from a group of 9 master's students using a 4-point ordinal relevance scale with TREC binary assessments on 38 topics. Around 25% of the documents originally judged as relevant by the TREC assessors were re-assessed as non-relevant, and around 1% of the documents originally judged as non-relevant were re-assessed as relevant. Al-Maskari et al. [1] also ran an experiment on 56 TREC-8 topics using 56 participants in an interactive search task. e study found a 37% dierence between TREC and non-TREC assessors. at is, out of the 2, 262 documents judged relevant by the non-TREC assessors, 834 of the documents were judged non-relevant by the TREC assessors. In both studies, there is a clear dierence between the TREC assessors who are topic originators, and the non-TREC assessors who oen are not."",null,null",null,null
39,"38,""To address dierences between TREC judges and secondary assessors, Bailey et al. [3] identied three classes of judges ­ gold, silver and bronze ­ as discussed in Section 1. Bailey et al. found that assessments generated by silver judges were oen comparable to gold judges, but that extra care was needed when using bronze level judgments. However, the study did not prescribe exactly how this might be accomplished. In this study, we focus on dierent types of bronze level of assessors, as they now represent the most common class of judges outside of evaluation campaigns such as TREC which are being employed in large scale assessment gathering initiatives."",null,null",null,null
40,"39,3 METHODS AND DATASETS,null,null",null,null
41,"40,e TREC 7 and 8 datasets are used in this study. We focus on,null,null",null,null
42,"41,""topics from these collections since they are widely believed to be among the most complete collections available [10], and provide"",null,null",null,null
43,"42,a strong ground truth when aempting to quantify reliability in re-assessment exercises. Our work builds on two previous stud-,null,null",null,null
44,"43,""ies using the same topic conguration, and which provide further"",null,null",null,null
45,"44,""details about the user study conguration [5, 6]. We use 4 dierent topics: the 2 highest and 2 lowest performing topics from the"",null,null",null,null
46,"45,""dataset were selected using the average precision of each topic, averaged over the 110 runs submied to TREC 2004 Robust track."",null,null",null,null
47,"46,""is approach, called average-average-precision (AAP), was initially described by Carteree et al. [4], and used to quantify topic"",null,null",null,null
48,"47,diculty. Topic #365 (el nino) and #410 (schengen agreement),null,null",null,null
49,"48,""have the 2 highest AAP scores, and topic #378 (euro opposition)"",null,null",null,null
50,"49,and #448 (,null,null",null,null
51,"50,) are the 2 lowest AAP scoring topics in,null,null",null,null
52,"51,ship losses,null,null",null,null
53,"52,""the collection. For assessment, 30 documents were chosen for each"",null,null",null,null
54,"53,""topic, in proportion to an existing distribution of graded document relevance judgments made by Sormunen [14]."",null,null",null,null
55,"54,""A total of 32, 40 and 43 assessors judged documents in the Lab, CFTopic and CF-Document experimental groups, respectively. For all crowdsourcing experiments, a mandatory explanation of relevance assignment per document was required, and manually checked as a quality control, to ensure that crowdsourcing participants were performing assessments in good faith. A total of 10 assessors, 5 from CF-Topic and 5 from CF-Document failed the sanity check, and their data was removed from the nal evaluation. All crowdsourcing experiments were conducted using the CrowdFlower platform in a manner similar to previously run studies [2]."",null,null",null,null
56,"55,""e setup for the CF-Document group was designed to be as exible as possible, with assessors free to judge any number of the 30 documents for any of the 4 topics which were assigned randomly by the system. is setup introduces challenges during nal data analysis, however, since assessors judged an unequal number of documents, as shown in Figure 1, and a comparison of agreement between assessors with the same level of precision requires an incomplete balanced block design to be constructed as described by Fleiss [7]. is results in a sparse matrix of relevance scores for the maximum number of unique documents (30 per topic in our case) across the 121 unique assessors who contributed judgments."",null,null",null,null
57,"56,""Krippendor's Alpha ( ) is a chance-corrected measure of agreement, and not aected by dierences in sample sizes or missing values, and therefore appropriate for analysis of our experimental data [8]. Cohen's Kappa ( ) which is more suited for categorical data [17] is also used to quantify assessment quality against a gold standard. e values produced by these metrics is between 1 and 1, where a level of 0 indicates agreement at the level predicted by chance, 1 signies perfect agreement between raters, and a negative score occurs when agreement is less than what is expected by chance alone."",null,null",null,null
58,"57,4 RESULTS AND DISCUSSION,null,null",null,null
59,"58,""Assessor reliability ­ measured by the mean pairwise agreement between each assessor and the Sormunen gold standard assessments ­ is used to assess the quality of the assessments from each experimental group. is analysis is then compared with a measure of assessment quality using only inter-rater agreement, in the absence of any ground truth."",null,null",null,null
60,"59,""Assessor Reliability. e pairwise overall average reliability score of the Lab, CF-Topic and CF-Document groups, measured using [Krippendor's , Cohen's ] is [0.687, 0.581], [0.407, 0.236] and [0.561, 0.522] respectively. e scores are calculated on binary foldings of the 4-level graded relevance levels ­ non-relevant (0), marginally relevant (1), relevant (2) and highly relevant (3). e marginally relevant (1) and non-relevant (0) judgments are binarized as non-relevant and the others as relevant as recommended by Scholer and Turpin [12]."",null,null",null,null
61,"60,""e results in Table 1 indicate Lab and CF-Document assessors are more reliable than CF-Topic assessors. e statistical signicance of the dierences is evaluated using an unpaired two-tailed t-test across the individual pairwise agreement scores, and reported in Table 2. For both and , the overall paern from highest to lowest reliability score measured using the Sormunen judgments"",null,null",null,null
62,"61,1090,null,null",null,null
63,"62,Short Research Paper,null,null",null,null
64,"63,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
65,"64,""Table 1: Average pairwise agreement between judges and Sormunen gold standard judgments, measured across All and individual topics using Krippendor's Alpha ( ) on a 4-levels of ordinal scale and Cohen's Kappa( ) on a binary scale."",null,null",null,null
66,"65,Krippendor's Alpha ( ),null,null",null,null
67,"66,Cohen's Kappa( ),null,null",null,null
68,"67,Lab CF-Topic CF-Document Lab CF-Topic CF-Document,null,null",null,null
69,"68,All,null,null",null,null
70,"69,0.687 0.407,null,null",null,null
71,"70,0.561,null,null",null,null
72,"71,0.581 0.236,null,null",null,null
73,"72,0.522,null,null",null,null
74,"73,el nino schengen agreement euro opposition ship losses,null,null",null,null
75,"74,0.843 0.622 0.665 0.617,null,null",null,null
76,"75,0.531 0.057 0.437 0.561,null,null",null,null
77,"76,0.725 0.380 0.377 0.704,null,null",null,null
78,"77,0.761 0.558 0.436 0.565,null,null",null,null
79,"78,0.277 0.111 0.112 0.416,null,null",null,null
80,"79,0.599 0.410 0.391 0.666,null,null",null,null
81,"80,""Table 2: Statistical signicance of Table 1 results, evaluated using an unpaired two-tailed t-test for all bronze assessors. Results for Krippendor's Alpha ( ) are shown below the diagonal line with ratings on a 4-level ordinal scale, while results for Cohen's Kappa ( ) are shown above the diagonal line with ratings on a binary scale, aening 0 and 1 to 0; and 2 and 3 to 1."",null,null",null,null
82,"81,Lab CF-Topic CF-Document,null,null",null,null
83,"82,""Lab [ , 0.687/ , 0.581]"",null,null",null,null
84,"83,""95% CI 0.123, 0.435 Lab (M,""""0.687, SD"""",0.214) CF-Topic (M,""""0.407, SD"""",0.390) t (65) ,"""" 3.583, p < 0.001 95% CI 0.142, 0.266 Lab (M"""",""""0.687, SD"""",0.214) CF-Document (M,""""0.561, SD"""",0.345) t (68) ,"""" 1.793, p """", 0.077"",null,null",null,null
85,"84,""CF-Topic 95% CI 0.211, 0.479 Lab (M,""""0.581, SD"""",0.308) CF-Topic (M,""""0.236, SD"""",0.244) t (65) ,"""" 5.082, p < 0.001 [ """", 0.407/ , 0.236]"",null,null",null,null
86,"85,""95% CI 0.325, 0.018 CF-Topic (M,""""0.407, SD"""",0.390) CF-Document (M,""""0.561, SD"""",0.345) t (71) ,"""" 1.781, p """", 0.079"",null,null",null,null
87,"86,CF-Document,null,null",null,null
88,"87,""95% CI 0.099, 0.216 Lab (M,""""0.581, SD"""",0.308) CF-Document (M,""""0.522, SD"""",0.347) t (68) ,"""" 0.739, p """", 0.462"",null,null",null,null
89,"88,""95% CI 0.426, 0.144 CF-Document (M,""""0.522, SD"""",0.347) CF-Topic (M,""""0.236, SD"""",0.244) t (71) ,"""" 4.026, p < 0.001"""""",null,null",null,null
90,"89,""[ , 0.561/ , 0.522]"",null,null",null,null
91,"90,""as a baseline is: Lab, CF-Document and CF-Topic respectively. One explanation for this trend might be that the Lab study is a more directed environment, and assessors know that they are being closely monitored the entire time. is could contribute to longer periods of focus, resulting in a higher overall agreement with the gold standard, and therefore a presumed higher overall quality of obtained judgments."",null,null",null,null
92,"91,""When comparing only the two crowdsourcing groups, the CFDocument assessors show higher reliability. is is a somewhat surprising result, since the judges assess fewer documents and therefore spend less time overall forming a notion of relevance for a particular topic. However, this lack of """"domain knowledge"""" might be counteracted by task completion time: an assessor in CF-Topic had to judge all 30 documents to get paid, and when an assessor encounters long or dicult documents at the tail of an assessment list, the likely outcome is that the assessor becomes less motivated to get any single judgment exactly right. Fatigue and motivation are known to inuence relevance judgment outcomes [9, 19], and perhaps contribute to the drop in quality. In contrast, CFDocument assessors may perceive that less eort is required on their behalf to judge a single topic-document pair before geing paid. ese """"micro"""" transactions could very well be a strong motivator for crowdsourced assessors, despite having an implicit startup cost in understanding the task at hand that is amortized when judging multiple documents for the same topic. We plan to study this phenomenon in more detail in future work."",null,null",null,null
93,"92,""Figure 2 and Figure 3 give further insight on the reliability levels (agreement with the gold standard) of individual CF-Topic and CFDocument assessors, respectively. Results for the Lab group were omied due to space limitations; the reliability score for this group was consistently well above > 0.2, with no negative scores for any assessors. A number of assessors in CF-Topic showed lower levels of agreement with the gold standard than expected by chance alone for 2 of the topics as shown in Figure 2. Reliability for the other 2 topics in this group is similar to the trend observed for the Lab assessors. Only one assessor's relative performance in the CF-Document setup deviated signicantly from the others, as shown in Figure 3. We plan to further investigate the reasons for why such low reliability scores were observed for some individual assessors in these groups in followup work. Note that all of these assessors passed manual sanity control measures, and appeared to be performing judgments in good faith."",null,null",null,null
94,"93,""Agreement. As can been seen in Table 3, overall agreement is higher in Lab, followed by CF-Document and CF-Topic, which are in the same relative order as the reliability scores when comparing against a gold standard, suggesting that inter-rater reliability is a reasonable proxy for the quality of judgments."",null,null",null,null
95,"94,""To further establish our belief of assessor reliability, we computed the median of the multiple assessments made for each document in each experimental group, and computed the Krippendor's Alpha ( ) agreement between individual assessors and this score, shown in Table 3 (right). e overall trend is again consistent with the ndings of Table 1."",null,null",null,null
96,"95,1091,null,null",null,null
97,"96,Short Research Paper,null,null",null,null
98,"97,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
99,"98,""Table 3: Inter-rater agreement (le) and majority vote (right) measured between assessors in the Lab, CF-Topic and CF-Document groups using Krippendor's alpha ( ) across All and individual topics with ratings on a 4-level ordinal scale. e number of assessors for inter-rater agreement is shown in parenthesis next to each value."",null,null",null,null
100,"99,Topic All,null,null",null,null
101,"100,el nino schengen agreement euro opposition ship losses,null,null",null,null
102,"101,Inter-rater agreement,null,null",null,null
103,"102,Lab,null,null",null,null
104,"103,CF-Topic CF-Document,null,null",null,null
105,"104,0.657 (32) 0.426 (35) 0.530 (121),null,null",null,null
106,"105,0.845 (8) 0.634 (8) 0.565 (8) 0.558 (8),null,null",null,null
107,"106,0.394 (8) 0.170 (8) 0.464 (9) 0.377 (10),null,null",null,null
108,"107,0.682 (31) 0.500 (29) 0.431 (29) 0.471 (32),null,null",null,null
109,"108,Lab,null,null",null,null
110,"109,0.787,null,null",null,null
111,"110,0.917 0.691 0.867 0.710,null,null",null,null
112,"111,Majority vote CF-Topic CF-Document,null,null",null,null
113,"112,0.544,null,null",null,null
114,"113,0.663,null,null",null,null
115,"114,0.608 0.436 0.537 0.605,null,null",null,null
116,"115,0.771 0.542 0.599 0.799,null,null",null,null
117,"116, score  score,null,null",null,null
118,"117,a,null,null",null,null
119,"118,1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 -0.6,null,null",null,null
120,"119,Assessors,null,null",null,null
121,"120,b,null,null",null,null
122,"121,1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 -0.6,null,null",null,null
123,"122,Assessors,null,null",null,null
124,"123, score,null,null",null,null
125,"124,Figure 2: Reliability of CF-Topic assessors when compared with the Sormunen judgments using Krippendor's Alpha ( ) for the topics: (a) El nino; and (b) Schengen agreement.,null,null",null,null
126,"125,1,null,null",null,null
127,"126,0.8,null,null",null,null
128,"127,0.6,null,null",null,null
129,"128,0.4,null,null",null,null
130,"129,0.2,null,null",null,null
131,"130,0,null,null",null,null
132,"131,-0.2,null,null",null,null
133,"132,-0.4,null,null",null,null
134,"133,-0.6 Assessors,null,null",null,null
135,"134,Figure 3: Reliability of CF-Document assessors when compared to the Sormunen judments using Krippendor's Alpha ( ).,null,null",null,null
136,"135,""Geing gold standard relevance labels is rarely possible in a live judging scenario, but it is possible to compute inter-rater agreement between assessors, and use this to establish the quality of assessments. Our experiments conrm that using agreement between judges to gauge the quality of relevance judgments collected is indeed one possible approach to controlling the quality of judgments gathered by bronze level assessors."",null,null",null,null
137,"136,5 CONCLUSION,null,null",null,null
138,"137,""is study analyzed the quality of relevance judgments generated in three (of many possible) dierent sub-classes of bronze assessors, using Krippendor's Alpha ( ) and Cohen's Kappa ( ). e results of both metrics conrm the existence of assessment quality dierences among the three sub-classes of bronze assessors, warranting"",null,null",null,null
139,"138,""further study. Nevertheless, inter-rater agreement can be a reliable"",null,null",null,null
140,"139,tool to benchmark the quality of relevance judgments when gold,null,null",null,null
141,"140,standard judgments are not readily available.,null,null",null,null
142,"141,Acknowledgment. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP170102231 and DP140103256).,null,null",null,null
143,"142,REFERENCES,null,null",null,null
144,"143,""[1] A. Al-Maskari, M. Sanderson, and P. Clough. 2008. Relevance judgments between TREC and Non-TREC assessors. In Proc. SIGIR. 683­684."",null,null",null,null
145,"144,""[2] O. Alonso and S. Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Inf. Proc. & Man. 48, 6 (2012), 1053­1066."",null,null",null,null
146,"145,""[3] P. Bailey, N. Craswell, I. Soboro, P. omas, A.P. de Vries, and E. Yilmaz. 2008. Relevance assessment: are judges exchangeable and does it maer. In Proc. SIGIR. 667­674."",null,null",null,null
147,"146,""[4] B. Carteree, V. Pavlu, H. Fang, and E. Kanoulas. 2009. Million ery Track 2009 Overview.. In Proc. TREC."",null,null",null,null
148,"147,""[5] T.T. Damessie, F. Scholer, and J.S. Culpepper. 2016. e Inuence of Topic Diculty, Relevance Level, and Document Ordering on Relevance Judging. In Proc. ADCS. 41­48."",null,null",null,null
149,"148,""[6] T.T. Damessie, F. Scholer, K. Ja¨rvelin, and J.S. Culpepper. 2016. e eect of document order and topic diculty on assessor agreement. In Proc. ICTIR. 73­76."",null,null",null,null
150,"149,""[7] J.L. Fleiss. 1981. Balanced incomplete block designs for inter-rater reliability studies. Applied Psychological Measurement 5, 1 (1981), 105­112."",null,null",null,null
151,"150,""[8] A.F. Hayes and K. Krippendor. 2007. Answering the call for a standard reliability measure for coding data. Comm. Methods and Measures 1, 1 (2007), 77­89."",null,null",null,null
152,"151,""[9] G. Kazai, J. Kamps, and N. Milic-Frayling. 2013. An analysis of human factors and label accuracy in crowdsourcing relevance judgments. Inf. Retr. 16, 2 (2013), 138­178."",null,null",null,null
153,"152,""[10] X. Lu, A. Moat, and J. S. Culpepper. 2016. e eect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445."",null,null",null,null
154,"153,""[11] T. Saracevic. 2007. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. J. Amer. Soc. Inf. Sc. Tech. 58, 13 (2007), 1915­1933."",null,null",null,null
155,"154,[12] F. Scholer and A. Turpin. 2009. Metric and relevance mismatch in retrieval evaluation. In Proc. AIRS. 50­62.,null,null",null,null
156,"155,""[13] F. Scholer, A. Turpin, and M. Sanderson. 2011. antifying test collection quality based on the consistency of relevance judgements. In Proc. SIGIR. 1063­1072."",null,null",null,null
157,"156,[14] E. Sormunen. 2002. Liberal relevance criteria of TREC-: Counting on negligible documents?. In Proc. SIGIR. 324­330.,null,null",null,null
158,"157,""[15] M. Stefano. 1997. Relevance: e whole history. J. Amer. Soc. Inf. Sc. 48, 9 (1997), 810­832."",null,null",null,null
159,"158,""[16] R. Tang and P. Solomon. 1998. Toward an understanding of the dynamics of relevance judgment: An analysis of one person's search behavior. Inf. Proc. & Man. 34, 2 (1998), 237­256."",null,null",null,null
160,"159,""[17] A.J. Viera and J.M. Garre. 2005. Understanding interobserver agreement: the kappa statistic. Fam. Med. 37, 5 (2005), 360­363."",null,null",null,null
161,"160,""[18] E.M. Voorhees. 2000. Variations in relevance judgments and the measurement of retrieval eectiveness. Inf. Proc. & Man. 36, 5 (2000), 697­716."",null,null",null,null
162,"161,""[19] J. Wang. 2011. Accuracy, agreement, speed, and perceived diculty of users' relevance judgments for e-discovery. In Proc. of SIGIR Inf. Ret. for E-Discovery Workshop., Vol. 1."",null,null",null,null
163,"162,1092,null,null",null,null
164,"163,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Adapting Markov Decision Process for Search Result Diversification,null,null",null,null
4,"3,""Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng"",null,null",null,null
5,"4,""CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences {xialong,zengwei}@so ware.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn"",null,null",null,null
6,"5,ABSTRACT,null,null",null,null
7,"6,""In this paper we address the issue of learning diverse ranking models for search result diversi cation. Typical methods treat the problem of constructing a diverse ranking as a process of sequential document selection. At each ranking position, the document that can provide the largest amount of additional information to the users is selected, because the search users usually browse the documents in a top-down manner. us, to select an optimal document for a position, it is critical for a diverse ranking model to capture the utility of information the user have perceived from the preceding documents. Existing methods usually calculate the ranking scores (e.g., the marginal relevance) directly based on the query and the selected documents, with heuristic rules or handcra ed features."",null,null",null,null
8,"7,""e utility the user perceived at each of the ranks, however, is not explicitly modeled. In this paper, we present a novel diverse ranking model on the basis of continuous state Markov decision process (MDP) in which the user perceived utility is modeled as a part of the MDP state. Our model, referred to as MDP-DIV, sequentially takes the actions of selecting one document according to current state, and then updates the state for the chosen of the next action. e transition of the states are modeled in a recurrent manner and the model parameters are learned with policy gradient. Experimental results based on the TREC benchmarks showed that MDP-DIV can signi cantly outperform the state-of-the-art baselines."",null,null",null,null
9,"8,KEYWORDS,null,null",null,null
10,"9,learning to rank; search result diversi cation; Markov decision process,null,null",null,null
11,"10,""ACM Reference format: Long Xia, Jun Xu , Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng. 2017. Adapting Markov Decision Process for Search Result Diversi cation. In Proceedings of SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080775"",null,null",null,null
12,"11,* Corresponding author: Jun Xu.,null,null",null,null
13,"12,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: h p://dx.doi.org/10.1145/3077136.3080775"",null,null",null,null
14,"13,1 INTRODUCTION,null,null",null,null
15,"14,""In many information retrieval tasks, one important goal involves providing search results that covers a wide range of topics for a search query, called search result diversi cation [1]. One of the key problems in search result diversi cation is ranking, speci cally, how to develop a ranking model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents."",null,null",null,null
16,"15,""Typical approaches to search result diversi cation, including the heuristic approaches and the learning approaches, treat the process of constructing a diverse ranking as a problem of sequential document selection. At each ranking position, the additional amount of information (utility) a document can provide is estimated, on the basis of the user query and the documents ranked ahead. e document that can provide maximal additional utility is selected."",null,null",null,null
17,"16,""e sequential document selection matches well with the user activity of browsing the search results: search users usually browse the search results in a top-down manner. us, to accurately select the document at each of the positions, it is critical for a diverse ranking algorithm to model the utility of information the users have already perceived from the preceding documents."",null,null",null,null
18,"17,""Several methods for diverse ranking have been developed and applied to document retrieval. Di erent criteria are adopted in these methods to estimate the new utility a candidate document can provide. For example, in the representative heuristic approach of maximal marginal relevance (MMR) [2], the marginal relevance, which is de ned as a sum of the query-document relevance and the maximal document distance, is used as the utility. In x AD [20], another widely used diverse ranking model, the utility is de ned so as to explicitly account for the relationship between documents retrieved for the original query and the possible aspects underlying this query, in the form of sub-queries. In recent years, machine learning methods have been proposed and applied to search result diversi cation [18, 24­27, 31]. Typical diverse learning models, including the relational learning to rank (R-LTR) [31] and its variations [24­26], de ne the utilities as the linear combinations of the relevance features and the novelty features."",null,null",null,null
19,"18,""All the existing methods on diverse ranking [2, 20, 31] are designed to estimate the utility of a candidate document directly based on the user query and the preceding documents, calculated either by the carefully designed heuristics (e.g., the scoring functions in MMR and x AD) or as a linear combination of the handcra ed relevance features and novelty features (e.g., the scoring function in R-LTR). e utility perceived by the users from the preceding documents, however, is not explicitly modeled and fully utilized."",null,null",null,null
20,"19,""In this paper we propose to formalize the construction of a diverse ranking as a process of sequential decision making, which can"",null,null",null,null
21,"20,535,null,null",null,null
22,"21,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
23,"22,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
24,"23,""be modeled with a continuous state Markov decision process (MDP). e new diverse ranking model, referred to as MDP-DIV, model"",null,null",null,null
25,"24,""the user perceived utility of information as a part of its MDP state. Speci cally, in MDP-DIV, a document ranking with M positions is considered as a sequence of M discrete time steps where each time step corresponds to a ranking position. e ranking of documents, thus, is formalized as a sequence of M decisions and each action corresponds to selecting one document from the candidate set. At each time step, the agent receives the environment's state, which models the user's dynamic state on the perceived utility, starting from the rst ranking position. Based on the received state, the agent chooses an action. One time step later, as a consequence of the action, the search users perceive some additional utility from the new selected document, and the system transit to a new state."",null,null",null,null
26,"25,""e transition function, which maps old state and the selected document to a new state, is implemented in a recurrent manner. At each time step, the chosen of the action depends on a policy, which is a function maps from the current state to a probability distribution of selecting each actions."",null,null",null,null
27,"26,""Reinforcement learning is employed to train the model parameters. Given a set of labeled queries, at each time step, the agent can receive a numerical action-dependent reward which can be de ned upon the diversity evaluation measures. e policy gradient algorithm of REINFORCE [22] is adopted to adjust the model parameters so that expected long-term discounted rewards in terms of the diversity evaluation measure is maximized. In the testing phase, the system fully trusts the learned policy. Given a query and the associated documents, the action with the maximal probability is selected at each ranking position."",null,null",null,null
28,"27,""Advantages of the proposed model include: 1) explicitly modeling the dynamic state on the user perceived utility of information in diverse ranking learning, which uni es the relevance and novelty and can be utilized as the criterion for selecting documents; 2) ability to conduct diverse ranking learning in an end-to-end manner, achieving a diverse ranking model with no need of handcra ing features; 3) ability to learn a ranking model towards to a diversity evaluation measure, via involving the measure in the training."",null,null",null,null
29,"28,""To evaluate the e ectiveness of MDP-DIV, we conducted experiments on the basis of TREC benchmark datasets. e experimental results showed that MDP-DIV can signi cantly outperform the state-of-the-art diverse ranking approaches including the heuristic methods of MMR, x AD, and the learning methods of R-LTR, PAMM, and PAMM-NTN. We analyzed the results and showed that MDP-DIV improved the performances through 1) optimizing the diversity evaluation measures in training, 2) modeling the dynamic user state on the perceived utility, and 3) utilizing both the immediate rewards and the long-term returns in training phase."",null,null",null,null
30,"29,2 RELATED WORK,null,null",null,null
31,"30,2.1 Search result diversi cation,null,null",null,null
32,"31,It is a common practice to formalize the construction of a diverse ranking list in search as a process of sequential document selection.,null,null",null,null
33,"32,is is based on the observation that in diverse ranking the additional utility a document can provide depends on not only the document itself but also the preceding documents. Di erent models designed di erent criteria for estimating the utility the search users,null,null",null,null
34,"33,""can perceive from a candidate document. Following the idea, Carbonell and Goldstein [2] proposed the maximal marginal relevance criterion to guide the selection of the documents. At each iteration, the document with the highest marginal relevance score is selected, where the score is a linear combination of the query-document relevance and the maximum distance of the document to the documents in current result set, in other words, novelty. e marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. Based on MMR, Guo and Sanner [7] proposed the probabilistic latent MMR model. x AD [19] directly models di erent aspects underlying the original query in the form of sub-queries, and estimates the utility as the relevance of the retrieved documents to each identi ed sub-query. PM-2 [5] treats the problem of nding a diverse search result as nding a proportional representation for the document ranking. Hu et al. [9] proposed a utility function that explicitly leverages the hierarchical intents of queries and selects the documents that maximize diversity in the hierarchical structure. Evaluation methods have also developed based on the intent hierarchies [23]. He et al. [8] proposed to combine the implicit and explicit topic representations for constructing be er diverse rankings. Gollapudi and Sharma [6] proposed an axiomatic approach to result diversi cation."",null,null",null,null
35,"34,""Machine learning techniques, which automatically learn the ranking models from the human labeled data, have been applied to construct diverse ranking models. Most of learning approaches still adopt sequential document selection as the basic framework, and the additional utility a candidate document can provide is usually modeled as a sum of the relevance score and the novelty score. For example, Zhu et al. [31], Xia et al. [24], and Xu et al. [26] employed a set of handcra ed relevance features and novelty features to calculate the relevance score and the novelty score, respectively. Both of the scores are de ned as linear combinations of the features. Xia et al.[25] proposed to model the novelty score with the deep learning model of neural tensor networks. SVM-DIV [18] propose to construct a diverse ranking with the diversity criterion only. Structured output learning [11] and deep learning models [15] have also been employed to address the problem of learning diverse rankings."",null,null",null,null
36,"35,""Existing methods calculates the ranking scores directly based on the query and the selected documents, with the heuristic rules or the ranking features. ough it is a critical issue for constructing optimal diverse rankings, the dynamic utility the search user perceived from the preceding documents is still not explicitly modeled and fully utilized in current diverse ranking methods."",null,null",null,null
37,"36,2.2 MDP for information retrieval,null,null",null,null
38,"37,""In this paper we employ MDP for constructing diverse ranking model, which has been widely used in variant IR applications. For example, in [13], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [30], the log-based document re-ranking is also modeled as a POMDP to improve the re-ranking performances. MDP is also used for building recommender systems. For example, [21] designed an MDP-based recommendation model for taking"",null,null",null,null
39,"38,536,null,null",null,null
40,"39,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
41,"40,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
42,"41,""both the long-term e ects of each recommendation and the expected value of each recommendation into account. Besides the MDP, researchers also employed the bandits model for constructing diverse ranking [18] and optimizing IR system [28]."",null,null",null,null
43,"42,""Recent advances in deep learning makes it possible to incorporate deep learning methods with sequential decision making. In the literature of vision, Mnih et al. [16] a empts to implement a entional processing in a deep learning framework. Lu and Yang[12] proposes POMDP-Rec, a neural-optimized POMDP algorithm, for building a collaborative ltering recommender system."",null,null",null,null
44,"43,""ough MDP has been applied to various information retrieval tasks, applying it to learning to rank and search result diversi cation is hard. e di culties lie in how to formalize diverse ranking under the MDP framework and how to convert the human labels to the supervision information that can be utilized by MDP. In this paper, we propose to formulate the diverse ranking learning as a problem of learning an MDP model."",null,null",null,null
45,"44,3 MARKOV DECISION PROCESS,null,null",null,null
46,"45,""In the paper, we employ continuous state MDP[17, 22], a widely used sequential decision making model, for learning the diverse ranking. An MDP is composed by states, actions, rewards, policy, and transitions, and represented by a tuple S, A,T , R,  :"",null,null",null,null
47,"46,""States S is a set of states. For instance, in this paper we de ne the state as a tuple consisting of preceding document ranking, candidate documents, and the utility the user perceived from the preceding documents."",null,null",null,null
48,"47,""Actions A is a discrete set of actions that an agent can take. e actions available may depend on the state s, denoted as A(s)."",null,null",null,null
49,"48,""Transition T is the state transition function st+1 ,"""" T (st , at ) which speci es a function which maps a state st into a new state st+1 in response to the action selected at ."""""",null,null",null,null
50,"49,""Reward r ,"""" R(s, a) is the immediate reward, also known as reinforcement. It gives the immediate reward of taking action a at state s."""""",null,null",null,null
51,"50,""Policy  (a|s) describes the behaviors of an agent, which is a probability distribution over the possible actions.  is usually optimized to decide how to move around in the state space to optimize the long term return."",null,null",null,null
52,"51,""e agent and environment interact at each of a sequence of discrete time steps, t ,"""" 0, 1, 2, · · · . At each time step t the agent receives some representation of the environment's state, st  S, and on that basis selects an action at  A(st ), where A(st ) is the set of actions available in state st . One time step later, in part as a consequence of its action, the agent receives a numerical reward, rt +1  R and nds itself in a new state st +1 """","""" T (st , at ). Figure 1 illustrates the agent-environment interaction in MDP."""""",null,null",null,null
53,"52,4 MDP FORMULATION OF DIVERSE RANKING,null,null",null,null
54,"53,""In this paper, we employ the continuous state MDP to model the construction of the diverse ranking."",null,null",null,null
55,"54,4.1 e basic model,null,null",null,null
56,"55,""Suppose we are given a query q, which is associated with a set of retrieved documents X ,"""" {x1, · · · , xM }  X, where both the"""""",null,null",null,null
57,"56,state st,null,null",null,null
58,"57,""st ,"""" [Zt, Xt, ht]"""""",null,null",null,null
59,"58,reward rt,null,null",null,null
60,"59,""rt ,"""" R(st, at)"""""",null,null",null,null
61,"60,rt+1,null,null",null,null
62,"61,st+1,null,null",null,null
63,"62,Agent,null,null",null,null
64,"63,action at,null,null",null,null
65,"64,sample at  ,null,null",null,null
66,"65,Environment,null,null",null,null
67,"66,Figure 1: e agent-environment interaction in MDP.,null,null",null,null
68,"67,""query q and the documents xi are represented as L-dimensional preliminary representations, i.e., the vectors learned by the doc2vec"",null,null",null,null
69,"68,""model [10], and X is the set of all possible documents. e goal of"",null,null",null,null
70,"69,diverse ranking is to construct a model that can rank the documents,null,null",null,null
71,"70,so that the top ranked documents cover a wide range of subtopics,null,null",null,null
72,"71,for a search query.,null,null",null,null
73,"72,""Supervised learning approaches can be used to construct the model. Suppose we are given N labeled training queries {(q(n), X (n), (n))}nN,""""1, where (n) denotes the human labels on the documents, in the form of a binary matrix. (n)(i, j) """", 1 if document xi(n) contains the j-th subtopic of q(n) and 0 otherwise1. e learning of a"",null,null",null,null
74,"73,""diverse ranking model, thus, can be considered as the learning the"",null,null",null,null
75,"74,parameters in an MDP model in which each time step corresponds,null,null",null,null
76,"75,""to a ranking position. e states, actions, rewards, transitions, and"",null,null",null,null
77,"76,policy of the MDP are set as:,null,null",null,null
78,"77,""States S: We design the state at time step t as a triple st ,"""" [Zt , Xt , ht ], where Zt """", {x(n)}nt ,""""1 is the sequence of t preceding documents, where x(n) is the document ranked at position n. Note that we de ne Z0 """",""""  is a empty sequence; Xt  2X is the set of candidate documents; ht  RK is a vector that encodes the user perceived utility from preceding documents in Zt , as well as the information need based on q."""""",null,null",null,null
79,"78,""At the beginning (t ,"""" 0), the state is initialized as s0 """", [Z0 ,"""" , X0 """","""" X , h0]: Z0 is initialized as an empty sequence , the candidate set X0 contains all of the M documents in X , and h0 is initialized as the user's initial information needs, implemented with a"""""",null,null",null,null
80,"79,nonlinear transformation of the query:,null,null",null,null
81,"80,""h0 ,""""  (Vq q),"""""",null,null",null,null
82,"81,(1),null,null",null,null
83,"82,""where q  RL is the preliminary representation of the user issued query, Vq  RK×L is the transformation matrix, and  (x) is the"",null,null",null,null
84,"83,nonlinear sigmoid function applied to each of the entries:,null,null",null,null
85,"84,"" (x) ,""""  ( x1, · · · , xK ) """","",null,null",null,null
86,"85,1,null,null",null,null
87,"86,1 + e-x1,null,null",null,null
88,"87,"","",null,null",null,null
89,"88,·,null,null",null,null
90,"89,·,null,null",null,null
91,"90,·,null,null",null,null
92,"91,"","",null,null",null,null
93,"92,1,null,null",null,null
94,"93,+,null,null",null,null
95,"94,1 e -x K,null,null",null,null
96,"95,.,null,null",null,null
97,"96,""Actions A: At each time step t, the A(st ) is the set of actions the agent can choose, each corresponds to selecting a document"",null,null",null,null
98,"97,""from Xt . at is, the action at the time step t, at  A(st ) selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at ."",null,null",null,null
99,"98,""1Some datasets also use graded judgements. In this paper, we assume that all labels are binary."",null,null",null,null
100,"99,537,null,null",null,null
101,"100,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
102,"101,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
103,"102,""TransitionT : e transition functionT : S ×A  S also consists of three parts, as shown in the following Equation (2):"",null,null",null,null
104,"103,""st +1 ,"""" T (st , at )"""""",null,null",null,null
105,"104,"","""" T ([Zt , Xt , ht ], at )"""""",null,null",null,null
106,"105,(2),null,null",null,null
107,"106,"","""" Zt  {xm(at )}, Xt \ {xm(at )},  (Vxm(at ) + Wht ) ,"""""",null,null",null,null
108,"107,""where  concatenates the old sequence Zt with xm(at ), V  RK×L is the document-state transformation matrix, and W  RK×K is the state-state transformation matrix. At each time step t, based on state st the system chooses an action at . en, the system moves to time step t + 1 and the system transits to a new state st+1: First, the system appends the selected document to the end of Zt , generating a new document sequence; Second, the selected document at step t is removed from the candidate set: Xt +1 ,"""" Xt \ {xm(at )}. us, the number of actions the agent can choose at step t + 1 is reduced by one. ird, the information from the user's last state and the selected document are combined together to form a new user state."""""",null,null",null,null
109,"108,""Note that in the initialization of h, the parameter Vq is used for transforming the query to state. In the state transformation, another parameter V is used for transforming the selected document to state."",null,null",null,null
110,"109,e se ing is based on the consideration that they have di erent goals: Vq is for transforming the query q which represents the information needs of the search users; V is for transforming the documents x which contain the utility that can be perceived by the users for ful lling the information needs.,null,null",null,null
111,"110,""Also note that though the state transition function is implemented in a recurrent fashion, they have striking di erence with recurrent neural networks (RNN): in MDP-DIV the input at time step t depends on the output (action) at the time step t - 1."",null,null",null,null
112,"111,""Reward R: e reward can be considered as an evaluation of the quality of the selected document. In search result diversi cation, the diversity evaluation measures are used to evaluate the quality of a ranking. Most of these measures are calculated in a sequential manner. us, it is natural to de ne the reward function on the basis of the diversity evaluation measures. For example, based on the diversity evaluation measure of -DCG, we can de ne the reward function as the promotion of -DCG caused by choosing the action at :"",null,null",null,null
113,"112,""R -DCG(st , at ) ,""""  -DCG[t + 1] -  -DCG[t],"""""",null,null",null,null
114,"113,""where -DCG[t] is the discounted cumulative gain [4] at the t-th position, and the -DCG value at the rank 0 is de ned as zero:  -DCG[0] , 0. 2"",null,null",null,null
115,"114,""Similarly, on the basis of diversity evaluation measure of Srecall [29], we can also de ne another reward which is the promotion of S-recall by the action:"",null,null",null,null
116,"115,""RS-recall(st , at ) ,"""" S-recall[t + 1] - S-recall[t],"""""",null,null",null,null
117,"116,""where S-recall[t] is the S-recall value at the t-th position, and Srecall[0] , 0."",null,null",null,null
118,"117,""Since the training algorithm learns the model parameters under the supervision of the rewards, de ning the rewards according to a diversity evaluation measure can guide the training process to achieve an optimal model in terms of that evaluation measure."",null,null",null,null
119,"118,""2 e calculation of reward is based on the document sequence Zt in st , the selected documents xm(at ), and the relevance labels of these documents. Here we assume that the state st also contains the document labels in the training phase."",null,null",null,null
120,"119,""Policy  (a|s): e policy  : A × S  [0, 1] de nes the probabi-"",null,null",null,null
121,"120,""lity of selecting each action. Given current state st ,"""" [Zt , Xt , ht ] and a possible action at , the policy  is de ned as a normalized so -max function whose input is the bilinear product of the utility"""""",null,null",null,null
122,"121,and the selected document:,null,null",null,null
123,"122,""exp  (at |[Zt , Xt , ht ]) ,"",null,null",null,null
124,"123,xTm(at )Uht Z,null,null",null,null
125,"124,"","",null,null",null,null
126,"125,(3),null,null",null,null
127,"126,where U  RL×K is the parameter in the bilinear product and Z is the normalization factor:,null,null",null,null
128,"127,""Z,"",null,null",null,null
129,"128,exp xTm(a)Uht .,null,null",null,null
130,"129,a A(st ),null,null",null,null
131,"130,""Construction of a diverse ranking for the queries in the training data can be formalized as: given a user query q, a set of M candidate documents X , and the corresponding human labels , the system state is initialized as s0 , [Z0 ,"""" , X0 """","""" X , h0 """",""""  (Vq q)]. en, at each of the time steps t """","""" 0, · · · , M - 1, the agent receives the state st """","""" [Zt , Xt , ht ], chooses an action at which selects the document xm(at ) from the candidate set, and places it to the rank t +1. Moving to the next step t + 1, the state becomes st +1 """","""" [Zt +1, Xt +1, ht +1]. On the basis of the human labels for the query, the agent receives immediate reward rt+1 """","""" R([Zt , Xt , ht ], at ), which could be used as supervision for training the model parameters. e process is repeated until the candidate set becomes empty."""""",null,null",null,null
132,"131,""Note that in online ranking/testing phase, there is no reward available because the queries are unlabeled. To construct a diverse ranking, we fully trust the learned policy and choose the action with maximal probability at each time step."",null,null",null,null
133,"132,""Next, we will discuss the o -line training algorithm and online ranking algorithm."",null,null",null,null
134,"133,4.2 Learning with policy gradient,null,null",null,null
135,"134,""e model has parameters  ,"""" {Vq, U, V, W} to learn. Inspired by the REINFORCE [22] algorithm of policy gradient, we devised a novel algorithm which can learn the parameters toward the diversity evaluation measure. e algorithm is referred as MDP-DIV and shown in Algorithm 1. e Algorithm 2 shows the procedure of sampling an episode for Algorithm 1."""""",null,null",null,null
136,"135,""e basic idea of Algorithm 1 is updating the parameters via Monte-Carlo stochastic gradient ascent. At each iteration, an episode (consisting a sequence of M states, actions, and rewards) is sampled according to current policy. en, at each time step t of the sampled episode, the model parameters are adjusted according to the gradients of the parameters  log  (at |st ; ), scaled by the step size , discount rate  t , and the long-term return Gt , where Gt is de ned as the discounted sum of the rewards from position t:"",null,null",null,null
137,"136,M -1-t,null,null",null,null
138,"137,""Gt ,"",null,null",null,null
139,"138,"" k rt +k+1,"",null,null",null,null
140,"139,(4),null,null",null,null
141,"140,""k ,0"",null,null",null,null
142,"141,""where M , |X | is the number documents in the candidate set. Note"",null,null",null,null
143,"142,""that if  ,"""" 1, G0 is exactly the evaluation measure calculated at the nal rank of the document list, i.e., -DCG@M or S-recall@M."""""",null,null",null,null
144,"143,""Intuitively, the se ing of Gt let the parameters move most in the directions so that the favor actions can yield the highest return."",null,null",null,null
145,"144,538,null,null",null,null
146,"145,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
147,"146,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
148,"147,Algorithm 1 MDP-DIV learning,null,null",null,null
149,"148,Algorithm 2 SampleEpisode,null,null",null,null
150,"149,""Input: Labeled training set D ,"""" {(q(n), X (n), (n))}nN"""",""""1, learning rate , discount factor  , and reward function R"""""",null,null",null,null
151,"150,""Input: Parameters  ,"""" {Vq, U, V, W}, q, X , , and R Output: An episode"""""",null,null",null,null
152,"151,""Output:  ,"""" {Vq, U, V, W}"""""",null,null",null,null
153,"152,""1: Initialize s  [, X ,  (Vq q)]{Equation (1)}"",null,null",null,null
154,"153,""1: Initialize  ,"""" {Vq, U, V, W}  random values in [-1, 1] 2: repeat"""""",null,null",null,null
155,"154,""2: M  |X | 3: E , (){empty episode}"",null,null",null,null
156,"155,""3: for all (q, X , )  D do"",null,null",null,null
157,"156,""4: for t , 0 to M - 1 do"",null,null",null,null
158,"157,4:,null,null",null,null
159,"158,""(s0, a0, r1, · · · , sM-1, aM-1, rM )  SampleEpisode(, q, X , , R) 5: A  A(s) {Possible actions according to X in state s}"",null,null",null,null
160,"159,""{Algorithm (2), and M , |X |}"",null,null",null,null
161,"160,6: for all a  A do,null,null",null,null
162,"161,5:,null,null",null,null
163,"162,""for t , 0 to M - 1 do"",null,null",null,null
164,"163,7:,null,null",null,null
165,"164,P(a)   (a|s; ),null,null",null,null
166,"165,6:,null,null",null,null
167,"166,Gt ,null,null",null,null
168,"167,""M -1-t k ,0"",null,null",null,null
169,"168, k rt +k+1,null,null",null,null
170,"169,{Equation,null,null",null,null
171,"170,(4)},null,null",null,null
172,"171,7:,null,null",null,null
173,"172,   +  t Gt  log  (at |st ; ) {Equation (5)},null,null",null,null
174,"173,8:,null,null",null,null
175,"174,end for,null,null",null,null
176,"175,""8: end for 9: Sample an action a^  A, according to P 10: r  R(s, a^){Calculation on the basis of }"",null,null",null,null
177,"176,9: end for,null,null",null,null
178,"177,""11: Append (s, a^, r ) to the tail of E"",null,null",null,null
179,"178,10: until converge,null,null",null,null
180,"179,""12: [Z, X , h]  s"",null,null",null,null
181,"180,11: return ,null,null",null,null
182,"181,""13: s  Z  {xm(a^)}, X \ {xm(a^)},  (Vxm(a^) + Wh)"",null,null",null,null
183,"182,14: end for,null,null",null,null
184,"183,""15: return E ,"""" (s0, a0, r1, · · · , sM-1, aM-1, rM )"""""",null,null",null,null
185,"184,""e gradient of MDP-DIV at time step t is  log  (at |st ; ), which the direction that most increase the probability of repeating"",null,null",null,null
186,"185,""the action at on future visits to state st , and is de ned as"",null,null",null,null
187,"186,""where Vht-1 can be unrolled in a similar way. At t ,"""" 0, Vh0 is:"""""",null,null",null,null
188,"187,"" log  (at |st ; ) ,  f (at |st )-"",null,null",null,null
189,"188,""a At  f (a|st ) exp { f (a|st )} , a At exp { f (a|st )}"",null,null",null,null
190,"189,(5),null,null",null,null
191,"190,""where f (a|st ) ,"""" xTm(a)(Uht ), and  f (a|st ) """","""" {U f (a|st ),"""""",null,null",null,null
192,"191,""Vq f (a|st ), V f (a|st ), W f (a|st ) , where"",null,null",null,null
193,"192,""Vh0 , V (Vq q) ,"""" 0K,L,1,K , where 0K,L,1,K  RK ×L×1×K is a tensor of zeros."""""",null,null",null,null
194,"193,""Wht , W (Vxm(at-1) + Wht -1) , diag(ht  (1 - ht )) W(Vxm(at-1) + Wht -1)"",null,null",null,null
195,"194,""U f (a|st ) , xm(a)hTt ."",null,null",null,null
196,"195,"","""" diag(ht  (1 - ht )) IK,K,K,K ht -1 + (Wht -1) WT ,"""""",null,null",null,null
197,"196,""As for Vq f (a|st ), V f (a|st ), and W f (a|st ), they can be calculated in a similar way:"",null,null",null,null
198,"197,""Vq f (a|st ) ,"""" Vq ht UT xm(a),"""""",null,null",null,null
199,"198,""where IK,K,K,K  RK ×K ×K ×K is an identity tensor, and Wht -1 can be unrolled in a similar way. At t ,"""" 0, Wh0 is:"""""",null,null",null,null
200,"199,""Wh0 , W (Vq q) ,"""" 0K,K,1,K ,"""""",null,null",null,null
201,"200,""V f (a|st ) ,"""" (Vht ) UT xm(a), W f (a|st ) """","""" (Wht ) UT xm(a), where Vq ht , Vht , and Wht can be calculated recursively: Vq ht """", Vq  (Vxm(at-1) + Wht -1)"",null,null",null,null
202,"201,""where 0K,K,1,K  RK ×K ×1×K is a tensor of zeros. Compared with conventional REINFORCE algorithm, MDP-DIV"",null,null",null,null
203,"202,""is based on a modi ed MDP model in which the user state of perceived utility is initialized with query and modeled in a recurrent manner. us, in the training phase, MDP-DIV needs to estimate the policy function, as well as the functions for state initialization"",null,null",null,null
204,"203,"", diag(ht  (1 - ht )) Vq (Wht -1) ,"""" diag(ht  (1 - ht )) Vq ht -1 WT ,"""""",null,null",null,null
205,"204,""and state transition. In [16], similar idea was presented for extracting information from images. In this paper we adapt the model for the task of search result diversi cation."",null,null",null,null
206,"205,""where 1 is an K-dimensional vector of ones, operator """""""" denotes the element-wise vector product, operator """"diag"""" generates an K ×K diagonal matrix according to the input vector, and Vq ht-1 can be further unrolled in a similar way. At t ,"""" 0, Vq h0 is:"""""",null,null",null,null
207,"206,""Vq h0 , Vq  (Vq q) ,"""" diag(h0  (1 - h0))IK,L,K,Lq, where IK,L,K,L  RK×L×K×L is an identity tensor."""""",null,null",null,null
208,"207,""Vht , V (Vxm(at-1) + Wht -1) , diag(ht  (1 - ht )) V(Vxm(at-1) + Wht -1) ,"""" diag(ht  (1 - ht )) IK,L,K,Lxm(at-1) + (Vht -1) WT ,"""""",null,null",null,null
209,"208,4.3 Online ranking,null,null",null,null
210,"209,""In the online ranking, the ranking system receives a user query"",null,null",null,null
211,"210,""q and the associated documents X ,"""" {x1, · · · , xM }. Since there exists no human label for calculating the immediate rewards, the"""""",null,null",null,null
212,"211,system fully relies on the learned policy  for generating the diverse,null,null",null,null
213,"212,""ranking, as shown in Algorithm 3. A er initializing with q, the"",null,null",null,null
214,"213,algorithm makes a sequence of greedy decisions: at each step the,null,null",null,null
215,"214,""action with the maximal probability is chosen (line 5 of Alglrithm 3),"",null,null",null,null
216,"215,and the action in return update the state for choosing the next action,null,null",null,null
217,"216,(line 7 and line 8 of Algorithm 3).,null,null",null,null
218,"217,e time complexity of the online ranking algorithm is of,null,null",null,null
219,"218,O,null,null",null,null
220,"219,""min{K L2 ,"",null,null",null,null
221,"220,LK,null,null",null,null
222,"221,2,null,null",null,null
223,"222,},null,null",null,null
224,"223,M,null,null",null,null
225,"224,(2+M 4,null,null",null,null
226,"225,),null,null",null,null
227,"226,+,null,null",null,null
228,"227,(M,null,null",null,null
229,"228,-,null,null",null,null
230,"229,1)(K 2,null,null",null,null
231,"230,+,null,null",null,null
232,"231,KL),null,null",null,null
233,"232,per query.,null,null",null,null
234,"233,e rst,null,null",null,null
235,"234,539,null,null",null,null
236,"235,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
237,"236,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
238,"237,Algorithm 3 MDP-DIV online ranking,null,null",null,null
239,"238,""Input: Parameters  ,"""" {Vq, U, V, W}, query q, documents X Output: Permutation of documents """""",null,null",null,null
240,"239,""1: Initialize s  [, X ,  (Vq q)]{Equation (1)} 2: M  |X | 3: for t ,"""" 0 to M - 1 do 4: A  A(s) {Possible actions according to X in state s} 5: a^  arg maxa A  (a|s; ){Choosing most possible action} 6:  [t + 1]  m(a^){Document xm(a^) is ranked at t + 1} 7: [Z, X , h]  s 8: s  [Z  {xm(a^)}, X \ {xm(a^)},  (Vxm(a^) + Wh)] 9: end for 10: return """""",null,null",null,null
241,"240,""part corresponds to calculating the policy for all of the possible actions at each iteration and the second part corresponds to updating the state for the next iteration. e term min{KL2, LK2} is for calculating the matrix multiplication xTm(at )U ht in the policy with di erent ways. In most cases L is larger than K. Please note that the online ranking algorithm actually runs M - 1 iterations for ranking M documents, because at the last iteration A(sM-1) contains only one action. Usually, K and L are not very large, e.g, we set K , 5 and L ,"""" 100 in our experiments. us, the online ranking algorithm is e cient if the candidate set is not very large. In our experiments, on average it takes about 20 milliseconds for ranking about 200 documents, on a server with 24GB memory and two Intel Xeon E5410 2.33GHz ad-Core processors. Note that in the analysis the time for document embedding is not taken into consideration as the document embeddings can be calculated o ine."""""",null,null",null,null
242,"241,4.4 eoretical analysis,null,null",null,null
243,"242,""e learning phase of MDP-DIV tries to optimize general diversity evaluation measures with reinforcement learning. e measures can be -DCG and S-recall, or any other measures that can be calculated at each of the ranking position. We explain why this is the case."",null,null",null,null
244,"243,""In the training, Monte-Carlo stochastic gradient ascent is used to conduct the optimization. Given a query q in the training set, we want to maximize the value V , which is the expected return of the query:"",null,null",null,null
245,"244,""max V (q) ,"""" E G0,"""""",null,null",null,null
246,"245,""where G0 is the discounted sum of the rewards, starting from position 0, as de ned in Equation (4). Please note G0 is the diversity evaluation measure if  ,"""" 1. us, maximizing V (q) is actually maximizing the expected diversity evaluation measure for the query."""""",null,null",null,null
247,"246,""According to the policy gradient theorem presented in [22], chapter 13, the gradient of the performance metric with respect to the parameters  on each query can be calculated as"",null,null",null,null
248,"247,""V () ,"""" Es,a Q (s, a) (a|s),"""""",null,null",null,null
249,"248,""where  is the discounted state distribution given a query q and model parameters, which is de ned as:"",null,null",null,null
250,"249,""(s |q; ) ,""""  t -1p(s0  s, t |qn ; ),"""""",null,null",null,null
251,"250,""t ,1"",null,null",null,null
252,"251,· · · q,null,null",null,null
253,"252,""h0 , (Vqq)"",null,null",null,null
254,"253,""T (a0, s0)"",null,null",null,null
255,"254,""s0 ,"""" [Z0, X0, h0]"""""",null,null",null,null
256,"255,""T (aM 2, sM 2) sM 1 ,"""" [ZM 1, XM 1, hM 1]"""""",null,null",null,null
257,"256,a0,null,null",null,null
258,"257,"","",null,null",null,null
259,"258,arg,null,null",null,null
260,"259,max,null,null",null,null
261,"260,a,null,null",null,null
262,"261,(a|s0),null,null",null,null
263,"262,xm(a0),null,null",null,null
264,"263,aM,null,null",null,null
265,"264,1,null,null",null,null
266,"265,"","",null,null",null,null
267,"266,arg,null,null",null,null
268,"267,max,null,null",null,null
269,"268,a,null,null",null,null
270,"269,(a|sM,null,null",null,null
271,"270,1),null,null",null,null
272,"271,xm(aM 1),null,null",null,null
273,"272,"" [1] , m(a0)"",null,null",null,null
274,"273,···,null,null",null,null
275,"274,"" [M ] , m(aM 1)"",null,null",null,null
276,"275,Figure 2: Online document ranking in MDP-DIV.,null,null",null,null
277,"276,""where p(s0  s, t |qn ; ) is the probability of transitioning from the initial state s0 given the query q in t steps [22]. Q (s, a) is the expected return starting from s, taking the action a and therea er following the policy  :"",null,null",null,null
278,"277,""Q (s, a) , E [Gt |st ,"""" s, at """", a]."",null,null",null,null
279,"278,Monte-Carlo method is used to estimate the gradient. Speci -,null,null",null,null
280,"279,""cally, given a sampled episode s0, a0, r1, · · · , sM-1, aM-1, rM and a speci c time step t, the gradient can be estimated as [22]"",null,null",null,null
281,"280,""V () ,  sample t"",null,null",null,null
282,"281,"" (a|st )Q (st , a)"",null,null",null,null
283,"282,a A(st ),null,null",null,null
284,"283,"", t"",null,null",null,null
285,"284, (a|st ) ·,null,null",null,null
286,"285,a A(st ),null,null",null,null
287,"286,Q,null,null",null,null
288,"287,(st,null,null",null,null
289,"288,"","",null,null",null,null
290,"289,a),null,null",null,null
291,"290, (a|st  (a|st ),null,null",null,null
292,"291,),null,null",null,null
293,"292,"",  sample"",null,null",null,null
294,"293,t,null,null",null,null
295,"294,Q,null,null",null,null
296,"295,(st,null,null",null,null
297,"296,"","",null,null",null,null
298,"297,at,null,null",null,null
299,"298,),null,null",null,null
300,"299, (at |st  (at |st ),null,null",null,null
301,"300,),null,null",null,null
302,"301,"",  sample t Gt  log  (at |st )."",null,null",null,null
303,"302,""e rst ,"""" sample replaces s by its sample st , which is sampled according to ; the second """","""" sample replaces a by its sample at , which is sampled according to  ; and the third """", sample replaces the therea er"",null,null",null,null
304,"303,decision process guided by  with the sampled episode. Note that,null,null",null,null
305,"304,""E [Gt |st , at ] ,"""" Q (st , at ) and  log  (at |st ) """","",null,null",null,null
306,"305, (at |st (at |st ),null,null",null,null
307,"306,),null,null",null,null
308,"307,.,null,null",null,null
309,"308,We can see that the updating rule in Algorithm 1 exactly follows,null,null",null,null
310,"309,""the estimated gradients presented above. us, we can conclude"",null,null",null,null
311,"310,MDP-DIV tries to optimize general diversity evaluation measures,null,null",null,null
312,"311,""with Monte-Carlo stochastic gradient ascent when  , 1."",null,null",null,null
313,"312,4.5 Advantages,null,null",null,null
314,"313,""MDP-DIV provides an elegant approach to modeling user's dynamic state on the perceived utility during the browsing of the diverse ranking results. More importantly, it is a method that can be justi-"",null,null",null,null
315,"314,""ed from the theoretical viewpoint, as discussed above. In addition, MDP-DIV has several other advantages when compared with the existing diverse ranking learning methods such as SVM-DIV, R-LTR and PAMM etc."",null,null",null,null
316,"315,""First, MDP-DIV can conduct an end-to-end learning of the diverse ranking model, which achieves a model with no need of handcra ing relevance features and novelty features. e inputs to the ranking model are the preliminary representations of the queries and the documents, e.g., the distributed representations learned by the doc2vec model. In contrast, all existing diverse ranking learning methods heavily depend on the handcra ed relevance"",null,null",null,null
317,"316,540,null,null",null,null
318,"317,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
319,"318,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
320,"319,""features and/or novelty features. It has been widely observed that high quality features are critical for constructing diverse ranking, while designing the features, especially designing the novelty features, is very di cult in real applications [25]. MDP-DIV solves the issue via learning a ranking model that needs only the preliminary representations of the queries and the documents."",null,null",null,null
321,"320,""Second, MDP-DIV utilizes both the immediate rewards and the long-term returns as the supervision information during its training. Speci cally, given an episode, the parameters are updated a er receiving each of the immediate rewards (line 5-8 of Algorithm 1). Meanwhile, the updating rule also utilizes the long-term return Gt , which accumulates all of the future rewards (line 6-7 of Algorithm 1), to re-scale the step size. In contrast, existing methods that directly optimize evaluation measures are only based on a evaluation measure calculated at a xed position [24, 26] on the basis of whole ranking. Our empirical analysis in Section 5.3.2 also showed that training with both the rewards and the returns can achieve be er ranking accuracies."",null,null",null,null
322,"321,""ird, MDP-DIV makes use of a uni ed criterion, the additional utility a search user can perceive, for selecting documents at each iteration. In contrast, the criterion adopted by most existing methods, e.g., the marginal relevance, consists of two individual factors: the relevance and the novelty. Heuristic diverse ranking model x AD tried to replace these two factors with """"the relevance to the underlying sub-queries"""", which has shown to be more reasonable and e ective. In this paper, we also showed that under the MDP framework, the document selection criterion can be uni ed to """"the perceived utility"""", which has shown to be simple in concept and be powerful in the real applications."",null,null",null,null
323,"322,5 EXPERIMENTS,null,null",null,null
324,"323,""We conducted experiments to test the performances of MDP-DIV using a combination of four TREC benchmark datasets: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), TREC 2011 Web Track (WT2011), and TREC 2012 Web Track (WT2012)."",null,null",null,null
325,"324,5.1 Experimental settings,null,null",null,null
326,"325,""e training of MDP-DIV model need lots of labeled queries because it has a large number of parameters. In experiments, for e ective training of the model parameters, we combined four TREC datasets, achieving a new dataset with 200 queries, and in total about 45,000 labeled documents. Each query includes several subtopics identi ed by the TREC assessors. e document relevance labels are made at the subtopic level and the labels are binary3."",null,null",null,null
327,"326,""All the experiments were carried out on the ClueWeb09 Category B data collection4, which is comprised of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied to the documents as preprocessing. For each query, the initial ranking is generated by"",null,null",null,null
328,"327,""ery-likelihood language model[14]. We conducted 5-fold crossvalidation experiments. We randomly split the queries into ve even subsets. At each fold, three subsets were used for training, one was used for validation, and one was used for testing. e results reported were the average over the ve trials."",null,null",null,null
329,"328,3WT2011 has graded judgements. In this paper we treat them as binary. 4h p://boston.lti.cs.cmu.edu/data/clueweb09,null,null",null,null
330,"329,e TREC o cial evaluation metrics for the diversity task were,null,null",null,null
331,"330,""used in the experiments, including the ERR-IA [3] and -NDCG [4]."",null,null",null,null
332,"331,ey measure the diversity of a result list by explicitly rewarding,null,null",null,null
333,"332,diversity and penalizing redundancy observed at every rank. Follo-,null,null",null,null
334,"333,""wing the default se ings in o cial TREC evaluation program, the"",null,null",null,null
335,"334,parameter  in these evaluation measures are set to 0.5. We also,null,null",null,null
336,"335,used traditional diversity measures of subtopic recall (denoted as,null,null",null,null
337,"336,S-recall) [29]. All of the measures are computed over the top-k,null,null",null,null
338,"337,""search results (k , 5 and k , 10)."",null,null",null,null
339,"338,We compared MDP-DIV with several state-of-the-arts baselines,null,null",null,null
340,"339,""in search result diversi cation, including the heuristic methods:"",null,null",null,null
341,"340,MMR [2]: a heuristic approach in which the document is se-,null,null",null,null
342,"341,lected according to maximal marginal relevance.,null,null",null,null
343,"342,x AD [19]: a representative approach which explicitly models,null,null",null,null
344,"343,di erent aspects underlying the original query in the form of sub-,null,null",null,null
345,"344,queries.,null,null",null,null
346,"345,PM-2 [5]: a method of optimizing proportionality for search,null,null",null,null
347,"346,result diversi cation.,null,null",null,null
348,"347,We also compared MDP-DIV with the learning methods:,null,null",null,null
349,"348,SVM-DIV [27]: a learning approach which utilizes structural,null,null",null,null
350,"349,SVMs to optimize the subtopic coverage.,null,null",null,null
351,"350,R-LTR [31]: a state-of-the-art learning approach developed in,null,null",null,null
352,"351,the relational learning to rank framework. Following the practice,null,null",null,null
353,"352,""in [31], we used the results of R-LTRmin in which the relation function was de ned as the minimal distance of the candidate"",null,null",null,null
354,"353,document to the selected documents,null,null",null,null
355,"354,PAMM [24]: another learning algorithm under R-LTR frame-,null,null",null,null
356,"355,work. PAMM directly optimizes diversity evaluation measure using,null,null",null,null
357,"356,""structured Perceptron. Following the practice in [24], we con gu-"",null,null",null,null
358,"357,red the PAMM algorithm to directly optimize -NDCG@10 in our,null,null",null,null
359,"358,""experiments, and set the number of sampled positive rankings per query  + , 5 and the number of sampled negative rankings per"",null,null",null,null
360,"359,""query  - , 20."",null,null",null,null
361,"360,NTN-DIV: a learning approach which automatically learns no-,null,null",null,null
362,"361,velty features based on neural tensor networks. Following the,null,null",null,null
363,"362,""practice in [25], we con gured the learning of NTN-DIV algorithm"",null,null",null,null
364,"363,to directly optimize -NDCG@10 and the number of tensor slices,null,null",null,null
365,"364,is 7.,null,null",null,null
366,"365,MDP-DIV and the baseline of NTN-DIV need preliminary re-,null,null",null,null
367,"366,presentations of the queries and the documents as their inputs. In,null,null",null,null
368,"367,""the experiments, we used the query vector and document vector"",null,null",null,null
369,"368,generated by the doc2vec [10] to represent the document. Doc2vec,null,null",null,null
370,"369,model was trained on all of the documents in Web Track dataset,null,null",null,null
371,"370,""and the number of vector dimensions were set to 100. For training the model, we used the distributed bag of words (DBOW) model5."",null,null",null,null
372,"371,e learning rate is set to 0.025 and the window size is set to 8.,null,null",null,null
373,"372,e MDP-DIV also has some parameters. e reward function,null,null",null,null
374,"373,""in MDP-DIV was set as either R -DCG or RS-recall, denoted as MDPDIV(-DCG) and MDP-DIV(S-recall), respectively. In all of the ex-"",null,null",null,null
375,"374,""periments, the learning rate  is tuned on the basis of the validation"",null,null",null,null
376,"375,""set. We set the discounting parameter  ,"""" 1, which means that the"""""",null,null",null,null
377,"376,""return is the undiscounted sum of the future rewards, which makes"",null,null",null,null
378,"377,""the long term return in Equation (4) becomes Gt ,"",null,null",null,null
379,"378,""M -1-t k ,0"",null,null",null,null
380,"379,rt,null,null",null,null
381,"380,+k +1 .,null,null",null,null
382,"381,It makes the training algorithm optimizes the diversity evaluation,null,null",null,null
383,"382,measure of -DCG and S-recall.,null,null",null,null
384,"383,5h p://radimrehurek.com/gensim/tutorial.html,null,null",null,null
385,"384,541,null,null",null,null
386,"385,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
387,"386,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
388,"387,5.2 Experimental results,null,null",null,null
389,"388,""Table 1 reports the performances of our approach and all of the baseline methods in terms of the six diversity performance metrics, including -NDCG@5, -NDCG@10, S-recall@5, S-recall@10, ERRIA@5, and ERR-IA@10. Boldface indicates the highest score among all runs. From the results we can see that, in terms of the six diversity evaluation metrics, both MDP-DIV(-DCG) and MDPDIV(S-recall) outperformed all of the baseline methods, including the heuristic method of MMR, x AD, PM-2 and learning methods of R-LTR, PAMM(-NDCG), and NTN-DIV(-NDCG). We conducted signi cance testing (t-test) on the improvements of our approaches over the best baseline NTN-DIV(-NDCG). e results indicate that the improvements are signi cant (p-value < 0.05), in terms of all of the evaluation measures."",null,null",null,null
390,"389,""Comparing the results of the MDP-DIV(-DCG) and MDP-DIV(Srecall), we can see that MDP-DIV(-DCG) trained with -DCG (se ing -DCG as reward function) performed be er in terms of NDCG@5 and -NDCG@10. Similarly, MDP-DIV(S-recall) trained with S-recall (se ing S-recall as reward function) performed be er in terms of S-recall@5 and S-recall@10. e results indicate that MDP-DIV can indeed enhance diverse ranking performance in terms of a measure by using the measure as reward function in training6. e result agrees well with the theoretical analysis shown in Section 4.4."",null,null",null,null
391,"390,5.3 Discussion,null,null",null,null
392,"391,""We conducted experiments to show the reasons that MDP-DIV outperformed the baselines, using the results of MDP-DIV(-DCG) on one trial of the cross validation as examples."",null,null",null,null
393,"392,""5.3.1 E ects of modeling user perceived utility. We analyzed how the user state on the perceived utility e ects the selection of documents in MDP-DIV. Speci cally, based on the trained MDP-DIV model, we tracked the online ranking process for query number 93 """"ambiguous"""", which contains ve subtopics. Figure 3 shows the details of the rst three document selection steps, including the transition of the user dynamic state hi , the ranking score f (at |st ) ,"""" xTm(at )Uht for each of the actions7, and the constructed document ranking. Due to the space limitation, we only showed the ve top ranked documents d1, · · · , d5, corresponding to the documents of enwp03-28-04544, en0007-80-16124, en0094-80-42411, en0006-08-03878, and en0010-24-38000 in the Clueweb09 collection, respectively. e subtopics covered by each of the documents are shown in the square brackets."""""",null,null",null,null
394,"393,""From Figure 3, we can see that ht was updated a er choosing each action, indicating the changes of the user state a er perceiving the utility provided by the selected document. At step 0, the selected document d2 covered subtopics 3 and 5. At step 1, as the consequence of the action the user state was updated, and the ranking score of d4 (with the covered subtopic 5) was suppressed from 0.46 to 0.35, while the ranking scores of the other three documents (d1, d3, and d5, with uncovered subtopics) were promoted. e results indicate that the user state h1 captured the utility provided"",null,null",null,null
395,"394,""6Here we consider  -NDCG and  -DCG as """"one"""" measure as the only di erence between them is the normalization factor. 7In the online ranking, the selection of actions can be implemented as directly based on the ranking scores instead of based on the probabilities  (at |st )."",null,null",null,null
396,"395,h0,null,null",null,null
397,"396,h1,null,null",null,null
398,"397,0.50,null,null",null,null
399,"398,0.56,null,null",null,null
400,"399,0.53,null,null",null,null
401,"400,0.53,null,null",null,null
402,"401,0.49,null,null",null,null
403,"402,0.55,null,null",null,null
404,"403,0.40,null,null",null,null
405,"404,0.51,null,null",null,null
406,"405,q,null,null",null,null
407,"406,0.60,null,null",null,null
408,"407,doc:subtopics,null,null",null,null
409,"408,ranking score,null,null",null,null
410,"409,0.61,null,null",null,null
411,"410,doc:subtopics,null,null",null,null
412,"411,ranking score,null,null",null,null
413,"412,""{ d1 : [2] 0.51 d2 : [3, 5] 1.19 d3 : [1, 4] 1.05 d4 : [5] 0.46"",null,null",null,null
414,"413,""{ d1 : [2] 0.84 d3 : [1, 4] 1.07 d4 : [5] 0.35 d5 : [1, 4] 1.12"",null,null",null,null
415,"414,""d5 : [1, 4] 1.01"",null,null",null,null
416,"415,ranking: h,null,null",null,null
417,"416,""d2 : [3, 5]"",null,null",null,null
418,"417,""d5 : [1, 4]"",null,null",null,null
419,"418,h2,null,null",null,null
420,"419,0.53,null,null",null,null
421,"420,0.48,null,null",null,null
422,"421,0.51,null,null",null,null
423,"422,···,null,null",null,null
424,"423,0.42,null,null",null,null
425,"424,0.59,null,null",null,null
426,"425,doc:subtopics,null,null",null,null
427,"426,ranking score,null,null",null,null
428,"427,""{ d1 : [2] 0.89 d3 : [1, 4] 0.84"",null,null",null,null
429,"428,d4 : [5] 0.34,null,null",null,null
430,"429,d1 : [2],null,null",null,null
431,"430,··· i,null,null",null,null
432,"431,Figure 3: e online ranking process for query number 93.,null,null",null,null
433,"432,""by d2, which made the ranking model focusing on documents that can provide the largest amount of new information. As the result, d5, which contains two new subtopics 1 and 4, was selected at step 1. Similarly, at step 2 state vector h2 captured the utility provided by d2 and d5 and making the model to select d1, which contains a new subtopic 2. In contrast, the ranking scores for d3 and d4, whose subtopics had been covered by the preceding documents, were suppressed. e phenomenon clearly indicates MDP-DIV can e ectively capture the user perceived utility of information in its state, and utilize it for generating diverse rankings."",null,null",null,null
434,"433,""5.3.2 E ects of using immediate rewards in training. One advantage of MDP-DIV is that it has the ability of utilizing the immediate rewards as the supervision in training, which makes the training more e ective and e cient. We tried to verify the e ectiveness and e ciency of using the immediate rewards in the training phase. Speci cally, we modi ed the training Algorithm 1 so that the model parameters were updated only at the end of an episode (i.e., se ing the iteration variable t in the line 5 of Algorithm 1 starts from M). In this way, the modi ed algorithm only utilizes the long term return of the whole episode for training, denoted as """"MDPDIV(ReturnOnly)"""". Figure 4 shows the performance curves of MDPDIV(-DCG) and MDP-DIV(ReturnOnly) trained with -DCG, on the test data of one trail in the cross validation. e performances of other baseline methods on the same cross validation trail are also shown in the gure."",null,null",null,null
435,"434,""From the results, we can see that MDP-DIV(-DCG) outperformed the MDP-DIV(ReturnOnly) in terms of both convergency rate and the converged performances. e result indicates that utilizing the immediate rewards in MDP-DIV(-DCG) leads to an e ective and e cient training algorithm. Note that in contrast, most existing learning approaches to diverse ranking, including R-LTR, PAMM, and NTN-DIV, can only utilize the accumulated information on the whole ranking as supervision in their training phase. For example, R-LTR uses the likelihood of the whole document rankings, and PAMM uses the prede ned evaluation measure calculated based on the whole ranking. e experimental results showed one reason why MDP-DIV(-DCG) can outperform these baselines."",null,null",null,null
436,"435,""We also noticed that the converged MDP-DIV(ReturnOnly) model still outperformed the baseline methods including SVM-DIV, R-LTR, PAMM, and NTN-DIV, indicating that modeling the user's dynamic state on the perceived utility with MDP is e ective."",null,null",null,null
437,"436,542,null,null",null,null
438,"437,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
439,"438,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
440,"439,Table 1: Performance comparison of all methods on TREC web track datasets.,null,null",null,null
441,"440,Method,null,null",null,null
442,"441,MMR x AD PM-2 SVM-DIV R-LTR PAMM( -NDCG) NTN-DIV( -NDCG) MDP-DIV(S-recall) MDP-DIV( -DCG),null,null",null,null
443,"442, -NDCG@5,null,null",null,null
444,"443,0.2753 0.3165 0.3047 0.3030 0.3498 0.3712 0.3962 0.4156 0.4189,null,null",null,null
445,"444, -NDCG@10,null,null",null,null
446,"445,0.2979 0.3941 0.3730 0.3699 0.4132 0.4327 0.4577 0.4734 0.4762,null,null",null,null
447,"446,S-recall@5,null,null",null,null
448,"447,0.4388 0.4933 0.4910 0.5122 0.5397 0.5561 0.5817,null,null",null,null
449,"448,0.6123 0.6102,null,null",null,null
450,"449,S-recall@10,null,null",null,null
451,"450,0.5151 0.6043 0.6012 0.6230 0.6511 0.6612 0.6872,null,null",null,null
452,"451,0.7155 0.7117,null,null",null,null
453,"452,ERR-IA@5,null,null",null,null
454,"453,0.2005 0.2314 0.2298 0.2268 0.2521 0.2619 0.2773 0.2963 0.2988,null,null",null,null
455,"454,ERR-IA@10,null,null",null,null
456,"455,0.2309 0.2890 0.2814 0.2726 0.3011 0.3029 0.3285 0.3477 0.3494,null,null",null,null
457,"456,0.55,null,null",null,null
458,"457,3,null,null",null,null
459,"458,0.45,null,null",null,null
460,"459,0.35,null,null",null,null
461,"460,0.25,null,null",null,null
462,"461,0.15 0,null,null",null,null
463,"462,MDP-DIV(ReturnOnly) NTN-DIV PAMM R-LTR SVM-DIV,null,null",null,null
464,"463,500,null,null",null,null
465,"464,1000,null,null",null,null
466,"465,1500,null,null",null,null
467,"466,2000,null,null",null,null
468,"467,2500,null,null",null,null
469,"468,iteration,null,null",null,null
470,"469,""Figure 4: e performance curves on the test data for MDPDIV(-DCG), and the modi ed MDP-DIV(-DCG) in which the training only involves the long-term returns. e performances of other baselines are shown as horizontal lines."",null,null",null,null
471,"470,2.5,null,null",null,null
472,"471,2,null,null",null,null
473,"472,train(sample) train(arg max) test(arg max),null,null",null,null
474,"473,1.5,null,null",null,null
475,"474,0,null,null",null,null
476,"475,500,null,null",null,null
477,"476,1000,null,null",null,null
478,"477,1500,null,null",null,null
479,"478,2000,null,null",null,null
480,"479,2500,null,null",null,null
481,"480,iteration,null,null",null,null
482,"481,""Figure 5: e performance curves in terms of -DCG on the training data (""""train(arg max)"""") and the test data (""""test(arg max)""""). e average performances of the sampled rankings over all training queries are also shown (""""train(sample)"""")."",null,null",null,null
483,"482,""5.3.3 Analysis of convergence and online ranking criterion. We conducted experiments to test whether the ranking accuracy in terms of the evaluation measure can be continuously improved, as the training of MDP-DIV goes on."",null,null",null,null
484,"483,""Speci cally, we tested the MDP-DIV(-DCG) models generated at each of the training iteration in one trail of the cross validation."",null,null",null,null
485,"484,""e performances in terms of -DCG at the last position of the whole document ranking is reported. For each model, the average performances over all of the training queries (or the testing queries) are reported. Figure 5 shows the performance curves on the training data (solid red line and denoted as """"train(arg max)"""") and on the test data (dashed yellow line and denoted as """"test(arg max)""""). For these two curves, the document rankings for the queries are generated by the online ranking Algorithm 3. e document rankings can also be generated through sampling during the training, via the episode sampling Algorithm 2. e average performances of the sampled rankings for all the training queries are also shown in the"",null,null",null,null
486,"485,""gure (blue dots and denoted as """"train(sample)""""). From the results shown in Figure 5, we can see that on both of"",null,null",null,null
487,"486,""the training set and test set, the ranking accuracies of MDP-DIV(DCG) steadily improves, as the training goes on. e experimental"",null,null",null,null
488,"487,""results also showed that the ranking accuracies of the sampled rankings (by Algorithm 2) has an obvious trend of steadily improving with some random noise, as the training goes on."",null,null",null,null
489,"488,""Comparing the sampled rankings (""""train(sample)"""") and the ranking generated by the online ranking algorithm (""""train(arg max""""), we can see that at the beginning of the training phase, the sampled rankings can achieve be er -DCG values than the rankings generated by the online ranking algorithm, on the basis of the training queries. As the training went on and a er about 200 iterations, the online ranking algorithm outperformed the sampling method, and the trend remains to the end of the training. e phenomenon was repeated in other experiments. We analyzed the reasons."",null,null",null,null
490,"489,""e online ranking algorithm (Algorithm 3) fully trusts the learned ranking model when generating the document ranking, i.e., a^  arg maxa A  (a|s; ). In contrast, the sampled rankings are generated according to the same ranking model while with some randomness. At the beginning of the training phase, the model parameters are far from their optimal values. In many cases, fully trusting the policy leads to bad decisions and generating rankings with low performances. e sampling method, in contrast, may make be er decisions due to the random natural of sampling. As"",null,null",null,null
491,"490,543,null,null",null,null
492,"491,Session 5A: Retrieval Models and Ranking 3,null,null",null,null
493,"492,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
494,"493,""the training goes on, the model parameters gradually converge to nearly optimal values. Fully trusting the learned policy has the advantages of achieving stable and (nearly) optimal decisions in most cases. e sampling method, however, hurts from unstable results due to the random noise. e results clearly indicate that, fully trusting the learned model (as that of in Algorithm 3) in the online ranking phase is a good criterion, given the model is well trained."",null,null",null,null
495,"494,6 CONCLUSION AND FUTURE WORK,null,null",null,null
496,"495,""In this paper we have proposed a novel approach to learning diverse ranking model for search result diversi cation, referred to as MDP-DIV. In contrast to existing methods, MDP-DIV explicitly models the dynamic utility the search users perceived during the browsing of the ranking result. e dynamic utility is modeled with a continuous state MDP and the model parameters are estimated with reinforcement learning. MDP-DIV o ers several advantages: no need for handcra ing ranking features, optimizing diversity evaluation measures in training, utilizing both immediate rewards and long-term returns as supervision, and high accuracy in ranking. Experimental results based on the TREC benchmark datasets show that MDP-DIV can signi cantly outperform the baseline methods."",null,null",null,null
497,"496,7 ACKNOWLEDGEMENTS,null,null",null,null
498,"497,""e work was funded by the National Key R&D Program of China under Grant No. 2016QY02D0405, 973 Program of China under Grant No. 2014CB340401 and 2012CB316303, the National Natural Science Foundation of China (NSFC) under Grants No. 61232010, 61472401, 61433014, 61425016, and 61203298, the Key Research Program of the CAS under Grant No. KGZD-EW-T03-2, and the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102."",null,null",null,null
499,"498,REFERENCES,null,null",null,null
500,"499,""[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM '09). ACM, New York, NY, USA, 5­14."",null,null",null,null
501,"500,""[2] Jaime Carbonell and Jade Goldstein. 1998. e Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '98). ACM, New York, NY, USA, 335­336."",null,null",null,null
502,"501,""[3] Olivier Chapelle, Shihao Ji, Ciya Liao, Emre Velipasaoglu, Larry Lai, and SuLin Wu. 2011. Intent-based Diversi cation of Web Search Results: Metrics and Algorithms. Inf. Retr. 14, 6 (Dec. 2011), 572­592. DOI:h p://dx.doi.org/10.1007/ s10791- 011- 9167- 7"",null,null",null,null
503,"502,""[4] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ cher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '08). ACM, New York, NY, USA, 659­666."",null,null",null,null
504,"503,""[5] Van Dang and W. Bruce Cro . 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversi cation. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 65­74."",null,null",null,null
505,"504,[6] Sreenivas Gollapudi and Aneesh Sharma. 2009. An Axiomatic Approach for Result Diversi cation. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). 381­390.,null,null",null,null
506,"505,""[7] Shengbo Guo and Sco Sanner. 2010. Probabilistic Latent Maximal Marginal Relevance. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '10). ACM, New York, NY, USA, 833­834."",null,null",null,null
507,"506,""[8] Jiyin He, Vera Hollink, and Arjen de Vries. 2012. Combining Implicit and Explicit Topic Representations for Result Diversi cation. In Proceedings of the 35th"",null,null",null,null
508,"507,""International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 851­860. [9] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversi cation Based on Hierarchical Intents. In Proceedings of the"",null,null",null,null
509,"508,""24th ACM International on Conference on Information and Knowledge Management (CIKM '15). ACM, New York, NY, USA, 63­72. [10] oc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014. 1188­1196. h p://jmlr.org/ proceedings/papers/v32/le14.html [11] Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2009. Enhancing Diversity, Coverage and Balance for Summarization rough Structure Learning. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). ACM, New York, NY, USA, 71­80. [12] Zhongqi Lu and Qiang Yang. 2016. Partially Observable Markov Decision Process for Recommender Systems. CoRR abs/1608.07793 (2016). [13] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings of the 37th International ACM"",null,null",null,null
510,"509,""SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 587­596. [14] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu¨tze. 2008. Introduction to Information Retrieval. Cambridge University Press, NY, USA. [15] Lilyana Mihalkova and Raymond Mooney. 2009. Learning to Disambiguate Search eries from Short Sessions. In Machine Learning and Knowledge Discovery in Databases. Lecture Notes in Computer Science, Vol. 5782. Springer. [16] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent Models of Visual A ention. In Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS'14). MIT Press, Cambridge, MA, USA, 2204­2212. [17] Martin L. Puterman. 2008. Markov Decision Processes. John Wiley & Sons, Inc. [18] Filip Radlinski, Robert Kleinberg, and orsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 784­791. [19] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting ery Reformulations for Web Search Result Diversi cation. In Proceedings of the 19th International Conference on World Wide Web (WWW '10). 881­890. [20] Rodrygo L. T. Santos, Jie Peng, Craig Macdonald, and Iadh Ounis. 2010. Explicit Search Result Diversi cation through Sub-queries. Springer Berlin Heidelberg, Berlin, Heidelberg, 87­99. [21] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265­1295. [22] Richard S. Su on and Andrew G. Barto. 2016. Reinforcement Learning: An Introduction (2nd ed.). MIT Press. [23] Xiaojie Wang, Zhicheng Dou, Tetsuya Sakai, and Ji-Rong Wen. 2016. Evaluating Search Result Diversity Using Intent Hierarchies. In Proceedings of the 39th"",null,null",null,null
511,"510,""International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). ACM, New York, NY, USA, 415­424. [24] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). 113­122. [25] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). 395­404. [26] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM Trans. Intell. Syst. Technol. 8, 3, Article 41 (Jan. 2017), 26 pages. [27] Yisong Yue and orsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 1224­1231. [28] Yisong Yue and orsten Joachims. 2009. Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML '09). ACM, New York, NY, USA, 1201­1208. [29] Cheng Xiang Zhai, William W. Cohen, and John La erty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In"",null,null",null,null
512,"511,""Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval (SIGIR '03). 10­17. [30] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Contentfree Document Re-ranking. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 1139­1142. [31] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversi cation. In Proceedings of the 37th International"",null,null",null,null
513,"512,""ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 293­302."",null,null",null,null
514,"513,544,null,null",null,null
515,"514,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,An Extended Relevance Model for Session Search,null,null",null,null
4,"3,Nir Levine,null,null",null,null
5,"4,Technion - Israel Institute of Technology,null,null",null,null
6,"5,""Haifa, Israel 32000 levin.nir1@gmail.com"",null,null",null,null
7,"6,Haggai Roitman,null,null",null,null
8,"7,""IBM Research - Haifa Haifa, Israel 31905 haggai@il.ibm.com"",null,null",null,null
9,"8,Doron Cohen,null,null",null,null
10,"9,""IBM Research - Haifa Haifa, Israel 31905 doronc@il.ibm.com"",null,null",null,null
11,"10,ABSTRACT,null,null",null,null
12,"11,""e session search task aims at best serving the user's information need given her previous search behavior during the session. We propose an extended relevance model that captures the user's dynamic information need in the session. Our relevance modelling approach is directly driven by the user's query reformulation (change) decisions and the estimate of how much the user's search behavior a ects such decisions. Overall, we demonstrate that, the proposed approach signi cantly boosts session search performance."",null,null",null,null
13,"12,1 INTRODUCTION,null,null",null,null
14,"13,""We propose an extended relevance model for session search. Relevance models aim at identifying terms (words, concepts, etc) that are relevant to a given (user's) information need [5]. Within a session, user's information need, expressed as a sequence of one or more queries [1], may evolve over time. User's search behavior during the session may be utilized as an additional relevance feedback source by the underlying search system [1]. Given user's session history (i.e., previous queries, result impressions and clicks), the goal of the session search task is to best serve the user's newly submi ed query in the session [1]."",null,null",null,null
15,"14,""We derive a relevance model that aims at """"tracking"""" the user's dynamic information need by observing the user's search behavior so far during the session. To this end, the proposed relevance model is driven by the user's query reformulation decisions. Our relevance modelling approach relies on previous studies that suggest that user query change decisions may (at least partially) be explained by the previous user search behavior in the session [4, 9, 12]. We utilize the derived relevance model for re-ranking the search results that are retrieved for the current user information need in the session. Overall, we demonstrate that, our relevance modeling approach can signi cantly boost session search performance compared to many other alternatives that also utilize session data."",null,null",null,null
16,"15,2 RELATED WORK,null,null",null,null
17,"16,""Few previous works have also utilized the session context (i.e., previous queries, retrieved results and clicks) as an implicit feedback"",null,null",null,null
18,"17,Work was done during a summer internship in IBM Research - Haifa.,null,null",null,null
19,"18,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080664"",null,null",null,null
20,"19,""source for re ning the user's query [3, 8, 10, 11]. To this end, the query language model was either combined with the language models of previous queries [11] or retrieved (clicked) results [8, 10]. In addition, di erent query score aggregation strategies for session search were explored [3]. Yet, none of these previous works have actually considered the user's query change process itself as a possible implicit feedback source."",null,null",null,null
21,"20,""Several recent works have studied various query reformulation (change) behaviors during search sessions [4, 9, 12]. Among the various features that were studied, word-level features were found to best explain the changes in user queries during search sessions [4, 9]. A notable feature was found to be the occurrence of query (changed) words in the contents of results that the user previously viewed or clicked [4, 9, 12]."",null,null",null,null
22,"21,""Few previous works have also utilized query change for the session search task (e.g., [6, 12]). Common to such works is the modeling of user queries and their change as states and actions within various Reinforcement Learning inspired query weighting and aggregation schemes [7]; In this work we take a rather more """"traditional"""" approach, inspired by the relevance model framework [5]."",null,null",null,null
23,"22,3 APPROACH,null,null",null,null
24,"23,3.1 Session model,null,null",null,null
25,"24,""Session search is a multi-step process, where at each step t, the user may submit a new query qt . e search system then retrieves the top-k documents Dq[kt] from a given corpus D that best match the user's query1. e user may then examine the results list; each result usually includes a link to the actual content and is accompanied with a summary snippet. e user may also decide to click on one or more of the results in the list in order to examine their actual content. Let Cqt denote the corresponding set of clicked results in Dq[kt]. In case the user decides to continue and submits a subsequent query, step t ends and a new step t + 1 begins. Let Sn-1 represent the session history (i.e., user queries, retrieved result documents, and clicked results) that was """"recorded"""" prior to the current (latest) submi ed user query qn . On each step 1  t  n - 1, the session history is represented by a tuple St ,"""" Qt , Dt , Ct . Qt """","""" (q1, q2, . . . , qt ) is the sequence of queries submi ed by the user up to step t . Dt """","""" (Dq[k1], Dq[k2], . . . , Dq[kt]) is the corresponding sequence of (top-k) retrieved result lists. Ct """","""" (Cq1 , Cq2 , . . . , Cqt ) further represents the corresponding sequence of user clicks."""""",null,null",null,null
26,"25,""1With k , 10 in the TREC session track benchmarks [1] that we use later on in our evaluation."",null,null",null,null
27,"26,865,null,null",null,null
28,"27,Short Research Paper,null,null",null,null
29,"28,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
30,"29,3.2 Information need dynamics,null,null",null,null
31,"30,e session search task is to best answer the current user's query,null,null",null,null
32,"31,qn while considering Sn-1 [1]. Let I denote the user's (hidden) information need in the session. e goal of our relevance mod-,null,null",null,null
33,"32,""elling approach is, therefore, to be er capture the user's informa-"",null,null",null,null
34,"33,tion need I which may evolve during the session. In order to cap-,null,null",null,null
35,"34,""ture such dynamics, let It further represent the user's information"",null,null",null,null
36,"35,""need at step t. We now assume that, It depends both on the previ-"",null,null",null,null
37,"36,ous (dynamic) information need It-1 prior to query qt submission,null,null",null,null
38,"37,and the possible change in such need It,null,null",null,null
39,"38,def,null,null",null,null
40,"39,"","",null,null",null,null
41,"40,It -1  It ; It,null,null",null,null
42,"41,is,null,null",null,null
43,"42,assumed be to implied by the change the user has made to her pre-,null,null",null,null
44,"43,vious query qt -1 to obtain query qt .,null,null",null,null
45,"44,3.3 ery change as relevance feedback,null,null",null,null
46,"45,""We utilize the user's query reformulation (change) process during the session as an implicit relevance feedback for estimating the change in the user's information need It . As been suggested by previous works [4, 9, 12], user's query changed terms may actually occur in the contents of previously viewed (clicked) search results in St-1. is, therefore, may (partially) explain how the user decided to reformulate her query from qt-1 to qt [4, 9, 12]. Our proposed relevance model aims at exploiting such query changed term occurrences within the contents of previously viewed (clicked) results so as to discover those terms w (over some vocabulary V ) that are the most relevant to the current user's information need In . As a consequence, such terms may be used for query expansion aiming to be er serve the current user's information need In ."",null,null",null,null
47,"46,""Given query qt , compared to the previous query qt-1, there can be three main query change types, namely term retention, addition and removal [4, 9, 12]. User term retention, given by the set of terms that appear in both query qt and qt-1 and denoted qt, usually represent the (general) thematic aspects of the user's information need [4, 9, 12]. Added terms (denoted qt+) are those terms that the user added to query qt-1 to obtain query qt . A user may add new related terms that were encountered in previous results so as to improve the chance of nding relevant content [4]. On the other hand, a user may remove terms from a previous query qt-1 (further denoted qt-) in order to terminate a subtask or trying to improve bad performing queries [4]."",null,null",null,null
48,"47,3.4 Relevance model derivation,null,null",null,null
49,"48,""Similar to previous works on relevance models [5], our goal is to discover those terms w ( V ) that are the most relevant to the user's information need In ; To this end, given the user's current query qn and session history Sn-1, let Sn denote our estimate of the relevance (language) model. On each step 1  t  n, such estimation is given by the following rst-order autoregressive model:"",null,null",null,null
50,"49,p(w,null,null",null,null
51,"50,|,null,null",null,null
52,"51,St,null,null",null,null
53,"52,),null,null",null,null
54,"53,def,null,null",null,null
55,"54,"","",null,null",null,null
56,"55,""t p(w | St-1 ) + (1 - t )p(w | Ft ),"",null,null",null,null
57,"56,(1),null,null",null,null
58,"57,""where  Ft now denotes the feedback model which depends on the user's (reformulated) query qt . While St-1 estimates the dynamic information need prior to step t (i.e., It-1),  Ft captures the relative change in such need at step t (i.e., It )."",null,null",null,null
59,"58,t further controls the relative importance we assign to model,null,null",null,null
60,"59,""exploitation (i.e., St-1 ) versus model exploration (i.e.,  Ft ). t parameter is dynamically determined based on the relevance model's"",null,null",null,null
61,"60,self-clarity at step t [2]. Self-clarity estimates how much the prior,null,null",null,null
62,"61,""model St-1 already """"covers"""" the feedback model  Ft ; formally:"",null,null",null,null
63,"62,t,null,null",null,null
64,"63,def,null,null",null,null
65,"64,"","",null,null",null,null
66,"65,· exp-DK L (Ft,null,null",null,null
67,"66,""St -1 ),"",null,null",null,null
68,"67,(2),null,null",null,null
69,"68,""where   [0, 1] and DK L( Ft St-1 ) is the Kullback-Leibler divergence between the two (un-smoothed) language models [13]."",null,null",null,null
70,"69,""Finally, given qn , the current user's query in the session, we de-"",null,null",null,null
71,"70,rive the relevance model Sn by inductively applying Eq. 1 (with,null,null",null,null
72,"71, S0,null,null",null,null
73,"72,def,null,null",null,null
74,"73,"","",null,null",null,null
75,"74,0). We next derive the feedback model  Ft .,null,null",null,null
76,"75,3.5 Feedback model derivation,null,null",null,null
77,"76,""Our estimate of  Ft aims at discovering those terms (in qt , qt -1 or others in V ) that are most relevant to the change in user's dynamic"",null,null",null,null
78,"77,""information need from It-1 to It (i.e., It ). Given queries qt and qt-1, we rst classify their occurring terms w  according to their role in the query change. Let qt further denote the set of terms w  that are classi ed to the same type of query change (i.e., qt  {qt , qt +, qt -})."",null,null",null,null
79,"78,Our relevance model now relies on the fact that query changed,null,null",null,null
80,"79,terms may also occur within the contents of results that were pre-,null,null",null,null
81,"80,""viously viewed (or clicked) by the user [4, 9, 12]. erefore, on"",null,null",null,null
82,"81,""each step t, let Ft denote the set of results that are used for (implicit) relevance feedback. We determine the set of results to be"",null,null",null,null
83,"82,""included in Ft as follows. If up to step t < n there is at least one clicked result, then we assign Ft ,"""" 1j t Cqj . Otherwise, we rst de ne a pseudo information need Qt. Qt represents a (crude) estimate of the user's (dynamic) information need up to step t and"""""",null,null",null,null
84,"83,""is obtained by concatenating the text of all observed queries in Qt (with each query having the same importance, following [11]). We"",null,null",null,null
85,"84,then de ne Ft as the set of top-m results in 1j t Dqj with the,null,null",null,null
86,"85,highest query-likelihood given Qt (representing pseudo-clicks). Let,null,null",null,null
87,"86,p[µ,null,null",null,null
88,"87,](w,null,null",null,null
89,"88,|x,null,null",null,null
90,"89,),null,null",null,null
91,"90,def,null,null",null,null
92,"91,"","",null,null",null,null
93,"92,t,null,null",null,null
94,"93,f,null,null",null,null
95,"94,""(w,"",null,null",null,null
96,"95,x,null,null",null,null
97,"96,)+µ,null,null",null,null
98,"97,t,null,null",null,null
99,"98,f,null,null",null,null
100,"99,""(w , D) |D|"",null,null",null,null
101,"100,|x |+µ,null,null",null,null
102,"101,now denote the Dirichlet smoothed,null,null",null,null
103,"102,language model of text x with parameter µ [13]. Inspired by the,null,null",null,null
104,"103,""RM1 relevance model [5], we estimate  Ft as follows:"",null,null",null,null
105,"104,def,null,null",null,null
106,"105,""p(w | Ft ) ,"",null,null",null,null
107,"106,p[0](w |d ) ·,null,null",null,null
108,"107,""p(d |qt )p(qt ) , (3)"",null,null",null,null
109,"108,d Ft,null,null",null,null
110,"109,qt,null,null",null,null
111,"110,""where p(qt ) denotes the (prior) likelihood that, while reformulating query qt-1 into qt , the user will choose to either add (i.e., qt +), remove (i.e., qt -) or retain (i.e., qt ) terms. Such likelihood can be pre-estimated [9] (i.e., parameterized); e.g., similarly"",null,null",null,null
112,"111,""to the QCM approach [12]. Yet, for simplicity, in this work we as-"",null,null",null,null
113,"112,""sume that every user query change action has the same odds (i.e.,"",null,null",null,null
114,"113,p(qt ),null,null",null,null
115,"114,"","",null,null",null,null
116,"115,1 3,null,null",null,null
117,"116,).,null,null",null,null
118,"117,""Please note that, the main di"",null,null",null,null
119,"118,erence between our,null,null",null,null
120,"119,estimate of  Ft and the RM1 model is in the way the later scores documents in Ft . Such score in RM1 is based on a given query,null,null",null,null
121,"120,""qt [5], with no further distinction between the role that each query"",null,null",null,null
122,"121,term plays or the fact that some of the terms are actually removed,null,null",null,null
123,"122,""terms that appeared in the previous query qt-1. Similar to RM1,"",null,null",null,null
124,"123,we,null,null",null,null
125,"124,further,null,null",null,null
126,"125,estimate p(d |qt ),null,null",null,null
127,"126,d,null,null",null,null
128,"127,p(qt |d ) p(qt |d,null,null",null,null
129,"128,Ft,null,null",null,null
130,"129,),null,null",null,null
131,"130,.,null,null",null,null
132,"131,866,null,null",null,null
133,"132,Short Research Paper,null,null",null,null
134,"133,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
135,"134,User added or retained terms are those terms that are preferred,null,null",null,null
136,"135,""to be included in the feedback documents Ft . On the other hand, removed terms are those terms that should not appear in Ft [4]. In accordance, we de ne:"",null,null",null,null
137,"136,p(qt,null,null",null,null
138,"137,|d,null,null",null,null
139,"138,),null,null",null,null
140,"139,def,null,null",null,null
141,"140,"","",null,null",null,null
142,"141,""p[µ](w |d ),"",null,null",null,null
143,"142,w qt,null,null",null,null
144,"143,""1 - w qt- p[0](w |d ),"",null,null",null,null
145,"144,""qt  {qt, qt +} qt , qt - (4)"",null,null",null,null
146,"145,""In order to avoid query dri , on each step t, we further anchor"",null,null",null,null
147,"146,the feedback model  Ft to the query model qt [5] as follows:,null,null",null,null
148,"147,p(w,null,null",null,null
149,"148,|,null,null",null,null
150,"149, Ft,null,null",null,null
151,"150,),null,null",null,null
152,"151,def,null,null",null,null
153,"152,"","",null,null",null,null
154,"153,""(1 - t )p[0](w |qt ) + t p(w | Ft ),"",null,null",null,null
155,"154,(5),null,null",null,null
156,"155,where t,null,null",null,null
157,"156,def,null,null",null,null
158,"157,"","",null,null",null,null
159,"158,"" · sim(qt , qn ) is a dynamic query anchoring"",null,null",null,null
160,"159,""parameter,   [0, 1] and sim(qt , qn ) is calculated using the (idf-"",null,null",null,null
161,"160,boosted) Generalized-Jaccard similarity measure; i.e.:,null,null",null,null
162,"161,""min (t f (w, qt ), t f (w, qn )) · id f (w)"",null,null",null,null
163,"162,s i m (qt,null,null",null,null
164,"163,"","",null,null",null,null
165,"164,qn,null,null",null,null
166,"165,),null,null",null,null
167,"166,def,null,null",null,null
168,"167,"","",null,null",null,null
169,"168,w qt qn,null,null",null,null
170,"169,""max (t f (w, qt ), t f (w, qn )) · id f (w)"",null,null",null,null
171,"170,(6),null,null",null,null
172,"171,w qt qn,null,null",null,null
173,"172,""According to t de nition, the similar query qt is to the current query qn , the more relevant is the query change in user's information need It (modelled by  Ft ) is assumed to be to the current user's information need In; erefore, less query anchoring e ect is assumed to be needed using query qt ."",null,null",null,null
174,"173,4 EVALUATION 4.1 Datasets,null,null",null,null
175,"174,2011,null,null",null,null
176,"175,2012,null,null",null,null
177,"176,2013,null,null",null,null
178,"177,(train),null,null",null,null
179,"178,(test),null,null",null,null
180,"179,(test),null,null",null,null
181,"180,Sessions,null,null",null,null
182,"181,Sessions,null,null",null,null
183,"182,76,null,null",null,null
184,"183,98,null,null",null,null
185,"184,87,null,null",null,null
186,"185,eries/session,null,null",null,null
187,"186,3.7±1.8,null,null",null,null
188,"187,3.0±1.6,null,null",null,null
189,"188,5.1±3.6,null,null",null,null
190,"189,Topics,null,null",null,null
191,"190,Sessions/topic,null,null",null,null
192,"191,1.2±0.5,null,null",null,null
193,"192,2.0±1.0,null,null",null,null
194,"193,2.2±1.0,null,null",null,null
195,"194,Judged docs/topic 313±115,null,null",null,null
196,"195,372±163,null,null",null,null
197,"196,268±117,null,null",null,null
198,"197,Collection,null,null",null,null
199,"198,Name,null,null",null,null
200,"199,ClueWeb09B ClueWeb09B ClueWeb12B,null,null",null,null
201,"200,#documents,null,null",null,null
202,"201,""28,810,564 28,810,564 15,700,650"",null,null",null,null
203,"202,Table 1: TREC session track benchmarks,null,null",null,null
204,"203,""Our evaluation is based on the TREC 2011-2013 session tracks [1] (see benchmarks details in Table 1). e Category B subsets of the ClueWeb09 (2011-2012 tracks) and ClueWeb12 (2013 track) collections were used. Each collection has nearly 50M documents. Documents with spam score below 70 were ltered out. Documents were indexed and searched using the Apache Solr2 search engine. Documents and queries were processed using Solr's English text analysis (i.e., tokenization, Poter stemming, stopwords, etc)."",null,null",null,null
205,"204,2h p://lucene.apache.org/solr/,null,null",null,null
206,"205,4.2 Baselines,null,null",null,null
207,"206,We compared our proposed relevance modelling approach (hereina er denoted SRM3) with several di erent types of baselines.,null,null",null,null
208,"207,""is includes state-of-the-art language modeling methods that utilize session context data (i.e., previous queries, viewed or clicked results); namely FixedInt [8] (with  ,"""" 0.1,  """", 1.0 following [8]) and its Bayesian extension BayesInt [8] (with µ ,"""" 0.2,  """","""" 5.0, following [8]) ­ both methods combine the query qn model with the history queries Qn-1 and clicks Cn-1 centroid models; BatchUp [8] (with µ """","""" 2.0,  """","""" 15.0, following [8]) which iteratively interpolates the language model of clicks that occur up to each step t using a batched approach; and the Expectation Maximization (EM) based approach [10] (hereina er denoted LongTEM with q """","""" 0, C """", 20 and N C ,"""" 1, following [10]), which rst interpolates each query qt model with its corresponding session history model (based on both clicked (C) and non-clicked (NC) results in Dq[kt]); the (locally) interpolated query models are then combined based on their relevant session history using the EM-algorithm [10]."""""",null,null",null,null
209,"208,""Next, we implemented two versions of the Relevance Model [5]. e rst is the basic RM3 model, denoted RM3(qn ), learned using the last query qn and the top-m retrieved documents as pseudo relevance feedback. e second, denoted RM3(Qn ), uses the pseudo information need Qn (see Section 3.5) instead of qn . We also implemented two query aggregation methods, namely: QA(uniform) which is equivalent to submi ing Qn as the query [11]; the second, denoted QA(decay), further applies an exponential decay approach to prefer recent queries to earlier ones (with decay parameter  ,"""" 0.92, following [3, 12]). We further implemented three versions of the ery Change Model (QCM) ­ an MDP-inspired query weighting and aggregation approach [12]. Following [12] recommendation, QCM's parameters were set as follows  """","""" 2.2,  """","""" 1.8,  """","""" 0.07,  """", 0.4 and  ,"""" 0.92. e three QCM versions are the basic QCM approach [12]; QCM(SAT) which utilizes only """"""""satis ed"""""""" clicks (i.e., clicks whose dwell-time is at least 30 seconds [12]); and QCM(DUP) which ignores duplicate session queries [12]. Finally, in order to evaluate the relative e ect of the query-change driven feedback model (i.e.,  Ft ), we implemented a variant of SRM by replacing the query-change driven score of Eq. 3 with the RM1 document score (i.e., p(d |qn )). Let SRM(QC) and SRM(RM1) further denote the query-change and """"""""RM1- avoured"""""""" variants of SRM, respectively. It is important to note that, SRM(RM1) still relies on the dynamic relevance model updating formula (see Eq. 1) and the dynamic coe cients t and t ­ both further depend on the session dynamics (captured by St-1 and  Ft )."""""",null,null",null,null
210,"209,4.3 Setup,null,null",null,null
211,"210,""Our evaluation is equivalent to the TREC 2011-2012 RL4 and TREC 2013 RL2 sub-tasks [1]. To this end, given each session's (last) query qn, we rst retrieved the top-2000 documents with the highest query likelihood (QL) score4 to qn . Documents were then reranked using the various baselines by multiplying their (initial)"",null,null",null,null
212,"211,""3Stands for """"Session-Relevance Model"""". 4For this we used Solr's LMSimilarity with Dirchlet smoothing parameter µ , 2500 which is similar to Indri's default parameter."",null,null",null,null
213,"212,867,null,null",null,null
214,"213,Short Research Paper,null,null",null,null
215,"214,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
216,"215,TREC 2012,null,null",null,null
217,"216,TREC 2013,null,null",null,null
218,"217,Method Initial retrieval FixInt [8] BayesInt [8] BatchUp [8] LongTEM [10] RM3(qn ) [5] RM3(Qn ) QA(uniform) [11] QA(decay) [3] QCM [12] QCM(SAT) [12] QCM(DUP) [12] SRM(RM1) SRM(QC),null,null",null,null
219,"218,nDCG@10 0.249r q 0.333r q 0.334r q 0.320r q 0.332r q 0.311r q 0.305r q 0.301r q 0.303r q 0.329r q 0.298r q 0.299r q 0.348q 0.356r,null,null",null,null
220,"219,nDCG 0.256r q 0.296r q 0.297r q 0.288r q 0.295r q 0.284r q 0.284r q 0.282r q 0.284r q 0.262r q 0.281r q 0.281r q,null,null",null,null
221,"220,0.300,null,null",null,null
222,"221,0.304,null,null",null,null
223,"222,nERR@10 0.302r q 0.380r q 0.382r q 0.368r q 0.389r q 0.369r q 0.354r q 0.352r q 0.353r q 0.306r q 0.347r q 0.350r q 0.395q 0.405r,null,null",null,null
224,"223,MRR 0.594r q 0.679r q 0.674r q 0.664r q 0.667r q 0.654r q 0.647r q 0.646r q 0.645r q 0.574r q 0.635r q 0.631r q 0.699q 0.716r,null,null",null,null
225,"224,nDCG@10 0.113r q 0.165r q 0.171r q 0.181r q 0.167r q 0.134r q 0.153r q 0.160r q 0.163r q 0.158r q 0.158r q 0.160r q 0.188q 0.193r,null,null",null,null
226,"225,nDCG 0.105r q 0.132r q 0.131r q,null,null",null,null
227,"226,0.134 0.131r q 0.122r q 0.129r q 0.130r q 0.131r q 0.129r q 0.129r q 0.130r q,null,null",null,null
228,"227,0.137,null,null",null,null
229,"228,0.138,null,null",null,null
230,"229,nERR@10 0.140r q 0.209r q 0.208r q 0.233r q 0.205r q 0.161r q 0.203r q 0.204r q 0.207r q 0.201r q 0.202r q 0.208r q 0.240q 0.248r,null,null",null,null
231,"230,MRR 0.390r q 0.544r q 0.527r q 0.581r q 0.530r q 0.422r q 0.553r q 0.546r q 0.550r q 0.535r q 0.545r q 0.559r q 0.601q 0.612r,null,null",null,null
232,"231,""Table 2: Evaluation results. e r and q superscripts denote signi cant di erence with SRM(RM1) and SRM(QC), respectively (p < 0.05)."",null,null",null,null
233,"232,""QL score with the score determined by each method. e document scores of the various language model baselines (i.e., FixInt, BayesInt, BatchUp, LongTEM and the variants of RM3 and SRM ) were further determined using the KL-divergence score [13]; where each baseline's learned model was clipped using a xed cuto of 100 terms [13]. e TREC session track trec eval tool5 was used for measuring retrieval performance. Using this tool, we measured the nDCG@10, nDCG (@2000), nERR@10 and MRR of each baseline. Finally, we tuned the RM3 and SRM's free parameters6 using the TREC 2011 track as a train set. e parameters were optimized so as to maximize MAP. e TREC 2012-2013 tracks were used as the test sets."",null,null",null,null
234,"233,4.4 Results,null,null",null,null
235,"234,""e evaluation results are summarized in Table 2. e rst row reports the quality of the initial retrieval. Overall, compared to the various alternative baselines, the two SRM variants provided signi cantly be er performance; with at least +6.6%, +2.4%, +4.1% and +5.3% be er performance in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. e results clearly demonstrate the dominance of the session-context sensitive language modeling approaches (and the two SRM variants among them) over the other alternatives we evaluated. Furthermore, SRM's consideration of the user's query-change process as an additional relevance feedback source results in a more accurate estimate of the user's information need."",null,null",null,null
236,"235,""Next, compared to the RM3 variants, it is clear from the results that a dynamic relevance modeling approach that is driven by query-change (such as SRM) is a be er choice for the session search task. Moving from an ad-hoc relevance modelling approach (i.e., one that only focuses on the last query in the session) to a session-context sensitive approach provides signi cant boost in performance; with at least +14%, +7.0%, +9.8% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks."",null,null",null,null
237,"236,""5h p://trec.nist.gov/data/session/12/session eval main.py 6  {0.1, 0.2, . . . , 0.9},   {0.1, 0.2, . . . , 0.9}, m  {5, 10, . . . , 100}"",null,null",null,null
238,"237,""We further observe that, compared to the baseline methods that"",null,null",null,null
239,"238,""implement various query aggregation and scoring schemes (i.e.,"",null,null",null,null
240,"239,""QA and QCM variants), a query-expansion strategy based on the"",null,null",null,null
241,"240,user's dynamic information need (such as the one implemented by,null,null",null,null
242,"241,SRM variants) provides a much be er alternative; with at least,null,null",null,null
243,"242,""+18.5%, +6.1%, +15.1% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks."",null,null",null,null
244,"243,""Finally, comparing the two SRM variants side-by-side, it be-"",null,null",null,null
245,"244,""comes even more clear that, using the query-change as an addi-"",null,null",null,null
246,"245,tional relevance feedback source is the be er choice; with at least,null,null",null,null
247,"246,""+2.3%, +1.0%, +2.5% and +1.8% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. Please"",null,null",null,null
248,"247,""recall that, SRM(QC) was trained with a xed and equal-valued"",null,null",null,null
249,"248,""priors p(qt ). Hence, a further improvement may be obtained by be er tuning of these priors."",null,null",null,null
250,"249,REFERENCES,null,null",null,null
251,"250,""[1] Ben Cartere e, Paul Clough, Mark Hall, Evangelos Kanoulas, and Mark Sanderson. Evaluating retrieval over sessions: e trec session track 2011-2014. In Proceedings of SIGIR'2016."",null,null",null,null
252,"251,[2] Steve Cronen-Townsend and W. Bruce Cro . antifying query ambiguity. In Proceedings of HLT'2002.,null,null",null,null
253,"252,[3] Dongyi Guan and Hui Yang. ery aggregation in session search. In Proceedings of DUBMOD'2014.,null,null",null,null
254,"253,[4] Jiepu Jiang and Chaoqun Ni. What a ects word changes in query reformulation during a task-based search session? In Proceedings of CHIIR'2016.,null,null",null,null
255,"254,[5] Victor Lavrenko and W. Bruce Cro . Relevance based language models. In Proceedings of SIGIR'2001.,null,null",null,null
256,"255,""[6] Jiyun Luo, Xuchu Dong, and Hui Yang. Session search by direct policy learning. In Proceedings of ICTIR'2015."",null,null",null,null
257,"256,""[7] Jiyun Luo, Sicong Zhang, Xuchu Dong, and Hui Yang. Designing states, actions, and rewards for using pomdp in session search. In Proceedings of ECIR'2005, pages 526­537. Springer, 2015."",null,null",null,null
258,"257,""[8] Xuehua Shen, Bin Tan, and ChengXiang Zhai. Context-sensitive information retrieval using implicit feedback. In Proceedings of SIGIR'2005."",null,null",null,null
259,"258,""[9] Marc Sloan, Hui Yang, and Jun Wang. A term-based methodology for query reformulation understanding. Inf. Retr., 18(2):145­165, April 2015."",null,null",null,null
260,"259,""[10] Bin Tan, Xuehua Shen, and ChengXiang Zhai. Mining long-term search history to improve search accuracy. In Proceedings of KDD'2006."",null,null",null,null
261,"260,""[11] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. Lexical query modeling in session search. In Proceedings of ICTIR'2016."",null,null",null,null
262,"261,""[12] Hui Yang, Dongyi Guan, and Sicong Zhang. e query change model: Modeling session search as a markov decision process. ACM Trans. Inf. Syst., 33(4):20:1­ 20:33, May 2015."",null,null",null,null
263,"262,""[13] Chengxiang Zhai and John La erty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), April 2004."",null,null",null,null
264,"263,868,null,null",null,null
265,"264,,null,null",null,null

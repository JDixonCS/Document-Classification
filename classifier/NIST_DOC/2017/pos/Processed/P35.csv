,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 1A: Evaluation 1,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Can Deep E ectiveness Metrics Be Evaluated Using Shallow Judgment Pools?,null,null",null,null
4,"3,Xiaolu Lu,null,null",null,null
5,"4,""RMIT University Melbourne, Australia"",null,null",null,null
6,"5,Alistair Mo at,null,null",null,null
7,"6,""e University of Melbourne Melbourne, Australia"",null,null",null,null
8,"7,J. Shane Culpepper,null,null",null,null
9,"8,""RMIT University Melbourne, Australia"",null,null",null,null
10,"9,ABSTRACT,null,null",null,null
11,"10,""Increasing test collection sizes and limited judgment budgets create measurement challenges for IR batch evaluations, challenges that are greater when using deep e ectiveness metrics than when using shallow metrics, because of the increased likelihood that unjudged documents will be encountered. Here we study the problem of metric score adjustment, with the goal of accurately estimating system performance when using deep metrics and limited judgment sets, assuming that dynamic score adjustment is required per topic due to the variability in the number of relevant documents. We seek to induce system orderings that are as close as is possible to the orderings that would arise if full judgments were available."",null,null",null,null
12,"11,""Starting with depth-based pooling, and no prior knowledge of sampling probabilities, the rst phase of our two-stage process computes a background gain for each document based on rank-level statistics. e second stage then accounts for the distributional variance of relevant documents. We also exploit the frequency statistics of pooled relevant documents in order to determine a threshold for dynamically determining the set of topics to be adjusted. Taken together, our results show that: (i) be er score estimates can be achieved when compared to previous work; (ii) by se ing a global threshold, we are able to adapt our methods to di erent collections; and (iii) the proposed estimation methods reliably approximate the system orderings achieved when many more relevance judgments are available. We also consider pools generated by a two-strata sampling approach."",null,null",null,null
13,"12,KEYWORDS,null,null",null,null
14,"13,Test collection; relevance assessment; pooling; shallow judgments.,null,null",null,null
15,"14,1 INTRODUCTION,null,null",null,null
16,"15,""Batch evaluations are performed by calculating a metric score based on a set of judged documents. Despite ve decades of success, this """"Cran eld/TREC"""" paradigm also faces challenges. One of the key issues is that realistic collection sizes now greatly exceed the budget available to perform human judgments. """"Pooling-to-depth-d"""" is one widely-used approach [25], in which documents in the union of the top-d lists returned from a set of contributing systems are judged, but other documents are not. e pooling depth d is ideally"",null,null",null,null
17,"16,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080793"",null,null",null,null
18,"17,""determined by the needs of the e ectiveness metric to be used, but in reality is also constrained by the experimental budget. Although pooling has identi ed the majority of relevant documents in earlier collections [30], there is growing evidence that this is not true for the web collections that are now the norm [2, 11]."",null,null",null,null
19,"18,""e uncertainty in e ectiveness measurement in large collections is the key emphasis of our work here, focusing on how to estimate evaluation scores when reduced judgment sets are used."",null,null",null,null
20,"19,""is is not a new problem, and a range of prediction mechanisms have been proposed [1, 22, 23, 27, 28], mainly focusing on predicting system orderings. We focus on prevailing pool-based test collection construction methods, as these best match our methodology, and on deep evaluation metrics, noting that pool depth has a lesser impact on shallow evaluation metrics such as ERR [6]. Alternative approaches using direct sampling exploit prior knowledge of the probability of each document being judged, and are applied during pool construction, on the assumption that all systems requiring measurement have been identi ed. But that process makes it di cult to infer scores for any new systems that get added later. On the other hand, pooling selects documents based on the assumption that top-ranked documents are both more likely to be relevant, and hence more in uential in computing e ectiveness scores. In this more general se ing there is no a priori knowledge of the system scores, and while that means that regression cannot be applied, new systems can be considered. We also argue that the decision to apply score adjustment should be done on a per topic basis. Robertson [17] notes that topics vary in terms of the number of potential relevant documents, and that this can have a signi cant impact on evaluation scores. Dynamically identifying when to perform score adjustment is thus a second challenge that must be considered."",null,null",null,null
21,"20,""e end objective of an evaluation goes beyond the metric scores, of course; in the end we wish to be able to compare and choose between systems, meaning that it is also important that the score estimations are concordant with the system orderings that would arise if full knowledge were available. Since the la er is measured according to a reference point which may not be known, there is no clear optimization goal, another complication that we address."",null,null",null,null
22,"21,""ese various considerations lead to two questions: Research estion 1: For each topic, how can we estimate the evaluation score of a system using a shallow pooling depth? Research estion 2: Can stable system rankings be achieved using the adjusted scores? In considering these two questions, we perform experiments using several di erent ad-hoc test collections and a range of modeled pool depths. Our results show that: (i) a two-stage optimization framework generates more accurate score estimations than previous approaches; (ii) topic-based adjustment thresholds identi ed using early TREC collections allow additional improvements in"",null,null",null,null
23,"22,35,null,null",null,null
24,"23,Session 1A: Evaluation 1,null,null",null,null
25,"24,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
26,"25,""estimation accuracy; and (iii) the adjusted evaluation scores yield be er approximations of the """"true"""" system rankings than do the unadjusted scores. In addition to standard pooling methods, we also consider two-strata sampling [24]."",null,null",null,null
27,"26,2 RELATED WORK,null,null",null,null
28,"27,""Incomplete Judgments and Evaluation Bias. Two types of bias arise in batch evaluations: pooling depth bias [19] and system bias [20]. e rst is caused by the use of shallow pools, and the second by performance underestimation for systems that did not contribute to the pool. Both are a result of documents appearing in the ranking for which judgments are not available. e simplest response to unjudged documents is to stipulate that anything not examined in the pooling process is not relevant. Zobel [30] challenged this notion using a series of leave-one-out experiments, and showed for several early TREC collections that while it was likely there were indeed further relevant documents that had not been identi ed, system bias was nevertheless within acceptable levels. However, on more recent web collections, there is growing evidence that this situation may not be assumed [2, 11]."",null,null",null,null
29,"28,""Other responses to the issue of unjudged documents have been proposed. Buckley and Voorhees [3] describe BPref, which balances the rank positions of documents judged as non-relevant and relevant, and ignores unjudged documents. In a related approach, Sakai [18] considers condensed lists, which compute scores using a"",null,null",null,null
30,"29,""ltered ranking containing only judged documents, and nds that standard metrics give higher discriminative ratios than achieved by BPref. However, the condensed list methodology has not been shown to be stable when comparing relative system orderings using Kendall's  or discrimination ratios [19, 20]. Score Estimation / Collection Construction. Documents without judgments are not distributed randomly in ranked result lists."",null,null",null,null
31,"30,""erefore, sample-based collection construction approaches have been suggested to support statistical inference [1, 22, 23, 27, 28]. Yilmaz and Aslam [27] present an inferred Average Precision (AP) metric that uses an expectation model, and can be coupled with a sampling process to select documents to be judged. eir InfAP metric uses uniform random sampling during collection construction. When compared with standard TREC-style pooling, the results produced by InfAP were strongly correlated with AP. However, this sampling process is random, and retrieval systems return documents in rank order, meaning that relevant documents are more likely to be returned at the top of the list if the system is e ective."",null,null",null,null
32,"31,""e use of non-random sampling has also been explored. Yilmaz et al. [28] extended their previous work, proposing metrics XInfAP and XInfNDCG, based on a strati ed sampling process. In contrast, Aslam et al. [1] consider the use of importance sampling for the same task, proposing statAP, which estimates the expectation of AP. e key di erence between InfAP and statAP is that statAP is designed to generate the optimal distribution estimates using all of the contributing systems. Voorhees [24] further examines the e ect of sampling methods on inferred metrics."",null,null",null,null
33,"32,""A recent study by Schnabel et al. [23] also used importance sampling, this time in conjunction with Discounted Cumulative Gain (DCG). e key idea in their approach was to use the probability of"",null,null",null,null
34,"33,""relevance with respect to rank information when determining the sample distribution. ey provide an analysis on how to derive the optimal sampling distributions under di erent system comparison se ings [22]. Using the proposed framework, any metric can be reformulated in the form of expectations and be estimated directly from the sampling process. Mo at et al. [14] had earlier examined targeted pooling and document judgment order in conjunction with the Rank-Biased Precision (RBP) metric."",null,null",null,null
35,"34,""Score Estimation Based on Pooling Methods. Estimation in traditional pooling techniques has also received considerable a ention [4, 7, 9, 10, 16, 26]. Most existing techniques focus on adjusting the bias which exists between pooled and unpooled systems. Webber and Park [26] proposed two methods to perform score adjustment."",null,null",null,null
36,"35,""e rst uses an adjustment factor, which is computed from the contributing systems. Each contributing system has an error value assigned when it is le out of the training process, and the mean of those values is applied to any new system to be measured. e second approach requires a set of common topics with """"complete judgments"""". A similar calculation is performed in order to obtain the adjustment factor, but restricted to the subset of common topics. To obtain additional adjustment accuracy, Webber and Park introduced randomization to build an unbiased estimator."",null,null",null,null
37,"36,""Recent work by Lipani et al. [9] using a precision metric outperformed the rst method of Webber and Park. eir """"anti-Precision"""" measurement is similar in spirit to the residual computed by RBP [15]. Lipani et al. [9] compute adjustment factors using the leaveone-run out methodology, and then improve their previous approach by computing an average distribution [10]."",null,null",null,null
38,"37,""e closest work to our current approach is that of Ravana and Mo at [16]. ey focus on pooling depth bias, proposing three methods to estimate the e ect of unjudged documents, using the residual that can be computed for weighted-precision metrics [15]."",null,null",null,null
39,"38,""eir rst method uses a background estimation based on a static scaling factor; the second assumes that the percentage of relevant but unjudged documents can be derived directly from the known score component; and the third uses a parametric combination of the rst two. Lu et al. [12] subsequently de ne the same problem in terms of the anticipated e ectiveness gain as a function of ranking depth. Based on di erent assumptions derived from the underlying gain distributions, they propose several alternatives, and compare the estimates achieved. ey empirically show that relatively simple models can be used to estimate gain values for unjudged documents."",null,null",null,null
40,"39,""An approach due to Bu¨ cher et al. [4] directly predicts the relevance of unjudged documents, using two types of classi ers trained with the existing pool to predict the relevance of unjudged document in a new system. Although the e ectiveness of the classi er is low, their results show that classi cation does help maintain similar system orderings when measured via Kendall's  . Jayasinghe et al. [7] take a similar approach, and show that reliably predicting document relevance is o en di cult."",null,null",null,null
41,"40,3 PRELIMINARIES AND BASELINES,null,null",null,null
42,"41,""Pools. Figure 1 shows the construction of a pool for one topic, with sj,i (on the le ) corresponding to the j th document in the run for system Si , and with the corresponding documents (on the"",null,null",null,null
43,"42,36,null,null",null,null
44,"43,Session 1A: Evaluation 1,null,null",null,null
45,"44,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
46,"45,T,null,null",null,null
47,"46,S1 s11,null,null",null,null
48,"47,S2 s12,null,null",null,null
49,"48,s21 s31 s41 s51 ... sd1 ...,null,null",null,null
50,"49,s22 s32 s42 s52 ... sd2 ...,null,null",null,null
51,"50,sk1 sk2,null,null",null,null
52,"51,S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,null,null",null,null
53,"52,...,null,null",null,null
54,"53,... ... ... ... ... ... ... ... ...,null,null",null,null
55,"54,Sn s1n s2n s3n s4n s5n ... sdn ... skn,null,null",null,null
56,"55,Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,null,null",null,null
57,"56,... sdn+1,null,null",null,null
58,"57,... skn+1,null,null",null,null
59,"58,Rank,null,null",null,null
60,"59,1 2 3 4 .5. . .d. .,null,null",null,null
61,"60,k,null,null",null,null
62,"61,S1,null,null",null,null
63,"62,D1 D3 D2 D7 D6 ... D10 ... D49,null,null",null,null
64,"63,Complete Set J,null,null",null,null
65,"64,J,null,null",null,null
66,"65,S2 S3 . . . Sn Sn+1 ,null,null",null,null
67,"66,D2 D1 D6 D5 D3 ... D6 ...,null,null",null,null
68,"67,D3 D7 D2 D8 D5 ... D1 ...,null,null",null,null
69,"68,... ... ... ... ... ... ... ...,null,null",null,null
70,"69,D3 D4 D7 D2 D1 ... D5 ...,null,null",null,null
71,"70,D4 D2 D8 D1 D9 ... D3 ...,null,null",null,null
72,"71,D50 D30 . . . D18 D6,null,null",null,null
73,"72,System Matrix: S,null,null",null,null
74,"73,M@k: M1 M2 M3 . . . Mn Mn+1,null,null",null,null
75,"74,Figure 1: Pooling process for a topic T . e le matrix is a rankbased representation; the right one shows the equivalent document,null,null",null,null
76,"75,""identi ers. e two boxes indicate two possible sets of pooled documents, the larger to depth d, and the smaller to some depth d < d. e metric M is evaluated at some depth k, where k may or may not be less than or equal to d or d ."",null,null",null,null
77,"76,""right), each potentially retrieved by multiple systems at di erent rank positions. Hence, a document D can also be represented by its rank-position information, D, (pD,1, pD,2, . . . , pD,n ) , in which pD,i is the rank returned for D by contributing system Si . Metric evaluation to depth d for systems S1 to Sn requires that the documents in the set , {D | minni,""""1 pD,i  d } be judged. at is, both matrices can be further mapped to a matrix of relevance Rd×n in which rj,i is a relevance, or gain, value."""""",null,null",null,null
78,"77,""If there is insu cient judgment volume available, a shallower pool might be formed, with documents D for which minni,""""1 pD,i > d not judged, and elements in Rd×n le without values. Unknown relevance labels may also arise for a new system Sn+1, regardless of the pooling and evaluation depths. In this framework, the rst of"""""",null,null",null,null
79,"78,the two research questions proposed in Section 1 can be split into,null,null",null,null
80,"79,""two aspects: (1a) for each topic, how do we estimate the scores of"",null,null",null,null
81,"80,a system using a set of shallow pooled judgments; and (1b) which,null,null",null,null
82,"81,topics may assume that unjudged documents are not relevant and,null,null",null,null
83,"82,which ones should not.,null,null",null,null
84,"83,One method for dealing with missing data is to compute expected,null,null",null,null
85,"84,""gains as a function of retrieval rank [12]. However, modeling rele-"",null,null",null,null
86,"85,vance as a function of rank only considers the LHS representation,null,null",null,null
87,"86,""in Figure 1, and ignores that documents can have multiple ranks."",null,null",null,null
88,"87,Addressing that limitation is a key part of our work here.,null,null",null,null
89,"88,""Metric Residuals. Suppose that for some topic T , a set of docu-"",null,null",null,null
90,"89,ments results from pooling to depth d (Figure 1). Consider the,null,null",null,null
91,"90,""ranked list returned by some system Si ,"""" (s1,i , s2,i , . . . , sk,i ) and let rj,i represent the gain of the document at rank j, normally (but not necessarily) a value in [0, 1]. e e ectiveness Mi of Si when computed to depth k by a weighted-precision metric M is:"""""",null,null",null,null
92,"91,k,null,null",null,null
93,"92,""Mi ,"""" M@k (Si , ) """","",null,null",null,null
94,"93,""rj,i · WM (j) ,"",null,null",null,null
95,"94,(1),null,null",null,null
96,"95,""j ,1"",null,null",null,null
97,"96,""sj,i "",null,null",null,null
98,"97,""where WM (j) is the weight assigned by the metric at depth j, with"",null,null",null,null
99,"98,"" j ,1"",null,null",null,null
100,"99,WM,null,null",null,null
101,"100,(j,null,null",null,null
102,"101,),null,null",null,null
103,"102,"","",null,null",null,null
104,"103,""1 [13, 15]; and where the restriction sj,i"",null,null",null,null
105,"104,is,null,null",null,null
106,"105,""required to ensure that only de ned values of rj,i are included."",null,null",null,null
107,"106,""A corresponding residual i can then be computed, quantifying the"",null,null",null,null
108,"107,metric weighting associated with the unjudged documents [15]:,null,null",null,null
109,"108,k,null,null",null,null
110,"109,""i ,"",null,null",null,null
111,"110,rmax · WM (j ) +,null,null",null,null
112,"111,""rmax · WM (j ) ,"",null,null",null,null
113,"112,(2),null,null",null,null
114,"113,""j ,""""1 sj,i"""""",null,null",null,null
115,"114,""j,k +1"",null,null",null,null
116,"115,""where rmax is the maximum possible gain. Either term might be zero, depending on whether Si contributed to the pool, on the relationship between the evaluation depth k and the pooling depth d, and on whether WM (j) ,"""" 0 when j > k, as occurs with truncated"""""",null,null",null,null
117,"116,metrics.,null,null",null,null
118,"117,ere is a three-way tension between metric depth (quanti ed as,null,null",null,null
119,"118,the expected point reached in the ranking in the corresponding user,null,null",null,null
120,"119,""model [13]); accuracy of measurement, captured by the residual; and the cost | | of performing the judgments. For example, in RBP"",null,null",null,null
121,"120,""the tail residual (the second component in Equation 2) is given by pk , and if p ,"""" 0.5, k  10 is su cient. Similar calculations apply"""""",null,null",null,null
122,"121,""for ERR [6]. But in either case, the rst term of Equation 2 might be"",null,null",null,null
123,"122,""non-zero for new runs. Furthermore, even the tail residuals might become large for deeper metrics, for example, RBP with p , 0.95."",null,null",null,null
124,"123,""Truncated (that is, non-in nite) metrics such as Scaled DCG at"",null,null",null,null
125,"124,""depth 100, SDCG@100, also require deep pools if the residual is to be moderately bounded. e same requirement must apply by"",null,null",null,null
126,"125,implication to other deep metrics such as Average Precision.,null,null",null,null
127,"126,""Problem De nition. Consider a set of n contributing systems {S1, S2, . . . , Sn }. For one topic T , let d be a pooling depth at which"",null,null",null,null
128,"127,it is believed that a majority of the relevant documents occurring,null,null",null,null
129,"128,in the runs of those systems have been identi ed. We refer to this,null,null",null,null
130,"129,""set of judgments as the """"complete set"""". Let d < d be a shallower pooling depth, with judgments forming an incomplete set  . Given a weighted precision metric M, the e ectiveness score of Si evaluated using M and to depth d is denoted as Mi ,"""" M@d (Si , ), with a residual of i . Similarly, an estimated metric score based on judgments to depth d < d, is denoted as M^ i """","""" Ed (M@d (Si , )) where Ed (·) is an estimation function for the same metric at depth d. Following Lu et al. [12], the estimation error i is then de ned as:"""""",null,null",null,null
131,"130,i,null,null",null,null
132,"131,"","",null,null",null,null
133,"132,Mi 0,null,null",null,null
134,"133,-,null,null",null,null
135,"134,M^ i,null,null",null,null
136,"135,""if M^ i < Mi ,"",null,null",null,null
137,"136,""if Mi  M^ i  Mi + i ,"",null,null",null,null
138,"137,(3),null,null",null,null
139,"138,M^ i - (Mi + i ) if M^ i > Mi + i .,null,null",null,null
140,"139,""is de nition respects the residual range, and only gives non-zero"",null,null",null,null
141,"140,""values if the estimated e ectiveness falls outside the score range arising from the use of at depth d. e challenge is to develop a method Ed (·) that estimates the depth-d e ectiveness score of a contributing system based on a subset of the judgments, and minimizes the average value of i ."",null,null",null,null
142,"141,""In the experiments in Section 6 we report the RMS aggregate of the i values computed, across systems and topics; and, as a """"percentage accurate"""", the fraction of those values that are zero."",null,null",null,null
143,"142,""Lower-Bound Estimation. A simple approach is to take Ed (x ) ,"""" x, that is M^ i """","""" Mi , where Mi is the score for system Si when evaluated using , and assert that documents outside do not"""""",null,null",null,null
144,"143,alter the score. Taking unjudged documents to be not relevant is the,null,null",null,null
145,"144,""normal default in batch evaluation, and is a valid estimator. But the"",null,null",null,null
146,"145,""estimation quality depends on the breadth of the pool, and whether"",null,null",null,null
147,"146,a majority of relevant documents have been identi ed. When there,null,null",null,null
148,"147,37,null,null",null,null
149,"148,Session 1A: Evaluation 1,null,null",null,null
150,"149,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
151,"150,J,null,null",null,null
152,"151,Fitted Gain,null,null",null,null
153,"152,Rank ,null,null",null,null
154,"153,1,null,null",null,null
155,"154,S1 s11,null,null",null,null
156,"155,2 3 4 .5. . .d. .,null,null",null,null
157,"156,s21 s31 s41 s51 ... sd1 ...,null,null",null,null
158,"157,k,null,null",null,null
159,"158,sk1,null,null",null,null
160,"159,S2 s12 s22 s32 s42 s52 ... sd2 ... sk2,null,null",null,null
161,"160,S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,null,null",null,null
162,"161,...,null,null",null,null
163,"162,... ... ... ... ... ... ... ... ...,null,null",null,null
164,"163,Sn s1n s2n s3n s4n s5n ... sdn ... skn,null,null",null,null
165,"164,Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,null,null",null,null
166,"165,... sdn+1,null,null",null,null
167,"166,... skn+1,null,null",null,null
168,"167,Gain Vector:,null,null",null,null
169,"168,""ni,1 r1i/n ni,1 r2i/n ni,1 r3i/n ni,1 r4i/n"",null,null",null,null
170,"169,""ni,1 r5i/n"",null,null",null,null
171,"170,g,null,null",null,null
172,"171, Fitting,null,null",null,null
173,"172,G1(k),null,null",null,null
174,"173,g011 g021 g031 g041 g051 ... g0d1 ...,null,null",null,null
175,"174,g0k1,null,null",null,null
176,"175,G2(k),null,null",null,null
177,"176,g012 g022 g032 g042 g052 ... g0d2 ... g0k2,null,null",null,null
178,"177,G3(k),null,null",null,null
179,"178,g013 g023 g033 g043 g053 ... g0d3 ... g0k3,null,null",null,null
180,"179,...,null,null",null,null
181,"180,... ... ... ... ... ... ...,null,null",null,null
182,"181,...,null,null",null,null
183,"182,Figure 2: Overview of rank-based estimation for a single topic. e judgments are used to infer an observed gain vector g; each,null,null",null,null
184,"183,of a set of m functions G (k ) is then ed to g.,null,null",null,null
185,"184,""are still many unjudged relevant documents, this estimator results in underestimation of system performance. Reasonably good rank correlation between the estimates and the true score over a set of systems can be obtained, but there is no guarantee that the performance of each of the systems has been accurately measured."",null,null",null,null
186,"185,""Interpolative Estimation. A second baseline is provided by the RM interpolative estimator proposed by Ravana and Mo at [16], who scale the metric score across the residual, assuming that unjudged documents are relevant at the same rate as judged ones:"",null,null",null,null
187,"186,""M^ i , Mi /(1 - i ) ."",null,null",null,null
188,"187,(4),null,null",null,null
189,"188,""A collection-based background probability is used when i , 1. is estimator assumes that gain is accrued at the same rate across"",null,null",null,null
190,"189,""all of the documents retrieved by the system, both judged and unjudged. Although more robust than the LB estimator, it does not allow the likelihood of relevance to decrease as the pool is extended from d to d."",null,null",null,null
191,"190,""Rank-Based Estimators. Lu et al. [12] introduce rank-based estimation, illustrated in Figure 2. e judgments are used to estimate expected gain as a function of rank on a per-topic basis. ose rank-based fractional gain predictions are then used for unjudged documents ­ interpolated at depths up to d , and extrapolated from d to the metric evaluation depth k. Lu et al. explore alternative estimation functions, measuring the prediction error using the mechanism described in Equation 3, and nd that while improvements are possible, no single estimator works consistently well across all collections and topics. Rank-based estimation also has the drawback of ignoring the fact that a document can appear at di erent ranks for di erent systems; and hence potentially assigns di erent gain estimates to the same document in di erent runs, a representational issue that usually leads to a biased estimation [29]. As a further drawback, an entire row in the system matrix S (see Figure 1) must be judged in order to compute the expected gain, limiting construction methods to pooling or sampling by rank, and possibly excluding strati ed sampling processes."",null,null",null,null
192,"191,Sampling-Based Estimation. Other sampling approaches can also be used when forming the judgment set. Voorhees [24] de-,null,null",null,null
193,"192,""scribes a two-strata sampling method, which consists of shallow pooled judgments to some depth d , and then 10% random sampling to depth d in a second set s . ese judgments allow computation of inferred recall-based metrics, and also inferred versions"",null,null",null,null
194,"193,""of weighted-precision metrics, with M^ i for system i calculated as:"",null,null",null,null
195,"194,k,null,null",null,null
196,"195,k,null,null",null,null
197,"196,""M^ i ,"",null,null",null,null
198,"197,""rj,i · WM (j) +  ·"",null,null",null,null
199,"198,""rj,i · WM (j) ,"",null,null",null,null
200,"199,(5),null,null",null,null
201,"200,""j ,""""1 sj,i """""",null,null",null,null
202,"201,""j ,""""1 sj,i  s"""""",null,null",null,null
203,"202,where,null,null",null,null
204,"203,-1,null,null",null,null
205,"204,k,null,null",null,null
206,"205,k,null,null",null,null
207,"206,"","",null,null",null,null
208,"207,WM (j) ·,null,null",null,null
209,"208,WM (j),null,null",null,null
210,"209,""j ,""""1 sj,i"""""",null,null",null,null
211,"210,""j ,""""1 sj,i  s"""""",null,null",null,null
212,"211,and where the second term in Equation 5 estimates the total gain,null,null",null,null
213,"212,""associated with documents contained in the second stratum. Here,  is the interpolation estimator. Note that Equation 5 only adapts the RM method for sample based judgments."",null,null",null,null
214,"213,4 TWO-STAGE ESTIMATION,null,null",null,null
215,"214,""Overview of the Framework. To compute score estimates, we propose a two-stage framework, guided by a uni ed optimization goal, and built on a set of m  1 per-topic rank-level estimators."",null,null",null,null
216,"215,e overall structure of this mechanism is described in Algorithm 1.,null,null",null,null
217,"216,""We omit the process of obtaining rank-level estimations, discussed"",null,null",null,null
218,"217,""brie y in the previous section, and in detail by Lu et al. [12]. at is, we assume as our starting point here that m di erent rank-based estimators have been generated, each derived from the judged documents D  , and that values for a set of gain functions have been computed, with 0j, the gain associated with an unjudged document that appears in the j th position of any of the n system"",null,null",null,null
219,"218,Algorithm 1 Estimation Framework,null,null",null,null
220,"219,Input: System matrix Sk×n ; partial relevance judgments with 2[D] the gain associated with document D for D  and,null,null",null,null
221,"220,unde ned otherwise; and a set of m rank-level background,null,null",null,null
222,"221,""gain estimates, 0j, for 1  j  k and 1   m, with 0,  0j, | 1  j  k and 1[D]  1 [D] | 1   m ."",null,null",null,null
223,"222,""Output: Values 2[D], gain estimates for the documents D  "",null,null",null,null
224,"223,1: for D  \ do 2[D]  0,null,null",null,null
225,"224,2:   C,null,null",null,null
226,"225,""CV( , S) // compute coe cient of variance"",null,null",null,null
227,"226,3: if  >  then,null,null",null,null
228,"227,// adjust only if  exceeds threshold,null,null",null,null
229,"228,4: for  1 to m do,null,null",null,null
230,"229,5:,null,null",null,null
231,"230,for D  do 1 [D]  0,null,null",null,null
232,"231,6:,null,null",null,null
233,"232,""w1opt  arg min L h1 ( 0, , w1) | D "",null,null",null,null
234,"233,""w1 [0, 1]n"",null,null",null,null
235,"234,7:,null,null",null,null
236,"235,for D  do,null,null",null,null
237,"236,8:,null,null",null,null
238,"237,""1 [D]  h1 0, , w1opt"",null,null",null,null
239,"238,9:,null,null",null,null
240,"239,end for,null,null",null,null
241,"240,10: end for,null,null",null,null
242,"241,""11: w2opt  arg min L h2 ( 1[D], w2) | D  w2 [0, 1]m"",null,null",null,null
243,"242,12: // get nal per-document estimation,null,null",null,null
244,"243,13: for D  \ do,null,null",null,null
245,"244,14:,null,null",null,null
246,"245,""2[D]  h2 1[D], w2opt"",null,null",null,null
247,"246,15: end for,null,null",null,null
248,"247,16: end if,null,null",null,null
249,"248,17: return 2,null,null",null,null
250,"249,38,null,null",null,null
251,"250,Session 1A: Evaluation 1,null,null",null,null
252,"251,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
253,"252,""rankings, as predicted by the th of the m di erent estimators."",null,null",null,null
254,"253,""Prior to forming the new combined estimates, we rst compute the coe cient of covariance  from the judgment set [5], in order to"",null,null",null,null
255,"254,""determine whether to use a background """"unjudged are not relevant"""" predictor. Estimation is computed by steps 4 to 15, with h1 (·) and h2 (·) two parametric combining functions, in which the parameters are obtained by minimizing a loss function L(·). We discuss the details of Algorithm 1, including the rationale behind the use of  ,"",null,null",null,null
256,"255,in the next few paragraphs.,null,null",null,null
257,"256,""First Stage. As noted already, one problem with rank-based estimators is the potential inconsistency across runs of the gain a ached"",null,null",null,null
258,"257,""to any particular document. As always, we assume that one topic is being addressed; the goal in the rst stage is to aggregate the m × n per-document estimates across the m estimators and n systems into a smaller set of m estimates per document. at is, the m rank-level estimators are treated separately at rst, in the loop at step 4, to obtain a consistent background gain for each document D for each model, denoted 1 [D]. is is done via a combining function h1 (·) that maps a vector to a single value. Several options for h1 (·) are available, with the choice between them depending on assumptions"",null,null",null,null
259,"258,about system quality and the degree to which the systems are cor-,null,null",null,null
260,"259,""related. For simplicity, we assume that the systems are independent and that they vary in quality. erefore, for each document D, a natural combining function is to compute a weighted average, with h1 (step 6) parameterized by an n-element weighting vector w1 that is speci c to the th estimator:"",null,null",null,null
261,"260,n,null,null",null,null
262,"261,""D  , h1 ( 0, , w1 | D) ,"",null,null",null,null
263,"262,""0pD,i, · w1i"",null,null",null,null
264,"263,""i ,1"",null,null",null,null
265,"264,n,null,null",null,null
266,"265,(6),null,null",null,null
267,"266,""with w1i ,"""" 1 and w1i  [0, 1] ,"""""",null,null",null,null
268,"267,""i ,1"",null,null",null,null
269,"268,""and where 0pD,i, applies the th estimator to the rank at which document D appears in the i th of the n runs. One practical issue is that a document may not be retrieved by all systems in their top-k ranked lists, where k is the maximum depth of lists returned. In such cases the rank-based background gain of that document for that system is set to the modeled gain at depth k."",null,null",null,null
270,"269,""To compute a value for w1, we consider the aggregation process as an optimization problem, where the goal is to minimize the"",null,null",null,null
271,"270,estimation error. e estimation error has two granularities: (i) the,null,null",null,null
272,"271,total error of system e ectiveness score calculated using ; and,null,null",null,null
273,"272,(ii) the total error of estimating the background gain of the labeled,null,null",null,null
274,"273,""documents. From either perspective, we can formalize an objective function L and use it at step 6 of Algorithm 1. Consider the rst case, with the system matrix as shown in Figure 1. We de ne L as:"",null,null",null,null
275,"274,2,null,null",null,null
276,"275,nk,null,null",null,null
277,"276,""La (·) ,"",null,null",null,null
278,"277,""WM (j) · (h1 (·, w1 | sj,i ) - rj,i ) , (7)"",null,null",null,null
279,"278,""i,1 j,""""1 sj,i """""",null,null",null,null
280,"279,""where WM (j) · h1 (·, w1 | sj,i ) is the estimated background gain for document sj,i  , and rj,i is the known relevance value of that same document. As noted, La minimizes the overall estimation"",null,null",null,null
281,"280,error of the evaluation scores for the set of systems.,null,null",null,null
282,"281,""e second alternative uses the document-position representation (pD,1, pD,2, . . . , pD,n ):"",null,null",null,null
283,"282,""Lb (·) ,"",null,null",null,null
284,"283,D,null,null",null,null
285,"284,n 2,null,null",null,null
286,"285,""WM (pD,i ) · (h1 (·, w1 | D) - rD ) , (8)"",null,null",null,null
287,"286,""i ,1"",null,null",null,null
288,"287,""in which rD is the relevance value of document D and is included only once per document, rather than once per document-rank."",null,null",null,null
289,"288,""When compared to Equation 7, which considers estimation er-"",null,null",null,null
290,"289,""rors at the system level, this loss function is focused at the per-"",null,null",null,null
291,"290,""document level, seeking to minimize the overall estimation error"",null,null",null,null
292,"291,for the weighted gain of each document. Either Equation 7 or Equa-,null,null",null,null
293,"292,""tion 8 can be used at step 6 of Algorithm 1, with the combination function h1 (·) and constraints de ned in Equation 6. e result is the computation of a sequence of w1opt vectors, one for each of the m di erent rank-level estimators."",null,null",null,null
294,"293,Second Stage. Multiple ing models have been proposed because di erent assumptions about the underlying relevance distributions,null,null",null,null
295,"294,""across all systems are plausible, with a risk that no single model"",null,null",null,null
296,"295,""covers the true hypothesis space. Indeed, the limited non-random"",null,null",null,null
297,"296,training data means that we may su er from a high variance if only,null,null",null,null
298,"297,""one model is considered. erefore, a """"meta"""" optimizer is also used,"",null,null",null,null
299,"298,""combining results from the rst stage, as described by steps 11"",null,null",null,null
300,"299,""to 15. A weighted average is used in this role too, considering each document D, together with the estimated background gains generated by the m previous computations, 1[D]. at combiner, h2 (·) (step 11), is de ned via the m-vector w2 as:"",null,null",null,null
301,"300,m,null,null",null,null
302,"301,""D  , h2 1[D], w2 ,"""" 1 [D] · w2 ,"""""",null,null",null,null
303,"302,"",1"",null,null",null,null
304,"303,m,null,null",null,null
305,"304,(9),null,null",null,null
306,"305,""with w2 ,"""" 1 and w2  [0, 1] ."""""",null,null",null,null
307,"306,"",1"",null,null",null,null
308,"307,""Both La and Lb can be used in step 11, but may not necessarily be the same. Note that the m-vector w2opt, computed at step 11 as the minimizing value for Equation 9, provides an indication of"",null,null",null,null
309,"308,the importance of individual optimizers from the previous stage.,null,null",null,null
310,"309,Previous work has shown that the expected error of combining loss,null,null",null,null
311,"310,functions is smaller than the average error on results output by,null,null",null,null
312,"311,each optimizer in isolation from the rst stage [29].,null,null",null,null
313,"312,""Computing the Coe cient of Variance. e score adjustment and estimation process has been presented on a per-topic basis, with an underlying assumption that a shallow judgment pool cannot identify a majority of the relevant documents. However, some topics may have only a small number of relevant documents, and a shallow depth may be su cient to identify most of them, with adjustment unnecessary. Only if deeper pooling would identify further relevant documents can score adjustment have an e ect on system e ectiveness scores. Hence a coe cient of variance [5] is computed for the relevant documents in the shallow pool and used as an indicator, as described in step 2."",null,null",null,null
314,"313,""Pooling is treated as a sampling with replacement process, with an unknown probability of a relevant document being sampled. Although the nal judgment process considers only the documents in the pool, a document returned by multiple systems has a selection"",null,null",null,null
315,"314,39,null,null",null,null
316,"315,Session 1A: Evaluation 1,null,null",null,null
317,"316,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
318,"317,frequency. e intuition behind  is to make use of that frequency,null,null",null,null
319,"318,information to describe the sample coverage of relevant documents.,null,null",null,null
320,"319,""Consider the system matrix S in Figure 1 and a pooling depth d . Each document sj,i (1  j  d , 1  i  n) has a multiplicity in Sd ×n ; we then group them by that frequency count. Let fi be the number of relevant documents appearing i times in Sd ×n , R ,"""" i fi the number of relevant documents, and C """", i i · fi be"",null,null",null,null
321,"320,""the total occurrence count of relevant documents. For example, if"",null,null",null,null
322,"321,""only D8 and D1 in Figure 1 are identi ed as relevant documents, then we have f1 ,"""" 1, f3 """","""" 1, and R """", 2 and C ,"""" 4. Based on these elements, the coe cient of variance,  , is estimated via [5]:"""""",null,null",null,null
323,"322,2,null,null",null,null
324,"323,"","",null,null",null,null
325,"324,max,null,null",null,null
326,"325,|R | 1-f1 /C,null,null",null,null
327,"326,C,null,null",null,null
328,"327,i i · (i - 1) · · (C - 1),null,null",null,null
329,"328,fi,null,null",null,null
330,"329,-,null,null",null,null
331,"330,""1,"",null,null",null,null
332,"331, 0,null,null",null,null
333,"332,.,null,null",null,null
334,"333,(10),null,null",null,null
335,"334,""When  ,"""" 0, the probability of sampling a relevant document follows a uniform distribution; and when  is high, the distribution"""""",null,null",null,null
336,"335,""is skewed, and it is likely that more relevant documents exist due to"",null,null",null,null
337,"336,""the low sampling coverage. Based on this, we have two hypotheses:"",null,null",null,null
338,"337,Hypothesis 1:  tends to decrease as pooling depth increases.,null,null",null,null
339,"338,""Hypothesis 2: ere is a threshold  , where if  <  , then the existence of unjudged documents will only negligibly a ect the estimate of the system performance, and they can be ignored."",null,null",null,null
340,"339,""e rst hypothesis is easy to understand, because increasing the pooling depth increases the sample size, and increases the sampling coverage. e second hypothesis assumes that the score can be dynamically adjusted based on a threshold. If this is correct, then a point at which the total estimation error is minimal can be observed. Otherwise, we must conclude that a shallow pool is not su cient for nding relevant documents, and adjustment must be applied to all topics in all evaluations."",null,null",null,null
341,"340,""Discussion. We have described two possible realizations of loss functions, and one option for the combining functions h1 (·) and h2 (·). More sophisticated mechanisms are also possible. For example, the relationship between systems might be leveraged to derive a be er h1 (·) and its constraint."",null,null",null,null
342,"341,Note also that although our process targets the problem of es-,null,null",null,null
343,"342,""timating the e ectiveness of runs that contribute to the pool, it is"",null,null",null,null
344,"343,possible to apply the same process to estimate the score of a new,null,null",null,null
345,"344,""system, and is demonstrated empirically in Section 6. Section 6"",null,null",null,null
346,"345,also shows that the framework can be applied to the judgments,null,null",null,null
347,"346,""constructed using two-strata sampling [24], incorporating the addi-"",null,null",null,null
348,"347,tional information provided in the second stratum.,null,null",null,null
349,"348,5 COMPARING SYSTEM RANKINGS,null,null",null,null
350,"349,""Section 3 already de ned i , a score-based evaluation criterion. But we are also interested in comparing system orderings as a measure"",null,null",null,null
351,"350,of usefulness of an estimation regime.,null,null",null,null
352,"351,""Kendall's Distance. is distance metric is widely used to measure the similarity between ranked lists, and counts the number of inverted pairs between two n-item orderings. Let i, j represent the pairwise relationship between the e ectiveness metric means S¯i and S¯j of systems Si and Sj over a set of topics according to one measurement regime, with i, j  {-1, 0, 1} indicating that S¯i < S¯j ,"",null,null",null,null
353,"352,""that S¯i ,"""" S¯j , and that S¯i > S¯j , respectively; and let i, j be the corresponding values for a second measurement regime and the system"""""",null,null",null,null
354,"353,""means that it induces, for example, using pooling to a di erent depth. en Kendall's normalized  distance is the number of pairs 1  i < j  n in which i, j · i, j < 0, divided by n(n - 1)/2 to bring it into the range 0    1, with 0 meaning """"identical""""."",null,null",null,null
355,"354,Statistical Weighting. Paired t-tests are o en used to quantify the,null,null",null,null
356,"355,""strength of the relationship between two systems, and the values i, j and i, j might be thought of as being continuous rather than ternary. Kumar and Vassilvitskii [8] describe a weighted  distance"",null,null",null,null
357,"356,""that counts the strength of each discordant pair, focusing solely on cases where i, j · i, j < 0. In practice we are not only interested in the discordant pairs, but also in pairs that are deemed to be"",null,null",null,null
358,"357,signi cantly di erent according to one of the measurement regimes,null,null",null,null
359,"358,""but not the other, even if their overall relationship is concordant. Suppose that S¯i > S¯j according to the rst measurement, and"",null,null",null,null
360,"359,""that a paired one-tail statistical test across topics yields pi, j . Values of pi, j near zero indicate a signi cant superiority of Si over Sj ; values close to 0.5 indicate that it is by chance. If we de ne"",null,null",null,null
361,"360,""i, j ,"""" 00..05 - pi, j"""""",null,null",null,null
362,"361,""pj,i - 0.5 "",null,null",null,null
363,"362,""if S¯i > S¯j if S¯i ,"""" S¯j if S¯i < S¯j ,"""""",null,null",null,null
364,"363,""then -0.5  i, j  0.5 is a real-valued quantity that captures"",null,null",null,null
365,"364,both the direction and strength of the relationship between the two,null,null",null,null
366,"365,systems according to the rst measurement regime. We compute,null,null",null,null
367,"366,""i, j similarly using a second measurement approach, and then, to compare the alternative rankings of n systems induced by the two"",null,null",null,null
368,"367,""measurement techniques, calculate"",null,null",null,null
369,"368,""dist ,"",null,null",null,null
370,"369,"" · |i, j - i, j | ,"",null,null",null,null
371,"370,(11),null,null",null,null
372,"371,1i <j n,null,null",null,null
373,"372,""where   0 is an additional scaling factor. For example, if  ,"""" |i, j | then the strength of the relationship between Si and Sj according to the rst measurement regime also in uences the measured distance. Overall, if dist  0, then the two measurement regimes agree in terms of both the direction of each pairwise relationship Si versus Sj , and also its strength. If dist is substantially greater then zero, then the two measurement regimes give rise to many system"""""",null,null",null,null
374,"373,pairs for which there are non-trivial disagreements (including in,null,null",null,null
375,"374,""both discords and in concords) over the strength of the measured relationships. Compared with Kendall's  distance, Equation 11 operates over continuous values, which makes it both resistant"",null,null",null,null
376,"375,""to inconclusive changes in rank position, and also sensitive to di erences in which the direction of the relationship between Si and Sj stays the same, but the statistical strength varies markedly."",null,null",null,null
377,"376,6 EXPERIMENTS,null,null",null,null
378,"377,""e experiments described in this section include: (i) a post-hoc analysis for testing two hypotheses proposed in Section 4, and se ing the threshold  ; (ii) evaluating prediction accuracy using RMSE and Acc% as de ned by Lu et al. [12]; (iii) system ordering stability evaluation using the distance metric de ned in Equation 11 with  ,"""" 1, and using normalized  distance; and (iv) a case study covering the ClueWeb 2010 (CW10) task."""""",null,null",null,null
379,"378,40,null,null",null,null
380,"379,Session 1A: Evaluation 1,null,null",null,null
381,"380,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
382,"381,Dataset d |S |,null,null",null,null
383,"382,Judgments per topic,null,null",null,null
384,"383,2-strata,null,null",null,null
385,"384,""d , 10 d , 20 d , 30 d , d"",null,null",null,null
386,"385,TREC5 100 76 272 (13) 512 (10) 747 (8) 2298 (4) ­,null,null",null,null
387,"386,TREC9 100 59 174 (11) 322 (8) 462 (7) 1382 (4) 294 (7),null,null",null,null
388,"387,TREC10 100 54 182 (13) 335 (10) 480 (9) 1402 (5) 303 (9),null,null",null,null
389,"388,Rob04 100 42 75 (25) 139 (18) 206 (15) 710 (7) 134 (15),null,null",null,null
390,"389,TB04 80 33 164 (31) 313 (27) 453 (25) 1121 (19) 270 (25),null,null",null,null
391,"390,TB05 100 34 111 (41) 202 (36) 291 (33) 878 (25) 187 (33),null,null",null,null
392,"391,TB06 50 39 141 (31) 270 (26) 394 (23) 633 (19) ­,null,null",null,null
393,"392,CW10 20 21 98 (30) ­,null,null",null,null
394,"393,­ 187 (28) ­,null,null",null,null
395,"394,Table 1: Datasets used: d is the original pooling depth and provides the reference point for metric scores; d is a notional pooling depth used our experimentation; and |S | is the number of contributing,null,null",null,null
396,"395,runs. Only Adhoc Task runs are used. e middle four column pairs,null,null",null,null
397,"396,""show the number of judgments averaged across topics at each pooling depth d , and the percentage of relevant documents. e last"",null,null",null,null
398,"397,""column shows the statistics when using two-strata sampling [24],"",null,null",null,null
399,"398,averaged over topics and over ten random iterations.,null,null",null,null
400,"399,""Experimental Setup. e collections and con guration parameters used in our experiments are shown in Table 1. We also measured a range of behavior using the TREC7 and TREC8 collections, but do not include them here because those two collections were used as part of the post-hoc analysis and parameter se ing. Scripts are available to reproduce all of the various results given here1."",null,null",null,null
401,"400,""Pooling to di erent depths is simulated using the identi ed contributing systems, and the average number of judgments required per topic at di erent pool depths is also shown in Table 1, together with the corresponding percentage of documents identi ed as being relevant. In the experiments measuring rank stability, we also examine the two-strata sampling method described by Voorhees, and averages over ten runs for this randomized approach are included in the table. For the Robust04 task the last 49 topics are used, and judged to a depth of 100; for other tasks, we use all of the original topic set and judgments. Our goal in collection selection was to capture as much variety as possible. TREC5 and Rob04 use the NewsWire document collection, TREC9 and TREC10 use WT10G, a small web collection, and TB04/05/06 use the GOV2 web collection."",null,null",null,null
402,"401,""e ClueWeb 2010 task (CW10) uses the largest web collection but also has fewer contributing systems and a shallow pool depth. It is representative of newer collections, which are large, and have more uncertainty associated with the judgment coverage ­ the core issue which motivated our investigation. We show results for this dataset as a practical application of our work, noting that a pooling depth of d , 20 cannot provide a ground truth for a deep metric [11]."",null,null",null,null
403,"402,""We use RBP with p ,"""" 0.95 for training and for all testing, as a representative weighted-precision metric. RBP supports graded relevance (needed to make use of the estimated background gains we generate); allows residuals to be computed; and with p """","""" 0.95 gives similar system orderings to AP and NDCG [15]. e estimated background gain of each document generated via training using RBP0.95 can also be used to compute other weighted-precision measures, such as the truncated metric SDCG@k when k > d."""""",null,null",null,null
404,"403,1h ps://github.com/xiaolul/opt est,null,null",null,null
405,"404,""We consider ve methods for predicting e ectiveness scores,"",null,null",null,null
406,"405,""three of which are baselines. e rst baseline is the lower bound, LB, which assumes unjudged documents are not relevant; the second is the interpolative estimator of Ravana and Mo at [16] (Equation 4), denoted RM; and the third is the linear model Lin. that is the best of the rank-based approaches described by Lu et al. [12]. ey"",null,null",null,null
407,"406,""are compared to the loss functions de ned in Equations 7 and 8, denoted La and Lb respectively, with the same loss function used in both stages, and aggregation via Equations 6 and 9."",null,null",null,null
408,"407,""We use Linear, Zipf and Discrete Weibull models as initial rankbased estimators [12], and hence have m ,"""" 3. Two experiments explore rank stability, categorized by how the judgment set is con-"""""",null,null",null,null
409,"408,structed: (i) pooling based judgments; and (ii) two-strata sampling,null,null",null,null
410,"409,based judgments. Rank stability is measured using the approaches,null,null",null,null
411,"410,discussed in Section 5. e same baselines are used in the rst rank,null,null",null,null
412,"411,""stability evaluation. However, for the sample-based judgments, we consider the metrics InfRBP (p ,"""" 0.95) de ned by Equation 5, and Yilmaz and Aslam's InfAP [27] as baselines. roughout the ex-"""""",null,null",null,null
413,"412,""periments, the system scores (plus residuals) and system orderings"",null,null",null,null
414,"413,""computed using the same metric, but evaluated at the full pool depth (that is, at k ,"""" d), are taken as the """"""""gold standard"""""""". e truncated metric AP@d is computed as described by Sakai [21]. Setting  . We rst test the two hypotheses in Section 4, with  in Equation 10 normalized by the number of systems. Average (over topics)  values are plo ed against pool depth in the le -hand plot in Figure 3, showing that  decreases as the pooling depth increases."""""",null,null",null,null
415,"414,""is is as expected, since the increasing pooling depth results in"",null,null",null,null
416,"415,""a more complete judgment set. Among the plo ed datasets the TB06 collection has the largest average  , and corresponds to a high relevance rate (Table 1). TREC5 is a relatively complete test collection, and hence has the lowest  among the datasets plo ed."",null,null",null,null
417,"416,""e center pane in Figure 3 shows the distribution of  across topics for d ,"""" 10. Although  is usually low for TREC5, there are still some topics that have high values. e same pa ern is"""""",null,null",null,null
418,"417,""also observable for Rob04 and TREC10. Based on our hypothesis,"",null,null",null,null
419,"418,""this observation indicates that, on earlier TREC collections, not all topics necessarily require score adjustment even at d , 10."",null,null",null,null
420,"419,""To set  we use the earlier datasets TREC5, TREC7 and TREC8 and perform a post-hoc analysis, noting that the majority of relevant"",null,null",null,null
421,"420,""documents have been identi ed in these collections, and hence"",null,null",null,null
422,"421,that the computed RMSE should be close to the true error. e,null,null",null,null
423,"422,""right-hand pane in Figure 3 shows TREC5 outcomes, with three rank-based models plo ed. Weibull (Wei.) may be an overestimate due to the shaping parameter, and Linear (Lin.) tends to provide low estimates due to the monotonically decreasing nature of the"",null,null",null,null
424,"423,""model [12]. At rst, neither of the score estimation methods works be er than the lower bound LB, but as  increases, fewer topics need to be estimated, and when  ,"""" 0.018, both estimation methods outperform LB. Similar cross-overs occur for TREC7 and TREC8."""""",null,null",null,null
425,"424,""Prediction Accuracy. We then employed  ,"""" 0.018 for the other datasets, obtaining the results shown in Table 2. When  """","""" 0 the Lb method outperforms all three baselines (LB, RM and Lin.) in terms of RMSE and Acc%, while the La approach has a higher RMSE than Lb and on earlier datasets (TREC9, TREC10) is slightly worse than the LB and Lin. baselines. at is, the loss function La provides poorer coverage of the true hypothesis space than does Lb . e RM"""""",null,null",null,null
426,"425,41,null,null",null,null
427,"426,Session 1A: Evaluation 1,null,null",null,null
428,"427,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
429,"428,Mean  (10-3) Percentage (%) RMSE (10-3),null,null",null,null
430,"429,30,null,null",null,null
431,"430,Dataset,null,null",null,null
432,"431,TREC5 Rob04,null,null",null,null
433,"432,TREC10 TB06,null,null",null,null
434,"433,25,null,null",null,null
435,"434,20,null,null",null,null
436,"435,15,null,null",null,null
437,"436,10 10 20 30 40 50 60 70 80 90 100,null,null",null,null
438,"437,Pooling Depth,null,null",null,null
439,"438,50 Datasets,null,null",null,null
440,"439,Rob04 TREC10,null,null",null,null
441,"440,40,null,null",null,null
442,"441,TB06 TREC5,null,null",null,null
443,"442,30,null,null",null,null
444,"443,20,null,null",null,null
445,"444,10,null,null",null,null
446,"445,0,null,null",null,null
447,"446,0 4 8 12 16 20 24 28 32,null,null",null,null
448,"447, (10-3),null,null",null,null
449,"448,50,null,null",null,null
450,"449,Method,null,null",null,null
451,"450,LB Lin Wei,null,null",null,null
452,"451,45,null,null",null,null
453,"452,40,null,null",null,null
454,"453,0,null,null",null,null
455,"454,4,null,null",null,null
456,"455,8 12 16 20 24,null,null",null,null
457,"456, (10-3),null,null",null,null
458,"457,""Figure 3: Le :  relative to pooling depth d . Middle: distribution of  per topic when d ,"""" 10. Right: impact of the threshold on the training set TREC5, with d """", 10."",null,null",null,null
459,"458,Dataset d,null,null",null,null
460,"459,LB,null,null",null,null
461,"460,Lin. RM,null,null",null,null
462,"461,La,null,null",null,null
463,"462,Lb,null,null",null,null
464,"463,"" , 0  , 0.018"",null,null",null,null
465,"464,"" , 0  , 0.018"",null,null",null,null
466,"465,"" , 0  , 0.018"",null,null",null,null
467,"466,10 TREC9 20,null,null",null,null
468,"467,30,null,null",null,null
469,"468,0.031 (45) 0.012 (59) 0.006 (67),null,null",null,null
470,"469,0.056 (22) 0.025 (32) 0.012 (45),null,null",null,null
471,"470,0.037 (31) 0.031 (44) 0.013 (51) 0.012 (60) 0.006 (66) 0.006 (68),null,null",null,null
472,"471,0.038 (26) 0.030 (44) 0.013 (42) 0.012 (58) 0.005 (63) 0.006 (68),null,null",null,null
473,"472,0.031 (41) 0.031 (46) 0.010 (62) 0.012 (61) 0.005 (71) 0.006 (68),null,null",null,null
474,"473,10 TREC10 20,null,null",null,null
475,"474,30,null,null",null,null
476,"475,0.038 (39) 0.016 (53) 0.007 (64),null,null",null,null
477,"476,0.064 (16) 0.030 (25) 0.015 (37),null,null",null,null
478,"477,0.034 (25) 0.033 (31) 0.015 (45) 0.014 (51) 0.007 (61) 0.007 (63),null,null",null,null
479,"478,0.036 (13) 0.031 (27) 0.016 (36) 0.014 (50) 0.007 (55) 0.007 (62),null,null",null,null
480,"479,0.027 (34) 0.028 (38) 0.012 (53) 0.012 (55) 0.006 (66) 0.006 (66),null,null",null,null
481,"480,10 Rob04 20,null,null",null,null
482,"481,30,null,null",null,null
483,"482,0.046 (21) 0.020 (34) 0.008 (49),null,null",null,null
484,"483,0.088 (5) 0.040 (9) 0.020 (14),null,null",null,null
485,"484,0.043 (21) 0.039 (20) 0.015 (33) 0.016 (34) 0.007 (49) 0.007 (52),null,null",null,null
486,"485,0.045 (9) 0.039 (17) 0.016 (26) 0.015 (32) 0.007 (47) 0.006 (53),null,null",null,null
487,"486,0.039 (20) 0.035 (20) 0.013 (38) 0.016 (35) 0.005 (59) 0.006 (55),null,null",null,null
488,"487,10 0.117 (14) 0.082 (14) 0.082 (15) 0.087 (14) 0.072 (15) 0.077 (15) 0.073 (16) 0.077 (16) TB04 20 0.053 (21) 0.039 (23) 0.039 (25) 0.041 (25) 0.035 (28) 0.037 (28) 0.033 (32) 0.036 (31),null,null",null,null
489,"488,30 0.026 (26) 0.020 (39) 0.018 (39) 0.019 (38) 0.015 (45) 0.016 (44) 0.015 (44) 0.016 (43),null,null",null,null
490,"489,10 0.125 (6),null,null",null,null
491,"490,0.080 (5),null,null",null,null
492,"491,0.085 (7) 0.085 (7),null,null",null,null
493,"492,0.070 (6) 0.070 (8),null,null",null,null
494,"493,0.067 (7) 0.067 (9),null,null",null,null
495,"494,TB05 20 0.056 (10) 0.041 (10) 0.039 (13) 0.039 (13) 0.034 (14) 0.034 (14) 0.033 (18) 0.033 (19),null,null",null,null
496,"495,30 0.028 (16) 0.022 (18) 0.021 (24) 0.021 (24) 0.018 (24) 0.018 (24) 0.017 (29) 0.017 (29),null,null",null,null
497,"496,10 0.089 (24) 0.065 (43) 0.059 (43) 0.059 (43) 0.047 (55) 0.047 (55) 0.053 (51) 0.053 (50) TB06 20 0.033 (40) 0.023 (68) 0.021 (66) 0.021 (66) 0.013 (81) 0.013 (81) 0.017 (73) 0.017 (73),null,null",null,null
498,"497,30 0.013 (58) 0.007 (87) 0.006 (85) 0.006 (85) 0.003 (94) 0.003 (94) 0.005 (89) 0.005 (89),null,null",null,null
499,"498,""Table 2: RMSE and Acc% scores for RBP0.95 for all estimation methods, with d the depth of the reduced pool, and the reference depth d of"",null,null",null,null
500,"499,each dataset as listed in Table 1. Bold numbers are the lowest RMSE and highest Acc% for that collection at that depth.,null,null",null,null
501,"500,""approach performs poorly on all of the earlier datasets, for which the assumption that unjudged documents are equivalent to judged ones is inappropriate. On the larger collections such as TB04/05/06, the gain decreases at a slower rate, making the assumptions in RM more appropriate. e LB approach has similar issues, seen in the TB04/05/06 collections. However, for TB06, smaller RMSE (and larger Acc%) values are achieved when compared to the other collections. is is because the reference depth d ,"""" 50 is smaller, resulting in larger residuals. As shown in Figure 3, some of the topics may not necessarily require a score adjustment process, especially in the earlier test collections. is explains why the LB estimator works well on those collections. As expected, applying a threshold  improves the estimation for both La and for the Lin. model, on TREC9, TREC10 and Rob04 test collections. Unsurprisingly, on TB04/05/06, only minor score changes are observed when  """","""" 0.018 is used, because the computed  values are larger than"""""",null,null",null,null
502,"501,""the threshold, indicating low coverage of the relevant documents"",null,null",null,null
503,"502,identi ed. e only unexpected observation occurs on the TB04,null,null",null,null
504,"503,""test collection, where the threshold falsely identi es Topic 734 as"",null,null",null,null
505,"504,""having a """"su cient"""" sampling of relevant documents, but around"",null,null",null,null
506,"505,""48% in the nal judged set are relevant, which increases the RMSE"",null,null",null,null
507,"506,""value. Table 3 shows the results for a leave-one-group-out experiment at d , 10 (with  ,"""" 0), demonstrating the applicability of the framework in adjusting for both system and pooling depth bias."""""",null,null",null,null
508,"507,""System Ordering Stability on Pooling-Based Judgments. e system orderings derived from the score estimates when compared against the orderings at the reference depth of k ,"""" d are shown in Figure 4. Kendall's  correlation was also computed, but the closely-related  distance is used here since it has a strictly positive value. In the rst row, when normalized  distance is measured, the estimation framework gives orderings close to the reference"""""",null,null",null,null
509,"508,42,null,null",null,null
510,"509,Session 1A: Evaluation 1,null,null",null,null
511,"510,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
512,"511,12,null,null",null,null
513,"512,Method,null,null",null,null
514,"513,12,null,null",null,null
515,"514,Method,null,null",null,null
516,"515,12,null,null",null,null
517,"516,Method,null,null",null,null
518,"517,LB RM Lb,null,null",null,null
519,"518,LB RM Lb,null,null",null,null
520,"519,LB RM Lb,null,null",null,null
521,"520,Lin. La,null,null",null,null
522,"521,Lin. La,null,null",null,null
523,"522,Lin. La,null,null",null,null
524,"523,8,null,null",null,null
525,"524,8,null,null",null,null
526,"525,8,null,null",null,null
527,"526, Distance (10-2),null,null",null,null
528,"527, Distance (10-2),null,null",null,null
529,"528, Distance (10-2),null,null",null,null
530,"529,4,null,null",null,null
531,"530,4,null,null",null,null
532,"531,4,null,null",null,null
533,"532,0,null,null",null,null
534,"533,10,null,null",null,null
535,"534,20,null,null",null,null
536,"535,30,null,null",null,null
537,"536,40,null,null",null,null
538,"537,50,null,null",null,null
539,"538,60,null,null",null,null
540,"539,Pooling Depth,null,null",null,null
541,"540,0,null,null",null,null
542,"541,10,null,null",null,null
543,"542,20,null,null",null,null
544,"543,30,null,null",null,null
545,"544,40,null,null",null,null
546,"545,50,null,null",null,null
547,"546,60,null,null",null,null
548,"547,Pooling Depth,null,null",null,null
549,"548,0,null,null",null,null
550,"549,10,null,null",null,null
551,"550,20,null,null",null,null
552,"551,30,null,null",null,null
553,"552,40,null,null",null,null
554,"553,50,null,null",null,null
555,"554,60,null,null",null,null
556,"555,Pooling Depth,null,null",null,null
557,"556,Distance,null,null",null,null
558,"557,20 16 12,null,null",null,null
559,"558,8 4 0,null,null",null,null
560,"559,10,null,null",null,null
561,"560,Method,null,null",null,null
562,"561,LB RM Lb Lin. La,null,null",null,null
563,"562,20,null,null",null,null
564,"563,30,null,null",null,null
565,"564,40,null,null",null,null
566,"565,50,null,null",null,null
567,"566,60,null,null",null,null
568,"567,Pooling Depth,null,null",null,null
569,"568,Distance,null,null",null,null
570,"569,20 16 12,null,null",null,null
571,"570,8 4 0,null,null",null,null
572,"571,10,null,null",null,null
573,"572,Method,null,null",null,null
574,"573,LB RM Lb Lin. La,null,null",null,null
575,"574,20,null,null",null,null
576,"575,30,null,null",null,null
577,"576,40,null,null",null,null
578,"577,50,null,null",null,null
579,"578,60,null,null",null,null
580,"579,Pooling Depth,null,null",null,null
581,"580,Distance,null,null",null,null
582,"581,20 16 12,null,null",null,null
583,"582,8 4 0,null,null",null,null
584,"583,10,null,null",null,null
585,"584,Method,null,null",null,null
586,"585,LB RM Lb Lin. La,null,null",null,null
587,"586,20,null,null",null,null
588,"587,30,null,null",null,null
589,"588,40,null,null",null,null
590,"589,50,null,null",null,null
591,"590,60,null,null",null,null
592,"591,Pooling Depth,null,null",null,null
593,"592,""Figure 4: System ordering comparisons (RBP0.95) for ve estimators. e rst row uses normalized  distance; the second row uses dist (Equation 11). e columns (from le ) show Rob04, TB04, and TB05, with reference lists using LB at d ,"""" 100, d """", 80 and d ,"""" 100, respectively."""""",null,null",null,null
594,"593,Dataset LB,null,null",null,null
595,"594,RM,null,null",null,null
596,"595,Lin.,null,null",null,null
597,"596,La,null,null",null,null
598,"597,Lb,null,null",null,null
599,"598,Rob04 TB04 TB05 TB06,null,null",null,null
600,"599,0.060 (19) 0.126 (4) 0.068 (11) 0.060 (9) 0.050 (19) 0.181 (11) 0.202 (11) 0.131 (9) 0.117 (11) 0.119 (11) 0.170 (6) 0.141 (5) 0.125 (4) 0.110 (4) 0.110 (5) 0.125 (22) 0.185 (32) 0.112 (35) 0.090 (48) 0.086 (46),null,null",null,null
601,"600,""Table 3: RMSE and Acc% for leave-out-one-group experiments with d ,"""" 10 throughout, averages across groups assuming that"""""",null,null",null,null
602,"601,each group in turn is omi ed from pool construction (RBP0.95).,null,null",null,null
603,"602,""ordering across a range of nominal pool depths d . e RM approach performs well on TB04/05, agreeing with the results in Table 2. However, as noted above,  is sensitive to swaps that might be inconclusive. e bo om row of Figure 4 shows the dist measure of Equation 11. Overall, there are situations in which LB performs poorly, and situations in which RM performs poorly. e Lin., La , and Lb methods consistently provide the highest agreements."",null,null",null,null
604,"603,""We also carried out paired t-tests and calculated the discrimination ratio for a signi cance level p ,"""" 0.05, and compared against the original discrimination ratios. e Lin., La , and Lb estimation methods used with all have only a small e ect on discrimination"""""",null,null",null,null
605,"604,ratio when compared to the use of LB and .,null,null",null,null
606,"605,System Ordering Stability on Sample-Based Judgments. We,null,null",null,null
607,"606,also show the applicability of our methods on the judgment set,null,null",null,null
608,"607,""constructed using a two-strata sampling method [24], which has"",null,null",null,null
609,"608,been empirically shown to assist when computing inferred metrics.,null,null",null,null
610,"609,""On this set of judgments we compute InfAP using trec eval, and InfRBP as de ned in Equation 5. Figure 6 shows that La , Lb and InfRBP give rise to stable system orderings, with normalized "",null,null",null,null
611,"610,Estimation Methods,null,null",null,null
612,"611,""d',20 CW10"",null,null",null,null
613,"612,""d',20 TB05"",null,null",null,null
614,"613,""d',60 TB05"",null,null",null,null
615,"614,UB,null,null",null,null
616,"615,RM,null,null",null,null
617,"616,dist,null,null",null,null
618,"617,0.125,null,null",null,null
619,"618,Lb,null,null",null,null
620,"619,0.100,null,null",null,null
621,"620,0.075,null,null",null,null
622,"621,La,null,null",null,null
623,"622,0.050,null,null",null,null
624,"623,0.025,null,null",null,null
625,"624,Lin,null,null",null,null
626,"625,0.000,null,null",null,null
627,"626,LB,null,null",null,null
628,"627,LB Lin La Lb RM UB LB Lin La Lb RM UB LB Lin La Lb RM UB,null,null",null,null
629,"628,Estimation Methods,null,null",null,null
630,"629,Figure 5: Normalized  distance between system orderings gen-,null,null",null,null
631,"630,erated by di erent estimation methods based on a pool of depth,null,null",null,null
632,"631,""d ,"""" 20, and on TB05 based on pool depths of d """", 20 and d , 60."",null,null",null,null
633,"632,""distance scores below 0.05 across all collections. When dist is measured, La outperforms InfRBP on all collections but TREC9, while Lb outperforms InfRBP except on TB05. e slightly worse outcome for La on TREC9 is a consequence of the increase in the number of signi cantly di erent system pairs. Note the more"",null,null",null,null
634,"633,variable outcomes generated when InfAP is used as the metric,null,null",null,null
635,"634,driving the system orderings.,null,null",null,null
636,"635,""Predictions in ClueWeb. As a nal test of our approach, we examine the CW10 collection. It has a shallow pool depth (d ,"""" 20), meaning that validation is not possible, as there is no deep-pool reference ordering. Instead, we compute the normalized  distance between each pair of estimation methods, and simply record how"""""",null,null",null,null
637,"636,""much the rankings di er, as shown in Figure 5. e UB estimator assumes that all unjudged documents are relevant. As a reference"",null,null",null,null
638,"637,43,null,null",null,null
639,"638,Session 1A: Evaluation 1,null,null",null,null
640,"639,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
641,"640,Method,null,null",null,null
642,"641,100,null,null",null,null
643,"642,10,null,null",null,null
644,"643,InfAP La,null,null",null,null
645,"644,InfRBP Lb,null,null",null,null
646,"645, Distance (10-2) Distance,null,null",null,null
647,"646,40 4,null,null",null,null
648,"647,2,null,null",null,null
649,"648,1,null,null",null,null
650,"649,TREC9,null,null",null,null
651,"650,TREC10,null,null",null,null
652,"651,Rob04,null,null",null,null
653,"652,Collection,null,null",null,null
654,"653,TB04,null,null",null,null
655,"654,TB05,null,null",null,null
656,"655,20,null,null",null,null
657,"656,Method,null,null",null,null
658,"657,10,null,null",null,null
659,"658,InfAP La,null,null",null,null
660,"659,InfRBP Lb,null,null",null,null
661,"660,TREC9,null,null",null,null
662,"661,TREC10,null,null",null,null
663,"662,Rob04,null,null",null,null
664,"663,Collection,null,null",null,null
665,"664,TB04,null,null",null,null
666,"665,TB05,null,null",null,null
667,"666,""Figure 6: System ordering comparisons on a two-strata sampled judgment set, repeated ten times. Judgments are to depth d ,"""" 10, plus a 10% random sample of remaining documents to depth 100 to form the second stratum. Note the logarithmic vertical scales."""""",null,null",null,null
668,"667,""point, we also compute the same values for TB05, at two depths, d , 20 and d ,"""" 60. At the la er depth all estimation approaches tend to agree with each other. On TB05, all of the estimation results, including UB, tend to agree on the system ordering. However, on CW10, there is clear uncertainty, con rming that d """", 60 is a more robust pool depth for TB05 than is d , 20 on either TB05 or CW10 when seeking to apply RBP0.95 as an evaluation metric. Great caution should be exercised when the d , 20 CW10 judgments are used for anything other than shallow metrics."",null,null",null,null
669,"668,7 CONCLUSIONS,null,null",null,null
670,"669,""We have presented new methods to improve system comparisons in batch IR evaluation, with the key idea being to predict a gain value for each unjudged document. We show that estimation is a viable technique to predict scores for deep evaluation metrics when limited judgments are available, including the case when the judgments are obtained using strati ed sampling rather than pooling. One important aspect of our approach is to make decisions on when to adjust topics, instead of treating all topics equally."",null,null",null,null
671,"670,""A secondary contribution is the development of a new technique to more precisely compare system orderings. By focusing on swaps that are conclusive, our weighted rank correlation coe cient dist can be used to measure the stability of a variety of estimation techniques. Using dist, we show that estimation improves our ability to score and compare systems using limited judgments."",null,null",null,null
672,"671,""It must be noted, however, that the estimation is built on the m rank-based ed models, each of which requires that when constructing the judgment set, documents up to some rank d be fully judged. is means that for some sampling-based judgment approaches, the proposed method is not applicable. Second, while we show that our estimation methods can also account for system bias to some extent, outcomes might be further improved by introducing more randomization into the optimization framework. Hence, in answer to the question posed in the title, our answer remains a somewhat cautious """"be er than before"""", rather than a """"yes""""."",null,null",null,null
673,"672,Funding. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP140103256 and DP170102231).,null,null",null,null
674,"673,REFERENCES,null,null",null,null
675,"674,""[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. 2006. A statistical method for system evaluation using incomplete judgments. In Proc. SIGIR. 541­548."",null,null",null,null
676,"675,""[2] C. Buckley, D. Dimmick, I. Soboro , and E. M. Voorhees. 2007. Bias and the limits of pooling for large collections. Inf. Retr. 10, 6 (2007), 491­508."",null,null",null,null
677,"676,[3] C. Buckley and E. M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proc. SIGIR. 25­32.,null,null",null,null
678,"677,""[4] S. Bu¨ cher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboro . 2007. Reliable information retrieval evaluation with incomplete and biased judgements. In Proc. SIGIR. 63­70."",null,null",null,null
679,"678,""[5] A. Chao and S. Lee. 1992. Estimating the number of classes via sample coverage. J. American Statistical Association 87, 417 (1992), 210­217."",null,null",null,null
680,"679,""[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proc. CIKM. 621­630."",null,null",null,null
681,"680,""[7] J. K. Jayasinghe, W. Webber, M. Sanderson, and J. S. Culpepper. 2014. Improving test collection pools with machine learning. In Proc. Aust. Doc. Comp. Symp. 2­9."",null,null",null,null
682,"681,[8] R. Kumar and S. Vassilvitskii. 2010. Generalized distances between rankings. In Proc. WWW. 571­580.,null,null",null,null
683,"682,""[9] A. Lipani, M. Lupu, and A. Hanbury. 2015. Spli ing water: Precision and antiprecision to reduce pool bias. In Proc. SIGIR. 103­112."",null,null",null,null
684,"683,""[10] A. Lipani, M. Lupu, E. Kanoulas, and A. Hanbury. 2016. e solitude of relevant documents in the pool. In Proc. CIKM. 1989­1992."",null,null",null,null
685,"684,""[11] X. Lu, A. Mo at, and J. S. Culpepper. 2016. e e ect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445."",null,null",null,null
686,"685,""[12] X. Lu, A. Mo at, and J. S. Culpepper. 2016. Modeling relevance as a function of retrieval rank. In Proc. AIRS. 3­15."",null,null",null,null
687,"686,""[13] A. Mo at, P. omas, and F. Scholer. 2013. Users versus models: What observation tells us about e ectiveness metrics. In Proc. CIKM. 659­668."",null,null",null,null
688,"687,""[14] A. Mo at, W. Webber, and J. Zobel. 2007. Strategic system comparisons via targeted relevance judgments. In Proc. SIGIR. 375­382."",null,null",null,null
689,"688,""[15] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Information Systems 27, 1 (2008), 2:1­2:27."",null,null",null,null
690,"689,""[16] S. D. Ravana and A. Mo at. 2010. Score estimation, incomplete judgments, and signi cance testing in IR evaluation. In Proc. AIRS. 97­109."",null,null",null,null
691,"690,[17] S. E. Robertson. 2007. On document populations and measures of IR e ectiveness. In Proc. ICTIR. 9­22.,null,null",null,null
692,"691,[18] T. Sakai. 2007. Alternatives to BPref. In Proc. SIGIR. 71­78. [19] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,null,null",null,null
693,"692,pool depth bias. In Proc. SIGIR. 691­692. [20] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,null,null",null,null
694,"693,""system bias. In Proc. CIKM. 581­590. [21] T. Sakai. 2014. Metrics, statistics, tests. In Bridging Between Information Retrieval"",null,null",null,null
695,"694,""and Databases, N. Ferro (Ed.). Springer, 116­163. [22] T. Schnabel, A. Swaminathan, P. I. Frazier, and T. Joachims. 2016. Unbiased"",null,null",null,null
696,"695,""comparative evaluation of ranking functions. In Proc. ICTIR. 109­118. [23] T. Schnabel, A. Swaminathan, and T. Joachims. 2015. Unbiased ranking evaluation"",null,null",null,null
697,"696,on a budget. In Proc. WWW. 935­937. [24] E. M. Voorhees. 2014. e e ect of sampling strategy on inferred measures. In,null,null",null,null
698,"697,Proc. SIGIR. 1119­1122. [25] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiment and Evaluation in,null,null",null,null
699,"698,Information Retrieval. e MIT Press. [26] W. Webber and L. A. F. Park. 2009. Score adjustment for correction of pooling,null,null",null,null
700,"699,bias. In Proc. SIGIR. 444­451. [27] E. Yilmaz and J. A. Aslam. 2008. Estimating average precision when judgments,null,null",null,null
701,"700,""are incomplete. Knowledge and Information Systems 16, 2 (2008), 173­211. [28] E. Yilmaz, E. Kanoulas, and J. A. Aslam. 2008. A simple and e cient sampling"",null,null",null,null
702,"701,method for estimating AP and NDCG. In Proc. SIGIR. 603­610. [29] Z. Zhou. 2012. Ensemble Methods: Foundations and Algorithms. CRC press. [30] J. Zobel. 1998. How reliable are the results of large-scale information retrieval,null,null",null,null
703,"702,experiments?. In Proc. SIGIR. 307­314.,null,null",null,null
704,"703,44,null,null",null,null
705,"704,,null,null",null,null

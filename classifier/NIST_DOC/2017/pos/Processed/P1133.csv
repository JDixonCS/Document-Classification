,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge,null,null",null,null
4,"3,Arman Cohan,null,null",null,null
5,"4,""Information Retrieval Lab, Dept. of Computer Science Georgetown University"",null,null",null,null
6,"5,arman@ir.cs.georgetown.edu,null,null",null,null
7,"6,ABSTRACT,null,null",null,null
8,"7,""Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles."",null,null",null,null
9,"8,KEYWORDS,null,null",null,null
10,"9,""Text Summarization, Scienti c Text, Information Retrieval"",null,null",null,null
11,"10,1 INTRODUCTION,null,null",null,null
12,"11,""In scienti c literature, related work is often referenced along with a short textual description regarding that work which we call citation text. Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16])."",null,null",null,null
13,"12,""While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18]. For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17]. This problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives."",null,null",null,null
14,"13,""We present an approach for addressing such concerns by adding the appropriate context from the reference article to the citation texts. Enriching the citation texts with relevant context from the reference paper helps the reader to better understand the context for the ideas, methods or ndings stated in the citation text."",null,null",null,null
15,"14,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080740"",null,null",null,null
16,"15,Nazli Goharian,null,null",null,null
17,"16,""Information Retrieval Lab, Dept. of Computer Science Georgetown University"",null,null",null,null
18,"17,nazli@ir.cs.georgetown.edu,null,null",null,null
19,"18,""A challenge in citation contextualization is the discourse and terminology variations between the citing and the referenced authors. Hence, traditional IR models that rely on term matching for"",null,null",null,null
20,"19,nding the relevant information are ine ective. We propose to address this challenge by a model that utilizes,null,null",null,null
21,"20,""word embeddings and domain speci c knowledge. Speci cally, our approach is a retrieval model for nding the appropriate context of citations, aimed at capturing terminology variations and paraphrasing between the citation text and its relevant reference context."",null,null",null,null
22,"21,""We perform two sets of experiments to evaluate the performance of our system. First, we evaluate the relevance of extracted contexts intrinsically. Then we evaluate the e ect of citation contextualization on the application of scienti c summarization. Experimental results on TAC 2014 benchmark show that our approach signi cantly outperforms several strong baselines in extracting the relevant contexts. We furthermore, demonstrate that our contextualization models can enhance summarizing scienti c articles."",null,null",null,null
23,"22,2 CONTEXTUALIZING CITATIONS,null,null",null,null
24,"23,""Given a citation text, our goal is to extract the most relevant context"",null,null",null,null
25,"24,to it in the reference article. These contexts are essentially certain,null,null",null,null
26,"25,""textual spans within the reference article. Throughout, colloquially,"",null,null",null,null
27,"26,we refer to the citation text as query and reference spans in the,null,null",null,null
28,"27,reference article as documents. Our approach extends Language,null,null",null,null
29,"28,Models for IR (LM) by incorporating word embeddings and domain,null,null",null,null
30,"29,ontology to address shortcomings of LM for this research purpose.,null,null",null,null
31,"30,""The goal in LM is to rank a document d according to the conditional probability p(d |q)  p(q|d ) , qi q p(qi |d ) where qi shows the tokens in the query q. Estimating p(qi |d ) is often achieved by max-"",null,null",null,null
32,"31,imum likelihood estimate from term frequencies with some sort of,null,null",null,null
33,"32,""smoothing. Using Dirichlet smoothing [21], we have:"",null,null",null,null
34,"33,p(qi |d ),null,null",null,null
35,"34,"","",null,null",null,null
36,"35,""f (qi , d ) + µ p(qi |C) w V f (w, d ) + µ"",null,null",null,null
37,"36,(1),null,null",null,null
38,"37,""where f (qi , d ) shows the frequency of term qi in document d, C"",null,null",null,null
39,"38,""is the entire collection, V is the vocabulary and µ the Dirichlet"",null,null",null,null
40,"39,""smoothing parameter. In the citation contextualization problem, (i)"",null,null",null,null
41,"40,the target reference sentences are short documents and (ii) there,null,null",null,null
42,"41,exist terminology variations between the citing author and the,null,null",null,null
43,"42,""referenced author. Hence, the citation terms usually do not appear"",null,null",null,null
44,"43,""in the documents and relying only on the frequencies of citation terms in the documents (f (qi , d )) for estimating p(qi |d ) yields an almost uniform smoothed distribution that is unable to decisively"",null,null",null,null
45,"44,distinguish between the documents.,null,null",null,null
46,"45,""Embeddings. Distributed representation (embedding) of a word w in a eld F is a mapping w  Fn where words semantically similar to w will be ideally located close to it. Given a query q, we rank the documents d according to the following scoring function"",null,null",null,null
47,"46,1133,null,null",null,null
48,"47,Short Research Paper,null,null",null,null
49,"48,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
50,"49,normalized dot product,null,null",null,null
51,"50,normalized dot product,null,null",null,null
52,"51,1.0,null,null",null,null
53,"52,normalized logit,null,null",null,null
54,"53,1.0,null,null",null,null
55,"54,normalized logit,null,null",null,null
56,"55,0.8,null,null",null,null
57,"56,0.8,null,null",null,null
58,"57,0.6,null,null",null,null
59,"58,0.6,null,null",null,null
60,"59,0.4,null,null",null,null
61,"60,0.4,null,null",null,null
62,"61,0.2,null,null",null,null
63,"62,0.0 0,null,null",null,null
64,"63,500,null,null",null,null
65,"64,1000,null,null",null,null
66,"65,1500,null,null",null,null
67,"66,2000,null,null",null,null
68,"67,0.2,null,null",null,null
69,"68,0.0 0,null,null",null,null
70,"69,200 400 600 800 1000,null,null",null,null
71,"70,Figure 1: Dot product of embeddings and its logit for a sample word and its top most similar words (top 2000 and 1000).,null,null",null,null
72,"71,which leverages this property:,null,null",null,null
73,"72,p(qi |d ),null,null",null,null
74,"73,"","",null,null",null,null
75,"74,""fsem (qi , d ) + µ p(qi |C ) w V fsem (w, d ) + µ"",null,null",null,null
76,"75,(2),null,null",null,null
77,"76,where fsem is a function that measures semantic relatedness of the,null,null",null,null
78,"77,""query term qi to the document d and is de ned as: fsem (qi , d ) ,"",null,null",null,null
79,"78,""dj d s (qi , dj ); where dj 's are document terms and s (qi , dj ) is the"",null,null",null,null
80,"79,relatedness between the the query term and document term which,null,null",null,null
81,"80,is calculated by applying a similarity function to the distributed,null,null",null,null
82,"81,representations of qi and dj . We use a transformation () of dot products between the unit vectors e (qi ) and e (dj ) corresponding to the embeddings of the terms qi and dj for the similarity function:,null,null",null,null
83,"82,""s (qi , dj ) ,  (e (qi ).e (dj )); if e (qi ).e (dj ) > "",null,null",null,null
84,"83,0;,null,null",null,null
85,"84,otherwise,null,null",null,null
86,"85,We rst explain the role of  and then the reason for considering,null,null",null,null
87,"86,the function  instead of raw dot product.  is a parameter that,null,null",null,null
88,"87,controls the noise introduced by less similar words. Many unrelated,null,null",null,null
89,"88,word vectors have non-zero similarity scores and adding them up introduces noise to the model and reduces the performance.  's function is to set the similarity between unrelated words to zero,null,null",null,null
90,"89,""instead of a positive number. To identify an appropriate value for  , we select a random set of words from the embedding model and calculate the average and standard deviation of pointwise absolute"",null,null",null,null
91,"90,value of similarities between terms from these two samples. We then select the threshold  to be two standard deviations larger than the average to only consider very high similarity values (this,null,null",null,null
92,"91,choice was empirically justi ed).,null,null",null,null
93,"92,Examining term similarity values between words shows that,null,null",null,null
94,"93,there are many terms with high similarities associated with each,null,null",null,null
95,"94,""term and these values are not highly discriminative. We apply a transfer function  to the dot product e (qi ).e (dj ) to dampen the e ect of less similar words. In other words, we only want highly"",null,null",null,null
96,"95,related words to have high similarity values and similarity should,null,null",null,null
97,"96,quickly drop as we move to less related words. We use the logit,null,null",null,null
98,"97,function for  to achieve this dampening e ect:,null,null",null,null
99,"98, (x,null,null",null,null
100,"99,),null,null",null,null
101,"100,"","",null,null",null,null
102,"101,log(,null,null",null,null
103,"102,1,null,null",null,null
104,"103,x -,null,null",null,null
105,"104,x,null,null",null,null
106,"105,),null,null",null,null
107,"106,Figure 1 shows this e ect. The purple line is the normalized dot,null,null",null,null
108,"107,product of a sample word with the most similar words in the model.,null,null",null,null
109,"108,""As illustrated, the similarity score di erences among top words"",null,null",null,null
110,"109,""is not very discriminative. However, applying the logit function"",null,null",null,null
111,"110,(green line) causes the less similar words to have lower similarity,null,null",null,null
112,"111,values to the target word. Domain knowledge. Successful word embedding methods have,null,null",null,null
113,"112,previously shown to be e ective in capturing syntactic and semantic,null,null",null,null
114,"113,relatedness between terms. These co-occurrence based models are,null,null",null,null
115,"114,""data driven. On the other hand, domain ontologies and lexicons"",null,null",null,null
116,"115,that are built by experts include some information that might not,null,null",null,null
117,"116,""be captured by embedding methods [8]. Therefore, using domain"",null,null",null,null
118,"117,knowledge can further help the embedding based retrieval model;,null,null",null,null
119,"118,we incorporate it in our model in the following ways: 1) Retro tting: Faruqui et al. [6] proposed a model that uses the,null,null",null,null
120,"119,constraints on WordNet lexicon to modify the word vectors and,null,null",null,null
121,"120,pull synonymous words closer to each other. To inject the domain,null,null",null,null
122,"121,""knowledge in the embeddings, we apply this model on two domain speci c ontologies, namely, M and Protein Ontologies (PO)1."",null,null",null,null
123,"122,We chose these two biomedical domain ontologies because they,null,null",null,null
124,"123,are in the same domain as the articles in the TAC dataset. M,null,null",null,null
125,"124,is a broad ontology that consists of biomedical terms and PO is a,null,null",null,null
126,"125,more focused ontology related to biology of proteins and genes. 2) Interpolating in the LM: We also directly incorporate the do-,null,null",null,null
127,"126,main knowledge in the retrieval model; we modify the LM into the,null,null",null,null
128,"127,following interpolated LM with parameter :,null,null",null,null
129,"128,""p(qi |d ) , p1 (qi |d ) + (1 - )p2 (qi |d ) where p1 is estimated using Eq. 2 and p2 is similar to p1 except that"",null,null",null,null
130,"129,we replace fsem with the function font which considers domain,null,null",null,null
131,"130,ontology in calculating similarities:,null,null",null,null
132,"131,""font (qi , d ),"",null,null",null,null
133,"132,""1,"",null,null",null,null
134,"133,""s2 (qi , dj ); s2 (qi , dj ),"""" ,"""""",null,null",null,null
135,"134,""if qi ,dj if qi dj"",null,null",null,null
136,"135,dj d,null,null",null,null
137,"136,""0, o.w. "",null,null",null,null
138,"137,""where   [0, 1] is a parameter and qi  dj shows that there is"",null,null",null,null
139,"138,an is-synonym relation in ontology between qi and dj 2.,null,null",null,null
140,"139,3 EXPERIMENTS,null,null",null,null
141,"140,Data. We use the TAC 2014 Biomedical Summarization benchmark3.,null,null",null,null
142,"141,This dataset contains 220 scienti c biomedical journal articles and,null,null",null,null
143,"142,313 total citation texts where the relevant contexts for each citation,null,null",null,null
144,"143,""text are annotated by 4 experts. Baselines. To our knowledge, the only published results on TAC"",null,null",null,null
145,"144,""2014 is [4], where the authors utilized query reformulation (QR)"",null,null",null,null
146,"145,""based on UMLS ontology. In addition to [4], we also implement sev-"",null,null",null,null
147,"146,eral other strong baselines to better evaluate the e ectiveness of our,null,null",null,null
148,"147,model: 1) BM25; 2) VSM: Vector Space Model that was used in [4]; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling,null,null",null,null
149,"148,with LDA smoothing which is a recent extension of the LMD to,null,null",null,null
150,"149,also account for the latent topics [10]. All the baseline parameters,null,null",null,null
151,"150,""are tuned for the best performance, and the same preprocessing is"",null,null",null,null
152,"151,applied to all the baselines and our methods. Our methods. We rst report results based on training the em-,null,null",null,null
153,"152,""beddings on Wikipedia (WEWiki). Since TAC dataset is in biomedical domain, many of the biomedical terms might be either out-"",null,null",null,null
154,"153,of-vocabulary or not captured in the correct context using gen-,null,null",null,null
155,"154,""eral embeddings, therefore we also train biomedical embeddings (WEBio)4. In addition, we report results for biomedical embeddings with retro tting (WEBio+rtrft), as well as interpolating domain knowledge (WEBio+dmn)"",null,null",null,null
156,"155,3.1 Intrinsic Evaluation,null,null",null,null
157,"156,""First, we analyze the e ectiveness of our proposed approach for contextualization intrinsically. That is, we evaluate the quality of the"",null,null",null,null
158,"157,1https://www.nlm.nih.gov/mesh/; http://pir.georgetown.edu/pro/ 2The values of the parameters  and  were selected empirically by grid search,null,null",null,null
159,"158,3,null,null",null,null
160,"159,http://www.nist.gov/tac/2014/BiomedSumm/ 4We train biomedical embeddings on TREC Genomics 2004 and 2006 collections (both,null,null",null,null
161,"160,Wikipedia and Genomics embeddings were trained using gensim implementation of,null,null",null,null
162,"161,""Word2Vec, negative sampling, window size of 5 and 300 dimensions."",null,null",null,null
163,"162,1134,null,null",null,null
164,"163,Short Research Paper,null,null",null,null
165,"164,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
166,"165,""Table 1: Results on TAC 2014 dataset. c-P, c-R, c-F: character o set Precision, Recall and F-1 scores; R : R ; cP@K: character o set precision at K. shows statistical signi cant improvement over the best baseline performance (two-tailed t-test, p<0.05). Values are percentages."",null,null",null,null
167,"166,Method,null,null",null,null
168,"167,c-P c-R c-F nDCG R -1 R -2 R -3 c-P@1 c-P@5,null,null",null,null
169,"168,VSM [4],null,null",null,null
170,"169,20.5 24.7 21.2 48.1 49.5 26.4 20.0 31.9 26.1,null,null",null,null
171,"170,BM25,null,null",null,null
172,"171,19.5 18.6 17.8 38.1 43.6 23.2 16.3 25.5 24.2,null,null",null,null
173,"172,DESM [12],null,null",null,null
174,"173,20.3 23.8 22.3 45.6 50.3 26.2 20.6 32.5 26.5,null,null",null,null
175,"174,LMD-LDA [10] 22.6 24.8 22.3 46.0 48.3 26.4 20.1 31.4 27.7,null,null",null,null
176,"175,QR [4],null,null",null,null
177,"176,22.2 29.4 23.8 49.8 50.6 27.2 21.8 37.7 28.1,null,null",null,null
178,"177,WEWiki WEBio WEBio+rtrft WEBio+dmn,null,null",null,null
179,"178,21.8 28.5 23.2 52.8 50.0 26.9 20.9 36.5 29.9,null,null",null,null
180,"179,23.9 31.2 25.5 57.1 51.9 29.2 23.1 46.2 34.1 24.8 33.6 26.4 58.3 52.4 30.7 24.0 55.5 34.9 25.4 33.0 27.0 59.8 53.0 30.6 24.4 56.1 37.1,null,null",null,null
181,"180,""Table 2: Top relevant words to the word """"expression"""" according to embeddings trained on Wikipedia vs. Genomics."",null,null",null,null
182,"181,General (Wikipedia),null,null",null,null
183,"182,interpretation sense emotion function intension manifestation expressive,null,null",null,null
184,"183,Biomedical (Genomics),null,null",null,null
185,"184,upregulation mrna,null,null",null,null
186,"185,induction protein,null,null",null,null
187,"186,abundance gene,null,null",null,null
188,"187,downregulation,null,null",null,null
189,"188,extracted citation contexts using our contextualization methods in terms of how accurate they are with respect to human annotations.,null,null",null,null
190,"189,""Evaluation. We consider the following evaluation metrics for assessing the quality of the retrieved contexts for each citation from multiple aspects: (i) Character o set overlaps of the retrieved contexts with human annotations in terms of precision (c-P), recall (c-R) and F-score (c-F). These are the recommended metrics for the task per TAC5. (ii) nDCG: we treat any partial overlaps with the gold standard as a correct context and then calculate the nDCG scores. (iii) R -N scores: To also consider the content similarity of the retrieved contexts with the gold standard, we calculate the R scores between them. (iv) Character precision at K (c-P@K): Since we are usually interested in the top retrieved spans, we consider character o set precision only for the top K spans and we denote it with """"c-P@K""""."",null,null",null,null
191,"190,Results. The results of intrinsic evaluation of contextualization are presented in Table 1. Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics. This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by [4] which improves over other baselines.,null,null",null,null
192,"191,""We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WEwiki and QR in the Table). However, using the domain speci c embeddings (WEBio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word """"expression"""" gives better intuition why is that the case. As shown, using general embeddings (left column in the table), the most similar words to """"expression"""" are those related to"",null,null",null,null
193,"192,5 https://tac.nist.gov/2014/BiomedSumm/guidelines.html,null,null",null,null
194,"193,Table 3: Breakdown of our best model's character F-score (cF) by quartiles of human performance measured by c-P.,null,null",null,null
195,"194,Quartiles (c-P),null,null",null,null
196,"195,Q1 Q2 Q3 Q4,null,null",null,null
197,"196,c-F of our model 16.14 25.41 33.72 37.50 (mean ± stdev.) ±20.20 ±7.78 ±5.81 ±5.93,null,null",null,null
198,"197,""the general meaning of it. However, many of these related words are not relevant in the biomedical context. In the biomedical context, """"expression"""" refers to """"the appearance in a phenotype attributed to a particular gene"""". As shown on the right column, the domain speci c embeddings (Bio) trained on genomics data are able to capture this meaning. This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13]. Last two rows in Table 1 show incorporating the domain knowledge in the model which results in signi cant improvement over the best baseline in terms of most metrics (e.g. 14% and 16% c-F improvements). This shows that domain ontologies provide additional information that the domain trained embeddings may not contain. While both our methods of incorporating domain ontologies prove to be e ective, interpolating domain knowledge directly (WEBio +dmn) has the edge over retro tting (WEBio +rtrft). This is likely due to the direct e ect of ontology on the interpolated language model, whereas in retro tting, the ontology rst a ects the embeddings and then the context extraction model."",null,null",null,null
199,"198,""To analyze the performance of our system more closely, we took the context identi ed by 1 annotator as the candidate and the other 3 as gold standard and evaluated the precision to obtain an estimate of human performance on each citation. We then divided the citations based on human performance to 4 groups by quartiles. Table 3 shows our system's performance on each of these groups. We observe that, when human precision is higher (upper quartiles in the table), our system also performs better and with more con dence (lower std). Therefore, the system errors correlate well with human disagreement on the correct context for the citations. Averaged over the 4 annotators for each citation, the mean precision was 56.7% (note that this translates to our c-P@1 metric). In Table 1, we observe that our best method (c-P@1 of 56.1%) is comparable with average human precision score (c-P@1 of 56.7%) which further demonstrates the e ectiveness of our model."",null,null",null,null
200,"199,3.2 External evaluation,null,null",null,null
201,"200,""Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15]. However; as argued in section 1, citation texts do not always accurately re ect the original paper. We show how adding context from the original paper can address this concern, while keeping the bene ts of citation-based summarization. Speci cally, we compare how using no contextualization, versus various proposed contextualization approaches a ect the quality of summarization. We apply the following well-known summarization algorithms on the set of citation texts, and the retrieved citation-contexts: LexRank, LSAbased, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details). We then compare the e ect of our proposed contextualization methods using the standard R -N summarization evaluation metrics."",null,null",null,null
202,"201,""Results. The results of external evaluation are illustrated in Table 4. The rst row (""""No context"""") shows the performance of each"",null,null",null,null
203,"202,1135,null,null",null,null
204,"203,Short Research Paper,null,null",null,null
205,"204,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
206,"205,Table 4: E ect of contextualization on summarization.,null,null",null,null
207,"206,Columns are summarization algorithms and rows show ci-,null,null",null,null
208,"207,tation contextualization approaches. No Context uses only,null,null",null,null
209,"208,citations without any contextualization. Evaluation metrics,null,null",null,null
210,"209,are R,null,null",null,null
211,"210,(R ) scores. () shows statistically signi cant im-,null,null",null,null
212,"211,provement over the best baseline performance (p<0.05).,null,null",null,null
213,"212,KLSUM LexRank,null,null",null,null
214,"213,LSA,null,null",null,null
215,"214,SumBasic,null,null",null,null
216,"215,Method,null,null",null,null
217,"216,R1 R2 R1 R2 R1 R2 R1 R2,null,null",null,null
218,"217,No Context,null,null",null,null
219,"218,36.0 8.3 41.3 10.8 34.7 6.5 38.7 8.7,null,null",null,null
220,"219,VSM [4],null,null",null,null
221,"220,35.3 7.9 40.0 9.9 33.5 6.2 39.5 9.4,null,null",null,null
222,"221,BM25,null,null",null,null
223,"222,35.5 8.0 39.8 9.9 33.7 6.0 38.9 8.6,null,null",null,null
224,"223,DESM [12],null,null",null,null
225,"224,36.3 8.7 40.2 10.4 32.6 6.5 38.3 7.9,null,null",null,null
226,"225,LMD-LDA [10] 38.4 9.1 43.1 11.0 37.8 7.6 40.1 8.9,null,null",null,null
227,"226,QR [4],null,null",null,null
228,"227,39.9 10.2 43.8 11.7 38.9 8.0 40.1 8.6,null,null",null,null
229,"228,WEWiki WEBio,null,null",null,null
230,"229,39.7 10.2 42.7 11.8 38.0 8.0 40.2 9.2 41.7 11.7 45.6 13.8 40.3 9.1 42.4 12.6,null,null",null,null
231,"230,WEBio+rtrft 42.9 12.2 46.2 11.6 40.0 8.9 41.3 9.7 WEBio+dmn 44.0 13.4 47.3 13.6 42.3 10.4 44.0 11.7,null,null",null,null
232,"231,""summarization approach solely on the citations without any contextualization. The next 5 rows show the baselines and last 4 rows are our proposed contextualization methods. As shown, e ective contextualization positively impacts the generated summaries. For example, our best method is """"WEBio + dmn"""" which signi cantly improves the quality of generated summaries in terms of R over the ones without any context. We observe that two low-performing baseline methods for contextualization according to Table 1 (""""VSM"""" and """"BM25"""") also do not result in any improvements for summarization. Therefore, the intrinsic quality of citation contextualization has direct impact on the quality of generated summaries. These results further demonstrate that e ective contextualization is helpful for scienti c citation-based summarization."",null,null",null,null
233,"232,4 RELATED WORK,null,null",null,null
234,"233,""Related work has mostly focused on extracting the citation text in the citing article (e.g. [1]). In this work, given the citation texts, we focus on extracting its relevant context from the reference paper. Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20]. Our proposed model utilizes word embeddings and the domain knowledge. Embeddings have been recently used in general information retrieval models. Vuli and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation. Mitra et al. [12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations."",null,null",null,null
235,"234,5 CONCLUSIONS,null,null",null,null
236,"235,Citation texts are textual spans in a citing article that explain certain contributions of a reference paper. We presented an e ective model for contextualizing citation texts (associating them with the,null,null",null,null
237,"236,appropriate context from the reference paper). We obtained statisti-,null,null",null,null
238,"237,cally signi cant improvements in multiple evaluation metrics over,null,null",null,null
239,"238,""several strong baseline, and we matched the human annotators"",null,null",null,null
240,"239,precision. We showed that incorporating embeddings and domain,null,null",null,null
241,"240,knowledge in the language modeling based retrieval is e ective for,null,null",null,null
242,"241,situations where there are high terminology variations between,null,null",null,null
243,"242,the source and the target (such as citations and their reference,null,null",null,null
244,"243,context). Citation contextualization not only can help the readers,null,null",null,null
245,"244,""to better understand the citation texts but also as we demonstrated,"",null,null",null,null
246,"245,they can improve other downstream applications such as scienti c,null,null",null,null
247,"246,""document summarization. Overall, our results show that citation"",null,null",null,null
248,"247,contextualization enables us to take advantage of the bene ts of,null,null",null,null
249,"248,""citation texts, while ensuring accurate dissemination of the claims,"",null,null",null,null
250,"249,ideas and ndings of the original referenced paper.,null,null",null,null
251,"250,ACKNOWLEDGEMENTS,null,null",null,null
252,"251,We thank the three anonymous reviewers for their helpful com-,null,null",null,null
253,"252,ments and suggestions. This work was partially supported by Na-,null,null",null,null
254,"253,tional Science Foundation (NSF) through grant CNS-1204347.,null,null",null,null
255,"254,REFERENCES,null,null",null,null
256,"255,""[1] Amjad Abu-Jbara and Dragomir Radev. 2012. Reference scope identi cation in citing sentences. In NAACL-HLT. ACL, 80­90."",null,null",null,null
257,"256,[2] Arman Cohan and Nazli Goharian. 2015. Scienti c Article Summarization Using Citation-Context and Article's Discourse Structure. In EMNLP. 390­400.,null,null",null,null
258,"257,[3] Arman Cohan and Nazli Goharian. 2017. Scienti c document summarization via citation contextualization and scienti c discourse. International Journal on Digital Libraries (2017).,null,null",null,null
259,"258,""[4] Arman Cohan, Luca Soldaini, and Nazli Goharian. 2015. Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach. In NAACL-HLT. 1042­1048."",null,null",null,null
260,"259,""[5] Anita de Waard and Henk Pander Maat. 2012. Epistemic modality and knowledge attribution in scienti c discourse: A taxonomy of types and overview of features. In Workshop on Detecting Structure in Scholarly Discourse. ACL, 47­55."",null,null",null,null
261,"260,""[6] Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar, Chris Dyer, Eduard Hovy, and A. Noah Smith. 2015. Retro tting Word Vectors to Semantic Lexicons. In NAACL-HLT. Association for Computational Linguistics, 1606­1615."",null,null",null,null
262,"261,""[7] Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth J.F. Jones. 2015. Word Embedding Based Generalized Language Model for Information Retrieval. In SIGIR. ACM, 795­798."",null,null",null,null
263,"262,""[8] Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with similarity estimation. Computational Linguistics (2015)."",null,null",null,null
264,"263,""[9] Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, and Min-Yen Kan. 2016. Overview of the CL-SciSumm 2016 Shared Task.. In BIRNDL@ JCDL."",null,null",null,null
265,"264,""[10] Fanghong Jian, Jimmy Xiangji Huang, Jiashu Zhao, Tingting He, and Po Hu. 2016. A simple enhancement for ad-hoc information retrieval via topic modelling. In SIGIR. ACM, 733­736."",null,null",null,null
266,"265,""[11] Qiaozhu Mei and ChengXiang Zhai. 2008. Generating Impact-Based Summaries for Scienti c Literature.. In ACL, Vol. 8. 816­824."",null,null",null,null
267,"266,""[12] Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. 2016. A dual embedding space model for document ranking. CoRR arXiv:1602.01137 (2016)."",null,null",null,null
268,"267,""[13] Ramesh Nallapati, Bowen Zhou, and Mingbo Ma. 2016. Classify or Select: Neural Architectures for Extractive Document Summarization. arXiv:1611.04244 (2016)."",null,null",null,null
269,"268,""[14] Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Mining text data. Springer, 43­76."",null,null",null,null
270,"269,[15] Vahed Qazvinian and Dragomir R. Radev. 2008. Scienti c Paper Summarization Using Citation Summary Networks (COLING '08). 689­696.,null,null",null,null
271,"270,""[16] Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing Citation Contexts for Information Retrieval. In CIKM. ACM, 213­222."",null,null",null,null
272,"271,""[17] Ágnes Sándor and Anita De Waard. 2012. Identifying claimed knowledge updates in biomedical research articles. In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse. ACL, 10­17."",null,null",null,null
273,"272,[18] Simone Teufel and Marc Moens. 2002. Summarizing scienti c articles: experiments with relevance and rhetorical status. Computational linguistics 28 (2002).,null,null",null,null
274,"273,[19] Ivan Vuli and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In SIGIR.,null,null",null,null
275,"274,""[20] Stephen Wan, Cécile Paris, and Robert Dale. 2009. Whetting the appetite of scientists: Producing summaries tailored to the citation context. In JCDL. 59­68."",null,null",null,null
276,"275,""[21] Chengxiang Zhai and John La erty. 2004. A study of smoothing methods for language models applied to information retrieval. TOIS 22, 2 (2004), 179­214."",null,null",null,null
277,"276,1136,null,null",null,null
278,"277,,null,null",null,null

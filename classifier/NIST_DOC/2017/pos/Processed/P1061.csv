,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,A Metric for Sentence Ordering Assessment Based on Topic-Comment Structure,null,null",null,null
4,"3,Liana Ermakova,null,null",null,null
5,"4,""LISIS, CNRS-ESIEE-INRA-UPEM Universite´ de Lorraine 5 boulevard Descartes"",null,null",null,null
6,"5,""Champs-sur-Marne, France 77454 liana.ermakova@univ-lorraine.fr"",null,null",null,null
7,"6,Josiane Mothe,null,null",null,null
8,"7,""IRIT, URM5505 CNRS ESPE, Universite´ de Toulouse"",null,null",null,null
9,"8,""118 route de Narbonne Toulouse, France 31062 josiane.mothe@irit.fr"",null,null",null,null
10,"9,Anton Firsov,null,null",null,null
11,"10,Perm State University 15 Bukireva st.,null,null",null,null
12,"11,""Perm, Russia 614990 a rsov@mail.ru"",null,null",null,null
13,"12,ABSTRACT,null,null",null,null
14,"13,""Sentence ordering (SO) is a key component of verbal ability. It is also crucial for automatic text generation. While numerous researchers developed various methods to automatically evaluate the informativeness of the produced contents, the evaluation of readability is usually performed manually. In contrast to that, we present a selfsu cient metric for SO assessment based on text topic-comment structure. We show that this metric has high accuracy."",null,null",null,null
15,"14,KEYWORDS,null,null",null,null
16,"15,""Information retrieval, evaluation, text coherence, sentence ordering, topic, comment, information structure, topic-comment structure"",null,null",null,null
17,"16,1 INTRODUCTION,null,null",null,null
18,"17,Sentence order (SO) has a strong in uence on text perception and understanding [1]. Let consider the following example:,null,null",null,null
19,"18,""Example 1.1. e Nibelung is the dwarf Alberich, and the ring in question is the one he fashions from the Rhine Gold. Wagner's opera title Der Ring des Nibelungen is most literally rendered in English as e Ring of the Nibelung."",null,null",null,null
20,"19,""e text is hardly comprehensible. When we are reading the Nibelung or the ring in question, we are asking ourselves what's it all about? which Nibelung? what question? even if in the next sentence it becomes clearer. Let us now reverse the two sentences:"",null,null",null,null
21,"20,""Example 1.2. Wagner's opera title Der Ring des Nibelungen is most literally rendered in English as e Ring of the Nibelung. e Nibelung is the dwarf Alberich, and the ring in question is the one he fashions from the Rhine Gold."",null,null",null,null
22,"21,""Now, it is clear that the Nibelung and the ring in question explain the opera title e Ring of the Nibelung. ese examples illustrate that appropriate SO is crucial for readability. Automatic text generation, particularly multi-document extractive summarization, systems also face SO problem [3, 4]. We distinguish two tasks related to SO: (1) to produce the best order for a given set"",null,null",null,null
23,"22,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080720"",null,null",null,null
24,"23,""of sentences and (2) to measure the order quality, i.e. determining how well a given list of sentences is ordered. In this paper, we focus on the la er task. In order to measure the quality of SO, if a gold standard is available, then correlation between the ground truth SO and the system order (e.g. Kendall or Spearman rank correlation coe cients) can be used [19]. e requirement of a gold standard containing the correct SO limits the usage of such methods. Indeed, gold standard is o en not available or not obvious to build manually. In contrast, in this paper we propose a self-su cient metric for text coherence assessment that does not require additional data. We evaluate the quality of the metric using the framework proposed in [12]. To evaluate the text coherence, we use a linguistic approach based on the topic-comment structure of the text and inter-sentence similarity."",null,null",null,null
25,"24,A clause-level topic is the phrase in a clause that the rest of the,null,null",null,null
26,"25,""clause is understood to be about, and the comment is what is being"",null,null",null,null
27,"26,""said about the topic. According to [24], the topic does not provide new information"",null,null",null,null
28,"27,""but connects the sentence to the context. us, the topic and the comment are opposed in terms of the given/new information. e contraposition of the given/new information is called information structure or topic-comment structure. Going back to Example 1.1, the Nibelung and the ring in question from the rst sentence are expected to be already known by the reader, i.e. they represent topics. However, only the next sentence provides the necessary information. In contrast, in Example 1.2 the rst mention of the ring and the Nibelung was given at the end of the rst sentence ( e Ring of the Nibelung) and then is detailed in the second sentence. In the rst sentence, Wagner's opera title Der Ring des Nibelungen incarnates the topic and is most literally rendered in English as e Ring of the Nibelung corresponds to the comment. In the second sentence, e Nibelung and the ring in question refers to topic, while the comment parts are presented by is the dwarf Alberich and is the one he fashions from the Rhine Gold."",null,null",null,null
29,"28,""Although, in literature topic-comment structure has been exploited for document re-ranking [13], classi cation [5], and text summarization [11], to our knowledge, it has never been applied for SO. e contribution of this paper is a completely automatic approach for SO evaluation based on topic-comment structure of a text that requires only shallow parsing and has linear complexity. Our metric considers the pairwise term similarities of the topics and the comments of the adjacent sentences in a text since word repetition is one of the formal signs of text coherence [1]."",null,null",null,null
30,"29,1061,null,null",null,null
31,"30,Short Research Paper,null,null",null,null
32,"31,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
33,"32,2 STATE OF THE ART,null,null",null,null
34,"33,""Current methods to evaluate readability are based on the familiarity of terms and syntax complexity [8]. Word complexity may be estimated by humans [7, 14, 29] or according to its length [30]. Researches also propose to use language models [8, 27]. Usually assessors assign a score to the readability of a text in some range [1]. Syntactical errors, unresolved anaphora, redundant information and coherence in uence readability and therefore the score may depend on the number of these mistakes [26]. BLEU and edit distance may be applied for relevance judgment as well as for readability evaluation. ese metrics are semi-automatic because they require a gold standard. Another set of methods is based on syntax analysis which may be combined with statistics (e.g. sentence length, depth of a parse tree, omission of personal verbs, rate of prepositional phrases, noun and verb groups) [6, 25, 32, 33], but they remain suitable only for the readability evaluation of a particular sentence and, therefore, cannot be used for assessing extracts. Lapata applies the greedy algorithm maximizing the total probability on a text corpus as well as using a speci c ordering to verb tenses [18]. Louis and Nenkova use a hidden Markov model in which the coherence between adjacent sentences is viewed as transition rules between di erent topics [23]. Barzilay and Lapata introduce an entity grid model where sentences are mapped into discourse entities with their grammatical roles [2]. Entity features are used to compute the probability of transitions between adjacent sentences. en machine learning classi ers are applied. Elsner and Charniak add co-reference features [10]. Lin et al. ameliorate the model by discourse relations [21]. e entity grid model and its extensions require syntactical parsing. e disadvantages of these models are data sparsity, domain dependence and computational complexity."",null,null",null,null
35,"34,""e closest work to ours is [12] that proposes an automatic approach for SO assessment where the similarity between adjacent sentences is used as a measure of text coherence. However, it assigns equal scores to initial and inverse SO due to the symmetric similarity measure. In contrast, our topic-comment based method assigns higher score to the text in Example 1.2 than 1.1."",null,null",null,null
36,"35,3 TOPIC-COMMENT STRUCTURE FOR SO,null,null",null,null
37,"36,""Although it is not the core element of our method, in order to be er understand the topic-comment structure of texts of di erent genres, we manually examined 10 documents randomly chosen from three datasets (30 texts in total): (1) Wikipedia; (2) TREC Robust1; (3) TREC WT10G (for collection details see Section 4). We looked at topic-topic (TT), comment-topic (CT), topic-comment (TC) and comment-comment (CC) inter-sentence relations in the texts, i.e. how frequently a topic (or a comment) of a clause became a topic (or a comment) in posterior clauses. We found that for all collections, the most frequent relation is TT, then follows CT. TT+CT compose more than 65% of the relationships that we found, whatever the collection is; it is more than 80% for Wikipedia. CC is more rare and TC is the most uncommon relation, especially in Wikipedia."",null,null",null,null
38,"37,""is preliminary analysis convinced us that using the topiccomment structure could be useful to evaluate readability and that weighting these relations could be a good cue. However, for a scalable method the text structure has to be extracted or annotated"",null,null",null,null
39,"38,1 trec.nist.gov,null,null",null,null
40,"39,""automatically. Several parsers have been developed to extract text structure such as HILDA [17] that implements topic changes or SPADE [28] which extracts rhetorical relations and has been used in [22] for example to re-rank documents. ese parsers are based on deep analysis of linguistic features and are hardly usable when large volumes of texts are involved. Moreover, they view the topiccomment relation as a remark on the statement while we consider a topic as the phrase that the rest of the clause is understood to be about as in [13]."",null,null",null,null
41,"40,""e information structure is opposed to formal structure of a clause with grammatical elements as constituents. In contrast to a grammatical subject that is a merely grammatical category, a topic refers to the information or pragmatic structure of a clause and how it is related to other clauses. However, in a simple English clause, a topic usually coincides with a subject. One of the exceptions are expletives (e.g. it is raining) that have only a comment part [15]. Since the unmarked word order in English is Subject - Verb - Object (SVO), we can assume that a topic is usually placed before a verb. As in [13], we also assume that if a subordinate clause provides details on an object, it is rather related to a comment part. us, in our method we split a sentence into two parts by a personal verb (not in nitive nor participle) where the rst part is considered to be a topic while the rest is viewed as a comment. As opposed to other methods from the literature, this method requires only part-of-speech tagging and its computational complexity is linear over the number of words as well as the number of sentences in a text."",null,null",null,null
42,"41,""e key idea of our method is that in a coherent text there are relations between topic (or comment) parts of the adjacent sentences and these relations are manifested by word repetition. We represent topic and comment parts of a sentence by bag-ofwords. In order to capture the topic-comment relation, we calculate the similarity between them. We propose to use term and noun based similarities. Since the frequencies of TT, TC, CT and CC di er, it seems reasonable to weight the inter-sentence relationship between topic and comment. us, we compute the score between two adjacent sentences si-1 and si as the weighted cosine similarity between them:"",null,null",null,null
43,"42,""sc(si-1, si )"",null,null",null,null
44,"43,"","",null,null",null,null
45,"44,|,null,null",null,null
46,"45,|si,null,null",null,null
47,"46,1 -1 ||,null,null",null,null
48,"47,|,null,null",null,null
49,"48,|si,null,null",null,null
50,"49,|,null,null",null,null
51,"50,|,null,null",null,null
52,"51,[wt,null,null",null,null
53,"52,t,null,null",null,null
54,"53,(Ti -1,null,null",null,null
55,"54,· Ti ),null,null",null,null
56,"55,+ wct (Ci-1 · Ti ) + wtc (Ti-1 · Ci ) + wcc (Ci-1 · Ci )] (1),null,null",null,null
57,"56,""where ||·|| is the length of the corresponding vector, Ti and Ci refer to the bag-of-words representations of topic or comment part of the i-th sentence respectively, the scalar product is marked by ·, wtt , wct , wtc , and wcc  [0, 1] indicate the weights of the TT, TC, CT and CC relations within the text. We estimate text coherence as an average score between adjacent sentences in a text S , (si )i|S,1| :"",null,null",null,null
58,"57,Coh(S ),null,null",null,null
59,"58,"","",null,null",null,null
60,"59,1 |S |-1,null,null",null,null
61,"60,|S |,null,null",null,null
62,"61,""sc(si-1, si )"",null,null",null,null
63,"62,""i ,2"",null,null",null,null
64,"63,(2),null,null",null,null
65,"64,4 EVALUATION,null,null",null,null
66,"65,""We conducted two series of experiments. For the rst evaluation, we used three datasets: (1) Wikipedia dump, (2) TREC Robust, and (3) WT10G. e rst dataset is a cleaned English Wikipedia"",null,null",null,null
67,"66,1062,null,null",null,null
68,"67,Short Research Paper,null,null",null,null
69,"68,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
70,"69,""XML dump of 2012 without notes, history and bibliographic references [3]. We selected 32,211 articles retrieved by the search engine Terrier2 for the queries from INEX/CLEF Tweet Contextualization Track 2012-2013 [3]. TREC (Text Retrieval Conference) Robust dataset is an unspammed collection of news articles from e Financial Times 1991-1994, Federal Register 1994, Foreign Broadcast Information Service, and e LA Times [31]. We used 193,022 documents retrieved for 249 topics from the Robust dataset. In contrast, WT10G is a snapshot of 1997 of Internet Archive with documents in HTML format, some of which are spam [16]. We retrieved 88,879 documents for 98 topics from TREC Web track 2000-2001. Documents from Robust and WT10G may contain spelling or other errors."",null,null",null,null
71,"70,""As the rst baseline we used a probabilistic graphic model proposed in [18] hereina er referred to as Lapata. Because of page number constraints, we are not detailing this method in this paper."",null,null",null,null
72,"71,e probabilities were learned from the Wikipedia dataset. For evaluation we calculated text score as the average score between the adjacent sentences. e second baseline TSP is a special case of our approach with equal weights for all relations. We also examined a variant of this method where the similarity is based on noun only (TSPNoun). We estimated the text coherence as the average cosine similarity between the neighboring sentences.,null,null",null,null
73,"72,""As in [2, 9, 20, 21, 23], we compare scores assigned to initial documents and the same documents but with randomly permuted sentences. is pairwise evaluation approach was justi ed in [21]. As in previous approaches, we assumed that the best SO is produced by a human and a good metric should re ect that by assigning higher score to initial SO. Besides, we hypothesized that a good metric has small degradation of results provoked by small permutation in SO and greater rate of shu ing provokes larger e ect since the obtained order is remoter from the human-made one."",null,null",null,null
74,"73,""erefore, as in [12], we consider the following types of datasets: (1) Source collection (O), (2) Rn-collection (Rn), (3) R-collection (R). R-collection is derived from the source collection by shu ing all sentences within each document. Rn-collection is generated from the source collection by a random shi of n sentences within each document. We used R1 and R2 collections. e introduction of transitional Rn-collections di ers from the approaches used in [2, 9, 20, 21, 23]."",null,null",null,null
75,"74,We calculated system accuracy which shows the number of times a system prefers the original order over its permutation divided by the total number of test pairs.,null,null",null,null
76,"75,is approach for metric evaluation is completely automatic and requires only a text corpus.,null,null",null,null
77,"76,""We conducted the second set of experiments on two corpora that are widely used for SO assessment: (1) airplane Accidents from the National Transportation Safety Board and (2) articles about Earthquakes from the North American News Corpus [9, 21, 23]. Each of these corpora has 100 original texts and for each document 20 permutations (2000 in total). We compared our accuracy results with those reported in the literature, namely entity grid models (Content + Egrid, Content + HMM-prodn, Content + HMM-d-seq, Egrid + HMM-prodn, Egrid + HMM-d-seq,"",null,null",null,null
78,"77,2terrier.org is a search engine platform developed by the University of Glasgow,null,null",null,null
79,"78,Robust Wikipedia,null,null",null,null
80,"79,""Egrid + Content + HMM-prodn, Egrid + Content + HMM-dseq, Egrid + Content + HMM-prodn + HMM-d-seq)3, discourse relation based approaches (Type+Arg+Sal,Arg+Sal, Type+Sal, Type+Arg, Baseline+Type+Arg+Sal)4, probabilistic content model (Probabilistic content)5 and topic based model (Topic-relaxed)5."",null,null",null,null
81,"80,Table 1: % of times where initial order is scored higher/low-,null,null",null,null
82,"81,er/equally than/to permuted text,null,null",null,null
83,"82,""Data Method O>R1 R1>O O,R1 O>R2 R2>O O,R2 O>R R>O O,R Lapata 38.80 44.07 17.14 38.33 49.26 12.42 30.25 58.96 10.79 TSP 58.04 26.20 15.75 67.13 25.10 7.77 81.43 13.86 4.71"",null,null",null,null
84,"83,TSPNoun 40.64 19.16 40.21 52.96 21.70 25.35 73.17 14.58 12.25 TopCom 58.86 25.99 15.16 68.12 24.72 7.16 83.64 12.39 3.96 TCNoun 41.04 19.89 39.08 53.21 22.53 24.25 73.82 14.83 11.35 Lapata 40.85 50.42 8.73 41.45 55.00 3.55 35.02 63.09 1.89,null,null",null,null
85,"84,TSP 57.15 29.09 13.76 65.85 28.58 5.57 81.77 15.47 2.76 TSPNoun 44.68 23.67 31.66 55.94 26.87 17.20 75.46 18.56 5.98 TopCom 57.66 29.23 13.11 66.18 28.91 4.92 82.63 15.45 1.92 TCNoun 45.14 24.45 30.41 56.07 27.85 16.07 75.57 19.36 5.07,null,null",null,null
86,"85,Lapata 42.78 51.30 5.92 42.37 55.57 2.06 32.33 66.66 1.01 TSP 54.35 24.02 21.62 65.42 24.81 9.78 84.99 12.07 2.95,null,null",null,null
87,"86,TSPNoun 36.22 15.78 48.00 49.00 19.38 31.62 76.69 13.41 9.90 TopCom 54.31 24.46 21.22 65.24 25.37 9.38 85.72 11.69 2.59 TCNoun 36.42 16.21 47.38 48.91 20.04 31.06 76.84 13.69 9.47,null,null",null,null
88,"87,WT10G,null,null",null,null
89,"88,24500,null,null",null,null
90,"89,24400,null,null",null,null
91,"90,24300,null,null",null,null
92,"91,7000 24200,null,null",null,null
93,"92,24100 6000,null,null",null,null
94,"93,24000,null,null",null,null
95,"94,""TT (Wct,.5; Wtc,.75; Wcc,1)"",null,null",null,null
96,"95,5000 23900 4000 23800,null,null",null,null
97,"96,""CT (Wtt,.25; Wtc,.75T; TWcc,1)"",null,null",null,null
98,"97,""TC (Wtt,.25; Wct,.5; CWTcc,1)"",null,null",null,null
99,"98,3000 23700 23600,null,null",null,null
100,"99,""CC (Wtt,.25; Wct,.5;TWCtc,.75)"",null,null",null,null
101,"100,2000,null,null",null,null
102,"101,0.25,null,null",null,null
103,"102,0.5,null,null",null,null
104,"103,0.75,null,null",null,null
105,"104,1,null,null",null,null
106,"105,CC 24500,null,null",null,null
107,"106,""Figure 1: Correlation between wtt , wct , wtc , wcc & accuracy"",null,null",null,null
108,"107,""In Table 1, O, R, R1 and R2 refer to the initial sentence order and the permutations described above and O > /< /, R· shows the proportion of times where initial order was scored higher/lower/equally than/to permuted text for the best set of parameter values wtt ,"""" 0.25, wct """","""" 0.5, wtc """","""" 0.75, and wcc """","""" 1. Topic-comment term based method is denoted by TopCom. For all collections according to the number of times where the original order was ranked higher than the shu ed one O > R, the topic-comment approach outperformed the simple similarity-based metrics and Lapata's baseline. Smaller permutations in sentence order provoke smaller changes in the score. In general noun-based similarity is less accurate than all term based methods. It could be caused by lower probability of non-zero similarity between the adjacent sentences. However, both topic-comment based methods showed be er results than their analogues that do not consider text information structure. We varied the coe cients (wtt , wct , wtc , wcc )  {0.25, 0.5, 0.75, 1}4 on the Wikipedia collection. Figure 1 visualizes the correlation between the number of times where the initial document is preferred to shu ed one O > R and each coe cient with the xed values of others. Smaller values of wtt , and wct refer to higher O > R, while be er results correspond to higher wtc and wcc ."""""",null,null",null,null
109,"108,Table 2 presents the results of accuracy on articles about Earthquakes and airplane Accidents reports. On the Accidents dataset,null,null",null,null
110,"109,3reported as in [23] 4reported as in [21] 5reported as in [9],null,null",null,null
111,"110,1063,null,null",null,null
112,"111,Short Research Paper,null,null",null,null
113,"112,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
114,"113,Table 2: Accuracy (%),null,null",null,null
115,"114,Method,null,null",null,null
116,"115,Accidents Earthquakes,null,null",null,null
117,"116,TSP,null,null",null,null
118,"117,88.7,null,null",null,null
119,"118,73.5,null,null",null,null
120,"119,TSPNoun,null,null",null,null
121,"120,89,null,null",null,null
122,"121,60.5,null,null",null,null
123,"122,TopCom,null,null",null,null
124,"123,86.3,null,null",null,null
125,"124,75.1,null,null",null,null
126,"125,TCNoun,null,null",null,null
127,"126,87,null,null",null,null
128,"127,60.4,null,null",null,null
129,"128,Content + Egrid,null,null",null,null
130,"129,76.8,null,null",null,null
131,"130,90.7,null,null",null,null
132,"131,Content + HMM-prodn,null,null",null,null
133,"132,74.2,null,null",null,null
134,"133,95.3,null,null",null,null
135,"134,Content + HMM-d-seq,null,null",null,null
136,"135,82.1,null,null",null,null
137,"136,90.3,null,null",null,null
138,"137,Egrid + HMM-prodn,null,null",null,null
139,"138,79.6,null,null",null,null
140,"139,93.9,null,null",null,null
141,"140,Egrid + HMM-d-seq,null,null",null,null
142,"141,84.2,null,null",null,null
143,"142,91.1,null,null",null,null
144,"143,Egrid + Content + HMM-prodn,null,null",null,null
145,"144,79.5,null,null",null,null
146,"145,95.0,null,null",null,null
147,"146,Egrid + Content + HMM-d-seq,null,null",null,null
148,"147,84.1,null,null",null,null
149,"148,92.3,null,null",null,null
150,"149,Egrid + Content + HMM-prodn + HMM-d-seq 83.6,null,null",null,null
151,"150,95.7,null,null",null,null
152,"151,Probabilistic content,null,null",null,null
153,"152,74,null,null",null,null
154,"153,-,null,null",null,null
155,"154,Topic-relaxed,null,null",null,null
156,"155,94,null,null",null,null
157,"156,-,null,null",null,null
158,"157,Baseline,null,null",null,null
159,"158,89.93,null,null",null,null
160,"159,83.59,null,null",null,null
161,"160,Type+Arg+Sal,null,null",null,null
162,"161,89.38,null,null",null,null
163,"162,86.50,null,null",null,null
164,"163,Arg+Sal,null,null",null,null
165,"164,87.06,null,null",null,null
166,"165,85.89,null,null",null,null
167,"166,Type+Sal,null,null",null,null
168,"167,86.05,null,null",null,null
169,"168,82.98,null,null",null,null
170,"169,Type+Arg,null,null",null,null
171,"170,87.87,null,null",null,null
172,"171,82.67,null,null",null,null
173,"172,Baseline+Type+Arg+Sal,null,null",null,null
174,"173,91.64,null,null",null,null
175,"174,89.72,null,null",null,null
176,"175,""we obtained the results comparable with the state of the art. For the Earthquakes articles, the accuracy of our system is slightly lower. It can be explained by the following facts: (1) models are trained and tested separately for each dataset [9, 21, 23]; (2) datasets are very homogeneous (some articles are similar up to 90% of words) and, as noted in [9], very constrained in terms of subject and style. In contrast, the coe cients for our method were learned from the Wikipedia collection. is proves that our metric is general and not restricted by a collection but it demonstrates the results comparable with the state of the art machine learning based approaches."",null,null",null,null
177,"176,5 CONCLUSIONS,null,null",null,null
178,"177,""We introduced a novel self-su cient metric for SO assessment based on topic-comment structure. It has linear complexity and requires only POS-tagging. We evaluated our method on three test collections where it demonstrated high accuracy and signi cantly outperformed similarity-based baselines as well as a transition probability based approach. e evaluation results allow drawing conclusions that (1) topic-comment methods are more e ective than simple similarity based approaches; (2) in general, noun-based similarity is less accurate. In contrast to the state of the art approaches, our method is general and not restricted by a collection but it demonstrates comparable results. One of the promising direction of the future work is the integration of co-reference resolution, synonyms and IDF. Another possible improvement is applying syntactic parsing and linguistic templates for topic-comment structure extraction."",null,null",null,null
179,"178,REFERENCES,null,null",null,null
180,"179,""[1] Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring Strategies for Sentence Ordering in Multidocument News Summarization. Journal of Arti cial Intelligence Research (2002), 35­55. 17."",null,null",null,null
181,"180,""[2] Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entitybased approach. Computational Linguistics 34, 1 (2008), 1­34."",null,null",null,null
182,"181,""[3] Patrice Bellot, Ve´ronique Moriceau, Josiane Mothe, Eric SanJuan, and Xavier Tannier. 2013. Overview of INEX 2013. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization. LNCS, Vol. 8138. 269­281. DOI: h p://dx.doi.org/10.1007/978-3-642-40802-1 27"",null,null",null,null
183,"182,""[4] Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka. 2010. A bo om-up approach to sentence ordering for multi-document summarization. Information processing & management 46, 1 (2010), 89­109."",null,null",null,null
184,"183,[5] Abdelhamid Bouchachia and R Mi ermeir. 2003. A neural cascade architecture for document retrieval. In Proc. of the International Joint Conference on Neural,null,null",null,null
185,"184,""Networks, 2003., Vol. 3. IEEE, 1915­1920. [6] Jieun Chae and Ani Nenkova. 2009. Predicting the uency of text with shallow"",null,null",null,null
186,"185,""structural features: case studies of machine translation and human­wri en text. Proc. of the 12th Conference of the European Chapter of the ACL (2009), 139­147. [7] J. S. Chall and E. Dale. 1995. Readability revisited: e new Dale­Chall readability. MA: Brookline Books, Cambridge. [8] Kevyn Collins- ompson and Jamie Callan. 2004. A Language Modeling Approach to Predicting Reading Di culty. Proc. of HLT/NAACL 4 (2004). [9] Micha Elsner, Joseph L. Austerweil, and Eugene Charniak. 2007. A Uni ed Local and Global Model for Discourse Coherence.. In HLT-NAACL. 436­443. [10] Micha Elsner and Eugene Charniak. 2008. Coreference-inspired Coherence Modeling. In Proc. of the 46th Annual Meeting of the ACL on Human Language Technologies: Short Papers (HLT-Short '08). ACL, Stroudsburg, PA, USA, 41­44. [11] Liana Ermakova. 2015. A Method for Short Message Contextualization: Experiments at CLEF/INEX. In Experimental IR Meets Multilinguality, Multimodality,"",null,null",null,null
187,"186,""and Interaction: 6th International Conference of the CLEF Association, CLEF'15, Toulouse, France, September 8-11, 2015, Proceedings. Springer International Publishing, Cham, 352­363. DOI:h p://dx.doi.org/10.1007/978-3-319-24027-5 38 [12] Liana Ermakova. 2016. Automatic Sentence Ordering Assessment Based on Similarity. In Proc. of EVIA 2016, Tokyo, Japan, 07/06/2016. NII. [13] Liana Ermakova and Josiane Mothe. 2016. Document re-ranking based on topiccomment structure. In X IEEE International Conference RCIS, Grenoble, France, June 1-3, 2016. 1­10. [14] E. Fry. 1990. A readability formula for short passages. Journal of Reading 8 (1990), 594­597. 33. [15] Michael Go¨tze, Stephanie Dipper, and Stavros Skopeteas. 2007. Information"",null,null",null,null
188,"187,""Structure in Cross-Linguistic Corpora: Annotation Guidelines for Phonology, Morphology, Syntax, Semantics, and Information Structure. Interdisciplinary Studies on Information Structure (ISIS), Working papers of the SFB 632, Vol. 7. [16] David Hawking and Nick Craswell. 2002. Overview of the TREC-2001 web track. NIST special publication (2002), 61­67. [17] Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, and others. 2010. HILDA: a discourse parser using support vector machine classi cation. Dialogue & Discourse 1, 3 (2010). [18] Mirella Lapata. 2003. Probabilistic Text Structuring: Experiments with Sentence Ordering. Proc. of ACL (2003), 542­552. [19] Guy Lebanon and John La erty. 2002. Cranking: Combining rankings using conditional probability models on permutations. Machine Learning: Proc. of the Nineteenth International Conference (2002), 363­370. [20] Jiwei Li and Eduard H. Hovy. 2014. A Model of Coherence Based on Distributed Sentence Representation.. In EMNLP, Alessandro Moschi i, Bo Pang, and Walter Daelemans (Eds.). ACL, 2039­2048. [21] Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically Evaluating Text Coherence Using Discourse Relations. In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies - vol. 1. ACL, Stroudsburg, PA, USA, 997­1006. [22] Christina Lioma, Birger Larsen, and Wei Lu. 2012. Rhetorical Relations for Information Retrieval. In Proc. of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval. 931­940. [23] Annie Louis and Ani Nenkova. 2012. A Coherence Model Based on Syntactic Pa erns. In Proc. of EMNLP-CoNLL '12. ACL, Stroudsburg, PA, USA, 1157­1168. [24] V. Mathesius and J. Vachek. 1975. A Functional Analysis of Present Day English on a General Linguistic Basis. Mouton. [25] A. Mu on, M. Dras, S. Wan, and R. Dale. 2007. Gleu: Automatic evaluation of sentence­level uency. ACL'07 (2007), 344­351. [26] Eric SanJuan, Ve´ronique Moriceau, Xavier Tannier, Patrice Bellot, and Josiane Mothe. 2012. Overview of the INEX 2011 estion Answering Track (QA@INEX). In Focused Retrieval of Content and Structure, Shlomo Geva, Jaap Kamps, and Ralf Schenkel (Eds.). Lecture Notes in Computer Science, Vol. 7424. Springer Berlin Heidelberg, 188­206. [27] L. Si and J. Callan. 2001. A statistical model for scienti c readability. Proc. of the tenth international conference on Information and knowledge management (2001), 574­576. [28] Radu Soricut and Daniel Marcu. 2003. Sentence Level Discourse Parsing Using Syntactic and Lexical Information. In Proc. of NAACL '03 on Human Language Technology - vol. 1. ACL, 149­156. [29] AJ Stenner, Ivan Horabin, Dean R Smith, and Malbert Smith. 1988. e lexile framework. Durham, NC: MetaMetrics (1988). [30] Jade Tavernier and Patrice Bellot. 2011. Combining relevance and readability for INEX 2011 estion­Answering track. (2011), 185­195. [31] Ellen M. Voorhees and Donna Harman. 2000. Overview of the Sixth Text REtrieval Conference (TREC­6). [32] S. Wan, R. Dale, and M. Dras. 2005. Searching for grammaticality: Propagating dependencies in the viterbi algorithm. Proc. of the Tenth European Workshop on Natural Language Generation (2005). [33] S. Zwarts and M. Dras. 2008. Choosing the right translation: A syntactically informed classi cation approach. Proc. of the 22nd International Conference on Computational Linguistics (2008), 1153­1160."",null,null",null,null
189,"188,1064,null,null",null,null
190,"189,,null,null",null,null

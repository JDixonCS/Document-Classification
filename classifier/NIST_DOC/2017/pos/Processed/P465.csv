,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Information Retrieval Meets Game Theory: The Ranking Competition Between Documents' Authors,null,null",null,null
4,"3,Nimrod Raifer,null,null",null,null
5,"4,""Technion, Israel nimo@campus.technion.ac.il"",null,null",null,null
6,"5,Fiana Raiber,null,null",null,null
7,"6,""Yahoo Research, Israel ana@yahoo-inc.com"",null,null",null,null
8,"7,Moshe Tennenholtz,null,null",null,null
9,"8,""Technion, Israel moshet@ie.technion.ac.il"",null,null",null,null
10,"9,ABSTRACT,null,null",null,null
11,"10,""In competitive search se ings as the Web, there is an ongoing ranking competition between document authors (publishers) for certain queries. e goal is to have documents highly ranked, and the means is document manipulation applied in response to rankings. Existing retrieval models, and their theoretical underpinnings (e.g., the probability ranking principle), do not account for post-ranking corpus dynamics driven by this strategic behavior of publishers. However, the dynamics has major e ect on retrieval e ectiveness since it a ects content availability in the corpus. Furthermore, while manipulation strategies observed over the Web were reported in past literature, they were not analyzed as ongoing, and changing, post-ranking response strategies, nor were they connected to the foundations of classical ad hoc retrieval models (e.g., content-based document-query surface level similarities and document relevance priors). We present a novel theoretical and empirical analysis of the strategic behavior of publishers using these foundations. Empirical analysis of controlled ranking competitions that we organized reveals a key strategy of publishers: making their documents (gradually) become similar to documents ranked the highest in previous rankings. Our theoretical analysis of the ranking competition as a repeated game, and its minmax regret equilibrium, yields a result that supports the merits of this publishing strategy. We further show that it can be predicted with high accuracy, and without explicit knowledge of the ranking function, whether documents will be promoted to the highest rank in our competitions. e prediction utilizes very few features which quantify changes of documents, speci cally with respect to those previously ranked the highest."",null,null",null,null
12,"11,KEYWORDS,null,null",null,null
13,"12,ad hoc retrieval; game theory; ranking competition,null,null",null,null
14,"13, e paper is based on work done while the author was at the Technion.,null,null",null,null
15,"14,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080785"",null,null",null,null
16,"15,Oren Kurland,null,null",null,null
17,"16,""Technion, Israel kurland@ie.technion.ac.il"",null,null",null,null
18,"17,1 INTRODUCTION,null,null",null,null
19,"18,""Ad hoc document retrieval models are o en based on the assumption of a xed document corpus -- i.e., corpus dynamics is not accounted for. e core challenge is estimating the relevance of a document to the query. e probability ranking principle (PRP) [25] is the theoretical foundation of this practice: to maximize user utility, documents should be ranked by their relevance probabilities."",null,null",null,null
20,"19,""In practice, document corpora are not static as documents are changed, created or removed. Some of the corpus dynamics, specifically, in competitive search se ings (e.g., the Web), results from ranking incentives of document authors, henceforth referred to as publishers. at is, publishers might modify documents to promote them in rankings induced for queries of interest. ese modi cations are referred to as search engine optimization (SEO) [16]. Spam ltering, and more generally, using document quality measures as features in learning-to-rank methods [4], are examples of approaches for rank-penalizing documents that have gone through unwarranted modi cations (a.k.a., black-hat SEO [16])."",null,null",null,null
21,"20,""However, existing retrieval approaches, and their theoretical foundations, do not account for future corpus dynamics driven by rankings. For example, it was recently shown that the PRP is sub-optimal in competitive retrieval se ings [3] as it can lead to decreased content breadth in the corpus, among other issues."",null,null",null,null
22,"21,""To estimate post-ranking corpus dynamics, speci cally, that caused by responses of publishers to rankings (i.e., document modi cations), analysis of the strategic behavior of publishers is called for. While types and techniques of SEO strategies were discussed in past work [16], these were not studied as response strategies with respect to rankings induced for speci c queries. Rather, they were presented as general actions observed on the Web (e.g., keyword stu ng and content copying)."",null,null",null,null
23,"22,""Furthermore, there are no studies, to the best of our knowledge, that analyze publishers' strategies with respect to retrieval models and their foundations; namely, the e ect, over time, on features used for ranking. Such analysis is important for incorporating strategy predictions (estimates) in, and addressing their e ects on, retrieval approaches. A case in point, it was shown that if the actual writing quality of publishers for topics is known, then this information can be used in non-deterministic retrieval models to promote content breadth in the corpus, and therefore improve search e ectiveness along time [3]. More generally, analysis of the strategic behavior of publishers is crucial for se ing theoretical foundations for handling post-ranking corpus dynamics. e same way user modeling is"",null,null",null,null
24,"23,465,null,null",null,null
25,"24,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
26,"25,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
27,"26,""important for interactive information retrieval models [30], modeling the strategic behavior of publishers in response to induced rankings is important for addressing post-ranking corpus dynamics in retrieval models."",null,null",null,null
28,"27,""We present a novel initial theoretical and empirical analysis of the (temporal) strategic behavior of publishers in terms of changes they introduce to documents in response to induced rankings. e analysis is performed in the context of classical ad hoc retrieval models in two respects. First, we focus on content-based retrieval and accordingly content manipulation. Analyzing post-ranking strategies of changing hypertext, hyperlinks and a ecting clicks or any additional signal that can be used for relevance estimation is outside the scope of this paper. Nevertheless, we note that (i) content-based relevance estimates (e.g., Okapi BM25 and languagemodel-based estimates) are among the most important ones used in learning-to-rank approaches applied over Web data [20]; (ii) content manipulation techniques are quite pervasive, speci cally, over the Web [16]; and, (iii) for experiments we use a state-of-the-art learning-to-rank approach applied with content-based estimates. Second, we empirically study content manipulation in terms of the building blocks of classical, content-based, retrieval methods."",null,null",null,null
29,"28,ese include document-query surface-level similarities [14] and query-independent document relevance priors [4].,null,null",null,null
30,"29,""Performing empirical analysis of the """"ranking competition"""" between publishers whose incentive is to have their documents ranked high, even if assuming the availability of a large-scale log of a search engine, is a major challenge due to the numerous dynamic aspects that a ect this competition. Over the Web, pages emerge and disappear, the search engine's index coverage changes rapidly, the ranking function, as well as estimates it utilizes, change throughout time and across sets of users and queries. Furthermore, di erent publishers cannot necessarily employ the same document modi cations, and many modi cations are not content-based as the ranking function also considers non content-based relevance signals."",null,null",null,null
31,"30,""Given that our goal, as described above, is to study the strategic behavior of publishers in the scope of the foundations of classical content-based retrieval models, we performed controlled empirical analysis by organizing ranking competitions between students in a course. Two basic conditions were set in these competitions. First, the students were not aware of the ranking function, nor of the actual features it used. Second, the students were incentivized to write quality documents that would be ranked high by the ranking function. As shown below, the dataset allowed to gain interesting and important observations about potential strategic behavior of publishers in a ranking competition."",null,null",null,null
32,"31,""An important observation that emerged in the competition analysis that we present is that publishers were gradually making their documents become more similar, in several respects, to those most highly ranked in previous rankings1. An interesting fundamental question that follows is whether this competing strategy can be theoretically justi ed given the information available to publishers: observations of past rankings and li le to no knowledge of the ranking function. To address this question, we present a novel game theoretic analysis of the ranking competition as a repeated"",null,null",null,null
33,"32,""1 is strategy is conceptually reminiscent of the black-hat weaving and stitching content-based SEO techniques applied over the Web [6, 16] where content from legitimate pages is copied to spam pages so as to promote them."",null,null",null,null
34,"33,game [1]. Our main theoretical result with respect to the minmax regret equilibrium of the game [17] provides formal support to the merits of this publishing (competing) strategy.,null,null",null,null
35,"34,""In addition to analyzing the ranking competition theoretically and empirically, we set as a goal to predict whether a document would be ranked the highest given that this was not the case in the previous ranking; the predictor does not have explicit knowledge of the ranking function. Interestingly, relying on very few features that quantify the extent to which the document was changed and became similar to a document previously ranked the highest can yield high accuracy prediction. ese features are inspired by the cluster hypothesis [18], and more speci cally, one of the important operational premises that it gave rise to: """"similar documents should receive similar retrieval scores"""" [9]. us, in lack of knowledge of the ranking function, the predictor essentially uses inter-document similarities as proxies for retrieval score similarities."",null,null",null,null
36,"35,Our contributions can be summarized as follows.,null,null",null,null
37,"36,· We present the rst dataset of query-based ranking competitions between publishers. e focus is on content manipulation.,null,null",null,null
38,"37,· We present an empirical analysis of publishers' strategies employed in the competitions.,null,null",null,null
39,"38,· We present a novel game theoretic analysis of the ranking competition as a repeated game. e main result of analyzing the minmax equilibrium of the game provides formal support to the merits of a key strategy employed by publishers in our games.,null,null",null,null
40,"39,""· We show that, in our se ing, it is possible to predict with high accuracy whether a document will be promoted to the highest rank in the next ranking. e prediction is based on very few features and does not rely on explicit knowledge of the ranking function."",null,null",null,null
41,"40,2 RELATED WORK,null,null",null,null
42,"41,""ere is much work on identifying, characterizing and addressing unwarranted (a.k.a. black-hat SEO [16]) actions of publishers [6]. In contrast, we focus on the strategic behavior of publishers when applying legitimate content-based manipulations."",null,null",null,null
43,"42,""Studies of the dynamic aspects of interactive retrieval focus on changes of queries and the ranking function (e.g., [15, 27, 30, 31]). Changes of clickthrough pa erns were also studied [27]. e dynamics of the collection as a result of the ranking competition, which is our focus, was not addressed."",null,null",null,null
44,"43,""ere has been work on studying and predicting the dynamics of the Web collection (e.g., [23, 26]), where the main operational goals were improving crawling policies and personalizing content delivery. Past versions of a Web page were used to improve its representation for ranking [13]. However, in contrast to our work, the dynamics has not been studied with respect to the ranking competition between publishers."",null,null",null,null
45,"44,""Recently, the publishers' ranking competition was analyzed using a game theoretic approach [3]. In contrast to our work, the assumption was that publishers have full knowledge of the ranking function, the competition was not analyzed as a repeated game, and no empirical analysis was presented."",null,null",null,null
46,"45,466,null,null",null,null
47,"46,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
48,"47,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
49,"48,""e merits of non-deterministic ranking functions from [3] were argued using a simulation of a ranking competition between publishers who stu query terms in documents [2]. In contrast with our work, publishers were assumed to know the basic (Okapi BM25) ranking function, there was no theoretical analysis of the competition and no analysis of non-simulated (real) ranking competitions."",null,null",null,null
50,"49,""A game theoretic approach was used to devise query-based ranking mechanisms that (i) maximize social welfare for ambiguous queries, by diversifying search results that are assumed to be scanned using random sequential search [12]; and (ii) balance relevance and monetization [11]. In contrast to our work, the competition between documents' authors (publishers) was not studied."",null,null",null,null
51,"50,""Game theoretical analysis has also been applied for adversarial classi cation [8, 10] and for optimizing learning-to-rank functions in non-adversarial retrieval se ings [28]. We address the competitive (adversarial) ad hoc retrieval se ing using di erent theoretical and empirical analyses."",null,null",null,null
52,"51,3 GAME THEORETIC ANALYSIS,null,null",null,null
53,"52,We analyze the ranking competition as a repeated game [1]. Analyzing the minmax regret equilibrium of the game yields a formal result that helps to explain a key strategy employed by publishers in the ranking competitions we organized as described in Section 5.,null,null",null,null
54,"53,""In what follows we assume that a query q, and some document ranking function (details below), have been xed. Let N ,"""" {1, 2, . . . , n} be a set of n publishers (documents' authors) that would like to have their documents ranked high for q. Let Di be a nite set of documents that publisher i can (or might) write to convey the information she wants to share. For ease of presentation, and to avoid technical tie-breaking issues, assume Di  Dj """",""""  for any i, j  N , i j. Let D """", ni,1Di be the set of all documents that can be wri en by the publishers."",null,null",null,null
55,"54,""We assume a complete linear ordering over D, denoted <. Such ordering can be based, for example, on a single (numeric) feature in a document representation2. Alternatively, the distance, under some representation, to a document which serves as a reference point (e.g., a document ranked the highest at some point) can serve to induce the ordering. us, for ease of exposition we can associate D with elements in the interval [0, 1]. A document ranking function for q is a mapping r : D  +. For simplicity (and avoiding tiebreaking), we assume r (di ) r (dj ) for any di , dj  D."",null,null",null,null
56,"55,""De nition 3.1. RSP(D1, . . . , Dn ) ,"""" RSP(D) denotes the single peak ranking functions. ese are functions r de ned over D, such that for no d  D, there are di , dj  D, di < d < dj such that r (di ) > r (d) and r (dj ) > r (d)."""""",null,null",null,null
57,"56,""For example, linear learning-to-rank functions [20] are single peak with respect to each feature. e negative KL divergence used in the language modeling framework [19] is a single peak function over the multinomial distributions in the simplex by the virtue of being a concave function. However, the most e ective ranking functions (e.g., those utilizing non-linear learning-to-rank methods) are not single peak. Nevertheless, it is important to keep in mind that we analyze the dynamics from the point of view of publishers"",null,null",null,null
58,"57,""2In this case, the analysis below applies to each feature in a document representation assuming that the values of others were xed."",null,null",null,null
59,"58,who have no information about the ranking function except for,null,null",null,null
60,"59,""that inferred by observing induced rankings. at is, publishers"",null,null",null,null
61,"60,""may assume, and act based on the belief, that the ranking function"",null,null",null,null
62,"61,""is single peak. Indeed, as shown in Section 5, the participants"",null,null",null,null
63,"62,(publishers) in our ranking competitions can be viewed as searching,null,null",null,null
64,"63,for the structure of a single-peak ranking function for various,null,null",null,null
65,"64,""features, although the ranking function is not single-peak."",null,null",null,null
66,"65,Below we care only about the relative ranking of documents in,null,null",null,null
67,"66,""D; thereby, we consider the possible total ordering induced by the"",null,null",null,null
68,"67,ranking function over D; there are nitely many such orderings.,null,null",null,null
69,"68,With a slight abuse of notation we will therefore refer to RSP(D),null,null",null,null
70,"69,as the set of possible single-peak orderings of documents in D.,null,null",null,null
71,"70,Let D0,null,null",null,null
72,"71,"","",null,null",null,null
73,"72,""{d 0 ,"",null,null",null,null
74,"73,1,null,null",null,null
75,"74,.,null,null",null,null
76,"75,.,null,null",null,null
77,"76,"". , dn0 }"",null,null",null,null
78,"77,be,null,null",null,null
79,"78,an,null,null",null,null
80,"79,initial,null,null",null,null
81,"80,set,null,null",null,null
82,"81,of,null,null",null,null
83,"82,documents,null,null",null,null
84,"83,where,null,null",null,null
85,"84,di0  Di is the initial document published by publisher i. We assume that each i  N possess no information at the beginning,null,null",null,null
86,"85,""about the function r  RSP(D), beyond knowing it is a single-peak"",null,null",null,null
87,"86,""ordering. Consider t rounds, l ,"""" 1, 2, . . . , t, in each of which each"""""",null,null",null,null
88,"87,""player i chooses a document di  Di , and obtains a utility of 1 if di"",null,null",null,null
89,"88,""is ranked rst and 0 otherwise. Herein, a publisher or her document"",null,null",null,null
90,"89,""is called """"winner"""" if the document was the highest ranked; all other"",null,null",null,null
91,"90,""publishers and their documents are called """"losers"""". Let TO(D) be the set of possible total ordering over D. Notice that selecting di  Di for every i  N determines an ordering over the selected documents by the single-peak function r  RSP(D). e strategy of i at round l is de ned as a function from the history of previously"",null,null",null,null
92,"91,""selected actions and outcomes (i.e., orderings) of all publishers, to"",null,null",null,null
93,"92,""the document selected by publisher i. e outcome at each round can be associated with a subset R  RSP(D) of the possible single peak functions, as it rules out particular orderings. Henceforth, R is referred to as the knowledge state, as it captures the set of currently"",null,null",null,null
94,"93,possible single peak orderings based on the observations received. e publishers ranking game just described is a repeated game [1].,null,null",null,null
95,"94,""In a repeated game, the same game is repeatedly played in rounds"",null,null",null,null
96,"95,""(iterations). Speci cally, at each round a publisher publishes a"",null,null",null,null
97,"96,""document, but a strategy in each round may relate to all information"",null,null",null,null
98,"97,""observed so far; e.g., the documents published and rankings induced"",null,null",null,null
99,"98,""in previous iterations. Accordingly, given the initial document set"",null,null",null,null
100,"99,""D0, and the total number of rounds t, the set of possible strategies for player i is denoted Si (t, di0).3 e utility Ui (t, di0, s1, . . . , sn ), where sj  Sj (t, dj0) for every j  N , is the sum of utilities of player i in rounds 1, 2, . . . , t given the corresponding strategies."",null,null",null,null
101,"100,We now introduce a slight modi cation to the utility obtained by player i in a round to capture the cost of modifying documents. is,null,null",null,null
102,"101,cost re ects both the actual e ort involved in changing a document,null,null",null,null
103,"102,""and the """"penalty"""" incurred by potentially dri ing from the actual"",null,null",null,null
104,"103,""document i planned to publish. Assume there is some negligible cost C, i.e., C |D| < 1, where eC > 0 is the cost for changing document d to document d in distance e (assume standard distance on [0, 1]) in a single round. Formally, the utility of publisher i in round l will be"",null,null",null,null
105,"104,based on its ranking (either rst or not) minus the cost of changing the document wri en in round l - 1.,null,null",null,null
106,"105,""Given the game described above, a major challenge is to de ne an"",null,null",null,null
107,"106,appropriate solution concept which predicts behavior in the game.,null,null",null,null
108,"107,e classical solution concept in game theory is the celebrated Nash,null,null",null,null
109,"108,""3Si (t, d 0) encodes all possible documents published by i at any round of the game given the previous potential orderings."",null,null",null,null
110,"109,467,null,null",null,null
111,"110,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
112,"111,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
113,"112,""equilibrium, which is a strategy pro le, one for each player, for which unilateral deviations are not bene cial (i.e., any single player cannot gain by deviating from her strategy assuming the others stick to their strategies). is solution concept always exists in"",null,null",null,null
114,"113,""nite games with complete information if players are allowed to use mixed strategies, and has been also extended to games with incomplete information where there are Bayesian assumptions about the actual game being played. However, our se ing does not exhibit such stylized assumptions, and we need to appeal to other solution concepts. In particular, in a minmax regret equilibrium [17], we consider strategy pro les such that each player (publisher) minimizes her regret when compared to the best response she could have played had she known the exact environment state (e.g., the exact ranking function) assuming others stick to their strategies; and this holds for all players simultaneously."",null,null",null,null
115,"114,""Given a strategy pro le s ,"""" (s1, . . . , sn ) the regret of i is maxxUi (t, di0, x, s-i ) - Ui (t, di0, s); s-i denotes the strategy pro le applied by all players except for i. A strategy pro le s """","""" (s1, . . . , sn ) is minmax regret equilibrium if for every i, si minimizes regret given s-i [17]. Given the de ned publishers game we can now show that:"""""",null,null",null,null
116,"115,T rium.,null,null",null,null
117,"116,3.2. Any publishers game has a minmax regret equilib-,null,null",null,null
118,"117,""P . We construct the following equilibrium. Let R be the knowledge state at the beginning of a given round l. At the beginning of round 1 all ranking functions in RSP(D) are possible, while"",null,null",null,null
119,"118,at each following round the knowledge state can only shrink in,null,null",null,null
120,"119,""terms of the number of ranking functions it contains. Let Vl  [0, 1]"",null,null",null,null
121,"120,be the set of documents which correspond to possible peaks of the,null,null",null,null
122,"121,functions in the knowledge state R; let dil-1 be the most recent,null,null",null,null
123,"122,document published by i. Let,null,null",null,null
124,"123,l i,null,null",null,null
125,"124,Di,null,null",null,null
126,"125, Vl,null,null",null,null
127,"126,such that |,null,null",null,null
128,"127,l i,null,null",null,null
129,"128,- dil -1 |,null,null",null,null
130,"129,is minimal; if two documents have this minimal distance one is,null,null",null,null
131,"130,arbitrarily selected. e document published by i in round l would,null,null",null,null
132,"131,be,null,null",null,null
133,"132,l i,null,null",null,null
134,"133,.,null,null",null,null
135,"134,(In,null,null",null,null
136,"135,the,null,null",null,null
137,"136,rst round it is di0.) We now prove that this strategy,null,null",null,null
138,"137,of i minimizes its regret.,null,null",null,null
139,"138,""Let Vt be the knowledge state at the beginning of the last round t; Vt may result from arbitrary publishers' behavior in rounds 1, 2, . . . , t - 1. No publisher j i will publish a document not in Vt as"",null,null",null,null
140,"139,""otherwise she cannot win (i.e., this strategy would be dominated). Hence, i's publishing a document out of Vt is dominated by publish-"",null,null",null,null
141,"140,""ing the previous document. ( is has no cost, and publishing out"",null,null",null,null
142,"141,""of Vt cannot result in a win.) On the other hand, since any  Vt"",null,null",null,null
143,"142,""can be a winner, the worst regret would be for not publishing"",null,null",null,null
144,"143,t i,null,null",null,null
145,"144,as de ned above.,null,null",null,null
146,"145,is is because,null,null",null,null
147,"146,t i,null,null",null,null
148,"147,might,null,null",null,null
149,"148,be,null,null",null,null
150,"149,the,null,null",null,null
151,"150,winner,null,null",null,null
152,"151,from,null,null",null,null
153,"152,this,null,null",null,null
154,"153,""point on, by the virtue of being in Vt , but it incurs minimal cost."",null,null",null,null
155,"154,""us, minimizing regret in the last round is achieved by selecting"",null,null",null,null
156,"155,t i,null,null",null,null
157,"156,as,null,null",null,null
158,"157,prescribed.,null,null",null,null
159,"158,By,null,null",null,null
160,"159,""induction,"",null,null",null,null
161,"160,using,null,null",null,null
162,"161,the,null,null",null,null
163,"162,argument,null,null",null,null
164,"163,from,null,null",null,null
165,"164,above,null,null",null,null
166,"165,results in i's strategy minimizing regret in every round.,null,null",null,null
167,"166,Two corollaries follow the proof:,null,null",null,null
168,"167,C,null,null",null,null
169,"168,3.3. e above constructed equilibrium is also a sub-,null,null",null,null
170,"169,game perfect equilibrium.,null,null",null,null
171,"170,""Namely, if an arbitrary sequence of documents has been selected up to round l < t, then in the remaining game, given the information provided so far on the potential peaks, following each player's"",null,null",null,null
172,"171,strategy in the remaining rounds is still a minmax regret equilibrium.,null,null",null,null
173,"172,C,null,null",null,null
174,"173,3.4. Losers at round l - 1 will publish in round l,null,null",null,null
175,"174,""documents that become closer to (i.e., more similar) to that of the"",null,null",null,null
176,"175,winner from round l - 1.,null,null",null,null
177,"176,""P . Assume wlog that a publisher who lost round l - 1 published dj  [0, 1] that satis es dj < dw where dw  [0, 1] was the winning document. As selecting any dj < dj is dominated given the knowledge state gathered, and the regret for publishing dj > dw is higher than that of publishing dj such that dw > dj > dj , we"",null,null",null,null
178,"177,get the corresponding phenomenon. Notice that this will also imply,null,null",null,null
179,"178,that current winners will not change their documents.,null,null",null,null
180,"179,""us, Corollary 3.4 helps to explain a key strategy employed by publishers in the competitions we organized as we show below; namely, mimicking the winners."",null,null",null,null
181,"180,4 DATA,null,null",null,null
182,"181,""As discussed in Section 1, our goal is to analyze content-based ranking competitions so as to shed light on the strategic behavior of publishers. Since there are no publicly available datasets that can be used to that end, we organized repeated ranking competitions. e resulting dataset is available at h ps://github.com/asrcdataset/asrc. (See details in Appendix A.) We next describe the essentials of the competition."",null,null",null,null
183,"182,""Fi y two senior-undergrad and grad students in an information retrieval course were the publishers. e competition included 31 di erent repeated matches, each of which was with respect to a di erent TREC's ClueWeb09 query. Each student participated in three matches. Five students competed against each other in all matches except for one in which six students competed."",null,null",null,null
184,"183,""e competition was run for eight rounds; i.e., there were eight matches per query. Before the rst round, an example of a relevant document was provided for each match (query). Students were incentivized by course-grade rewards to edit their documents along the rounds so as to have them ranked as high as possible.4 As from the second round, students participating in a match were presented with the ranking of documents submi ed in the previous round by all competitors in the same match."",null,null",null,null
185,"184,""All documents were unstructured plain text of up to 150 terms. e document ranking model was based on the state-of-the-art LambdaMART [29] learning-to-rank approach integrating three classes of features. e rst are query-dependent features, such as eryTermsRatio (ratio of query terms appearing in a document) and LMIR.DIR (language-model-based similarity of a document to the query). e second class of features are query-independent document quality measures [4, 21], including Entropy (entropy of the term distribution in a document) and StopwordsRatio (stopwords to non-stopwords ratio in a document). Increased entropy and occurrence of stopwords a est to content breadth and hence to high prior probability of relevance [4]."",null,null",null,null
186,"185,""e feature in the third class, SimInit, was used to incentivize students to write documents that dri from the initial relevant document shared by all students competing in the same match: it is"",null,null",null,null
187,"186,4Students were assigned with unique IDs and all data was anonymized.,null,null",null,null
188,"187,468,null,null",null,null
189,"188,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
190,"189,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
191,"190,# of publishers # of publishers,null,null",null,null
192,"191,10 8 6 4 2 0 0 1 2 3 4 5 6 7 8 9 10 11 # of matches won,null,null",null,null
193,"192,50 40 30 20 10,null,null",null,null
194,"193,0 1234567 # of consecutive matches won,null,null",null,null
195,"194,Figure 1: e number of (consecutive) matches won (x-axis) by a given number of publishers (y-axis).,null,null",null,null
196,"195,""based on the language-model similarity of a document to the initial relevant document. We note that in practical scenarios, publishers would rarely change their documents so they will not include the information originally intended for sharing. Indeed, in the theoretical analysis presented in Section 3, a cost was assigned to changes of documents. Yet, as we show below, the conclusions we draw about strategies are aligned with our theoretical results."",null,null",null,null
197,"196,""Documents (manually) classi ed as keyword stu ed were penalized in the ranking. Information about the ranking function and the features it utilizes was not disclosed to students. e resulting collection contains (i) 1279 documents: 31 initial relevant documents and 1248 documents created by students, 897 of which are unique5; (ii) keyword stu ng annotations; and (iii) exhaustive relevance judgments. Appendix A provides additional details of the collection and ranking model."",null,null",null,null
198,"197,5 EMPIRICAL ANALYSIS,null,null",null,null
199,"198,""In the following analysis, winner (loser) is a document (or publisher thereof) which was (not) ranked rst in a match."",null,null",null,null
200,"199,5.1 Analysis of wins,null,null",null,null
201,"200,""Figure 1 (le ) presents the number of matches won by a given number of publishers (students). e competition included 248 distinct matches (8 rounds × 31 matches per round). Each student was assigned with exactly 3 queries; hence, the maximum number of matches a student could win is 24. We see that only two of the students did not win even a single match, a esting to the students' engagement in the competition. e maximum number of matches won was 11, less than half of the maximal possible number of wins, indicating that the competition was dynamic."",null,null",null,null
202,"201,Figure 1 (right) presents the number of consecutive matches won by a given number of students; the maximum is the number of rounds (eight). We see that most students could retain the rst rank for at most three rounds. Only a small number of students retained the rst rank in more than four rounds. is nding further a ests to the strong competition held between the students.,null,null",null,null
203,"202,5.2 Analysis of strategies,null,null",null,null
204,"203,""By Corollary 3.4, to win matches, losers in previous rounds will publish documents that become similar to that of the winner from the preceding round. Accordingly, we next analyze the similarity"",null,null",null,null
205,"204,""5Several students submi ed the same document over a few rounds; e.g., if the document was the highest ranked in a previous round."",null,null",null,null
206,"205,of documents that did not win a match (losers) to the winner over a series of rounds in which these documents remained losers. e similarity to the winner is estimated with respect to some of the features used to rank documents which were presented in Section 4.,null,null",null,null
207,"206,""e eryTermsRatio and LMIR.DIR features quantify the querydocument match; LMIR.DIR is a representative query-document surface-level similarity estimate [14]. e Entropy and StopwordsRatio features are among the most e ective query independent content-based document relevance priors reported in the literature [4]. Hence, the analysis of the strategic behavior of publishers we present next relies on estimates that constitute the foundations of classical content-based ad hoc retrieval approaches."",null,null",null,null
208,"207,Figure 2 depicts the average values of the features for documents that were losers in at least four consecutive rounds before winning a match6. We distinguish between documents whose feature value four rounds before winning a match was lower than or equal to that of the winner (LW) and those whose feature value was higher than that of the winner (L>W). We also present the average feature values of winners (W).,null,null",null,null
209,"208,""We see in Figure 2 that the average feature values of winners remain relatively stable along the competition; thus, winner documents, o en wri en by di erent publishers, tend to be quite similar along a few dimensions (features)."",null,null",null,null
210,"209,""Figure 2 also shows that, in general, Entropy o en decreases along rounds and eryTermsRatio increases. is a ests to high content repetition in winner documents that might result from high occurrence of query terms. SimInit decreases which is potentially due to our rewarding diversi cation with respect to the initial relevant document."",null,null",null,null
211,"210,""More generally, we observe a clear trend throughout the competition: feature values of loser documents which became winners were becoming closer, o en gradually converging, to those of winners from previous rounds regardless of their initial values. at is, in lack of knowledge of features used for document ranking, losers were mimicking winners and thereby indirectly a ecting these features. is nding is in accordance with Corollary 3.4."",null,null",null,null
212,"211,6 PREDICTING WINNERS,null,null",null,null
213,"212,""Given that loser publishers apply the strategy of mimicking the winners, an interesting challenge rises: leveraging aspects of this strategy to predict, without using explicit knowledge of the ranking function, which loser publisher in round l - 1 will win round l assuming that a previous loser indeed wins this round.7"",null,null",null,null
214,"213,""For prediction, we represent each document as a feature vector and de ne two sets of features (details below) that quantify the extent to which the document becomes more similar to the winner of the previous round. e features in the rst set are estimates of this similarity on a macro level, where documents are treated as"",null,null",null,null
215,"214,""6Similar trends were observed for other features used by the ranking model and for losers that lost in at least three or ve consecutive rounds. ese results are omi ed as they convey no further insight. 7Predicting which publisher will win round l regardless if it won round l - 1 is a challenge for future work. As stated in the proof of Corollary 3.4, and as observed in the competitions, winners did not tend to change their documents. is is a fundamental di erence with the dynamics of loser documents which makes this prediction task challenging. For example, many of the dynamics-based features de ned below for predicting whether a loser will turn to a winner are degenerated for winner documents which do not change."",null,null",null,null
216,"215,469,null,null",null,null
217,"216,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
218,"217,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
219,"218,QueryTermsRatio 1.0,null,null",null,null
220,"219,LMIR.DIR 7.0,null,null",null,null
221,"220,Entropy 4.4,null,null",null,null
222,"221,StopwordsRatio 1.0,null,null",null,null
223,"222,SimInit 0.8,null,null",null,null
224,"223,LW,null,null",null,null
225,"224,0.8,null,null",null,null
226,"225,5.0,null,null",null,null
227,"226,4.2,null,null",null,null
228,"227,0.8,null,null",null,null
229,"228,0.6,null,null",null,null
230,"229,L>W,null,null",null,null
231,"230,W,null,null",null,null
232,"231,0.6,null,null",null,null
233,"232,3.0,null,null",null,null
234,"233,4.0,null,null",null,null
235,"234,0.6,null,null",null,null
236,"235,0.4,null,null",null,null
237,"236,0.4 -4 -3 -2 -1 0,null,null",null,null
238,"237,1.0 -4 -3 -2 -1 0,null,null",null,null
239,"238,3.8 -4 -3 -2 -1 0,null,null",null,null
240,"239,0.4 -4 -3 -2 -1 0,null,null",null,null
241,"240,0.2 -4 -3 -2 -1 0,null,null",null,null
242,"241,""Figure 2: Averaged feature values of documents that were losers in at least four consecutive rounds before becoming winners, and whose feature values four rounds before winning were either lower or equal (`LW') or higher (`L>W') than those of the winner. `W': averaged feature value of the corresponding winners. x-axis: (minus) number of rounds before a document won a match. e values of LMIR.DIR are scaled by 100."",null,null",null,null
243,"242,whole units. e features in the second set are micro level similarity estimates that allow to analyze the potential actions taken by publishers to make their documents similar to the winner.,null,null",null,null
244,"243,6.1 Features,null,null",null,null
245,"244,""e features in the rst set, henceforth Macro features, are estimates of the bag-of-terms textual similarities (denoted SIM) between the document in round l (D), the document wri en by the same publisher in the previous round l - 1 (PD) and the winner of the previous round l - 1 (PW). e Cosine between tf.idf vector representations of documents is the similarity estimate. ree estimates are used: SIM(D,PD), SIM(D,PW) and SIM(PD,PW)."",null,null",null,null
246,"245,""Using these inter-document similarity measures is inspired by the cluster hypothesis [18] which states that """"closely associated documents tend to be relevant to the same requests"""". More speci cally, an important operational manifestation of the cluster hypothesis is the premise that e ective retrieval methods should assign similar documents with similar retrieval scores [9]. Based on the premise, given that the predictor we devise has no explicit knowledge of the ranking method used, inter-document similarities can potentially serve as proxies for similarities between retrieval scores."",null,null",null,null
247,"246,""e features in the second set, henceforth Micro features, focus on potential actions of publishers to make their documents similar to PW, the winner of the previous round. A document becomes similar to the winner, based on a bag-of-terms representation, if terms from the winner are added and terms not in the winner are removed. Accordingly, given a set S of terms, ADD(PW) and RMV(PW) are the number of unique terms t  S used in PW that were added to, or removed from, the document, respectively. Similarly, ADD( PW) and RMV( PW) are the number of unique terms t  S not used in PW that were added to or removed from the document, respectively. We de ne three term sets S: (i) query terms ( ery), (ii) frequent terms, speci cally, stopwords (Stopwords), and (iii) non-frequent terms not in the query ( ery Stopwords).8"",null,null",null,null
248,"247,""Overall, we use 15 features: 3 Macro (SIM(D,PD), SIM(D,PW), SIM(PD,PW)) and 12 Micro ({ADD(PW), RMV(PW), ADD( PW), RMV( PW) } × { ery, Stopwords, ery Stopwords})."",null,null",null,null
249,"248,""e Macro features, which quantify temporal inter-document similarity changes, are ranking-model agnostic. e Micro features"",null,null",null,null
250,"249,8A term is considered a stopword if it is among the 100 most frequent alphanumeric terms in the ClueWeb09 Category B corpus.,null,null",null,null
251,"250,""are based on temporal changes of addition/deletion of terms. While term-based information (e.g., query-terms occurrence) would be used by any reasonable ranker, the prediction model uses no explicit knowledge about how this information is used by the non-linear ranker applied in the competition, nor about other features used for ranking."",null,null",null,null
252,"251,6.2 Prediction setup,null,null",null,null
253,"252,""In each round of the competition, queries for which the winner of the previous round remained the winner were discarded, as our goal is to predict which loser publisher in a previous round will win the current round. us, the number of queries considered in each round ranges from 6 to 26 (out of all 31 queries)."",null,null",null,null
254,"253,""We used the features9 from Section 6.1 for binary classi cation with logistic regression (LReg), linear SVM (LSVM), polynomial SVM (PSVM) and random forests (RForest) via the scikit-learn library [22]; the two classes are winner and loser. To train the classi ers and set hyper-parameter values, we used leave-one-out cross validation over rounds. e documents submi ed by students with respect to all considered queries in a round served for testing; those submi ed in the remaining six rounds, excluding the rst, served for training. Prediction was performed per query: the document in the current round which was wri en by a loser publisher from the previous round and which was assigned the highest classi cation score was predicted the winner; all other documents were predicted to be losers."",null,null",null,null
255,"254,""Prediction e ectiveness is measured using Accuracy: the percentage of documents correctly predicted as winners or losers, and F1: harmonic mean of Precision and Recall.10 Values are averaged over queries and test folds. Statistically signi cant e ectiveness di erences are determined using the two-tailed paired t-test with p  0.05 applied over queries."",null,null",null,null
256,"255,""e hyper-parameter values of the classi ers were selected to optimize Accuracy over the train set. For LReg, LSVM and PSVM, the value of the regularization parameter is in {1, 10, 50, 100}.11"",null,null",null,null
257,"256,""e degree of the polynomial SVM (PSVM) was in {2, 3, 4, 5}. e number of trees and leaves for RForest were selected from {10, 50,"",null,null",null,null
258,"257,""9Feature values were min-max normalized per query. 10 Precision is the fraction of correctly predicted winners out of all documents predicted to be winners. Recall is the fraction of winners correctly predicted as winners. 11LReg, LSVM and PSVM were trained with L1 regularization."",null,null",null,null
259,"258,470,null,null",null,null
260,"259,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
261,"260,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
262,"261,""Table 1: Prediction e ectiveness of the four classi ers (LReg, LSVM, PSVM, RForest) and the baselines. e performance di erences between each of the classi ers and each of the baselines are statistically signi cant. All the di erences with RForest are statistically signi cant. Bold: the best result in a row. Note: F1 of AllLosers is 0 due to zero Recall."",null,null",null,null
263,"262,Random Majority AllWinners AllLosers LReg LSVM PSVM RForest,null,null",null,null
264,"263,Accuracy 0.627,null,null",null,null
265,"264,F1,null,null",null,null
266,"265,0.242,null,null",null,null
267,"266,0.685 0.363,null,null",null,null
268,"267,0.247 0.396,null,null",null,null
269,"268,0.753 0.000,null,null",null,null
270,"269,0.849 0.859 0.867 0.695 0.712 0.730,null,null",null,null
271,"270,0.878 0.752,null,null",null,null
272,"271,""100, 500} and {10, 20, 30}, respectively. All other hyper-parameters were set to their default values [22]."",null,null",null,null
273,"272,6.3 Prediction e ectiveness,null,null",null,null
274,"273,""Main result. We compare the prediction e ectiveness of the aforementioned classi ers with that of four baselines. All prediction algorithms predict as winner(s) documents whose publishers lost the previous round. (i) Random: a single winner is randomly selected; (ii) Majority: the document whose publisher won the majority of past rounds for the query is predicted the winner (ties are broken arbitrarily); (iii) AllWinners: all documents are predicted winners, in which case only one document per query is correctly predicted; and, (iv) AllLosers: all documents are predicted losers, in which case all but one of the documents are correctly predicted as losers. e results are presented in Table 1. Although the four classi ers (LReg, LSVM, PSVM and RForest) utilize no knowledge of the ranking model, they predict with high e ectiveness the winner of the current round. Moreover, the di erences in prediction e ectiveness between each of the classi ers and each of the four baselines are substantial and statistically signi cant. ese ndings a est to the ability to predict winners from previous losers in our competitions based on macro-level and micro-level manipulation strategies of publishers."",null,null",null,null
275,"274,""Among the four classi ers, the lowest performance is posted by LReg, while the highest is posted by RForest. Hence, in the analysis to follow we focus on RForest."",null,null",null,null
276,"275,""Feature analysis. We next study the relative e ectiveness of the sets of features used in RForest. Recall that the 15 features belong to two sets: Macro and Micro. e Micro features belong to three subsets: ery, Stopwords and ery Stopwords. In Table 2 we compare the prediction e ectiveness of training RForest using di erent combinations of these (sub)sets of features. We present for reference the e ectiveness of the Majority, AllWinners and AllLosers baselines. We see that using even a single (sub)set of features yields prediction e ectiveness that statistically signi cantly surpasses that of the baselines. Among the three subsets of Micro features, the query-term-based features ( ery) are the most e ective. Integrating all three subsets leads to prediction e ectiveness that always statistically signi cantly surpasses that of using either one or two of the subsets. We also see that using Micro features alone leads to slightly higher e ectiveness than using only Macro features; the di erence is not statistically signi cant. Yet, combining both sets yields the highest prediction e ectiveness. ese"",null,null",null,null
277,"276,""ndings suggest that the Micro and Macro features, as well as the three subsets of Micro features, are complementary to some extent."",null,null",null,null
278,"277,""Table 2: Using subsets of features for prediction. All di erences with respect to Majority, AllWinners, AllLosers and Macro+Micro are statistically signi cant. Bold: best result in a column."",null,null",null,null
279,"278,Majority AllWinners AllLosers,null,null",null,null
280,"279,ery Stopwords,null,null",null,null
281,"280,""ery Stopwords ery+Stopwords ery+ ery Stopwords Stopwords+ ery Stopwords Micro , ery+ Stopwords+ ery Stopwords Macro Macro+ ery Macro+Stopwords Macro+ ery Stopwords"",null,null",null,null
282,"281,Macro+Micro (all features),null,null",null,null
283,"282,Accuracy,null,null",null,null
284,"283,0.685 0.247 0.753,null,null",null,null
285,"284,0.821 0.809 0.796 0.826 0.825 0.813 0.837 0.836 0.851 0.849 0.847,null,null",null,null
286,"285,0.878,null,null",null,null
287,"286,F1,null,null",null,null
288,"287,0.363 0.396 0.000,null,null",null,null
289,"288,0.635 0.594 0.587 0.650 0.648 0.617 0.673 0.671 0.702 0.694 0.692,null,null",null,null
290,"289,0.752,null,null",null,null
291,"290,We next study the e ectiveness of individual features. Table 3 presents the Accuracy of ablation tests performed upon RForest.12 We also report MRR: the mean di erence between the reciprocal ranks of the actual winner when documents are ranked in descending and ascending order of individual feature values. We rst see that removing any single feature statistically signi cantly hurts Accuracy. is a ests to the complementary nature of the features.,null,null",null,null
292,"291,""e negative MRR of SIM(D,PD) indicates, as expected, that to win a match, a loser publisher should change her document with respect to the previous round. e positive MRR of SIM(PD,PW) and SIM(D,PW) suggest that the document should be similar to the winner (from the previous round) in the previous and current rounds so as to win the match. is nding is aligned with Corollary 3.4."",null,null",null,null
293,"292,""e MRR of features in the ery and Stopwords subsets indicate that adding (removing) query terms is always good (bad) practice for becoming the winner, regardless of whether these terms were used by the winner. is nding is further supported by the observations about eryTermsRatio in Section 5.2. In contrast, removing (adding) frequent terms, i.e., stopwords, is always good (bad) practice, regardless of the use of stopwords by the winner. e MRR of features in the ery Stopwords subset, which refers to terms that are neither query terms nor stopwords, imply that to"",null,null",null,null
294,"293,12Similar pa erns were observed for F1. ese results are omi ed as they convey no additional insight.,null,null",null,null
295,"294,471,null,null",null,null
296,"295,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
297,"296,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
298,"297,Table 3: Ablation tests: Accuracy of RForest when trained without one feature. RForest's Accuracy with all features is 0.878. All di erences with RForest are statistically signi cant. MRR: the mean reciprocal ranks di erence of the winner when ranking documents in descending and ascending order of feature values.,null,null",null,null
299,"298,Macro Features,null,null",null,null
300,"299,Micro Features,null,null",null,null
301,"300,Feature,null,null",null,null
302,"301,""SIM(D,PD) SIM(D,PW) SIM(PD,PW)"",null,null",null,null
303,"302,Ablation,null,null",null,null
304,"303,0.829 0.837 0.820,null,null",null,null
305,"304,MRR,null,null",null,null
306,"305,-0.136 0.168 0.184,null,null",null,null
307,"306,Feature,null,null",null,null
308,"307,ADD(PW) RMV(PW) ADD( PW) RMV( PW),null,null",null,null
309,"308,ery,null,null",null,null
310,"309,Ablation MRR,null,null",null,null
311,"310,0.844 0.851 0.840 0.834,null,null",null,null
312,"311,0.130 -0.043,null,null",null,null
313,"312,0.104 -0.620,null,null",null,null
314,"313,Stopwords,null,null",null,null
315,"314,Ablation MRR,null,null",null,null
316,"315,0.840 0.843 0.856 0.847,null,null",null,null
317,"316,-0.219 0.043,null,null",null,null
318,"317,-0.023 0.060,null,null",null,null
319,"318,ery Stopwords,null,null",null,null
320,"319,Ablation MRR,null,null",null,null
321,"320,0.841 0.857 0.849 0.837,null,null",null,null
322,"321,0.183 -0.081 -0.053,null,null",null,null
323,"322,0.029,null,null",null,null
324,"323,""win a match a document should become more similar to the winner by adding and not removing terms that were used by the winner (positive MRR of ADD(PW) and negative MRR of RMV(PW)), as well as removing and not adding terms that were not used by the winner (positive MRR of RMV( PW) and negative MRR of ADD( PW)). ese manipulations which do not directly a ect the query-document similarity estimates a ect other features used by the ranking model (e.g., Entropy)."",null,null",null,null
325,"324,7 CONCLUSIONS,null,null",null,null
326,"325,""We presented an initial theoretical and empirical study of the strategic behavior of publishers (documents' authors) in query-based ranking competitions. e publishers' goal is promoting their documents in rankings using li le available information, mainly about past rankings. Analysis of ranking competitions that we organized revealed that to achieve their goal, publishers were making their documents similar to those ranked the highest in previous rounds. A game theoretic analysis of the competition yielded a result that provides formal support to the merits of this strategy. We also showed that high accuracy prediction of whether a document will be promoted to the rst rank in our competitions can be achieved using very few features which quantify document changes."",null,null",null,null
327,"326,Acknowledgments We thank the reviewers for their comments. is work was supported in part by a Google Faculty Research,null,null",null,null
328,"327,Award.,null,null",null,null
329,"328,A THE RANKING COMPETITIONS,null,null",null,null
330,"329,""We next discuss the competition guidelines provided to students (Section A.1), the incentives for participating in the competition (Section A.2), the queries and examples of relevant documents (Section A.3) and the ranking function used (Section A.4)."",null,null",null,null
331,"330,A.1 Guidelines,null,null",null,null
332,"331,""To alleviate the task for students, and to increase their engagement in the competition, the length of all documents was limited to 150 terms. Students were instructed to write unstructured plain text documents."",null,null",null,null
333,"332,Duplication of other documents (determined based on a bagof-terms comparison) resulted in the duplicate document being ranked last. e students were permi ed to copy parts of other documents from the competition or the Web. Students were guided,null,null",null,null
334,"333,""to write documents of the highest quality avoiding slang and informal language. e use of black hat SEO techniques [16], such as keyword stu ng, was discouraged by telling the students that the ranking function will penalize low quality documents, partly based on human annotations. We informed the students that they could use the provided examples of relevant documents, but that the documents they create need not necessarily be relevant."",null,null",null,null
335,"334,A.2 Incentives,null,null",null,null
336,"335,""e incentive for participating in the competition was earning extra credit points for the exam. For each query, a student earned two thirds of a point if her document was ranked rst for a query in a match. A third of a point was given to all other students competing with respect to the same query (i.e., the same match)."",null,null",null,null
337,"336,In the rst half of the competition many students did not (signi cantly) update their documents even if these were not ranked,null,null",null,null
338,"337,""rst. erefore, we further incentivized the students by changing the reward mechanism as from the h round. e student whose document was ranked rst for a query was reworded one point. Students whose documents were ranked second and third were rewarded two thirds and third of a point, respectively. Students whose documents were ranked lower did not receive any credit."",null,null",null,null
339,"338,A.3 eries and initial relevant documents,null,null",null,null
340,"339,""We used the titles of 31 topics selected from 1-200 from TREC 20092012 as queries. e preference was selecting queries with clear commercial intent, since they were more likely to stir up competition as is the case on the Web. at is, having a document ranked high ( rst) with respect to these queries should lead to increased (monetary) pro ts to the document's publisher on the Web. e selected queries focused mostly on topics related to products or services. Examples include """"used car parts"""", """"cheap internet"""" and """"gmat prep classes"""". e queries were randomly assigned to students ensuring that two students will not compete against each other in more than two di erent matches; the assignments were not changed throughout the competition."",null,null",null,null
341,"340,""As already noted, for each query we provided a single example of a relevant document. e goal was to provide the students with information regarding the underlying information need as the queries are very short. To produce these relevant documents, we"",null,null",null,null
342,"341,rst used the TREC topic description as a query in a commercial search engine. We extracted from the highly ranked documents,null,null",null,null
343,"342,472,null,null",null,null
344,"343,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
345,"344,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
346,"345,% of keyword stuffed documents % of relevant documents,null,null",null,null
347,"346,12,null,null",null,null
348,"347,100,null,null",null,null
349,"348,3,null,null",null,null
350,"349,8,null,null",null,null
351,"350,75,null,null",null,null
352,"351,4,null,null",null,null
353,"352,4,null,null",null,null
354,"353,50,null,null",null,null
355,"354,5,null,null",null,null
356,"355,0 12345678 round,null,null",null,null
357,"356,25 12345678 round,null,null",null,null
358,"357,""Figure 3: e percentage of documents annotated as keyword stu ed (le ) and relevant (right) by at least 3, 4 or 5 annotators, averaged over queries per each of the eight competition rounds."",null,null",null,null
359,"358,candidate window passages of up to 150 terms. e passages were annotated for relevance by four annotators. We kept extracting passages for each query until a passage was judged relevant by at least three annotators. is passage then served as the initial relevant document example for all students competing for the query.,null,null",null,null
360,"359,A.4 Ranking model,null,null",null,null
361,"360,We next describe the ranking model used for all queries in each round of every match in the competition.,null,null",null,null
362,"361,""A.4.1 Learning-to-rank. We used a learning-to-rank (LTR) approach with 25 features to rank the documents. Most of the features (22) were all those used in Microso 's learning-to-rank datasets13 for the """"whole document"""" except for the Boolean Model, Vector Space Model and LMIR.ABS features. As noted above, the documents in our competition are unstructured plain text. us, all the features are computed only for the entire document. Since documents in our competitive se ing are prone to manipulation, we used three additional features which were shown to be highly e ective for spam classi cation [21] and Web retrieval [4]: (i) the ratio between the number of stopwords and non-stopwords in a document, (ii) the percentage of stopwords in a stopword list that appear in the document, and (iii) the entropy of the term distribution in a document [4]. For the two stopword-based features, the list of stopwords was composed of the 100 most frequent alphanumeric terms in the ClueWeb09 Category B corpus [21]."",null,null",null,null
363,"362,""e ClueWeb09 category B dataset with queries 1-200 was used to learn the LTR model. Speci cally, the model was applied upon the 1000 documents most highly ranked by using LMIR.DIR, i.e., the negative cross entropy between the unsmoothed and Dirichletsmoothed (with µ ,"""" 1000) unigram language models induced from the query and documents, respectively14. We used LambdaMART [29] via the RankLib library15 to integrate the di erent features. e number of trees and leaves were selected from {100, 250, 500, 750, 1000} and {10, 25, 50}, respectively. NDCG@5 served for optimization when learning the model. In each round of the competition, we added the (unjudged) documents submi ed by students in all matches to the ClueWeb09 Category B corpus to"""""",null,null",null,null
364,"363,""13www.research.microso .com/en-us/projects/mslr 14We deliberately did not remove suspected spam documents from the initial document ranking, e.g., using Waterloo's spam classi er [7]. is practice allows learning a model using low quality (e.g., spam) documents. 15 www.lemurproject.org/ranklib.ph"",null,null",null,null
365,"364,""have more updated values of corpus statistics, e.g., inverse document frequency (idf). Yet, we did not re-train the ranker. e Indri toolkit was used for indexing and retrieval16. We applied Krovetz stemming upon queries and documents and removed stopwords on the INQUERY list only from queries. e LMIR.JM feature was used with  ,"""" 0.1; for BM25, we set k1 """", 1.2 and b , 0.75."",null,null",null,null
366,"365,""A.4.2 Results diversification. To encourage students to considerably change their documents rather than introduce minor modi cations to the initially provided relevant document, starting from the second round, they were advised to diversify their documents with respect to the relevant document. To further encourage diversi cation, we applied the MMR method [5] with respect to the initial relevant document dinit. Accordingly, the score assigned to document d with respect to query q is score(q, d) d,""""ef rank(d, LT R) - (1 - )rank(d, dinit), where  """","""" 0.5, rank(d, LT R) is the rank of d in a ranking of all the documents in a match induced by the LTR method and rank(d, dinit) is the rank of d in a ranking created based on the similarity with dinit; here, the rank of the lowest ranked document is 1. e similarity with dinit was computed using LMIR.DIR treating d as the query."""""",null,null",null,null
367,"366,""A.4.3 Keyword stu ing. Keyword stu ng [16], speci cally of query terms, is one of the most applicable manipulation approaches the students could employ to promote their unstructured plain text documents in rankings. To avoid rewarding excessive keyword stu ng, and to encourage writing of high quality documents, each document was manually classi ed as keyword stu ed or not17."",null,null",null,null
368,"367,e annotation was performed via CrowdFlower18; each document was judged by ve annotators from English speaking countries19.,null,null",null,null
369,"368,""e inter-annotator agreement for keyword stu ng, computed using the free-marginal multi-rater kappa measure [24], is 0.88. A document classi ed as keyword stu ed by at least four annotators was rank-penalized: with probability 0.5 it was swapped with the next document in the ranking. If several consecutively ranked documents were keyword stu ed, then only the lowest ranked document was penalized."",null,null",null,null
370,"369,""In Figure 3 (le ) we present for each round the percentage of documents classi ed as keyword stu ed by at least three, four or"",null,null",null,null
371,"370,ve annotators averaged over queries. We can see a mostly downward trend until the h round. In the h round we observe the lowest percentage of keyword stu ed documents. Starting from the,null,null",null,null
372,"371,""h round the percentage of keyword stu ed documents gradually increases. We hypothesis that in the rst half of the competition students' engagement gradually decreased. In the second half, as from the h round in which the rewards for having a document ranked high substantially increased, students started using manipulated texts even more so as to have their documents ranked high. In the h round, there might have been some confusion due to the introduction of a new reward mechanism."",null,null",null,null
373,"372,""16 www.lemurproject.org/indri 17A document was annotated as keyword stu ed if it contained excessive repetition of words which seemed unnatural or arti cially introduced. 18www.crowd ower.com 19Annotators were also instructed to classify documents as spam if they were hard to understand, non-cohesive, did not make any sense or were useless to anyone seeking information. Yet, none of the documents was classi ed as spam."",null,null",null,null
374,"373,473,null,null",null,null
375,"374,Session 4B: Retrieval Models and Ranking 2,null,null",null,null
376,"375,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
377,"376,22,null,null",null,null
378,"377,95,null,null",null,null
379,"378,1,null,null",null,null
380,"379,16,null,null",null,null
381,"380,80,null,null",null,null
382,"381,MAP@5 NDCG@5,null,null",null,null
383,"382,2,null,null",null,null
384,"383,10,null,null",null,null
385,"384,65,null,null",null,null
386,"385,3,null,null",null,null
387,"386,4 12345678,null,null",null,null
388,"387,round,null,null",null,null
389,"388,50 12345678 round,null,null",null,null
390,"389,""Figure 4: e MAP@5 (le ) and NDCG@5 (right) performance of the ranking induced by the retrieval method in each round. Binary relevance judgments were induced for computing MAP@5 by considering a document relevant if its relevance grade was at least 1, 2 or 3."",null,null",null,null
391,"390,A.5 Ranking e ectiveness,null,null",null,null
392,"391,""All documents in the collection were judged for relevance. Annotators were presented with both the title and description of each TREC topic, and were asked to classify a document as relevant if it satis ed the information need stated in the description. As was the case with keyword stu ng annotation, each document was judged by ve annotators from English speaking countries via CrowdFlower. e inter-annotator agreement rate, computed using the free-marginal multi-rater kappa measure [24], was 0.67. Four-scale graded relevance judgments were generated using the annotations as follows. A document judged relevant by less than three annotators was labeled as non-relevant (0). Documents judged relevant by at least three, four or ve annotators were labeled as marginally relevant (1), fairly relevant (2) and highly relevant (3), respectively."",null,null",null,null
393,"392,""As noted above, to address the potential manipulation of documents by students, the retrieval method used in the competition (i) was based on a learning-to-rank approach with multiple features, (ii) incorporated highly e ective document-quality measures and (iii) penalized keyword stu ed documents. Figure 3 (right) presents the percentage of documents classi ed relevant by at least three, four or ve annotators per round averaged over queries. We see that, in general, the percentage of relevant documents decreased over the course of the competition. While many of the documents were judged relevant by at least three annotators, far fewer documents were judged relevant by at least four or ve annotators. is"",null,null",null,null
394,"393,nding a ests to the negative e ects of SEO. In Figure 4 we present the MAP@5 and NDCG@5 e ectiveness,null,null",null,null
395,"394,""of the document ranking induced by the retrieval method in each of the eight competition rounds. We see that the e ectiveness of the ranking has gradually decreased over rounds, which can be partially a ributed to the fact that fewer relevant documents were generated by students as seen in Figure 3. We also see that in the"",null,null",null,null
396,"395,""rst two rounds the e ectiveness of the ranking was much higher than that in the rounds to follow. We found that in the rst two rounds students used the initially provided relevant documents without signi cantly changing them. A er the second round, in which the retrieval method was changed by applying diversi cation with respect to the given relevant document (see Section A.4.2), students started diversifying their documents by introducing noise, using non-relevant information and applying content manipulation."",null,null",null,null
397,"396,REFERENCES,null,null",null,null
398,"397,[1] R. Aumann and M. Maschler. 1995. Repeated Games with Incomplete Information. MIT Press.,null,null",null,null
399,"398,[2] Ran Ben-Basat and Elad Kravi. 2016. e ranking game. In Proceedings of the 19th International Workshop on Web and Databases. 7.,null,null",null,null
400,"399,""[3] Ran Ben-Basat, Moshe Tennenholtz, and Oren Kurland. 2015. e Probability Ranking Principle is Not Optimal in Adversarial Retrieval Se ings. In Proceedings of ICTIR. 51­60."",null,null",null,null
401,"400,""[4] Michael Bendersky, W. Bruce Cro , and Yanlei Diao. 2011. ality-biased ranking of web documents. In Proceedings of WSDM. 95­104."",null,null",null,null
402,"401,""[5] Jaime G. Carbonell and Jade Goldstein. 1998. e Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In Proceedings of SIGIR. 335­336."",null,null",null,null
403,"402,""[6] Carlos Castillo and Brian D. Davison. 2010. Adversarial Web Search. Foundations and Trends in Information Retrieval 4, 5 (2010), 377­486."",null,null",null,null
404,"403,""[7] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2011. E cient and e ective spam ltering and re-ranking for large web datasets. Informaltiom Retrieval Journal 14, 5 (2011), 441­465."",null,null",null,null
405,"404,""[8] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004. Adversarial Classi cation. In Proceedings of KDD. 99­108."",null,null",null,null
406,"405,[9] Fernando Diaz. 2005. Regularizing Ad Hoc Retrieval Scores. In Proceedings of CIKM. 672­679.,null,null",null,null
407,"406,[10] Ran El-Yaniv and Mordechai Nisenson. 2010. On the Foundations of Adversarial Single-Class Classi cation. CoRR (2010).,null,null",null,null
408,"407,""[11] K r Eliaz and Ran Spiegler. 2011. A simple model of search engine pricing. e Economic Journal 121, 556 (2011), F329­F339."",null,null",null,null
409,"408,""[12] K r Eliaz and Ran Spiegler. 2016. Search design and broad matching. American Economic Review 106, 3 (2016), 563­586."",null,null",null,null
410,"409,[13] Jonathan L. Elsas and Susan T. Dumais. 2010. Leveraging temporal dynamics of document content in relevance ranking. In Proceedings of WSDM. 1­10.,null,null",null,null
411,"410,""[14] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In Proceedings of SIGIR. 49­56."",null,null",null,null
412,"411,""[15] Norbert Fuhr. 2008. A probability ranking principle for interactive information retrieval. Information Retrieval 11, 3 (2008), 251­265."",null,null",null,null
413,"412,""[16] Zolta´n Gyo¨ngyi and Hector Garcia-Molina. 2005. Web Spam Taxonomy. In Proceedings of AIRWeb 2005, First International Workshop on Adversarial Information Retrieval on the Web. 39­47."",null,null",null,null
414,"413,[17] Nathanael Hya l and Craig Boutilier. 2004. Regret Minimizing Equilibria and Mechanisms for Games with Strict Type Uncertainty. In Proceedings of UAI. 268­277.,null,null",null,null
415,"414,""[18] N. Jardine and C. J. van Rijsbergen. 1971. e use of hierarchic clustering in information retrieval. Information Storage and Retrieval 7, 5 (1971), 217­240."",null,null",null,null
416,"415,""[19] John D. La erty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR. 111­119."",null,null",null,null
417,"416,""[20] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer. I­XVII, 1­285 pages."",null,null",null,null
418,"417,""[21] Alexandros Ntoulas, Marc Najork, Mark Manasse, and Dennis Fe erly. 2006. Detecting spam web pages through content analysis. In Proceedings of WWW. 83­92."",null,null",null,null
419,"418,""[22] Fabian Pedregosa, Gae¨l Varoquaux, Alexandre Gramfort, Vincent Michel,"",null,null",null,null
420,"419,""Bertrand irion, Olivier Grisel, Mathieu Blondel, Peter Pre enhofer, Ron"",null,null",null,null
421,"420,""Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,"",null,null",null,null
422,"421,""Ma hieu Brucher, Ma hieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825­2830."",null,null",null,null
423,"422,[23] Kira Radinsky and Paul N. Benne . 2013. Predicting content change on the web. In Proceedings of WSDM. 415­424.,null,null",null,null
424,"423,""[24] Justus J. Randolph. 2016. Online Kappa Calculator (2008). Retrieved February 6,"",null,null",null,null
425,"424,h p://justus.randolph.name/kappa. (2016). [25] Stephen E. Robertson. 1977. e Probability Ranking Principle in IR. Journal,null,null",null,null
426,"425,""of Documentation (1977), 294­304. Reprinted in K. Sparck Jones and P. Wille (eds), Readings in Information Retrieval, pp. 281­286, 1997. [26] Ae´cio S. R. Santos, Bruno Pasini, and Juliana Freire. 2016. A First Study on Temporal Dynamics of Topics on the Web. In Proceedings of WWW. 849­854. [27] Marc Sloan and Jun Wang. 2012. Dynamical information retrieval modelling: a portfolio-armed bandit machine approach. In Proceedings WWW. 603­604. [28] Hong Wang, Wei Xing, Kaiser Asif, and Brian D. Ziebart. 2015. Adversarial Prediction Games for Multivariate Losses. In Proceedings of NIPS. 2728­2736. [29] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Information Retrieval 13, 3 (2010), 254­270. [30] Grace Hui Yang, Marc Sloan, and Jun Wang. 2016. Dynamic Information Retrieval Modeling. Morgan & Claypool Publishers. [31] Yinan Zhang and Chengxiang Zhai. 2015. Information Retrieval as Card Playing: A Formal Model for Optimizing Interactive Retrieval Interface. In Proceedings of SIGIR. 685­694."",null,null",null,null
427,"426,474,null,null",null,null
428,"427,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Session 4A: Evaluation 2,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Comparing In Situ and Multidimensional Relevance Judgments,null,null",null,null
4,"3,Jiepu Jiang,null,null",null,null
5,"4,""Center for Intelligent Information Retrieval, University of Massachuse s"",null,null",null,null
6,"5,Amherst jpjiang@cs.umass.edu,null,null",null,null
7,"6,Daqing He,null,null",null,null
8,"7,""School of Computing and Information, University of Pi sburgh"",null,null",null,null
9,"8,dah44@pi .edu,null,null",null,null
10,"9,James Allan,null,null",null,null
11,"10,""Center for Intelligent Information Retrieval, University of Massachuse s"",null,null",null,null
12,"11,Amherst allan@cs.umass.edu,null,null",null,null
13,"12,ABSTRACT,null,null",null,null
14,"13,""To address concerns of TREC-style relevance judgments, we explore two improvements. e rst one seeks to make relevance judgments contextual, collecting in situ feedback of users in an interactive search session and embracing usefulness as the primary judgment criterion. e second one collects multidimensional assessments to complement relevance or usefulness judgments, with four distinct alternative aspects examined in this paper--novelty, understandability, reliability, and e ort."",null,null",null,null
15,"14,""We evaluate di erent types of judgments by correlating them with six user experience measures collected from a lab user study. Results show that switching from TREC-style relevance criteria to usefulness is fruitful, but in situ judgments do not exhibit clear bene ts over the judgments collected without context. In contrast, combining relevance or usefulness with the four alternative judgments consistently improves the correlation with user experience measures, suggesting future IR systems should adopt multi-aspect search result judgments in development and evaluation."",null,null",null,null
16,"15,""We further examine implicit feedback techniques for predicting these judgments. We nd that click dwell time, a popular indicator of search result quality, is able to predict some but not all dimensions of the judgments. We enrich the current implicit feedback methods using post-click user interaction in a search session and achieve be er prediction for all six dimensions of judgments."",null,null",null,null
17,"16,KEYWORDS,null,null",null,null
18,"17,Relevance judgment; search experience; implicit feedback.,null,null",null,null
19,"18,1 INTRODUCTION,null,null",null,null
20,"19,""Test collection-based IR evaluation relies on human assessments of search result quality. e most popular method is the Cran eldstyle relevance judgments [9], such as the approach used in TREC [10], where assessors (usually trained experts) judge a preassigned set of search results one a er another using criteria that focus on topical relevance. is method had achieved great success but also a racted criticism such as focusing solely on topical relevance and ignoring real users' perceptions of the usefulness of results in a particular search context. We examine two directions to improve this status quo."",null,null",null,null
21,"20,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080840"",null,null",null,null
22,"21,""One direction is to incorporate context into assessments. at is, the value of a search result depends on the scenario and context of accessing the result. Belkin et al. [5] proposed to evaluate interactive search systems by the usefulness of each interaction for accomplishing a search task. We can apply this model to search result judgments--to assess the usefulness of a click (the perceived usefulness of a clicked result). is intrinsically requires us to switch from relevance to usefulness as the primary judgment criteria, and to collect in situ judgments to take into account the particular time and context of accessing a search result."",null,null",null,null
23,"22,""Two recent e orts [25, 36] examined this direction. Kim et al. [25] collected users' in situ feedback of clicked results a er they had"",null,null",null,null
24,"23,""nished examining the results. However, they restricted the in situ feedback to """"thumbs-up"""" or """"thumbs-down"""". Mao et al. [36] asked users to assess the usefulness of the clicked results a er a search session without considering the particular context. Both studies reported improved correlations with search experience measures comparing to TREC-style relevance judgments by external assessors. However, neither study excluded the in uence of the di erence between searchers and external assessors on relevance judgments."",null,null",null,null
25,"24,""e other direction is to use a combination of multiple aspects of judgments. Many previous studies tried to complement relevance with seemingly reasonable dimensions, such as novelty [6, 55], understandability [41, 56], credibility [39, 46, 51, 53], readability [42, 49], e ort [20, 50, 54], freshness [11], etc. Multidimensional judgments are also popular approaches used in user-centric evaluation models [19, 27, 52]. However, most previous IR studies had only examined one particular alternative dimension to relevance, and they had not veri ed the value of multidimensional judgments by correlating with user experience measures."",null,null",null,null
26,"25,""We evaluate and compare these two directions. We collected users' search result judgments from six dimensions (relevance, usefulness, novelty, understandability, reliability, and e ort) in two se ings--an in situ one that happened right a er users had nished examining a clicked search result (called in situ judgments), and a context-independent one collected a er a search session (called post-session judgments). We evaluate the two types of judgments on six dimensions by correlating with six search experience measures collected from a laboratory user study. We also examined implicit feedback methods for predicting these judgments."",null,null",null,null
27,"26,We examine the following questions in the rest of this article:,null,null",null,null
28,"27,· Do in situ judgments be er correlate with search experience measures than context-independent (post-session) ones? Do multiple dimensions of judgments help relevance/usefulness judgments be er correlate with search experience measures? Which dimensions of judgments should we collect to improve a particular user experience measure? Section 3 seeks answers to these questions.,null,null",null,null
29,"28,405,null,null",null,null
30,"29,Session 4A: Evaluation 2,null,null",null,null
31,"30,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
32,"31,Figure 1: A screenshot of the search interface and the in situ judgments interface.,null,null",null,null
33,"32,· Can we e ectively predict di erent search result judgments using implicit feedback signals? Section 4 and Section 5 examine techniques for addressing this challenge.,null,null",null,null
34,"33,2 USER STUDY,null,null",null,null
35,"34,We designed a user study to collect search result judgments. e user study asked participants to work on di erent tasks in an experimental search system. We recorded users' search behavior and collected their in situ and post-session search result judgments.,null,null",null,null
36,"35,2.1 Experiment Design,null,null",null,null
37,"36,e user study employed a 2×2 within-subject design to balance di erent types of search tasks. e tasks come from the TREC session tracks [7] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classi cation framework [28]. e targeted task product is either factual (to locate facts) or intellectual (to enhance the user's understanding of a topic). e goal of a task is either speci c (clear and fully developed) or amorphous (an ill-de ned or unclear goal that may evolve along with the user's exploration).,null,null",null,null
38,"37,We divided participants into groups of four. Participants in the same group worked on the same four tasks (one task for each type) but using a di erent sequence (rotated using a Latin square). We assigned di erent tasks to di erent groups to increase task diversity.,null,null",null,null
39,"38,""For each task, the participants went through two stages: · Search stage (10 minutes). e participants performed an in-"",null,null",null,null
40,"39,""teractive search session to address the task. ey could submit and reformulate any queries and click on any search results. After clicking on a result's link, the participants switched to the result webpage in a new browser tab. When they had nished examining the result and turned back to the SERP, the participant needed to provide in situ judgments on the clicked results before they could resume the search session. Figure 1 shows the screenshots of the search interface and the in situ judgments. · Judgment stage (about 10 minutes). e participants rated their search experience in the session and nished post-session judgments on each result they visited in the session. Section 2.2 introduces details of the in situ and post-session judgments. As Figure 1 shows, the interface of the experimental system is similar to popular web search engines. e system redirected users' queries to Google and returned ltered Google search results. e system only showed the """"10-blue links"""", vertical search results"",null,null",null,null
41,"40,""(except image verticals), and related queries. Other SERP elements were removed to simplify the user study. e system displayed results in the same way they would appear on Google. e main di erence between our system and Google in SERP design was that our system showed task description on the top of a SERP (to help participants recall task requirements) and we showed related searches on the right side of a SERP."",null,null",null,null
42,"41,""e participants spent about 100 minutes to nish an experiment. First, they worked on a training task (including all the steps) for 10 minutes. en, they worked on four formal tasks, spending about 20 minutes on each task. We required the participants to take a 5-minute break a er two formal tasks to reduce fatigue."",null,null",null,null
43,"42,2.2 Collecting Search Result Judgments,null,null",null,null
44,"43,We collected search result judgments in two di erent scenarios: · In situ judgments ­ participants assessed a clicked result when,null,null",null,null
45,"44,they had nished examining it and turned back to the SERP. · Post-session judgments ­ the judgments collected a er a search,null,null",null,null
46,"45,session (in the judgment stage). e in situ judgments measure the participants' perceptions of,null,null",null,null
47,"46,""the clicked result at (roughly) the same time and contexts they visit the result. e approach is similar to Kim et al. [25], except that we adopted di erent measures to assess search results. In the search stage, we instructed the participants to examine results as they would normally do when using a search engine in their daily lives. For example, they did not need to fully read a result and they could abandon examining. Particularly, they were instructed that during the in situ judgments, they should not revisit the result for the purpose of answering the judgment questions (and we did not o er a link for revisiting in the in situ judgment interface). is is to ensure that the in situ judgments only measure participants' perceptions of the latest click activity."",null,null",null,null
48,"47,""e post-session judgments resemble the TREC-style relevance judgments, where the assessors judge results without a particular search context and in a random order--they are asked to judge a set of results one a er another in detail. In our post-session judgments, the assessors are real searchers. We asked them to judge the set of results they visited in the session. We instructed them to examine the results in a be er detail in the post-session judgments. e system also required participants to revisit each clicked result and spend at least 30 seconds to judge a result."",null,null",null,null
49,"48,406,null,null",null,null
50,"49,Session 4A: Evaluation 2,null,null",null,null
51,"50,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
52,"51,Table 1: estions for collecting search result judgments and users' search experience.,null,null",null,null
53,"52,Search Result Judgments,null,null",null,null
54,"53,Topical Relevance (TRel),null,null",null,null
55,"54,Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),null,null",null,null
56,"55,""estion & Options How relevant is this webpage? · Key (3): this page or site is dedicated to the topic; authoritative and comprehensive; it is worthy of being a top result. · Highly Relevant (2): the content of this page provides substantial information on the topic. · Relevant (1): the content of this page provides some information on the topic, which may be minimal. · Not Relevant or Spam (0). In Situ: How much useful information did you get from this web page? From 1 (none) to 7 (a lot of). Post-session: How much useful information did this web page provide for the task? From 1 (none) to 7 (a lot of). How much new information did you get from this web page? From 1 (none) to 7 (a lot of). How much e ort did you spend on this web page? From 1 (none) to 7 (a lot of). How di cult was it for you to follow the content of this web page? From 1 (very di cult) to 7 (very easy). How trustworthy is the information in this web page? From 1 (not at all trustworthy) to 7 (very trustworthy)."",null,null",null,null
57,"56,Search Experience Measures Satisfaction (Sat) Frustration (Frus) System Helpfulness (Help) Goal Success (Succ) Session E ort (S.Eff) Di culty (Diff),null,null",null,null
58,"57,estion & Options How satis ed were you with your search experience? From 1 (very unsatis ed) to 7 (very satis ed). How frustrated were you with this task? From 1 (not frustrated) to 7 (very frustrated). How well did the system help you in this task? From 1 (very badly) to 7 (very well). How well did you ful ll the goal of this task? From 1 (very badly) to 7 (very well). How much e ort did this task take? From 1 (minimum) to 7 (a lot of). How di cult was this task? From 1 (very easy) to 7 (very di cult).,null,null",null,null
59,"58,We collected users' in situ and post-session judgments of six different measures. Table 1 shows the detailed questions and options.,null,null",null,null
60,"59,""· TREC relevance (TRel) ­ the de facto standard of relevance judgments due to the popularity of TREC test collections. We collected TRel using the criteria of the latest TREC web track [10]. As Table 1 shows, the criteria focus on topical relevance. We excluded the relevance level Nav (the correct homepage of a navigational query) from the original TREC criteria because our search tasks do not include navigational search."",null,null",null,null
61,"60,""· Usefulness (Usef) ­ Following Belkin et al.'s model [5] and Mao et al.'s study [36], we collected users' perceptions regarding the usefulness of the clicked results."",null,null",null,null
62,"61,""· Novelty (Nov) ­ Novelty was o en assessed algorithmically in previous studies based on sub-topic or """"nugget"""" level relevance judgments [8, 40, 43, 55]. In contrast, we collect users' explicit novelty judgments."",null,null",null,null
63,"62,· Understandability (Under) ­ the easiness of understanding the content of the result. Recent studies incorporated understandability into search result ranking [41] and evaluation [56].,null,null",null,null
64,"63,""· Reliability (Relia) ­ the reliability, credibility, and trustworthy of the information presented in the result [39, 46, 51] (here we do not distinguish the three constructs)."",null,null",null,null
65,"64,· E ort ­ Yilmaz et al. [54] and Verma et al. [50] examined e ort as a dimension of evaluating search result.,null,null",null,null
66,"65,""e following table summarizes the measures collected in in situ and post-session judgments. We only collected TRel in post-session judgments because the TREC criteria do not consider context. We only collected Nov and Effort during in situ judgments because the participants of a pilot study reported confusions assessing the two measures twice. In the rest of this paper, we will use .i and .p su xes to denote in situ and post-session judgments, respectively. For example, Usef.i denotes users' in situ usefulness judgments."",null,null",null,null
67,"66,""Except for TRel, we collected judgments using a 7-point Likert scale, because a previous study [48] showed that assessors approximate the optimal level of con dence when using a 7-point scale for relevance judgments. TRel used a di erent scale so that it is"",null,null",null,null
68,"67,TREC relevance (TRel) Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),null,null",null,null
69,"68,In Situ (.i),null,null",null,null
70,"69,Post-session (.p),null,null",null,null
71,"70,consistent with the TREC web track (as a representative example of the state-of-the-art relevance judgment methods).,null,null",null,null
72,"71,2.3 Search Experience Measures,null,null",null,null
73,"72,""In the judgment stage, participants rated their search experience in a session. We collected six representative user experience measures used in previous studies of information retrieval and recommender systems--satisfaction (Sat) [17, 21, 26, 35, 36, 45], goal success (Succ) [1, 18], frustration (Frus) [12, 13], task di culty (Diff) [4, 15, 29, 31, 32], the helpfulness of the system (Help) [19] and the total e ort spent (S.Eff) [27]. Table 1 includes the questions."",null,null",null,null
74,"73,2.4 Rationale of Experiment Design,null,null",null,null
75,"74,""e way we balance di erent types of tasks is similar to previous studies [22, 24, 30, 33, 36]. However, we acknowledge that the selected tasks cannot cover all varieties. It is also worth noting that the TREC session track tasks [7] are more complex than regular web search requests such as navigational search."",null,null",null,null
76,"75,""Our study aims to collect both in situ judgments and user behaviors related to the clicked results. is poses challenges to the experiment design. On the one hand, we hope to collect accurate in situ judgments, which o en requires multi-item measurements [27, 52]. On the other hand, interrupting participants for in situ judgments breaks the ow of search session and can a ect their subsequent search behaviors. To balance between the two purposes, we made a few compromises in experiment design, e.g., we only collected six popular dimensions of judgments, and we simply used one question to measure each dimension."",null,null",null,null
77,"76,407,null,null",null,null
78,"77,Session 4A: Evaluation 2,null,null",null,null
79,"78,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
80,"79,Table 2: Spearman's correlation () matrix of di erent judgments for the 727 unique clicks.,null,null",null,null
81,"80,In Situ Judgments,null,null",null,null
82,"81,Post-session Judgments,null,null",null,null
83,"82,Usef.i Novelty Effort Under.i Relia.i TRel Usef.p Under.p,null,null",null,null
84,"83,Novelty,null,null",null,null
85,"84,0.67,null,null",null,null
86,"85,-,null,null",null,null
87,"86,-,null,null",null,null
88,"87,-,null,null",null,null
89,"88,-,null,null",null,null
90,"89,-,null,null",null,null
91,"90,-,null,null",null,null
92,"91,-,null,null",null,null
93,"92,In Situ,null,null",null,null
94,"93,E ort,null,null",null,null
95,"94,0.22,null,null",null,null
96,"95,Understandability,null,null",null,null
97,"96,0.20,null,null",null,null
98,"97,0.24,null,null",null,null
99,"98,-,null,null",null,null
100,"99,0.14 -0.45,null,null",null,null
101,"100,-,null,null",null,null
102,"101,-,null,null",null,null
103,"102,-,null,null",null,null
104,"103,-,null,null",null,null
105,"104,-,null,null",null,null
106,"105,-,null,null",null,null
107,"106,-,null,null",null,null
108,"107,Reliability,null,null",null,null
109,"108,0.42,null,null",null,null
110,"109,0.37,null,null",null,null
111,"110,0.05,null,null",null,null
112,"111,0.26,null,null",null,null
113,"112,-,null,null",null,null
114,"113,-,null,null",null,null
115,"114,-,null,null",null,null
116,"115,-,null,null",null,null
117,"116,Topical Relevance,null,null",null,null
118,"117,0.63,null,null",null,null
119,"118,0.46,null,null",null,null
120,"119,0.16,null,null",null,null
121,"120,0.14,null,null",null,null
122,"121,0.42,null,null",null,null
123,"122,-,null,null",null,null
124,"123,-,null,null",null,null
125,"124,-,null,null",null,null
126,"125,Post-session,null,null",null,null
127,"126,Usefulness Understandability,null,null",null,null
128,"127,0.72 0.20,null,null",null,null
129,"128,0.52,null,null",null,null
130,"129,0.16,null,null",null,null
131,"130,0.18 -0.36,null,null",null,null
132,"131,0.18 0.68,null,null",null,null
133,"132,0.43 0.83 0.29 0.18,null,null",null,null
134,"133,0.24,null,null",null,null
135,"134,-,null,null",null,null
136,"135,Reliability,null,null",null,null
137,"136,0.43,null,null",null,null
138,"137,0.38,null,null",null,null
139,"138,0.04,null,null",null,null
140,"139,0.22,null,null",null,null
141,"140,0.82 0.48,null,null",null,null
142,"141,0.51,null,null",null,null
143,"142,0.31,null,null",null,null
144,"143,e reported values are estimated from 1000 bootstrap samples (we used strati ed sampling to balance user and task dependency).,null,null",null,null
145,"144,""While examining search behaviors, we excluded the time spent on answering in situ judgments from dwell time. On average the participants spent 57.1 seconds on a clicked result and 12.1 seconds to answer the ve in situ judgment questions."",null,null",null,null
146,"145,2.5 Collected Data,null,null",null,null
147,"146,We recruited 28 participants (16 are female) through iers posted on the campuses of two universities in the United States. We required participants to be English native speakers to exclude the in uence of language uency on relevance judgments [16]. All the participants were undergraduate or graduate students studying di erent elds.,null,null",null,null
148,"147,""ey were reimbursed $15 per hour. We collected 112 sessions by 28 participants on 28 tasks. Each participant worked on four unique tasks and each task was performed by four unique users. In total, we collected 537 queries (4.8 per session) and 736 clicks (6.6 per session) on 727 unique sessionURL pairs (9 cases of revisiting). We exclude the 9 cases of revisiting from the analysis (about 1% of the data) to simply the analysis."",null,null",null,null
149,"148,3 IN SITU VS. POST-SESSION JUDGMENTS,null,null",null,null
150,"149,3.1 Correlation of Di erent Judgments,null,null",null,null
151,"150,""Table 2 reports the correlation of di erent judgments, which are generally consistent with previous studies. For example, relevance and usefulness positively correlate with novelty and reliability [52], understandability negatively correlates with e ort [50], etc. We examined the relationship of the judgments in another article [23]."",null,null",null,null
152,"151,""Note that Mao et al. [36] reported a weak correlation (0.332) of searchers' post-session usefulness judgments and external assessors' relevance judgments. However, Table 2 shows that TRel and Usef.p are strongly correlated ( ,"""" 0.83) when both of them are assessed by searchers. is suggests that the low correlation reported by Mao et al. [36] may be mostly due to the disparity between searchers and external assessors, rather than the di erence between using relevance or usefulness as the judgment criteria."""""",null,null",null,null
153,"152,3.2 Correlating with User Experience,null,null",null,null
154,"153,""We evaluate di erent search result judgments by correlating with (regressing) users' search experience measures in a session. is is based on the assumption that the """"quality"""" of the clicked results in a session can in uence users' search experience in that session--thus, a reasonable search result judgment (assumed to indicate certain """"quality""""), or a reasonable set of judgments, should also correlate with users' search experience in a session."",null,null",null,null
155,"154,3.2.1 Regression Analysis. We use multilevel regression analysis,null,null",null,null
156,"155,to examine the relationship between the judgments of the clicked,null,null",null,null
157,"156,results and users' search experience in a session. e dependent,null,null",null,null
158,"157,variables (DVs) are each of the six search experience measures. e,null,null",null,null
159,"158,independent variables (IVs) include the statistics of judgments re-,null,null",null,null
160,"159,""garding the clicked results in a session (such as the mean, maximum,"",null,null",null,null
161,"160,""and minimum ratings). For TRel, Usef.i, and Usef.p, we include"",null,null",null,null
162,"161,""the mean, maximum, and minimum ratings of the clicked results in"",null,null",null,null
163,"162,a session as IVs in the regression analysis. For other search result,null,null",null,null
164,"163,""judgments, we only include the maximum and minimum ratings of"",null,null",null,null
165,"164,the clicked results as IVs. is is because the mean ratings of the,null,null",null,null
166,"165,""other measures o en highly correlate with those of TRel and Usef,"",null,null",null,null
167,"166,causing multicollinearity issues for regression analysis.,null,null",null,null
168,"167,""For each user experience measure (the DV), we examine six"",null,null",null,null
169,"168,di erent models that include di erent judgments as IVs.,null,null",null,null
170,"169,· Unidimensional & Context-independent ­ Model 1 and 2,null,null",null,null
171,"170,only include context-independent search result judgments from,null,null",null,null
172,"171,a single dimension--Model 1 includes the statistics of TRel and,null,null",null,null
173,"172,Model 2 includes those of Usef.p.,null,null",null,null
174,"173,· Unidimensional & In Situ ­ Model 3 includes in situ judg-,null,null",null,null
175,"174,ments from a single dimension (the statistics of Usef.i) as IVs.,null,null",null,null
176,"175,· Multidimensional & Context-independent ­ Model 4 and,null,null",null,null
177,"176,5 extend Model 1 and 2 to include other dimensions of judg-,null,null",null,null
178,"177,""ments (the statistics of Under.p, Relia.p, Nov, and Effort)."",null,null",null,null
179,"178,Note that Model 4 and 5 include two in situ judgments (Nov,null,null",null,null
180,"179,and Effort) because we did not collect post-session judgments,null,null",null,null
181,"180,on these two dimensions (as discussed in § 2.2).,null,null",null,null
182,"181,· Multidimensional & In Situ ­ Model 6 extends Model 3 to,null,null",null,null
183,"182,""include other dimensions of judgments (the statistics of Under.i,"",null,null",null,null
184,"183,""Relia.i, Nov, and Effort)."",null,null",null,null
185,"184,Contextindependent,null,null",null,null
186,"185,In Situ,null,null",null,null
187,"186,Unidimensional,null,null",null,null
188,"187,1 TRel only 2 Usef.p only,null,null",null,null
189,"188,3 Usef.i only,null,null",null,null
190,"189,Multidimensional,null,null",null,null
191,"190,4 TRel + others 5 Usef.p + others,null,null",null,null
192,"191,6 Usef + others,null,null",null,null
193,"192,""All six models also include the same set of control variables, including: gender (Male or Female), age (four levels; 0 for 18­24, 1 for 25­30, 2 for 31­40, and 3 for Over 40), highest degree obtained or expected (Undergraduate or Graduate), the expertise of using web search engines (SE Expertise) rated using a Likert scale from 1 (very badly) to 5 (very well), task product and goal, user's familiarity with the topic of the task (Topic Familiarity) rated using a Likert scale from 1 (very unfamiliar) to 7 (very familiar), and the number of clicks (# clicks) and queries (# queries) in the session."",null,null",null,null
194,"193,408,null,null",null,null
195,"194,Session 4A: Evaluation 2,null,null",null,null
196,"195,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
197,"196,Table 3: e adjusted R2 of di erent regression models.,null,null",null,null
198,"197,Models,null,null",null,null
199,"198,Sat Frus Succ S.Eff Help,null,null",null,null
200,"199,Base (control only) 0.12 0.06 0.11 0.06 0.11,null,null",null,null
201,"200,1 TRel,null,null",null,null
202,"201,0.23 0.09 0.18 0.10 0.16,null,null",null,null
203,"202,2 Usef.p,null,null",null,null
204,"203,0.25 0.15 0.36 0.17 0.18,null,null",null,null
205,"204,3 Usef.i,null,null",null,null
206,"205,0.29 0.14 0.35 0.16 0.22,null,null",null,null
207,"206,4 TRel + others 0.31 0.25 0.33 0.33 0.31,null,null",null,null
208,"207,4 vs. 1,null,null",null,null
209,"208,**,null,null",null,null
210,"209,**,null,null",null,null
211,"210,**,null,null",null,null
212,"211,**,null,null",null,null
213,"212,**,null,null",null,null
214,"213,5 Usef.p + others 0.30 0.26 0.42 0.37 0.31,null,null",null,null
215,"214,5 vs. 2,null,null",null,null
216,"215,**,null,null",null,null
217,"216,**,null,null",null,null
218,"217,**,null,null",null,null
219,"218,**,null,null",null,null
220,"219,**,null,null",null,null
221,"220,6 Usef.i + others 0.30 0.27 0.34 0.37 0.27,null,null",null,null
222,"221,6 vs. 3,null,null",null,null
223,"222,**,null,null",null,null
224,"223,**,null,null",null,null
225,"224,*,null,null",null,null
226,"225,* and ** indicate p < 0.05 and p < 0.01 by F-test.,null,null",null,null
227,"226,Diff,null,null",null,null
228,"227,0.03 0.09 0.18 0.22 0.21,null,null",null,null
229,"228,** 0.26,null,null",null,null
230,"229,** 0.33,null,null",null,null
231,"230,**,null,null",null,null
232,"231,We examine multicollinearity between variables using variance,null,null",null,null
233,"232,""in ation factor (VIF). e IVs of all models satisfy VIF < 4, the com-"",null,null",null,null
234,"233,monly suggested threshold (4­10) for concerns of multicollinearity issues [37]. Table 3 reports the adjusted R2 of the six models for,null,null",null,null
235,"234,regressing the six dimensions of search experience.,null,null",null,null
236,"235,""3.2.2 TREC Relevance vs. Usefulness. We rst compare TREC relevance criteria (TRel) and post-session usefulness judgments (Usef.p). is is a revisit of Mao et al.'s study [36], which compared searchers' usefulness judgments and external assessors' relevance judgments. Here we collected both judgments from real searchers, removing the in uence caused by the di erence between searchers and external annotators in relevance judgments. e regression analysis suggest that switching from TREC relevance to usefulness is fruitful, consistently enhancing the ability of the regression models to correlate with user experience (by adjusted R2)."",null,null",null,null
237,"236,""Models 1 and 2 include the mean, maximum, and minimum TRel or Usef.p ratings of the clicked results. Model 2 consistently explains the six search experience measures be er than Model 1 (by adjusted R2). We note that usefulness (Usef.p) seems to be particularly be er than TREC relevance (TRel) in terms of correlating with goal success (Succ), with adjusted R2 , 0.36 vs 0.18."",null,null",null,null
238,"237,""Models 4 and 5 further include other dimensions of judgments as IVs. is helps compare TRel and Usef.p judgments with other search result judgments as controls. Still, we consistently observe that Model 5 explains the six search experience measures be er than or as well as model 4 . ese results verify that usefulness is indeed a be er criteria of relevance judgments than TREC-style relevance (in terms of correlating with users' search experience)."",null,null",null,null
239,"238,""3.2.3 In Situ vs. Context-independent (Post-session) Judgments. We further compare in situ and post-session judgments in both unidimensional and multidimensional se ings. Results suggest in situ usefulness judgments have be er correlations with a few (but not all) user experience measures than post-session usefulness judgments. However, a er combining search result judgments from di erent dimensions, in situ judgments show limited advantages over post-session ones."",null,null",null,null
240,"239,""Models 3 and 2 include the mean, maximum, and minimum Usef.i or Usef.p ratings of the clicked results as IVs. Results show Model 3 explains satisfaction (Sat), helpfulness (Help), and task di culty (Di ) slightly be er than Model 2 , with about 0.04 di erence in adjusted R2."",null,null",null,null
241,"240,""We further compare in situ and post-session judgments in a multidimensional se ing, using a combination of Usef.p/Usef.i"",null,null",null,null
242,"241,""and other four judgments as IVs (Models 5 and 6 ). Results show that the post-session multidimensional model ( 5 ) be er correlates with search success (Succ) than the in situ one (adjusted R2 0.42 vs 0.34), but the la er also be er correlates with task di culty (adjusted R2 0.26 vs. 0.21). Overall, no evidence suggests either model is consistently be er than another in terms of correlating with users' search experience measures."",null,null",null,null
243,"242,""Even though Model 3 (Usef.i only) performs slightly be er than Model 2 (Usef.p only), results suggest limited advantages of in situ judgments over post-session ones in terms of correlating with search experience measures. We suspect a possible reason is that a 10-minute session is not long enough to trigger su cient di erences between in situ and post-session judgments. Although we expect to observe a greater di erence between in situ and post-session judgments in longer sessions, we believe a substantial proportion of web search sessions are no longer than 10 minutes, which may not bene t much from in situ judgments. In addition, it also requires a more complex experiment design to collect in situ judgments."",null,null",null,null
244,"243,""3.2.4 Unidimensional vs. Multidimensional Judgments. We further compare models using a combination of multiple aspects of judgments (Models 4 , 5 , and 6 ) with those using a single dimension (Models 1 , 2 , and 3 ). Results suggest that it is almost always helpful (enhancing the correlation with most of the six search experience measures signi cantly) to complement either relevance or usefulness with the alternative dimensions."",null,null",null,null
245,"244,""Models 4 and 5 explain all six dimensions of search experience measures signi cantly be er than Models 1 and 2 , suggesting that multidimensional judgments are almost always helpful for TREC-style relevance judgments (TRel) and post-session usefulness judgments (Usef.p). We also note that in situ usefulness judgments (Usef.i) worked particularly well for correlating with users' satisfaction (Sat) and goal success (Succ), such that combining with more dimensions of judgments adds li le to the model."",null,null",null,null
246,"245,""Results demonstrate that multidimensional search result judgments are helpful, complementing unidimensional judgments and yielding be er correlation with search experience measures. is also suggests the advantages of multidimensional search result judgments over the in situ one--the former can consistently improve relevance/usefulness to be er correlate with almost all user experience measures, while the la er shows limited advantages."",null,null",null,null
247,"246,3.3 Which Dimensions To Judge?,null,null",null,null
248,"247,""A crucial issue of information retrieval is deciding which criteria to use to rank search results. We come to initial answers by looking into the standardized coe cients () of Model 5 (Table 4) as an example due to its superiority over other models. e standardized coe cient  stands for the magnitude of change in the DV (relative to its standard deviation) caused by one-unit change in the IV (relative to the IV's standard deviation) while other variables being equal. e coe cients of the model indicate how changes in the """"quality"""" of the clicked results will (theoretically) a ect users' search experience in a session. Table 4 suggests that: · To enhance user satisfaction, a search system should present"",null,null",null,null
249,"248,useful and novel results--both Usef.p (mean) and Nov (max) show signi cant positive e ects on Sat in Model 5 .,null,null",null,null
250,"249,409,null,null",null,null
251,"250,Session 4A: Evaluation 2,null,null",null,null
252,"251,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
253,"252,Table 4: Multilevel regression: standardized coe cients () of independent variables for Model 5 ­ Usef.p + others.,null,null",null,null
254,"253,Independent,null,null",null,null
255,"254,DV: session-level search experience,null,null",null,null
256,"255,Variables,null,null",null,null
257,"256,Sat Frus Succ S.Eff Help Diff,null,null",null,null
258,"257,Gender: Male Age Degree: Graduate SE Expertise,null,null",null,null
259,"258,0.10 -0.05 -0.03,null,null",null,null
260,"259,0.12,null,null",null,null
261,"260,0.19 0.09 0.06 0.00 0.16 0.00 -0.03 -0.02 -0.11 0.00 0.05 0.01 0.13 -0.08 0.23 0.04 0.08 0.01 0.12 -0.00,null,null",null,null
262,"261,Product: Factual Goal: Speci c Topic Familiarity,null,null",null,null
263,"262,0.02 -0.02 -0.06 -0.09 -0.00 0.02 0.02 -0.07 0.04 0.05 0.07 0.01 0.10 -0.23 0.17 -0.20 0.19 -0.24,null,null",null,null
264,"263,# clicks,null,null",null,null
265,"264,0.20 -0.12 0.17 -0.07 0.18 -0.01,null,null",null,null
266,"265,# queries,null,null",null,null
267,"266,-0.36 0.17 -0.25 0.16 -0.35 -0.02,null,null",null,null
268,"267, Usef.p (mean) 0.23 -0.38 0.36 -0.36 0.08 -0.43,null,null",null,null
269,"268, Usef.p (max)  Usef.p (min),null,null",null,null
270,"269,0.16 0.09 0.22 -0.07 0.11 -0.08 0.01 0.19 -0.04 0.19 0.01 0.18,null,null",null,null
271,"270, Nov (max)  Nov (min),null,null",null,null
272,"271,0.24 -0.10 0.18 -0.09 -0.01 -0.20 -0.08 -0.06,null,null",null,null
273,"272,0.25 -0.11 0.07 -0.00,null,null",null,null
274,"273, Under.p (max)  Under.p (min),null,null",null,null
275,"274,0.09 -0.27 0.16 -0.08,null,null",null,null
276,"275,0.30 -0.15 0.14 -0.26,null,null",null,null
277,"276,0.14 -0.22 0.29 -0.27,null,null",null,null
278,"277, Relia.p (max) -0.13 -0.08 0.01 0.05 -0.03 0.06,null,null",null,null
279,"278, Relia.p (min) 0.06 0.01 -0.05 0.08 -0.07 0.04,null,null",null,null
280,"279, Effort (max) -0.12 0.16 0.08 0.28 -0.13 0.02,null,null",null,null
281,"280, Effort (min) Adjusted R2,null,null",null,null
282,"281,0.21 0.04 0.12 0.01 0.25 0.02 0.30 0.26 0.42 0.37 0.31 0.26,null,null",null,null
283,"282,""Light and dark shadings indicate p < 0.05 and 0.01, respectively."",null,null",null,null
284,"283,""· To reduce user frustration, a search system should o er results that are useful and easy-to-understand--both Usef.p (mean) and Under.p (max) show signi cant negative e ects on Frus."",null,null",null,null
285,"284,""· To help users successfully reach the goal (Succ), a search system should retrieve useful, novel, and easy-to-understand results-- Usef.p (mean), Nov (max), and Under.p (max) show signi cant positive e ects on Succ."",null,null",null,null
286,"285,""· To reduce the total e ort of a search session, the system should retrieve easy-to-understand results and avoid those requiring too much e ort--Under.p (min) shows a signi cant negative e ect on S.Eff and Effort (max) shows a positive one."",null,null",null,null
287,"286,""· To be er help users in a session (enhance the helpfulness of the system), a system should retrieve novel and easy-to-understand results--both Nov (max) and Under.p (max) show signi cant positive e ects on Help."",null,null",null,null
288,"287,""· To reduce the perceived task di culty, we need to retrieve useful and easy-to-understand results--both Usef.p (mean) and Under.p (min) show signi cant negative e ects on Diff. e coe cients suggest that the mean usefulness of the clicked"",null,null",null,null
289,"288,""results is helpful for explaining all six search experience measures (has statistically signi cant coe cients). In addition, novelty, understandability, and e ort also signi cantly relate to many di erent search experience measures, suggesting they are useful complements to usefulness in search result judgments. In contrast, reliability shows no signi cant e ect on any of the six user experience measures in Model 5 . However, we suspect this is because the top-ranked results returned by Google are mostly reliable ones, which makes reliability a less important judgment measure among the clicked results."",null,null",null,null
290,"289,Table 5: Statistics of the absolute di erence of two users' ratings on the same results (||).,null,null",null,null
291,"290,Usef.i Effort Nov Relia.i Under.i TRel Usef.p Relia.p Under.p,null,null",null,null
292,"291,| | mean (SD) 1.55 (1.45) 1.52 (1.25) 1.60 (1.47) 1.23 (1.21) 1.18 (1.23) 0.63 (0.68) 1.53 (1.54) 1.38 (1.35) 1.08 (1.31),null,null",null,null
293,"292,""|| , 0 25.9% 22.9% 26.4% 31.3% 34.8% 48.3% 29.9% 30.8% 38.8%"",null,null",null,null
294,"293,||  1 58.2% 57.7% 54.2% 67.2% 68.2% 89.1% 60.7% 62.2% 76.6%,null,null",null,null
295,"294,||  2 79.1% 76.1% 78.1% 86.1% 88.1% 100.0% 77.6% 81.1% 90.5%,null,null",null,null
296,"295,3.4 Variability of Judgments,null,null",null,null
297,"296,""We further examine the variability of judgments among di erent searchers, because in many practical scenarios we may have to train and evaluate retrieval systems based on relevance judgments made by external assessors. We suspect di erent users may have a greater degree of inconsistencies in their in situ judgments than their post-session ones (due to the contextual nature of the former). However, results do not support this conjecture well."",null,null",null,null
298,"297,""We examine the absolute di erence of two users' ratings on the same result. Table 5 reports the mean absolute di erence and the distribution. e mean absolute di erence for in situ and postsession usefulness judgments (Usef.i and Usef.p) are very close (1.55 vs. 1.53). e mean absolute di erence of post-session reliability judgments (Relia.p) is slightly higher than that for in situ ones (Relia.i) (1.38 vs. 1.23), but that for post-session understandability judgments (Under.p) is also slightly lower than the in situ ones (Under.i, 1.08 vs. 1.18). Overall, no evidence suggests that either in situ or post-session judgments is more or less consistent than the other across di erent users."",null,null",null,null
299,"298,""Further, we note that di erent users' reliability and understandability judgments seem more consistent than those for usefulness, e ort, and novelty judgments, regardless of performed in an in situ se ing or a post-session one. is suggests that usefulness, e ort, and novelty judgments may su er from inter-rate consistency by a greater extent, while inter-rate agreement is less likely a concern for understandability and reliability judgments. However, since users judged TRel by a di erent scale, it remains unclear how do the other ve judgments compare with standard TREC relevance judgments in terms of inter-rate consistency."",null,null",null,null
300,"299,3.5 Summary,null,null",null,null
301,"300,""To sum up, this section discloses both opportunities and challenges for future search result judgments. · Opportunity ­ Since a combination of multidimensional judg-"",null,null",null,null
302,"301,""ments explains user experience measures be er than using relevance or usefulness alone, we expect that an appropriate ranking of search results by multiple criteria may potentially yield be er user experience as well. e results in Table 4 also help select ranking criteria according to a targeted user experience measure. · Challenge ­ Extending current judgments from a single dimension to multiple aspects largely increases the cost of judgments."",null,null",null,null
303,"302,is is a crucial issue for the scalability of multidimensional judgments. e following sections address this concern by predicting multidimensional judgments using implicit feedback techniques.,null,null",null,null
304,"303,410,null,null",null,null
305,"304,Session 4A: Evaluation 2,null,null",null,null
306,"305,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
307,"306,Table 6: Implicit feedback features and their correlation with di erent search result quality measures.,null,null",null,null
308,"307,Pearson's r with search result judgments,null,null",null,null
309,"308,Click Dwell Time Features,null,null",null,null
310,"309,Note,null,null",null,null
311,"310,TRel Usef.p Nov Effort Under.p Relia.p,null,null",null,null
312,"311,T1 Click dwell time (log).,null,null",null,null
313,"312,0.38,null,null",null,null
314,"313,0.43 0.41,null,null",null,null
315,"314,0.36,null,null",null,null
316,"315,0.12,null,null",null,null
317,"316,0.34,null,null",null,null
318,"317,T2 T3 T4 T5,null,null",null,null
319,"318,(t - µ )/ . t is the result's dwell time; µ is average click dwell time;  is the standard deviation of click dwell time. T3-5 are based on personalized versions of µ and  .,null,null",null,null
320,"319,all clicks by user by task by length,null,null",null,null
321,"320,0.31,null,null",null,null
322,"321,0.34 0.30,null,null",null,null
323,"322,0.32,null,null",null,null
324,"323,0.31,null,null",null,null
325,"324,0.36 0.38,null,null",null,null
326,"325,0.32,null,null",null,null
327,"326,0.31,null,null",null,null
328,"327,0.35 0.30,null,null",null,null
329,"328,0.32,null,null",null,null
330,"329,0.29,null,null",null,null
331,"330,0.33 0.29,null,null",null,null
332,"331,0.31,null,null",null,null
333,"332,0.06 0.09 0.06 0.06,null,null",null,null
334,"333,0.24 0.24 0.24 0.24,null,null",null,null
335,"334,Follow-up ery Features,null,null",null,null
336,"335,Q1,null,null",null,null
337,"336,Q2,null,null",null,null
338,"337,e number of terms in the next query found in the URL/title/body,null,null",null,null
339,"338,Q3 of the result.,null,null",null,null
340,"339,URL title body,null,null",null,null
341,"340,TRel,null,null",null,null
342,"341,-0.04 -0.03 -0.03,null,null",null,null
343,"342,Usef.p,null,null",null,null
344,"343,0.03 -0.00 -0.03,null,null",null,null
345,"344,Nov,null,null",null,null
346,"345,-0.01 -0.00,null,null",null,null
347,"346,0.10,null,null",null,null
348,"347,Effort,null,null",null,null
349,"348,-0.02 0.01 0.06,null,null",null,null
350,"349,Under.p,null,null",null,null
351,"350,-0.02 -0.00,null,null",null,null
352,"351,0.03,null,null",null,null
353,"352,Relia.p,null,null",null,null
354,"353,0.03 -0.02 -0.02,null,null",null,null
355,"354,Q4,null,null",null,null
356,"355,e percentage of terms in the next query found in the,null,null",null,null
357,"356,Q5 URL/title/body of the result.,null,null",null,null
358,"357,Q6,null,null",null,null
359,"358,URL title body,null,null",null,null
360,"359,0.02 0.09 0.02 -0.04,null,null",null,null
361,"360,0.01,null,null",null,null
362,"361,0.08,null,null",null,null
363,"362,0.07 0.10 0.06 -0.00,null,null",null,null
364,"363,0.05,null,null",null,null
365,"364,0.07,null,null",null,null
366,"365,0.17,null,null",null,null
367,"366,0.18 0.21,null,null",null,null
368,"367,0.04,null,null",null,null
369,"368,0.13,null,null",null,null
370,"369,0.18,null,null",null,null
371,"370,Q7,null,null",null,null
372,"371,e number of newly added query terms in the next query refor-,null,null",null,null
373,"372,Q8 mulation found in the URL/title/body of the result.,null,null",null,null
374,"373,Q9,null,null",null,null
375,"374,URL title body,null,null",null,null
376,"375,0.03 -0.01 -0.03 -0.06 0.07 0.04 0.00 -0.07 0.07 0.07 0.13 -0.04,null,null",null,null
377,"376,0.02,null,null",null,null
378,"377,0.02,null,null",null,null
379,"378,0.01 -0.00,null,null",null,null
380,"379,0.04 -0.02,null,null",null,null
381,"380,Q10,null,null",null,null
382,"381,e number of removed query terms in the next query reformula-,null,null",null,null
383,"382,Q11 tion found in the URL/title/body of the result.,null,null",null,null
384,"383,Q12,null,null",null,null
385,"384,URL title body,null,null",null,null
386,"385,0.01 0.01 -0.00 -0.07 -0.04 -0.12,null,null",null,null
387,"386,0.06 0.09 0.06 -0.09,null,null",null,null
388,"387,0.03 -0.06,null,null",null,null
389,"388,0.08,null,null",null,null
390,"389,0.07 0.06,null,null",null,null
391,"390,0.01,null,null",null,null
392,"391,-0.09,null,null",null,null
393,"392,-0.04,null,null",null,null
394,"393,Q13,null,null",null,null
395,"394,e mean/max/min log likelihood scores between the full content,null,null",null,null
396,"395,Q14 of the result and follow-up queries.,null,null",null,null
397,"396,Q15,null,null",null,null
398,"397,mean max min,null,null",null,null
399,"398,0.22 0.23 0.22 -0.03,null,null",null,null
400,"399,0.19,null,null",null,null
401,"400,0.23,null,null",null,null
402,"401,0.22 0.23 0.21 -0.03,null,null",null,null
403,"402,0.17,null,null",null,null
404,"403,0.18,null,null",null,null
405,"404,0.15 0.19 0.19 -0.01,null,null",null,null
406,"405,0.17,null,null",null,null
407,"406,0.19,null,null",null,null
408,"407,Follow-up Click Features,null,null",null,null
409,"408,TRel Usef.p Nov Effort Under.p Relia.p,null,null",null,null
410,"409,C1,null,null",null,null
411,"410,e mean/max/min similarity between the title of the result and,null,null",null,null
412,"411,C2 the titles of clicked results in follow-up searches.,null,null",null,null
413,"412,C3,null,null",null,null
414,"413,mean max min,null,null",null,null
415,"414,0.04,null,null",null,null
416,"415,0.05 0.12,null,null",null,null
417,"416,0.02,null,null",null,null
418,"417,0.04,null,null",null,null
419,"418,0.01,null,null",null,null
420,"419,0.05,null,null",null,null
421,"420,0.04 0.09,null,null",null,null
422,"421,0.00,null,null",null,null
423,"422,0.06,null,null",null,null
424,"423,0.00,null,null",null,null
425,"424,0.06,null,null",null,null
426,"425,0.08 0.10,null,null",null,null
427,"426,0.01,null,null",null,null
428,"427,-0.01,null,null",null,null
429,"428,0.03,null,null",null,null
430,"429,C4,null,null",null,null
431,"430,e mean/max/min similarity between the snippet of the result,null,null",null,null
432,"431,C5 and the snippets of clicked results in follow-up searches.,null,null",null,null
433,"432,C6,null,null",null,null
434,"433,mean max min,null,null",null,null
435,"434,-0.00 -0.04,null,null",null,null
436,"435,0.08,null,null",null,null
437,"436,0.01 -0.06,null,null",null,null
438,"437,0.11,null,null",null,null
439,"438,0.01 -0.06,null,null",null,null
440,"439,0.10,null,null",null,null
441,"440,0.04 -0.05,null,null",null,null
442,"441,0.07,null,null",null,null
443,"442,0.02,null,null",null,null
444,"443,0.03,null,null",null,null
445,"444,0.01 -0.04,null,null",null,null
446,"445,0.06,null,null",null,null
447,"446,0.09,null,null",null,null
448,"447,C7,null,null",null,null
449,"448,e mean/max/min similarity between the full content of the result,null,null",null,null
450,"449,C8 and the full contents of SAT clicks (dwell time > 30s) in follow-up,null,null",null,null
451,"450,C9 searches.,null,null",null,null
452,"451,C10,null,null",null,null
453,"452,e mean/max/min similarity between the title of the result and,null,null",null,null
454,"453,C11 the titles of skipped results in follow-up searches.,null,null",null,null
455,"454,C12,null,null",null,null
456,"455,mean max min mean max min,null,null",null,null
457,"456,0.19,null,null",null,null
458,"457,0.23 0.09,null,null",null,null
459,"458,0.01,null,null",null,null
460,"459,-0.02,null,null",null,null
461,"460,0.10,null,null",null,null
462,"461,0.20 0.20 0.05 -0.00,null,null",null,null
463,"462,0.00,null,null",null,null
464,"463,0.08,null,null",null,null
465,"464,0.12 0.17 0.07 -0.00 -0.01,null,null",null,null
466,"465,0.07,null,null",null,null
467,"466,0.09,null,null",null,null
468,"467,0.11 0.11,null,null",null,null
469,"468,0.02,null,null",null,null
470,"469,0.05,null,null",null,null
471,"470,0.05,null,null",null,null
472,"471,0.11 0.09 0.06 -0.01,null,null",null,null
473,"472,0.05,null,null",null,null
474,"473,0.02,null,null",null,null
475,"474,0.09 0.13 0.13 -0.05,null,null",null,null
476,"475,0.00,null,null",null,null
477,"476,0.05,null,null",null,null
478,"477,C13,null,null",null,null
479,"478,e mean/max/min similarity between the snippet of the result,null,null",null,null
480,"479,C14 and the snippets of skipped results in follow-up searches.,null,null",null,null
481,"480,C15,null,null",null,null
482,"481,mean max min,null,null",null,null
483,"482,0.01,null,null",null,null
484,"483,0.03 0.03,null,null",null,null
485,"484,0.11,null,null",null,null
486,"485,-0.05,null,null",null,null
487,"486,0.03,null,null",null,null
488,"487,0.02 0.01 -0.01 -0.00,null,null",null,null
489,"488,0.02 -0.02,null,null",null,null
490,"489,0.10,null,null",null,null
491,"490,0.12 0.12,null,null",null,null
492,"491,0.13,null,null",null,null
493,"492,-0.05,null,null",null,null
494,"493,0.11,null,null",null,null
495,"494,""Light and dark shadings indicate the correlation is signi cant at 0.05 and 0.01 levels, respectively."",null,null",null,null
496,"495,4 PREDICTION,null,null",null,null
497,"496,""is section introduces our techniques for predicting multidimensional judgments of clicked results from search logs. We model the prediction task as a regression problem--the input is features related to a target click, the output is the predicted judgment score of the clicked result. We use gradient boosted regression trees (GBRT) for prediction. Table 6 lists the prediction features. Due to the limited space, we only report results for predicting TRel and the judgments included in Model 5 --Usef.p, Nov, Effort, Under.p, and Relia.p. However, the described approach can also e ectively predict other search result judgments as well."",null,null",null,null
498,"497,4.1 Click Dwell Time Features,null,null",null,null
499,"498,""Click dwell time (T1) is one of the most widely used implicit feedback measure. As Table 6 shows, T1 does not correlate much with understandability, but it still has 0.3­0.4 correlations (signi cant at 0.01 level) with other measures."",null,null",null,null
500,"499,T2­T5 measure the deviation of a click's dwell time from the mean dwell time (µ) of a group of clicks (normalized by the standard deviation  ). T2 computes µ and  based on all clicks in the training sets. T3 is based on clicks by the same user. T4 is based on clicks in sessions with the same task type. T5 is based on clicks on documents with similar length (we divide the clicked results into ten bins by length and compute µ and  of a click based on its bin).,null,null",null,null
501,"500,4.2 Follow-up ery Features,null,null",null,null
502,"501,Follow-up query features are based on the intuition that a clicked result may in uence follow-up query reformulation in a session.,null,null",null,null
503,"502,""us, we can infer the quality of a click from queries issued a er the clicked result in the same session."",null,null",null,null
504,"503,Q1­Q6 match the terms in the immediate follow-up query with the target click. Q7­Q12 match the newly added and removed terms in the immediate follow-up query reformulation with the target click. Q13­Q15 match the target click with all follow-up queries.,null,null",null,null
505,"504,411,null,null",null,null
506,"505,Session 4A: Evaluation 2,null,null",null,null
507,"506,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
508,"507,""Many of the follow-up query features (such as Q6 and Q13­Q15) have signi cant correlations with the search result quality measures, con rming that the intuition is reasonable. We also note that Q6 and Q13­Q15 have stronger correlations with understandability than click dwell time features."",null,null",null,null
509,"508,4.3 Follow-up Click Features,null,null",null,null
510,"509,""Similar to follow-up query features, we may also infer the quality of a target click based on follow-up clicks in a session."",null,null",null,null
511,"510,""C1­C6 measure the similarity between the target click and followup clicks. C7­C9 measure the similarity with follow-up satisfactory (SAT) clicks. C10­C15 measure the similarity with follow-up skipped results (unclicked results ranked higher than a clicked result). Some features have signi cant correlations with the search result quality measures, suggesting they may be useful predictors."",null,null",null,null
512,"511,4.4 Prior-to-click Features (Baseline),null,null",null,null
513,"512,""Prior-to-click features include the existing techniques that predict search result quality measures using information available before users clicking on the result. In this paper, they serve as the baseline for the implicit feedback features. We include a full list of prior-toclick features in an online appendix1."",null,null",null,null
514,"513,""We incorporate di erent prior-to-click features for predicting di erent measures. e shared features for all six measures include the rank of the result by Google search, ad hoc search models (QL, BM25, DFR [3], and SDM [38]), and session search models [14, 47]."",null,null",null,null
515,"514,e unique features for predicting each measure are: · TRel ­ a subset of LETOR features [34]. · Usef.p ­ a subset of LETOR features [34] and a subset of the,null,null",null,null
516,"515,""usefulness features by Mao et al. [36] that do not rely on postclick information. · Nov ­ the similarity of the click with previous clicks and higher ranked results in the same SERP (motivated by previous work on novelty-based search result diversi cation [6, 43, 44, 55]). · Effort ­ Yilmaz et al. [54] and Verma et al. [50]. · Under.p ­ Palo i et al. [41, 42]. · Relia.p ­ Olteanu et al. [39] and Wawer et al. [51]. Our prior-to-click features are representatives of the state-of-theart techniques for predicting each dimension of judgments without using implicit feedback. However, we did not include features that we do not have the resource to calculate, which include link structure based features and social media popularity features such as Twi er mention. Note this may reduce the e ectiveness of predicting reliability since the excluded features take about 1/3 of the features by Olteanu et al. [39] and Wawer et al. [51]."",null,null",null,null
517,"516,5 EVALUATION,null,null",null,null
518,"517,5.1 Experiment Settings,null,null",null,null
519,"518,""We evaluate prediction (regression) by the Pearson's correlation between the predicted values and actual judgments (prediction correlation) and the root mean square error (RMSE) of the predicted values. Note that the RMSE for predicting di erent measures is not comparable-- rst, TREC relevance ranges from 0­3 while others from 1­7; second, their distributions vary a lot. Here we only report"",null,null",null,null
520,"519,1 h p://ciir.cs.umass.edu/downloads/mdrel/,null,null",null,null
521,"520,prediction correlation for its easy interpretability. e results of RMSE is highly consistent with that using prediction correlation.,null,null",null,null
522,"521,""e dataset for evaluation includes multidimensional judgments on the 727 unique clicked results. We use 10-fold cross validation for evaluation (using eight folds for training, one for validation, and one for testing). We randomly shu e the dataset 10 times and apply 10-fold cross-validation for each random shu ing of the whole dataset--this generates prediction results on 10 × 10 ,"""" 100 test folds in total (note that we are not using a 100-fold cross validation). We report the mean and standard deviation (SD) of prediction correlation on the test folds. We note that the prediction correlation reported in this section is di erent from and cannot be compared with the correlation in Table 6, which are computed for the whole dataset without cross validation."""""",null,null",null,null
523,"522,5.2 Click Dwell Time Features,null,null",null,null
524,"523,""Current techniques for inferring search result quality from logs rely on click dwell time. Results ( 1 in Table 7) suggest the click dwell time features work reasonably well for predicting usefulness, novelty, and e ort, but they have di culties inferring the understandability and reliability of results."",null,null",null,null
525,"524,""e click dwell time features ( 1 ) are e ective predictors for usefulness, novelty, and e ort. For these three measures, the predicted values have about 0.3­0.4 mean Pearson's correlation with the actual judgments, which is comparable to that for predicting TREC relevance (mean r ,"""" 0.35). However, the click dwell time features perform much worse for predicting understandability and reliability. On average the predicted and actual judgments have only 0.10 and 0.22 correlation, suggesting it is necessary to incorporate new implicit feedback signals."""""",null,null",null,null
526,"525,5.3 Follow-up ery and Click Features,null,null",null,null
527,"526,We extend click dwell time to include signals from follow-up search activities. Results suggest the new features are helpful.,null,null",null,null
528,"527,""e follow-up query ( 2 ) and click features ( 3 ) alone have limited prediction capability. However, combining them with the click dwell time features ( 4 ) consistently produces be er prediction than using click dwell time features alone ( 1 ): except for e ort, the prediction correlation for the other ve measures using feature set 4 is signi cantly be er than that for click dwell time features ( 1 ). is indicates that follow-up queries and clicks indeed provide useful implicit feedback that are complementary to click dwell time."",null,null",null,null
529,"528,""e follow-up query and click features are particularly helpful for predicting reliability. Combining them with the click dwell time features improves the mean correlation of prediction from 0.22 to 0.36. e new features are also helpful for predicting TREC relevance and usefulness as well. is partly con rms our intuition--the quality of a clicked result may in uence follow-up search activities, making it possible to infer the quality of a clicked result based on what happened a erward in the session."",null,null",null,null
530,"529,""e new features also improved the mean prediction correlation for understandability from 0.10 to 0.20. However, we note the combination of all implicit feedback features still does not work well for predicting understandability (mean r ,"""" 0.20). is suggests that, compared with other judgments, it is more challenging to predict understandability based on the implicit feedback information."""""",null,null",null,null
531,"530,412,null,null",null,null
532,"531,Session 4A: Evaluation 2,null,null",null,null
533,"532,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
534,"533,Table 7: e e ectiveness of di erent features for predicting multidimensional search result judgments.,null,null",null,null
535,"534,Mean (SD) Pearson's r between true and predicted judgments over the test folds,null,null",null,null
536,"535,Features,null,null",null,null
537,"536,TRel,null,null",null,null
538,"537,Usef.p,null,null",null,null
539,"538,Nov,null,null",null,null
540,"539,Effort,null,null",null,null
541,"540,Under.p,null,null",null,null
542,"541,Relia.p,null,null",null,null
543,"542,1 Click Dwell Time,null,null",null,null
544,"543,0.35 (0.11),null,null",null,null
545,"544,0.40 (0.11),null,null",null,null
546,"545,0.42 (0.11),null,null",null,null
547,"546,0.31 (0.10),null,null",null,null
548,"547,0.10 (0.14),null,null",null,null
549,"548,0.22 (0.13),null,null",null,null
550,"549,2 Follow-up ery,null,null",null,null
551,"550,0.19 (0.11),null,null",null,null
552,"551,0.17 (0.14),null,null",null,null
553,"552,0.13 (0.13),null,null",null,null
554,"553,0.12 (0.11),null,null",null,null
555,"554,0.14 (0.12),null,null",null,null
556,"555,0.19 (0.12),null,null",null,null
557,"556,3 Follow-up Click,null,null",null,null
558,"557,0.15 (0.12),null,null",null,null
559,"558,0.20 (0.11),null,null",null,null
560,"559,0.11 (0.12),null,null",null,null
561,"560,0.14 (0.11),null,null",null,null
562,"561,0.11 (0.12),null,null",null,null
563,"562,0.17 (0.12),null,null",null,null
564,"563,4 All ( 1 + 2 + 3 ),null,null",null,null
565,"564,0.39 (0.09),null,null",null,null
566,"565,0.46 (0.08),null,null",null,null
567,"566,0.45 (0.09),null,null",null,null
568,"567,0.33 (0.11),null,null",null,null
569,"568,0.20 (0.13),null,null",null,null
570,"569,0.36 (0.12),null,null",null,null
571,"570,1 vs. 4,null,null",null,null
572,"571,**,null,null",null,null
573,"572,**,null,null",null,null
574,"573,*,null,null",null,null
575,"574,**,null,null",null,null
576,"575,**,null,null",null,null
577,"576,5 Prior-to-click,null,null",null,null
578,"577,0.36 (0.10),null,null",null,null
579,"578,0.29 (0.10),null,null",null,null
580,"579,0.28 (0.11),null,null",null,null
581,"580,0.13 (0.12),null,null",null,null
582,"581,0.20 (0.14),null,null",null,null
583,"582,0.18 (0.13),null,null",null,null
584,"583,4 vs. 5,null,null",null,null
585,"584,**,null,null",null,null
586,"585,**,null,null",null,null
587,"586,**,null,null",null,null
588,"587,**,null,null",null,null
589,"588,**,null,null",null,null
590,"589,6 All+Prior-to-click,null,null",null,null
591,"590,0.45 (0.08),null,null",null,null
592,"591,0.49 (0.09),null,null",null,null
593,"592,0.47 (0.09),null,null",null,null
594,"593,0.39 (0.09),null,null",null,null
595,"594,0.26 (0.12),null,null",null,null
596,"595,0.40 (0.11),null,null",null,null
597,"596,5 vs. 6,null,null",null,null
598,"597,**,null,null",null,null
599,"598,**,null,null",null,null
600,"599,**,null,null",null,null
601,"600,**,null,null",null,null
602,"601,**,null,null",null,null
603,"602,**,null,null",null,null
604,"603,* and ** indicate the di erence is statistically signi cant at 0.05 and 0.01 levels by two-tail paired t -test.,null,null",null,null
605,"604,5.4 Comparing to Prior-to-click Features,null,null",null,null
606,"605,An important application of implicit feedback techniques is to infer relevance labels from search logs. Aggregating inferred relevance labels or implicit feedback signals from past search logs may help rank search results in the future [2]. We examine whether or not implicit feedback techniques can serve a similar purpose for multidimensional judgments.,null,null",null,null
607,"606,""e combination of the implicit feedback features and the priorto-click features ( 6 ) generated signi cantly be er prediction results on all the six judgments than using the prior-to-click features alone ( 5 ). is suggests that the implicit feedback features are indeed helpful and complementary to the prior-to-click features for predicting these judgments. We also note that the improvements in mean prediction correlation can be as large as over 0.2 (such as for predicting reliability and e ort). However, even combining the two sets of features still cannot adequately predict understandability (mean r , 0.26)."",null,null",null,null
608,"607,6 DISCUSSION AND CONCLUSION,null,null",null,null
609,"608,""A crucial issue of information retrieval is deciding which criteria to use to rank search results. We compared two seemingly reasonable directions for improving current TREC-style relevance judgments. One direction is to collect in situ search result judgments. e other one is to complement a single dimension of judgments (such as relevance or usefulness) by combining with other aspects. We found that the la er direction seems more e ective and versatile-- using a combination of di erent dimensions of judgments, we can almost always improve correlation with user experience measures."",null,null",null,null
610,"609,""We envision future search engines should rank results by multiple aspects. We also o ered initial suggestions on which criteria to adopt and when to adopt them. We further examined and improved implicit feedback techniques for predicting multiple judgments, addressing the scalability concern of applying multidimensional judgments in real web search applications."",null,null",null,null
611,"610,Our study makes the following contributions: · We evaluated and compared in situ usefulness judgments with,null,null",null,null
612,"611,""regular relevance/usefulness judgments by searchers. We show that using usefulness as the judgment criteria is fruitful, but in situ judgments do not show clear bene ts over regular ones. · We evaluate multidimensional search result judgments considering four alternative aspects other than relevance/usefulness. We show that multidimensional judgments be er correlate with user"",null,null",null,null
613,"612,""experience measures than using relevance/usefulness judgments alone. We also note that multidimensional judgments is a be er direction for improving TREC-style relevance judgments. · Our study also discloses the connections between di erent user experience measures and various dimensions of search result judgments. is o ers practical suggestions for system design, such as the appropriate dimensions to judge search results for the purpose of improving a particular user experience measure. · We successfully generalize implicit feedback signals to include follow-up searches and clicks in a search session to help click dwell time be er predict multidimensional judgments. To the best of our knowledge, we are also the rst to examine the e ectiveness of implicit feedback approaches for predicting novelty, understandability, reliability, and e ort."",null,null",null,null
614,"613,Our work also sheds lights on a few critical areas for exploration in the future:,null,null",null,null
615,"614,""An important line of future work is to provide more accurate criteria for search result ranking and evaluation. Based on a regression analysis, we have already o ered initial suggestions on what criteria to use and when to use them, as discussed in Section 3.3. We note that, with a su ciently large dataset, one can possibly learn a prediction model for search experience measures by taking multidimensional judgments of results as input. Such a model can further address issues such as what are the proper weights to put on di erent aspects when ranking search results. It may also solve the discrepancy between o ine evaluation measures and user experience measures, and ultimately serve as a be er objective function for training ranking models."",null,null",null,null
616,"615,""Another important application is to perform multidimensional ranking of search results based on implicit feedback signals and other information. We have already demonstrated that implicit feedback approaches can infer judgments of usefulness, novelty, e ort, and reliability with reasonable accuracy comparing to those for relevance labels. Aggregating such inferred judgments from past search logs may serve as useful features for performing multidimensional search result ranking in the future. However, we also note that our current technique needs to be improved to be er infer understandability of results from search logs."",null,null",null,null
617,"616,""We do admit certain limitations in our current study. First, our analysis and experiments are solely based on data collected from one laboratory user study, which is limited in both scale and representativeness. We suggest that further studies employ larger"",null,null",null,null
618,"617,413,null,null",null,null
619,"618,Session 4A: Evaluation 2,null,null",null,null
620,"619,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
621,"620,""datasets to verify our ndings. Second, it is worth noting that our way of collecting in situ judgments in uenced users' natural search behaviors. We observed in our log that users spent on average 12.1 seconds to nish the in situ judgments. us some particular user behavior pa erns may vary when applied to another scenario (without interrupting users for in situ judgments). ird, we also note that we only collected search result judgments for the clicked results, while it remains unclear to which extent the ndings can be generalized to the unclicked ones. Last but not least, the collected post-session judgments are more or less in uenced by the search session and the in situ judgments (although we meant to collect context-independent judgments such as to compare with contextual ones). It is also worth noting that our post-session judgments are not fully representative of the existing TREC-style approach."",null,null",null,null
622,"621,7 ACKNOWLEDGMENT,null,null",null,null
623,"622,""is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor."",null,null",null,null
624,"623,REFERENCES,null,null",null,null
625,"624,""[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: A game for modeling di erent types of web search success using interaction data. In SIGIR '11, pages 345­354, 2011."",null,null",null,null
626,"625,""[2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR '06, pages 19­26, 2006."",null,null",null,null
627,"626,""[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002."",null,null",null,null
628,"627,""[4] J. Arguello. Predicting search task di culty. In ECIR '14, pages 88­99, 2014. [5] N. J. Belkin, M. J. Cole, and J. Liu. A model for evaluation of interactive infor-"",null,null",null,null
629,"628,""mation retrieval. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [6] J. Carbonell and J. Goldstein. e use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR '98, pages 335­336, 1998. [7] B. Cartere e, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: e TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016. [8] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ cher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, pages 659­666, 2008. [9] C. W. Cleverdon. e evaluation of systems used in information retrieval. In Proceedings of the International Conference on Scienti c Information, pages 687­ 698, 1959. [10] K. Collins- ompson, C. Macdonald, P. Benne , F. Diaz, and E. Voorhees. TREC 2014 web track overview. In TREC 2014, 2014. [11] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR '11, pages 95­104, 2011. [12] H. A. Feild and J. Allan. Modeling searcher frustration. In HCIR '09, pages 5­8, 2009. [13] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR '10, pages 34­41, 2010. [14] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13, pages 453­462, 2013. [15] J. Gwizdka. Revisiting search task di culty: Behavioral and individual di erence measures. In ASIS&T '08, 2008. [16] P. Hansen and J. Karlgren. E ects of foreign language and task scenario on relevance assessment. J. Doc., 61(5):623­639, 2005. [17] A. Hassan. A semi-supervised approach to modeling web search satisfaction. In SIGIR '12, pages 275­284, 2012. [18] A. Hassan, R. Jones, and K. L. Klinkner. Beyond DCG: User behavior as a predictor of a successful search. In WSDM '10, pages 221­230, 2010. [19] R. Hu and P. Pu. A study on user perception of personality-based recommender systems. In UMAP '10, pages 291­302, 2010. [20] J. Jiang and J. Allan. Adaptive e ort for search evaluation metrics. In ECIR '16, pages 187­199, 2016."",null,null",null,null
630,"629,""[21] J. Jiang, A. Hassan Awadallah, X. Shi, and R. W. White. Understanding and predicting graded search satisfaction. In WSDM '15, pages 57­66, 2015."",null,null",null,null
631,"630,""[22] J. Jiang, D. He, and J. Allan. Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. In SIGIR '14, pages 607­616, 2014."",null,null",null,null
632,"631,""[23] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017."",null,null",null,null
633,"632,""[24] D. Kelly, J. Arguello, A. Edwards, and W.-c. Wu. Development and evaluation of search tasks for IIR experiments using a cognitive complexity framework. In ICTIR '15, pages 101­110, 2015."",null,null",null,null
634,"633,""[25] J. Y. Kim, J. Teevan, and N. Craswell. Explicit in situ user feedback for web search results. In SIGIR '16, pages 829­832, 2016."",null,null",null,null
635,"634,""[26] J. Kiseleva, E. Crestan, R. Brigo, and R. Di el. Modelling and detecting changes in user satisfaction. In CIKM '14, pages 1449­1458, 2014."",null,null",null,null
636,"635,""[27] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, H. Soncu, and C. Newell. Explaining the user experience of recommender systems. User Modeling and User-Adapted Interaction, 22(4-5):441­504, 2012."",null,null",null,null
637,"636,""[28] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, 2008."",null,null",null,null
638,"637,""[29] C. Liu, J. Liu, and N. J. Belkin. Predicting search task di culty at di erent search stages. In CIKM '14, pages 569­578, 2014."",null,null",null,null
639,"638,""[30] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task di culty for di erent task types. In ASIS&T '10, 2010."",null,null",null,null
640,"639,""[31] J. Liu, C. Liu, M. Cole, N. J. Belkin, and X. Zhang. Exploring and predicting search task di culty. In CIKM '12, pages 1313­1322, 2012."",null,null",null,null
641,"640,""[32] J. Liu, C. Liu, J. Gwizdka, and N. J. Belkin. Can search systems detect users' task di culty?: Some behavioral signals. In SIGIR '10, pages 845­846, 2010."",null,null",null,null
642,"641,""[33] J. Liu, C. Liu, X. Yuan, and N. J. Belkin. Understanding searchers' perception of task di culty: Relationships with task type. In ASIS&T '11, 2011."",null,null",null,null
643,"642,""[34] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 workshop on learning to rank for information retrieval, pages 3­10, 2007."",null,null",null,null
644,"643,""[35] Y. Liu, Y. Chen, J. Tang, J. Sun, M. Zhang, S. Ma, and X. Zhu. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In SIGIR '15, pages 493­502, 2015."",null,null",null,null
645,"644,""[36] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016."",null,null",null,null
646,"645,""[37] S. Menard. Applied Logistic Regression Analysis. Sage, 1997. [38] D. Metzler and W. B. Cro . A markov random eld model for term dependencies."",null,null",null,null
647,"646,""In SIGIR '05, pages 472­479, 2005. [39] A. Olteanu, S. Peshterliev, X. Liu, and K. Aberer. Web credibility: Features"",null,null",null,null
648,"647,""exploration and credibility prediction. In ECIR '13, pages 557­568, 2013. [40] P. Over. e TREC interactive track: An annotated bibliography. Inf. Process."",null,null",null,null
649,"648,""Manage., 37(3):369­381, 2001. [41] J. Palo i, L. Goeuriot, G. Zuccon, and A. Hanbury. Ranking health web pages"",null,null",null,null
650,"649,""with relevance and understandability. In SIGIR '16, pages 965­968, 2016. [42] J. Palo i, G. Zuccon, and A. Hanbury. e in uence of pre-processing on the"",null,null",null,null
651,"650,""estimation of readability of web documents. In CIKM '15, pages 1763­1766, 2015. [43] D. Ra ei, K. Bharat, and A. Shukla. Diversifying web search results. In WWW"",null,null",null,null
652,"651,""'10, pages 781­790, 2010. [44] R. L. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result"",null,null",null,null
653,"652,""diversi cation. Inf. Retr., 15(5):478­502, 2012. [45] A. Schuth, K. Hofmann, and F. Radlinski. Predicting search satisfaction metrics"",null,null",null,null
654,"653,""with interleaved comparisons. In SIGIR '15, pages 463­472, 2015. [46] J. Schwarz and M. Morris. Augmenting web pages and search results to support"",null,null",null,null
655,"654,""credibility assessment. In CHI '11, pages 1245­1254, 2011. [47] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using"",null,null",null,null
656,"655,""implicit feedback. In SIGIR '05, pages 43­50, 2005. [48] R. Tang, W. M. Shaw, Jr., and J. L. Vevea. Towards the identi cation of the optimal"",null,null",null,null
657,"656,""number of relevance categories. J. Am. Soc. Inf. Sci., 50(3):254­264, 1999. [49] J. van Doorn, D. Odijk, D. M. Roijers, and M. de Rijke. Balancing relevance"",null,null",null,null
658,"657,""criteria through multi-objective optimization. In SIGIR '16, pages 769­772, 2016. [50] M. Verma, E. Yilmaz, and N. Craswell. On obtaining e ort based judgements for"",null,null",null,null
659,"658,""information retrieval. In WSDM '16, pages 277­286, 2016. [51] A. Wawer, R. Nielek, and A. Wierzbicki. Predicting webpage credibility using"",null,null",null,null
660,"659,""linguistic features. In WWW '14 Companion, pages 1135­1140, 2014. [52] Y. Xu and Z. Chen. Relevance judgment: What do information users consider"",null,null",null,null
661,"660,""beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57(7):961­973, 2006. [53] Y. Yamamoto and K. Tanaka. Enhancing credibility judgment of web search"",null,null",null,null
662,"661,""results. In CHI '11, pages 1235­1244, 2011. [54] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and"",null,null",null,null
663,"662,""e ort: An analysis of document utility. In CIKM '14, pages 91­100, 2014. [55] C. X. Zhai, W. W. Cohen, and J. La erty. Beyond independent relevance: Methods"",null,null",null,null
664,"663,""and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10­17, 2003. [56] G. Zuccon. Understandability biased evaluation for information retrieval. In"",null,null",null,null
665,"664,""ECIR '16, pages 280­292, 2016."",null,null",null,null
666,"665,414,null,null",null,null
667,"666,,null,null",null,null

,sentence,label,data
0,",sentence,label,data",null,null
1,"0,Short Research Paper,null,null",null,null
2,"1,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
3,"2,Enhancing Recurrent Neural Networks with Positional Attention for Question Answering,null,null",null,null
4,"3,""Qin Chen1, Qinmin Hu1, Jimmy Xiangji Huang2, Liang He1,3 and Weijie An1"",null,null",null,null
5,"4,""1Department of Computer Science & Technology, East China Normal University, Shanghai, China 2Information Retrieval & Knowledge Management Research Lab, York University, Toronto, Canada"",null,null",null,null
6,"5,""3Shanghai Engineering Research Center of Intelligent Service Robot, Shanghai, China"",null,null",null,null
7,"6,""{qchen,wjan}@ica.stc.sh.cn,{qmhu,lhe}@cs.ecnu.edu.cn,jhuang@yorku.ca"",null,null",null,null
8,"7,ABSTRACT,null,null",null,null
9,"8,""Attention based recurrent neural networks (RNN) have shown a great success for question answering (QA) in recent years. Although significant improvements have been achieved over the nonattentive models, the position information is not well studied within the attention-based framework. Motivated by the effectiveness of using the word positional context to enhance information retrieval, we assume that if a word in the question (i.e., question word) occurs in an answer sentence, the neighboring words should be given more attention since they intuitively contain more valuable information for question answering than those far away. Based on this assumption, we propose a positional attention based RNN model, which incorporates the positional context of the question words into the answers' attentive representations. Experiments on two benchmark datasets show the great advantages of our proposed model. Specifically, we achieve a maximum improvement of 8.83% over the classical attention based RNN model in terms of mean average precision. Furthermore, our model is comparable to if not better than the state-of-the-art approaches for question answering."",null,null",null,null
10,"9,1 INTRODUCTION,null,null",null,null
11,"10,""Recurrent neural networks (RNN) have been widely used for question answering (QA) due to its good performance [8­10]. In the RNN based QA models, each word in a question or an answer sentence is represented with a hidden vector first. Then, all the hidden vectors are aggregated for sentence representations. Afterwards, the best answer is selected from a candidate answer pool according to the sentence similarity."",null,null",null,null
12,"11,""One major challenge in RNN is how to aggregate the hidden vectors for sentence representations. Recently, the attention mechanism has shown its effectiveness for the attentive sentence representations in many NLP tasks including QA [1, 8, 9, 14]. In particular, a weight is automatically generated for each word via attention, and the sentence is represented as the weighted sum of the hidden vectors. Various attention mechanisms have been proposed in previous studies. In [8], the attentive weights of the words in answers relied on the hidden representation of questions. Santos et al. [1] proposed"",null,null",null,null
13,"12,""Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080699"",null,null",null,null
14,"13,""a two-way attention mechanism, where the attentive weights for the question (answer) were influenced by the answer (question) representation according to the word-by-word interaction matrix. However, these attentions relied on the hidden vectors, which may excessively concern the words near the end of the sentence due to the abundant semantic accumulation over the word sequence in RNN. To alleviate the attention bias problem, Wang et al. [9] proposed three inner attention methods, which added the attention information before the hidden representations and achieved the state-of-the-art performance in QA."",null,null",null,null
15,"14,""To the best of our knowledge, all the previous attention mechanisms neglect the positional context, which has been extensively studied for performance boosting in information retrieval (IR). In [3] and [17], the occurrence positions of the query terms were modeled with various kernels and then integrated into traditional IR models to enhance the retrieval performance. Inspired by the effectiveness of the positional context in IR, we attempt to incorporate it into classical attentions to enhance the performance of RNN based QA. Specifically, it is assumed that if a word in the question (i.e., question word) occurs in an answer sentence, it will have an influence on the neighboring context. In other words, the neighboring words should be given more attention than those far away since they may contain more question relevant information. Based on this assumption, we propose a Positional Attention based RNN (RNN-POA) model, which models the position-aware influence of the question word for answers' attentive representations. To be specific, we first present a position-aware influence propagation strategy, in which the influence of the question word is propagated to other positions in the answer sentence by a distance-sensitive kernel. Then, a position-aware influence vector for each word is generated in the hidden space, according to the accumulated influence propagated by all the question words occurring in the answer. After that, the position-aware influence vector is integrated into the classical attention mechanism for answers' attentive representations. We perform experiments on two publicly available benchmark datasets, namely TREC-QA and WikiQA. The results show that our positional attention can significantly outperform the classical attention which does not involve any position information. Furthermore, the performance of our proposed RNN-POA model is comparable to the state-of-the-art approaches for question answering."",null,null",null,null
16,"15,""The main contributions of our work are as follows: (1) as far as we know, it is the first attempt to investigate the effectiveness of the positional context for answers' attentive representations; (2) we propose a positional attention based RNN model, which has been proved to be effective to boost the QA performance; (3) our positional attention approach can help alleviate the attention bias"",null,null",null,null
17,"16,993,null,null",null,null
18,"17,Short Research Paper,null,null",null,null
19,"18,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
20,"19,""problem by utilizing the position information of the question words, instead of the semantic accumulated hidden representations."",null,null",null,null
21,"20,2 PROPOSED MODEL,null,null",null,null
22,"21,2.1 Framework of RNN-POA,null,null",null,null
23,"22,""Figure 1(b) shows the framework of our positional attention based RNN (RNN-POA) model for answer representations. To have a better comparison, the classical attention based question representation is also shown in Figure 1(a). We adopt the bidirectional long shortterm memory (BLSTM) [2] model for sentence modeling, which takes the pre-trained word embeddings as the input, and generates the hidden vectors by recurrent updates [2]."",null,null",null,null
24,"23,""To obtain the composite representations of the questions, we adopt the classical attention used in [14], which solely relies on the hidden vectors for the attentive weight generation. Regarding to the answer, we propose a positional attention approach, and perform additional steps upon the classical attention as follows: (1) find the occurrence positions of the question words in an answer sentence; (2) propagate the influence of the question words to other positions with our position-aware influence propagation strategy; (3) generate the position-aware influence vector for each word in the answer sentence according to the propagated influence; (4) incorporate the position-aware influence vector into the classical attention mechanism."",null,null",null,null
25,"24,""With the attentive representations of both questions and answers, various similarity functions can be utilized to measure the relevance between them. We utilize the Manhattan distance similarity function with l1 norm (Formula (1)), which performs slightly better than the other alternatives such as cosine similarity as indicated in [5]:"",null,null",null,null
26,"25,""sim(rq , ra ) , exp(-||rq - ra ||1)"",null,null",null,null
27,"26,(1),null,null",null,null
28,"27,Hidden Vector,null,null",null,null
29,"28,ra,null,null",null,null
30,"29,Positional Attention,null,null",null,null
31,"30,rq Attention,null,null",null,null
32,"31,Hidden Vector,null,null",null,null
33,"32,Word Embedding,null,null",null,null
34,"33,Word Embedding Find occurrence positions,null,null",null,null
35,"34,Word Question,null,null",null,null
36,"35,Position-aware Influence Propagation,null,null",null,null
37,"36,Answer,null,null",null,null
38,"37,Word,null,null",null,null
39,"38,Question,null,null",null,null
40,"39,(a),null,null",null,null
41,"40,Position-aware Influence Vector (b),null,null",null,null
42,"41,Figure 1: (a) question representation with classical attention based RNN; (b) answer representation with positional attention based RNN.,null,null",null,null
43,"42,2.2 Position-aware Influence Propagation,null,null",null,null
44,"43,""Based on our previous assumption, a question word will have an influence on the neighboring context if it occurs in an answer sentence. Here we model the position-aware influence propagation with the Gaussian kernel, which has been proved to be effective for"",null,null",null,null
45,"44,""position modeling in IR [3, 4, 17]:"",null,null",null,null
46,"45,Kernel (u),null,null",null,null
47,"46,"","",null,null",null,null
48,"47,exp(,null,null",null,null
49,"48,-u2 2 2,null,null",null,null
50,"49,),null,null",null,null
51,"50,(2),null,null",null,null
52,"51,where u is the distance between the question word and the current,null,null",null,null
53,"52,""word,  is a parameter which restricts the propagation scope, and"",null,null",null,null
54,"53,Kernel (u) denotes the obtained influence corresponding to the,null,null",null,null
55,"54,distance of u based on the kernel.,null,null",null,null
56,"55,Note that the position-aware influence is diminishing when the,null,null",null,null
57,"56,""distance increases. In particular, when u ,"""" 0 (i.e., the current word"""""",null,null",null,null
58,"57,""is exactly a question word), the maximum propagated influence is"",null,null",null,null
59,"58,""obtained. As to the propagation scope  , the optimal value may vary"",null,null",null,null
60,"59,""from words to words. In this paper, we apply a constant  value"",null,null",null,null
61,"60,""for all question words, and focus on incorporating the positional"",null,null",null,null
62,"61,context into attentions.,null,null",null,null
63,"62,2.3 Position-aware Influence Vector,null,null",null,null
64,"63,""In this section, in order to model the influence in a high-dimensional space for attentions, we will demonstrate how to obtain the positionaware influence vector for each word in an answer sentence. First, we assume that the influence for a specific distance follows the Gaussian distributions over the hidden dimensions. Then, an influence base matrix K is defined based on the assumption, where each column denotes the influence base vector corresponding to a specific distance. More formally, each element of K is defined as:"",null,null",null,null
65,"64,""K(i, u)  N (Kernel (u),  )"",null,null",null,null
66,"65,(3),null,null",null,null
67,"66,""where K(i, u) denotes the influence corresponding to the distance"",null,null",null,null
68,"67,""of u in the i's dimension, and N is the normal density with an"",null,null",null,null
69,"68,expected value of Kernel (u) and standard deviation of  .,null,null",null,null
70,"69,""With the influence base matrix, the influence vector for a word"",null,null",null,null
71,"70,at a specific position is obtained by accumulating the influence of,null,null",null,null
72,"71,all the question words occurring in the answer:,null,null",null,null
73,"72,""pj , Kcj"",null,null",null,null
74,"73,(4),null,null",null,null
75,"74,""where pj denotes the accumulated influence vector for the word at position j, and cj is a distance count vector which measures the"",null,null",null,null
76,"75,""count of question words with various distances. Specifically, for the"",null,null",null,null
77,"76,""word at position j, the count of question words with a distance of"",null,null",null,null
78,"77,""u, namely cj (u), is calculated as:"",null,null",null,null
79,"78,""cj (u) , [(j - u)  pos (q)] + [(j + u)  pos (q)] (5)"",null,null",null,null
80,"79,q Q,null,null",null,null
81,"80,""where Q represents a question containing multiple question words, q is a word in Q, pos (q) denotes the set of q's occurrence positions in an answer sentence, and [·] is an indicator function which equals"",null,null",null,null
82,"81,to 1 if the condition satisfies and otherwise equals to 0.,null,null",null,null
83,"82,2.4 Positional Attention,null,null",null,null
84,"83,""In most previous attention mechanisms, the attentive weight of"",null,null",null,null
85,"84,""a word relies on the hidden representations, while the position"",null,null",null,null
86,"85,""information is not well investigated. In this section, we propose"",null,null",null,null
87,"86,""a positional attention approach, which incorporates the position-"",null,null",null,null
88,"87,aware influence of the question words into answers' attentive rep-,null,null",null,null
89,"88,""resentations. Specifically, the attentive weight of a word at position j in the answer sentence is formulated as:"",null,null",null,null
90,"89,""j ,"",null,null",null,null
91,"90,""exp(e (hj , pj ))"",null,null",null,null
92,"91,l k,null,null",null,null
93,"92,"",1"",null,null",null,null
94,"93,exp,null,null",null,null
95,"94,(e,null,null",null,null
96,"95,(hk,null,null",null,null
97,"96,"","",null,null",null,null
98,"97,pk,null,null",null,null
99,"98,),null,null",null,null
100,"99,),null,null",null,null
101,"100,(6),null,null",null,null
102,"101,994,null,null",null,null
103,"102,Short Research Paper,null,null",null,null
104,"103,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
105,"104,""where hj is the hidden vector at position j based on RNN, pj is the accumulated position-aware influence vector obtained by Formula (4), l denotes the sentence length, and e (·) is a score function which measures the word importance based on the hidden vector and the"",null,null",null,null
106,"105,""position-aware influence vector. More formally, the score function"",null,null",null,null
107,"106,is defined as:,null,null",null,null
108,"107,""e (hj , pj ) , vT tanh(WH hj + WP pj + b)"",null,null",null,null
109,"108,(7),null,null",null,null
110,"109,""where WH and WP are matrices, b is a bias vector, tanh is the hyperbolic tangent function, v is a global vector and vT denotes its"",null,null",null,null
111,"110,""transpose. WH , WP , b and v are the parameters. With the obtained attentive weights, an answer sentence is rep-"",null,null",null,null
112,"111,resented by the weighted sum of all the hidden vectors:,null,null",null,null
113,"112,l,null,null",null,null
114,"113,""ra , j hj"",null,null",null,null
115,"114,(8),null,null",null,null
116,"115,""j ,1"",null,null",null,null
117,"116,3 EXPERIMENTS,null,null",null,null
118,"117,3.1 Experimental Setup,null,null",null,null
119,"118,""Datasets and Evaluation Metrics. We conduct experiments on two public question answering datasets: TREC-QA and WikiQA. TREC-QA was created by Wang et al. [11] based on the TREC QA track data. WikiQA [13] is an open domain question-answering dataset in which all answers are collected from the Wikipedia. Each dataset is split into 3 parts, i.e., train, dev and test, and the statistics are presented in Table 1. To evaluate the model performance, we adopt the mean average precision (MAP) and mean reciprocal rank (MRR), which are the primary metrics used in QA [1, 9]."",null,null",null,null
120,"119,""Table 1: Dataset Statistics. """"Avg QL"""" and """"Avg AL"""" denote the average length of questions and answers."",null,null",null,null
121,"120,Dataset,null,null",null,null
122,"121,# of quetions Avg QL (train/dev/test) (train/dev/test),null,null",null,null
123,"122,Avg AL (train/dev/test),null,null",null,null
124,"123,TREC-QA 1162/65/68 7.57/8.00/8.63 23.21/24.9/25.61 WikiQA 873/126/243 7.16/7.23/7.26 25.29/24.59/24.59,null,null",null,null
125,"124,""Parameter Settings. For the word embeddings, we use the 100dimensional GloVe [6] word vectors1. The parameters in BLSTM are shared between questions and answers, which has been shown to be effective to improve the performance [9]. The dimension of the hidden vectors and the position-aware influence vectors is set to 50. Regarding to the propagation scope  (in Formula (2)), we investigate a list of values ranging from 5 to 55 with an interval of 10. The value of  (in Formula (3)) is empirically set to 0.1. We adopt the cross-entropy loss as the training objective, and utilize the Adadelta [16] algorithm for parameter update. The optimal parameters are obtained based on the best MAP performance on the development (dev) set."",null,null",null,null
126,"125,3.2 Effect of Positional Attention,null,null",null,null
127,"126,""To investigate the effect of our positional attention approach, two basic baselines which do not involve the position information, namely average pooling (""""AVG"""") [10] and the classical attention (""""ATT"""") used in [14], are integrated into the BLSTM based RNN model for"",null,null",null,null
128,"127,1 http://nlp.stanford.edu/data/glove.6B.zip,null,null",null,null
129,"128,""comparisons. Table 2 shows the performance of various models. Statistical significant tests are performed based on the paired t-test at the 0.05 level. The symbols as  and represent significant improvements over """"AVG"""" and """"ATT"""" respectively. We observe that the classical attention mechanism slightly outperforms the average pooling method by capturing part of the informative words in answers. However, it does not pay specifical attention to the question words and their surrounding context, which loses some useful information for question answering. In our positional attention approach, the importance of the question word and the surrounding context is explicitly highlighted via the position-aware influence propagation of the question words. Therefore, we can achieve significant improvements over the two baselines on both QA datasets, and the maximum improvement is as high as 8.83% in terms of MAP."",null,null",null,null
130,"129,Table 2: Performance of various models.,null,null",null,null
131,"130,Model,null,null",null,null
132,"131,TREC-QA MAP MRR,null,null",null,null
133,"132,RNN-AVG 0.7064,null,null",null,null
134,"133,RNN-ATT 0.7180 RNN-POA 0.7814*,null,null",null,null
135,"134,0.8086,null,null",null,null
136,"135,0.8121 0.8513*,null,null",null,null
137,"136,WikiQA MAP MRR,null,null",null,null
138,"137,0.6889,null,null",null,null
139,"138,0.6961 0.7212*,null,null",null,null
140,"139,0.6999,null,null",null,null
141,"140,0.7085 0.7312*,null,null",null,null
142,"141,3.3 Performance Comparisons,null,null",null,null
143,"142,""To further evaluate the effectiveness of our proposed model, we compare it with the recent work in QA. Table 3 and Table 4 summarize the results on TREC-QA and WikiQA respectively. For TREC-QA, five strong baselines are used for comparisons: (1) a combination of the BLSTM model and BM25 model [10]; (2) inner attention based RNN models which added the attention information before the hidden representations [9]; (3) a convolutional neural network (CNN) based architecture using both the hidden features and the statistical features for ranking [7]; (4) a learning-to-rank method which leveraged the word alignment features and lexical features for ranking [12]; (5) an extended LSTM framework which incorporated CNN and built the attention matrix after sentence representations [8]. As to WikiQA, in addition to [8] and [9] mentioned above, we make a comparison with other two strong baselines: (1) a bigram CNN model with average pooling [13]; (2) a CNN model which used an interactive attention matrix for the attentive representations [15]."",null,null",null,null
144,"143,""It is observed that we achieve the new state-of-the-art performance on TREC-QA in terms of both MAP and MRR. Regarding to WikiQA, our RNN-POA model outperforms all the strong baselines except the inner attention based RNN models [9]. This validates our previous assumption that the words close to the question words should be given more attention than those far away. In addition, it has been proved to be effective for incorporating the positional context into answers' attentive representations."",null,null",null,null
145,"144,3.4 Investigation of Propagation Scope ,null,null",null,null
146,"145,""The parameter  (in Formula (2)) controls the influence propagation scope of the question words. For a certain distance, the propagated position-aware influence increases when  grows. Figure 2 plots the MAP and MRR metrics over a set of  values ranging from 5 to 55 with a step of 10. We observe that the general tendency for each"",null,null",null,null
147,"146,995,null,null",null,null
148,"147,Short Research Paper,null,null",null,null
149,"148,""SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan"",null,null",null,null
150,"149,Table 3: Performance comparisons on TREC-QA. The work marked with  used the cleaned dataset.,null,null",null,null
151,"150,System,null,null",null,null
152,"151,MAP MRR,null,null",null,null
153,"152,Wang and Nyberg (2015) [10] Wang et al. (2016) [9]  Severyn and Moschitti (2015) [7] Wang and Ittycheriah (2015) [12] Santos et al. (2016) [8],null,null",null,null
154,"153,0.7134 0.7369 0.7459 0.7460 0.7530,null,null",null,null
155,"154,0.7913 0.8208 0.8078 0.8200 0.8511,null,null",null,null
156,"155,RNN-POA,null,null",null,null
157,"156,0.7814 0.8513,null,null",null,null
158,"157,Table 4: Performance comparisons on WikiQA,null,null",null,null
159,"158,System,null,null",null,null
160,"159,MAP MRR,null,null",null,null
161,"160,Yang et al. (2015) [13] Santos et al. (2016) [8] Yin et al. (2015) [15] Wang et al. (2016) [9],null,null",null,null
162,"161,0.6520 0.6886 0.6921 0.7341,null,null",null,null
163,"162,0.6652 0.6957 0.7108 0.7418,null,null",null,null
164,"163,RNN-POA,null,null",null,null
165,"164,0.7212 0.7312,null,null",null,null
166,"165,""evaluation metric is similar. Specifically, the performance increases when  grows at first. Then, it decreases and tends to be stable when  becomes larger. On the whole, a  value between 15 and 35 is recommended to be a reliable setting in our experiments."",null,null",null,null
167,"166,(a) MAP,null,null",null,null
168,"167,(b) MRR,null,null",null,null
169,"168,Figure 2: Impact of the propagation scope ,null,null",null,null
170,"169,3.5 A Case Study,null,null",null,null
171,"170,""To have an intuitive understanding of our proposed RNN-POA model, we draw a word heatmap for a case based on the classical attention and our positional attention respectively in Figure 3. Obviously, to answer this question, we should focus on the words """"George Warrington"""". However, the classical attention cares more about some irrelevant words such as """"market"""". Although these words have some semantic relations with the words in the question (e.g., the word """"market"""" co-occurs frequently with the words """"chief """" and """"executive""""), the semantically related words are not necessarily useful for question answering. In contrast, our positional attention approach concerns more about the question words such as """"Amtrak"""", """"president"""", """"chief """" and """"executive"""", as well as the surrounding context such as """"George Warrington"""", which provides more valuable clues for question answering. That is why our proposed positional attention approach can achieve much better performance than the classical attention method."",null,null",null,null
172,"171,4 CONCLUSIONS,null,null",null,null
173,"172,""In this paper, we propose a positional attention based RNN (RNNPOA) model, which incorporates the positional context of the question words into answers' attentive representations. The experimental results on two benchmark datasets show the overwhelming"",null,null",null,null
174,"173,Q: who is the president or chief executive of amtrak?,null,null",null,null
175,"174,""Long term success here has to do with doing it right ,",null,null
176,"getting it right and increasing market share. said RNN-ATT George Warrington, Amtrak's president and chief"",null,null",null,null
177,"175,executive.,null,null",null,null
178,"176,""Long term success here has to do with doing it right ,",null,null
179,"getting it right and increasing market share. said RNN-POA George Warrington, Amtrak's president and chief"",null,null",null,null
180,"177,executive.,null,null",null,null
181,"178,Figure 3: An example of RNN-ATT and RNN-POA.,null,null",null,null
182,"179,""superiority of our proposed model over the basic baselines which do not incorporate any position information. Furthermore, compared with the state-of-the-art approaches in question answering, our proposed model can achieve a considerable performance by merely incorporating the position information into the classical attention mechanism. In the future, we will investigate more strategies to model the position influence of the question words."",null,null",null,null
183,"180,ACKNOWLEDGMENTS,null,null",null,null
184,"181,""This research is supported by the National Key Technology Support Program (No.2015BAH01F02), Science and Technology Commission of Shanghai Municipality (No.16511102702), and Shanghai Municipal Commission of Economy and Information Under Grant Project (No.201602024). This research is also supported by a Discovery grant from the Natural Sciences and Engineering Research Council (NSERC) of Canada and an NSERC CREATE award."",null,null",null,null
185,"182,REFERENCES,null,null",null,null
186,"183,""[1] Cicero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. arXiv preprint arXiv:1602.03609 (2016)."",null,null",null,null
187,"184,""[2] Alex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks 18, 5 (2005), 602­610."",null,null",null,null
188,"185,""[3] Baiyan Liu, Xiangdong An, and Jimmy Xiangji Huang. 2015. Using term location information to enhance probabilistic information retrieval. In SIGIR. 883­886."",null,null",null,null
189,"186,""[4] Jun Miao, Jimmy Xiangji Huang, and Zheng Ye. 2012. Proximity-based rocchio's model for pseudo relevance. In SIGIR. 535­544."",null,null",null,null
190,"187,[5] Jonas Mueller and Aditya Thyagarajan. 2016. Siamese Recurrent Architectures for Learning Sentence Similarity. In AAAI. 2786­2792.,null,null",null,null
191,"188,""[6] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP. 1532­1543."",null,null",null,null
192,"189,[7] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In SIGIR. 373­382.,null,null",null,null
193,"190,""[8] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. LSTM-based deep learning models for non-factoid answer selection. arXiv:1511.04108 (2015)."",null,null",null,null
194,"191,""[9] Bingning Wang, Kang Liu, and Jun Zhao. 2016. Inner attention based recurrent neural networks for answer selection. In ACL. 1288­1297."",null,null",null,null
195,"192,[10] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering.. In ACL. 707­712.,null,null",null,null
196,"193,""[11] Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32."",null,null",null,null
197,"194,[12] Zhiguo Wang and Abraham Ittycheriah. 2015. FAQ-based Question Answering via Word Alignment. arXiv preprint arXiv:1507.02628 (2015).,null,null",null,null
198,"195,""[13] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering.. In EMNLP. 2013­2018."",null,null",null,null
199,"196,""[14] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In NAACL-HLT. 1480­1489."",null,null",null,null
200,"197,""[15] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. 2015. ABCNN: Attention-based convolutional neural network for modeling sentence pairs. In arXiv preprint arXiv:1512.05193."",null,null",null,null
201,"198,[16] Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012).,null,null",null,null
202,"199,""[17] Jiashu Zhao, Jimmy Xiangji Huang, and Ben He. 2011. CRTER: using cross terms to enhance probabilistic information retrieval. In SIGIR. 155­164."",null,null",null,null
203,"200,996,null,null",null,null
204,"201,,null,null",null,null

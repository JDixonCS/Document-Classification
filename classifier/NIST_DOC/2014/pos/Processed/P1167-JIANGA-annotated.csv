,sentence,label,data
,,,
0,Necessary and Frequent Terms in Queries,null,null
,,,
1,Jiepu Jiang,null,null
,,,
2,Center for Intelligent Information Retrieval School of Computer Science,null,null
,,,
3,University of Massachusetts Amherst,null,null
,,,
4,jpjiang@cs.umass.edu,null,null
,,,
5,James Allan,null,null
,,,
6,Center for Intelligent Information Retrieval School of Computer Science,null,null
,,,
7,University of Massachusetts Amherst,null,null
,,,
8,allan@cs.umass.edu,null,null
,,,
9,ABSTRACT,null,null
,,,
10,"Vocabulary mismatch has long been recognized as one of the major issues affecting search effectiveness. Ineffective queries usually fail to incorporate important terms and/or incorrectly include inappropriate keywords. However, in this paper we show another cause of reduced search performance: sometimes users issue reasonable query terms, but systems cannot identify the correct properties of those terms and take advantages of the properties. Specifically, we study two distinct types of terms that exist in all search queries: (1) necessary terms, for which term occurrence alone is indicative of document relevance; and (2) frequent terms, for which the relative term frequency is indicative of document relevance within the set of documents where the term appears. We evaluate these two properties of query terms in a dataset. Results show that only 1/3 of the terms are both necessary and frequent, while another 1/3 only hold one of the properties and the final third do not hold any of the properties. However, existing retrieval models do not clearly distinguish terms with the two properties and consider them differently. We further show the great potential of improving retrieval models by treating terms with distinct properties differently.",null,null
,,,
11,Categories and Subject Descriptors,null,null
,,,
12,"H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ query formulation, retrieval models.",null,null
,,,
13,General Terms,null,null
,,,
14,"Performance, Experimentation, Human Factors.",null,null
,,,
15,Keywords,null,null
,,,
16,Query; term frequency; term occurrence.,null,null
,,,
17,1. INTRODUCTION,null,null
,,,
18,"Term frequency (TF) is widely used as an important heuristic in retrieval models [1­3]. The assumption is that documents with comparatively higher frequencies of query terms are more likely to be relevant. However, we suspect that in many cases this assumption does not hold. Instead, users may adopt some query terms to simply include or exclude documents regardless of the occurrences of the terms ­ that is, in those cases TF does not indicate the relevance of documents as long as the terms appear. In such cases, retrieval models that heavily exploit TF may",null,null
,,,
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 06­11, 2014, Gold Coast, QLD, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 $15.00. http://dx.doi.org/10.1145/2600428.2609536",null,null
,,,
20,incorrectly rank some non-relevant documents with high frequencies of the terms to the top.,null,null
,,,
21,"We define the following two properties of query terms. We say that a term is necessary to a topic if most relevant documents contain the term. Documents with no occurrences of the necessary term are unlikely to be relevant. In comparison, we say that a term is frequent to a topic if relevant documents usually have relatively more occurrences of the term comparing to the non-relevant ones. Documents in which the frequent term appears many times are more likely relevant compared to those where the term appears less frequently. Note that the two properties do not conflict with each other: a term can be both necessary and frequent.",null,null
,,,
22,"We hypothesize that both necessary and frequent terms exist in user queries, but some query terms may only conform to one of the two properties. We study the following research questions:",null,null
,,,
23,RQ1: Do query terms differ with respect to the two properties? We examine the two properties of query terms in a dataset based on term occurrences in relevant and non-relevant documents.,null,null
,,,
24,RQ2: How do users perceive the two properties of query terms? Do users' opinions agree with those learned from the dataset and do users agree with each other? We ask assessors to annotate query terms regarding the two properties and analyze the results.,null,null
,,,
25,"RQ3: Assuming we know the properties of query terms, can we improve search performance by treating terms differently? We show a simple approach that can achieve 35% improvement in nDCG@10 compared to the query likelihood model, if it knows these properties of query terms. Results suggests great potential for improving search performance by identifying properties of query terms and treating them differently in retrieval models.",null,null
,,,
26,2. EVALUATION OF TERM PROPERTIES,null,null
,,,
27,"In this section, we define indicators and examine query term properties in the TREC Robust 2004 dataset.",Y,null
,,,
28,2.1 Indicators of Term Properties,null,null
,,,
29,We denote the degree to which a query term w is necessary to a,null,null
,,,
30,"topic by P(X,""1|R), the probability of observing w in the set of""",null,null
,,,
31,"relevant documents, R. X,1 refers to the occurrence of w in a",null,null
,,,
32,document regardless of its frequency. In a dataset with R being,null,null
,,,
33,"judged, we can estimate P(X,""1|R) by Equation (1), where: N is""",null,null
,,,
34,the total number of documents in R; Nw is the number of documents in R where w appears at least once. The greater the,null,null
,,,
35,"value of P(X,""1|R), the more necessary the term to the topic.""",null,null
,,,
36,P^ ,null,null
,,,
37,X,null,null
,,,
38,1|,null,null
,,,
39,R,null,null
,,,
40,Nw N,null,null
,,,
41,-1,null,null
,,,
42,We evaluate to what degree a query term w is frequent to a,null,null
,,,
43,"topic by comparing P(w|R) and P(w|NR), where NR is the set of",null,null
,,,
44,non-relevant documents. P(w|R) is the probability of the term w in,null,null
,,,
45,"relevant documents, which is estimated by Equation (2), where:",null,null
,,,
46,P(w|d) is the probability of w in the multinomial document,null,null
,,,
47,language model of d; each document d in R has an equal weight,null,null
,,,
48,1/N to contribute to P(w|R). We estimate P(w|d) using maximum,null,null
,,,
49,likelihood estimation with Dirichlet smoothing [4]. The parameter,null,null
,,,
50, is selected to optimize the nDCG@10 of query likelihood model,null,null
,,,
51,1167,null,null
,,,
52,1.0 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.0,null,null
,,,
53,0,null,null
,,,
54,"P(X,1|R)",null,null
,,,
55,75 150 225 300 375 450 525 600,null,null
,,,
56,(a),null,null
,,,
57,10000.000 1000.000,null,null
,,,
58,100.000 10.000 1.000 0 0.100 0.010,null,null
,,,
59,0.001,null,null
,,,
60,P(w|R)/P(w|NR),null,null
,,,
61,75 150 225 300 375 450 525 600,null,null
,,,
62,(b),null,null
,,,
63,16.000 8.000,null,null
,,,
64,4.000 2.000 1.000,null,null
,,,
65,0 0.500 0.250,null,null
,,,
66,0.125,null,null
,,,
67,"P(w|R, X,""1)/P(w|NR, X"",1)",null,null
,,,
68,75 150 225 300 375 450 525 600,null,null
,,,
69,(c),null,null
,,,
70,"Figure 1. Distribution of P(X,""1|R), P(X"",1|R)/P(X,""1|NR), P(w|R)/P(w|NR), and P(w|R, X"",""1)/P(w|NR, X"",1) on 663 query terms.",null,null
,,,
71,"in the dataset. We estimate P(w|NR) in a similar form but among the set of non-relevant documents. The greater the value of P(w|R) compared to P(w|NR), the more frequent is the term w.",null,null
,,,
72,P^ w |,null,null
,,,
73,R,null,null
,,,
74,1 N,null,null
,,,
75, dR,null,null
,,,
76,Pw|,null,null
,,,
77,d,null,null
,,,
78,-2,null,null
,,,
79,It should be noted that we can easily observe P(w|R) > P(w|NR),null,null
,,,
80,"when w is necessary for R but rarely appears in NR. Therefore, we",null,null
,,,
81,further examine a stronger form of the frequent term property:,null,null
,,,
82,"within the set of documents where w appears at least once,",null,null
,,,
83,relatively higher frequency of the term indicates greater likelihood,null,null
,,,
84,of relevance. We quantify this stronger property by comparing,null,null
,,,
85,"P(w|X,""1,R) and P(w|X"",""1,NR). The two probabilities are estimated""",null,null
,,,
86,"similar to Equation (2), but within the set of relevant and non-",null,null
,,,
87,relevant documents where w appears at least once.,null,null
,,,
88,2.2 Evaluation,null,null
,,,
89,"We calculate the indicators related to term properties in TREC Robust 2004 dataset. The dataset includes 250 queries and 663 query terms (counting multiple occurrences of the same term in different queries). We remove the Indri standard stopwords and stem using the Krovetz stemmer when processing documents and queries. Figure 1(a), 1(b), and 1(c) show the distribution of P(X,""1|R), P(w|R)/P(w|NR), and P(w|R,X"",""1)/P(w|NR,X"",1) for the 663 query terms.",Y,null
,,,
90,"Results show that it is very common to use query terms that do not hold the two properties. As shown in Fig. 1(a), among the 663 query terms, only 18.5% are fully necessary ­ i.e., P(X,1|R),1 ­ and 44.8% roughly hold the necessary property ­ P(X,""1|R)0.8. Moreover, 33% of the query terms do not hold the necessary property (P(X"",""1|R)<0.5), and 50% of the queries have at least one such term. Using query terms with the frequent term property is also very common in the dataset: Figures 1(b) and 1(c) show that 475 out of the 663 query terms (71.6%) hold the basic frequent term property, but only 373 (56.3%) hold the stronger form where P(w|R,X"",""1)/P(w|NR,X"",""1)>1. Among the 250 queries, 57.8% have at least one term that does not hold the frequent term property and 75.1% have at least one term that does not hold the stronger form of the frequent term property.""",null,null
,,,
91,"We further evaluate the relation between search effectiveness and using query terms that do not hold the two properties. Figure 2 shows the average nDCG@10 of queries in which at least one term's value of the three indicators is less than P, where P ranges from 0.1 to 1.0. Results suggest that queries with terms that do not hold either of the two properties are less effective. For example, for the set of queries with at least one term's value of P(X,""1|R) < 0.5, the nDCG@10 of these queries is only 0.356, less effective than those of the 250 queries on average. For queries with terms that do not hold either of the two properties, search performance declined by a greater magnitude. However, we noticed that for queries with terms that have P(w|R)/P(w|NR) < P ranging from 0.2 to 0.6, there are no apparent differences in the queries' search""",null,null
,,,
92,"performance. This indicates that P(w|R)/P(w|NR) is less indicative of term's search effectiveness. In following discussions, we use the stronger form of frequent term property and adopt P(w|R,X,""1)/P(w|NR,X"",1) as the indicator.",null,null
,,,
93,0.5,null,null
,,,
94,"nDCG@10 of queries P(X,1|R) < P",null,null
,,,
95,0.45,null,null
,,,
96,nDCG@10 of queries P(w|R)/P(w|NR) < P,null,null
,,,
97,"nDCG@10 of queries P(w|R,X,""1)/P(w|NR,X"",1) < P",null,null
,,,
98,0.4,null,null
,,,
99,0.35,null,null
,,,
100,0.3,null,null
,,,
101,0.25,null,null
,,,
102,0.2,null,null
,,,
103,0.15,null,null
,,,
104,0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 ALL P,null,null
,,,
105,"Figure 2. nDCG@10 of queries with at least one term for which the three indicators < P. P ranges from 0.1 to 1.0. ""ALL"" shows the average nDCG@10 of ALL queries.",null,null
,,,
106,"Necessary Terms P(X,1|R)0.8",null,null
,,,
107,"Frequent Terms P(w|R,X,""1)/P(w|NR,X"",1)>1",null,null
,,,
108,74 terms,null,null
,,,
109,224 terms,null,null
,,,
110,149 terms,null,null
,,,
111,216 terms are neither necessary nor frequent.,null,null
,,,
112,Figure 3. Overlap of terms conforming to two properties.,null,null
,,,
113,"We further show the overlap of query terms conforming to the two properties in Figure 3. Among the 663 terms, 224 (33.8%) are both necessary and frequent, but 223 (33.6%) only hold one of the two properties. The remaining 216 terms (32.6%) are neither necessary nor frequent. This suggests different strategies should be adopted to improve ineffective queries. The 216 terms that do not hold either property are not indicative of document relevance and would be better removed. For the 74 terms only having the necessary term property, we should prefer documents where the term appears but do not give further credit to high term frequency. For the 149 terms only having the frequent term property, we should prefer documents where the term appears many times over those where the term appears only once or twice, but it may be risky to filter out documents without any occurrence of the term.",null,null
,,,
114,"To summarize, our results show that whether or not a term holds the two properties affects the search effectiveness of queries. In the dataset, only 1/3 of the query terms hold both properties. Another 1/3 hold only one of the two properties. The other 1/3 have neither property. This suggests that we may",null,null
,,,
115,1168,null,null
,,,
116,"improve search systems in two different ways: identify terms with these properties and adopt different ranking criteria; predict terms without any of the two properties and discount the effects of such terms in ranking. In further sections, we explore such potentially by assuming we can correctly identify properties of terms.",null,null
,,,
117,3. USER JUDGMENTS OF PROPERTIES,null,null
,,,
118,"In this section, we study whether or not users can make correct judgments of these two term properties. This is meaningful for two reasons. First, if the users make poor judgments on query properties, it provides a new explanation for ineffective queries. Second, it the users can make correct judgments, systems may benefit from providing query languages allowing users to express their sense of the properties.",null,null
,,,
119,"We asked 10 users to annotate 100 TREC queries selected from the TREC Robust 2004 dataset (Topic 301-400). Each user annotated 15 queries, with 10 overlapping with another two users. For example, the first user annotated query 301-315, the second user on query 311-325, ... , and the last user on query 391-400 as well as 301-305. This resulted in 10 users' annotations on the 100 queries. For 50 queries, we have only one user's annotation, and for the other 50, we have two users' annotations, so that we can study users' agreements on the properties of query terms. For each query term, we asked users two yes/no questions as follows. We say that a user annotated a query term as necessary or frequent if the answer on Q1 or Q2 is yes, respectively.",Y,null
,,,
120,Q1: I believe most of the relevant results should have this word. Results that do not contain this word are unlikely to be useful.,null,null
,,,
121,Q2: I believe this word should appear many times in relevant results. Results in which the word appears only once or twice are less likely to be useful.,null,null
,,,
122,"We found that pairs of users have some agreement on whether or not a term is necessary, but their opinions are rather independent of each other on the frequent terms. Among the 126 query terms involving two users' annotations, users agreed in 67% of the cases regarding whether or not a term is necessary. However, they agreed only in 48% of the cases on whether a term is frequent.",null,null
,,,
123,Table 1. Correctness of user annotation of term properties.,null,null
,,,
124,Property,null,null
,,,
125,P,null,null
,,,
126,Num Y/N by P,null,null
,,,
127,Num Y/N by Users,null,null
,,,
128,User Acc / Prior,null,null
,,,
129,Class,null,null
,,,
130,Prec,null,null
,,,
131,Rec,null,null
,,,
132,0.8 88/164 Necessary,null,null
,,,
133,0.5 145/107,null,null
,,,
134,201/51,null,null
,,,
135,0.50/0.65,null,null
,,,
136,Y N,null,null
,,,
137,201/51,null,null
,,,
138,0.63/0.57,null,null
,,,
139,Y N,null,null
,,,
140,0.41 0.93 0.88 0.27 0.63 0.87 0.63 0.30,null,null
,,,
141,1.0 124/128 Frequent,null,null
,,,
142,0.8 156/96,null,null
,,,
143,165/87,null,null
,,,
144,0.60/0.51,null,null
,,,
145,Y N,null,null
,,,
146,165/87,null,null
,,,
147,0.61/0.62,null,null
,,,
148,Y N,null,null
,,,
149,0.57 0.76 0.66 0.45 0.67 0.71 0.48 0.44,null,null
,,,
150,"Table 1 shows the accuracy of users' annotations of query term properties comparing to those evaluated by the values of P(X,""1|R) and P(w|R,X"",""1)/P(w|NR,X"",1). Results show that in general it is difficult for users to make correct judgments on the query terms' properties. If we use P(X,""1|R)>0.5 as the criteria for necessary terms, users' judgments are slightly better than a classifier using prior probability of the classes (accuracy 0.63 versus 0.57). When we use P(w|R,X"",""1)/P(w|NR,X"",""1)>1.0 as the threshold for frequent terms, users did also only slightly better than a classifier using prior probabilities (accuracy 0.60 versus 0.51). The accuracy and precision of user judgments look not useful. Moreover, when we adopt different criteria for term properties, e.g. P(X"",""1|R)>0.8, users' judgments may even be worse than a classifier using the prior probability of classes.""",null,null
,,,
151,"To conclude, the results of user annotation on query term properties show that it is very difficult for users to select the properties of query terms prior to looking at search results. Users also agree only slightly with others on whether a term property applies. Specifically, users' judgments on frequent terms are completely independent of others.",null,null
,,,
152,4. SYSTEMS USING TERM PROPERTIES,null,null
,,,
153,"In this section, we explore the potential of improving retrieval systems assuming we know the properties of terms correctly. The prediction of term properties is left for future work.",null,null
,,,
154,4.1 Approaches,null,null
,,,
155,"Let q be a query. We assume we know the set of necessary terms qN and the set of frequent terms qF. Note that qN and qF can be empty set, and a term in q may be in neither qN nor qF. We rank a document d by Equation (3), where: we assume qN and qF are independent given d; each term in qN and qF are generated independently of other terms from d by different process PN(w|d) and PF(w|d).",null,null
,,,
156,"P qN , qF | d ",null,null
,,,
157, PqN | d  PqF | d ,null,null
,,,
158,-3,null,null
,,,
159,  PN w | d    PF w | d ,null,null
,,,
160,wqN,null,null
,,,
161,wqF,null,null
,,,
162,"We calculate PN(w|d) and PF(w|d) in Eq(4) and Eq(5). In Eq(4), we calculate PN(w|d) as the probability of selecting a term w from d's vocabulary Vd ignoring the frequency of terms in d. |Vd| is the size of d's vocabulary. PN(w) is the probability that w exists in a the vocabulary of a document in the whole corpus. In a corpus of",null,null
,,,
163,"k documents, we estimate PN(w) as Eq(6). N is a parameter for smoothing. PF(w|d) is simply the probability of a term w from the multinomial document language model of d, estimated using",null,null
,,,
164,maximum likelihood estimation with Dirichlet smoothing. In our,null,null
,,,
165,"experiments, we set F to the value that can maximize nDCG@10 of using all terms as qF and no term as qN for retrieval (equivalent to query likelihood model). In contrast, we set N to the value that can maximize nDCG@10 of using all terms as qN and no term as qF for retrieval.",null,null
,,,
166,P^N,null,null
,,,
167,w,null,null
,,,
168,|,null,null
,,,
169,d,null,null
,,,
170,1,null,null
,,,
171,N Vd,null,null
,,,
172, PN ,null,null
,,,
173, N,null,null
,,,
174,w,null,null
,,,
175,-4,null,null
,,,
176,P^F,null,null
,,,
177,w,null,null
,,,
178,|,null,null
,,,
179,d,null,null
,,,
180,c,null,null
,,,
181,"w,",null,null
,,,
182,d,null,null
,,,
183,  F ,null,null
,,,
184,d  F,null,null
,,,
185,PF,null,null
,,,
186,w,null,null
,,,
187,-5,null,null
,,,
188, P^N w ,null,null
,,,
189,1 k,null,null
,,,
190,d,null,null
,,,
191,1 Vd,null,null
,,,
192,-6,null,null
,,,
193,"For a necessary term w in qN, PN(w|d) totally ignores the frequency of w in d. Its value depends only on whether or not w",null,null
,,,
194,"appears in d. In addition, it favors documents with a small",null,null
,,,
195,vocabulary. (This is intuitively correct because observing w in d is,null,null
,,,
196,less informative if d is very long and has a large vocabulary.),null,null
,,,
197,"When we put all the query terms into qF and none into qN, Equation (3) falls back to the query likelihood language model.",null,null
,,,
198,4.2 Search Effectiveness,null,null
,,,
199,"In this section, we evaluate the approaches proposed above by assuming different sets of necessary and frequent terms. Table 2 shows the results. For ""qN"" and ""qF"" in Table 2, ""none"" means do not use any terms, ""all"" means using all query terms, and ""best"" means using the best possible combination of query terms (the set of query terms that leads to the best nDCG@10).",null,null
,,,
200,"We first evaluate the effectiveness of PN(w|d) and PF(w|d) on different set of terms individually. Unsurprisingly, using all terms as necessary terms (N++) performs worse than using all terms as frequent terms (F++ and also Query Likelihood). However,",null,null
,,,
201,1169,null,null
,,,
202,"simply ignoring term frequencies of all documents still achieved nDCG@10 as high as 0.293. This indicates that solely considering term occurrences is still useful in many cases. However, simply using all terms as both necessary and frequent terms (N++F++) did not result in any improvements.",null,null
,,,
203,"We further examine whether removing inappropriate terms from qN or qF can lead to improved search performance. As shown in Table 2, removing inappropriate terms from qF can potentially improve nDCG@10 from 0.438 (F++) to 0.514 (F+), and from 0.436 (N++F++) to 0.528 (F++F+). Similarly, removing terms from qN can potentially improve nDCG@10 from 0.293 (N++) to 0.329 (N+), and from 0.436 (N++F++) to 0.503 (N+F++). When we remove inappropriate words from both qN and qF (N+F+), we can potentially improve nDCG@10 to 0.590, which is about 35% improvements comparing to QL and N++F++. This suggests that there is great potentiality of improving search performance if we can predict correctly the frequent and necessary words.",null,null
,,,
204,"However, it should be noted that the best set of terms for qN and qF are dependent of each other. When we use the best set of qN in N+F++ and the best set of qF in N++F+ for retrieval (N+F+ local), there will be 10% decline of nDCG@10 comparing to N+F+. Besides, we found that a part of the improvement of search performance comes from removing inappropriate terms from both qN and qF. If we restrict that all the query terms should be in at least one of qN and qF (N+F+ (-rmv)), the nDCG@10 declined from 0.590 to 0.552, although still a substantial improvement comparing to F++ (QL).",null,null
,,,
205,"We further examine whether using the indicators of properties in section 2, i.e., P(X,""1|R) and P(w|R,X"",""1)/P(w|NR,X"",""1), can effectively select the appropriate set of terms for qN and qF to enhance search performance. We examined a simple rule-based approach as follows. We start with all query terms in qF and no terms in qN. We remove terms in qF if P(w|R,X"",""1)/P(w|NR,X"",1) < 1.05. If the removed term has P(X,""1|R)>0.2, we add the term into qN. Besides, we add all terms with P(X"",""1|R)>0.95 into qN. This simply rule-based approach (N+F+ P) improves nDCG@10 by 8.7% comparing to F++ (using all terms for qF). This suggests that the two indicators are effective criterion of selecting qN and qF. However, the performance of the selected qN and qF cannot be compared with the best possible qN and qF in N+F+. This indicates that the two indicators are not enough for selecting qF and qN. The exploration of predictors for qF and qN is left for future works.""",null,null
,,,
206,"Earlier, we showed that users made poor judgments on the properties of query terms. To further verify the quality of users' judgments, we select terms into qN and qF if users answered yes in Q1 and Q2. As shown in Table 2, this approach reduces search",null,null
,,,
207,Table 2. Potential improvements of search performance.,null,null
,,,
208,Label F++ (QL) F+ N++ N+ N++F++ N++F+ N+F++ N+F+ N+F+ local N+F+ (-rmv),null,null
,,,
209,N+F+ P,null,null
,,,
210,N+F+ user,null,null
,,,
211,F+RM,null,null
,,,
212,qN none none all best all all best best best.L best,null,null
,,,
213,P(X|R),null,null
,,,
214,user,null,null
,,,
215,none,null,null
,,,
216,"qF all best none none all best all best best.L best P(w|R,X) / P(w|NR,X)",null,null
,,,
217,user,null,null
,,,
218,RM100,null,null
,,,
219,nDCG@10 0.438 0.514 0.293 0.329 0.436 0.528 0.503 0.590 0.541 0.552,null,null
,,,
220,0.476,null,null
,,,
221,0.416,null,null
,,,
222,0.644,null,null
,,,
223,Change / Baseline -,null,null
,,,
224,+17.4% / F++ -,null,null
,,,
225,+12.3% / N++ -,null,null
,,,
226,+21.1% / N++F++ +15.4% / N++F++ +35.3% / N++F++ +24.1% / N++F++ +26.6% / N++F++,null,null
,,,
227,#NAME?,null,null
,,,
228,QL: nDCG@10 0.443 (100 queries),null,null
,,,
229,-,null,null
,,,
230,* N/F in the run labels refers to qN/qF; ++ means using all terms; + means using selected query terms.,null,null
,,,
231,performance. The nDCG@10 is 0.416 (N+F+ user) versus 0.443 in QL on the same set of 100 queries. This further confirms that it is difficult for users to make useful judgments on term properties.,null,null
,,,
232,"So far we limit the set of query terms among those being issued by the users, and the improvements of search performance mainly comes from correct identification of the necessary terms and the frequent terms. We compare our approach with query expansion on the potential of improving search performance. We estimate the true relevance model based on qrels, and use the top 100 terms (""RM100"") as qF for search. As shown in Table 2, solely working on the set of query terms issued by users, N+F+ is not much worse than F+RM (true relevance model) on nDCG@10, which extensively exploits the representative terms in relevant results.",null,null
,,,
233,5. FUTURE WORK,null,null
,,,
234,"In this preliminary study, we show that retrieval models that exploit term frequency can potentially be improved substantially by separately considering TF for some query terms and counting only occurrence or non-occurrence for some other query terms. This conclusion comes from our findings that query terms hold different properties. Specifically, sometimes the frequencies of terms do not indicate document relevance as long as the terms appear. In such cases, existing retrieval models may incorrectly rank documents with high term frequencies to the top. Queries with terms lacking either property are less effective in general.",null,null
,,,
235,"Future work on this topic mainly focuses on the prediction of an appropriate set of terms in qN and qF. As discussed in section 4, though values of the two indicators can effectively predict qN and qF, it is far from perfect and the two indicators are also computed based on known relevance judgments.",null,null
,,,
236,"Our study is closely related but different from the recent work of term necessity prediction by Zhao and Callan [5, 6]. Zhao et al. focused on predicting P(w|R) and aimed at solving term mismatch by selecting terms with highly predicted P(w|R) values for query expansion. In comparison, we do not expand the query but aim at recognizing the correct properties of query terms that are issued by the users. The two approaches follow different directions but may potentially be combined. As shown in Table 2, our approach may have substantial improvements on search performance that is comparable to those can be achieved by predicting P(w|R).",null,null
,,,
237,ACKNOWLEDGEMENT,null,null
,,,
238,"This work was supported in part by the Center for Intelligent Information Retrieval and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",null,null
,,,
239,6. REFERENCES,null,null
,,,
240,"[1] Lafferty, J. and Zhai, C. 2001. Document language models, query models, and risk minimization for information retrieval. In Proc. SIGIR'01: 111-119.",null,null
,,,
241,"[2] Ponte, J.M. and Croft, W.B. 1998. A language modeling approach to information retrieval. Proc. SIGIR'98: 275-281.",null,null
,,,
242,"[3] Robertson, S.E. et al. 1995. Okapi at TREC-3. NIST Special Publication 500-226: Proceedings of the Third Text REtrieval Conference (TREC-3).",Y,null
,,,
243,"[4] Zhai, C. and Lafferty, J. 2001. A study of smoothing methods for language models applied to Ad Hoc information retrieval. In Proc. SIGIR'01: 334­342.",null,null
,,,
244,"[5] Zhao, L. and Callan, J. 2012. Automatic term mismatch diagnosis for selective query expansion. In Proc. SIGIR'12: 515-524.",null,null
,,,
245,"[6] Zhao, L. and Callan, J. 2010. Term necessity prediction. In Proc. CIKM'10: 259­268.",null,null
,,,
246,1170,null,null
,,,
247,,null,null

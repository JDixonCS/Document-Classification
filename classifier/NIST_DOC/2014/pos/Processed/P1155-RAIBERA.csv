,sentence,label,data
0,The Correlation between Cluster Hypothesis Tests and the Effectiveness of Cluster-Based Retrieval,null,null
1,Fiana Raiber fiana@tx.technion.ac.il,null,null
2,Oren Kurland kurland@ie.technion.ac.il,null,null
3,"Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel",null,null
4,ABSTRACT,null,null
5,"We present a study of the correlation between the extent to which the cluster hypothesis holds, as measured by various tests, and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval. We show that the correlation can be affected by several factors, such as the size of the result list of the most highly ranked documents that is analyzed. We further show that some cluster hypothesis tests are often negatively correlated with one another. Moreover, in several settings, some of the tests are also negatively correlated with the relative effectiveness of cluster-based retrieval.",null,null
6,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
7,"Keywords: cluster hypothesis, cluster-based retrieval",null,null
8,1. INTRODUCTION,null,null
9,"The cluster hypothesis states that ""closely associated documents tend to be relevant to the same requests"" [19]. The hypothesis plays a central role in information retrieval. Various tests were devised for estimating the extent to which the hypothesis holds [5, 20, 3, 17]. Furthermore, inspired by the hypothesis, document retrieval methods that utilize document clusters were proposed (e.g., [10, 11, 6, 7, 15]).",null,null
10,"There are, however, only a few reports regarding the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval with respect to document-based retrieval [20, 3, 13]. Some of these are contradictory: while it was initially argued that Voorhees' nearest neighbor cluster hypothesis test is not correlated with retrieval effectiveness [20], it was later shown that this test is actually a good indicator for the effectiveness of a specific cluster-based retrieval method [13].",null,null
11,"The aforementioned reports focused on a single cluster hypothesis test (the nearest neighbor test), used a specific retrieval method which is not state-of-the-art and were evaluated using small documents collections which were mostly composed of news articles. Here, we analyze the correla-",null,null
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609533.",null,null
13,"tion between cluster hypothesis tests and the relative effectiveness of cluster-based retrieval with respect to documentbased retrieval using a variety of tests, state-of-the-art retrieval methods and collections.",null,null
14,"We found that (i) in contrast to some previously reported results [3], cluster hypothesis tests are in many cases either negatively correlated with one another or not correlated at all; (ii) cluster hypothesis tests are often negatively correlated or not correlated at all with the relative effectiveness of cluster-based retrieval methods; (iii) the correlation between the tests and the relative effectiveness of the retrieval methods is affected by the number of documents in the result list of top-retrieved documents that is analyzed; and, (iv) the type of the collection (i.e., Web vs. newswire) is a strong indicator for the effectiveness of cluster-based retrieval when applied over short retrieved document lists.",null,null
15,2. RELATED WORK,null,null
16,"The correlation between cluster hypothesis tests was studied using small document collections, most of which were composed of news articles [3]. We, on the other hand, use a variety of both (small scale) newswire and (large scale) Web collections. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval methods was studied using only a single test -- Voorhees' nearest neighbor test [20, 13]. Each study also focused on a different cluster-based retrieval method. This resulted in contradictory findings. In contrast, we use several cluster hypothesis tests and retrieval methods.",null,null
17,"Document clusters can be created either in a query dependent manner, i.e., from the list of documents most highly ranked in response to a query [21] or in a query independent fashion from all the documents in a collection [5, 10]. In this paper we study the correlation between cluster hypothesis tests and the effectiveness of retrieval methods that utilize query dependent clusters [6, 7, 15]. The reason is threefold. First, these retrieval methods were shown to be highly effective. Second, we use for experiments large-scale document collections; clustering all the documents in these collections is computationally difficult. Third, the cluster hypothesis was shown to hold to a (much) larger extent when applied to relatively short retrieved lists than to longer ones or even to the entire corpus [18].",null,null
18,3. CLUSTER HYPOTHESIS TESTS AND,null,null
19,CLUSTER-BASED RETRIEVAL,null,null
20,To study the correlation between tests measuring the extent to which the cluster hypothesis holds and the effective-,null,null
21,1155,null,null
22,"ness of cluster-based retrieval methods, we use several tests and (state-of-the-art) retrieval methods.",null,null
23,"Let Dinit be an initial list of n documents retrieved in response to query q using some retrieval method. The retrieval method scores document d by score(q, d). (Details of the scoring function used in our experiments are provided in Section 4.) All the cluster hypothesis tests and the retrieval methods that we consider operate on the documents in Dinit. In what follows we provide a short description of these tests and methods.",null,null
24,Cluster hypothesis tests. The first test that we study con-,null,null
25,ceptually represents the Overlap test [5]. The test is based,null,null
26,"on the premise that, on average, the similarity between two",null,null
27,relevant documents should be higher than the similarity be-,null,null
28,"tween a relevant and a non-relevant document. Formally,",null,null
29,let R(Dinit) be the set of relevant documents in Dinit and,null,null
30,N (Dinit) the set of non-relevant documents; nR and nN de-,null,null
31,"note the number of documents in R(Dinit) and N (Dinit), re-",null,null
32,spectively. The score assigned by the Overlap test to Dinit is,null,null
33,1,null,null
34,P,null,null
35,"( ) nR(nR-1) di,dj R(Dinit),di,dj",null,null
36,"sim(di,dj )+sim(dj ,di)",null,null
37,"; sim(·, ·) is 1 ( ) nRnN",null,null
38,"P di R(Dinit),dj N (Dinit)",null,null
39,"sim(di,dj )+sim(dj ,di)",null,null
40,an inter-text similarity measure described in Section 4.1 This,null,null
41,score is averaged over all the tested queries for which nR and,null,null
42,nN are greater than 1 to produce the final test score.,null,null
43,We next consider Voorhees' Nearest Neighbor test (NN),null,null
44,[20]. For each relevant document di ( R(Dinit)) we count the number of relevant documents among di's k - 1 nearest neighbors in Dinit; k is a free parameter. These counts are,null,null
45,averaged over all the relevant documents retrieved for all the,null,null
46,tested queries. The nearest neighbors of di are determined,null,null
47,"based on sim(di, dj).",null,null
48,The Density test [3] is defined here as the ratio between,null,null
49,the average number of unique terms in the documents in,null,null
50,Dinit and the number of terms in the vocabulary. The un-,null,null
51,"derlying assumption is, as for the tests from above, that rel-",null,null
52,evant documents are more similar to each other than they,null,null
53,"are to non-relevant documents. Now, if the number of terms",null,null
54,"that are shared by documents in the initial list is high, then",null,null
55,presumably relevant documents could be more easily distin-,null,null
56,guished from non-relevant ones.,null,null
57,We also explore the Normalized Mean Reciprocal Distance,null,null
58,test (nMRD) [17]. The test is based on using a complete,null,null
59,relevant documents graph. Each vertex in the graph rep-,null,null
60,resents a different document in R(Dinit); each pair of ver-,null,null
61,tices is connected with an edge. The edge weight repre-,null,null
62,sents the distance between the documents. The distance,null,null
63,"between documents di and dj is defined as the rank of dj in a ranking of all the documents d  Dinit (d ,"" di) that is created using sim(di, d); the rank of the highest""",null,null
64,ranked document is 1. The score assigned by the nMRD test,null,null
65,to D is P ; init,null,null
66,1,null,null
67,nR,null,null
68,"PnR i,1",null,null
69,1 log2 i+1,null,null
70,"1 di,dj R(Dinit),di,""dj spd(di,dj )""",null,null
71,"spd(di, dj) is the shortest path distance between di and dj",null,null
72,in the graph. This score is averaged over all tested queries,null,null
73,for which nR > 1 to produce the final nMRD score.,null,null
74,Cluster-based document retrieval methods. Let C l(Dinit),null,null
75,be the set of clusters created from the documents in Dinit using some clustering algorithm. All the cluster-based re-,null,null
76,"1We use both sim(di, dj) and sim(dj, di) as the similarity measure that was used for experiments is asymmetric. Further details are provided in Section 4.",null,null
77,trieval methods that we consider re-rank the documents in,null,null
78,Dinit using information induced from clusters in C l(Dinit).,null,null
79,The interpolation-f method (Interpf in short) [6] directly,null,null
80,ranks the documents in Dinit. The score assigned to docu-,null,null
81,ment,null,null
82,d,null,null
83,(,null,null
84,Dinit ),null,null
85,is,null,null
86," score(q,d)",null,null
87,P di Dinit,null,null
88,"score(q,di )",null,null
89,+,null,null
90,(1,null,null
91,-,null,null
92,),null,null
93,; P cC,null,null
94,l(Dinit ),null,null
95,"sim(q,c)sim(c,d)",null,null
96,P di Dinit,null,null
97,P cC l(Dinit),null,null
98,"sim(q,c)sim(c,di )",null,null
99,is,null,null
100,a,null,null
101,free,null,null
102,parameter.,null,null
103,The cluster-based retrieval methods that we consider next,null,null
104,"are based on a two steps procedure. First, the clusters in",null,null
105,C l(Dinit) are ranked based on their presumed relevance to,null,null
106,"the query. Then, the ranking of clusters is transformed to a",null,null
107,ranking over the documents in Dinit by replacing each cluster,null,null
108,with its constituent documents (and omitting repeats).,null,null
109,"The AMean and GMean methods [12, 16] rank the clus-",null,null
110,ters based on the arithmetic and geometric mean of the orig-,null,null
111,"inal retrieval scores of the documents in a cluster, respec-",null,null
112,"tively. Specifically, AMean assigns cluster c with the score",null,null
113,1 |c|,null,null
114,P,null,null
115,dc,null,null
116,"score(q,",null,null
117,d),null,null
118,where,null,null
119,|c|,null,null
120,is,null,null
121,the,null,null
122,number,null,null
123,of,null,null
124,documents,null,null
125,in,null,null
126,1,null,null
127,c.,null,null
128,The,null,null
129,score,null,null
130,assigned,null,null
131,to,null,null
132,c,null,null
133,by,null,null
134,GMean,null,null
135,is,null,null
136,Q,null,null
137,dc,null,null
138,"score(q,",null,null
139,d),null,null
140,|c|,null,null
141,.,null,null
142,Another cluster ranking method that we use is Clus-,null,null
143,tRanker [7]. ClustRanker assigns cluster c with the score,null,null
144," + cent(c)sim(q,c)",null,null
145,P ci C,null,null
146,l(Dinit ),null,null
147,"cent(ci )sim(q,ci )",null,null
148,(1-),null,null
149,P dc,null,null
150,"score(q,d)sim(c,d)cent(d)",null,null
151,P ci C l(Dinit),null,null
152,P dci,null,null
153,"score(q,d)sim(ci ,d)cent(d)",null,null
154,;,null,null
155,cent(d),null,null
156,and,null,null
157,cent(c) are estimates of the centrality of a document d in,null,null
158,"Dinit and that of a cluster c in C l(Dinit), respectively. These",null,null
159,estimates are computed using a PageRank algorithm that,null,null
160,"utilizes inter-document and inter-cluster similarities [9, 7].",null,null
161,We also use the recently proposed state-of-the-art,null,null
162,ClustMRF cluster ranking method [15]. ClustMRF uses,null,null
163,Markov Random Fields which enable to integrate various,null,null
164,types of cluster-relevance evidence.,null,null
165,4. EXPERIMENTAL SETUP,null,null
166,"Experiments were conducted using the datasets specified in Table 1. WSJ, AP and ROBUST are small (mainly) newswire collections. WT10G is a small Web collection and GOV2 is a crawl of the .gov domain. CW09B is the Category B of the ClueWeb09 collection and CW09A is its Category A English part. We use two additional settings, CW09BF and CW09AF, for categories B and A [2], respectively. These settings are created by filtering out from the initial ranking documents that were assigned with a score below 50 and 70 by Waterloo's spam classifier for CW09B and CW09A, respectively. Thus, the initial lists, Dinit, used for these two settings presumably contain fewer spam documents.",null,null
167,corpus,null,null
168,# of docs # of unique terms data,null,null
169,queries,null,null
170,WSJ AP,null,null
171,"173,252 242,918",null,null
172,"ROBUST 528,155",null,null
173,WT10G GOV2 CW09B CW09BF CW09A CW09AF,null,null
174,"1,692,096 25,205,179 50,220,423",null,null
175,"503,903,810",null,null
176,"186,689 259,501 663,700 4,999,228 39,251,404 87,262,413",null,null
177,"507,500,897",null,null
178,Disks 1-2 Disks 1-3,null,null
179,Disks 4-5 (-CR),null,null
180,WT10g GOV2,null,null
181,"151-200 51-150 301-450, 600-700 451-550 701-850",null,null
182,ClueWeb09 Cat. B 1-200,null,null
183,ClueWeb09 Cat. A 1-200,null,null
184,Table 1: TREC data used for experiments.,null,null
185,1156,null,null
186,The Indri toolkit was used for experiments2. Titles of,null,null
187,topics served for queries. We applied Krovetz stemming to,null,null
188,documents and queries. Stopwords were removed only from,null,null
189,queries using INQUERY's list [1].,null,null
190,We use the nearest neighbor clustering algorithm to cre-,null,null
191,"ate the set of clusters C l(Dinit) [4]. A cluster is created from each document di  Dinit. The cluster contains di and the k - 1 documents dj  Dinit (dj , di) with the high-",null,null
192,"est sim(di, dj). We set k , 5. Recall that k is also the",null,null
193,number of nearest neighbors in the NN cluster hypothesis,null,null
194,test. Using such small overlapping clusters was shown to be,null,null
195,"highly effective, with respect to other clustering schemes, for",null,null
196,"cluster-based retrieval [4, 11, 7, 14, 15].",null,null
197,"The similarity between texts x and y, sim(x, y), is defined",null,null
198,as,null,null
199," exp -CE

pDx ir[0](·)",null,null
200,pDy ir[µ],null,null
201, (·) ;,null,null
202,CE,null,null
203,is,null,null
204,the,null,null
205,cross,null,null
206,en-,null,null
207,tropy measure and pDz ir[µ](·) is the Dirichlet-smoothed (with,null,null
208,the smoothing parameter µ) unigram language model in-,null,null
209,"duced from text z [8]. We set µ , 1000 in our experi-",null,null
210,ments [22]. This similarity measure was found to be highly,null,null
211,"effective, specifically for measuring inter-document similar-",null,null
212,"ities, with respect to other measures [9]. The measure is",null,null
213,"used to create Dinit -- i.e., score(q, d) d,""ef sim(q, d) --3 and""",null,null
214,"to compute similarities between the query, documents and",null,null
215,clusters. We represent a cluster by the concatenation of its,null,null
216,"constituent documents [10, 6, 8, 7]. Since we use unigram",null,null
217,language models the similarity measure is not affected by,null,null
218,the concatenation order.,null,null
219,To study the correlation between two cluster hypothesis,null,null
220,"tests, we rank the nine experimental settings (WSJ, AP, RO-",null,null
221,"BUST, WT10G, GOV2 and the ClueWeb09 settings) based",null,null
222,on the score assigned to them by each of the tests. Kendall's-,null,null
223, correlation between the rankings of experimental settings,null,null
224,is the estimate for the correlation between the tests. We note,null,null
225,that Kendall's- is a rank correlation measure that does not,null,null
226,depend on the actual scores assigned to the settings by the,null,null
227,tests. Kendall's- ranges from -1 to +1 where -1 represents,null,null
228,"perfect negative correlation, +1 represents perfect positive",null,null
229,"correlation, and 0 means no correlation.",null,null
230,The correlation between a cluster hypothesis test and the,null,null
231,relative effectiveness of a cluster-based retrieval method is,null,null
232,also measured using Kendall's- . The experimental settings,null,null
233,are ranked with respect to a cluster-based retrieval method,null,null
234,by the performance improvement it posts over the original,null,null
235,"document-based ranking. Specifically, the ratio between the",null,null
236,Mean Average Precision at cutoff n (MAP@n) of the rank-,null,null
237,ing induced by the method and the MAP@n of the initial,null,null
238,ranking is used; n is the number of documents in Dinit.,null,null
239,"The free-parameter values of Interpf, ClustRanker and",null,null
240,ClustMRF were set using 10-fold cross validation. Query,null,null
241,IDs were used to create the folds; MAP@n served for the,null,null
242,optimization criterion in the learning phase. The value of,null,null
243, which is used in Interpf and ClustRanker is selected from,null,null
244,"{0, 0.1, . . . , 1}. To compute the document and cluster cen-",null,null
245,"trality estimates in ClustRanker, the dumping factor and the",null,null
246,number of nearest neighbors that are used in the PageRank,null,null
247,"algorithm were selected from {0.1, 0.2, . . . , 0.9} and {5, 10, 20,",null,null
248,"30, 40, 50}, respectively. The implementation of ClustMRF",null,null
249,follows that in [15].,null,null
250,"2www.lemurproject.org/indri 3Thus, the initial ranking is induced by a standard languagemodel-based approach.",null,null
251,"n , 50 n , 100 n , 250 n , 500",null,null
252,Overlap NN Density nMRD,null,null
253,Overlap NN Density nMRD,null,null
254,Overlap NN Density nMRD,null,null
255,Overlap NN Density nMRD,null,null
256,Overlap,null,null
257,1.000 -0.171 -0.778,null,null
258,0.278,null,null
259,1.000 -0.354 -0.833 -0.222,null,null
260,1.000 -0.329 -0.611 -0.556,null,null
261,1.000 -0.588 -0.778 -0.611,null,null
262,NN,null,null
263,-0.171 1.000 0.229,null,null
264,-0.229,null,null
265,-0.354 1.000 0.412 0.000,null,null
266,-0.329 1.000 0.569 0.329,null,null
267,-0.588 1.000 0.650 0.402,null,null
268,Density,null,null
269,-0.778 0.229 1.000,null,null
270,-0.056,null,null
271,-0.833 0.412 1.000 0.389,null,null
272,-0.611 0.569 1.000 0.722,null,null
273,-0.778 0.650 1.000 0.722,null,null
274,nMRD,null,null
275,0.278 -0.229 -0.056,null,null
276,1.000,null,null
277,-0.222 0.000 0.389 1.000,null,null
278,-0.556 0.329 0.722 1.000,null,null
279,-0.611 0.402 0.722 1.000,null,null
280,Table 2: The correlation between cluster hypothesis tests (measured in terms of Kendall's- ). n is the number of documents in Dinit.,null,null
281,5. EXPERIMENTAL RESULTS,null,null
282,"The correlations between the cluster hypothesis tests are presented in Table 2 for different values of n. With the exception of the Overlap test, we can see that the correlation between all other pairs of tests increases with increasing values of n, but can be negative or zero for low values of n. The Overlap test is negatively correlated with all the other tests across almost all values of n.",null,null
283,"A decent positive correlation is attained between Density and NN for n  100. For n  250 a decent positive correlation is also attained between nMRD and NN. While nMRD is a global test that considers the relations between all the documents in Dinit, NN is a more local test that only considers the relations between a document and its nearest neighbors.",null,null
284,"For n  {250, 500}, nMRD and Density are the most correlated tests. This finding is surprising since these tests are based on completely different properties of the initial list. While nMRD is based on directly measuring interdocument similarities, the Density test is based on the number of unique terms in the documents which presumably attests to the ability to differentiate between relevant and non-relevant documents.",null,null
285,Cluster-based document retrieval methods. We next,null,null
286,"study the correlation between the cluster hypothesis tests and the relative effectiveness of cluster-based retrieval methods. For reference, we report the correlation numbers with respect to a ranking of the experimental settings induced by the size of the corresponding collections (Size). The results are presented in Table 3.",null,null
287,"We observe a negative correlation between the Density test and all five cluster-based retrieval methods for n ,"" 50. This finding can be explained as follows. First, the size of the collection is positively correlated with the number of terms in the vocabulary. (Refer back to Table 1.) Now, by definition, this number is negatively correlated with Density. Second, the size of the collection is positively correlated with the effectiveness of cluster-based retrieval methods as observed in Table 3 for the Size correlations for n "","" 50. We note that here, the Web collections are larger than the newswire collections and are in general noisier. Thus, we conclude that the type of the collection, i.e., Web vs. newswire, can have""",null,null
288,1157,null,null
289,"n , 50 n , 100 n , 250 n , 500",null,null
290,Overlap NN,null,null
291,Interpf AMean,null,null
292,0.761 -0.029 0.111 0.000,null,null
293,GMean,null,null
294,0.333 -0.229,null,null
295,ClustRanker 0.778 -0.057,null,null
296,ClustMRF,null,null
297,0.889 -0.171,null,null
298,Interpf,null,null
299,0.500 0.000,null,null
300,AMean,null,null
301,-0.222 -0.118,null,null
302,GMean,null,null
303,-0.333 -0.059,null,null
304,ClustRanker 0.611 0.059,null,null
305,ClustMRF,null,null
306,0.722 -0.295,null,null
307,Interpf,null,null
308,-0.254 0.486,null,null
309,AMean GMean ClustRanker ClustMRF,null,null
310,-0.611 -0.333 -0.056,null,null
311,0.500,null,null
312,0.150 0.210 0.150 -0.210,null,null
313,Interpf AMean GMean ClustRanker ClustMRF,null,null
314,-0.111 -0.444 -0.444 -0.222,null,null
315,0.389,null,null
316,0.155 0.340 0.279 0.402 -0.155,null,null
317,Density nMRD Size,null,null
318,-0.648 -0.333 -0.444 -0.667 -0.778,null,null
319,0.028 0.278 -0.056 0.167 0.167,null,null
320,0.725 0.286 0.457 0.743 0.857,null,null
321,-0.333 0.056 0.167,null,null
322,-0.444 -0.556,null,null
323,-0.167 0.556 0.333,null,null
324,-0.278 -0.056,null,null
325,0.400 -0.114 -0.229,null,null
326,0.514 0.629,null,null
327,0.254 0.333 0.389 0.000 -0.333,null,null
328,0.111 0.444 0.444 0.333 -0.167,null,null
329,0.028 0.500 0.556 -0.056 -0.278,null,null
330,0.167 0.500 0.611 0.278 0.000,null,null
331,-0.203 -0.400 -0.400,null,null
332,0.057 0.400,null,null
333,-0.057 -0.457 -0.457 -0.286,null,null
334,0.229,null,null
335,"Table 3: The correlation between cluster hypothesis tests and the relative effectiveness of cluster-based retrieval methods. The Size ""test"" ranks experimental settings by the number of documents in the collections. n is the number of documents in Dinit.",null,null
336,an influence on the effectiveness of cluster-based retrieval methods for short retrieved lists.,null,null
337,"Another observation that we make based on Table 3 is that for n , 50 the correlation attained for the nMRD and NN tests is often lower than that attained for Overlap and Size. The relatively high positive correlation attained for the Overlap and the Size tests for n ,"" 50 suggests that these tests are very strong indicators for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval when applied to short retrieved lists. For larger values of n, NN and nMRD, as well as Density, start to post more positive correlations while the reverse holds for Overlap and Size.""",null,null
338,"We can also see that none of the retrieval methods is correlated only positively or only negatively with all the tests for any fixed value of n. In addition, only in a few cases a test is either only positively or only negatively correlated with all the retrieval methods for a fixed value of n. Thus, we conclude that the correlation between the effectiveness of a retrieval method and a cluster hypothesis test can substantially vary (both positively and negatively) across retrieval methods and tests.",null,null
339,6. CONCLUSIONS,null,null
340,"We studied the correlation between cluster hypothesis tests and cluster-based retrieval effectiveness. We showed that the correlation between the two depends on the specific tests and methods that are used, and on the number of documents in the result list that is analyzed. We also showed that the type of the collection, i.e., Web or newswire, can be a stronger indicator for the relative effectiveness of cluster-based retrieval with respect to document-based retrieval, for short retrieved lists, than tests designed for estimating the extent to which the cluster hypothesis holds.",null,null
341,Acknowledgments We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.,null,null
342,7. REFERENCES,null,null
343,"[1] J. Allan, M. E. Connell, W. B. Croft, F.-F. Feng, D. Fisher, and X. Li. INQUERY and TREC-9. In Proceedings of TREC-9, pages 551­562, 2000.",null,null
344,"[2] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal, 14(5):441­465, 2011.",null,null
345,"[3] A. El-Hamdouchi and P. Willett. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science, 13:361­365, 1987.",null,null
346,"[4] A. Griffiths, H. C. Luckhurst, and P. Willett. Using interdocument similarity information in document retrieval systems. Journal of the American Society for Information Science, 37(1):3­11, 1986.",null,null
347,"[5] N. Jardine and C. J. van Rijsbergen. The use of hierarchic clustering in information retrieval. Information Storage and Retrieval, 7(5):217­240, 1971.",null,null
348,"[6] O. Kurland. Re-ranking search results using language models of query-specific clusters. Journal of Information Retrieval, 12(4):437­460, August 2009.",null,null
349,"[7] O. Kurland and E. Krikon. The opposite of smoothing: A language model approach to ranking query-specific document clusters. Journal of Artificial Intelligence Research, 41:367­395, 2011.",null,null
350,"[8] O. Kurland and L. Lee. Clusters, language models, and ad hoc information retrieval. ACM Transactions on information systems, 27(3), 2009.",null,null
351,"[9] O. Kurland and L. Lee. PageRank without hyperlinks: Structural reranking using links induced by language models. ACM Transactions on information systems, 28(4):18, 2010.",null,null
352,"[10] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In Proceedings of SIGIR, pages 186­193, 2004.",null,null
353,"[11] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",null,null
354,"[12] X. Liu and W. B. Croft. Evaluating text representations for retrieval of the best group of documents. In Proceedings of ECIR, pages 454­462, 2008.",null,null
355,"[13] S.-H. Na, I.-S. Kang, and J.-H. Lee. Revisit of nearest neighbor test for direct evaluation of inter-document similarities. In Proceedings of ECIR, pages 674­678, 2008.",null,null
356,"[14] F. Raiber and O. Kurland. Exploring the cluster hypothesis, and cluster-based retrieval, over the web. In Proceedings of CIKM, pages 2507­2510, 2012.",null,null
357,"[15] F. Raiber and O. Kurland. Ranking document clusters using markov random fields. In Proceedings of SIGIR, pages 333­342, 2013.",null,null
358,"[16] J. Seo and W. B. Croft. Geometric representations for multiple documents. In Proceedings of SIGIR, pages 251­258, 2010.",null,null
359,"[17] M. D. Smucker and J. Allan. A new measure of the cluster hypothesis. In Proceedings of ICTIR, pages 281­288, 2009.",null,null
360,"[18] A. Tombros, R. Villa, and C. van Rijsbergen. The effectiveness of query-specific hierarchic clustering in information retrieval. Information Processing and Management, 38(4):559­582, 2002.",null,null
361,"[19] C. J. van Rijsbergen. Information Retrieval. Butterworths, second edition, 1979.",null,null
362,"[20] E. M. Voorhees. The cluster hypothesis revisited. In Proceedings of SIGIR, pages 188­196, 1985.",null,null
363,"[21] P. Willett. Query specific automatic document classification. International Forum on Information and Documentation, 10(2):28­32, 1985.",null,null
364,"[22] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proceedings of SIGIR, pages 334­342, 2001.",null,null
365,1158,null,null
366,,null,null

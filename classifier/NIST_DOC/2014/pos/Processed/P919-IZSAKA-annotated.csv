,sentence,label,data
,,,
0,The Search Duel: A Response to a Strong Ranker,null,null
,,,
1,Peter Izsak,null,null
,,,
2,"Technion, Israel",null,null
,,,
3,peteriz@tx.technion.ac.il,null,null
,,,
4,Fiana Raiber,null,null
,,,
5,"Technion, Israel",null,null
,,,
6,fiana@tx.technion.ac.il,null,null
,,,
7,Oren Kurland,null,null
,,,
8,Moshe Tennenholtz,null,null
,,,
9,"Technion, Israel",null,null
,,,
10,Microsoft Research and,null,null
,,,
11,kurland@ie.technion.ac.il,null,null
,,,
12,"Technion, Israel",null,null
,,,
13,moshet@ie.technion.ac.il,null,null
,,,
14,ABSTRACT,null,null
,,,
15,"How can a search engine with a relatively weak relevance ranking function compete with a search engine that has a much stronger ranking function? This dual challenge, which to the best of our knowledge has not been addressed in previous work, entails an interesting bi-modal utility function for the weak search engine. That is, the goal is to produce in response to a query a document result list whose effectiveness does not fall much behind that of the strong search engine; and, which is quite different than that of the strong engine. We present a per-query algorithmic approach that leverages fundamental retrieval principles such as pseudofeedback-based relevance modeling. We demonstrate the merits of our approach using TREC data.",null,null
,,,
16,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Retrieval models,null,null
,,,
17,"Keywords: search engine competition, dueling algorithms",null,null
,,,
18,1. INTRODUCTION,null,null
,,,
19,"We revisit the classic ad hoc relevance ranking problem from a competition perspective. Rather than addressing a single ranking system, we consider a duel in which a search problem -- i.e., a query representing an information need -- is presented to two players. One of the players has a relevance ranking function that is considerably ""weaker"" (i.e., less effective) than that of the other player. Yet, his goal is to produce a ranking that is competitive with that created by the player with the stronger ranking function.",null,null
,,,
20,"On the theory side, this type of interaction is modeled in the context of the recently introduced dueling algorithms [9]. Our goal is to introduce a pragmatic manifestation in the context of an adversarial retrieval setting (e.g., the Web).",null,null
,,,
21,"The ranking functions employed by leading Web search engines are remarkably effective. However, there is still much room for improving retrieval effectiveness due to various reasons. Many of these are related to the adversarial",null,null
,,,
22,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609474.",null,null
,,,
23,"nature of the retrieval setting (e.g., search engine optimization efforts). Furthermore, it is impossible for a search engine to index all possible documents in a large-scale and dynamically changing collection such as the Web. This reality provides some hope for a search engine with a relatively weak ranking function to compete with a search engine that has a much stronger ranking function.",null,null
,,,
24,"A potential approach to addressing the duel challenge is to try to explicitly learn the ranking function of the strong search engine. However, this approach falls short in our competitive setting. That is, some of the most important information types utilized by the strong ranker may not be available to the weak ranker; e.g., those based on user engagement information such as clickthrough data.",null,null
,,,
25,"We propose a per-query competitive approach. We let the weak search engine observe the output (i.e., the result list of the most highly ranked documents) of the strong search engine for a query. Using this list, which is treated as a pseudo feedback set, we induce a relevance model [10]. The model is used to modify the ranking of the weak search engine. The modification is based on a bi-modal criteria: retrieval effectiveness (in terms of relevance) and diversification with respect to the results presented by the strong search engine. The motivation for diversification is based on the following realization. Users of the strong search engine would have no incentive to switch to (or consider) the weak engine if they are presented with the same results.",null,null
,,,
26,"Empirical evaluation performed with TREC data attests to the effectiveness of our approach. For example, we show that the approach can be used to boost the retrieval effectiveness of weak rankers to a level competitive with that of strong rankers, while maintaining relatively low overlap with the strong rankers' result lists. The approach also substantially outperforms a highly effective fusion method that merges the results of the strong and weak search engines.",Y,null
,,,
27,"This paper describes a preliminary, and the first (to the best of our knowledge), attempt to address the interesting and practical challenge of a search engine duel. Naturally, an abundance of research challenges, in addition to those we address here, arise. We discuss some of these in Section 5.",null,null
,,,
28,2. RELATED WORK,null,null
,,,
29,"As mentioned, from a theory perspective, the work on dueling algorithms [9] deals with a setting similar to ours. Although the emphasis is on computing minimax strategies, the basic building block is computing the response of one",null,null
,,,
30,919,null,null
,,,
31,"agent to another. However, the model is stylized and does not refer to the realistic search duel we discuss here.",null,null
,,,
32,"There is a large body of work on merging document lists that were retrieved in response to a query from the same corpus [4] or from different corpora [1]. Our use of a relevance model induced from one list to re-rank another list is conceptually reminiscent of work on using inter-document similarities between two lists for re-ranking [11]. However, in contrast to all these results merging approaches which aim to maximize only relevance, our methods are designed to also minimize overlap with the strong engine's result list.",null,null
,,,
33,"There is work on training a ranker for one domain (language) and applying it to another [7, 8]. In contrast, we do not assume different domains or languages and we do not train a ranker but rather use a pseudo-feedback-based relevance model.",null,null
,,,
34,"We note that existing methods for diversifying search results (e.g., [2, 12]) focus on a single retrieved list and on a notion of diversification -- i.e., coverage of query aspects -- which is different than that we address here; that is, the overlap with another result list.",null,null
,,,
35,3. SEARCH ENGINE RESPONSES,null,null
,,,
36,FRAMEWORK,null,null
,,,
37,"Let q be a query which is fixed here and after. Let C be a corpus of documents upon which two search engines perform a search in response to q. One of the search engines, henceforth referred to as strong, is assumed to have a more effective relevance ranking function than that of the other -- the so called weak engine. Specifically, the ranking induced by the strong engine in response to q is assumed to be of higher effectiveness than that induced by the weak engine.",null,null
,,,
38,"We use L[sntr] ong and L[wne]ak to refer to the lists of the n documents that are the most highly ranked by the strong and weak engines, respectively. The goal we pursue is devising an effective response strategy for the weak engine, given that it has access to the list L[sntr] ong of the strong engine.1 By response we mean producing a new result list, L[wne]ak;response, composed of n documents, that will replace the original list, L[wne]ak; yet, L[wne]ak can be used to produce the response.",null,null
,,,
39,"A key question is what makes a response ""effective"". Obviously, L[wne]ak;response should be of the highest possible effectiveness, preferably, not significantly lower than that of L[sntr] ong. Supposedly, then, a highly effective response is setting L[wne]ak;response d,""ef L[sntr] ong; that is, having the weak engine replicate the result list produced by the strong engine. However, assuming that the strong search engine already has a well established, and wide, base of users, such a response is not likely to result in any incentive for users to switch engines. Therefore, the second criterion we set for an effective response is that the overlap, in terms of shared documents, between L[wne]ak;response and L[sntr] ong will be minimal; i.e., the goal is to differentiate the weak engine from the strong engine.""",null,null
,,,
40,"In summary, the weak engine should produce a result list that is as diverse as possible, and competitive in terms of effectiveness, with respect to the result list produced by the",null,null
,,,
41,"1In practical settings, the weak engine can potentially record, from time to time, the results produced by the strong search engine, specifically, in response to common queries.",null,null
,,,
42,"strong engine. In doing so, the weak engine can use its original list L[wne]ak and that of the strong engine, L[sntr] ong.",null,null
,,,
43,3.1 Relevance Modeling as a Basis for Response Strategies,null,null
,,,
44,"The basic assumption underlying the strategies that we employ below is that there are relevant documents in L[wne]ak that are not in L[sntr] ong. The reason could be, for example, a different coverage of the indexes of the two engines.",null,null
,,,
45,"Yet, it is not necessarily the case that these documents are",null,null
,,,
46,ranked high enough due to the relatively weak relevance,null,null
,,,
47,"ranking function of the weak engine. Thus, we use a relevance language model R [10] induced from L[skt]rong -- the k ( n) documents that are the highest ranked in L[sntr] ong -- to re-rank L[wne]ak; L[wne]ak;re-rank denotes the resultant ranked list. As R reflects a model of relevance of the strong engine",null,null
,,,
48,"with respect to the query q, we assume that the ranking of L[wne]ak;re-rank is of higher effectiveness than that of L[wne]ak. We provide empirical support to this assumption in Section 4. The re-ranking of L[wne]ak using R is based on the cross entropy between R and the language models induced from the",null,null
,,,
49,documents. Details regarding (relevance) language model,null,null
,,,
50,induction are provided in Section 4.1.,null,null
,,,
51,"In what follows we present several strategies of producing the final result list of the weak engine, L[wne]ak;response, using the lists L[wne]ak;re-rank and L[sntr] ong.",null,null
,,,
52,3.2 Response Strategies,null,null
,,,
53,"The first response strategy, WeakReRank, is simply using L[wne]ak;re-rank for L[wne]ak;response. The source for diversity with the strong list L[sntr] ong is the assumed existence of documents in L[wne]ak that are not in L[sntr] ong. The presumably improved effectiveness of L[wne]ak;re-rank is due to the way it was created; that is, re-ranking L[wne]ak using a relevance model induced from L[sntr] ong.",null,null
,,,
54,The second response strategy is a probabilistic round robin,null,null
,,,
55,"procedure, henceforth referred to as ProbRR. We create L[wne]ak;response top down by scanning the lists L[wne]ak;re-rank and L[sntr] ong also top down. The next document selected for L[wne]ak;response is taken from L[wne]ak;re-rank with probability p and from L[sntr] ong with probability 1-p. (Documents that were already selected are skipped.) Smaller values of p result in more documents taken from L[sntr] ong. Accordingly, the overlap of the final result list L[wne]ak;response with L[sntr] ong can increase and the effectiveness is potentially maintained at the same level as that of L[sntr] ong. Hence, ProbRR enables a certain degree of control over the effectiveness and diversification of L[wne]ak;re-rank with respect to L[sntr] ong using different values of p. However, L[wne]ak;re-rank can contain documents that are also in L[sntr] ong. Thus, higher values of p do not necessarily directly translate to increased diversity with respect to L[sntr] ong.",null,null
,,,
56,"To directly control the level of diversity of L[wne]ak;response with respect to L[sntr] ong, we examine a variant of ProbRR termed ProbResRR -- the third response strategy we propose. Instead of using L[wne]ak;re-rank and L[sntr] ong we use",null,null
,,,
57,920,null,null
,,,
58,"L[wne]ak;re-rank \ L[sntr] ong and L[sntr] ong; i.e., we remove from L[wne]ak;re-rank documents that are in L[sntr] ong and maintain the ranking of the residual documents. We then apply the same procedure described above for ProbRR to create the final result list L[wne]ak;response. Using larger values of p results in increased diversity with respect to L[sntr] ong.",null,null
,,,
59,4. EVALUATION,null,null
,,,
60,4.1 Experimental Setup,null,null
,,,
61,"We used the Web tracks of TREC 2009­2011, henceforth TREC-2009, TREC-2010, and TREC-2011, to create the strong vs. weak search engine setting. We focused on runs submitted for the ClueWeb09 category B collection which is composed of around 50 million documents.",Y,null
,,,
62,"We randomly selected 30 pairs of runs from all those submitted and which contain at least 1000 documents as results for each query in the track. In each pair of runs, the result lists of the run whose MAP (@1000) is higher serve for the lists of the strong engine, L[sntr] ong, and those of the run with the lower MAP serve for the lists of the weak engine, L[wne]ak. Each result list contains n ,"" 1000 documents. We report average performance over the 30 samples. Thus, here, the strong and weak engines are represented by """"averages"""" over runs of which one is on average more effective than the other.""",null,null
,,,
63,Titles of TREC topics serve for queries. Stopwords on the INQUERY list were removed from queries but not from documents. The Indri toolkit (www.lemurproject.org/indri) was used for experiments.,Y,null
,,,
64,"To evaluate retrieval performance, we use MAP (@1000) and NDCG (@20). Statistically significant differences of performance, computed over the 30 pairs of runs, were determined using the two-tailed paired t-test at a 95% confidence level. To measure the diversity of the result list of the weak engine with respect to that of the strong engine, we use the overlap (i.e., number of shared documents) at the top ten (OV@10) and twenty (OV@20) ranks of the two lists.",null,null
,,,
65,"We use Dirichlet-smoothed document language models with the smoothing parameter set to 1000. For the relevance model R, we use the rank-based RM3 model [5] which is constructed from the top k (,"" 10) documents in the strong engine's result list. (We use ranks rather than retrieval scores as the latter are not assumed to be known.) The number of terms used by RM3, and its query anchoring parameter, are set to the default values of 50 and 0.5, respectively. Using this (under optimized) default parameter setting allows to demonstrate the potential of using the relevance modeling idea as a basis for producing responses.""",null,null
,,,
66,"As a reference comparison response strategy we use the highly effective CombMNZ fusion method [6] to merge the result lists of the weak and strong engines. Document scores induced from ranks, as suggested in [3], are used in CombMNZ. As all other fusion methods, CombMNZ addresses (explicitly) only one of the two criteria for effective response strategy -- i.e., retrieval effectiveness.",null,null
,,,
67,4.2 Experimental Results,null,null
,,,
68,The performance numbers of all methods are presented in Table 1. We use ProbRR(p) and ProbResRR(p) to indicate that the ProbRR and ProbResRR response strategies were used with the probability parameter p.,null,null
,,,
69,"Table 1 shows that Strong is much more effective than Weak for both MAP and NDCG; the differences are substantial and statistically significant for all experimental settings. Furthermore, the overlap between Strong and Weak, as measured by OV@10 and OV@20, is low. Thus, the experimental setting we used adheres to the problem definition: (i) the strong search engine is (much) stronger in terms of retrieval effectiveness, and (ii) the overlap between the result lists of the strong and weak search engines is not high.",null,null
,,,
70,"We also see in Table 1 that WeakReRank is quite an effective response strategy; specifically, in comparison to a highly effective fusion method (CombMNZ) in terms of both retrieval effectiveness and overlap at top ranks with Strong. WeakReRank's retrieval effectiveness can be statistically significantly worse than that of Strong. However, WeakReRank outperforms Weak for MAP and NDCG, with all the improvements being statistically significant. Although the overlap at top ranks of WeakReRank with Strong is larger than that of Weak, it is still quite low with respect to that of the other strategies considered. These findings attest to the effectiveness of using relevance modeling based on the result list of the strong search engine so as to re-rank the result list of the weak search engine.",null,null
,,,
71,"Table 1 also shows that ProbRR is a highly effective response strategy in many cases. Evidently, the balance between retrieval effectiveness and overlap with Strong can be effectively controlled via the parameter p. Although in terms of overlap with Strong, ProbRR is somewhat less effective than WeakReRank and CombMNZ, in terms of retrieval effectiveness it is substantially better than the two and often better than Strong.",null,null
,,,
72,"It is also evident in Table 1 that ProbResRR is less effective than ProbRR for MAP and NDCG. However, the overlap numbers for ProbResRR are lower than those for ProbRR. This finding is not surprising because ProbRR uses the re-ranked list of the weak engine while ProbResRR uses the residual documents in the same list that remain after the removal of documents that also appear in the result list of the strong engine.",null,null
,,,
73,The effectiveness-diversity tradeoff. We next study the,null,null
,,,
74,"effect of the parameter p used in ProbRR and ProbResRR on the tradeoff between the retrieval effectiveness of the response list of the weak search engine, as measured using MAP, and its overlap with the result list produced by the strong search engine, measured using OV@10.",null,null
,,,
75,"Figure 1 presents the results of setting p to values in {0, 0.1, . . . , 1}; p ,"" 0 amounts to using only the result list of the strong search engine in ProbRR and ProbResRR, while p "", 1 amounts to using the result list L[wne]ak;re-rank.",null,null
,,,
76,"We can see that for ProbResRR, MAP decreases with increasing values of p. The reason is that fewer documents that appear in the result list of the strong engine are used. Furthermore, ProbRR outperforms ProbResRR for all p > 0. In addition, we see that ProbRR can attain its optimal performance for 0 < p < 1 (i.e., outperform both Strong and WeakReRank) which echoes findings in work on fusion [4].",null,null
,,,
77,"For both ProbRR and ProbResRR, OV@10 decreases with increasing values of p, as fewer documents are selected from the result list of the strong engine. As expected, for the same value of p (> 0), the OV@10 of ProbRR is higher than that of ProbResRR. Yet, for the same value of overlap, the MAP of ProbRR is higher than that of ProbResRR.",null,null
,,,
78,921,null,null
,,,
79,Strong,null,null
,,,
80,Weak WeakReRank,null,null
,,,
81,CombMNZ,null,null
,,,
82,ProbRR(0.2) ProbRR(0.5) ProbRR(0.7) ProbResRR(0.2) ProbResRR(0.5) ProbResRR(0.7),null,null
,,,
83,MAP,null,null
,,,
84,17.1w,null,null
,,,
85,11.4s 16.7w,null,null
,,,
86,13.4sw,null,null
,,,
87,18.3sw 18 7s,null,null
,,,
88,.w 18.6sw 15.4sw 11.2s 8.2sw,null,null
,,,
89,TREC-2009 NDCG OV@10 OV@20,Y,null
,,,
90,26.5w -,null,null
,,,
91,-,null,null
,,,
92,19.5s 30 3s,null,null
,,,
93,.w,null,null
,,,
94,21.6sw,null,null
,,,
95,27.8sw 29.2sw 29.7sw 23.7sw 18.4s 13.9sw,null,null
,,,
96,17 4 .,null,null
,,,
97,35,null,null
,,,
98,40.9,null,null
,,,
99,87.2 67.5 55.4 81.0 52.2 32.6,null,null
,,,
100,19 4 .,null,null
,,,
101,34.5,null,null
,,,
102,43.6,null,null
,,,
103,87.1 67.6 54.8 80.5 52.4 33.2,null,null
,,,
104,MAP,null,null
,,,
105,19.9w,null,null
,,,
106,13.6s 16.5sw,null,null
,,,
107,16.4sw,null,null
,,,
108,20 6s .w,null,null
,,,
109,20.2w 19.4w 17.9sw 12.8s 9.0sw,null,null
,,,
110,TREC-2010 NDCG OV@10 OV@20,Y,null
,,,
111,25 2 .w,null,null
,,,
112,-,null,null
,,,
113,-,null,null
,,,
114,16.9s 19.8sw,null,null
,,,
115,18.9sw,null,null
,,,
116,24.1sw 22.6sw 21.5sw 21.8sw 16.0s 11.7sw,null,null
,,,
117,19 7 .,null,null
,,,
118,29.3,null,null
,,,
119,38.3,null,null
,,,
120,85.2 63.3 49.7 79.4 50.0 29.9,null,null
,,,
121,23 3 .,null,null
,,,
122,30.5,null,null
,,,
123,42.1,null,null
,,,
124,85.0 64.4 50.6 79.8 50.5 29.8,null,null
,,,
125,MAP,null,null
,,,
126,19.3w,null,null
,,,
127,13.1s 17.6sw,null,null
,,,
128,17.2sw,null,null
,,,
129,21.7sw 22 3s,null,null
,,,
130,.w 22.0sw 19.2w 15.4sw 12.0s,null,null
,,,
131,TREC-2011 NDCG OV@10 OV@20,Y,null
,,,
132,28.0w -,null,null
,,,
133,-,null,null
,,,
134,21.6s 27.9w,null,null
,,,
135,24.0sw,null,null
,,,
136,28.9sw 29.6sw 29 7s,null,null
,,,
137,.w 26.2sw 21.7s 17.6sw,null,null
,,,
138,18 6 .,null,null
,,,
139,30.3,null,null
,,,
140,45.1,null,null
,,,
141,86.0 65.6 51.4 80.3 50.4 30.4,null,null
,,,
142,20 7 .,null,null
,,,
143,31.1,null,null
,,,
144,47.5,null,null
,,,
145,86.3 66.1 52.8 80.0 50.2 30.6,null,null
,,,
146,"Table 1: Main result table. The numbers in the parentheses for the ProbRR and ProbResRR strategies are the value of the p parameter. The highest result in a column for MAP and NDCG, and the lowest for OV@10 and OV@20, is boldfaced. `s' and `w' mark statistically significant differences, for MAP and NDCG, with Strong and Weak, respectively.",null,null
,,,
147,MAP OV@10,null,null
,,,
148,MAP OV@10,null,null
,,,
149,MAP OV@10,null,null
,,,
150,TREC-2009 20,null,null
,,,
151,18,null,null
,,,
152,16,null,null
,,,
153,14,null,null
,,,
154,12,null,null
,,,
155,10,null,null
,,,
156,8,null,null
,,,
157,6,null,null
,,,
158,4,null,null
,,,
159,ProbRR (MAP),null,null
,,,
160,ProbRR (OV@10),null,null
,,,
161,2,null,null
,,,
162,ProbResRR (MAP),null,null
,,,
163,ProbResRR (OV@10) 0,null,null
,,,
164,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
,,,
165,p,null,null
,,,
166,100 80 60 40 20 0 1,null,null
,,,
167,TREC-2010 22,Y,null
,,,
168,20,null,null
,,,
169,18,null,null
,,,
170,16,null,null
,,,
171,14,null,null
,,,
172,12,null,null
,,,
173,10,null,null
,,,
174,8,null,null
,,,
175,6,null,null
,,,
176,4,null,null
,,,
177,ProbRR (MAP) ProbRR (OV@10),null,null
,,,
178,2,null,null
,,,
179,ProbResRR (MAP),null,null
,,,
180,ProbResRR (OV@10) 0,null,null
,,,
181,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
,,,
182,p,null,null
,,,
183,100 80 60 40 20 0 1,null,null
,,,
184,TREC-2011 24,Y,null
,,,
185,22,null,null
,,,
186,20,null,null
,,,
187,18,null,null
,,,
188,16,null,null
,,,
189,14,null,null
,,,
190,12,null,null
,,,
191,10,null,null
,,,
192,8,null,null
,,,
193,ProbRR (MAP),null,null
,,,
194,ProbRR (OV@10),null,null
,,,
195,6,null,null
,,,
196,ProbResRR (MAP),null,null
,,,
197,ProbResRR (OV@10) 4,null,null
,,,
198,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9,null,null
,,,
199,p,null,null
,,,
200,100 80 60 40 20 0 1,null,null
,,,
201,Figure 1: The effect of the parameter p on MAP and OV@10. The y-axis on the right of a figure is the range of OV@10. The y-axis on the left of a figure is the range of MAP.,null,null
,,,
202,"Thus, we arrive to the conclusion that tuning the parameter p in ProbRR and ProbResRR helps to effectively control the balance between retrieval effectiveness and overlap (diversity) with the strong engine. Furthermore, ProbRR is a more effective response strategy than ProbResRR as it allows to attain improved level of retrieval effectiveness for the same level of overlap with the strong search engine.",null,null
,,,
203,5. CONCLUSIONS AND FUTURE WORK,null,null
,,,
204,"We presented the first (preliminary) attempt to address the search engine duel problem; namely, how can a search engine with a relatively weak relevance ranking function compete with a search engine with a much stronger relevance ranking function? We devised an algorithmic response framework which consists of several strategies that can be used by the weak search engine. We consider a response to be effective if it results in improved search effectiveness and the search results are different than those presented by the strong engine. Empirical evaluation demonstrated the merits of our response strategies and shed some light on the (relevance) effectiveness-diversity tradeoff embodied in our bi-modal criteria for response effectiveness.",null,null
,,,
205,"Devising additional criteria for response effectiveness, along with developing additional corresponding response strategies, is the first future venue we intend to explore. We also plan on devising response strategies for the strong engine.",null,null
,,,
206,Acknowledgments We thank the reviewers for their comments. This work has been supported by and carried out at the Technion-Microsoft Electronic Commerce Research Center. This work has also been supported in part by Microsoft Research through its Ph.D. Scholarship Program.,null,null
,,,
207,6. REFERENCES,null,null
,,,
208,"[1] J. Callan. Distributed information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 5, pages 127­150. Kluwer Academic Publishers, 2000.",null,null
,,,
209,"[2] J. G. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. of SIGIR, pages 335­336, 1998.",null,null
,,,
210,"[3] G. V. Cormack, C. L. A. Clarke, and S. Bu¨ttcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proc. of SIGIR, pages 758­759, 2009.",null,null
,,,
211,"[4] W. B. Croft. Combining approaches to information retrieval. In W. Croft, editor, Advances in information retrieval, chapter 1, pages 1­36. Kluwer Academic Publishers, 2000.",null,null
,,,
212,"[5] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.",null,null
,,,
213,"[6] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.",null,null
,,,
214,"[7] W. Gao, J. Blitzer, and M. Zhou. Using english information in non-english web search. In Proc. of the 2nd ACM workshop on Improving Non English Web Searching, iNEWS, pages 17­24, 2008.",null,null
,,,
215,"[8] W. Gao, P. Cai, K.-F. Wong, and A. Zhou. Learning to rank only using training data from related domain. In Proc. of SIGIR, pages 162­169, 2010.",null,null
,,,
216,"[9] N. Immorlica, A. T. Kalai, B. Lucier, A. Moitra, A. Postlewaite, and M. Tennenholtz. Dueling algorithms. In Proc. of STOC, pages 215­224, 2011.",null,null
,,,
217,"[10] V. Lavrenko and W. B. Croft. Relevance-based language models. In Proc. of SIGIR, pages 120­127, 2001.",null,null
,,,
218,"[11] L. Meister, O. Kurland, and I. G. Kalmanovich. Re-ranking search results using an additional retrieved list. Information Retrieval, 14(4):413­437, 2011.",null,null
,,,
219,"[12] R. L. T. Santos, C. Macdonald, and I. Ounis. Exploiting query reformulations for web search result diversification. In Proc. of WWW, pages 881­890, 2010.",null,null
,,,
220,922,null,null
,,,
221,,null,null

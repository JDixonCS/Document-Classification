,sentence,label,data
0,Principled Dictionary Pruning for Low-Memory Corpus Compression,null,null
1,Jiancong Tong,null,null
2,Anthony Wirth,null,null
3,Justin Zobel,null,null
4,jctong@nbjl.nankai.edu.cn awirth@unimelb.edu.au jzobel@unimelb.edu.au,null,null
5,"College of Computer and Control Engineering, Nankai University, China Department of Computing and Information Systems, The University of Melbourne, Australia",null,null
6,ABSTRACT,null,null
7,"Compression of collections, such as text databases, can both reduce space consumption and increase retrieval efficiency, through better caching and better exploitation of the memory hierarchy. A promising technique is relative Lempel-Ziv coding, in which a sample of material from the collection serves as a static dictionary; in previous work, this method demonstrated extremely fast decoding and good compression ratios, while allowing random access to individual items. However, there is a trade-off between dictionary size and compression ratio, motivating the search for a compact, yet similarly effective, dictionary.",null,null
8,"In previous work it was observed that, since the dictionary is generated by sampling, some of it (selected substrings) may be discarded with little loss in compression. Unfortunately, simple dictionary pruning approaches are ineffective. We develop a formal model of our approach, based on generating an optimal dictionary for a given collection within a memory bound. We generate measures for identification of low-value substrings in the dictionary, and show on a variety of sizes of text collection that halving the dictionary size leads to only marginal loss in compression ratio. This is a dramatic improvement on previous approaches.",null,null
9,Categories and Subject Descriptors,null,null
10,E.4 [Coding and Information Theory]: [Data compaction and compression]; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval--Search process,null,null
11,Keywords,null,null
12,Corpus compression; string algorithms; retrieval efficiency; optimization,null,null
13,1. INTRODUCTION,null,null
14,"Compression plays a key role in the efficiency of large-scale information retrieval systems such as web search engines [1, 2, 7, 19, 28, 31]. In particular, compression of stored data",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609576.",null,null
16,"can enable both reduced storage demands and improved retrieval speed, through lower data transfer costs and better caching. For web-scale collections, a repository compression scheme must meet several constraints: that documents can be retrieved and decompressed in any order; that memory requirements are reasonable, regardless of the size of the collection; and that new material can be added to the repository. Underlying this, good compression effectiveness must be achieved and decompression speed must be high.",null,null
17,"An approach to compression that meets these goals is relative Lempel-Ziv factorization (RLZ) [9, 15, 30]. In RLZ, the collection text is parsed into a contiguous sequence of fragments, where each fragment is sourced from an external static dictionary. RLZ dramatically outperforms repository adaptations of general-purpose compression approaches, for example based on the Lempel-Ziv (LZ) family [29], as it can exploit global properties of the collection.",null,null
18,"RLZ uses a portion of the to-be-compressed data as the external dictionary [9, 15]. In the dictionary-generation method proposed by Hoobin et al. [9], fixed-size blocks of data (say 1 KB) are sampled from the repository and then concatenated to form the dictionary. In this way, a dictionary of any given size can be generated by simply varying the number and size of samples. With a sufficient number of samples ­ say, a million ­ there is an extremely high likelihood that all common strings are present in the dictionary, and (as we confirm in our experiments reported here) excellent compression can be achieved, easily outperforming, for example, Huffman-based methods [28].",null,null
19,"However, it is also the case that some strings are sampled many times (as would be expected, statistically), meaning that there is extensive redundancy in the dictionary and it is larger than required. Hoobin et al. [10] observed that some parts of the dictionary were rarely, or even never, used. As an illustration, with the first 1 GB of documents in GOV2 [6] as the test collection, and 5% of the collection sampled (with a sample block size of 1 KB) as the test dictionary, we compress the test collection relative to the test dictionary. The reference frequency of each byte in the dictionary is the number of times that byte is referred to by an LZ factor in the compressed collection. Figure 1 visualizes the reference frequency for 32 randomly chosen blocks, and shows that some of the dictionary is indeed little used.",null,null
20,Pruning. We wish to obtain the best possible compression,null,null
21,"performance for a given dictionary size. As shown in previous work [9], and as we show here, larger dictionaries give better compression. But to achieve fast decoding and ran-",null,null
22,283,null,null
23,"ref ,"" [0, 5)""",null,null
24,"ref ,"" [5, 20) ref "","" [20, Inf)""",null,null
25,n-th byte vector,null,null
26,8 12 16 20 24 28 32,null,null
27,14,null,null
28,1 128 256 384 512 640 768 896 1024,null,null
29,n-th byte in the byte vector,null,null
30,"Figure 1: Reference dictionary frequency heat-map for 32 1-KB blocks randomly extracted from a 50 MB RLZ dictionary. The darker the point, the higher the frequency of use in the factorization.",null,null
31,"dom access, RLZ also requires the dictionary to be small enough to reside in memory. In a mobile environment, the limits on transmission speed and storage space make it valuable to keep the dictionary as small as possible, while still maintaining a good compression ratio. Excessive dictionary size may also be a disadvantage when data is compressed and mirrored in two remote servers.",null,null
32,"However, optimal dictionary pruning is a difficult problem. In a dictionary D, there are (|D|2) candidate segments to remove. Removal of a single segment has an unpredictable effect on the remainder of the dictionary. A substring of length may be identical in the first - 1 bytes to some other substring, or may be entirely different to any other material; several low-usage substrings may be identical but for one byte; deletion of a substring creates new substrings by concatenation of the material to the left and right; and so on. We have found in our experiments that na¨ive approaches to pruning do not give good results.",null,null
33,"In this paper, we formulate dictionary pruning as an optimization problem. Pruning is similar to known NP-hard compression problems, so we propose a heuristic measure to measure the `value' (in terms of contribution to compression performance) of each byte in the dictionary. This measure guides our iterative scheme for pruning the dictionary in a principled way.",null,null
34,"As we do not alter the decompression-time costs, there is no impact on the impressive retrieval speed that was originally reported. The results for compression show that we can substantially reduce the dictionary size with only a small fraction of the compression degradation of other methods. For example, on the 426 GB GOV2 collection and a dictionary of 1000 MB, the data is reduced to 10.271% of its original size; with the existing method [10], halving the dictionary to 500 MB increased compressed size by 0.276% (in ab-",null,null
35,"solute terms), whereas with our method it increases by only 0.005%. Halving again to 250 MB gives increases of 3.181% (previous method) and 0.636% (our method), respectively, compared to the 1000 MB dictionary. Our method shows how to halve dictionary size with virtually no impact on compression, and thus has the potential to yield substantial gains in practice for data retrieval, storage, and transmission.",null,null
36,2. RELATED WORK,null,null
37,"Compression has been extensively employed and studied in the area of text retrieval systems. In this paper, we focus on compression of the text collection [8], a very different problem to inverted index compression [32] and full-text index compression [20].",null,null
38,"The general-purpose LZ family [29] can be viewed as an on-line dictionary-based compression scheme. A sliding window captures repetition in the data; these previously observed strings act as an internal dynamic dictionary. The compression observed on a single document tends to be poor, since insufficient material is available to build a representative dictionary; to adapt these approaches to repositories, typically one concatenates and compresses blocks of documents together. While this can provide good compression, it means that a whole block must be transmitted and decompressed to access a single document, greatly reducing the value and applicability of the method.",null,null
39,"There are several approaches based on off-line or static dictionaries. Word-based methods using Huffman codes have attracted considerable interest in information retrieval research [28], but have shortcomings. In particular, they are limited to cases where the characteristics of the data (for example, that it consists of ASCII text that can be parsed into words) are known in advance; and compression performance is relatively poor. Another family is based on dictionary inference [3, 4, 18, 24]. These methods use the data (or a large part of it) to infer a dictionary represented as a simple hierarchical grammar, and then replace the bytes or words with references to tokens in the dictionary. They have the general strong disadvantage that the data must be held in memory during dictionary inference. An alternative, proposed by Kuruppu et al. [14] for genomic data, is to construct the grammar iteratively in an offline manner, which can yield reasonable compression performance, but is extremely slow and the resulting dictionary tends to be large.",null,null
40,"A further class of methods is based on delta compression, which is primarily designed for sharing of versions of material and requires that the whole repository be used as a dictionary of long strings [11, 13, 21]. While it has superficial similarities to our problem, such methods do not by themselves constitute a solution for repository compression; moreover, these methods require an underlying compression method of the kind being explored here.",null,null
41,"RLZ. In this paper, we examine how to improve, subject to",null,null
42,"a bound on the dictionary size, the compression efficiency of RLZ [9]. This off-line dictionary compression algorithm has been applied to genomes [15, 16, 17] as well as text. In RLZ, a fixed number, say k, of blocks (that is, substrings) of fixed length, say 1 KB, are sampled from the original byte sequences of the collection and concatenated, in their original order, to form the dictionary (Figure 2). This dictionary remains unchanged during compression and decompression.",null,null
43,284,null,null
44,"The data is then factorized with respect to the dictionary using a greedy strategy inspired by the LZ77 factorization algorithm of Chen et al. [5], with the help of a suffix array.",null,null
45,l1,null,null
46,l2,null,null
47,l3,null,null
48,2,null,null
49,p1 l1,null,null
50,p4 l4 p2 l2,null,null
51,l4 p3 l3,null,null
52,RLZ Factorization Routine,null,null
53,p1 l1,null,null
54,p2 l2,null,null
55,p3 l3,null,null
56,p4 l4,null,null
57,Factor Encoding,null,null
58,2,null,null
59,Routine,null,null
60,... ...,null,null
61,Figure 2: Overview of RLZ. (a) The collection is sampled: blocks are concatenated (in collection order) to form a dictionary. (b) Each document is factorized relative to the dictionary. These factors are then encoded and concatenated (in document order) to constitute the compressed representation of the collection.,null,null
62,"Each factor is represented as a pair (p, l): p is the position in dictionary D where the factor starts, and l is the length of the factor. To represent a character c that does not appear in the dictionary, (p, l) ,"" (c, 0). These factors are then encoded with standard simple methods such as byte-oriented codes.""",null,null
63,"A particular attraction of RLZ is the speed of decompression. Hoobin et al. [9] report experiments where in the best case RLZ decompresses at around 18,000 GOV2 [6] documents per second (or around 120 per second, with random access), compared to around 2600 documents per second (or around 70­100 per second, with random access) with previous methods. They show that RLZ is consistently superior in decompression speed to previous methods, including retrieval of uncompressed text.",null,null
64,"The work of Hoobin et al. [10] is the only previous examination of the dictionary pruning problem. In their approach, they removed blocks from the original dictionary based on the statistics derived from the initial compression and then reconstructed a smaller dictionary with less redundancy. This strategy serves as our major baseline and is described in detail in Section 3.2. We show that this approach to redundancy elimination does not provide satisfactory results when the pruned amount is large.",null,null
65,3. PRUNING THE DICTIONARY,null,null
66,"In this section, we first present a formulation of the dictionary pruning problem. We describe and discuss straightforward redundancy elimination approaches, as baselines. We then explain our method, which like the other approaches is heuristic but is based on observations that arise from formal analysis of the problem.",null,null
67,"Notation. We let [x..y] be a representation of the sequence {x, x + 1, . . . , y - 1}, and A[x..y] stand for the array, or substring, A[x], A[x + 1], . . . , A[y - 1].",null,null
68,3.1 Problem formulation,null,null
69,"Let C represent the collection of files, indeed a concatenation of the files. Samples are generated from the text, that is, we choose blocks (substrings) from C, and concatenate these, in C order, to form a dictionary D of (initial) specified size. The aim is to prune the dictionary D, while maximizing the compression effectiveness.",null,null
70,"In general, a factorization F of C with respect to a dictionary D (however obtained) is a sequence of M factors {(p1, l1), (p2, l2), . . . , (pM , lM )} that expand to C, as defined by requirement (1) below. Let Li be the length of the text collection represented by the first i factors. That is L0 , 0 and Li ,"" Li-1 + max(li, 1), acknowledging that li "", 0 is `code' for a single character. The requirement is that",null,null
71,"LM , |C|",null,null
72,and,null,null
73,"C[Li-1..Li] ,",null,null
74,D[pi..pi + li] ci,null,null
75,"li > 0 (1) li , 0",null,null
76,"(In our implementations, we do not allow factors to span document boundaries. For simplicity, here, we consider the collection to be one contiguous string.)",null,null
77,"The encoding of the factors varies in size due to the properties of variable-byte representations of the lengths {li}. For the purpose of the optimization, the variation is unlikely to be important because, for each of the great majority of factors, a single byte suffices to encode its length. We therefore make the simplifying, and established [25], assumption that all non-zero-length factors are encoded with the same number of bytes, f , and that each zero-length factor consumes one byte. Hence the compression effectiveness (which we call `cost') of F, with m non-zero-length factors, is f m + (M - m), leading to this characterization of the pruning problem:",null,null
78,"Given a collection C, a dictionary D, and a required reduction in dictionary size , remove at least  bytes from D (to get D ) so that the cost of the factorization F of C with respect to D is minimized.",null,null
79,"Unless otherwise specified, all pruning algorithms in this paper leave the remaining parts of the dictionary in the same order as prior to the pruning.",null,null
80,"This problem is similar to the dictionary selection problem of Storer and Szymanski [25]. In their model, however, the dictionary can itself be factorized and the aim is to minimize the total compressed representation of both the dictionary and the collection. This is very similar to one of the compression effectiveness measures we describe in Section 4, the archived ratio. Although there are differences--our model has a bound on the uncompressed dictionary size, and is generated from samples--we expect that their NP-hardness results can be applied to our formulation.",null,null
81,"Given this formulation, we could annotate each byte in the dictionary by the number of times it is referenced, as calculated in Algorithm 1.",null,null
82,"Intuitively, the dictionary can be pruned by removing bytes, or strings of bytes, that have low numbers of references; and indeed this is the method we pursue below. However, we note that in general this may not be optimal.",null,null
83,285,null,null
84,Algorithm 1 Calculating byte reference frequencies.,null,null
85,"Input: Factorization F, Dictionary size d",null,null
86,Output: Vector of byte reference frequencies r[0..d],null,null
87,"1: procedure Freq(F, d)",null,null
88,2: r[0..d]  0,null,null
89,"3: for (p, l)  F do",null,null
90,4:,null,null
91,for j  [p..p + l] do,null,null
92,5:,null,null
93,r[j]  r[j] + 1,null,null
94,6: return r,null,null
95,"For example, if we remove the unreferenced substring abc but also remove b from abcd elsewhere in the dictionary, the removal of abc can imply increased costs. That is, where abc could previously be encoded with a single reference, it might now require three. Given that we are beginning with an effective dictionary, our new statement of the problem implies that we need to reduce the dictionary without destroying useful factors or unnecessarily increasing the number of factors. Heuristics for identifying the material to delete from a dictionary should attempt to minimize fragmentation of factors, as well as to remove material that has a relatively low reference count.",null,null
96,"Both our in-principle analysis of the problem and our experiments highlight the degree to which deletions from a dictionary have unpredictable effects on the factorization. Removal of a single character from a factor used early in a document can cause the entire remaining factorization to change. In our investigation, we discovered no simple measure that reliably quantified the impact of deletions from a dictionary.",null,null
97,3.2 Previous and preliminary methods,null,null
98,"Block-level redundancy elimination. As discussed above,",null,null
99,"Hoobin et al. [10] proposed an elimination method based on block removal, which we show as Algorithm 2 and refer to as REM . A frequency counter is maintained for each block in the dictionary. Each time a factor is generated, the frequency counter of the corresponding block in which the factor occurs is incremented. The blocks with the lowest counters are then removed, until the size bound on the dictionary is achieved. In this algorithm, Factorize(C, D) in line 2 corresponds to the RLZ algorithm [9, 15].",null,null
100,Algorithm 2 Pruning the dictionary by removing least frequently referred-to blocks [10].,null,null
101,"Input: Text Collection C, Original Dictionary D, Block size",null,null
102,", Target dictionary reduction ",null,null
103,Output: Pruned dictionary D,null,null
104,1: R[0..|D|/]  0,null,null
105,Block reference counts,null,null
106,"2: F  Factorize(C, D)",null,null
107,"3: for each (p, l) in F do",null,null
108,4: for i  [p/..(p + l - 1)/ + 1] do blocks involved,null,null
109,5:,null,null
110,R[i]  R[i] + 1,null,null
111,"6: Based on R[] counts, discard / least-frequently used blocks",null,null
112,7: return D  remaining blocks,null,null
113,"Algorithm REM of Hoobin et al. [10] compresses better, and is faster, than LZMA when the dictionary size is halved,",null,null
114,"while it outperforms ZLIB even with a ten-fold reduction in dictionary size. However, this strategy is far from optimal. Figure 1 shows that there are striking differences between the reference frequency of various parts of a sample block. As REM treats the block as the unit of elimination, it may remove a highly useful substring from the dictionary, should it happen to reside in an otherwise useless block. Elimination at a finer granularity is needed, prompting the following techniques.",null,null
115,Chunk-level redundancy elimination. Hash-based finger-,null,null
116,"print technology has been widely used for the tasks of duplicate identification and redundancy elimination [12, 13, 21, 22, 23, 26]. Algorithm 3 identifies redundant chunks using the Rabin-Karp rolling hash function [12].",null,null
117,Algorithm 3 Pruning the dictionary by removing redundant fixed-length substrings.,null,null
118,"Input: Original dictionary D, Chunk size c Output: Pruned dictionary D 1: Using a rolling hash, calculate hash of each c-gram in D 2: Identify the identical length-c substrings 3: Discard the redundant substrings 4: return D  remainder of dictionary",null,null
119,"If c is large, then Algorithm 3 may not reduce the dictionary much, as only repeated strings of length c or greater are pruned. There are more chunk candidates to remove for small c, but then long, useful strings tend to be broken up in unpredictable ways. In preliminary experiments, we found that this chunk-level reduction does lead to slightly better compression than REM. It is, however, considerably less effective than our subsequent techniques; space constraints compel us to omit the details.",null,null
120,Byte-level filtering. Instead of identifying redundant parts,null,null
121,"of the dictionary at a chunk level, individual redundant bytes could be removed. Based on the factorization of the collection, the least-frequently used bytes are removed from the dictionary, until the dictionary is sufficiently small. The remaining bytes are kept in original order. Preliminary results show that such filtering does outperform chunk-level elimination (Algorithm 3). However, performance degrades drastically when too much material is removed, because such filtering makes the pruned dictionary too fragmented, which dramatically increases the number of factors. Again, we omit the details of these preliminary experiments.",null,null
122,3.3 Contribution-aware pruning method,null,null
123,"A major challenge in dictionary pruning is how to choose the segments to remove. In other words, it is essential to estimate as precisely as possible the consequences of removing a particular substring. In principle, one method is to compress the text collection against a version of the dictionary from which a particular candidate segment has been excluded. Then, based on the resulting compression ratio, we can tell which segments have the least effect on the compression. However, in practice, re-evaluation of the compression for each candidate segment is out of the question.",null,null
124,"Instead, we propose a measure to estimate a segment's `contribution' to the compression if it is kept in the pruned dictionary. To calculate this measure, only the dictionary it-",null,null
125,286,null,null
126,"self is required, not the collection. The segment is factorized against the `pruned' dictionary, that is, against a notional version of the dictionary in which the candidate segment is absent (Figure 3). With the constraint that the factors of a candidate segment should not overlap the segment itself, the standard Factorize routine of RLZ suffices.",null,null
127,Figure 3: Estimating the value of a candidate segment by factorizing it against the rest of the dictionary.,null,null
128,"In Figure 3, the candidate segment s (in a dotted bubble) can be described by three factors that appear in the rest of the dictionary. Thus each reference to this segment of the dictionary will produce three factors when being compressed using the pruned dictionary.",null,null
129,"New measure. Here, we develop an estimate of the effect",null,null
130,"of removing a segment s from dictionary D. Suppose s is exactly the target of some string t in the factorization of C against D. If the dictionary D - s were used instead of D, then t would be factorized the same way that s is factorized against D - s. To estimate this effect, we calculate nfac(s, D), which is the number of factors that s generates when factorized against D-s. For the second candidate segment in Figure 3, this value is three. If a notional t's target were s, then it would now require nfac(s, D) factors.",null,null
131,"More generally, (part of) s may be (part of) the target of some string t. Now, when the collection is factorized against D - s, that string t might instead be factorized differently. However, it is possible that the part of t whose target is part of s is factorized in the same way as the common subsegment of s is against D - s. On average, if t targets only the subsegment s in s, then it would incur nfac(s, D) × |s |/|s| new factors when factorized against D - s. Counting this from the point of view of s, we consider the average number of times each byte of s is a target, denoted by Fre(s, r). This is is r[i]/|s|, where r ,"" Freq(F , |D|). When multiplied by nfac(s, D), this is a rough estimate of the number of new factors appearing in the factorization of C when s is removed from D. We refer to this measure as FF (frequency & factor), and propose the removal of segments that have the lowest FF values.""",null,null
132,"Were it included in a dictionary, segment s would consume |s| space. Therefore we also introduce the per-byte measure FFL(s, D), which is FF(s, D)/|s|.",null,null
133,"Though these two measures are only an approximation, this `factorizing and counting' strategy provides us with an estimate of the effect of removing a segment. Importantly, it is relatively cheap to calculate.",null,null
134,CARE algorithm. Our contribution-aware reduction algo-,null,null
135,"rithm (CARE) may be applied as a one-off procedure, or iteratively. We start by describing the core of the process, in",null,null
136,"Algorithm 4. It removes from the dictionary those segments that have low FFL (or FF) values. Importantly, to control the number of candidate segments, we consider only segments of length at least , containing no byte with reference frequency greater than . These candidates, S, are, in practice, found via a greedy heuristic, Candidates(D, r, , ), based on the reference-frequency vector r ,"" Freq(F, |D|). Given a starting point in the string, a segment of bytes with frequency at most  and of maximal length is found. Should this segment's length be at least , it is added to S, otherwise it is ignored. The search for candidates resumes with the next byte whose frequency is at most . By design, the candidate segments do not overlap.""",null,null
137,Algorithm 4 Pruning the dictionary using CARE.,null,null
138,"Input: Text collection C, Original dictionary D, Byte-",null,null
139,"frequency threshold , Length threshold , Desired dic-",null,null
140,tionary size reduction ,null,null
141,Output: Pruned dictionary D,null,null
142,"1: procedure O-Pruning(, , )",null,null
143,One-off pruning,null,null
144,"2: F  Factorize(C, D)",null,null
145,"3: r  Freq(F, |D|)",null,null
146,"4: S  Candidates(D, r, , )",null,null
147,5: if |s|   then,null,null
148,Out of candidates,null,null
149,sS,null,null
150,6:,null,null
151,return D,null,null
152,7: for each s in S do,null,null
153,8:,null,null
154,"Calculate Fre(s, r) and",null,null
155,9:,null,null
156,"Execute Factorize(s, D) and calculate nfac(s, D)",null,null
157,10:,null,null
158,"FFL(s, D)  Fre(s, r) × nfac(s, D)/|s|",null,null
159,"11: Based on the FFL-values, discard segments in S from D until length discarded is at least .",null,null
160,12: return D  remainder of dictionary,null,null
161,"Algorithm 4 is called O-Pruning as it is a one-off process. However, it can be applied iteratively: at each step the dictionary size is reduced by a specified amount. Our results show that iterating this procedure with a `small' amount removed from the dictionary each time results in different outcomes to those of pruning the dictionary in a single step.",null,null
162,4. PERFORMANCE EVALUATION,null,null
163,"In most of our experiments we use subsets of GOV2 [6]. The collections small, medium, large, and full correspond to the first 1 GB, 10 GB, 100 GB, and all (426 GB) of the documents, respectively. We first study in detail the effectiveness of our method by carrying out experiments with various settings on both the small and medium datasets, then we repeat the experiments on the large and full datasets to demonstrate the scalability of our method. For each experiment, an original dictionary is generated by the sampling technology described in [9, 10]; we then prune each dictionary to a variety of fixed sizes.",null,null
164,"The baselines we use are the plain sampling strategy [9], which we call ORI , and the previous redundancy elimination (or pruning) method REM [10]. Unless indicated otherwise, 1 KB is the default for both the sample block size used during original RLZ dictionaries generation and reduced unit size in REM. The coding schemes used to compress the position and length of factors are Z (ZLIB, zlib.net) and V (VBYTE [27]), respectively. This combination achieves the fastest compression time and is only marginally worse than",null,null
165,287,null,null
166,"the best, but much slower, combination (ZZ) reported in previous work [9] in terms of compression ratio.",null,null
167,"Since the superiority of RLZ over the compression libraries ZLIB and LZMA has already been established [9], we do not examine these latter two further. CARE is only concerned with the construction of the dictionary, so it does not affect the decompression process or the compressed data layout, and thus does not affect the retrieval time of RLZ. Therefore our evaluations do not include retrieval speed.",null,null
168,"As shown by Hoobin et al. [9], we can achieve a compression ratio of less than 10% for a large collection (GOV2, 426 GB), where compression ratio is the final compressed size as a percentage of the original size. This is achieved with a dictionary ratio of only 0.5% or less (the ratio of dictionary size to the collection size). However, to maintain the same compression ratio as an unpruned dictionary, for a small collection (say, 4 GB), we have to increase the dictionary ratio to 20%­30%. Since the dictionary ratio and the compression ratio are both relative to the uncompressed data size, we can introduce a new performance measure (AR, for archived ratio) that covers them both. The AR is the compressed size of the dictionary and collection together, as a percentage of the uncompressed collection size. We use a standard tool (7zip) to compress the dictionary, to represent the size it would occupy during archiving. In other words, AR is the size required for storage or transmission of a repository that has been compressed with RLZ.",null,null
169,5. EXPERIMENTAL RESULTS,null,null
170,"Our experiments involve parameters that trade against each other in complex ways. For example, as can be observed in these experiments, the dictionary size is not a function of the collection size, for a given compression ratio. As another example, a given dictionary size can be achieved by sampling; or can be achieved by pruning from a larger dictionary. The pruning can be achieved directly, or iteratively. We thus need to report IDS , the initial dictionary size; the PDS , or pruned dictionary size; the ICR, or initial compression ratio; and the step, which is the amount the dictionary size is reduced in each pruning iteration. Ultimately, we wish to discover the best compression ratio available for a given dictionary size which, as we show, is given by our new method CARE.",null,null
171,"For a fair comparison, we first generate equal-sized dictionaries with each method. That is, given an IDS and a sequences of pruned sizes, we compare directly sampling (ORI) to achieve the pruned size to sampling to the original size then applying REM and CARE to achieve the pruned size. In this first experiment, the CARE algorithm uses FFL as the measure for the removal of candidate segments from the dictionary, and the pruning strategy is one-off (noniterative). Only the segments with maximum byte frequency at most  and length at least  may be removed. Sensitivity to these parameters is discussed later.",null,null
172,"Figure 4(a) shows that, with the small dataset, CARE is consistently better than ORI in terms of compression ratio for same-sized dictionaries. The ORI line represents the result of using a range of initial dictionary sizes; the REM and CARE lines are the result of using a specified initial dictionary size (250 MB) and then pruning. As the pruning continues, CARE outperforms REM when the dictionaries are reduced by a a third or more. Meanwhile REM gets dramatically worse, and after a two-fold reduction in dictio-",null,null
173,Compression Ratio (%),null,null
174,22,null,null
175,ORI,null,null
176,REM,null,null
177,20 CARE,null,null
178,18,null,null
179,16,null,null
180,14,null,null
181,12,null,null
182,10,null,null
183,250,null,null
184,200,null,null
185,150,null,null
186,100,null,null
187,50,null,null
188,Dictionary Size (MB),null,null
189,"(a) dataset: small (1 GB),  ,"" 10,  "", 20",null,null
190,19,null,null
191,18,null,null
192,ORI REM,null,null
193,17 CARE,null,null
194,16,null,null
195,15,null,null
196,14,null,null
197,13,null,null
198,12,null,null
199,11,null,null
200,10 2000 1750 1500 1250 1000 750 500 Dictionary Size (MB),null,null
201,"(b) dataset: medium (10 GB),  ,"" 10,  "", 20",null,null
202,Compression Ratio (%),null,null
203,"Figure 4: Compression ratio achieved by different construction strategies. The initial dictionary size is 250 MB and 2000 MB for the small and medium datasets, respectively. Pruning in CARE is applied one-off; FFL is used as the measure.",null,null
204,nary size becomes poorer than the commensurate directlysampled dictionary. The same patterns are also observed in Figure 4(b) for the medium dataset.,null,null
205,"We also investigate the impact of pruning on AR for each method. As depicted in Figures 5(a) and 5(b), in contrast to the compression ratio, AR does not change monotonically as the dictionary is pruned. At first AR slightly decreases, because the saving in dictionary size is greater than the loss in compression ratio. For example, when the dictionary for medium dataset is pruned from 2000 MB to 1500 MB (310 MB to 268 MB, in terms of compressed size), we save 0.4% in compressed dictionary size while losing around 0.1% in compression ratio. However, with the dictionary size further reduced, AR increases instead. The 7zip utility compresses the dictionary so well (around 15%-16% of original) that the gap between the sizes of the compressed dictionaries is dramatically narrowed. Therefore, the saving in dictionary size is eventually overwhelmed by the increase in compression ratio.",null,null
206,Though pruning the dictionary will eventually lead to a poorer archived ratio and compression ratio than was avail-,null,null
207,288,null,null
208,Archived Ratio (%),null,null
209,24,null,null
210,ORI,null,null
211,22,null,null
212,REM CARE,null,null
213,20,null,null
214,18,null,null
215,16,null,null
216,14,null,null
217,250,null,null
218,200,null,null
219,150,null,null
220,100,null,null
221,50,null,null
222,Dictionary Size (MB),null,null
223,"(a) dataset: small (1 GB),  ,"" 10,  "", 20",null,null
224,21,null,null
225,20,null,null
226,ORI REM,null,null
227,19 CARE,null,null
228,18,null,null
229,17,null,null
230,16,null,null
231,15,null,null
232,14,null,null
233,13,null,null
234,12 2000 1750 1500 1000 750 500 250 Dictionary Size (MB),null,null
235,"(b) dataset: medium (10 GB),  ,"" 10,  "", 20",null,null
236,Archived Ratio (%),null,null
237,"Figure 5: Archived ratio achieved by different construction strategies. The initial dictionary size is 250 MB and 2000 MB for the small and medium datasets, respectively. Pruning in CARE is applied one-off; FFL is used as the measure.",null,null
238,"able with the original dictionary size, for a given (uncompressed) dictionary size, it can give a much better AR. For example, on medium, compare ORI at 1000 MB (AR , 12.4%) to CARE at 1000 MB (AR ,"" 10.9%), having pruned a 2000 MB dictionary to 1000 MB. That is, it is better to build a large dictionary and prune than to directly create a small dictionary.""",null,null
239,"We next compare the two CARE measures proposed in Section 3.3, with regard to evaluation of candidate segments. Table 1 shows that FFL is superior to FF in every setting. Note that the data in the ORI, REM, and FFL (CARE) columns is identical to that illustrated in Figure 4. The results here also reveal that each setting of CARE is consistently better than REM when there is significant pruning. Table 1 shows that the loss of compression ratio caused by CARE pruning (compare to the IDS) is less than 1% even when the dictionary size is reduced by half.",null,null
240,"Varying removal candidates. So far, we have examined",null,null
241,only one-off pruning. We investigate the choice of the two,null,null
242,Table 1: One-off pruning using CARE with different measures (FF & FFL). The values of  and  are the same as those in Figure 4.,null,null
243,Data Set and,null,null
244,IDS /ICR,null,null
245,"small (1GB) (IDS,250MB) (ICR,10.68%)",null,null
246,medium (10GB),null,null
247,"(IDS,2000MB) (ICR,10.03%)",null,null
248,PDS (MB),null,null
249,Compression Ratio (%),null,null
250,ORI REM,null,null
251,CARE FF FFL,null,null
252,200 150 100,null,null
253,50,null,null
254,1750 1500 1250 1000,null,null
255,750 500,null,null
256,11.85 13.09 14.65 17.09,null,null
257,10.52 11.07 11.68 12.39 13.23 14.35,null,null
258,10.78 12.95 16.81 22.22,null,null
259,10.03 10.12 10.85 12.65 15.47 18.77,null,null
260,11.26 12.53 14.12 16.43,null,null
261,10.09 10.46 11.05 11.79 12.67 13.78,null,null
262,10.83 11.39 12.74 15.28,null,null
263,10.09 10.17 10.39 10.86 11.75 13.15,null,null
264,"Table 2: Iterative pruning using CARE with different combinations of (, ) for the FF & FFL measures on small dataset (1 GB), IDS,""250 MB, ICR"",10.68%.",null,null
265,PDS REM,null,null
266,Compression Ratio (%),null,null
267,"(20, 20)",null,null
268,"(10, 20)",null,null
269,"(10, 50)",null,null
270,(MB),null,null
271,FF FFL FF FFL FF FFL,null,null
272,200 10.78 11.57 11.10 11.13 10.83 11.26 10.83 150 12.95 12.71 11.79 12.13 11.32 12.40 11.33 100 16.81 14.23 12.98 13.41 12.48 13.85 12.51,null,null
273,50 22.22 16.49 15.05 15.25 14.88 15.82 14.92,null,null
274,"arguments--the upper limit of frequency  and the lower limit of length --in the context of iterative pruning. Table 2 presents the results of three different combinations of these two arguments on the small dataset. The differences in compression ratio among the combinations are small. Optimizing the parameter selection is a research question we leave for the future, but these results suggest that, no matter which combinations we use, CARE is consistently better than REM. The results on the medium dataset are much the same. In the following evaluations, the combinations (, ) ,"" (10, 20) and (10, 50), are set as the default values in experiments for the small and medium dataset, respectively. These were chosen based on initial experiments.""",null,null
275,"As described in Section 3.3, there are two ways to progress the pruning. One is to prune the dictionary to a fixed volume in an one-off manner, while the other is to iteratively reduce the dictionary multiple times by a fixed step. Figures 6(a) and 6(b) show that the iterative strategy is consistently better than the one-off method. And as the reduction continues, the advantage of the iterative strategy increases. The results also demonstrate that FFL remains superior to FF as a measurement of the value of segments. The iterative FF CARE algorithm (I-FF) in Figure 6(b) runs out of candidates after being pruned to 750 MB, so there is no corresponding result for a dictionary of 500 MB.",null,null
276,Varying dictionary sizes. Next we studied the impact of,null,null
277,"the step size  on the effectiveness of different strategies. The results in Table 3 show that we can achieve better compression by choosing smaller step sizes, though dictionary construction is slower. However, the improvement achieved",null,null
278,289,null,null
279,Compression Ratio (%),null,null
280,17,null,null
281,16,null,null
282,O-FF I-FF,null,null
283,O-FFL,null,null
284,15,null,null
285,I-FFL,null,null
286,14,null,null
287,13,null,null
288,12,null,null
289,11,null,null
290,10,null,null
291,250,null,null
292,200,null,null
293,150,null,null
294,100,null,null
295,50,null,null
296,Dictionary Size (MB),null,null
297,"(a) dataset: small (1 GB),  ,"" 10,  "", 20",null,null
298,14.0,null,null
299,13.5,null,null
300,O-FF I-FF,null,null
301,13.0,null,null
302,O-FFL I-FFL,null,null
303,12.5,null,null
304,12.0,null,null
305,11.5,null,null
306,11.0,null,null
307,10.5,null,null
308,10.0 2000 1750 1500 1250 1000 750 500,null,null
309,Dictionary Size (MB),null,null
310,"(b) dataset: medium (10 GB),  ,"" 10,  "", 20",null,null
311,Compression Ratio (%),null,null
312,Figure 6: Compression ratio of fixed-size dictionaries generated by CARE with one-off (O) and iterative (I) strategies. FFL is used as the measure.,null,null
313,"by fine granularity is very small. Thus, we continue to use the previous settings in our remaining experiments. For the purpose of this table, we ignored settings where the gap between the IDS and PDS was not a multiple of the step size.",null,null
314,"Table 4 shows the results of CARE dictionary reduction with different initial dictionary sizes (IDS). The results suggest that, by starting from a larger IDS, we end up with a better dictionary for each specified final size. The reason is that the larger dictionary represents the collection better, while the multiple rounds of pruning reduce the dictionary to the most valuable substrings. The results on the small dataset (not shown) support this observation.",null,null
315,"We also present results for ORI and REM on the medium dataset in Table 5. By comparing Tables 4 and 5, we observe that, for REM, a smaller IDS leads to a better pruned dictionary. When REM starts with a larger dictionary, more blocks must be removed, which makes the pruned dictionary less representative of the collection. More significantly, even the worst case in CARE is better than the best case in REM (except when the pruned volume is small). The results on the small dataset (not shown) are similar.",null,null
316,Table 3: Iterative pruning using CARE with various progressive pruning differences.,null,null
317,Data Set,null,null
318,"small (1GB) (IDS,250)",null,null
319,"medium (10GB) (IDS,2000)",null,null
320,PDS (MB),null,null
321,200 150 100,null,null
322,50,null,null
323,PDS (MB) 1500 1000,null,null
324,500,null,null
325,Compression Ratio (%),null,null
326,Difference per step (),null,null
327,50 MB 20 MB 10 MB,null,null
328,10.83,null,null
329,­,null,null
330,10.81,null,null
331,11.32 11.27 11.25,null,null
332,12.48,null,null
333,­,null,null
334,12.36,null,null
335,14.88 14.77 14.69,null,null
336,Difference per step (),null,null
337,500 MB 250 MB 100 MB,null,null
338,10.17 10.16 10.15,null,null
339,10.80 10.76 10.74,null,null
340,12.99 12.96 12.86,null,null
341,"Table 4: Iterative pruning using CARE with different IDSs on medium dataset (10 GB), step  , 250 MB.",null,null
342,PDS,null,null
343,(MB) 1250 1000,null,null
344,500,null,null
345,Compression Ratio (%),null,null
346,"IDS,2000MB IDS,1500MB IDS,1000MB",null,null
347,"ICR,10.03% ICR,11.07% ICR,12.39%",null,null
348,10.36,null,null
349,11.02,null,null
350,­,null,null
351,10.76,null,null
352,11.26,null,null
353,­,null,null
354,12.98,null,null
355,12.99,null,null
356,13.05,null,null
357,Table 5: Performance of dictionary construction methods (ORI and REM) on medium dataset (10 GB).,null,null
358,Constructed Dictionary Size (MB),null,null
359,1250 1000 500,null,null
360,ORI,null,null
361,11.68 12.39 14.35,null,null
362,Compression Ratio (%),null,null
363,REM with IDS (MB),null,null
364,"IDS,2000 IDS,1500 IDS,1000",null,null
365,10.85,null,null
366,11.07,null,null
367,­,null,null
368,12.65,null,null
369,11.65,null,null
370,­,null,null
371,18.77,null,null
372,17.82,null,null
373,15.72,null,null
374,"The impact of the block size on the effectiveness of RLZ is not studied in previous work [9, 10]. Table 6 shows that the sample size has little effect on CARE, and only very limited impact on the other methods.",null,null
375,Larger collections. After our extensive studies of CARE,null,null
376,"on both the small and medium datasets, we repeat the experiments on the large dataset as well as the full GOV2 collection. Table 7 demonstrates that CARE significantly outperforms REM as expected. Halving the dictionary size using CARE causes less than 0.3% loss in compression ratio, while the compression ratio is 1% better than that of the commensurate size of the dictionary constructed by ORI. When reducing the dictionary to only a quarter of its original size, compression loss is only around 1.4%, while REM suffers around 6.5%­7%.",null,null
377,"In Table 7 we also report the compressibility of different pruned dictionaries (7zip is used here). The results show that the dictionaries are less compressible after pruning, which means they now contain less redundancy. For commensurable size of pruned dictionaries, the number of factors generated by REM is far larger than that by CARE, explaining why CARE outperforms REM in compression ratio. For example, with a 250 MB dictionary (IDS,""1000 MB), REM produces 5.64 billion factors, CARE 3.55 billion factors.""",null,null
378,290,null,null
379,"Table 6: Performance of dictionary pruning methods using various sample block sizes on the small and medium datasets. CARE uses FFL with  ,"" 10,  "","" 20, iteratively.""",null,null
380,PDS (MB),null,null
381,200 150 100,null,null
382,50,null,null
383,Compression Ratio(%),null,null
384,"small (IDS ,"" 250 MB, step "", 50 MB) medium (IDS ,"" 2000 MB, step "", 250 MB)",null,null
385,"sample , 1 KB",null,null
386,"sample , 8 KB",null,null
387,"sample , 1 KB",null,null
388,"sample , 8 KB",null,null
389,"ICR , 10.68%",null,null
390,"ICR , 10.81%",null,null
391,"ICR , 10.13%",null,null
392,"ICR , 9.96%",null,null
393,REM CARE REM CARE REM CARE REM CARE,null,null
394,10.78,null,null
395,10.83,null,null
396,11.46,null,null
397,10.90,null,null
398,10.85,null,null
399,10.36,null,null
400,11.25,null,null
401,10.25,null,null
402,12.95,null,null
403,11.32,null,null
404,13.01,null,null
405,11.33,null,null
406,12.65,null,null
407,10.76,null,null
408,12.38,null,null
409,10.68,null,null
410,16.81,null,null
411,12.48,null,null
412,15.94,null,null
413,12.48,null,null
414,15.47,null,null
415,11.56,null,null
416,14.13,null,null
417,11.50,null,null
418,22.22,null,null
419,14.88,null,null
420,21.64,null,null
421,14.86,null,null
422,18.77,null,null
423,12.98,null,null
424,17.01,null,null
425,12.94,null,null
426,PDS (MB),null,null
427,1250 1000,null,null
428,750 500,null,null
429,"Table 7: Performance of dictionary construction methods on large dataset (100 GB). CARE uses FFL,  ,"" 100,  "","" 20, iteratively.""",null,null
430,PDS,null,null
431,"IDS , 2000 MB ICR , 11.71%",null,null
432,"IDS , 1000 MB ICR , 12.96%",null,null
433,(MB) ORI REM CARE ORI REM CARE,null,null
434,Compression Ratio (%),null,null
435,1500 12.23 11.76 11.73 ­,null,null
436,­,null,null
437,­,null,null
438,1000 12.98 13.43 11.98 ­,null,null
439,­,null,null
440,­,null,null
441,750 ­,null,null
442,­,null,null
443,­,null,null
444,13.45 13.06 12.97,null,null
445,500 14.26 18.20 13.12 14.26 15.12 13.25,null,null
446,250 ­,null,null
447,­,null,null
448,­,null,null
449,15.60 20.11 14.40§,null,null
450,Compressed Dictionary Size (MB),null,null
451,1500 215 256 256,null,null
452,­,null,null
453,­,null,null
454,­,null,null
455,1000 146 206 210,null,null
456,­,null,null
457,­,null,null
458,­,null,null
459,750 ­,null,null
460,­,null,null
461,­,null,null
462,111 132 133,null,null
463,500 75 127 115,null,null
464,75 106 109,null,null
465,250 ­,null,null
466,­,null,null
467,­,null,null
468,38 64,null,null
469,59,null,null
470,"§ In order to obtain complete results, we changed  from 100 to 200 here, as the process run out of candidates at this point.",null,null
471,"Table 8 shows that all the conclusions drawn above also hold on the whole GOV2 corpus (426 GB). For example, a CARE-based dictionary of 500 MB can give compression as good as that originally available with 1000 MB. Table 9 shows results for the Wikipedia dataset, which is very different from GOV2. The Wikipedia data is highly structured, with many common elements repeated from page to page, whereas GOV2 contains highly diverse material from every branch of the US government. However, the compression results are very similar, as is the relative behavior of the different algorithms.",null,null
472,"In both tables, reducing the dictionary by a quarter with REM or CARE has almost no impact on compression ratio; the tiny changes (improvements in a couple of cases!) are due to the effect of different but nearly equivalent factors being chosen. For greater reductions, however, the CARE method again exhibits much better performance, with very slow degradation in compression ratio compared to the alternatives. These results show that our CARE method has proved much the most effective way of reducing dictionary size, and also show the benefit of starting with a large dictionary which is then progressively reduced.",null,null
473,6. CONCLUSIONS AND FUTURE WORK,null,null
474,Relative Lempel-Ziv factorization is an efficient compression algorithm that provides both good compression ratio,null,null
475,"Table 8: Performance of dictionary construction methods on GOV2 (426 GB). CARE uses FFL,  ,"" 200,  "","" 20, iteratively.""",null,null
476,PDS,null,null
477,"IDS , 2000 MB ICR , 9.419%",null,null
478,"IDS , 1000 MB ICR , 10.271%",null,null
479,(MB) ORI REM CARE ORI REM CARE,null,null
480,Compression Ratio (%),null,null
481,1500 9.789 9.414 9.425,null,null
482,­,null,null
483,­,null,null
484,­,null,null
485,1000 10.271 9.588 9.437,null,null
486,­,null,null
487,­,null,null
488,­,null,null
489,750,null,null
490,­,null,null
491,­,null,null
492,­ 10.645 10.262 10.272,null,null
493,500 11.083 12.371 10.036 11.083 10.547 10.276,null,null
494,250,null,null
495,­,null,null
496,­,null,null
497,­ 11.987 13.452 10.907§,null,null
498," These numbers are slightly different from those reported in [9]. This is caused by the slight difference between the size of our dictionaries (e.g., 2000 MB versus 2 GB).",null,null
499,"§  adjusted from 200 to 800 here. (In fact, we found that by simply setting  as the ratio of collection size over dictionary size will always guarantee the sufficiency of the candidates.)",null,null
500,"Table 9: Performance of different dictionary construction methods on Wikipedia dataset (251 GB). CARE uses FFL  ,"" 200,  "","" 20, iteratively.""",null,null
501,PDS,null,null
502,"IDS , 2000 MB ICR , 8.688%",null,null
503,"IDS , 1000 MB ICR , 9.900%",null,null
504,(MB) ORI REM CARE ORI REM CARE,null,null
505,Compression Ratio (%),null,null
506,1500 9.202 8.708 8.738,null,null
507,­,null,null
508,­,null,null
509,­,null,null
510,1000 9.900 9.342 8.898,null,null
511,­,null,null
512,­,null,null
513,­,null,null
514,750,null,null
515,­,null,null
516,­,null,null
517,­ 10.383 9.909 9.901,null,null
518,500 11.096 11.369 9.787 11.096 10.517 10.052,null,null
519,250,null,null
520,­,null,null
521,­,null,null
522,­ 12.226 12.561 11.066,null,null
523,"and fast retrieval. Though it only requires a relatively small dictionary, compared with the size of the collection to be compressed, the dictionary size is still an essential concern as it must be maintained in memory.",null,null
524,"We first formulate the dictionary pruning problem as an optimization problem and then propose heuristic strategies for pruning the dictionary while maintaining compression effectiveness. Our main heuristic can be calculated efficiently by factoring segments of the dictionary against the dictionary itself. By identifying and eliminating low-value segments, we can markedly reduce the volume of the dictionary without significant loss of compression performance.",null,null
525,"RLZ may be deployed on mobile devices, where a fixed dictionary can be used to reduce download requirements. In",null,null
526,291,null,null
527,"such a context, dictionary size must be kept small, and the value of these kinds of savings is accentuated.",null,null
528,"In our view, we should next refine our understanding of the dictionary optimization problem. The consequent pruning algorithms could start with much larger dictionaries, which are then progressively reduced, and we hypothesize that compression will be even more effective. However, the existing results are already significantly superior to any current alternative, and provide a practical method for largescale corpus compression.",null,null
529,7. ACKNOWLEDGMENTS,null,null
530,"We thank Christopher Hoobin for providing the source code of RLZ. This work is partially supported by The Australian Research Council, NSF of China (61373018, 11301288), Program for New Century Excellent Talents in University (NCET-13-0301) and Fundamental Research Funds for the Central Universities(65141021). Jiancong would also like to thank the China Scholarship Council (CSC) for the State Scholarship Fund.",null,null
531,8. REFERENCES,null,null
532,"[1] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern Information Retrieval - The Concepts and Technology Behind Search, Second edition. Addison-Wesley, 2011.",null,null
533,"[2] S. Bu¨ttcher, C. L. A. Clarke, and G. V. Cormack. Information Retrieval - Implementing and Evaluating Search Engines. MIT Press, 2010.",null,null
534,"[3] A. Cannane and H. E. Williams. General-purpose compression for efficient retrieval. JASIST, 52(5):430­437, 2001.",null,null
535,"[4] A. Cannane and H. E. Williams. A general-purpose compression scheme for large collections. ACM Transactions on Information Systems, 20(3):329­355, 2002.",null,null
536,"[5] G. Chen, S. J. Puglisi, and W. F. Smyth. Lempel-Ziv factorization using less time & space. Mathematics in Computer Science, 1(4):605­623, 2008.",null,null
537,"[6] C. Clarke, N. Craswell, and I. Soboroff. Overview of the trec 2004 terabyte track. In TREC, 2004.",null,null
538,"[7] W. B. Croft, D. Metzler, and T. Strohman. Search Engines - Information Retrieval in Practice. Addison-Wesley, 2009.",null,null
539,"[8] P. Ferragina and G. Manzini. On compressing the textual web. In WSDM, pages 391­400, 2010.",null,null
540,"[9] C. Hoobin, S. J. Puglisi, and J. Zobel. Relative Lempel-Ziv factorization for efficient storage and retrieval of web collections. PVLDB, 5(3):265­273, 2011.",null,null
541,"[10] C. Hoobin, S. J. Puglisi, and J. Zobel. Sample selection for dictionary-based corpus compression. In SIGIR, pages 1137­1138, 2011.",null,null
542,"[11] J. J. Hunt, K.-P. Vo, and W. F. Tichy. Delta algorithms: An empirical analysis. ACM Transactions on Software Engineering and Methodology, 7(2):192­214, 1998.",null,null
543,"[12] R. M. Karp and M. O. Rabin. Efficient randomized pattern-matching algorithms. IBM Journal of Research and Development, 31(2):249­260, 1987.",null,null
544,"[13] P. Kulkarni, F. Douglis, J. D. LaVoie, and J. M. Tracey. Redundancy elimination within large collections of files. In USENIX Annual Technical Conference, General Track, pages 59­72, 2004.",null,null
545,"[14] S. Kuruppu, B. Beresford-Smith, T. C. Conway, and J. Zobel. Iterative dictionary construction for compression of large dna data sets. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 9(1):137­149, 2012.",null,null
546,"[15] S. Kuruppu, S. J. Puglisi, and J. Zobel. Relative Lempel-Ziv compression of genomes for large-scale storage and retrieval. In SPIRE, pages 201­206, 2010.",null,null
547,"[16] S. Kuruppu, S. J. Puglisi, and J. Zobel. Optimized relative Lempel-Ziv compression of genomes. In ACSC, pages 91­98, 2011.",null,null
548,"[17] S. Kuruppu, S. J. Puglisi, and J. Zobel. Reference sequence construction for relative compression of genomes. In SPIRE, pages 420­425, 2011.",null,null
549,"[18] N. J. Larsson and A. Moffat. Offline dictionary-based compression. In DCC, pages 296­305, 1999.",null,null
550,"[19] C. D. Manning, P. Raghavan, and H. Schu¨tze. Introduction to Information Retrieval. Cambridge University Press, 2008.",null,null
551,"[20] G. Navarro and V. M¨akinen. Compressed full-text indexes. ACM Computing Surveys, 39(1), 2007.",null,null
552,"[21] Z. Ouyang, N. D. Memon, T. Suel, and D. Trendafilov. Cluster-based delta compression of a collection of files. In WISE, pages 257­268, 2002.",null,null
553,"[22] A. Peel, A. Wirth, and J. Zobel. Collection-based compression using discovered long matching strings. In CIKM, pages 2361­2364, 2011.",null,null
554,"[23] P. Shilane, M. Huang, G. Wallace, and W. Hsu. WAN-optimized replication of backup datasets using stream-informed delta compression. ACM Transations on Storage, 8(4):1­26, 2012.",null,null
555,"[24] P. Skibinski, S. Grabowski, and S. Deorowicz. Revisiting dictionary-based compression. Software: Practice and Experience, 35(15):1455­1476, 2005.",null,null
556,"[25] J. A. Storer and T. G. Szymanski. Data compression via textual substitution. Journal of the ACM, 29(4):928­951, 1982.",null,null
557,"[26] T. Suel, P. Noel, and D. Trendafilov. Improved file synchronization techniques for maintaining large replicated collections over slow networks. In ICDE, pages 153­164, 2004.",null,null
558,"[27] H. E. Williams and J. Zobel. Compressing integers for fast file access. The Computer Journal, 42(3):193­201, 1999.",null,null
559,"[28] I. H. Witten, A. Moffat, and T. C. Bell. Managing Gigabytes: Compressing and Indexing Documents and Images, Second Edition. Morgan Kaufmann, 1999.",null,null
560,"[29] J. Ziv and A. Lempel. A universal algorithm for sequential data compression. IEEE Transactions on Information Theory, 23(3):337­343, 1977.",null,null
561,"[30] J. Ziv and N. Merhav. A measure of relative entropy between individual sequences with application to universal classification. IEEE Transactions on Information Theory, 39(4):1270­1279, 1993.",null,null
562,"[31] N. Ziviani, E. Silva de Moura, G. Navarro, and R. A. Baeza-Yates. Compression: A key for next-generation text retrieval systems. IEEE Computer, 33(11):37­44, 2000.",null,null
563,"[32] J. Zobel and A. Moffat. Inverted files for text search engines. ACM Computing Surveys, 38(2), 2006.",null,null
564,292,null,null
565,,null,null

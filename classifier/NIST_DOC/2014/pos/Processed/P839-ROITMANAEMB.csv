,sentence,label,data
0,Using the Cross-Entropy Method to Re-Rank Search Results,null,null
1,"Haggai Roitman, Shay Hummel",null,null
2,"IBM Research - Haifa Haifa 31905, Israel",null,null
3,"{haggai,shayh}@il.ibm.com",null,null
4,Oren Kurland,null,null
5,"Faculty of Industrial Engineering and Management, Technion Haifa 32000, Israel",null,null
6,kurland@ie.technion.ac.il,null,null
7,ABSTRACT,null,null
8,"We present a novel unsupervised approach to re-ranking an initially retrieved list. The approach is based on the Cross Entropy method applied to permutations of the list, and relies on performance prediction. Using pseudo predictors we establish a lower bound on the prediction quality that is required so as to have our approach significantly outperform the original retrieval. Our experiments serve as a proof of concept demonstrating the considerable potential of the proposed approach. A case in point, only a tiny fraction of the huge space of permutations needs to be explored to attain significant improvements over the original retrieval.",null,null
9,Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Retrieval Models,null,null
10,"General Terms: Algorithms, Experimentation",null,null
11,"Keywords: Re-ranking, Optimization, Performance Prediction",null,null
12,1. INTRODUCTION,null,null
13,"We present a novel unsupervised approach to the challenge of re-ranking a document list that was retrieved in response to a query so as to improve retrieval effectiveness. The approach utilizes a Monte-Carlo-based optimization method, named the Cross Entropy (CE) method [13], which is applied to permutations of the list. The approach relies on a retrieval performance predictor that can be applied to any ranking of the list.",null,null
14,"Given the reliance on performance prediction, we present a novel pseudo predictor that enables to fully control the prediction quality. We use the pseudo predictor in our approach to set a lower bound on the prediction quality that is needed so as to have our approach significantly outperform the initial ranking. Further empirical evaluation provides a proof of concept for our approach. Specifically, via the exploration of a tiny fraction of the huge space of all possible permutations, our approach finds highly effective permutations. The retrieval effectiveness of these permutations is substantially, and statistically significantly better, than that of the original ranking of the list.",null,null
15,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00.",null,null
16,http://dx.doi.org/10.1145/2600428.2609454.,null,null
17,2. RELATED WORK,null,null
18,"The Cross Entropy (CE) method [13] that is used in our approach is a Monte Carlo framework for rare event estimation and combinatorial optimization. The CE method has been previously applied in many domains such as machine learning, simulation, networks, etc. [13]. To the best of our knowledge, our work is the first to use the CE method in the information retrieval domain.",null,null
19,"Our approach relies on predicting the retrieval performance of permutations of a document list. Applying performance prediction to select one of two retrieved lists was explored in some work [2, 6, 3, 11]. However, the conclusions regarding the resultant effectiveness of using the proposed predictors were inconclusive. In contrast, we do not present a concrete predictor. Rather, we devise a pseudo predictor that enables to control prediction quality, and accordingly set a lower bound on the prediction quality required so as to have our approach outperform the initial ranking.",null,null
20,"Using a simulation study, a lower bound on the prediction quality required for effective selective query expansion was set [8]. While this work focused on performance prediction over queries, our approach relies on prediction over rankings for the same query. Furthermore, our approach is not committed to any ranking paradigm. In addition, rather than use a simulation, we propose a novel pseudo predictor that allows to control prediction quality.",null,null
21,"Finally, we note that some list-wise learning to rank approaches [10] are also based on finding effective permutations of the same list, although not with the Cross Entropy method that we employ. Permutations are explored during the training phase and a ranker is induced. In contrast, our approach employs optimization over permutations as an unsupervised re-ranking mechanism.",null,null
22,3. FRAMEWORK,null,null
23,3.1 Problem Definition,null,null
24,Let Dqk denote the list of the k documents in a corpus D that are the most highly ranked by some initial search performed in response to query q. Let Dqk denote the set of all k! possible permutations (rankings) of the documents in Dqk. Let   Dqk denote a single permutation of Dqk and let (d) further denote the position (rank) of document d ( Dqk) in . Let Q() denote the retrieval performance (effectiveness) of the permutation  ( Dqk ).,null,null
25,"The goal is to find a permutation  ( Dqk ) such that Q() is maximized. Unfortunately, finding an optimal per-",null,null
26,839,null,null
27,"mutation is NP-Complete [1]. In addition, with no prior relevance judgement on the documents in Dqk, the true performance Q() for any given permutation   Dqk is unknown. Hence, the performance of any given permutation can only be predicted, and the task becomes even more challenging.",null,null
28,"Let Q() denote the predicted performance of the permutation  ( Dqk ). Potential predictors may utilize any available pre-retrieval features [9] (e.g., induced from the query q and the corpus D), post-retrieval features (e.g., induced from the result list Dqk or the permutation ) or their combination [4].",null,null
29,3.2 An Optimization Approach,null,null
30,"We next propose an optimization approach that effectively finds ""promising"" permutations which have the best",null,null
31,predicted performance according to a given predictor Q(). Since the resultant retrieval effectiveness of the optimiza-,null,null
32,"tion procedure depends on the prediction quality of the predictor employed, we empirically derive a lower bound for the prediction quality of ""effective"" predictors. That is, if a predictor with a prediction quality higher than the lower bound is used in our approach then the approach is guaranteed to find -- as determined based on the benchmarks we have used -- as a solution a permutation  whose retrieval performance is better than that of the initial ranking.",null,null
33,3.2.1 Optimization using the Cross Entropy method,null,null
34,"We propose a Monte-Carlo optimization approach to our permutation-based re-ranking task. The approach, which we term Cross Entropy Re-ranking Optimization (CERO), uses the Cross Entropy (CE) method [13]. Within the CE method, optimal solutions to hard problems (such as that we try to solve) are modeled as rare events whose occurrence probability is effectively estimated [13]. Given such estimates, optimal solutions can then be efficiently generated. Under a certain condition, the CE method is expected to converge to the optimal solution [12].",null,null
35,We now describe our algorithm and provide its pseudo code in Algorithm 1. The algorithm gets as an input the,null,null
36,"initial ranked list Dqk , a performance predictor Q () and several tuning parameters (mentioned below) that control the learning rate and convergence of the algorithm. The algorithm iteratively explores permutations in Dqk using random sampling. To this end, the algorithm induces a probability space of ""promising"" permutations over Dqk using the feedback it gets about the relative performance of permutations that were explored in previous iterations. It is easy to show that for a given k, a unique bijection exists between the permutation set Dqk and the set of all k! possible Hamiltonian paths in a complete graph with k nodes. Therefore, random permutations can be efficiently drawn by sampling Hamiltonian paths using a simple constrained random walk method [13]. Let P(ti,j) denote the the probability for a single step transition between node i and node j in the graph, which corresponds to the event (dj) ,"" (di) + 1, i.e., document di is ranked in  one position before document dj. With no prior knowledge on the permutations probability space, the algorithm is initialized with the uniform probability (having the maximum entropy) and Dqk is considered as the current best performing permutation. The algorithm's""",null,null
37,Algorithm 1 Cross Entropy Re-ranking Optimization,null,null
38,"1: input: Dqk , Q () , N, , ",null,null
39,2: initialize:,null,null
40,"3: P(0i,j) ,",null,null
41,1 k-1,null,null
42,",",null,null
43,"i,j",null,null
44,"0,",null,null
45,"i,j",null,null
46,"4:  , Dqk 5: t , 1 6: loop 7: Randomly draw N permutations l  Dqk using P t-1",null,null
47,"8: if Q () < maxl,""1,...,N Q (l) then""",null,null
48,9:,null,null
49," , arg maxl,""1,...,N Q(l)""",null,null
50,10: end if,null,null
51,11: Sort permutations l according to Q (l) 12: Let t be the sample (1-)-quantile of the performances:,null,null
52,"Q (l);l ,"" 1, .., N""",null,null
53,"13: for i ,"" 1, . . . , k; j "","" 1, . . . , k do""",null,null
54,14:,null,null
55,"P(ti,j ) ,",null,null
56,"N l,1",null,null
57,I {l (dj,null,null
58,"),l (di )+1}I {Q(l )t }",null,null
59,"N l,1",null,null
60,I {Q(l )t },null,null
61,15:,null,null
62,"P(ti,j ) ,""P(ti-,j1) +(1-)P(ti,j )""",null,null
63,16: end for,null,null
64,17: if t converged then,null,null
65,18:,null,null
66,stop and return ,null,null
67,19: else,null,null
68,20:,null,null
69,"t,t+1",null,null
70,21: end if,null,null
71,22: end loop,null,null
72,"goal is to converge (via cross entropy minimization) to the unknown probability space of optimal permutations, from which optimal solutions can be generated [13].",null,null
73,"On each iteration t, N random permutations are sampled based on the last induced promising permutations probability space P t-1. Next, the predicted performance of each sampled permutation is calculated and the performance of the current best performing permutation  is updated accordingly. ""New"" promising permutations are then explored by first sorting the sampled permutations according to their predicted performance and then updating the transition probabilities based on the top-N  performing samples, whose minimum performance is t. Given t, each transition probability P(ti,j) is induced according to the relative number of permutations out of the top-N  permutations (which have predicted performance equal to or higher than t) that also ranked document di one position above document dj. A fixed smoothing scheme, controlled by parameter , further allows to trade between the exploration (given by P(ti,j)) and the exploitation (given by P(ti-,j1)) of the algorithm.",null,null
74,The algorithm continuous until some convergence criteria is met. In this work we follow the convergence criteria suggested in [13] and stop the algorithm if the sample (1 - )-performance quantile t does not change within several consecutive iterations.,null,null
75,3.2.2 A criterion for effective prediction,null,null
76,"The CERO algorithm is generic and can employ any performance predictor. Naturally, however, the prediction quality of the predictor has significant impact on the retrieval effectiveness of the ranking produced by the algorithm.",null,null
77,"Thus, we turn to devise a method for determining the lower bound of prediction quality that will result in the CERO algorithm outperforming the initial ranking of Dqk. The bound is independent of a prediction approach.",null,null
78,"Herein, we measure retrieval performance using average precision (AP@k); i.e., Q() in our case is the AP of the per-",null,null
79,840,null,null
80,corpus,null,null
81,GOV2 WT10G TREC8,null,null
82,# of documents,null,null
83,"25,205,179 1,692,096 528,155",null,null
84,queries,null,null
85,701-850 451-550 401-450,null,null
86,disks,null,null
87,GOV2 WT10g 4&5-{CR},null,null
88,Table 1: TREC data used for experiments.,null,null
89,"mutation . Following standard practice in work on queryperformance prediction [4], prediction quality is measured by the Pearson correlation between the true AP of permu-",null,null
90,"tations (Q()) and their predicted performance (Q()). To derive a lower bound on prediction quality, we next",null,null
91,"present an approach for generating pseudo AP predictors, whose prediction quality can be controlled. Following previous observations [5], we assume that true AP values follow a normal distribution1.",null,null
92,We first normalize the AP of the permutation  ( Dqk ):,null,null
93,Qnorm(),null,null
94,",",null,null
95,Q() - EDqk,null,null
96,(AP ) ;,null,null
97,(1),null,null
98,V arDqk (AP ),null,null
99,"EDqk (AP ) and V arDqk (AP ) are the expectation and the variance of the true AP values of the permutations in Dqk , respectively. The two statistics can be estimated using max-",null,null
100,"imum likelihood estimation for normal distribution, by sam-",null,null
101,pling a large enough random (uniform) sample of permuta-,null,null
102,"tions in Dqk (e.g., N ,"" 1000). Since AP follows a normal distribution, we get that as k  , for any permutation""",null,null
103,"  Dqk : Qnorm()  N (0, 1) [5]. Proposition 1 defines a -correlated pseudo AP predictor;",null,null
104,"that is, a predictor with a  prediction quality (i.e., Pearson",null,null
105,correlation with true AP). The proof is quite straightforward,null,null
106,and is ommitted due to space considerations.,null,null
107,"Proposition 1. Given a query q, initial result list Dqk, and permutation   Dqk , a -correlated pseudo AP predic-",null,null
108,"tor, denoted Q(), is obtained as follows:",null,null
109,"Q() , Qnorm() + 1 - 2X",null,null
110,(2),null,null
111,"where Qnorm() is the normalized true AP value according to Eq. 1 and X  N (0, 1).",null,null
112,4. EVALUATION,null,null
113,4.1 Setup,null,null
114,"The TREC corpora and queries used for experiments are specified in Table 1. Titles of TREC topics served for queries. The Apache Lucene2 search library (version 4.3) was used for the experiments. Documents and queries were processed using Lucene's default analysis (i.e., tokenization, stemming, stopwords, etc). For each query, 100 documents were retrieved using Lucene's implementation, employed with default free-parameter values, of each of the following retrieval methods: vector space model (TF-IDF), query-likelihood (QL with Dirichlet smoothing) and Okapi BM25. Thus, we",null,null
115,1The assumption was further verified in our experiments using the 2 goodness-of-fit test. Details are omitted due to space considerations. 2http://lucene.apache.org,null,null
116,"obtained three initial lists, Dqk, composed of k ,"" 100 documents, for each query. Mean average precision (MAP@k) serves as the evaluation measure. Statistically significant differences of performance are measured using the paired t-test with a 95% confidence level.""",null,null
117,"Each initial list was re-ranked using the CERO algorithm employed with pseudo AP predictors of varying prediction quality levels. To control prediction quality, the pseudo predictors were generated according to Eq. 2; the prediction quality was varied from  , 0.05 (worst predictor) to  ,"" 1.0 (best predictor). Following previous recommendations [13], the algorithm's learning parameters were set as follows: N "","" 1000,  "", 0.01 and  , 0.7.",null,null
118,"Eff ciency considerations. To implement CERO, we used",null,null
119,"a parallelized version of the Cross Entropy method [7]. On average, CERO converged in 21.32 iterations (with a 15.6 standard deviation). CERO explores at each iteration a maximum of N ,"" 1000 permutations. Thus, all together, CERO considered only about 21k - 36k permutations out of the 100! possible permutations of the initial list.""",null,null
120,4.2 Results,null,null
121,CERO's effectiveness. We first study the potential of our,null,null
122,"permutation-based optimization approach; specifically, in finding highly effective permutations in the huge space (100!) of permutations. To this end, we neturilize the effect of prediction quality by applying CERO with a ""predictor"" which reports the true AP of the considered permutations (i.e.,  , 1.0). The resultant MAP performance is presented in Table 2. As reference comparisons we use the initial ranking and an optimal re-ranking where all relevant documents from the initial list are positioned at the highest ranks.",null,null
123,"We can see in Table 2 that, overall, CERO results in very good approximations. Specifically, CERO's MAP is at least as 91% as good as that of the optimal MAP. Recall from above that CERO explores only a tiny fraction of all permutations of the documents in the result list. It is worth noting that an even better approximation may be obtained by finer tuning of the algorithm (e.g., following [12]). We leave this exploration for future work, and view the results presented in Table 2 as a solid proof of concept for the optimization approach we have employed.",null,null
124,The effect of prediction quality. In Table 3 we present,null,null
125,"the effect of the prediction quality of the pseudo AP predictors used in CERO on its MAP retrieval performance. Evidently, and as should be expected, the higher the prediction quality (), the better the performance. Furthermore, as from  ,"" 0.3 CERO improves over the initial ranking for all the retrieval methods and across all corpora; for   0.35 the improvements are always statistically significant. With higher prediction quality, the improvements over the initial ranking become very substantial.""",null,null
126,5. CONCLUSIONS AND FUTURE WORK,null,null
127,We presented a novel approach to re-ranking an initially retrieved list. The approach is based on applying the Cross Entropy optimization method [13] upon permutations of the list. Query-performance predictors are used to evaluate the performance of permutations. Empirical evaluation pro-,null,null
128,841,null,null
129,"Initial Optimal CERO ( , 1.0)",null,null
130,BM25,null,null
131,.151 .274 .251 (92%),null,null
132,GOV2 QL,null,null
133,.172 .296 .275 (93%),null,null
134,TF-IDF,null,null
135,.137 .253 .232 (91%),null,null
136,BM25,null,null
137,.156 .352 .320 (91%),null,null
138,WT10G QL,null,null
139,.164 .391 .367 (94%),null,null
140,TF-IDF,null,null
141,.158 .349 .322 (92%),null,null
142,BM25,null,null
143,.198 .400 .367 (92%),null,null
144,TREC8 QL,null,null
145,.206 .407 .373 (92%),null,null
146,TF-IDF,null,null
147,.190 .388 .356 (92%),null,null
148,"Table 2: The MAP of the initial retrieval, optimal re-ranking and the CERO algorithm when employed with the true AP as the ""predictor"". The percentages are with respect to Optimal.",null,null
149,"Initial  , 1.0  , .95  , .90  , .85  , .80  , .75  , .70  , .65  , .60  , .55  , .50  , .45  , .40  , .35  , .30  , .25  , .20  , .15  , .10  , .05",null,null
150,BM25,null,null
151,.151 .251 (66%) .249 (65%) .245 (62%) .242 (60%) .237 (57%) .232 (54%) .222 (47%) .222 (47%) .215 (43%) .207 (37%) .199 (32%) .192 (27%) .182 (20%) .170 (12%) .161 (6%),null,null
152,.154 (2%) .143 (-5%) .134 (-11%) .120 (-20%) .119 (-21%),null,null
153,GOV2,null,null
154,QL,null,null
155,.172 .275 (60%) .272 (58%) .267 (56%) .262 (53%) .255 (49%) .248 (44%) .245 (42%) .231 (35%) .229 (33%) .219 (28%) .214 (24%) .202 (18%) .194 (13%) .180 (5%),null,null
156,.173 (1%) .165 (-4%) .150 (-13%) .144 (-16%) .141 (-18%) .137 (-20%),null,null
157,TF-IDF,null,null
158,.137 .232 (69%) .231 (69%) .225 (64%) .225 (64%) .218 (59%) .212 (55%) .208 (52%) .203 (48%) .198 (45%) .194 (42%) .181 (32%) .173 (27%) .172 (25%) .157 (15%),null,null
159,.144 (5%) .135 (-2%) .125 (-8%) .120 (-12%) .117 (-14%) .110 (-20%),null,null
160,BM25,null,null
161,.156 .320 (105%) .320 (105%) .320 (104%) .319 (104%) .311 (99%) .309 (97%) .301 (93%) .301 (92%) .292 (87%) .290 (85%) .280 (79%) .260 (66%) .257 (64%) .211 (35%) .199 (27%) .172 (10%),null,null
162,.151 (-3%) .134 (-14%) .110 (-30%) .79 (-50%),null,null
163,WT10G,null,null
164,QL,null,null
165,.164 .367 (124%) .362 (121%) .361 (120%) .354 (115%) .355 (116%) .342 (108%) .345 (110%) .333 (103%) .328 (100%) .312 (90%) .303 (85%) .272 (66%) .258 (57%) .246 (50%) .222 (35%),null,null
166,.177 (8%) .139 (-15%) .124 (-24%) .119 (-28%) .93 (-43%),null,null
167,TF-IDF,null,null
168,.158 .322 (104%) .321 (104%) .317 (101%) .317 (101%) .313 (99%) .309 (96%) .303 (93%) .296 (88%) .293 (86%) .285 (81%) .284 (81%) .260 (65%) .249 (58%) .225 (43%) .213 (35%),null,null
169,.172 (9%) .130 (-18%) .112 (-29%) .105 (-33%) .103 (-35%),null,null
170,BM25,null,null
171,.198 .367 (85%) .364 (83%) .361 (82%) .358 (81%) .354 (79%) .348 (76%) .342 (72%) .332 (67%) .325 (64%) .311 (57%) .303 (53%) .285 (44%) .267 (35%) .244 (23%) .221 (11%),null,null
172,.201 (1%) .167 (-16%) .145 (-27%) .123 (-38%) .115 (-42%),null,null
173,TREC8,null,null
174,QL,null,null
175,.206 .373 (81%) .371 (80%) .370 (80%) .364 (77%) .362 (76%) .358 (74%) .345 (68%) .344 (67%) .332 (62%) .321 (56%) .310 (51%) .293 (42%) .268 (30%) .248 (21%) .227 (10%) .194 (-6%) .159 (-22%) .148 (-28%) .133 (-35%) .122 (-41%),null,null
176,TF-IDF,null,null
177,.190 .356 (87%) .356 (87%) .354 (86%) .348 (83%) .342 (80%) .339 (78%) .330 (73%) .322 (70%) .317 (67%) .304 (60%) .294 (54%) .278 (46%) .259 (36%) .237 (24%) .212 (12%) .179 (-6%) .165 (-13%) .139 (-27%) .120 (-37%) .109 (-43%),null,null
178,Table 3: The MAP of the initial retrieval and of CERO when using different -correlated pseudo AP predictors. marks a statistically significant improvement over the initial ranking. Numbers in italics are lower than those for the initial ranking. The reported percentages are with respect to the initial ranking.,null,null
179,"vided a proof of concept for our approach. That is, the optimization procedure finds highly effective permutations by exploring only a tiny fraction of the space of all possible permutations. In addition, we devised novel pseudo predictors that allow to carefully control prediction quality and to infer the minimal prediction quality required for our approach to (significantly) outperform the original ranking.",null,null
180,Our main plan for future work is devising query-performance predictors that yield a prediction quality that is higher than that we established as a lower bound for effective application of our approach. We note that almost all previously proposed query-performance predictors [4] are not suited for this task as they operate over different queries rather than over different lists retrieved for the same query.,null,null
181,Acknowledgment,null,null
182,We would like to thank David Carmel for discussions on an earlier version of this work. Oren Kurland's work is supported in part by the Israel Science Foundation under grant no. 433/12 and by a Google faculty research award.,null,null
183,6. REFERENCES,null,null
184,"[1] N. Alon. Ranking tournaments. SIAM Journal on Discrete Mathematics, 20(1):137­142, 2006.",null,null
185,"[2] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. of ECIR, pages 127­137, 2004.",null,null
186,"[3] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.",null,null
187,"[4] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Synthesis Lectures on Information Concepts, Retrieval, and Services. Morgan & Claypool Publishers, 2010.",null,null
188,"[5] B. Carterette, J. Allan, and R. Sitaraman. Minimal test collections for retrieval evaluation. In Proc. of SIGIR, pages 268­275, 2006.",null,null
189,"[6] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. A language modeling framework for selective query expansion. Technical Report IR-338, Center for Intelligent Information Retrieval, University of Massachusetts, 2004.",null,null
190,"[7] G. E. Evans, J. M. Keith, and D. P. Kroese. Parallel cross-entropy optimization. In Proc. of WSC, pages 2196­2202, 2007.",null,null
191,"[8] C. Hauff and L. Azzopardi. When is query performance prediction effective? In Proc. of SIGIR, pages 829­830, 2009.",null,null
192,"[9] C. Hauff, D. Hiemstra, and F. de Jong. A survey of pre-retrieval query performance predictors. In Proc. of CIKM, pages 1419­1420, 2008.",null,null
193,"[10] T.-Y. Liu. Learning to Rank for Information Retrieval. Springer, 2011.",null,null
194,"[11] X. Liu and W. B. Croft. Experiments on retrieval of optimal clusters. Technical Report IR-478, Center for Intelligent Information Retrieval (CIIR), University of Massachusetts, 2006.",null,null
195,"[12] L. Margolin. On the convergence of the cross-entropy method. Annals of Operations Research, 134(1):201­214, 2005.",null,null
196,"[13] R. Y. Rubinstein and D. P. Kroese. The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation and machine learning. Springer, 2004.",null,null
197,842,null,null
198,,null,null

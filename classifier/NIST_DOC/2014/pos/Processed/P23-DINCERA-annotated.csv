,sentence,label,data
,,,
0,Hypothesis Testing for the Risk-Sensitive Evaluation of Retrieval Systems,null,null
,,,
1,B. Taner Dinçer,null,null
,,,
2,"Dept of Statistics & Computer Engineering Mugla University Mugla, Turkey",null,null
,,,
3,dtaner@mu.edu.tr,null,null
,,,
4,Craig Macdonald and Iadh Ounis,null,null
,,,
5,"School of Computing Science University of Glasgow Glasgow, UK",null,null
,,,
6,{firstname.lastname}@glasgow.ac.uk,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"The aim of risk-sensitive evaluation is to measure when a given information retrieval (IR) system does not perform worse than a corresponding baseline system for any topic. This paper argues that risk-sensitive evaluation is akin to the underlying methodology of the Student's t test for matched pairs. Hence, we introduce a risk-reward tradeoff measure TRisk that generalises the existing URisk measure (as used in the TREC 2013 Web track's risk-sensitive task) while being theoretically grounded in statistical hypothesis testing and easily interpretable. In particular, we show that TRisk is a linear transformation of the t statistic, which is the test statistic used in the Student's t test. This inherent relationship between TRisk and the t statistic, turns risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential analysis. Specifically, we demonstrate using past TREC data, that by using the inferential analysis techniques introduced in this paper, we can (1) decide whether an observed level of risk for an IR system is statistically significant, and thereby infer whether the system exhibits a real risk, and (2) determine the topics that individually lead to a significant level of risk. Indeed, we show that the latter permits a state-of-the-art learning to rank algorithm (LambdaMART) to focus on those topics in order to learn effective yet risk-averse ranking systems.",Y,null
,,,
9,Categories and Subject Descriptors: H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval; G3.3 [Probability and Statistics]: Experimental design,null,null
,,,
10,"Keywords: Risk-Sensitive Evaluation, Student's t Test",null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"Various paradigms for the evaluation of information retrieval (IR) systems rely on many topics to produce reliable estimates of their effectiveness. For instance, in the TREC series of evaluation forums, 50 topics is generally seen as the minimum for producing a reliable test collection [2, 25]. However, in more recent times, the evaluation of systems has increasingly focused upon their robustness - ensuring that a given IR system performs well on difficult topics (as",Y,null
,,,
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609625 .",null,null
,,,
14,"investigated by the TREC Robust track [24]), or at least as well as a baseline system (which is known as risk-sensitive evaluation [26]). Recently, the TREC 2013 Web track introduced a risk-sensitive task, which assessed how systems could perform effectively yet without exhibiting large losses compared to a pre-determined baseline system [10].",Y,null
,,,
15,"In such a risk-sensitive evaluation, the risk associated with an IR system is defined as the risk of performing a given particular topic less effectively than a given baseline system [8, 9, 26]. In particular, the URisk risk-sensitive evaluation measure [26] calculates the absolute difference of an effectiveness measure (e.g. NDCG) between a given retrieval system and the baseline system, in a manner that more strongly emphasises decreases with respect to the baseline (known as risk) than gains (reward). A parameter   0 controls the riskreward tradeoff towards losses in effectiveness compared to the baseline, where  , 0 weights risk and rewards equally.",null,null
,,,
16,"In this paper, we argue that in the current practice of risk-sensitive evaluation based on URisk, any amount of loss in an IR system's average effectiveness, observed on a particular set of topics, is considered enough in magnitude to infer that the system exhibits a ""real risk"". However, from a statistical viewpoint, such an inferential decision may be said to be valid only if the observed amount of loss cannot be attributed to chance fluctuation. Otherwise, it will be equally likely that the corresponding system may or may not be under a real risk, meaning that it is possible that the system can perform every topic with a score higher than that of the baseline system on another set of topics that could be drawn from the population of topics. On the other hand, it is also possible that the observed amount of loss in a particular system's average effectiveness can be attributed to a chance fluctuation, while the corresponding performance losses for some individual topics are statistically significant in magnitude. In other words, significant performance losses for a few topics may not result in a significant total loss on average, given a relatively large set of topics.",null,null
,,,
17,"Hence, we advocate that risk-sensitive evaluation can actually provide the necessary basis for (i) testing the significance of the observed amount of loss in a given IR system's average effectiveness, called inferential risk analysis in this paper, and (ii) testing the significance of the corresponding losses for individual topics, called exploratory risk analysis.",null,null
,,,
18,"Indeed, we show that the URisk risk-reward tradeoff measure is actually a linear transformation of the t statistic, as used in the Student's t test. Therefore, using this statistical interpretation of URisk based upon hypothesis testing, this paper proposes a new risk-reward tradeoff measure, TRisk, which is a linear transformation of the existing URisk measure, yet is theoretically grounded upon the Student's t test",null,null
,,,
19,23,null,null
,,,
20,"for testing the significance of the observed amount of loss in a given IR system's average effectiveness. For  ,"" 0, TRisk is equivalent to the standard t statistic used typically in the Student's t test for testing the null hypothesis of equality in the population mean effectiveness for two IR systems. However, for  > 0, the URisk measure emphasises performance losses compared to the baseline effectiveness. This raises challenges in the estimation of the standard error of the calculated URisk scores. For this reason, we propose the use of the Jackknife technique (or leave-one-out) [11], which is a re-sampling technique for estimating the bias and the standard error of any estimate. The Jackknife technique serves two purposes: firstly, to allow the empirical verification of the estimation of the standard error of URisk as valid; and secondly, for testing the significance of the corresponding performance losses for individual topics.""",null,null
,,,
21,"From a practical perspective, a risk-sensitive evaluation serves two objectives: firstly, as a step further than the classical evaluation of IR systems, which takes into account the stability or variance of retrieval results across queries as well as for the average retrieval effectiveness [8, 9]; and secondly, as a technique for jointly optimising the retrieval effectiveness and robustness of retrieval frameworks such as learning to rank [26]. Indeed, compared to the existing URisk measure, this paper contributes to both objectives, by exploiting the theory of statistical hypothesis testing for allowing meaningful interpretation of risk-sensitive evaluation scores, and also by allowing a learning to rank technique, namely LambdaMART, to focus on those topics that lead to a significant level of risk, in order to learn effective yet risk-averse ranking systems. The remainder of this paper is structured as follows: Section 2 provides an overview of risk-sensitive evaluation practices, including URisk; Section 3 relates the URisk measure to the t statistic, and hence proposes the new TRisk risk-sensitive evaluation measure, and discusses the estimation of the standard error. Section 4 and Section 5 describe new forms of analysis, inferential and exploratory respectively, that arise from the TRisk measure, and demonstrate their application upon the TREC 2012 Web track. Next, Section 6 shows how TRisk can improve the robustness of the LambdaMART state-of-the-art learning to rank technique. Finally, we review some related work and provide concluding remarks in Sections 7 & 8, respectively.",Y,null
,,,
22,2. RISK-SENSITIVE EVALUATION,null,null
,,,
23,"Different approaches in IR such as query expansion [1, 5] and learning to rank [17] behave differently across topics, often improving the effectiveness for some of the topics while degrading performance for others. This results in a high variation in effectiveness across the topics. To address such variation, there has been an increasing focus on the effective tackling of difficult topics in particular (e.g. through the TREC Robust track [23]), or more recently, on the risksensitive evaluation of systems across many topics [8, 9, 26].",Y,null
,,,
24,"Originally, the aim of risk-sensitive evaluation [9] was to provide new analysis techniques for quantifying and visualising the risk-reward tradeoff of any retrieval strategy that requires a balance between risk and reward. Hence, it facilitates the quest for ranking strategies that are more robust in retrieval effectiveness compared to a baseline retrieval strategy ­ robust in the sense of the stability or variance of the retrieval results across topics, while achieving good average performance over all topics.",null,null
,,,
25,The variance with respect to a given baseline system b over a given set of topics Q with c topics can then be measured as,null,null
,,,
26,"a risk function FRisk, which takes into account the downsiderisk of a new system r (i.e. performing a topic worse than the baseline) is defined in [26] as follows:",null,null
,,,
27,FRisk,null,null
,,,
28,",",null,null
,,,
29,1 c,null,null
,,,
30,c,null,null
,,,
31,"max [0, (bi - ri)] ,",null,null
,,,
32,-1,null,null
,,,
33,"i,1",null,null
,,,
34,"where ri and bi are respectively the score of the system r and the score of the baseline system b on topic i, as measured by a retrieval effectiveness measure (e.g. NDCG@20, ERR@20 [6]). Similarly, a reward function FReward, which takes into account the upside-risk (i.e. performing a topic better than the baseline) is defined as:",null,null
,,,
35,FReward,null,null
,,,
36,",",null,null
,,,
37,1 c,null,null
,,,
38,c,null,null
,,,
39,"max [0, (ri - bi)] .",null,null
,,,
40,-2,null,null
,,,
41,"i,1",null,null
,,,
42,"Thereby, the overall gain in the retrieval effectiveness of r with respect to b can be expressed as:",null,null
,,,
43,"UGain , FReward - FRisk.",null,null
,,,
44,-3,null,null
,,,
45,"Next, a single measure, URisk [26], which allows the riskreward tradeoff to be adjusted, was defined:",null,null
,,,
46,"URisk , UGain -  · FRisk",null,null
,,,
47,",",null,null
,,,
48,1 c,null,null
,,,
49,q + (1 + ),null,null
,,,
50,"q , (4)",null,null
,,,
51,qQ+,null,null
,,,
52,qQ -,null,null
,,,
53,"where q ,"" rq - bq. The left summand in the square brackets, which is the sum of the score differences q for all q where rq > bq (i.e., q  Q+), gives the total win (or upsiderisk) with respect to the baseline. Orthogonally, the right summand, which is the sum of the score differences q for all q where rq < bq, gives the total loss (or downside-risk). The risk sensitivity parameter   0 controls the tradeoff between reward and risk (or win and loss):  "","" 0 results in a pure gain model, while for higher , the penalty for under-performing with respect to the baseline is increased: typically  "","" 1, 5, 10 [10].""",null,null
,,,
54,"In this paper, we extend the original aforementioned aim of risk-sensitive evaluation with the following contributions: 1. A well-established statistical hypothesis testing theory for risk-sensitive evaluations from which arises a new risk measure TRisk (Section 3), to turn risk-sensitive evaluation from a descriptive analysis to a fully-fledged inferential analysis (Section 4). 2. A method for exploratory risk analysis that can identify the topics that commit real levels of risk (Section 5). 3. Adaptations of the proposed TRisk measure that can enhance the robustness of the state-of-the-art LambdaMART learning to rank technique, compared to URisk, without degradations in overall effectiveness, where the learned model adaptively adjusts with respect to the risk level committed by individual topics (Section 6).",null,null
,,,
55,3. THE NEW TRISK MEASURE,null,null
,,,
56,"Without loss of generality, at  ,"" 0, the risk-reward tradeoff measure URisk reduces to the UGain formula in Eq. (3), which can be expressed as the average gain over c topics:""",null,null
,,,
57,UGain,null,null
,,,
58,",",null,null
,,,
59,1 c,null,null
,,,
60,c,null,null
,,,
61,i,null,null
,,,
62,",",null,null
,,,
63,1 c,null,null
,,,
64,c,null,null
,,,
65,(ri - bi).,null,null
,,,
66,-5,null,null
,,,
67,"i,1",null,null
,,,
68,"i,1",null,null
,,,
69,"In the context of statistics, UGain refers to the sample mean of paired score differences, d¯, for two IR systems (the system under evaluation r and the baseline system b):",null,null
,,,
70,24,null,null
,,,
71,"d¯ ,",null,null
,,,
72,r¯ - ¯b,null,null
,,,
73,",",null,null
,,,
74,1 c,null,null
,,,
75,c,null,null
,,,
76,"(ri - bi) , UGain",null,null
,,,
77,-6,null,null
,,,
78,"i,1",null,null
,,,
79,"and in the context of evaluating IR systems, this refers to the difference in average effectiveness between two IR systems, r¯-¯b, where r¯ and ¯b are respectively the average effectiveness of system r and the average effectiveness of the baseline system b over c topics.",null,null
,,,
80,"On the other hand, the Student's t statistic for matched pairs, as is commonly applied when testing the significance of results between two systems, can be expressed as:",null,null
,,,
81,t,null,null
,,,
82,",",null,null
,,,
83,d¯ SE(d¯),null,null
,,,
84,",",null,null
,,,
85,r¯ - ¯b SE(d¯),null,null
,,,
86,",",null,null
,,,
87,-7,null,null
,,,
88,"Within Eq. (7), the standard error of paired sample mean, SE(d¯), can be estimated as follows:",null,null
,,,
89,SE(d¯),null,null
,,,
90,",",null,null
,,,
91,sd c,null,null
,,,
92,",",null,null
,,,
93,-8,null,null
,,,
94,"where sd , c-1 (i - d¯)2 is the paired sample standard",null,null
,,,
95,"deviation. Hence, we argue that the Student's t statistic of",null,null
,,,
96,Eq. (7) is actually a linear transformation of UGain from,null,null
,,,
97,"Eq. (3), which we call TGain:",null,null
,,,
98,TGain,null,null
,,,
99,",",null,null
,,,
100,UGain S E (UGain ),null,null
,,,
101,",",null,null
,,,
102,c sd,null,null
,,,
103,×,null,null
,,,
104,UGain .,null,null
,,,
105,-9,null,null
,,,
106,"This transformation can be referred to as studentisation (c.f., t-scores) [14], which in fact is a type of standardisation (i.e., z-scores). Standardisation is a monotonic linear transformation, which transforms any given set of data to a set with zero mean and unit variance, while preserving the original data distribution in shape.",null,null
,,,
107,"The t-score of a raw UGain measurement, TGain, differs from the raw measurement in two important aspects. First, given a set of IR systems, a test collection, and a baseline system, the systems' ranking to be obtained on the basis of TGain will not necessarily be concordant with the systems' ranking to be obtained on the basis of UGain, since the t statistic takes into account the inherent variation in the observed paired score differences ri - bi across the topics, i.e., SE(UGain). Second, given a particular baseline system, the two TGain scores to be obtained on two different test collections for the same IR system are comparable with each other in magnitude, at least in theory [7], while the two UGain scores are not, as typical in the case of the two raw effectiveness scores to be yielded from a standard effectiveness measure, such as mean average precision [28].",null,null
,,,
108,"Having shown how TGain can be defined as a linear transformation of UGain, based upon the t statistic, we now examine URisk, which allows the risk-reward tradeoff to be controlled by the  parameter. For   0, the t statistic based on URisk, which we call TRisk, can be expressed as follows:",null,null
,,,
109,TRisk,null,null
,,,
110,",",null,null
,,,
111,URisk S E (URisk,null,null
,,,
112,),null,null
,,,
113,.,null,null
,,,
114,-10,null,null
,,,
115,"Although both the TGain formula in Eq. (9) and the TRisk formula in Eq. (10) stem from the classical t statistic in Eq. (7), the estimation of the standard error in URisk, the estimation of SE(URisk) within TRisk, is not as straightforward as in the case of SE(UGain), for the reason that the URisk formula reweighs the score differences i in averaging, proportionally to , for each topic i where ri < bi, as opposed to UGain. Hence, in the remainder of this section, we propose two methods to estimate SE(URisk): A",null,null
,,,
116,"speculative parametric estimator SEx¯ that is an analogy to the paired sample standard deviation sd (Section 3.1); and a nonparametric Jackknife Estimator SEJ , based on the leaveone-out Jackknife technique (Section 3.2). Indeed, later in Section 3.3, we use the Jackknife Estimator SEJ to show the validity of the speculative SEx¯ estimator.",null,null
,,,
117,"On the other hand, TRisk has several advantages over URisk. Firstly, it can be easily interpreted for an inferential analysis of risk. Indeed, we will later show in Section 4 that in order to test the significance of an observed riskreward tradeoff score between a particular IR system and a provided baseline system, one can use TRisk as the test statistic of the Student's t test for matched pairs.",null,null
,,,
118,"Secondly, TRisk permits the identification of topics that commit significant risk or not ­ we call this exploratory risk analysis ­ which we present later in Section 5.",null,null
,,,
119,"Finally, this exploratory risk analysis leads to new risksensitive measures that can be directly integrated into the LambdaMART learning to rank technique, to produce learned models that exhibit less risk than those obtained from URisk whilst not degrading effectiveness, as explained in Section 6.",null,null
,,,
120,3.1 Parametric Estimator of SE(URisk),null,null
,,,
121,Let the random variable Xi denote the risk-reward tradeoff score between system r and baseline b for topic i:,null,null
,,,
122,"Xi ,",null,null
,,,
123,i (1 + )i,null,null
,,,
124,if ri > bi if ri < bi,null,null
,,,
125,-11,null,null
,,,
126,"for i ,"" 1, 2, . . . , c and a predefined value of   0. Then, the""",null,null
,,,
127,"standard error of URisk, SE(URisk) can be approximated by the standard error of the sample mean x¯:",null,null
,,,
128,SEx¯,null,null
,,,
129,",",null,null
,,,
130,sx c,null,null
,,,
131,",",null,null
,,,
132,-12,null,null
,,,
133,"where s2x ,"" c-1 (xi - x¯)2. Here, the sample mean x¯ corresponds to the URisk score considered as the arithmetic mean of the sample of the observed individual topic risk-reward""",null,null
,,,
134,"tradeoff scores x1, x2, . . . , xc at a predefined value of :",null,null
,,,
135,x¯,null,null
,,,
136,",",null,null
,,,
137,URisk,null,null
,,,
138,",",null,null
,,,
139,1 c,null,null
,,,
140,c,null,null
,,,
141,xi.,null,null
,,,
142,"i,1",null,null
,,,
143,-13,null,null
,,,
144,"This parametric estimator of SE(URisk), SEx¯, is speculative and hence its validity might be compromised to some extent. Therefore, we empirically verify the validity of SEx¯ in estimating SE(URisk) by means of comparing it with a nonparametric re-sampling technique, called the Jackknife [21], which we present in Section 3.2. Indeed, by comparing the two estimates of SE(URisk) (i.e., the parametric estimate SEx¯ of Eq. (12) and the nonparametric Jackknife estimate of SE(URisk)), one can decide whether an inference to be made on the basis of the TRisk statistic is valid. If the two estimates agree with each other, such an inference may be said to be valid, otherwise its validity is compromised.",null,null
,,,
145,3.2 Jackknife Estimate of SE(URisk),null,null
,,,
146,"In this paper, the Jackknife technique is employed for a purpose which serves two different aims: 1) as a mechanism of the empirical verification of the validity of an inference to be made based on the TRisk statistic in Eq. (10), and 2) as a mechanism for exploratory risk analysis.",null,null
,,,
147,"Jackknife, which is also known as the Quenouille-Tukey Jackknife or leave-one-out, was first introduced by Quenouille [18] and then developed by Tukey [21]. Tukey used the Jackknife technique to determine how an estimate is affected by the subsets of observations when discordant values",null,null
,,,
148,25,null,null
,,,
149,"(i.e., outlier data) are present. In the presence of discordant values, it is expected that the Jackknife technique could reduce the bias in the estimate. Although the original objective of Jackknife is to detect outliers, in principle it is a re-sampling technique for estimating the bias and the standard error of any estimate [11]. In Jackknife, the same test is repeated by leaving one subject out each time: this explains why this technique is also referred to as leave-one-out.",null,null
,,,
150,"Let the random variables X1, X2, . . . , Xc denote a random sample of size c, such that Xi is drawn identically and independently from a distribution F for i ,"" 1, 2, . . . , c. Suppose that the goal is to estimate an unknown parameter  of F . It can be shown that  can be estimated by a statistic ^, which is derived from an observed sample x1, x2 . . . , xc from F , with a measurable amount of sampling error [15].""",null,null
,,,
151,"An unbiased estimator ^ is a statistic whose expected value E(^) is equal to the true value of the population parameter of interest , i.e., E(^) , . The amount of bias associated with an estimator is therefore given by:",null,null
,,,
152,"bias(^) , E(^ - ) , E(^) - .",null,null
,,,
153,-14,null,null
,,,
154,"We denote as X(i) the sub-sample without the datum Xi. There are in total c sub-samples of size c-1 for i ,"" 1, 2, . . . , c:""",null,null
,,,
155,"X(i) ,"" X1, X2, . . . , Xi-1, Xi+1, . . . , Xc. Next, let the estimate derived from the ith sub-sample X(i)""",null,null
,,,
156,"be denoted as ^(i), and the mean over c sub-samples be:",null,null
,,,
157,^(.),null,null
,,,
158,",",null,null
,,,
159,1 c,null,null
,,,
160,c,null,null
,,,
161,^(i).,null,null
,,,
162,"i,1",null,null
,,,
163,-15,null,null
,,,
164,"The Jackknife estimate of bias, which is actually a nonparametric estimate of E(^ - ), is defined as follows [21]:",null,null
,,,
165,biasJ (^),null,null
,,,
166,",",null,null
,,,
167,(c -,null,null
,,,
168,1)(^(.),null,null
,,,
169,-,null,null
,,,
170,^),null,null
,,,
171,",",null,null
,,,
172,(c - 1) c,null,null
,,,
173,c,null,null
,,,
174,(^(i) - ^).,null,null
,,,
175,"i,1",null,null
,,,
176,"and, in accordance, the bias-reduced Jackknife estimate of  is defined as ~ , ^ - biasJ (^) , c^ - (c - 1)^(.).",null,null
,,,
177,Tukey [21] showed that the Jackknife technique can also,null,null
,,,
178,"be used to estimate the variance of ^ by introducing the so-called pseudo-values, ~(i) ,"" c^ - (c - 1)^(i), such that""",null,null
,,,
179,varJ (^),null,null
,,,
180,",",null,null
,,,
181,1 c(c - 1),null,null
,,,
182,c,null,null
,,,
183,~(i),null,null
,,,
184,- ~,null,null
,,,
185,2,null,null
,,,
186,",",null,null
,,,
187,(c - 1) c,null,null
,,,
188,c,null,null
,,,
189,^(i) - ^(.),null,null
,,,
190,2,null,null
,,,
191,.,null,null
,,,
192,"i,1",null,null
,,,
193,"i,1",null,null
,,,
194,This nonparametric Jackknife estimate of variance gives the empirical estimate of the standard error of ^:,null,null
,,,
195,"SE(^) , varJ (^).",null,null
,,,
196,-16,null,null
,,,
197,"For the TRisk statistic in Eq. (10), the standard error of",null,null
,,,
198,"URisk, SE(URisk), can hence be estimated by substituting URisk into Eq. (16) as ^:",null,null
,,,
199,"SEJ , varJ (URisk).",null,null
,,,
200,-17,null,null
,,,
201,3.3 Empirical Validation of SE(URisk),null,null
,,,
202,"The nonparametric estimator SEJ is an alternative to the parametric estimator SEx¯ (Eq. (12)). In this section, we empirically compare these estimates of SE(URisk) with each other, to assess the validity of the result of a hypothesis test to be performed using TRisk as the test statistic. In general, if the two estimates agree, the test result may be said to be valid, and otherwise its validity will be compromised. As a result, nonparametric methods can help to alleviate doubts about the validity of the analysis performed [14].",null,null
,,,
203,"In the following, we compare the estimates using the submitted runs to the TREC Web track. In particular, the provided baseline run for the TREC 2013 Web track risksensitive task is based on the Indri retrieval platform. However, as the submitted runs and results for the TREC 2013 campaign were not yet publicly available at the time of writing, in the following we perform an empirical study based on runs submitted to the TREC 2012 Web track. Indeed, the 2013 track coordinators have made available a set of Indri runs on the TREC 2012 Web track topics1 that correspond to the TREC 2013 baseline runs - in our results, we use the 2012 equivalent run to the 2013 pre-determined baseline, the so-called indriCASP. We report the URisk values obtained using the official TREC 2012 evaluation measure, ERR@20.",Y,null
,,,
204,"Table 1 reports the parametric estimates (SEx¯) and the nonparametric Jackknife estimates (SEJ ) of the standard errors associated with the average risk-reward tradeoff scores (URisk), calculated for each of the TREC 2012 Web track top 8 ad-hoc runs over c ,"" 50 topics, with respect to the indriCASP baseline, applying several risk-sensitivity parameter values of  "","" 0, 1, 5, 10. From the results, it can be observed that the two estimates, SEx¯ and SEJ agree with each other for each of the 8 runs. In fact, over all of the 48 runs submitted to the TREC 2012 Web track, we observe a Root Mean Square Error (RMSE) of 0.000 between SEx¯ and SEJ . Thus, we conclude that it is highly likely that it would be valid to conduct an inferential risk analysis upon those TREC 2012 runs based on the new risk-reward tradeoff measure TRisk (Eq. (10)), regardless of how SE(URisk) is estimated. An example of inferential risk analysis based on TRisk follows in the next section.""",Y,null
,,,
205,4. INFERENTIAL RISK ANALYSIS,null,null
,,,
206,"The goal of the classical evaluation of IR systems is to decide whether one IR system is better in retrieval effectiveness than another on the population of topics. This goal can be formulated into a (two-sided) null hypothesis, as given by:",null,null
,,,
207,"H0 : µr , µb or H0 : µr - µb ,"" 0,""",null,null
,,,
208,-18,null,null
,,,
209,"against the alternative hypothesis H1 : µr ,"" µb, where µr and µb represent respectively the population mean performance of the system r and the population mean performance of the baseline system b. The test statistic for this null hypothesis is the t statistic (Eq. (7)), since the larger values of t are evidence against the null hypothesis H0 : µr - µb "","" 0. Below, we describe the hypothesis testing of H0 in abstract terms, before explaining how it can be applied to TRisk (Section 4.1) and illustrating its application upon the TREC 2012 Web track runs (Section 4.2).""",Y,null
,,,
210,"In order to decide how much difference between the two sample means r¯ and ¯b is assumed to be large enough to reject the null hypothesis, we should first determine how much difference can be attributed to a chance fluctuation. It can be shown that, under the null hypothesis H0, the sampling (or null) distribution of the test statistic t can be approximated by a Student's t distribution with df ,"" c - 1 degrees of freedom for any population distribution with finite mean µ and variance 2 > 0, because of the central limit theorem [12]. Thus, at a predefined significance level of  (typically  "","" 0.05 for 95% confidence), two standard deviations (±t(/2,df) × SE(d¯)) determine the maximum difference that can be attributed to chance fluctuation, where in between the critical values ±t(/2,df) the area under the Student's t""",null,null
,,,
211,1https://github.com/trec-web/trec-web-2013,Y,null
,,,
212,26,null,null
,,,
213,"Table 1: Calculated risk-reward tradeoff scores, URisk for the TREC 2012 Web track top 8 ad-hoc runs at the risk-sensitivity parameter values of  ,"" 0, 1, 5, 10, along with the parametric estimates SEx¯ and the nonparametric Jackknife estimates SEJ of the associated standard errors SE(URisk). indriCASP is the baseline.""",Y,null
,,,
214,",0",null,null
,,,
215,",1",null,null
,,,
216,",5",null,null
,,,
217," , 10",null,null
,,,
218,ERR@20 URisk SEx¯ SEJ URisk SEx¯ SEJ URisk SEx¯ SEJ URisk SEx¯ SEJ,null,null
,,,
219,uogTrA44xi,null,null
,,,
220,0.3132 0.1185 0.0528 0.0528 0.0556 0.0739 0.0739 -0.1959 0.1755 0.1755 -0.5104 0.3091 0.3091,null,null
,,,
221,srchvrs12c09,null,null
,,,
222,0.3049 0.1102 0.0479 0.0479 0.0679 0.0644 0.0644 -0.1015 0.1489 0.1489 -0.3133 0.2619 0.2619,null,null
,,,
223,DFalah121A,null,null
,,,
224,0.2920 0.0974 0.0425 0.0425 0.0467 0.0632 0.0632 -0.1558 0.1588 0.1588 -0.4089 0.2827 0.2827,null,null
,,,
225,QUTparaBline,null,null
,,,
226,0.2901 0.0954 0.0448 0.0448 0.0385 0.0672 0.0672 -0.1893 0.1703 0.1703 -0.4740 0.3033 0.3033,null,null
,,,
227,utw2012fc1,null,null
,,,
228,0.2195 0.0248 0.0449 0.0449 -0.0558 0.0705 0.0705 -0.3782 0.1849 0.1849 -0.7813 0.3314 0.3314,null,null
,,,
229,ICTNET12ADR2 0.2149 0.0203 0.0416 0.0416 -0.0495 0.0637 0.0637 -0.3286 0.1648 0.1648 -0.6774 0.2950 0.2950,null,null
,,,
230,indriCASP,null,null
,,,
231,0.1947,null,null
,,,
232,*,null,null
,,,
233,*,null,null
,,,
234,*,null,null
,,,
235,*,null,null
,,,
236,*,null,null
,,,
237,*,null,null
,,,
238,*,null,null
,,,
239,*,null,null
,,,
240,*,null,null
,,,
241,*,null,null
,,,
242,*,null,null
,,,
243,*,null,null
,,,
244,irra12c,null,null
,,,
245,0.1723 -0.0223 0.0410 0.0410 -0.1182 0.0693 0.0693 -0.5014 0.1904 0.1904 -0.9805 0.3437 0.3437,null,null
,,,
246,qutwb,null,null
,,,
247,0.1659 -0.0287 0.0462 0.0462 -0.1342 0.0791 0.0791 -0.5560 0.2194 0.2194 -1.0832 0.3969 0.3969,null,null
,,,
248,"distribution sums up to (1 - ). If an observed t-score is greater than t(/2,df), or less than -t(/2,df), one can reject H0 with 100%(1 - ) confidence, denoted as the p-value.",null,null
,,,
249,4.1 Inference Based on TGain and TRisk,null,null
,,,
250,"The above protocol of hypothesis testing is referred to as the Student's t test for matched pairs, or paired t test for short, in statistics. Hence, in the context of risk-sensitive evaluation, the TGain formula in Eq. (9) stands for the test statistic t. In fact, at  ,"" 0, testing the significance of an observed risk-reward tradeoff score between r and b (i.e. an observed UGain score) is akin to testing the significance of the observed difference between r¯ and ¯b.""",null,null
,,,
251,"To test the significance of an observed UGain score, one can therefore compare the corresponding TGain score with the two-sided critical ±t(/2,df) values at a desired level of significance . If -t(/2,df)  TGain  t(/2,c-1), the observed UGain score can be attributed to chance fluctuation, meaning that the observed gain in the performance of the system r with respect to the baseline system b is not statistically significant. In such a case, it is equally likely that the observed UGain score may or may not occur on another topic sample drawn from the population. Otherwise, if TGain  -t(/2,c-1) or TGain  t , (/2,c-1) one can however be sure that a UGain score at least as extreme as the observed score would occur on 100(1 - )% of the topic samples that could be drawn from the population.",null,null
,,,
252,"Both TGain and TRisk stem from the t statistic. Indeed, for  ,"" 0, TGain "","" TRisk, while for  > 0, SE(URisk) was shown to be valid in Section 3.3. Hence, we argue that an equivalent inferential analysis can be conducted upon the TRisk scores that have been calculated based on URisk. In the following, we provide an illustration of such inferential analysis upon runs submitted to the TREC 2012 Web track, but the same inferential analysis methodology could be applied for any risk-sensitive evaluation scenario.""",Y,null
,,,
253,4.2 Inferential Analysis of Web Track Runs,null,null
,,,
254,"Given a particular IR system, a baseline system, and a set of c topics, one can use the paired t test for testing the significance of the calculated average tradeoff score between risk and reward over the c topics, URisk, by comparing the corresponding t-score, TRisk, with the critical values ±t(/2,df) at a desired level of significance . To illustrate such an analysis, Table 2 reports the URisk risk-reward tradeoff scores based on ERR@20, and the corresponding TRisk scores for the 8 highest performing TREC 2012 ad-hoc runs, given the baseline run indriCASP (we omit other submitted runs for brevity, however the following analysis would be equally applicable to them). As the TREC 2012 Web track has 50",Y,null
,,,
255,"topics, for a significance level of  ,"" 0.05, the critical values for TRisk are ±t(0.025,49) "", ±2.",null,null
,,,
256,"In Table 2, the URisk scores to which a two-sided paired t test gives significance are those that have a corresponding TRisk score less than -2 or greater than +2. For example, at  ,"" 0, the calculated URisk scores of the top 4 runs are significant with a p-value less than 0.05. This means that, under the null hypothesis H0 : µr "","" µb, given another sample of 50 topics from the population, the probability of observing a risk-reward tradeoff score, between any one of these 4 runs and the baseline run indriCASP, that is as extreme or more extreme than the one that was observed is less than 0.05, i.e. the associated p-values. Since TRisk > 0, for those runs, the declared significance counts in favour of """"reward"""" against """"risk"""". Thus, one can conclude, with 95% confidence, that the expected per topic effectiveness of each of the top 4 runs is, on average, higher than the expected per topic effectiveness of the baseline run indriCASP on the population of topics. In other words, given a topic from the population, it is highly likely that any one of the top 4 runs will not perform worse for that topic than indriCASP. This suggests, as a result, that those top runs do not exhibit a real risk that is generalisable to the population of topics.""",null,null
,,,
257,"On the other hand, a run with TRisk < -2 at  ,"" 0 will be under a real risk, though among the shown top 8 TREC 2012 runs there is no such run. For those runs with -2  TRisk < +2, such as utw2012fc1 and qutwb, the risk analysis performed here is inconclusive, since the associated URisk scores can be attributed to chance fluctuation, i.e. it is equally likely that they may or may not be under a real risk.""",Y,null
,,,
258,"Next, we observe from Table 2 that as  increases, the observed tradeoffs between risk and reward for each run changes in favour of risk compared to reward, hence the runs exhibiting significant URisk scores change. For example, each of the runs with significant URisk scores at  ,"" 0 (i.e., the top 4 runs) have a URisk score that can be attributed to a chance fluctuation at  "","" 10, while, in contrast, those runs whose URisk scores can be attributed to chance fluctuation at  "","" 0 (i.e., the last 4 runs) have a significant URisk score at  "", 10.",null,null
,,,
259,"Figure 1 shows the change in the TRisk scores of the TREC 2012 top 8 ad-hoc runs for several risk-sensitivity  parameter values from 0 to 15. From the figure, we observe that for  > 5 the TRisk scores for all runs are negative in sign, and for the last 4 runs the calculated URisk scores can be considered statistically significant (i.e., TRisk > -2.0 for  > 5). It is also observed that, even for  ,"" 15, the calculated URisk scores of the top 4 TREC runs can still be attributed to chance fluctuation.""",Y,null
,,,
260,"As a result, the inferential analysis performed so far suggests that, in general, none of the 8 top TREC 2012 ad-hoc",Y,null
,,,
261,27,null,null
,,,
262,"Table 2: URisk and TRisk scores risk-reward tradeoff scores for the top 8 TREC 2012 ad-hoc runs at  ,"" 0, 1, 5, 10,""",null,null
,,,
263,where the baseline is indriCASP. The underlined URisk scores are those for which a two-tailed paired t test,null,null
,,,
264,gives significance with p < 0.05 - i.e. exhibit a TRisk score greater than +2 or less than -2.,null,null
,,,
265,",0",null,null
,,,
266,",1",null,null
,,,
267,",5",null,null
,,,
268," , 10",null,null
,,,
269,URisk TRisk p-value URisk TRisk p-value URisk TRisk p-value URisk TRisk p-value,null,null
,,,
270,uogTrA44xi,null,null
,,,
271,0.1185 2.2440 0.029 0.0556 0.7528 0.455 -0.1959 -1.1163 0.270 -0.5104 -1.6512 0.105,null,null
,,,
272,srchvrs12c09,null,null
,,,
273,0.1102 2.3034 0.026 0.0679 1.0541 0.297 -0.1015 -0.6817 0.499 -0.3133 -1.1961 0.237,null,null
,,,
274,DFalah121A,null,null
,,,
275,0.0974 2.2899 0.026 0.0467 0.7401 0.463 -0.1558 -0.9808 0.332 -0.4089 -1.4466 0.154,null,null
,,,
276,QUTparaBline 0.0954 2.1305 0.038 0.0385 0.5723 0.570 -0.1893 -1.1116 0.272 -0.4740 -1.5626 0.125,null,null
,,,
277,utw2012fc1,null,null
,,,
278,0.0248 0.5526 0.583 -0.0558 -0.7914 0.432 -0.3782 -2.0457 0.046 -0.7813 -2.3574 0.022,null,null
,,,
279,ICTNET12ADR2 0.0203 0.4869 0.629 -0.0495 -0.7775 0.441 -0.3286 -1.9942 0.052 -0.6774 -2.2960 0.026,null,null
,,,
280,irra12c,null,null
,,,
281,-0.0223 -0.5446 0.588 -0.1182 -1.7038 0.095 -0.5014 -2.6335 0.011 -0.9805 -2.8525 0.006,null,null
,,,
282,qutwb,null,null
,,,
283,-0.0287 -0.6226 0.536 -0.1342 -1.6956 0.096 -0.5560 -2.5342 0.015 -1.0832 -2.7295 0.009,null,null
,,,
284,Figure 1: The change in standardised TRisk scores for the top TREC 2012 ad-hoc runs for 0    15.,Y,null
,,,
285,3 Upper Critical Value,null,null
,,,
286,2,null,null
,,,
287,1,null,null
,,,
288,"Null Hypothesis H0:µr , µb",null,null
,,,
289,uogTrA44xi srchvrs12c09 DFalah121A QUTparaBline utw2012fc1 ICTNET12ADR2 indriCASP irra12c qutwb,null,null
,,,
290,0,null,null
,,,
291,TRisk score,null,null
,,,
292,-1 Lower Critical Value,null,null
,,,
293,-2,null,null
,,,
294,-3 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15,null,null
,,,
295, value,null,null
,,,
296,"runs are under a real risk of performing any given topic from the population worse than the baseline run indriCASP, on average. In particular, there can be no significant reduction in risk that could be attained for the top 4 systems, given a baseline system with the average retrieval effectiveness of indriCASP. On the other hand, a significant reduction in risk could be attained, on average, for the last 4 systems, particularly for  > 5.",null,null
,,,
297,"Lastly, in Table 2, it is notable that the high URisk scores do not necessarily imply high TRisk scores, because of the fact that each system would in general have a different inherent variation in ri -bi across topics (i.e. SE(URisk)) from that of the other systems. For example, consider the runs uogTrA44xi and srchvrs12c09. At  ,"" 0, uogTrA44xi has a URisk score (0.1185) higher than the URisk score (0.1102) of srchvrs12c09, while srchvrs12c09 has a higher TRisk score than uogTrA44xi, i.e. 2.3034 vs. 2.2440. This shows that a ranking of retrieval systems obtained based on TRisk will not necessarily be concordant with the ranking of systems obtained based on URisk.""",null,null
,,,
298,5. EXPLORATORY RISK ANALYSIS,null,null
,,,
299,"In the previous section, the risk analysis that we performed could hide significant performance losses on individual topics. Nevertheless, one can perform an exploratory risk analysis to determine those individual topics on which the observed risk-reward tradeoff score between a given IR system and the baseline system (i.e., xi) is statistically significant. In the following, we provide a definition for exploratory risk analysis (Section 5.1), which we later illustrate upon the TREC 2012 Web track runs (Section 5.2).",Y,null
,,,
300,5.1 Definition,null,null
,,,
301,"The TRisk measure permits the topic-by-topic analysis of risk-reward tradeoff measurements, which we refer to as ex-",null,null
,,,
302,ploratory risk analysis. Such an analysis is implicitly sug-,null,null
,,,
303,gested by the t statistic itself. The t statistic in Eq. (7) can,null,null
,,,
304,be rewritten as follows:,null,null
,,,
305,"t,",null,null
,,,
306,d¯ SE(d¯),null,null
,,,
307,",",null,null
,,,
308,1 c,null,null
,,,
309,"c i,1",null,null
,,,
310,(ri,null,null
,,,
311,-,null,null
,,,
312,bi),null,null
,,,
313,sd/ c,null,null
,,,
314,",",null,null
,,,
315, c c,null,null
,,,
316,"c i,1",null,null
,,,
317,ri,null,null
,,,
318,#NAME?,null,null
,,,
319,bi,null,null
,,,
320,.,null,null
,,,
321,-19,null,null
,,,
322,"In here,",null,null
,,,
323,each component of the sum ti,null,null
,,,
324,",",null,null
,,,
325,: ri -bi,null,null
,,,
326,sd,null,null
,,,
327,gives the,null,null
,,,
328,standardised score of the observed difference in effectiveness,null,null
,,,
329,"between the system r and the baseline system b on topic i,",null,null
,,,
330,"for i ,"" 1, 2, . . . , c.""",null,null
,,,
331,"In analogy, the TRisk measure, which stems from the t statistic, can be rewritten as:",null,null
,,,
332,"TRisk ,",null,null
,,,
333,URisk SEx¯,null,null
,,,
334,",",null,null
,,,
335,1 c,null,null
,,,
336,c,null,null
,,,
337,"i,1",null,null
,,,
338,xi,null,null
,,,
339,sx/ c,null,null
,,,
340,",",null,null
,,,
341, c c,null,null
,,,
342,"c i,1",null,null
,,,
343,xi sx,null,null
,,,
344,",",null,null
,,,
345,-20,null,null
,,,
346,"where each component of the sum, in this case, gives the",null,null
,,,
347,standardised score of the individual topic risk-reward trade-,null,null
,,,
348,"off measurements x1, x2, . . . , xc:",null,null
,,,
349,TRi,null,null
,,,
350,",",null,null
,,,
351,xi sx,null,null
,,,
352,.,null,null
,,,
353,-21,null,null
,,,
354,"In a similar manner that we compare the calculated TRisk score of a given IR system with the two-sided critical values ±t(/2,df) to decide whether the system exhibits a significant level of risk on average (Section 4), to decide whether an observed loss (or gain) on a particular topic i is significant, we can compare the component TRi score with the same critical values ±t(/2,df) , at a desired significance level of . If -t(/2,df)  TRi  t(/2,df), the observed loss (or gain) can be attributed to chance fluctuation, and otherwise it can be considered statistically significant.",null,null
,,,
355,"Indeed, this is one of the typical methods of outlier detection in statistics [14]. Recall that the original objective of Jackknife is to detect outliers [21]. The TRisk measure can also be expressed in terms of the Jackknife estimate of bias, following Wu [29]:",null,null
,,,
356,TRisk,null,null
,,,
357,",",null,null
,,,
358,URisk SEJ,null,null
,,,
359,",",null,null
,,,
360,1 c,null,null
,,,
361,"c i,1",null,null
,,,
362,(c,null,null
,,,
363,-,null,null
,,,
364,1) (^(i) SEJ,null,null
,,,
365,-,null,null
,,,
366,^),null,null
,,,
367,.,null,null
,,,
368,-22,null,null
,,,
369,"Here, each component of the sum:",null,null
,,,
370,"TJi ,",null,null
,,,
371,(c - 1) (^(i) SEJ,null,null
,,,
372,- ^),null,null
,,,
373,",",null,null
,,,
374,"(c - 1) (x¯(i) - x¯) , (23) varJ (x¯)",null,null
,,,
375,gives the standardised Jackknife estimate of bias in URisk due to leaving the topic risk-reward score xi out of the sam-,null,null
,,,
376,"ple x1, x2, . . . , xc, where x¯ , URisk and x¯(i) is the URisk score to be obtained when the ith topic is leaved out of the",null,null
,,,
377,"topic set in use, for i ,"" 1, 2, . . . , c.""",null,null
,,,
378,"In general, both the TRi statistic in Eq. (21) and the TJi statistic in Eq. (23) can be used for the purpose of exploratory risk analysis. However, there is a certain difference",null,null
,,,
379,28,null,null
,,,
380,"between them in theory. Using TRi , we can decide whether an observed performance loss on topic i is significant, by comparing the topic risk-reward score xi with the maximum score that can be attributed to chance fluctuation, but as if the single datum xi is the whole sample. In contrast, using TJi , we can make the same decision by comparing the observed difference between two URisk scores, x¯(i) - x¯, with the maximum difference that can be attributed to chance fluctuation. Since we showed in Section 3.3 that the two estimates of the standard error for each TREC run are in perfect agreement (i.e. SEx¯  SEJ ), we argue that this theoretical difference has no practical consequences. Hence, in the following, we provide an illustration of exploratory risk analysis on the TREC 2012 Web track runs, based on TJi alone. However, initial experiments showed no differences between TRi and TJi .",Y,null
,,,
381,5.2 Exploratory Analysis of Web Track Runs,null,null
,,,
382,"Figure 2 shows the standardised Jackknife estimate of bias in the URisk scores calculated for two TREC runs, namely uogTrA44xi and qutwb at  ,"" 0, 5, 10, 15 for the 50 TREC 2012 Web track topics, where indriCASP is the baseline. This standardised Jackknife estimate of bias, TJi is estimated by leaving one TREC 2012 Web track topic out of the set of topics {151, 152, . . . , 200} in turn. In the figure, the topics that result in a significant performance loss (gain) for the corresponding systems with respect to indriCASP, at the significance level of  "","" 0.05, are those which have a TJi score less than -2 (greater than 2, respectively). Horizontal lines at -2 and +2 are shown to aid clarity.""",Y,null
,,,
383,"From Figure 2, at  ,"" 0 it can be observed that uogTrA44xi has more significant wins in number than qutwb, and less significant losses. This shows in detail why the declared significance for uogTrA44xi in Section 4 counts in favour of reward against risk, while the observed tradeoff between risk and reward can be attributed to chance fluctuation for qutwb, with respect to the baseline indriCASP.""",null,null
,,,
384,"In general, both of the runs uogTrA44xi and qutwb exhibit considerable performance losses with respect to indriCASP on the same topics, including 166, 172, 174, 175, and 191, out of which 2 are significant for uogTrA44xi (i.e., 166 and 175) and 4 are significant for qutwb (i.e., 166, 172, 175, and 191), at  ,"" 0. In particular, consider the topic 166, on which the magnitude of the TJi score is nearly the same for both runs. It is notable here that, as  increases, the significance of that topic relatively doubles for uogTrA44xi, while for qutwb it nearly remains the same. The situation is also similar for topic 175, though the TJi score of uogTrA44xi at  "", 0 is small in magnitude compared to that of qutwb.",null,null
,,,
385,"This is one of the important differences between TRisk and URisk in assessing the risk associated with IR systems. Given a particular topic i, the same amount of performance loss with respect to a provided baseline effectiveness can lead to different TJi (and TRi ,"" xi/SEx¯) scores for different IR systems, depending on the variation in the observed riskreward tradeoff across the topics (i.e., different SEx¯ for different systems), while leading to the same topic risk-reward score, xi, for i "","" 1, 2, . . . , c. As  increases, the topic riskreward score xi increases proportionally for both of the runs uogTrA44xi and qutwb. However, the tradeoff counts, on average, significantly in favour of reward against risk for uogTrA44xi, whereas, it counts neither in favour of reward nor against risk for qutwb, as shown in Section 4. Thus, the same margin of increase in topic risk-reward tradeoff score xi in""",null,null
,,,
386,favour of risk should lead to a relatively higher level of risk,null,null
,,,
387,"for uogTrA44xi than that for qutwb, in a way that TJi did. Assessing the level of risk that a topic commits for a given",null,null
,,,
388,IR system relative to the level of risk associated with the sys-,null,null
,,,
389,"tem on average is a property unique to the measures TJi and TRi . Besides the use of these measures for exploratory risk analysis, this property also enables adaptive risk-sensitive",null,null
,,,
390,"optimisation within a learning to rank technique, as we ex-",null,null
,,,
391,plain in the next section.,null,null
,,,
392,6. ADAPTIVE RISK OPTIMISATION,null,null
,,,
393,"In this section, we describe how to exploit the new riskreward tradeoff measure TRisk (Eq. (10)) in learning robust ranking models that maximises average retrieval effectiveness while minimising risk-reward ratio, in the context of the state-of-the-art LambdaMART learning to rank technique [30]. As discussed below, Wang et al. [26] proposed to integrate URisk (Eq.(4)) within LambdaMART to achieve risk sensitive optimisation, by using  to penalise risk during the learning process. However, URisk considers topics equally regardless of the level of risk they commit. In contrast, we propose to adaptively change the level of risksensitivity, so that the total risk-sensitivity is distributed across the topics proportionally to the level of risk each topic commits. In the following: Section 6.1 provides an overview of the LambdaMART objective function, while Section 6.3 describes the integration of URisk within LambdaMART; Section 6.3 explains our proposed adaptive risk-sensitive optimisation approaches, with the experimental setup & results following in Sections 6.4 & 6.5, respectively.",null,null
,,,
394,6.1 LambdaMART,null,null
,,,
395,"LambdaMART [30] is a state-of-the-art learning to rank technique, which won the 2011 Yahoo! learning to rank challenge. It can be described as a tree-based technique, in that its resulting learned model takes the form of an ensemble of regression trees, which is used to predict the score of each document given the document's feature values. During learning, LambdaMART creates a sequence of gradient boosted regression trees that improve an effectiveness metric. In general, for our purposes2, it is sufficient to state that LambdaMART's objective function is based upon the product of two components: (i) the derivative of a crossentropy that originates from the RankNet learning to rank technique [3] calculated between the scores of two documents a and b, and (ii) the absolute change M in an evaluation measure M due to the swapping of documents a and b [4]. Therefore the final gradient naew of a document a within the objective function is obtained over all pairs of documents that a participates in for query q:",null,null
,,,
396,"naew ,",null,null
,,,
397,ab · |Mab|,null,null
,,,
398,"b,a",null,null
,,,
399,"where ab is RankNet's cross-entropy derivative, and Mab is the change in an evaluation measure M by swapping documents a and b. Various IR evaluation measures are suitable for use as M , including NDCG and MAP, as they have been shown to satisfy a consistency property [4]: for a pair of documents a and b where a is ranked higher than b, if the relevance label of a is higher than b, then a ""degrading"" swap of a and b must result in a decrease in M (i.e. M  0), and orthogonally M  0 for ""improving"" swaps.",null,null
,,,
400,"2Further details on LambdaMART can be found in [4, 26].",null,null
,,,
401,29,null,null
,,,
402,Figure 2: and qutwb,null,null
,,,
403,"Bar graph showing the at  ,"" 0, 5, 10, 15, where""",null,null
,,,
404,standardised indriCASP is,null,null
,,,
405,Jackknife estimate the baseline.,null,null
,,,
406,of,null,null
,,,
407,bias,null,null
,,,
408,in,null,null
,,,
409,the,null,null
,,,
410,"URisk ,",null,null
,,,
411,"TJi ,",null,null
,,,
412,for,null,null
,,,
413,uogTrA44xi,null,null
,,,
414,uogTrA44xi,null,null
,,,
415,qutwb,null,null
,,,
416,"TJ at ,0 i",null,null
,,,
417,2 0 -2,null,null
,,,
418,"TJ at ,5 i",null,null
,,,
419,"TJ at ,10 i",null,null
,,,
420,4 2 0 -2 -4,null,null
,,,
421,4 2 0 -2 -4,null,null
,,,
422,4 2 0 -2 -4,null,null
,,,
423,151 156 161 166 171 176 181 186 191 196,null,null
,,,
424,151 156 161 166 171 176 181 186 191 196,null,null
,,,
425,"TJ at ,15 i",null,null
,,,
426,6.2 Risk-Sensitive Optimisation,null,null
,,,
427,"Wang et al. [26] demonstrated that a more robust learned model could be obtained from LambdaMART if the M is replaced by the difference in URisk for a given swap of two documents, denoted T . In doing so, their implementation weights the value of M by  + 1 only for the topics with down-side risk, while for the topics with up-side risk it leaves M as is, T , M . T was shown to exhibit the consistency property iff the underlying evaluation measure M is consistent (e.g. as obtained from NDCG).",null,null
,,,
428,6.3 Adaptive Risk-Sensitive Optimisation,null,null
,,,
429,"Compared to URisk, TRisk is grounded in the theory of hypothesis testing and produces values that are easily interpretable ­ as shown in Section 4. However, as a linear transformation of URisk, the direct application of TRisk as T within LambdaMART to attain risk-sensitive optimisation cannot offer marked improvements on the resulting learned models. On the other hand, the exploratory risk analysis of Section 5 offers a promising direction, as it permits the learning to rank process to adaptively focus on topics depending upon the level of risk that they commit. In this section, we propose two new models of adaptive risk-sensitive optimisation that exploit the standardised topic risk-reward tradeoff scores (TRi , Eq. (21)), but which differ on which individual topics they operate on. In particular, the first model, SemiAdaptive Risk-sensitive Optimisation (SARO), focuses only on the topics with down-side risk and augments only the corresponding M values. In contrast, the Fully Adaptive Risk-sensitive Optimisation (FARO) model operates on all topics and augments every M value. Hence, compared to URisk as used in [26], FARO and SARO both alter the importance of riskier topics within the learning process.",null,null
,,,
430,"In URisk, M is multiplied by  + 1 if the topic commits a downside risk3. This amounts to a static level of sensitivity for each topic, irrespective of the level of risk that the topic commits. In contrast, based on the standardised topic",null,null
,,,
431,"3This follows directly from the definition of Eq. (4), however the consistency proof in Section 4.3.2 of [26] defines T for different scenarios.",null,null
,,,
432,"risk-reward tradeoff scores, TRi (Eq. (21)), we propose to adaptively adjust  so that the total level of sensitivity can",null,null
,,,
433,be distributed across the topics proportional to the levels,null,null
,,,
434,"of risk that they commit. In order to achieve this, for each",null,null
,,,
435,topic we must estimate the probability of observing a risk-,null,null
,,,
436,"reward score greater than the actual observed TRi score. Technically speaking, we need to estimate the cumulative",null,null
,,,
437,"probability P r (Z  TRi ), where TRi is the observed riskreward tradeoff score and Z is the corresponding standard",null,null
,,,
438,"normal variable of TRi for all topics i ,"" 1, 2, .., c. For large sample sizes (generally agreed to be  30), the distribu-""",null,null
,,,
439,tion of the t statistic in Eq. (7) can be approximated by,null,null
,,,
440,"the standard normal probability distribution function, with",null,null
,,,
441,"zero mean and unit variance [15]. Thus, the probability",null,null
,,,
442,"P r (Z  TRi ), which is the probability of a topic risk-reward score greater than TRi , can be estimated by the standard normal cumulative distribution function (·), as follows:",null,null
,,,
443,"P r (Z  TRi )  1 -  (TRi ) ,",null,null
,,,
444,-24,null,null
,,,
445,"for i ,"" 1, 2, . . . , c. (Z) is a monotonically increasing func-""",null,null
,,,
446,"tion of the standard normal random variable Z, where 0 ",null,null
,,,
447,"(Z ,"" z)  1 for -  z  , and at Z "","" 0, (Z) "","" 0.5. Hence, we can replace the original  in T as  as follows:""",null,null
,,,
448," , [1 - (TRi )] · .",null,null
,,,
449,-25,null,null
,,,
450,"where 0    . As the level of risk TRi committed by topic i increases,  also increases. By substituting  into T (as defined by Wang et al. [26]), this augments the M values for every topic with a weight proportional to the level of risk that each topic commits.",null,null
,,,
451,"The application of  differs between the SARO and FARO models. In particular, SARO only addresses the down-side risk, as in the case of URisk. Indeed, under the null hypothesis H0 : µr ,"" µb, the higher the level of down-side risk (i.e. the larger the size of the difference ri - bi < 0), the higher the probability of observing a topic risk-reward tradeoff score greater than the observed score (P r (Z  TRi )). Hence, SARO varies  from 0 to , according to the downside risk of each topic.""",null,null
,,,
452,"On the other hand, FARO operates on all topics. Indeed, for the topics with up-side risk, FARO gives lower weights",null,null
,,,
453,30,null,null
,,,
454,to the topics that more strongly outperform the baseline,null,null
,,,
455,"system (i.e. as the difference ri - bi > 0 increases). At the extreme, if topic i exhibits maximal improvements over the baseline (i.e. ri - bi ,"" 1), then (TRi ) "","" 1, and hence topic i has minimal emphasis on the learner. In other words, the learner focuses on improving the riskier topics. FARO operates on all topics, by redefining T as follows:""",null,null
,,,
456,"T  ,"" (1 + ) × M,""",null,null
,,,
457,-26,null,null
,,,
458,"Moreover, for  ,"" 0,  "","" 0, hence T  "","" M , i.e. the gain-only LambdaMART, as for URisk.""",null,null
,,,
459,"Finally, we informally comment on the consistency of SARO and FARO: For both models, we calculate SE(URisk) after the first iteration of boosting within LambdaMART, and",null,null
,,,
460,"not for each considered swap ­ we found this to be sufficient to obtain accurate estimates of SE(URisk); Next, the consistency of SARO follows from URisk, as our replacement of  with , as 0    . For FARO, T  only changes sign with M , again as 0    . Hence, as long as M is consistent, both SARO and FARO are also consistent.",null,null
,,,
461,6.4 Experimental Setup,null,null
,,,
462,"We implement the URisk, SARO and FARO models within the Jforests implementation [13] of LambdaMART4. Experiments are conducted using the large MSLR-Web10k learning to rank dataset5, as used by Wang et al. [26]. This dataset encompasses 9,685 queries with labelled documents obtained from a commercial web search engine. For each ranked document for each query, a range of 136 typical query-independent, query-dependent and query features are provided.",null,null
,,,
463,"We use identical hyper-parameters for LambdaMART to those described by Wang et al. [26], namely: the minimum number of documents in each leaf m ,"" 500, 1000, the number of leaves l "","" 50, the number of trees in the ensemble nt "", 800 and the learning rate r ,"" 0.075. The best m value is chosen for each of the five folds using the validation topic set, based on the NDCG@10 performance of the original LambdaMART algorithm, and used for all experiments for that fold thereafter. For the calculation of risk measures, like [26], we use the ranking obtained from the BM25.whole.document feature as the baseline system. The NDCG@10 performance of this baseline is 0.309.""",null,null
,,,
464,"The performances obtained for LambdaMART upon the MSLR-Web10k in terms of NDCG@1 and NDCG@10 are similar in magnitude to those reported by Wang et al. [26], however we note some differences in the risk profile. Such differences are expected given the different implementations: Wang et al. [26] used a private implementation of LambdaMART, while we use and adapt an open source machine learning toolkit for URisk, SARO and FARO. Nevertheless, the reported results allow valid conclusions to be drawn, including identical conclusions to [26] on the impact of using URisk within LambdaMART.",null,null
,,,
465,6.5 Results for SARO and FARO,null,null
,,,
466,"Table 3 reports the effectiveness and robustness results for FARO and SARO along with URisk, for  ,"" 1, 5, 10, 206. In the table, the gain over the baseline effectiveness is ex-""",null,null
,,,
467,"4All of our code has been integrated to Jforests, available at https://code.google.com/p/jforests/ 5http://research.microsoft.com/en-us/projects/mslr/ 6,0 is equivalent to the normal LambdaMART algorithm.",null,null
,,,
468,"Table 3: Results for SARO, FARO and URisk.",null,null
,,,
469," , 0  , 1  , 5  , 10  , 20",null,null
,,,
470,NDCG@1 (URisk) NDCG@1 (SARO),null,null
,,,
471,0.472 0.468 0.458 0.442 0.423 - 0.470 0.463 0.455 0.439,null,null
,,,
472,NDCG@1 (FARO),null,null
,,,
473,- 0.468 0.467 0.470 0.469,null,null
,,,
474,NDCG@10 (URisk) NDCG@10 (SARO),null,null
,,,
475,0.480 0.478 0.470 0.458 0.448 - 0.479 0.474 0.468 0.458,null,null
,,,
476,NDCG@10 (FARO),null,null
,,,
477,- 0.479 0.477 0.479 0.478,null,null
,,,
478,Risk/Reward (URisk) 0.172 0.168 0.164 0.176 0.185 Risk/Reward (SARO) - 0.167 0.164 0.169 0.177,null,null
,,,
479,Risk/Reward (FARO) - 0.170 0.171 0.171 0.172,null,null
,,,
480,Loss/Win (URisk) Loss/Win (SARO),null,null
,,,
481,0.281 0.278 0.267 0.272 0.275 - 0.267 0.266 0.272 0.270,null,null
,,,
482,Loss/Win (FARO),null,null
,,,
483,- 0.272 0.274 0.271 0.277,null,null
,,,
484,Loss (URisk ) Loss (SARO) Loss (FARO),null,null
,,,
485,Win (URisk ) Win (SARO) Win (FARO),null,null
,,,
486,2080 -,null,null
,,,
487,7400 -,null,null
,,,
488,2059 1996,null,null
,,,
489,2025 7417 7470,null,null
,,,
490,7451,null,null
,,,
491,1992,null,null
,,,
492,1992 2040 7468 7476 7452,null,null
,,,
493,2019,null,null
,,,
494,2024 2040 7427 7437 7469,null,null
,,,
495,2040 2010,null,null
,,,
496,2060 7406 7441,null,null
,,,
497,7429,null,null
,,,
498,Loss > 20% (URisk) Loss > 20% (SARO),null,null
,,,
499,Loss > 20% (FARO),null,null
,,,
500,1180 -,null,null
,,,
501,1130 1036 1124 1124 1152 1145,null,null
,,,
502,1036,null,null
,,,
503,1046 1155,null,null
,,,
504,1042 1032 1172,null,null
,,,
505,"pressed as the risk (Eq. (1)) to reward (Eq. (2)) ratio (i.e., the ""Risk/Reward"" rows). Similarly, the number of topics that the risk-sensitive optimisation contributed to reward against risk is expressed as the loss to win ratio (i.e., the ""Loss/Win"" rows). Raw numbers of losses and wins associated with each  value for each model are also shown. Finally the ""Loss > 20%"" rows show, for each model, the number of topics on which the relative loss in performance over the BM25 baseline was higher than 20%7.",null,null
,,,
506,"As expected, since the semi-adaptive risk-sensitive optimisation (SARO) and the risk-sensitive optimisation based on URisk focus on only those topics with down-side risk, there is a steady decrease in average retrieval effectiveness (i.e., NDCG@1 and NDCG@10), as the risk-sensitivity parameter value of  increases. Nevertheless, SARO results in a decrease in average retrieval effectiveness that is less than URisk, for all  values. In contrast, the fully adaptive risk-sensitive optimisation (FARO) maintains the average retrieval effectiveness nearly constant across all  values, as well as the values of the quality and robustness measures, namely the risk-reward ratio and the loss-win ratio.",null,null
,,,
507,"For SARO, the observed values of the two quality and robustness metrics (risk-reward ratio and loss-win ratio) are better than for URisk across the  values. For the metric ""Loss > 20%"", they are comparable between SARO and URisk, given a topic sample as large as 9685 in size.",null,null
,,,
508,"Next, for FARO, the observed values of the two quality and robustness metrics are comparable with that of the risksensitive optimisation based on URisk across  values, and for the metric, ""Loss > 20%"" the observed values for FARO are slightly worse than that of both URisk and SARO.",null,null
,,,
509,"To summarise, the empirical evidence in Table 3 suggest that (i) FARO is best suited for retrieval tasks that are not tolerant to any loss in average effectiveness but also require robustness in effectiveness across the topics, and (ii) SARO suits retrieval tasks that require primarily robustness but are tolerant to some loss in the achievable average effectiveness.",null,null
,,,
510,"7Similar measures are reported in [26]. With 9685 topics, all NDCG differences are statistically significant.",null,null
,,,
511,31,null,null
,,,
512,7. RELATED WORK,null,null
,,,
513,"To the best of our knowledge, this paper is the first work examining risk-sensitive evaluation from the perspective of statistical inference. Indeed, while there has been some investigation into measures of robustness in the literature, such as Geometric-Mean Average Precision [24], developed within the context of the TREC 2004 Robust track, this paper advances upon the URisk measure, first proposed in [26] in 2012. The TRisk measure is the test statistic counterpart of URisk, which enables hypothesis testing on the level of risk associated with a given IR system. As a result, it facilitates adaptive risk-sensitive optimisation within learning to rank.",Y,null
,,,
514,"Outside of risk-sensitive evaluation, statistical hypothesis testing has a long history within IR. Van Rijsbergen [22] noted that ""there are no known statistical tests applicable to IR"". However, later, Hull [32] recommended various hypothesis tests for the evaluation of retrieval experiments, including the Student's t test for matched pairs. Zobel [31] was the first to apply re-sampling techniques in IR, by using a leaveone-out technique for assessing the effect of pooling on the effectiveness measurements and the significance of hypothesis tests, including the paired t test and the Wilcoxon signed rank test. Later, Smucker et al. [19, 20] and also Urbano et al. [27] investigated nonparametric re-sampling techniques, such as the bootstrapping and permutation tests, for the purposes of the evaluation of retrieval experiments.",null,null
,,,
515,"Finally, much work in developing effective learning to rank techniques has occurred in the last few years, as reviewed by Liu [16]. Macdonald et al. [17] examined how the choice of evaluation measure encoded within their loss functions impacted upon the effectiveness of various learning to rank techniques. In particular, it is notable that the AdaRank technique [16, Ch. 4] focuses on hard queries using boosting. Taking a different approach, Wang et al. [26] proposed a risk-sensitive optimisation for the state-of-the-art LambdaMART technique, based on their URisk measure. We further extend URisk to the new TRisk measure within this paper, which is both theoretically founded, and results in more effective and less risky learning to rank.",null,null
,,,
516,8. CONCLUSIONS,null,null
,,,
517,"This paper proposed the new TRisk measure for risk-sensitive evaluation, which is theoretically grounded within hypothesis testing. It easily allows inferential hypothesis testing of risk, as well as the exploratory identification of topics that commit significant levels of risk. In particular, we showed how TRisk could be integrated within the state-of-the-art LambdaMART learning to rank technique, to permit effective yet risk-averse retrieval. Indeed, compared to the existing URisk measure, we attain higher effectiveness with comparable or better risk/reward tradeoffs. For future work, we believe that there is a huge scope to build further effective and risk-averse adaptations for learning to rank upon TRisk, other than SARO and FARO, and beyond LambdaMART.",null,null
,,,
518,9. REFERENCES,null,null
,,,
519,"[1] G. Amati, C. Carpineto, and G. Romano. Query difficulty, robustness, and selective application of query expansion. In Proc. ECIR, 2004.",null,null
,,,
520,"[2] C. Buckley and E. M. Voorhees. Evaluating evaluation measure stability. In Proc. SIGIR, 2000.",null,null
,,,
521,"[3] C. J. C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, N. Hamilton, and G. Hullender. Learning to rank using gradient descent. In Proc. ICML, 2005.",null,null
,,,
522,"[4] C. J. C. Burges. From RankNet to Lambdarank to LambdaMART: An overview. Technical Report MSR-TR-2010-82, 2010.",null,null
,,,
523,"[5] D. Carmel, E. Farchi, Y. Petruschka, and A. Soffer. Automatic query refinement using lexical affinities with maximal information gain. In Proc. SIGIR, 2002.",null,null
,,,
524,"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc. CIKM, 2009.",null,null
,,,
525,"[7] J. Cohen. Statistical Power Analysis for the Behavioral Sciences. 2nd edition, 1988.",null,null
,,,
526,[8] K. Collins-Thompson. Accounting for stability of retrieval algorithms using risk-reward curves. In Proc. Future of Evaluation in IR Workshop. SIGIR. 2009.,null,null
,,,
527,"[9] K. Collins-Thompson. Reducing the risk of query expansion via robust constrained optimization. In Proc. CIKM, 2009.",null,null
,,,
528,"[10] K. Collins-Thompson, P. N. Bennett, F. Diaz, C. Clarke, and E. Voorhees. TREC 2013 Web Track Guidelines.",Y,null
,,,
529,"[11] B. Efron. Bootstrap methods: Another look at the jackknife. Annals of Statistics, (7):1­26, 1979.",null,null
,,,
530,"[12] H. Fisher. A History of the Central Limit Theorem: From Classical to Modern Probability Theory. Springer, 2010.",null,null
,,,
531,"[13] Y. Ganjisaffar, R. Caruana and C. Lopes. Bagging Gradient-Boosted Trees for High Precision, Low Variance Ranking Models. In Proc. SIGIR, 2011.",null,null
,,,
532,"[14] D. C. Hoaglin, F. Mosteller, and J. W. Tukey, editors. Understanding robust and exploratory data analysis. 1983.",null,null
,,,
533,"[15] R. V. Hogg, A. T. Craig, and J. W. McKean. Introduction to Mathematical Statistics. 6th edition, 2004.",null,null
,,,
534,"[16] T.-Y. Liu. Learning to rank for information retrieval. Foundations & Trends in IR, 3(3):225­331, 2009.",null,null
,,,
535,"[17] C. Macdonald, R. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Information Retrieval, 16(5):584-628, 2013.",null,null
,,,
536,"[18] M. Quenouille. Approximate tests of correlation in time series. J. Royal Statistical Society, (11):18­84, 1949.",null,null
,,,
537,"[19] M. D. Smucker, J. Allan, and B. Carterette. A comparison of statistical significance tests for information retrieval evaluation. In Proc. CIKM, 2007.",null,null
,,,
538,"[20] M. D. Smucker, J. Allan, and B. Carterette. Agreement among statistical significance tests for information retrieval evaluation at varying sample sizes. In Proc. SIGIR, 2009.",null,null
,,,
539,"[21] J. W. Tukey. Bias and confidence in not quite large samples. Annals of Mathematical Statistics, 29(2):614, 1958.",null,null
,,,
540,"[22] C. van Rijsbergen. Information Retrieval. 2nd edition, 1979.",null,null
,,,
541,"[23] E. M. Voorhees. Overview of the TREC 2003 robust retrieval track. In Proc. TREC, 2003.",Y,null
,,,
542,"[24] E. M. Voorhees. The TREC Robust retrieval track. SIGIR Forum, 39(1):11­20, 2005.",Y,null
,,,
543,"[25] E. M. Voorhees and C. Buckley. The effect of topic set size on retrieval experiment error. In Proc. SIGIR, 2002.",null,null
,,,
544,"[26] L. Wang, P. N. Bennett, and K. Collins-Thompson. Robust ranking models via risk-sensitive optimization. In Proc. SIGIR, 2012.",null,null
,,,
545,"[27] J. Urbano, M. Marrero, and D. Mart´in. On the measurement of test collection reliability. In Proc. SIGIR, 2013.",null,null
,,,
546,"[28] W. Webber, A. Moffat, and J. Zobel. Score standardization for inter-collection comparison of retrieval systems. In Proc. SIGIR, 2008.",null,null
,,,
547,"[29] C. F. J. Wu. Jackknife, bootstrap and other resampling methods in regression analysis. Annuals of Statistics, 14:1261­1350, 1986.",null,null
,,,
548,"[30] Q. Wu, C. J. C. Burges, K. M. Svore, and J. Gao. Ranking, boosting, and model adaptation. Technical Report MSR-TR-2008-109, 2008.",null,null
,,,
549,"[31] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In Proc. SIGIR, 1998.",null,null
,,,
550,"[32] J. Zobel. Using statistical testing in the evaluation of retrieval experiments. In Proc. SIGIR, 1993.",null,null
,,,
551,32,null,null
,,,
552,,null,null

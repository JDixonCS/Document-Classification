,sentence,label,data
,,,
0,The Effect of Sampling Strategy on Inferred Measures,null,null
,,,
1,Ellen M. Voorhees,null,null
,,,
2,National Institute of Standards and Technology,null,null
,,,
3,ellen.voorhees@nist.gov,null,null
,,,
4,ABSTRACT,null,null
,,,
5,"Using the inferred measures framework is a popular choice for constructing test collections when the target document set is too large for pooling to be a viable option. Within the framework, different amounts of assessing effort is placed on different regions of the ranked lists as defined by a sampling strategy. The sampling strategy is critically important to the quality of the resultant collection, but there is little published guidance as to the important factors. This paper addresses this gap by examining the effect on collection quality of different sampling strategies within the inferred measures framework. The quality of a collection is measured by how accurately it distinguishes the set of significantly different system pairs. Top-K pooling is competitive, though not the best strategy because it cannot distinguish topics with large relevant set sizes. Incorporating a deep, very sparsely sampled stratum is a poor choice. Strategies that include a top-10 pool create better collections than those that do not, as well as allow Precision(10) scores to be directly computed.",null,null
,,,
6,Categories and Subject Descriptors,null,null
,,,
7,H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation,null,null
,,,
8,Keywords,null,null
,,,
9,Test collections; Incomplete judgments; Sampling,null,null
,,,
10,1. INTRODUCTION,null,null
,,,
11,"Test collections drive much of the research in information retrieval. Obtaining human judgments regarding document relevance is the most expensive direct cost of building a test collection, so recent research has focused on developing evaluation mechanisms to support fair comparison of retrieval results using relatively small amounts of judging effort. One such mechanism is the judgment-set-building process that supports the computation of extended inferred estimates of traditional evaluation measures using many fewer judgments",null,null
,,,
12,"This paper is authored by an employee(s) of the United States Government and is in the public domain. Non-exclusive copying or redistribution is allowed, provided that the article citation is given and the authors and agency are clearly identified as its source. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. 2014 ACM 978-1-4503-2257-7/14/07 http://dx.doi.org/10.1145/2600428.2609524 .",null,null
,,,
13,"than would be required to reliably compute the traditional scores directly [4, 5]. This framework has proved popular because it is convenient to use in practice and supports a variety of evaluation measures.",null,null
,,,
14,"Empirical evaluation of the inferred measures shows the estimates can have high fidelity to their directly-computed counterparts, but the quality of the estimates depends on the sampling strategy used to select which documents to judge. Despite the importance of the sampling strategy on estimate quality, there is little guidance in the literature on how best to sample to obtain good estimates. This paper thus examines the question of how the sampling strategy used in creating judgment sets affects the quality of the resulting test collection when using extended inferred measures.",null,null
,,,
15,2. METHODOLOGY,null,null
,,,
16,"A test collection is a triple containing a document set, a set of topics, and a set of relevance judgments which we will call a qrels. A run is the output of a retrieval system for each of the topics in the collection. This output is assumed to be a list of documents ranked by decreasing similarity to the topic. The quality of a run is evaluated using the mean over all topics in the collection of some metric that computes a per-topic score based on the ranks at which relevant documents are retrieved. This work considers three measures, mean average precision (MAP), normalized discounted cumulative gain (NDCG), and precision at 10 documents retrieved (P(10)) and their inferred counterparts.",null,null
,,,
17,"Ideally, qrels would be complete meaning that all documents are judged for all topics. Complete judgments are infeasible for all but the tiniest of document sets, however, so instead some subset of documents is judged. In pooling [2], the top K results from each of a set of runs are combined to form the pool and only those documents in the pool are judged. Runs are subsequently evaluated by assuming that all unpooled (and hence unjudged) documents are not relevant. The inferred measures framework uses stratified sampling strategies and estimates the values of the measures based on the judgments obtained on sampled documents.",null,null
,,,
18,Both pooling and the inferred measures framework construct a judgment set based on a set of runs. The framework focuses different amounts of assessing effort on different regions of the ranked lists (the strata) by assigning a sampling rate to each stratum. The strata are defined by disjoint sets of contiguous ranks. A document belongs to the stratum defined by the smallest rank at which the document was retrieved across the set of runs. The sampling rate is the probability of selecting a document from the stra-,null,null
,,,
19,1119,null,null
,,,
20,"tum to be judged. The inferred measures use the knowledge of a stratum's size, sampling rate, and number of relevant documents found from among the judged set to estimate the total number of relevant documents in the stratum, and combines the different strata's estimates to compute a final global estimate of the number of relevant documents for a topic. Similar local estimates are made for the number of relevant documents retrieved by an individual run based on the number of judged documents retrieved by that run in each of the strata.",null,null
,,,
21,"A sampling strategy is the combination of strata definition and sampling rate per stratum. Different sampling strategies applied to the same run set create different test collections since the judged set of documents depends on the sampling strategy. The quality of a sampling strategy is thus measured by the quality of the test collections it induces. To measure the quality of a test collection, we use the accuracy of determining the set of significantly different run pairs as compared to a gold standard set of runs pairs (similar to the approach used by Bompada, et al. [1]).",null,null
,,,
22,"In general, the larger the assessment budget (i.e., maximum number of human judgments that can be obtained), the better a test collection will be. Thus it is important to control for the total number of documents judged when comparing sampling strategies. In practice, this means that stratum size and sampling rate must be traded off against one another: small strata can have large sampling rates but large strata must be sampled more sparsely.",null,null
,,,
23,"We consider the following strategies in this work. pool: A single, exhaustively judged (thus shallow) stra-",null,null
,,,
24,"tum. The same documents as in pooling are judged, but subsequent evaluation uses the inferred measures.",null,null
,,,
25,"1stratum: A single stratum drawn to a moderate depth and using a moderate sampling rate. In this study, the 1stratum strategy uses a depth of 100 and a sampling rate of 30%.",null,null
,,,
26,"2strata: An exhaustively judged small initial stratum plus a moderate depth and sampling rate second stratum. This strategy is motivated by the fact that it allows P(depth) to be computed exactly, and it concentrates the most assessment effort at ranks that have a large effect on the measures' scores. In this study, the 2strata strategy consists of all documents in ranks 1­10 plus a 10% sample of documents in ranks 11­100.",null,null
,,,
27,"3strata: An exhaustively judged small initial stratum, a moderate depth, moderate sampling rate second stratum, and a deep, very sparsely sampled final stratum. The addition of a large, sparsely sampled stratum is motivated by the hope that it will provide a better estimate of the total number of relevant documents. In this study, the 3strata strategy consists of all documents from ranks 1­10 in union with a 10% sample of documents from ranks 11­20 and a 1% sample of documents from ranks 21­1000.",null,null
,,,
28,"To examine the quality of a sampling strategy, we use an existing test collection and create a set of sampled collections containing a subset of the original collection's judgments. We then compare the average behavior of the sampled collections to that of the original collection, which we treat as gold standard behavior. We use two existing TREC collections as the base collection in this work: the collection built in the the TREC-8 ad hoc retrieval task and the collection built in the TREC 2012 Medical Records track.",Y,null
,,,
29,"The TREC-8 ad hoc track collection contains about 528,000 mostly newswire documents and 50 topics. Binary relevance judgments were created through pooling with K , 100. This collection was chosen for this study because it is a high quality test collection that has been used in many similar studies; evaluation scores computed on this collection are as close to Truth as we are likely to ever get. We use traditional evaluation measures computed over the official qrels as the gold standard for this collection.",Y,null
,,,
30,"The original motivation for this study was unexpectedly noisy scores that resulted from using inferred measures in the TREC 2011 Medical Records track [3]. The 2012 Medical Records track collection is similar to the 2011 collection in that it uses the same document set and similar topics, but it contains a better set of relevance judgments to use as ground truth. The document set is approximately 17,000 medical visit reports and there are 47 topics. Judgment sets were created using a two strata strategy. The first stratum contains all documents retrieved between ranks 1­15 from 88 runs, and the second stratum is drawn from ranks 16­ 100 with a sampling rate of 25%. Documents were judged on a three-way scale of relevant, partially relevant and not relevant. For this collection, the gold standard scores are the inferred scores computed using the entire qrels created in the track. The computation of infNDCG uses a gain value of 1 for partially relevant documents and 2 for fully relevant documents.",null,null
,,,
31,"Call a run that contributed to the judgment sets for the base collection a judged run1. To test a sampling strategy, we randomly split the set of judged runs in half, and apply the sampling strategy to only the runs in one half. Once a judgment set is produced, we evaluate all runs submitted to the original TREC track using it. The entire process is repeated 50 times, each time called a trial, with each trial using a different random split of judged runs. All sampling strategies use precisely the same split of judged runs in each trial, but necessarily produce different judgment sets from that split.",Y,null
,,,
32,3. EXPERIMENTAL RESULTS,null,null
,,,
33,"Using the set of sampled judgment sets, we can compare the sampling strategies along three dimensions: the number of judged documents required, the accuracy of determining significantly different run pairs, and the accuracy of estimates of the total number of relevant documents, R.",null,null
,,,
34,3.1 Judgment set size,null,null
,,,
35,"As mentioned above, comparisons of sampling strategies must control for the total number of judgments a strategy requires, called the judgment set size. The mean judgment size over the 50 trials for each sampling strategy is given in Table 1.",null,null
,,,
36,"Note that it is possible that a document selected to be judged might not actually have a judgment in the base collection's qrels. When this happens, we ignore the selection (i.e., we mark it as not selected). To account for this effect, the sampling rate actually used in constructing the sample judgment sets was greater than the target rate such that the number of judgments obtained approximated the number the target rate would produce (while always sampling",null,null
,,,
37,"1While 129 runs were submitted to the TREC-8 ad hoc task, only 71 runs were judged. All submitted runs were judged for the TREC 2013 Medical Records collection.",Y,null
,,,
38,1120,null,null
,,,
39,Table 1: Mean judgment set size per strategy,null,null
,,,
40,TREC-8,Y,null
,,,
41,TREC 2012,Y,null
,,,
42,Mean,null,null
,,,
43,Mean,null,null
,,,
44,ID,null,null
,,,
45,Size,null,null
,,,
46,ID,null,null
,,,
47,Size,null,null
,,,
48,pool15 179.7 pool15 197.9,null,null
,,,
49,pool25 296.0,null,null
,,,
50,1stratum 323.2 1stratum 255.9,null,null
,,,
51,2strata 221.6 2strata 227.8,null,null
,,,
52,3strata 199.4,null,null
,,,
53,"from the appropriate stratum). The rates given in the definitions of the strata are the target rates, which are also the effective sampling rates. The strata sizes and sampling rates provide only coarse control of the resulting judgment set size, however, and there are sampling strategies the base collection cannot support because there are too many missing judgments in its qrels. The TREC 2012 collection cannot support a pooled strategy with a depth greater than 15, nor does it support the exploration of a 3strata strategy. We use a pool depth of both 15 and 25 for the TREC-8 collection since depth 25 is a better match for the other methods' judgment set sizes on that collection while depth 15 is used for the TREC 2012 collection.",Y,null
,,,
54,"The lack of fine control means there is a wider spread in the range of judgment set sizes across the strategies than is ideal. However, the 1stratum strategy is the strategy with the largest judgment set size, and as will be seen below, it is not a very good strategy even with the larger number of judgments.",null,null
,,,
55,3.2 Collection quality,null,null
,,,
56,"We use the accuracy of detecting the set of significantly different run pairs as the measure of a sampled collection's quality. For a given measure, we first compute the set of significantly different run pairs in the base collection using a paired t-test with  ,"" 0.05. Using the same test for a sampled collection, we calculate the number of pairs in each of the following five categories: true positive: the run pair is significantly different in both""",null,null
,,,
57,"collections and the collections agree as to which system is better; true negative: the run pair is not significantly different in either collection; miss: the run pair is significantly different in the base collection, but not the sampled collection; false alarm: the run pair is not significantly different in the base collection, but is in the sampled collection; inversion: the run pair is significantly different in both collections, but the collections disagree as to which is the better run. The accuracy of the significant pairs determination is then computed as the number of correct classifications over the sum of correct and incorrect classifications where inversions are counted as both a false alarm and a miss. Accuracy scores are given in Table 2, which shows the mean value as well as the minimum and maximum accuracy values observed across the 50 trials. The 1stratum strategy is consistently the worst strategy for the NDCG and P(10) measures, and is only somewhat more competitive for MAP. This strategy is the only strategy examined that does not judge all of the documents retrieved in the top 10 ranks, but instead samples uniformly from the top 100 ranks. Since all three measures strongly emphasize",null,null
,,,
58,Table 2: Accuracy of reproducing the set of signifi-,null,null
,,,
59,cantly different run pairs,null,null
,,,
60,a) TREC-8,null,null
,,,
61,infAP,null,null
,,,
62,infNDCG,null,null
,,,
63,infP10,null,null
,,,
64,pool15,null,null
,,,
65,0.906,null,null
,,,
66,0.922,null,null
,,,
67,0.959,null,null
,,,
68,"[0.855, 0.926] [0.873, 0.945] [0.920, 0.975]",null,null
,,,
69,pool25,null,null
,,,
70,0.929,null,null
,,,
71,0.947,null,null
,,,
72,0.978,null,null
,,,
73,"[0.889, 0.943] [0.901, 0.966] [0.950, 0.987]",null,null
,,,
74,1stratum,null,null
,,,
75,0.878,null,null
,,,
76,0.884,null,null
,,,
77,0.868,null,null
,,,
78,"[0.839, 0.911] [0.836, 0.918] [0.834, 0.904]",null,null
,,,
79,2strata,null,null
,,,
80,0.918,null,null
,,,
81,0.924,null,null
,,,
82,0.953,null,null
,,,
83,"[0.891, 0.939] [0.879, 0.943] [0.932, 0.962]",null,null
,,,
84,3strata,null,null
,,,
85,0.834,null,null
,,,
86,0.912,null,null
,,,
87,0.926,null,null
,,,
88,"[0.786, 0.874] [0.880, 0.934] [0.912, 0.937]",null,null
,,,
89,b) TREC 2012,null,null
,,,
90,infAP,null,null
,,,
91,infNDCG,null,null
,,,
92,infP10,null,null
,,,
93,pool15,null,null
,,,
94,0.888,null,null
,,,
95,0.883,null,null
,,,
96,0.917,null,null
,,,
97,"[0.862, 0.911] [0.855, 0.907] [0.885, 0.956]",null,null
,,,
98,1stratum,null,null
,,,
99,0.93,null,null
,,,
100,0.82,null,null
,,,
101,0.895,null,null
,,,
102,"[0.903, 0.948] [0.783, 0.858] [0.853, 0.924]",null,null
,,,
103,2strata,null,null
,,,
104,0.935,null,null
,,,
105,0.904,null,null
,,,
106,0.967,null,null
,,,
107,"[0.901, 0.953] [0.880, 0.926] [0.953, 0.978]",null,null
,,,
108,the quality of the top of the ranked list--though MAP less so than P(10) and NDCG--unjudged documents in these ranks increase the variability of the final estimated score much more than unjudged documents later in the ranked list do. Thus it is beneficial to concentrate more of the judging resources at the very top of the document ranked lists.,null,null
,,,
109,"The 3strata strategy is the strategy used for the TREC 2011 Medical Records collection whose noisy score estimates prompted this study. Its behavior on the TREC-8 collection confirms that it is a poor choice of sampling strategy. The problem is that in this strategy the stratum for which the least information is known receives the greatest emphasis. The computation of the inferred measures uses as an estimate of a run's number of relevant retrieved in a particular stratum, S, a smoothed fraction of the number of judged relevant documents in S retrieved by the run over the number of judged documents in S retrieved by the run. When a run retrieves few (or none) of the documents that were selected to be judged in the stratum but relatively many documents from the stratum, the estimate can be far afield of the true value. For example, if a run retrieves only one document in S that was judged, and that document is relevant, the estimated number of relevant retrieved is .99998 times the number of documents in S that are retrieved by the run. Any strategy that includes a large stratum that is sampled very sparsely will be affected by this behavior.",Y,null
,,,
110,"The relative quality of the pool and 2strata strategies is more complex. The 2strata strategy is more accurate for the TREC 2012 collection. The pooling strategy is generally more accurate for the TREC-8 collection, but there is a clear dependency on the pool depth. The next section will show that the 2strata strategy produces better estimates of the number of relevant documents for both collections.",Y,null
,,,
111,3.3 Estimating R,null,null
,,,
112,"Part of the computation of extended inferred measures is creating an estimate of the total number of relevant documents for a topic. This estimate has a large impact on the quality of the final estimates of NDCG and MAP scores. Since NDCG is defined over a fixed set of ranks (100 in this work), while MAP considers the entire relevant set, the quality of the estimate of R has a bigger impact on infAP than it does on infNDCG.",null,null
,,,
113,1121,null,null
,,,
114,Pool25,null,null
,,,
115,2strata,null,null
,,,
116,Figure 1: Estimates of R on the TREC-8 collection.,Y,null
,,,
117,Table 3: Mean correlation between estimated and,null,null
,,,
118,gold-standard R,null,null
,,,
119,TREC-8 TREC 2012,Y,null
,,,
120,1stratum,null,null
,,,
121,0.97,null,null
,,,
122,0.98,null,null
,,,
123,2strata,null,null
,,,
124,0.94,null,null
,,,
125,0.99,null,null
,,,
126,pool25,null,null
,,,
127,0.91,null,null
,,,
128,--,null,null
,,,
129,pool15,null,null
,,,
130,0.87,null,null
,,,
131,0.94,null,null
,,,
132,"Table 3 gives the mean over the 50 trials of the Pearson correlation between the per-topic estimate and goldstandard values of R, the number of relevant documents. The gold-standard value of R for the TREC 2012 collection is the estimate produced using the entire set of runs submitted to the Medical Records track. The estimates based on half of the runs are very highly correlated with the original estimates, though the pooling strategy produces less good estimates than the other strategies. The gold-standard value of R for the TREC-8 collection is the count of the the number of relevant in the original qrels. The estimated R values are again highly correlated with true R, but the correlations are weaker for the TREC-8 collection than the TREC 2012 collection. The pooling strategies exhibit the weakest correlations, and the correlations get weaker as the pool depth gets smaller.",Y,null
,,,
133,"Despite a good overall correlation, the pooling strategies underestimate R for all but the smallest of relevant sets, meaning that they cannot distinguish topics that truly have a small relevant set. Figure 1 shows the estimates of R per topic for the TREC-8 collection for the pool25 (left) and 2strata (right) sampling strategies. Individual topics are plotted on the x-axis and the number of documents on the y-axis. The minimum and maximum estimates of R across the 50 trials are plotted by a line connecting the two points (which runs off the top of the graph if the maximum estimate exceeds the graph's largest y-value). The mean across the 50 trials of the estimated R is plotted as an x on that line. The gold-standard value of R is plotted as a circle. For the pool25 strategy, nine topics have a mean estimated R",Y,null
,,,
134,"smaller than 20. The gold-standard and estimated R values for these topics are shown below, which has a correlation of just 0.42.",null,null
,,,
135,Gold: 13 22 16 6 28 13 17 17 65 Est: 12.7 17.8 15.0 6.0 13.7 9.8 17.0 17.0 16.0,null,null
,,,
136,4. CONCLUSION,null,null
,,,
137,"The inferred measures framework can construct test collections that have high fidelity with their traditionallyconstructed counterparts using relatively few judgments provided an appropriate sampling strategy is used in the construction process. The framework does not work well with large, sparsely sampled strata nor with strategies that do not exhaustively judge a small top stratum. Restricting all judgments to the very top ranks (i.e., pooling) is also not a good strategy, though, because such samples are unable to accurately estimate R. Thus, what this paper called the 2strata strategy--using an exhaustively judged small initial stratum coupled with a moderate depth, moderate sampling rate stratum--is a practical and effective sampling strategy to use for inferred measures.",null,null
,,,
138,5. REFERENCES,null,null
,,,
139,"[1] T. Bompada, C.-C. Chang, J. Chen, R. Kumar, and R. Shenoy. On the robustness of relevance measures with incomplete judgments. In Proceedings of SIGIR 2007, pages 359­366, 2007.",null,null
,,,
140,"[2] K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an ""ideal"" information retrieval test collection. British Library Research and Development Report 5266, 1975.",null,null
,,,
141,"[3] E. M. Voorhees. The TREC Medical Records track. In Proceedings of ACM Conference on Bioinformatics, Computational Biology and Biomedical Informatics, pages 239­246, 2013.",Y,null
,,,
142,"[4] E. Yilmaz and J. A. Aslam. Estimating average precision when judgments are incomplete. Knowledge Information Systems, 16:173­211, 2008.",null,null
,,,
143,"[5] E. Yilmaz, E. Kanoulas, and J. A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In Proceedings of SIGIR 2008, pages 603­610, 2008.",null,null
,,,
144,1122,null,null
,,,
145,,null,null

,sentence,label,data
,,,
0,Entity Query Feature Expansion using Knowledge Base Links,null,null
,,,
1,"Jeffrey Dalton, Laura Dietz, James Allan",null,null
,,,
2,Center for Intelligent Information Retrieval School of Computer Science,null,null
,,,
3,"University of Massachusetts Amherst Amherst, Massachusetts",null,null
,,,
4,"{jdalton, dietz, allan}@cs.umass.edu",null,null
,,,
5,ABSTRACT,null,null
,,,
6,"Recent advances in automatic entity linking and knowledge base construction have resulted in entity annotations for document and query collections. For example, annotations of entities from large general purpose knowledge bases, such as Freebase and the Google Knowledge Graph. Understanding how to leverage these entity annotations of text to improve ad hoc document retrieval is an open research area. Query expansion is a commonly used technique to improve retrieval effectiveness. Most previous query expansion approaches focus on text, mainly using unigram concepts. In this paper, we propose a new technique, called entity query feature expansion (EQFE) which enriches the query with features from entities and their links to knowledge bases, including structured attributes and text. We experiment using both explicit query entity annotations and latent entities. We evaluate our technique on TREC text collections automatically annotated with knowledge base entity links, including the Google Freebase Annotations (FACC1) data. We find that entity-based feature expansion results in significant improvements in retrieval effectiveness over state-of-the-art text expansion approaches.",null,null
,,,
7,Categories and Subject Descriptors,null,null
,,,
8,H.3.3 [Selection Process]: [Information Search and Retrieval],null,null
,,,
9,Keywords,null,null
,,,
10,Entities; Ontologies; Information Retrieval; Information Extraction,null,null
,,,
11,1. INTRODUCTION,null,null
,,,
12,"Today's commercial web search engines are increasingly incorporating entity data from structured knowledge bases into search results. Google uses data from their Knowledge Graph and Google Plus, Yahoo! has Web Of Objects, Bing incorporates Facebook and Satori entities, and Facebook searches over entities with Graph Search. However, the majority of content created on the web remains unstructured text in the form of web pages, blogs, and microblog posts. For many search tasks, these documents will continue to be the main",Y,null
,,,
13,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609628.",null,null
,,,
14,"source of content for users. In this work, we address the task of ad hoc document retrieval leveraging entity links to knowledge bases in order to improve the understanding and representation of text documents and queries. We demonstrate that this gain in semantic understanding results in significant improvements in retrieval effectiveness.",null,null
,,,
15,"We bridge the gap between entities and text using automatic information extraction to identify entities and link them to a knowledge base. The task of `entity linking' to a knowledge base has received significant attention, with one major venue being the Text Analysis Conference (TAC) Knowledge Base Population (KBP) Entity Linking Task [17]. In this task traditional named entities (people, geo-political entities, and organizations) are linked to a knowledge base derived from Wikipedia. Beyond TAC, there is increasing interest in more general concept entities, with the task of `wikifying' [28, 16, 19] documents by linking them to Wikipedia. Beyond information extraction, content owners are augmenting HTML markup with embedded structured data through standardized markup efforts such as schema.org. A study from 2012 showed that 30% of web documents contain embedded structured data in RDFa or Microformats [26].",Y,null
,,,
16,"Google recently released the FACC1 dataset [15] for the TREC ClueWeb09 and ClueWeb12 web collections. The dataset contains automatically extracted entity mentions from web documents that are linkable to the Freebase knowledge base [6]. Freebase is a publicly available general purpose knowledge base with over 42 million entities and over 2.3 billion facts.1 The FACC1 dataset is the first publicly available web-scale collection of entity linked documents. In addition to annotated documents, the FACC1 data also contains explicit manual annotations for the TREC web track queries. We present one of the first published experiments using this data for retrieval.",Y,null
,,,
17,"For this work, we define an entity broadly to be a thing or concept that exists in the world or fiction, such as a person, a battle, a film, or a color. We focus primarily on entities that are linked to two existing publicly available knowledge bases, Wikipedia and Freebase. We use a combination of both of these knowledge bases because they provide complementary information. Wikipedia provides rich text and link associations. Freebase provides a significantly larger database of concepts, many of whom may not meet Wikipedia's standards for notability, with structured data in RDF, including categories and types.",null,null
,,,
18,"Our work addresses two fundamental research areas using entity annotations for ad hoc retrieval. The first is the representation of both queries and documents with linked entities. What features, if any, improve retrieval effectiveness? The second is inferring latent",null,null
,,,
19,"1As of January 27, 2014 according to Freebase.com",null,null
,,,
20,365,null,null
,,,
21,"entities (and more importantly, features of entities and terms) for an information need.",null,null
,,,
22,"The FACC1 annotations include entity annotations for queries. However, these annotations are limited to entities that are explicitly mentioned, where we hypothesize that many more latent entities are relevant to the users' information need. For example, consider the TREC query about [Barack Obama family tree]. There are explicit query entities, [Barack_Obama] and [Family_Tree]. There are also relevant latent entities such as [Ann_Dunham], [Michelle_Obama], [Barack_Obama_Sr], [Ireland], [Kenya], [DNA], and others.",null,null
,,,
23,"One issue is that explicit entity mentions have the same fundamental problems of query-document mismatch as words. For example, a document on the topic of Obama's family history may not explicitly refer to a [Family_Tree], but may refer to other related entities, such as a [Family_Crest] and [Genealogy]. In addition, for many existing collections, no explicit entity annotations for queries exist. In both cases, it is important to infer related entities and expand the query representation.",null,null
,,,
24,"Entities provide a wealth of rich features that can be used for representation. These include text as well as structured data. Some of the important attributes that we highlight for these experiments include: fine-grained type information (athlete, museum, restaurant), category classifications, and associations to other entities. Although we do not explore them in detail in this work we also observe that the knowledge base contains rich relational data with attributes and relations to other entities. These attributes include: gender, nationality, profession, geographical information (latitude, longitude), and temporal attributes (such as birth and death), and many more depending on the type of entity.",null,null
,,,
25,"We hypothesize that the language in the document contexts of entity mentions differs from that found in Wikipedia or in the knowledge base description. But, mentions of the entity are also contained in text documents across the entire corpus. To address this, we propose new query-specific entity context models extracted from snippets in the feedback documents surrounding the entity's annotations. We further hypothesize that this context information will allow us to identify entities that are relevant to the query and use their presence as signals of document relevance.",null,null
,,,
26,"To summarize, the main contributions of this work are:",null,null
,,,
27,· Introducing new query expansion techniques with featurebased enrichment using entity links to a knowledge base,null,null
,,,
28,· Demonstrating significant improvements in retrieval effectiveness when entity features are combined with existing text approaches,null,null
,,,
29,· Proposing a new entity modeling technique for building queryspecific context models that incorporate evidence from uncertainty inherent in automatic information extraction,null,null
,,,
30,· Performing the first published experiments using the FACC1 Freebase annotations for ad hoc document retrieval,null,null
,,,
31,· Analyzing the ClueWeb09 FACC1 annotations for their use in retrieval applications,null,null
,,,
32,· Providing new entity-annotated query datasets for the TREC web track queries that substantially improve entity recall,Y,null
,,,
33,"The remainder of the paper is structured as follows. Section 2 provides retrieval model background. In Section 3, we introduce the new feature expansion approach and introduce the entity context feedback model (3.4). We experimentally evaluate our approach in Section 4 on standard TREC test collections including: Robust'04, ClueWeb09B, and ClueWeb12B. Connections to related work are discussed in Section 6 before concluding.",Y,null
,,,
34,2. BACKGROUND,null,null
,,,
35,2.1 Notation,null,null
,,,
36,We distinguish notationally between random variables in upper case (e.g. E) and possible assignments (e) in lower case. We denote count statistics of a configuration e in a sequence of as ei.,null,null
,,,
37,2.2 Log-linear Models,null,null
,,,
38,"Graphical models [20], such as Markov Random Fields (MRF), are a popular tool in both information extraction and information retrieval. Dependencies between two (or more) variables (e.g. Queries and Documents) are encoded by factor functions that assign a nonnegative score to each combination of variable settings. Regarding the factor function in log space allows for arbitrary scores.",null,null
,,,
39,"Factor functions (or similarity functions) between two variables are indicated by  (e.g. (Q, W )) which is assumed to be of loglinear form. This means that  is determined by an inner product of weight vector  and feature vector f in log-space.",null,null
,,,
40,2.3 Retrieval Models,null,null
,,,
41,"The query likelihood (QL) retrieval model can be represented as a factor between a multi-word query, and a document represented as a bag of words as (Q, D) ,"" wiQ (wi, D).""",null,null
,,,
42,"Within this framework, we adopt both the QL model and the widely used Sequential Dependence Model (SDM) [24], which incorporates word unigrams, adjacent word bigrams, and adjacent word proximity. The feature function used to match words, W to a document is a Dirichlet smoothed probability:",null,null
,,,
43,"(W,",null,null
,,,
44,D),null,null
,,,
45,",",null,null
,,,
46,log,null,null
,,,
47,"#(W,",null,null
,,,
48,D),null,null
,,,
49,+,null,null
,,,
50,µ,null,null
,,,
51,"#(W,C) |C|",null,null
,,,
52,-1,null,null
,,,
53,|D| + µ,null,null
,,,
54,"This approach generalizes to bigrams ""W1, W2"" and unordered term proximity. Furthermore, we can apply it to different kinds of vocabularies, such as entity identifiers or categories with appropriate redefinition of the document length |D| and collection statistics.",null,null
,,,
55,2.4 Query Expansion,null,null
,,,
56,"One commonly used query expansion model is the Relevance Model (RM) [22]. It is a pseudo-relevance feedback approach that uses retrieved documents to estimate the query topic. Relevant expansion terms are extracted and used in combination with the original query (the RM3 variant). We use this as our baseline text-based expansion model. Beyond unigrams, Metzler and Croft propose a generalized model, Latent Concept Expansion (LCE) [25], which models arbitrary expansion vocabularies such as words, entity identifiers, types or categories [25].",null,null
,,,
57,"In both relevance modeling and LCE, the formulation is similar. Assuming that the retrieval score represents the probability of the document under the query, e.g. p(D|Q), document-wise multinomial distributions over a vocabulary p(V |D) are combined via a mixture model.",null,null
,,,
58,"p(V |Q) , p(V |d)p(D , d|Q)",null,null
,,,
59,-2,null,null
,,,
60,d,null,null
,,,
61,"Hyperparameters of this approach are the number of expansion documents, number of expansion features, and a balance parameter for weighting the original query against the expanded query, which are further weighted according to P (V |Q).",null,null
,,,
62,"The document probability, p(D , d|Q) is typically derived from the retrieval score s(d) by exponentiation and re-normalization over the domain of expansion documents. The document specific",null,null
,,,
63,366,null,null
,,,
64,"Table 1: Example expansion terms for the query ""Obama Family Tree""",null,null
,,,
65,Words family tree genealogy surname history crest,null,null
,,,
66,Entity ID Barack_Obama Michelle_Obama Family_Tree Family_Crest Barack_Hussein_Obama_Sr Family_History,null,null
,,,
67,Wiki Categories cat:first_families_u.s. cat:political_families_u.s. cat:bush_family cat:american_families_english cat:american_families_german cat:business_families_u.s.,null,null
,,,
68,Freebase Type /people/family /book/book_subject /location/country /film/film_subject /base/presidentialpets/first_family /base/webisphere/topic,null,null
,,,
69,#combine( #sdm( obama family tree ) #sdm( [Barack_Obama] [Family_Tree] ) #sdm( {US President} {Politician}) #sdm( [Michelle_Obama] [Ireland] [Kenya]),null,null
,,,
70,),null,null
,,,
71,Figure 1: Example expansion of query C09-1 with entities [] and Freebase types {}.,null,null
,,,
72,a) Annotated Query,null,null
,,,
73,W Q,null,null
,,,
74,M,null,null
,,,
75,E,null,null
,,,
76,b) Knowledge Base,null,null
,,,
77,E,null,null
,,,
78,A,null,null
,,,
79,C,null,null
,,,
80,W,null,null
,,,
81,T,null,null
,,,
82,distribution of features is derived under the multinomial assumption,null,null
,,,
83,"by p(V |d) ,",null,null
,,,
84,#(V d) V #(V d),null,null
,,,
85,.,null,null
,,,
86,3. ENTITY QUERY FEATURE MODEL,null,null
,,,
87,In this section we introduce the representation of queries and documents using linked entities and provide background on the models we use throughout this work.,null,null
,,,
88,"In a preprocessing step, documents in the collection are annotated with entity links. Entity links establish a bidirectional reference from words to entities in the KB, and indirectly to Freebase types and Wikipedia categories and further related entities in the knowledge base (called neighbors, henceforth). We index these different vocabulary representations for each document in different fields. Our retrieval engine supports proximity and approximate matches with respect to each of the vocabularies.",null,null
,,,
89,"The goal is to derive expansions across the different kinds of vocabularies such as words W , entities E, types T , and categories C to retrieve annotated documents with the goal of maximizing document retrieval effectiveness.",null,null
,,,
90,"Figure 1 details expansions for the ClueWeb09B query 1 ""obama family tree"" for the words, entities and Freebase types. The first three entries constitute words, entities and types directly mentioned in the query, where the last entry includes other relevant entities. A sample of the expansion terms from our system on this query are given in Table 1.",Y,null
,,,
91,"Expansions in different vocabularies can be derived through multiple options. Entity linking the query provides very precise indicators, but may also miss many of the relevant entities. Alternative expansion entities can be found using pseudo-relevance feedback on the document collection containing entity annotations or alternatively by issuing the query against an index of the knowledge base and considering top-ranked entries. Figure 2 gives an overview of all possibilities studied in this work, which we detail in this section.",null,null
,,,
92,3.1 Annotated Query,null,null
,,,
93,"The query, Q, is given as a sequence of keywords w1w2, ...w|Q|. Aside from representing the query Q by their query word representation W , we can annotate the query in the same way we preprocess",null,null
,,,
94,c) KB Feedback,null,null
,,,
95,Q,null,null
,,,
96,E,null,null
,,,
97,d) Corpus Feedback,null,null
,,,
98,M,null,null
,,,
99,E,null,null
,,,
100,Q,null,null
,,,
101,D,null,null
,,,
102,W,null,null
,,,
103,e) Entity Context,null,null
,,,
104,E,null,null
,,,
105,W,null,null
,,,
106,D,null,null
,,,
107,Q User query,null,null
,,,
108,D Collection document,null,null
,,,
109,E Knowledge base entity W Words,null,null
,,,
110,A Entity alias,null,null
,,,
111,T Freebase type,null,null
,,,
112,C Wikipedia Category M Entity mention,null,null
,,,
113,Figure 2: Overview over feature sources.,null,null
,,,
114,the documents before indexing. This provides annotations for all entity mentions M in the query terms together with a link to the KB entity E (cf. Figure 2a),null,null
,,,
115,"Resolution through the entity provides further information about its type, category and name alias information. We can additionally infer indirectly related entities by following hyperlinks on Wikipedia articles or exploiting Freebase relations (cf. Figure 2b).",null,null
,,,
116,"For instance, terms on the entity's Wikipedia article provide a resource for related words W . These are derived through a hierarchical multinomial model by integrating over mentions and entities",null,null
,,,
117,"f ExplWiki(Q, W ) ,",null,null
,,,
118,p(W |E)p(E|M ) p(M |Q),null,null
,,,
119,M,null,null
,,,
120,E,null,null
,,,
121,367,null,null
,,,
122,"In the equation, p(M |Q) is a uniform distribution over annotated mentions and p(E|M ) is the entity disambiguation confidence and p(W |E) refers to the language model of the entity's article.",null,null
,,,
123,"In addition to words, we access different alternative names A in the knowledge base through the entity link. Our knowledge base contains name aliases of different confidences, e.g. title versus anchor text, which we take into account through the multinomial distribution p(A|E).",null,null
,,,
124,3.2 KB Feedback,null,null
,,,
125,"An alternative route can be taken by issuing the query against a search index containing all knowledge base entries (cf. Figure 2c). The ranking of articles can be interpreted as a distribution over entities, encoded in the feature f KB(Q, E) which is obtained by exponentiating and renormalizing the retrieval score sQ(E) of the entity under the query.",null,null
,,,
126,"1 f KB(Q, E) , Z exp sQ(E)",null,null
,,,
127,"Here, Z ensures normalization across the feedback entities. For instance we can derive a distribution over words W from the",null,null
,,,
128,"highest ranked entities. This has been found to be effective in the related work [3, 34]. Further vocabularies over name aliases, related entities, types and categories can be derived as explained above.",null,null
,,,
129,3.3 Corpus Feedback,null,null
,,,
130,"We can also apply a variation on pseudo-relevance feedback which we extend to document annotations (cf. Figure 2d). The unaltered relevance model provides feature f RM(Q, W ) by integrating p(D|Q) and p(W |D) over documents in the feedback set.",null,null
,,,
131,"In a similar fashion we can derive a distribution over all mentions M , denoted by the feature f RM(Q, M ). Mentions include both the string that is linked to an entity as well as unlinked Named Entity Spans (NERs). Even if these mentions M cannot be linked to the knowledge base, they provide useful entity-like information for expansion, as used by Callan et al. [7].",null,null
,,,
132,"For linked mentions, the entity link disambiguation probability gives an alternative indicator for relevant entities E.",null,null
,,,
133,"f RM(Q, E) ,",null,null
,,,
134,p(E|M )p(M |D) p(D|Q),null,null
,,,
135,DM,null,null
,,,
136,"The disambiguation confidence distribution p(E|M ) has an effect in cases where multiple entities have a high disambiguation probability for a given mention. In the experimental section we explore options ranging from noisy indicators to high-precision links, such as using only the highest ranked entity or additionally applying a NIL classification. In these conservative options we define p(E|M ) , 1 for the most confident (and non-NIL) linked entity E and 0 otherwise.",null,null
,,,
137,"From the distribution over entities, we can follow the connection to the knowledge base (cf. Figure 2b) and derive distribution over name aliases, types, categories, and neighbor entities.",null,null
,,,
138,3.4 Entity Context Model,null,null
,,,
139,"The corpus feedback provides distributions over entities. However, it is likely that relevant entities are referred to in a query-specific way which differs from the global term distribution in the knowledge base. For instance in the query ""obama family tree"" we expect the entity [Barack_Obama] to be referred to by personal names and less via his political function. Also, some related entities (in particular family members) are more important than others.",null,null
,,,
140,"Our goal is to develop query-specific distributions over name aliases and related entities by inspecting the local context surrounding entity annotations for co-occurrences of entities with words and pairs of entities. In our experiments, we create three versions of each entity's query-specific context model, varying the size of the context snippets: 8 words on either side of a mention, 50 words on either side, or one sentence, where sentence boundaries are determined by a sentence-splitter.",null,null
,,,
141,"From each feedback document D and each annotated entity mention, M , we build entity context snippets using the only contextual window around the annotation. For each entity, E, we aggregate all snippets by weighting them by the document retrieval probability p(D|Q).",null,null
,,,
142,"The entity context model for a given entity, E, provides a distribution over words, W , which is used for the context model feature f ECM(E, W ). Likewise, the entity context model provides a distribution over co-occurring neighboring entities E as f ECM(E, E ). And by following the link to the knowledge base, features over co-occurring name aliases, types, and categories.",null,null
,,,
143,3.5 Learning Feature Weights,null,null
,,,
144,"So far we introduced several features f for query expansion with words, entities, mentions, types, categories, and neighbors using various options to traverse available information sources, each representing a path in Figure 2.",null,null
,,,
145,The large number of features renders grid-tuning approaches infeasible. We exploit that our model falls into the family of loglinear models and can therefore be efficiently estimated with a learning-to-rank approach.,null,null
,,,
146,"For every feature, f , we build the expansion model induced by this feature only. For example from f RM(Q, E) we build an expansion model over entities pRM (E) by normalizing across all entity candidates E .",null,null
,,,
147,"pRM (E) ,",null,null
,,,
148,"f RM(Q, E) E f RM(Q, E )",null,null
,,,
149,"For every document d in our training set, we compute the retrieval score under the RM1 expansion model pRM (E) using only the k highest ranked entities weighted by their respective probability under the expansion model.",null,null
,,,
150,"Following this procedure, each feature function f (Q, V ) is converted for all vocabularies V into a feature vector for documents d in the training set. We use a log-linear learning-to-rank approach to optimize the retrieval effectiveness for the target metric under this feature function. This provides the parameter vector  which corresponds to the weights for each expansion model, when retrieving rankings for test queries.",null,null
,,,
151,"By incorporating the retrieval score from the original query Q as a special feature function, this also determines the RM3 balance weight on the original query with respect to all expansion models.",null,null
,,,
152,4. EXPERIMENTAL SETUP,null,null
,,,
153,"This section details the tools and datasets used for our experiments. Results are available online.2 The retrieval experiments described in this section are implemented using Galago,3 an open source search engine. The structured query language supports exact matching, phrases, and proximity matches needed for our retrieval models. A summary of the document collections used in these experiments is presented in Table 2. The corpora include both newswire (Robust04)",null,null
,,,
154,2http://ciir.cs.umass.edu/downloads/eqfe/ 3http://www.lemurproject.org/galago.php,null,null
,,,
155,368,null,null
,,,
156,Table 2: Test Collections Statistics.,null,null
,,,
157,Name Robust04 ClueWeb09-B ClueWeb12-B,null,null
,,,
158,"Documents 528,155",null,null
,,,
159,"50,220,423 52,343,021",null,null
,,,
160,"Topic Numbers 301-450, 601-700",null,null
,,,
161,1-200 1-50,null,null
,,,
162,"and web pages (ClueWeb). During indexing and retrieval, both documents and query words are stemmed using the Krovetz stemmer [21]. Stopword removal is performed on word features using the INQUERY 418 word stop list. For the web collections, the stopword list is augmented with a small collection of web-specific terms, including ""com"", ""html"", and ""www"". We use title queries which contain only a few keywords.",null,null
,,,
163,"Across all collections retrieval and feedback model parameters are learned or tuned using 5-fold cross-validation. Instead of selecting a single number of feedback documents or entities, we include expansion feature models with different hyperparameters and learn a weighted combination of these along with other features. We include expansion features from one, ten, and twenty feedback entities and documents. We optimize parameters  with a coordinate-ascent learning algorithm provided in the open source learning-to-rankframework RankLib.4 Parameters are optimized for mean average precision (MAP) effectiveness directly.",null,null
,,,
164,"Retrieval effectiveness is evaluated with standard measures, including mean average precision (MAP) at 1000. Because several of our collections focus on web search, where precision at the early ranks is important, we also report normalized discounted cumulative gain (NCGD@20) and expected reciprocal rank (ERR@20).",null,null
,,,
165,We now describe the aspects of the entities in documents and queries for each collection in more detail.,null,null
,,,
166,4.1 Knowledge Base,null,null
,,,
167,"We use a combination of Wikipedia and Freebase as knowledge bases in these experiments. Many of the Freebase entities are contained in Wikipedia. We use the Freebase schema to map between the two knowledge bases (using attributes: /wikipedia/en_title and /wikipedia/en). These knowledge resources provide the entity features used for query expansion from linked entities described in Section 3. Our Wikipedia collection is derived from a Freebasegenerated dump of the English Wikipedia from January 2012, which contains over 3.8 million articles. For each Wikipedia entity we extract an entity representation consisting of its article text, canonical name, categories, and a distribution over aliases from redirects, Wikipedia-internal anchor text, and web anchor text from the Google cross-lingual dictionary [31]. In these experiments we also use a subset of the Freebase data: machine identifiers (MIDs), types, and aliases.",Y,null
,,,
168,4.2 Robust'04,Y,null
,,,
169,"No publicly available entity annotations exist for Robust04 queries or documents. We do not exploit explicit entity annotations in queries, reducing the model in 3.1 to only the words in the title query. For document analysis, we use the extraction tools in the Factorie [23] NLP library. We use Factorie to perform tokenization, sentence segmentation, named entity recognition, part-of-speech tagging, dependency parsing, and entity mention finding. The entity mentions detected by Factorie are linked to the knowledge base",null,null
,,,
170,4http://people.cs.umass.edu/~vdang/ranklib. html,null,null
,,,
171,"using our state-of-the-art entity linking system, KB Bridge [11], which is trained on the TAC KBP entity linking data from 20092012. For each mention, the entity linker provides a distribution over the top fifty most probable entities. Based on the TAC evaluation data, the linker has an F1 score of approximately 80-85%. We note that this entity linker is trained to detect and link traditional named entities (people, organization, and geo-political entities) and may not detect or link conceptual entities. Because of limited resources we do not entity link all documents in the Robust04 collection. Instead, we pool the top one hundred documents from all of the baseline text retrieval runs. For our resulting experiments we perform re-ranking on this pooled set of documents using the entity linking features. We use the top documents as sources for extracting both text and entity features.",null,null
,,,
172,4.3 ClueWeb09 and ClueWeb12,Y,null
,,,
173,"We perform experiments using two web datasets from the TREC web track. The first is the ClueWeb09 dataset. For the queries, we use the title queries, but some entity annotations are derived from the descriptions. The Google FACC1 data provides explicit entity annotations for the web track queries (2009-2012) queries, created by automatically entity linking and manually correcting entities in the text of the topic descriptions. We found these to be missing significant numbers of entities and so manually revised these annotations to improve recall and fix several annotation errors. We discuss these revisions in Section 5.3. For the documents, we use the ClueWeb09 Category-B collection, which consists of 50 million pages, including Wikipedia. For ClueWeb09-B, we apply spam filtering with a threshold of 60, using the Waterloo spam scores [9].",Y,null
,,,
174,"We use the ClueWeb12 collection with the TREC web tack 2013 queries, using only the titles. Similar to Robust04, there are no explicit entity annotations. We do not apply spam filtering on the ClueWeb12 documents because hard spam filtering was shown to hurt all the baseline retrieval runs.",Y,null
,,,
175,5. EXPERIMENTAL EVALUATION,null,null
,,,
176,"The effectiveness of our query feature expansion is compared with state-of-the-art word-based retrieval and expansion models. Our baseline retrieval model is the Sequential Dependence Model (SDM) [24]. We also compare to two baseline expansion models. The first is an external feedback model, which uses the Wikipedia knowledge base as a text collection and extracts terms from the top ranked article, which we call WikiRM1. Models similar to WikiRM1 were shown to be effective for these collections in previous work [3, 34]. The second baseline uses collection ranking from the SDM model and builds a collection relevance model, which we call SDM-RM3. For ClueWeb12 we also report an official baseline using Indri's query likelihood model (Indri-QL).",null,null
,,,
177,5.1 Overall Performance of EQFE,null,null
,,,
178,"The overall retrieval effectiveness across different methods and collections is presented in Table 3 and Figure 3. Our EQFE model is the best performer on MAP for Robust04 and best on NDCG@20, ERR@20 and MAP on the ClueWeb12B collection. A paired t-test with -level 5% indicates that the improvement of EFQE over SDM is statistically significant for both. For ClueWeb09B, the EQFE numbers are slightly worse, but no significant difference is detected among the competing methods. The helps/hurts analysis reveals that EQFE helps a few more times than it hurts in ClueWeb09B. (cf. 4).",Y,null
,,,
179,"In order to analyze whether the EQFE method particularly improves difficult or easy queries, we sub-divide each set into percentiles according to the SDM baseline. In Figure 4 the queries are organized from most difficult to easiest. The 5% of the hardest",null,null
,,,
180,369,null,null
,,,
181,Table 3: Summary of results comparing EQFE for <title> queries across the three test collections.,null,null
,,,
182,Model SDM WikiRM1 SDM-RM3 EQFE,null,null
,,,
183,MAP 26.15 27.41 29.38 32.77,null,null
,,,
184,Robust04 P@20 NDCG@20 37.52 42.37 37.71 42.81 38.82 43.44 38.00 42.40,Y,null
,,,
185,MAP 11.43 11.39 11.43 11.00,null,null
,,,
186,ClueWeb09B,Y,null
,,,
187,ERR@20 NDCG@20,null,null
,,,
188,13.63,null,null
,,,
189,21.4,null,null
,,,
190,15.29,null,null
,,,
191,22.56,null,null
,,,
192,13.63,null,null
,,,
193,21.4,null,null
,,,
194,14,null,null
,,,
195,21.12,null,null
,,,
196,MAP 4.18 4.00 3.53 4.67,null,null
,,,
197,ClueWeb12B,Y,null
,,,
198,ERR@20 NDCG@20,null,null
,,,
199,9.15,null,null
,,,
200,12.61,null,null
,,,
201,9.31,null,null
,,,
202,12.8,null,null
,,,
203,7.61,null,null
,,,
204,11,null,null
,,,
205,10,null,null
,,,
206,14.61,null,null
,,,
207,map ndcg20 ndcg20,null,null
,,,
208,0.35,null,null
,,,
209,0.3,null,null
,,,
210,0.25,null,null
,,,
211,0.2,null,null
,,,
212,0.15,null,null
,,,
213,0.1,null,null
,,,
214,0.05,null,null
,,,
215,0,null,null
,,,
216,sdm,null,null
,,,
217,rm,null,null
,,,
218,wikiRm1 EQFE,null,null
,,,
219,(a) Robust04,Y,null
,,,
220,0.25,null,null
,,,
221,0.2,null,null
,,,
222,0.15,null,null
,,,
223,0.1,null,null
,,,
224,0.05,null,null
,,,
225,0,null,null
,,,
226,sdm,null,null
,,,
227,rm,null,null
,,,
228,wikiRm1 EQFE,null,null
,,,
229,(b) ClueWeb09B,Y,null
,,,
230,0.18,null,null
,,,
231,0.16,null,null
,,,
232,0.14,null,null
,,,
233,0.12,null,null
,,,
234,0.1,null,null
,,,
235,0.08,null,null
,,,
236,0.06,null,null
,,,
237,0.04,null,null
,,,
238,0.02,null,null
,,,
239,0.00 sdm,null,null
,,,
240,rm wikiRm1 EQFE indri-ql,null,null
,,,
241,(c) ClueWeb12B,Y,null
,,,
242,Figure 3: Mean retrieval effectiveness with standard error bars.,null,null
,,,
243,0.9,null,null
,,,
244,sdm,null,null
,,,
245,0.8,null,null
,,,
246,rm,null,null
,,,
247,wikiRm1,null,null
,,,
248,0.7,null,null
,,,
249,EQFE,null,null
,,,
250,0.6,null,null
,,,
251,0.5,null,null
,,,
252,0.4,null,null
,,,
253,0.3,null,null
,,,
254,0.2,null,null
,,,
255,0.1,null,null
,,,
256,0,null,null
,,,
257,0%-5%,null,null
,,,
258,5%-25% 25%-50% 50%-75% 75%-95% 95%-100%,null,null
,,,
259,difficulty percentile according to sdm,null,null
,,,
260,(a) Robust04,Y,null
,,,
261,0.8,null,null
,,,
262,sdm,null,null
,,,
263,0.7,null,null
,,,
264,rm,null,null
,,,
265,wikiRm1,null,null
,,,
266,0.6,null,null
,,,
267,EQFE,null,null
,,,
268,0.5,null,null
,,,
269,0.4,null,null
,,,
270,0.3,null,null
,,,
271,0.2,null,null
,,,
272,0.1,null,null
,,,
273,0,null,null
,,,
274,0%-5%,null,null
,,,
275,5%-25% 25%-50% 50%-75% 75%-95% 95%-100%,null,null
,,,
276,difficulty percentile according to sdm,null,null
,,,
277,(b) ClueWeb09B,Y,null
,,,
278,0.45,null,null
,,,
279,sdm,null,null
,,,
280,0.4,null,null
,,,
281,rm,null,null
,,,
282,wikiRm1,null,null
,,,
283,0.35,null,null
,,,
284,EQFE,null,null
,,,
285,indri-ql,null,null
,,,
286,0.3,null,null
,,,
287,0.25,null,null
,,,
288,0.2,null,null
,,,
289,0.15,null,null
,,,
290,0.1,null,null
,,,
291,0.05,null,null
,,,
292,0,null,null
,,,
293,0%-5%,null,null
,,,
294,5%-25% 25%-50% 50%-75% 75%-95% 95%-100%,null,null
,,,
295,difficulty percentile according to sdm,null,null
,,,
296,(c) ClueWeb12B,Y,null
,,,
297,"Figure 4: Mean retrieval effectiveness across different query-difficulties, measured according to the percentile of the SDM method.",null,null
,,,
298,370,null,null
,,,
299,map ndcg20 ndcg20,null,null
,,,
300,W doc 20 W sdm,null,null
,,,
301,E doc-ent 20 t1nn E doc-ent 20 all E doc-ent 20 t1 A kb 20 A kb 10 W kb 5 E ecm 50 A kb 5 E ecm sent W kb 1 W ecm 50 W ecm sent E ecm 8 C doc-ent 20 W ecm 8 A kb 1 E kb 50 E kb 20 A doc-ent 20 all A doc-ent 20 t1 E kb 10,null,null
,,,
302,M doc-ent 20 ner A doc-ent 20 t1nn,null,null
,,,
303,T doc-ent 20 A ecm 8 E kb 1,null,null
,,,
304,M doc-ent 10 ner C kb 1,null,null
,,,
305,A ecm sent A ecm 50,null,null
,,,
306,W doc-ent all 20 W doc-ent t1 20 W doc-ent t1 10 W doc-ent all 10 W doc-ent t1nn 20 W doc-ent t1nn 10,null,null
,,,
307,T kb 1 W doc-ent t1 1 W doc-ent all 1 W doc-ent t1nn 1,null,null
,,,
308,Table 4: Queries EFQE helped versus hurt over SDM baseline.,null,null
,,,
309,Robust04 ClueWeb09B ClueWeb12B,Y,null
,,,
310,Queries Helped 173 68 26,null,null
,,,
311,Queries Hurt 47 65 8,null,null
,,,
312,"queries are represented by the left-most cluster of columns, the 5% of the easiest queries in the right-most cluster of columns, the middle half is represented in two middle clusters (labeled ""25%-50%"" and ""50%-75%"").",null,null
,,,
313,"This analysis shows that EQFE especially improves hard queries. For Robust04 and ClueWeb12B EQFE outperforms all methods, except for the top 5% of the easiest queries (cf. 4a and 4c). For ClueWeb09B all queries in the difficult bottom half (cf. 4b) are improved. We want to point out that we achieve this result despite having on average 7 unjudged documents in the top 20 and 2.5 unjudged documents in the top 10 (in both the ""5%-25%"" and ""25%50%"" cluster), which are counted as negatives in the analysis.",Y,null
,,,
314,"The WikiRM1 method, which is the most similar expansion method to EQFE, demonstrates the opposite characteristic, outperforming EQFE only on ""easiest"" percentiles.",null,null
,,,
315,5.2 Feature-by-Feature Study,null,null
,,,
316,"We study the contribution of each of the features by re-ranking the pooled documents according to the feature score alone and measuring the retrieval effectiveness in MAP. The results for each collection are shown in Figure 5. It shows a subset of the top expansion features. The label on the x-axis has three attributes of the entity features: the vocabulary type, feedback source, and number of expansion terms. The vocabulary types are (A,E,C,W ,M , and T from Figure 2). The source is the original query (Q), query annotation (query ann), corpus feedback (doc), corpus feedback using entity features (doc - ent), knowledge base feedback (kb), and entity context model feedback (ecm). The last number in the description usually indicates the number of feedback terms (1, 5, 10, 20, and 50). For ecm it indicates the size of the context model window. We note that for several classes of features have similar names. These are variations of the same expansion feature. For example, the most confident entity (t1), the most confident entity whose score is above the NIL threshold (t1nn), or any entity above the NIL threshold (all).",null,null
,,,
317,"Entity identifiers E are top features across all collections, but every collection prefers entities expanded by a different paradigm: For Robust from corpus feedback, for ClueWeb09B from the entity context model, and for ClueWeb12B from knowledge base expansion with five entities.",null,null
,,,
318,"For the Robust04 collection, our study confirms that query keywords are highly indicative of relevance and accordingly words from corpus feedback are strong features. This is in not the case for the ClueWeb collections.",null,null
,,,
319,"For both ClueWeb collections, the entity context model with window size 8 performs well. Further, name aliases from both corpus feedback and from entity context models are highly effective, even where the entity identifiers themselves are not. We believe this is because the recall of the entity identifiers in the FACC1 data is limited. Here the name aliases bridge this annotation gap.",null,null
,,,
320,"We note that certain vocabularies such as categories and types do not perform well on their own, but likely help in combination with other features.",null,null
,,,
321,map,null,null
,,,
322,map,null,null
,,,
323,0.30 0.25 0.20 0.15 0.10 0.05 0.00,null,null
,,,
324,0.12 0.10 0.08 0.06 0.04 0.02 0.00,null,null
,,,
325,0.040 0.035 0.030 0.025 0.020 0.015 0.010 0.005 0.000,null,null
,,,
326,robust clueweb09b clueweb12b,Y,null
,,,
327,map,null,null
,,,
328,Figure 5: Features sorted by retrieval effectiveness on its own.,null,null
,,,
329,E ecm 8 A doc-ent 20 all,null,null
,,,
330,W ecm 8 A kb 10 W kb 1,null,null
,,,
331,W ecm 50 W sdm E kb 1,null,null
,,,
332,A ecm 50 A kb 1,null,null
,,,
333,A doc-ent 20 t1nn A doc-ent 20 t1,null,null
,,,
334,E doc-ent 20 t1nn E kb 20,null,null
,,,
335,W doc-ent t1 10 W doc-ent all 10,null,null
,,,
336,A kb 20 A kb 5,null,null
,,,
337,W doc-ent all 20 W doc-ent t1nn 10,null,null
,,,
338,W doc-ent all 1 W doc-ent t1nn 1,null,null
,,,
339,W doc-ent t1 1 W doc 20,null,null
,,,
340,E doc-ent 20 all E ecm 50,null,null
,,,
341,W doc-ent t1 20 C doc-ent 20 A ecm 8 C kb 1 T doc-ent 20 T kb 1,null,null
,,,
342,W doc-ent t1nn 20 W kb 5 E kb 50 E kb 10,null,null
,,,
343,E doc-ent 20 t1,null,null
,,,
344,W sdm W kb 5 W kb 1 W ecm 8 W ecm 50 A kb 20 A ecm 8 A kb 10 A kb 5 A ecm 50 W doc 20 E doc-ent 20 t1 E doc-ent 20 all M doc-ent 20 ner A kb 1 E doc-ent 20 t1nn M doc-ent 10 ner A doc-ent 20 t1nn A doc-ent 20 t1 A doc-ent 20 all E ecm 50 E ecm 8 W doc-ent t1 20 W doc-ent all 20 W doc-ent t1nn 20 W doc-ent t1nn 10 W doc-ent t1 10 W doc-ent all 10 C kb 1 C doc-ent 20 W doc-ent t1nn 1 W doc-ent t1 1 W doc-ent all 1 T doc-ent 20 T kb 1 E kb 50 E kb 20 E kb 1 E kb 10,null,null
,,,
345,371,null,null
,,,
346,Table 5: Different classes of entities are more prevalent in different data set. Number of queries that mention each entity class.,null,null
,,,
347,Dataset,null,null
,,,
348,Overall Freebase NER PER/ORG/LOC,null,null
,,,
349,Robust04,Y,null
,,,
350,249,null,null
,,,
351,243 85,null,null
,,,
352,49,null,null
,,,
353,Clueweb09,Y,null
,,,
354,200,null,null
,,,
355,191 108,null,null
,,,
356,80,null,null
,,,
357,ClueWeb12,null,null
,,,
358,50,null,null
,,,
359,48 26,null,null
,,,
360,16,null,null
,,,
361,Table 6: Mean Average Precision over subsets of Robust04 queries that mention entities of respective classes.,null,null
,,,
362,Method Overall Freebase NER PER/ORG/LOC,null,null
,,,
363,SDM MSE EQFE,null,null
,,,
364,26.15 30.49 32.77,null,null
,,,
365,26.61 31.11 31.02 36.45 33.33 38.28,null,null
,,,
366,27.72 31.98 33.31,null,null
,,,
367,5.3 Error Analysis of ClueWeb09,null,null
,,,
368,We now perform an analysis of the ClueWeb09 results to better understand why EQFE using entity feature expansion does not significantly improve the results. This case is particularly surprising because it is the only dataset where explicit entity query annotations are available.,Y,null
,,,
369,"We first examine the FACC1 query annotations. The FACC1 dataset contains entity annotations for 94 of the 200 queries. Upon inspecting the annotations, we found that despite manual labeling, many entities were not annotated. The queries were manually re-annotated, resulting in 191 of the 200 queries containing an entity. The revised annotations are available on our website.5 We used the revised query entity annotations for our experiments on ClueWeb09B.",Y,null
,,,
370,"The remaining queries without entity links are interesting. Several contain entities that are not noteworthy enough to be included in existing public knowledge bases, including:""jax chemical company"", ""fickle creek farm"", ""sit and reach test"", and ""universal animal cuts"". The remaining are not entity-centric without clearly defined concepts: ""getting organized"" and ""interview thank you"".",null,null
,,,
371,"However, even after coverage of queries is improved, the feature does not appear to help overall. To better understand the contribution (or lack thereof) of explicit entities in the query, we evaluated a query model that uses only entity identifiers to retrieve documents. Surprisingly, the entities alone have poor effectiveness, with a MAP of 0.048, an NDCG@20 of 0.162, and an ERR@20 of 0.123. This is less than half the effectiveness of the SDM baseline. We observe that 72.5% of the documents returned using the entity model are unjudged. The retrieved results differ significantly from the pool of judged documents. Further assessments are required to assess the model effectiveness. Beyond unjudged documents, we also examine the potential for explicit entities by analyzing the relevance judgments.",null,null
,,,
372,"We analyze the potential for explicit entities using all of the judged documents for the queries. We find that 37.4% of the relevant documents in ClueWeb09B do not contain an explicit query entity. The largest source of missing entities in documents are those in Wikipedia. Missing entity links for Wikipedia accounts for 24.6% of the documents. The FACC1 annotations do not contain annotations for the majority of Wikipedia articles in ClueWeb09B. Of the relevant documents that contain at least one entity, 43% of these contain at least one mention of an explicit query entity. This indicates that 57% of the remaining relevant documents do not contain the explicit query entity and cannot be matched using this feature alone. The reasons for the mismatch is an area for future work. It is caused by both missing entity links as well as fundamental query-document mismatch.",Y,null
,,,
373,5.4 Entity Analysis of queries,null,null
,,,
374,In this section we further study the entity characteristics of these datasets. How common are different classes of entities in the,null,null
,,,
375,5http://ciir.cs.umass.edu/downloads/eqfe/,null,null
,,,
376,queries? We manually classify the presence of entities in the queries for all of the datasets.,null,null
,,,
377,"The queries are labeled with three classes of entities. The first is the most general, whether the entity occurs in Freebase. The second whether it contains a named entity that would be detected by a typical entity recognition (NER) system. The last class of entities is narrower and is restricted to people, organizations, and locations. For each query we classify whether or not an entity of a particular class appears in the query. We do not examine the number of entities in the query or the centrality of the entity to the query.",null,null
,,,
378,"The entity classification statistics are shown in Table 5. We observe that between 95% and 98% of the queries contain at least one mention of a Freebase entity. Many of the entities in the queries are general concepts, such as `mammals', `birth rates', `organized crime', and `dentistry'. For the web queries, approximately half the queries (54% and 52%) contain a named entity. The distribution of the types in the web queries is similar. A smaller percentage of queries for Robust04 contain named entities, only 34%. One reason for this is that web queries are more likely to contain brand names, actors, songs, and movies. Examples of these include `Ron Howard,', `I will survive', `Nicolas Cage', `Atari', `Discovery Channel', `ESPN', and `Brooks Brothers'.",null,null
,,,
379,"When the entities are restricted to people, organizations, and locations the fraction of queries containing entities decreases further. The fraction of entities that fall into this limited class is between 59% and 74% of the queries containing named entities overall. These entities belong to the ""MISC"" category and include diseases, songs, movies, naval vessels, drugs, nationalities, buildings, names of government projects, products, treaties, monetary currencies, and others. These appear to be common in queries and more emphasis should be placed on finer grained entity type classification.",null,null
,,,
380,5.5 Effectiveness on Robust04,Y,null
,,,
381,"In this section we describe an analysis of the effectiveness of the previously described entity query classes for the Robust04 dataset. We study the behavior of three retrieval models: sequential dependence model (SDM), multiple source expansion (MSE) [3], and entity-based feature expansion (EQFE). The results are shown in Table 6.",null,null
,,,
382,"We observe that the EQFE expansion model is the best performing model across all classes of queries. We also note that queries that contain entities perform better for all retrieval models. The differences with queries containing Freebase entities are small, which is not surprising because most of the queries contain at least one entity. EQFE performs consistently better than the other models for all classes of queries.",null,null
,,,
383,"The most interesting finding is the comparison of queries with named entities (NER). Queries containing named entities, but not restricted to PER/ORG/LOC show a difference over the other classes of queries. It demonstrates that the queries with `MISC' entities perform better than other classes of entity queries for all models. The gains are the largest for this class of queries for EQFE compared with the baseline SDM retrieval model.",null,null
,,,
384,372,null,null
,,,
385,6. RELATED WORK,null,null
,,,
386,6.1 Query Expansion,null,null
,,,
387,"Query expansion techniques have been well studied for many models [22, 29]. Unlike most models, our approach goes beyond words or even features of words and includes features from entity links. The mostly closely related work is Latent Concept Expansion (LCE) model proposed by Metzler and Croft [25]. It builds upon the sequential dependence retrieval framework and introduces the idea of using arbitrary features for expansion. However, although a general framework is proposed they find improvements using only unigram features. Another well-known expansion model is Latent Concept Analysis from Xu and Croft [33], which selects `concepts', limited to unigram and phrase features that co-occur near query terms in top ranked documents. The contribution of words versus phrases was not tested. In contrast, we use words, phrases, and structured entity attributes in EQFE to improve retrieval effectiveness.",null,null
,,,
388,6.2 Entity Retrieval,null,null
,,,
389,"Using entities in retrieval is an area that has been well studied. In particular, the research area of retrieving entities has received significant recent attention. Entity retrieval was studied at the TREC entity retrieval track [2], at INEX with the entity ranking [12] and linked data tracks [32], the workshop on Entity Oriented and Semantic Search [1, 5], and other venues. In contrast, we focus on document retrieval leveraging entity annotations. Exploiting entity links and other types of semantic annotations is an area of open research. The workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR) [4, 18] has run over the last five years, and highlights the need for continued research in this area.",Y,null
,,,
390,6.3 World Knowledge,null,null
,,,
391,"Using Wikipedia as a source of world knowledge has been demonstrated to improve a variety of tasks, including retrieval. It is a common source of external query expansion [13, 3, 34]. Wikipedia entities as a basis for semantic representation demonstrated significant gains for a variety of NLP tasks. These tasks include semantic relatedness [14], document clustering [14], and entity linking [10]. We demonstrate that leveraging structured attributes of knowledge base entities similarly provides substantial gains in effectiveness for retrieval.",null,null
,,,
392,6.4 Entity Context Model,null,null
,,,
393,"Building entity context models from their surrounding representation has been studied in the past. In 1994, Conrad and Utt [8] used all paragraphs in the corpus surrounding named entity mentions to represent the entity, allowing free text queries to find names associated with a query. Ten years later, Raghavan et al. [27] extended that idea to use language modeling as a representation and showed that these models could successfully be used to cluster, classify, or answer questions about entities. In these cases, the entity's context was a paragraph or a fixed number of words surrounding all mentions of the entity in the corpus. More recently, the work of Schlaefer et al. [30] expanded the representation of a Wikipedia entity using extracted ""text nuggets"" from the web for use in the Watson question answering system. Nuggets that were scored as relevant to the entity were used as its context, even if the nugget did not contain an actual mention.",null,null
,,,
394,"Our entity context model (ECM) differs from existing work in three key ways. First, it uses state-of-the-art disambiguated entity links. If there are multiple ambiguous mentions of the same name, the contexts are separated based on their linked entity. Also, we do",null,null
,,,
395,"this for all types of concepts that exist in the knowledge base rather than just traditional named entities (person, organization, location).",null,null
,,,
396,"Second, our context models are query focused. We construct an entity context model from documents retrieved in response to the query. This change is important for large corpora because for entities with multiple diverse topics a representation across the entire collection will blend these topics together and lose their distinguishing characteristics. For example, the ClueWeb09 query [obama family tree] focuses on aspects of Obama's family life and relationships to relatives, which is a relatively obscure topic when compared with more popular aspects such as ""obamacare.""",Y,null
,,,
397,"Finally, our approach captures not just words and phrases surrounding the mention, but structured annotations from co-occurring entities: their mentions and features of them, including types and categories. We also incorporate the uncertainty of extracted features, both the source relevance and entity link probability.",null,null
,,,
398,7. CONCLUSION,null,null
,,,
399,"We have shown that features derived from linked entities can be used to improve the effectiveness of document retrieval. In qualitatively different collections (Robust04 and ClueWeb12B), the EQFE method was on average the strongest performer compared to several state-of-the-art baselines. These are some of the first reported results using the FACC1 Freebase annotations for ad hoc retrieval.",Y,null
,,,
400,"One limitation of this work is that it depends upon the success and accuracy of the entity annotations and linking. It would be useful to understand the accuracy and utility more robust detection of entities such as `poverty', or `term limits' rather than focusing primarily on people, organizations, and locations.",null,null
,,,
401,"Our results are also affected by entities that are detectable but that are not in the knowledge base ­ e.g., `fickle creek farms'. For these entities, there is no knowledge base entry to leverage, so the simplest solution is to consider only the unstructured word features. Lastly, we also described and successfully incorporated an entity context model that represents an entity by the language surrounding its mentions in the context of the query.",null,null
,,,
402,"This work presents a first step leveraging large-scale knowledge resources that have become available in the last several years. We expect that as these knowledge resources mature that entity-based representations of both queries and documents will grow in importance, supporting increasingly complex information needs.",null,null
,,,
403,Acknowledgements,null,null
,,,
404,"This work was supported in part by the Center for Intelligent Information Retrieval, in part by IBM subcontract #4913003298 under DARPA prime contract #HR001-12-C-0015, and in part by NSF grant #IIS-0910884. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",null,null
,,,
405,8. REFERENCES,null,null
,,,
406,"[1] K. Balog, D. Carmel, A. P. de Vries, D. M. Herzig, P. Mika, H. Roitman, R. Schenkel, P. Serdyukov, and T. T. Duc. The first joint international workshop on entity-oriented and semantic search (JIWES). In ACM SIGIR Forum, volume 46, pages 87­94. ACM, 2012.",null,null
,,,
407,"[2] K. Balog, P. Serdyukov, and A. P. de Vries. Overview of the TREC 2011 entity track. In Proceedings of the Text REtrieval Conference (TREC), 2011.",null,null
,,,
408,"[3] M. Bendersky, D. Metzler, and W. B. Croft. Effective query formulation with multiple information sources. In Proceedings of the fifth ACM international conference on Web",null,null
,,,
409,373,null,null
,,,
410,"search and data mining, WSDM '12, pages 443­452, New York, NY, USA, 2012. ACM.",null,null
,,,
411,"[4] P. Bennett, E. Gabrilovich, J. Kamps, and J. Karlgren. Sixth Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR'13). In In Proceedings of CIKM '13, pages 2543­2544, New York, NY, USA, 2013. ACM.",null,null
,,,
412,"[5] R. Blanco, H. Halpin, D. M. Herzig, P. Mika, J. Pound, H. S. Thompson, and T. T. Duc. Entity search evaluation over structured web data. In Proceedings of the 1st international workshop on entity-oriented search workshop (SIGIR 2011), ACM, New York, 2011.",null,null
,,,
413,"[6] K. Bollacker, C. Evans, P. Paritosh, T. Sturge, and J. Taylor. Freebase: a collaboratively created graph database for structuring human knowledge. In In Proceedings of SIGMOD '08, pages 1247­1250, New York, NY, USA, 2008. ACM.",null,null
,,,
414,"[7] J. P. Callan, W. B. Croft, and J. Broglio. Trec and tipster experiments with inquery. Information Processing & Management, 31(3):327­343, 1995.",null,null
,,,
415,"[8] J. G. Conrad and M. H. Utt. A system for discovering relationships by feature extraction from text databases. In SIGIR'94, pages 260­270. Springer, 1994.",null,null
,,,
416,"[9] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets, Apr. 2010.",null,null
,,,
417,"[10] S. Cucerzan. TAC Entity Linking by Performing Full-document Entity Extraction and Disambiguation. In Proceedings of the Text Analysis Conference 2011, 2011.",null,null
,,,
418,"[11] J. Dalton and L. Dietz. A Neighborhood Relevance Model for Entity Linking. In Proceedings of the 10th International Conference in the RIAO series (OAIR), RIAO '13, New York, NY, USA, May 2013. ACM.",null,null
,,,
419,"[12] G. Demartini, T. Iofciu, and A. P. De Vries. Overview of the INEX 2009 entity ranking track. In Focused Retrieval and Evaluation, pages 254­264. Springer, 2010.",null,null
,,,
420,"[13] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '06, pages 154­161, New York, NY, USA, 2006. ACM.",null,null
,,,
421,"[14] E. Gabrilovich and S. Markovitch. Computing semantic relatedness using Wikipedia-based explicit semantic analysis. In Proceedings of the 20th international joint conference on Artifical intelligence, IJCAI'07, pages 1606­1611, San Francisco, CA, USA, 2007. Morgan Kaufmann Publishers Inc.",null,null
,,,
422,"[15] E. Gabrilovich, M. Ringgaard, and A. Subramanya. FACC1: Freebase annotation of ClueWeb corpora, version 1 (release date 2013-06-26, format version 1, correction level 0), June 2013.",Y,null
,,,
423,"[16] D. W. Huang, Y. Xu, A. Trotman, and S. Geva. Focused access to XML documents. chapter Overview of INEX 2007 Link the Wiki Track, pages 373­387. Springer-Verlag, Berlin, Heidelberg, 2008.",null,null
,,,
424,"[17] H. Ji, R. Grishman, and H. Dang. Overview of the TAC2011 knowledge base population track. In Text Analysis Conference, 2011.",null,null
,,,
425,"[18] J. Kamps, J. Karlgren, and R. Schenkel. Report on the Third Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR). SIGIR Forum, 45(1):33­41, May 2011.",null,null
,,,
426,"[19] R. Kaptein, P. Serdyukov, and J. Kamps. Linking wikipedia to the web. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information",null,null
,,,
427,"retrieval, SIGIR '10, pages 839­840, New York, NY, USA, 2010. ACM. [20] D. Koller and N. Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.",null,null
,,,
428,"[21] R. Krovetz. Viewing morphology as an inference process. In Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, pages 191­202. ACM, 1993.",null,null
,,,
429,"[22] V. Lavrenko and W. B. Croft. Relevance-Based Language Models. In Proceedings of the ACM SIGIR 01 conference, pages 120­127, 2001.",null,null
,,,
430,"[23] A. Mccallum, K. Schultz, and S. Singh. Factorie: Probabilistic programming via imperatively defined factor graphs. In In Advances in Neural Information Processing Systems 22, pages 1249­1257, 2009.",null,null
,,,
431,"[24] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '05, pages 472­479, New York, NY, USA, 2005. ACM.",null,null
,,,
432,"[25] D. Metzler and W. B. Croft. Latent concept expansion using markov random fields. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 311­318, New York, NY, USA, 2007. ACM.",null,null
,,,
433,"[26] P. Mika and T. Potter. Metadata statistics for a large web corpus. In Proceedings of the Linked Data Workshop (LDOW) at the International World Wide Web Conference, 2012.",null,null
,,,
434,"[27] H. Raghavan, J. Allan, and A. McCallum. An exploration of entity models, collective classification and relation description. In KDD Workshop on Link Analysis and Group Detection, pages 1­10, 2004.",null,null
,,,
435,"[28] L. Ratinov, D. Roth, D. Downey, and M. Anderson. Local and global algorithms for disambiguation to wikipedia. In ACL, 2011.",null,null
,,,
436,"[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System: Experiments in Automatic Document Processing, Prentice-Hall Series in Automatic Computation, chapter 14, pages 313­323. Prentice-Hall, Englewood Cliffs NJ, 1971.",null,null
,,,
437,"[30] N. Schlaefer, J. C. Carroll, E. Nyberg, J. Fan, W. Zadrozny, and D. Ferrucci. Statistical source expansion for question answering. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 345­354, New York, NY, USA, 2011. ACM.",null,null
,,,
438,"[31] V. I. Spitkovsky and A. X. Chang. A Cross-Lingual dictionary for english wikipedia concepts. In Conference on Language Resources and Evaluation, 2012.",null,null
,,,
439,"[32] Q. Wang, J. Kamps, G. R. Camps, M. Marx, A. Schuth, M. Theobald, S. Gurajada, and A. Mishra. Overview of the INEX 2012 linked data track. In INitiative for the Evaluation of XML Retrieval (INEX), 2011.",null,null
,,,
440,"[33] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst., 18(1):79­112, Jan. 2000.",null,null
,,,
441,"[34] Y. Xu, G. J. F. Jones, and B. Wang. Query Dependent Pseudo-relevance Feedback Based on Wikipedia. In Proceedings of the 32Nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '09, pages 59­66, New York, NY, USA, 2009. ACM.",null,null
,,,
442,374,null,null
,,,
443,,null,null

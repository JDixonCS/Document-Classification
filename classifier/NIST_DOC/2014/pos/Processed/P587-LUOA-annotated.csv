,sentence,label,data
,,,
0,Win-Win Search: Dual-Agent Stochastic Game in Session Search,null,null
,,,
1,"Jiyun Luo, Sicong Zhang, Hui Yang",null,null
,,,
2,"Department of Computer Science, Georgetown University",null,null
,,,
3,"{jl1749,sz303}@georgetown.edu, huiyang@cs.georgetown.edu",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"Session search is a complex search task that involves multiple search iterations triggered by query reformulations. We observe a Markov chain in session search: user's judgment of retrieved documents in the previous search iteration affects user's actions in the next iteration. We thus propose to model session search as a dual-agent stochastic game: the user agent and the search engine agent work together to jointly maximize their long term rewards. The framework, which we term ""win-win search"", is based on Partially Observable Markov Decision Process. We mathematically model dynamics in session search, including decision states, query changes, clicks, and rewards, as a cooperative game between the user and the search engine. The experiments on TREC 2012 and 2013 Session datasets show a statistically significant improvement over the state-of-the-art interactive search and session search algorithms.",Y,null
,,,
6,Categories and Subject Descriptors,null,null
,,,
7,H.3.3 [Information Systems ]: Information Storage and Retrieval--Information Search and Retrieval,null,null
,,,
8,Keywords,null,null
,,,
9,Dynamic Information Retrieval Modeling; POMDP; Stochastic Game; Session Search,null,null
,,,
10,1. INTRODUCTION,null,null
,,,
11,"Users often need a multi-query session to accomplish a complex search task. A session usually starts with the user writing a query, sending it to the search engine, receiving a list of ranked documents ordered by decreasing relevance, then examining the snippets, clicking on the interesting ones, and spending more time reading them; we call one such sequence a ""search iteration."" In the next iteration, the user modifies the query or issues a new query to start the search again. As a result, a series of search iterations form, which include a series of queries q1, ..., qn, a series of returned documents D1, ..., Dn, and a series of clicks C1, ..., Cn, some of which are SAT clicks (satisfactory clicked documents [9]). The session stops when the user's information need is satisfied or the user abandons the search [6]. The information retrieval (IR) task in this setting is called session search [7,",null,null
,,,
12,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609629.",null,null
,,,
13,Figure 1: A Markov chain of decision states in session search. (S: decision states; q: queries; A: user actions such as query changes; D: documents).,null,null
,,,
14,"11, 17, 22, 24, 25, 30, 31]. Table 1 lists example information needs and queries in session search.",null,null
,,,
15,"We are often puzzled about what drives a user's search in a session and why they make certain moves. We observe that sometimes the same user behavior, such as a drift from one subtopic to another, can be explained by opposite reasons: either the user is satisfied with the search results and moves to another sub information need, or the user is not satisfied with the search results and leaves the previous search path. The complexity of users' decision making patterns makes session search quite challenging [4, 29].",null,null
,,,
16,"Researchers have attempted to find out the causes of topic drifting in session search. The causes under study include personalization [29], task types [20, 24], and previous documents' relevance [11]. A user study is usually needed to draw conclusions about user intent. However, the focus of this paper is not on identifying user intent. Instead, we simplify the complexity of users' decision states into a cross product of only two dimensions: whether previously retrieved documents are relevant and whether the user would like to explore the next sub information need. Our work differs from existing work in that we consider that a session goes through a series of hidden decision states, with which we design a statistical retrieval model for session search. Our emphasis is an effective retrieval model, not a user study to identify the query intent.",null,null
,,,
17,"The hidden decision states form a Markov chain in session search. A Markov chain is a memoryless random process where the next state depends only on the current state [18]. Figure 1 illustrates a Markov chain of hidden decision states for TREC 2013 Session 9. In a session, a user's judgment of the retrieved documents in the previous iteration affects or even decides the user's actions in the next iteration. A user's actions could include clicks, query changes, reading the documents, etc. The user's gain, which we call reward, is the amount of relevant information that he or she obtains in the retrieved documents. The reward motives the later user actions. If the decision states are known, we can use a Markov Decision Process (MDP) to model the process. However, in session search, users' decision states are hidden. We therefore model session search as a Partially Observable Markov Decision Process (POMDP) [18].",Y,null
,,,
18,587,null,null
,,,
19,Table 1: Information needs and queries (examples are from TREC 2013 Session Track).,Y,null
,,,
20,Session 2: Information Need,null,null
,,,
21,Session 2: Queries,null,null
,,,
22,"You want to buy a scooter. So you're inter- q1,scooter brands",null,null
,,,
23,"q2,scooter brands reliable q3,scooter",null,null
,,,
24,"ested in learning more facts about scooters q4,scooter cheap",null,null
,,,
25,"q5,scooter review",null,null
,,,
26,"q6,scooter price",null,null
,,,
27,including:what brands of scooters are out,null,null
,,,
28,"q7,scooter price",null,null
,,,
29,"q8,scooter stores",null,null
,,,
30,"q9,where to buy scooters",null,null
,,,
31,there? What brands of scooters are reliable? Which scooters are cheap? Which stores sell scooters? which stores sell the best scooters?,null,null
,,,
32,Session 9: Information Need,null,null
,,,
33,Session 9: Queries,null,null
,,,
34,"You want to know more about old US coins. q1,old us coins",null,null
,,,
35,"q2,collecting old us coins q3,selling old us coins",null,null
,,,
36,"Relevant information to you includes value of old US coins, types of old US coins, old US silver dollar, q4,""selling old """"usa coins""""""",null,null
,,,
37,"how to start collecting old US coins, how to sell old US coins and how to buy them, where to buy those coins.",null,null
,,,
38,Session 87: Information Need,null,null
,,,
39,Session 87: Queries,null,null
,,,
40,"Suppose you're planning a trip to the United States.You will be there for a month and able to travel within a 150-mile radius of your destination.With that constraint, what are the best cities to consider as possible destinations?",null,null
,,,
41,"q1,best us destinations",null,null
,,,
42,"q2,distance new york boston",null,null
,,,
43,"q3,maps.bing.com",null,null
,,,
44,"q4,maps",null,null
,,,
45,"q5,bing maps",null,null
,,,
46,"q6,hartford tourism",null,null
,,,
47,"q7,bing maps",null,null
,,,
48,"q8,hartford visitors",null,null
,,,
49,"q9,hartford connecticut tourism",null,null
,,,
50,"q10,hartford boston travel",null,null
,,,
51,"q11,boston tourism",null,null
,,,
52,"q12,nyc tourism",null,null
,,,
53,"q13,philadelphia nyc distance",null,null
,,,
54,"q14,bing maps",null,null
,,,
55,"q15,philadelphia washington dc distance",null,null
,,,
56,"q16,bing maps",null,null
,,,
57,"q17,philadelphia tourism q18,washington dc tourism",null,null
,,,
58,"q19,philadelphia nyc travel q20,philadelphia nyc train q21,philadelphia nyc bus",null,null
,,,
59,"In fact, not only the user, but also the search engine, makes decisions in a Markov process. A search engine takes in a user's feedback and improves its retrieval algorithm iteration after iteration to achieve a better reward too. The search engine actions could include term weighting, turning on or turning off one or more of its search techniques, or adjusting parameters for the techniques. For instance, based on the reward, the search engine can select p in deciding the top p documents used in pseudo relevance feedback.",null,null
,,,
60,"We propose to model session search as a dual-agent stochastic game. When there is more than one agent in a POMDP, the POMDP becomes a stochastic game (SG). The two agents in session search are the user agent and the search engine agent. In contrast to most two-player scenarios such as chess games in game theory, the two agents in session search are not opponents to each other; instead, they cooperate: they share the decision states and work together to jointly maximize their goals. We term the framework ""win-win search"" for its efforts in ensuring that both agents arrive at a winwin situation. One may argue that in reality a commercial search engine and a user may have different goals and that is why some commercial search engines put their sponsors high in the returned results. However, this paper focuses on the win-win setting and assume a common interest ­ fulfilling the information needs ­ for both agents.",null,null
,,,
61,"The challenges of modeling session search as a stochastic game lie in how to design and determine the decision states and actions of each agent, how to observe their behaviors, and how to measure the rewards and set the optimization goals. We present the details in Sections 4 and 5. As a retrieval framework, we pay more attention to the search engine agent. When the search engine makes decisions, it picks a decision that jointly optimizes the common interest.",null,null
,,,
62,"We evaluate the win-win search framework on TREC 2012 & 2013 Session data. TREC (Text REtrieval Conference) 2010 - 2013 Session Tracks [20, 21] have spurred a great deal of research in session search [3, 10, 11, 14, 24]. The tracks provide interaction data within a session and aim to retrieve relevant documents for the last query qn in the session. The interaction data include queries, top returned documents, user clicks, and other relevant information such as dwell time. Document relevance is judged based on information need for the entire session, not just the last query. In this paper, all examples are from TREC 2013. Our experiments show that the proposed framework achieves statisti-",Y,null
,,,
63,cally significant improvements over state-of-the-art interactive search and session search algorithms.,null,null
,,,
64,"The remainder of this paper is organized as follows: Section 2 presents the related work, Section 3 provides preliminaries for POMDP. Section 4 details the win-win search framework, Section 5 elaborates the optimization, Section 6 evaluates the framework and Section 7 concludes the paper.",null,null
,,,
65,2. RELATED WORK,null,null
,,,
66,2.1 Session search,null,null
,,,
67,"Session search has attracted a great amount of research from a variety of approaches [3, 11, 22, 25, 33]. They can be grouped into log-based methods and content-based methods.",null,null
,,,
68,"There is a large body of work using query logs to study queries and sessions. Feild and Allan [8] proposed a taskaware model for query recommendation using random walk over a term-query graph formed from logs. Song and He's work [27] on optimal rare query suggestion also used random walk, with implicit feedback in logs. Wang et al. [30] utilized the latent structural SVM to extract cross-session search tasks from logs. Recent log-based approaches also appear in the Web Search Click Data (WCSD) workshop series.1",null,null
,,,
69,"Content-based methods directly study the content of the query and the document. For instance, Raman et al. [25] studied a particular case in session search where the search topics are intrinsically diversified. Content-based session search also include most research generated from the recent TREC Session Tracks [20, 21]. Guan et al. [10] organized phrase structure in queries within a session to improve retrieval effectiveness. Jiang et al. [14] proposed an adaptive browsing model that handles novelty in session search. Jiang and He [13] further analyzed the effects of past queries and click-through data on whole-session search effectiveness.",Y,null
,,,
70,"Others study even more complex search ­ search across multiple sessions [22, 24, 30]. Kotov et al. [22] proposed methods for modeling and analyzing users' search behaviors in multiple sessions. Wang et al. [30] identified cross-session search by investigating inter-query dependencies learned from user behaviors.",null,null
,,,
71,"Our approach is a content-based approach. However, it uniquely differs from other approaches by taking a Markov process point of view to study session search.",null,null
,,,
72,1http://research.microsoft.com/en-us/um/people/ nickcr/wscd2014,null,null
,,,
73,588,null,null
,,,
74,2.2 Relevance feedback,null,null
,,,
75,"Session search is closely related to relevance feedback, a traditional IR research field. Classic relevance feedback methods include Rocchio [16], pseudo relevance feedback [2], and implicit relevance feedback [27] based on user behaviors such as clicks and dwell time. Recently, researchers have investigated new forms of relevance feedback. Jin et al. [15] employed a special type of click ­ ""go to the next page"" ­ as relevance feedback to maximize retrieval effectiveness over multi-page results. Zhang et al. [33] modeled query changes between adjacent queries as relevance feedback to improve retrieval accuracy in session search.",null,null
,,,
76,"These relevance feedback approaches only considers oneway communication from the user to the search engine [15, 33]. On the contrary, this paper explicitly sets up a two-way feedback channel where both parties transmit information.",null,null
,,,
77,2.3 MDP and POMDP in IR,null,null
,,,
78,"Markov Decision Process (MDP) is an important topic in Artificial Intelligence (AI). An MDP can be solved by a family of reinforcement learning algorithms. Kaelbling et al. [18] brought techniques from operational research to choose the optimal actions in partially observable problems, and designed algorithms for solving Partially Observable Markov Decision Processes (POMDPs). IR researchers have just begun showing interests in MDP and POMDP [11, 30, 32] in finding solutions for IR problems.",null,null
,,,
79,"Early work on interactive search modeling by Shen et al. [26] used a Bayesian decision-theoretic framework, which is closely related to the MDP approaches. The QCM model proposed by Guan et al. [11] models session search as an MDP and effectively improves the retrieval accuracy. However, [11] used queries as states while we use a set of welldesigned hidden decision states. In addition, we explicitly model the stochastic game played between two agents, the user and the search engine, while [11] focused on just the search engine. Another difference is that we model a wide range of actions including query changes, clicks, and document content while [11] only used query changes.",null,null
,,,
80,"Yuan and Wang [32] applied POMDP for sequential selection of online advertisement recommendation. Their mathematical derivation shows that belief states of correlated ads can be updated using a formula similar to collaborative filtering. Jin et al. [15] modeled Web search as a sequential search for re-ranking documents in multi-page results. Their hidden states are document relevance and the belief states are given by a multivariate Gaussian distribution. They consider ""ranking"" as actions and ""clicking-on-the-next-page"" as observations. In win-win search, we present a different set of actions, observations, and messages between two agents. The fundamental difference between our approach and theirs is that we model the retrieval task as a dual-agent cooperative game while [15] uses a single agent.",null,null
,,,
81,3. PRELIMINARIES: MDP AND POMDP,null,null
,,,
82,"Markov Decision Process provides the basics for the winwin search framework. An MDP is composed by agents, states, actions, reward, policy, and transitions [19]. An agent takes inputs from the environment and outputs actions. The actions in turn influences the states of the environment. An MDP can be represented by a tuple < S, A, T, R >:",null,null
,,,
83,"States S is a discrete set of states. In session search, they can be queries [11] or hidden decision states (Section 4.2).",null,null
,,,
84,Actions A is a discrete set of actions that an agent can,null,null
,,,
85,"take. For instance, user actions include query changes, clicks,",null,null
,,,
86,and reading the returned documents or snippets.,null,null
,,,
87,"Transition T is the state transition function T (si, a, sj) ,"" P (sj|si, a). It is the probability of starting in state si, taking action a, and ending in state sj. The sum over all actions gives the total state transition probability between si and sj T (si, sj) "", P (sj|si); which is similar to the state transition probability in the Hidden Markov Model (HMM) [1].",null,null
,,,
88,"Reward r ,"" R(s, a) is the immediate reward, also known""",null,null
,,,
89,as reinforcement. It gives the expected immediate reward of,null,null
,,,
90,taking action a at state s.,null,null
,,,
91,Policy  describes the behaviors of an agent. A non-,null,null
,,,
92,stationary policy is a sequence of mapping from states to,null,null
,,,
93,actions.  is usually optimized to decide how to move around,null,null
,,,
94,in the state space to optimize the long term reward,null,null
,,,
95," t,1",null,null
,,,
96,r.,null,null
,,,
97,Value function and Q-function Given a policy  at,null,null
,,,
98,"time t, a value function V calculates the expected long term reward starting from state s inductively: V,t(s) ,"" R(s, t(s))+  s T (s, a "","" t(s), s )V,t+1(s ), where the initial value V,t"",1(s) ,"" R(s, a "", t,""1(s)), s is the current state, s""",null,null
,,,
99,"is the next state, a ,"" t(s) is any valid action for s at t,""",null,null
,,,
100, is a future discount factor. Usually an auxiliary func-,null,null
,,,
101,"tion, called the Q-function, is used for a pair of (s, a): Q(st, a) ,"" R(st, a) +  a P (st|st+1, a) maxa Q(st+1, a), where R(st, a) is the immediate reward at t, a is any valid action at t + 1. Note that V (s) "","" maxa Q(s, a).""",null,null
,,,
102,Q-Learning Reinforcement learning (RL) algorithms pro-,null,null
,,,
103,vides solutions to MDPs [19]. The most influential RL al-,null,null
,,,
104,gorithm is Q-learning. Given a Q-function and a starting,null,null
,,,
105,"state s, the solution can be a greedy policy that at each",null,null
,,,
106,"step, it takes the action that maximizes the Q-function:",null,null
,,,
107,"(s) ,"" arg maxa R(s, a) +  a T (s, a, s )Q(s , a) , where""",null,null
,,,
108,"the base case maximizes R(s1, a). Partially Observable MDP (POMDP) When states",null,null
,,,
109,are unknown and can only be guessed through a probabilistic,null,null
,,,
110,"distribution, an MDP becomes a POMDP [18]. POMDP is",null,null
,,,
111,"represented by a tuple < S, A, T, , O, B, R >, where S, A, R",null,null
,,,
112,"are the same as in MDP. Since the states are unknown, the",null,null
,,,
113,"transition function T models transitions between beliefs, not",null,null
,,,
114,transitions between states any more: T : B × A × B ,null,null
,,,
115,"[0, 1]. Belief B is the set of beliefs defined over S, which",null,null
,,,
116,indicates the probability that an agent is at a state s. It,null,null
,,,
117,is also known as belief state. Observations  is a discrete,null,null
,,,
118,set of observations that an agent makes about the states. O,null,null
,,,
119,is the observation function which represents a probabilistic,null,null
,,,
120,distribution for making observation  given action a and,null,null
,,,
121,landing in the next state s . A major difference between an,null,null
,,,
122,HMM and a POMDP is that POMDP considers actions and,null,null
,,,
123,rewards while HMM does not.,null,null
,,,
124,4. THE WIN-WIN SEARCH FRAMEWORK,null,null
,,,
125,"A session can be viewed as a Markov chain of evolving states (Figure 1). Every time when a new query is issued, both the user and the search engine transition into a new state. In our setting, the two agents work together to achieve a win-win goal.",null,null
,,,
126,4.1 Model Illustration,null,null
,,,
127,"Figure 2 shows the proposed dual-agent SG, which is represented as a tuple < S, Au, Ase, u, se, u, se, O, B, T, R >.",null,null
,,,
128,S is the decision states that we will present in Section 4.2.,null,null
,,,
129,589,null,null
,,,
130,Table 2: Symbols in the dual-agent stochastic game.,null,null
,,,
131,Name,null,null
,,,
132,Symbol Meanings,null,null
,,,
133,State,null,null
,,,
134,S the four hidden decision states in Figure 3,null,null
,,,
135,User action,null,null
,,,
136,"Au add query terms, remove query terms, keep query terms",null,null
,,,
137,Search engine action,null,null
,,,
138,"Ase increase/decrease/keep term weights, adjust search techniques, etc",null,null
,,,
139,Message from user to search engine u clicked and SAT clicked documents,null,null
,,,
140,Message from search engine to user se top k returned documents,null,null
,,,
141,User's observation,null,null
,,,
142,u observations that the user makes from the world,null,null
,,,
143,Search engine's observation,null,null
,,,
144,se observations that the search engine makes from the world and from the user,null,null
,,,
145,User reward,null,null
,,,
146,Ru relevant information the user gains from reading the documents,null,null
,,,
147,Search engine reward,null,null
,,,
148,Rse nDCG that the search gains by returning documents,null,null
,,,
149,Belief state,null,null
,,,
150,B belief states generated from the belief updater and shared by both agents,null,null
,,,
151,Figure 2: Dual-agent stochastic game.,null,null
,,,
152,"Au, Ase, u, and se are the actions. We divide the actions into two types: domain-level actions A, what an agent acts on the world directly, and communication-level actions , also known as messages, which only go between the agents. User actions Au are mainly query changes [11] while search engine actions Ase are term weighting schemes and adjustments to search techniques. Both u and se are sets of relevant documents, that an agent uses to inform the other agent about what they consider as relevant. (Section 4.3)",null,null
,,,
153," is the observation that an agent can draw from the world or from the other agent. O is the observation function that maps states and actions to observations: O : S × A   or O : S ×   . Note that the actions can be domain-level actions A or messages , or a combination of both. (S. 4.4)",null,null
,,,
154,"B is the set of belief states that shared by both agents. The beliefs are updated every time when an observation happens. There are two types of belief: B·, beliefs before the messages and B·, beliefs after the messages. (Section 4.5)",null,null
,,,
155,The reward function R is defined over B×A  R. It is the amount of document relevance that an agent obtains from the world. Rse is the nDCG score (normalized Discounted Cumulative Gain [20]) that the search engine gains for the documents it returns. Ru is the relevance that the user gains from reading the documents. Our retrieval algorithm jointly optimize both Ru and Rse. (Section 5),null,null
,,,
156,"Table 2 lists the symbols and their meanings in the dualagent SG. The two agents share the decision states and beliefs but differ in actions, messages, and policies. Although they also make different observations, both contribute to the belief updater; the difference is thus absorbed. As a retrieval model, we only pay attention to the search engine policy se : B  A. The following describes their interactions in the stochastic game:",null,null
,,,
157,"1. At search iteration t ,"" 0, both agents begin in the same initial state S0.""",null,null
,,,
158,"2. t increases by one: t ,"" t + 1. The user agent writes a query qt and takes the tth user agent action atu, which is a query change from the previous query.""",null,null
,,,
159,"3. (*) The search engine agent makes observations se from the world and updates its before-message-beliefstate bt·se based on O(St, atu, se).",null,null
,,,
160,"4. The search engine runs its optimization algorithm and picks the best policy se, which maximizes the joint long term rewards for both agents. Following the policy, it takes actions atse. This is where the model performs retrieval.",null,null
,,,
161,"5. Search engine action atse results in a set of documents Dt, which are returned as message tse sent from the search engine agent to the user agent through the world.",null,null
,,,
162,"6. (*) The user agent receives message tse and observes u. If the user would like to stop the search, the process ends. Otherwise, the user updates the aftermessage-belief-state bt·u based on O(St, tse, u).",null,null
,,,
163,"7. Based on the current beliefs, the user agent sends its feedback messages tu to inform the search engine agent. tu are clicks, some of which are SAT clicks. It contains a set of documents Dclicked.",null,null
,,,
164,"8. (*) The search engine agent observes se from the world and updates its after-message-belief-state bt·se based on O(St, tu, se).",null,null
,,,
165,"9. The user agent picks a policy u, which we don't study here, and continues to send out actions atu+1 in the form of query changes. The world moves into a new state st+1. t , t + 1. The process repeats from step 3.",null,null
,,,
166,"Steps 3, 6, and 8 happen after making an observation from the world or from the other agent. They then all involve a belief update. In the remainder of this section, we present the details of states (Section 4.2), actions (Section 4.3), observation functions (Section 4.4), and belief updates (Section 4.5) for win-win search.",null,null
,,,
167,4.2 States,null,null
,,,
168,"We often observe that the same user behavior in session search may be motivated by different reasons. For instance, in TREC 2013 Session 2 (Table 1), a user searches for ""scooter brands"" as q1 and finds that the 6th returned document with title ""Scooter Brands - The Scooter Review - The Scooter Review"" is relevant. The user clicks this document and reads it for 48 seconds, which we identify as a SAT click since it lasts more than 30 seconds [12]. Next, the user adds a new term `reliable' into q1 to get q2 ,""scooter brands reliable. We notice that `reliability' does not appear in any previously retrieved documents D1. It suggests that the user is inspired to add `reliability' from somewhere else, such as from personal background knowledge or the in-""",Y,null
,,,
169,590,null,null
,,,
170,"formation need. In this case, she finds relevant documents from the previously retrieved documents but still decides to explore other aspects about the search target.",null,null
,,,
171,"On the contrary, in TREC 2013 Session 9 q1 (Table 1), another user searches for ""old US coins"" and also finds relevant documents, such as a document about ""... We buy collectible U.S.A. coins for our existing coin collector clients..."". He adds a new term `collecting' to get the next query q2 ""collecting old us coins"". After reducing the query terms and document terms into their stemmed forms, the added term `collecting' does appear in this document as we can see. It suggests that the user selects a term from the retrieval results and hones into the specifics. In this case, he finds relevant documents in the previously retrieved documents and decides to exploit the same sub information need and investigate it more.",Y,null
,,,
172,"We observe that even if both users show the same search behavior, e.g. adding terms, the reasons vary: one is adding the new search term because the original search results are not satisfactory, while the other is because the user wants to look into more specifics. This makes us realize that document relevance and users' desire to explore are two independent dimensions in deciding how to form the next query.",null,null
,,,
173,"Inspired by earlier research on user intent and task types [24, 28] and our own observations, we propose four hidden decision making states for session search. They are identified based on two dimensions: 1) ""relevant dimension"" ­ whether the user thinks the returned documents are relevant, and 2) ""exploration dimension"" ­ whether the user would like to explore another subtopic. The two dimensions greatly simplify the complexity of user modeling in session search. The relatively small number of discrete states enables us to proceed with POMDP and its optimization at low cost.",null,null
,,,
174,"The cross-product of the two dimensions result in four states: i) user finds a relevant document from the returned documents and decides to explore the next sub information need (relevant and exploration, e.g. scooter price  scooter stores), ii) user finds relevant information and decides to stay in the current sub information need to look into more relevant information (relevant and exploitation, e.g. hartford visitors  hartford connecticut tourism), iii) user finds out that the returned documents are not relevant and decides to stay and try out different expressions for the same sub information need (non-relevant and exploitation, e.g. philadelphia nyc travel  philadelphia nyc train), iv) user finds out that documents are not relevant and decides to give up and move on to another sub information need (nonrelevant and exploration, e.g. distance new york boston  maps.bing.com ).",null,null
,,,
175,"Figure 3 shows the decision state diagram for win-win search. The subscriptions stand for {RT ,"" Relevantexploi T ation, RR "","" RelevantexploRation, N RT "","" N onRelevant exploiT ation, N RR "", N onRelevantexploRation}. We insert a dummy starting query q0 before any real query and it always goes to SNRR. The series of search iterations in a session move in the decision states from one to the next. A sequence of states can be time stamped and presented as st ,"" Sm, where t "","" 1, 2, ..., n and m "","" {RT, RR, N RT, N RR}.""",null,null
,,,
176,4.3 Actions,null,null
,,,
177,"There are two types of actions in our framework, domainlevel actions and communications-level actions.",null,null
,,,
178,Figure 3: States.,null,null
,,,
179,4.3.1 Domain-Level Actions,null,null
,,,
180,"The domain-level actions Au and Ase represent the actions directly performed on the world (document collection) by the user agent and by the search engine agent, respectively.",null,null
,,,
181,"The common user actions include writing a query, clicking a document, SAT clicking a document, reading a snippet, reading a document, changing a query, and eye-tracking the documents. In this paper, we only study query changes and clicks as user actions. However, the framework can be easily adopted for other types of user actions.",null,null
,,,
182,"Query changes q [11] consist of added query terms +qt ,"" qt\qt-1, removed query terms -qt "","" qt-1\qt, and theme terms qtheme "","" LongestCommon Subsequence(qt, qt-1). For example, in Session 87 , given q19"",philadelphia nyc travel and q20,""philadelphia nyc train, we obtain the following query changes: qtheme "","" LCS(q19, q20) "","" """"philadelphia"""", -q20 "","" """"travel"""", and +q20 "","" """"train"""". All stopwords and function words are removed.""",null,null
,,,
183,"The search engine domain-level actions Ase include increasing, decreasing, and maintaining the term weights, as well as adjusting parameters in one or more search techniques. We present the details in Sections 5 and 6.",null,null
,,,
184,4.3.2 Communications-Level Actions (Messages),null,null
,,,
185,The second type of actions are communication-level actions (messages) u and se. They are actions that only performed between agents.,null,null
,,,
186,"In our framework, the messages are essentially documents that an agent thinks are relevant. u is the set of documents that the user sends out; we define them as the clicked documents Dclicked. In TREC 2013 Session, 31% search iterations contain SAT clicked documents. 23.9% sessions contain 1 to 4 SAT clicked documents, and a few sessions, for instance Sessions 45, 57 and 72, contain around 10 SAT clicked documents. 88.7% SAT clicked documents appear in the top 10 retrieved results.",Y,null
,,,
187,"Similarly, se is the set of documents that the search engine sends out. They are the top k returned documents (k ranges from 0 to 55 in the TREC setting). They demonstrate what documents the search engine thinks are the most relevant. In TREC 2013, 2.8% (10) search iterations return less than 10 documents, 90.7% (322) return exactly 10, 5.1% (18) return 1020, and 1.4% (5) return 2055 documents.",Y,null
,,,
188,4.4 Observations,null,null
,,,
189,Section 4.1 illustrates the win-win search framework and the interactions between agents. This section shows how we calculate the observation functions.,null,null
,,,
190,"The observation function O(sj, at, t), defined as P (t|sj, at), is the probability of observing t   when agents take action at and land on state sj. The first type of observation",null,null
,,,
191,591,null,null
,,,
192,"is related to relevance. In Section 4.1 Step 8, after the user sends the message u (user clicks) out at Step 7, the search engine updates its after-message-belief-state b·se based on its observation of user clicks. The observation function for `Relevant' states is:",null,null
,,,
193,"O(st,""Rel, u, t"",Rel) ,,de,f, P (t , Rel|st ,"" Rel, u)""",null,null
,,,
194,-1,null,null
,,,
195,It can be written as,null,null
,,,
196,". P (t,""Rel,st"",""Rel,u)""",null,null
,,,
197,"P (st,""Rel,u)""",null,null
,,,
198,"By taking P (st ,",null,null
,,,
199,"Rel, u) as a constant, we can approximate it by P (t ,",null,null
,,,
200,"Rel, st ,"" Rel, u) "", P (st , Rel|t ,"" Rel, u)P (t "",",null,null
,,,
201,"Rel, u). Given that user clicks u are highly correlated",null,null
,,,
202,"to t, we can approximate P (st , Rel|t ,"" Rel, u) by""",null,null
,,,
203,"P (st , Rel|t ,"" Rel). Further, by taking P () as a con-""",null,null
,,,
204,"stant, we have",null,null
,,,
205,"O(st,""Rel, u, t"",Rel)  P (st , Rel|t , Rel)P (t ,"" Rel, u)  P (st "", Rel|t , Rel)P (t , Rel|u)",null,null
,,,
206,"(2) Similarly, we have",null,null
,,,
207,"O(st,""Non-Rel, u, t"",Non-Rel)  P (st , Non-Rel|t , Non-Rel)P (t , Non-Rel|u)",null,null
,,,
208,"(3) as well as O(st,""Non-Rel, u, t"",Rel) and O(st,""Rel, u, t"",Non-Rel).",null,null
,,,
209,"Based on whether a SATClick exists or not, we calculate the probability of the SG landing at the ""Relevant"" states or the ""Non-Relevant"" states (the first dimension of hidden decision states). At search iteration t, if the set of previously returned documents leads to one or more SAT clicks, the current state is likely to be relevant, otherwise non-relevant. That is to say,",null,null
,,,
210,st is likely to be,null,null
,,,
211,Relevant,null,null
,,,
212,if  d  Dt-1 and,null,null
,,,
213,d is SATClicked,null,null
,,,
214,Non-Relevant otherwise.,null,null
,,,
215,"Based on this intuition, we calculate P (t , Rel|u) and P (t , Non-Rel|u) as:",null,null
,,,
216,"P (t , Rel|u) , P ( SATClicks  Dct-lic1ked) (4)",null,null
,,,
217,"P (t , Non-Rel|u) , P ( SATClicks  Dct-lic1ked) (5)",null,null
,,,
218,"The conditional probability of observations P (st , Rel|t ,",null,null
,,,
219,"Rel) and P (st , Non-Rel|t , Non-Rel) can be calculated",null,null
,,,
220,"by maximum likelihood estimation (MLE). For instance,",null,null
,,,
221,P (st,null,null
,,,
222,",",null,null
,,,
223,Rel|,null,null
,,,
224,",",null,null
,,,
225,Rel),null,null
,,,
226,",",null,null
,,,
227,#,null,null
,,,
228,of #,null,null
,,,
229,observed true relevant of observed relevant,null,null
,,,
230,",",null,null
,,,
231,"where ""#",null,null
,,,
232,"of observed true relevant"" is the number of times where the",null,null
,,,
233,previously returned document set Dt-1 contain at least one,null,null
,,,
234,SAT clicks and those SAT clicked documents are indeed rel-,null,null
,,,
235,"evant documents in the ground truth. ""# of observed rele-",null,null
,,,
236,"vant"" is the number of times where Dt-1 contains at least",null,null
,,,
237,one SAT clicks. The ground truth of whether the SG lands,null,null
,,,
238,"on a ""Relevant"" state is generated by documents whose rele-",null,null
,,,
239,vance grades  3 (relevant to highly relevant). The relevance,null,null
,,,
240,are judged by NIST assessors [21].,null,null
,,,
241,The second type of observation is related to exploitation,null,null
,,,
242,vs. exploration. This corresponds to a combined observa-,null,null
,,,
243,"tion at Step 3 and the previous Step 6 (Section 4.1), where",null,null
,,,
244,the SG update the before-message-belief-state b·se for a,null,null
,,,
245,user action au (query change) and a search engine message,null,null
,,,
246,"se,""Dt-1, the top returned documents at the previous it-""",null,null
,,,
247,eration. The search engine agent makes observations about,null,null
,,,
248,exploitation vs. exploration (the second dimension of hidden,null,null
,,,
249,decision states) by:,null,null
,,,
250,"O(st,""Exploitation, au"",""qt, se"",""Dt-1, t"",Exploitation)  P (st , Exploitation|t , Exploitation) ×P (t ,"" Exploitation|qt, Dt-1)""",null,null
,,,
251,"(6) O(st,""Exploration,au"",""qt, se"",""Dt-1, t"",Exploration)  P (st , Exploration|t , Exploration) ×P (t ,"" Exploration|qt, Dt-1)""",null,null
,,,
252,-7,null,null
,,,
253,The search engine can guess the hidden states based on,null,null
,,,
254,the following intuition:,null,null
,,,
255, Exploration  ,null,null
,,,
256,st is likely to be Exploitation  ,null,null
,,,
257,"if (+qt ,  and +qt / Dt-1 ) or (+qt ,  and -qt ,  ) if (+qt ,  and +qt  Dt-1 ) or (+qt ,  and -qt ,  )",null,null
,,,
258,"The idea is that given that Dt-1 is the message from search engine and au ,"" q is the message from user, if added query terms +q appear in Dt-1, it is likely that the user stays at the same sub information need from iteration t - 1 to t for `exploitation'. On the other hand, if the added terms +q do not appear in Dt-1, it is likely that the user moves to the next sub information need from iteration t - 1 to t for `exploration'. In addition, if there is no added terms (+qt is empty) but there are deleted terms ( -qt is not empty), it is likely that the user goes to a broader topic to explore. If +qt and -qt are both empty, it means there is no change to the query, it is likely to fall into exploitation.""",null,null
,,,
259,"Hence, P (t|qt, Dt-1) can be calculated as:",null,null
,,,
260,"P (t ,"" Exploration|qt, Dt-1) "", P (+qt ,   +qt / Dt-1)",null,null
,,,
261,"+P (+qt ,   -qt , )",null,null
,,,
262,"(8) P (t ,"" Exploitation|qt, Dt-1) "", P (+qt ,   +qt  Dt-1)",null,null
,,,
263,"+P (+qt ,   -qt , )",null,null
,,,
264,-9,null,null
,,,
265,where Dt-1 include all clicked documents and all snippets,null,null
,,,
266,that are ranked higher than the last clicked document at,null,null
,,,
267,iteration t - 1. User actions au include the current query,null,null
,,,
268,"changes +qt and -qt. In fact, P (t|qt, Dt-1) needs to",null,null
,,,
269,"be calculated for each specific case. For instance, P (t ,",null,null
,,,
270,"Exploration|a ,"" `delete term', qt, Dt-1) "",",null,null
,,,
271,#,null,null
,,,
272,of #,null,null
,,,
273,observed true explorations due of observed explorations due to,null,null
,,,
274,to deleting terms deleting terms,null,null
,,,
275,.,null,null
,,,
276,Here we only,null,null
,,,
277,"calculate for the actions with ""deleted terms"". ""# of ob-",null,null
,,,
278,"served explorations"" is the number of observed explorations",null,null
,,,
279,suggesting that the user is likely to explore another subtopic,null,null
,,,
280,"based on Eq. 8, while ""# of observed true explorations"" is",null,null
,,,
281,the number of observed explorations judged positive by hu-,null,null
,,,
282,man accessors in a ground truth. The annotations can be found online.2,null,null
,,,
283,"The conditional probability P (st , Exploitation|t ,",null,null
,,,
284,Exploitation) is,null,null
,,,
285,calculated,null,null
,,,
286,as,null,null
,,,
287,# of #,null,null
,,,
288,observed true exploitations of observed exploitations,null,null
,,,
289,",",null,null
,,,
290,where,null,null
,,,
291,# of observed exploitations is the number of observed ex-,null,null
,,,
292,ploitations suggesting that the user is likely to exploit the,null,null
,,,
293,"same subtopic (based on Eq. 9), and ""# of observed true ex-",null,null
,,,
294,"ploitations"" is the number of observed exploitations that are",null,null
,,,
295,"judged positive in the ground truth. P (st , Exploration|t ,",null,null
,,,
296,Exploration) is calculated in a similar way.,null,null
,,,
297,4.5 Belief Updates,null,null
,,,
298,"At every search iteration the belief state b is updated twice; once at Step 3, another at Step 8. It reflects the interaction and cooperative game between the two agents. 2The manual annotations for ""exploration"" transitions can be found at www.cs.georgetown.edu/~huiyang/win-win.",null,null
,,,
299,592,null,null
,,,
300,"A belief bt(si) is defined as P (si|at, bt). The initial belief states can be calculated as: b0(si , Sz) , P (si , Sx)P (si ,"" Sy), where x  {R "","" Rel, N R "","" N on-Rel}, y  {R "","" exploRation, T "","" exploiT ation}, z is the cross-product of""",null,null
,,,
301,Table 3: Dataset statistics.,null,null
,,,
302,TREC 2012 TREC 2013,Y,null
,,,
303,#Sessions,null,null
,,,
304,98,null,null
,,,
305,87,null,null
,,,
306,#Search topics,null,null
,,,
307,48,null,null
,,,
308,49,null,null
,,,
309,"x and y and z  {RR, RT, N RR, N RT }. In addition, 0 ",null,null
,,,
310,#Queries,null,null
,,,
311,297,null,null
,,,
312,442,null,null
,,,
313,"b(si)  1 and si b(si) , 1. The belief update function is bt+1(sj) ,"" P (sj|t, at, bt)""",null,null
,,,
314,Avg. session length,null,null
,,,
315,3.03,null,null
,,,
316,Max session length,null,null
,,,
317,11,null,null
,,,
318,Avg. #sessions per topic,null,null
,,,
319,2.04,null,null
,,,
320,5.08 21 1.78,null,null
,,,
321,by taking into account new observations t. It is updated,null,null
,,,
322,from iteration t to iteration t + 1:,null,null
,,,
323,The formula matches well with common search scenarios,null,null
,,,
324,"bt+1(sj ) ,d,e,f,"" P (sj |t, at, bt)""",null,null
,,,
325,"O(sj , at, t) ,",null,null
,,,
326,"siS T (si, at, sj )bt(si)",null,null
,,,
327,"P (t|at, bt)",null,null
,,,
328,where the user makes decisions about their next actions,null,null
,,,
329,based on the most relevant document(s) they examined in,null,null
,,,
330,-10,null,null
,,,
331,the previous run of retrieval. Such a document we call it,null,null
,,,
332,maximum rewarding document(s). We use document with,null,null
,,,
333,"where si and sj are two states, i, j  {RR, RT, N RR, N RT }. t indices the search iterations, and O(sj, at, t) ,"" P (t|sj, at) is calculated based on Section 4.4. P (t|at, bt) is the normalization factor to keep siS b(si) "","" 1. For notation simplicity, we will only use a to represent actions from now""",null,null
,,,
334,the largest P (qt-1|dt-1) as the maximum rewarding docu-,null,null
,,,
335,"ment. P (qt-1|dt-1) is calculated as 1- tqt-1 {1 - P (t|dt-1)},",null,null
,,,
336,where,null,null
,,,
337,P (t|dt-1),null,null
,,,
338,",",null,null
,,,
339,", #(t,dt-1 )",null,null
,,,
340,|dt-1 |,null,null
,,,
341,"#(t, dt-1) is",null,null
,,,
342,the,null,null
,,,
343,number,null,null
,,,
344,of,null,null
,,,
345,"occurrences of term t in document dt-1, and |dt-1| is the",null,null
,,,
346,document length.,null,null
,,,
347,"on. However, it is worthy noting that actions can be both",null,null
,,,
348,By optimizing both long term rewards for the user and,null,null
,,,
349,domain-level actions a and messages .,null,null
,,,
350,"for the search engine, we learn the best policy  and use it",null,null
,,,
351,"Transition probability T (si, at, sj) is defined as P (sj|si, at, bt).",null,null
,,,
352,It,null,null
,,,
353,is,null,null
,,,
354,can,null,null
,,,
355,be,null,null
,,,
356,calculated,null,null
,,,
357,as,null,null
,,,
358,"T (si, at, sj )",null,null
,,,
359,",",null,null
,,,
360,", #T ransition(si,at,sj )",null,null
,,,
361,"#T ransition(si,at,s)",null,null
,,,
362,"where Transition (si, at, sj) is the sum of all transitions that",null,null
,,,
363,"starts at state si, takes action at, and lands at state sj.",null,null
,,,
364,to predict the next action for the search engine. The joint optimization for the dual-agent SG can be represented as:,null,null
,,,
365,ase,null,null
,,,
366,",",null,null
,,,
367,arg max,null,null
,,,
368,a,null,null
,,,
369,"Qse(b, a) + Qu(b, au)",null,null
,,,
370,-14,null,null
,,,
371,"T ransition (si, at, s) is the sum of all transitions that starts at state si and lands at any state by action at.",null,null
,,,
372,"where ase  Ase at t ,"" n and n is the number of search iterations in a session, i.e., the session length.""",null,null
,,,
373,"Finally, equals to P",null,null
,,,
374,"taking (t|sj ,",null,null
,,,
375,"O(sj , at, t) ,"" P (t|sj , at), which at, bt) when we consider beliefs, and T""",null,null
,,,
376,"also (si, at,",null,null
,,,
377,sj ),null,null
,,,
378,"In win-win search, Ase can include many search engine actions. One type of actions is adjusting a query's term weight.",null,null
,,,
379,","" P (sj|si, at, bt), the updated belief can be written as:""",null,null
,,,
380,Assuming the query is reformulated from the previous query,null,null
,,,
381,bt+1(sj ),null,null
,,,
382,",",null,null
,,,
383,"P (t|sj , at, bt)",null,null
,,,
384,"siS P (sj |si, at, bt)bt(si) P (t|at, bt)",null,null
,,,
385,"P (t|sj , at, bt) ,",null,null
,,,
386,"siS P (sj |si, at, bt)bt(si)",null,null
,,,
387,"skS P (t|sk, at) siS P (sk|si, at)bt(si)",null,null
,,,
388,-11,null,null
,,,
389,"where bt(si) is P (si|at, bt), whose initial value is b0(si).",null,null
,,,
390,"by adding +q or deleting -q. That is to say, Ase ,"" {increasing, decreasing, or keeping term weights}. The term weights are increased or decreased by multiplying a factor. We also use a range of search techniques/algorithms as action options for the search engine agent. They are reported in Section 6. Based on Eq. 14, the win-win search framework picks the optimal search engine action.""",null,null
,,,
391,5. JOINT OPTIMIZATION AND RETRIEVAL 6. EVALUATION,null,null
,,,
392,"After every search iteration, we decide the actions for the search engine agent. We employ Q-learning [18] to find out the optimal action. For all a  Ase, we write the search engine's Q-function, which represents the search engine agent's long term reward, as:",null,null
,,,
393,"We evaluate the proposed framework on TREC 2012 and TREC 2013 Session Tracks [20, 21]. The session logs are collected from users through a search system by the track organizers. The topics, i.e., information need (Table 1), are provided to users. The session logs record all URLs dis-",Y,null
,,,
394,"played to the user, snippets, clicks, and dwell time. Table 3",null,null
,,,
395,"Qse(b, a) ,"" (b, a)+""",null,null
,,,
396,P,null,null
,,,
397,"(|b,",null,null
,,,
398,"au,",null,null
,,,
399,"se)P (|b,",null,null
,,,
400,u),null,null
,,,
401,max,null,null
,,,
402,a,null,null
,,,
403,Qse(b,null,null
,,,
404,",",null,null
,,,
405,a),null,null
,,,
406,shows the dataset statistics. The task is to retrieve a ranked,null,null
,,,
407,-12,null,null
,,,
408,"list of 2,000 documents for the last query in a session. Doc-",null,null
,,,
409,"where the reward for a belief state b is (b, a) ,"" sS b(s)R(s, a). P (|b, au, se) corresponds to Eq. 8 and Eq. 9 and P (|b, u)""",null,null
,,,
410,corresponds to Eq. 4 and Eq. 5. b is the belief state up-,null,null
,,,
411,"ument relevance is judged based on the whole-session relevance. We use the official TREC evaluation metrics in our experiments. They include nDCG@10, nERR@10, nDCG,",Y,null
,,,
412,dated by Eq. 11.,null,null
,,,
413,and MAP [21]. The ground truth relevant documents are,null,null
,,,
414,"In win-win search, we take into account both the search",null,null
,,,
415,provided by TREC.,Y,null
,,,
416,"engine reward and the user reward. As in [11], we have Qu calculated as the long term reward for the user agent:",null,null
,,,
417,"The corpora used in our experiments are ClueWeb09 CatB (50 million English web pages crawled in 2009, used in TREC",Y,null
,,,
418,"2012), and ClueWeb12 CatB (50 million English web pages",Y,null
,,,
419,"Qu(b, au) ,"" R(s, au) +  au T (st|st-1, Dt-1) maxst-1 Qu(st-1, au) crawled in 2012, used in TREC 2013). Documents with the""",Y,null
,,,
420,","" P (qt|d) +  a P (qt|qt-1, Dt-1, a) maxDt-1 P (qt-1|Dt-1)""",null,null
,,,
421,Waterloo spam scores [5] less than 70 are filtered out. All,null,null
,,,
422,-13,null,null
,,,
423,duplicated documents are removed.,null,null
,,,
424,which recursively calculates the reward starting from q1 and continues with the policy until qt. P (qt|d) is the current reward that the user gains through reading the documents.,null,null
,,,
425,"We compare our system with the following systems: Lemur [23] (language modeling + Dirichlet smoothing), PRF (Pseudo Relevance Feedback in Lemur assuming the top 20 docu-",null,null
,,,
426,maxDt-1 P (qt-1|Dt-1) is the maximum of the past rewards.,null,null
,,,
427,"ments are relevant), Rocchio (relevance feedback that as-",null,null
,,,
428,593,null,null
,,,
429,Table 4: Search accuracy on TREC 2012 Session (,Y,null
,,,
430,indicates a statistical significant improvement over,null,null
,,,
431,"Rocchio at p < 0.05 (t-test, one-sided));  indicates a",null,null
,,,
432,statistical significant improvement over QCM+DUP,null,null
,,,
433,"at p < 0.05 (t-test, one-sided)).",null,null
,,,
434,Approach,null,null
,,,
435,nDCG@10 nDCG MAP nERR@10,null,null
,,,
436,Lemur,null,null
,,,
437,0.2474 0.2627 0.1274 0.2857,null,null
,,,
438,TREC median 0.2608 0.2468 0.1440 0.2626,Y,null
,,,
439,TREC best,Y,null
,,,
440,0.3221 0.2865 0.1559 0.3595,null,null
,,,
441,PRF,null,null
,,,
442,0.2074 0.2335 0.1065 0.2415,null,null
,,,
443,Rocchio,null,null
,,,
444,0.2446 0.2714 0.1281 0.2950,null,null
,,,
445,Rocchio-CLK,null,null
,,,
446,0.2916 0.2866 0.1449,null,null
,,,
447,0.3366,null,null
,,,
448,Rocchio-SAT,null,null
,,,
449,0.2889 0.2836 0.1467 0.3254,null,null
,,,
450,QCM+DUP,null,null
,,,
451,0.2742 0.2560 0.1537 0.3221,null,null
,,,
452,QCM SAT,null,null
,,,
453,0.3350 0.3054 0.1534 0.1534,null,null
,,,
454,Win-Win,null,null
,,,
455,0.2941 0.2691 0.1346 0.3403,null,null
,,,
456,Table 5: Search accuracy on TREC 2013 Session (,Y,null
,,,
457,indicates a statistical significant improvement over,null,null
,,,
458,"Rocchio at p < 0.01 (t-test, one-sided));  indicates a",null,null
,,,
459,statistical significant improvement over QCM+DUP,null,null
,,,
460,"at p < 0.01 (t-test, one-sided)).",null,null
,,,
461,Approach,null,null
,,,
462,nDCG@10 nDCG MAP nERR@10,null,null
,,,
463,Lemur,null,null
,,,
464,0.1147,null,null
,,,
465,0.1758 0.0926,null,null
,,,
466,0.1314,null,null
,,,
467,TREC median 0.1531,Y,null
,,,
468,­,null,null
,,,
469,­,null,null
,,,
470,­,null,null
,,,
471,TREC best,Y,null
,,,
472,0.1952,null,null
,,,
473,­,null,null
,,,
474,­,null,null
,,,
475,­,null,null
,,,
476,PRF,null,null
,,,
477,0.1061,null,null
,,,
478,0.1701 0.0787,null,null
,,,
479,0.1245,null,null
,,,
480,Rocchio,null,null
,,,
481,0.132,null,null
,,,
482,0.1924 0.1060,null,null
,,,
483,0.1549,null,null
,,,
484,Rocchio-CLK,null,null
,,,
485,0.1315,null,null
,,,
486,0.1929 0.1060,null,null
,,,
487,0.1546,null,null
,,,
488,Rocchio-SAT,null,null
,,,
489,0.1147,null,null
,,,
490,0.1758 0.0926,null,null
,,,
491,0.1314,null,null
,,,
492,QCM+DUP,null,null
,,,
493,0.1316,null,null
,,,
494,0.1929 0.1060,null,null
,,,
495,0.1547,null,null
,,,
496,QCM SAT,null,null
,,,
497,0.1186,null,null
,,,
498,0.1754 0.0939,null,null
,,,
499,0.1425,null,null
,,,
500,Win-Win,null,null
,,,
501,0.2026 0.2609 0.1290 0.2328,null,null
,,,
502,"sumes the top 10 previous retrieved documents are relevant), Rocchio-CLK (implicit relevance feedback that assumes only previous clicked documents are relevant), Rocchio-SAT (implicit relevance feedback that assumes only previous SATclicked documents are relevant), QCM+DUP (the QCM approach proposed by [11]), and QCM SAT (a variation of QCM by [33]). We choose Rocchio (a state-of-the-art interactive search algorithm) and QCM+DUP (a state-of-the-art session search algorithm) as two baseline systems and all other systems are compared against them. TREC median and TREC best scores are also included for reference. Note that TREC best are an aggregation from the best scores of each individual submitted TREC runs; it is not a single search system.",Y,null
,,,
503,6.1 Search Accuracy,null,null
,,,
504,"Our run, win-win, implements six retrieval technologies. They are: (1) increasing weights of the added terms (+q) by a factor of x,""{1.05, 1.10, 1.15, 1.20, 1.25, 1.5, 1.75 or 2}; (2) decreasing weights of the added terms by a factor of y "",""{ 0.5, 0.57, 0.67, 0.8, 0.83, 0.87, 0.9 or 0.95}; (3) the term weighting scheme proposed in [11] with parameters , , ,  set as 2.2, 1.8, 0.4, 0.92; (4) a PRF (Pseudo Relevance Feedback) algorithm which assumes the top p retrieved documents are relevant while p ranges from 1 to 20; (5) an adhoc variation of win-win, which directly uses the last query in a session to perform retrieval; and (6) a brute-force variation of win-win, which combines all queries in a session, extracts all unique query terms from them, and weights them equally. Win-win examine 21 search engine action options in total to""",null,null
,,,
505,"find out the optimal action that maximizes the joint long term reward Qse(b, a) + Qu(b, au) for both agents.",null,null
,,,
506,"Table 4 shows the search accuracy of all systems under comparison for TREC 2012 Session Track. We can see that win-win search is better than most systems except QCM SAT. It statistically significantly outperforms Rocchio by 20%, Lemur by 18.9%, and PRF by 41.8% in nDCG@10 (p-value<.05, one-side t-test). It also outperforms RocchioCLK, Rocchio-SAT and QCM+DUP, but the results are not statistically significant. The trends for other evaluation metrics are similar to nDCG@10.",Y,null
,,,
507,"Table 5 shows the search accuracy of all systems for TREC 2013 Session Track. Since we only indexed ClueWeb12 CatB, after spam reduction, many relevant CatA documents are not included in the CatB collection. To evaluate the systems fairly, we created a filtered ground truth which only consists of relevant documents in CatB. The results are shown in Table 5. We can see that win-win is the best run among all systems. It shows statistically significant gain (p-value<.01, one-sided t-test) over all other systems across all evaluation metrics. Particularly, the proposed approach achieves a significant 54% improvement of nDCG@10 comparing to QCM+DUP. The experimental results support that our approach is highly effective.",Y,null
,,,
508,6.2 Immediate Search Accuracy,null,null
,,,
509,"TREC Session tasks request for retrieval results for the last query in a session. Theoretically, however, win-win search can optimize at every search iteration throughout a session. We hence compare our approach (the Win-Win run) with the top returned documents provided by TREC (the Original run) in terms of immediate search accuracy. We define immediate search accuracy at i as an evaluation score that measures search accuracy at search iteration i. The evaluation scores used are nDCG@10 and nERR@10.",Y,null
,,,
510,"We report the averaged immediate search accuracy for all sessions. It is worthy noting that session lengths vary. To average across sessions with different lengths, we make all sessions equals to the maximum session length in a dataset. TREC 2012 and 2013 Session have different maximum session lengths; they are 11 and 21, respectively. When a session is shorter than the maximum session length, we use the retrieval results from its own last iteration as the retrieval results for iterations beyond its own last iteration. In addition, since TREC did not provide any retrieval results for the last query, the Original runs has no value at the last iteration.",Y,null
,,,
511,"Figures 4 and 5 plot the immediate search accuracy for TREC 2012 & 2013 Session Tracks averaged over all sessions. We observe that win-win search's immediate search accuracy is statistically significantly better than the Original run at every iteration. In Figure 4, win-win outperforms Original since iteration 2 in nDCG@10 and outperforms it since iteration 3 in nERR@10. At the last iteration, winwin outperforms Original by a statistically significant 27.1% in nDCG@10 (p-value<.05, one-sided t-test). We observe similar trends in Figure 5. Another interesting finding is that win-win search's immediate search accuracy increases while the number of search iterations increases. In Figure 4, the nDCG@10 starts at 0.2145 at the first iteration and increases dramatically 37.1% to 0.2941 at the last iteration. It suggests that by involving more search iterations, i.e., learning from more interactions between the user and the search",Y,null
,,,
512,594,null,null
,,,
513,Figure 4: TREC 2012 Im- Figure 5: TREC 2013 Immediate Search Accuracy. mediate Search Accuracy.,Y,null
,,,
514,Figure 7: Factual and Specific sessions.,null,null
,,,
515,Figure 8: Factual and Amorphous sessions.,null,null
,,,
516,"Figure 6: Long sessions (length >,"" 4). Transition probabilities are listed with actions: Add (A), Remove (R), and Keep (K).""",null,null
,,,
517,"engine, win-win is able to monotonically improve its search accuracy.",null,null
,,,
518,6.3 State Transitions,null,null
,,,
519,This experiment investigates how legitimate the proposed states are in presenting the hidden mental states of users.,null,null
,,,
520,"First, we use examples to demonstrate the state transitions in sessions. Session 87 (Table 1) is a long session with 21 queries. The chain of decision states identified for this session based on techniques presented in Sections 4.2 and 4.4 is: SNRR(q1,best us destination)  SRT (q2,distance new york boston)  SNRT  SNRR  SNRR  SRR  SRR  SNRR  SRT  SRT  SRR(q11,boston tourism)  SNRR(q12,nyc tourism)  SNRR(q13,philadelphia nyc distance)  SNRR  SRT  SNRR  SRR  SNRT  SNRT (q19,philadelphia nyc travel)  SNRT (q20,philadelphia nyc train)  SNRT (q21 ,""philadelphia nyc bus). Our states correctly suggests that the user is in the exploration states (RR, NRR, NRR) from q11 to q13, while he keeps changing queries to explore from city to city (boston, new york city, and philadelphia). The user eventually finds the cities, philadelphia and nyc, that fulfill the information need ­ """"best US destinations within a 150-mile radius"""". During the last 3 queries, the user exploits the current subtopic (philadelphia and nyc) to find out more specifics on transportations (travel, train, bus) about them. Our system correctly recognizes the last three states as exploitation states (NRT, NRT, NRT). This example suggests that the proposed states are able to reflect the real user decision states quite accurately.""",null,null
,,,
521,"Second, we examine state transition patterns in long sessions since they contain enough transitions for us to study. Figure 6 plots state probabilities, state transition probabilities, and that under different user actions for long sessions (sessions with 4 or more queries). The data are combined from both TREC 2012 & 2013 Session Tracks. We notice that NRR (non-relevant and exploration) is the most com-",null,null
,,,
522,"mon state (42.4%). This reflects that a user spend may a long time to explore while receiving non-relevant documents. On the contrary, the RR state (relevant and exploration) is the least common state (11.3%).Moreover, we see that state transitions are not uniformly distributed. For instance, the transition from NRT to both relevant states (RT and RR) are very rare (in total 5.65%). In addition, we notice that actions are related to state transition probabilities. There are 90.8% transitions generated by adding terms and among all the transitions with removing terms, 84.8% of them lead to exploitation states (RT or NRT).",null,null
,,,
523,"Third, we find that state probability distribution and state probability transitions differ among different session types. We plot the state probabilities and transition probabilities in Figures 7 to 10 for four different TREC session types, which were created along two aspects: search target (factual or intellectual ) and goal quality (specific or amorphous). Suggested by [24], the difficulty levels of the session types usually are FactualSpecific < IntellectualSpecific < FactualAmorphous < IntellectualAmorphous. An interesting finding is that as the session difficult level increases, the transition probability from state NRT (non-relevant and exploitation) to state RT (relevant and exploitation) becomes lower: FactualSpecific (0.25), IntellectualSpecific (0.2), FactualAmorphous (0.12), IntellectualAmorphous(0.1). It suggests that the harder the task is, the greater the necessity to explore rather than to exploit, when the user is not satisfied with the current retrieval results. In addition, we observe that Intellectual sessions (Figures 9 and 10) have a larger probability, 0.1548, to be in the RR (relevant and exploration) state than the other session types (on average 0.1018).",Y,null
,,,
524,7. CONCLUSION,null,null
,,,
525,"This paper presents a novel session search framework, winwin search, that uses a dual-agent stochastic game to model the interactions between user and search engine. With a careful design of states, actions, and observations, the new framework is able to perform efficient optimization over a finite discrete set of options. The experiments on TREC Ses-",Y,null
,,,
526,595,null,null
,,,
527,Figure 9: Intellectual and Specific sessions.,null,null
,,,
528,Figure 10: Intellectual and Amorphous sessions.,null,null
,,,
529,sion 2012 and 2013 datasets show that the proposed framework is highly effective for session search.,null,null
,,,
530,"Session search is a complex IR task. The complexity comes from the involvement of many more factors other than just terms, queries and documents in most existing retrieval algorithms. The factors include query reformulations, clicks, time spent to examine the documents, personalization, query intent, feedback, etc. Most existing work on sessions and task-based search focuses on diving into one aspect. Through significantly simplifying the factors, we realize the integration of all the factors in a unified framework. For example, we simplify users' decision states into only four states, and discretize user actions and search engine actions into a finite number of options. Such simplification is necessary in creating practical search systems.",null,null
,,,
531,"This paper views the search engine as an autonomous agent, that works together with user, another autonomous agent, to collaborate on a shared task ­ fulfilling the information needs. This view assumes that the search engine is more like a ""decision engine"". Session search can be imagined as two agents exploring in a world full of information, searching for the goal in a trial-and-error manner. Here we assume a cooperative game between the two agents. However, as we mentioned in the introduction, the search engine agent can of course choose a different goal. It will be very interesting to see how to still satisfy the user to achieve winwin. We hope our work calls for future adventures in the fields of POMDP in IR and game theory in IR.",null,null
,,,
532,8. ACKNOWLEDGMENT,null,null
,,,
533,"This research was supported by NSF grant CNS-1223825. Any opinions, findings, conclusions, or recommendations expressed in this paper are of the authors, and do not necessarily reflect those of the sponsor.",null,null
,,,
534,9. REFERENCES,null,null
,,,
535,"[1] L. E. Baum and T. Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6), 1966.",null,null
,,,
536,"[2] G. Cao, J.-Y. Nie, J. Gao, and S. Robertson. Selecting good expansion terms for pseudo-relevance feedback. In SIGIR '08.",null,null
,,,
537,"[3] B. Carterette, E. Kanoulas, and E. Yilmaz. Simulating simple user behavior for system effectiveness evaluation. In CIKM '11.",null,null
,,,
538,"[4] M.-A. Cartright, R. W. White, and E. Horvitz. Intentions and attention in exploratory health search. In SIGIR '11.",null,null
,,,
539,"[5] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Inf. Retr., 14(5), Oct. 2011.",null,null
,,,
540,"[6] A. Diriye, R. White, G. Buscher, and S. Dumais. Leaving so soon?: Understanding and predicting web search abandonment rationales. In CIKM '12.",null,null
,,,
541,"[7] C. Eickhoff, K. Collins-Thompson, P. N. Bennett, and S. Dumais. Personalizing atypical web search sessions. In WSDM '13.",null,null
,,,
542,[8] H. Feild and J. Allan. Task-aware query recommendation. In SIGIR '13.,null,null
,,,
543,"[9] S. Fox, K. Karnawat, M. Mydland, S. Dumais, and T. White. Evaluating implicit measures to improve web search. ACM Trans. Inf. Syst., 23(2), Apr. 2005.",null,null
,,,
544,"[10] D. Guan, H. Yang, and N. Goharian. Effective structured query formulation for session search. In TREC '12.",null,null
,,,
545,"[11] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13.",null,null
,,,
546,[12] Q. Guo and E. Agichtein. Ready to buy or just browsing?: detecting web searcher goals from interaction data. In SIGIR '10.,null,null
,,,
547,[13] J. Jiang and D. He. Different effects of click-through and past queries on whole-session search performance. In TREC '13.,null,null
,,,
548,"[14] J. Jiang, D. He, and S. Han. Pitt at trec 2012 session track. In TREC '12.",Y,null
,,,
549,"[15] X. Jin, M. Sloan, and J. Wang. Interactive exploratory search for multi page search results. In WWW '13.",null,null
,,,
550,[16] T. Joachims. A probabilistic analysis of the rocchio algorithm with tfidf for text categorization. 1997.,null,null
,,,
551,[17] R. Jones and K. L. Klinkner. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In CIKM '08.,null,null
,,,
552,"[18] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1):99­134, 1998.",null,null
,,,
553,"[19] L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: a survey. J. Artif. Int. Res., 4(1), May 1996.",null,null
,,,
554,"[20] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2012 session track. In TREC'12.",Y,null
,,,
555,"[21] E. Kanoulas, B. Carterette, M. Hall, P. Clough, and M. Sanderson. Overview of the trec 2013 session track. In TREC'13.",Y,null
,,,
556,"[22] A. Kotov, P. N. Bennett, R. W. White, S. T. Dumais, and J. Teevan. Modeling and analysis of cross-session search tasks. In SIGIR '11.",null,null
,,,
557,[23] Lemur Search Engine. http://www.lemurproject.org/. [24] J. Liu and N. J. Belkin. Personalizing information retrieval for,null,null
,,,
558,multi-session tasks: The roles of task stage and task type. In SIGIR '10.,null,null
,,,
559,"[25] K. Raman, P. N. Bennett, and K. Collins-Thompson. Toward whole-session relevance: Exploring intrinsic diversity in web search. In SIGIR '13.",null,null
,,,
560,"[26] X. Shen, B. Tan, and C. Zhai. Implicit user modeling for personalized search. In CIKM '05.",null,null
,,,
561,[27] Y. Song and L.-w. He. Optimal rare query suggestion with implicit user feedback. In WWW '10.,null,null
,,,
562,"[28] A. R. Taylor, C. Cool, N. J. Belkin, and W. J. Amadio. Relationships between categories of relevance criteria and stage in task completion. Information Processing & Management, 43(4), 2007.",null,null
,,,
563,"[29] J. Teevan, S. T. Dumais, and D. J. Liebling. To personalize or not to personalize: Modeling queries with variation in user intent. In SIGIR '08.",null,null
,,,
564,"[30] H. Wang, Y. Song, M.-W. Chang, X. He, R. W. White, and W. Chu. Learning to extract cross-session search tasks. In WWW '13.",null,null
,,,
565,"[31] R. W. White, I. Ruthven, J. M. Jose, and C. J. V. Rijsbergen. Evaluating implicit feedback models using searcher simulations. ACM Trans. Inf. Syst., 23(3), July 2005.",null,null
,,,
566,[32] S. Yuan and J. Wang. Sequential selection of correlated ads by pomdps. In CIKM '12.,null,null
,,,
567,"[33] S. Zhang, D. Guan, and H. Yang. Query change as relevance feedback in session search. In SIGIR '13.",null,null
,,,
568,596,null,null
,,,
569,,null,null

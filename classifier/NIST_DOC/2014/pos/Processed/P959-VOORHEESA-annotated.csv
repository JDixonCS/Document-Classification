,sentence,label,data
,,,
0,"On Run Diversity in ""Evaluation as a Service""",null,null
,,,
1,"Ellen M. Voorhees,1 Jimmy Lin,2 and Miles Efron3",null,null
,,,
2,"1 National Institute of Standards and Technology 2 The iSchool, University of Maryland, College Park 3 Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign",null,null
,,,
3,"ellen.voorhees@nist.gov, jimmylin@umd.edu, mefron@illinois.edu",null,null
,,,
4,ABSTRACT,null,null
,,,
5,"Evaluation as a service (EaaS) is a new methodology that enables community-wide evaluations and the construction of test collections on documents that cannot be distributed. The basic idea is that evaluation organizers provide a service API through which the evaluation task can be completed. However, this concept violates some of the premises of traditional pool-based collection building and thus calls into question the quality of the resulting test collection. In particular, the service API might restrict the diversity of runs that contribute to the pool: this might hamper innovation by researchers and lead to incomplete judgment pools that affect the reusability of the collection. This paper shows that the distinctiveness of the retrieval runs used to construct the first test collection built using EaaS, the TREC 2013 Microblog collection, is not substantially different from that of the TREC-8 ad hoc collection, a high-quality collection built using traditional pooling. Further analysis using the `leave out uniques' test suggests that pools from the Microblog 2013 collection are less complete than those from TREC-8, although both collections benefit from the presence of distinctive and effective manual runs. Although we cannot yet generalize to all EaaS implementations, our analyses reveal no obvious flaws in the test collection built using the methodology in the TREC 2013 Microblog track.",null,null
,,,
6,Categories and Subject Descriptors: H.3.4 [Information Storage and Retrieval]: Systems and Software--Performance Evaluation,null,null
,,,
7,Keywords: test collections; reusability; meta-evaluation,null,null
,,,
8,1. INTRODUCTION,null,null
,,,
9,"Large-scale community-wide evaluations such as TREC operationalize the Cranfield paradigm by building retrieval test collections that support IR research [2]. A test collection consists of a corpus of documents, a set of information needs called topics, and relevance judgments that specify which documents are relevant for which topics. A funda-",null,null
,,,
10,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609484.",null,null
,,,
11,"mental assumption in the paradigm is that researchers can acquire a test collection's document corpus. But what if this is not possible? This was the challenge in the TREC Microblog track evaluation, where the corpus is comprised of tweets posted on Twitter. Since the company's terms of service forbid redistribution of tweets, a test collection built on tweets cannot be distributed in a traditional manner.",null,null
,,,
12,"The solution developed by the track organizers, termed ""evaluation as a service"" (EaaS), was to provide an API through which the tweet collection can be accessed for completing the evaluation task [4, 3]. This API, consisting of a basic keyword search interface, was the only method through which track participants could access the collection. The operational deployment of the EaaS model was successful in attracting participation at TREC 2013, which provides encouraging evidence that EaaS could be generalized to other evaluation scenarios involving sensitive data (e.g., medical or desktop search). Before expanding the use of the approach, however, we need to know whether EaaS has any substantive impact on the quality of the collections produced.",null,null
,,,
13,"One concern about the EaaS model is a potential lack of diversity in the retrieval runs used to build the test collection. The criticism is this: if everyone must use the API, won't they all end up doing basically the same thing? Diversity is important from at least two perspectives. Previous work has tied the reusability of a collection built using pooling to the diversity of the retrieval runs that form the pools [1]. Diversity in participant submissions also serves as a proxy indicator that researchers are trying different (novel) retrieval techniques. The contribution of this paper is an analysis of the diversity of the pools in the TREC 2013 Microblog track (using EaaS) compared to previous evaluations--including the first two years of the Microblog track in TRECs 2011 and 2012--where participants had access to the entire corpus. We first propose a novel metric, called Run Average Overlap (RAO), that measures the overall distinctiveness of one retrieval run with respect to a given set of runs. In a second analysis, we apply another test of reusability, the leave out uniques (LOU) test. Analyses show no obvious differences in the characteristics of the pools, suggesting that the EaaS model can be used to build reusable collections.",Y,null
,,,
14,2. BUILDING POOLED COLLECTIONS,null,null
,,,
15,Pooling [8] is a method for building test collections when the target document corpus is too large to make exhaustive relevance judgments for every document for every topic. A pooled collection is built from a set of retrieval runs sub-,null,null
,,,
16,959,null,null
,,,
17,Table 1: Test collection statistics.,null,null
,,,
18,Name Docs Topics Depth Runs Groups,null,null
,,,
19,TREC-8 528 K,null,null
,,,
20,50 100 71,null,null
,,,
21,40,null,null
,,,
22,MB2012 16 M,null,null
,,,
23,60 100 121,null,null
,,,
24,33,null,null
,,,
25,MB2013 243 M,null,null
,,,
26,60,null,null
,,,
27,90 71,null,null
,,,
28,20,null,null
,,,
29,"mitted by participants of the process, typically as part of a community-wide evaluation such as TREC.1 A run is the output of a retrieval system for each topic in the collection; documents are assumed to be ordered by decreasing likelihood of relevance to the topic. The pool for a topic is comprised of the union of the top K (called the pool depth) documents retrieved for that topic by each run. Only documents in the pool are judged for relevance by a human; all other documents are assumed to be not relevant.",Y,null
,,,
30,"Test collections are research tools that are most useful when they are reusable, that is, when they fairly evaluate retrieval systems that did not contribute to their construction [1]. Thus, a reusable test collection can be used to evaluate systems even after the original evaluation that created the collection has concluded. For a pooled collection to be reusable, the pool must contain enough of the relevant documents in the collection to be an unbiased sample of the relevant documents. In practice, the completeness of a pool is a function of (at least) the pool depth, the effectiveness of the runs that comprise it, the diversity of the runs, and the true number of relevant documents [10, 9].",null,null
,,,
31,"Many of the test collections built in TREC, including the ad hoc collections and the Microblog collections, were built through pooling. The TREC ad hoc collections, especially the TREC-8 collection, have been subject to a number of analyses and have been used in hundreds of retrieval experiments whose outcomes have proven to generalize. Thus, these collections are generally accepted as high-quality test collections [5]. Our basic premise is that the TREC 2013 Microblog collection is reusable if it displays characteristics similar to the TREC ad hoc collections. The particular characteristics of interest are the distinctiveness of the runs over which the pools are formed and the number of relevant documents found by a single participant.",Y,null
,,,
32,"More details about the three collections used in this paper are given in Table 1. (Our experiments also considered the ad hoc test collections from TREC-6 and TREC-7, but they yielded similar results, and so for brevity we only present results from TREC-8 here.) The first column of the table gives the name of the collection as used in the remainder of this paper. The corpus for TREC-8 consists of approximately half a million (mostly) newswire articles. The pools were predominantly constructed from the 71 ad hoc runs that were judged (from a total of 129 submitted runs), but runs from some other tracks contributed small numbers of documents to the pools as well.2 The other two collections were built as part of the TREC Microblog tracks in 2012 and 2013, whose document corpora consist of tweets posted to Twitter. The document corpus used for the MB2012 track (called Tweets11) contains approximately 16 million tweets, which were distributed by publishing a list of tweet ids that participants then fetched [7, 6]. The EaaS model enabled the MB2013 corpus to be much larger, about 243 million",Y,null
,,,
33,1http://trec.nist.gov 2Values reported in Table 1 include only the ad hoc runs.,Y,null
,,,
34,"tweets [4]. Relevance judgments for the MB collections were made on a 3-point scale (""not relevant"", ""relevant"", ""highly relevant""), but in this work we ignore the different degrees of relevance and use both higher grades as ""relevant"".",null,null
,,,
35,"The final column of Table 1 gives the number of participants that contributed runs. TREC generally allows participating groups to submit more than a single run to a given task. In practice, runs submitted by the same participant tend to be much more similar to each other than they are to runs from other participants. Analyses of run diversity need to account for this phenomenon.",null,null
,,,
36,3. RUN AVERAGE OVERLAP,null,null
,,,
37,"Our biggest concern regarding the EaaS model is whether the common API imposed on participants restricts their retrieval approaches to the extent that different participants are compelled to submit essentially similar runs. In order to answer this question, we introduce a novel metric, Run Average Overlap (RAO), which is a measure of the tendency of a set of runs to retrieve documents in common.",null,null
,,,
38,"Run Average Overlap is computed for a given run with respect to a specific set of runs. In what follows we compute the RAO score for each run in a pool set with respect to the pool set. Say that a run O retrieves document d for a topic t if d occurs in the top K ranks for t in O. When K is set equal to the pool depth, as done here, O retrieves d if it contributes d to the pool. Let P be the total number of participants that have runs in the run set, and Pd be the number of distinct participants that retrieve d. Finally, let T be the total number of topics and n be the number of documents retrieved by O for t. Then:",null,null
,,,
39,"1X1X 1 RAO(O) ,",null,null
,,,
40,T t n d Pd,null,null
,,,
41,"In words, the RAO score for a run is the mean over all topics",null,null
,,,
42,"of the score computed for individual topics, where the per-",null,null
,,,
43,topic score is the mean over the documents retrieved by O of,null,null
,,,
44,the reciprocal of the number of participants (including itself),null,null
,,,
45,that retrieve that document. The greater the RAO score the,null,null
,,,
46,more distinctive the result set. The minimum RAO score is,null,null
,,,
47,1:00 PM,null,null
,,,
48,which signifies that O retrieved only documents that all,null,null
,,,
49,"other participants also retrieved. The maximum score is 1.0,",null,null
,,,
50,which means that no other participant retrieved any docu-,null,null
,,,
51,ment that O retrieved. Note that document distinctiveness,null,null
,,,
52,for the purposes of computing RAO is computed over partic-,null,null
,,,
53,"ipants, and thus the number of runs submitted by any given",null,null
,,,
54,participant is not an issue.,null,null
,,,
55,While RAO scores close to,null,null
,,,
56,1:00 PM,null,null
,,,
57,for all runs in a pool set,null,null
,,,
58,"indicate that the runs are not diverse, the presence of runs",null,null
,,,
59,with high scores is not sufficient to conclude that the pool,null,null
,,,
60,is appropriately diverse. The RAO score is computed over,null,null
,,,
61,"all retrieved documents, not relevant documents. Distinc-",null,null
,,,
62,"tive but ineffective runs are easy to produce (for example,",null,null
,,,
63,by selecting random documents from the collection) but un-,null,null
,,,
64,"helpful for pool building. Thus, the RAO score must be",null,null
,,,
65,used in conjunction with an effectiveness score to ensure,null,null
,,,
66,the presence of distinctive and effective runs in the pool set.,null,null
,,,
67,We use R-precision as the effectiveness measure here since it,null,null
,,,
68,was the primary metric for the TREC 2013 Microblog track.,Y,null
,,,
69,Figure 1 shows plots of RAO vs. R-precision for the three,null,null
,,,
70,collections. Each point in a graph represents one pool run.,null,null
,,,
71,Manual runs (runs for which there was some human pro-,null,null
,,,
72,cessing in the construction of the run) are plotted as open,null,null
,,,
73,960,null,null
,,,
74,TREC-8 1,Y,null
,,,
75,MB2012 1,null,null
,,,
76,MB2013 1,null,null
,,,
77,0.8,null,null
,,,
78,0.8,null,null
,,,
79,0.8,null,null
,,,
80,0.6,null,null
,,,
81,0.6,null,null
,,,
82,0.6,null,null
,,,
83,RAO RAO RAO,null,null
,,,
84,0.4,null,null
,,,
85,0.4,null,null
,,,
86,0.4,null,null
,,,
87,0.2,null,null
,,,
88,0.2,null,null
,,,
89,0.2,null,null
,,,
90,0,null,null
,,,
91,0,null,null
,,,
92,0.2 0.4 0.6 0.8,null,null
,,,
93,1,null,null
,,,
94,R-Precision,null,null
,,,
95,0,null,null
,,,
96,0,null,null
,,,
97,0.2 0.4 0.6 0.8,null,null
,,,
98,1,null,null
,,,
99,R-Precision,null,null
,,,
100,0,null,null
,,,
101,0,null,null
,,,
102,0.2 0.4 0.6 0.8,null,null
,,,
103,1,null,null
,,,
104,R-Precision,null,null
,,,
105,Figure 1: Run Average Overlap (y-axis) vs. mean R-precision (x-axis) for pool runs. Manual runs are plotted as open squares and automatic runs as filled circles.,null,null
,,,
106,"squares while automatic (i.e., non-manual) runs are plotted as filled circles. A highly distinctive and highly effective run would fall in the upper right-hand corner of the graph.",null,null
,,,
107,"The plots for the MB2013 and the TREC-8 collections are very similar. These collections exhibit a general inverse relationship between effectiveness and distinctiveness (points run roughly from upper left to lower right). This is simply a manifestation of the fact that there are many fewer relevant documents than non-relevant documents, and thus many fewer effective retrieved sets than ineffective retrieved sets. But, importantly, we see that both the TREC-8 and MB2013 collections contain outlier runs that are both distinctive and effective--these represent high-quality manual runs that contributed to the pools.",Y,null
,,,
108,"The graph for the MB2012 collection, which was not constructed using the EaaS model, is different from the others. The pool runs for MB2012 have a much narrower range of effectiveness and have a much greater RAO score on average. Note, however, that this is the one collection that does not have any runs that are outlier effective runs, and overall effectiveness is relatively poor. Thus, the large RAO scores are probably another manifestation of the tendency for ineffective runs to be different.",null,null
,,,
109,4. UNIQUELY RETRIEVED RELEVANT,null,null
,,,
110,"In our second analysis, we examined uniquely retrieved relevant documents, which are relevant documents that were contributed to the pool by only a single participant. The leave out uniques test (LOU) [1] gauges the reusability of a test collection by examining the effect on evaluation scores of a participant's runs when the participant's uniquely retrieved relevant documents are removed from the judgment set. The test computes the evaluation scores that would have resulted had the participant not been involved in the collection building process.",null,null
,,,
111,"Figure 2 plots the results of the LOU test for each of the three collections. Each run is plotted as a point, with manual runs shown as empty boxes and automatic runs as filled circles. The x-axis is the MAP score of the run as computed using the official relevance judgment set for that collection, and the y-axis is the MAP score as computed using the judgment set with the participant's unique relevants removed. When both scores are the same, the point falls on the diagonal line. A run whose MAP score decreases",null,null
,,,
112,Table 2: Collection characteristics on uniquely retrieved relevant documents and LOU test results.,null,null
,,,
113,% of total relevant that are unique max % of total uniques retrieved by a single participant,null,null
,,,
114,TREC-8 24.0,Y,null
,,,
115,42.2,null,null
,,,
116,MB2012 30.7,null,null
,,,
117,19.7,null,null
,,,
118,MB2013 39.6,null,null
,,,
119,43.2,null,null
,,,
120,mean % diff in LOU scores (auto runs only) max % diff in LOU scores (auto runs only) # runs with diff in scores > 1% (auto only) / total,null,null
,,,
121,0.14 0.84 0/54,null,null
,,,
122,0.64 8.36 16/112,null,null
,,,
123,1.12 7.95 15/57,null,null
,,,
124,"when unique relevants are removed falls below the line, and a run whose MAP score improves when unique relevants are removed lies above the line.",null,null
,,,
125,"The presence of distinctive and effective manual runs in the TREC-8 and MB2013 collections is obvious in the LOU test results. These runs fall well below the line of equal scores, and are the only runs to do so. The degradation in MAP scores is not surprising given that the participants who submitted these runs contributed large numbers of unique relevant documents to the pools. As shown in the second row of Table 2, for these two collections the participant who submitted the most effective manual runs contributed more than 40% of the unique relevant documents. In contrast, the largest percentage of unique relevant documents contributed by a single participant in the MB2012 collection is just 20%. The percentage of total relevant documents that are uniquely retrieved relevants is given in the first row of the table. Here the TREC-8 and MB2013 collections differ: the MB2013 collection has a much larger percentage of total relevant documents that are unique.",null,null
,,,
126,"A large percentage of total relevant documents that are uniquely retrieved relevant documents is one indication that a test collection may be less reusable, and the LOU test results are consistent with this observation. In Figure 2, all of the automatic runs are on the line of equal scores for the TREC-8 collection, but there is more movement away from the line for the two Microblog collections. The final three rows of Table 2 quantify this movement. The table gives statistics regarding the distribution of the percentage difference in MAP scores in the LOU test when considering",null,null
,,,
127,961,null,null
,,,
128,TREC-8 1,Y,null
,,,
129,MB2012 1,null,null
,,,
130,MB2013 1,null,null
,,,
131,0.8,null,null
,,,
132,0.8,null,null
,,,
133,0.8,null,null
,,,
134,LOU MAP LOU MAP LOU MAP,null,null
,,,
135,0.6,null,null
,,,
136,0.6,null,null
,,,
137,0.6,null,null
,,,
138,0.4,null,null
,,,
139,0.4,null,null
,,,
140,0.4,null,null
,,,
141,0.2,null,null
,,,
142,0.2,null,null
,,,
143,0.2,null,null
,,,
144,0,null,null
,,,
145,0,null,null
,,,
146,0.2 0.4 0.6 0.8,null,null
,,,
147,1,null,null
,,,
148,Original MAP,null,null
,,,
149,0,null,null
,,,
150,0,null,null
,,,
151,0.2 0.4 0.6 0.8,null,null
,,,
152,1,null,null
,,,
153,Original MAP,null,null
,,,
154,0,null,null
,,,
155,0,null,null
,,,
156,0.2 0.4 0.6 0.8,null,null
,,,
157,1,null,null
,,,
158,Original MAP,null,null
,,,
159,"Figure 2: Results of ""leave out uniques"" test. Original MAP scores are plotted on the x-axis and LOU MAP scores on the y-axis. Manual runs are plotted as open squares and automatic runs as filled circles.",null,null
,,,
160,"only automatic runs whose original MAP scores were at least 0.1 (because percentage differences are artificially magnified for ineffective runs). The statistics computed are the mean of the percentage difference over these automatic runs, the maximum percentage difference observed in the automatic runs, and the count of the number of runs that have a percentage difference greater than 1% (out of the total). No TREC-8 automatic run has a percentage difference greater than 1%, and since differences that small are within the level of evaluation noise [9], the collection is regarded as highly reusable. The MB2013 collection has a mean percentage difference of about 1%, with 15 runs (out of 57) having at least a 1% difference and a maximum difference of about 8%.",Y,null
,,,
161,"Note that this amount of change in LOU test results is not a consequence of using the EaaS model. In fact, the large percentage of relevant documents that are unique is actually confirmation that the evaluation method accommodated distinctive runs (i.e., the API did not hamper researchers' ability to generate runs that are both effective and distinctive), and the MB2012 collection, which was not built using EaaS, has similar characteristics. The likely explanation is the size of pools relative to the size of the corpus [1]. Corpora used in the two Microblog collections are much larger than the TREC-8 collection, so the judgment pools represent a much smaller percentage of the collection than for the TREC-8 collection. Also note that this level of change is unlikely to have much impact in the utility of the collection as a research tool. As can be seen in Figure 2, the relative ordering of runs is largely stable even in the presence of MAP score differences. At worst, researchers who encounter many unjudged tweets in top ranks in new runs should be more cautious in their conclusions.",Y,null
,,,
162,5. CONCLUSION,null,null
,,,
163,"The evaluation as a service model for constructing retrieval test collections can offer significant advantages over traditional construction techniques, but only if it does not fundamentally hamper innovation and it leads to highquality resources. This paper examined the one collection that has been built using the EaaS model to date, the TREC 2013 Microblog collection, and found its run diversity to be similar to the high-quality TREC-8 ad hoc collection. The results of the leave out uniques test suggest that pools from the Microblog 2013 collection are less complete than pools",Y,null
,,,
164,"from TREC-8, but both collections strongly benefit from the presence of distinctive and effective manual runs.",null,null
,,,
165,"Of course, these findings are for a single collection and a specific API. This implementation provides a generous number of documents in response to a query and a generous allotment of queries per participant. These choices contribute to good design, as a more restrictive API would likely have impacted submitted runs and the resulting test collection negatively. Although we cannot yet make statements about the EaaS model in general, the TREC 2013 Microblog collection does offer an existence proof that a high-quality retrieval test collection can be constructed using this new method.",null,null
,,,
166,6. ACKNOWLEDGMENTS,null,null
,,,
167,"This work has been supported in part by NSF under awards IIS-1217279 and IIS-1218043. Any opinions, findings, conclusions, or recommendations expressed are the authors' and do not necessarily reflect those of the sponsor.",null,null
,,,
168,7. REFERENCES,null,null
,,,
169,"[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10:491­508, 2007.",null,null
,,,
170,"[2] D. Harman. Information Retrieval Evaluation. Morgan & Claypool Publishers, 2011.",null,null
,,,
171,"[3] J. Lin and M. Efron. Evaluation as a service for information retrieval. ACM SIGIR Forum, 47(2):8­14, 2013.",null,null
,,,
172,"[4] J. Lin and M. Efron. Overview of the TREC-2013 microblog track. In TREC, 2013.",Y,null
,,,
173,"[5] C. D. Manning, P. Raghavan, and H. Schu¨tze. Evaluation in information retrieval. In Introduction to Information Retrieval, chapter 8, pages 153­154. Cambridge University Press, 2009.",null,null
,,,
174,"[6] R. McCreadie, I. Soboroff, J. Lin, C. Macdonald, I. Ounis, and D. McCullough. On building a reusable Twitter corpus. In SIGIR, 2012.",null,null
,,,
175,"[7] I. Ounis, C. Macdonald, J. Lin, and I. Soboroff. Overview of the TREC-2011 microblog track. In TREC, 2011.",Y,null
,,,
176,"[8] K. Sparck Jones and C. van Rijsbergen. Report on the need for and provision of an ""ideal"" information retrieval test collection. British Library Research and Development Report 5266, 1975.",null,null
,,,
177,"[9] E. M. Voorhees. The philosophy of information retrieval evaluation. In CLEF, 2002.",null,null
,,,
178,"[10] J. Zobel. How reliable are the results of large-sacle information retrieval experiments? In SIGIR, 1998.",null,null
,,,
179,962,null,null
,,,
180,,null,null

,sentence,label,data
0,Extending Test Collection Pools Without Manual Runs,null,null
1,Gaya K. Jayasinghe,null,null
2,"RMIT University Melbourne, Australia gaya.jayasinghe@rmit.edu.au",null,null
3,William Webber,null,null
4,"William Webber Consulting Melbourne, Australia",null,null
5,william@williamwebber.com,null,null
6,J. Shane Culpepper,null,null
7,"RMIT University Melbourne, Australia shane.culpepper@rmit.edu.au",null,null
8,Mark Sanderson,null,null
9,"RMIT University Melbourne, Australia mark.sanderson@rmit.edu.au",null,null
10,ABSTRACT,null,null
11,"Information retrieval test collections traditionally use a combination of automatic and manual runs to create a pool of documents to be judged. The quality of the final judgments produced for a collection is a product of the variety across each of the runs submitted and the pool depth. In this work, we explore fully automated approaches to generating a pool. By combining a simple voting approach with machine learning from documents retrieved by automatic runs, we are able to identify a large portion of relevant documents that would normally only be found through manual runs. Our initial results are promising and can be extended in future studies to help test collection curators ensure proper judgment coverage is maintained across complete document collections.",null,null
12,Categories and Subject Descriptors,null,null
13,"H.3.3 [Information Storage & Retrieval]: Information Search & Retrieval--clustering, retrieval models, search & selection process",null,null
14,General Terms,null,null
15,"Information retrieval, Evaluation, Test collection construction",null,null
16,1. INTRODUCTION,null,null
17,"Successful evaluation and reproducibility of experiments in information retrieval (IR) depends on building reusable test collections composed of documents, topics, and relevance judgments. Ideally every document in a collection would be assessed against each topic, but this approach does not scale. So judgments are normally produced for a sample of the corpus, known as a pool, all other documents are assumed to be not relevant. This sample needs to be representative of the entire collection and robust enough to evaluate entirely new search algorithms. The genesis of pooling dates back to the 1970s [12].",null,null
18,"To produce relevance judgments, the organizers of TREC, CLEF, NTCIR, and other such conferences invite researchers to submit the top-i documents retrieved for a set of topics from a specified corpus",null,null
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609473.",null,null
20,"[14, 15] (typically i ,"" 1, 000). The sets of documents are known as automatic runs. Across the runs, the top-j ranked documents for each topic are gathered for relevance assessment (typically j is set to 50 or 100). Such a practice seems to consistently identify most of the relevant documents, but provides no guarantee on the judgment coverage for documents retrieved by new IR approaches [4, 9, 16]. Test collections tend to have a bias towards the systems contributing to the pool, and may not reliably evaluate novel IR systems that retrieve unjudged but relevant documents.""",null,null
21,"In an attempt to ""future proof"" test collections, the organizers of the evaluation conferences commonly encourage submissions of manual runs, where humans can reformulate queries and/or merge results from multiple queries [1] before a final set of top-i documents is submitted. Such runs are generally highly effective and contribute many unique relevant documents to the judgment pool. However, manual runs are not always available when building a collection, so in this short paper we ask:",null,null
22,Research question: Can we construct reliable IR test collections using only automatic retrieval runs?,null,null
23,Our contribution: We describe a methodology that can be used to construct reusable test collections in the absence of manual retrieval runs. We evaluate a simple voting approach combined with machine learning to show that we can achieve collection coverage similar to pooling generated with manual runs.,null,null
24,2. BACKGROUND,null,null
25,"Efficiently building test collections for evaluation of IR systems is a well-studied problem [10]. Early research concentrated on more efficient ways for assessors to scan pools, with the objective of judging more documents with a given budget or identifying a sufficient number of relevant documents as quickly as possible. Zobel [16] showed that the number of relevant documents in a collection varies from topic to topic. He suggested that assessors should focus their effort on judging topics with more relevant documents. For each topic, the number of relevant documents found so far were used to estimate the expected ratio of relevant documents in the remaining unjudged block. Each topic was assessed until relevant documents were depleted beyond an economically viable limit to assess the block.",null,null
26,"The idea of focusing assessor effort on the most fruitful sources of relevant documents was also applied to IR systems that contribute to a pool. Just as some topics have more relevant documents than others, some systems retrieve more relevant documents than others. Using this insight, Cormack et al. [5] described a move-tofront pooling approach which ensured that documents from the IR systems producing the most relevant documents were moved to the",null,null
27,915,null,null
28,916,null,null
29,"We also use Kendall's  to measure pairwise inversions between two rankings of runs, the first using full TREC relevance assessments and the second using relevance assessments generated from the union of the first and second pools formed by each of our methods. Using a convention from Voorhees [13], if the Kendall's  correlation is  0.9, the rankings are considered equivalent.",null,null
30,Metric,null,null
31,MAP P@10 P@20 P@30 P@100,null,null
32,Borda count,null,null
33,0.0778 0.1306 0.1122 0.1020 0.0743,null,null
34,ML,null,null
35,0.0268 0.0531 0.0378 0.0361 0.0167,null,null
36,Combined,null,null
37,0.1507 0.1714 0.1500 0.1367 0.0916,null,null
38,Table 1: Effectiveness on finding relevant documents in MRJ. A : significant improvement (p < 0.01) compared to Borda count.,null,null
39,Depth (k) Borda count ML Combined,null,null
40,50,null,null
41,15.22 4.52,null,null
42,19.00,null,null
43,100,null,null
44,24.19 5.45,null,null
45,29.83,null,null
46,150,null,null
47,29.30 6.71,null,null
48,37.28,null,null
49,171,null,null
50,31.36 7.11,null,null
51,39.53,null,null
52,200,null,null
53,33.75 7.97,null,null
54,42.33,null,null
55,Table 2: Percentage of MRJ documents found in top (k) of the proposed rankings. implies a similar assessment effort to traditional pooling method. A : significant improvement (p < 0.05) compared to Borda count.,null,null
56,5. RESULTS,null,null
57,"The analysis is presented in Table 1. The combined method is significantly better than the other two when evaluated with MAP. The same trend is observed when measuring using precision, but none of the differences are significant. Using only the ML method produces worse results than either Borda count or combined.",null,null
58,"Note that the relatively low reported effectiveness numbers in Table 1 are largely a byproduct of evaluating using only the unique relevant documents in MRJ and not the entire second pool. We cannot make any claims about new documents retrieved by the ML method since a large portion of retrieved documents using this method are not judged, compared to other two approaches. In fact, 9, 817 of the top-200 documents returned across all 50 topics using only ML (98.17%) are currently unjudged. Therefore, we have to assume that these documents are not relevant until all of the documents returned are judged. In future work, we hope to investigate the full impact unjudged documents have on our classifier method in more detail.",null,null
59,"In Table 2 we measure the proportion of documents that were found to be relevant in the second pool. Again a similar trend of differences are seen, but with significant improvements across all measurements up to k , 200 for the combined method.",null,null
60,5.1 Discussion,null,null
61,"As indicated in Figure 1, the majority of documents uniquely judged in the manual runs (MRJ) are also retrieved by the automatic runs (ARU+ARJ). However, few appear in the first pool as they (i.e. ARJ) are not ranked highly enough to be judged. In fact, 88% of the documents judged as relevant that are uniquely pooled by manual runs could be found in the first pool, if a pool depth of 1, 000 was used.",null,null
62,"If there were no manual runs in a test collection (i.e. no MRJ), the effectiveness of IR systems producing results similar to such",null,null
63,runs would be underestimated and any improvements would go unnoticed. It would appear that manual retrieval runs still play a critical role in improving the re-usability of test collections.,null,null
64,Relevant documents exclusively pooled by manual retrieval runs,null,null
65,10 1000,null,null
66,Number of Relevant documents,null,null
67,0,null,null
68,Combined Estimates for combined,null,null
69,0,null,null
70,50,null,null
71,100,null,null
72,150,null,null
73,200,null,null
74,Depth of ranking (K),null,null
75,"Figure 2: The number of MRJ documents, and estimated number of relevant documents in the top-k of the combined ranked list on TREC GOV2 dataset and TREC topics 801 ­ 850.",null,null
76,Metric,null,null
77,MAP P@10 P@20 P@30 P@100,null,null
78,Borda count,null,null
79,0.3415 0.3571 0.3551 0.3401 0.2337,null,null
80,ML,null,null
81,0.4872 0.5082 0.4684 0.4299 0.2624,null,null
82,Combined,null,null
83,0.5049 0.5694 0.4959 0.4497 0.2555·,null,null
84,"Table 3: Just considering the documents in MRJ, how effective are ranking algorithms on retrieving relevant documents? Significant improvements (p < 0.01 and p < 0.05) compared to Borda count are denoted with a  and ·.",null,null
85,"Judging the ranked lists of the combined method up to a depth k identifies a subset of the relevant documents uniquely pooled by manual retrieval runs. However, we still know little about the large number of unjudged documents in the ranked lists produced by the combined method. If we assume the proportion of relevant documents among unjudged documents in these ranked lists is the same as the proportion found among judged documents in the same ranked list up to the same depth, we can estimate the total number of relevant documents that would have been found in the same depth of the ranking. Figure 2 illustrates the estimated number of relevant documents, along with the number of known relevant documents found.",null,null
86,"Missing judgments for a large portion of the ranked lists from the proposed methods is one potential reason for the low retrieval effectiveness of those methods. Therefore, we calculate retrieval effectiveness on the intersection of the second pool with MRJ, Table 3. (Note, the first pool and the ranking functions remains the same.) The ML method now re-ranks a subset of unique documents top-j ranked by manual runs. The ranking produced by ML show significant improvements for all considered evaluation metrics compared to Borda count. The combined method achieves a better effectiveness than ML for all evaluation metrics considered, except p@100. This is due to ranking only the subset of documents top ranked by the Borda count. Re-ranking a carefully retrieved",null,null
87,917,null,null
88,1,null,null
89,1,null,null
90,MAP P@10 P@20 P@30 P@100,null,null
91,0.9,null,null
92,0.8,null,null
93,Kendall's Tau,null,null
94,0.9,null,null
95,Kendall's Tau,null,null
96,0.8,null,null
97,0,null,null
98,50,null,null
99,100,null,null
100,150,null,null
101,200,null,null
102,Depth,null,null
103,0.7,null,null
104,MAP P@10 P@20 P@30 P@100,null,null
105,5,null,null
106,10,null,null
107,15,null,null
108,20,null,null
109,25,null,null
110,30,null,null
111,Automatic systems,null,null
112,Figure 3: Kendall's  correlation of IR system rankings for varying depths of assessing documents using combined method.,null,null
113,subset of documents for topics with ML is an effective approach to locate new documents to be pooled and judged.,null,null
114,"Whenever a new approach for pool composition is proposed, we would like to be able to quantify how well the approach ranks IR systems compared to the original method. A Kendall's  ranking correlation for varying depths of assessing documents with the proposed approach for various evaluation metric are shown in Figure 3. Here, we consider all 80 submitted runs rather than only the subset originally used for pooling. Manual retrieval runs are viewed as novel approaches to retrieval. The Kendall's  correlation for MAP is above 0.9 beyond a depth of 100. A budget similar to original assessment permits processing up to a depth of 171 documents, which demonstrates the validity of the proposed approach in the absence of manual retrieval runs.",null,null
115,Another question of interest is how small the automatic runs pool can be when there are no manual runs. In Figure 4 we introduce runs incrementally in order starting with the run contributing the fewest relevant documents. When 20 or more automatic retrieval runs are pooled the Kendall  correlation for MAP exceeds 0.9.,null,null
116,6. CONCLUSION,null,null
117,"In this paper, we present a methodology for building reusable evaluation pools in the absence of manual retrieval runs. Our approach can discover many relevant documents that were previously only found by manual retrieval runs. The approach demonstrates the potential of finding relevant documents that are not currently possible using current pooling approaches. However, the true efficacy of our approach cannot be properly assessed until all of the newly retrieved documents are judged. We plan to investigate this in future work. Nonetheless, our initial results are promising as we are already able to achieve a similar IR system ranking to previous approaches which depended heavily on manual runs to add the necessary diversity to the assessment pool.",null,null
118,Acknowledgments,null,null
119,This work was supported in part by the Australian Research Council (DP130104007). Dr. Culpepper is the recipient of an ARC DECRA Research Fellowship (DE140100275).,null,null
120,"Figure 4: Kendall's  correlation of IR system rankings with varying number of automatic systems in the pool. Automatic systems are added in the order least contributing system to most, and the ranking produced by the combined method is processed to a depth of 200.",null,null
121,References,null,null
122,"[1] C. Buckley, D. Dimmick, I. Soboroff, and E. Voorhees. Bias and the limits of pooling for large collections. Information Retrieval, 10(6): 491­508, 2007.",null,null
123,"[2] S. Büttcher, C. L. A. Clarke, and I. Soboroff. The TREC 2006 terabyte track. In TREC-2006, volume 6, page 39, 2006.",null,null
124,"[3] S. Büttcher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboroff. Reliable information retrieval evaluation with incomplete and biased judgements. In SIGIR, pages 63­70, 2007.",null,null
125,"[4] B. Carterette, E. Gabrilovich, V. Josifovski, and D. Metzler. Measuring the reusability of test collections. In WSDM, pages 231­240, 2010.",null,null
126,"[5] G. V. Cormack, C. R. Palmer, and C. L. A. Clarke. Efficient construction of large test collections. In SIGIR, pages 282­289, 1998.",null,null
127,"[6] R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871­1874, June 2008.",null,null
128,"[7] R. Krovetz. Viewing morphology as an inference process. In SIGIR, pages 191­202, Pittsburgh, Pennsylvania, USA, 1993.",null,null
129,"[8] A. Moffat, W. Webber, and J. Zobel. Strategic system comparisons via targeted relevance judgments. In SIGIR, pages 375­382, 2007.",null,null
130,"[9] T. Sakai. The unreusability of diversified search test collections. In EVIA, June 2013.",null,null
131,"[10] M. Sanderson. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4 (4):247­375, 2010.",null,null
132,"[11] I. Soboroff and S. Robertson. Building a filtering test collection for TREC 2002. In SIGIR, pages 243­250, 2003.",null,null
133,"[12] K. Spärck Jones and C. J. Van Rijsbergen. Report on the need for and provision of an ""ideal"" information retrieval test collection. Technical report, British Library Research and Development Report 5266, 1975.",null,null
134,"[13] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information processing & management, 36(5):697­716, 2000.",null,null
135,"[14] E. M. Voorhees. The philosophy of information retrieval evaluation. In Evaluation of cross-language information retrieval systems, pages 355­370. Springer, 2002.",null,null
136,"[15] E. M. Voorhees and D. K. Harman. TREC: Experiment and evaluation in information retrieval, volume 63. MIT press Cambridge, 2005.",null,null
137,"[16] J. Zobel. How reliable are the results of large-scale information retrieval experiments? In SIGIR, pages 307­314, 1998.",null,null
138,918,null,null
139,,null,null

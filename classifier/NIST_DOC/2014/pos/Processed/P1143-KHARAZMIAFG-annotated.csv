,sentence,label,data
,,,
0,Using Score Differences for Search Result Diversification,null,null
,,,
1,Sadegh Kharazmi Mark Sanderson Falk Scholer,null,null
,,,
2,"RMIT University & NICTA, Melbourne, Australia",null,null
,,,
3,"{sadegh.kharazmi, mark.sanderson, falk.scholer}@rmit.edu.au",null,null
,,,
4,David Vallet,null,null
,,,
5,"NICTA, Sydney, Australia",null,null
,,,
6,david.vallet@nicta.com.au,null,null
,,,
7,ABSTRACT,null,null
,,,
8,"We investigate the application of a light-weight approach to result list clustering for the purposes of diversifying search results. We introduce a novel post-retrieval approach, which is independent of external information or even the full-text content of retrieved documents; only the retrieval score of a document is used. Our experiments show that this novel approach is beneficial to effectiveness, albeit only on certain baseline systems. The fact that the method works indicates that the retrieval score is potentially exploitable in diversity.",null,null
,,,
9,Categories and Subject Descriptors,null,null
,,,
10,"H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval­retrieval models, search process",null,null
,,,
11,General Terms,null,null
,,,
12,"Algorithm, Theory, Experimentation",null,null
,,,
13,Keywords,null,null
,,,
14,Diversity; Score Difference; Clustering,null,null
,,,
15,1. INTRODUCTION,null,null
,,,
16,"User queries submitted to an Information Retrieval (IR) system are often ambiguous at different levels [7]. To address such ambiguity, IR systems attempt to diversify search results, so that they cover a wide range of possible interpretations (aspects, intents or subtopics) of a query. Consequently, the number of redundant items in a search result list should be decreased, while the likelihood that a user will be satisfied with any of the displayed results should become higher.",null,null
,,,
17,"In traditional IR, the estimated relevance of a document, which is used to determine the ranking of search results, depends primarily on query-document similarity. In diversified retrieval, search result rankings are based not only on querydocument similarity, but also on the other documents that",null,null
,,,
18,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609530.",null,null
,,,
19,"have been retrieved prior to the current document under consideration (i.e., document-document similarity).",null,null
,,,
20,"Many of the proposed diversification techniques take a greedy approach, comparing a document to all previously retrieved documents, or the subtopics of a query. Also, they may use additional information, such as past user interactions, to identify which of the possible subtopics of a query are more likely to be interesting to the user. Most effective diversification approaches in the literature use techniques that focus on coverage, favoring documents that cover as many novel subtopics of a query as possible. This is in contrast to earlier techniques that focus on novelty, estimating the newness of a document with respect to those already retrieved. Novelty-based techniques usually exploit implicit information, such as differences in document content.",null,null
,,,
21,"One source of implicit information derived from search results that appears to have never been investigated are differences in retrieval scores: score differences. Retrieval systems will usually, in response to a query, return a list of documents sorted by a relevance score, indicating the degree to which a query and document match. When analyzing a retrieved document list, the differences between the scores of adjacent retrieved documents differ, and this variation might be exploitable. Two documents that receive similar relevance scores are likely to share similar features; they might therefore address the same subtopics of a user's query. Conversely, two adjacent documents that have a large score difference are likely to have fewer features in common, which suggests that the documents might cover different query subtopics.",null,null
,,,
22,Our research question is to ask if we can exploit score differences to help with search result diversification.,null,null
,,,
23,"We develop a simple non-greedy diversification approach that uses differences between the scores of the initially retrieved documents. The approach is experimentally investigated using the TREC framework, comparing to baselines and state-of-the-art diversification approaches.",null,null
,,,
24,2. RELATED WORK,null,null
,,,
25,"There are two key approaches to diversifying search results, based on explicit or implicit evidence [9]. The explicit approaches [1, 10] match retrieved documents to the subtopics of a query, which are ""pre-derived"" from external sources such as a query log or taxonomies. Implicit approaches attempt to diversify based on a representation of already-retrieved documents.",null,null
,,,
26,"Maximal Marginal Relevance (MMR) [2] is perhaps the most widely-studied implicit approach. Here, a diverse set of",null,null
,,,
27,1143,null,null
,,,
28,"results (S) is built incrementally from an initial retrieved list (R). The results are picked from R using a greedy approach where, in each iteration, the document that is most novel is selected. Novelty in this case is defined as the mean contentbased dissimilarity between the candidate document and the already selected documents in S. A tuning parameter  defines the trade-off between relevance and diversity.",null,null
,,,
29,"Inspired by Modern Portfolio Theory (MPT) in finance, Wang et al. introduced a new implicit approach that analyses the expected mean and variance of the return of a portfolio [11]. Facility Location Analysis (FLA) was introduced by Zuccon et.al [12] to improve the MPT approach.",null,null
,,,
30,"Explicit approaches are focused on query subtopics, which can be derived from a pre-defined taxonomy such as the Open Directory Project1 (ODP), internal document features, query logs, or online resources [1, 8, 9]. The two most effective explicit diversification approaches are xQuAD [10] and IASelect [1].",null,null
,,,
31,All of these approaches use an iterative greedy selection approach to rank the most diverse documents.,null,null
,,,
32,3. SCORE DIFFERENCES,null,null
,,,
33,"We hypothesize that documents with similar features (e.g., content, aspects covered, length) will be allocated (by ranking functions) similarity scores that are close together. Conversely, documents with different features are likely to be allocated similarity scores that are further apart.",null,null
,,,
34,"Let D1, D2, . . . , DN be an initial ranking of documents which are ordered by a ranking function s(D), and  be a difference threshold parameter. Then if",null,null
,,,
35,|s(Di) - s(Di+1)| <  s(Di+1),null,null
,,,
36,"we assume that the documents cover the same query subtopic. If the value is  , it indicates that the two documents belong to different subtopics.",null,null
,,,
37,"To test our hypothesis, we set up an experiment to measure the score differences between pairs of adjacent ranked documents that either covered the same or different subtopics of a query. We used the documents, queries, and diversity relevance judgments from the TREC 2009­2011 Web Tracks [3, 4]. Documents were ranked using the Dirichletsmoothed language model from the Indri IR system2 with default parameter settings. The query subtopics covered by individual answer documents are defined in the TREC relevance judgments. All non-relevant documents were assigned to an extra ""non-relevant"" subtopic. In total, 148 topics (TREC Web Track 2009, 2010 and 2011 queries) were used and for each topic, and pairs of documents in the top 100 positions in a ranked list were examined.",Y,null
,,,
38,"We tested the Language Modeling (LM) and Okapi BM25 ranking functions, which are widely used in retrieval research, and have been shown to be effective ranking functions [6]. First, using language modelling as the ranking function to score documents, our analysis shows that for pairs of documents where there is no change in subtopic, the mean measured score difference is 0.065. Conversely, when the subtopic changes, the mean difference in scores is 0.073. Although the differences are small, a pairwise permutation test indicates that they are statistically significant (p",Y,null
,,,
39,"1http://www.dmoz.org/ 2Version 5.2, http://lemurproject.org/indri.php",null,null
,,,
40,< 0.01). This analysis suggests that score differences have the potential to be used as a technique to help with result diversification.,null,null
,,,
41,"We repeated the same experiment using BM25. The mean measured score difference when there is no change in subtopic is 0.069, versus 0.071 when the subtopic changes. A pairwise permutation test indicates that these differences are not statistically significant. We therefore hypothesise that score differences are less likely to work well when the BM25 ranking function is used as a base run.",null,null
,,,
42,4. DIVERSIFYING RESULTS USING SCORE DIFFERENCES,null,null
,,,
43,"To apply the score differences technique for result diversification, first, the score difference between each pair of documents, starting at rank position 1, was calculated. The top 100 documents were then re-ranked by decreasing size of the score difference between each document and the document above it. The documents with the biggest difference between the paired documents would now be top ranked, and they should be documents covering different subtopics.",null,null
,,,
44,"After some experimentation with this simple approach, we found that it was better to re-rank documents based on a linear combination of the rank positions in the initial ranking and score differences, as shown in Algorithm 1. This approach to diversification, RankScoreDiff, does not use any information apart from the similarity scores from an initial retrieval run. It is therefore an implicit diversification approach.",null,null
,,,
45,Algorithm 1 RankScoreDiff(L),null,null
,,,
46,L  ScoreDiff(L),null,null
,,,
47,for 1 < i  |L| do,null,null
,,,
48,Score(L[i]),null,null
,,,
49,1 Rank(L[i]),null,null
,,,
50,+,null,null
,,,
51,1 Rank(L,null,null
,,,
52,[i]),null,null
,,,
53,end for,null,null
,,,
54,Sort L on Score(L[i]),null,null
,,,
55,5. EXPERIMENTAL SETUP,null,null
,,,
56,"We investigated the effectiveness of RankScoreDiff as a diversification approach using the diversity task framework of the TREC Web Track from 2009­2011, which comprises 148 queries. Results are reported over the Clueweb category B collection. The Clueweb Online Services3 were used to retrieve the top 100 documents for each query, using the same ranking functions and version of Indri described in Section 3. The top 100 documents were diversified using the methods under test.",Y,null
,,,
57,"Effectiveness was measured with -nDCG, a widely-used metric that incorporates both relevance and diversity into a single score. The parameter , which sets the relative importance of these two evaluation considerations, was set to 0.5, as recommended by the creators of the measure [5]. In the subsequent presentation of results, two-tailed paired t-tests are used to evaluate statistical significance.",null,null
,,,
58,5.1 Implementation and Tuning,null,null
,,,
59,"For diversity approaches that require explicitly defined subtopics as an input (xQuAD and IASelect), two sources of",null,null
,,,
60,3http://boston.lti.cs.cmu.edu/Services/,null,null
,,,
61,1144,null,null
,,,
62,Runs Initial Run (LM) MMR MPT FLA-MPT RankScoreDiff xQuAD RankScoreDiff + xQuAD IASelect RankScoreDiff + IASelect,null,null
,,,
63,-nDCG@5,null,null
,,,
64,0.235 0.233 0.235 0.240 0.246 * 0.246  0.258 * ,null,null
,,,
65,0.266 0.283 *  ,null,null
,,,
66,ODP Subtopics -nDCG@10,null,null
,,,
67,0.276 0.274 0.277 0.280,null,null
,,,
68,0.274 0.286  0.291 ,null,null
,,,
69,0.298 0.315 *  ,null,null
,,,
70,-nDCG@20 0.315 0.317 0.316 0.320 0.324 * 0.326  0.332 0.337 0.347 *,null,null
,,,
71,-nDCG@5,null,null
,,,
72,0.235 0.233 0.232 0.240 0.246 * 0.318 **  0.318 **  0.321 **  0.323 ** ,null,null
,,,
73,TREC Subtopics,Y,null
,,,
74,-nDCG@10 -nDCG@20,null,null
,,,
75,0.276,null,null
,,,
76,0.315,null,null
,,,
77,0.274,null,null
,,,
78,0.317,null,null
,,,
79,0.277,null,null
,,,
80,0.319,null,null
,,,
81,0.28,null,null
,,,
82,0.32,null,null
,,,
83,0.274 0.357 **  0.358 **  0.365 **  0.367 ** ,null,null
,,,
84,0.324 * 0.396 ** 0.397 **  0.400 ** 0.405 **,null,null
,,,
85,"Table 2: Effectiveness of diversification approach using language modeling as baseline. For approaches that need explicit representation of subtopics, TREC official subtopics and ODP subtopics were used.",Y,null
,,,
86,Implicit Explicit,null,null
,,,
87,Method,null,null
,,,
88,MMR MPT F LA - M P T,null,null
,,,
89,xQuADODP I AS electODP xQuADT rec IASelectT rec,null,null
,,,
90,Spearman's  0.92 0.86 0.83 0.78 0.75 0.67 0.71,null,null
,,,
91,Table 1: The Spearman correlation of RankScoreDiff with other diversification approaches in terms of effectiveness measured by -nDCG@20.,null,null
,,,
92,"subtopic definitions were used: first, the TREC Web Track official subtopics; and second, subtopics derived from the ODP using TextWise4 services, with three levels of categorization to generate subtopics. These subtopics represent an upper-bound on effectiveness (perfect knowledge from the relevance judgments), and a reasonable but imperfect approach, respectively.",Y,null
,,,
93,"All approaches used in the experiments were trained to provide the best possible uniform diversification, to ensure that comparisons between the methods are fair. For approaches that required the tuning of parameters, this was carried out using 10-fold cross-validation to determine the best value on each collection. Parameters were tuned at increments of 0.1, and the best  value obtained as 0.8 for the ODP subtopics and 0.9 for the official TREC subtopics.",Y,null
,,,
94,5.2 Experimental Results,null,null
,,,
95,"We investigated the impact on effectiveness of the proposed approach as a diversification feature and compared it to existing approaches in the literature [1, 2, 10, 11, 12].",null,null
,,,
96,"Table 1 shows the Spearman correlation between our proposed approach and other diversification approaches. The results show that, in general, RankScoreDiff is more strongly correlated with implicit diversification approaches than with explicit approaches; the difference with the latter becomes more pronounced when the TREC (perfect) subtopics are available.",null,null
,,,
97,"The results of our effectiveness experiments are shown in Tables 2 and 3, for the LM and BM25 initial retrieval runs, respectively. To carry out a detailed analysis, different baselines were considered. For this reason the following comparisons were made:",null,null
,,,
98,· A significant difference between the measured technique and the initial run is shown using * (p < 0.05) and ** (p < 0.01).,null,null
,,,
99,4http://www.textwise.com/,null,null
,,,
100,"· A significant difference between the measured technique and implicit approaches (MMR, MPT, FLAMPT and RankScoreDiff), which are independent of external knowledge such as subtopics, is shown using  (p < 0.05) and  (p < 0.01). (The symbol indicates that a technique is significantly better than all four of the implicit approaches at the specified level.)",null,null
,,,
101,"· A significant difference between a state-of-the-art explicit diversification method (xQuAD or IASelect), compared to RankScoreDiff combined with that method, is shown using  (p < 0.05) and  (p < 0.01).",null,null
,,,
102,Language model as a baseline,null,null
,,,
103,"Table 2 shows the results when using LM as an initial retrieval run. It can be seen that RankScoreDiff (row 5) significantly improves over the base run (row 1) for -nDCG@5 and -nDCG@20. Although there is some marginal improvement in comparison with other implicit approaches (rows 2-4), this improvement is only significant for -nDCG@5. The results suggest that RankScoreDiff is competitive in comparison with implicit approaches, but it is not as good as the explicit approaches (rows 6 and 8). However, RankScoreDiff can also be used in combination with the explicit approaches (rows 7 and 9 of the table).",null,null
,,,
104,"Using ODP subtopics, the combination of RankScoreDiff with an explicit approach improves over using the explicit approach on its own in most cases. The improvement is significant for -nDCG@5 and -nDCG@10 when IASelect and RankScoreDiff are combined. Using the TREC (perfect) subtopics, marginal improvements are obtained when combining RankScoreDiff with the explicit approaches, however the combined approach is not significantly different compared with the original explicit approach. In addition, the combined approaches are always significantly better than the base run, and are usually significantly better than the implicit approaches.",null,null
,,,
105,OKAPI BM25 as a baseline,null,null
,,,
106,"Table 3 shows results when using BM25 as a baseline run. The improvements in effectiveness over the base run are marginal for all implicit approaches, including RankScoreDiff.",null,null
,,,
107,"Furthermore, Table 3 shows that for the ODP subtopics, even the explicit approaches do not lead to significant improvements over the base run. Similarly, combining RankScoreDiff with xQuAD and IASelect for the ODP subtopics does not improve significantly on the baseline, and in some cases reduces effectiveness. When using TREC (perfect) subtopics, all explicit approaches (on their own, or combined",Y,null
,,,
108,1145,null,null
,,,
109,Runs Initial Run (BM25) MMR MPT FLA-MPT RankScoreDiff xQuAD RankScoreDiff + xQuAD IASelect RankScoreDiff + IASelect,null,null
,,,
110,-nDCG@5 0.268 0.266 0.270 0.275 0.270 0.273 0.275 0.263 0.256,null,null
,,,
111,ODP Subtopics -nDCG@10 0.300 0.300 0.301 0.305 0.298 0.309 0.309 0.294 0.288,null,null
,,,
112,-nDCG@20 0.336 0.337 0.336 0.340 0.335 0.344 0.339 0.331 0.323,null,null
,,,
113,-nDCG@5,null,null
,,,
114,0.268 0.266 0.272 0.275 0.270 0.335 **  0.341 **  0.348 ** 0.343 ** ,null,null
,,,
115,TREC Subtopics,Y,null
,,,
116,-nDCG@10 -nDCG@20,null,null
,,,
117,0.3,null,null
,,,
118,0.336,null,null
,,,
119,0.3,null,null
,,,
120,0.337,null,null
,,,
121,0.302,null,null
,,,
122,0.337,null,null
,,,
123,0.305,null,null
,,,
124,0.34,null,null
,,,
125,0.298,null,null
,,,
126,0.335,null,null
,,,
127,0.377 ** ,null,null
,,,
128,0.407 **,null,null
,,,
129,0.378 ** ,null,null
,,,
130,0.409 **,null,null
,,,
131,0.389 ** ,null,null
,,,
132,0.420 **,null,null
,,,
133,0.384 ** ,null,null
,,,
134,0.413 **,null,null
,,,
135,"Table 3: Effectiveness of diversification approach using Okapi (BM25) as baseline. For approaches that need explicit representation of subtopics, TREC official subtopics and ODP subtopics were used.",null,null
,,,
136,"with RankScoreDiff) improve significantly over the base run and over the implicit approaches. The combination of RankScoreDiff and xQuAD could marginally improve over xQuAD on its own for -nDCG@5, while this is not the case for IASelect.",null,null
,,,
137,"Overall, the results in Tables 2 and 3 show that RankScoreDiff is equivalent in effectiveness with other implicit approaches, although with no significant improvement over strong base runs. However, in the absence of perfect subtopics, RankScoreDiff can potentially be used in combination with explicit approaches to provide a boost in effectiveness. We note that a particular feature of RankScoreDiff is that it is computationally much less intensive than all other diversification approaches, being based only on information that is already available with a base run.",null,null
,,,
138,6. CONCLUSIONS,null,null
,,,
139,"This paper examined a novel approach that uses the differences in original retrieval scores as evidence of diversity, based on the assumption that similar documents will receive similar retrieval scores with respect to a given query, and that similar documents could represent a similar subtopic. We experimentally evaluated the use of a score difference technique to diversify search results. In contrast with existing diversification techniques, which need additional document representations or external subtopics, our proposed approach only needs the relevance score provided by a ranking function. From the results, diversifying using score differences is competitive with other implicit diversification approaches. However, none of these approaches regularly lead to significant improvements over a base run. When perfect subtopic knowledge is not available, the RankScoreDiff approach can potentially boost the effectiveness of state-ofthe-art explicit diversification techniques.",null,null
,,,
140,Our analysis of the distribution of score differences showed that the approach is directly affected by the ranking function that generates the initial retrieval scores.,null,null
,,,
141,"In future work, we plan to investigate how particular features of ranking functions interact with the score differences approach. For example, a parameterised ranking function such as BM25 allows individual effects such as length normalisation, or the relative emphasis of TF and IDF effects, to be isolated and explored.",null,null
,,,
142,8. REFERENCES,null,null
,,,
143,"[1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. Diversifying search results. In Proc. WSDM, pages 5­14. ACM, 2009.",null,null
,,,
144,"[2] J. Carbonell and J. Goldstein. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proc. SIGIR, pages 335­336. ACM, 1998.",null,null
,,,
145,"[3] C. Clarke, N. Craswell, and I. Soboroff. Overview of the TREC 2009 web track. In Proc. TREC, 2009.",null,null
,,,
146,"[4] C. Clarke, N. Craswell, I. Soboroff, and G. Cormack. Preliminary overview of the trec 2010 web track. In Proc. TREC, volume 10, 2010.",null,null
,,,
147,"[5] C. Clarke, M. Kolla, G. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ttcher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In Proc. SIGIR, pages 659­666. ACM, 2008.",null,null
,,,
148,"[6] W. B. Croft, D. Metzler, and T. Strohman. Search engines: Information retrieval in practice. Addison-Wesley Reading, 2010.",null,null
,,,
149,"[7] F. Radlinski, P. Bennett, B. Carterette, and T. Joachims. Redundancy, diversity and interdependent document relevance. In Proc. SIGIR, volume 43, pages 46­52. ACM, 2009.",null,null
,,,
150,"[8] F. Radlinski and S. Dumais. Improving personalized web search using result diversification. In Proc. SIGIR, pages 691­692. ACM, 2006.",null,null
,,,
151,"[9] R. L. T. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result diversification. Information Retrieval, 2012.",null,null
,,,
152,"[10] R. L. T. Santos, J. Peng, C. Macdonald, and I. Ounis. Explicit search result diversification through sub-queries. In Proc. ECIR, pages 87­99, Milton Keynes, UK, 2010. Springer.",null,null
,,,
153,"[11] J. Wang and J. Zhu. Portfolio theory of information retrieval. In Proc. SIGIR, pages 115­122. ACM, 2009.",null,null
,,,
154,"[12] G. Zuccon, L. Azzopardi, D. Zhang, and J. Wang. Top-k retrieval using facility location analysis. In Proc. ECIR, pages 305­316. Springer, 2012.",null,null
,,,
155,7. ACKNOWLEDGMENTS,null,null
,,,
156,"This work was supported in part by the Australian Research Council (DP130104007), as well as NICTA Victoria which is funded by both the Federal and State governments.",null,null
,,,
157,1146,null,null
,,,
158,,null,null

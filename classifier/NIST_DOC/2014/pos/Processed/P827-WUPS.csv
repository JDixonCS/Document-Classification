,sentence,label,data
0,Search Result Diversification via Data Fusion,null,null
1,Shengli Wu and Chunlan Huang,null,null
2,"School of Computer Science and Telecommunication Engineering Jiangsu University, Zhenjiang, China 212013",null,null
3,"swu@ujs.edu.cn, palaceo77@163.com",null,null
4,ABSTRACT,null,null
5,"In recent years, researchers have investigated search result diversification through a variety of approaches. In such situations, information retrieval systems need to consider both aspects of relevance and diversity for those retrieved documents. On the other hand, previous research has demonstrated that data fusion is useful for improving performance when we are only concerned with relevance. However, it is not clear if it helps when both relevance and diversity are both taken into consideration. In this short paper, we propose a few data fusion methods to try to improve performance when both relevance and diversity are concerned. Experiments are carried out with 3 groups of top-ranked results submitted to the TREC web diversity task. We find that data fusion is still a useful approach to performance improvement for diversity as for relevance previously.",null,null
6,Categories and Subject Descriptors,null,null
7,H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval ­ retrieval models,null,null
8,General Terms,null,null
9,"Algorithms, Experimentation, Measurement, Performance",null,null
10,Keywords,null,null
11,"Search result diversification, data fusion, linear combination, weight assignment",null,null
12,1. INTRODUCTION,null,null
13,"In recent years, researchers have taken various approaches to investigate search result diversification [3, 1]. In such situations, information retrieval systems need to consider both relevance and diversity for those retrieved documents. In this short paper, we aim to find out if and how data fusion can help with this.",null,null
14,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609451.",null,null
15,"Previous research on data fusion (such as in [2, 6, 8]) demonstrates that it is possible to improve retrieval performance when we only consider relevance. Now with the new dimension of diversification, we need to re-evaluate the technology. In particular, some fusion methods need to be modified to accommodate for the new situation.",null,null
16,"We may divide data fusion methods into two broad categories, according to how they deal with component results: equal-treatment and biased methods. As their names would suggest, the former treats all component results equally, while the latter does not. CombSum, CombMNZ, and the Condorcet method belong to the first category, while the linear combination method is a representative of the second category. Equal-treatment methods can likely be used in the new situation without modification, but the linear combination method needs more consideration.",null,null
17,"In linear combination, weight assignment is a key issue for achieving good fusion performance and a considerable number of weight assignment methods have been proposed. Generally speaking, we need to consider two factors for weight assignment. One is the performance of every component retrieval system involved, and the other is the dissimilarity (or distance) between those component systems/results. For the information retrieval systems involved, well-performing systems should be given greater weights , while systems performing poorly should be assigned smaller weights. On the other hand, smaller weights should be assigned to those results that are similar to the others, while greater weights should be assigned to those results that are more different to the others. When assigning weights, we may take into consideration performance or dissimilarity, or even both together. It is also possible to use some machine learning techniques, known as ""learning to rank"", to train weights using some training data. This is especially popular for combining results at the feature level. These methods are aimed at optimizing a goal that is related to retrieval effectiveness measured by a given metric such as average precision. Because the metrics used for result diversification (e.g., ERR-IA@20) are very different from metrics such as average precision, almost all methods in this category cannot be directly used for result diversification.",null,null
18,"In this paper, we are going to investigate data fusion methods, especially linear combination, for result diversification. Experiments are carried out to evaluate them with 3 groups of results submitted to the TREC web diversity task between 2009 and 2011. Experiments show that the proposed methods perform well and have the potential to be used for this purpose in practice.",null,null
19,827,null,null
20,Table 1: Information of 3 groups of results submitted to the web diversity task in TREC,null,null
21,TREC 2009,null,null
22,TREC 2010,null,null
23,TREC 2011,null,null
24,Run,null,null
25,ERR-IA@20 Run,null,null
26,ERR-IA@20 Run,null,null
27,ERR-IA@20,null,null
28,MSRAACSF,null,null
29,0.2144 qirdcsuog3,null,null
30,0.2051 ICTNET11DVR3 0.4764,null,null
31,MSDiv3,null,null
32,0.2048 THUIR10DvNov,null,null
33,0.3355 liaQEWikiAnA,null,null
34,0.2287,null,null
35,uogTrDYCcsB,null,null
36,0.1922 UAMSD10aSRfu,null,null
37,0.2423 msrsv2011d1,null,null
38,0.4994,null,null
39,UamsDancTFb1 0.1774 UCDSIFTDiv,null,null
40,0.2100 UAmsM705tFLS,null,null
41,0.4378,null,null
42,mudvimp,null,null
43,0.1746 UMd10IASF,null,null
44,0.2546 uogTrA45Nmx2,null,null
45,0.5284,null,null
46,UCDSIFTdiv,null,null
47,0.1733 uogTrB67xS,null,null
48,0.2981 UWatMDSqltsr,null,null
49,0.4939,null,null
50,NeuDiv1,null,null
51,0.1705 cmuWi10D,null,null
52,0.2484 uwBA,null,null
53,0.3986,null,null
54,THUIR03AbClu 0.1665 ICTNETDV10R2 0.3222 CWIcIA2t5b1,null,null
55,0.3487,null,null
56,Average,null,null
57,0.1842 Average,null,null
58,0.2645 Average,null,null
59,0.4265,null,null
60,Variance,null,null
61,0.0003 Variance,null,null
62,0.0024 Variance,null,null
63,0.0098,null,null
64,2. SEVERAL METHODS FOR RESULT,null,null
65,DIVERSIFICATION,null,null
66,"As aforementioned, weight assignment is a key issue for",null,null
67,"the linear combination method. In this section, we look at",null,null
68,"different ways of dealing with this issue. Firstly, we may",null,null
69,consider the performance of the retrieval system in question,null,null
70,"and its similarity with other retrieval systems separately, so",null,null
71,that we may obtain two types of weights. Then a combi-,null,null
72,nation of these two types of weights can be used to fuse,null,null
73,results. Note that performance and dissimilarity are two in-,null,null
74,"dependent factors. On the one hand, performance of a result",null,null
75,concerns the ranking positions of relevant documents and,null,null
76,how diversified those relevant documents are in the ranked,null,null
77,"list of documents; on the other hand, the dissimilarity of two",null,null
78,or more results is concerned with how different the ranking,null,null
79,positions of the same documents are in two or more indi-,null,null
80,"vidual results, making no distinction between relevant and",null,null
81,non-relevant documents.,null,null
82,Suppose there are a group of information retrieval sys-,null,null
83,"tems ir1, ir2,...,irt, with some training data available for us",null,null
84,to measure their performance and the dissimilarity between,null,null
85,"them. We further assume that their performances are p1,",null,null
86,"p2,..., pt, respectively, as measured by a given metric (e.g.,",null,null
87,"ERR-IA@20), so that we may then assign the value of a function of pi (such as pi, pi2, and so on) to wi for (1  i  t),",null,null
88,as the performance-related weight of iri.,null,null
89,Different approaches are possible for calculating the dis-,null,null
90,similarity (or similarity) between two results. One approach,null,null
91,is to refer to each result as a set of documents and calculate,null,null
92,the overlap rate between two results (sets). It is also possi-,null,null
93,ble to calculate the correlation (such as Spearman's ranking,null,null
94,coefficient or Kendall's tau coefficient) of two ranked lists of,null,null
95,documents. If all the documents in the two results are as-,null,null
96,"sociated with proper scoring information, then score-based",null,null
97,methods such as the Euclidean distance or city block dis-,null,null
98,tance can be used. In the following we discuss two different,null,null
99,ways of doing it.,null,null
100,Let us consider the top-n documents in all component re-,null,null
101,"sults. Suppose that document dij, in result ri, appears or is referred to in cij of the other t - 1 results, then all n",null,null
102,top-ranked documents of ri are referred in all other results,null,null
103,"refi ,",null,null
104,"n j,1",null,null
105,cij,null,null
106,times.,null,null
107,"For each document dij, the maxi-",null,null
108,"mum times it can appear in the other t - 1 results is t - 1,",null,null
109,This fact can be used to define the dissimilarity of ri to other,null,null
110,results as,null,null
111,"1 disi , n",null,null
112,n,null,null
113,(t - 1 - cij ) t-1,null,null
114,(1),null,null
115,"j,1",null,null
116,disi are always in the range of 0 and 1. We may define disi or a function of them as the dissimilarity-related weights. Methods using this definition are referred to as reference-based methods later in this paper. One advantage of using such methods for dissimilarity is that we can obtain the weights for component systems by considering all the documents of them together.,null,null
117,"An alternative of calculating the dissimilarity between results is to compare documents' ranking difference for each pair of them. Let us consider the n top-ranked documents in both results rA and rB. Suppose that m (m  n) documents appear in both rA and rB, and (n - m) of them appear in only one of them. For those n - m documents that only appear in one of the results, we simply assume that they occupy the places from rank n + 1 to rank 2n - m whilst retaining the same relative orders in the other result. Thus we can calculate the average rank difference of all the documents in both results and use it to measure the dissimilarity of rA and rB. To summarize, we have",null,null
118,"v(rA, rB)",null,null
119,",",null,null
120,1,null,null
121,di,null,null
122,{,null,null
123,rA,null,null
124,di,null,null
125,rB,null,null
126,|pA(di) - pB(di)|,null,null
127,n,null,null
128,m,null,null
129,"i,""1,2,...,m""",null,null
130,+,null,null
131,dirAdi/rB |pA(di) - (n + i)| n-m,null,null
132,"i,""1,2,...,n-m""",null,null
133,+,null,null
134,di / rA di rB,null,null
135,|pB,null,null
136,(di) n,null,null
137,- -,null,null
138,(n m,null,null
139,+,null,null
140,i)|,null,null
141,},null,null
142,(2),null,null
143,"i,""1,2,...,n-m""",null,null
144,Here pA(di) and pB(di) denote the rank position of di in,null,null
145,"rA and rB, respectively.",null,null
146,1 n,null,null
147,is,null,null
148,the,null,null
149,normalization,null,null
150,coefficient,null,null
151,"which guarantees that v(rA, rB) is in the range of 0 and 1.",null,null
152,"Based on Equation 2, the dissimilarity weight of ri (1  i ",null,null
153,t) is defined as,null,null
154,1,null,null
155,"j,i",null,null
156,"disi , t - 1",null,null
157,"v(ri, rj )",null,null
158,(3),null,null
159,"j,""1,2,...,t""",null,null
160,Methods that use this definition are referred to as ranking difference based methods. No matter how we obtain,null,null
161,828,null,null
162,"the weights for dissimilarity, we may combine dissimilarity-",null,null
163,"related weights with performance-related weights. Different options, such as pi disi, pi2 disi, pi disi2, and so on, might be used to obtain weight wi for information retrieval system iri. At the fusion stage, the linear combination method uses the following equation to calculate scores:",null,null
164,t,null,null
165,"g(d) , wi  si(d)",null,null
166,(4),null,null
167,"i,1",null,null
168,where g(d) is the global score that document d obtains dur-,null,null
169,"ing data fusion, si(d) is the (normalized) score that document d obtains from information retrieval system iri (1  i  t), and wi is the weight assigned to system iri. All the documents can be ranked according to the global scores they",null,null
170,obtain.,null,null
171,3. EXPERIMENTS,null,null
172,"In the 3 successive years from 2009 to 2011, the web track of TREC used the collection of ""ClueWeb09"". The collection consists of roughly 1 billion web pages crawled from the Web.",null,null
173,"3 groups of results are chosen for the experiment. They are 8 top-ranked results 1 (measured by ERR-IA@20) submitted to the diversity task in the TREC 2009, 2010, and 2011 web track. The information about all the selected results is summarized in Table 1.",null,null
174,"As we know, it is harder to get improvement over better component results through data fusion. However, the purpose of the experiments is going to see if we can obtain even better results by fusing a number of top-ranked results submitted.",null,null
175,"In the 3 aforementioned groups of results, the 2009 group has the lowest average effectiveness (.1842), the lowest best effectiveness (.2144), and the smallest variance (.0003); the 2011 group has the highest average effectiveness (.4265), the highest best effectiveness (.5284), and the largest variance (.0098); for all three metrics the 2010 group comes second (average is .2645, best is .3356, variance is .0024). We will see that the effectiveness of the fused results is affected by these factors.",null,null
176,"In the 2009 group, M SRAACSF [4] is the best performer (ERR IA@20: 0.2144). This run is submitted by Microsoft Research Asia in Beijing. For a given query, sub-topics are mined from different sources including anchor texts, search result clusters, and web sites at which search results are located; and documents are ranked by considering both relevance and diversity of mined sub-topics.",null,null
177,"In the 2010 group, T HU IR10DvN ov is the best among all 8 runs selected for the experiment. Its performance is 0.3355 when measured by ERR IA@20. Two other runs msrsv3div and uwgym (baseline) are slightly better than T HU IR10DvN ov. The technical details of this run are not known because we cannot find the corresponding report for this. We speculate that this run is submitted by a research group in Tsinghua University.",null,null
178,"In the 2011 group, uogT rA45N mx2 [7] is the best performer (ERR IA@20: 0.5284). This run is submitted by the",null,null
179,1msrsv3div and uwgym in 2010 and UDCombine2 in 2011 are not chosen because they include much fewer documents than the others and using them would cause problems in calculating weights for the linear combination method and in the fusion process as well.,null,null
180,Table 2: Performance (measured by ERR-IA@20) of a group of data fusion methods (p denotes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column),null,null
181,Group best result p p2,null,null
182,dis  p(Eq.1) dis  p2(Eq.1) dis2  p(Eq.1),null,null
183,dis  p(Eq.3) dis  p2(Eq.3) dis2  p(Eq.3),null,null
184,2009 0.2144 0.2544 (18.66%) 0.2499 (16.56%) 0.2552 (19.03%) 0.2492 (16.23%) 0.2548 (18.84%) 0.2553 (19.08%) 0.2503 (16.74%) 0.2534 (18.19%),null,null
185,2010 0.3355 0.3567 (6.32%) 0.3684 (9.81%) 0.3548 (5.75%) 0.3705 (10.43%) 0.3533 (5.31%) 0.3531 (5.25%) 0.3658 (9.03%) 0.3562 (6.17%),null,null
186,2011 0.5284 0.5398 (2.16%) 0.5343 (1.12%) 0.5398 (2.16%) 0.5355 (1.34%) 0.5410 (2.38%) 0.5388 (1.97%) 0.5347 (1.19%) 0.5330 (0.87%),null,null
187,Ave. 0.3551 0.3836 9.05% 0.3842 9.16% 0.3833 8.60% 0.3851 9.33% 0.3830 8.84% 0.3824 8.77% 0.3836 8.99% 0.3809 8.41%,null,null
188,IR research group at Glasgow University. It uses Terrier,null,null
189,with a component xQuAD for search result diversification.,null,null
190,The primary idea is to find useful information of sub-topics,null,null
191,by sending the initial query to three commercial web search,null,null
192,engines.,null,null
193,"In each year group, 50 queries are divided into 5 groups:",null,null
194,"1-10, 11-20, 21-30, 31-40, and 41-50. 4 arbitrary groups",null,null
195,"of them are used as training queries, while the remaining",null,null
196,one group is used for fusion test. This is referred to as the,null,null
197,five-fold cross validation method in statistics and machine,null,null
198,learning [5]. Every result is evaluated using ERR-IA@20,null,null
199,over training queries to obtain the performance weight pi.,null,null
200,"On the other hand, either Equation 1 or Equations 2 and 3",null,null
201,are used with training data to obtain disi for the dissimilar-,null,null
202,"ity weight. After that, we try 5 different ways of combining the weights: pi, pi2, pi  disi, pi2  disi, and pi  disi2.",null,null
203,"In order to fuse component results by linear combination,",null,null
204,reliable scores are required for all the documents required.,null,null
205,"In this study, we use the reciprocal function [2]. According",null,null
206,"to [2], the reciprocal function is very good for converting",null,null
207,"rankings into scores. For any resultant list r ,"" <d1, d2,...,""",null,null
208,"dn>,",null,null
209,a,null,null
210,score,null,null
211,of,null,null
212,1 i+60,null,null
213,is,null,null
214,assigned,null,null
215,to,null,null
216,document,null,null
217,di,null,null
218,at,null,null
219,rank,null,null
220,i.,null,null
221,Experimental results are shown in Tables 2 and 3. Two,null,null
222,"metrics, ERR-IA@20 and -nDCG@20, are used to evaluate",null,null
223,all the fusion methods. The best component result is used,null,null
224,as the baseline. When calculating dissimilarity weights by,null,null
225,"reference based method, or Equation 1, we use the top 100",null,null
226,documents in all component results. We have also tried,null,null
227,"some other options, including the top 50 and the top 200,",null,null
228,though the experimental results are omitted here since they,null,null
229,are so similar to what we observed for the top 100. When,null,null
230,"using rank difference based method, or Equations 2 and 3,",null,null
231,829,null,null
232,Table 3: Performance (measured by -nDCG@20) of a group of data fusion methods (p denotes performance-related weight and dis denotes dissimilarity-related weight; dis is calculated using either Equation 1 or Equation 3; the figures in parentheses indicate the improvement rate of each method over the best component result; the figures in bold indicate the highest value in the column),null,null
233,Group best result p p2,null,null
234,dis  p(Eq.1) dis  p2(Eq.1) dis2  p(Eq.1),null,null
235,dis  p(Eq.3) dis  p2(Eq.3) dis2  p(Eq.3),null,null
236,2009 0.3653 0.4130 (13.06%) 0.4108 (12.46%) 0.4141 (13.36%) 0.4100 (12.24%) 0.4150 (13.61%) 0.4138 (13.28%) 0.4108 (12.46%) 0.4126 (12.95%),null,null
237,2010 0.4745 0.5071 (6.87%) 0.5226 (10.14%) 0.5057 (6.58%) 0.5241 (10.45%) 0.5045 (6.32%) 0.5054 (6.51%) 0.5202 (9.63%) 0.5084 (7.14%),null,null
238,2011 0.6298 0.6510 (3.37%) 0.6468 (2.70%) 0.6513 (3.41%) 0.6477 (2.84%) 0.6522 (3.56%) 0.6506 (3.30%) 0.6478 (3.00%) 0.6488 (3.17%),null,null
239,Ave. 0.4869 0.5237 7.77% 0.5267 8.43% 0.5237 7.78% 0.5273 8.51% 0.5239 7.83% 0.5233 7.70% 0.5263 8.36% 0.5233 7.75%,null,null
240,"to calculate dissimilarity weights, we use all the documents in each component result.",null,null
241,"From Tables 2 and 3, we can see that all the data fusion methods involved perform better than the best component result. However, improvement rates vary from one year group to another. For all the data fusion methods involved, the largest improvement of over 10% occurs in the 2009 year group, which is followed by the 2010 year group with improvement between 5% and 11%, while the smallest improvement of less than 4% occurs in the 2011 year group. According to [8], the target variable of performance improvement of the fused result over the best component result is affected by a few factors. Among other factors, the variance of performance of all the component results and the performance of the best component result (see Table 1) have negative effect on the target variable. This can partially explain what we observe: all data fusion methods do the best in the 2009 data set, the worst in the 2011 data set, and the medium in the 2010 data set.",null,null
242,"Intuitively, such a phenomenon is understandable. If a component result is very good and a large percentage of relevant documents in multiple categories are retrieved and top-ranked, then it must be very difficult to make any further improvements over this result; on the other hand, if some of the results are much poorer than the others, then it is very difficult for the fused result to outperform the best component result. Anyway, in all 3 data sets, all of the fused results exhibit improvements over the best component result.",null,null
243,"If we compare performance-related weights to combined weights, it is not always the case that combined weights can achieve greater improvement. However, if we examine the greatest improvement in each case, it always happens",null,null
244,"when some form of combined weights is used. On average over three year groups, dis  p2(Eq.1) performs the best no matter if ERR-IA@20 or -nDCG@20 is used for evaluation. This suggests that dis  p2(Eq.1) is a very good option for the combined weight.",null,null
245,4. CONCLUSIONS,null,null
246,"In this short paper we have reported our investigation on the search result diversification problem via data fusion. Especially we focus on the linear combination method. Two options of calculating dissimilarity weights and several options of combining performance-related weights and dissimilarity-related weights have been proposed. Experiments with 3 groups of results submitted to the TREC web diversity task show that all the data fusion methods perform well and better than the best component result. Among those methods proposed, a combined weight of square performance and dissimilarity (calculated by comparing ranking difference of pair-wise results) outperforms the others on average.",null,null
247,"In summary, the experiments demonstrate that data fusion is still a useful technique for performance improvement when addressing search result diversification.",null,null
248,5. REFERENCES,null,null
249,"[1] E. Aktolga and J. Allan. Sentiment diversification with different biases. In Proceedings of the 36th Annual International ACM SIGIR Conference, pages 593­602, Dublin, Ireland, July 2013.",null,null
250,"[2] G. V. Cormack, C. L. A. Clarke, and S. Bu¨ttcher. Reciprocal rank fusion outperforms Condorcet and individual rank learning mthods. In Proceedings of the 32nd Annual International ACM SIGIR Conference, pages 758­759, Boston, MA, USA, July 2009.",null,null
251,"[3] V. Dang and W. B. Croft. Term level search result diversification. In Proceedings of the 36th Annual International ACM SIGIR Conference, pages 603­612, Dublin, Ireland, July 2013.",null,null
252,"[4] Z. Dou, K. Chen, R. Song, Y. Ma, S. Shi, and J. Wen. Microsoft Research Asia at the web track of TREC 2009. In Proceedings of The Eighteenth Text REtrieval Conference, Gaithersburg, Maryland, USA, November 2009.",null,null
253,"[5] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (Volumn 2), pages 1137­1145, Montreal, Canada, August 1995.",null,null
254,"[6] J. H. Lee. Analysis of multiple evidence combination. In Proceedings of the 20th Annual International ACM SIGIR Conference, pages 267­275, Philadelphia, Pennsylvania, USA, July 1997.",null,null
255,"[7] R. McCreadie, C. Macdonald, R. Santos, and I. Ounis. University of Glasgow at TREC 2011: Experiments with terrier in crowdsourcing, microblog, and web tracks. In Proceedings of The Twentieth Text REtrieval Conference, Gaithersburg, Maryland, USA, November 2011.",null,null
256,"[8] S. Wu and S. McClean. Performance prediction of data fusion for information retrieval. Information Processing & Management, 42(4):899­915, July 2006.",null,null
257,830,null,null
258,,null,null

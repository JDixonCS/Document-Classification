,sentence,label,data
0,Utilizing Relevance Feedback in Fusion-Based Retrieval,null,null
1,Ella Rabinovich,null,null
2,"IBM Research Labs, Haifa",null,null
3,Israel,null,null
4,ellak@il.ibm.com,null,null
5,Ofri Rom,null,null
6,Thomson Reuters,null,null
7,ofri.rom@ thomsonreuters.com,null,null
8,Oren Kurland,null,null
9,"Faculty of IE&M, Technion",null,null
10,Israel,null,null
11,kurland@ie.technion.ac.il,null,null
12,ABSTRACT,null,null
13,"Work on using relevance feedback for retrieval has focused on the single retrieved list setting. That is, an initial document list is retrieved in response to the query and feedback for the most highly ranked documents is used to perform a second search. We address a setting wherein the list for which feedback is provided results from fusing several intermediate retrieved lists. Accordingly, we devise methods that utilize the feedback while exploiting the special characteristics of the fusion setting. Specifically, the feedback serves two different, yet complementary, purposes. The first is to directly rank the pool of documents in the intermediate lists. The second is to estimate the effectiveness of the intermediate lists for improved re-fusion. In addition, we present a meta fusion method that uses the feedback for these two purposes simultaneously. Empirical evaluation demonstrates the merits of our approach. As a case in point, the retrieval performance is substantially better than that of using the relevance feedback as in the single list setting. The performance also substantially transcends that of a previously proposed approach to utilizing relevance feedback in fusion-based retrieval.",null,null
14,"Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval]: Relevance feedback, Retrieval models",null,null
15,"Keywords: fusion, relevance feedback",null,null
16,1. INTRODUCTION,null,null
17,"It is a well-established fact that using (positive) relevance feedback in ad hoc retrieval helps to substantially improve retrieval effectiveness [29, 30]. Usually, relevance feedback, if available, is provided for the documents most highly ranked by some initial search performed in response to the query. Then, information induced from the feedback documents is used for a second retrieval.",null,null
18,"Here we address the challenge of utilizing relevance feedback in fusion-based retrieval [12]. That is, the document list for which feedback is provided results from merging (fusing) several intermediate lists that were produced using dif-",null,null
19,"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR'14, July 6­11, 2014, Gold Coast, Queensland, Australia. Copyright 2014 ACM 978-1-4503-2257-7/14/07 ...$15.00. http://dx.doi.org/10.1145/2600428.2609573.",null,null
20,"ferent retrievals from the same corpus in response to the query. Thus, our main goal is to address the questions of whether and how relevance feedback can be effectively utilized while accounting for the special characteristics of the fusion setting. Furthermore, we opt for an approach that is not committed to a specific retrieval framework (e.g., vector space, language modeling) as we operate in a fusion setting.",null,null
21,"We present retrieval methods that use the relevance feedback for two different, yet complementary, purposes. The first is to directly rank the pool of documents in the intermediate lists. The second is to estimate the effectiveness of the intermediate lists so as to re-fuse them. Several listeffectiveness estimates are proposed based on the observation that this is essentially an evaluation task with minimal (incomplete) relevance judgments. To simultaneously leverage both purposes just described, we present a meta fusion method. The method fuses the direct ranking induced over the pool with that created by the re-fusion of the intermediate lists.",null,null
22,"Empirical evaluation performed with TREC corpora attests to the merits of our approach. Specifically, the retrieval performance is substantially better than that attained by treating the relevance feedback as in the standard single retrieved list setting; i.e., disregarding the fact that the list for which feedback is provided results from fusing intermediate lists. Furthermore, our approach is substantially more effective than the only previously proposed method to utilizing relevance feedback in fusion-based retrieval [4].",null,null
23,2. RELATED WORK,null,null
24,"There is a large body of work on using relevance feedback for retrieval [29, 30, 9]. In contrast to our work, the fusionbased retrieval setting has not been specifically addressed, with the exception of some work which is discussed below. As already noted, we show that leveraging the special characteristics of the fusion setting when utilizing the relevance feedback is of much merit.",null,null
25,"There has been much work on fusing document lists that were retrieved from the same corpus in response to a query (e.g., [7, 14, 21, 22, 28, 36, 2, 12, 3, 10, 26, 39, 5, 23, 32, 6, 31, 38]). However, to the best of our knowledge, there has only been a single report on using relevance feedback in a fusion-based retrieval setting [4]. A fusion method served for active feedback, that is, selecting documents to be judged by the user through an iterative process. We do not address the active feedback task. However, as a fusion approach that utilizes a feedback set of documents was proposed, specifically, to estimate list effectiveness for re-fusion [4], we use",null,null
26,313,null,null
27,"this work for reference comparison. The list-effectiveness estimate proposed in this work [4] is different than those we present here. Furthermore, in contrast to our approach, the feedback was not used to directly rank documents in the intermediate lists. Accordingly, the integration of the two purposes that the feedback can be used for -- direct ranking and list-effectiveness estimation for re-fusion -- is not proposed in contrast to our work. In Section 4.2.4 we empirically show that our approach is substantially more effective in utilizing the relevance feedback.",null,null
28,"In work on fusion, the intermediate retrieved lists (or segments thereof) were weighted (i) uniformly (which is the most common case), (ii) using unsupervised approaches [39, 38], or (iii) based on past performance of the retrieval method as determined using a train set of queries [7, 2, 23, 32, 6, 31]. In Section 4.2.5 we demonstrate the relative merits of using our proposed list-effectiveness estimates which utilize relevance feedback to weight the lists.",null,null
29,3. RETRIEVAL FRAMEWORK,null,null
30,"Let q and D be a query and a corpus of documents, re-",null,null
31,"spectively. Suppose that the documents lists L1, . . . , Lm,",null,null
32,"each composed of n documents, were retrieved from D in",null,null
33,"response to q by m different retrievals. These can be based,",null,null
34,"for example, on different query representations, document",null,null
35,"representations, and ranking functions [12]. In what follows",null,null
36,we use the notation d  L to indicate that document d is in,null,null
37,the list L; SL(d) is d's normalized (non negative) score in,null,null
38,"L; if d  L, we set SL(d) d,ef 0. Details regarding the score",null,null
39,normalization approach are provided in Section 4.1.,null,null
40,The goal of fusion methods is to merge the lists into one,null,null
41,"result list, Lfuse. For example, the CombSUM method [14]",null,null
42,scores,null,null
43,d,null,null
44,by,null,null
45,SCombSUM (d),null,null
46,"d,ef",null,null
47,P,null,null
48,Li :dLi,null,null
49,SLi (d).,null,null
50,"Thus,",null,null
51,docu-,null,null
52,ments that are ranked high in many of the lists are rewarded.,null,null
53,"The CombMNZ method [14, 22] further rewards documents",null,null
54,"that appear in many of the lists: SCombMNZ (d) d,ef |{Li :",null,null
55,d  Li}|SCombSUM (d).,null,null
56,3.1 Using Relevance Feedback,null,null
57,"As in previous work on using relevance feedback [30, 9], we assume that a user scans the list she is presented with, Lfuse in our case, top down until she encounters r documents that are relevant to the information need she expressed using the query q. We use Rq[r](Lfuse) (henceforth Rq) to denote the set of these relevant documents, and F (Lfuse) to denote the set of all documents she scanned and therefore judged; i.e., F (Lfuse) \ Rq are the non-relevant documents the user encountered. We note that the user need not be aware of the fact that the result list she scans (Lfuse) was produced by fusing intermediate lists. Our goal is to devise retrieval methods that use information induced from F (Lfuse).",null,null
58,"Several of the approaches that we present use some query expansion method. The method takes as input several documents -- the relevant ones (Rq) in our case -- the query q, and some corpus-based term statistics. The method then produces a query model, Mq;Rq , that can be used to rank documents; the score assigned to document d is S(d; Mq;Rq ). For example, in Rocchio's method [29], the query model is a tf.idf-based vector where cosine is often used as the scoring function. In the mixture model [41] and relevance model [20] approaches, the query model is a unigram language model; documents are ranked by the cross entropy between",null,null
59,the query model and their language models. In Section 4.1 we provide the details of the query expansion method used for experiments. We hasten to point out that our methods are not committed to a specific query expansion approach.,null,null
60,"Following standard practice in work on utilizing relevance feedback [30], we can use Mq;Rq to rank the entire corpus; CorpusRank denotes this approach. We also study a method, FusedListReRank, which uses Mq;Rq to re-rank Lfuse rather than to rank the entire corpus.",null,null
61,"However, CorpusRank and FusedListReRank do not account for the special characteristics of the retrieval setting we address here. That is, the fact that the list Lfuse, for which relevance feedback is provided, results from fusing intermediate retrieved lists. The methods we present below do exploit this fact.",null,null
62,3.2 Exploiting the Special Characteristics of the Fusion Setting,null,null
63,We start with the simple observation that fusion-based re-,null,null
64,"trieval is a two steps procedure. First, a pool of documents,",null,null
65,Dpool,null,null
66,"d,ef",null,null
67,S,null,null
68,i,null,null
69,Li,null,null
70,",",null,null
71,is,null,null
72,created,null,null
73,by,null,null
74,the,null,null
75,different,null,null
76,retrievals.,null,null
77,"Then,",null,null
78,"list-specific properties of documents in the pool (e.g., docu-",null,null
79,ment scores in the lists) are used to rank the pool. Accord-,null,null
80,"ingly, we devise methods that rank Dpool using the relevance",null,null
81,"feedback. Following common practice in work on fusion [12],",null,null
82,documents not in the pool are assigned with a zero score in,null,null
83,all methods.,null,null
84,"Our first method, PoolRank, ranks Dpool using the query",null,null
85,model Mq;Rq which was induced from the relevant documents and the query:,null,null
86,"SP oolRank(d) d,ef S(d; Mq;Rq ).",null,null
87,(1),null,null
88,"The pool contains documents ""considered"" relevant by retrievals which were based only on the query. Thus, using information induced from relevant documents to re-rank it can potentially improve retrieval effectiveness.",null,null
89,"Our second method, ReFuse, uses the relevance feedback to weight the intermediate lists Li so as to re-fuse them. The development of ReFuse is guided by probabilistic retrieval principles as described next.",null,null
90,The goal of probabilistic retrieval methods is to estimate the probability p(d|Iq) that d is relevant to the information need Iq expressed by q. Relevance is determined with respect to the information need rather than with respect to the query which is a signal about the information need.,null,null
91,"To estimate p(d|Iq) using information induced from the intermediate lists, and inspired by some recent work on predicting query-performance for fusion [25], we can write",null,null
92,"p(d|Iq) ,"" X p(d|Iq, Li)p(Li|Iq),""",null,null
93,(2),null,null
94,Li,null,null
95,if we assume that p(Li|Iq) is a probability distribution over the intermediate lists.,null,null
96,"Following common practice in work on aspect and mixture models [16], we first make the assumption that d is independent of Iq given Li. Then, we get the estimate",null,null
97,"p^(d|Iq) d,""ef X p^(d|Li)p^(Li|Iq),""",null,null
98,(3),null,null
99,Li,null,null
100,"where p^ denotes an estimate for p. That is, the probability that d is relevant to Iq is estimated based on estimates for",null,null
101,314,null,null
102,"its association with the intermediate lists (p^(d|Li)); the impact of a list Li depends on its effectiveness (relevance) with respect to Iq (p^(Li|Iq)). Thus, Equation 3 reflects a transition from using q as an explicit signal about Iq to using the intermediate lists that were retrieved in response to q as a pseudo signal about Iq.",null,null
103,"The mixture model just described is the conceptual basis of linear fusion methods [2].1 For example, the CombSUM method [14], which was mentioned above, is a linear fusion method: normalized document scores in the lists serve for list-association measures (p^(d|Li)); and, uniform list-effectiveness estimates (p^(Li|Iq)) are used.",null,null
104,"In the absence of relevance feedback, estimating the effectiveness of the intermediate lists is a difficult task [2, 39]. However, here, some relevance feedback is provided, although for the fused list Lfuse and not for the intermediate lists. In Section 3.2.1 we present a few measures that use this relevance feedback to estimate the effectiveness of the intermediate lists. Using these estimates in the linear fusion framework results in our ReFuse method that scores d ( Dpool) by:",null,null
105,"SReF use(d) d,ef X wIq (Li; F (Lfuse))SLi (d); (4)",null,null
106,Li :dLi,null,null
107,wIq (Li; F (Lfuse)) is Li's estimated effectiveness with re-,null,null
108,"spect to Iq; and, d's score in list association measure as",null,null
109,Li in,null,null
110,",CSoLmib(dS)U, sMer.2ves",null,null
111,as,null,null
112,the,null,null
113,document-,null,null
114,Meta Fusion. The relevance feedback served two different,null,null
115,"purposes in the PoolRank and ReFuse methods; namely, to directly rank the pool and to estimate the effectiveness of the intermediate lists so as to re-fuse them, respectively. We next integrate these approaches.",null,null
116,"Instead of making the independence assumption that led from Equation 2 to Equation 3 -- i.e., that d is independent of Iq given Li -- we estimate p(d|Iq, Li) using p^^(d|Iq)+(1- )p^(d|Li); p^^(d|Iq) is some estimate for p(d|Iq);  is a free parameter. Using the estimate for p(d|Iq, Li) in Equation 2, applying some algebra, and using the assumption from above that p(Li|Iq) is a distribution over the intermediate lists, we derive a new estimate for p(d|Iq):",null,null
117,p^^(d|Iq) + (1 - ) X p^(d|Li)p^(Li|Iq).,null,null
118,(5),null,null
119,Li,null,null
120,"This estimate ""backs off"" from some direct estimate (p^^(d|Iq))",null,null
121,to the mixture-based estimate from Equation 3.,null,null
122,"For the direct estimate, p^^(d|Iq), we use the normalized",null,null
123,score assigned by PoolRank to d:,null,null
124,. SP oolRank (d),null,null
125,P d Dpool,null,null
126,SP oolRank (d),null,null
127,This is a probability distribution over the entire corpus as,null,null
128,"documents not in Dpool are assigned with a 0 score. The resultant estimate is based on using the query model Mq;Rq , which was induced from the relevant documents and used in",null,null
129,"PoolRank for ranking, as a representation for Iq.",null,null
130,"Then, following Equation 5 we interpolate the direct esti-",null,null
131,mate just described with the normalized score assigned by,null,null
132,"1We write ""conceptual"" to emphasize the fact that the actual measures that are used in work on linear fusion methods are not necessarily valid probability distributions [2]. 2Document d is associated only with lists in which it ap-",null,null
133,"pears, because SLi (d) d,ef 0 if d  Li.",null,null
134,ReFuse to d:,null,null
135,. SReF use(d),null,null
136,P d Dpool,null,null
137,SReF use(d),null,null
138,This is a distribution,null,null
139,over the entire corpus which is based on the linear mixture,null,null
140,"model described in Equation 3. Accordingly, we arrive to our MetaFuse method that scores d by:3",null,null
141,SMetaF use(d),null,null
142,"d,ef",null,null
143,P,null,null
144,d,null,null
145,SP oolRank(d) S Dpool P oolRank,null,null
146,(d,null,null
147,),null,null
148,(6),null,null
149,+,null,null
150,(1,null,null
151,-,null,null
152,),null,null
153,SReF use(d),null,null
154,P,null,null
155,d Dpool,null,null
156,SReF use(d),null,null
157,.,null,null
158,"The name MetaFuse is coined based on the following observation. The PoolRank method induces a ranking over Dpool using Mq;Rq . This ranking is essentially linearly fused, using Equation 6, with a second ranking of Dpool which was created by ReFuse. The ReFuse ranking is by itself the result of linearly fusing the rankings of the intermediate lists L1, . . . , Lm using list-effectiveness estimates in Equation 4.",null,null
159,"For  , 1 and  ,"" 0, MetaFuse becomes PoolRank and ReFuse, respectively. More generally, the higher the value of , the more weight is put on the ranking produced by using the query model that was induced from the relevant documents. Lower values of  result in more emphasis on the re-fusion of the intermediate lists that are weighted using information induced from the feedback documents.""",null,null
160,3.2.1 Estimating list effectiveness,null,null
161,"We now turn to address the task of estimating the effectiveness of an intermediate retrieved list Li with respect to Iq using the feedback document set, F (Lfuse). The estimate, denoted wIq (Li; F (Lfuse)), is used in Equation 4 for the ReFuse method, which is then used in MetaFuse in Equation 6.",null,null
162,"It is important to note that documents in Li, even if are relevant, might not be among those for which relevance feedback is available. Recall that the relevance feedback was provided for the documents most highly ranked in the fused list Lfuse. Thus, the challenge is estimating retrieval effectiveness with incomplete (minimal) relevance judgments.",null,null
163,"The first list-effectiveness estimate that we consider is the standard average precision measure, AP. AP is computed using the feedback set, F (Lfuse), and treats unjudged documents -- i.e., those not in F (Lfuse) -- as non relevant.",null,null
164,"To address the scarcity of relevance judgments in our setting, we also consider infAP [40]. This is a state-of-the-art retrieval effectiveness measure that was designed as an approximation to average precision (AP); specifically, for cases of incomplete relevance judgments. An important difference between infAP and AP is that the former differentiates between unjudged and non-relevant documents and the latter does not. We compute infAP based on the feedback set F (Lfuse). Documents not in F (Lfuse) are treated as unjudged. For comparison purposes, we also consider a variant of infAP, termed infAPonlyRel, which is computed using only the set of relevant documents, Rq; i.e., all other documents are treated as unjudged.4",null,null
165,"3Experiments -- actual numbers are omitted as they convey no additional insight -- showed that using a weighted geometric mean of the normalized scores of PoolRank and ReFuse yields performance that is very similar to that of using the weighted arithmetic mean from Equation 6. 4We note that in contrast to the case for the original setting in which infAP was introduced [40], here infAP is not",null,null
166,315,null,null
167,"It is worth noting at this point that we could have potentially used information induced from the non-relevant documents (F (Lfuse) \ Rq) to also improve the query model, Mq;Rq , which is used in PoolRank. However, utilizing negative feedback to improve retrieval performance has long been known as an extremely hard task [17, 29, 37] with the potential merits confined to very difficult queries [37, 18].",null,null
168,"The development of the third and fourth list-effectiveness estimates, referred to as Kendall- and Pearson, is inspired by work on query-performance prediction [33]. The query model, Mq;Rq , which was induced from the relevant documents, is used to re-rank Li; the re-ranked list is denoted ReRank(Li). As Li was not created using relevance feedback, it is presumably less effective than ReRank(Li). Consequently, the latter can serve as a positive reference comparison to the former for estimating effectiveness [33]; i.e., we assume that the more similar the rankings of Li and ReRank(Li), the higher the effectiveness of Li. We use Kendall's- and Pearson's correlation coefficient to measure the similarity between Li and ReRank(Li). While Kendall's- is based on document ranks, Pearson's correlation coefficient depends on document scores. As the values assigned by the two measures are in [-1, 1] we shift and re-scale them to [0, 1] for consistency with the estimates described above. The resultant values serve as Li's effectiveness estimates.",null,null
169,4. EVALUATION,null,null
170,4.1 Experimental Setup,null,null
171,"The methods we presented in Section 3 utilize relevance feedback. The feedback is provided for the documents at the top ranks of a result list which is produced by fusing several intermediate retrieved lists. Thus, to evaluate the effectiveness of the methods, we use runs submitted to different tracks of TREC as the intermediate lists.",null,null
172,"Table 1 provides the details of the TREC tracks used for experiments. We used the ad hoc tracks of TREC3, TREC7 and TREC8, and the Web track of TREC9. These were also used in prior work on fusion [3, 26, 4, 5, 23, 32, 19]. We randomly sample 5 runs from all those submitted to a track and which contain at least 100 documents as a result for every query. (We refer to these runs as candidates in Table 1.) We use 30 such random samples; each sample constitutes an experimental setting. The retrieval effectiveness numbers that we report are averages over these 30 samples (settings). The n ,"" 100 most highly ranked documents in a run per query serve for an intermediate retrieved list.5 Retrieval scores in the lists are min-max normalized [22, 27, 24]. Then, the five lists for each query are fused using the CombMNZ method [14, 22], which was described in Section 3. CombMNZ is a highly effective fusion approach that commonly serves as a baseline in work on fusion [3, 26, 23, 32, 19].""",null,null
173,"To create the set of feedback documents, F (Lfuse), we scan the list Lfuse which was produced by CombMNZ top down until r relevant documents are accumulated or the",null,null
174,"necessarily a statistical estimate for AP. Yet, the empirical results presented in Section 4.2 attest to the merits of using infAP as a list effectiveness estimate in our setting. 5It was argued, based on the ""skimming effect"" principle [2], and empirically demonstrated [34, 35, 8, 19], that there are clear merits in fusing relatively short lists.",null,null
175,TREC,null,null
176,TREC3 TREC7 TREC8 TREC9,null,null
177,Data,null,null
178,Disks 1&2 Disks 4&5-CR Disks 4&5-CR WT10G,null,null
179,Queries,null,null
180,151-200 351-400 401-450 451-500,null,null
181,# of candidate runs,null,null
182,38 86 113 64,null,null
183,Table 1: TREC data used for experiments. Candidate runs are those that contain at least 100 results for every query.,null,null
184,"end of the list is reached.6 F (Lfuse) is the set of documents scanned. Documents in F (Lfuse) are either relevant or non-relevant as determined by using TREC's qrels files. (Documents in F (Lfuse) with no judgement in the qrels file are considered not relevant as is standard.) The set of relevant documents in F (Lfuse) was denoted Rq in Section 3.1. We present results for r  {1, . . . , 5}.",null,null
185,Titles of TREC topics serve for queries. Tokenization and Porter stemming were applied to queries and documents using the Lemur toolkit7 which was used for experiments.,null,null
186,"Query expansion method. As described in Section 3, a",null,null
187,few of our approaches use some query expansion method.,null,null
188,The method produces a query model Mq;Rq that can be used for ranking. For experiments we use the effective rele-,null,null
189,"vance model number 3 (RM3) [20, 1] as the query expansion",null,null
190,"method. When using relevant documents to construct RM3,",null,null
191,"which is a unigram language model, the probability assigned",null,null
192,to,null,null
193,term,null,null
194,w,null,null
195,is,null,null
196,"[20,",null,null
197,1]:,null,null
198,(1 - )p(w|q) +,null,null
199, r,null,null
200,P,null,null
201,dRq,null,null
202,p(w|d);,null,null
203,is,null,null
204,a free parameter; p(w|q) is the maximum likelihood esti-,null,null
205,mate of term w with respect to q; p(w|d) is the probability,null,null
206,assigned to w by a Jelinek-Mercer smoothed unigram lan-,null,null
207,guage model induced from document d with smoothing pa-,null,null
208,"rameter  [20];  , 0.1 following previous recommendations",null,null
209,[20]. It is common practice [1] to clip the relevance model,null,null
210,by using only the  terms to which it assigns the highest,null,null
211,probability; these terms' probabilities are sum-normalized,null,null
212,to yield a valid probability distribution. To rank documents,null,null
213,"using RM3, we use the cross entropy between the relevance",null,null
214,model and their (smoothed) unigram language models [20].,null,null
215,"To this end, we use Dirichlet smoothed document language",null,null
216,models with the smoothing parameter set to 1000 [41]. We,null,null
217,note that RM3 interpolates the query language model with,null,null
218,a linear mixture of the language models of the given relevant,null,null
219,"documents. Therefore, it is the language-model-based ana-",null,null
220,"logue of Rochhio's method [29, 20]. The latter interpolates",null,null
221,"the query vector with the centroid (i.e., linear mixture) of",null,null
222,the vectors of relevant documents.,null,null
223,"Evaluation metrics. To evaluate retrieval effectiveness, we",null,null
224,"use the mean average precision at cutoff n ,"" 100 (MAP@100), which is the size of the intermediate lists that are fused, and the precision of the top 10 document (p@10). Statistically significant differences of performance are determined using the two-tailed paired t-test computed at a 95% confidence level based on the average performance per query over the 30 samples of runs.""",null,null
225,"6In the very few cases (specifically, 0.8% of the cases for TREC9) that fusing a sample of 5 lists (for a specific query) results in a list with no relevant documents, we omit this sample. 7www.lemurproject.org",null,null
226,316,null,null
227,CombMNZ,null,null
228,CorpusRank FusedListReRank PoolRank ReFuse MetaFuse,null,null
229,TREC3,null,null
230,"r,1",null,null
231,"r,2",null,null
232,MAP p@10 MAP p@10,null,null
233,20.3 69.3,null,null
234,22.6 72.8 22.5 76.1c 24.1cf 75.1c 20.2cpf 68.4cpf 24.9cpfr 76.2cr,null,null
235,20.3 69.3,null,null
236,24.9 77.5 23.4c 78.8 25.6cf 78.2 20.8cpf 70.3cpf 26.0cpfr 79.0r,null,null
237,TREC7,null,null
238,"r,1",null,null
239,"r,2",null,null
240,MAP p@10 MAP p@10,null,null
241,21.1 53.7,null,null
242,22.8 56.9 22.8 57.3 23.4c 57.1 22.1 56.5 25.1cpfr 59.3cpfr,null,null
243,21.1 53.7,null,null
244,25.1 60.9 24.2 60.6 25.4f 60.8 22.7fp 57.7 27.3cpfr 63.3cpfr,null,null
245,TREC8,null,null
246,"r,1",null,null
247,"r,2",null,null
248,MAP p@10 MAP p@10,null,null
249,23.9 54.3,null,null
250,25.2 58.8 25.2 59.6 25.9c 59.4 25.4 56.9f 27.8cpfr 60.9r,null,null
251,23.9 54.3,null,null
252,27.2 61.6 26.5 61.6 27.6f 61.8 26.0p 58.2cpf 29.2cpfr 63.8cpfr,null,null
253,TREC9,null,null
254,"r,1",null,null
255,"r,2",null,null
256,MAP p@10 MAP p@10,null,null
257,19.8 35.2,null,null
258,28.0 42.2 27.3 43.6 28.8f 43.9 22.6cpf 38.1fp 29.3crf 44.8r,null,null
259,19.8 35.2,null,null
260,33.2 47.8 31.5c 47.8 33.5f 48.4 23.4cpf 39.5cpf 33.6fr 48.5r,null,null
261,"Table 2: Main result table. Boldface marks the best result in a column. Italics marks performance that is statistically significantly better than that of CombMNZ. 'c', 'f ', 'p' and 'r' mark statistically significant differences with CorpusRank, FusedListReRank, PoolRank and ReFuse, respectively.",null,null
262,"An important question in evaluating the retrieval effectiveness of methods that utilize relevance feedback is whether to consider for evaluation the given set of relevant documents [9]. To compare our methods to each other with respect to various aspects (e.g., the number relevant documents), we consider the given relevant documents in the evaluation presented in Sections 4.2.1, 4.2.2 and 4.2.3. We note that our methods do not directly position the given relevant documents at the highest ranks of the final result list. Thus, this evaluation also enables to study their ability to rank high the given relevant documents. When comparing our methods with reference comparisons in Sections 4.2.4 and 4.2.5, we use a residual corpus approach for evaluation [9]: the given relevant documents (and in Section 4.2.4 also the given non-relevant documents) are not considered in the evaluation. For completeness, we re-visit the comparison of our methods using the residual corpus evaluation approach in Section 4.2.6. We note that in all cases the final result list that is evaluated contains n , 100 documents.",null,null
263,Free-parameter values. Our methods that use RM3 de-,null,null
264,"pend on its free parameters. The MetaFuse method has an additional free parameter, . The free-parameter values, in all methods, are set using leave-one-out cross validation performed over queries. MAP serves as the optimization criterion for the train set. The parameter  in MetaFuse is set to values in {0, 0.1, . . . , 1}. The values of  and , which are used by RM3, are selected from {0.5, 0.8, 0.9, 1} and {10, 50, 75}, respectively8.",null,null
265,4.2 Experimental results,null,null
266,4.2.1 Main result,null,null
267,"We show in Section 4.2.3 that the infAP list-effectiveness estimate is more effective than the others we consider. Hence, unless otherwise stated, the results presented for ReFuse and MetaFuse are based on using infAP. In practical scenarios, very few relevant documents are available as feedback (if at all). Thus, in Table 2 we present results for r  {1, 2}. Below we study the effect of further varying r.",null,null
268,"We see in Table 2 that, as is expected, all methods that use the relevance feedback are more effective -- almost always to a statistically significant degree ­ than CombMNZ which",null,null
269,"8The optimal value of  as determined over the train sets of queries was in most cases  0.9; that of  was in most cases in {50, 75}. As a case in point for performance sensitivity analysis, setting  , 0.9 and  ,"" 50 in MetaFuse often results in MAP performance very similar to that attained using leave-one-out cross validation, except for TREC9.""",null,null
270,does not utilize it. In Section 4.2.6 we show that the same finding holds with the residual-corpus evaluation approach.,null,null
271,"We can also see in Table 2 that CorpusRank, which ranks the entire corpus using the relevance model, is somewhat more effective in terms of MAP than FusedListReRank; FusedListReRank uses the relevance model to re-rank the list produced by CombMNZ (Lfuse). In terms of p@10, the reverse often holds. However, the performance differences are statistically significant in very few cases.",null,null
272,We now turn to analyze the performance of the methods that leverage the special characteristics of the fusion setting when exploiting the relevance feedback. PoolRank uses the relevance model to rank the pool of documents in the intermediate retrieved lists. Its performance is better in most relevant comparisons (track × evaluation measure) than that of CorpusRank and FusedListReRank; in quite a few cases the performance improvements are statistically significant.,null,null
273,"The ReFuse method uses the relevance feedback to estimate the effectiveness of the intermediate lists so as to refuse them. Its performance is worse than that of the CorpusRank, FusedListReRank and PoolRank methods; these use the relevance model induced from the relevant documents to directly rank documents. Yet, we see that the performance of ReFuse is superior to that of CombMNZ in a vast majority of the relevant comparisons. CombMNZ uses uniform list-effectiveness estimates, while ReFuse utilizes infAP (here) with the given feedback to estimate list effectiveness.",null,null
274,"The most effective method in Table 2 is MetaFuse. Specifically, MetaFuse always outperforms -- often substantially and to a statistically significant degree -- CorpusRank and FusedListReRank. These two methods use the relevance feedback as in the single retrieved list case; i.e., they do not leverage the fact that feedback is provided for a list which results from fusion. MetaFuse does leverage this fact by integrating PoolRank and ReFuse. Thus, we conclude that there is much merit in exploiting the special characteristics of the fusion setting when using the relevance feedback.",null,null
275,"We also see in Table 2 that although PoolRank is always more effective than ReFuse, MetaFuse that integrates the two yields performance that transcends that of both; the improvements are statistically significant in most relevant comparisons. This finding attests to the fact that the two purposes for which the relevance feedback is used in MetaFuse -- direct ranking of documents and list-effectiveness estimation for re-fusion -- are complementary.",null,null
276,Varying the number of relevant documents. Figure 1 ex-,null,null
277,"hibits the effect of varying r, the number of given relevant documents, on the performance of the various methods.",null,null
278,317,null,null
279,MAP MAP MAP MAP,null,null
280,0.32,null,null
281,CombMNZ,null,null
282,CorpusRank,null,null
283,0.3,null,null
284,FusedListReRank PoolRank,null,null
285,ReFuse,null,null
286,0.28,null,null
287,MetaFuse,null,null
288,TREC3,null,null
289,CombMNZ,null,null
290,0.34,null,null
291,CorpusRank,null,null
292,FusedListReRank,null,null
293,0.32,null,null
294,PoolRank ReFuse,null,null
295,MetaFuse,null,null
296,0.3,null,null
297,TREC7,null,null
298,0.34,null,null
299,CombMNZ,null,null
300,CorpusRank,null,null
301,FusedListReRank,null,null
302,0.32,null,null
303,PoolRank ReFuse,null,null
304,MetaFuse,null,null
305,0.3,null,null
306,TREC8,null,null
307,0.45,null,null
308,CombMNZ,null,null
309,CorpusRank,null,null
310,0.4,null,null
311,FusedListReRank PoolRank,null,null
312,ReFuse,null,null
313,MetaFuse,null,null
314,0.35,null,null
315,TREC9,null,null
316,0.26,null,null
317,0.28,null,null
318,0.28,null,null
319,0.3,null,null
320,0.24,null,null
321,0.26,null,null
322,0.22,null,null
323,0.24,null,null
324,0.26,null,null
325,0.25,null,null
326,0.2,null,null
327,0.22,null,null
328,0.24,null,null
329,0.2,null,null
330,1,null,null
331,2,null,null
332,3,null,null
333,4,null,null
334,5,null,null
335,1,null,null
336,2,null,null
337,3,null,null
338,4,null,null
339,5,null,null
340,1,null,null
341,2,null,null
342,3,null,null
343,4,null,null
344,5,null,null
345,1,null,null
346,2,null,null
347,3,null,null
348,4,null,null
349,5,null,null
350,r,null,null
351,r,null,null
352,r,null,null
353,r,null,null
354,Figure 1: The effect of varying the number of relevant documents (r) on MAP performance. Note: figures are not to the same scale.,null,null
355,"Our first observation is that the performance of all methods increases with increasing values of r. The CorpusRank, FusedListReRank and PoolRank methods directly rank documents using the relevance model which is constructed from the r relevant documents. Thus, these methods benefit from the improved quality of the relevance model when constructed from more relevant documents. The ReFuse method uses the feedback (relevant and non-relevant documents) with infAP to estimate the effectiveness of the intermediate lists so as to re-fuse them. Thus, increasing r, which means more feedback, results in more reliable estimates. Naturally then, the effectiveness of MetaFuse, which integrates PoolRank and ReFuse, improves with increasing values of r.",null,null
356,"We also see in Figure 1 that the relative performance patterns of the methods across the values of r are consistent with those exhibited in Table 2 for r  {1, 2}. For example, PoolRank is in many cases more effective than CorpusRank and FusedListReRank. This provides further support to the relative merits of using the relevance model to rank the pool of documents in the intermediate lists with respect to using it to (re-)rank the entire corpus or the final fused list.",null,null
357,"We observe in Figure 1 that although PoolRank is consistently more effective than ReFuse, MetaFuse which integrates them is in most cases more effective than both. We note that most of the improvements over ReFuse, and many of the improvements over PoolRank, specifically for TREC7 and TREC8, were found to be statistically significant.",null,null
358,"With increasing values of r the performance difference between MetaFuse and PoolRank become smaller. (For r > 2 in TREC9 the performance is almost identical.) The reason is that the performance differences between PoolRank and ReFuse become larger when increasing r. Indeed, the relative performance improvements of PoolRank with increasing values of r are larger than those of ReFuse. This finding means that the improvements in the quality of the relevance model used in PoolRank have relatively more impact on the resultant retrieval effectiveness than those of the list effectiveness estimates used in ReFuse for re-fusion.",null,null
359,4.2.2 Balancing the roles of the relevance feedback,null,null
360,"The parameter  in MetaFuse controls the balance between the two purposes (roles) that the relevance feedback serves. Higher values of  result in more reliance on using the relevance model in PoolRank to rank the documents in the pool; for  ,"" 1, MetaFuse becomes PoolRank. Lower values of  result in more reliance on using the relevance feedback to estimate list effectiveness for re-fusion in ReFuse; specifi-""",null,null
361,"cally,  ,"" 0 amounts to ReFuse. In Figure 2 we present the effect of varying  on the performance of MetaFuse. The other free parameters of the methods (i.e., those of the relevance model) are set using leave-one-out cross validation.""",null,null
362,"We see in Figure 2 that for  > 0 MetaFuse outperforms ReFuse (MetaFuse with  , 0). The reason is that MetaFuse integrates ReFuse with PoolRank (MetaFuse with  ,"" 1) and the latter outperforms the former. Yet, often, the best performance of MetaFuse is attained for  < 1. This finding attests to the merit in using the relevance feedback simultaneously to directly rank documents in the pool (PoolRank) and to estimate list effectiveness for re-fusion (ReFuse).""",null,null
363,A general trend observed in Figure 2 is that the optimal value of  rises when increasing the number of relevant documents (r). This finding can be explained by the fact that the performance of PoolRank improves to a much larger extent than that of ReFuse when increasing the value of r as discussed above for Figure 1.,null,null
364,4.2.3 Comparing list-effectiveness estimates,null,null
365,One of the two purposes for which relevance feedback is,null,null
366,"used in our methods -- specifically, in ReFuse (Equation 4)",null,null
367,that is used by MetaFuse -- is to estimate the effectiveness,null,null
368,"of the intermediate lists. Insofar, infAP was used in the eval-",null,null
369,uation. We now turn to study the performance of ReFuse,null,null
370,when using all the list-effectiveness estimates proposed in,null,null
371,Section 3.2.1. Recall that we are provided with relevance,null,null
372,feedback for the set F (Lfuse) of the documents most highly,null,null
373,"ranked in the fused list Lfuse. Thus, for many documents",null,null
374,in the intermediate lists there are no relevance judgments.,null,null
375,"For comparison, we study two additional list-effectiveness",null,null
376,estimates which are applied in ReFuse and which do not use,null,null
377,the relevance feedback. The first is uniform that considers,null,null
378,all intermediate lists to be of the same effectiveness. Us-,null,null
379,ing ReFuse with uniform amounts to the CombSUM fusion,null,null
380,method [14] mentioned in Section 3.,null,null
381,"The second estimate is overlap [38]. For a list Li, the",null,null
382,overlap,null,null
383,is,null,null
384,defined,null,null
385,as,null,null
386,P,null,null
387,"j,i",null,null
388,2|Li T Lj | |Li|+|Lj |,null,null
389,--,null,null
390,"i.e.,",null,null
391,the,null,null
392,normalized,null,null
393,sum of its overlap with all other lists. Conceptually similar,null,null
394,list-effectiveness estimates were used in other work on fusion,null,null
395,[39] and in work on evaluating the effectiveness of search,null,null
396,systems without relevance judgments [34]. The premise is,null,null
397,that inter-list similarity (in terms of shared documents) in-,null,null
398,dicates effective retrieval. The performance of ReFuse using,null,null
399,the list-effectiveness estimates is presented in Figure 3.,null,null
400,318,null,null
401,MAP MAP MAP MAP,null,null
402,MAP MAP MAP MAP,null,null
403,TREC3 0.28,null,null
404,0.27,null,null
405,0.26,null,null
406,0.25,null,null
407,0.24,null,null
408,0.23,null,null
409,0.22,null,null
410,"r,1",null,null
411,"r,2",null,null
412,0.21,null,null
413,"r,3 r,4",null,null
414,0.2,null,null
415,"r,5",null,null
416,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
417,TREC7,null,null
418,0.3,null,null
419,0.28,null,null
420,0.26,null,null
421,0.24,null,null
422,"r,1",null,null
423,0.22,null,null
424,"r,2 r,3",null,null
425,"r,4",null,null
426,0.2,null,null
427,"r,5",null,null
428,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
429,TREC8,null,null
430,0.31,null,null
431,0.3,null,null
432,0.29,null,null
433,0.28,null,null
434,0.27,null,null
435,0.26,null,null
436,0.25,null,null
437,"r,1",null,null
438,"r,2",null,null
439,0.24,null,null
440,"r,3",null,null
441,"r,4",null,null
442,0.23,null,null
443,"r,5",null,null
444,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 ,null,null
445,TREC9,null,null
446,0.38,null,null
447,0.36,null,null
448,0.34,null,null
449,0.32,null,null
450,0.3,null,null
451,0.28,null,null
452,0.26,null,null
453,"r,1 r,2",null,null
454,"r,3",null,null
455,0.24,null,null
456,"r,4",null,null
457,"r,5",null,null
458,0.22,null,null
459,0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1,null,null
460,"Figure 2: The performance of MetaFuse for different values of the number of relevant documents (r) when varying .  , 1 amounts to PoolRank and  , 0 amounts to ReFuse. Note: figures are not to the same scale.",null,null
461,TREC3,null,null
462,TREC7,null,null
463,TREC8,null,null
464,TREC9,null,null
465,0.23,null,null
466,AP,null,null
467,infAP 0.225 infAPonlyRel,null,null
468,Kendall,null,null
469,0.22,null,null
470,Pearson,null,null
471,uniform,null,null
472,0.215,null,null
473,overlap,null,null
474,0.21,null,null
475,0.205,null,null
476,0.2,null,null
477,AP,null,null
478,0.25,null,null
479,infAP,null,null
480,infAPonlyRel,null,null
481,Kendall,null,null
482,0.24,null,null
483,Pearson,null,null
484,uniform,null,null
485,overlap,null,null
486,0.23,null,null
487,0.22,null,null
488,0.29,null,null
489,AP infAP,null,null
490,infAPonlyRel,null,null
491,0.28,null,null
492,Kendall,null,null
493,Pearson,null,null
494,uniform,null,null
495,0.27,null,null
496,overlap,null,null
497,0.26,null,null
498,0.25,null,null
499,0.28,null,null
500,AP,null,null
501,infAP,null,null
502,infAPonlyRel,null,null
503,0.26,null,null
504,Kendall,null,null
505,Pearson,null,null
506,uniform,null,null
507,overlap,null,null
508,0.24,null,null
509,0.22,null,null
510,0.195,null,null
511,0.21,null,null
512,0.24,null,null
513,0.2,null,null
514,0.19,null,null
515,0.23,null,null
516,0.2,null,null
517,1,null,null
518,2,null,null
519,3,null,null
520,4,null,null
521,5,null,null
522,1,null,null
523,2,null,null
524,3,null,null
525,4,null,null
526,5,null,null
527,1,null,null
528,2,null,null
529,3,null,null
530,4,null,null
531,5,null,null
532,1,null,null
533,2,null,null
534,3,null,null
535,4,null,null
536,5,null,null
537,r,null,null
538,r,null,null
539,r,null,null
540,r,null,null
541,Figure 3: The performance of ReFuse with various list-effectiveness estimates. Note: figures are not to the same scale,null,null
542,"Our first observation based on Figure 3 is that all listeffectiveness estimates are almost always more effective than the uniform estimate. We also see that the overlap measure, which does not use the relevance feedback, is more effective than the Pearson and Kendall- estimates that do use it. However, overlap is often substantially less effective than the other estimates that use the relevance feedback, namely, infAP, infAPonlyRel and AP.",null,null
543,"We see in Figure 3 that, in general, the performance of ReFuse when employed with the list-effectiveness estimates that use the relevance feedback tends to increase when the number of relevant documents (r) increases. The increase for Pearson and Kendall- is, however, extremely small. Recall that these two estimates, in contrast to infAP, infAPonlyRel and AP, do not estimate list effectiveness directly; rather, via the comparison of the list with its re-ranked version attained by using the relevance model. Thus, it may come as no surprise that using Pearson and Kendall- in ReFuse yields performance that is inferior (often substantially) to that of using infAP, infAPonlyRel and AP.",null,null
544,"It is evident in Figure 3 that using in ReFuse the infAP estimate, which was used insofar in the experiments reported above, results in the best overall performance. infAP is the only estimate that directly exploits the non-relevant documents in F (Lfuse) by differentiating them from unjudged documents -- i.e., documents not in F (Lfuse). Using infAPonlyRel, which treats non-relevant documents as unjudged, and AP which treats non-relevant and unjudged documents the same, result in performance that is often somewhat inferior to that of using infAP.",null,null
545,"With increased number of relevant documents (r) the performance of using AP becomes closer to that of using infAP. (For almost all values of r for TREC9 the performance is almost identical.) The reason is that AP becomes more robust when more relevant documents are available. In contrast, the performance of using infAPonlyRel with increased r becomes more inferior to that of using infAP, because infAPonlyRel treats the non-relevant documents as unjudged. Note that increased r is likely to result in increased number of non-relevant documents in F (Lfuse) by the virtue of the experimental setting described in Section 4.1.",null,null
546,4.2.4 Comparison with the Hedge method,null,null
547,"As noted in Section 2, there is a single previous report on using relevance feedback in the context of fusion-based retrieval [4]. TREC runs were fused using relevance judgments obtained through an iterative process of active relevance feedback based on the Hedge method [15]. Here, we use the approach as a reference comparison that fuses the intermediate lists using the feedback documents (F (Lfuse)).",null,null
548,The loss of document d in F (Lfuse) with respect to an intermediate list Li in which it appears is:,null,null
549,l(d; Li),null,null
550,"d,ef",null,null
551,1,null,null
552,(-1)rel(d),null,null
553,tmax,null,null
554,X,null,null
555,2,null,null
556,1; j,null,null
557,(7),null,null
558,"j,tk",null,null
559,"tk is the rank of d in Li; rel(d) is 1 if d is relevant and 0 if it is not; tmax ,"" |Dpool| is the size of the pool of documents in the intermediate lists. Then, Li's weight is defined as:""",null,null
560,wIq (Li; F (Lfuse)),null,null
561,"d,ef",null,null
562," , P d:dLi F",null,null
563,(Lf use ),null,null
564,l(d;Li ),null,null
565,(8),null,null
566,319,null,null
567,MAP MAP MAP MAP,null,null
568,TREC3,null,null
569,TREC7,null,null
570,TREC8,null,null
571,TREC9,null,null
572,0.24 0.22,null,null
573,0.2,null,null
574,0.23,null,null
575,0.21,null,null
576,0.24,null,null
577,0.18,null,null
578,0.22,null,null
579,0.2,null,null
580,0.22,null,null
581,0.16,null,null
582,0.21,null,null
583,0.19,null,null
584,0.14,null,null
585,0.2,null,null
586,0.18,null,null
587,0.2,null,null
588,0.19,null,null
589,0.17,null,null
590,0.12,null,null
591,0.18,null,null
592,0.17,null,null
593,Hedge ReFuse(Hedge),null,null
594,0.16,null,null
595,ReFuse PoolRank,null,null
596,0.15,null,null
597,MetaFuse,null,null
598,0.16 Hedge,null,null
599,0.15 ReFuse(Hedge) ReFuse,null,null
600,0.14,null,null
601,PoolRank,null,null
602,MetaFuse,null,null
603,0.13,null,null
604,0.18,null,null
605,Hedge,null,null
606,0.16,null,null
607,ReFuse(Hedge) ReFuse,null,null
608,PoolRank,null,null
609,MetaFuse,null,null
610,0.14,null,null
611,0.1,null,null
612,0.08,null,null
613,Hedge ReFuse(Hedge),null,null
614,ReFuse,null,null
615,0.06,null,null
616,PoolRank,null,null
617,MetaFuse,null,null
618,0.04,null,null
619,1,null,null
620,2,null,null
621,3,null,null
622,4,null,null
623,5,null,null
624,1,null,null
625,2,null,null
626,3,null,null
627,4,null,null
628,5,null,null
629,1,null,null
630,2,null,null
631,3,null,null
632,4,null,null
633,5,null,null
634,1,null,null
635,2,null,null
636,3,null,null
637,4,null,null
638,5,null,null
639,r,null,null
640,r,null,null
641,r,null,null
642,r,null,null
643,"Figure 4: Comparison with the Hedge method [4]. In contrast to the case in previous figures, a special residual corpus approach is used for evaluation wherein the given relevant and non-relevant documents are not considered in the evaluation. Note: figures are not to the same scale.",null,null
644,"where  is a free parameter with a value in {0.1, . . . , 0.9}. If",null,null
645,"Li  F (Lfuse) ,  then wIq (Li; F (Lfuse)) d,ef 0. The fusion score of d ( Dpool) is its average weighted loss",null,null
646,"over all lists, computed as if it is non-relevant (rel(d) d,ef 0):",null,null
647,SHedge(d),null,null
648,"d,ef",null,null
649,X l(d; Li)wIq (Li;,null,null
650,F (Lfuse)).,null,null
651,(9),null,null
652,Li,null,null
653,"If d  Li, then l(d; Li) is set in Equation 9 to the average loss of all the documents in Dpool \ Li, where these are treated as if they are not relevant and positioned below the documents in Li (i.e., at ranks 101 to tmax , |Dpool|).",null,null
654,"The original implementation of Hedge as an iterative active feedback approach positioned the given feedback documents at the highest ranks of the final result list [4]. Such direct positioning calls for a residual-corpus approach for evaluation [9]. Specifically, here, the documents in F (Lfuse) are removed from all evaluated rankings and the residual rankings serve for evaluation.",null,null
655,"Our ReFuse and MetaFuse methods use the infAP listeffectiveness estimate which was shown above to be the most effective among those considered. In addition, we also study an instance of our ReFuse method, ReFuse(Hedge), in which the list-effectiveness estimate is that defined in Equation 8 and used by Hedge. The parameter  used by Hedge and ReFuse(Hedge) is set using leave-one-out cross validation performed over the queries in a track. Recall that the free-parameter values of our methods are also set using leaveone-out cross validation. Figure 4 presents the results.",null,null
656,"We first see in Figure 4 that in contrast to Figures 1, 2 and 3 the curves are (almost always) monotonically decreasing with increasing values of r. The reason is that we use here a residual corpus approach for evaluation wherein all feedback documents are not considered for evaluation.",null,null
657,"Figure 4 shows that ReFuse outperforms ReFuse(Hedge). This means that infAP is a more effective list-effectiveness estimate than that used by Hedge (Equation 8). Furthermore, in all tracks, except for TREC3, ReFuse outperforms Hedge. Both are linear fusion methods that differ in the listweighting function, and in the scores assigned to documents; in ReFuse the retrieval scores of a document in the lists are used, and in Hedge Equation 9 is used.",null,null
658,"We also see in Figure 4 that PoolRank and MetaFuse substantially outperform Hedge. A vast majority of the performance improvements for PoolRank, and all of them for MetaFuse, were found to be statistically significant.",null,null
659,4.2.5 Comparison with past-performance-based estimation of list effectiveness,null,null
660,"ReFuse, and therefore MetaFuse, use the relevance feed-",null,null
661,back to estimate the effectiveness of the intermediate lists,null,null
662,so as to re-fuse them. We now turn to compare them with,null,null
663,methods that estimate list effectiveness based on the past,null,null
664,performance of the retrieval method used to create the list.,null,null
665,Past performance is determined using a train set of queries.,null,null
666,"In our experimental setting, the intermediate lists are de-",null,null
667,rived from TREC runs. Each run contains the results of a,null,null
668,retrieval method for the queries in a track. We used a leave-,null,null
669,one-out cross validation procedure across queries throughout,null,null
670,"the evaluation. Thus, all queries in a run except for the one",null,null
671,"at hand serve as the train set. Based on this set, the past",null,null
672,performance of the retrieval method is determined.,null,null
673,The Learning method [2] is a linear fusion method that,null,null
674,uses Equation 4 as is the case for ReFuse. For a list effec-,null,null
675,tiveness estimate it uses the MAP (mean average precision),null,null
676,of the run computed over the train set of queries.,null,null
677,ProbFuse is a highly effective fusion method [23]. It uses,null,null
678,the train query set (henceforth Q) to estimate the effective-,null,null
679,ness of segments of the intermediate lists retrieved for a test,null,null
680,query.,null,null
681,"Specifically, p^(dk|L) d,ef",null,null
682,P 1,null,null
683,"|Rk,q |",null,null
684,is |Q|,null,null
685,"qQ |Rk,q |+|Nk,q |",null,null
686,"an estimate for the probability that a document, denoted",null,null
687,"dk, in the k'th segment of an intermediate list L, will be relevant to some query. |Rk,q | and |Nk,q | are the number",null,null
688,"of documents marked as relevant and non-relevant, respectively, to a train query q; these documents appear in the",null,null
689,"k'th segment of an intermediate list in the train set which was retrieved for q in the same run that L belongs to. That is, Rk,q and Nk,q are documents retrieved in response to q",null,null
690,by the same retrieval method that produced L.,null,null
691,A document d in the pool (Dpool) of documents retrieved,null,null
692,for a test,null,null
693,query,null,null
694,is,null,null
695,scored by,null,null
696,SP robF use(d),null,null
697,"d,ef",null,null
698,P,null,null
699,Li,null,null
700,1 k,null,null
701,p^(dk,null,null
702,|Li,null,null
703,"),",null,null
704,where k is the number of the segment of Li in which d ap-,null,null
705,"pears; if d  Li then p^(dk|Li) d,ef 0 for all k. We use the",null,null
706,train set of queries (with MAP serving as the optimization,null,null
707,criterion) to also set the number of segments in the inter-,null,null
708,"mediate lists to a value in {2, 4, 6, 8, 10, 15, 20, 25, 30, 40, 50}.",null,null
709,(Recall that the lists are composed of 100 documents.),null,null
710,For an additional reference comparison we use CombMNZ,null,null
711,"[14, 22] which essentially utilizes uniform list-effectiveness",null,null
712,estimates. (Refer back to Section 3 for details.) As Learn-,null,null
713,"ing, ProbFuse and CombMNZ do not use the relevance feed-",null,null
714,320,null,null
715,MAP MAP MAP MAP,null,null
716,TREC3,null,null
717,TREC7,null,null
718,TREC8,null,null
719,TREC9,null,null
720,0.24,null,null
721,0.22,null,null
722,0.24,null,null
723,0.2,null,null
724,0.18,null,null
725,0.22,null,null
726,0.2,null,null
727,0.22,null,null
728,0.16,null,null
729,0.2,null,null
730,0.18,null,null
731,0.2,null,null
732,0.14,null,null
733,0.12,null,null
734,0.16,null,null
735,0.18,null,null
736,0.18,null,null
737,0.1,null,null
738,CombMNZ 0.16 Learning,null,null
739,ProbFuse ReFuse,null,null
740,0.14 MetaFuse,null,null
741,0.14 CombMNZ Learning ProbFuse,null,null
742,0.12 ReFuse MetaFuse,null,null
743,0.16 CombMNZ Learning,null,null
744,0.14 ProbFuse ReFuse,null,null
745,0.12 MetaFuse,null,null
746,0.08 CombMNZ Learning,null,null
747,0.06 ProbFuse ReFuse,null,null
748,0.04 MetaFuse,null,null
749,1,null,null
750,2,null,null
751,3,null,null
752,4,null,null
753,5,null,null
754,1,null,null
755,2,null,null
756,3,null,null
757,4,null,null
758,5,null,null
759,1,null,null
760,2,null,null
761,3,null,null
762,4,null,null
763,5,null,null
764,1,null,null
765,2,null,null
766,3,null,null
767,4,null,null
768,5,null,null
769,r,null,null
770,r,null,null
771,r,null,null
772,r,null,null
773,Figure 5: Comparison with fusion methods that estimate list effectiveness based on past performance of the retrieval method. A residual corpus evaluation approach is used where the given relevant documents are not considered in the evaluation. Note: figures are not to the same scale.,null,null
774,"back, specifically, the given relevant documents, we use a residual-corpus approach to evaluation [9]: the given relevant documents are removed from all evaluated rankings and the residual rankings serve for evaluation. Figure 5 presents the performance numbers. All curves are monotonically decreasing due to the residual-corpus evaluation approach.",null,null
775,"We see in Figure 5 that all methods outperform CombMNZ in almost all cases. We also see that in many cases ReFuse outperforms Learning. The main exceptions are for a small number of relevant documents (r) for TREC3 and TREC7. It is important to recall that the feedback is provided for the fused list (Lfuse) and not for the intermediate lists. Thus, the feedback available for the intermediate lists is scarce as was discussed in Section 3.2.1. Thus, we conclude that estimating list effectiveness with minimal relevance feedback can often result in better fusion performance than that of estimating list effectiveness using (much) information about the past performance of the retrieval method.",null,null
776,"We also see in Figure 5 that ProbFuse outperforms ReFuse and Learning. This comes as no surprise because ProbFuse uses estimates for the effectiveness of segments of the retrieved lists (and higher segments are rewarded) while ReFuse and Learning use estimates for the entire lists. Thus, a future direction is integrating the feedback-based list effectiveness estimates of ReFuse with the segment-based ones of ProbFuse. Yet, as shown in Figure 5, our MetaFuse method that utilizes the relevance feedback, but does not rely on past performance of the retrieval method to estimate list effectiveness, consistently outperforms ProbFuse; many of the improvements are substantial and statistically significant.",null,null
777,4.2.6 Further evaluation using the residual corpus approach,null,null
778,The main comparison of our approaches which was presented in Section 4.2.1 was based on considering the given relevant documents for the evaluation. Here we compare the methods' performance with the residual corpus approach [9]: the given relevant documents are removed from any ranking that is evaluated and the residual ranking serves for evaluation. Figure 6 presents the performance curves.,null,null
779,"We observe in Figure 6 the same relative performance patterns observed in Figure 1; the latter was based on evaluation that considers the given relevant documents. Specifically, (i) all methods that use the relevance feedback perform (almost always) better than CombMNZ which does not use it; we note that almost all of these improvements are sta-",null,null
780,"tistically significant; (ii) using the relevance model induced from the relevant documents to rank only the the pool of documents in the intermediate retrieved lists (PoolRank) is often more effective than using it to rank the entire corpus (CorpusRank) or to re-rank the final fused list (FusedListReRank); (iii) the methods that use the relevance model to directly rank documents (CorpusRank, FusedListReRank, PoolRank, MetaFuse) are more effective than the ReFuse method that uses the relevance feedback to estimate the effectiveness of the intermediate lists so as to re-fuse them; and, (iv) our MetaFuse method, which uses the relevance feedback to both directly rank documents and re-fuse the intermediate lists, is the most effective. Many of the improvements it posts over the other methods are substantial and were found to be statistically significant.",null,null
781,5. CONCLUSIONS AND FUTURE WORK,null,null
782,"We addressed the challenge of using relevance feedback in the fusion-based retrieval setting. That is, the feedback is provided for the documents most highly ranked in a list that results from fusing several intermediate retrieved lists.",null,null
783,"We devised methods that use the relevance feedback for two different, yet complementary, purposes. The first is directly ranking the pool of documents in the intermediate lists. The second is estimating the effectiveness of the intermediate lists so as to re-fuse them. We presented a meta fusion approach that uses the feedback for these two purposes simultaneously.",null,null
784,"Empirical evaluation demonstrated the merits of our approach. For example, the resultant retrieval performance is much better than that of using the feedback as in the single retrieved list setting; i.e., ignoring the fact that the feedback is provided for a list that results from fusion.",null,null
785,"We plan to explore additional list-effectiveness estimates to be used in our approach. Adapting our methods to use pseudo feedback, rather than true feedback, is another interesting future venue.",null,null
786,6. ACKNOWLEDGMENTS,null,null
787,We thank the reviewers for their comments. This paper is based upon work done in part while Ofri Rom was at the Technion. Oren Kurland's work is supported in part by the Israel Science Foundation under grant no. 433/12 and by Google's and Yahoo!'s faculty research awards.,null,null
788,321,null,null
789,MAP MAP MAP MAP,null,null
790,0.28 0.26 0.24 0.22 0.2 0.18 0.16,null,null
791,1,null,null
792,TREC3,null,null
793,CombMNZ,null,null
794,0.26,null,null
795,CorpusRank,null,null
796,FusedListReRank,null,null
797,0.24,null,null
798,PoolRank,null,null
799,ReFuse MetaFuse,null,null
800,0.22,null,null
801,0.2,null,null
802,0.18,null,null
803,0.16,null,null
804,0.14,null,null
805,0.12,null,null
806,2,null,null
807,3,null,null
808,4,null,null
809,5,null,null
810,1,null,null
811,r,null,null
812,TREC7,null,null
813,CombMNZ,null,null
814,CorpusRank,null,null
815,0.26,null,null
816,FusedListReRank,null,null
817,PoolRank ReFuse,null,null
818,0.24,null,null
819,MetaFuse,null,null
820,0.22,null,null
821,0.2,null,null
822,0.18,null,null
823,0.16,null,null
824,0.14,null,null
825,0.12,null,null
826,2,null,null
827,3,null,null
828,4,null,null
829,5,null,null
830,1,null,null
831,r,null,null
832,TREC8,null,null
833,CombMNZ CorpusRank,null,null
834,0.2,null,null
835,FusedListReRank,null,null
836,PoolRank,null,null
837,0.18,null,null
838,ReFuse,null,null
839,MetaFuse,null,null
840,0.16,null,null
841,0.14,null,null
842,0.12,null,null
843,0.1,null,null
844,0.08,null,null
845,0.06,null,null
846,2,null,null
847,3,null,null
848,4,null,null
849,5,null,null
850,1,null,null
851,r,null,null
852,TREC9,null,null
853,CombMNZ CorpusRank FusedListReRank,null,null
854,PoolRank ReFuse,null,null
855,MetaFuse,null,null
856,2,null,null
857,3,null,null
858,4,null,null
859,5,null,null
860,r,null,null
861,Figure 6: Performance comparison of our methods using the residual corpus evaluation approach where the given relevant documents are not considered for evaluation. Note: figures are not to the same scale.,null,null
862,7. REFERENCES,null,null
863,"[1] N. Abdul-Jaleel, J. Allan, W. B. Croft, F. Diaz, L. Larkey, X. Li, M. D. Smucker, and C. Wade. UMASS at TREC 2004 -- novelty and hard. In Proc. of TREC-13, pages 715­725, 2004.",null,null
864,"[2] C. C. V. ant Garrison W. Cottrell. Fusion via linear combination of scores. Information Retrieval, 1(3):151­173, 1999.",null,null
865,"[3] J. A. Aslam and M. Montague. Models for metasearch. In Proc. of SIGIR, pages 276­284, 2001.",null,null
866,"[4] J. A. Aslam, V. Pavlu, and R. Savell. A unified model for metasearch, pooling, and system evaluation. In Proc. of CIKM, pages 484­491, 2003.",null,null
867,"[5] J. A. Aslam, V. Pavlu, and E. Yilmaz. Measure-based metasearch. In Proc. of SIGIR, pages 571­572, 2005.",null,null
868,"[6] N. Balasubramanian and J. Allan. Learning to select rankers. In Proc. of SIGIR, pages 855­856, 2010.",null,null
869,"[7] B. T. Bartell, G. W. Cottrell, and R. K. Belew. Automatic combination of multiple ranked retrieval systems. In Proc. of SIGIR, pages 173­181, 1994.",null,null
870,"[8] S. M. Beitzel, E. C. Jensen, A. Chowdhury, O. Frieder, D. A. Grossman, and N. Goharian. Disproving the fusion hypothesis: An analysis of data fusion via effective information retrieval strategies. In Proc. of SAC, pages 823­827, 2003.",null,null
871,"[9] C. Buckley and S. Robertson. Relevance feedback track overview: TREC 2008. In Proc. of TREC-17, 2008.",null,null
872,"[10] A. Chowdhury, O. Frieder, D. A. Grossman, and M. C. McCabe. Analyses of multiple-evidence combinations for retrieval strategies. In Proc. of SIGIR, pages 394­395, 2001.",null,null
873,"[11] W. B. Croft, editor. Advances in Information Retrieval: Recent Research from the Center for Intelligent Information Retrieval. Number 7 in The Kluwer International Series on Information Retrieval. Kluwer, 2000.",null,null
874,"[12] W. B. Croft. Combining approaches to information retrieval. In Croft [11], chapter 1, pages 1­36.",null,null
875,"[13] W. B. Croft and J. Lafferty, editors. Language Modeling for Information Retrieval. Number 13 in Information Retrieval Book Series. Kluwer, 2003.",null,null
876,"[14] E. A. Fox and J. A. Shaw. Combination of multiple searches. In Proc. of TREC-2, 1994.",null,null
877,"[15] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computing Systems Science, 55(1):119­139, 1997.",null,null
878,"[16] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning from dyadic data. In Proc. of NIPS, pages 466­472, 1998.",null,null
879,"[17] E. Ide. New experiments in relevance feedback. In Salton G. (Ed.), The SMART Retrieval System (pp. 337-354). Englewood Cliffs, N. J.: Prentice-Hall, Inc, 1971.",null,null
880,"[18] M. Karimzadehgan and C. Zhai. Improving retrieval accuracy of difficult queries through generalizing negative document language models. In Proc. of CIKM, pages 27­36, 2011.",null,null
881,"[19] A. K. Kozorovitzky and O. Kurland. Cluster-based fusion of retrieved lists. In Proc. of SIGIR, pages 893­902, 2011.",null,null
882,"[20] V. Lavrenko and W. B. Croft. Relevance models in information retrieval. In Croft and Lafferty [13], pages 11­56.",null,null
883,"[21] J. H. Lee. Combining multiple evidence from different properties of weighting schemes. In Proc. of SIGIR, pages 180­188, 1995.",null,null
884,"[22] J. H. Lee. Analyses of multiple evidence combination. In Proc. of SIGIR, pages 267­276, 1997.",null,null
885,"[23] D. Lillis, F. Toolan, R. W. Collier, and J. Dunnion. Probfuse: a probabilistic approach to data fusion. In Proc. of SIGIR, pages 139­146, 2006.",null,null
886,"[24] I. Markov, A. Arampatzis, and F. Crestani. Unsupervised linear score normalization revisited. In Proc. of SIGIR, pages 1161­1162, 2012.",null,null
887,"[25] G. Markovits, A. Shtok, O. Kurland, and D. Carmel. Predicting query performance for fusion-based retrieval. In Proc. of CIKM, pages 813­822, 2012.",null,null
888,"[26] M. Montague and J. A. Aslam. Condorcet fusion for improved retrieval. In Proc. of CIKM, pages 538­548, 2002.",null,null
889,"[27] M. H. Montague and J. A. Aslam. Relevance score normalization for metasearch. In Proc. of CIKM, pages 427­433, 2001.",null,null
890,"[28] K. B. Ng and P. P. Kantor. An investigation of the preconditions for effective data fusion in information retrieval: A pilot study, 1998.",null,null
891,"[29] J. J. Rocchio. Relevance feedback in information retrieval. In G. Salton, editor, The SMART Retrieval System: Experiments in Automatic Document Processing, pages 313­323. Prentice Hall, 1971.",null,null
892,"[30] I. Ruthven and M. Lalmas. A survey on the use of relevance feedback for information access systems. Knowledge Engineering Review, 18(2):95­145, 2003.",null,null
893,"[31] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. Lambdamerge: merging the results of query reformulations. In Proc. of WSDM, pages 795­804, 2011.",null,null
894,"[32] M. Shokouhi. Segmentation of search engine results for effective data-fusion. In Proc. of ECIR, pages 185­197, 2007.",null,null
895,"[33] A. Shtok, O. Kurland, and D. Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proc. of SIGIR, 2010.",null,null
896,"[34] I. Soboroff, C. K. Nicholas, and P. Cahan. Ranking retrieval systems without relevance judgments. In Proc. of SIGIR, pages 66­73, 2001.",null,null
897,"[35] T. Tsikrika and M. Lalmas. Merging techniques for performing data fusion on the web. In Proc. of CIKM, pages 127­134, 2001.",null,null
898,"[36] C. C. Vogt and G. W. Cottrell. Predicting the performance of linearly combined IR systems. In Proc. of SIGIR, pages 190­196, 1998.",null,null
899,"[37] X. Wang, H. Fang, and C. Zhai. A study of methods for negative relevance feedback. In Proc. of SIGIR, pages 219­226, 2008.",null,null
900,"[38] S. Wu. Data fusion in information retrieval. Springer, 2012.",null,null
901,"[39] S. Wu and F. Crestani. Data fusion with estimated weights. In Proc. of CIKM, pages 648­651, 2002.",null,null
902,"[40] E. Yilmaz and J. A. Aslam. Estimating average precision with incomplete and imperfect judgments. In Proc. of CIKM, pages 102­111, 2006.",null,null
903,"[41] C. Zhai and J. D. Lafferty. A study of smoothing methods for language models applied to ad hoc information retrieval. In Proc. of SIGIR, pages 334­342, 2001.",null,null
904,322,null,null
905,,null,null

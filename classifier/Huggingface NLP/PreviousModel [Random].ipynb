{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+eH9Gu0vzx9S8+8ShoKI1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kilMTAdmrokQ"},"outputs":[],"source":["# Load data into Pandas dataframe\n","df = pd.read_csv(\"526892.csv\", encoding=\"cp1252\", index_col=0)\n","print(\"DataFrame:\", df)"]},{"cell_type":"code","source":["'''\n","from datasets import Dataset\n","from datasets import load_dataset\n","import random\n","\n","\n","def read_csv_file(file_path):\n","    return pd.read_csv(file_path, header=0, index_col=0, encoding=\"utf-8\", delimiter=\"\\t\", quoting=csv.QUOTE_NONE, on_bad_lines=\"skip\", engine='python')\n","\n","train_df = read_csv_file(\"COVID_Train_Set.csv\")\n","print(train_df)\n","test_df = read_csv_file(\"COVID_Test_Set.csv\")\n","print(test_df)\n","\n","data_files = {\"train\": train_df, \"test\": test_df}\n","print(data_files)\n","\n","covid_dataset = Dataset.from_dict(data_files)\n","print(covid_dataset)\n","\n","# Convert the train data list to a dataset object\n","covid_dataset[\"train\"] = Dataset.from_dict(covid_dataset[\"train\"])\n","\n","# Shuffle and select the first 1000 samples\n","#covid_sample = covid_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n","\n","# Convert the dataset object back to a list if necessary\n","#covid_sample = covid_dataset.to_pydict()[\"sentence\"]\n","'''"],"metadata":{"id":"VIY5vmOxs36y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","'''from datasets import load_dataset\n","from datasets import Dataset\n","import random\n","\n","data_files = {\"train\": \"COVID_Train_Set.csv\", \"test\": \"COVID_Test_Set.csv\"}\n","covid_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n","\n","# Convert the train data list to a dataset object\n","covid_dataset[\"train\"] = Dataset.from_dict(covid_dataset[\"train\"])\n","\n","# Shuffle and select the first 1000 samples\n","covid_sample = covid_dataset[\"train\"].shuffle(seed=42).select(range(1000))\n","\n","# Convert the dataset object back to a list if necessary\n","covid_sample = covid_sample.to_pydict()[\"text\"]\n","\n","# Peek at the first few examples\n","'''"],"metadata":{"id":"WsLWyP_ks5mx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''from datasets import Dataset\n","from datasets import load_dataset\n","import random\n","\n","def read_csv_file(file_path):\n","    return pd.read_csv(file_path, header=0, index_col=0, encoding=\"utf-8\", delimiter=\"\\t\", quoting=csv.QUOTE_NONE, on_bad_lines=\"skip\", engine='python')\n","\n","train_df = read_csv_file(\"COVID_Train_Set.csv\")\n","test_df = read_csv_file(\"COVID_Test_Set.csv\")\n","\n","data_files = {\"train\": train_df, \"test\": test_df}\n","\n","covid_dataset = Dataset.from_dict(data_files)\n","\n","# Convert the train data list to a dataset object\n","covid_dataset[0][\"train\"] = Dataset.from_dict(covid_dataset[0][\"train\"])\n","'''"],"metadata":{"id":"5HONrZOIs7Po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Process sentences as sequences\n","sentences = df.iloc[0:2, 0]\n","sequences = sentences.tolist()\n","print(sequences)"],"metadata":{"id":"sVsil5ZFs8jh","colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"status":"error","timestamp":1684802472713,"user_tz":240,"elapsed":206,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"373de98a-b71c-4796-e536-ba631e40554c"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-f9e2072ff566>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Process sentences as sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"code","source":["# Define tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","print(tokenizer)\n","\n","# Define tokenize function\n","def tokenize_function(example):\n","    return tokenizer(example[\"sentence\"], truncation=True)\n","\n","# Same as before\n","checkpoint = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n"],"metadata":{"id":"6uyXnAOLs9sB","executionInfo":{"status":"error","timestamp":1685147608864,"user_tz":240,"elapsed":194,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"9ff9ae92-b4b8-4f62-b123-a659c71fd9ee","colab":{"base_uri":"https://localhost:8080/","height":241}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-70ba82b39224>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Define tokenize function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"]}]},{"cell_type":"code","source":["# Use a CUDA device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n","\n","# Setup the model\n","model = AutoModelForSequenceClassification.from_pretrained(checkpoint).to(device)\n","#model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n","\n","model.classifier = nn.Linear(model.classifier.in_features, 1).to(device)\n","\n","# Freeze all parameters except the final layer\n","for param in model.parameters():\n","    param.requires_grad = False\n","for param in model.classifier.parameters():\n","    param.requires_grad = True\n","\n","# Print the modified model architecture\n","print(model)"],"metadata":{"id":"28SoDaf9s_P4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert label list to tensor and one-hot encode it\n","labels = torch.tensor(list(df.iloc[:5, 1]))\n","print(\"Labels:\", labels)\n","labels_one_hot = torch.nn.functional.one_hot(labels.long(), num_classes=2)\n","print(\"Labels One Hot\", labels_one_hot)\n","\n"],"metadata":{"id":"5oiUp40etAop","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1685146192793,"user_tz":240,"elapsed":5,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"cd798dde-f785-4881-c44f-7a8f87a78daa"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c32dfafad04e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert label list to tensor and one-hot encode it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels_one_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels One Hot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_one_hot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["# Convert data to Dataset format 1\n","train_data_dict = {\"sentence\": list(df.iloc[:5, 0]), \"label\": list(df.iloc[:5, 1])}\n","print(\"Train Data Dict:\", train_data_dict)\n","dev_data_dict = {\"sentence\": list(df.iloc[5:7, 0]), \"label\": list(df.iloc[5:7, 1])}\n","print(\"Dev Data Dict:\", train_data_dict)\n","test_data_dict = {\"sentence\": list(df.iloc[7:10, 0]), \"label\": list(df.iloc[7:10, 1])}\n","print(\"Test Data Dict:\", train_data_dict)\n","train_dataset = Dataset.from_dict(train_data_dict)\n","print(\"Train Dataset:\", train_dataset)\n","dev_dataset = Dataset.from_dict(dev_data_dict)\n","print(dev_dataset)\n","test_dataset = Dataset.from_dict(test_data_dict)\n","print(test_dataset)\n","\n","# Convert data to Dataset format 2\n","data_dict = {\"sentence\": list(df.iloc[0:2, 0]), \"label\": list(df.iloc[0:2, 1])}\n","raw_dataset = Dataset.from_dict(data_dict)\n","\n","# Create dictionary with \"sentence1\" and \"label\" as keys and the respective columns from the pandas dataframe as values\n","data_dict = {\"sentence\": list(df.iloc[0:10, 0]), \"label\": list(df.iloc[0:10, 1])}\n","\n","# Convert dictionary to Dataset object 2\n","dataset = Dataset.from_dict(data_dict)\n","print (dataset)"],"metadata":{"id":"z3svucm9tCFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize sentences\n","batch = tokenizer(list(df.iloc[0:10, 0]), padding=True, truncation=True, return_tensors=\"pt\").to(device)\n","print(batch)\n","\n","# Tokenize sentences\n","train_tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n","print(train_tokenized_dataset)\n","dev_tokenized_dataset = dev_dataset.map(tokenize_function, batched=True)\n","print(dev_tokenized_dataset)\n","test_tokenized_dataset = test_dataset.map(tokenize_function, batched=True)\n","print(test_tokenized_dataset)\n"],"metadata":{"id":"d2k_3srxtDFp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add labels to batch\n","batch[\"labels\"] = torch.tensor(df.iloc[0:10, 1].values).unsqueeze(1).to(device)  # Add unsqueeze here\n","#batch[\"labels\"] = torch.tensor(df.iloc[0:10, 1].values).to(device)  # Add unsqueeze here\n","\n","# This is a new one\n","batch[\"labels\"] = torch.tensor([1] * len(sequences))\n","print(batch[\"labels\"])"],"metadata":{"id":"V4wAmNz7tD-6","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1684800280558,"user_tz":240,"elapsed":227,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"ae6d2cb8-285e-4136-f649-faf564d6cf6f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-0a46f2ec668a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Add labels to batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add unsqueeze here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#batch[\"labels\"] = torch.tensor(df.iloc[0:10, 1].values).to(device)  # Add unsqueeze here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This is a new one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["# Set output format\n","columns_to_return = [\"input_ids\", \"attention_mask\", \"label\"]\n","train_tokenized_dataset.set_format(type='torch', columns=columns_to_return)\n","print(train_tokenized_dataset)\n","dev_tokenized_dataset.set_format(type='torch', columns=columns_to_return)\n","print(dev_tokenized_dataset)\n","test_tokenized_dataset.set_format(type='torch', columns=columns_to_return)\n","print(test_tokenized_dataset)"],"metadata":{"id":"S3bFxHDntE3h","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1684805302614,"user_tz":240,"elapsed":268,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"c919c90a-c850-42a7-cf28-84cb01a179f4"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-af27eef2d98f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set output format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcolumns_to_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_tokenized_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'torch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokenized_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdev_tokenized_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'torch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_to_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_tokenized_dataset' is not defined"]}]},{"cell_type":"code","source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n","print(data_collator)"],"metadata":{"id":"FU7z7uamtGN4","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"error","timestamp":1685376258477,"user_tz":240,"elapsed":8,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"87ac7f14-fdaa-4447-983a-fceffb0cdb41"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5420adb01ed0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'DataCollatorWithPadding' is not defined"]}]},{"cell_type":"code","source":["batch_size = 8\n","train_dataloader = torch.utils.data.DataLoader(train_tokenized_dataset, batch_size=batch_size, collate_fn=data_collator)\n","dev_dataloader = torch.utils.data.DataLoader(dev_tokenized_dataset, batch_size=batch_size, collate_fn=data_collator)\n","test_dataloader = torch.utils.data.DataLoader(test_tokenized_dataset, batch_size=batch_size, collate_fn=data_collator)"],"metadata":{"id":"auQnRiAQtIJb","colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"status":"error","timestamp":1685376244119,"user_tz":240,"elapsed":166,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"386512f1-a725-4a58-917f-e724cac4998a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-77caf0464cd2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdev_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_tokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_tokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["batch_size = 1\n","dataloader = torch.utils.data.DataLoader(tokenized_dataset, batch_size=batch_size)\n","print(dataloader)\n","for batch in dataloader:\n","    input_ids = batch[\"input_ids\"]\n","    print(input_ids)\n","    attention_mask = batch[\"attention_mask\"]\n","    print(attention_mask)\n","    label = batch[\"label\"]\n","    print(label)\n","    inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label.to(torch.long), \"targets\": targets}\n","    print(inputs)\n","    outputs = model(**inputs)\n","    print(outputs)\n","    loss = outputs.loss\n","    print(loss)\n","    logits = outputs.logits\n","    print(logits)"],"metadata":{"id":"lA6iWUH-tJp4","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"error","timestamp":1685378573475,"user_tz":240,"elapsed":8,"user":{"displayName":"José Dixon","userId":"01547349263215539853"}},"outputId":"a8aebb6d-a7fe-4576-db26-41c833d040f2"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-49a262a1c86c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","source":["optimizer = AdamW(model.parameters())\n","#output = model(**batch).logits.squeeze(1)  # Squeeze the output to get [batch_size]\n","output = model(**batch).logits  # Squeeze the output to get [batch_size]\n","loss = nn.BCEWithLogitsLoss()(output, batch[\"labels\"].float())\n","loss.backward()\n","optimizer.step()"],"metadata":{"id":"XQg-uhyRtLWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define compute_metrics function\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    metric = load_metric(\"f1\")\n","    return metric.compute(predictions=preds, references=labels)"],"metadata":{"id":"VMkmsVAKtMqC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n","training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    evaluation_strategy = \"epoch\",   # evaluation strategy to adopt during training\n","    learning_rate=2e-5,             # learning rate\n","    per_device_train_batch_size=8,  # batch size per device during training\n","    per_device_eval_batch_size=8,   # batch size for evaluation\n","    num_train_epochs=3,             # total number of training epochs\n","    weight_decay=0.01,              # strength of weight decay\n","    push_to_hub=False,\n",")\n","print(training_args)"],"metadata":{"id":"oDAu6rt0tNpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,                     \n","    args=training_args,              \n","    train_dataset=train_tokenized_dataset,         \n","    eval_dataset=dev_tokenized_dataset,            \n","    data_collator=data_collator,                     \n","    tokenizer=tokenizer,                            \n","    compute_metrics=compute_metrics,                  \n",")\n","print(trainer)"],"metadata":{"id":"oGpOChEltOpN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test set\n","#predictions = trainer.predict(test_dataset)\n","predictions = trainer.predict(tokenized_datasets[\"test\"])\n","#print(predictions.predictions.shape, predictions.label_ids.shape)\n","preds = np.argmax(predictions.predictions, axis=-1)\n","test_results = trainer.evaluate(test_tokenized_dataset)\n","print(test_results)"],"metadata":{"id":"guyquNtztPiR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute and print the F1 score\n","metric = load_metric(\"f1\")\n","predictions = trainer.predict(test_tokenized_dataset)\n","preds = predictions.predictions.argmax(-1)\n","f1_score = metric.compute(predictions=preds, references=predictions.label_ids)\n","print(f1_score)"],"metadata":{"id":"IDKbOAaTtQd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RBDkXVgQtQsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"q2sXb9gBtJuY"},"execution_count":null,"outputs":[]}]}
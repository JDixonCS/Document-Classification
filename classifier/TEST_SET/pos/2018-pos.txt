Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks

Yue Feng, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng
1 University of Chinese Academy of Sciences, Beijing, China 2 CAS Key Lab of Network Data Science and Technology,
Institute of Computing Technology, Chinese Academy of Sciences
{fengyue,zengwei}@software.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn

ABSTRACT
The goal of search result diversi cation is to select a subset of documents from the candidate set to satisfy as many di erent subtopics as possible. In general, it is a problem of subset selection and selecting an optimal subset of documents is NP-hard. Existing methods usually formalize the problem as ranking the documents with greedy sequential document selection. At each of the ranking position the document that can provide the largest amount of additional information is selected. It is obvious that the greedy selections inevitably produce suboptimal rankings. In this paper we propose to partially alleviate the problem with a Monte Carlo tree search (MCTS) enhanced Markov decision process (MDP), referred to as M2Div. In M2Div, the construction of diverse ranking is formalized as an MDP process where each action corresponds to selecting a document for one ranking position. Given an MDP state which consists of the query, selected documents, and candidates, a recurrent neural network is utilized to produce the policy function for guiding the document selection and the value function for predicting the whole ranking quality. The produced raw policy and value are then strengthened with MCTS through exploring the possible rankings at the subsequent positions, achieving a better search policy for decision-making. Experimental results based on the TREC benchmarks showed that M2Div can signi cantly outperform the state-of-the-art baselines based on greedy sequential document selection, indicating the e ectiveness of the exploratory decision-making mechanism in M2Div.
KEYWORDS
Diverse ranking; Markov decision process; Monte Carlo tree search
ACM Reference Format: Yue Feng, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng . 2018. From Greedy Selection to Exploratory Decision-Making: Diverse Ranking with Policy-Value Networks. In SIGIR '18: The 41st International ACM SIGIR Conference on Research Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3209979
 Corresponding author: Jun Xu
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3209979

1 INTRODUCTION
One important goal in many information retrieval tasks involves providing search results that covers a wide range of topics for a search query, called search result diversi cation [1]. The goal of search result diversi cation can be formalized as selecting a minimal subset of documents from the candidate set to cover as many di erent subtopics as possible. Since the novelty of a document depends on the other selected documents, selecting an optimal subset of documents amounts to the problem subset selection and its complexity is in general NP-hard.
Typical approaches treat search result diversi cation as ranking the documents based on their relevance as well as the novelty. Greedy sequential document selection has been widely adopted to construct the diverse document ranking, that is, the document ranking is constructed step by step. At each step the ranking model selects one document from the candidate set for the current ranking position. Usually, the document with the maximal amount of additional utility, a.k.a. marginal relevance, is selected.
A number of diverse ranking algorithms have been developed under the greedy document selection framework. Di erent algorithms utilize di erent criteria to estimate the additional utility a candidate document can provide. For example, the representative approach of maximal marginal relevance (MMR) [3] uses the sum of the query-document relevance and the maximal document distance (referred to as marginal relevance) as the utility. xQuAD [25] de nes the utility so as to explicitly account for the relationship between documents retrieved for the original query and the possible subqueries. In recent years, machine learning based methods have been proposed for conducting diverse ranking [23, 31, 32, 34, 36, 39]. The relational learning to rank (R-LTR) [39] and its variations [31, 32, 34] de ne the utilities based on the relevance features and the novelty features. MDP-DIV adapted the Markov decision process (MDP) to model the document ranking process. The utility of a document is estimated based on the MDP state, which consists of the query, the preceding documents, and the remaining candidates [33].
The greedy sequential document selection simpli es the ranking process and can accelerate the online ranking. However, the rankings produced by greedy document selection are inevitably suboptimal. At each ranking position, the greedy selection mechanism only considers the possibilities at the current ranking position (i.e., estimates the utility of each candidate document if it were selected). Thus, greedy document selection will select the locally optimal document at each ranking position. However, a sequence of the locally optimal documents cannot lead to the globally optimal diverse ranking, because the utilities of the documents are not independent. The selection of a document at one position will change

125

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

the utilities of the remaining candidate documents and thereafter a ects the subsequent decisions. In general the ranking algorithm need to explore the whole ranking space if the optimal ranking is mandatory. However, this is usually infeasible in real applications because of the huge space size: there exist N ! di erent rankings for N documents.
Inspired by the success and methodology of the AlphaGo [27] and AlphaGo Zero [28] for the Game of Go, in this paper we propose to enhance the MDP model for diverse ranking [33] with the Monte Carlo tree search (MCTS), for alleviating the suboptimal ranking problem. The new ranking model, referred to as M2Div (stands for MCTS enhanced MDP for Diverse ranking), makes use of an MDP to model the sequential document selection process of diverse ranking. At each time step (corresponding to a ranking position), based on the user query and the preceding document ranking, a recurrent neural network (RNN) is used to produce the policy (a distribution over the candidate documents) for guiding the document selection and the value for estimating the whole document ranking quality (e.g., in terms of -NDCG@M). To alleviate the problem of suboptimal diverse ranking, in stead of greedily selecting a document with the predicted raw policy, M2Div conducts an exploratory decision making: an MCTS is conducted to explore the possible document rankings at the subsequent positions, resulting a strengthened search policy for conducting the real document selection at current position. Since it has explored more future possible document rankings, the search policy has higher probability to select a globally optimal document than the predicted raw policy. Moving to the next iteration, the above process is continued until the candidate set is empty.
Reinforcement learning is used to train the model parameters. In the training phase, at each training iteration and for each training query, an MCTS guided by the current policy function and value function is conducted at each ranking position. The MCTS produces a search policy for the document selection. Then the model parameters are adjusted to minimize the loss function. The loss function consists of two terms: 1) the squared error between the predicted value and the nal quality of the whole document ranking in terms of -NDCG@M; and 2) the cross entropy of the predicted raw policy and the search policy for document selection. Stochastic gradient descent is utilized for conducting the optimization.
To evaluate the e ectiveness of M2Div, we conducted experiments on the basis of TREC benchmark datasets. The experimental results showed that M2Div can signi cantly outperform the stateof-the-art diverse ranking approaches that using greedy sequential decision making, including the heuristic based diverse ranking methods of MMR and xQuAD, and the machine learning based diverse ranking methods of PAMM and MDP-DIV. We analyzed the results and showed that the exploratory decision-making mechanism in M2Div does help to improve the ranking performances.
2 RELATED WORK
2.1 Search result diversi cation
It is a common practice to formalize the construction of a diverse ranking list in search as a process of greedy sequential decision making. Existing research focus on designing e ective criteria to

estimate the utility a document can provide. Carbonell and Goldstein [3] proposed the maximal marginal relevance criterion, which is a linear combination of the query-document relevance and the document novelty, to select the document. xQuAD [24] directly models di erent aspects of a query and estimates the utility as the relevance of the retrieved documents to each identi ed aspects. Hu et al. [13] proposed a utility function that explicitly leverages the hierarchical intents of queries and selects the documents that maximize diversity in the hierarchical structure. See also [2, 4, 7, 9­ 11, 22, 29]
Machine learning techniques have been applied to construct diverse ranking, also adopting the greedy sequential decision making as the basic framework. The key problem becomes how to automatically learn the utility function on the basis of training queries. Some researchers de ne the utility as a linear combination of the handcrafted relevance features and novelty features [23, 31, 34, 39]. The novelty term in the utility function can be modeled with the deep learning model of neural tensor networks [32]. Jiang et al. used recurrent neural networks and max-pooling to model subtopic information explicitly with the attention mechanism [14, 15]. Xia et al. [33] proposed to model the dynamics of the document utility with MDP and learning the model parameters with policy gradient. Other learning approaches please refer to [18, 21, 23, 34, 35, 37].
2.2 Reinforcement learning for IR
The reinforcement learning has been widely used in variant IR applications. For example, in [20], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [38], the log-based document re-ranking is modeled as a POMDP. [33] and [30] propose to model the process of constructing a document ranking with MDP, for the ranking tasks of search result diversi cation and relevance ranking, respectively. Muti-armed bandit, another type of reinforcement learning model, is also widely applied to rank learning. For example, [23] proposes two online learning bandit algorithms to learn a diverse ranking of documents based on users clicking behaviors. [37] formalizes the interactively optimizing of information retrieval systems as a dueling bandit problem and [16] proposes cascading bandits to identify K most attractive document for users. See also [12]
Reinforcement learning models are also used for building recommender systems. For example, [26] designs an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. Lu and Yang [19] proposes POMDP-Rec, a neuraloptimized POMDP algorithm, for building a collaborative ltering recommender system.
In this paper, we also adopt the reinforcement learning model of MDP to formalize the diverse ranking process in search result diversi cation.
3 MDP AND MCTS
We employ Markov decision process and Monte Carlo tree search to model diverse ranking process.

126

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

3.1 Markov decision process
MDP provides a mathematical framework for modeling the sequential decision making process with the agent-environment interface. The key components of an MDP include:
States S is a set of states. For instance, in this paper we de ne the state as a tuple consists of the query, the preceding document ranking, and the candidate documents.
Actions A is a discrete set of actions that an agent can take. The actions available may depend on the state s, denoted as A(s).
Policy p describes the behaviors of an agent, which is a probabilistic distribution over the possible actions. p is usually optimized to maximize the long term return.
Transition T is the state transition function st+1 = T (st , at ) which speci es a function that maps a state st into a new state st+1 in response to the action selected at .
Value: state value function V : S  R is a function that predicts the long term return of the whole episode, on the basis of the current state s under the policy p.
An MDP model is running as follows: the agent and environment interact at each of a sequence of discrete time steps, t = 0, 1, 2, · · · . At each time step t the agent receives some representation of the environment's state, st  S, and on that basis selects an action at  A(st ). One time step later, in part as a consequence of its action, the agent nds itself in a new state st+1 = T (st , at ). Usually the goal of reinforcement learning is to achieve maximum long term return, that is, to maximize the value V (s0).
3.2 Monte Carlo tree search
The decisions made by an MDP (e.g., selecting the most con dent actions according to the policy) may get suboptimal results. Theoretically the system has to explore the whole space of decision sequences to get a global optimal result. However, this is usually not feasible. MCTS is a tool to conduct heuristic search inside the whole space, more likely to produce a better decision sequence than that of produced by the greedy decisions.
Given the time step t, the policy function p, and the state value function V , MCTS aims at searching a strengthened policy for making better decisions. The MCTS consists of four phases: selection, expansion, simulation, and back-propagation:
Selection: Starting at the root node R, MCTS recursively selects the optimal child nodes until a leaf node L is reached.
Expansion: If L is not a terminal node (i.e. it does not end the episode) then create one or more child nodes for L (each corresponds a possible action) and select one child node C according to the predicted policy.
Simulation/Evaluation: MCTS runs a simulation from C until a result is achieved. In the AlphaGo Zero [28] the simulation is replaced with the value function for accelerating the tree search. That is, the result of simulation is estimated by the value function.
Back-propagation: Update the statistics stored in the current move sequence with the simulation or estimated result.
The MCTS outputs a search policy  over the actions, which is utilized to choose the action at time step t. The iteration is continued until the whole episode is generated.

Policy-Value network LSTM

state '%

*($|'%) "('%)

raw policy !

value function "

MCTS search policy #

'%() Environment

action $%~#

Figure 1: The agent-environment interaction of M2Div.

4 DIVERSE RANKING WITH POLICY-VALUE
NETWORKS
In this section, we introduce the proposed M2Div model, which makes use of MDP and MCTS for modeling the diverse ranking process and for strengthening the policy for selecting documents at each of the MDP iteration, respectively. The agent-environment interaction of M2Div is illustrated in Figure 1. Each of the MDP time step corresponds to a ranking position. At time step t (t = 0, 1, · · · ), the policy-value network receives the environment state st and makes use of an LSTM to produce the representation of the state st . After that, guided by the current policy function p and value function V , an MCTS search is executed. The output of MCTS is a strengthened new search policy  . The action at is then selected according to the strengthened policy  , which chooses a document from the candidate set and places it to the ranking position t + 1. Moving to the next time step t + 1, the system nds itself in a new state st+1 and the process is repeated until all of the documents are ranked.
4.1 MDP formulation of diverse ranking
Suppose we are given a query q, which is associated with a set of retrieved documents X = {x1, · · · , xM }  X, where both the query q and the documents xi are represented with their preliminary representations, i.e., the vectors learned by the doc2vec model [17], and X is the set of all possible documents. The goal of diverse ranking is to construct a model that can rank the documents so that the top ranked documents cover a wide range of subtopics of a search query.
The construction of a diverse ranking can be considered as a process of sequential decision making with an MDP in which each time step corresponds to a ranking position. The states, actions, transition function, value function, and policy function of the MDP are set as:
States S: We design the state at time step t as a triple st = [q, Zt , Xt ], where q is the preliminary representation of the user issued query; Zt = {x(n)}nt =1 is the sequence of t preceding documents, where x(n) is the document ranked at position n; Xt is the set of candidate documents. At the beginning (t = 0), the state is initialized as s0 = [q, , X ], where  is the empty sequence and X contains all of the M retrieved candidate documents.

127

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Actions A: At each time step t, the A(st ) is the set of actions the agent can choose, each corresponds to a document from Xt . That is, the action at  A(st ) at the time step t selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at .
Transition T : The transition function T : S × A  S is de ned as follows:
st +1 = T (st , at ) = T ([q, Zt , Xt ], at ) (1)
= q, Zt  {xm(at )}, Xt \ {xm(at )} ,
where  appends xm(at ) to Zt and \ removes xm(at ) from Xt . At each time step t, based on state st the system chooses an action at . Then, the system moves to time step t + 1 and the system transits to a new state st+1: rst, the query q is kept unchanged; second, the selected document is appended to the end of Zt , generating a new document sequence; nally, the selected document at step t is removed from the candidate set: Xt +1 = Xt \ {xm(at )}. Thus, the number of actions the agent can choose at t + 1 is reduced by one.
Value function V : The state value function V : S  R is a scalar evaluation, estimating the quality of the whole document ranking (an episode) based on the input state. The value function is learned so as to approximate a prede ned evaluation measure (e.g., -NDCG@M).
In this paper, we make use of the long short term memory (LSTM) to summarize the input state s as a real vector, and then de ne the value function as nonlinear transformation of the weighted sum of the LSTM outputs:

V (s) =  ( w, LSTM(s) + b ) ,

(2)

where w and b are the weight vector and the bias to be learned

during training, and  (x) =

1 1+e -x

is the nonlinear sigmoid func-

tion. The deep neural network model LSTM: S  RL maps a

state to a real vector where L is the number of dimensions. Given

s = [q, Z = {x1, x2, · · · , xt }, Xt ], where xk (k = 1, · · · , t) is the document ranked at k-th position and represented with its doc2vec

embedding. LSTM outputs a representation hk for position k as:

fk = (Wf xk + Uf hk-1 + bf ), ik = (Wi xk + Ui hk-1 + bi ), ok = (Wo xk + Uo hk-1 + bo ), ck =fk  ck-1 + ik  tanh(Wc xk + Uc hk-1 + bc ), hk =ok  tanh(ck ),

where h and c are initialized with the query q:

[h0, c0] = [ (Vh q),  (Vc q)],
operator "" denotes the element-wise product and " " is applied to each of the entries; the variables fk , ik , ok , ck and hk denote the forget gate's activation vector, input gate's activation vector, output gate's activation vector, cell state vector, and output vector of the LSTM block, respectively. Wf , Wi , Wo, Uf , Ui , Uo, Vh, Vc , bf , bi , bo are weight matrices and bias vectors need to be learned during training. The output vector and cell state vector at the t-th cell are concatenated as the output of LSTM:

LSTM(s) =

ht T , ct T

T
.

(3)

Policy function p: The policy p(s) de nes a function that takes the state as input and output a distribution over all of the possible actions a  A(s). Speci cally, each probability in the distribution is a normalized soft-max function whose input is the bilinear product of the LSTM de ned in Equation (3) and the selected document:

p(a|s) =

exp xTm(a)Up LSTM(s)

,

a A(s) exp xTm(a )Up LSTM(s)

where Up is the parameter in the bilinear product. Thus, the policy function p(s) is:

p(s) = p(a1|s), · · · , p(a |A(s)| |s) .

(4)

4.2 Strengthening policy with MCTS

Selecting documents greedily with the predicted raw policy p in

Equation (4) may lead to suboptimal rankings because the policy

only summarizes the historical information and has no idea on

how the action at will in uent the future decisions. Let's use an example to show the limitation of greedy selection. Suppose that the

query q has 6 subtopics, and the 3 retrieved candidate documents d1, d2, and d3 cover the subtopics of {1, 2, 3, 6}, {1, 2, 5}, and {1, 2, 6}, respectively. The greedy selection with raw policy prefers d1 for the rst position, as it covers the most number of subtopics. Thus,

the document ranking constructed with the greedy policy could be

[d1, d2, d3]

and

S-recall@2

=

5 6

.

However,

if

the

ranking

algorithm

could explore (part of) the whole ranking space, it will found a

better document ranking in terms of S-recall@2: [d2, d3, d1] with S-recall@2 = 1. The example clearly indicate that greedy selection

could lead to suboptimal rankings.

To alleviate the issue, following the practices in AlphaGo [27]

and AlphaGo Zero [28], we propose to conduct lookahead searches

in the ranking space with MCTS. Speci cally, at each ranking posi-

tion t, an MCTS search is executed, guided by the current policy

function p and value function V , and output a strengthened new

search policy  . It is believed that the search policy  will select a

much better action (document) for the ranking position than that

of selected by the raw policy p in Equation (4). This is because the

lookahead MCTS tries to explore the whole ranking space and can

partially alleviate the suboptimal ranking problem.

Figure 2 illustrates the MCTS process and Algorithm 1 shows

the details. Each node of the tree corresponds to an MDP state. The

tree search algorithm takes a root node sR , number of search times K, value function V , policy function p, human labels , and the

evaluation measure R as inputs. Note that and R are only used

at the training time. The algorithm iterate K times and outputs

a strengthened search policy  for selecting a document for root

node sR . Suppose that each edge e(s, a) (the edge from state s to the state T (s, a)) of the MCTS tree stores an action value Q(s, a), visit

count N (s, a), and prior probability P(s, a). The raw policy p(sR ) at the root state sR is strengthened with the following steps:
Selection (line 3 to line 7 of Algorithm 1): Each of the K it-

erations starts from the root state sR and iteratively selects the documents that maximize an upper con dence bound. Speci cally,

at each time step t of each simulation, an action at is selected from

128

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Selection

Repeat K times

Evaluation

Expansion

Back-propagation

!, # = & ' = {)*, )+, ), ... }

!, # = & ' = {)*, )+, ), ... }

/+1

2+3 max

!, # = {)*} ' = {)+, ), ... }

!, # = {)+} ' = {)*, ),, ... }

!, # = {)*} ' = {)+, ), ... }

/+1 !, #, '

/+1 2+3

/+1

max

!, #, ' !, #, ' !, #, '

!, #, ' !, #, '

!, # = {)+} ' = {)*, ),, ... } !, #, ' !, #, '

4 = 5(7)

!, # = & ' = {)*, )+, ), ... }

!, # = & ' = {)*, )+, ), ... }

4

2=

2 :+4 :+>

!, # = {)*} ' = {)+, ), ... }

!, # = {)+} ' = {)*, ),, ... }

!, # = {)*} ' = {)+, ), ... }

!, # = {)+} ' = {)*, ),, ... }

!, #, ' !, #, ' !, #, ' !, #, ' !, #, '

2=9 :=9 ; = <(=>|7)

2=9 :=9 ; = <(=@|7)

!, #, ' !, #, '

2 :+4

2= :+>

4

!, #, ' !, #, '

!, #, '

!, #, ' !, #, '

Figure 2: The MCTS process for calculating the strengthened search policy  .

state st so as to maximize action value plus a bonus:

at

=

arg

max(Q
a

(st

,

a)

+

U

(st

,

a)),

(5)

where   0 is the tradeo coe cient, and the bonus U (st , a) is de ned as

U (st , a) = p(a|st )

a A(st ) N (st , a ) ,
1 + N (st , a)

where p(a|st ) is the predicted probability by the policy function p(st ), A(st ) is the set of available actions (documents) at state st . U (st , a) is proportional to the prior probability but decays with repeated visits to encourage exploration.
Evaluation and expansion (line 8 to line 19 of Algorithm 1):
When the traversal reaches a leaf node sL, the node is evaluated either with the value function V (sL) or with the prede ned performance measure if the node is the end of an episode and the
human labels are available. Speci cally, at the test phase or online
ranking phase, there is no human label information available. sL will be evaluated with the value function V (sL). At the training phase, if sL is not the end of the episode, V (sL) is used to conduct the evaluation. If sL is the last node of the episode (cannot be expanded), it is evaluated with the true performance measure (e.g.,
-NDCG@M) at the position. Line 10 and line 18 of Algorithm 1
shows the evaluation details. Please note that following the practice
in AlphaGo Zero program, we use the value function instead of
rollouts for evaluating a node.
Then, the leaf node sL may be expanded (line 11 to line 16 of Algorithm 1). Each edge from the leaf position sL (corresponds to each action a  A(sL)) is initialized as: P(sL, a) = p(a|sL) (Equation (4)), Q(sL, a) = 0, and N (sL, a) = 0. In this paper all of the available actions of sL are expanded.
Back-propagation and update (line 20 to line 28 of Algo-
rithm 1): At the end of evaluation, the action values and visit counts
of all traversed edges are updated. For each edge e(s, a), the prior

probability P(s, a) is kept unchanged, and Q(s, a) and N (s, a) are

updated:

Q(s, a) 

Q

(s,

a

)×N (s,a)+V N (s,a)+1

(s

L

)

,

(6)

N (s, a)  N (s, a) + 1.

Calculate the strengthened search policy (line 29 to line 32 of Algorithm 1): After iterating K times, the strengthened search policy  for the root node sR can be calculated according to the visit counts N (sR , a) of the edges starting from sR :

 (a|sR ) =

N (sR , a)

,

a A(sR ) N (sR , a )

(7)

for all a  A(sR ).

4.3 Training with reinforcement learning
The model has parameters  = {Wf , Wi , Wo, Uf , Ui , Uo, Up , bf , bi , bo, Vh, Vc , w} to learn. In the training phase, suppose we are given N labeled training queries {(q(n), X (n), (n))}nN=1, where (n) denotes the human labels on the documents, in the form of a binary matrix. (n)(i, j) = 1 if document xi(n) contains the j-th subtopic of q(n) and 0 otherwise.
Algorithm (2) shows the training procedure. First, the parameters  is initialized to random weights in [-1, 1]. At each subsequent iteration, for each query, a ranking of documents is generated with current parameter setting. At each ranking position t, an MCTS search is executed, using previous iteration of value function and policy function, and a document xm(at ) is selected according to the search probabilities t .
The ranking terminates when the candidate is empty or the ranking exceeds the maximum length de ned by the prede ned evaluation measure R. These sampled documents consist a permutation of documents  , which is then evaluated with the evaluation measure to give a ground-truth return:
r = R( , ).

129

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Algorithm 1 TreeSearch

Input: root state sR , value function V , and policy function p, number of search times K, trade-o parameter , human labels ,

and performance evaluation function R

Output: Tree search probabilities 

1: for k = 0 to K - 1 do

2: sL  sR 3: {Selection}

4: while sL is not a leaf node do

5:

a  arg maxa A(sL) Q(sL, a) +  · U (sL, a){Equation (5),

using the P, Q, and N stored in the corresponding edges}

6:

sL  child node pointed by edge (sL, a)

7: end while

8: {Evaluation and expansion}

9: if sL can be expanded then

10:

 V (sL) {simulate with value function V }

11:

for all a  A(sL) do

12:

Expand a new edge e which connects to a new node

s = [q, sL .Z  {xm(a)}, sL .X \ {xm(a)}]

13:

e.P  p(a|sL) {initialize the prior probability}

14:

e.Q  0

15:

e.N  0

16:

end for

17: else

18:



V (sL) R(sL .Z, )

R = NULL or otherwise

=  {Case 1: predict

with V at test phase (R =NULL or = ); Case 2: directly

set to the ground truth (e.g., -NDCG@M) at the training

phase}

19: end if

20: {Back-propagation}

21: while sL sR do

22:

s  parent of sL

23:

e  edge from s to sL

24:

e .Q



e .Q ×e .N + e .N +1

{Equation (6)}

25:

e.N  e.N + 1

26:

sL  s

27: end while

28: end for

29: {calculate tree search probabilities}

30: for all a  A(sR ) do

31:

 (a|sR ) 

a

e (sR , a ). N A(sR ) e (sR, a

).N

{e(s, a)

is

the

edge

from

s

to

the state by taking action a}

32: end for

33: return 

Here R can be any diverse ranking evaluation measure such as NDCG@M etc. The data generated at each time step E = {(st , t )}Tt=1 and the nal evaluation r are utilized as the ground-truth signals
in training for adjusting the value function. The model parameters
are adjusted to minimize the error between the predicted value
V (st ) and the evaluation of the ranking in terms of the chosen evaluation measure, and to maximize the similarity of the raw policy
p(st ) to the search policy t . Speci cally, the parameters  are adjusted by gradient descent on a loss function that sums over

Algorithm 2 Train diverse ranking model

Input: Labeled training set D = {(q(n), X (n), (n))}nN=1, learning rate , number of search steps K, MCTS trade-o parameter ,

and evaluation measure R

Output: 

1: Initialize   random values in [-1, 1]

2: repeat

3: for all (q, X , )  D do

4:

s  [q, , X ]

5:

M  |X |

6:

E = (){empty episode}

7:

for t = 0 to M - 1 do

8:

  TreeSearch(s, V , p, K, , , R) {Algorithm (1): tree

search using s as root, with current }

9:

a = arg maxa A(s)  (a|s) {select the best document}

10:

 (t + 1)  m(a){document xm(a) is ranked at t + 1}

11:

E  E  {(s,  )}

12:

s  [q, s.Z  {xm(a)}, s.X \ {xm(a)}]

13:

end for

14: r  R( , ){evaluating the generalized ranking}

15:





 -

(E,r ) 

{Update

parameters.

is de ned in

Equation (8)}

16: end for

17: until converge

18: return 

the mean-squared error and cross-entropy losses, respectively:

|E |
(E, r ) =
t =1

(V

(st

)

-

r

)2

+

a



A(st

)

t

(a

|st

)

log

1 p (a |st

)

.

(8)

Figure 3 illustrates the construction of the loss given a training

query. The model parameters are trained by back propagation and

stochastic gradient descent. Speci cally, we use AdaGrad [8] on all

parameters in the training process.

Please note that following the practices in [28], the search tree

constructed at the t-th iteration (line 8 to line 12 of Algorithm 2)

is reused at subsequent steps: the child node corresponding to the

selected document (chosen action) becomes the new root node; the

subtree below this child is retained along with all its statistics, while

the remainder of the tree is discarded.

4.4 Online ranking
The construction of a diverse ranking for an online query is shown in Algorithm 3. Given a user query q, a set of M retrieved documents X , the system state is initialized as s0 = [q, Z0 = , X0 = X ]. Then, at each of the time steps t = 0, · · · , M - 1, the agent receives the state st = [q, Zt , Xt ] and searches the policy  with MCTS, on the basis of the value function V and policy function p. Then, it chooses an action a according  , selects the document xm(a) from the candidate set, and places it to the rank t + 1. Moving to the next step t + 1, the state becomes st +1 = [q, Zt +1, Xt +1]. The process is repeated until the candidate set becomes empty.
Considering that the MCTS is time consuming and may be infeasible in some online ranking tasks, the online ranking algorithm can also skip the tree search and directly use the raw policy for

130

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

/, 1* = 3

5"~!"

/, 1" = {5"}

5#~!# /, 1# = {5", 5#} 5?~!? /, 1? = {5", 5# ... }

@* = {5", 5#, 5$ ... }

@" = {5#, 5$, 5B ... }

@# = {5$, 5B, 5C ... }

@? = {... }

 
 
 

!" /, 1* = 3

LSTM

%('|)*) V()*)

%"

-"

!"

7

!# /, 1" = {5"}

LSTM

%('|)") V()")

%#

-#

!#

7

!$ /, 1# = {5", 5#}

LSTM

%('|)#) V()#)

%$

-$

!$

7

7 = 9NDCG

Figure 3: Construction of loss function for a training query.

Algorithm 3 Diverse ranking with Monte Carlo tree search

Input: query q, documents X = {x1, · · · , xM }, number of search

steps K, MCTS trade-o parameter , value function V , and

policy function p

Output: Permutation of documents 

1: s  [q, , X ]

2: M  |X |

3: for t = 0 to M - 1 do

4:



TreeSearch(s, V , p, K, , , NULL) with MCTS,

p(s)

without MCTS

{Case 1: ranking with search policy. No human labels and

evaluation function are available at the test/online ranking

phase; Case 2: ranking with raw policy for accelerating

online ranking speeds.}

5: a  arg maxa A(s)  (a|s) 6:  (t + 1)  m(a){document xm(a) is ranked at t + 1} 7: [q, Z, X ]  s

8: s  [q, Z  {xm(a)}, X \ {xm(a)}]

9: end for

10: return 

ranking, denoted with "without MCTS" in the line 4 of Algorithm 3. In the experiments, we observed that the M2Div without MCTS can still outperform the baselines. This is because the training process of M2Div can generate high quality episodes to train the model parameters with the help of MCTS, which leads to more accurate policy function p.
4.5 Di erence with AlphaGo Zero
M2Div is inspired by the AlphaGo Zero program. It enjoys a number of merits from AlphaGo Zero, including the shared neural network for estimating policies and values, lookahead MCTS for strengthening the raw policy, and the compounded loss that simultaneously optimizes the value function and the policy function etc. However, M2Div has made a number of fundamental modi cations for search result diversi cation.
First, the formalizations of the tasks are di erent. AlphaGo Zero formalizes the playing of Go as an alternating Markov game where

each action corresponds a move, the states are the raw board positions, the value function approximates the probability of winning, and the next state depends not only on the chosen action but also on the move by the opponent. M2Div, on the other hand, formalizes the ranking of documents as planning with an MDP where each action corresponds to a document selection. The MDP states consists of the query, the preceding document ranking, and the remaining candidates. The value function approximates a diverse ranking evaluation measure (e.g., -NDCG@M). The state transition is fully determined by the current state and the chosen action.
Second, the supervision signals for learning the model parameters are di erent. AlphaGo Zero uses the results of self-play (-1 for loss, 0 for draw, and 1 for win) as the supervision signals and the value function is tted to the results. The task of diverse ranking, however, is not a Markov game. It is di cult (also unnecessary) to execute the self-play. In the training phase, M2Div resorts to the human labels and the prede ned evaluation measure to generate supervision information. Speci cally, the -NDCG@M of each generated episode is calculated and used as the ground-truth to t the value function (the rst part of Equation (8)). In this way, M2Div drives the training process so as to directly optimize the evaluation measure of -NDCG@M. Note that -NDCG@M can be replaced with any other diverse ranking evaluation measures.
Third, the shared deep neural networks for calculating the policies and values are di erent. AlphaGo Zero makes use of a residual network which takes the raw board position as its inputs and outputs the a probability distribution over moves, and a probability of the current player winning in position. The raw boards can be considered as some xed sized images. In M2Div, the MDP states consist of the queries and sequences of documents etc. Residual network cannot handle the state data as the queries/documents are raw texts, and the lengths of the document sequences vary in di erent time steps. To address the issue, M2Div rst represents the query and document sequence as the state vector of an LSTM (Equation 3). The policy and the value are then calculated on the basis of the representations outputted by the LSTM.
5 EXPERIMENTS
We conducted experiments to test the performances of M2Div using a combination of four TREC benchmark datasets: TREC 2009  2012 Web Track datasets (WT2009, WT2010, WT2011, and WT2012).
5.1 Experimental settings
In our experiments, for e ective training of the model parameters and following the practices in [33], we combined four TREC datasets and constructed a new dataset with 200 queries and in total about 45,000 labeled documents. Each query includes several subtopics identi ed by the TREC assessors. The document relevance labels are made at the subtopic level and the labels are binary1.
All the experiments were carried out on the ClueWeb09 Category B data collection2, which is comprised of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied to the documents as preprocessing. We conducted 5-fold cross-validation experiments.
1WT2011 has graded judgements and we treat them as binary in the experiments. 2 http://boston.lti.cs.cmu.edu/data/clueweb09

131

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

We randomly split the queries into ve even subsets. At each fold, three subsets were used for training, one was used for validation, and one was used for testing. The results reported were the average over the ve trials.
The TREC o cial diversity evaluation metrics of -NDCG [6] and ERR-IA [5] were used in the experiments, including. They measure the diversity of a result list by explicitly rewarding diversity and penalizing redundancy observed at every rank. Following the default settings in o cial TREC evaluation program, the parameter  in these evaluation measures are set to 0.5.
We compared M2Div with several state-of-the-art baselines in search result diversi cation:
MMR [3]: a heuristic approach in which the document is selected according to maximal marginal relevance.
xQuAD [24]: a representative method which explicitly models di erent aspects underlying original query in form of subqueries.
PM-2 [7]: a method of optimizing proportionality for search result diversi cation.
We also compared MDP-DIV with the learning methods: SVM-DIV [36]: a learning approach which utilizes structural SVMs to optimize the subtopic coverage. R-LTR [39]: a learning approach developed in the relational learning to rank framework. PAMM [31]: a learning approach that directly optimizes diversity evaluation measure using structured Perceptron. NTN-DIV [32]: a learning approach which automatically learns novelty features based on neural tensor networks. MDP-DIV [33]: a state-of-the-art learning approach which uses an MDP for modeling the diverse ranking process. Following the practice in [33], we con gured the reward function in MDP-DIV as -DCG and the discounting parameter  = 1. M2Div, and the baselines of MDP-DIV and NTN-DIV need preliminary representations of the queries and the documents as their inputs. Following the practices in [33], in the experiments we used the query vector and document vector generated by the doc2vec [17] to represent the document. Doc2vec model was trained on all of the documents in Web Track dataset and the number of vector dimensions were set to 100. For training the model, we used the distributed bag of words model3. The learning rate was set to 0.025 and the window size was set to 8. The M2Div also has some parameters to tune: the training evaluation measure function R was set to -NDCG@5, making the value function V to approximate -NDCG@5. In the training phrase, the MCTS also was set to control the maximal length of the search depth less than 5. That is, the condition at line 9 of Algorithm 1 is true if the length of the tree depth is less than 5 and the candidate set is not empty. In all of the experiments, the learning rate , the number of search times K, the tree search trade-o parameter , and the number of LSTM hidden units h were tuned based on the validation set and set to  = 0.01, K = 5000,  = 3.0, and h = 5. In online ranking phase, M2Div has two versions: selecting the documents with the raw policy p or with the MCTS strengthened policy  . These two versions are respectively denoted as "M2Div(without MCTS)" and "M2Div(with MCTS)". The source code of M2Div is available at https://github.com/sweetalyssum/M2DIV.
3 http://radimrehurek.com/gensim/tutorial.html

Table 1: Performance comparison of all methods on TREC web track datasets.

Method

-NDCG@5 -NDCG@10 ERR-IA@5 ERR-IA@10

MMR xQuAD PM-2
SVM-DIV R-LTR PAMM( -NDCG) NTN-DIV( -NDCG) MDP-DIV( -DCG) M2Div(without MCTS) M2Div(with MCTS)

0.2753 0.3165 0.3047 0.3030 0.3498 0.3712 0.3962 0.4189
0.4386 0.4424

0.2979 0.3941 0.3730 0.3699 0.4132 0.4327 0.4577 0.4762
0.4835 0.4852

0.2005 0.2314 0.2298 0.2268 0.2521 0.2619 0.2773 0.2988
0.3435 0.3459

0.2309 0.2890 0.2814 0.2726 0.3011 0.3029 0.3285 0.3494
0.3668 0.3686

5.2 Experimental results
Table 1 reports the performances of our approach and all of the baseline methods in terms of diversity performance metrics including -NDCG@5, -NDCG@10, ERR-IA@5, and ERR-IA@10. Boldface indicates the highest scores among all runs. From the results we can see that, in terms of the four diversity evaluation metrics, "M2Div (with MCTS)" and "M2Div (without MCTS)" outperformed all of the baseline methods, including the heuristic method of MMR, xQuAD, PM-2 and learning methods of SVM-DIV, R-LTR, PAMM(-NDCG), NTN-DIV(-NDCG), and MDP-DIV(-DCG). We conducted significant testing (t-test) on the improvements of our approaches over the best baseline MDP-DIV(-DCG). The results indicate that most of the improvements are signi cant (p-value < 0.05 and denoted with `*' in Table 1).
From the results we can see that "M2Div (without MCTS)", which did not conduct MCTS at the online time, still outperformed all of the baselines including "MDP-DIV(-DCG)", indicating that the MCTS conducted at the training time can generate better episodes to estimate the model parameters, achieving better raw policy p for ranking. Note that "M2Div(with MCTS)", which conducted MCTS at the online time, further improved the ranking accuracies and performed the best among all of the methods. The results indicate that the MCTS can improve the raw policies p at both the training and the online ranking.
5.3 Discussion
In this section, we conducted experiments to investigate how M2Div works and why it can outperform the baselines, using the experimental results on the rst fold of the data as example.
5.3.1 E ects of Monte Carlo tree search. One key step in M2Div is the MCTS which outputs the search policy  . It is very likely that the search policy  (s) are better than raw policy p(s) in terms of choosing optimal documents.
We conducted experiments to show the e ectiveness of the MCTS in online ranking. Speci cally, based on the trained M2Div model, we tracked the online ranking process for query number 148 ("martha stewart and imclone"). The red real curve in Figure 4 shows the -NDCG values of the document ranking generated by  . The blue dashed lines in Figure 4 show the -NDCG values at

132

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

1.0

0.8

®-NDCG

0.6

0.4

0.2
search policy

raw policy

0.0

0

1

2

3

4

5

position

Figure 4: -NDCG at the top 5 positions for the ranking generated by the search policy  (red real curve) for query number 148. Blue dashed lines show the -NDCG values if the documents were selected by the raw policy p.

®-NDCG@5

0.7

0.6

0.5

0.4

0.3

0.2

search policy

raw policy

0.1

0

20

40

60

80

100

120

140

iteration

Figure 5: Performance curves of the document rankings generated with the raw policy p and with the search policy  , w.r.t. the training iterations. The curves illustrate the averaged performances over all of the training queries.

these 5 positions if the corresponding document was chosen by the raw policy p (note the preceding documents are still selected by  ). From the results, we can see that  improved p at the positions of 1, 3, and 5, showing the e ectiveness of MCTS at the online ranking.
We also conducted experiments to show the e ectiveness of the MCTS in o ine training. Speci cally, at the end of each training iteration, based on current ranking model, we tested the performances of the document ranking constructed with the raw policy p and with the MCTS search policy  , in term of -NDCG@5 on the training queries. Figure 5 shows the averaged -NDCG@5 scores among all of the training queries at each training iterations. It is not surprise that the document rankings generated by the search policy  are superior to that of generated by the raw policy p, at all of the training iterations. The results indicate that the lookahead MCTS can generate better diverse document rankings for training the model parameters, at all of the training iterations. The results also partially explained why "M2Div(without MCTS)", which did not conduct MCTS at the online time, can still outperform the baselines. The MCTS conducted at the training time produced better diverse document rankings for training the model parameters.
5.3.2 Ranking with policy or with value? In "M2Div(without MCTS)", we rely on the raw policy p for ranking the documents. In principle, the value function V can also be used for ranking, that is, at each state s a document (action) a^ is selected if a^ = arg maxa A(s) V (T (s, a)). We conducted experiments to show the performances of these two approaches and Figure 6 shows the test performance curves of the document rankings with the policy function and with the value function. From the results, we can see that the document rankings generated by the raw policy p are more stable and better than that of generated by the value function V . One possible reason for the phenomenon is that the ranking performance measure -NDCG@5 are not easy to estimate accurately, especially at the early stages of the ranking procedure. This is why p in stead of V is adopted in "M2Div(without MCTS)".

®-NDCG@5

0.50 0.45 0.40 0.35 0.30 0.25 0.20 0.15
0

policy value

20

40

60

80

100

120

140

iteration

Figure 6: Performance curves of the document rankings generated with the policy function and with the value function, w.r.t. training iterations. The curves illustrates the averaged performances over all of the test queries.

6 CONCLUSION
In this paper we have proposed a novel approach to learning diverse ranking model for search result diversi cation, referred to as M2Div. In contrast to existing methods that greedily select a locally optimal document at each of the ranking positions, M2Div conducts an exploratory decision making with the lookahead MCTS and thus can select a better document. MDP is used to model the ranking process and reinforcement learning is utilized to train the model parameters. M2Div o ers several advantages: ranking with both the shared policy function and the value function, high accuracy in ranking, and exibility of trading-o between accuracy and online ranking speed. Experimental results based on the TREC benchmark

133

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

datasets show that M2Div can signi cantly outperform the state-ofthe-art baselines including the heuristic methods of MMR, xQuAD and learning methods of R-LTR, PAMM, and MDP-DIV.
7 ACKNOWLEDGMENTS
This work was funded by the 973 Program of China under Grant No. 2014CB340401, the National Key R&D Program of China under Grants No. 2016QY02D0405, the National Natural Science Foundation of China (NSFC) under Grants No. 61773362, 61425016, 61472401, 61722211, and 20180290, and the Youth Innovation Promotion Association CAS under Grants No. 20144310, and 2016102.
REFERENCES
[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM '09). 5­14.
[2] Sumit Bhatia. 2011. Multidimensional Search Result Diversi cation: Diverse Search Results for Diverse Users. In Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '11). ACM, New York, NY, USA, 1331­1332.
[3] Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '98). 335­336.
[4] Ben Carterette and Praveen Chandar. 2009. Probabilistic Models of Ranking Novel Documents for Faceted Topic Retrieval. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). 1287­1296.
[5] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). 621­630.
[6] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '08). ACM, New York, NY, USA, 659­666.
[7] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversi cation. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 65­74.
[8] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. J. Mach. Learn. Res. 12 (July 2011), 2121­2159.
[9] Sreenivas Gollapudi and Aneesh Sharma. 2009. An Axiomatic Approach for Result Diversi cation. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). 381­390.
[10] Shengbo Guo and Scott Sanner. 2010. Probabilistic Latent Maximal Marginal Relevance. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '10). ACM, 833­834.
[11] Jiyin He, Vera Hollink, and Arjen de Vries. 2012. Combining Implicit and Explicit Topic Representations for Result Diversi cation. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 851­860.
[12] Katja Hofmann, Shimon Whiteson, and Maarten Rijke. 2013. Balancing Exploration and Exploitation in Listwise and Pairwise Online Learning to Rank for Information Retrieval. Inf. Retr. 16, 1 (Feb. 2013), 63­90.
[13] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversi cation Based on Hierarchical Intents. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management (CIKM '15). ACM, New York, NY, USA, 63­72.
[14] Zhengbao Jiang, Zhicheng Dou, Xin Zhao, Jian-Yun Nie, Ming Yue, and Ji-Rong Wen. 2018. Supervised Search Result Diversi cation via Subtopic Attention. IEEE Transactions on Knowledge and Data Engineering (2018).
[15] Zhengbao Jiang, Ji-Rong Wen, Zhicheng Dou, Wayne Xin Zhao, Jian-Yun Nie, and Ming Yue. 2017. Learning to Diversify Search Results via Subtopic Attention. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 545­554.
[16] Branislav Kveton, Csaba Szepesvári, Zheng Wen, and Azin Ashkan. 2015. Cascading Bandits: Learning to Rank in the Cascade Model. CoRR abs/1502.02763 (2015).
[17] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014. 1188­1196.

[18] Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2009. Enhancing Diversity, Coverage and Balance for Summarization Through Structure Learning. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). ACM, New York, NY, USA, 71­80.
[19] Zhongqi Lu and Qiang Yang. 2016. Partially Observable Markov Decision Process for Recommender Systems. CoRR abs/1608.07793 (2016).
[20] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings of the 37th International ACM
SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 587­596. [21] Lilyana Mihalkova and Raymond Mooney. 2009. Learning to Disambiguate Search Queries from Short Sessions. In Machine Learning and Knowledge Discovery in Databases. Lecture Notes in Computer Science, Vol. 5782. Springer. [22] Filip Radlinski and Susan Dumais. 2006. Improving Personalized Web Search Using Result Diversi cation. In Proceedings of the 29th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval (SIGIR '06). ACM, New York, NY, USA, 691­692. [23] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 784­791. [24] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting Query Reformulations for Web Search Result Diversi cation. In Proceedings of the 19th International Conference on World Wide Web (WWW '10). 881­890. [25] Rodrygo L. T. Santos, Jie Peng, Craig Macdonald, and Iadh Ounis. 2010. Explicit Search Result Diversi cation through Sub-queries. [26] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265­1295. [27] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. 2016. Mastering the game of Go with deep neural networks and tree search. Nature 529, 7587 (2016), 484­489. [28] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. 2017. Mastering the game of go without human knowledge. Nature 550, 7676 (2017), 354. [29] Xiaojie Wang, Zhicheng Dou, Tetsuya Sakai, and Ji-Rong Wen. 2016. Evaluating Search Result Diversity Using Intent Hierarchies. In Proceedings of the 39th
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). ACM, New York, NY, USA, 415­424. [30] Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Reinforcement Learning to Rank with Markov Decision Process. In Proceedings of the 40th
International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17). 945­948. [31] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). 113­122. [32] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). 395­404. [33] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, and Xueqi Cheng. 2017. Adapting Markov Decision Process for Search Result Diversi cation. In Proceed-
ings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17). 535­544. [34] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM Trans. Intell. Syst. Technol. 8, 3, Article 41 (Jan. 2017), 26 pages. [35] Hai-Tao Yu, Adam Jatowt, Roi Blanco, Hideo Joho, Joemon Jose, Long Chen, and Fajie Yuan. 2017. A concise integer linear programming formulation for implicit search result diversi cation. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining. ACM, 191­200. [36] Yisong Yue and Thorsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 1224­1231. [37] Yisong Yue and Thorsten Joachims. 2009. Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML '09). 1201­1208. [38] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Contentfree Document Re-ranking. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 1139­1142. [39] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversi cation. In Proceedings of the 37th International ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 293­302.

134

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Modeling Diverse Relevance Patterns in Ad-hoc Retrieval

Yixing Fan,, Jiafeng Guo,, Yanyan Lan,, Jun Xu,, Chengxiang Zhai and Xueqi Cheng,
University of Chinese Academy of Sciences, Beijing, China CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology,
Chinese Academy of Sciences, Beijing, China Department of Computer Science University of Illinois at Urbana-Champaign, IL, USA
fanyixing@software.ict.ac.cn,{guojiafeng,lanyanyan,junxu,cxq}@ict.ac.cn,czhai@illinois.edu

ABSTRACT
Assessing relevance between a query and a document is challenging in ad-hoc retrieval due to its diverse patterns, i.e., a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' need. Such diverse relevance patterns require an ideal retrieval model to be able to assess relevance in the right granularity adaptively. Unfortunately, most existing retrieval models compute relevance at a single granularity, either document-wide or passage-level, or use fixed combination strategy, restricting their ability in capturing diverse relevance patterns. In this work, we propose a data-driven method to allow relevance signals at different granularities to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals by modeling the semantic matching between a query and each passage of a document. The global decision layer accumulates local signals into different granularities and allows them to compete with each other to decide the final relevance score. Experimental results demonstrate that our HiNT model outperforms existing state-of-the-art retrieval models significantly on benchmark ad-hoc retrieval datasets.
CCS CONCEPTS
· Information systems  Learning to rank;
KEYWORDS
relevance patterns; ad-hoc retrieval; neural network
ACM Reference Format: Yixing Fan,, Jiafeng Guo,, Yanyan Lan,, Jun Xu,, Chengxiang Zhai and Xueqi Cheng,. 2018. Modeling Diverse Relevance Patterns in Adhoc Retrieval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3209978.3209980
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3209980

1 INTRODUCTION
A central question in ad-hoc retrieval is how to learn a generalizable function that can well assess relevance between a query and a document. One of the major difficulties for relevance assessment lies in that there might be diverse relevance patterns between a query and a document. As revealed by the evaluation policy in TREC ad-hoc task [6, 32], "a document is judged relevant if any piece of it is relevant (regardless of how small the piece is in relation to the rest of the document)1". In other words, a document could be relevant to a query as a whole or partially as long as it provides sufficient information for users' needs. Such diverse relevance patterns might be highly related to the heterogeneity of long documents in ad-hoc retrieval. As discussed by Robertson and Walker [30], there are two underlying hypotheses concerning document structures in relevance judgement, i.e. verbosity hypothesis and scope hypothesis [30]. With the Verbosity Hypothesis a long document might be relevant to a query as a whole, while with the Scope Hypothesis the relevant parts could be in any position of a long document, and thus it could be partially relevant to a query.
The diverse relevance patterns call for a retrieval model to be able to assess relevance at the right granularity adaptively in ad-hoc retrieval. Unfortunately, most existing retrieval models operate at a single granularity, either document-wide or passage-level. Specifically, document-wide approaches compare a document as a whole to a query. For example, most probabilistic retrieval methods (e.g., BM25 or language models) and learning-to-rank models [8, 15] rely on document-wide feature statistics for relevance computation. Obviously, such document-wide approaches are difficult to model finer-granularity relevance signals, leading to potential biases on the competition between long and short documents [25]. On the other hand, Passage-level approaches segment a document into passages and aggregate passage-level signals for relevance computation [4, 20, 31]. However, the performance of existing passagebased approaches is mixed when applied to a variety of test beds [35] by only using simple manually designed operations over the passage-level signals. There have been a few efforts [2, 35] trying to combine both document-wide and passage-level methods. For example, Bendersky et al. [2] integrated the query-similarity on a document and its passages using document-homogeneity. Wang et al. [35] combined the document retrieval results with passage retrieval results using a heuristic function [19]. However, by using a fixed combination strategy, these models cannot fully capture the diverse relevance patterns for different query-document pairs.
1 http://trec.nist.gov/data/reljudge_eng.html

375

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Recently, deep neural models have been applied to ad-hoc retrieval. These data-driven methods have shown their expressive power in end-to-end learning relevance matching patterns between queries and documents [10, 14, 23]. However, most existing neural matching models, either representation-focused [14] or interactionfocused [10], belong to the document-wide approaches. For example, the representation-focused models aim to learn a document representation to compare with the query representation, while the interaction-focused models learn from a matching matrix/histogram between a document and a query. To the best of our knowledge, so far there have been no neural matching model proposed to learn relevance signals from both document-wide and passage-level explicitly for modeling diverse relevance patterns in ad-hoc retrieval.
In this paper, we propose a data-driven method to automatically learn relevance signals at different granularities (i.e. passage-level and document-wide), and allow them to compete with each other for final relevance assessment. Specifically, we propose a HIerarchical Neural maTching model (HiNT) which consists of two stacked components, namely local matching layer and global decision layer. The local matching layer focuses on producing a set of local relevance signals between a query and each passage of a document. Many well-known information retrieval (IR) heuristics that characterize the relevance matching between a query and a passage can be encoded in this layer for high quality signal generation. Specifically, we employ a spatial GRU model [34] for the relevance matching between a query and a passage, which can well capture semantic relations, proximities, and term importance. The global decision layer aims to accumulate passage-level signals into different granularities and allow them to compete with each other to form the final relevance score. Flexible strategies are applied in this layer to model diverse relevance patterns. Specifically, we utilize a hybrid network architecture to accumulate local signals, and select signals from both passage-level and document-wide to generate the final relevance score.
We evaluate the effectiveness of the proposed model based on two representative ad-hoc retrieval benchmark datasets from the LETOR collection [28]. For comparison, we take into account several well-known traditional retrieval models, learning to rank models, and deep neural matching models. These models belong to document-wide, passage-level and hybrid approaches. The empirical results show that our model can outperform all the baselines in terms of all the evaluation metrics. We also provide detailed analysis on HiNT model, and conducte case studies to verify the diverse relevance patterns captured by our model over different query-document pairs.
2 RELATED WORK
A large number of retrieval methods have been proposed in the past few decades [4, 23, 28, 30]. Without loss of generality, we divide existing methods into three folds, namely document-wide approaches, passage-level approaches and hybrid approaches, based on what kind of relevance signals they rely on for relevance assessment. We will briefly review these studies in the follows.

2.1 Document-wide Approaches
Document-wide approaches, by its name, collect and make relevance assessment based on document-wide signals. There have been a large number of retrieval models under this branch. Firstly, traditional retrieval models [1, 30, 39], collect lexical matching signals (e.g., term frequency) from the whole document, and make relevance assessment under some probabilistic framework. For example, the well-known BM25 model [30] collects term frequencies and document length and employs a scoring function derived under the 2poisson model to compute relevance based on these document-wide signals. Secondly, most machine learning based retrieval methods, including learning to rank models and deep learning models, are also belong to this branch. For learning to rank methods [3, 15, 38], they typically involve two stages, a feature construction stage and a model learning stage. The feature construction stage could be viewed as to define (or collect) relevance signals for a document given a query. Typically, there are three type of features, including query dependent features, document independent features, and query-document dependent features. Most of these features are defined at document level, such as tf-idf scores, BM25 scores, and PageRank. Based on these features, linear [15] or non-linear [3, 38] models are learned to produce the relevance score by optimizing some ranking based loss functions. For deep learning methods [10, 14, 24, 26], they can be categorized into two types according to their architectures [10], namely representation-focused models and interaction-focused models. The representation-focused models, such as ARCI [13] and DSSM [14], aim to learn a low-dimensional abstractive representation for the whole document, and compare it with the query representation. The interaction-focused models, such as DRMM [10] and Duet [24], learn from a matching matrix/histogram between a document and a query to produce a set of document-wide matching signals for final relevance prediction. Although in interaction-focused models, document-wide signals are usually generated from local signals, there is no competition between document-wide signals and local signals for capturing diverse relevance patterns.
Since document-wide approaches take document as a whole, these methods are often difficult to model fine-granularity relevance signals. Meanwhile, by using document-wide statistics as relevance signals, it often leads to certain bias on the competition between long and short documents [25], since long documents are likely to contain stronger signals on average.
2.2 Passage-level Approaches
As opposed to document-wide methods, passage-level methods collect signals from passages to make relevance assessment on the document. Note here we focus on document retrieval using passage-level signals, and will not include the work taking passages as retrieval units [16].
In passage-level methods, documents are usually pre-segmented into small passages. Callan [4] studied how passages can be defined, and how passage signals can be incorporated into document retrieval. In [20], Liu et al. computed a language model for each passage as relevance signals. The final assessment is made by choosing the highest score from all the passages. Their results showed passage-based retrieval can provide more reliable performance than

376

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 1: The Architecture of the Hierarchical Neural Matching Model.

full document retrieval. In [21], Lv et al. proposed a positional language model in which the relevance score at each position can propagate to nearby positions within certain distance. In other words, each position can be viewed as a "soft passage" by aggregating language model scores within a context window. Based on the signals at each position, they proposed three strategies, namely best position strategy, multi-position strategy and multi- strategy, for final relevance assessment.
As we can see, passages are convenient text units for local signal collection to support flexible relevance assessment over documents. However, previous passage-level methods often employed simplified aggregation strategies, thus cannot well capture the diverse relevance patterns for different query-document pairs.
2.3 Hybrid Approaches
There also have been a few efforts [2, 35, 37] trying to combine both document-wide and passage-based methods. For example, Callan [4] conducted experiments on four TREC 1 and 2 collections and concluded that it was always better to combine documentwide scores and passage-level scores. A later study by Xi et al. [37] re-examined fixed-size window passages on TREC 4 and 5. Contrary to Callan [4], they did not obtain an improvement by linearly combine passage-level score and document-level score. Wang et al. [35] proposed a discriminative probabilistic model in capturing passage-level signals, and combined the document-level scores and passage-level scores through a heuristic function (i.e., CombMNZ function [19]). However, by using a unified combination strategy, these models cannot fully capture the diverse relevance patterns in different query-document pairs, leading to the mixed performance on different datasets [35, 37].
3 HIERARCHICAL NEURAL MATCHING MODEL
In this work, we introduce a HIerarchical Neural maTching (HiNT) model for ad-hoc retrieval to explicitly model the diverse relevance patterns. In an abstract level, the model consists of two stacked components, namely local matching layer and global decision layer. The local matching layer employs deep matching networks to automatically learn the passage-level relevance signals ; The global decision

layer accumulates passage-level signals into different granularities and allows them to compete with each other to generate the final relevance score. The architecture is illustrated in Figure 1. We will describe these components in detail in the follows.
3.1 Local Matching Layer
The local matching layer focuses on producing a set of passage-level relevance signals by modeling the relevance matching between a query and each passage of a document. Formally, each document D is first represented as a set of passages D = [P1, P2, ..., PK ], where K denotes the number of passages in a document. Then, a set of passage-level relevance signals E = [e1, e2, ..., eK ] are produced by applying some relevance matching model f over a query Q and each passage Pi .
ei = f (Pi , Q), i = 1, . . . , K .
There are two major questions concerning this layer, i.e., how to define the passage Pi and how to define the relevance matching model f . For passages, there have been three types of definitions: discourse, semantic, and window [4]. Discourse passages are defined based upon textual discourse units (e.g., sentences, paragraphs, and sections). Semantic passages are based upon the subject or content of the text (e.g., TextTiling). Window passages are obtained based upon a number of words. Among these methods, window passage is the most widely adopted due to its simplicity but surprisingly effectiveness as demonstrated by many previous passage-level retrieval models [2, 4, 31, 35].
For the relevance matching model, in general, any model that can address the relevance matching between a query and a passage can be leveraged here. For example, one may employ statistical language model [39], or use manually defined features [28, 36], or even employ some deep models for text matching [14, 34]. However, the quality of the passage-level signals produced by the relevance matching model is a critical foundation for the final relevance assessment. Therefore, we argue that a relevance matching model should be able to encode many well-known IR heuristics to be qualified in this layer. According to previous studies, such heuristics at least include the modeling of exact matching and semantic matching [7, 10], proximity [33], term importance [10] and so on.

377

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 2: The Architecture of Relevance Matching Network.

Based on these above ideas, in this work, we propose to use fixed-size window to define the passage, and employ an existing spatial GRU model [34] for the relevance matching between a query and each passage. This spatial GRU model is good at modeling the matching between two pieces of texts based on primitive word features, and has shown better performances as compared with other deep matching models [34]. We now describe the specific implementation in the follows.

3.1.1 The Input Layer. Following the idea in [11], term vectors
are employed as basic representations so that rich semantic relations
between query and document terms can be captured. Formally, both
query and document are represented as a sequence of term vectors denoted by Q = [w(1Q), ..., w(MQ)] and D = [w(1D), ..., w(ND)], where wi(Q), i = 1, ..., M and w(jD), j = 1, ..., N denotes a query term vector and a document term vector, respectively. To obtain the passages,
we follow previous approaches [2, 4, 35] to use fixed-size sliding
window to segment the document into passages. In this way, the passage is defined as P = [w(1P ), ..., w(LP )], where L denotes the window size.

3.1.2 Deep Relevance Matching Network. The architecture of the
relevance matching network is shown in Figure 2. Firstly, the term-
level interaction matrix is constructed based on the term vectors
from the query-passage pair. Here we constructed two matching matrices, a semantic matching matrix Mcos and an exact matching matrix (i.e., xor-matrix) Mxor , defined as follows:

Micjos

=

wi(Q )w(jP ) |wi(Q) | · |w(jP

)

|

,

Mixjor =

1, 0,

i f wi(Q) = w(jP ) otherwise

.

The key idea of two input matrices is to distinguish the exact matching signals from the semantic matching signals explicitly since the former provides critical information for ad-hoc retrieval as suggested by [7, 10]. Note that in Mcos exact matching and semantic matching signals are mixed together. To further incorporate term importance, we extend each element of Mij to a three-dimensional

vector Sij = [xi , yj , Mij ] by concatenating two corresponding compressed term embeddings as in [27], where xi = wQi  Ws and yj = w(jP)  Ws , here, Ws is the transformation parameter to be learned.
Based on these two query-passage interaction tensors, a spatial GRU (Gated Recurrent Units) is applied to generate the relevance matching evidences. The spatial GRU, also referred to as 2-dimensional Gated-RNN, is a special case of multidimensional RNN [9]. It is a recursive model which scans the input tensor from top left to bottom right:
-Hicjos = (-Hic-os1, j ,-Hic,ojs-1,-Hic-os1, j-1, Sicjos), -Hixjor = (-Hix-or1, j ,-Hix,ojr-1,-Hix-or1, j-1, Sixjor ), where  denotes the spatial GRU unit as described in [34], -Hicjos and-Hixjor denotes the hidden state of the spatial GRU over Scos and Sxor , respectively. We can take the last hidden representation HM,L as the matching output. The local relevance evidence -e is then generated by concatenating the two matching outputs:
-e = [-HcMo,sL,-HxMo,rL].
Furthermore, in order to enrich the relevance signals, we also applied the spatial GRU in the reverse direction, i.e., from bottom right to top left. The final passage-level signal is defined as the concatenation of the two-direction matching signals:
e = [-e , e-].
3.2 Global Decision Layer
Based on passage-level signals generated in the previous step, the global decision layer attempts to accumulate these signals into different granularities and allow them to compete with each other for final relevance assessment. As we have discussed before, the relevance patterns of a query-document pair can be rather flexible and diverse, allowing a document to be relevant to a query partially or as a whole. Accordingly, the global decision layer is expected to be able to accommodate various decision strategies, rather than using some restrictive combination rules [2, 35] .
In this work, we propose to employ a hybrid neural network architecture which has sufficient expressive power to support flexible relevance patterns. Before we describe the hybrid model, we first introduce two basic models under some simplified relevance assumptions.
1. Independent Decision (ID) Model assumes the independence among passage-level signals, and selects top-k signals directly for final relevance assessment. This model is under the assumption that a document is relevant if any piece of it can provide sufficient relevance information. Specifically, as shown in Figure 3(a), a dimension-wise k-max pooling layer is first applied over the passage-level signals to select top-k signals, and the selected signals are then concatenated and projected into a multi-layer perceptron to get the final decision score. 2. Accumulative decision (AD) Model accumulates the passage-level signals in a sequential manner, and selects top-k accumulated signals for relevance assessment. Here,

378

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 3: Different decision models based on the collected passage signals.

we adopt long-short term memory network (LSTM) [12], a powerful model for variable-length sequential data, to accumulate the relevance signals from each passage. Specifically, as shown in Figure 3(b), we first feed the passage-level signals into LSTM sequentially to generate the accumulated relevance signals at different positions.Based on the accumulated relevance signals, we then apply a dimension-wise k-max pooling layer to select top-k signals, and feed the selected signals into a multi-layer perceptron for final relevance assessment. Here, we also applied the LSTM in the reverse direction to accumulate the relevance signals, as user's reading can be in any direction of the document[29]. Note here if we directly use the last/first hidden state in LSTM as signals for relevance assessment, it would reduce to a document-wide method. By using k-max pooling over all the positions, we actually assume the relevance could be based on a text span flexible in scale, ranging from multiple passages to the whole document.
Based on the two basic models, now we introduce the Hybrid Decision (HD) Model as a specific implementation of the global decision layer. The HD model is a mixture of the previous two models, and picks top-k signals from passage-level or accumulated signals for final relevance assessment. Obviously, this is the most flexible relevance model, which allows a document to be assessed as a whole or partially adaptively. Specifically, as depicted in figure 3(c), we allow the relevance signals from different passages to compete with accumulated signals. Note here in order to make a fair competition, for the passage-level signals, we conduct an additional non-linear transformation to ensure a similar scale to the accumulated relevance signals.
vt = tanh(Wv et + bv ),
where vt denotes the t-th transformed passage signals, Wv and bv are parameters to be learned. We then apply a dimension-wise k-max pooling layer to select top-k signals, and feed the selected signals into a multi-layer perceptron for final assessment.
3.3 Model Training
Since the ad-hoc retrieval task is fundamentally a ranking problem, we utilize the pairwise ranking loss such as hinge loss to train our

Table 1: Statistics of the datasets used in this study.

#queries #docs #q_rel #rel_per_q

MQ2007 1692 65,323 1455

10.3

MQ2008 784 14,384 564

3.7

model. Specifically, given a triple (q, d+, d-), where d+ is ranked higher than d- with respect to a query q, the loss function is defined as:
L(q, d+, d-;  ) = max(0, 1 - s(q, d+) + s(q, d-)),
where s(q, d) denotes the relevance score for (q, d), and  includes the parameters in both local matching layer and global decision layer. The optimization is relatively straightforward with standard backpropagation. We apply stochastic gradient decent method Adam [17] with mini-batches(100 in size), which can be easily parallelized on a single machine with multi-cores.
4 EXPERIMENT
In this section, we conduct experiments to demonstrate the effectiveness of our proposed model on benchmark collections.
4.1 Experimental Settings
We first introduce our experimental settings, including datasets, baseline methods/implementations, and evaluation methodology.
4.1.1 Data Sets. To evaluate the performance of our model, we conducted experiments on two LETOR benchmark datasets [28]: Million Query Track 2007 (MQ2007) and Million Query Track 2008 (MQ2008). We choose these two datasets according to three criteria: 1) there is a large number of queries, 2) the original document content is available, and 3) the dataset is public. The first two criterias are important for learning deep neural models for ad-hoc retrieval, and the last one is critical for reproducibility. Both datasets use the GOV2 collection which includes 25 million documents in 426 gigabytes. The details of the two datasets are given in Table 1. As we can see, there are 1692 queries on MQ2007 and 784 queries on MQ2008. The number of queries with at least one relevant document is 1455 and 564, respectively. The average number of relevant document per query is about 10.3 and 3.7 on MQ2007 and MQ2008, respectively.

379

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

For pre-processing, all the words in documents and queries are white-space tokenized, lower-cased, and stemmed using the Krovetz stemmer [18]. Stopword removal is performed on query and document words using the INQUERY stop list [5]. Words occurred less than 5 times in the collection are removed from all the document. We further segmented documents into passages for all the models using passage-level information. We utilized fixed-size sliding window without overlap to generate passages. We have also studied the performance of different window size in Section 4.5.
4.1.2 Baselines Methods. We adopt three types of baselines for comparison, including traditional retrieve models, learning to rank models and deep matching models.
For traditional retrieval models, we consider both documentwide methods, passage-level methods, and hybrid methods:
BM25: The BM25 model [30] is a classical and highly effective document-wide retrieval model. MSP: The Max-Scoring Passage model [20] utilizes language model for each passage and rank the document according to the score of their best passage. PLM: The passage language model [2] integrates passagelevel and document-wide language model scores according to the document homogeneity for ad-hoc retrieval. PPM: The probabilistic passage model [35] is a discriminative probabilistic model in capturing passage-level signals, and combines document retrieval scores with passage retrieval scores through a linear interpolate function.
Learning to rank models include
AdaRank: AdaRank [38] is a representative pairwise model which aims to directly optimize the performance measure based on boosting approach. Here we utilize NDCG as the performance measure function. LambdaMart: LambdaMart [3] is a representative listwise model that uses gradient boosting to produce an ensemble of retrieval models. It is the state-of-the-art learning to rank algorithm.
Here, AdaRank and LambdaMart were implemented using RankLib2, which is a widely adopted learning to rank tool. All the learning to rank models leveraged the 46 human designed features from LETOR. Furthermore, since our model utilized passage-level information, we introduced 9 passage-based features for fair comparison. Specifically, we calculated tf-idf, BM25 and language model scores for each query-passage pair, and picked the maximum, minimum and average scores across passages as the new features for a document. We applied the full set of features (original+passage features) on both two learning to rank models for additional comparison, denoted by AdaRank(+P) and LambdaMart(+P), respectively.
Deep matching models include
DSSM: DSSM [14] is a neural matching model proposed for Web search. It consists of a word hashing layer, two nonlinear hidden layers, and an output layer. DRMM: DRMM [10] is a neural relevance model designed for ad-hoc retrieval. It consists of a matching histogram mapping, a feed forward matching network and a term gating network.
2 https://sourceforge.net/p/lemur/wiki/RankLib/

Duet: Duet [24] is a joint model which learns local lexical matching and global semantic matching together. DeepRank: DeepRank [27] is a state-of-the-art deep matching model which models relevance by simulating the human judgement process.
Here, for DSSM, we directly utilize the trained model3 released by their authors since training these complex models on small benchmark datasets could lead to severe over-fitting problem. For DeepRank4, we use the code released by their authors. For Duet model, we train it by ourselves since there is no trained model released. To avoid overfitting, we reduce the parameters of the convolutional network and fully-connected network to adapt the model to the limited size of LETOR datasets. Specifically, we set the filter size as 10 in both local model and global model, and the hidden size as 20 in the fully-connected layer. Other parameters are the same as the original paper.
We refer to our proposed model as HiNT5. For network configurations (e.g., numbers of layers and hidden nodes), we tuned the hyper-parameters via the validation set. Specifically, in the local matching layer, the dimension of the spatial GRU is set to 2 which is tuned in [1, 2, 3, 4]. In the global decision layer, the dimension of LSTM is set to 6 which is tuned in [4, 5, 6, 7, 8, 9, 10], the k-max pooling size is set to 10 which is tuned in [1, 5, 10, 15, 20], and the multi-layer perceptron is a 2-layers feed forward network without hidden layers. All the trainable parameters are initialized randomly by uniform distribution within [-0.1, 0.1]. Overall, the number of trainable parameters is about 930 in our HiNG model. Note that the MQ2008 dataset has much smaller query and document size, we find it is not sufficient to train deep models purely based on this dataset. Therefore, for all the deep models, we chose to use the trained model on MQ2007 as the initialization and fine tuned the model on MQ2008.
For all deep models based on term vector inputs, we used 50dimension term vectors. The term vectors were trained on wikipedia corpus6 using the CBOW model [22] with the default parameters7. Specifically, we used 10 as the context window size, 10 negative samples and a subsampling of frequent words with sampling threshold of 10-4. Out-of-vocabulary words are randomly initialized by sampling values uniformly from (-0.02, 0.02).
4.1.3 Evaluation Methodology. We follow the data partition on this dataset in Letor4.0 [28], and 5 fold cross-validation is conducted to minimize over-fitting as in [10]. Specifically, the parameters for each model are tuned on 4-of-5 folds. The last fold in each case is used for evaluation. This process is repeated 5 times, once for each fold. The results reported are the average over the 5 folds.
As for evaluation measure, precision (P), mean average precision (MAP) and normalized discounted cumulative gain (NDCG) at position 1, 5, and 10 were used in our experiments. We performed significant tests using the paired t-test. Differences are considered statistically significant when the p-value is lower than 0.05.
3 https://www.microsoft.com/en-us/research/project/dssm/ 4 https://github.com/pl8787/textnet-release 5 https://github.com/faneshion/HiN T 6 http://en.wikipedia.org/wiki/Wikipedia_database 7 https://code.google.com/archive/p/word2vec/

380

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Analysis on local matching layer on MQ2007. Significant improvement or degradation with respect to our implementation (Sxor +Scos+spatial GRU) is indicated (+/-) (p-value  0.05)

Local Matching Layer
Mxor +MLP Mcos +MLP Mhist +MLP Mxor +spatial GRU Mcos+spatial GRU Mxor +Mcos+spatial GRU Sxor +Scos+spatial GRU

Exact matching

    

Semantic matching
 
  

IR Heuristics Exact/Semantic Distinguished

 

Proximity
   

Term Importance


Performance

P@10
0.384- 0.329- 0.393- 0.387- 0.396- 0.405- 0.418

NDCG@10
0.435- 0.344- 0.447- 0.444- 0.449- 0.470- 0.490

MAP
0.461- 0.386- 0.469- 0.465- 0.470- 0.484- 0.502

4.2 Analysis on the HiNT Model
In this section we conducted experiments to compare different implementations of the two components in the HiNT model. Through these experiments, we try to gain a better understanding of the model.
4.2.1 Analysis on Local Matching Layer. As mentioned in Section 3.1, the local matching layer should be able to encode many well-known IR heuristics in order to well capture the relevance matching between a query and a passage. The heuristics at least include the modeling of exact matching and semantic matching signals, the differentiation between them, the modeling of proximity, the term importance, and so on. Here we conduct experiments to test a variety of implementations of the local matching layer which encode different IR heuristics by fixing the rest parts of the model.
The implementations include: (1) We apply a multi-layer perceptron over the exact matching matrix Mxor to produce the passagelevel signals. In this way, only exact matching signals are encoded into the passage-level signal; (2) We apply a multi-layer perceptron over the semantic matching matrix Mcos to produce the passagelevel signals. In this way, both exact and semantic matching signals are encoded but mixed together; (3) We follow the idea in [10] to turn the semantic matching matrix Mcos into matching histograms Mhist , and use a multi-layer perceptron to produce the passage-level signals. In this way, both exact matching and semantic matching signals are encoded and these two types of signals are differentiated by using the histogram; (4) We apply a spatial GRU over the exact matching matrix Mxor to produce the passage-level signals. In this way, only exact matching signals and proximity are encoded into the signals; (5) We apply a spatial GRU over the semantic matching matrix Mcos to produce the passage-level signals. In this way, exact matching and semantic matching signals are mixed and encoded together with proximity information. (6) We use a spatial GRU over both exact matching and semantic matching signals. Here, the exact matching and semantic matching signals can be clearly differentiated. Finally, we use our proposed implementation, i.e., a spatial GRU over the Sxor and Scos tensors, which can encode exact matching signals, semantic matching signals, proximity and term importance. The different implementations of the local matching layer as well as their performance results are shown in Table 2.
From the results we observe that, when modeling exact matching signals alone, Mxor + MLP can already obtain reasonably good retrieval performance. It indicates that exact matching signals are

performance (%)

HiNTID HiNTAD HiNTHD

50.2

50

49.0

48.3

48

47.2

46.4

46

44.6

44

42

41.8 40.5

40 38.9

38

36 P@10

NDCG@10

MAP

Figure 4: Performance comparison of HiNT over different decision models on MQ2007.

critical in ad-hoc retrieval [10]. Meanwhile, the performance drops significantly when semantic matching signals are mixed with exact matching signals (Mcos + MLP), but increases if these two types of signals are clearly differentiated (Mhist + MLP). These results demonstrate that semantic matching signals are also useful for retrieval, but should better be distinguished from exact matching signals if the deep model itself (e.g., MLP) cannot differentiate them. If the deep model can somehow implicitly distinguish these two types of signals, e.g., spatial GRU using input gates, we can observe better performance on the semantic matching matrix (Mcos + spatialGRU ) than that on the exact matching matrix (Mxor + spatialGRU ). However, we can further observe performance increase if we explicitly distinguish exact matching and semantic matching signals (Mxor + Mcos + spatialGRU ). Besides, the local matching layers using spatial GRU can in general obtain better results, indicating that proximity is very helpful for retrieval. Finally, by further considering term importance, our proposed implementation (Sxor + Scos + spatialGRU ) can outperform all the variants significantly. All the results demonstrate the importance of encoding a variety of IR heuristics in the local matching layer for a successful relevance judgement model.
4.2.2 Analysis on Global Decision Layer. We further study the effect of different implementations of the global decision layer. Here we compare the proposed hybrid decision model (i.e., HiNTHD) with the two basic decision models introduced in Section 3.2 (i.e.,

381

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Comparison of different retrieval models over the MQ2007 and MQ2008 datasets. Significant improvement or degradation with respect to HiNT is indicated (+/-) (p-value  0.05).

Model Name BM25 MSP PLM PPM
AdaRank LambdaMart AdaRank(+P) LambdaMart(+P)
DSSM DRMM Duet DeepRank HiNT
Model Name BM25 MSP PLM PPM
AdaRank LambdaMart AdaRank(+P) LambdaMart(+P)
DSSM DRMM Duet DeepRank HiNT

P@1 0.427- 0.361- 0.416- 0.431- 0.449- 0.481- 0.457- 0.484- 0.345- 0.450- 0.473- 0.508 0.515
P@1 0.408- 0.332- 0.396- 0.412- 0.434- 0.449- 0.428- 0.441- 0.341- 0.450- 0.452- 0.482- 0.491

P@5 0.388- 0.358- 0.389- 0.393- 0.403- 0.418- 0.408- 0.427- 0.359- 0.417- 0.428- 0.452- 0.461
P@5 0.337- 0.314- 0.326- 0.338- 0.342 0.346 0.345 0.348 0.284- 0.337- 0.341- 0.359- 0.367

MQ2007

P@10 0.366- 0.350- 0.371- 0.370- 0.372- 0.384- 0.380- 0.391- 0.352- 0.388- 0.398- 0.412- 0.418

NDCG@1 0.358- 0.302- 0.348- 0.361- 0.394- 0.412- 0.393- 0.413- 0.290- 0.380- 0.409- 0.441
0.447

MQ2008

P@10
0.245 0.236- 0.240- 0.241-
0.243 0.249 0.247 0.249 0.221- 0.242- 0.240- 0.252

NDCG@1 0.344- 0.283- 0.327- 0.350- 0.368- 0.376- 0.368- 0.372- 0.286- 0.381- 0.385- 0.406-

0.255 0.415

NDCG@5 0.384- 0.341- 0.377- 0.392- 0.410- 0.421- 0.408- 0.427- 0.335- 0.408- 0.431- 0.457- 0.463
NDCG@5 0.461- 0.415- 0.438- 0.464- 0.468 0.471 0.475 0.479 0.378- 0.466- 0.471- 0.496 0.501

NDCG@10 0.414- 0.378- 0.413- 0.424- 0.436- 0.446- 0.438- 0.454- 0.371- 0.440- 0.453- 0.482- 0.490
NDCG@10 0.220- 0.193- 0.208- 0.220- 0.221 0.230 0.225 0.232 0.178- 0.219- 0.216 0.240 0.244

MAP 0.450- 0.422- 0.449- 0.453- 0.460- 0.468- 0.467- 0.473- 0.409- 0.467- 0.474- 0.497 0.502
MAP 0.465- 0.426- 0.452- 0.468- 0.476 0.478 0.478 0.480 0.391- 0.473- 0.476- 0.498- 0.505

HiNTID and HiNTAD) by fixing the rest parts of the model. The comparison results are shown in Figure 4. As we can see, the simplest relevance model HiNTID performs worst. It seems that selecting passage-level signals independently might be too simple to capture diverse relevance patterns. Meanwhile, HiNTAD performs better than HiNTID, indicating that it is more beneficial to make relevance assessment based on accumulated signals from a variety of text spans. Finally, HiNTHD achieves the best performance in terms of all the evaluation measures. This further indicates that there might be very diverse relevance patterns across different query-document pairs. By allowing competition between passage-level and accumulated signals, the expressive power of the HD model is the largest, leading to the best performance among the three variants.
4.3 Comparison of Retrieval Models
In this section, we compare our proposed HiNT against existing retrieval models over the two benchmark datasets. Note here we refer HiNT to the model using hybrid decision model based on both exact matching and semantic matching tensors. The main results of our experiments are summarized in Table 3.
Firstly, for the traditional models, we can see that BM25 is a strong baseline which performs better than MSP. The relative poor performance of MSP indicates that it is deficient in capturing the diverse relevance patterns by only using the passage-level signals.

By integrating document-wide with passage-level signals, the performance of PLM and PPM is mixed compared with BM25, demonstrating the deficiency of the simple combination strategy, which is consistent with previous findings [35]. Secondly, all the learning to rank models perform significantly better than the traditional retrieval models. This is not surprising since learning to rank models can make use of rich features, where BM25 scores and LM scores are typical features among them. Among the two learning to rank models, LambdaMart performs better. Moreover, we can see that adding passage-level features might improve the performance of learning to rank models, but not consistent on different datasets. For example, the performance of AdaRank and LambdaMart in terms of P@1 on MQ2008 drops when adding passage features. Thirdly, as for the deep matching models, we can see that DSSM obtain relatively poor performances on both datasets, even cannot compete with the traditional retrieval models. This is consistent with previous findings [10], showing that one may not work well on ad-hoc retrieval by only leveraging the cosine similarity between high-level abstract representations of short query and long document. As for DRMM and Duet, they have achieved relative better performance compared with DSSM. This may due to the fact that they are specifically designed for the relevance matching in adhoc retrieval, and they have incorporated important IR heuristics into their model. However, they can only reach comparable performance as learning to rank models by only using document-wide

382

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 5: Examples of different decision strategies over the query-document pair.

matching signals. The recently introduced DeepRank achieves a relative better performance by simulating the human judgement process. However, DeepRank aggregates relevance signals from query-centric contexts to form document-wide relevance score for each query term, it cannot well capture the diverse relevance patterns between query-document pairs.
Finally, we observe that HiNT can outperform all the existing models in terms of all the evaluation measures on both datasets by allowing the competition between the passage-level signals and document-wide signals explicitly. It is worth noting that in the learning to rank methods, there are many human designed features including those document-wide and passage-level matching signals, as well as those about the quality of the document (e.g., PageRank). While in our HiNT, all the assessment are purely learned from the primitive word features of queries and documents. Therefore, the superior performance of HiNT suggests the importance and effectiveness of modeling the diverse relevance patterns for ad-hoc retrieval.
4.4 Impact of Passage Size
Since we leverage the fixed-size sliding windows to segment a document into passages, we would like to study the effect of the passage size on the ranking performance. Here we report the performance results on MQ2007 with the passage size set as 10, 50, 100, 200, 300 and 500 words. As shown in Figure 6, the performance first increases and then drops with the increase of the passage size. The possible reason might be that too small passage size may hurt the quality of passage signals (i.e., relevance matching between the query and the passage) due to the information sparsity, while too large passage size would produce limited number of coarse passagelevel signals which restrict the ability of the global decision layer. Our results shows that the best performance can be achieved when the passage size is set to 100 words on MQ2007.

performance (%)

52 50 48 47.5
46.4
46 44 42 40 39.7 38
10

P@10

NDCG@10

50.2

49.0

49.2

48.5 48.1

47.3

MAP
48.3 47.6
47.2 46.7

41.8

41.0

40.3

40.4

40.2

50

100

200

300

500

passage size

Figure 6: Performance comparison of HiNT over different passage sizes on MQ2007.

4.5 Case Study
To better understand what can be learned by HiNT, here we conduct some case studies. For better visualization and analysis, we simplified our model by replacing k-max pooling with max pooling so that only the most significant signal is used in decision. Based on the learned model, we pick up a query and a relevant document, and plot all the signals E used in the hybrid decision model along with the corresponding document content. Here each small bar in E denotes a passage or accumulated signal at that position, with the color corresponding to the signal strength. We highlight the final selected signal with a red box and the corresponding passages with green background color.
As shown in Figure 5, we can find two significantly different decision strategies between a query and a document. In the first case, the document is relevant to a query because of a strong passage signal. By checking the query and the document, we find that

383

Session 3D: Learning to Rank II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

the query is "average annual salary in 1975" which conveys very specific information need, and the passage at the 5-th position (i.e., the strongest signal) contains a table whose content can well address this information need. In the second case, the document is relevant to a query because of a strong accumulated signal. Again by checking the query and the document, we find that the query is about the "cause of migraine headache" which is informational, and the document is mostly relevant to this query with many passages addressing this problem (i.e., from the beginning to the 15-th passage).
The two cases show that there are indeed quite diverse relevance patterns in real-world retrieval scenario, and our HiNT model can capture these diverse relevance patterns successfully.
5 CONCLUSIONS
In this paper, we have introduced a hierarchical neural matching model to capture the diverse relevance patterns in ad-hoc retrieval. The model consists of two components, namely local matching layer and global decision layer. We employed deep neural network in both layers to support high-quality relevance signal generation and flexible relevance assessment strategies, respectively. Experimental results on two benchmark datasets demonstrate that our model can outperform all the baseline models in terms to all the evaluation metrics, especially state-of-the-art learning to rank methods that use manually designed features.
For future work, it would be interesting to try other implementation of the two components in HiNT, e.g., to employ some attentionbased neural network for the global decision layer. We would also like to expand our model to accommodate features beyond relevance matching, e.g. PageRank, to help improve the retrieval performance.
6 ACKNOWLEDGMENTS
This work was funded by the 973 Program of China under Grant No. 2014CB340401, the National Natural Science Foundation of China (NSFC) under Grants No. 61425016, 61472401, 61722211, and 20180290, the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102, and the National Key R&D Program of China under Grants No. 2016QY02D0405.
REFERENCES
[1] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 357­389.
[2] Michael Bendersky and Oren Kurland. 2008. Utilizing passage-based language models for document retrieval. In European Conference on Information Retrieval. Springer, 162­174.
[3] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11 (2010), 23­581.
[4] James P Callan. 1994. Passage-level evidence in document retrieval. In SIGIR. Springer-Verlag New York, Inc., 302­310.
[5] James P Callan, W Bruce Croft, and John Broglio. 1995. TREC and TIPSTER experiments with INQUERY. Information Processing & Management 31, 3 (1995), 327­343.
[6] Charles LA Clarke, Falk Scholer, and Ian Soboroff. 2005. The TREC 2005 Terabyte Track.. In TREC.
[7] Hui Fang and ChengXiang Zhai. 2006. Semantic term matching in axiomatic approaches to information retrieval. In SIGIR. ACM, 115­122.
[8] Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram Singer. 2003. An efficient boosting algorithm for combining preferences. Journal of machine learning research 4, Nov (2003), 933­969.
[9] Alex Graves and Jürgen Schmidhuber. 2009. Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in neural information

processing systems. 545­552. [10] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance
matching model for ad-hoc retrieval. In CIKM. ACM, 55­64. [11] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. Semantic matching
by non-linear word transportation for information retrieval. In CIKM. ACM, 701­ 710. [12] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735­1780. [13] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In NIPS. 2042­2050. [14] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM. ACM, 2333­2338. [15] Thorsten Joachims. 2006. Training linear SVMs in linear time. In SIGKDD. ACM, 217­226. [16] Marcin Kaszkiel and Justin Zobel. 1997. Passage retrieval revisited. In SIGIR, Vol. 31. ACM, 178­185. [17] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). [18] Robert Krovetz. 1993. Viewing morphology as an inference process. In SIGIR. ACM, 191­202. [19] Joon Ho Lee. 1997. Analyses of multiple evidence combination. In ACM SIGIR Forum, Vol. 31. ACM, 267­276. [20] Xiaoyong Liu and W. Bruce Croft. 2002. Passage retrieval based on language models. In CIKM. 375­382. [21] Yuanhua Lv and Cheng Xiang Zhai. 2009. Positional language models for information retrieval. In SIGIR. 299­306. [22] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111­3119. [23] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv preprint arXiv:1705.01509 (2017). [24] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match using Local and Distributed Representations of Text for Web Search. In Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 1291­1299. [25] Seung-Hoon Na. 2015. Two-stage document length normalization for information retrieval. ACM Transactions on Information Systems (TOIS) 33, 2 (2015), 8. [26] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2016. A study of matchpyramid models on ad-hoc retrieval. arXiv preprint arXiv:1606.04648 (2016). [27] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng. 2017. DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. In CIKM. ACM, 257­266. [28] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A benchmark collection for research on learning to rank for information retrieval. Information Retrieval 13, 4 (2010), 346­374. [29] Keith Rayner. 1998. Eye movements in reading and information processing: 20 years of research. Psychological bulletin 124, 3 (1998), 372. [30] Stephen E Robertson and Steve Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. In SIGIR. Springer-Verlag New York, Inc., 232­241. [31] Gerard Salton, James Allan, and Chris Buckley. 1993. Approaches to passage retrieval in full text information systems. In SIGIR. ACM, 49­58. [32] Mark Sanderson. 2010. Test collection based evaluation of information retrieval systems. Now Publishers Inc. [33] Tao Tao and ChengXiang Zhai. 2007. An exploration of proximity measures in information retrieval. In SIGIR. ACM, 295­302. [34] Shengxian Wan, Yanyan Lan, Jun Xu, Jiafeng Guo, Liang Pang, and Xueqi Cheng. 2016. Match-SRNN: Modeling the Recursive Matching Structure with Spatial RNN. arXiv preprint arXiv:1604.04378 (2016). [35] Mengqiu Wang and Luo Si. 2008. Discriminative probabilistic models for passage based retrieval. In SIGIR. ACM, 419­426. [36] Ho Chung Wu, Robert WP Luk, Kam-Fai Wong, and KL Kwok. 2007. A retrospective study of a hybrid document-context based retrieval model. Information processing & management 43, 5 (2007), 1308­1331. [37] Wensi Xi, Richard Xu-Rong, Christopher SG Khoo, and Ee-Peng Lim. 2001. Incorporating window-based passage-level evidence in document retrieval. Journal of information science 27, 2 (2001), 73­80. [38] Jun Xu and Hang Li. 2007. Adarank: a boosting algorithm for information retrieval. In SIGIR. ACM, 391­398. [39] Chengxiang Zhai and John Lafferty. 2001. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. In SIGIR. ACM, New York, NY, USA, 334­342. DOI:http://dx.doi.org/10.1145/383952.384019

384

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling

Chenyan Xiong
Carnegie Mellon University cx@cs.cmu.edu
Jamie Callan
Carnegie Mellon University callan@cs.cmu.edu
ABSTRACT
This paper presents a Kernel Entity Salience Model (KESM) that improves text understanding and retrieval by better estimating entity salience (importance) in documents. KESM represents entities by knowledge enriched distributed representations, models the interactions between entities and words by kernels, and combines the kernel scores to estimate entity salience. The whole model is learned end-to-end using entity salience labels. The salience model also improves ad hoc search accuracy, providing effective ranking features by modeling the salience of query entities in candidate documents. Our experiments on two entity salience corpora and two TREC ad hoc search datasets demonstrate the effectiveness of KESM over frequency-based and feature-based methods. We also provide examples showing how KESM conveys its text understanding ability learned from entity salience to search.
KEYWORDS
Text Understanding, Entity Salience, Entity-Oriented Search
ACM Reference Format: Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Tie-Yan Liu. 2018. Towards Better Text Understanding and Retrieval through Kernel Entity Salience Modeling. In Proceedings of The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '18). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3209982
1 INTRODUCTION
Natural language understanding has been a long desired goal in information retrieval. In search engines, the process of text understanding begins with the representations of query and documents. The representations can be bag-of-words, the set of words in the text, or bag-of-entities, which uses automatically linked entity annotations to represent texts [10, 20, 25, 29].
With the representations, the next step is to estimate the term (word or entity) importance in text, which is also called term salience
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3209982

Zhengzhong Liu
Carnegie Mellon University liu@cs.cmu.edu
Tie-Yan Liu
Microsoft Research tie-yan.liu@microsoft.com
estimation [8, 9]. The ability to know which terms are salient (important and central) to the meaning of texts is crucial to many text-related tasks. In ad hoc search, the document ranking is often determined by the salience of query terms in them, which is typically estimated by combining frequency-based signals such as term frequency and inverse document frequency [5].
Effective as it is, frequency is not equal to salience. For example, a Wikipedia article about an entity may not repeat the entity the most frequently; a person's homepage may only mention her name once; a frequently mentioned term may be a stopword. In word-based retrieval, many approaches have been developed to better estimate term importance [3]. However, in entity-based representations [20, 26, 29], while entities convey richer semantics [1], entity salience estimation is a rather immature task [8, 9] and its effectiveness in search has not yet been explored.
This paper focuses on improving text understanding and retrieval by better estimating entity salience in documents. We present a Kernel Entity Salience Model (KESM) that estimates entity salience end-to-end using neural networks. Given annotated entities in a document, KESM represents them using Knowledge Enriched Embeddings and models the interactions between entities and words using a Kernel Interaction Model [27]. In the entity salience task [9], the kernel scores from the interaction model are combined by KESM to estimate entity salience, and the whole model, including the Knowledge Enriched Embeddings and Kernel Interaction Model, is learned end-to-end using a large number of salience labels.
KESM also improves ad hoc search by modeling the salience of query entities in candidate documents. Given a query-document pair and their entities, KESM uses its kernels to model the interactions of query entities with the entities and words in the document. It then merges the kernel scores to ranking features and combines these features to rank documents. In ad hoc search, KESM can either be trained end-to-end when sufficient ranking labels are available, or be first pre-trained on the salience task and then adapted to search as a salience ranking feature extractor.
Our experiments on a news corpus [9] and a scientific proceeding corpus [29] demonstrate KESM's effectiveness in the entity salience task. It outperforms previous frequency-based and feature-based models by large margins, while requires much less linguistic preprocessing than the feature-based model. Our analyses find that KESM has a better balance on popular (head) entities and rare (tail) entities when predicting salience. In contrast, frequency-based or feature-based methods are heavily biased towards the most popular entities--less attractive to users as they are more expected. Also,

575

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

KESM is less sensitive to document length while frequency-based methods are not as effective on shorter documents.
Our experiments on TREC Web Track search tasks show that KESM's text understanding ability in estimating entity salience also improves search accuracy. The salience ranking features from KESM, pre-trained on the news corpus, outperform both word-based and entity-based features in learning to rank, despite various differences in the salience and search tasks. Our case studies find interesting examples showing that KESM favors documents centering on query entities over those merely mentioning them. We find it encouraging that the fine-grained text understanding ability of KESM--the ability to model the consistency and interactions between entities and words in texts--is indeed valuable to ad hoc search.
The next section discusses related work. Section 3 describes the Kernel Entity Salience Model and its application to entity salience estimation. Section 4 discusses its application to ad hoc search. Experimental methodology and results for entity salience are presented in Sections 5 and Section 6. Those for ad hoc search are in Sections 7 and Section 8. Section 9 concludes.
2 RELATED WORK
Representing and understanding texts is a key challenge in information retrieval. The standard approaches in modern information retrieval represent a text by a bag-of-words; they model term importance using frequency-based signals such as term frequency (TF), inverse document frequency (IDF), and document length [5]. The bag-of-words representation and frequency-based signals are the backbone of modern information retrieval and have been used by many unsupervised and supervised retrieval models [5, 14].
Nevertheless, bag-of-words and frequency-based statistics only provide shallow text understanding. One way to improve the text understanding is to use more meaningful language units than words in text representations. These approaches include the first generation of search engines that were based on controlled vocabularies [5] and also the recent entity-oriented search systems which utilize knowledge graphs in search [7, 15, 20, 24, 29]. In these approaches, texts are often represented by entities, which introduce information from knowledge graphs to search systems.
In both word-based and entity-based text representations, frequency signals such as TF and IDF provide good approximations for the importance or salience of terms (words or entities) in the query or documents. However, solely relying on frequency signals limits the search engine's text understanding capability; many approaches have been developed to improve term importance estimation.
In the word space, the query term weighting research focuses on modeling the importance of words or phrases in the query. For example, Bendersky et al. use a supervised model to combine the signals from Wikipedia, search log, and external collections to better estimate term importance in verbose queries [2]; Zhao and Callan predict the necessity of query terms using evidence from pseudo relevance feedback [30]; word embeddings have also been used as features in supervised query term importance prediction [31]. These methods in general leverage extra signals to model how important a term is to capture search intents. They can improve the performance of retrieval models compared to frequency-based term weighting.

The word importance in documents can also be estimated by graph-based approaches [3, 18, 21]. Instead of using isolated words, the graph-based approaches connect words by co-occurrence or proximity. Then graph ranking algorithms, for example, PageRank, are used to estimate term importance in a document. The graph ranking scores reflect the centrality and connectivity of words and are able to improve standard retrieval models [3, 21].
In the entity space, modeling term importance is even more crucial. Unlike word-based representations, the entity-based representations are often automatically constructed and inevitably include noises. The noisy query entities have been a major bottleneck for entity-oriented search and often required manual cleaning [7, 10, 15]. Along this line, a series of approaches have been developed to model the importance of entities in a query, for example, latent-space learning to rank [23] and hierarchical ranking models [26]. These approaches learn the importance of query entities and the ranking of documents jointly using ranking labels. The features used to describe the entity importance include IR-style features [23] and NLP-style features from entity linking [26].
Nevertheless, previous research on modeling entity salience mainly focused on query representations, while the entities in document representations are still weighted by frequencies, i.e. in the bag-of-entities model [26, 29]. Recently, Dunietz and Gillick [9] proposed the entity salience task using the New York Times corpus [22]; they consider the entities that are annotated in the expert-written summary to be salient to the article, enabling them to automatically construct millions of training data. Dojchinovski et al. constructed a deeper study and found that crowdsource workers consider entity salience an intuitive task [8]. Both of them demonstrated that the frequency of an entity is not equal to its salience; a supervised model with linguistic and semantic features is able to outperform frequency significantly, though mixed findings have been found with graph-based methods such as PageRank.

3 KERNEL ENTITY SALIENCE MODEL
This section presents our Kernel Entity Salience Model (KESM). Compared to the feature-based salience models [8, 9], KESM uses neural networks to learn the representation of entities and their interactions for salience estimation.
The rest of this section first describes the overall architecture of KESM and then how it is applied to the entity salience task.

3.1 Model Architecture
As shown in Figure 1, KESM includes two main components: the Knowledge Enriched Embedding (Figure 1a) and the Kernel Interaction Model (Figure 1b).
Knowledge Enriched Embedding (KEE) encodes each entity e into its distributed representation vìe . It is achieved by first using an embedding layer that maps the entity to an embedding:

e -V eì.

Entity Embedding

V is the parameters of the embedding layer to be learned. An advantage of entities is that they are associated with external
semantics in the knowledge graph, for example, synonyms, descriptions, types, and relations. Instead of only using eì, KEE enriches

576

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Target Entity "Barack Obama"

Entity Embedding

Knowledge Enriched Embedding (KEE)

Obama American politician
presidency


Max
... CNN ... Pooling

Description Words

(a) Knowledge Enriched Embedding (KEE)

KEE of Document

 ,E

Entities

...

Entity

...

... Kernels

KEE of Target Entity

Cosine Similarity

RBF Kernels

Word

...

... Kernels

Embeddings of

...

Document Words

 ,W

(b) Kernel Interaction Model (KIM)

Figure 1: KESM Architecture. (a): Entities are represented using embeddings enriched by their descriptions. (b): The salience of an entity in a document is estimated by kernels that model its interactions with entities and words in the document. Squares are continuous vectors (embeddings) and circles are scalars (cosine similarities).

the entity representation with its description, for example, the first paragraph of its Wikipedia page.
Specifically, given the description D of the entity e, KEE uses a Convolutional Neural Network (CNN) to compose the words in D: {w1, ..., wp , ..., wl }, into one embedding:

wp -V wìp , Cp = W c · wìp:p+h , vìD = max(C1, ..., Cp , ..., Cl-h ).

Word Embedding CNN Filter
Description Embedding

It embeds the words into wì using the embedding layer, composes the

word embeddings using CNN filters, and generates the description embeddings vìD using max-pooling. W c and h are the weights and
length of the CNN.

vìD is then combined with the entity embedding eì by projection:

vìe = W p · (eì  vìD).

KEE Embedding

 is the concatenation operator and W p is the projection weights.

vìe is the KEE vector for e. It incorporates the external information from the knowledge graph and is to be learned as part of KESM.

Kernel Interaction Model (KIM) models the interactions of a

target entity with entities and words in the document using their

distributed representations.

Given a document d, its annotated entities E = {e1, ...ei ..., en }, and its words W = {w1, ...wj ..., wm }, KIM models the interactions of a target entity ei with E and W using kernels [6, 27]:

KI M(ei , d) = (ei , E)  (ei , W).

(1)

The entity kernels (ei , E) model the interaction between ei and document entities E:

(ei , E) = {1(ei , E), ...k (ei , E)..., K (ei , E)},

(2)

2

cos(vìei , vìej ) - µk

k (ei , E) = exp -
ej E

2k2

.

(3)

vìei and vìej are the KEE embeddings of ei and ej . k (ei , E) is the k-th RBF kernel with mean µk and variance k2. If (µk = 1, k  ), k counts the entity frequency. Otherwise, it models the interactions

between the target entity ei and other entities in the KEE representation space. One view of kernels is that they count the number of entities whose similarities with ei are in its region (µk , k2); the other view is that the kernel scores are the votes from other entities
in a certain neighborhood (kernel region) of the current entity. Similarly, the word kernels (ei , W) model the interactions be-
tween ei and document words W:

(ei , W) = {1(ei , W), ...k (ei , W)..., K (ei , W)}, (4)

k (ei , W) =

exp

wj W

-

cos(vìei , wìj ) - µk 2k2

2

.

(5)

wìj is the word embedding of wj , mapped by the same embedding parameters (V ). The word kernels k (ei , W) model the interactions between ei and document words, gathering `votes' from words for ei in the corresponding kernel regions.
For each entity ei , KEE encodes it to vìei and KIM models its interactions with entities and words in the document. The kernel scores KI M(ei , d) include signals from three sources: the description of the entity in the knowledge graph, its interactions with the docu-
ment entities, and its interactions with the document words. The
utilization of these kernel scores depends on the specific task: entity
salience estimation (Section 3.2) or document ranking (Section 4).

3.2 Entity Salience Estimation

The application of KESM in the entity salience task is simple. Combining the KIM kernel scores gives the salience score of the corresponding entity:

f (ei , d) = W s · KI M(ei , d) + bs .

(6)

f (ei , d) is the salience score of ei in d. W s and bs are parameters for salience estimation.
Learning: The entity salience training data are labels about document-entity pairs that indicate whether the entity is salient to the document. The salience label of entity ei to document d is:

y(ei , d) =

+1, -1,

if ei is a salient entity in d; otherwise.

577

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Ranking Score ,

Ranking Features

...

Document
Family of Barack Obama... ............

Word Embeddings
...

Entity Linking

"Obama"
...
"Family"

KEE ...

Document Entities

Log Sum
Word
... Kernels ...
KIM
... Entity ...
Kernels

Entity Obama Family Tree Linking
Query

KEE
"Obama" "Family Tree" Query Entities

Figure 2: Ranking with KESM. KEE embeds the entities. KIM calculates the kernel scores of query entities VS. document entities and words. The kernel scores are combined to ranking features and then to the ranking score.

We use pairwise learning to rank [14] to train KESM:

max(0, 1 - f (e+, d) + f (e-, d)),

(7)

e +,e - d

w.r.t. y(e+, d) = +1 & y(e-, d) = -1.

The loss function enforces KESM to rank the salient entities e+ ahead of the non-salient ones e- within the same document.
In the entity salience task, KESM is trained end-to-end by backpropagation. During training, the gradients from the labels are first propagated to the Kernel Interaction Model (KIM) and then the Knowledge Enriched Embedding (KEE). KESM updates the kernel weights; KIM converts the gradients from kernels to `expectations' on the distributed representations--how the entities and words should be allocated in the space to better reflect salience; KEE updates its embeddings and parameters according to these `expectations'. The knowledge learned from the training labels is encoded and stored in the model parameters, mainly the embeddings [27].

4 RANKING WITH ENTITY SALIENCE
This section presents the application of KESM in ad hoc search. Ranking: Knowing which entities are salient in a document in-
dicates a deeper text understanding ability [8, 9]. The improved text understanding should also improve search accuracy: the salience of query entities in a document reflects how focused the document is on the query, which is a strong indicator of relevancy. For example, a web page that exclusively discusses Barack Obama's family is more relevant to the query "Obama Family Tree" than those that just mention his family members.

Table 1: Datasets used in the entity salience task. New York Times are news articles and salient entities are those in the expert-written news summaries. Semantic Scholar are paper abstracts and salient entities are those in the titles.

# of Documents Entities Per Doc Salience Per Doc Unique Word Unique Entity

New York Times Train Dev Test 526k 64k 64k 198 197 198 27.8 27.8 28.2 609k 278k 281k 622k 319k 317k

Semantic Scholar Train Dev Test 800k 100k 100k
66 66 66 7.3 7.3 7.3 921k 300k 301k 331k 162k 162k

The ranking process of KESM following this intuition is illustrated in Figure 2. It first calculates the kernel scores of the query entities in the document using KEE and KIM. Then it merges the kernel scores from multiple query entities to ranking features and uses a ranking model to combine these features.
Specifically, given query q, query entities Eq , candidate document d, document entities Ed , and document words Wd , the ranking score is calculated as:

f (q, d) = W r · (q, d),

(8)

(q, d) =

log

ei Eq

KI M(ei , d) |Ed |

.

(9)

KI M(ei , d) are the kernel scores of the query entity ei in document d, calculated by the KIM and KEE modules described in last section. |Ed | is the number of entities in d. W r is the ranking parameters and (q, d) are the salience ranking features.
Several adaptations have been made to apply KESM in search. First, Equation (9) normalizes the kernel scores by the number of entities in the document (|Ed |), making them more comparable across different documents. In the entity salience task, this is not required because the goal is to distinguish salient entities from nonsalient ones in the same document. Second, there can be multiple entities in the query and their kernel scores need to be combined to model query-document relevance. The combination is done by log-sum, following language model approaches [5].
Learning: In the search task, KESM is trained using standard pairwise learning to rank and relevance labels:

max(0, 1 - f (q, d+) + f (q, d-)).

(10)

d+ D+,d- D-

D+ and D- are the relevant and irrelevant documents. f (q, d+) and f (q, d-) are the ranking scores calculated by Equation (8).

There are two ways to train KESM for ad hoc search. First, when

sufficient ranking labels are available, for example, in commercial

search engines, the whole KESM model can be learned end-to-end

by back-propagation from Equation (10). On the other hand, when

not enough ranking labels are available for end-to-end learning,

the KEE and KIM can be first trained using the labels from the entity salience task. Only the ranking parameters W r need to be learned

from relevance labels. As a result, the knowledge learned from the

salience labels is adapted to ad hoc search through the ranking

features, which can be used in any learning to rank system.

578

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Entity salience features used by the LeToR baseline [9]. The features are extracted via various natural language processing techniques, as listed in the Source column.

Name Frequency First Location Head Word Count Is Named Entity Coreference Count Embedding Vote

Description The frequency of the entity The location of the first sentence that contains the entity The frequency of the entity's first head word in parsing Whether the entity is considered as a named entity The coreference frequency of the entity's mentions Votes from other entities through cosine embedding similarity

Source Entity Linking Entity Linking Dependency Parsing Named Entity Recognition Entity Coreference Resolution Entity Embedding (Skip-gram)

5 EXPERIMENTAL METHODOLOGY FOR ENTITY SALIENCE ESTIMATION
This section presents the experimental methodology for the entity salience task. It mainly follows the setup by Dunietz and Gillick [9] with some revisions to facilitate the applications in search. An additional dataset is also introduced.
Datasets1 used include New York Times and Semantic Scholar. The New York Times corpus has been used in previous work [9]. It includes more than half million news articles and expert-written summarizes [22]. Among all entities annotated on a news article, those that also appear in the summary of the article are considered as salient entities; others are not [9]. The Semantic Scholar corpus contains one million randomly sampled scientific publications from the index of SemanticScholar. org, the academic search engine from Allen Institute for Artificial Intelligence. The full texts of the papers are not released. Only the abstract and title of the paper content are available. We treat the entities annotated on the abstract as the candidate entities of a paper and those also annotated on the title as salient. The entity annotations on both corpora are Freebase entities linked by TagMe [11]. All annotations are included to ensure coverage, which is important for effective text representations [20, 29]. The statistics of the two corpora are listed in Table 1. The Semantic Scholar corpus has shorter documents (paper abstracts) and a smaller entity vocabulary because its papers are mostly in the computer science and medical science domains. Baselines: Three baselines from previous research are compared: Frequency, PageRank, and LeToR. Frequency [9] estimates the salience of an entity by its term frequency. It is a straightforward but effective baseline in many related tasks. IDF is not as effective in entity-based text representations [20, 29], so we used only frequency counts. PageRank [9] estimates the salience score of an entity using its PageRank score [3]. We conduct a supervised PageRank on a fully connected graph. The nodes are the entities in the document. The edges are the embedding similarities of the connected nodes. The entity embeddings are configured and learned in the same manner as KESM. Similar to previous work [9], PageRank is not as effective in the salience task. The results reported are from the best setup we found: a one-step random walk linearly combined with Frequency. LeToR [9] is a feature-based learning to rank (entity) model. It is trained using the same pairwise loss with KESM, which we found more effective than the pointwise loss used in prior research [9].
1Available at http://boston.lti.cs.cmu.edu/appendices/SIGIR2018-KESM/

We re-implemented the features used by Dunietz and Gillick [9]. As listed in Table 2, the features are extracted by various linguistic and semantic techniques including entity linking, dependency parsing, named entity recognition, and entity coreference resolution. Besides the standard Frequency count, the Head Word Count considers syntactic signals when counting entities; the Coreference Count considers all mentions that refer to an entity as its appearances when counting frequency.
The entity embeddings are trained on the same corpus using Google's Word2vec toolkit [19]. Entity linking is done by TagMe; all entities are kept [20, 29]. Other linguistic and semantic preprocessing are done by the Stanford CoreNLP toolkit [16].
Compared to Dunietz and Gillick [9], we do not include the headline feature because it uses information from the expert-written summary and does not improve the performance much anyway; we also replace the head-lex feature with Embedding Vote which has similar effectiveness but is more efficient.
Evaluation Metrics: We use the ranking-focused evaluation metrics: Precision@{1, 5} and Recall@{1, 5}. These metrics circumvent the problem of selecting a cutoff threshold for each individual document in classification evaluation metrics [9]. Statistical significances are tested by permutation test with p < 0.05.
Implementation Details: The hyper-parameters of KESM are configured following popular choices or previous research. The dimension of entity embeddings, word embeddings, and CNN filters are all set to 128. The kernel pooling layers use the same pre-defined kernels as in previous research [27]: one exact match kernel (µ = 1,  = 1e - 3) and ten soft match kernels equally splitting the cosine similarity range [-1, 1] (µ  {-0.9, -0.7, ..., 0.9} and  = 0.1). The length of the CNN used to encode entity description is set to 3 which is tri-gram. The entity descriptions are fetched from Freebase. The first 20 words (the gloss sentence) of the description are used. The words or entities that appear less than 2 times in the training corpus are replaced by "Unk_word" or "Unk_entity".
The parameters include the embeddings V , the CNN weights W c , the projection weights W p , and the kernel weights W s , bs . They are learned end-to-end using Adam optimizer, size 64 mini-batching, and early-stopping on the development split. V is initialized by the skip-gram embeddings of words and entities jointly trained on the training corpora, which takes several hours [26]. With our PyTorch implementation, KESM usually only needs one pass on the training data and converges within several hours on a typical GPU. In comparison, LeToR takes days to extract its features because parsing and coreference are costly.

579

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Entity salience performances on New York Times and Semantic Scholar. (E), (W), and (K) mark the resources used by
KESM: Entity kernels, Word kernels, and Knowledge enrichment. KESM is the full model. Relative performances over LeToR are
shown in the percentages. W/T/L are the number of documents a method improves, does not change, and hurts, compared to LeToR. , , §, and ¶ mark the statistically significant improvements over Frequency, PageRank, LeToR§, and KESM (E)¶.

Method Frequency PageRank LeToR KESM (E) KESM (EK) KESM (EW) KESM

Precision@1

0.5840

-8.53%

0.5845

-8.46%

0.6385

­

0.6470§ 0.6528§ ¶ 0.6767§ ¶

+1.33% +2.24% +5.98%

0.6866§¶ +7.53%

New York Times

Precision@5

Recall@1

0.4065 0.4069

-11.82% 0.0781 -11.73% 0.0782

-11.92% -11.80%

0.4610 0.4782§ 0.4769§ 0.5018§ ¶ 0.5080§ ¶

­ +3.73% +3.46% +8.86% +10.21%

0.0886 0.0922§ 0.0920§ 0.0989§ ¶ 0.1010§ ¶

­ +4.03% +3.82% +11.57% +13.93%

Recall@5

0.2436

-14.44%

0.2440

-14.31%

0.2848

­

0.3049§ 0.3026§ 0.3277§ ¶

+7.05% +6.27% +15.08%

0.3335§¶ +17.10%

W/T/L 5,622/38,813/19,154 5,655/38,841/19,093
­/­/­ 19,778/27,983/15,828 18,619/29,973/14,997 22,805/26,436/14,348 23,290/26,883/13,416

Method Frequency PageRank LeToR KESM (E) KESM (EK) KESM (EW) KESM

Precision@1

0.3944

-9.99%

0.3946

-9.94%

0.4382

­

0.4793§ +9.38% 0.4901§¶ +11.84%

0.5097§¶ +16.31%

0.5169§¶ +17.96%

Semantic Scholar

Precision@5

Recall@1

0.2560 0.2561

-11.38% 0.1140 -11.34% 0.1141

-12.23% -12.11%

0.2889 0.3192§ 0.3161§ 0.3311§ ¶ 0.3336§ ¶

­ +10.51% +9.43% +14.63% +15.47%

0.1299 0.1432§ 0.1492§ ¶ 0.1555§ ¶ 0.1585§ ¶

­ +10.26% +14.91% +19.77% +22.09%

Recall@5

0.3462

-13.67%

0.3466

-13.57%

0.4010

­

0.4462§ 0.4449§

+11.27% +10.95%

0.4671§¶ +16.50%

0.4713§¶ +17.53%

W/T/L 11,155/64,455/24,390 11,200/64,418/24,382
­/­/­ 27,735/56,402/15,863 28,191/54,084/17,725 32,592/50,428/16,980 32,420/52,090/15,490

6 SALIENCE EVALUATION RESULTS
This section first presents the overall evaluation results for the entity salience task. Then it analyzes the advantages of modeling salience over counting frequency.
6.1 Entity Salience Performance
Table 3 shows the experimental results for the entity salience task. Frequency provides reasonable estimates of entity salience. The most frequent entity is often salient to the document; the Precision@1 is rather high, especially on the New York Times corpus. PageRank barely improves Frequency, although its embeddings are trained by the salience labels. LeToR, on the other hand, significantly improves both Precision and Recall of Frequency [9], which is expected as it has much richer features from various sources.
KESM outperforms all baselines significantly. Its improvements over LeToR are more than 10% on both datasets with only one exception: Precision@1 on New York Times. The improvements are also robust: About twice as many documents are improved (Win) than hurt (Loss).
We also conducted ablation studies on the source of evidence in KESM. Those marked with (E) include the entity kernels; those with (W) include word kernels; those with (K) enrich the entity embeddings with description embeddings. All variants include the entity kernels (E); otherwise the performances significantly dropped in our experiments.
KESM performs better than all of its variants, showing that all three sources contributed. Individually, KESM (E) outperforms all baselines. Compared to PageRank, the only difference is that KESM (E) uses kernels to model the interactions which are much more

powerful than the raw embedding similarities used in PageRank [27]. KESM (EW) always significantly outperforms KESM (E). The interaction between an entity and document words conveys useful information, the distributed representations make them easily comparable, and the kernels model the word-entity interactions effectively. Knowledge enrichment (K) provides mixed results. A possible reason is that the training data is large enough to train good entity embeddings. Nevertheless, we find that adding the external knowledge makes the model stable and converged faster.
6.2 Modeling Salience VS. Counting Frequency
This experiment provides two analyses that study the advantage of KESM over counting frequency.
Ability to Model Tail Entities. The first advantage of KESM is that it is able to model the salience of less frequent (tail) entities. To demonstrate this effect, Figure 3 illustrates the distribution of predicted-salient entities in different frequency ranges. The entities with top k highest predicted scores are predicted-salient, while k is the number of salient entities in the ground truth.
In both datasets, the frequency-based methods are highly biased towards the head entities: The top 0.1% most popular entities receive almost two-times more salience predictions from Frequency than in ground truth. This is an intrinsic bias of frequency-based methods which not only limits their effectiveness but also attractiveness-- less unexpected entities are selected.
In comparison, the distributions of KESM are much closer to the ground truth. KESM does a better job in modeling tail entities because it estimates salience not only by frequency but also by modeling the interactions between entities and words. A tail entity can be estimated salient if many other entities and words in the

580

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

50%

50%

40%

40%

30%

30%

20%

20%

10%

10%

0%

0%

<0.1% [0.1%, [0.5%, [1%, [2%, [3%, [4%, >5%

<0.1% [0.1%, [0.5%, [1%, [2%, [3%, [4%, >5%

0.5%) 1%) 2%) 3%) 4%) 5%)

0.5%) 1%) 2%) 3%) 4%) 5%)

Frequency LeToR KESM Ground Truth

Frequency LeToR KESM Ground Truth

(a) New York Times

(b) Semantic Scholar

Figure 3: The distribution of salient entities predicted by different models. The entities are binned by their frequencies in testing data. The bins are ordered from most frequent (Top 0.1%) to less frequent (right). The x-axes mark the percentile range of each group. The y-axes are the fraction of salient entities in each bin. The histograms are ordered the same as the legends.

0.6

0.6

0.4

0.4

0.2

0.2

0 175 518 876 1232 32k (20%) (40%) (60%) (80%) (100%)
Frequency LeToR KESM

0 122 172 228 302 1323 (20%) (40%) (60%) (80%) (100%)
Frequency LeToR KESM

(a) New York Times

(b) Semantic Scholar

Figure 4: Performances on documents with varying lengths (number of words). The x-axes are the maximum length of the documents and the percentile of each group. The y-axes mark the performances on Precision@5. The histograms are ordered the same as the legends.

document are closely related to it. For example, there are many entities and words describing various aspects of an entity in its Wikipedia page; the entities and words on a personal homepage are probably related to the person. These entities and words can `vote up' the title entity or the person because they are strongly connected to it/her. The ability to model such interactions with distributed representations and kernels is the main source of KESM's text understanding capability.
Reliable on Short Documents. The second advantage of KESM is its reliability on short texts. To demonstrate it, we analyzed the performances of models on documents of varying lengths. Figure 4 groups the testing documents into five bins by their lengths (number of words), ordered from short (left) to long (right). Their upper bounds and percentiles are marked on the x-axes. The Precision@5 of corresponding methods are marked on the y-axes.
Both Frequency and LeToR (whose features are also mostly frequency-based) are less reliable on shorter documents. The advantages of KESM are more significant when documents are shorter, while even in the longest bins where documents have thousands of words, KESM still outperforms Frequency and LeToR. Solely counting frequency is not sufficient to understand documents. The interactions between words and entities provide richer evidence and help KESM perform more reliably on shorter documents.

7 EXPERIMENTAL METHODOLOGY FOR AD HOC SEARCH
This section presents the experimental methodology for the ad hoc search task. It follows a popular setup in recent entity-oriented search research [26]2.
Datasets are from the TREC Web Track ad hoc search tasks, a widely used search benchmark. It includes 200 queries for the ClueWeb09 corpus and 100 queries for the ClueWeb12 corpus. The `Category B' subsets of the two corpora and corresponding relevance judgments are used.
The ClueWeb09-B rankings re-ranked the top 100 documents retrieved by sequential dependency model (SDM) queries [17] with standard post-retrieval spam filtering [7]. On ClueWeb12-B13, SDM queries are not better than unstructured queries, and spam filtering provides mixed results; thus, we used unstructured queries and no spam filtering on this dataset, as in prior research [26]. All documents were parsed by Boilerpipe to title and body fields [13]. The query and document entities are from Freebase and were annotated by TagMe [11]. All entities are kept. It leads to high coverage and medium precision, the best setting found in prior research [25].
Evaluation Metrics are NDCG@20 and ERR@20, official evaluation metrics of TREC Web Tracks. Statistical significances are tested by permutation test (randomization test) with p < 0.05.
Baselines: The goal of our experiments is to explore the usage of entity salience modeling in ad hoc search. To this purpose, our experiments focus on evaluating the effectiveness of KESM's entity salience features in standard learning to rank; the proper baselines are the ranking features from word-based matches (IRFusion) and entity-based matches (ESR [29]). Unsupervised retrieval with words (BOW) and entities (BOE) are also included.
BOW is the base retrieval model, which is SDM on ClueWeb09-B and Indri language model on ClueWeb12-B.
BOE is the frequency-based retrieval with bag-of-entities [26]. It uses TagMe annotations and exact-matches query and documents in the entity space. It performs similarly to the entity language model [20] as they use the same information.
IRFusion uses standard word-based IR features such as language model, BM25, and TFIDF, applied to body and title fields. It is obtained from previous research [26].
2Available at http://boston.lti.cs.cmu.edu/appendices/SIGIR2017_word_entity_duet/

581

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Ad hoc search accuracy of KESM when used as ranking features in learning to rank. Relative performances over IRFusion
are shown in the percentages. W/T/L are the number of queries a method improves, does not change, or hurts, compared with IRFusion. , , §, and ¶ mark the statistically significant improvements over BOE, IRFusion, ESR§, and ESR+IRFusion¶. BOW is
the base retrieval model, which is SDM in ClueWeb09-B and language model in ClueWeb12-B13.

Method BOW BOE IRFusion ESR KESM
ESR+IRFusion KESM+IRFusion

ClueWeb09-B

NDCG@20

ERR@20

0.2496

-5.26% 0.1387

-10.20%

0.2294

-12.94% 0.1488

-3.63%

0.2635 0.2695 0.2799

­ 0.1544 +2.30% 0.1607 +6.24% 0.1663

­ +4.06% +7.68%

0.2791

+5.92% 0.1613

+4.46%

0.2993§¶ +13.58% 0.1797§¶ +16.38%

W/T/L 62/38/100 74/25/101
­/­/­ 80/39/81 85/35/80
91/34/75 98/35/67

ClueWeb12-B13

NDCG@20

0.1060 -12.02%

0.1173

-2.64%

0.1205

­

0.1166

-3.22%

0.1301§ +7.92%

ERR@20

0.0863

-6.67%

0.0950

+2.83%

0.0924

­

0.0898

-2.81%

0.1103§¶ +19.35%

0.1281

+6.30% 0.0951

+2.87%

0.1308§ +8.52% 0.1079§¶ +16.77%

W/T/L 35/22/43 44/19/37
­/­/­ 30/23/47 43/25/32
45/24/31 43/23/34

Table 5: Ranking performances of IRFusion, ESR, and KESM with title or body field individually. Relative performances (percent-
ages) and Win/Tie/Loss are calculated by comparing with IRFusion on the same field.  and  mark the statistically significant improvements over IRFusion and ESR, also on the same field.

Method IRFusion-Title ESR-Title KESM-Title
IRFusion-Body ESR-Body KESM-Body

ClueWeb09-B

NDCG@20

0.2584 -3.51%

0.2678

­

0.2780 +3.81%

ERR@20

0.1460 -5.16%

0.1540

­

0.1719 +11.64%

0.2550 +0.48% 0.1427 -3.44%

0.2538

­ 0.1478

­

0.2795 +10.13% 0.1661 +12.37%

W/T/L 83/48/69
­/­/­ 91/46/63
80/46/74 ­/­/­
96/39/65

ClueWeb12-B13

NDCG@20

ERR@20

0.1187 +6.23% 0.0894

+3.14%

0.1117

­ 0.0867

­

0.1199 +7.36% 0.0923 +6.42%

0.1115 +4.61% 0.0892

-3.51%

0.1066

­ 0.0924

­

0.1207 +13.25% 0.1057 +14.44%

W/T/L 41/23/36
­/­/­ 35/28/37
36/30/34 ­/­/­
43/24/33

ESR is the entity-based ranking features obtained from previous research [26]. It includes both exact and soft match signals in the entity space [29]. The differences with KESM are that in ESR, the query and documents are represented by frequency-based bagof-entities [29] and the entity embeddings are pre-trained in the relation inference task [4].
Implementation Details: As discussed in Section 4, the TREC benchmarks do not have sufficient relevance labels for effective end-to-end learning; we pre-trained the KEE and KIM of KESM using the New York Time corpus and used them to extract salience ranking features. The entity salience features are combined by the same learning to rank model (RankSVM [12]) as used by IRFusion and ESR, with the same cross validation setup [26]. Similar to ESR, the base retrieval score is included as a feature in KESM. In addition, we also concatenate the features of ESR or KESM to IRFusion to evaluate their effectiveness when combined with word-based features. The resulting feature sets ESR+IRFusion and KESM+IRFusion were evaluated exactly the same as they were individually.
As a result, the comparisons of KESM with LeToR and ESR hold out all other factors and directly investigate the effectiveness of the salience ranking features in a widely used learning to rank model (RankSVM). Given the current exploration stage of entity salience in information retrieval, we believe this is more informative than mixing entity salience signals into more sophisticated ranking systems [23, 26], in which many other factors come into play.

8 SEARCH EVALUATION RESULTS
This section presents the evaluation results and case study in the ad hoc search task.
8.1 Overall Result
Table 4 lists the ranking evaluation results. The three supervised methods, IRFusion, ESR, and KESM, all use the exact same learning to rank model (RankSVM) and only differ in their features. ESR+IRFusion and KESM+IRFusion concatenate the two feature groups and use RankSVM to combine them.
On both ClueWeb09-B and ClueWeb12-B13, KESM features are more effective than IRFusion and ESR features. On ClueWeb12B13, KESM individually outperforms other features significantly by 8 - 20%. On ClueWeb09-B, KESM provides more novel ranking signals; KESM+IRFusion significantly outperforms ESR+IRFusion. The fusion on ClueWeb12-B13 (KESM+LeToR) is not as successful perhaps because of the limited ranking labels on ClueWeb12-B13.
To better investigate the effectiveness of entity salience in search, we evaluated the features on individual document fields. Table 5 shows the ranking accuracies of the three feature groups when only the title field (Title) or the body field (Body) is used. As expected, KESM is more effective on the body field than on the title field: Titles are less noisy and perhaps all title entities are salient--not much new information is provided by salience modeling; on the other hand, body texts are longer and more complicated, providing more opportunities for better text understanding.

582

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 6: Examples from queries that KESM improved or hurt, compared to ESR. Documents are selected from those that ESR and KESM disagreed. The descriptions are manually written to reflect the main topics of the documents.

Query ER TV Show Wind Power Hurricane Irene Flooding in Manville NJ
Query Fickle Creek Farm
Illinois State Tax
Battles in the Civil War

Query Entities "ER (TV Series)" "TV Program"
"Wind Power "
"Hurricane Irene" "Flood"; "Manville, NJ"
Query Entities "Malindi Fickle" "Stream"; "Farm"
"Illinois"; "State Government"
"US Tax" "Battles" "Civil War"

Cases that KESM Improved ESR Preferred Document clueweb09-enwp02-22-20096 "List of films in Wiki without article" clueweb12-0200wb-66-32730 "Home solar power systems" clueweb12-0705wb-49-04059 "Disaster funding for Hurricane Irene"
Cases that KESM Hurt ESR Preferred Document clueweb09-en0003-97-27345 "Hotels near Fickle Creak" clueweb09-enwp01-67-20725 "Sales taxes in the United States, Wikipedia" clueweb09-enwp03-20-07742 "List of American Civil War battles"

KESM Preferred Document clueweb09-enwp00-55-07707 "ER ( TV series ) - Wikipedia" clueweb12-0009wb-54-01932 "Wind energy | Alternative Energy HQ" clueweb12-0715wb-81-29281 "Videos and news about Hurricane Irene"
KESM Preferred Document clueweb09-en0005-66-00576 "List of breading farms" clueweb09-en0011-23-05274 "Retirement-related general purpose taxes by State" clueweb09-enwp01-30-04139 "List of wars in the Muslim world"

The salience ranking features also behave differently with ESR and IRFusion. As shown by the W/T/L ratios in Table 4 and Table 5, more than 70% query rankings are changed by KESM. The ranking evidence provided by KESM features is from the interactions of query entities with the entities and words in the candidate documents. This evidence is learned from the entity salience corpus and is hard to be described by traditional frequency-based features.
8.2 Case Study
The last experiment provides case studies on how KESM transfers its text understanding ability to search, by comparing the rankings of KESM-Body with ESR-Body. Both ESR and KESM match query and documents in the entity space, but ESR uses frequency-based bag-ofentities to represent documents while KESM uses entity salience. We picked the queries where KESM-Body improved or hurt compared to ESR-Body and manually examined the documents they disagreed. The examples are listed in Table 6.
The improvements from KESM are mainly from its ability to determine whether a candidate document emphasizes the query entities or just mentions the query terms. As shown in the top half of Table 6, KESM promotes documents where the query entities are more salient: the Wikipedia page about the ER TV show, a homepage about wind power, and a news article about the hurricane. On the other hand, ESR's frequency-based ranking might be confused by web pages that only partially talk about the query topic. It is hard for ESR to exclude those web pages because they also mention the query entities multiple times.
Many errors KESM made are due to the lack of text understanding on the query side. KESM focuses on modeling the salience of entities in the candidate documents and its ranking model treats all query entities equally. As shown in the lower half of Table 6, the query entities may contain errors, for example, "Malindi Fickle", or general entities that blur the (perhaps implied) query intent, for example "Civil War", "State government", and "US Tax'. These query entities

do not align well with the information needs and thus mislead KESM. Modeling the entity salience in queries is a different task which is more about understanding search intents. To address these error cases may require a deeper fusion of KESM in more sophisticated ranking systems that can handle noisy query entities [26, 28].
9 CONCLUSION
This paper presents KESM, the Kernel Entity Salience Model that estimates the salience of entities in documents. KESM represents entities and words with distributed representations, models their interactions using kernels, and combines the kernel scores to estimate entity salience. The semantics of entities in the knowledge graph--their descriptions--are also incorporated to enrich entity embeddings. In the entity salience task, the whole model is trained end-to-end using automatically generated salience labels.
In addition to the entity salience task, KESM is also applied to ad hoc search and ranks documents by the salience of query entities in them. It calculates the kernel scores of query entities in the document, combines them to salience ranking features, and uses a ranking model to predict the query-document ranking score. When ranking labels are scarce, the ranking features can be extracted by pre-trained distributed representations and kernels from the entity salience task and then used by standard learning to rank. These ranking features convey KESM's text understanding ability learned from entity salience labels to search.
Our experiments on two entity salience corpora, a news corpus (New York Times) and a scientific publication corpus (Semantic Scholar), demonstrate the effectiveness of KESM in the entity salience task. Significant and robust improvements are observed over frequency and feature-based methods. Compared to those baselines, KESM is more robust on tail entities and shorter documents; its Kernel Interaction Model is more powerful than the raw embedding similarities in modeling term interactions. Overall, KESM is a stronger model with a more powerful architecture.

583

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Our experiments on ad hoc search were conducted on the TREC Web Track queries and two ClueWeb corpora. In both corpora, the salience features provided by KESM trained on the New York Times corpus outperform both word-based ranking features and frequency-based entity-oriented ranking features, despite differences between the salience task and the ranking task. The advantages of the salience features are more observed on the document bodies on which deeper text understanding is required.
Our case studies on the winning and losing queries of KESM illustrate the influences of the salience ranking features: they distinguish documents in which the query entities are the core topic from those where the query entities are only partial to their central ideas. Interestingly, this leads to both winning cases--better text understanding leads to more accurate search--and also losing cases: when the query entities do not align well with the underlying search intent, emphasizing them ends up misleading the document ranking.
We find it very encouraging that KESM successfully transfers the text understanding ability from entity salience estimation to search. Estimating entity salience is a fine-grained text understanding task that focuses on the detailed interactions between entities and words. Previously it was uncommon for text processing techniques at this granularity to be as effective in information retrieval. Often shallower methods worked better for search. However, the finegrained text understanding provided by KESM--the interaction and consistency between query entities with the document entities and words--actually improves the ranking accuracy. We view this work as an encouraging step from "search by matching" to "search with meanings" [1] and hope it will motivate more future explorations towards this direction.
10 ACKNOWLEDGMENTS
This research was supported by National Science Foundation (NSF) grant IIS-1422676 and DARPA grant FA8750-12-2-0342 under the DEFT program. Any opinions, findings, and conclusions in this paper are the authors' and do not necessarily reflect the sponsors'.
REFERENCES
[1] Hannah Bast, Björn Buchhold, Elmar Haussmann, and others. 2016. Semantic search on text and knowledge bases. Foundations and Trends in Information Retrieval 10, 2-3 (2016), 119­271.
[2] Michael Bendersky, Donald Metzler, and W. Bruce Croft. 2011. Parameterized concept weighting in verbose queries. In Proceedings of the 34th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2011). ACM, 605­614.
[3] Roi Blanco and Christina Lioma. 2012. Graph-based term weighting for information retrieval. Information Retrieval 15, 1 (2012), 54­92.
[4] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NIPS 2013). NIPS, 2787­2795.
[5] W Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. Search Engines: Information Retrieval in Practice. Addison-Wesley Reading.
[6] Zhuyun Dai, Chenyan Xiong, Jamie Callan, and Zhiyuan Liu. 2018. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining (WSDM 2018). ACM, 126­134.
[7] Jeffrey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2014). ACM, 365­374.
[8] Milan Dojchinovski, Dinesh Reddy, Tomás Kliegr, Tomas Vitvar, and Harald Sack. 2016. Crowdsourced Corpus with Entity Salience Annotations.. In Proceedings of

the 10th Edition of the Languge Resources and Evaluation Conference (LREC 2016). [9] Jesse Dunietz and Daniel Gillick. 2014. A New Entity Salience Task with Millions
of Training Examples.. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2014). ACL, 205­ 209. [10] Faezeh Ensan and Ebrahim Bagheri. 2017. Document retrieval model through semantic linking. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 181­190. [11] Paolo Ferragina and Ugo Scaiella. 2010. Fast and accurate annotation of short texts with Wikipedia pages. arXiv preprint arXiv:1006.3498 (2010). [12] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2002). ACM, 133­142. [13] Christian Kohlschütter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text features. In Proceedings of the third ACM international conference on Web Search and Data Mining (WSDM 2010). ACM, 441­450. [14] Tie-Yan Liu. 2009. Learning to rank for Information Retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009), 225­331. [15] Xitong Liu and Hui Fang. 2015. Latent entity space: A novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18, 6 (2015), 473­503. [16] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014). ACL, 55­60. [17] Donald Metzler and W Bruce Croft. 2005. A Markov random field model for term dependencies. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2005). ACM, 472­479. [18] Rada Mihalcea and Paul Tarau. 2004. TextRank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing (EMNLP 2004). ACL, 404­411. [19] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 27th Advances in Neural Information Processing Systems 2013 (NIPS 2013). NIPS, 3111­3119. [20] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document retrieval using entity-based language models. In Proceedings of the 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016). ACM, 65­74. [21] François Rousseau and Michalis Vazirgiannis. 2013. Graph-of-word and TWIDF: New approach to ad hoc IR. In Proceedings of the 22nd ACM International Conference on Conference on Information & Knowledge Management (CIKM 2013). ACM, 59­68. [22] Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium, Philadelphia 6, 12 (2008), e26752. [23] Chenyan Xiong and Jamie Callan. 2015. EsdRank: Connecting query and documents through external semi-structured data. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management (CIKM 2015). ACM, 951­960. [24] Chenyan Xiong and Jamie Callan. 2015. Query expansion with Freebase. In Proceedings of the fifth ACM International Conference on the Theory of Information Retrieval (ICTIR 2015). ACM, 111­120. [25] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2016. Bag-of-Entities representation for ranking. In Proceedings of the sixth ACM International Conference on the Theory of Information Retrieval (ICTIR 2016). ACM, 181­184. [26] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-entity duet representations for document ranking. In Proceedings of the 40th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2017). ACM, 763­772. [27] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. 2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of the 40th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2017). ACM, 55­64. [28] Chenyan Xiong, Zhengzhong Liu, Jamie Callan, and Eduard H. Hovy. 2017. JointSem: Combining query entity linking and entity based document ranking. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM 2017). 2391­2394. [29] Chenyan Xiong, Russell Power, and Jamie Callan. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 26th International Conference on World Wide Web (WWW 2017). ACM, 1271­1279. [30] Le Zhao and Jamie Callan. 2010. Term necessity prediction. In Proceedings of the 19th ACM International on Conference on Information and Knowledge Management (CIKM 2010). ACM, 259­268. [31] Guoqing Zheng and James P. Callan. 2015. Learning to reweight terms with distributed representations. In Proceedings of the 38th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2015). ACM, 575­584.

584

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

On-the-fly Table Generation

Shuo Zhang
University of Stavanger shuo.zhang@uis.no
ABSTRACT
Many information needs revolve around entities, which would be better answered by summarizing results in a tabular format, rather than presenting them as a ranked list. Unlike previous work, which is limited to retrieving existing tables, we aim to answer queries by automatically compiling a table in response to a query. We introduce and address the task of on-the- y table generation: given a query, generate a relational table that contains relevant entities (as rows) along with their key properties (as columns). This problem is decomposed into three speci c subtasks: (i) core column entity ranking, (ii) schema determination, and (iii) value lookup. We employ a feature-based approach for entity ranking and schema determination, combining deep semantic features with taskspeci c signals. We further show that these two subtasks are not independent of each other and can assist each other in an iterative manner. For value lookup, we combine information from existing tables and a knowledge base. Using two sets of entity-oriented queries, we evaluate our approach both on the component level and on the end-to-end table generation task.
CCS CONCEPTS
· Information systems  Environment-speci c retrieval; Users and interactive retrieval; Retrieval models and ranking; Search in structured data;
KEYWORDS
Table generation; structured data search; entity-oriented search
ACM Reference Format: Shuo Zhang and Krisztian Balog. 2018. On-the- y Table Generation. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3209988
1 INTRODUCTION
Tables are popular on the Web because of their convenience for organizing and managing data. Tables can also be useful for presenting search results [20, 31]. Users often search for a set of things, like music albums by a singer, lms by an actor, restaurants nearby, etc. In a typical information retrieval system, the matched entities are presented as a list. Search, however, is often part of a larger work task, where the user might be interested in speci c attributes
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3209988

Krisztian Balog
University of Stavanger krisztian.balog@uis.no

E Video albums of Taylor Swift

Search

Title

Released data

Label

Formats

S

CMT Crossroads: Taylor Swift and ... Jun 16, 2009 Big Machine

DVD

Journey to Fearless Speak Now World Tour-Live

Oct 11, 2011 Shout! Factory Blu-ray, DVD V
Nov 21, 2011 Big Machine CD/Blu-ray, ...

The 1989 World Tour Live

Dec 20, 2015 Big Machine Streaming

Figure 1: Answering a search query with an on-the- y generated table, consisting of core column entities E, table schema S, and data cells V .
of these entities. Organizing results, that is, entities and their attributes, in a tabular format facilitates a better overview. E.g., for the query "video albums of Taylor Swift," we can list the albums in a table, as shown in Fig. 1.
There exist two main families of methods that can return a table as answer to a keyword query by: (i) performing table search to nd existing tables on the Web [4, 5, 19, 20, 25, 36], or (ii) assembling a table in a row-by-row fashion [31] or by joining columns from multiple tables [20]. However, these methods are limited to returning tables that already exist in their entirety or at least partially (as complete rows/columns). Another line of work aims to translate a keyword or natural language query to a structured query language (e.g., SPARQL), which can be executed over a knowledge base [29]. While in principle these techniques could return a list of tuples as the answer, in practice, they are targeted for factoid questions or at most a single attribute per answer entity. More importantly, they require data to be available in a clean, structured form in a consolidated knowledge base. Instead, we propose to generate tables on the y in a cell-by-cell basis, by combining information from existing tables as well as from a knowledge base, such that each cell's value can originate from a di erent source.
In this study, we focus on relational tables (also referred to as genuine tables [27, 28]), which describe a set of entities along with their attributes [15]. A relational table consists of three main elements: (i) the core column entities E, (ii) the table schema S, which consists of the table's heading column labels, corresponding to entity attributes, and (iii) data cells, V , containing attribute values for each entity. The task of on-the- y table generation is de ned as follows: answering a free text query with an output table, where the core column lists all relevant entities and columns correspond the attributes of those entities. This task can naturally be decomposed into three main components:
(1) Core column entity ranking, which is about identifying the entities to be included in the core column of the table.
(2) Schema determination, which is concerned with nding out what should be the column headings of the table, such that these attributes can e ectively summarize answer entities.
(3) Value lookup, which is to nd the values of corresponding attributes in existing tables or in a knowledge base.

595

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Core column entity ranking
(Section 3)
E

Query (q)
E
S

Schema determination
(Section 4)
S

Value lookup (Section 5)

V Figure 2: Overview of our table generation approach.

The rst subtask is strongly related to the problem of entity retrieval [12], while the second subtask is related to the problem of attribute retrieval [14]. These two subtasks, however, are not independent of each other. We postulate that core column entity ranking can be improved by knowing the schema of the table, and vice versa, having knowledge of the core column entities can be leveraged in schema determination. Therefore, we develop a framework in which these two subtasks can be performed iteratively and can reinforce each other. As for the third subtask, value lookup, the challenge there is to nd a distinct value for an entity-attribute pair, with a traceable source, from multiple sources.
In summary, the main contributions of this work are as follows:
· We introduce the task of on-the- y table generation and propose an iterative table generation algorithm (Sect. 2).
· We develop feature-based approaches for core column entity ranking (Sect. 3) and schema determination (Sect. 4), and design an entity-oriented fact catalog for fast and e ective value lookup (Sect. 5).
· We perform extensive evaluation on the component level (Sect. 7) and provide further insights and analysis (Sect. 8).
The resources developed within this study are made publicly available at https://github.com/iai-group/sigir2018-table.
2 OVERVIEW
The objective of on-the- y table generation is to assemble and return a relational table as the answer in response to a free text query. Formally, given a keyword query q, the task is to return a table T = (E, S, V ), where E = e1, . . . en is a ranked list of core column entities, S = s1, . . . sm is a ranked list of heading column labels, and V is an n-by-m matrix, such that ij refers to the value in row i and column j of the matrix (i  [1..n], j  [1..m]). According to the needed table elements, the task boils down to (i) searching core column entities, (ii) determining the table schema, and (iii) looking up values for the data cells. Figure 2 shows how these three components are connected to each other in our proposed approach.
2.1 Iterative Table Generation Algorithm
There are some clear sequential dependencies between the three main components: core column entity ranking and schema determination need to be performed before value lookup. Other than that, the former two may be conducted independently of and parallel to each other. However, we postulate that better overall performance

Algorithm 1: Iterative Table Generation

Data: q, a keyword query Result: T = (E, S, V ), a result table

1 begin 2 E0  rankEntites(q, {});
3 S0  rankLabels(q, {});

4 t 0;

5 while ¬terminate do

6

t t+1;

7

Et  rankEntites(q, St -1);

8

St  rankLabels(q, Et -1);

9 end 10 V  lookupValues(Et , St ); 11 return (Et , St , V )

12 end

may be achieved if core column entity ranking and schema determination would supplement each other. That is, each would make use of not only the input query, but the other's output as well. To this end, we propose an iterative algorithm that gradually updates core column entity ranking and schema determination results.
The pseudocode of our approach is provided in Algorithm 1, where rankEntites(), rankLabels(), and lookupValues() refer to the subtasks of core column entity ranking, schema determination, and value lookup, respectively. Initially, we issue the query q to search entities and schema labels, by rankEntites(q, {}) and rankLabels(q, {}). Then, in a series of successive iterations, indexed by t, core column entity ranking will consider the top-k ranked schema labels from iteration t - 1 (rankEntites(q, St-1)). Analogously, schema determination will take the top-k ranked core column entities from the previous iteration (rankLabels(q, Et-1)). These steps are repeated until some termination condition is met, e.g., the rankings do not change beyond a certain extent anymore. We leave the determination of a suitable termination condition to future work and will use a xed number of iterations in our experiments. In the nal step of our algorithm, we look up values V using the core column entities and schema (lookupValues(Et , St )). Then, the resulting table (Et , St , V ) is returned as output.
2.2 Data Sources
Another innovative element of our approach is that we do not rely on a single data source. We combine information both from a collection of existing tables, referred to as the table corpus, and from a knowledge base. We shall assume that there is some process in place that can identify relational tables in the table corpus, based on the presence of a core column. We further assume that entities in the core column are linked to the corresponding entries in the knowledge base. The technical details are described in Sect. 6. Based on the information stored about each entity in the knowledge base, we consider multiple entity representations: (i) all refers to the concatenation of all textual material that is available about the entity (referred to as "catchall" in [12]), (ii) description is based on the entity's short textual description (i.e., abstract or summary), and (iii) properties consists of a restricted set of facts (property-value pairs) about the entity. We will use DBpedia in our experiments,

596

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Query

Entity

Dense layer
Hidden layers

Matching matrix (n  m)
... Top-k entries

Output layer
Matching Degree
Figure 3: Architecture of the DRRM_TKS deep semantic matching method.

but it can be assumed, without loss of generality, that the above information is available in any general-purpose knowledge base.

3 CORE COLUMN ENTITY RANKING
In this section, we address the subtask of core column entity ranking: given a query, identify entities that should be placed in the core column of the generated output table. This task is closely related to the problem of ad hoc entity retrieval. Indeed, our initial scoring function is based on existing entity retrieval approaches. However, this scoring can be iteratively improved by leveraging the identi ed table schema. Our iterative scoring function combines multiple features as ranking signals in a linear fashion:

scoret (e, q) = wi i (e, q, St -1) ,

(1)

i

where i is a ranking feature and wi is the corresponding weight. In the rst round of the iteration (t = 0), the table schema is not yet available, thus St-1 by de nition is an empty list. For later iterations (t > 0), St-1 is computed using the methods described in Sect. 4.
For notational convenience, we shall write S to denote the set of top-k schema labels from St-1. In the remainder of this section, we
present the features we developed for core column entity ranking;
see Table 1 for a summary.

3.1 Query-based Entity Ranking
Initially, we only have the query q as input. We consider term-based and semantic matching as features.
3.1.1 Term-based matching. There is a wide variety of retrieval models for term-based entity ranking [12]. We rank documentbased entity representations using Language Modeling techniques. Despite its simplicity, this model has shown to deliver competitive performance [12]. Speci cally, following [12], we use the all entity representation, concatenating all textual material available about a given entity.

Table 1: Features used for core column entity retrieval.

Feature

Iter. (t)

Term-based matching

1: LM (q, ea )

0

Deep semantic matching

2: DRRM_T KS (q, ed )

0

3: DRRM_T KS (q, ep )

0

4: DRRM_T KS (s, ed )

1

5: DRRM_T KS (s, ep )

1

6: DRRM_T KS (q  s, ed  ep )  1

Entity-schema compatibility

7: ESC (S, e)

1

3.1.2 Deep semantic matching. We employ a deep semantic

matching method, referred to as DRRM_TKS [9]. It is an enhance-

ment of DRRM [11] for short text, where the matching histograms

are replaced with the top-k strongest signals. Speci cally, the entity

and the query are represented as sequences of embedding vectors,

denoted

as e

=

[w

e 1

,

w2e

,

...,

wne

]

and q

=

[w1q , w2q , ..., wmq ].

An n ×m

matching matrix M is computed for their joint representation, by

setting Mij

=

wie

·

(w

q j

)

. The values of this matrix are used as

input to the dense layer of the network. Then, the top-k strongest

signals, based on a softmax operation, are selected and fed into the

hidden layers. The output layer computes the nal matching score

between the query and entity. The architecture of DRRM_TKS is

shown in Fig. 3.

We instantiate this neural network with two di erent entity rep-

resentations: (i) using the entity's textual description, ed , and (ii) using the properties of the entity in the knowledge base, ep . The

matching degree scores computed using these two representations,

DRRM_T KS (q, ed ) and DRRM_T KS (q, ep ), are used as ranking features 2 and 3, respectively.

3.2 Schema-assisted Entity Ranking

After the rst iteration, core column entity ranking can be assisted by utilizing the determined table schema from the previous iteration. We present a number of additional features that incorporate schema information.
3.2.1 Deep semantic matching. We employ the same neural network as before, in Sect. 3.1.2, to compute semantic similarity by considering the table schema. Speci cally, all schema labels in S are concatenated into a single string s. For the candidate entities, we keep the same representations as in Sect. 3.1.2. By comparing all schema labels s against the entity, we obtain the schema-assisted deep features DRRM_T KS (s, ed ) and DRRM_T KS (s, ep ). Additionally, we combine the input query with the schema labels, q  s, and match it against a combined representation of the entity, ed  ep , where  refers to the string concatenation operation. The resulting matching score is denoted as DRRM_T KS (q  s, ed  ep ).
3.2.2 Entity-schema compatibility. Intuitively, core column entities in a given table are from the same semantic class, for example, athletes, digital products, lms, etc. We aim to capture their semantic compatibility with the table schema, by introducing a measure called entity-schema compatibility.
We compare the property labels of core column entities E against schema S to build the compatibility matrix C. Element Cij of the

597

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Features used for schema determination.

Feature

Iter. (t)

Column population

1: P (s |q)

0

2: P (s |q, E)

1

Deep semantic matching

3: DRRM_T KS (s, q)  0

Attribute retrieval

4: AR(s, E)

1

Entity-schema compatibility 5: ESC (s, E)

1

matrix is a binary indicator between the jth schema label and the ith entity, which equals to 1 if entity ei has property sj . To check if an entity has a given property, we look for evidence both in the
knowledge base and in the table corpus. Formally:

Ci j

=

1,
 0,

if matchK B (ei , sj )  matchT C (ei , sj ) otherwise .



where matchK B (ei , sj ) and matchT C (ei , sj ) are binary indicator

functions. The former is true if entity ei has property sj in the

knowledge base, the latter is true if there exists a table in the ta-

ble corpus where ei is a core column entity and sj is a schema

label. Then, the entity-schema compatibility score, which is used

as ranking feature 7, is computed as follows:

ESC (S, ei )

=

1 |S |

Cij .
j

For example, for query "Apollo astronauts walked on the Moon" and schema {country, date of birth, time in space, age at rst step, ...}, the ESC scores of entities Alan Shepard, Charles Duke, and Bill Kaysing are 1, 0.85, and 0.4, respectively. The former two are Apollo astronauts who walked on the Moon, while the latter is a writer claiming that the six Apollo Moon landings were a hoax.

4 SCHEMA DETERMINATION

In this section, we address the subtask of schema determination, which is to return a ranked list of labels to be used as heading column labels (labels, for short) of the generated output table. The initial ranking is based on the input query only. Then, this ranking is iteratively improved by also considering the core column entities. Our scoring function is de ned as follows:

scoret (s, q) = wi i (s, q, Et -1),

(2)

i

where i is a ranking feature with a corresponding weight wi . For the initial ranking (t = 0), core column entities are not yet available, thus Et-1 is an empty list. For successive iterations (t > 0), Et-1
is computed using the methods described in Sect. 3. Since we are
only interested in the top-k entities, and not their actual retrieval scores, we shall write E to denote the set of top-k entities in Et-1.
Below, we discuss various feature functions i for this task, which
are also summarized in Table 2.

4.1 Query-based Schema Determination
At the start, only the input query q is available for ranking labels. To collect candidate labels, we rst search for tables in our table corpus that are relevant to the query. We let T denote the set of top-k ranked tables. Following [35], we use BM25 to rank tables

based on their textual content. Then, the column heading labels are extracted from these tables as candidates: S = {s |s  TS ,T  T }.
4.1.1 Column population. Zhang and Balog [35] introduce the task of column population: generating a ranked list of column labels to be added to the column headings of a given seed table. We can adapt their method by treating the query as if it was the caption of the seed table. Then, the scoring of schema labels is performed according to the following probabilistic formula:

P (s |q) = P (s |T )P (T |q) ,
T T
where related tables serve as a bridge to connect the query q and label s. Speci cally, P (s |T ) is the likelihood of the schema label given table T and is calculated based on the maximum edit distance [16], dist,1 between the s and the schema labels of T:

P

(s

|T

)

=

1, 

maxs TS dist (s, s )  

(3)

0, otherwise .



The probability P (T |q) expresses the relevance of T given the query,

and is set proportional to the table's retrieval score (here: BM25).

4.1.2 Deep semantic matching. We employ the same neural net-

work architecture as in Sect. 3.1.2 for comparing labels against the

query. For training the network, we use our table corpus and treat

table captions as queries. All caption-label pairs that co-occur in an

existing table are treated as positive training instances. Negative

training instances are sampled from the table corpus by select-

ing candidate labels that do not co-occur with that caption. The

resulting matching score, DRRM_T KS (s, q), is used as feature 3.

4.2 Entity-assisted Schema Determination

After the initial round, schema determination can be assisted by considering the set of top-k core column entities, E. The set of candidate labels, from before, is expanded with (i) schema labels from tables that contain any of the entities in E in their core column and (ii) the properties of E in the knowledge base.
4.2.1 Entity enhanced column population. We employ a variant of the column population method from [35] that makes use of core column entities:

P (s |q, E) = P (s |T )P (T |q, E) .
T

The schema label likelihood P (s |T ) is computed the same as before, cf. Eq. (3). The main di erence is in the table relevance estimation component, which now also considers the core column entities:

P (T |E)P (T |q) P (T |q, E) = P (T )2 .

Here, P (T |E) is the fraction of the core column entities covered by

a related table, i.e., |TE  E|/|E|, and P (T |q) is the same as in §4.1.1.

4.2.2 A ribute retrieval. Attribute retrieval refers to the task

of returning a ranked list of attributes that are relevant given a

set of entities [14]. Using the core column entities as input, we

employ the method proposed by Kopliku et al. [14], which is a

linear combination of several features:

AR(s, E)

=

1 |E|

e E

match(s, e,T ) +drel (d, e) +sh(s, e) +kb (s, e)

.

1Note that despite the name used in [16], it is in fact a similarity measure.

598

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

The components of this formula are as follows:
· match(s, e,T ) compares the similarity between an entity and a schema label with respect to a given table T . We take T to be the table that is the most relevant to the query (arg maxT T P (T |q)). This matching score is the di erence between the table match score and shadow match score:

match(s, e,T ) = match(e,T ) - match(e, shadow (a)) .

The table match score is computed by representing both the entity and table cells Tx as term vectors, then taking the maximum cosine distance between the two:

match(e,T ) = maxTx T cos (e,Tx ) .
For latter component, the notion of a shadow area is introduced: shadow (a) is set of cells in the table that are in the same row with e or are in the same column with the s. Then, the shadow match score is estimated as:

match(e, shadow (a)) = max cos(e,Tx ) .
Tx  shadow(a)

· drel (d, e) denotes the relevance of the document d that con-

tains T :

drel (e)

=

#results - rank (d ) #r esul t s

,

where #results is the number of retrieved results for entity

e and rank (d ) is the rank of document d within this list.

· sh(s, e) corresponds to the number of search results returned

by a Web search engine to a query " s of e ," where s and e

are substituted with the label and entity, respectively. If the

base-10 logarithm of the number of hits exceeds a certain threshold (106 in [14]) then the feature takes a value of 1,

otherwise it is 0.

· kb (s, e) is a binary score indicating whether label s is a prop-

erty of entity e in the knowledge base (i.e., s  ep ).

4.2.3 Entity-schema compatibility. Similar to Sect. 3.2.2, we em-

ploy the entity-schema compatibility feature for schema determination as well. As before, C is a compatibility matrix, where Cij denotes whether entity ei has property sj . The ESC score is then computed as follows:

ESC (sj , E)

=

1 |E|

i

Cij .

5 VALUE LOOKUP
Having the core column entities and the schema determined, the last component in our table generation approach is concerned with the retrieval of the data cells' values. Formally, for each row (entity) i  [1..n] and column (schema label) j  [1..m], our task is to nd the value Vij . This value may originate from an existing table in our table corpus or from the knowledge base. The challenges here are twofold: (i) how to match the schema label sj against the labels of existing tables and knowledge base predicates, and (ii) how to deal with the case when multiple, possibly con icting values may be found for a given cell.
We go about this task by rst creating a catalogue V of all possible cell values. Each possible cell value is represented as a quadruple e, s, , p , where e is an entity, s is a schema label, is a value, and p is provenance, indicating the source of the information.

The source may be a knowledge base fact or a particular table in the table corpus. An entity-oriented view of this catalog is a ltered set of triples where the given entity stands as the rst component of the quadruple: eV = { s, , p | e, s, , p  V }. We select a single value for a given entity e and schema label s according to:
score( , e, s, q) = max conf (p, q) ,
s , ,p eV match(s,s )
where match(s, s ) is a soft string matching function (detailed in Sect. 6.3) and conf (p, q) is the con dence associated with provenance p. Motivated by the fact that the knowledge base is expected to contain high-quality manually curated data, we set the con dence score such that the knowledge base is always given priority over the table corpus. If the schema label does not match any predicate from the knowledge base, then we chose the value from the table that is the most relevant to the query. That is, conf (p, q) is based on the corresponding table's relevance score; see Sect. 7.3 for the details. Notice that we pick a single source for each value rather than aggregating evidence from multiple sources. The reason for that is that on the user interface, we would like to display a single traceable source where the given value originates from.
6 EXPERIMENTAL SETUP
Queries, dataset, data preprocessing methods and relevance assessments are introduced in this section.
6.1 Test Queries
We use two sets of queries in our experiments:
QS-1 We consider list type queries from the DBpedia-Entity v2 test collection [12], that is, queries from SemSearch LS, TREC Entity, and QALD2. Out of these, we use the queries that have at least three highly relevant entities in the ground truth. This set contains 119 queries in total.
QS-2 The RELink Query Collection [21] consists of 600 complex entity-relationship queries that are answered by entity tuples. That is, the answer table has two or three columns (including the core entity column) and all cell values are entities. The queries and corresponding relevance judgments in this collection are obtained from Wikipedia lists that contain relational tables. For each answer table, human annotators were asked to formulate the corresponding information need as a natural language query, e.g., " nd peaks above 6000m in the mountains of Peru."
For both sets, we remove stop words and perform spell correction.
6.2 Data Sources
We rely on two main data sources simultaneously: a knowledge base and a table corpus.
6.2.1 Knowledge base. The knowledge base we use is DBpedia (version 2015-10). We consider entities for which a short textual description is given in the dbo:abstract property (4.6M in total). We limit ourselves to properties that are extracted from Wikipedia infoboxes.
6.2.2 Table corpus. We use the WikiTables corpus [3], which contains 1.65M tables extracted from Wikipedia. The mean number

599

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

of rows is 11 and the median is 5. For columns, the mean is 5 and the median is 4. We preprocess tables as follows. For each cell that contains a hyperlink we check if it points to an entity that is present in DBpedia. If yes, we use the DBpedia identi er of the linked entity as the cell's content (with redirects resolved); otherwise, we replace the link with the anchor text (i.e., treat it as a string).
Further, each table is classi ed as relational or non-relational according to the existence of a core entity column and the size of the table. We set the following conditions for detecting the core column of a table: (i) the core column should contain the most entities compared to other columns; (ii) if there are more than one columns that have the highest number of entities, then the one with lowest index, i.e., the leftmost one, is regarded as the core column; (iii) the core column must contain at least two entities. Tables without a core column or having less than two rows or columns are regarded as non-relational. In the end, we classify the WikiTables corpus into 973,840 relational and 678,931 non-relational tables. Based on a random sample of 100 tables from each category, we nd that all the sampled tables are correctly classi ed.
6.3 Schema Normalization
Di erent schema labels may be used for expressing the same meaning, e.g., "birthday" vs. "day of birth" or "nation" vs. "country." For the former case, where similar terms are used, we employ a FastJoin match [26] to normalize the strings (with stopwords removed). Speci cally, we take the maximum edit distance as in [16] to measure string similarity. When it exceeds a threshold of  , we regard them as the same label. We set  as 0.8 which is consistent with [16], where headings are matched for table column join. For the latter case, where di erent terms are used, we consider predicates connecting the same subject and object as synonyms. These pairs are then checked and erroneous ones are eliminated manually. Whenever schema labels are compared in the paper, we use their normalized versions.
6.4 Relevance Assessments
For QS-1, we consider the highly relevant entities as the ground truth for the core column entity ranking task. For the task of schema determination, we annotated all candidate labels using crowdsourcing. Speci cally, we used the CrowdFlower platform and presented annotators with the query, three example core column entities, and a label, and asked them to judge the relevance of that label on a three point scale: highly relevant, relevant, or non-relevant. Each query-entity-label triple was annotated by at least three and at most
ve annotators. The labelling instructions were as follows: a label is highly relevant if it corresponds to an essential table column for the given query and core column entities; a label is relevant when it corresponds to a property shared by most core column entities and provides useful information, but it is not essential for the given query; a label is non-relevant otherwise (e.g., hard to understand, not informative, not relevant, etc.). We take the majority vote to decide the relevance of a label. Statistically, we have 7000 triples annotated, and on average, there are 4.2 highly relevant labels, 1.9 relevant labels, and 49.4 non-relevant labels for each query. The Fleiss' Kappa test statistics for inter-annotator agreement is 0.61, which is considered as substantial agreement [10]. For the value lookup task, we sampled 25 queries and fetched values from the

table corpus and the knowledge base. We again set up a crowdsourcing experiment on CrowdFlower for annotation. Given a query, an entity, a schema label, a value, and a source (Wikipedia or DBpedia page), three to ve annotators were asked to validate if the value can be found and whether it is correct, according to the provided source. Overall, 14,219 table cell values were validated. The total expense of the crowdsourcing experiments was $560.
QS-2: Since for this query set we are given the ground truth in a tabular format, based on existing Wikipedia tables, we do not need to perform additional manual annotation. The main entities are taken as the ground truth for the core column entity ranking task, heading labels are taken as the ground truth for the schema determination task, and the table cells (for a sample of 25 queries) are taken as the ground truth for the value lookup task.
6.5 Evaluation Measures
We evaluate core column entity ranking and schema determination in terms of Normalized Discounted Cumulative Gain (NDCG) at cut-o points 5 and 10. The value lookup task is measured by Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). To test signi cance, we use a two-tailed paired t-test and write / to denote signi cance at the 0.05 and 0.005 levels, respectively.
7 EXPERIMENTAL EVALUATION
We evaluate the three main components of our approach, core column entity ranking, schema determination, and value lookup, and assess the e ectiveness of our iterative table generation algorithm.
7.1 Core Column Entity Ranking
We discuss core column entity ranking results in two parts: (i) using only the query as input and (ii) leveraging the table schema as well.
7.1.1 ery-based Entity Ranking. The results are reported in top block of Table 3. The following methods are compared:
LM For term-based matching we use Language Modeling with Dirichlet smoothing, with the smoothing parameter set to 2000, following [12]. This method is also used for obtaining the candidate set (top 100 entities per query) that are reranked by the methods below.
DRRM_TKS We train the deep matching model using 5-fold cross-validation. We use a four-layer architecture, with 50 nodes in the input layer, two hidden layers in the feed forward matching networks, and one output layer. The optimizer is ADAM [13], with hinge loss as the loss function. We set the learning rate to 0.0001 and we report the results after 50 iterations.2 We employ two instantiations of this network, using entity descriptions (ed ) and entity properties (ep ) as input.
Combined We combine the previous three methods, with equal weights, using a linear combination (cf. Eq. 1). Later, in our analysis in Sect. 8.2, we will also experiment with learning the weights for the combination.
On the rst query set, QS-1, LM performs best of the single rankers. Combining it with deep features results in 16% and 9% relative improvement for NDCG@5 and NDCG@10, respectively. On QS-2,
2We also experimented with C-DSSM and DSSM. However, their overall performance was much lower than that of DRRM_TKS for this task.

600

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Core column entity ranking results. The top block of the table uses only the keyword query as input. The bottom block of the table uses the table schema; Round #1­#3 rely on automatically determined schema, while the Oracle method uses the ground truth schema. Statistical significance for query-based entity ranking is compared against LM, for schema-assisted entity ranking is compared against the Combined method.

Method

QS-1

QS-2

NDCG@5 NDCG@10 NDCG@5 NDCG@10

Query-based Entity Ranking (Round #0)

LM DRRM_TKS (ed ) DRRM_TKS (ep )
Combined

0.2419 0.2015
0.1780 0.2821

0.2591 0.2028 0.1808 0.2834

0.0708 0.0823
0.0501 0.0540 0.1089 0.1083 0.0852 0.0920

Schema-assisted Entity Ranking

Round #1 Round #2 Round #3 Oracle

0.3012 0.2892 0.3369 0.3221 0.3445 0.3250 0.3518 0.3355

0.1232 0.1201 0.1307 0.1264 0.1345 0.1270 0.1587 0.1555

a slightly di erent picture emerges. The best individual ranker is DRRM_TKS using entity properties. Nevertheless, the Combined method still improves signi cantly over the LM baseline.
7.1.2 Schema-assisted Entity Ranking. Next, we also consider the table schema for core column entity ranking. The results are presented in the bottom block of Table 3. Note that on top of to the three features we have used before, we have four additional features (cf. Table 1). As before, we use uniform weight for all features. We report results for three additional iterations, Rounds #1­#3, where the schema is taken from the previous iteration of the schema determination component. Further, we report on an Oracle method, which uses the ground truth schema. In all cases, we take the top 10 schema labels (k = 10); we analyze the e ect of using di erent k values in Sect. 8.1. These methods are to be compared against the Combined method, which corresponds to Round #0. We nd that our iterative algorithm is able to gradually improve results, in each iteration, for both of the query sets and evaluation metrics; with the exception of QS-1 in Round #1, all improvements are highly signi cant. Notice that the relative improvement made between Round #0 and Round #3 is substantial: 22% and 86% in terms of NDCG@5 for QS-1 and QS-2, respectively.

7.2 Schema Determination
Schema determination results are presented in two parts: (i) using only the query as input and (ii) also leveraging core column entities.
7.2.1 ery-based Schema Determination. In the top block of Table 4 we compare the following three methods:
CP We employ the column population method from [35] to determine the top 100 labels for each query. Following [16], the  parameter for the edit distance threshold is set to 0.8. This method is also used for obtaining the candidate label set (top 100 per query) that is re-ranked by the methods below.

Table 4: Schema determination results. The top block of the table uses only the keyword query as input. The bottom block of the table uses the core column entities as well; Round #1­#3 rely on automatic entity ranking, while the Oracle method uses the ground truth entities. Statistical significance for query-based schema determination is compared against CP, for entity-assisted entity ranking is compared against the Combined method.

Method

QS-1
NDCG@5 NDCG@10

QS-2
NDCG@5 NDCG@10

Query-based Entity Ranking (Round #0)

CP

0.0561

DRRM_TKS 0.0380 Combined 0.0786

0.0675
0.0427 0.0878

0.1770
0.0920 0.2310

0.2092
0.1415 0.2695

Entity-assisted Schema Determination

Round #1 Round #2 Round #3 Oracle

0.1676 0.1775 0.1910 0.2002

0.1869 0.2046 0.2136 0.2434

0.3342 0.3614 0.3683 0.4239

0.3845 0.4143 0.4350 0.4825

DRRM_TKS We use the same neural network architecture as for core column entity ranking. For training the network, we make use of Wikipedia tables. If an entity and a schema label co-occur in an existing Wikipedia table, then we consider it as a positive pair. Negative training instances are generated by sampling, for each entity, a set of schema labels that do not co-occur with that entity in any existing table. In the end, we generate a total of 10.7M training examples, split evenly between the positive and negative classes.
Combined We combine the above two methods in a linear fashion, with equal weights (cf. Eq. 2). Later, in our analysis in Sect. 8.2, we will also experiment with learning the weights for the combination.
We nd that the CP performs better than DRRM_TKS, especially on the QS-2 query set. The Combined method substantially and signi cantly outperforms both of them, with a relative improvement of 40% and 30% over CP in terms of NDCG@5 on QS-1 and QS-2, respectively.
7.2.2 Entity-assisted Schema Determination. Next, we incorporate three additional features that make use of core column entities (cf. Table 2), using uniform feature weights. For the attribute retrieval feature (§4.2.2), we rely on the Google Custom Search API to get search hits and use the same parameter setting (feature weights) as in [14]. For all features, we use the top 10 ranked entities (and analyze di erent k values later, in Sect. 8.1).
The results are shown in the bottom block of Table 4. Already Round #1 shows a signi cant jump in performance compared to the Combined method (corresponding to Round #0). Subsequent iterations results in further improvements, reaching a relative improvement of 243% and 159% for Round #3 in terms of NDCG@5 for QS-1 and QS-2, respectively. Judging from the performance of the Oracle method, there is further potential for improvement, especially for QS-2.

601

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 5: Value lookup results.

QS-1 Source MAP MRR

KB

0.7759 0.7990

TC

0.1614 0.1746

KB+TC 0.9270 0.9427

QS-2 MAP MRR
0.0745 0.0745 0.9564 0.9564 0.9564 0.9564

7.3 Value Lookup
For value lookup evaluation we take the core column entities and schema labels from the ground truth. This is to ensure that this component is evaluated on its own merit, without being negatively in uenced by errors that incur earlier in the processing pipeline. In our evaluation, we ignore cells that have empty values according to the ground truth (approximately 12% of the cells have empty values in the Wikitables corpus). The overall evaluation results are reported in Table 5. We rely on two sources for value lookup, the knowledge base (KB) and the table corpus (TC). Overall, we reach excellent performance on both query sets. On QS-1, the knowledge base is the primary source, but the table corpus also contributes new values. On QS-2, since all values originate from existing Wikipedia tables, using the knowledge base does not bring additional bene ts. This, however, is the peculiarity of that particular dataset. Also, according to the ground truth there is a single correct value for each cell, hence the MAP and MRR scores are the same for QS-2.
8 ANALYSIS
In this section, we conduct further analysis to provide insights on our iterative algorithm and on feature importance.
8.1 Iterative Algorithm
We start our discussion with Fig. 5, which displays the overall e ectiveness of our iterative algorithm on both tasks. Indeed, as it is clearly shown by these plots, our algorithm performs well. The improvements are the most pronounced when going from Round #0 to Round #1. Performance continues to rise with later iterations, but, as it can be expected, the level of improvement decreases over time. The rightmost bars in the plots correspond to the Oracle method, which represents the upper limit that could be achieved, given a perfect schema determination method for core column entity ranking and vice versa. We can observe that for core column entity ranking on QS-1 (Fig. 5a), has already reached this upper performance limit at iteration #3. For the other task/query set combinations there remains some performance to be gained. It is left for future work to devise a mechanism for determining the number of iterations needed.
Next, we assess the impact of the number of feedback items leveraged, that is, the value of k when using the top-k schema labels in core column entity ranking and top-k entities in schema determination. Figure 6 shows how performance changes with di erent k values. For brevity, we report only on NDCG@10 and note that a similar trend was observed for NDCG@5. We nd that the di erences between the di erent k values are generally small, with k = 10 being a good overall choice across the board.
To further analyze how individual queries are a ected over iterations, Table 6 reports the number of queries that are helped (),

Table 6: The number queries helped (NDCG@100.05), hurt (NDCG@10-0.05), and unchanged (remaining) for core column entity ranking (CCER) and schema determination (SD).

QS-1
Round #0 vs. #1 Round #0 vs. #2 Round #0 vs. #3
QS-2
Round #0 vs. #1 Round #0 vs. #2 Round #0 vs. #3

CCER -
43 38 38 50 30 39 49 26 44
-
166 82 346 173 74 347 173 72 349

SD -
52 7 60 61 5 53 59 2 58
-
386 56 158 388 86 126 403 103 94

hurt (), and remained unchanged (-). We de ne change as a difference of 0.05 in terms of NDCG@10. We observe that with the exception of schema determination on QS-2, the number of queries hurt always decreases between successive iterations. Further, the number of queries helped always increases from Round #1 to #3.
Lastly, we demonstrate how results change over the course of iterations, we show one speci c example table in Fig. 4 that is generated in response to the query "Towns in the Republic of Ireland in 2006 Census Records."
8.2 Parameter Learning
For simplicity, we have so far used all features with equal weights for core column entity ranking (cf. Eq. 1) and schema determination (cf. Eq. 2). Here, we aim to learn the feature weights from training data. In Tables 7 and 8 we report results with weights learned using
ve-fold cross-validation. These results are to be compared against the uniform weight settings in Tables 3 and 4, respectively. We notice that on QS-1, most evaluation scores are lower with learned weights than with uniform weights, for both core column entity ranking and schema determination. This is due to the fact that queries in this set are very heterogeneous [12], which makes it di cult to learn weights that perform well across the whole set. On QS-2, according to expectations, learning the weights can yield

Figure 4: Generated table in response to the query "Towns in the Republic of Ireland in 2006 Census Records." Relevant entities and schema labels are boldfaced.

602

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) CCER QS-1

(b) CCER QS-2

(c) SD QS-1

(d) SD QS-2

Figure 5: Performance change across iterations for core column entity ranking (CCER) and schema determination (SD).

(a) CCER QS-1

(b) CCER QS-2

(c) SD QS-1

(d) SD QS-2

Figure 6: Impact of the cuto parameter k for Core Column Entity Ranking (CCER) and Schema Determination (SD).

Table 7: Core column entity retrieval results with parameters learned using ve-fold cross-validation. In parentheses are the relative improvements w.r.t. using uniform weights.

Method

QS-1
NDCG@5 NDCG@10

QS-2
NDCG@5 NDCG@10

Round #0 Round #1 Round #2 Round #3 Oracle

0.2523 (-11%) 0.2782 (-8%) 0.3179 (-6%) 0.3192 (-7%) 0.3017 (-14%)

0.2653 (-6%) 0.1003 (+18%) 0.2772 (-4%) 0.1308 (+6%) 0.3180 (-1%) 0.1367 (+5%) 0.3109 (-4%) 0.1395 (+4%) 0.3042 (-9%) 0.1728 (+9%)

0.1048 (+14%) 0.1252 (+4%) 0.1323 (+5%) 0.1339 (+5%) 0.1630 (+5%)

Table 8: Schema determination results with parameters learned using ve-fold cross-validation. In parentheses are the relative improvements w.r.t. using uniform weights.

Method

QS-1

NDCG@5

NDCG@10

QS-2
NDCG@5 NDCG@10

Round #0 Round #1 Round #2 Round #3 Oracle

0.0928 (+18%) 0.1663 (-1%) 0.1693 (-5%) 0.1713 (-10%) 0.1719 (-14%)

0.1064 (+21%) 0.2326 (+1%) 0.2066 (+11%) 0.3865 (+16%) 0.2212 (+8%) 0.3889 (+8%) 0.2321 (+9%) 0.3915 (+6%) 0.2324 (-5%) 0.4678 (+10%)

0.2710 (+1%) 0.4638 (+12%) 0.4599 (+11%) 0.4620 (+6%) 0.5307 (+10%)

up to 18% and 21% relative improvement for core column entity ranking and schema determination, respectively.
8.3 Feature Importance
To measure the importance of individual features, we use their average learned weights (linear regression coe cients) across all iterations. The ordering of features for core column entity ranking and QS-1 is: 1 (0.566) > 7 (0.305) > 6 (0.244) > 2 (0.198) > 5 (0.127) > 4 (0.09) > 3 (0.0066). For QS-2 it is: 7 (0.298) > 1 (0.148) > 3 (0.108) > 4 (0.085) > 5 (0.029) > 2 (-0.118) > 6 (-0.128). Overall, we nd the term-based matching (Language Modeling) score (1) and our novel entity-schema compatibility score (7) to be the most important features for core column entity ranking. Turning to schema determination, on QS-1 the ordering is: 5 (0.23) > 3 (0.076) > 1 (-0.035) > 2 (-0.072) > 4 (-0.129). For QS-2 it is: 5 (0.27) > 4 (0.181) > 1 (0.113) > 3 (0.018) > 2 (-0.083). Here, entity-schema compatibility (5) is the single most important feature on both query sets.
9 RELATED WORK
Research on web tables has drawn increasing research attention. We focus on three main related areas: table search, table augmentation, and table mining.

Table search refers to the task of returning a ranked list of tables (or tabular data) for a query. Based on the query type, table search can be categorized as keyword-based search [4, 5, 19, 20, 25] or tablebased search [1, 7, 16, 17, 19, 30]. Zhang and Balog [36] propose a set of semantic features and fusion-based similarity measures [34] for table retrieval with respect to a keyword query. Focusing on result diversity, Nguyen et al. [19] design a goodness measure for table search and selection. There are some existing table search engines, e.g., Google Fusion Tables [4]. Table search is often regarded as a fundamental step in other table related tasks. For example, Das Sarma et al. [7] take an input table to search row or column complement tables whose elements can be used for augmenting a table with additional rows or columns.
Table augmentation is about supplementing a table with additional elements, e.g., new columns [2, 4, 7, 16, 30, 33]. Zhang and Balog [35] propose the tasks of row and column population, to augment the core column entity set and column heading labels. They capture relevant data from DBpedia and the WikiTables corpus. Search based on attributes, entities and classes is de ned as relational search, which can be used for table column augmentation. Kopliku et al. [14] propose a framework to extract and rank attributes from web tables. Data completion refers to the problem of lling in empty table cells. Yakout et al. [30] address three core

603

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

tasks: augmentation by attribute name, augmentation by example, and attribute discovery by searching similar tables. Each of these tasks is about extracting table cell data from existing tables. In case that no existing values are captured, Ahmadov et al. [1] introduce a method to extract table values from related tables and/or to predict them using machine learning methods.
Table mining is to explore and utilize the knowledge contained in tables [3, 5, 22, 25, 32]. Munoz et al. [18] recover Wikipedia table semantics and store them as RDF triples. A similar approach is taken in [5] based on tables extracted from a Google crawl. Instead of mining the entire table corpus, a single table stores many facts, which could be answers to questions. Given a query, Sun et al. [24] identify possible entities using an entity linking method and represent them as a two-node graph question chain, where each node is an entity. Table cells of the KB table are decomposed into relational chains, which are also two-node graphs connecting two entities. The task then boils downing to matching question and table cell graphs using a deep matching model. A similar task is addressed by Yin et al. [32] using a full neural network. Information extracted from tables can be used to augment existing knowledge bases [8, 23]. Another line of work concerns table annotation and classi cation. By mining column content, Zwicklbauer et al. [37] propose a method to annotate table headers. Studying a large number of tables in [6], a ne-grained table type taxonomy is provided for classifying web tables.
10 CONCLUSION
We have introduced the task of on-the- y table generation, which aims to answer queries by automatically compiling a relational table in response to a query. This problem is decomposed into three speci c subtasks: (i) core column entity ranking, (ii) schema determination, and (iii) value lookup. We have employed a feature-based approach for core column entity ranking and schema determination, combining deep semantic features with task-speci c signals. We have further shown that these two subtasks are not independent of each other and have developed an iterative algorithm, in which the two reinforce each other. For value lookup, we have entity-oriented fact catalog, which allows for fast and e ective lookup from multiple sources. Using two sets of entity-oriented queries, we have demonstrated the e ectiveness of our method. In future work, we wish to consider more heterogeneous table corpus in addition to Wikipedia tables, i.e., arbitrary tables from the Web.
REFERENCES
[1] Ahmad Ahmadov, Maik Thiele, Julian Eberius, Wolfgang Lehner, and Robert Wrembel. 2015. Towards a Hybrid Imputation Approach Using Web Tables.. In Proc. of BDC '15. 21­30.
[2] Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2013. Methods for Exploring and Mining Tables on Wikipedia. In Proc. of IDEA '13. 18­26.
[3] Chandra Sekhar Bhagavatula, Thanapon Noraset, and Doug Downey. 2015. TabEL: Entity Linking in Web Tables. In Proc. of ISWC 2015. 425­441.
[4] Michael J. Cafarella, Alon Halevy, and Nodira Khoussainova. 2009. Data Integration for the Relational Web. Proc. of VLDB Endow. 2 (2009), 1090­1101.
[5] Michael J. Cafarella, Alon Halevy, Daisy Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: Exploring the Power of Tables on the Web. Proc. of VLDB Endow. 1 (2008), 538­549.
[6] Eric Crestan and Patrick Pantel. 2011. Web-scale Table Census and Classi cation. In Proc. of WSDM '11. 545­554.
[7] Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and Cong Yu. 2012. Finding Related Tables. In Proc. of SIGMOD '12. 817­828.

[8] Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, Wilko Horn, Ni Lao, Kevin Murphy, Thomas Strohmann, Shaohua Sun, and Wei Zhang. 2014. Knowledge Vault: A Web-scale Approach to Probabilistic Knowledge Fusion. In Proc. of KDD '14. 601­610.
[9] Yixing Fan, Liang Pang, JianPeng Hou, Jiafeng Guo, Yanyan Lan, and Xueqi Cheng. 2017. MatchZoo: A Toolkit for Deep Text Matching. arXiv preprint arXiv:1707.07270 (2017).
[10] J.L. Fleiss et al. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin 76 (1971), 378­382.
[11] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Croft. 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In Proc. of CIKM '16. 55­64.
[12] Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. DBpedia-Entity V2: A Test Collection for Entity Search. In Proc. of SIGIR '17. 1265­1268.
[13] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. http://arxiv.org/abs/1412.6980.
[14] Arlind Kopliku, Mohand Boughanem, and Karen Pinel-Sauvagnat. 2011. Towards a Framework for Attribute Retrieval. In Proc. of CIKM '11. 515­524.
[15] Oliver Lehmberg, Dominique Ritze, Robert Meusel, and Christian Bizer. 2016. A Large Public Corpus of Web Tables Containing Time and Context Metadata. In Proc. of WWW '16 Companion. 75­76.
[16] Oliver Lehmberg, Dominique Ritze, Petar Ristoski, Robert Meusel, Heiko Paulheim, and Christian Bizer. 2015. The Mannheim Search Join Engine. Web Semant. 35 (2015), 159­166.
[17] Girija Limaye, Sunita Sarawagi, and Soumen Chakrabarti. 2010. Annotating and Searching Web Tables Using Entities, Types and Relationships. Proc. of VLDB Endow. 3 (2010), 1338­1347.
[18] Emir Munoz, Aidan Hogan, and Alessandra Mileo. 2014. Using Linked Data to Mine RDF from Wikipedia's Tables. In Proc. of WSDM '14. 533­542.
[19] Thanh Tam Nguyen, Quoc Viet Hung Nguyen, Weidlich Matthias, and Aberer Karl. 2015. Result Selection and Summarization for Web Table Search. In ISDE '15. 425­441.
[20] Rakesh Pimplikar and Sunita Sarawagi. 2012. Answering Table Queries on the Web Using Column Keywords. Proc. of VLDB Endow. 5 (2012), 908­919.
[21] Pedro Saleiro, Natasa Milic-Frayling, Eduarda Mendes Rodrigues, and Carlos Soares. 2017. RELink: A Research Framework and Test Collection for EntityRelationship Retrieval. In Proc. of SIGIR '17. 1273­1276.
[22] Sunita Sarawagi and Soumen Chakrabarti. 2014. Open-domain Quantity Queries on Web Tables: Annotation, Response, and Consensus Models. In Proc. of KDD '14. 711­720.
[23] Yoones A. Sekhavat, Francesco Di Paolo, Denilson Barbosa, and Paolo Merialdo. 2014. Knowledge Base Augmentation using Tabular Data. In Proc. of LDOW '14.
[24] Huan Sun, Hao Ma, Xiaodong He, Wen-tau Yih, Yu Su, and Xifeng Yan. 2016. Table Cell Search for Question Answering. In Proc. of WWW '16. 771­782.
[25] Petros Venetis, Alon Halevy, Jayant Madhavan, Marius Paca, Warren Shen, Fei Wu, Gengxin Miao, and Chung Wu. 2011. Recovering Semantics of Tables on the Web. Proc. of VLDB Endow. 4 (2011), 528­538.
[26] Jiannan Wang, Guoliang Li, and Jianhua Feng. 2014. Extending String Similarity Join to Tolerant Fuzzy Token Matching. ACM Trans. Database Syst. 39, 1 (2014), 1­45.
[27] Yalin Wang and Jianying Hu. 2002. Detecting Tables in HTML Documents. In Proc. of DAS '02. 249­260.
[28] Yalin Wang and Jianying Hu. 2002. A Machine Learning Based Approach for Table Detection on the Web. In Proc. of WWW '02. 242­250.
[29] Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural Language Questions for the Web of Data. In Proc. of EMNLP-CoNLL '12. 379­390.
[30] Mohamed Yakout, Kris Ganjam, Kaushik Chakrabarti, and Surajit Chaudhuri. 2012. InfoGather: Entity Augmentation and Attribute Discovery by Holistic Matching with Web Tables. In Proc. of SIGMOD '12. 97­108.
[31] Mohan Yang, Bolin Ding, Surajit Chaudhuri, and Kaushik Chakrabarti. 2014. Finding Patterns in a Knowledge Base Using Keywords to Compose Table Answers. Proc. VLDB Endow. 7, 14 (2014), 1809­1820.
[32] Pengcheng Yin, Zhengdong Lu, Hang Li, and Ben Kao. 2016. Neural Enquirer: Learning to Query Tables in Natural Language. In Proc. of IJCAI '16. 2308­2314.
[33] Shuo Zhang, Vugar Abdulzada, and Krisztian Balog. 2018. SmartTable: A Spreadsheet Program with Intelligent Assistance. In Proc. of SIGIR '18.
[34] Shuo Zhang and Krisztian Balog. 2017. Design Patterns for Fusion-Based Object Retrieval. In Proc. of ECIR '17. 684­690.
[35] Shuo Zhang and Krisztian Balog. 2017. EntiTables: Smart Assistance for EntityFocused Tables. In Proc. of SIGIR '17. 255­264.
[36] Shuo Zhang and Krisztian Balog. 2018. Ad Hoc Table Retrieval using Semantic Similarity. In Proc. of WWW '18. 1553­1562.
[37] Stefan Zwicklbauer, Christoph Einsiedler, Michael Granitzer, and Christin Seifert. 2013. Towards Disambiguating Web Tables. In Proc. of ISWC-PD '13. 205­208.

604

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Dynamic Shard Cutoff Prediction for Selective Search

Hafeezul Rahman Mohammad
Carnegie Mellon University hmohamma@cs.cmu.edu

Keyang Xu
Petuum Inc. xky0714@gmail.com

Jamie Callan
Carnegie Mellon University callan@cs.cmu.edu
ABSTRACT
Selective search architectures use resource selection algorithms such as Rank-S or Taily to rank index shards and determine how many to search for a given query. Most prior research evaluated solutions by their ability to improve efficiency without significantly reducing early-precision metrics such as P@5 and NDCG@10.
This paper recasts selective search as an early stage of a multistage retrieval architecture, which makes recall-oriented metrics more appropriate. A new algorithm is presented that predicts the number of shards that must be searched for a given query in order to meet recall-oriented goals. Decoupling shard ranking from deciding how many shards to search clarifies efficiency vs. effectiveness trade-offs, and enables them to be optimized independently. Experiments on two corpora demonstrate the value of this approach.
ACM Reference Format: Hafeezul Rahman Mohammad, Keyang Xu, Jamie Callan, and J. Shane Culpepper. 2018. Dynamic Shard Cutoff Prediction for Selective Search. In Proceedings of SIGIR '18. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3209978.3210005
1 INTRODUCTION
Selective search is a distributed search architecture that avoids searching the entire corpus for each query. When the index is built, it is divided into small, topically-oriented shards. During retrieval, first shards (or resources) are ranked by their likelihood of returning documents relevant to the query, and then only the most queryrelevant shards are searched. The accuracy and efficiency of the selective search architecture depends upon the number of shards that are searched (the cutoff ). Searching too few shards harms accuracy, while searching too many shards harms efficiency.
While a large body of work now exists around this technology [16, 19­21, 23­25], many interesting problems remain. In this paper we focus on two related issues. First, distributed search is increasingly viewed as an early-stage retrieval process where the task is to efficiently collect as many possibly relevant documents before
The first two authors contributed equally.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210005

J. Shane Culpepper
RMIT University shane.culpepper@rmit.edu.au
applying more expensive learning-to-rank algorithms [5, 28, 37, 44]. As such, optimizing for early precision metrics such as ERR [8] and NDCG@10 [17] during selective search may not be desirable. Second, the ideal number of shards to search can depend heavily on the resource selection algorithm, the desired search type (recall-driven or early-precision-driven), and the specific query.
In spite of the importance of selecting the right number of shards with respect to targeted evaluation, this aspect of selective search has not been studied extensively by prior research. Some approaches treat the cutoff as a parameter to be tuned for a query set; that is, the same value is used for every query [2, 4, 13, 23, 40, 41]. Other approaches treat it as part of the resource selection problem, where the result is a shard ranking and a cutoff. For example, SUSHI [45] selects shards that are expected to have documents in the top-n of the final ranking, Taily [1] sets the cutoff based on an estimate of the minimum number of relevant documents in each shard, and Rank-S [25] sets the cutoff using a rank-based decay function of the shard's relevance score. Query-based cutoffs produced by algorithms such as SUSHI, Taily, and Rank-S are appealing, however there has been little study of the prediction accuracy.
Another limitation in past work is the assumption that singlepass retrieval using BM25 or language models and focusing on early precision is sufficient. In this scenario, only a few shards are required for most queries, and less accurate cutoff predictions tend to not hurt efficiency. However, complex multi-stage ranking pipelines are now common [9, 37]. If selective search is used in a pipeline for early-stage retrieval, recall should be a priority [31], and maximizing for recall often means searching more shards initially.
Query-specific shard cutoff prediction ­ the problem of predicting the number of shards to search for each query ­ depends on the size of the desired result set. Thus, we distinguish between early precision and high recall search requirements. An early precision scenario measures accuracy in the first few ranked documents (e.g., 1 . . . 10), and thus is likely to be more efficient because fewer shards are searched. This is the scenario studied most often in prior work. In contrast, a high recall scenario attempts to find all relevant documents for a query, and can require thousands of documents to be returned. Thus, it is likely to require more shards to be searched. A robust shard cutoff prediction method should be effective, stable, and usable for both early precision and high recall search scenarios.
This paper presents a new, feature-based approach to queryspecific shard cutoff prediction that is easily tuned for early precision or high recall, and can be used in conjunction with any resource selection algorithm. The research and experiments presented in this paper are designed to answer the following five research questions.

85

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

· RQ1: How accurate are existing shard cutoff predictions? · RQ2: How accurate are existing shard rankings? · RQ3: Are ranker-independent cutoff predictions effective? · RQ4: How do the competing goals of precision-oriented and
recall-oriented selective search affect tradeoffs between efficiency and effectiveness? · RQ5: Is it necessary or useful for the shard cutoff prediction algorithm to be trained for a specific resource selection algorithm?
The next section reviews prior research on selective search, resource ranking, and measuring the similarity of search results. Section 3 describes our new approach to predicting shard cutoffs. Section 4 describes our experimental methodology and evaluation. Section 5 reports experimental results. Section 6 concludes.
2 RELATED WORK
Two types of prior research relate to query-specific shard cutoff prediction. Resource selection algorithms rank shards for a query; algorithms often used with selective search also make query-specific decisions about how many shards to search. Rank similarity measures are also related, since the goal of selective search is to produce rankings equivalent to exhaustive search, albeit with less effort.
2.1 Resource Selection for Selective Search
Resource selection (resource ranking) estimates the relevance of index shards for a specific query, and imposes an ordering on shard traversal. There are three general approaches to resource selection for selective search: term-based, sampled-based and feature-based.
Term-based methods use summary term statistics to model each shard. These are used to estimate the shard's relevance to a query [4, 15]. For example, Taily [1] uses the mean and variance of term frequency (TF) within a shard to estimate the number of its documents that would be highly-ranked by an exhaustive search system; shards that are likely to contain more than a specified number number of highly-ranked documents (e.g., v = 50) are selected.
Sample-based methods combine samples of documents from each shard into a common index, known as a centralized sample index (CSI). The query is run against the CSI. Each top-ranked document is treated as a vote for the shard from which it was sampled. Algorithms such as ReDDE [41], SUSHI [45], CRCS [40] and Rank-S [25] differ primarily in how they conduct voting. Markov and Crestani [33] and Sener et al. [39] provide detailed analyses of these algorithms. The ReDDE score for shard R is nR · wR , where nR is the number of documents that R contributed to the top-n of the CSI ranking, and wR is the ratio of shard size to sample size. CRCS considers the rank of the document in the CSI ranking, so that documents at higher ranks contribute more than those at lower ranks. Rank-S uses an exponential function, scoreCSI (q, di ) · B-i , to discount the contribution of the i·th ranked document from the CSI ranking, where B controls the rate of decay. Rank-S selects all shards with scores above a threshold.
Feature-based methods use a variety of features, such as summary statistics, the scores of term-based algorithms, the scores of sample-based algorithms, the query's category, and presence of certain terms, to estimate a shard's relevance to the query. Binary

classification [2], regression models [7], and learning-to-rank [12] have been used to learn the models.
Usually the number of shards to search is either a static parameter [2, 12, 41] or is tightly-integrated with the resource selection algorithm [1, 25]. One exception is ShRkC [22], a feature-based, regression-based shard cutoff predictor. Although independent of any resource selection algorithm, ShRkC is trained using data from a desired resource selection algorithm. As in most work on selective search, it was trained and evaluated for early-precision metrics.

2.2 Rank Similarity

Prior studies have also explored how to measure the similarity

between two ranked result lists. These approaches can be used

to compare search results without explicitly requiring relevance

judgments [3, 14, 18, 26, 34, 42]. More recently, Webber et al. [46] proposed Rank-Biased Overlap (RBO), which calculates the expected

average overlap that the user observes in comparing the two lists.

The key difference between RBO and previous approaches is that

a user bias to higher ranking documents is incorporated, and the lists being compared can be disjoint. This means the metric can

compare incomplete rankings. This idea was generalized by Tan

and Clarke [43] who showed that the idea can be used for any

utility based evaluation metric. The resulting family of metrics,

called Maximized Effectiveness Difference (MED), can be computed

using any gain function, and target specific metrics such as ERR [8],

DCG [17], or RBP [36]. Unlike previous approaches, MED directly

transfers assumptions about user behavior from any chosen ef-

fectiveness measure to maximize a similarity score that serves as

the corresponding rank similarity measure. For example, MEDRBP

maximizes: S(A) -S(B) = (1 -)(

iK=1(ai -bi )i-1 +

 i =K

+1

i

-1

),

where A and B represent two ranking lists, K stands for the maxi-

mum depth for calculation and  is the user persistence for RBP.

In this work, we use MEDRBP in a manner originally described by Clarke et al. [10] to measure stagewise loss between an exhaustive

run and the subset of documents aggregated by selective search, as

this allows us to experiment with larger sets of queries that do not

have relevance judgments associated with them.

3 QUERY-SPECIFIC SHARD CUTOFF PREDICTION
Query-specific shard cutoff prediction can be framed as a machine learning problem: Given a query q and a set of index shards S, train a model that can predict the number of shards K that should be searched to optimize a given metric. This framework requires
defining a set of features that will provide clues about the number of shards to search for query q; obtaining training data; and selecting a machine learning algorithm. The metric to be optimized is assumed to be related to task requirements, for example, NDCG@10 (an early precision scenario) or MAP (a high recall scenario), and thus outside of our control. Among the different requirements, prior research
provides the least guidance about the training data.

3.1 Features
The features used in this research are motivated by two ideas about what affects the number of shards to search for a query. The first idea is based on the premise that difficult queries need to search

86

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Term statistics used to generate query-dependent features. Statistics 2-8 are computed for all 7 similarity measures. Each term has 51 of these features in total.
Term Statistics
1. Number of documents containing the term 2. Maximum similarity score 3. First quartile similarity score 4. Third quartile similarity score 5. Arithmetic mean of similarity scores 6. Harmonic mean of similarity scores 7. Median of similarity scores 8. Variance of similarity scores 9. Geometric mean aggregation of all 49 scores in 2-8
Table 2: The 147 query-dependent features. The numbers in brackets show the number of features for that type. Feature types 2-8 are computed for all 7 similarity measures using term statistics from Table 1. Interquartile range is the difference between the first and third quartile similarity score.
Query Features
1. Query length (1) 2. Arithmetic mean of document frequency (1) 3. Arithmetic mean of Geometric mean aggregation (1) 4. Arithmetic mean of maximum scores (7) 5. Arithmetic mean of median score (7) 6. Arithmetic mean of mean scores (14) 7. Arithmetic mean of score variances (7) 8. Arithmetic mean of score interquartile ranges (7) 9. For each feature in Table 1, the minimum across terms (51) 10. For each feature in Table 1, the maximum across terms (51)
more shards while simpler queries do not. The second idea is that the number of shards being search also depends on the distribution of the similarity scores of a query with the documents across all the shards. Thus, two different types of features were investigated.
Corpus features describe how well query q matches the corpus. We incorporate 147 corpus features into our model which have previously been used for query difficulty prediction and other preretrieval tasks [6, 11, 30, 32]. These features describe query characteristics (e.g. length), and aggregated statistics for each query term (e.g. maximum score, harmonic/arithmetic mean/median score, and geometric mean) from a range of similarity functions (TF·IDF, BM25, query likelihood, term probability, Bose-Einstein, DPH, and DFR). When the index is constructed, features are easily pre-computed for each term. Table 1 outlines the term-specific features used in this work. During retrieval, features for query terms are fetched from a term dictionary and combined to produce simple, query-specific features. Table 2 shows the query-specific features used.
Shard-distribution features characterize the distribution of a shard-specific feature across the set of shards. For each term, three aggregated shard-level statistics (maximum, mean, and variance) are constructed using a range of similarity functions as above (TF·IDF, BM25, query likelihood, term probability, Bose-Einstein, DPH and DFR). For a given query, we find the arithmetic mean of these term-level statistics for each shard and then normalize the

Table 3: The 42 shard-distribution features. The numbers in brackets show the number of features for that type.
Shard Distribution Features
1. Entropy of arithmetic mean of mean scores across shards (7) 2. Entropy of arithmetic mean of maximum scores across shards (7) 3. Entropy of arithmetic mean of score variances across shards (7) 4. KL-Divergence of arithmetic mean of mean scores with a uniform reference distribution (7) 5. KL-Divergence of arithmetic mean of maximum scores with a uniform reference distribution (7) 6. KL-Divergence of arithmetic mean of score variances with a uniform reference distribution (7)
mean scores to form a valid probability distribution across all of the shards. The cross-entropy and KL-Divergence of these distribution scores across the set of shards form our feature set. We use a total of 42 shard distribution features (Table 3). We also investigated shard distribution features based on the maximum, cross-entropy, and KL-Divergence of Taily scores across shards. They performed about the same as the features in Table 3, and combining the two sets provided little gain, thus we omit those results. We surmise that the two sets of feature captured the same information.
These shard-distribution features provide better signals to the learning algorithm when changing the search goal from earlyprecision to recall-oriented. When the distribution is heavily skewed, the majority of the relevant documents will be concentrated into a few shards, and hence fewer shards can be searched; but when it is less skewed or more uniform, relevant documents will be scattered across many shards, and more shards should be searched.
3.2 Training Data
Typically selective search systems are compared to exhaustive search systems that search all shards. The goal of selective search is to deliver results that are at least as accurate, but at a lower (typically much lower) computational cost. In principle, it is possible for selective search to be more accurate than exhaustive search, but in prior research this behavior was observed only occasionally and only at early precision cut-offs (e.g. 1 . . . 5). Outperforming exhaustive search in terms of effectiveness appears to be possible, but remains an elusive goal in practice. In this work, exhaustive search results are treated as "gold standard" results. If we are able to achieve the same effectiveness as exhaustive search while searching only a subset of the collection, efficiency is improved.
Here, a training instance is a tuple ((q, S), K), where (q, S) is a set of features extracted for query q and a set of shards S, and K is the number of shards to search. The value of K is determined by a three-step process. First, an exhaustive search for query q retrieves n documents. Second, a resource-selection algorithm produces a shard ranking for q. Third, the documents returned are analyzed to determine the number of shards K that must be searched to produce results comparable to the reference ­ exhaustive search. A complete discussion of exhaustive search is deferred until Section 4 because it is an experimental detail, and any ideal document ranking over the entire collection can be used in practice. Shard Ranking. Shards can be ranked by resource selection algorithms such as ReDDE, Taily, and Rank-S. However, using a specific

87

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Find_k (rd,e , rs , , Kmax ): # rd,e : A document ranking produced by exhaustive search # rs : A ranking of shards # : Maximum acceptable difference between doc rankings # Kmax : A maximum value for K K =1 while (K  Kmax ) { Use the top K shards in rs to create a document ranking rd,K if (difference (rd,K , rd,e ) < )
break;
Increment K return K

Figure 1: Algorithm to calculate the query-specific shard cutoff K.

resource selection algorithm makes the training data sensitive to

that algorithm, which may have unintended consequences. For

example, different algorithms produce shard rankings of different

lengths. Taily ranks all shards; however, ReDDE, Rank-S, and other

sample-document algorithms rank only the shards that contributed

matching documents to a centralized sample index. The ranking may not contain all of the K shards necessary to produce a docu-

ment ranking comparable to exhaustive search.

We define an algorithm that generates a shard ranking compat-
ible with an exhaustive search document ranking. Given query q and exhaustive search ranking rd,e , the weight of shard s is:

depth

Wq,s =

Is rd,e [i]  pi-1

i =1

where Is indicates whether document rd,e [i] is located in shard s, p is the user persistence from MEDRBP, and depth is the maximum

depth for computing weights. We denote a ranking of shards by

Wq,s ­ a ranking compatible with exhaustive search ­ as rs,e . The value of p and depth depend on the evaluation metric being targeted.

Some queries require many shards to be searched in order to achieve a MEDRBP  . This occurs when index partitioning scat-

ters a topic across many shards rather than concentrating it in a few

shards, or when the query and relevant documents have little or

no overlapping vocabulary. Usually the percentage of such queries

is small; however they can have a disproportionate effect on the learning algorithm because the cutoff labels (K) for these queries

differ so dramatically from the labels for other training data. In
order to minimize the effect of such outliers, we set Kmax = 16 in our recall-driven experiments, and Kmax = 8 in our early-precision
experiments. These parameters were chosen empirically.

Query-Specific Shard Cutoff. A shard cutoff K is calculated using a simple iterative algorithm, shown in Figure 1. Beginning with K = 1, the top K shards are used to produce a document ranking. The similarity of rK , the document ranking produced with K shards, is compared to re , the exhaustive search document ranking produced
by searching all shards. If the two rankings are sufficiently similar,

or if a maximum has been reached, the algorithm stops and reports
K. Otherwise, K is incremented and the next value is tested. As rK is always a subset of re , an efficient implementation creates rK by removing from re documents that are in unselected shards.
The similarity between two search lists, rd,K and rd,e , is measured using MEDRBP. Clarke et al. [10] showed that the effectiveness

loss between multi-stage retrieval results can be accurately measured without explicitly requiring relevance judgments. A small MEDRBP value indicates that rd,K agrees with exhaustive search and large value denotes that they are different. A threshold  is used to find the label K. When the value of MEDRBP is lower than the threshold , it indicates the K shards are sufficient to generate a result comparable to exhaustive search. Previous experiments showed that MEDRBP < 0.2 correlates with no important difference between the two lists [10], so we use a target of  < 0.2. Summary. Training data is produced using only queries, exhaustive search, a shard ranking, a target similarity metric, and a taskspecific similarity threshold that adjusts training data for shallow or deep evaluation metrics. Relevance judgments for the queries are not required, making it easier to produce large, task-specific training data for this problem.
3.3 Learning Algorithms
The distribution of K cutoffs is expected to be skewed since selective search is effective at concentrating most topics in fewer shards than random allocations. However, skewed data can be difficult for some classes of machine learning algorithms, thus we explore the label distribution effects on two classes of regression algorithms.
Random Forest (RF) regression [27] was used to directly predict the value of K. A random forest is a meta regressor that fits multiple regression models on sub-samples of the data and uses averaging to improve the predictive accuracy and control over-fitting.
Quantile Regression (QR) is a modification of random forest regression that estimates the conditional median. This model better handles outliers in heavy-tailed distributions. Since the distribution of K can be heavily skewed for certain target metrics, this method tends to work well empirically. The parameter used to tune quantile regression is  , which controls the relative importance of quantile loss and standard regression loss.
4 EXPERIMENTAL METHODOLOGY
Dataset. Experiments were conducted on two widely used dataset collections: ClueWeb09-B, which contains around 50 million web pages and Gov2 which contains around 25 million documents from the US government web domains. Stopwords were removed using the default Indri stoplist. Stemming was done using the Krovetz stemmer. For ClueWeb09-B, the spam documents were removed during document retrieval using Waterloo spam scores and the score threshold for filtering spam was set to 50. Dataset Partitions. We used the QKLD-QInit partition defined by Dai et al. [12], which divides ClueWeb09-B into 123 shards and the Gov2 dataset into 199 shards of approximately equal size. These partitionings represent the state-of-the-art for these datasets for selective search, and are available on the authors' website1. Training and Testing Queries. For ClueWeb09-B, the 40,000 queries from the 2009 TREC Million Query Track (MQT) were filtered and used for training, and 200 queries from the 2009-2012 TREC Web Track (WT) were used for testing. Note that the 200 WT queries are contained in the original MQT query set, and were removed for the experiments, and queries with no matches in the index were also
1 http://boston.lti.cs.cmu.edu/appendices/SIGIR2017-Zhuyun-Dai/

88

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

removed. In total, 1,140 queries were removed and the final training corpus was a set of 38,600 queries. For Gov2, the 20, 000 queries from the 2007 and 2008 TREC Million Query Track were used for
training, and 150 queries from the 2004 and 2005 TREC Terabyte
track were used for testing. As above, testing queries and queries
with no matches in the index were removed from the training set, leaving a training corpus of 19, 800 queries. Exhaustive Search (Gold Standard) Document Rankings. The full index was searched by the Indri search engine. For ClueWeb09-
B, the unstructured queries were transformed into structured queries
in the Indri query language using two techniques that have been
effective in TREC evaluations: multiple representations, and se-
quential dependency models (SDM) [35]. For example, the 3-term query "ape bat cat" transformation is shown below.
#weight( 1 #combine( ape.title bat.title cat.title ) 2 #combine( ape.inlink bat.inlink cat.inlink ) 3 #weight( 1 #combine( ape.body bat.body cat.body ) 2 #combine( #1( ape.body bat.body ) #1( bat.body cat.body ) ) 3 #combine( #uw8( ape.body bat.body ) #uw8( bat.body cat.body ) ) ) )
1, 2 and 3 control the weight given to each representation. 1,2 and 3 control the weight the sequential dependency model places on matching anywhere in the body, in bigrams (#1), and in 8-term
unordered windows (#uw8) [35]. Parameters were determined us-
ing a parameter sweep on 200 queries from the TREC 2009-2012 Web Track topics for ClueWeb09-B. 1 = 0.20, 2 = 0.05, 3 = 0.75, 1 = 0.8, 2 = 0.1, and 3 = 0.1. Note that it is acceptable to use test data to set the exhaustive search query template parameters (1, . . . , 3) because in an operational environment the query template parameters would be carefully-tuned, fixed, and known.
For Gov2, the unstructured bag-of-words queries were trans-
formed into more effective structured queries by the sequential
dependency model (SDM) with parameters (0.85, 0.1, 0.05) [35]. Our
goal was to construct a competitive exhaustive baseline that is used
as the reference to measure an upper bound on the effectiveness
loss in the selective search environment.
Metrics. Early-precision experiments used P@5, NDCG@10, and Overlap@100 to measure accuracy. Recall-oriented experiments used Mean Average Precision (MAP) at rank 1000, RBP with p = 0.95 and Overlap@5000. P@5, NDCG@10, and MAP are included to enable comparison with prior research. In general care should
be taken when evaluating ClueWeb collections deeply [29]. Af-
ter exploring many options, we found Overlap@n to be the best
metric for evaluating selective search in an early-stage retrieval setting. Given rankings rd,e and rd,s of length n for exhaustive and selective search, Overlap@n = Count(rd,e  rd,s )/n. Results for multiple queries are macro-averaged. We used this metric because
it measures (only) how well selective search mimics exhaustive
search. We assume that the first stage of retrieval is a filtering
step, where the goal is to quickly find a set of candidate documents
that will be reordered by later retrieval stages, for example, using
learning-to-rank. Thus, the order of documents does not matter.

Table 4: Differences between labels and predicted cutoffs in Figures 2a-3b. Lower mean absolute error (MAE) and higher Pearson correlation coefficient (PCC) indicate better prediction.

ClueWeb09-B

Early-Precision

High-Recall

Rank-S Taily ShRkC RF QR Rank-S Taily ShRkC RF QR
MAE 1.31 1.34 2.99 1.67 1.14 2.91 2.84 4.85 2.31 1.94 PCC 0.37 0.34 0.26 0.41 0.44 0.38 0.39 0.28 0.53 0.64

Gov2

Early-Precision

High-Recall

Rank-S Taily ShRkC RF QR Rank-S Taily ShRkC RF QR
MAE 1.62 1.59 3.46 1.72 1.42 2.97 2.99 4.87 2.24 2.12 PCC 0.37 0.40 0.29 0.48 0.52 0.41 0.39 0.28 0.52 0.59

Efficiency was measured using CRES and CLAT [1]. CRES calculates resource usage for a query as the upper bound on the number of documents that match. CLAT measures query latency as the maximum number of documents that match in any selected shard. A
two one-sided test (TOST) of equivalence [38] was used to com-
pare results between exhaustive and selective search. The threshold for equivalence was set as 0.05 · µ, where µ is the mean value of exhaustive search for a specific metric. Equivalence is established
by rejecting the null hypothesis that selective search is at least 5%
worse than exhaustive search with a 95% confidence interval.
Baselines. Five resources selection methods were compared: Taily [1], ReDDE [41], Rank-S [25], learning to rank resources (L2RR) [13]
and ShRkC [22], a random forest based shard cutoff predictor that
uses ReDDE to generate the training data and shard cutoff estimator
features. A sample size of 1% was used for CSI-based experiments. ShRkC's random forest predictor has two parameters, mtry (the number of features to sample at each split in the learning process), and ntree (number of decision trees to fit for ensemble learning). We used mtry=p/3 where p is the total number of features, and ntree=500, which is consistent with prior work. Shard Cutoffs. Rank-S and Taily compute query-specific shard cutoffs that are influenced by parameters. Taily's parameters include v, the estimated number of relevant documents. Rank-S uses B, which controls the exponential decay of scores. ReDDE and L2RR use a query-independent shard cutoff (also pre-defined). In our ex-
periments, each resource selection algorithm used its own strategy,
but the parameters were tuned to produce shard cutoffs compatible
with early-precision and high-recall task requirements. For early-precision search, we used v = 25 (Taily) and B = 3.2
(Rank-S) when searching ClueWeb09-B; and v = 25 (Taily) and B = 2.6 (Rank-S) when searching Gov2. These differ from the v = 45 and B = 5 parameter values used in most prior Rank-S and Taily studies [21]. In our setting, the default parameter values were
less effective and easier to beat. 10-fold cross validation on the test
set produced parameters that most often gave Taily and Rank-S the
maximum performance. For recall-oriented selective search, which
has not been studied previously, we found empirically that 8 shards
for ClueWeb09-B, and 10 shards for Gov2 were usually enough to
achieve a result comparable with exhaustive search. The parameters
for Taily and Rank-S were tuned to search a similar number of

89

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Early-precision

(b) High-recall

Figure 2: The distributions of shard cutoff predictions for ClueWeb09-B under early-precision and high-recall conditions. The

x axis shows predicted cutoff values. The y axis shows the percentage of queries with each prediction.

(a) Early-precision

(b) High-recall

Figure 3: The distributions of shard cutoff predictions for Gov2 under early-precision and high-recall conditions. The x axis

shows predicted cutoff values. The y axis shows the percentage of queries with each prediction.

shards for each of the datasets. For this search scenario, we used v = 11 (Taily) and B = 1.6 (Rank-S) for ClueWeb09-B and v = 12 (Taily) and B = 1.6 (Rank-S) for Gov2. The query-independent shard cutoffs used by ReDDE and L2RR were set based on the average
number of shards searched by other resource selection methods in
each search scenario for both the datasets.
Search Scenarios. Query-specific shard cutoff prediction was studied in early-precision and recall-oriented settings. The following parameters were used: p: The RBP user persistence; : The MEDRBP threshold; and depth: The depth at which the document rankings rd,e and rd,s are compared. Our precision-oriented search parameters were p = 0.80,  = 0.08, and depth = 100. Our recall-oriented search parameters were p = 0.95,  = 0.06, and depth = 1, 000. A 5-fold cross validation was done on the test set to find the  parameters that most often gave the maximum performance. Also,
the variance in this parameter across several folds was almost neg-
ligible. It may seem counter-intuitive that early-precision uses a larger  than high-recall; this is due to the sensitivity of MEDRBP to ranking depth. MEDRBP assumes that all documents deeper than depth are a mismatch, which is a worst-case scenario. When depth is increased, actual mismatches are accounted for. A good MEDRBP value is easier to achieve with higher depth, and harder for lower depth, thus  and depth are inversely-related.

5 EXPERIMENTAL RESULTS
Four experiments investigated our research questions. Cutoff Prediction Comparisons. First we investigated the accuracy of query-specific cutoff prediction methods used with existing resource selection methods (RQ1) and our new cutoff prediction methods (RQ3). Each method was tuned or trained for earlyprecision and recall-oriented search on the ClueWeb09 and Gov2 datasets, as described in Sections 3 and 4.
Table 4 shows the mean average error (MAE) and Pearson correlation coefficient (PCC) for each method. MAE differences don't seem large for early-precision, but QR is clearly better under high-recall. Pearson correlation coefficients vary from [0.26 . . . 0.29] (ShRkC) and [0.34 . . . 0.41] (Taily and Rank-S) to [0.44 . . . 0.64] (QR).
Figures 2 and 3 show how well each method matches the distribution of the ground truth cutoff labels (Label) determined by the Find_k algorithm (Figure 1). Higher agreement indicates more accurate predictions. All methods are more accurate for the ClueWeb09 dataset than for the Gov2 dataset. In three out of four conditions, Taily is biased towards under-prediction. Rank-S, ShRkC, and RF over-predict the number of shards in all cases. QR is the most accurate of these methods at predicting shard cutoff labels; however, it over-predicts in three out of the four conditions.

90

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

ClueWeb09-B

Gov2

Early-Precision Oriented

Shard Ranking P@5 NDCG Overlap CRES CLAT @10 @100

Taily .370 .214 .623 .508 .180

Rank-S .375 .229 .673 .517 .178

ReDDE .386 .229 .708 .551 .190

L2RR .389 .234

rs,e

.409 .247

Exhaustive .390 .240

.734 .560 .189 .818 .534 .187
- 5.24 .330

Early-Precision Oriented

Shard Ranking P@5 NDCG Overlap CRES CLAT @10 @100

Taily .515 .340 .583 .191 .070

Rank-S .519 .342 .620 .189 .069

ReDDE .520 .363 .672 .237 .092

L2RR .597 .442

rs,e

.602 .447

Exhaustive .612 .441

.711 .193 .067 .832 .194 .071
- 2.655 .282

High-Recall Oriented

MAP Shard Ranking

RBP0.95 Overlap CRES CLAT

@1000

@5000

Taily .180 .261 (.339) .599 .811 .187

Rank-S .181 .279 (.349) .612 .811 .190

ReDDE .182 .281 (.345) .618 .853 .198

L2RR rs,e Exhaustive

.196 .293 (.304) .626 .896 .199 .202 .301 (.286) .709 .850 .195 .202 .292 (.309) - 5.24 .330

High-Recall Oriented

MAP Shard Ranking

RBP0.95 Overlap CRES CLAT

@1000

@5000

Taily .229 .461 (.049) .544 .346 .081

Rank-S .233 .479 (.054) .567 .324 .076

ReDDE .244 .483 (.045) .578 .373 .098

L2RR rs,e Exhaustive

.314 .499 (.034) .619 .325 .071 .323 .517 (.031) .701 .325 .077 .339 .508 (.030) - 2.655 .282

Table 5: Comparisons of shard ranking methods. RBP user persistence is 0.95. The number in brackets denotes the residual.

Note that the upward trend for Label at the right side of two figures is due to a long tail of queries with values above the K = 8 and K = 16 maximums for early-precision and high-recall conditions. Shard Ranking Comparisons. The second experiment explores shard ranking accuracy independently of shard cutoff estimates (RQ2), because an algorithm's accuracy at ranking shards may not match its accuracy at predicting the cutoff. This experiment uses the labels produced by the Find_k algorithm (Figure 1) as the cutoff prediction for all shard ranking algorithms. Thus, the only difference is how accurately different methods rank shards. We include rs,e , an ideal shard ranking produced from the exhaustive search document ranking (Section 3), to show the best-case scenario.
Rank-S and ReDDE may not generate a full shard ranking because they are sample-driven. Rank-S rankings are always less than or equal in length to ReDDE rankings, due to its exponential decay. When Rank-S returns fewer than K shards, its ranking is extended by appending shards from a ReDDE ranking, which performs similarly to Rank-S. When REDDE returns fewer than K shards, the Rank-S and ReDDE shard rankings are shorter than desired.
Table 5 shows the effectiveness and efficiency of each algorithm when searching the same number of index shards. L2RR produces rankings closest to exhaustive search, as measured by Overlap@n and RBP0.95; its values are higher than Taily, Rank-S and ReDDE. It also generates more accurate document rankings for both earlyprecision and recall-oriented metrics.
The residual values for RBP0.95 are high, which is expected in deeper evaluation scenarios when the relevance judgment pool depth is shallow. The number of unjudged documents is high in the recall-driven search scenario, and this effect should be explored further in future work. We have also observed that high residuals correlate with low accuracy scores, implying that shard ranking accuracy could be impacting the results.
All algorithms search the same number of shards, thus variations in CRES and CLAT are due to the sizes of selected shards. L2RR

selects larger shards than Taily, Rank-S, and ReDDE for ClueWeb09B, but smaller shards for Gov2. L2RR has many term-based features, thus it may favor shards with longer posting lists. This would be more likely to affect ClueWeb09, which has larger average shard sizes and a more skewed distribution of shard sizes.
Overall, L2RR balances effectiveness and efficiency better than Taily, Rank-S and ReDDE. Rank-S and ReDDE perform similarly and both outperform Taily. These results answer RQ2. Early Precision versus Recall Driven Search. The third experiment investigated the document ranking accuracy and efficiency of each method under early-precision and recall-oriented conditions. Each method used its own shard rankings and query-specific (Rank-S, Taily) or query-independent (ReDDE, L2RR) shard cutoff predictions. This experiment also included ReDDE shard ranking with ShRkC cutoff prediction as proposed by Kulkarni [22], and L2RR shard ranking with the new quantile regression (QR) and random forest (RF) cutoff predictions (Section 3). Cutoff prediction accuracy was measured using mean absolute error (MAE) relative to the test labels. Table 6 summarizes the results.
Taily, Rank-S, ReDDE, and L2RR produce higher accuracy, overlap, and computational costs in this experiment than in the second experiment; this is not surprising. The second experiment used the Find_k shard cutoffs (Figure 1), which assumes perfect shard ranking; however, none of the rankers are perfect. When using their own shard cutoffs, they search more shards, which produces higher accuracy and overlap at higher computational expense.
Taily was the most efficient, as indicated by low CRES; however, it was also the least accurate, as indicated by relevance and overlap metrics. This result is consistent with the first experiment, which showed that Taily frequently under-predicts the cutoff.
ReDDE with ShRkC cutoff predictions produces higher accuracy and overlap than ReDDE with query-independent cutoffs, but at higher computational cost. When shard rankers are inaccurate,

91

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

ClueWeb09-B

Gov2

Early-Precision Oriented

P@5

NDCG

Overlap

K¯

CRES CLAT MAE

@10 @100

(M) (M)

Taily Rank-S ReDDE L2RR ShRkC

.371
.393 .391 .409 .410

L2RR+QR45 .413 L2RR+RF .400

.221 .237
.229 .243 .244
.249 .241

.645 4.52 1.34 .521 .189 .690 4.37 1.31 .570 .189 .711 5 1.72 .559 .199 .744 5 1.72 .559 .201 .752 7.21 2.99 1.07 .231 .792 4.4 1.14 .530 .190 .789 5.1 1.67 .590 .211

Early-Precision Oriented

P@5

NDCG

Overlap

K¯

CRES CLAT MAE

@10 @100

(M) (M)

Taily .587 .400 Rank-S .590 .411

.594 5.48 1.59 .204 .071 .621 5.59 1.62 .231 .072

ReDDE .582
L2RR .595 ShRkC .597
L2RR+QR45 .616 L2RR+RF .596

.410
.438 .420
.446 .430

.683 5 1.58 .216 .068 .721 5 1.58 .218 .071 .743 7.20 3.46 .276 .106 .826 5.46 1.42 .224 .069 .824 6.30 1.72 .250 .091

Exhaustive .390 .240

- 123 - 5.24 .330

Exhaustive .612 .441

- 199 - 2.655 .282

High-Recall Oriented

Taily Rank-S ReDDE L2RR ShRkC

MAP

RBP0.95

Overlap

K¯

CRES CLAT MAE

@5000

(M) (M)

.173 .288(.318) .611 7.72 2.84 .841 .200

.182 .289(.308) .642 7.90 2.91 .843 .201

.181 .289(.338) .644 8 2.42 .871 .201

.192 .296(.317) .653 8 2.42 .893 .199

.197 .299(.308) .664 11.2 4.85 1.52 .230

L2RR+QR45 .198 .299(.308) .706 7.99 1.94 .872 .200 L2RR+RF .193 .294(.306) .703 8.60 2.31 .940 .205

High-Recall Oriented

Taily Rank-S ReDDE L2RR ShRkC

MAP

RBP0.95

Overlap

K¯

CRES CLAT MAE

@5000

(M) (M)

.291 .499(.044) .567 9.80 2.99 .341 .078

.300 .501(.042) .580 9.4 2.97 .353 .072

.301 .498(.058) .595 10 2.96 .391 .080

.321 .511(.049) .622 10 2.96 .381 .081

.320 .511(.044) .632 12.1 4.87 .683 .118

L2RR+QR45 .325 .516(.035) .690 9.71 2.12 .355 .074 L2RR+RF .324 .511(.043) .682 10.02 2.24 .430 .087

Exhaustive .202 .292 (.309) - 123 - 5.24 .330

Exhaustive .339 .508 (.030) - 199 - 2.655 .282

Table 6: Comparison of document ranking accuracy. K¯ is the average number of shards searched. MAE is the mean absolute error in predicting the number of shards that should be searched.  indicates statistical non-inferiority relative to exhaustive search. The best result for each metric is marked bold.

searching more shards improves accuracy but also increases costs. ShRkC greatly over-predicts shard cutoffs.
The L2RR, QR, and RF results use the L2RR shard ranking, but with different shard cutoff predictions. QR delivers the most accurate predictions, as measured by mean average error (MAE), which results in the highest accuracy and overlap values and some of the most efficient CRES and CLAT efficiency values. The RF predictor is less accurate than QR, which is consistent with the expectation that quantile regression is more effective when the underlying distribution is skewed.
QR was slightly more effective than exhaustive search for all relevance-based metrics in the early-precision experiments, and for RBP0.95 in high-recall experiments. We don't want to overemphasize these results, because beating exhaustive isn't the right goal for an early-stage ranker. However, these results remind us that it isn't necessary for Overlap@n to be 100% for selective search to deliver high-quality documents to the next stage rankers. Learned shard rankers with learned cutoffs are becoming very effective.
Figures 4a ­ 5b show effectiveness vs. efficiency tradeoffs among the different methods, and for QR with different values of its  parameter. The x-axis shows resource usage (CRES). The y-axis shows an early-precision or recall-oriented effectiveness metric. The goal is accuracy and computational costs close to rs,e .
Taily is more efficient than Rank-S, ReDDE and L2RR but less effective. L2RR is more effective than Taily, Rank-S, and ReDDE, but usually computationally expensive.

Quantile regression's  parameter enables tuning the efficiency vs. effectiveness tradeoff. A smaller  focuses more on efficiency; a larger  focuses more on recall. A reasonable range of parameter values produce better accuracy and efficiency than baseline algorithms.  = 0.45 ­ the value chosen using 10-fold cross-validation on the training set to minimize the mean average error (MAE) ­
best balances efficiency and effectiveness in both scenarios.
This third experiment shows that both quantile regression and
random forests with the features described in Section 3 produce shard cutoff predictions that deliver substantially higher Overlap@n values than all baseline methods on both datasets under early-
precision and high-recall conditions. QR gives a better mix of effec-
tiveness and efficiency than RF, and is easily tuned to give greater
control over the competing goals of efficiency and effectiveness. Training Labels Comparisons. The experiments above use rs,e , a shard ranking generated from exhaustive search results, to produce
the `gold standard' shard cutoffs used for training and testing. They
show that those cutoffs can be too aggressive for less perfect shard
ranking algorithms.
The Find_k algorithm (Figure 1) can use any shard ranking to generate training data for the QR predictor; it need not be rs,e . This experiment investigates using shard rankings produced by Taily and L2RR (rs,T aily and rs,L2RR ) to train QR predictors that may be more compatible with those algorithms (RQ5). This experiment omits
Rank-S and ReDDE because they are unable to generate complete
shard rankings. Results are shown in Table 7. Exhaustive search
results are shown for comparison, as in previous experiments.

92

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Early-precision oriented search.

(b) High-recall oriented search.

Figure 4: Efficiency-effectiveness tradeoffs for ClueWeb09-B. rs,e indicates using test labels with rs,e shard rankings.

(a) Early-precision oriented search.

(b) High-recall oriented search.

Figure 5: Efficiency-effectiveness tradeoffs for Gov2. rs,e indicates using test labels with rs,e shard rankings.

Training with shard rankings matched to exhaustive search (rs,e ) always produces more aggressive shard cutoff predictions and much more efficient search than training with shard rankings produced by Taily and L2RR. This result is to be expected, because rs,T aily and rs,L2RR cannot be better orderings than rs,e .
Combining Taily with a QR predictor trained for Taily is more effective across all relevance and overlap metrics than combining it with a general QR predictor trained from rs,e ; perhaps this is not surprising. However, combining L2RR with a QR predictor trained for L2RR is a little less accurate across most relevance and overlap metrics than combining it with a general QR predictor trained from rs,e ; in this case, pairing more accurate shard cutoff estimates with a more accurate shard ranker produces slightly better search results at much lower computational cost.
In answering RQ5, we conclude that if more accurate shard rankings (L2RR) are used, training with ranker-independent labels is more accurate. If less accurate resource selection algorithms are used, training with ranker-specific labels is more effective.
6 CONCLUSION
Previous studies treat selective search as a single stage retrieval method, and thus focus on optimizing early-precision metrics. We argue that selective search is a better choice for early-stage retrieval, thus evaluation should focus on high recall and how well selective search reproduces exhaustive search. We also argue that shard ranking and deciding how many shards to search should be studied separately, because they are separate sources of error.

There is substantial variation in the accuracy of shard cutoff decisions made by Taily and Rank-S, two algorithms often used for selective search; and by ShRkC, a newer shard cutoff predictor used with ReDDE. When attention is focused on ranking accuracy, Taily is the least accurate of the algorithms studied, the older ReDDE is surprisingly competitive, and the newer L2RR is the most effective.
This paper presents a new feature-based method of making query-specific shard cutoff decisions that can be trained for use with different shard ranking algorithms and/or tuned to satisfy early-precision or high-recall retrieval goals by adjusting (only) how training data is generated. The QR predictor produces higher agreement with exhaustive search results (Overlap@n) for Taily and L2RR than their default methods of predicting shard cutoffs.
Finally, although previous studies focused almost entirely on the accuracy of selective search at ranks 5-30, we show that selective search can deliver about 70% agreement with exhaustive search down to about rank 5,000, while requiring only 16-18% of the computational effort on two widely-studied datasets. These results support the argument that selective search is a good choice for early-stage retrieval in sophisticated multi-stage retrieval pipelines.
7 ACKNOWLEDGEMENTS
This research was supported by National Science Foundation (NSF) grant IIS-1302206 and the Australian Research Council's Discovery Projects Scheme (DP170102231). Any opinions, findings, and conclusions in this paper are the authors' and do not necessarily reflect those of the sponsors.

93

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

ClueWeb09-B

Early-precision search

Training K¯ P@5 Data

Taily rs,e Taily rs,Taily L2RR rs,e L2RR rs,L2RR

4.40 .371 7.89 .389 4.40 .413 7.23 .412

Exhaustive 123 .390

NDCG @10 .219 .221 .245 .218 .240

Overlap CRES CLAT @100 .643 .49 .175 .689 .88 .189 .792 .53 .190 .772 .87 .193
- 5.24 .330

Recall-oriented search

Gov2

Early-precision search

Training K¯ P@5 Data
Taily rs,e 5.48 .516 Taily rs,Taily 7.46 .549 L2RR rs,e 5.46 .615 L2RR rs,L2RR 7.84 .615
Exhaustive 199 .612

NDCG @10 .341 .342 .447 .390 .441

Overlap CRES CLAT @100 .592 .222 .072 .683 .284 .102 .826 .224 .069 .812 .292 .103
- 2.655 .282

Recall-oriented search

Training K¯ MAP RBP0.95 Overlap CRES CLAT

Data

@5000

Taily rs,e 7.99 .180 .270 (.339) .617 .83 .199 Taily rs,Taily 11.52 .189 .272 (.310) .646 1.28 .199 L2RR rs,e 7.99 .198 .299 (.308) .706 .84 .200 L2RR rs,L2RR 11.48 .199 .289 (.312) .707 1.32 .201

Exhaustive 123 .202 .292 (.309) - 5.24 .330

Training K¯ MAP RBP0.95 Overlap CRES CLAT

Data

@5000

Taily rs,e 9.71 .230 .474 (.031) .564 Taily rs,Taily 13.2 .297 .508 (.041) .608 L2RR rs,e 9.71 .325 .516 (.035) .690 L2RR rs,L2RR 12.9 .324 .498 (.039) .699

.342 .072 .712 .118 .355 .074 .709 .118

Exhaustive 199 .339 .508 (.030) - 2.655 .282

Table 7: A comparison of using different shard rankings to generate training data for the QR predictor.

REFERENCES
[1] R. Aly, D. Hiemstra, and T. Demeester. Taily: Shard selection using the tail of score distributions. In Proc. SIGIR, pages 673­682, 2013.
[2] J. Arguello, F. Diaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. In Proc. SIGIR, pages 315­322, 2009.
[3] C. Buckley. Topic prediction based on comparative retrieval rankings. In Proc. SIGIR, pages 506­507, 2004.
[4] J. P. Callan, Z. Lu, and W. B. Croft. Searching distributed collections with inference networks. In Proc. SIGIR, pages 21­28, 1995.
[5] B. B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and
J. Degenhardt. Early exit optimizations for additive machine learned ranking systems. In Proc. WSDM, pages 411­420, 2010. [6] D. Carmel and E. Yom-Tov. Estimating the Query Difficulty for Information Retrieval. Morgan & Claypool, 2010. [7] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. In Proc. CIKM, pages 1867­1870, 2009. [8] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. Expected reciprocal rank for graded relevance. In Proc. CIKM, pages 621­630, 2009. [9] R.-C. Chen, L. Gallagher, R. Blanco, and J. S. Culpepper. Efficient cost-aware cascade ranking in multi-stage retrieval. In Proc. SIGIR, pages 445­454, 2017. [10] C. L. A. Clarke, J. S. Culpepper, and A. Moffat. Assessing efficiency­effectiveness
tradeoffs in multi-stage retrieval systems without using relevance judgments. Inf. Retr., 19(4):351­377, 2016. [11] J. S. Culpepper, C. L. A. Clarke, and J. J. Lin. Dynamic cutoff prediction in multi-stage retrieval systems. In Proc. ADCS, pages 17­24, 2016. [12] Z. Dai, C. Xiong, and J. Callan. Query-biased partitioning for selective search. In Proc. CIKM, pages 1119­1128, 2016. [13] Z. Dai, Y. Kim, and J. Callan. Learning to rank resources. In Proc. SIGIR, 2017. [14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing top k lists. SIAM J. Discrete Math., 17(1):134­160, 2003. [15] L. Gravano, H. Garcia-Molina, and A. Tomasic. Gloss: Text-source discovery over the internet. ACM Trans. Database Systems, 24(2):229­264, 1999. [16] F. Hafizoglu, E. C. Kucukoglu, and I. S. Altingovde. On the efficiency of selective search. In Proc. ECIR, pages 705­712, 2017. [17] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Information Systems, 20(4):422­446, Oct. 2002. [18] M. G. Kendall. Rank correlation methods. Griffin, 1948. [19] Y. Kim, J. Callan, J. S. Culpepper, and A. Moffat. Load-balancing in distributed selective search. In Proc. SIGIR, pages 905­908, 2016. [20] Y. Kim, J. Callan, J. S. Culpepper, and A. Moffat. Does selective search benefit from WAND optimization? In Proc. ECIR, pages 145­158, 2016. [21] Y. Kim, J. Callan, J. S. Culpepper, and A. Moffat. Efficient distributed selective search. Inf. Retr., 20(3):221­252, 2017. [22] A. Kulkarni. Shrkc: Shard rank cutoff prediction for selective search. In Proc. SPIRE, pages 337­349, 2015. [23] A. Kulkarni and J. Callan. Document allocation policies for selective searching of distributed indexes. In Proc. CIKM, pages 449­458, 2010. [24] A. Kulkarni and J. Callan. Selective search: Efficient and effective search of large textual collections. ACM Trans. Information Systems, 33(4):17:1­17:33, 2015.

[25] A. Kulkarni, A. S. Tigelaar, D. Hiemstra, and J. Callan. Shard ranking and cutoff estimation for topically partitioned collections. In Proc. CIKM, pages 555­564, 2012.
[26] R. Kumar and S. Vassilvitskii. Generalized distances between rankings. In Proc. WWW, pages 571­580, 2010.
[27] A. Liaw and M. Wiener. Classification and regression by randomforest. R news, 2(3):18­22, 2002.
[28] T.-Y. Liu. Learning to rank for information retrieval. Found. Trends in Inf. Ret., 3 (3):225­331, 2009.
[29] X. Lu, A. Moffat, and J. S. Culpepper. The effect of pooling and evaluation depth on IR metrics. Inf. Retr., 19(4):416­445, 2016.
[30] C. Macdonald, N. Tonellotto, and I. Ounis. Learning to predict response times for online query scheduling. In Proc. SIGIR, pages 621­630, 2012.
[31] C. Macdonald, R. L. T. Santos, and I. Ounis. The whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.
[32] J. Mackenzie, J. S. Culpepper, R. Blanco, M. Crane, C. L. A. Clarke, and J. Lin. Query driven algorithm selection in early stage retrieval. In Proc. WSDM, pages 396­404, 2018.
[33] I. Markov and F. Crestani. Theoretical, qualitative, and quantitative analyses of small-document approaches to resource selection. ACM Trans. Information Systems, 32(2):9:1­9:37, 2014.
[34] M. Melucci. Weighted rank correlation in information retrieval evaluation. In Proc. AIRS, pages 75­86, 2009.
[35] D. Metzler and W. B. Croft. A Markov random field model for term dependencies. In Proc. SIGIR, pages 472­479, 2005.
[36] A. Moffat and J. Zobel. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Information Systems, 27(1):2:1­2:27, 2008.
[37] J. Pedersen. Query understanding at Bing. Invited talk, SIGIR, 2010. [38] D. J. Schuirmann. A comparison of the two one-sided tests procedure and the
power approach for assessing the equivalence of average bioavailability. Journal of Pharmacokinetics and Pharmacodynamics, 15(6):657­680, 1987. [39] E. Sener, I. H. Toroslu, and I. S. Altingovde. An analysis of resource selection with rank cut-off estimation in meta-search. In SIGIR Workshop on Heterogeneous Information Access, 2016. [40] M. Shokouhi. Central-rank-based collection selection in uncooperative distributed information retrieval. In Proc. ECIR, pages 160­172, 2007. [41] L. Si and J. P. Callan. Relevant document distribution estimation method for resource selection. In Proc. SIGIR, pages 298­305, 2003. [42] M. Sun, G. Lebanon, and K. Collins-Thompson. Visualizing differences in web search algorithms using the expected weighted hoeffding distance. In Proc. WWW, pages 931­940, 2010. [43] L. Tan and C. L. A. Clarke. A family of rank similarity measures based on maximized effectiveness difference. Trans. on Know. and Data Eng., 27(11):2865­ 2877, 2015.
[44] N. Tax, S. Bockting, and D. Hiemstra. A cross-benchmark comparison of 87 learning to rank methods. Inf. Proc. & Man., 51(6):757­772, 2015.
[45] P. Thomas and M. Shokouhi. SUSHI: scoring scaled samples for server selection. In Proc. SIGIR, pages 419­426, 2009.
[46] W. Webber, A. Moffat, and J. Zobel. A similarity measure for indefinite rankings. ACM Trans. Information Systems, 28(4):20:1­20:38, 2010.

94

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Multihop Attention Networks for Question Answer Matching

Nam Khanh Tran, Claudia Niederée
L3S Research Center, Leibniz Universität Hannover Hannover, Germany
{ntran,niederee}@L3S.de

ABSTRACT
Attention based neural network models have been successfully applied in answer selection, which is an important subtask of question answering (QA). These models often represent a question by a single vector and find its corresponding matches by attending to candidate answers. However, questions and answers might be related to each other in complicated ways which cannot be captured by single-vector representations. In this paper, we propose Multihop Attention Networks (MAN) which aim to uncover these complex relations for ranking question and answer pairs. Unlike previous models, we do not collapse the question into a single vector, instead we use multiple vectors which focus on different parts of the question for its overall semantic representation and apply multiple steps of attention to learn representations for the candidate answers. For each attention step, in addition to common attention mechanisms, we adopt sequential attention which utilizes context information for computing context-aware attention weights. Via extensive experiments, we show that MAN outperforms state-ofthe-art approaches on popular benchmark QA datasets. Empirical studies confirm the effectiveness of sequential attention over other attention mechanisms.
KEYWORDS
Answer selection, non-factoid QA, representation learning, attention mechanism
ACM Reference Format: Nam Khanh Tran, Claudia Niederée. 2018. Multihop Attention Networks for Question Answer Matching. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210009
1 INTRODUCTION
Answer selection (AS) is an important subtask of question answering (QA) that enables selecting the most suitable answer from a list of candidate answers in regard to the input question. One main challenge of this task lies in the complex and versatile semantic relations that can be observed between questions and answers. While for factoid QA the task of answer selection may be largely cast as a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210009

Table 1: An example of a question with an answer from the FiQA dataset. The segments in the answer are related to the segments in the question by the same color.
Question: Are companies in California obliged to provide invoices?
Ground-truth answer: We run into this all the time with our EU clients. As far as I can tell, the only requirements when it comes to invoicing have to do with sales tax, which is determined at the state level, and only in the case that items are taxable. It seems that the service provided to you is not taxable and so there is no obligation under Californian law to provide what you need.
textual entailment problem, for non-factoid QA what makes an answer better than another often depends on many factors. Different from many other matching tasks, the linguistic similarities between questions and answers may or may not be indicative for the good answers; depending on what the question is looking for, a good answer may come in different forms. Sometimes a correct answer completes the question precisely with the missing information, and in other scenarios, good answers need to elaborate part of the question to rationalize it, and so on. In other cases, the best answers can also be noisy and include extraneous information irrelevant to the question. In addition, while a good answer must relate to the question, they might not share common lexical units. For example, in the question in Table 1, "companies" is not directly mentioned in the answer. This issue may confuse simple word-matching systems.
These challenges consequently make the traditional models which are commonly based on lexical features [29, 30, 33] less effective compared to deep learning based methods [8, 28]. The neural models often follow the two step procedure: Firstly, representations of questions and answers are learned via a neural encoder such as long short-term memory (LSTM) networks or convolutional neural networks (CNN); Secondly, these representations of questions and answers are composed by an interaction function to produce an overall matching score. In the first step, each word in a question or an answer sequence is first represented with a hidden vector and then all the hidden vectors are aggregated for sequence representations. These models have shown very successful results in the AS task, however they still suffer from an important issue. The answers can be very long and contain lots of words that are not related to the question at hand, especially in non-factoid QA; consequently, the resulting representation might be distracted by non-useful information.
Recent years, attention-based models are proposed to deal with this challenge and have shown great success in many tasks such

325

Session 3C: Question Answering
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
as machine translation [2, 23], machine reading comprehension [11] and textual entailments [19]. In the AS task, attention-based approaches aim to focus on segments within the candidate answer that are most related to the question [24, 27]. The segments with a stronger focus are treated as more important and have more influence on the resulting representations. For example, in attentionbased LSTM models [24] as shown in Figure 2, a weight is automatically generated for each word in the answer via an attention model, and the answer is represented as the weighted sum of the hidden vectors. Various attention mechanisms have been proposed in previous studies in which additive attention [2] and multiplicative attention [20] are the two most commonly used. While additive attention is associated with a multi-layer perceptron for computing attention weights, multiplicative attention uses inner product for the weight estimations. Though these attention mechanisms have shown promising results in answer selection, they do not make use of surrounding word context when calculating attention weights, which has been proved to enhance the performance of LSTM based QA [5]. To address this issue, we adopt another attention mechanism, i.e. sequential attention [3] in which an additional LSTM is added to compute a context-aware weight for each hidden vector. This mechanism helps generate more accurate answer representation regarding to the question.
A common characteristic of the previous attention-based approaches is that the question is represented by one feature vector and a round of attention is applied for learning the representation of the answer. However, in many cases different segments of the answer can be related to different parts of the question. For example, in Table 1 the segment "the only requirements when it comes to invoicing have to do with sales tax" is relevant to "provide invoices" mentioned in the question, while "there is no obligation under Californian law" answers "California obliged" stated in the question. Consequently, using one feature vector pair for question answer matching may be not capable of capturing the complex semantic relations between questions and answers. This can lead to suboptimal results. Clearly, it is expected that the more aspects an answer covers the better the answer is. A good system should reflect this expectation.
In this paper, we propose Multihop Attention Networks (MANs) to deal with this problem. MANs locate, via multiple steps of attention, answer segments that are relevant to different aspects of the question. As illustrated in Figure 1(b), the MAN first uses the question vector to deduce the answer vector in the first attention layer, then the question vector is refined in the next step to learn the answer vector in the second attention layer. Each attention step gives a different attention distribution focusing on the segments that are relevant to one aspect of the question. Finally, we sum up the matching score in each step for scoring the answer. We perform experiments on both factoid QA and non-factoid QA datasets. Experimental results show that our proposed models obtain highly competitive results and outperform state-of-the-art approaches.
The main contributions of our paper can be summarized as follows:
· We are the first to investigate the effectiveness of sequential attention mechanism for answers' attentive representations in answer selection.

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA
Nam Khanh Tran, Claudia Niederée
· We propose Multihop Attention Networks which represent questions by multiple vectors and use multiple steps of attention for learning the representation of answers. By doing this, MANs can capture different semantic relations between questions and answers.
· We provide extensive experimental evidence of the effectiveness of our model on both factoid question answering and community-based question answering on different domains. Our proposed approach outperforms many other neural architectures on these datasets.
2 RELATED WORK
Our work is concerned with ranking question and answer pairs to select the most relevant answer for each question. Previous work on this task have primarily used feature engineering, linguistic tools, or external resources [29, 30, 33]. In [33], Yih et al. constructed semantic features based on WordNet and paired semantically related words based on word semantic relations. In [29, 30], the answer selection problem was transformed to a syntactical matching between the question and answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees [9, 21, 32]. However, apart from relying on the availability of additional resources, the effort of feature engineering and the systematic complexity introduced by the linguistic tools, such models have limited performance and are outperformed by modern deep learning approaches [22, 35].
Yu et al. [35] employed a convolutional neural network (CNN) for feature learning of QA pairs and subsequently applied logistic regression for prediction. Despite its simplicity, the approach outperforms all traditional approaches [21, 32, 33]. Another attractive quality of deep learning architectures is that features can be learned in an end-to-end fashion. Severyn et al. [22] presented a unified architecture that trains a convolutional neural network together with a multi-layer perceptron, in which features are learned while the parameters of the network are optimized for the task at hand. In addition to CNN, recurrent neural networks such as the long short-term memory (LSTM) networks are also very popular for learning sequence representation and have been widely adopted in QA [24, 25, 28]. In [28], Wang and Nyberg incorporate stacked LSTMs to learn a joint feature vector of question and answer for classification. In [25], Tan et al. combined CNN and LSTM into a hybrid architecture which utilizes the advantages of both architectures. However, these approaches are outperformed by models with attention mechanism.
Recently, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation [2, 23], machine reading comprehension [11], text summarization [20] and textual entailment [19]. Such models learn to focus their attention to specific parts of their input and most of them are based on a one-way attention, in which the attention is basically applied over one type of input based on another input (e.g. over target languages based on the source languages for machine translation, or over documents according to queries for reading comprehension). Most recently, several two-way attention mechanisms are proposed [7, 19, 34], where the information from two input items can influence the computation of each others representations. Both types of attention

326

Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

show similar performances on AS [7, 25], thus in this paper we only use the one-way attention. However, unlike the previous work, our proposed models use multiple steps of attention instead of one attention step only.
Additive attention [2] and multiplicative attention [20] are the two most commonly used attention mechanisms. Self-attention or intra-attention is a special case of the additive attention mechanism. It relates elements at different positions from a single sequence by computing attention between each pair of tokens. In recent works, it has been shown effective in natural language inference [16], reading comprehension [13] and neural machine translation [26]. Sequential attention [3] is another type of attention mechanisms which uses an additional bi-directional RNN layer. This additional layer allows local alignment information to be used when computing the attentional score for each token. It has shown promissing results on reading comprehension [3]. In this paper, we show that sequential attention can be well adopted for the AS task and obtain highly competitive results.
Our proposed MANs are also related to Dynamic Memory Networks (DMNs) [14] in the sense that we both use an iterative attention process instead of only one round of attention. However, each memory in DMNs depends on the memory in the previous step, aiming to narrow down their focus on individual facts or sentences in regard to the single question representation, while in MANs each attention step is applied independently for selecting the informative parts of answers relating to different aspects of the question.

3 APPROACHES
Although CNNs can be used for representation learning in the AS task, LSTMs have been shown to obtain better performances [7, 24]. Hence, in this work we base our attention-based models on a variation of the LSTM model. We first describe the basic framework for answer selection based on LSTMs, called QA-LSTM [24]. Next we describe in detail different attention-based models that build on top of the QA-LSTM framework. After that, we present our Multihop Attention Networks.

3.1 Long Short-Term Memory (LSTM)

Long Short-Term Memory (LSTM) [12] networks are a type of
recurrent neural network that are capable of learning long term dependencies across sequences. Given an input sequence X = (x1, x2, ..., xn ), where each xt is an E-dimension word vector (xt  RE ), the LSTM returns a sequence embedding or hidden vector ht with a size d (ht  Rd ) at every time step t defined as follows:

it =  (Wi xt + Uiht -1 + bi )

ot =  (Woxt + Uoht -1 + bo )

ft =  (Wf xt + Uf ht -1 + bf )

ut = tanh (Wu xt + Uuht -1 + bu )

(1)

Ct = it  ut + ft  Ct -1

ht = ot  tanh(Ct )

where W, b, U are the parameters of the LSTM network (W  Rd×E , U  Rd×d , b  Rd ) and  = {i, o, f , u} in which the input
i, forget f and output o are three gates, and Ct is the cell state.  is the sigmoid function and  denotes element-wise multiplication.

For the sake of brevity, we omit the technical details of LSTM which can be found in many related works.
Single-direction LSTMs suffer from the weakness of not making use of the contextual information provided by future tokens. Bidirectional LSTMs (biLSTMs) use both the previous and future context by processing the sequence in two directions, and generate two sequences of output vectors. The output for each token is the concatenation of the two vectors from both directions, i.e.
- - ht = ht  ht . The output of biLSTM layer is a sequence of hidden vectors H  RL×2d where L is the maximum sequence length and d is the dimensional size of LSTM.

3.2 LSTM for Answer Selection

The basic LSTM-based framework for answer selection (QA-LSTM)

[24] is shown as in Figure 1(c) (without the attention layer). Given an

input pair (q, a), where q = (q1, ..., ql ) is a sequence of word indices for question and a = (a1, ..., am ) is a sequence of word indices for
candidate answer, the word embeddings (WEs) of both q and a are

first retrieved by passing the sequences of word indices through a look-up layer. The parameters of this layer are W  RV ×E where

V is the size of vocabulary and E is the dimensionality of the word

embeddings. We initialize W with pretrained word embeddings

which is inline with previous works [24, 35].

A biLSTM is then applied separately over the two sequences

of WEs creating hidden vectors for the question and answer, i.e.

-

-

hq (t) = LSTM(hq (t - 1), qt )  LSTM(hq (t + 1), qt ) and ha (t) =

-

-

LSTM(ha (t - 1), at )  LSTM(ha (t + 1), at ). Subsequently, the final

representations oq and oa for question and answer, respectively,

can be taken by max or mean pooling over all the hidden vectors or

the last hidden vector. As discussed in [8, 25], sharing the same net-

work parameters is significantly better than using separate question

and answer parameters, and converges much faster. Therefore, we

follow the same procedure by using the same network parameters

for processing questions and candidate answers.

Finally, a cosine similarity sim(q, a) is defined to score the input

pair (q, a) and the hinge loss function is used as training objective.

L = max{0, M - sim(q, a+) + sim(q, a-)}

(2)

where a+ is a ground truth answer, a- is an incorrect answer randomly chosen from the entire answer space, and M is the margin which controls the extent of discrimination between positive QA pairs and corrupted QA pairs. We treat any question with more than one ground truth as multiple training examples. During training, for each question we randomly sample N negative answers, but only use the one with the highest L to update the model similar to [18, 24].

3.3 Attention Mechanisms
The aforementioned QA-LSTM is basically a siamese network [6] which might fail to notice a potential issue. The answers might be extremely long and contain lots of words that are not related to the question at hand, especially in non-factoid question answering. For example, in Table 1 the first sentence "we run into this all the time with our EU clients" does not relate directly to the question. Even if advanced neural networks are exploited, the resulting representation might still be distracted by non-useful information. Thus,

327

Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Nam Khanh Tran, Claudia Niederée

Question

Question

Question

"

"

P

sim

Answer

A #

(a)

"(%)

A

 (%)

+

A #(%)

"(')

A

 (') +

A #(')

"(()  (() sim
#(()

Answer Question

(b)
Answer Question

Question

" P
sim

Answer

A #

(c)

"(%)

A

 (%) +

A #(%)

"(')

A

 (') +

A #(')

"(()  (() sim
#(()

Answer

(d)
Answer

Figure 1: Traditional attention-based networks: (a) Interactive attention network; (c) Self-attention network; and our proposed MANs: (b) Multihop interactive attention network; (d) Multihop self-attention network. P: pooling layer, A: attention layer

a number of attention-based models for the answer vector generation have been proposed in order to alleviate this weakness by dynamically aligning the more informative parts of answers to the question. Conceptually, attention mechanisms give more weight to certain words which have more influence on the resulting representation. In the AS task the expectation is that words in the candidate answer that are more important with regard to the input question should receive larger weights. Most previous works such as [18, 24] proceed as follows: the input question is represented by a vector oq using last, max or average pooling and an attention model is used over a sequence of hidden vectors to learn the representation of the answer oa as shown in Figure 2. An attention mechanism which takes into account the question vector for computing the attention weights in learning the answer representation, is called as an interactive attention mechanism. In contrast, an attention mechanism which is employed only on the candidate answer, is considered as a self-attention or intra-attention mechanism. One of the advantages of the intra-attention mechanism is that questions and answers can be embedded into a joint vector space without being paired, so that arbitrary question and answer vectors in that space are directly comparable.
Let Ha = {ha (1), ha (2), ..., ha (m)} denote the hidden vectors of the answer after passing through the biLSTM layer. To produce the final representation of the answer, instead of using the last hidden

(a) (b)



)

*
* (1)

attention

" # $

* ()
...
...

)(1) )(2) )(3)

%

&

...

...

) ()
...

) ( )
...

...

...

Question

Answer

Figure 2: (a) The question vector representation and (b) The attention mechanism for answer vector generation

vector or average or max pooling, an additional attention layer is used as follows:

t  fattention o~f , ha (t )

oa = t ha (t )

(3)

t

where ha (t) is the hidden vector of the answer at time t. When
the interactive attention mechanism is used, o~f is often equal to the question representation oq as shown in Figure 1(a). When the

328

Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching

intra-attention mechanism is used, fattention only depends on ha (t) as in Figure 1(c). Next, we describe in detail the different implementations of fattention function.
MLP Attention: Additive attention (or multi-layer perceptron
attention) [2] is one of the most commonly used attention mech-
anisms. It is first used for answer selection by Tan et al. [24]. In [24], the attention function fattention is computed by a multi-layer perceptron network as follows:

m(t) = tanh Waha (t) + Wqoq

fattention o~f , ha (t ) = softmax wmT m(t )

(4)

where Wa,Wq are attentive weight matrices and wm are attentive weight vector. The size of the matrices and vector are often equal to the size of input vectors ha (t) and oq .
Bilinear Attention: This is another commonly used attention mechanism. For example, Chen et al. [4] found it effective in machine reading comprehension, Rush et al. [20] used it in abstractive summarization. Santos et al. [7] used this attention mechanism for AS task. In contrast to the additive attention, this attention mechanism makes use of a bilinear term instead of using a tanh layer to estimate the attention function fattention :

fattention o~f , ha (t ) = softmaxt oTq Wsha (t )

(5)

where Ws is a network parameter. Sequential Attention: The previous approaches to attention
select words with only very indirect consideration of their con-
text, Brarda et al. [3] address this issue by taking into account
explicit context sensitivity for computing the attention scoring function fattention . Specifically, instead of producing a single value of fattention for each word in the answer by using a bilinear term as the bilinear attention, a vector t is defined as

t = oq  ha (t )

(6)

where  is element-wise multiplication. The vector t is then fed

into a new biLSTM layer to get the representation: t = LSTM(- t -1, t

)hidLdSeTnMa(tt-ent +ti1o,nt).t

vector Finally,

the attention function fattention is computed as

fattention o~f , ha (t ) = softmaxt 1T t

(7)

Chen et al. [5] showed that utilizing context information can enhance the performance of LSTM based QA. Therefore, in this paper we aim to investigate the effectiveness of sequential attention in answer selection. Experimental results show that sequential attention can be well adapted for the AS task and outperform other attention mechanisms on different QA datasets.
Self-Attention: In contrast to aforementioned attention mechanisms where the question vector oq is used to learn the representation of the answer, in this attention mechanism the answer is autonomously embedded into the embedding space without being paired with the question, so that arbitrary question and answer vectors in the space are directly comparable. Generally, self-attention relates different positions of a single sequence in order to compute the final representation of the sequence. This has been successfully employed in a variety of tasks including reading comprehension, abstractive summarization and textual entailment [15]. In the con-
text of answer selection, fattention o~f , ha (t) can be estimated

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

merely based on ha (t) as follows:

s(t) = tanh (Wsha (t) + bs )

fattention o~f , ha (t ) = softmax wsT s(t )

(8)

where Ws , bs and ws are attention parameters.

3.4 Multihop Attention Networks
In many cases, the semantic relations between a question and an answer can be very complex. Different parts of the answer can relate to different aspects addressed by the question. For example, in Table 1, the question "are companies in California obliged to provide invoices" refers to two aspects invoices and California law and each aspect is covered by different parts of the answer. Consequently, using single vectors for the question and answer representations might not be able to uncover their complex semantic relations. This can lead to suboptimal results.
In this paper, we propose Multihop Attention Networks (MANs) to tackle this problem. Our models are represented in Figure 1(b) and 1(d). Unlike existing models [7, 24], we do not compress the question to a single representation, but instead use multiple vectors for the question representation. Each question vector is then used to match with the answer representation which is learned via an attention layer. Specifically, given a question and a candidate answer, the model first reads the question and the answer using a biLSTM layer. Then, it deploys an iterative matching process to uncover the semantic relations between the question and the answer. In this phase, it first attends to some parts of the question, then finds their corresponding matches by attending to the answer. In the next step, it gives more attention to other parts of the question and searches for their matching parts in the answer. After a fixed number of iterations, the model uses the sum of the matching scores of each step to rank the question-answer pair.
Let Hq = {hq (1), ..., hq (l)} denote the hidden vectors of the question after passing through the biLSTM layer. To obtain the representation of the question, one of three following mechanisms is usually used: last, mean, or max pooling. Last pooling takes the last vector hq (l), mean pooling averages all vectors and max pooling takes the element-wise maximum of Hq . In [15], Lin et al. proposed self-attention mechanism to replace the max pooling or averaging step. In this paper, we adapt this mechanism with some modifications for creating different question vector representations. Specifically, in each step k, the question representation oq(k) is computed as follows:

st(k) = tanh Wq(k)hq (t )  tanh Wm(k)mq(k-1)

t(k) = softmax ws(k)T st(k)

(9)

oq(k) = t(k)hq (t )
t

where Wq(k),Wm(k) and ws(k) are network parameters, mq(k) is a separate memory vector for guiding the next attention step. It is recur-
sively updated by

mq(k) = mq(k-1) + oq(k)

(10)

329

Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

The initial memory vector mq(0) is defined based on the context

vector oq(0) where

oq(0)

=

1 l

t

hq (t)

In each step, the question is represented by a vector oq(k) which focuses specifically on some aspects of the question. The vector oq(k) is then used as input for the attention models described in the previous section to extract the answer representation oa(k). After that, we compute the similarity between question and answer vec-

tors by their cosine similarity. This similarity score reflects on how

the answer relates to the corresponding parts of the question. After

performing K matching steps, the final similarity between the given

question and answer becomes

K

sim(q, a) = cos(oq(k), oa(k))

(11)

k

The overall architecture of this model when K = 2 is shown in

Figure 1(b). Figure 1(d) presents MAN with using self-attention

mechanism for the answer vector generation. In this case, we use

two separate attention models described in Equation 9, one model

for questions and another for answers. After each step, the same

procedure is applied as implied by Equation 11. It is important to

note that separate attention models are applied to questions and

answers but the same attention parameters are used for questions

(answers) in different steps. Therefore, our network parameters are

comparable to the models with a single attention layer [7, 24] but

we outperform the latter models on the datasets tested.

4 EXPERIMENTAL SETUP
4.1 Datasets
To evaluate the proposed approaches, we conduct an empirical evaluation based on three popular and well-studied benchmark datasets for both factoid and non-factoid question answering. In addition, we use another newly released dataset for the financial domain. These four datasets cover different domains and exhibit different characteristics:
· TREC-QA - This is a benchmark dataset created by Wang et al. [30] based on Text REtrieval Conference (TREC) QA track (8-13) data. The dataset contains a set of factoid questions, where candidate answers are limited to a single sentence. To enable direct comparison with the previous work, we follow the approach of train/dev/test questions selection from [28], in which all questions with only positive or negative answers are removed. In total, we have 1162 training questions, 65 development questions and 68 test questions. The maximum number of tokens for questions and answers are set to 11 and 60, respectively, the length of the vocabulary |V |=55060 and for each question there are 38 candidate answers on average.
· WikiQA - This is a recent popular benchmark dataset for open-domain question answering, based on factual questions from Wikipedia and Bing search logs. For each question, Yang et al. [31] selected Wikipedia pages and used sentences in the summary paragraph as candidates, which are then annotated on a crowdsourcing platform. We follow the same

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA
Nam Khanh Tran, Claudia Niederée
preprocessing steps as Yang et al., where questions with no correct candidate answers are excluded and answer sentences are truncated to 40 tokens. In total, we end up with 873 training questions, 126 development questions and 243 test questions. Since there are only few negative answers for each question in WikiQA, we extend it by randomly selecting a bunch of negative candidates from the answer pool. · InsuranceQA - This is a recently released large-scale nonfactoid QA dataset from the insurance domain created by Feng et al. [8]. In this work we use the first version of the dataset. The dataset is already divided into a training set, a validation set, and two test sets, in which a question may have multiple correct answers and normally the questions are much shorter than the answers. The average length of questions and answers in tokens are 7 and 95, respectively. Such difference imposes additional challenges for the AS task. For each question in the development and test sets, there is a set of 500 candidate answers, which include the ground-truth answers and randomly selected negative answers. · FiQA - This is a new non-factoid QA dataset from the financial domain which has been recently released for WWW 2018 Challenges.1 The dataset is built by crawling Stackexchange, Reddit and StockTwits in which part of the questions are opinionated, targeting mined opinions and their respective entities, aspects, sentiment polarity and opinion holder. We minimally preprocess the data only performing tokenization and lowercasing all words. To reduce the size of resulting vocabulary, we remove all rare words which occur less than 5 times. In this dataset questions and answers are longer than in other datasets, which will consequently bring extra challenges. The maximum number of tokens for questions and answers are set to 20 and 150 respectively. Following the setup for other datasets, we split this dataset into training, development and test sets as shown in Table 2. For each question in the development and test sets, we construct the answer pools by including the correct answer(s) and randomly selected candidates from the complete set of unique answers, as similar to InsuranceQA.
Table 2 presents some statistics about the datasets, including the number of questions in each set, average length of questions and answers as well as average number of candidate answers in the development and test sets.
4.2 Employed Baselines
In this section, we introduce the baselines used for comparison. For all datasets, we report performances of the basic matching model QA-LSTM and the models with a single attention layer described in Section 3.3 including MLP-LSTM, Bilinear-LSTM, Self-LSTM and Sequential-LSTM. Furthermore, we also introduce other baselines for each dataset separately.
· TREC-QA - The key competitors of this dataset are the CNN model of Severyn and Moschitti [22], the Attention-based Neural Matching model [7, 24] and the RNN with Positional Attention proposed by Chen et al. [5]. In addition, due to the
1 https://sites.google.com/view/fiqa/home

330

Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Table 2: The statistics of the four employed answer selection datasets. For WikiQA and TREC-QA we remove all questions that have no right or wrong answers.

Dataset

TREC-QA WikiQA

InsuranceQA

FiQA

# of questions (train/dev/test) Avg length of questions Avg length of answers Avg # of candidate answers

1162/65/68 8 28 38

873/126/243 6 25 9

12887/1000/1800x2 7 95 500

5999/323/324 11 135 500

long standing nature of this dataset, we also report works based on traditional feature engineering approaches [10, 30]. · WikiQA - The competitors of this dataset include the Paragraph Vector (PV) and PV + Cnt models [33], CNN + Cnt model [35] which are reported in the original WikiQA paper [31]. Furthermore, we report additional strong baselines including AP-CNN and AP-LSTM [7], ABCNN [34] and RNNPOA [5]. We also report the Pairwise Ranking MP-CNN model [18]. · InsuranceQA - The key competitors of this dataset are the CNN-based ARC-I/II architecture by Feng et al. [8], QALSTM from [24] along with AP-LSTM which are attentive pooling improvements of the former and Inner attentionbased RNN [27]. · FiQA - For this dataset, we reimplemented QA-LSTM [24] and different attention mechanisms on top of QA-LSTM for comparison.
We denote our proposed models as Multihop-MLP-LSTM, MultihopBilinear-LSTM, Multihop-Sequential-LSTM and Multihop-Self-LSTM which are MANs based on additive attention, bilinear attention, sequential attention and self-attention, respectively, for learning answers' attentive representations.
4.3 Hyperparameters and Training
This section describes the key evaluation protocol and metrics as well as implementation details of our experiments.
4.3.1 Evaluation Metrics. For the evaluation protocols we follow the prior work. Specifically, in TREC-QA and WikiQA we use the Mean Reciprocal Rank (MRR) and Mean Average Precision (MAP) metrics which are commonplace in IR and QA research. On the other hand, InsuranceQA and FiQA evaluate on Precision@1 (P@1) which is determined based on whether the top predicted answer is the ground truth. For all competitor methods, we report the performance results from the original paper.
4.3.2 Implementation Details and Hyperparameters. The models are implemented in Pytorch. The model parameters are optimized using Adam [1] optimizer with a learning rate of 0.001. A batch size of 100 are used for all datasets. The parameters are regularized with a per-minibatch L2 regularization strength of 10-5 and a dropout of d = 0.3 is also applied to prevent overfitting. The hidden layer size of LSTM models are the same as in previous works for a fair comparison. Specifically, for TREC-QA and InsuranceQA the sizes are set to 300 and 141 respectively as in [24]; the number for WikiQA is 141 as in [7]. For FiQA, we tried different numbers and found out that the size of 512 yields the best results. We tried different

margins M in the hinge loss function and finally fixed the margin to M = 0.2. A number of negative answers N = 50 was used during training. The number of attention steps K is tuned amongst {1, 2, 3} and we also experimented with the set of three vectors by using last, max and average pooling as different representations for the questions. We initialized the word embeddings with 300-dimensional Glove vectors [17] trained on 840 billion words. Embeddings for words not present in the Glove vectors are randomly initialized with each component sampled from the uniform distribution over [-0.25, 0.25]. The word embeddings are also part of the parameters and are optimized during training. Since sequences within a mini-batch have different lengths, we use a mask matrix to indicate the real length of each sequence. We trained all models for a maximum of 40 epochs. We take MAP scores for TREC-QA and WikiQA and P@1 scores for InsuranceQA and FiQA on the development set at every epoch and save the parameters of the network for the top three models. We report the best test score from the saved models. All experiments were conducted on a Linux machine with Nvidia GTX Ti 1080 GPU (12GB RAM). The code to reproduce the reported results and FiQA splits are publicly available at https://github.com/namkhanhtran/nn4nqa.
5 EXPERIMENTAL RESULTS
In this section, we present our empirical results on all datasets. For all reported results the best result is in boldface.
5.1 TREC-QA
Our results on TREC-QA dataset is summarized in Table 3. Firstly, we observe that all attention-based models outperform the basic matching model QA-LSTM by large margins. Second, the model based on sequential attention mechanism obtains a clear performance gain of around 3% on MAP/MRR against the model with additive or multiplicative attention mechanism. Compared to these models, Self-LSTM achieves comparable results though it does not use any query information for extracting answer representation. It also performs better than QA-LSTM, which indicates that selfattention mechanism can give better representations than simply max or average pooling method.
Furthermore, Table 3 shows that MANs outperform all models with only one attention layer. When using the same attention mechanism, the averages increase over the baselines with a single attention layer are 2% ­ 3% in terms of MAP/MRR. Specifically, Multihop-Sequential-LSTM gains an improvement of 1.6% on MAP and 2.8% on MRR compared to Sequential-LSTM. Similarly, Multihop-Bilinear-LSTM obtains 3.3% and 3.2% improvements in terms of MAP and MRR respectively. Multihop-MLP-LSTM also

331

Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Table 3: Experimental results on TREC-QA. Baselines for TREC-QA are reported in the first group. The second group shows the performance of models with a single attention layer. We report the performance of MANs in the last group.

Model

MAP MRR

Wang et al. [30] Heilman & Smith [10] Wang & Nyberg [28] CNN (Severyn & Moschitti) [22] AP-LSTM (Tan et al.) [24] AP-CNN (Santos et al.) [7] RNN-POA (Chen et al. [5])

0.603 0.609 0.713 0.746 0.753 0.753 0.781

0.685 0.692 0.791 0.808 0.830 0.851 0.851

QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM

0.737 0.764 0.755 0.759 0.797

0.810 0.839 0.832 0.830 0.865

Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM

0.768 0.788 0.771 0.813

0.849 0.864 0.864 0.893

shows some degree of improvement compared to MLP-LSTM. Based on self-attention mechanism, MAN outperforms the model with one attention layer by 1.2% on MAP and 3.4% on MRR. Overall, Multihop-Sequential-LSTM obtains the best results on TREC-QA dataset and surpasses the strong baseline RNN-POA [5] by 3.2% on MAP and 4.2% on MRR.
5.2 WikiQA
Table 4 reports the experimental results on WikiQA. First, we observe that MAN-based models outperform the models with a single attention layer. Multihop-Sequential-LSTM outperforms SequentialLSTM by 2% in terms of MAP and 2.3% in terms of MRR. MultihopBilinear-LSTM shows improvements of 3.8% on MAP and 3.9% on MRR compared to Bilinear-LSTM. Multihop-MLP-LSTM and Multihop-Self-LSTM also perform slightly better than MLP-LSTM and Self-LSTM, respectively. Overall, Multihop-Sequential-LSTM achieves the best results on WikiQA and shows some degree of improvement compared to the strongest baseline RNN-POA [5].
5.3 InsuranceQA
Table 5 reports the experimental results on InsuranceQA. Our proposed approaches achieve highly competitive performances on this dataset, where Multihop-Sequential-LSTM obtains the best P@1 performance overall. Our best model surpasses the strong baseline IARNN-Gate on both test sets. Although most MAN-based models show some degree of improvement compared to the models with a single attention layer, applying one step of attention seems to be sufficient on this dataset. Interestingly, Self-LSTM performs quite well on InsuranceQA dataset even outperforms some interactive attention-based models. Multihop-Self-LSTM does not show any improvement against Self-LSTM. In addition, Sequential-LSTM

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Nam Khanh Tran, Claudia Niederée

Table 4: Experimental results on WikiQA. Baselines for WikiQA are reported in the first group. The second group shows the performance of models with a single attention layer. We report the performance of MANs in the last group.

Model

MAP MRR

PV PV + Cnt CNN + Cnt AP-LSTM (Santos et al. [7]) AP-CNN (Santos et al. [7]) ABCNN (Yin et al. [34]) Rank MP-CNN (Rao et al. [18]) RNN-POA (Chen et al. [5])

0.511 0.599 0.652 0.670 0.689 0.692 0.701 0.721

0.516 0.609 0.665 0.684 0.696 0.710 0.718 0.731

QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM

0.654 0.686 0.677 0.693 0.702

0.665 0.695 0.686 0.704 0.715

Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM

0.703 0.715 0.702 0.722

0.712 0.725 0.710 0.738

Table 5: Experimental results on InsuranceQA. Baselines for InsuranceQA are reported in the first group. The second group shows the performance of models with a single attention layer. We report the performance of MANs in the last group.

Model

Test1 Test2

CNN (Feng et al. [8]) CNN with GESD (Feng et al. [8]) AP-LSTM (Tan et al. [24]) IARNN-Gate (Wang et al [27])

0.628 0.653 0.690 0.701

0.592 0.610 0.648 0.628

QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM

0.643 0.693 0.689 0.699 0.702

0.617 0.648 0.658 0.653 0.665

Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM

0.695 0.694 0.682 0.705

0.655 0.662 0.648 0.669

again shows better results than MLP-LSTM and Bilinear-LSTM on this dataset.
5.4 FiQA
The results of the proposed models are shown in Table 6. On this new dataset we observe similar behaviours to other datasets. Firstly,

332

Session 3C: Question Answering Multihop Attention Networks for Question Answer Matching

Table 6: Experimental results on FiQA. The first group shows the performance of models with a single attention layer. We report the performance of MANs in the second group.

Model

MAP MRR P@1

QA-LSTM MLP-LSTM Bilinear-LSTM Self-LSTM Sequential-LSTM

0.433 0.497 0.492 0.493 0.504

0.566 0.616 0.606 0.608 0.621

0.469 0.509 0.506 0.509 0.522

Multihop-MLP-LSTM Multihop-Bilinear-LSTM Multihop-Self-LSTM Multihop-Sequential-LSTM

0.498 0.507 0.488 0.529

0.613 0.631 0.619 0.655

0.519 0.534 0.522 0.567

Table 7: Effect of different number of attention steps on FiQA

Model

MAP MRR P@1

Sequential-LSTM

0.504 0.621 0.522

Multihop-Sequential-LSTM* Multihop-Sequential-LSTM (K=1) Multihop-Sequential-LSTM (K=2) Multihop-Sequential-LSTM (K=3)

0.514 0.527 0.529 0.523

0.636 0.649 0.655 0.644

0.543 0.561 0.567 0.546

attention-based models outperform the basic matching model QALSTM. MAN-based models perform better than the models with a single attention layer, in which Multihop-Sequential-LSTM obtains the best performance overall. More specifically, MultihopSequential-LSTM improves Sequential-LSTM by 4.5% in terms of P@1 while Multihop-Bilinear-LSTM shows an improvement of 2.8% on P@1 against Bilinear-LSTM. Multihop-MLP-LSTM indicates 1% enhancement on P@1 compared to MLP-LSTM whereas Multihop-Self-LSTM also increases over Self-LSTM by 1.3% in terms of P@1. Amongst interactive attention-based models SequentialLSTM outperforms MLP-LSTM and Bilinear-LSTM. Compared to these models, Self-LSTM shows highly competitive results.
Overall, we summarize the key findings of our experiments.
· Similar to previous work, we observe that attention-based models perform significantly better than basic matching models.
· Multihop Attention Networks are better in capturing the complex semantic relations between questions and answers and outperform the models with only one attention layer.
· Sequential attention can be well adopted for the AS task and gains considerably improvements compared to traditional attention mechanisms.
· Self-attention can produce better representations than simply max or average pooling method and obtain competitive results on the AS task.

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
5.5 Effect of Attention Steps
Table 7 shows the influence of the number of steps on performance on FiQA dataset. Multihop-Sequential-LSTM* is the model where we consider the vectors returned by using max, mean and last pooling as different representations for the questions. Overall, MultihopSequential-LSTM performs better than Sequential-LSTM in which with K = 2 Multihop-Sequential-LSTM obtains the best performance. This might be due to the fact that the questions are rather short and often express two different aspects at most.
5.6 A Case Study
Figure 3 depicts the heat map of a test question from FiQA that was correctly answered by Multihop-Sequential-LSTM. The stronger the color of a word in the question (answer), the larger the attention weight of that word. As can be seen in the figure, in the first step Multihop-Sequential-LSTM puts more focus on some segments of the question and the parts of the answer that have some interactions with the question segments. In the second step, the MAN gives more attention to other segments of the question and consequently some other parts of the answer get more attention.
6 CONCLUSIONS
In this paper, we have presented Multihop Attention Networks for question answer selection. Our proposed MANs use multiple vectors which focus on different parts of a question to represent the overall semantics of the question and then apply multiple steps of attention to learn representations for the candidate answers. In addition, we also showed that sequential attention mechanism can be well adapted for this task. The mechanism allows local alignment information to be used when computing attention weight for each token in a sequence. Experimental results showed that MANs outperform state-of-the-art approaches on popular benchmark QA datasets. Empirical studies also confirm the effectiveness of sequential attention over other attention mechanisms. For future work, we want to employ this architecture on other tasks such as natural language inference, reading comprehension. In addition, combining multihop attention with convolutional neural networks is another direction to explore.
ACKNOWLEDGMENTS
We would like to thank Tuan-Anh Hoang and anonymous reviewers for their helpful comments. This work was partially funded by the BMBF project eLabour (01UG1512C), and the DFG project Managed Forgetting (NI-1760/1-1).
REFERENCES
[1] Jimmy Ba and Diederik Kingma. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of International Conference of Learning Representations.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of International Conference of Learning Representations.
[3] Sebastian Brarda, Philip Yeres, and Samuel R. Bowman. 2017. Sequential Attention: A Context-Aware Alignment Function for Machine Reading. In Proceedings of the 2nd Workshop on Representation Learning for NLP. 75­80.
[4] Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2358­2367.

333

Session 3C: Question Answering SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Nam Khanh Tran, Claudia Niederée

Figure 3: Attention heat map from Multihop-Sequential-LSTM (K=2) for a correctly selected answer.

[5] Qin Chen, Qinmin Hu, Jimmy Xiangji Huang, Liang He, and Weijie An. 2017. Enhancing Recurrent Neural Networks with Positional Attention for Question Answering. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. 993­996.
[6] Sumit Chopra, Raia Hadsell, and Yann LeCun. 2005. Learning a Similarity Metric Discriminatively, with Application to Face Verification. In Proceedings of the 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Volume 1 - Volume 01. 539­546.
[7] Cicero dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. In CoRR, abs/1602.03609.
[8] Minwei Feng, Bing Xiang, Michael R. Glass, Lidan Wang, and Bowen Zhou. 2015. Applying deep learning to answer selection: A study and an open task. In Workshop on Automatic Speech Recognition and Understanding. 813­820.
[9] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. 1011­1019.
[10] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. 1011­1019.
[11] Karl Moritz Hermann, Tomás Kociský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching Machines to Read and Comprehend. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1. 1693­1701.
[12] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. (1997), 1735­1780.
[13] Minghao Hu, Yuxing Peng, and Xipeng Qiu. 2017. Reinforced Mnemonic Reader for Machine Comprehension. arXiv preprint arXiv:1705.02798 (2017).
[14] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. 2016. Ask Me Anything: Dynamic Memory Networks for Natural Language Processing. In Proceedings of The 33rd International Conference on Machine Learning. 1378­1387.
[15] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-Attentive Sentence Embedding. In International Conference on Learning Representations 2017 (Conference Track).
[16] Yang Liu, Chengjie Sun, Lei Lin, and Xiaolong Wang. 2016. Learning Natural Language Inference using Bidirectional LSTM model and Inner-Attention. CoRR (2016).
[17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Natural Language Processing (EMNLP). 1532­1543.
[18] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. 1913­ 1916.
[19] Tim Rocktäschel, Edward Grefenstette, Karl Moritz Hermann, Tomás Kociský, and Phil Blunsom. 2015. Reasoning about Entailment with Neural Attention. In arXiv preprint arXiv:1509.06664.
[20] Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A Neural Attention Model for Abstractive Sentence Summarization. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 379­389.

[21] Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic Feature Engineering for Answer Selection and Extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. 458­467.
[22] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. 373­382.
[23] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2. 3104­3112.
[24] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2016. Improved Representation Learning for Question Answer Matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 464­473.
[25] Ming Tan, Bing Xiang, and Bowen Zhou. 2015. LSTM-based Deep Learning Models for non-factoid answer selection. CoRR abs/1511.04108 (2015).
[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems 30. 6000­6010.
[27] Bingning Wang, Kang Liu, and Jun Zhao. 2016. Inner Attention based Recurrent Neural Networks for Answer Selection. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1288­ 1297.
[28] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 707­ 712.
[29] Mengqiu Wang and Christopher Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010). 1164­1172.
[30] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). 22­32.
[31] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2013­2018.
[32] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 858­867.
[33] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. Question Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1744­1753.
[34] Wenpeng Yin, Hinrich Schutze, Bing Xiang, and Bowen Zhou. 2016. ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs. Transactions of the Association for Computational Linguistics (2016), 259­272.
[35] Lei Yu, Karl M. Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep learning for answer sentence selection. In NIPS Deep Learning Workshop.

334

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Ranking Robustness Under Adversarial Document Manipulations

Gregory Goren
gregory.goren@campus.techion.ac.il Technion -- Israel Institute of Technology
Moshe Tennenholtz
moshet@ie.technion.ac.il Technion -- Israel Institute of Technology
ABSTRACT
For many queries in the Web retrieval setting there is an on-going ranking competition: authors manipulate their documents so as to promote them in rankings. Such competitions can have unwarranted effects not only in terms of retrieval effectiveness, but also in terms of ranking robustness. A case in point, rankings can (rapidly) change due to small indiscernible perturbations of documents. While there has been a recent growing interest in analyzing the robustness of classifiers to adversarial manipulations, there has not yet been a study of the robustness of relevance-ranking functions. We address this challenge by formally analyzing different definitions and aspects of the robustness of learning-to-rank-based ranking functions. For example, we formally show that increased regularization of linear ranking functions increases ranking robustness. This finding leads us to conjecture that decreased variance of any ranking function results in increased robustness. We propose several measures for quantifying ranking robustness and use them to analyze ranking competitions between documents' authors. The empirical findings support our formal analysis and conjecture for both RankSVM and LambdaMART.
ACM Reference Format: Gregory Goren, Oren Kurland, Moshe Tennenholtz, and Fiana Raiber. 2018. Ranking Robustness Under Adversarial Document Manipulations. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8-12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210012
1 INTRODUCTION
In adversarial retrieval settings (e.g., the Web) there is an on-going ranking competition for many queries: authors of some Web documents manipulate them so as to have them ranked high. The ranking competition can have various undesirable effects. First, ranking effectiveness can degrade due to adversarial changes of documents that result in having them ranked higher than they
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210012

Oren Kurland
kurland@ie.technion.ac.il Technion -- Israel Institute of Technology
Fiana Raiber
fiana@oath.com Yahoo Research
should; i.e., black-hat search engine optimization (SEO) [11]. Furthermore, rankings can potentially (rapidly) change due to small document perturbations that might be indiscernible.
Motivated by the ranking competitions that take place in the adversarial Web retrieval setting, and the growing body of work on robust classification -- specifically, with respect to adversarial manipulations [4, 6­10, 14, 20, 21, 25], we present the first (to the best of our knowledge) theoretical and empirical study of the robustness of document relevance-ranking functions to document manipulations. Our focus is learning-to-rank-based functions [13] where a document-query pair is represented as a feature vector.
We start by adapting a basic classifier-robustness notion used in recent work on robust classification [7, 20] to the case of a document ranking function. We formally analyze implications of applying this notion and highlight a notable drawback: the treatment of documents independently of each other -- i.e., this is a pointwise robustness perspective. However, ranking depends on the relative retrieval scores of documents. Hence, we formulate a definition of pairwise robustness that addresses the effect of small document changes on the relative ranking of pairs of documents. We formally analyze the implications of applying our pairwise robustness definition and the connections with pointwise robustness.
The different definitions of robustness that we propose are based on a worst-case scenario; namely, quantifying the minimal document change needed to change a ranking. Using the definitions to compare the robustness of different ranking functions can be quite difficult. Thus, we explore an additional aspect of robustness which we term stability (cf. [20]): changes of retrieval scores and relative ranking with respect to a given fixed change of a document. We then establish formal connections for linear (in features) ranking functions between the extent of their regularization and their stability. Motivated by these formal findings, we state a variance conjecture: the higher the variance of a learned ranking function, the less robust the rankings it induces. A ranking is considered robust if it does not significantly change due to small documents' changes. We propose a few methods of measuring ranking robustness.
While our motivation is to address documents' changes introduced by incentivized authors, our formal analysis makes no assumptions on the cause and nature of documents' changes. Thus, the analysis constitutes a general treatment of ranking robustness under document manipulations. Since, in practice, documents' changes in competitive retrieval settings are often incentivized (adversarial), we use for evaluation a recently published dataset of document ranking competitions held between students [17]. The

395

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

mere motivation to change documents in the competitions was to have them ranked high as possible.
The analysis of the ranking competitions provides support to the formal analysis and the variance conjecture. First, increased regularization of a linear ranking function (RankSVM [12]), which leads to reduced variance, results in improved ranking robustness. Second, increased regularization of a non-linear ranking function, namely, the state-of-the-art LambdaMART method [27], also results in improved ranking robustness. Third, LambdaMART induces rankings that are less robust than those induced by RankSVM; the former has higher variance than the latter.
Our contributions can be summarized as follows:
· We present the first formal and empirical analysis of the robustness of learning-to-rank-based relevance ranking functions to (adversarial) manipulations of documents.
· We formally demonstrate, and provide empirical support to, the connection between regularization of linear learning-to-rank functions and ranking robustness.
· Motivated by our formal findings, we post a conjecture about the connection between the variance of a ranking function and ranking robustness, and provide empirical support.
2 RANKING ROBUSTNESS
We assume that the following have been fixed: a query q, a document corpus D, and a document relevance ranking function f . As is the case for many learning-to-rank-based retrieval methods [13], the ranking function takes as input a feature vector d  m that represents the pair of document d ( D) and the query1 q; d[i] is the i'th component of d. Features can model query-document relations, query-independent properties of documents and documentindependent properties of the query [13]. The output of the ranking function (wlog) is a retrieval score f (d)  + used to induce a ranking over the corpus. We assume that ties of retrieval scores are consistently broken (e.g., using documents' IDs). We use L2 norms of vectors. For a linear (in features) ranking function f , f (d) d=ef w d + b; w is the weight vector and b is the intercept.
Our goal is to define and quantify notions of the robustness of ranking functions f to small perturbations of documents. Rather than directly address documents' changes (e.g., with respect to content, hyperlink, hypertext, etc.), we study the effects of the resultant perturbations of the corresponding feature vectors.
We first motivate our analysis of ranking robustness in Section 2.1. Then, in Section 2.2 we adapt a recently used classifierrobustness definition [6, 20] to ranking functions. The definition addresses documents independently. Since ranking is determined by the relative retrieval scores of documents, we study the notion of pairwise document robustness in Section 2.3; that is, we examine effects of documents' perturbations on the relative ranking of two documents. Given our formal findings in Section 2.3, we post a conjecture about the connection between ranking robustness and the variance of a ranking function in Section 2.4. Finally, in Section 2.5 we present a few methods to measure ranking robustness.
1We omit q from the feature-vector notation as it is fixed throughout the section.

2.1 Motivation
Much of the practical motivation for the recent line of work on robustness of classifiers comes from the vision realm [7, 8, 10, 20, 25]. The assumption is that small perturbations of images not discernible by humans should not result in changes to classification decisions.
We make a similar assumption about the ranking of documents. That is, a ranking should not (significantly) change as a result of small, potentially indiscernible, perturbations of documents2. A case in point, users might suspect the validity of rankings given rapid changes that are hard to explain (a.k.a., "explainable IR").
To further support our premise, we appeal to the cluster hypothesis which states that "closely associated documents tend to be relevant to the same requests" [22]. An important operational manifestation of the hypothesis is the premise that "similar documents should receive similar retrieval scores" [5]. That is, by the cluster hypothesis, similar documents would be relevant to the same requests (i.e., queries). Therefore, their retrieval scores which reflect relevance status should not be very different. In other words, small perturbations of documents should not result in significant changes of retrieval scores, and hence, significant ranking changes3.
Accordingly, below we focus on the effects of documents' perturbations on induced rankings; i.e., ranking robustness. The formal treatment of the effects on document relevance, and consequently ranking effectiveness, is an intriguing research venue at its own right which we leave for future work.
2.2 Pointwise Robustness
The recently adopted notion of classifier robustness was defined based on the minimal change of an object required to change the class to which the object is classified [7, 20]. We conceptually adapt this robustness notion to the case of retrieval scores. We term this robustness "pointwise" as documents are considered independently -- i.e., the effects of the change of a document retrieval score on its relative ranking with respect to other documents are not considered. To simplify the following definitions and analysis, and without loss of generality, we focus, unless otherwise stated, on the increase of a retrieval score rather than a change which could be negative.
Definition 1 (Pointwise Robustness). The pointwise robustness of ranking function f with respect to document d and query q is: point ( f ; d, q) d=ef minv m ||v|| s.t. f (d + v) > f (d). The robustness with respect to q is the expectation over all documents d ( D) sampled using some distribution P(D): point ( f ; q) d=ef EdP(D) point ( f ; d, q).
In other words, the pointwise robustness with respect to document d is the minimal extent of a change (i.e., norm of a vector) needed to be applied to d so as to increase d's retrieval score.
We now analyze the pointwise robustness of linear ranking functions: f (d) d=ef w d + b. RankSVM [12] and coordinate ascent [15] are examples of commonly used, effective linear ranking functions.
2In contrast to spamming, white-hat SEO attempts to promote documents in rankings often do not result in a significant drift with respect to the original document [17]. 3The cluster hypothesis was originally stated with respect to textual content of documents [22]. It could be that the document content does not change but its feature-vector representation does -- e.g., due to changes of hyperlinks and anchor text.

396

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Proposition 1. The pointwise robustness of a linear ranking function is less or equal to  for any  > 0.

Proof. Let d be some document. Then, f (d + v) > f (d) 

wv > 0. Suppose wlog that w [i] > 0. We define a vector v such that

v[j] = 0 for all j i and v[i] = . Thus, we get that wv > 0 and

point ( f ; d, q)  ||v|| = . Consequently, point ( f ; q)  .



There are various ranking functions whose robustness can be substantially higher than 0. For example, in gradient boosted regression trees which are the basis of the state-of-the-art LambdaMART ranking function [27], not every change of a document's feature vector necessarily results in a retrieval score change.

2.2.1 Pointwise stability. Definition 1 refers to the worst case scenario per document d. That is, the minimal change to d required so as to increase d's retrieval score. Indeed, as we showed above, every linear ranking function has, under this definition, robustness
that approaches 0. However, linear functions differ by the extent to
which a retrieval score changes with respect to a given magnitude ||v|| of a document change v. Therefore, pointwise robustness, as defined in Definition 1, cannot be used to quantify this specific
aspect of robustness, and we turn to the following definition:

Definition 2 (Pointwise Stability). A ranking function f is pointwise stable at level K ( +) with respect to a document change v if d, | f (d + v) - f (d)|  K ||v||. The lower K, f becomes more pointwise stable with respect to v.4
Contractive (Lipschitz) ranking functions f are pointwise stable at level K where K is the Lipschitz coefficient. Various ranking functions are contractive. For example, some neural-network architectures were shown to be contractive [20]5. Linear ranking functions are also contractive. Specifically, for f (d) d=ef w d + b we get, using the Cauchy-Schwarz inequality:
| f (d + v) - f (d)|  ||w ||||v||.
Hence, the Lipshitz bound K  ||w ||, and f is pointwise stable at level K  ||w ||. This observation has an important implication. In linear ranking functions, such as RankSVM, the loss function used for training is often regularized by adding ||w || where  is the parameter that controls the extent of regularization. Higher value of  results in decreased ||w || (i.e., stronger regularization) as the goal is to minimize the objective function (loss+regularization). We
thus arrive to:

Corollary 1. Ceteris paribus, stronger regularization of linear ranking functions increases pointwise stability.
We note that when using L2 norms, decreasing the norm of w (i.e., increasing regularization) is intended to improve generalization and prevent overfitting; in other words, stronger regularization increases the bias of the ranking function and decreases its variance. We re-visit this point below.
The pointwise analysis treats documents independently of each other. However, ranking is determined by the relative retrieval scores of documents. Specifically, a change to a retrieval score, as

4
Obviously,

a

function

stable

at

a

level

K

is

also

stable

at

any

level

>

K.

5Some classification architectures were shown to be contractive [20]. It can be shown

that using these architectures for regression (ranking) also yields a contractive function.

large as it may be, need not necessarily affect ranking. Hence, we now turn to address the robustness of ranking functions in terms of the relative ranking of pairs of documents.

2.3 Pairwise Robustness
Definition 3 (Pairwise Robustness). Let d1 and d2 be two documents such that f (d1)  f (d2). The pairwise robustness of f with respect to d1,d2 and the query q is: pair ( f ; d1, d2, q) d=ef minv m ||v|| s.t. f (d2 + v) > f (d1). The pairwise robustness of f with respect to the query is the expectation over document pairs: pair ( f ; q) d=ef Ed1P(D),d2P(D\{d:f (d )>f (d1) }) pair ( f ; d1, d2, q).
Pairwise robustness generalizes pointwise robustness: setting
d1 = d2 in Definition 3 results in the pointwise robustness definition
from Definition 1. Accordingly, it directly follows that:

Proposition 2. Pairwise robustness entails pointwise robustness but the reverse does not hold. That is, pair ( f ; q)  point ( f ; q).

In other words, the minimal document change (on average)
needed to change the relative ranking of a document with respect
to another document is higher than the minimal change needed
to have the document's retrieval score increase. This trivial ob-
servation touches on the fundamental difference between treating
documents independently and accounting for their relations.
Specifically, an important difference between pointwise and pair-
wise robustness is the potential dependence of the latter on dis-
tances between documents' feature vectors. For example, we now show that for a linear ranking function f (d) d=ef w d + b, the smallest change of d2 required to increase its retrieval score beyond that of d1 approaches its distance from d1. Furthermore, we show that for any linear ranking function, there exists a pair of documents' feature vectors for which the pairwise robustness of f is not smaller than the distance between d1 and d2.

Proposition 3. Let f be a linear ranking function. Then, d1, d2 s.t. d1 d2 and f (d1)  f (d2) and  > 0, pair ( f ; d1, d2, q)  ||d1 - d2|| +  . And, d1, d2 s.t. pair ( f ; d1, d2, q) > ||d1 - d2||.

Proof. Suppose wlog that w [i] > 0. Fix  > 0 and some d1 and d2 ( d1) s.t. f (d1)  f (d2). Let v = d1 - d2 +  where [i] =  and [j] = 0 for j i. Then, f (d2 + v) - f (d1) > 0 iff w [i] > 0. Since pair ( f ; d1, d2, q)  ||v|| and by the triangle inequality ||v||  ||d1 -d2|| +, we get that pair ( f ; d1, d2, q)  ||d1 -d2|| +.

We now turn to prove the second part of the proposition. Let

d1 d=ef -w and d2 d=ef -2w ; thus, f (d1) > f (d2). Furthermore, v:

f (d2 + v) - f (d1) > 0 iff vw > ||w ||2. By the Cauchy-Schwarz

inequality, vw  ||v||||w ||. Thus, f (d2 + v) - f (d1) > 0  ||v|| >

||w || = ||d1 - d2||. Thus, pair ( f ; d1, d2, q) > ||d1 - d2||.



Additional question we are interested in is the connection be-
tween pointwise stability of retrieval scores and pairwise robustness6, specifically, for linear functions: f (d) d=ef w d + b.

6Recall that we showed above that the pointwise robustness of linear ranking functions approaches 0; however, their pointwise stability varies across different functions.

397

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Proposition 4. Let f be a linear ranking function. There exist d1 and d2 s.t. pair ( f ; d1, d2, q) > Kmin where Kmin is the minimal level at which f is pointwise stable with respect to any v.

Proof. Let d1 = 2w and d2 = w . Note that f (d1) > f (d2). For

any v, f (d2 + v) > f (d1) iff wv > ||w ||2. Thus, by the Cauchy-

Schwarz inequality, ||w ||2 < wv  ||w ||||v||. Consequently,

pair ( f ; d1, d2, q)  ||v|| > ||w ||. As shown in Section 2.2, for

linear ranking functions | f (d+v) - f (d)|  ||w ||||v|| and therefore

Kmin  ||w || < pair ( f ; d1, d2, q).



Proposition 4 might seem counter intuitive at first glance. That is, the higher Kmin , which means reduced stability, the higher pair ( f ; d1, d2, q) can be -- the pairwise robustness for some pair of documents (or more precisely, their feature vectors). However, recall that Kmin is determined with respect to all possible v. Thus, it suffices that there is some v whose addition to some d increases substantially d's retrieval score and hence Kmin . The effect on pair ( f ; d1, d2, q) can only be an increase or no change.
There are two implications of these observations. First, pointwise
stability and pairwise robustness are, in general, complementary
properties of a ranking function. Second, pointwise stability should
mainly be used to contrast different ranking functions with respect to the same document change v.

2.3.1 Pairwise stability. Definition 3, as was the case for Definition 1, refers to the worst case scenario: the minimal extent of a change that can be introduced to document d2 which is ranked lower than d1 so as to have it ranked higher than d1. However, this robustness definition does not allow to compare the effect of a
given fixed change on rankings induced by different rankers. To address this task, we observe the following. Let d1 and d2 be
two documents. Their relative ranking is determined by sin( f (d2)- f (d1)). Thus, to estimate whether the relative ranking of the two documents changes after d2 was changed by v, we can examine
(d1, d2; v) d=ef | f (d2 + v) - f (d1) - f (d2) - f (d1) |.
The lower (d1, d2; v) the higher the likelihood that the score difference after the document change has the same sign as that before the
change; that is, the higher the likelihood that the relative ranking would not change. Now, (d1, d2; v) = | f (d2 + v) - f (d2)|. Thus, we are led to the following definition and consequence:
Definition 4 (Pairwise Stability). A ranking function f is pairwise stable at level K ( +) with respect to a document change v if d1 d2, (d1, d2; v)  K ||v||.
And, we got that f is pairwise stable at level K iff it is pointwise stable at level K; i.e., d, | f (d + v) - f (d)|  K ||v||.
Note that we essentially used f to classify pairs of documents. This practice is also the basis of RankSVM [12]: a ranking func-
tion is learned by using it as a classifier upon pairs of documents.
We argued that classification decisions are stable if the difference
between retrieval scores after a document has changed is close
to the difference before the change. A similar argument was used
in recent work on estimating classifier stability with respect to a
single object that has changed [20].

At the technical level, the pair-classification practice we have taken resulted in backing off from addressing both documents involved to only the one which has changed. Specifically, pairwise stability at a level K is attained with respect to a given change v for all document pairs iff the ranking function is pointwise stable at a level K with respect to v; i.e., the same change introduced to any document is not likely to result in a rank swap of this document with another document if the change of the document's retrieval score is not large. It is therefore important to highlight an additional difference between the definitions of pairwise robustness and stability. Robustness is computed as the worst case scenario for a given pair of documents and is aggregated over all document pairs via the expectation. Stability level is a constraint imposed on all document pairs. Accounting for all specific pairwise relations to derive a bound on stability level is a highly difficult task.
In Section 2.2.1 we showed that for a linear ranking function f , | f (d + v) - f (d)|  ||w ||||v||. Since (d1, d2; v) = | f (d2 + v) - f (d2)| we get that f is pairwise stable at a level K  ||w ||. Recall that the stronger the regularization of a linear function trained by minimizing loss + ||w ||, i.e., the higher the value of , the lower ||w ||. Hence, for a given small document change (i.e., small ||v||) we get that stronger regularization results in decreased likelihood of rank swaps. In other words:
Corollary 2. Ceteris paribus, stronger regularization of linear ranking functions results in more robust rankings; that is, the rankings are less likely to change as a result of changing documents.
Herein we use the terms "ranking robustness" and "ranking stability" interchangeably so as to refer to changes, or lack thereof, of a given ranking of a document list with respect to documents' changes; this is in contrast to the pointwise and pairwise analysis where we used the terms "robustness" and "stability" to refer to different notions. Measuring the robustness of a given ranking with respect to given documents' changes is a task we address in Section 2.5. Before delving into the details, we briefly discuss simultaneous changes of two documents. Then, in Section 2.4, we further discuss the connection between regularization and ranking robustness.
Simultaneous change of two documents. Heretofore, we discussed the pairwise case with respect to a change, v, of one of the two documents involved. Now, suppose that document d1 is changed by v1 and document d2 is simultaneously changed by v2. Similarly to the case above, we can examine
(d1, d2; v1, v2) d=ef | f (d2 + v2) - f (d1 + v1) - f (d2) - f (d1) |.
We can then adapt Definition 4 (pairwise stability) as follows:
Definition 5 (Simultaneous Pairwise Stability). A ranking function f is simultaneously pairwise stable at level K ( +) with respect to pairwise simultaneous changes v1 and v2 if d1 d2, (d1, d2; v1, v2)  K (||v1|| + ||v2||).
By the triangle inequality,
(d1, d2; v1, v2)  | f (d1 + v1) - f (d1)| + | f (d2 + v2) - f (d2)|.
Thus, if f is pointwise stable with respect to each of v1 and v2 at a level K, it is simultaneously pairwise stable at level K.

398

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

For a linear ranking function f (d) d=ef w d + b we get that:

(d1, d2; v1, v2) =

(1)

|wv2 - wv1|  ||w ||||v2 - v1||  ||w ||(||v1|| + ||v2||).

Thus, the function is simultaneously pairwise stable at level ||w ||. Hence, we arrive again to the conclusion that stronger regularization of linear ranking functions, which results in reduced ||w ||, yields rankings of increased robustness.

2.4 The Variance Conjecture

We established above the connection between stronger regulariza-

tion of linear ranking functions and increased ranking robustness.

Stronger regularization results in decreased variance and increased

bias of the ranking function.

For example, in RankSVM [12], the width of the margin of the sep-

arating hyperplane is

2
| |w | | . The lower

||w ||, as a result of stronger

regularization, the wider the margin. This results in higher bias and

lower variance of the learned ranking function. Indeed, larger mar-

gin means a more "robust/stable" decision surface, which resonates

with the fact that ranking robustness is higher. Accordingly, we

post the following variance conjecture for any ranking function:

Conjecture 1 (The variance conjecture). The lower the variance of a learned ranking function the more robust the rankings induced by the function7.

In Section 3 we provide some empirical support to the conjecture. Specifically, we show that reducing the variance of RankSVM, which is a linear ranker, and that of LambdaMART which is a non-linear ranker, results in increased ranking robustness.

2.5 Measuring Ranking Robustness
One of our goals is to empirically contrast the robustness of rank-
ings induced by different ranking functions. Suppose that L is a ranked document list retrieved using some
ranking function. Suppose that some of the authors of documents in L changed them (e.g., to have the documents ranked higher). After the changes, the ranking function is used to rank the documents in L again; the result is a ranked list L. The question is how to quantify the robustness of L with respect to the changes of documents it contains given that the resultant (re-)ranked list is L.
We can use any inter-ranking similarity or distance measure to quantify robustness. To compute these measures for L and L, we consider a document d before its change, and after its change, denoted d , as the same item.
Kendall's- distance (KT in short) is defined as the number of discordant pairs between two paired lists normalized with respect to the number of pairs of items in a list; its value is in [0, 1]. A
discordant pair is two items whose relative ranking in one list is
different than that in the other list. The higher the value, the higher the "distance" between the two lists8.

7Low variance corresponds to improved generalization of a ranker to unseen queries.

This is a different notion of robustness than that we focus on here.

8
In

contrast

to

Kendall's-

distance,

Kendall's-

coefficient

also

considers

concordant

pairs and its value is in [-1, 1].

KT treats equally swaps between documents at high ranks and

at low ranks. However, the former have higher effect on precision-

based retrieval effectiveness measures (e.g., average precision, p@k,

NDCG) than the latter. Thus, we also consider the inter-list similar-
ity measure RBO (rank-biased overlap) that differentiates swaps according to their ranks [26]. (We set the free parameter of RBO, p,

to 0.7.) Additional robustness measure we are interested in is "top
change" (TC): the value is 1 if the highest ranked document in L and L is different, and 0 otherwise.

All three measures just discussed do not consider the amount of

document change. However, changes in ranking due to small (po-

tentially indiscernible) document changes are a stronger evidence

for reduced robustness than those that result from large document

changes. Indeed, our pointwise (Definition 2), pairwise (Definition

4) and simultaneous pairwise (Definition 5) stability definitions cap-

ture this notion. Hence, we define normalized measures of ranking

robustness where normalization is with respect to the extent to

which two documents have changed. We normalize KT and TC;

normalizing RBO is more evolved and left for future work.

Let d1 and d2 be documents in L that might have changed to

d  and d  in L, respectively. Inspired by Definitions 4 and 5 and

1

2

Equation 1, we use the sum, difference and relative functions to

quantify changes to d1 and d2:

sum (d1, d2) d=ef

| |d
2

-

d2

||

+

| |d
1

-

d1 | |,

dif f (d1, d2) d=ef

| |d
2

-

d2 | |

-

| |d
1

-

d1

||

,

r el (d1, d2) d=ef

| | (d
2

-

d2 )

-

(d
1

-

d1 ) | | .

To normalize KT, rather than count the number of discordant
pairs of documents, for each discordant pair (d1,d2) we use the
1
value:  (d1,d2)+1 . We then simply sum the values for all discordant pairs. The resulting distance measures are denoted KT-sum, KTdiff and KT-rel, respectively. Note that the smaller the changes of

two documents which are a discordant pair, the higher the effect

of the swap between them -- i.e., the higher the distance and the

lower the robustness. It is also important to note that the values of
these measures are  0 but are not bound from above. If we were to use the standard weighted Kendall- distance (cf., [19]), then we
1
should have normalized by the sum of  (d1,d2)+1 over all documents pairs. Then, the upper bound would have been 1. However, while
weighted Kendall- differentiates the effect of discordant pairs on

the distance between two rankings, it does not allow to effectively

compare distances between different pairs of lists. A case in point,
if a list L contains two documents, then the standard weighted Kendall's- distance between L and L is equivalent to TC which

does not take into account the amount of documents' changes that

led to ranking changes.

To normalize TC we do the following. Suppose that d1 is the

highest ranked document in L and d  ( d ) is the highest ranked
21

document

in

L.

We

attribute

the

value

1  (d1,d2)+1

to

this

change

of the highest ranked document. Thus, the more d1 and d2 were

changed to become d  and d , respectively, the less weight we

1

2

attribute to this change. The resultant measures are: TC-sum, TC-

diff and TC-rel.

399

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

3 EMPIRICAL EXPLORATION
The goal of the exploration we present next is two-fold. First, studying the connection between the regularization of a linear ranking function and the robustness of the rankings it induces to (adversarial) documents' manipulations. According to the formal analysis presented in Section 2.3, the stronger the regularization, the more robust the rankings should be. The second goal is to empirically study Conjecture 1. That is, we explore the connection between the variance of the learned ranking function and ranking robustness using two ranking functions.
3.1 Experimental Setup
3.1.1 Dataset. To study the effect of adversarial document manipulations on ranking robustness, we used a recently published dataset that was created as a result of an on-going ranking competition [17]. The dataset is available at https://github.com/asrcdataset/.
The competition involved 31 repeated matches that lasted for 8 rounds; each match was with respect to a different query. The queries are a subset of TREC 2009-2012 topic titles that have a clear commercial intent and were more likely to stir up the competition. Students in an information retrieval course served as documents' authors. In the first round, in addition to the query itself, students were provided with an example relevant document, and were incentivized by bonus points to the course's grade to modify their documents so as to have them ranked as high as possible in the next round. Thus, the documents' modifications applied by the students can be considered as adversarial. There was no incentive, however, for producing documents relevant to the queries except for some general encouragement. Indeed, the percentage of relevant documents produced was declining throughout the competition [17]. Starting from the second round, students were presented with the ranking as well as the content of all documents submitted in the previous round in the match. To alleviate the task, documents were plain text and their length was restricted to 150 terms. Students had no prior knowledge of the ranking function and all data was anonymized. Statistics of the dataset is provided in Table 1.
The ranking function in all matches throughout the competition was based on LambdaMART [27] applied with a subset of the content-based features from Microsoft's learning-to-rank datasets9. In addition, several external modifications were made to ensure high ranking quality. First, each document in the dataset was labeled as valid, keyword stuffed or spam by five Figure Eight (https: //www.figure-eight.com/) annotators. (All annotations are available as part of the dataset.) Documents with at least four non-valid labels were penalized in the ranking. Second, documents similar to the example relevant document were demoted in the ranking. Third, documents were penalized in rankings if their content was duplicated from other documents in the previous round.
3.1.2 Ranking Functions. The exploration we present uses the documents created in the ranking competition just described. Rather than using the original ranking induced for each query in each round of the competition, we re-rank the documents using the ranking functions we study. One advantage of this practice is that we refrain from infusing noise caused by external interventions as
9 https://tinyurl.com/rmslr

Table 1: The dataset used for experiments [17].

Queries Rounds

31 Queries per student 3 8 Students per query 5 - 6

Students 52 Unique documents 897

Documents 1279 Relevant documents 1113

those describe above (e.g., penalizing documents whose content was duplicated), and we focus on the ranking functions of interest. Thus, while the ranking functions we study were not exactly those used in the competition, the manipulations of documents were adversarial as the only incentive was to have them ranked high, and the basic ranking function employed in the competition, LambdaMART, is highly effective. In other words, we measure ranking robustness by examining adversarial manipulations of documents introduced in response to a strong ranker.
We learned two families of ranking functions: RankSVM and LambdaMART. As was the case for the LambdaMART ranker used in the ranking competition, the ClueWeb09 Category B collection, henceforth ClueWeb, was used to train all ranking functions10. Titles of topics 1-200 from TREC 2009-2012 served for queries. We applied Krovetz stemming and removed stopwords on the INQUERY list from queries only. The Indri toolkit11 was used for experiments. To learn the ranking functions, 75% randomly sampled queries served for training and the remaining 25% for validation of hyperparameter values. Once hyper-parameter values were set, either using the procedure just described or to some predefined values as detailed below, all the queries were used to train the final ranking functions. These were then applied to rank documents in each round of the competition for each query. When we present evaluation for ClueWeb, we report the results of using five-fold cross validation. NDCG@20 served as the optimization criterion in all cases.
The ranking functions we use are trained with, and applied on, document-query feature vectors composed of 26 content-based features; 25 of these features are those used in the ranking competition [17]. We generated an additional query-independent document quality feature as follows. Instead of directly penalizing non-valid documents as was the case in the competition, we used these annotations to simulate Waterloo's spam classification scores [3]. Specifically, we used 100 - 20(k + s), where k and s are the number of keyword-stuffed and spam labels a document received, respectively; the scores for the example relevant documents were set to 100. Since k +s  5, the simulated scores are in [0, 100] as is the case for Waterloo's scores12. Thus, while the learned ranking function is based on Waterloo's scores, for the competition dataset it is applied with the human-annotation-based score we created which serves as a document quality measure.
3.1.3 Outline of Experiments. The goal of the regularizationbased experiments we report is to study the connection between the regularization of a linear RankSVM and LambdaMART and the robustness and effectiveness of rankings they induce. An additional
10The document collection created in the competition is too small to effectively learn ranking functions. 11 www.lemurproject.org/indri
12
Waterloo's classifier scores represent the percentage of documents in a collection that are "spammier" than the given document.

400

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

experiment is intended to contrast the ranking robustness of the two rankers. Hence, we now turn to describe the regularization approaches we have employed for the rankers.

Regularization of RankSVM. The objective function of RankSVM

can be expressed using hinge loss:

min L(w ; c) d=ef
w

1 w 2 2

+c
i, j, t

max(0, 1 - w (diqt

-

dqj t

)ldqiqt t

,

d

qt j

);

c(>

0)

is

a

regularization parameter; diqt

and

d

qt j

are

two

documents

roeftdriiqetviesdhfiogrheqruethryanqtth; aldqtiqtotf,ddjqqjtt

=1 and

if the relevance label for qt -1 otherwise. The objective

function can be re-written as follows [16]:

min L(w ; )
w

=

 ||w ||2 2

+

1 n

i, j, t

max(0, 1 - w (diqt

-

dqj t

)ldqiqt t

,

d

qt j

);

n is the number of document pairs across queries used for training;

c

=

1 n

.

Thus,

the

lower

the

value

of

c,

the

higher

the

value

of

,

and

hence the lower ||w || -- i.e., the regularization is stronger. Indeed,

lower c corresponds to higher margin of the separating hyperplane

(

|

2 |w

|

|

),

and

therefore

to

reduced

variance

and

increased

bias.

One of our goals is to study the correlation between ||w || and

ranking effectiveness and robustness. To that end, we vary the value
of c for learning w , and study the impact on induced rankings. We set c in these experiments to 189 different values in [0, 10000]13. We used SVMr ank 14 to train RankSVM; except for the value of c, all

other parameters were set to default values. We found high positive correlation between the value of c and the resultant ||w ||. Herein,

correlations are measured using Spearman coefficient, which is an

estimate of the monotonic relationship between variables, Pear-

son correlation, which is an estimate for linear relationship, and Kendall's- . The values of all three measures are in [-1, 1]: -1 and

1 indicate perfect negative and positive correlation, respectively.

The statistical significance of correlations (with respect to 0) is

determined at a 95% confidence level. The Spearman, Pearson and
Kendall correlations between c and ||w || are 0.951, 0.448 and 0.867, respectively; all the correlations are statistically significant15.

As noted above, in another experiment we performed, we com-

pared the robustness of rankings induced by RankSVM and LambdaMART. Here we set c to values in {0.001, 0.01, 0.1} using the

75-25 split for queries over ClueWeb described above. Recall that

NDCG@20 was the optimization criterion. The ranking compe-

tition did not have enough rounds so as to optimize directly for

robustness and then test it. Hence, we optimized for ranking ef-

fectiveness and evaluated the effect on robustness. We hasten to point out, however, that these values of c resulted in high ranking

robustness. We describe below how we measured robustness.

Regularization of LambdaMART. LambdaMART uses gradient boosted regression trees. The higher the number of trees and leaves, the lower the regularization, and the higher the variance.

13
Random

sampling

of

the

value

of

c

results

in

a

very

short

range

of

values

of

|

|w

|

|.

Hence, we sampled from [10x , 9 ·10x ] with steps of size 10x for x  {-4, -3, . . . , 2}.

We sampled from [1000, 10000] with steps of 1000. We further sampled 60 values uni-

formly from [0, 10000]; in addition, we used values in {1, 40, 80, 120, 160, . . . , 1000}

and {1, 45, 90, 135, 180, . . . , 1125}.

14 https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html

15We consider only values of c for which | |w | | < 15. See Section 3.2.1 for details.

We used RankLib's16 implementation of LambdaMART. To study the connection between the regularization of LambdaMART and the effectiveness and robustness of rankings it induces, we set (#leaves, #trees) to values in {(5, 150), (10, 160), (15, 170), . . . , (150, 440)}.
To compare the ranking robustness of LambdaMART with that of RankSVM, we set the number of leaves and trees to values in {250, 500} and {25, 50}, respectively, using the 75-25 split mentioned above. These values resulted in high ranking robustness. We now turn to describe how ranking robustness was evaluated.
3.1.4 Evaluating Ranking Robustness. Suppose that at round i of the competition for query q the set of documents created by the students is S. Now, we rank S using a ranking function to produce a ranked list Li . Then, we consider the set of documents created by the same students in round i + 1 of the competition for query q. Ranking this set yields the list Li+1. We can then apply upon Li and Li+1 each of the ranking-robustness measures we described in Section 2.5. For each query, we average these values over pairs of rounds i, i + 1; we then average the resultant values over queries. Thus, for each ranking function and robustness measure we get a single ranking robustness value.
Accordingly, we can measure the correlation between the regularization strength of RankSVM (as manifested in different values of ||w ||) and ranking robustness; and, between LambdaMART's regularization strength (with (#leaves, #trees) serving as a proxy) and its ranking robustness. As at the above, we use Spearman, Pearson and Kendall's- to measure correlations. Then, to contrast the ranking robustness of RankSVM and LambdaMART, we compare the values of the robustness measures. Statistically significant differences of retrieval performance and of values assigned by robustness measures are determined using the two-tailed paired t-test at a 95% confidence level. Recall that the robustness measure value for each query is the average over pairs of consecutive rounds. These values are compared for statistical significance over queries.
3.1.5 Evaluating Ranking Effectiveness. All documents in the competition were judged for relevance via Figure Eight by five annotators [17]. The relevance judgments are available as part of the dataset. Here, we consider a document relevant if it was labeled as such by at least three annotators. To estimate retrieval effectiveness we use MAP (mean average precision) and NDCG (normalized discounted cumulative gain) at cutoff 5; this is the number of students that participated in a competition per query17.
We also evaluate retrieval effectiveness over ClueWeb. In this case, MAP was computed for the top 1000 retrieved documents and NDCG for the top 20; the latter also served as the optimization metric for training LambdaMART.
3.2 Experimental Results
3.2.1 Ranking Robustness of RankSVM. In Figure 1 we present the values of the (unnormalized) robustness measures as a function of ||w || in RankSVM. Recall that high values of KT and TC attest to decreased ranking robustness, as these are measures of inter-list
16
https://tinyurl.com/ranklib 17To generate graded relevance judgments we followed Raifer at al. [17]: documents labeled relevant by 3, 4 and 5 annotators were considered marginally relevant, fairly relevant, and highly relevant, respectively; documents with at most 2 relevant labels were considered non-relevant.

401

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

KT TC
RBO

0.28 0.27 0.26 0.25 0.24 0.23 0.22 0.21
0

5

10

15

20

||w||

0.74 0.73 0.72 0.71 0.70 0.69 0.68 0.67
0

5

10

15

20

||w||

0.50 0.48 0.46 0.44 0.42 0.40 0.38 0.36
0

5

10

15

20

||w||

Figure 1: Robustness per ||w || in RankSVM.

Table 2: The correlation between ||w || and robustness measures' values for RankSVM. 'S', 'P' and 'K' stand for Spearman, Pearson and Kendall, respectively. `': the correlation is statistically significant.

KT KT-sum KT-diff KT-rel RBO TC TC-sum TC-diff TC-rel

S .748 .745 P .889 .872 K .574 .560

.678 .737 -.756 .528 .574 .850 .875 -.841 .567 .632 .509 .562 -.558 .374 .413

.475 .591 .483 .656 .336 .427

distance, and high values of RBO attest to increased robustness as this is an inter-list similarity measure. We see that increasing ||w ||, which means weaker regularization, results in decreased ranking
robustness when measured using KT and RBO. (The differences
with TC are discussed below.) This finding is aligned with our
formal analysis of linear ranking functions in Section 2.
To compute correlations between ranking robustness measures and ||w ||, we first observe the following in Figure 1. Our sampling of values of c, the regularization parameter in RankSVM, resulted in over-sampling of ||w || values somewhat larger than ||w || = 15. This can substantially affect correlation values. Hence, for the correlation
analysis to follow, and for studying RankSVM's ranking effectiveness as a function of ||w ||, we do not consider RankSVM functions with ||w ||  15. The resulting sample contains 71 RankSVM functions. We hasten to point out that this sampling did not change the
polarity of the correlations we found, nor their statistical significance, but only the actual correlation numbers which increased18.
Table 2 presents the correlation between ||w || and the values of all ranking-robustness measures. As can be seen, there are high, statistically significant, correlations between ||w || and the values of all robustness measures for all three correlation metrics (Spearman, Pearson and Kendall)19. Thus, the findings in Table 2 provide sup-
port to our formal analysis with respect to linear ranking functions

18
For

example,

without

removing

models

with

| |w

||



15,

the

Spearman

correlation

for KT, RBO and TC is: .33, -.466 and .284, respectively; Pearson correlation is

.806, -.780, and .503, respectively; Kendall's- numbers are: .246, -.343, and .217,

respectively. All correlations are statistically significant.
19
Recall that RBO is an inter-list similarity measure, in contrast to the other two; hence

increasing values of RBO attest to increased robustness.

Table 3: The correlation between #leaves and #trees in LambdaMART and the values of the robustness measures. 'S', 'P' and 'K' stand for Spearman, Pearson and Kendall, respectively. `': the correlation is statistically significant.

KT KT-sum KT-diff KT-rel RBO TC TC-sum TC-diff TC-rel

S .725 .733 P .674 .714 K .549 .568

.754 .727 -.600 .506 .521 .728 .732 -.615 .525 .530 .609 .563 -.453 .366 .366

.507 .547 .568 .577 .379 .398

-- RankSVM in this case: stronger regularization (decreased ||w ||) results in increased ranking robustness. This also provides support to our variance conjecture (Conjecture 1): increased ||w || means higher variance; thus, higher variance is indeed correlated with decreased ranking robustness for RankSVM.
In comparing the normalized robustness measures in Table 2 (X-sum, X-diff and X-rel measures) with their unnormalized counterparts we see the following. For KT, the normalized versions yield minor to moderate decrease of correlation with ||w ||, although the correlations remain statistically significant and high. Similarly, TCdiff yields lower correlation than TC. However, TC-sum and TC-rel yield higher correlations than TC.
Table 2 also shows that in almost all cases, KT-rel yields higher correlation than KT-sum and KT-diff, and TC-rel yields higher correlation than TC-sum and TC-diff. Recall from Section 2.5 that the X-rel measures use for normalization the norm of the difference between the vector change of one document and that of the other. This norm is ||v2 - v1|| used in the upper bound for simultaneous pairwise stability of a linear ranking function in Equation 1.
Finally, Table 2 shows that KT and its normalized variants, and RBO, yield higher correlation than TC and its variants. This is not a surprise as KT and RBO consider changes in the entire document list, while TC only considers changes at the highest rank; thus, TC is a less robust measure of ranking robustness. (These findings are aligned with the patterns observed in Figure 1.) We also see that RBO yields lower Pearson and Kendall correlation than KT. RBO attributes more weight to swaps of documents at high ranks and KT does not. However, the document lists are short (composed of 5 documents); hence, the differences in correlations potentially should not be attributed to different weighting of different ranks.
3.2.2 Ranking Robustness of LambdaMART. Table 3 shows the correlations between the number of leaves (#leaves) and trees (#trees) used to train LambdaMART, which were increased simultaneously, and the resulting values of ranking robustness. This increase corresponds to decreased regularization; i.e., higher variance. Indeed, Table 3 shows that according to all three correlation metrics and for all three ranking-robustness measures, robustness decreases with decreased regularization. This finding provides further support to our variance conjecture (Conjecture 1): ranking functions with higher variance yield rankings of decreased robustness. Thus, we attained support for the conjecture for two different rankers: the first is linear (RankSVM) and the second is not (LambdaMART).
Table 3 also shows that KT yields higher correlation than RBO which in turn yields higher correlation than TC. These findings are aligned with those presented above for RankSVM. We also

402

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Comparison of RankSVM and LambdaMART in terms of ranking robustness and effectiveness. The numbers in parentheses indicate the percentage of queries for which the robustness number attained for one ranking function is higher than that attained for the other. Percentages do not necessarily sum to 100 due to ties and rounding. `' marks statistically significant differences with LambdaMART. Bonferroni correction was applied for robustness comparisons.

KT TC RBO TC-diff TC-rel TC-sum KT-diff KT-rel KT-sum MAP NDCG

LambdaMART
0.343 (54.8) 0.599 (30.9) 0.613 (26.3) 0.361 (53.5) 0.246 (50.7) 0.213 (51.6) 2.071 (62.7) 1.399 (63.6) 1.204 (60.8) 0.825 (16.5) 0.837 (26.7)

RankSVM
0.264 (27.2) 0.401 (11.1) 0.703 (60.8) 0.219 (28.6) 0.153 (31.3) 0.133 (30.4) 1.505 (31.8) 1.008 (30.9) 0.874 (33.6)
0.846 (22.7)
0.854 (37.5)

see that the normalized versions of KT and TC (KT-X and TCX) yield increased correlations with respect to the unnormalized versions (KT and TC). This finding implies here to the importance of accounting not only for the change of ranking, but also for the change in documents that drove the change in ranking when measuring ranking robustness. Finally, we see that for TC, the normalized version TC-rel yields the highest correlations which is aligned with the findings above for both TC and KT. However, for KT here, KT-rel does not dominate KT-sum and KT-diff.
3.2.3 Comparing RankSVM and LambdaMART. In Table 4 we contrast the robustness and effectiveness of rankings induced by RankSVM and LambdaMART over the ranking-competition dataset. Recall that RBO measures the similarity between two lists, hence higher values attest to higher robustness. In contrast, KT and its normalized variants as well as TC and its normalized variants measure the differences between two lists; hence, lower values attest to higher robustness. We see that the ranked lists induced by RankSVM are (on average) more robust than those induced using LambdaMART according to all considered ranking-robustness measures; all the differences are statistically significant. Furthermore, the robustness of lists induced by RankSVM surpasses that of lists induced by LambdaMART for the majority of queries. (Refer to the numbers in parentheses.) These findings provide additional support to our variance conjecture. Since RankSVM is linear and LambdaMART is not, the variance of RankSVM is in general lower, and we saw that its ranking robustness is higher.
In the last two lines of Table 4 we compare the retrieval effectiveness of RankSVM and LambdaMART. Although the differences are not statistically significant, we see that lists induced by RankSVM are not only more robust but also somewhat more effective compared to LambdaMART on the competition dataset. The high MAP and NDCG values can be attributed to the fact that most documents generated by the students were judged to be relevant. (See Table 1.)

MAP NDCG

Table 5: Correlation between regularization of RankSVM (||w ||) and that of LambdaMART (#leaves and #trees) and re-
trieval effectiveness. Competition: Raifer et al.'s dataset [17]. `': the correlation is statistically significant.

RankSVM LambdaMART

Spearman Pearson Kendall

Competition ClueWeb

MAP NDCG MAP NDCG

-.777 -.787
.220 .259

-.918 -.897
.549 .636

-.559 -.564
.079
.141

MAP Competition
NDCG

ClueWeb

MAP NDCG

-.732 -.726 -.849 -.804

-.732 -.710 -.871 -.812

-.549 -.545 -.692 -.635

0.27 0.26 0.25 0.24 0.23 0.22 0.21
0

5

10

15

20

||w||

0.32 0.30 0.28 0.26 0.24
0

5

10

15

20

||w||

Figure 2: The connection between ||w || and retrieval effectiveness (MAP and NDCG) on ClueWeb of RankSVM.

3.2.4 Regularization and Retrieval Effectiveness. Table 5 presents the correlation between retrieval effectiveness (MAP and NDCG) and regularization for RankSVM (||w ||) and LambdaMART (#leaves and #trees). As most documents in the ranking-competition dataset are relevant, we also analyze retrieval performance for ClueWeb that has a much higher percentage of non-relevant documents.
Table 5 shows that except for the case of RankSVM over ClueWeb, there is high statistically significant positive correlation between increased regularization and retrieval effectiveness. (Decreasing ||w || and #leaves, #trees amounts to increased regularization.) Figure 2, which presents the connection between MAP/NDCG and ||w || of RankSVM on ClueWeb, shows a positive trend for ||w ||  5 and a negative trend for ||w ||  5. Substantially increasing (decreasing) ||w || can result in overfitting (underfitting). Thus, the correlations for RankSVM in Table 5 seem to be quite affected by a long range of underfitting which results in decreased retrieval performance; increasing ||w || then reduces underfitting and improves performance until the ranker overfits. In Figure 3 we see that, in general, decreased regularization of LambdaMART (i.e., higher number of leaves and trees) results in decreased MAP performance over ClueWeb. This finding is aligned with those that emerge based on Table 5.
4 RELATED WORK
There are notions of robustness of ad hoc retrieval functions different than that we examine here: the performance variance over queries [23] and the relative performance over queries with respect to another ranking function [24]. We study changes to rankings that result from documents' (adversarial) manipulations.

403

MAP MAP

Session 4A: Fairness & Robustness

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

0.245 0.240 0.235 0.230 0.225 0.220 0.215 0.210 0.205 0.200
0

20 40 60 80 100 120 140 160 #leaves

0.245 0.240 0.235 0.230 0.225 0.220 0.215 0.210 0.205 0.200
0

100

200

300

400

500

#trees

Figure 3: The connection between #leaves and #trees used in LambdaMART and its MAP effectiveness over ClueWeb.

The changes of a ranked document list that result from synthetic random noise introduced to documents were used to predict query performance [28]. We address a different setting -- adversarial document changes intended to promote documents in rankings -- and present a different analysis.
Much of the work on adversarial information retrieval focuses on identifying and addressing different types of spam (e.g., contentbased and hyperlink-based) [1, 2]. In contrast to our work, there was no formal and empirical analysis of ranking robustness with respect to (adversarial) documents' manipulations.
Recent work analyzes the strategies employed by documents' authors in ranking competitions [17]. In contrast, we analyze the robustness of ranking functions. We use the datasets of ranking competitions organized in this work [17] in our empirical analysis.
The probability ranking principle [18] was shown to be suboptimal in competitive retrieval settings, where authors manipulate their documents so as to have them highly ranked. However, the robustness of ranking functions was not studied as in our work.
There is a growing body of work on adversarial and robust classification; e.g., [4, 6­10, 14, 20, 21, 25]. The focus is on improving classifier's robustness to adversarial (often, minuscule) manipulations of objects and their feature values. In contrast to our work, the robustness of document ranking functions was not studied; specifically, the pairwise robustness notions we analyze, which are a core aspect of ranking robustness, were not studied.
The connection between neural network regularization and the stability of classification decisions has recently been demonstrated [20]. We demonstrate the connection between regularization of linear ranking functions and stability of retrieval scores, and more importantly, ranking robustness.
5 CONCLUSIONS AND FUTURE WORK
We presented a formal and empirical analysis of the robustness of rankings induced by feature-based relevance-ranking functions to (adversarial) manipulations of documents. We formally showed that increased regularization of linear ranking functions results in increased ranking robustness. Accordingly, we conjectured that increased variance of any learned ranking function results in decreased ranking robustness. We provided empirical support to our formal findings and the conjecture by analyzing ranking competitions where authors introduced adversarial changes to documents.
We plan to further study and improve the robustness of nonlinear learning-to-rank functions. We also intend to extend the robustness analysis to sets of queries; e.g., those representing the same information needs.

Acknowledgments We thank the reviewers for their comments.
This work was supported by funding from the European Research
Council (ERC) under the European Union's Horizon 2020 research
and innovation programme (grant agreement 740435).
REFERENCES
[1] 2005­2009. AIRWeb -- International Workshop on Adversarial Information Retrieval on the Web.
[2] Carlos Castillo and Brian D. Davison. 2010. Adversarial Web Search. Foundations and Trends in Information Retrieval 4, 5 (2010), 377­486.
[3] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2011. Efficient and effective spam filtering and re-ranking for large web datasets. Informaltiom Retrieval Journal 14, 5 (2011), 441­465.
[4] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004. Adversarial Classification. In Proc. of KDD. 99­108.
[5] Fernando Diaz. 2007. Regularizing query-based retrieval scores. Information Retrieval 10, 6 (2007), 531­562.
[6] Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. 2015. Analysis of classifiers' robustness to adversarial perturbations. CoRR abs/1502.02590 (2015).
[7] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2016. Robustness of classifiers: from adversarial to random noise. In Proc. of NIPS. 1624­1632.
[8] Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. 2017. The Robustness of Deep Networks: A Geometrical Perspective. IEEE Signal Processing Magazine 34, 6 (2017), 50­62.
[9] Amir Globerson and Sam T. Roweis. 2006. Nightmare at test time: robust learning by feature deletion. In Proc. of ICML. 353­360.
[10] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial Examples. In Proc. of ICLR.
[11] Zoltán Gyöngyi and Hector Garcia-Molina. 2005. Web Spam Taxonomy. In Proc. of AIRWeb 2005. 39­47.
[12] Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proc. of KDD. 217­226.
[13] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer. I­XVII, 1­285 pages.
[14] Daniel Lowd and Christopher Meek. 2005. Adversarial Learning. In Proc. of SIGKDD. 641­647.
[15] Donald Metzler and W. Bruce Croft. 2007. Linear feature-based models for information retrieval. Information Retrieval 10, 3 (2007), 257­274.
[16] Constantinos Panagiotakopoulos and Petroula Tsampouka. 2013. The Stochastic Gradient Descent for the Primal L1-SVM Optimization Revisited. In Proc. of ECML PKDD. 65­80.
[17] Nimrod Raifer, Fiana Raiber, Moshe Tennenholtz, and Oren Kurland. 2017. In-
formation Retrieval Meets Game Theory: The Ranking Competition Between Documents' Authors. In Proc. of SIGIR. 465­474. [18] Stephen E. Robertson. 1977. The Probability Ranking Principle in IR. Journal of Documentation (1977), 294­304. Reprinted in K. Sparck Jones and P. Willett (eds), Readings in Information Retrieval, pp. 281­286, 1997. [19] Grace S.Shieh. 1998. A weighted Kendall's tau statistic. Statistics & Probability Letters 39, 1 (1998). [20] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural networks. In Proc. of ICLR. [21] Thomas Tanay and Lewis D. Griffin. 2016. A Boundary Tilting Persepective on the Phenomenon of Adversarial Examples. CoRR abs/1608.07690 (2016). [22] C. J. van Rijsbergen. 1979. Information Retrieval (second ed.). Butterworths. [23] Ellen M. Voorhees. 2004. Overview of the TREC 2004 Robust Retrieval Track. In Proc. of TREC. [24] Lidan Wang, Paul N. Bennett, and Kevyn Collins-Thompson. 2012. Robust ranking models via risk-sensitive optimization. In Proc. of SIGIR. 761­770. [25] Yizhen Wang, Somesh Jha, and Kamalika Chaudhuri. 2017. Analyzing the Robustness of Nearest Neighbors to Adversarial Examples. CoRR abs/1706.03922 (2017).
[26] William Webber, Alistair Moffat, and Justin Zobel. 2010. A Similarity Measure for Indefinite Rankings. ACM Transactions on Information Systems 28, 4 (2010), 20:1­20:38.
[27] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Information Retrieval 13, 3 (2010), 254­270.
[28] Yun Zhou and Bruce Croft. 2006. Ranking robustness: a novel framework to predict query performance. In Proc. of CIKM. 567­574.

404

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Update Delivery Mechanisms for Prospective Information Needs: An Analysis of Attention in Mobile Users

Jimmy Lin, Salman Mohammed, Royal Sequiera, and Luchen Tan
David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada

ABSTRACT
Real-time summarization systems that monitor document streams to identify relevant content have a few options for delivering system updates to users. In a mobile context, systems could send push notifications to users' mobile devices, hoping to grab their attention immediately. Alternatively, systems could silently deposit updates into "inboxes" that users can access at their leisure. We refer to these mechanisms as push-based vs. pull-based, and present a twoyear contrastive study that attempts to understand the effects of the delivery mechanism on mobile user behavior, in the context of the TREC Real-Time Summarization Tracks. Through a cluster analysis, we are able to identify three distinct and coherent patterns of behavior. As expected, we find that users are likely to ignore push notifications, but for those updates that users do pay attention to, content is consumed within a short amount of time. Interestingly, users bombarded with push notifications are less likely to consume updates on their own initiative and less likely to engage in long reading sessions--which is a common pattern for users who pull content from their inboxes. We characterize users as exhibiting "eager" or "apathetic" information consumption behavior as an explanation of these observations, and attempt to operationalize our findings into design recommendations.
ACM Reference Format: Jimmy Lin, Salman Mohammed, Royal Sequiera, and Luchen Tan. 2018. Update Delivery Mechanisms for Prospective Information Needs: An Analysis of Attention in Mobile Users. In SIGIR '18: 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 812, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3209978.3210018
1 INTRODUCTION
Prospective information needs are typically posed against document streams such as posts on Twitter, RSS feeds, newswire articles, etc. For example, a user might be concerned about tensions on the Korean Peninsula and wishes to receive updates whenever there is a new development. Real-time summarization systems monitor these streams and identify documents that are relevant, typically operationalized as on-topic, non-redundant, and timely. The nature of prospective information needs means that relevant documents
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210018

may appear at any time, and thus an open question is how system output should be delivered to users. We explore this question in the context of mobile devices that are ubiquitous today.
There are two basic mechanisms for delivering updates from real-time summarization systems: In one approach, an update can be delivered to a user's mobile device as a push notification, typically accompanied by a visual and/or auditory signal that purposely interrupts the user to attract attention. Alternatively, updates can be silently deposited into an "inbox", waiting for the user to examine on the user's own initiative (much like email). These two mechanisms can be characterized as either "push-based" or "pullbased"--terminology we will use throughout this paper. Although in reality a system can adopt both mechanisms, for simplicity we examine either approach in isolation.
We present a two-year study drawn from the TREC Real-Time Summarization (RTS) Tracks involving over 50 users who evaluated live system updates on their mobile devices in situ, i.e., as they were going about their daily lives. These users interacted with a mobile app that employed either the push- or pull-based mechanism described above. We are specifically interested in three research questions focused on users' attention to system updates:
(RQ1) How do users in the push vs. pull treatments differ in terms of quantifiable behavioral characteristics?
(RQ2) Can we generalize patterns of user behavior within and across push- and pull-based update delivery?
(RQ3) Can we operationalize our findings into design recommendations for developers of real-time summarization systems?
Contributions. In answering the above research questions, we view our work as having the following contributions:
· We present, to our knowledge, the first contrastive study of pushvs. pull-based delivery mechanisms for addressing prospective information needs for mobile users. Since the main experimental manipulation was the delivery mechanism, we can attribute differences in user behavior to the system treatment.
· We present a novel methodology for data analysis that exploits different visualizations to empirically characterize how much and when users pay attention to updates from systems with different delivery mechanisms.
· We identify, via a clustering analysis, distinct and coherent patterns of user behaviors and the impact of delivery mechanisms. We hypothesize that users can be characterized as eager or apathetic with respect to information consumption, which provides an explanation of observed behaviors.
· We operationalize our findings into concrete design recommendations for system developers, identifying categories of users for which push notifications might or might not be valuable.

785

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

As an initial foray into information delivery mechanisms on mobile devices for real-time summarization systems, our work raises as many interesting questions as it answers. We are forthright in discussing the limitations of our study, most of which point directly to future avenues of exploration.
2 BACKGROUND AND RELATED WORK
The literature on prospective information needs dates back to the 1960s, with work on "selective dissemination of information" to help scientists stay up to date on relevant literature [8, 9]. The relationship between prospective and retrospective (i.e., ad hoc) information needs has long been noted [4], although the former has received far less attention over the years. However, two related research threads beginning in the 1990s substantially advanced our state of knowledge. The TREC Filtering Tracks, which ran from 1995 [11] to 2002 [17], brought modern evaluation methodologies and shared evaluations to bear on filtering document streams. The task can be best understood as binary classification on every document in the collection with respect to standing queries. The research program known as Topic Detection and Tracking (TDT) encompassed a number of related tasks around streams of news stories [1], including event detection, tracking, and linking. The Temporal Summarization Tracks at TREC, which ran from 2013 to 2015 [2], represented the evolution of TREC Filtering and TDT; the goal was to generate concise update summaries from news sources about unexpected events as they developed [7].
The Real-Time Summarization (RTS) Track at TREC, which provides the context for this work, is a direct descent of Temporal Summarization and captures a number of important differences compared to TREC Filtering and TDT. Both were formulated with an intelligence analyst in mind, with an emphasis on recall--e.g., in filtering the goal is to identify all relevant documents. In contrast, for delivery to mobile devices, we are concerned with selecting only a small set of the most relevant updates. Furthermore, in TREC Filtering and TDT, systems were forced to make immediate decisions about incoming documents; instead, real-time summarization systems are given the option of delaying decisions, which allows tradeoffs between output quality and timeliness.
Another major difference is that previous evaluations, including TDT, TREC Filtering, and Temporal Summarization, merely simulated the streaming nature of the document collection, whereas in RTS the participants were required to build working systems that operated on tweets posted in real time. This enabled an evaluation setup whereby system updates are delivered directly to users, matching real-world deployment scenarios. The RTS Track is an example of a "living labs" approach [19, 20] that attempts to evaluate systems in more realistic settings.
There is a rich body of work on interruptions in general and mobile push notifications in particular from the perspective of human­computer interaction [14]. However, we are not aware of any previous work specifically on prospective information needs, and thus our study represents a novel contribution to the literature.
Despite the term "summarization" in RTS, the systems we consider are very different from the multi-document summarization tasks conceived by the NLP community (e.g., DUC evaluations [5]). Those summarization tasks are retrospective in nature, and the

Stream of Tweets

Participating Systems

Evaluation broker

Mobile Users

Twitter API
Figure 1: The RTS evaluation setup: systems "listen" to the live Twitter sample stream and send results to the evaluation broker, which then delivers results to users, either via push notifications or silent deposit into users' inboxes.
goal is for a system to construct summaries of varying lengths "on demand". In contrast, RTS is designed to deliver relevant, timely, and novel updates incrementally over time.
3 EVALUATION METHODOLOGY
The Real-Time Summarization (RTS) Tracks at TREC 2016 and 2017, for which we were co-organizers, deployed a "living labs" evaluation setup whereby real users were recruited to provide relevance judgments in situ on their mobile devices. In this paper, we focus only on the "Scenario A" task, whose overall design is shown in Figure 1. Although the point of the RTS Track was to evaluate systems using judgments from the mobile users, in this study we analyze the behavior of the mobile users, treating the systems as black boxes. Here, we present the salient aspects of the evaluation methodology, referring readers to the track overviews for details [12, 13].
The evaluations occurred at pre-specified intervals during the summer of 2016 (10 days) and the summer of 2017 (8 days). The prospective information needs (called interest profiles) were distributed to participating groups well in advance. Due to their timedependent nature, different sets of interest profiles were used in each evaluation. A few weeks prior to the beginning of the evaluation period, we recruited mobile users: In 2016, the recruitment pool consisted of undergraduate and graduate students at the University of Waterloo. In 2017, the subject pool was additionally expanded to include developers of the participating systems. In total, 13 users participated in 2016 and 42 in 2017. All users were compensated $1 per 20 judgments (the same in both years). As part of the recruitment process, users were asked to "subscribe" to interest profiles, i.e., to indicate which ones they desired system updates for. In 2016, there were 203 profiles to choose from, all created by the organizers. In 2017, there were 188 profiles to choose from, a combination of organizer-created and user-suggested profiles. A large and diverse set of profiles meant that users were likely able to find profiles they were genuinely interested in.
In the 2016 evaluation, 18 participating groups submitted 41 system runs; in 2017, 15 groups submitted 41 runs. During the evaluation period, whenever a system identified a relevant tweet with respect to an interest profile, the system submitted the tweet id to the evaluation broker (see Figure 1). Each system was allowed to submit at most ten tweets per interest profile per day as not to overwhelm the user. The evaluation broker then "delivered" the update (tweet) to the mobile users who had subscribed to the interest profile, following the temporal interleaving strategy proposed by

786

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Qian et al. [16] and further examined by Roegiest et al. [18]. Note that each update was delivered only once, even if it was submitted by multiple systems at different times. The main difference between the two evaluations was the update delivery mechanism:
· In 2016 (RTS16), the mobile users installed a custom assessment app, and each delivered update was immediately rendered as a push notification containing both the tweet and the corresponding interest profile. Modulo device-specific settings, each notification was accompanied by an audible and/or tactile signal specifically designed to attract the user's attention. The user may choose to attend to the update immediately or to ignore it. Either way, the update is added to an assessment queue in the app on the user's mobile device. The user can access the app at any time to examine the queue of accumulated updates in reverse chronological order (i.e., most recent first). We call this the push condition, since system updates are directly pushed to users.
· In 2017 (RTS17), delivered updates were deposited in LIFO (LastIn, First-Out) queues we call the users' inboxes. This delivery was not accompanied by any attempt to interrupt the user. They could, anytime they desired, visit a web app (different from the app used in 2016) on their mobile devices to examine the queue of accumulated updates. The users were shown updates in reverse chronological order, and so the presentation order was the same in both years. We call this the pull condition, since the users are pulling updates from their inboxes at their leisure.
Irrespective of the delivery mechanism for each update (tweet), the user can make one of three judgments with respect to the associated interest profile: relevant, if the update contains relevant and novel information; redundant, if the update is relevant but contains redundant information (with respect to previously-seen updates); not relevant, if the update does not contain relevant information. However, as we explain below, our study does not specifically make use of these judgments.
One salient characteristic of the evaluation methodology is its in situ design, which realistically mirrors real-world deployment scenarios since users are going about their daily lives throughout the multi-day evaluation period. The updates are delivered in real time, but judgments are provided at the users' leisure under the two system treatments. Of course, some updates are never seen. How many updates a user consumes and the delay between update delivery and user action are key aspects of user attention that we examine, detailed next.
4 POPULATION-LEVEL ANALYSES
Given the experimental setup described above, this paper examines user attention to system updates under different delivery mechanisms (push vs. pull). To be precise, we recorded a user as "paying attention" or "consuming" an update as the moment the user submitted a relevance judgment in the mobile app. In actuality, though, these are distinct events, since the user first notices the update, reads the content, and finally provides a judgment. However, since tweets are short, we believe that reading time can be ignored without affecting our overall findings. Thus, in our analyses, we refer to "consuming an update" and "providing a relevance judgment" interchangeably as the point in time when a user "paid attention" to a system update.

To simplify analyses, we do not consider the actual relevance judgment, since consuming an update incurs effort and cost whether or not the update is relevant. For push notifications, the user has already been interrupted. For the pull condition, we note that effort models for batch relevance assessment in the literature (e.g., [21]) do not distinguish between judgment grades, so this is a reasonable simplification. Nevertheless, we return to discuss system quality as a confounding factor in Section 7. In this section, we focus on population-level analyses, and then use these findings to guide closer examination of individual users in Section 5.
4.1 Volume and Response Rate
We begin by defining volume and response rate, two straightforward ways of characterizing user behavior:
· Volume refers to the number of updates that a user has potentially been exposed to, which is affected by the number of interest profiles the user subscribed to as well as the nature of those information needs. In the push condition (RTS16), volume forms the upper bound on the number of push notifications the user could have received during the evaluation period. In the pull condition (RTS17), volume refers to the total number of messages that was deposited in the user's inbox during the evaluation period.
· Response rate refers to the proportion of updates that a user actually paid attention to (i.e., provided a relevance judgment).
Figure 2 shows the volume and response rates for 13 users in the push condition (RTS16, left) and 42 users in the pull condition (RTS17, right). The total height of each blue bar indicates volume and each orange bar indicates the number of updates judged, so the response rate is the ratio between the two. The bars are also annotated with the number of interest profiles that each user subscribed to and sorted by the total number of judgments provided. Each user is identified by a three letter alphanumeric code, prefixed with either RTS16 or RTS17--these identifiers are used to refer to specific users anonymously throughout the paper. We see that in both cases, a few users contributed many judgments, while many users contributed relatively few. This comes as no surprise, as the distribution of judgments is consistent with crowd-sourced relevance judgments, e.g., via Amazon Mechanical Turk [6].
It is quite obvious that the response rate for the push condition is much lower than for the pull condition. For the push condition, the maximum response rate was 46% for a volume of 7141 updates; only four (out of 13) users had response rates greater than 20%. For the pull condition, there were 9 users (out of 42) who consumed over 90% of the updates; of these, the maximum volume was 14477 updates. This finding makes sense as push notifications are disruptive. As a point of reference for the push condition, 5000 updates over an evaluation period of ten days translates into an average of 21 messages per hour (assuming a constant arrival rate). Factoring in sleeping time, we would already consider a response rate of nearly 50% (the most "diligent" user in RTS16) quite impressive.
The dramatic differences in response rates can be seen in Figure 3, which shows histograms of the response rates for both the push (green) and pull (purple) conditions, bucketed in increments of 10%. There is insufficient data to draw inferences about the shapes of the distributions, except to confirm that response rates are much lower for the push condition.

787

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

25000

9RlXPe

57616
43

-XdJPentV

20000

33

15000

10000 5000 0

10

15

16

12

8

12 10
10
2

12 4

25000

79

20000

71

57617

9ROXPH -XGJPHQWV

15000 10000
5000 0

68

59

48

35

24

25

33

29

14 1215

16
98 5 5

16

16

7

6 3433

3374

16

8 2531 53

5442

57616-109 57616-03y 57616-097 57616-06t 57616-12r 57616-05n 57616-188 57616-083 57616-22Z 57616-04V 57616-13X 57616-020 57616-07n
57617-AzB 57617-yh4 57617-b1D 57617-hDD 57617-7YG 57617-1AY 57617-xWS 57617-b1V 57617-ZVM 57617-yZ6 57617-1Bz 57617-9Dx 57617-VCy 57617--GZ 57617-C3H 57617-D5M 57617-Ti0 57617-P4X 57617-Q33 57617-FOG 57617-HX57617-PGF 57617-iXH 57617-hQ57617-4GG 57617-xG7 57617-FGC 57617-PBP 57617-YHY 57617-X4X 57617-Z20 57617-DSR 57617-RZZ 57617-WXA 57617-yXB 57617-hDI 57617-iV3 57617-XxO 57617-F4Z 57617-MV9 57617-WXQ 57617-29F

Figure 2: The volume and number of judgments received for the push condition (left) and the pull condition (right). Bars are sorted by the number of judgments provided and annotated with the number of profiles the user subscribed to.

Histogram of Response Rates

RTS17

10

RTS16

8

# Assessor

6

4

2

0

0.0

0.2

0.4

0.6

0.8

1.0

Response Rate

Figure 3: Histogram of response rates for RTS16 (push) and RTS17 (pull), bucketed in 10% increments.

Interestingly, it appears that volume and response rate vary independently in both the push and pull conditions: For example, in RTS16, many users subscribed to a similar number of profiles and hence received similar numbers of updates; yet their response rates varied greatly. In RTS17, we see users who received a high volume of updates (by subscribing to many profiles). In some cases, they attended to nearly all updates, and in other cases they ignored most of them. On the flip side, we see users who received a low volume of updates but attended to nearly all of them, and users who still ignored most updates.
4.2 Response Profiles
We define response delay, or simply delay, as the difference in time from when an update was delivered to when the user paid attention to it. In other words, how long did it take the user to notice and consume the update? To better understand user behavior, we can empirically characterize the temporal distribution of these delays. To simplify analyses, we bucket delays into one hour blocks--that is, how many updates did the user attend to within the first hour, the second hour, etc. under either the push or pull conditions? We

can normalize the absolute counts to yield a probability distribution. We refer to these as response profiles, which characterize empirically when each user pays attention to system updates. Note that compensation for participants in our studies was not tied to how quickly they provided judgments, and therefore these delays are not the result of experimental manipulation.
In Figure 4, we visualize these probability distributions as a heatmap. Each column represents a specific user: the push condition (RTS16) on the left and the pull condition (RTS17) on the right. Each row represents a particular hour; there are 13 rows in total: the first 12 represent individual hours, while the last row captures the remaining probability mass (longer than 12 hours). Our rationale for lumping all delays longer than 12 hours together is that beyond a certain point, we are operating outside the intended design space of real-time summarization systems. In the heatmap, color is used to encode the amount of probability mass in each condition--darker reds indicate more mass. Within each condition (push vs. pull), the columns (users) are sorted in ascending order of the amount of probability mass in the first row (i.e., the proportion of messages that the user paid attention to in the first hour).
From the heatmap, big differences between the push and pull conditions are readily apparent. In the push condition, for most users, the probability mass is concentrated in the first row, which means that users pay attention to updates within the first hour. This is expected: if systems interrupt users with push notifications, they will indeed pay attention--and therefore the delays are generally short. For a system designed to address prospective information needs, short delays seem like a desirable outcome. However, this must be balanced against a much lower response rate in the push condition (Figures 2 and 3). In fact, most updates are never seen at all. For users in the pull condition, we see a broad range in terms of the amount of probability mass in the first row. Interestingly, we also observe significant mass in the 12+ row for many users--we'll come back and expand on this observation later.
More formally, we can visualize the amount of mass in the first row of the heatmap in an empirical distribution function (EDF), shown in Figure 5 (separately for the push and pull conditions), where the y axis represents the cumulative probability and the x

788

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

1.0 1

2

3

0.8

4

5

6

0.6

7

8

0.4

9

10

11

0.2

12

12+

RTS16-R1T3SX16-R0T7Sn16-R09TS716-R0T6St16-R1T8S816-R0T5Sn16-R1T0SV16-R2T2SZ16-R0T8SP16-R1T2Sr16-R0T3Sy16-R0T2S016-04s RTS17-RWTuSA17-RFGTSC17-RsTCSy17-RhTDSD17-wROTS017R-FTlSd17-mRTQSu17-RYTHSY17-oRwTSw17-RaTpSo17-xRGT7S17-RaT5Sj 17-RiXTeS17-RnTPSP17R-HTuSJ17-RCT3SH17-RVTDSx17-mRTGSc17R-hTnSJ17-RyTwSS17-RcT4Sw17-bRNTDS17-RisT3S17R-JTdSZ17-mRTBSm17-RqTiMS17-yRuTBS17R-hTaSI17-R7TvSG17-yRhT4S17R-jsTVS17-RATzSB17-OR9TSF17R-ZTsSj17-RNTBSz17-xRWTSp17-RbT1Ss17R-tTXSn17-RQTdSG17-RNTASY17-uRQTSu17-uxl

0.0

Figure 4: Heatmap visualizing user response profiles: for each user (column), each cell shows the fraction of updates consumed in the n-th hour (row) after the system update was delivered. The last row represents delays more than 12 hours. The push condition (RTS16) is shown on the left, the pull condition (RTS17) on the right.

1.0

first hour

0.9

first 2 hours

first 3 hours

0.8

first 4 hours

0.7

first 5 hours first 6 hours

0.6

1.0

first hour

0.9

first 2 hours

first 3 hours

0.8

first 4 hours

0.7

first 5 hours first 6 hours

0.6

Cumulative Probability Cumulative Probability

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0.0

0.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Probability Mass

Probability Mass

Figure 5: Empirical distribution functions (EDFs) of the fraction of updates consumed within n hours under the push (left) and pull (right) conditions. Data for the blue lines (first hour) correspond to the first row of the heatmap in Figure 4; ticks on the bottom represent the probability mass in the first hour of those individual users.

axis represents the random variable--in this case, the proportion of updates consumed in the first hour (the top blue line). Each black tick at the bottom of each plot represents the probability mass in the first hour for an individual user. If we assume that our users are iid sampled from a particular population, then the EDF coarsely approximates the true CDF of the population (and the EDF converges to the CDF as the number of samples increases). If we further assume that the evaluations are drawing from a homogeneous population (more discussion in Section 7), then the differences between the EDFs can be attributed to differences in system treatment (i.e., delivery mechanism).
Focusing for now only on the top blue line (the first hour): How can we interpret these EDFs? It is more intuitive to consider one minus the cumulative distribution: these distributions show the fraction of users (1-y axis value) that have paid attention to at least some fraction of the updates in the first hour. For example, under the push condition, 20% of users or the top quintile (corresponding to 0.8 on the y axis) attended to at least 90% of the updates. In contrast, under the pull condition, the top quintile attended to at

least 35% of the updates (a much lower bound). The median user (y = 0.5) in the push condition paid attention to at least 70% of updates in the first hour; the median user in the pull condition paid attention to at least 20% of updates during the same period. It is worth emphasizing that these EDFs show delays for updates that the user paid attention to; the plots say nothing about updates that were never seen. Although users in the push condition have shorter delays, their response rates are also much lower.
Our analysis can be extended beyond the first hour to the n-th hour: that is, we can plot the EDF of users paying attention to updates within n hours. These curves for the push and pull conditions are shown in Figure 5, for each hour up to six hours (in different colors). The "rightward shift" of the curves indicate that for every passing hour of delay, more and more users pay attention to system updates. Within six hours, under the push condition, the median user (y = 0.5) would have consumed nearly all updates (that the user is ever going to read), while under the pull condition, the user would have only consumed around half of the updates (albeit out of a larger fraction of the total volume).

789

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

The combination of this analysis and the previous section forms one contribution of this paper--a novel methodology (using heatmaps and EDFs) to empirically characterize how much and when users pay attention to updates from real-time summarization systems under different delivery mechanisms. These findings provide an answer to (RQ1) "How do users in the push vs. pull treatments differ in terms of quantifiable behavioral characteristics?" There are quite obvious and easily quantifiable differences in user behavior: Overall, users have much lower response rates when bombarded with push notifications, but for the updates they do pay attention to, information is consumed quickly (typically within an hour).
4.3 Clustering Response Profiles
It is no surprise that push notifications lead to shorter response delays. What is surprising, though, is that even in the pull condition, there are users whose response profiles have probability mass concentrated in the early hours. Examining the heatmap in Figure 4, there are a few users from the pull condition whose behaviors are quite similar to those from the push condition, except that these users are not receiving push notifications!
We can formalize this observation by identifying clusters of users who have similar response profiles, i.e., by clustering the columns in the heatmap in Figure 4. This is accomplished by treating each profile (i.e., the bucketed distributions) as a feature vector and applying hierarchical clustering using cosine similarity as the distance metric (with average link merging). With this analysis we answer (RQ2): Can we generalize patterns of user behavior within and across push- and pull-based update delivery?
Results are presented in Figure 6 as a dendrogram showing the sequence of hierarchical merges that comprise the clustering. Each row is exactly the same as the corresponding column in Figure 4, except rotated; the leftmost column is a color code for presentational convenience. From the hierarchical structure of the merges we can identify three distinct clusters of profiles:
· Early-heavy distributions. In these response profiles (coded orange), probability mass is concentrated in the early hours, with short response delays. Within this cluster, however, we still observe gradations--some profiles have more mass in the early hours than others. These users are "eager" in consuming updates, typically with more frequent but shorter sessions.
· Late-heavy distributions. In these profiles (coded purple), the probability mass is concentrated in the 12+ bucket, meaning that these users do not consume updates until long after they have been delivered. However, we do observe degrees of late "heaviness", i.e., some profiles have more mass in the 12+ bucket than others. There is inevitably probability mass in the early hours because a user encounters older updates in the assessment interface only after consuming more recent updates (by design). These users are "apathetic" in consuming updates, with much fewer but longer sessions.
· Bimodal distributions. Interestingly, we observe a distinct third cluster of response profiles (coded blue), where the distribution of delays is bimodal. There is a non-negligible amount of probability mass in both the early bucket (first hour) and the late bucket (i.e., 12+ hours). In other words, these users consume some updates soon after delivery, but they also consume a significant number

RTS16-07n RTS17-C3H RTS17-apo RTS17-wO0 RTS17-WuA RTS17-hDD RTS17-sCy RTS17-oww RTS17-nPP RTS17-a5j RTS17-Fld RTS17-FGC RTS17-mQu RTS17-mBm RTS17-xWp RTS17-hnJ RTS17-VDx RTS17-mGc RTS17-ywS RTS17-bND RTS17-yh4 RTS17-7vG RTS17-YHY RTS17-iXe RTS17-HuJ RTS17-xG7 RTS17-uxl RTS17-uQu RTS16-22Z RTS16-08P RTS16-12r RTS16-03y RTS16-020 RTS16-04s RTS17-QdG RTS17-NAY RTS16-188 RTS16-10V RTS16-05n RTS17-NBz RTS17-Zsj RTS17-b1s RTS17-is3 RTS17-O9F RTS17-haI RTS17-JdZ RTS17-AzB RTS17-qiM RTS17-jsV RTS17-yuB RTS17-tXn RTS16-06t RTS16-097 RTS17-c4w RTS16-13X

1

2

3

4

5

6

7

8

9 10 11 12 12+

Figure 6: Hierarchical clustering of response profiles for the push and pull conditions, showing early-heavy (orange), late-heavy (purple), and bimodal (blue) distributions.

of old updates. These users appear to be both eager and apathetic, with a combination of long and short sessions.
It is worth emphasizing that these clusters (our color codes) are based on the induced hierarchical structures generated by the clustering algorithm. This lends credence to their empirical validity, as opposed to subjective "inkblot reading" by the authors. There are seven outliers that appear to defy our characterizations, which we have color-coded gray; these users, however, for the most part, provided few judgments. Two provided less than 100 judgments during the entire evaluation period, and all except for one provided less than 500 judgments.
Note that in this analysis we applied clustering over users from both the push (RTS16) and pull (RTS17) conditions, which means that the observed user behaviors represent a complex mix of users' intrinsic characteristics as well as the system treatment (i.e., push vs. pull). Nevertheless, two interesting observations jump out:
Observation #1: Some users in the pull condition behave as if they were in the push condition. The cluster of early-heavy distributions (orange) contains a mix of users from both the push and pull conditions. For RTS17 (pull condition) users, this means that they paid attention to most system updates within a short amount of time without the interruption of push notifications. As we examine more closely in Section 5, these users appear to be frequently returning to their inboxes without an external prompt.
Observation #2: No users in the push condition exhibit late-heavy and bimodal delay distributions. These findings are related, in that the second follows from the first. In other words, when a system pushes a notification, the user either pays attention to it within a short amount of time or never looks at it. As a result, we also do not see late-heavy or bimodal distributions.
Let us add some statistical rigor to this observation: Users can be classified into four categories (the three profiles above and a fourth "other" category corresponding to the rows color-coded gray), and

790

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

we wish to know if users from RTS16 and RTS17 are sampled from the same underlying distribution. Typically, Pearson's  2 test of homogeneity is the appropriate statistical test to apply, but our sample size (particularly RTS16) is smaller than the threshold where the  2 test is generally considered reliable. Instead, we applied Fisher's exact test, which indicates that we can reject the null hypothesis (p < 0.01) that the RTS16 and RTS17 observations are drawn from the same distribution. The biggest difference between these evaluations is the delivery mechanism (push vs. pull), although in Section 7 we discuss and rule out some possible confounds.
5 INDIVIDUAL-LEVEL ANALYSES
The previous section focused on population-level analyses of user behavior, where we are able to broadly characterize the effects of push vs. pull delivery. Clustering the delay distributions allowed us to identify three distinct types of user response profiles. However, such a coarse-grained analysis does not give us insight into what users are actually doing. In this section, we examine in detail the behavior of a few users, guided by the previous analyses.
5.1 Early-Heavy Distributions
In Figure 7 we show details of two users from the early-heavy distribution cluster: RTS16-03y (top) and RTS17-NAY (bottom). We refer the reader to the cluster dendrogram in Figure 6 to see how these two users relate to each other and the other users.1
The timeline scatterplots visualize all updates that each user consumed, i.e., each dot represents a judgment. Time (in UTC) runs along the y axis, from the top and flowing downward, measured in days from the beginning of the evaluation period. The x position indicates the response delay. The dots are superimposed on top of the delay distribution. As with all previous analyses, the distribution is divided into 13 buckets, one for each hour up to 12, and the final one for 12+ hours. In other words, the histogram shows exactly the same information as each row in Figure 6 (the cluster dendrogram) and each column in Figure 4 (the heatmap).
In these timelines, each horizontal "run" of blue dots represents a session with our mobile assessment app, where the width of each run indicates the number of judgments provided in that session. It is not immediately obvious why the sessions have time gaps, since users always consume updates in reverse chronological order and cannot encounter older updates unless they have assessed all earlier ones (by design). Such gaps arise when users consume many updates in a session and "overrun" their previous sessions temporally. This is illustrated in Figure 8, where the small squares represent delivered updates arranged on a timeline. At time A (the red block arrow), the user examined five updates, marked in yellow. The user then returned at time B (the green block arrow) and examined all the updates colored blue. After that, the interface would show older updates (shown in tan) and if the user continued to assess those updates, this "jump" in the age of the updates would show up as a gap in the response delays.
For user RTS16-03y (Figure 7, top), we see many frequent but short sessions. This user had a response rate of 46% (the highest),
1Our clustering analysis identified RTS17-xul and RTS17-uQu as more similar to the RTS16 users. However, these users supplied relatively few judgments, and thus we opted to present a user who was more engaged.

# Response

Time

Day 0

RTS16-03y

Day 1

Day 2

Day 3

Day 4

Day 5

Day 6

Day 7

Day 8

Day 9

Day 10 0 2 4 6 8 10 12 14 16 18 20 22 Response Time (hours)

Day 0

RTS17-NAY

Day 1

Day 2

Day 3

Day 4

Day 5

Day 6

Day 7

3000 2500 2000 1500 1000 500 0
3500 3000 2500 2000 1500 1000 500

# Response

Time

Day 8

0

0 2 4 6 8 10 12 14 16 18 20 22

Response Time (hours)

Figure 7: Two users with early-heavy delay distributions, RTS16-03y (top) and RTS17-NAY (bottom): frequent but (relatively) short sessions.

Time

A

B

Figure 8: Hypothetical example showing how "gaps" may arise in the timeline scatterplots shown in Figure 7.

subscribed to 10 profiles, and had a volume of 7141 updates. Nearly all updates were consumed within an hour after system delivery (in most cases, well within an hour); the user did not consume any updates older than two hours. It appears that the user was reacting quickly to the push notifications. However, because of the frequent interruptions, the sessions were never very long.
User RTS17-NAY (Figure 7, bottom) shares many similarities with RTS16-03y in having many short sessions (note that the x axis has the same scale). This user subscribed to 59 profiles and had a response rate of 44% on a volume of 13.9k updates. The vast majority of updates were consumed within two hours, which yields an early-heavy delay distribution. For this reason, our clustering analysis identified these two profiles as similar. While there are a number of longer sessions, their visual prominence in the timeline makes them seem more frequent than they actually are. From the

791

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Time # Response
Time # Response

Day 0

RTS17-a5j

350 Day 1

300 Day 2

250 Day 3

Day 4

200

Day 5

150

Day 6

100

Day 7

50

Day 8

0

0 4 8 12 16 20 24 28 32 36 40 44 48

Response Time (hours)

Figure 9: A user with a late-heavy delay distribution from RTS17: few but long sessions.

histogram, we see that there is relatively little probability mass in the 12+ bucket, so it is accurate to say that this user engaged in predominantly short sessions.
It is worth emphasizing that RTS17-NAY behaved as described without push notifications! The user visited the mobile app frequently to consume system updates without any prompting. Because the sessions were not triggered by interruptions (i.e., push notifications), the delays were longer, but the user was also more likely to read "deeply" for longer periods of time.
5.2 Late-Heavy Distributions
At the other end of the delay distribution spectrum, we see lateheavy distributions as a coherent and distinct cluster of users (the purple-coded users in Figure 6). One example selected from this group, user RTS17-a5j, is shown in Figure 9. The timeline visualization takes exactly the same form as above, but note the different time scale on the x axis. This user subscribed to five profiles and had a response rate of 85% on a volume of 1068 updates.
Here, we see far fewer sessions--in this case, averaging about once a day. However, these sessions were all quite long. That is, the user was engaging with our mobile assessment app and "deeply" reading system updates over relatively long spans of time. The result was that all the delays were quite long, and in fact, there were only updates with short delays because the user must examine newer updates before encountering older updates.
Many other users in this cluster exhibit similar behavior patterns. In fact, we see several examples of users engaging in very few (e.g., three) sessions with long observed delays. Note that if a user is encountering updates several days old, then we are operating outside the intended design space of RTS systems--and the assessment behavior is not very different from batch-style evaluation by TREC assessors. We observe that all late-heavy distributions are from pull users (RTS17). Although we had fewer users in the push condition (RTS16), we find this absence striking (more discussion below).
5.3 Bimodal Distributions
In addition to early-heavy and late-heavy distributions, which are in some ways not surprising, our clustering analysis also revealed

Day 0 Day 1 Day 2 Day 3 Day 4 Day 5 Day 6 Day 7

RTS17-xWp

1600 1400 1200 1000 800 600 400 200

Day 8 0

0 6 12 18 24 30 36 42 48 54 60 66
Response Time (hours)

Figure 10: A user with a bimodal delay distribution from RTS17: a combination of long and short sessions.

bimodal distributions--this can be clearly seen in the blue-coded block in Figure 6. From this group we show the timeline for user RTS17-xWp in Figure 10. This user subscribed to 14 profiles and had a response rate of 98% on a volume of 4515 updates.
Remarkably, from the timeline we see both short frequent sessions and periodic long sessions. Once again, we emphasize that this user did not receive push notifications, so the user was returning to the mobile app on the user's own initiative. This was happening frequently in short sessions where only a few updates were consumed at a time. However, the user also engaged in long, "deep reading" sessions that led to consumption of old updates, much like in the late-heavy distributions. For this reason, we see a clear bimodal distribution in delays.
6 SYNTHESIS AND RECOMMENDATIONS
Thus far, we have answered (RQ2): Yes, there are indeed distinct and coherent patterns of user behavior. At a high level, we see earlyheavy, late-heavy, and bimodal distributions of response delays. Users under the pull condition can exhibit any of these patterns, but push condition users are all early-heavy. Next, we synthesize our findings, provide an explanation of the observed behavior, and attempt to operationalize design recommendations for developers of real-time summarization systems (RQ3).
We hypothesize that users' information behavior with respect to real-time updates can be characterized as eager or apathetic. Eager users desire the most up-to-date information--they want updates now. We imagine an eager user as someone who might also compulsively check email. In contrast, apathetic users desire relevant information, but they don't really care about the recency of the updates delivered to them. We believe that eagerness or apathy is an intrinsic user characteristic, and such notions are not foreign in the literature. For example, Järvelin and Kekäläinen talk about patient and impatient users in the formulation of nDCG [10] and these constructs are captured in subsequent work on evaluation metrics (e.g., RBP [15]) and user models (e.g., [3]).
Within our framework, observed user behavior is the product of intrinsic user characteristics and system treatment. At a high level, we draw the following conclusions:

792

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Pull-based delivery allows users to manifest their intrinsic behavior. The critical point here is that the users engage the system for updates on their own initiative: whenever they want and for how ever long they please. Eager users still "check in" frequently (e.g., RTS17-NAY, shown on the bottom of Figure 7), even without system interruptions. On the other hand, apathetic users "check in" less frequently, but in each session they are prepared to engage with system updates and read "deeply". Finally, bimodal users exhibit a mixture of eager and apathetic behavior--they check in frequently and read a bit, but then come back periodically to catch up on updates they may have missed.
Push-based delivery suppresses user initiative. Why don't we observe any late-heavy or bimodal distributions in the push condition? Admittedly, our sample size is small, but this asymmetry is striking and statistically significant. We believe that this is because constantly bombarding the user with push notifications causes fatigue and suppresses any additional motivation to open up our app and engage with content on the user's own initiative. As a result, we observe only short (but frequent) sessions--none of the users in the push condition exhibit "deep" reading behavior, where the user engages with system updates for long periods of time.
From these two findings we can come up with design recommendations for system developers. From the system's perspective, the designer has two "knobs" to control: the volume of updates, which is directly related to algorithms and users' needs, and the delivery mechanism (push vs. pull). We return to discuss volume in Section 7, but for now let us hold this factor constant. The system developer typically wishes to shape what industry calls "engagement", which is operationalized in this context as response rate and delay (the two user response variables). At a high level, we already know (from Section 4), that push notifications trade off a lower response rate for shorter delays (i.e., getting information to users faster). With respect to engagement, we offer the following design recommendations, thus answering (RQ3):
For apathetic users, push notifications are unlikely to affect user behavior in terms of increasing engagement. In other words, if users fundamentally don't care about receiving up-to-date information, then no amount of A/B testing of delivery algorithms is going to make a difference. Developers should just leave such users alone. In fact, trying to engage apathetic users with push notifications does double damage in that (1) users will simply ignore or turn off notifications and (2) users will become less likely to engage on their own initiative. Such users are easy to identify, and system developers should simply exclude them altogether from attempts to optimize for push notification engagement.
For apathetic users, consider other modalities of update delivery. Since most updates encountered by late-heavy users are more than 12 hours old, daily email digests might be a more appropriate mechanism for delivering a batch of updates for deep reading. This is also explored in the RTS Track in "Scenario B" [12, 13].
For eager users, push notifications can decrease response delays, but developers must be aware of the costs. Despite similarities between early-heavy users from RTS16 and RTS17, users receiving push notifications still exhibit shorter delays overall. It is believable that eager users are less annoyed by push notifications and more likely

to pay attention to interruptions, and so push notifications can be an effective system intervention if shorter delays are desired.
However, the utility (from the user's perspective) of reducing delays for eager users is questionable, other than perhaps emergency scenarios (i.e., incoming ballistic missile) or a few specialized needs (e.g., information about a stock). For users who frequently check our mobile app, they will already encounter the updates the next time they "check in". Furthermore, as we argue above, there are costs to constant interruptions in terms of suppressing user initiative. For the designer of a production system, it is possible to operationalize this design principle by weighing the urgency (i.e., temporal utility of the update) against the mean time between user-initiated sessions.
We note that these design recommendations run somewhat counter to how modern social media organizations optimize engagement, which can be characterized as "more, more, more". For users with already high engagement, there is a tendency to want to push that engagement even higher. The thinking goes, "if the user is logging in every two hours, can we find additional mechanisms (e.g., push notifications) to get the user to log in every hour?" Rarely is consideration given to different types of users who may have different information consumption behaviors. This relates to how user buckets are selected in an A/B test: a random sample, for example, conflates users who may be fundamentally different. Increasing stimuli and treating users like a homogeneous population of lab rats provide little value to the user, and may be self-defeating from the system's perspective. For push notifications, we argue that interruptions take away users' motivation to engage with the system on their own initiative.
7 LIMITATIONS AND FUTURE WORK
Due to their complexity, all user studies exhibit methodological imperfections, and ours is no different. Nevertheless, TREC still remains the gold standard in multi-participant IR evaluations today, and we as co-organizers have adopted all evaluation best practices. The biggest difference between the 2016 and 2017 evaluations was the delivery mechanism (push vs. pull), but other differences are worth discussing:
· The evaluations used different interest profiles and examined different systems (although there were a number of teams that participated both years). In both evaluations, there were sufficient interest profiles to choose from such that users were likely interacting with topics they were interested in. Looking through TREC system papers, we did not notice any significant differences in algorithmic approaches across both years. Furthermore, due to the broker interleaving in delivering updates, users were seeing results from all systems in aggregate, and it is unlikely that there were significant differences in systems as a whole.
· The user studies recruited different participants each year. However, since our recruitment procedure did not substantively change and we were drawing from the same general student population, we are skeptical that this would be a source of major differences. Overall, we believe that our setup is comparable to a betweensubjects user study. Of course, within-subjects designs can lead to stronger conclusions, but the nature of the delivery mechanism makes such a design difficult.

793

Session 6D: Mobile User Behavior

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

· The two evaluations deployed different assessment interfaces; both were similar in terms of the overall flow, but graphical elements and other details differed. This difference was the direct result of needing push notifications in RTS16; without the requirement, we had greater flexibility for the RTS17 interface to simplify deployment with a web app. No doubt that interface design would have some effect on user behavior, but push vs. pull delivery remains the salient experimental manipulation.
While we cannot rule out that the above differences (or others) affect our findings, nothing jumps out as obviously suspect in our explanation of the large behavioral differences observed.
One obvious limitation of this work, as with most user studies, is the size of our user population. However, in comparison to typical academic user studies, our sample size is already quite large, considering the significant involvement of test subjects. Furthermore, this study was not possible without substantial investment from NIST and the framework of multi-participant evaluations over two years. We believe that our study already pushes the limits of scale for user studies that are realistically feasible in academia.
One direct implication of the user sample size is the strength of evidence supporting our findings, the most striking of which is that nearly all users in the push condition exhibit early-heavy behavior, whereas pull condition users exhibit three distinct profiles. Of course, it is difficult to prove the absence of a behavior, but the results of our significant testing (Section 4.3) do allow us to reject the null hypothesis that there are no significant differences between the push and pull conditions. Coupled with our explanation in terms of eager and apathetic information consumption behavior, we believe that our conclusions at minimum have face validity.
In answering (RQ1) and (RQ2), beyond our actual findings, we believe that our data analysis methodology (use of heatmaps, EDFs, clustering, etc.) is itself a contribution. This approach can be directly applied to analyze log data from production services (e.g., Twitter, news reading apps, etc.). With a much larger population of users, the next obvious step would be to fit a parametric family of models in order to characterize user populations (for example, Figure 5). We leave this for future work.
At a high level, our eager vs. apathetic categorization of users' information consumption behavior is of course too simplistic. While it serves as a good starting point, there are many factors to tease apart. System update volume and quality are two obvious issues (although note that volume and response rate vary independently, per analysis in Section 4.1): in our case, with a multi-participant evaluation, the mobile users receive interleaved results from all systems, which cover the entire spectrum of quality. It would be interesting future work to examine how users respond to relevant vs. non-relevant updates. This also factors into characteristics of user sessions, whose lengths would likely be influenced by how good system updates are.
Another interesting direction is to examine user motivation in more detail, which we could externally manipulate by the compensation for participation. Our studies paid subjects by the number of updates assessed, which incentivizes a large volume of judgments but provides no incentive for shorter delays. It might be interesting to modify compensation based on some function of delay or even a fixed compensation (regardless of users' actions).

Finally, we would like primarily observational user studies to inform the construction of well-specified user models that can then be applied to help developers iterate rapidly on different algorithms within requiring actual users. There is work on such models for ad hoc retrieval (e.g., [21]) and even formulations for streaming tasks [3], but we need richer models for prospective information seeking, as well as calibration and validation efforts.
8 CONCLUSIONS
This paper makes an initial foray into understanding how mobile users consume the output of real-time summarization systems. We have examined push vs. pull mechanisms, but ultimately, the best approach is likely to be a hybrid. Before we get there, however, there remain many unanswered questions for even simple setups. Our hope is that this work increases the community's interest in prospective information needs and spurs additional future work.
9 ACKNOWLEDGMENTS
This work was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada. We'd like to thank our RTS co-organizers M. Abualsaud, F. Diaz, N. Ghelani, R. McCreadie, D. Milajevs, A. Roegiest, and E. Voorhees for help running the evaluations, and all our mobile users for their participation.
REFERENCES
[1] J. Allan. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publishers, Dordrecht, The Netherlands.
[2] J. Aslam, F. Diaz, M. Ekstrand-Abueg, R. McCreadie, V. Pavlu, and T. Sakai. 2015. TREC 2015 Temporal Summarization Track Overview. TREC.
[3] G. Baruah, C. Clarke, and M. Smucker. 2015. Evaluating Streams of Evolving News Events. SIGIR. 675­684.
[4] N. Belkin and W. Croft. 1992. Information Filtering and Information Retrieval: Two Sides of the Same Coin? CACM 35, 12 (1992), 29­38.
[5] H. Dang. 2005. Overview of DUC 2005. DUC. [6] C. Grady and M. Lease. 2010. Crowdsourcing Document Relevance Assessment
with Mechanical Turk. NAACL HLT 2010 Workshop on AMT. 172­179. [7] Q. Guo, F. Diaz, and E. Yom-Tov. 2013. Updating Users about Time Critical Events.
ECIR. 483­494. [8] C. Hensley. 1963. Selective Dissemination of Information (SDI): State of the Art
in May, 1963. AFIPS Spring Joint Computer Conference. [9] E. Housman and E. Kaskela. 1970. State of the Art in Selective Dissemination of
Information. IEEE Transactions on Engineering Writing and Speech 13, 2 (1970), 78­83. [10] K. Järvelin and J. Kekäläinen. 2002. Cumulative Gain-Based Evaluation of IR Techniques. ACM Transactions on Information Systems 20, 4 (2002), 422­446. [11] D. Lewis. 1995. The TREC-4 Filtering Track. TREC. 165­180. [12] J. Lin, S. Mohammed, R. Sequiera, L. Tan, N. Ghelani, M. Abualsaud, R. McCreadie, D. Milajevs, and E. Voorhees. 2017. Overview of the TREC 2017 Real-Time Summarization Track. TREC. [13] J. Lin, A. Roegiest, L. Tan, R. McCreadie, E. Voorhees, and F. Diaz. 2016. Overview of the TREC 2016 Real-Time Summarization Track. TREC. [14] A. Mehrotra and M. Musolesi. 2018. Intelligent Notification Systems: A Survey of the State of the Art and Research Challenges. arXiv:1711.10171v2. [15] Alistair Moffat and Justin Zobel. 2008. Rank-Biased Precision for Measurement of Retrieval Effectiveness. TOIS 27, 1 (2008), Article 2. [16] X. Qian, J. Lin, and A. Roegiest. 2016. Interleaved Evaluation for Retrospective Summarization and Prospective Notification on Document Streams. SIGIR. 175­ 184. [17] S. Robertson and I. Soboroff. 2002. The TREC 2002 Filtering Track Report. TREC. [18] A. Roegiest, L. Tan, and J. Lin. 2017. Online In-Situ Interleaved Evaluation of Real-Time Push Notification Systems. SIGIR. 415­424. [19] A. Said, J. Lin, A. Bellogín, and A. de Vries. 2013. A Month in the Life of a Production News Recommender System. CIKM Workshop on Living Labs for Information Retrieval Evaluation. 7­10. [20] A. Schuth, K. Balog, and L. Kelly. 2015. Overview of the Living Labs for IR Evaluation (LL4IR) CLEF Lab 2015. CLEF. [21] M. Smucker and C. Clarke. 2012. Time-Based Calibration of Effectiveness Measures. SIGIR. 95­104.

794

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

An Axiomatic Analysis of Diversity Evaluation Metrics: Introducing the Rank-Biased Utility Metric

Enrique Amigó
NLP & IR Group at UNED Madrid, Spain
enrique@lsi.uned.es

Damiano Spina
RMIT University Melbourne, Australia damiano.spina@rmit.edu.au

Jorge Carrillo-de-Albornoz
NLP & IR Group at UNED Madrid, Spain
jcalbornoz@lsi.uned.es

ABSTRACT
Many evaluation metrics have been defined to evaluate the effectiveness ad-hoc retrieval and search result diversification systems. However, it is often unclear which evaluation metric should be used to analyze the performance of retrieval systems given a specific task. Axiomatic analysis is an informative mechanism to understand the fundamentals of metrics and their suitability for particular scenarios. In this paper, we define a constraint-based axiomatic framework to study the suitability of existing metrics in search result diversification scenarios. The analysis informed the definition of Rank-Biased Utility (RBU) ­ an adaptation of the well-known Rank-Biased Precision metric ­ that takes into account redundancy and the user effort associated to the inspection of documents in the ranking. Our experiments over standard diversity evaluation campaigns show that the proposed metric captures quality criteria reflected by different metrics, being suitable in the absence of knowledge about particular features of the scenario under study.
CCS CONCEPTS
· Information systems  Retrieval effectiveness;
KEYWORDS
Evaluation, Search result diversification, Axiomatic analysis
ACM Reference Format: Enrique Amigó, Damiano Spina, and Jorge Carrillo-de-Albornoz. 2018. An Axiomatic Analysis of Diversity Evaluation Metrics: Introducing the Rank-Biased Utility Metric. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3209978.3210024
1 INTRODUCTION
The development of better information retrieval systems is driven by how improvements are measured. The design of test collections and evaluation metrics that started with the Cranfield paradigm in the early 1960s allowed researchers to analyze the quality of different retrieval models in an automated and cost-effective way.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210024

Since then, many evaluation metrics have been proposed to measure the effectiveness of information retrieval systems [20, 22, 27].
Selecting a suitable set of metrics for a specific task is challenging. Comparing metrics empirically against user satisfaction or search effectiveness requires data that is often unavailable. Moreover, findings may be biased to the subjects, retrieval systems or other experimental factors.
An alternative consists of modeling theoretically the desirable properties of retrieval systems, as well as the abstraction of the expected users' behavior when performing a specific task. For instance, a metric that looks at how early the relevant document is retrieved in the ranking ­ such as Reciprocal Rank [26] ­ would be an appropriate metric to analyze the performance of systems on a single-item navigational task. However, is often challenging to come up with the proper evaluation tools for more complex search scenarios, as is the case of search result diversification [19]. In this context, the ranking of retrieved documents must be optimized in such a way that diverse query aspects are captured in the first positions. The challenge is that the evaluation of system outputs is affected by multiple variables such as: the deepness of ranking positions, the amount of documents in the ranking related to the same query aspect, relevance grades, the diversity of query aspects captured by single documents or the user's effort when inspecting the ranking.
Axiomatic analysis has been shown to be an effective methodology to better understand the foundamentals of evaluation metrics [3, 4, 10, 25]. In the context of evaluation, axiomatic approaches consist of a verifiable set of formal constraints that reflect which quality factors are captured by metrics, facilitating the metric selection in specific scenarios. To our knowledge, there is no comprehensive axiomatic analysis of the behavior of diversity metrics in the literature. This paper provides a set of ten formal constraints that focus on both retrieval and diversity quality dimensions.
We found that every constraint is satisfied at least by one metric. However, none of the existing diversity metrics satisfy all the proposed constraints simultaneously. In order to solve this gap, we define the metric Rank-Biased Utility (RBU) by integrating components from different metrics in order to capture every formal constraints. RBU is an adaptation of the well-known Rank-Biased Precision metric [16] that incorporates redundancy and the user's effort associated to the inspection of documents in the ranking. Our experiments using standard diversity test collections validate our axiomatic analysis. Results show that, satisfying every constraint with a single metric leads to unanimous evaluation decisions when compared against other existing metrics, i.e., RBU captures quality criteria which are reflected by different metrics. Therefore, this metric offers a solution in the absence of knowledge about the specific

625

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

characteristic of a diversity-oriented retrieval scenario. Moreover, the theoretical framework presented in this paper helps to decide which metric should be used.
The paper is organized as follows. Section 2 describes related work on evaluation of evaluation metrics. Section 3 introduces the formal constraints that we propose to analyze relevance and diversity properties of metrics. Section 4 provides a comprehensive analysis of existing diversity metrics according to these constraints and Section 5 defines the proposed RBU metric. Section 6 details the results of our experiments. Finally, Section 7 concludes the work.
2 RELATED WORK
There is no consensus of meta-evaluation criteria for search result diversification. Some works inherit meta-evaluation criteria from ad-hoc metrics such as sensitivity to system differences [11, 14, 17, 18]. This methodology however does not give information about to what extent metrics capture diversity properties. Smucker and Clarke [21] studied the correspondence between metric scores and user effort when exploring document rankings. This methodology has the advantage of being realistic ­ effort is calibrated from historical log data ­ but only focuses on partial quality aspects.
Most of works on diversity metrics are supported by descriptive analysis. In 2008, Clarke et al. [7] meta-evaluated -nDCG by analyzing the effect of modifying the diversity parameter  under different datasets. One year later, Agrawal et al. [1] checked the intent-aware scheme for diversification by studying the evaluation results of three search engines. Clarke et al. [8] proposed Noveltyand Rank-Biased Precision (NRBP), an extension of RBP [16] for diversification, joining properties of the original RBP metric, nDCG and intent-aware metrics. In 2010, Sakai et al. [17] compared their proposed approach to -NDCG and NRBP, in terms of metric agreement under different parameters. The authors considered some meta-evaluation criteria such as interpretability, computability or capability to accommodate graded relevance and score ranges. Three years later, Chandar and Carterette [5] evaluated their approach by studying correlation with previous metrics while reflecting other ranking quality issues. Luo et al. [14] proposed the Cube Test metric. They studied the effect of the metric parameters under synthetic system outputs, in the same manner than Clarke et al. [7]. Tangsomboon and Leelanupab [23] in 2014 and also Yu et al. [31] in 2017, supported their proposed metrics in terms of agreement and disagreement with previous metrics.
Not many works define a way of quantifying the suitability of metrics to capture diversity. An exception is the work by Golbus et al. [11] who defined Document Selection Sensitivity. This metameasure reports to what extent metrics are sensitive to document rankings containing relevant documents but different grades of diversity. Within this line, we define in this work Metric Unanimity (MU), which quantifies to what extent a metric is sensitive to quality aspects captured by other existing metrics.
On the other hand, metrics have been successfully analyzed in terms of formal constraints in ad-hoc retrieval scenarios [3, 10, 15]. The axiomatic methodology consists of identifying theoretical situations in which metrics should behave in a particular manner. This methodology has several strengths: it is objective, independent from datasets and it facilitates the interpretation of metrics. We

found only a few initial works in the context of formal constraints for search result diversification. For instance, Leelanupab et al. [13] reviewed the appropriateness of intent-aware, stating an extreme particular situation in which ERR-IA does not behave as expected. In our work, we meta-evaluate existing metrics on the basis of ten constraints that formalize desirable properties for ranking and diversity effectiveness.

3 AXIOMATIC CONSTRAINTS

3.1 Problem Formalization

We formalize the output of a document retrieval system as an or-
dered list of documents d = (d1, . . . , dn ) of length n, extracted from a collection of documents D. In order to express formal constraints,
we use dij to denote the result of swapping documents between positions i and j. Likewise, ddd denotes the result of replacing the document d with the document d  in the ranking d.

For search result diversification, we consider a set of query as-

pects T = {t1, . . . , tm }. For instance, users searching for a restau-

rant may be interested in the menu, the offers, opening times, etc.

Each aspect has an associated weight w (tj ) and the sum of all aspect

weights adds up to 1:

m j =1

w (tj

)

=

1.

On the other hand, r (di , tj )  [0 . . . 1] represents the graded

relevance of document di to the aspect tj . We assume the user's be-

havior follows the cascade model, i.e., the user inspects the ranking

sequentially from the top to the bottom, until either (i) the user's

information needs get satisfied or (ii) the user stops looking (i.e.,

user's patience is exhausted). Following the same user model than

the one used by Expected Reciprocal Rank [6], we consider rele-

vance as the suitability of the document to satisfy the user needs,

which has a negative correspondence with the probability of exploring more documents. Finally, we use Q (d) to denote the ranking

quality score, i.e., the score given by applying an evaluation metric Q to a given ranking d.

Our axiomatic approach consists of a set of ten formal constraints

that evaluation metrics may satisfy. These constraints are grouped

into two sets: relevance-oriented and diversity-oriented, that we

describe below.

In the definition of the constraints, we may refer to the follow-
ing conditions: single aspect (|T | = 1); balanced aspects (t  T (w (t ) = 1/|T |)); binary relevance (t, d (r (d, t )  {0, rc })); no aspect overlap (r (d, t ) > 0  t  t (r (d, t ) = 0)); and relevance contribution (r (d, t )  1). The last condition means that finding

new relevant documents about the same topic is always effective.

In other words, there is always room for new documents to fully

satisfy the user needs.

3.2 Relevance-Oriented Constraints
In order to isolate relevance from diversity and redundancy, for
these constraints we will assume single aspect and relevance contri-
bution. For the sake of legibility, we use the notation: r (d ) = r (d, t ). We
also denote drel and d¬rel as relevant and non-relevant documents, respectively. That is: i  1..n. r (di¬r el ) = 0 and r (dir el ) = rc . Under these assumptions, we import the five constraints proposed

626

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

by Amigó et al. [3] which capture previous axiomatic properties [10, 15].

Constraint 1 (Priority, Pri). Swapping items in concordance with their relevance increases the ranking quality score. Being k > 0:

r (di+k ) > r (di ) = Q dii+k > Q d

(1)

The next constraint is based on the intuition that the effect of relevance depends on the document ranking position. This constraint is also referred as top-heaviness:

Constraint 2 (Deepness, Deep). Correctly swapping contiguous items has more effect in early ranking positions:
r (di ) = r (dj ) < r (di+1) = r (dj+1) = Q dii+1 > Q djj+1 (2) where i < j.

The next constraint reflects that the effort spent by the user to inspect a long (deep) list of search results is limited. In other words, there is an area of the ranking that may never get explored by the user:

Constraint 3 (Deepness Threshold, DeepTh). Assuming binary relevance, there exists a value n large enough such that, retrieving only one relevant document at the top of the ranking is better than retrieving n relevant documents after n non-relevant documents:
n  N+. Q d1rel, . . . > Q d1¬rel, . . . , dn¬rel, d1rel, . . . , dnrel (3)

On the other hand, we can assume that there exists a (short) ranking area which is always explored by the user. In other words, at least a few documents are inspected by the user with a minimum effort. This means that, at the top of the ranking, the amount of captured relevant documents is more important than their relative rank positions.

Constraint 4 (Closeness Threshold, CloseTh). Assuming binary relevance, there exists a value m small enough such that retrieving one relevant document in the first position is worse than m relevant documents after m non-relevant documents:
m  N+. Q d1rel, . . . < Q d1¬rel, . . . , dm¬rel, d1rel, . . . , dmrel (4)

In some particular scenarios, however, this may not hold. For instance, in audio-only search scenarios, search results may be delivered sequentially one-at-a-time.
Finally, the amount of documents returned is also an aspect of the system quality. In the same manner that capturing diversity in the first positions is desirable, adding non-relevant documents to the end of the ranking should be penalized by metrics. In other words, the cutoff used by the system to stop returning search results has also an impact on users. Therefore, adding noise at the bottom of the ranking should decrease its effectiveness.

Constraint 5 (Confidence, Conf). Adding non-relevant documents decreases the score:

Q d > Q d, d¬rel

(5)

3.3 Diversity-Oriented Constraints
The first diversity-oriented constraint is related to the fact that the metric should be sensitive to the novelty of aspects covered by a single document:

Constraint 6 (Query Aspect Diversity, AspDiv). Covering

more aspects in the same document (i.e., without additional effort of

inspecting more documents) increases the score. Assuming relevance contribution (d, t : r (d, t )  1):

t  T . r (di, t ) > r (di , t ) = Q ddi di > Q d

(6)

To calculate the gain obtained by observing a new relevant doc-

ument in the ranking, most of the existing diversity metrics take

into account the number of previously observed documents that are

related with the same aspect. The more an aspect has been covered

earlier in the ranking, the less a new document relevant to this

aspect contributes to the gain. Formally:

Constraint 7 (Redundancy, Red). Assuming binary relevance, balanced aspects and no aspect overlap, and being d and d  documents relevant to different aspects r (d, t ) = r (d , t ) = rc , then:
|{di  d. r (di , t ) = rc }| > |{di  d. r (di , t ) = rc }| = (7)
Q d, d  > Q d, d

The Red constraint assumes binary relevance, by counting rele-
vant documents for each query aspect. In order to consider graded
relevance in previously observed documents, we can apply the
monotonicity principle. That is, if an aspect t is captured to a greater extent than a second aspect t  in every previously observed document, then the ranking is more redundant w.r.t. t than t . Formally:

Constraint 8 (Monotonic Redundancy, MRed). Assuming
two balanced aspects (T = {t, t }), relevance contribution, and being d and d  documents exclusively relevant to each aspect, 0 < r (d, t ) = r (d , t )  1 and r (d, t ) = r (d , t ) = 0:
di  d. r (di , t ) > r (di , t ) = Q d, d  > Q d, d (8)

Intuitively, as well as the exploration capacity or patience of the user is limited, the user's information need is also finite. This means that there should exists a certain point on which a new single piece of information completely satisfies user's information needs, in such a way that retrieving any other documents addressing the same query aspect is not beneficial. Formally:

Constraint 9 (Aspect Relevance Saturation, Sat). Assum-

ing no aspect overlap, there exists a finite relevance value rmax large

enough such that:

(r (dn, t ) = rmax )  (r (dn+1, t ) > 0) =

Q d  Q d, dn+1

(9)

Finally, the following constraint captures the relative weight of aspects w (t ) w.r.t. the user's information need:

Constraint 10 (Aspect Relevance, AspRel). Aspects with higher

weights have more effect in score of the ranking quality. Formally, assuming no aspect overlap, and being di and di documents that are relevant to different aspects that have not been observed before, j < i. r (dj , t ) = r (dj , t ) = 0, and r (di , t ) = r (di, t ) > 0 then:

w (t ) > w (t ) = Q ddi di > Q d

(10)

627

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

In summary, we have defined a total of ten constraints: five relevance-oriented constraints (Pri, Deep, DeepTh, CloseTh and Conf), and five constraints for search result diversification (AspDiv, Red, MRed, Sat, and AspRel). The next section provides an axiomatic analysis of the most popular retrieval and diversity metrics using these constraints.
4 METRIC ANALYSIS
In this section, we firstly analyze standard metrics designed to evaluate retrieval systems in non-diversified scenarios (i.e., singleaspect). Then we analyze the intent-aware family of metrics, as well as a number of popular diversity metrics.
4.1 Standard Metrics for Ad-hoc Retrieval
We analyze here metrics that do not consider multiple aspects of a query or topic, including: Precision at a cutoff k (P@k), Reciprocal Rank (RR) [26], Average Precision (AP), Rank-Biased Precision (RBP) [16], Expected Reciprocal Rank (ERR@k) [6] and Normalized Discounted Cumulative Gain (nDCG@k) [12].
RBP uses a parameter p that defines user's patience, modeled as the probability of the user to inspect the next document in the ranking. P@k, ERR and nDCG include a cutoff k that limits the rank positions considered in the evaluation measurement.1 The upper part of Table 1 summarizes the properties for the retrieval effectiveness metrics.
The constraints defined by Amigó et al. [3] assume that relevance judgments are binary. However, our axiomatic framework defines the constraints Pri and Deep over graded relevance (Eq. 1 and 2, respectively). Therefore, RR, AP and P@k become undefined.2
The rest of the analysis is inline with the one presented by Amigó et al. [3]: The other metrics (nDCG@k,ERR@k and RBP) satisfy Pri and Deep constraints by applying a relevance discounting factor depending on the depth of the ranking position. With regards to DeepTh (Eq. 3) and CloseTh (Eq. 4) constraints, metrics that rewards relevance in deep ranking positions such as AP or nDCG@k satisfy CloseTh but not DeepTh, while metrics that focus on the top of the ranking (P@k, RR and ERR@k) satisfy DeepTh but not CloseTh. RBP satisfies both CloseTh and DeepTh. The reason is that RBP is supported by a probabilistic user behavior model that takes into account the limitations of the ranking exploration process (i.e., user's patience). None of these metrics satisfy Conf.
This family of metrics are not applicable in the context of multiple query aspects. Therefore, they do not satisfy the diversityoriented constraints.
4.2 Intent-Aware Metrics
The intent-aware scheme [1] extends standard metrics such as AP or ERR to make them applicable to diversification scenarios. Firstly, each query aspect is evaluated independently and then a weighted average considering query aspect weights is computed. Being Mt (d) the score of d according to the metric M when only the relevance
1 Due to lack of space, here we focus on the formal properties of the metrics and we provide references to the definition and explanation of the metrics. 2Amigó et al. [3]'s analysis shows that P@k does not satisfy the Pri and Deep constraints, given that it does not consider the order of documents before position k .

to aspect t is considered:
M-IA(d) = w (t )Mt (d)
t T
The central part of Table 1 includes the properties for the intentaware version of the metrics discussed before. Intent-aware metrics converge to the corresponding standard effectiveness metric when the query has only one aspect. Consequently, they inherit the properties of the original metric over the relevance-oriented constraints Pri, Deep, DeepTh and CloseTh.
Let us now analyze the diversification-oriented constraints. Besides AP-IA@k, RR-IA and P-IA@k, which are undefined in the context of graded relevance judgments, the intent-aware metrics nDCG-IA@k, ERR-IA@k and RBP-IA satisfy the AspDiv constraint. If a document is relevant for several aspects, then the averaged score across query aspects increases.
Most of metrics do not satisfy Red and MRed. In the case of P-IA@k, the precision averaged across aspects in a certain cutoff k is independent from to which particular aspect the documents are relevant to.3 RR-IA@k neither satisfies Red given that is sensitive only to the first relevant document for each query aspect. In the case of AP-IA@k, the relevance contribution of a document to the aspect t is higher if relevant documents for t have been observed earlier in the ranking.4 nDCG-IA@k and RBP-IA also fail to satisfy the Red constraint. These two metrics are not sensitive to the relevance of previously observed documents. The contribution of documents depends on the rank position and the amount of relevant documents in the collection.
On the other hand, the metric ERR-IA@k satisfies both Red and MRed, due to the component j <i (1 - r (dj , t )) which estimates the probability of the user to be satisfied by previously observed documents according to graded relevance levels.
The Sat constraint is not satisfied by P-IA@k, AP-IA@k, nDCGIA@k nor RBP-IA. The reason is that all these metrics reward new relevant documents regardless the the gain obtained by previous observed documents. However, the saturation relevance for RRIA@k and ERR-IA@k is 1. Finally, the AspRel constraint by all the intent-aware metrics analyzed in this work, given that they all consider the first relevant document for each aspect in the ranking and all of them consider aspect weights w (t ).
4.3 Other Diversity Metrics
Besides the intent-aware metrics (M-IA), other metrics have been proposed to evaluate the effectiveness of search result diversification systems [19]. Zhai et al. [32] proposed Subtopic Recall (SRecall@k), which measures the number of aspects captured in the first k positions. Given that the metric only measures the coverage of aspects, does not satisfy Pri, Deep, CloseTh and Conf relevanceoriented constraints. The only diversity oriented constraint that

3
For

instance,

being

ni

the

amount

of

relevant

documents

for

the

aspect

ti

,

the

average

P@k

across aspects is:

1 |T |

ti T

ni k



ti T ni .

4The contribution of a relevant document in AP is proportional to the precision

achieved at the document's position, which is higher when relevant documents

appear in the previous positions. For instance, being Nr the fixed amount of rel-

evant documents for every aspect in the collection, and being dt , dt two doc-

uments related with aspect t , and dt a document related with aspect t  then:

AP-IA@2(dt ,

dt

)

=

1

1 Nr

+

1

2 Nr

>

1

1 Nr

+

1 2

1 Nr

= AP-IA@2(dt , dt )

628

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Properties ( = constraint satisfied, = constraint not satisfied) of existing retrieval and diversity effectiveness metrics.

Metric

Relevance-Oriented Constraints Pri Deep DeepTh CloseTh Conf

P@k RR
AP nDCG@k ERR@k RBP

P-IA@k RR-IA@k AP-IA nDCG-IA@k ERR-IA@k RBP-IA

S-Recall@k S-RR@100%
NRBP D#-Measure@k  -nDCG@k EU CT@k

RBU@k

Diversity-Oriented Constraints AspDiv Red MRed Sat AspRel

satisfies is Sat, given that S-Recall@k considers only the first relevant document for each query aspect and it does not consider aspect weights. Likewise, the metric S-RR@100% ­ an extension to RR also proposed by Zhai et al. [32], defined as the inverse of the rank position on which a complete coverage of aspects is obtained ­ satisfies the same properties as S-Recall@k.
Clarke et al. [7] proposed Novelty-Biased Discounted Cumulative Gain (-nDCG@k).5 This metric is defined as:

k
-nDCG@k (d) =
i =1

t T r (di , t )(1 -  )c (i,t ) log(i + 1)

where c (i, t ) represents the amount of documents previously ob-

served that capture the aspect t. Similarly to the original nDCG,

it satisfies Pri, Deep and CloseTh constraints. However, unlike
nDCG, DeepTh is also satisfied due to the redundancy factor (1 -  )c (i,t ), which also allows to satisfy Red. AspDiv is satisfied due to the additive relevance across aspects. In contrast, -nDCG@k

does not satisfy the constraints MRed and Sat. The reason is that the redundancy component (1 -  )c (i,t ) does not consider the rele-
vance grade of previously observed documents. Finally, this metric

does not consider the weight of aspects and therefore AspRel is

not satisfied.

Clarke et al. [8] proposed Novelty- and Rank-Biased Precision

(NRBP), and adaptation of RBP for search result diversification,

defined as:


NRBP(d) = pi-1

r (di , t )(1 -  )c (i,t )

i =1

t T

Similarly to the original RBP, NRBP satisfies all relevance-oriented

constraints except Conf, given that only relevant documents affect

the score. In terms of diversity-oriented constraints, NRBP behaves

5Note that given that the proposed formal constraints and experiments in this work
compare metrics at topic (or query) level, the normalization factor in metrics such as  -nDCG@k can be ignored.

similarly to -nDCG@k given that diversification is modeled in a similar manner. Sakai and Song [18] proposed the D#-Measure which combines a D-Measure (e.g., D-nDCG [17]) with the ratio of aspects captured in the first k positions (modeled by S-Recall@k):

D#-Measure@k (d) =  · S-Recall@k (d) + (1-) · D-Measure@k (d)

NRBP inherits the properties from nDCG-IA@k, which already satisfies DeepTh and AspRel. Therefore, the S-Recall@k component does not contribute with any additional constraint satisfaction.
None of previous metrics satisfy Conf. However, there exist in the literature utility-oriented metrics that penalyze non-relevant documents at the end of the ranking. Two examples are the Normalized Discounted Cumulated Utility (nDCU) [30], and the generalized version Expected Utility (EU) [29]. EU is very similar to -nDCG@k (d) but includes a cost factor. Being e the estimated effort for accessing one document, EU can be expressed as:

|d|

EU(d)

=

i =1

1

+

1 log(i )

r (t )r (di , t )(1 -  )c (i,t ) - e
t T

EU inherits the -nDCG@k (d) properties, but capturing AspRel

and Conf. However EU does still not satisfy MRed and Sat.

The Cube Test metric (CT@k) [14] satisfies Sat by adding a

saturation factor. Assuming a linear time effort w.r.t. the amount of

inspected documents, CT@k can be expressed as:

|d|

CT@k (d)

=

i =1

1 i

t T

r (t )r (di , t )(1

-  )c (i,t ) fSat

where fSat is 0 or 1 depending if the sum of relevance of documents for the aspect exceeds a certain saturation level. The reciprocal rank
1
discounting factor i affects the constraint CloseTh, rewarding the positions of documents over the amount of relevant documents

in top area. In addition, Conf is neither satisfied. There is no con-

tribution or penalty for documents with zero relevance.

629

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1 also includes the proposed metric Rank-Biased Utility (RBU), which we describe below.

5 RANK-BIASED UTILITY

The quality of a diversified ranking depends (at least) on the follow-

ing factors: (i) the position of relevant documents in the ranking;

(ii) the redundancy regarding each of the aspects covered by pre-

viously observed documents; (iii) the weights of the aspects seen

in the ranking and (iv) the effort ­ in terms of user cost or time ­

derived from inspecting relevant or non-relevant documents. The

analysis described in Section 4 shows that none of the existing met-

rics take into account all these factors. To fill this gap, we propose

Ranking-Biased Utility (RBU), which satisfies all the retrieval and

diversity-oriented formal constraints (see proofs in the appendix).

The analysis shows that RBP [16] is the only metric that satisfies

the four first relevance constraints, while ERR-IA@k [1, 6] is the

only metric that satisfies all the five diversity-oriented constraints.

Expected Utility (EU) is the only that satisfies Conf, capturing the

suitability of the ranking cutoff.

In order to satisfy every constraint, RBU combines the user ex-

ploration deepness model from RBP with the redundancy modeled in ERR-IA@k, and also adds the user effort component e in EU to

satisfy the Conf constraint. The metrics RBP and ERR-IA@k can be combined together un-

der the following user behavior assumptions: (i) The user has a

probability p to explore the next document and (ii) the user has a

probability r (dj , t ) to get gain from document dj for the topic t. Similarly to the ERR-IA@k, the probability of being satisfied by

document di after observing the documents that occur earlier in

the ranking is:

i -1
r (di , t ) (1 - r (dj , t ))
j =1

Analogously to the user model followed by RBP, the resulting contribution of a document di in the position i must be weighted according to pi :

i -1
pir (di , t ) (1 - r (dj , t ))
j =1

In order to satisfy AspRel, the weighted sum of contributions across aspects in T is:

i -1
pi w (t )r (di , t ) (1 - r (dj , t ))

t T

j =1

And the cumulative gain across rank positions until k is:

k

i -1

pi w (t )r (di , t ) (1 - r (dj , t ))

i=1 t  T

j =1

Similarly to EU, we define RBP in utility terms in order to capture

Conf. Being e the effort of observing a document, the rank biased

accumulated effort is weighted according to pi , that is:

k i =1

pi

e

.

Finally, combining the relevance contribution with the cumula-

tive effort, we obtain:

k
RBU@k(d) = pi

i -1
w (t )r (di , t ) 1 - r (dj , t ) - e (11)

i=1 t  T

j =1

RBU@k matches with the RBP-IA metric when assuming a zero effort (e = 0), and a small contribution of documents in terms of gain for query aspects,
i -1
r (di , t )  1 = 1 - r (dj , t )  1 =
j =1

RBU@k(d) = w (t ) pi-1r (di , t )1 - 0) = w (t ) RBPt (d)

t T

j i

t T

On the other hand, RBU@k is equivalent to the metric ERR-IA@k when the effort component is zero (e = 0), and the probability of exploring the next document is maximal (p = 1):

k

i -1

1i

w (t )r (di , t ) (1 - r (dj , t )) - 0 = w (t ) ERRt @k (d)

i=1 t  T

j =1

t T

We now discuss the role of the effort component e, which repre-
sents the cost inherently associated to inspect a new document in the ranking.6 For instance, if e = 0.1 and the inspected document di has a relevance of 0.1 to aspect ti , then the actual gain is zero:

r (di , t ) 1 - r (dj , t ) - e = 0.1 (1 - 0) - 0.1 = 0

j <i

j <i

We have introduced RBU@k and shown that the proposed metric satisfies all the relevance- and diversity-oriented formal con-

straints. The experiments described in the following sections compare RBU@k to other metrics in the context of standard evaluation campaigns for search result diversification.

6 EXPERIMENTS
We start defining our meta-evaluation metric. Then we evaluate the metrics in different scenarios based on the TREC Web Track 2014 adhoc retrieval task [9], which includes search result diversification. Finally, we corroborate our results under the context of the TREC Dynamic Domain task [28].7

6.1 Meta-evaluation: Metric Unanimity
We aim to quantify the ability of metrics to capture diversity in addition to traditional ranking quality aspects. For this purpose, we define the Metric Unanimity (MU). MU quantifies to what extent a metric is sensitive to quality aspects captured by other existing metrics. It follows a similar concept used by Strictness,8 proposed by Amigó et al. [3] for the ad-hoc retrieval scenario.
Our intuition is that, if a system improves another system for every quality criteria, this should be unanimously reflected by every metric. A metric that captures all quality criteria should reflect these improvements.
Considering the space of system output pair comparisons (i.e., Qd) > Q (d)), MU can be formalized as the Point-wise Mutual Information (PMI) between metric decisions and improvements
6In this work, the effort of inspecting or judging a relevant or non-relevant document is the same. We leave for future work the definition of formal constraints that consider these differences [21, 24]. 7Releasable data and scripts used in these experiments are available at https://github. com/jCarrilloDeAlbornoz/RBU. Diversity metrics and RBU are also included in the EvALL evaluation framework [2] http://evall.uned.es/. 8Strictness checks to what extent a metric can outscore metrics that achieve a low score according to other metrics.

630

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Metric Unanimity scores (MU) for the TREC Web Track 2014 ad-hoc retrieval task: official (Section 6.2) and simulated scenarios (Section 6.3). Given that normalization has not effect in terms of formal constraints and MU, which work at topic (query) level, normalized version of metrics behave similarly to the metric without normalization (e.g., MU(nDCG) = MU(DCG)) and therefore are not included.

Official

r (d ) = rand(0, r (d )) r (t ) = rand(0, r (t ))
|d| = rand(0, |d|)

Simulated Scenarios

r (d ) = rand(0, r (d )) r (t ) = rand(0, r (t ))
|d| = rand(0, 50)

RBUe= {0.001, 0.05, 0.1, 0.5 }, p=0.99  -DCG-IA@1000 ={0.1,0.25,0.5}
DCG@1000

0.8024 0.7956 0.7956

DCG-IA@1000

0.7956

EU ={0.1,0.25,0.5},e={0,0.05,0.1,0.5} 0.7956

ERR-IA@1000

0.7956

ERR@1000

0.7956

NRBPp={0.8,0.9,0.99}, ={0.1,0.25,0.5} 0.7956

AP

0.7926

AP-IA

0.7926

RBPp = {0.8, 0.9, 0.99 } P-IA@20

0.7911 0.7272

P@20

0.7192

RR-IA

0.6835

RR

0.6486

S-Recall@10

0.3965

S-Recall@20

0.3538

S-Recall@50

0.3065

S-Recall@100

0.2478

RBUe= {0.001, 0.05, 0.1, 0.5 }, p=0.99  -DCG-IA@1000 ={0.1,0.25,0.5,0.75}
DCG@1000

0.8568 0.7734 0.7734

DCG-IA@1000

0.7734

EU ={0.1,0.25,0.5,0.75},e={0,0.001,0.05,0.5} 0.7734

ERR-IA@1000

0.7734

ERR@1000

0.7734

AP

0.7734

AP-IA

0.7734

NRBPp={0.8,0.9,0.99}, ={0.1,0.25,0.5,0.75} 0.7734

RBPp=0.99

0.7717

P@{20,50}

0.7103

P-IA@{20,50}

0.7103

RR-IA

0.6704

RR

0.6082

S-Recall@10

0.4238

S-Recall@20

0.4084

S-Recall@50

0.3658

S-Recall@100

0.3007

RBUe= {0.001, 0.05, 0.1, 0.5 }, p= {0.8, 0.9, 0.99 }

0.9808

 -DCG-IA@{50,100,1000} ={0.1,0.25,0.5,0.75} 0.7709

DCG-IA@{50,100,1000}

0.7709

EU ={0.1,0.25,0.5,0.75},e={0,0.001,0.05,0.5} ERR-IA@{50,100,1000}

0.7709 0.7709

NRBPp={0.8,0.9,0.99}, ={0.1,0.25,0.5,0.75} DCG@{50,100,1000}

0.7709 0.7687

ERR@{50,100,1000}

0.7679

AP-IA

0.7642

AP

0.7627

RBPp = {0.8, 0.9, 0.99 } P-IA@20

0.7597 0.7077

P-IA@10

0.6888

RR-IA

0.6841

RR

0.6561

S-Recall@10

0.5137

S-Recall@20

0.4994

S-Recall@100

0.4831

S-Recall@50

0.4831

reported simultaneously by the rest of metrics. Formally, let be m a metric, M the rest of metrics, and a set of system outputs S. Being mi, j and Mi, j are statistical variables over system pairs (di , dj )  S2, indicating a system improvement according to the metric and to the rest of metrics, respectively: 9
mi, j  m(di ) > m(dj ) Mi, j  m  M. m(di )  m(dj )
Then MU is formalized as:

MUM, S (m) = PMI mi, j , Mi, j

= log

P (mi, j, Mi, j ) P (mi, j ) ·P (Mi, j )

Let us consider the following example illustrated by the Table below:

m1 m2 m3

S1 1 0.8 1 S2 0.5 0.3 0.2 S3 0.2 0.4 0.5

The example consists of three metrics and three system outputs.

We now compute the MU of the metric m1 regarding the rest

of metrics M = {m2, m3}. Here, there are 6 sorted pairs of sys-

tem outputs: (S1, S2),(S2, S1), (S1, S3), etc. The improvements re-

ported

by

m1

are:

m11, 2 ,

m11,

,
3

and

m12,

.
3

The

improvement

re-

ported simultaneously by the other metrics are: M1,2, M1,3, and

9The a priori probability of a system improvement for every metric is fixed P (mi, j ) =

1 2

.

That

is,

for

the

cases

on

which

two

system

outputs

obtain

the

same

score

m (di

)

=

m (dj ), we add 0.5 to the statistical count.

M3,2. m1 agrees with M in two cases. Therefore MUM (m1) =

log

2/6 3/6·3/6

= 0.415.

MU has four properties that we describe below.

Property 1. Capturing every unanimous improvement maximizes MU regardless the other decisions:

MUM, S (m) = log

P (mi, j , Mi, j ) 1 ·k

 P (mi, j , Mi, j )

2

Property 2. A metric mrand which assigns random or constant scores to every system outputs achieves a zero MU, capturing

the sensitivity of metrics:

MUM, S (mrand) = log

1 2

·

P (Mi, j )

1 2

·

P (Mi, j )

= log(1) = 0

Property 3. MU is asymmetric. A metric m can be unanimous

regarding the rest of metrics, while the rest of metrics are

not.

MU{m2,m3 } (m1) MU{m1,m3 } (m2) MU{m1,m2 } (m3)

Property 4. MU is not affected by the predominance of a certain family of metrics in the set M:

MUM{m }, S (m) = MUM{m,m, ...,m }, S (m)

6.2 Experiment 1: TREC Web Track 2014
This first experiments aims to measure MU in a standard diversification evaluation campaign: the TREC Web Track 2014 ad-hoc retrieval task [9]. In this benchmark, systems need to perform adhoc retrieval from the ClueWeb-12 collection, for a total of 50 test topics and return the top 10,000 documents. Some of the topics have

631

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

multiple aspects ­therefore, diversified rankings may be more effective. We use the 30 official runs submitted to the ad-hoc retrieval task and available at TREC's website.
Using our own implementation of the metrics, we execute over the official runs the following metrics: AP, RR, AP-IA and RR-IA which do not require any parameter; P@k, ERR@k, NDCG@k and their corresponding intent-aware variants, using k  {10, 20, 50, 100, 1000}; S-Recall@k, RBP, NRBP and -nDCG@k; with p  {0.8, 0.9, 0.99} and   {0.1, 0.25, 0.5, 0.75}; EU and our proposed metric RBU with the effort parameter e  {0.001, 0.05, 0.1, 0.5}.
For metrics that do not accept multiple query aspects, we consider the maximum relevance across aspects: r (d ) = maxt T (r (d, t )).
The first column in Table 2 shows the metrics ranked by MU. For the sake of clarity, the table includes for each metric the variant with highest MU. Results show that metrics that satisfy only a few constraints such as P@k or S-Recall@k are substantially less unanimous than the rest of metrics. This means that metrics with higher scores cover the same quality criteria captured by P@k or S-Recall@k, but these two metrics do not capture other criteria captured by the rest of metrics.
Our second observation is that a metric with a shallow cutoff (e.g., ERR@50) ­ i.e., it takes into account a few documents in the ranking ­ has lower MU score than its deep counterpart (e.g., ERR@1000). This behavior is consistent for every metric and variants. Likewise, higher values for the patience parameter p in RBP obtains higher MU scores. Intuitively, the shallower the metric is, the less probable is to capture improvements in deep ranking positions.
RBU obtains the highest scores, when p = 0.99 (i.e., the metric considers deep positions in the ranking) and all the tested values for the effort component e.
6.3 Experiment 2: Simulating Alternative Scenarios
In order to study the behavior of metrics under different situations and to corroborate our findings, we repeat the experiment described before after artificially modifying some parameters of the official TREC Web Track experimental setup.
The second column in Table 2 shows the results when:
(1) Enforcing all relevance judgments to be graded: we replace each discrete relevance value r by a random value between zero and r : r (d ) = rand(0..r (d )). This is related to the MRed constraint.
(2) Randomly assigning a certain weight to each aspect t in such a way that the sum of the weights for each topic (or query) adds up to 1: w (t ) = rand(0..1) and t T w (t ) = 1. This is related to the AspRel constraint.
(3) The ranking of documents returned by each system is manipulated by reducing randomly its length: |d| = rand(0, . . . , |d|). This variation simulates the situation in which systems should cut their output rankings according to their confidence of retrieving (or not) more relevant documents. This tuning is related to the Conf constraint, which is only satisfied by EU and the proposed metric.
As a result, the difference in terms of MU scores between RBU and the other metrics is larger in this simulated scenario. The experiment suggests that this effect is not due to the fact of satisfying any

single constraint, but satisfying several constraints simultaneously. Although EU satisfies Conf and ERR-IA@k satisfies MRed and Sat, RBU outperforms both metrics in terms of MU.
In all the previous experiments, we have seen that MU rewards the fact of considering deeper positions in the ranking. In order to isolate this variable, the next simulation (Table 2, third column) reduces the length of rankings substantially, by defining a random cutoff between 0 and 50: |d| = rand(0..50). Consequently, metrics that use a cutoff equal or greater than k = 50 will not be rewarded by MU. Remarkably, all the RBU variants with an effort parameter e higher than zero obtain the highest MU scores ­ RBU with e = 0 (omitted in the table) achieves a 0.7709 MU score.
This suggests that the effort component e plays an important role when evaluating rankings with different lengths.
6.4 Experiment 3: Considering Metrics and Default Parameters used in Official Evaluation
MU scores depend on the set of metrics in consideration. Therefore, the results could be biased by the selected metric set M and variants. In order to avoid this bias, we consider the official metrics and parameters used by the TREC Web Track organizers. In addition, to avoid the effect of implementation variations or bugs, we compare RBU (implemented by ourselves) against the official evaluation scores released by TREC (first column in Table 3).
In this case, AP-IA gets the highest MU score. In terms of RBU, we can see that p values and MU scores are correlated. This shows again than MU is biased by the the amount of documents in the ranking that are visible to the metric. Note that most of metrics proposed by the organizers use a cutoff no greater than k = 20. That is, most of metrics receive less information than AP-IA or NRBP, which take into account all the documents in the ranking.
In order to avoid this effect, we focus on metrics that apply the the cutoff k = 20, and we apply the same cutoff to RBU: RBU@2010 Maintaining the amount of documents visible to metrics constant, RBU achieves the same MU score (0.9556) for all the tested variants, obtaining the highest MU score among the metrics. This suggests that the RBU performance in terms of MU is not due to differences in the length of the observed ranking.
The high MU scores of RBU could be possibly due to the fact of having an explicit component for the user effort (e parameter), rather than the ability to capture other quality aspects such as diversity and redundancy. In order to isolate this variable, we consider only three RBU variants with zero value in the effort parameter (e = 0, p = {0.8, 0.9, 0.99}). Results at the bottom of second column in Table 3 show that RBU also outperforms the rest of metrics when e = 0.
6.5 Experiment 4: Validation using TREC Dynamic Domain Track
In order to check the robustness of our empirical conclusions, we repeat the same experiment over TREC Dynamic Domain 2015 [28], which includes 23 official runs. This track consists of an interactive
10In this experiment we use the official evaluation scores. Therefore, we cannot adapt AP-IA nor NRBP to this cutoff.

632

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: MU scores over official metrics in TREC Web Track 2014 and TREC Dynamic Domain Track 2015.

TREC Web Track 2014 (Official Metrics)

TREC Dynamic Domain 2015 (Official Metrics)

Official

k = 20

Official

AP-IA
RBUe= {0, 0.001, 0.05, 0.1, 0.5 }, p=0.99 RBUe= {0, 0.001, 0.05, 0.1, 0.5 }, p=0.9 RBUe= {0, 0.001, 0.05, 0.1, 0.5 }, p=0.8 { -DCG, -nDCG }@20

0.9771 0.9770..0.9766 0.9763..0.9760 0.9760..0.9750
0.9540

ERR-IA@20, nERR-IA@20

0.9539

NRBP, nNRBP

0.9509

{ ERR-IA, nERR-IA, -DCG, -nDCG }@10 0.9373

P-IA@20

0.9310

RBUe=, p=

0.9556

{ -nDCG, -nDCG }@20 0.9427

{ ERR-IA, nERR-IA }@20 0.9425

P-IA@20

0.9080

S-Recall@20

0.4141

k = 20, e = 0

RBUe= {0.001, 0.05, 0.1, 0.5 }, p=0.99 RBUe=0.001, p=0.9 RBUe= {0.05, 0.1 }, p=0.9 RBUe=0.5, p=0.9 RBUe=0.001, p=0.8 RBUe= {0.05, 0.1, 0.5 }, p=0.8 ACT@10
ERR (Arith. Mean)
CT@10

0.8488 0.8453 0.8441 0.8440 0.8406 0.8396 0.6276 0.5955 0.5938

P-IA@10 { -DCG, -nDCG }@5 { ERR-IA, nERR-IA }@5 P-IA@5 S-Recall@5 S-Recall@10 S-Recall@20

0.9071 0.9001 0.8999 0.8720 0.5573 0.5001 0.4515

RBUe=0, p= {0.8, 0.9, 0.99 } { -DCG, -nDCG }@20 { ERR-IA, nERR-IA }@20
P-IA@20
S-Recall@20

0.9556 0.9428 0.9425 0.9081 0.4146

RBUe=0, p= {0.8, 0.9, 0.99 } ERR (Harm. Mean) P@Recall P@Recall (modified) RR@10

0.5937 0.5912 0.1162 0.1044 0.1031

search scenario. Systems receive aspect-level feedback iteratively and need to dynamically retrieve as many relevant documents for aspects as possible, using as few iterations as possible. An important particularity of this task is that the system must predict the optimal ranking cutoff which is closely related with the Conf constraint. The official metrics used in this track are Cube Test (CT@k) and Averaged Cube Test (ACT@k) [14], which are included in our experiments.
The rightmost column in Table 3 shows that we obtain similar results: all the RBU variants are at the top of the metrics ranking. In this case, the user effort parameter e is important, given that it is necessary to outperform other metrics such as CT@k or ACT@k. In addition, we achieved again the same result when considering only one RBU variant, appearing at the top in terms of MU scores.
7 CONCLUSIONS
We defined an axiomatic framework to analyze diversity metrics and found that none of the existing metrics satisfy all the constraints. Inspired by this analysis, we proposed Rank-Biased Utility (RBU, Equation 11), which satisfies all the formal constraints. Our experiments over standard diversity evaluation campaigns show that the proposed metric has more unanimity than the official metrics used in the campaigns, i.e., RBU captures more quality criteria than the ones captured by other metrics. We believe our contributions would help researchers and analysts to define their evaluation framework (e.g., which evaluation metric should be used?) in order to analyze the effectiveness of systems in the context of scenarios involving search result diversification. Future work includes a further parameter sensitivity analysis of metrics, as well as the study of other meta-evaluation criteria such as sensitivity or robustness against noise.
Acknowledgments. This research was partially supported by the Spanish Government (project Vemodalen TIN2015-71785-R) and the Australian Research Council (project LP150100252). The authors wish to thank the reviewers for their valuable feedback.

REFERENCES
[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In Proc. WSDM. 5­14.
[2] Enrique Amigó, Jorge Carrillo-de Albornoz, Mario Almagro-Cádiz, Julio Gonzalo, Javier Rodríguez-Vidal, and Felisa Verdejo. 2017. EvALL: Open Access Evaluation for Information Access Systems. In Proc. SIGIR. 1301­1304.
[3] Enrique Amigó, Julio Gonzalo, and Felisa Verdejo. 2013. A General Evaluation Measure for Document Organization Tasks. In Proc. SIGIR. 643­652.
[4] Luca Busin and Stefano Mizzaro. 2013. Axiometrics: An Axiomatic Approach to Information Retrieval Effectiveness Metrics. In Proc. ICTIR. 8.
[5] Praveen Chandar and Ben Carterette. 2013. Preference Based Evaluation Measures for Novelty and Diversity. In Proc. SIGIR. 413­422.
[6] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In Proc. CIKM. 621­630.
[7] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Büttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proc. SIGIR. 659­666.
[8] Charles L. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. In Proc. ICTIR. 188­199.
[9] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and Ellen M Voorhees. 2015. TREC 2014 Web Track Overview. In Proc. TREC.
[10] Marco Ferrante, Nicola Ferro, and Maria Maistro. 2015. Towards a Formal Framework for Utility-oriented Measurements of Retrieval Effectiveness. In Proc. ICTIR. 21­30.
[11] Peter B. Golbus, Javed A. Aslam, and Charles L. A. Clarke. 2013. Increasing Evaluation Sensitivity to Diversity. Inf. Retr. 16, 4 (2013), 530­555.
[12] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Sys. 20 (2002), 422­446.
[13] Teerapong Leelanupab, Guido Zuccon, and Joemon M. Jose. 2013. Is IntentAware Expected Reciprocal Rank Sufficient to Evaluate Diversity?. In Proc. ECIR. 738­742.
[14] Jiyun Luo, Christopher Wing, Hui Yang, and Marti Hearst. 2013. The Water Filling Model and the Cube Test: Multi-dimensional Evaluation for Professional Search. In Proc. CIKM. 709­714.
[15] Alistair Moffat. 2013. Seven Numeric Properties of Effectiveness Metrics. In Proc. Asia Info. Retri. Soc. Conf. 1­12.
[16] Alistair Moffat and Justin Zobel. 2008. Rank-Biased Precision for Measurement of Retrieval Effectiveness. ACM Trans. Inf. Sys. 27, 1 (2008), 2:1­2:27.
[17] Tetsuya Sakai, Nick Craswell, Ruihua Song, Stephen Robertson, Zhicheng Dou, and Chin yew Lin. 2010. Simple Evaluation Metrics for Diversified Search Results. In Proc. EVIA. 42­50.
[18] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results Using Per-intent Graded Relevance. In Proc. SIGIR. 1043­1052.
[19] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2015. Search Result Diversification. Found. & Trends in IR 9, 1 (2015), 1­90.
[20] Falk Scholer, Diane Kelly, and Ben Carterette. 2016. Information Retrieval Evaluation Using Test Collections. Inf. Retr. 19, 3 (2016), 225­229.
[21] Mark D. Smucker and Charles L.A. Clarke. 2012. Time-based Calibration of Effectiveness Measures. In Proc. SIGIR. 95­104.

633

Session 5C: New Metrics

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

[22] Karen Sparck Jones and Cornelis J. van Rijsbergen. 1976. Information Retrieval Test Collections. J. Documentation 32, 1 (1976), 59­75.
[23] Ake Tangsomboon and Teerapong Leelanupab. 2014. Evaluating Diversity and Redundancy-Based Search Metrics Independently. In Proc. Aust. Doc. Comp. Symp. 42­49.
[24] Andrew Turpin, Falk Scholer, Kalvero Jarvelin, Mingfang Wu, and J. Shane Culpepper. 2009. Including Summaries in System Evaluation. In Proc. SIGIR. 508­515.
[25] Cornelis J. van Rijsbergen. 1974. Foundation of Evaluation. J. Documentation 30, 4 (1974), 365­373.
[26] Ellen M. Voorhees. 1999. The TREC-8 Question Answering Track Report. In Proc. TREC. 77­82.
[27] Ellen M. Voorhees and Donna K. Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. Vol. 1. MIT Press Cambridge.
[28] Hui Yang, John Frank, and Ian Soboroff. 2015. Overview of the TREC 2015 Dynamic Domain Track. In Proc. TREC.
[29] Yiming Yang and Abhimanyu Lad. 2009. Modeling Expected Utility of Multisession Information Distillation. In Proc. ICTIR. 164­175.
[30] Yiming Yang, Abhimanyu Lad, Ni Lao, Abhay Harpale, Bryan Kisiel, and Monica Rogati. 2007. Utility-based Information Distillation over Temporally Sequenced Documents. In Proc. SIGIR. 31­38.
[31] Haitao Yu, Adam Jatowt, Roi Blanco, Hideo Joho, and Joemon M. Jose. 2017. An In-depth Study on Diversity Evaluation: The Importance of Intrinsic Diversity. Inf. Proc. & Man. 53 (2017), 799­813.
[32] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In Proc. SIGIR. 10­17.

APPENDIX: FORMAL PROOFS

Proof. Rank-Biased Utility (RBU, Eq. 11) satisfies the constraints: Pri (Eq. 1), Deep (Eq. 2), DeepTh (Eq. 3) and CloseTh (Eq. 4). RBU

is defined as:

k

i -1

RBU@k(d) = pi

w (t )r (di ) (1 - r (dj, t )) - e

i =1 t T

j =1

In the context of these constraints, it is assumed that there is only a single aspect t for a given query or topic. Therefore, RBU can be expressed as:

k

i -1

RBU@k(d) = pi r (di ) (1 - r (dj, t )) - e

i =1

j =1

In addition, the condition relevance contribution is assumed, i.e., the rele-

vance of single documents does not completely cover the user information needs r (d )  1. Therefore, we can assume that

i -1

i -1

(1 - r (dj, t ))  1 = 1

j =1

j =1

Finally, the four constraints compare rankings with the same length. This

means that we can eliminate the user cost component e, which is e

k i =1

p

i

for every ranking in comparison. Under all these assumptions, RBU is

equivalent to the traditional RBP metric [16]:

k
RBU@k(d)  pi r (di ) = RBP@k(d)
i =1

According to the study by Amigó et al. [3], RBP satisfies the four constraints

enumerated above.

Proof. RBU satisfies the Conf constraint (Eq. 5). RBU can be expressed as:

k

i -1

k

RBU@k(d) = pi

w (t )r (di ) (1 - r (dj , t )) - e pi

i =1 t T

j =1

i =1

then

RBU@k d > RBU@k d, d ¬r el 

RBU@k(d) > RBU@k(d) - pn+1e  0 > -pn+1e

Proof. RBU satisfies the AspDiv constraint (Eq. 6). Under the

constraint conditions: RBU ddi di > RBU d is equivalent to:

i -1

i -1

pi

w (t )r (di, t ) (1 - r (dj , t )) > pi

w (t )r (di , t ) (i - r (dj , t )) 

t T

j =1

t T

j =1

w (t )r (di, t ) >

(w (t )r (di , t )) 

r (di, t ) >

(r (di , t )) 

t T

t T

t T

t T

t  T r (di, t ) > r (di , t )

Proof. RBU satisfies the Red constraint (Eq. 7). Under the constraint conditions:
RBU d, d > RBU d, d 

|d|

|d|

w (t )r (d, t ) (1 - r (dj , t )) > w (t )r (d, t ) (1 - r (dj , t )) 

j =1

j =1

|d|

|d|

(1 - r (dj , t )) > (1 - r (dj , t )) 

j =1

j =1

(1 - rc ) di d|r (di , t )=rc > (1 - rc ) d d|r (d, t )=rc 

di  d|r (di , t ) = rc > d  d|r (d, t ) = rc

Proof. RBU satisfies the MRed constraint (Eq. 8). Under the constraint conditions:
RBU d, d > RBU d, d 

|d|

|d|

w (t )r (d, t ) (1 - r (dj , t )) > w (t )r (d, t ) (1 - r (dj , t )) 

j =1

j =1

|d|

|d|

(1 - r (dj , t )) > (1 - r (dj , t ))  di  d. (r (di , t ) > r (di , t ))

j =1

j =1

Proof. RBU satisfies the Sat constraint (Eq. 9). There exists a relevance value r (dn, t ) = rmax = 1 large enough such that:

n

i -1

n

RBU d, dn+1 = pi

w (t )r (di ) (1 - r (dj , t )) - e pi +

i =1 t T

j =1

i =1

n-1

w (t )r (dn+1 )(1 - r (dn, t )) (1 - r (dj , t )) - epn+1

tT

j =1

Given that t  t (r (dn+1, t ) = 0), it is equivalent to:

n

i -1

n

RBU d, dn+1 = pi

w (t )r (di ) (1 - r (dj , t )) - e pi +

i =1 t T

j =1

i =1

n-1 w (t )r (dn+1 )(1 - r (dn, t )) (1 - r (dj , t )) - epn+1
j =1

Given that 1 - r (dn, t ) = 0, we obtain:

n

i -1

n

RBU d, dn+1 = pi

w (t )r (di ) (1 - r (dj , t )) - e pi + 0 = RBU d

i =1 t T

j =1

i =1

Proof. RBU satisfies the AspRel constraint (Eq. 10).

Under the constraint conditions:

RBU ddi di > RBU d 

i -1

i -1

w (t )r (d, t ) (1 - r (dj , t )) > w (t )r (d, t ) (1 - r (dj , t )) 

j =1

j =1

i -1

i -1

w (t )r (d, t ) 1 > w (t )r (d, t ) 1  w (t ) > w (t )

j =1

j =1

634

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Ranking Documents by Answer-Passage Quality

Evi Yulianti
RMIT University Melbourne, Australia

Ruey-Cheng Chen
SEEK Ltd. Melbourne, Australia

Falk Scholer
RMIT University Melbourne, Australia

W. Bruce Croft
RMIT University Melbourne, Australia
ABSTRACT
Evidence derived from passages that closely represent likely answers to a posed query can be useful input to the ranking process. Based on a novel use of Community Question Answering data, we present an approach for the creation of such passages. A general framework for extracting answer passages and estimating their quality is proposed, and this evidence is integrated into ranking models. Our experiments on two web collections show that such quality estimates from answer passages provide a strong indication of document relevance and compare favorably to previous passage-based methods. Combining such evidence can significantly improve over a set of state-of-the-art ranking models, including Quality-Biased Ranking, External Expansion, and a combination of both. A final ranking model that incorporates all quality estimates achieves further improvements on both collections.

Mark Sanderson
RMIT University Melbourne, Australia

KEYWORDS
Document ranking; quality estimation; answer passages
ACM Reference Format: Evi Yulianti, Ruey-Cheng Chen, Falk Scholer, W. Bruce Croft, and Mark Sanderson. 2018. Ranking Documents by Answer-Passage Quality. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210028
1 INTRODUCTION
It has long been thought that combining document-level and passagelevel evidence is an effective retrieval approach [8, 46]. Bendersky and Kurland [4], for example, showed that combining evidence from the best-matching passage in retrieved documents leads to increased retrieval effectiveness.
Different types of passages have been examined. Tombros and Sanderson [43] proposed so-called query biased summaries for use
This author is also affiliated with Universitas Indonesia. This work was conducted during her graduate studies at RMIT University The research work was primarily done at RMIT University
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210028

Figure 1: An example of questions from the CQA site, Yahoo! Answers, that are related to the given query "dinosaurs"
in search result pages. Later work provided evidence supporting the use of summaries as a passage representation to improve ad hoc retrieval [14, 22, 40]. Such summaries are created based on the degree of query-term matching, rather than document relevance. It remains to be seen if more effective passages can be found.
We investigate whether passages can be biased towards selecting text fragments that are more likely to bear answers to the query, and whether this new approach would give a better indication of underlying document relevance. The induced representation would tend to cover a richer set of text evidence rather than just the given query terms. We call these fragments answer passages.
We create answer passages by exploiting content in a specialized resource where high quality, human-curated question-answer structures are abundant: Community Question Answering (CQA) services. The text content on such services is utilized in a specific way: not to reuse or synthesize answers, but to provide an indication as to which text fragments in a document are likely to be part of an accepted answer. This "answer-bearingness" property can serve as a valuable document ranking signal.
While exploiting information from external resources to improve ranking is common [5, 12], to the best of our knowledge, no past work has studied using an external resource to improve the relevance estimate of document passages for ad hoc retrieval.
Our main contributions are: (1) We develop a new approach for representing passage-level evidence for ad hoc retrieval via a novel

335

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

use of CQA data; (2) The new approach provides a strong indication of document relevance, and is able to outperform many previous passage-based methods; combining text quality with evidence derived from the new representation leads to further improvements. Our experiments show that incorporating the new evidence significantly improves over state-of-the-art ranking models, including Quality-Biased Ranking (QSDM), External Expansion (EE), and a combination of both.
The remainder of this paper is organized as follows. Section 2 presents related work, followed by the motivation of this work in Section 3. Section 4 details our framework of passage extraction using external resources and document re-ranking using quality features derived from the answer passages. Sections 5 and 6 describe the experiment and the results. Discussion and concluding remarks are given in Sections 7 and 8.
2 BACKGROUND
Table 1 shows our categorization of approaches to document ranking: considering the object being scored (i.e. document or passage/summary) and the location of information that is exploited (i.e. local or external).1 Work listed in the top-left cell focuses on attempts to improve relevance estimation of a document using the local collection. The top-right cell lists work exploring the use of more focused text representations such as passages or summaries. The bottom-left cell lists work exploiting external resources for improving relevance estimation. A considerable amount of effort was invested in both directions, but the intersection, the bottom-right, has had less exploration. We now examine each cell in turn.
Document-Based Scoring Using Local Collection. Common retrieval models such as BM25 [38], language models [34], and DfR [2] are in this cell. Among these widely used models, Sequential Dependency Model (SDM) has consistently demonstrated strong effectiveness [29]. Lavrenko and Croft [23] implemented pseudorelevance feedback (PRF) within a language modeling framework. The basic idea of PRF [39] is assuming the initially retrieved topranked documents are relevant and then extracting the most frequent terms from them to improve retrieval effectiveness.
Kurland and Lee [20] leveraged link-based methods using interdocument similarities. Bendersky et al. [3] integrated document quality features in a quality-biased SDM (QSDM) framework, and showed the effectiveness of their approach over text- and link-based techniques. To the best of our knowledge, no previous work has reported superior performance to QSDM ranking.
Document-Based Scoring Using External Resources. Use of external resources to improve the relevance estimation of documents has also been tried [5, 12]. Diaz and Metzler [12] incorporated information from external corpora, such as the web collections, using a language modeling technique for PRF. External expansion was shown to be effective and was extended by Weerkamp et al. to the task of blog retrieval [45]. Bendersky et al. [5] also explored the use of term/concept statistics derived from external corpora, such as MSN query logs and Wikipedia, into the SDM method.
1Note, such a categorization excludes methods that either address more specific retrieval problems (e.g. clustering [36] or learning to rank [27]) or that exploit other data (e.g. link analysis [20] or user signals [1]).

Passage-Based Scoring Using Local Collection. Combining evidence from passages to improve ad hoc retrieval has been explored by Bendersky and Kurland [4], who showed that incorporating the best-matching passage into the original document language model [34] can significantly improve retrieval effectiveness. Krikon and Kurland [19] further explored the integration of documentbased, cluster-based, and passage-based information to improve document ranking. Relatively little work has considered using document summaries, as another passage representation, to improve retrieval effectiveness. More recently, He et al. [14] showed that combining summaries and documents improves the retrieval effectiveness of a document language model baseline.
However, as will be shown in our experiments, this approach does not improve over stronger retrieval models such as SDM, suggesting that the passage scoring is not effective in the presence of term proximity information. While the advantage of using passage representations in ad hoc retrieval appears evident, it is still an open question if further improvements can be made.
Passage-Based Scoring Using External Resources. To the best of our knowledge, using an external collection to better estimate the relevance of retrieved passages for ad hoc retrieval has not been explored. Much of the past work has focused on using external collections for the relevance estimation of documents, including Wikipedia [5, 28] and web collections [12].
CQA sites allow people to ask questions that are answered by other users in the community. The popularity of CQA, such as Yahoo! Answers, has grown rapidly; in 2016, over 3.0 million people in U.S. accessed Yahoo! Answers per month.2 Previous work has exploited CQA data for many purposes, such as: answering factoid and tips questions [6, 44], answering non-factoid queries [51], predicting information asker and web searcher satisfaction [25, 26], and evaluating answer quality [41]. We are not aware of previous work that has used CQA for improving document ranking.
3 HYPOTHESIS
Our work tests the following answer-bearingness hypothesis:
Documents that are likely to bear focused answers to the posed query should be ranked highly.
To test the hypothesis, CQA resources are exploited as proxies of an oracle "answer source", which is unattainable otherwise. A scoring rule is developed and used in a subsequent passage generation step to score any given passage according to how well its text content approximates the answer source data. Following Bendersky and Kurland [4], we assume that the best-scoring passage under this scoring rule can represent the full document in a quality-biased ranking framework, and therefore quality features derived from the best-scoring passage can directly benefit retrieval. A set of similar strategies was recently reviewed in passage retrieval [17], with an aim of improving the presentation of search results in general.
We now formally define the research questions:
RQ1 Can answer passages be exploited to improve document ranking compared to existing methods?
RQ2 Can incorporating quality features from answer passages improve ad hoc retrieval?
2 https://www.quantcast.com/answers.yahoo.com

336

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Ad hoc retrieval methodologies broken down in two axes, based on the object being scored (columns) and the resource used in relevance estimation (rows). Shaded methods are our addition to this work.

Local Collection External Resources

Document
Retrieval models: BM25, SDM, or DfR Pseudo relevance feedback [23, 39] Quality-biased ranking (QSDM) [3]
External expansion (MoRM) [12, 45] Weighted dependence model (WSD) [5]

Passage Passage-based LM [4, 14, 19]
Answer-passage quality

Our methodology allows for the creation of multiple passage representations for improving document ranking, which leads to a third research question:
RQ3 Does combining quality features from multiple passage representations make a stronger ranking model?

4 AN ANSWER-PASSAGE APPROACH
In passage retrieval, a two-phase approach is used to avoid needing to generate passage representations for all documents. We assume that an initial set of documents DQ with respect to query Q is first retrieved using a standard retrieval function such as BM25 or SDM, to serve as input to the passage retrieval module. Following this step, our answer-passage approach will exploit information from CQA data to induce passages that are likely to bear answers to query Q, and use this passage representation to re-rank documents.
We present two different methodologies in the coming sections for extracting and scoring answer passages. Section 4.1 presents a general probabilistic framework that involves external resources in the process of extracting answer passages. An alternative method, described in Section 4.2, leverages open-domain question answering to directly retrieve answer-reporting passages. On either type of representation, a final re-ranking step is performed based on the passage quality, which is described in Section 4.3.

4.1 A Probabilistic Framework
Our approach requires one basic functionality from the CQA resource: the ability to perform question retrieval so that the user can submit a query Q to retrieve a set of related questions and gain access to the respective answers AQ (see Section 5.1). The answers, AQ , are used to improve the estimation of term relevance [12]. In a standard language modeling framework [23], this relevance estimate p(t |Q) is written as:

p(t |Q) 

p(t |A) p(Q |A),

(1)

A  AQ

where p(t |A) is the relevance estimate of term t derived from answer A, and p(Q |A) is the retrieval score of answer A with respect to Q.

Improving Relevance Estimation of Terms. For term relevance p(t |A), we consider estimates that are in proportion to a given term weighting function. The following functions are discussed:

· Query Likelihood (QL) [34, 53]:

f (t, A) + µ p(t |C) . |A| + µ

(2)

· BM25 [38]:

f (t, A) (k1 + 1)

id f (t).

(3)

f (t, A) + k1

1

-b

+b

|A | avgA |A |

· Embedding Language Model (EMB):

t   T~

tA

A tA

p(t
A

, tA) p(t ,

1/
tA

|A |
) 1/

|A

|

.

(4)

The first two functions are based on commonly used retrieval
models, Query Likelihood (QL) [34, 53] and BM25 [38]. For QL, µ
controls the degree of Dirichlet smoothing and p(t |C) is the back-
ground (collection) language model. For BM25, k1 and b are parameters and avgA |A| is the average answer size. In these equations, f (t, A) denotes the frequency of term t within answer A.

The third term relevance estimate is based on word embed-

dings [31], which can serve as an alternative to more conventional

score functions. Our formulation differs from prior work [21, 52]

in the way the probability of jointly observing term t and answer

A is defined:

1/ |A |

p(t, A) 

tA A p(t, tA)

.

(5)

We postulate that the likelihood of jointly observing two terms t and t  in the same document context is proportional to a sigmoidal

transformation (with scale/location parameters  and x0) of the cosine similarity between the respective word vectors vt and vt:

p(t, t ) 

1

.

1 + exp (-(cos(vt , vt) - x0))

(6)

It can be shown that the relevance estimate p(t |A) as in (4) follows

this derivation. Practically, it suffices to compute the normalization factor in (4) over a smaller subset of terms T~  T .
p(Q |A) informs the degree of relevance of answer A with respect to query Q, so that in (1) more relevant answers have stronger influence over the inferred model p(t |Q). As CQA sites do not usually

reveal such scores or even the scoring rules, some distributional

assumptions are made for computing this estimate. One can assume that the query likelihood of answer A retrieved at the k-th position (within the set AQ ) is distributed logarithmically, in accordance with the Discounted Cumulative Gain (DCG) [16], or geometrically,

in accordance with the Rank-Biased Precision (RBP) metric [32]. For

simplicity, in this paper we focus on only the DCG variant, defined

as follows, as both variants showed comparable performance in our

preliminary experiments:

p(Q |A)  (log k + 1)-1 .

(7)

337

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Extracting Answer Passages. The next step is to incorporate the estimated term relevance into a passage algorithm to extract subdocument representations G that best approximate the retrieved answer-bearing content. Two approaches are taken: extracting fixed-length passages (PSG) and extracting summaries using integer linear programming (ILP). Note that, depending on the approach in use, G can either be a contiguous block of text or a set of sentences put together by using document summarization.
The first approach, PSG, is based on the use of fixed-length passages that are common in retrieval [8, 33]. Such representations do not (usually) stick to predefined sentence/paragraph boundaries and can be easily generated using a sliding window algorithm. From all passages in document D, prior work [4] suggests scoring them with a language modeling approach to choose one passage G with the maximum score. Our first approach follows this practice but uses the improved relevance estimates to evaluate passages:

GPSG

=

arg

max
G D

t G p(t |Q).

(8)

However, the answer-bearing content may not necessarily form a contiguous text block so that fixed-length passages will catch them. Redundant terms in a passage can also fill up the space easily without providing additional information, rendering the relevance estimate unreliable.
Our second approach, ILP, draws on document summarization to tackle these issues. It leverages integer linear programming to extract document summaries [13, 42, 47], with the core algorithm extended to incorporate term relevance estimates derived from CQA resources. This particular approach is taken in our framework for both the efficacy and the ease to incorporate external knowledge about topical relevance.3 The algorithm is optimized to select a set of sentences that maximize the coverage of answer-bearing terms in the generated summary G:

GILP

=

arg max
G D

L(G)

+

 R(G),

(9)

where:

L(G) = p(t |Q), R(G) = p(t |Q) s f (t, G) (10)

t G

t G

and |G| is less than or equal to some predefined K and s f (t, G) de-

notes the "sentence frequency" of term t in summary G. Both objec-

tive functions L(G) and R(G) are combined using a hyperparameter 0    1. The first objective will try to maximize summary-level

term coverage and reduce term repetition. The other sentence-level

objective will include more sentences with highly relevant terms.

4.2 Open-Domain Question Answering
We also implement an alternative answer-passage scoring framework based on a recent open-domain question answering model, called Document Reader (DR) [9]. The goal of open-domain question answering is to automatically extract text fragments ("answers") from a set of unstructured or free-format documents to address users' questions.

3More advanced approaches, such as submodular optimization [24], use sentenceto-sentence similarities rather than concept relevance to perform document summarization. It is not clear yet how CQA resources can be incorporated in this regard to improve the extraction of answer passages.

Table 2: List of passage quality features.

Feature PassageScore PassageOverlap NumSentences QueryOverlap AvgWordWeight AvgTermLen Entropy FracStops StopCover

Definition Objective value to score the passage Bigram overlap with respect to answers Number of sentences Number of query term occurrences Average passage term weight Average passage term length Shannon entropy of the term distribution Fraction of passage terms that are stopwords Fraction of stopwords appear in the passage

The DR model takes query Q and document D as input and returns a best-matching passage G = 1, 2, . . . , m  of m terms
that maximizes an answer span score, defined as follows:

GD R

=

arg

max
G D

max
1i j m

log pS

(i

|G, Q)

+

log pE (j

|G, Q).

(11)

In this formulation, the score being optimized indicates the loglikelihood of a passage G reporting an answer. The core idea behind
DR is to use recurrent neural networks to aggregate term-level evidence (i.e. features), and then for each passage term i estimate if the term starts or ends an answer span with respect to Q using attentive modeling [15]. The best scoring pair i , j  in a passage is identified to compute the final answer span score. Specifically, the two likelihood models pS and pE , for starting and ending an answer span, are defined as:

pS (i |G, Q)  exp(vTi WS vQ ), pE (j |G, Q)  exp(vTj WE vQ ),

(12)

where vi and vj are passage-term vectors, vQ denotes the query vector, andWS andWE indicate the bilinear mappings. Both passage-
term vectors and query-term vectors are derived from the hidden

states of two separate recurrent neural networks, and the query

vector is a weighted combination of the derived query-term vectors.

These definitions are given as follows:

v :   G = BiLSTMG (f :   G),
vq : q  Q = BiLSTMQ (eq : q  Q), (13)
vQ = softmax(WQ vq ) vq ,
q Q

where WQ is a linear mapping, f denotes the feature vector for passage term , and eq denotes the word embeddings for term q.

4.3 Passage Quality Based Ranking
A mix of novel and existing features are employed to estimate the quality of the produced passage, see Table 2. PassageScore denotes the score assigned to the best matching passage in the retrieved document. The score is combined with PassageOverlap to estimate the answer-bearingness level of a passage relative to a given query. PassageOverlap measures the term overlap between a document passage and its related CQA answers. NumSentences is employed as a quality feature based on the idea that a summary with too many short sentences is less likely to be relevant or informative. QueryOverlap has been used in previous studies on web

338

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Test collections used in our experiments.

Collection Topics

# Docs

GOV2

TREC Topics 701­850 25,205,179

ClueWeb09B TREC Web Topics 1­200 50,066,642

search ranking [1] and in query-biased summarization [30]. Other prior work [50] leveraged AvgWordWeight as a sentence feature to generate document summaries. Motivated by the effectiveness of document quality features used by Bendersky et al. [3], we adopt four non-HTML specific quality features to work at the passage level: AvgTermLen, Entropy, FracStops, and StopCover.
The proposed quality estimates are combined by using a featurebased linear ranking model (see (14)). Previous work has used a similar approach [3, 29], and in most cases combining evidence from different representations and different retrieval functions has been shown to be beneficial [11]. As was done in the QSDM framework [3], the SDM retrieval score is also included in the model:

D fSDM(q, D) + j j fj (q, G)

(14)

where the weights D + j j = 1, fj represents the j-th feature, and G represents the answer passage. The weights are learned using

a learning-to-rank algorithm described in Section 5.3.

5 EXPERIMENTS
A series of experiments was conducted to evaluate the effectiveness of the proposed ranking model using quality features extracted from the answer passages. Section 5.1 describes the data and evaluation metrics used in our experiments. Section 5.2 covers the details about baselines and Section 5.3 covers the parameter estimation.

5.1 Setup
The code and data used in this paper are made publicly available for interested readers to reproduce our results.4
Test Collections. Ranking experiments were conducted on two web test collections, GOV2 and CW09B (i.e. ClueWeb09B), using TREC Terabyte 2004­2006 and Web Track 2009­2012 "title" topics respectively. An overview of these data sets is provided in Table 3. Both web collections were indexed using the Indri search engine using Krovetz stemming without removing stopwords. The spam filter by Cormack et al. [10] was applied to CW09B, removing spam webpages with a score less than 70. Repeating the same experiments on un-filtered CW09B data leads to the same conclusions, with some slight decreases in absolute early precision (@10) but increases in recall-oriented metrics.
Retrieval Settings. Initially, a ranked list of 100 documents was retrieved using the SDM, following the configuration parameters suggested in the original paper (T , O , U ) = (0.85, 0.10, 0.05) [29]. This step is performed using the Indri search engine.5 The raw HTML content for each retrieved document was parsed by using BeautifulSoup6 and sentences extracted using the Stanford
4 https://github.com/rmit- ir/AnswerPassageQuality 5http://www.lemurproject.org/indri.php (version 5.9) 6https://www.crummy.com/software/BeautifulSoup/ (version 4.0)

CoreNLP toolkit.7 Stopwords were removed from the sentences (using the INQUERY list) and Krovetz stemming was performed.
External CQA Resources. The external CQA data were obtained from Yahoo! Answers (Y!A), by submitting our queries to the Y!A search engine and taking the best answer for each of the top ten matching questions. In Y!A, the best answer for each question is chosen by the person who posts the question.8 Our decision to use only the best answer for each question is to ensure good quality information [51]. There are three GOV2 queries (QID 703, 769, 816) and five CW09B queries (QID 95, 100, 138, 143, 146), however, that do not have any matching questions. Since the purpose of this research is to investigate how external evidence can be used to enhance summaries and document ranking, we remove these eight queries from this experiment (we return to the issue of the availability of suitable CQA answers in Section 6.5). The average number of related CQA answers per query in GOV2 and CW09B data are 9.52 and 9.74 (maximum of 10), respectively. The choice of ten as the number of related CQA answers per query is justified based on the result of an initial experiment, where we tried using 1, 5, 10, 20, 50, and 100, and found that according to several metrics, using a single answer is the least effective, while using ten answers gave the most effective results in most of the cases.
Word Embeddings. Two sets of word embeddings are used, both based on the fasttext package [7]. The first is a pre-trained set of one million word vectors based on the English Wikipedia data in 2017 of 16 billion tokens (denoted as EmbWiki), and the second is a set of five million word vectors trained on our custom crawl of Yahoo! Answers data of five billion tokens (denoted as EmbYA) using the skip-gram algorithm.9 Both sets of vectors are of 300 and 100 dimensions respectively.
Evaluation Metrics. To get a broader understanding to the effectiveness of the proposed method, six evaluation metrics are reported in this study. Top-k effectiveness as the focus of web search is represented by NDCG@10, NDCG@20, P@10, and P@20. The metric MRR, which is widely used in web question answering, is also included. Additionally, we report MAP@100, as our ranking experiment is limited to the top 100 initially retrieved documents. The two-tailed t-test is used for significance testing.
5.2 Baselines
The following baselines were selected and implemented:
· Sequential Dependence Model (SDM); · Passage-Based Language Model (MSP and SUM); · Quality-Biased Ranking (QSDM); · External Expansion (EE).
Passage-Based Language Models. A passage-based language model is a mixture of three models of the passage pG , the document pD , and the collection pC . The combined model usually takes the following form:
p(Q) = t Q [G pG (t ) + D pD (t ) + C pC (t)] ,
7https://stanfordnlp.github.io/CoreNLP/ (version 3.8.0) 8 http://yahooanswers.tumblr.com/post/80173794953/important-changes-to-answers 9The custom Yahoo! Answers crawl contains roughly 17 million question-answer pairs submitted to Yahoo! Answers between 2013 and 2016.

339

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

under the constraint that the mixture weights sum to one. The passage model pG and the mixture weights might be implemented slightly differently across methods.
Two variants, MSP and SUM, are implemented in this paper. The first model is based on a top-performing variant MSP[length] from Bendersky and Kurland [4]. It locates the best-matching passage G in the document by maximizing the maximum-likelihood estimate pG across a set of candidates, with one key parameter D set by using the document homogeneity estimate h[lenth]. A second approach, called SUM, based on query-biased summarization [14] was shown to be competitive to gradient boosting regression trees. Following the proposed setting [14], the MEAD package [35] is used to implement this method, combining four features: Centroid, Position, Length, and QueryCosine with the default weights.
Quality-Biased Ranking (QSDM). The quality-biased ranking method [3] is commonly referred to as the state of the art in web document ranking with TREC collections. The method is a linear model that combines the SDM score and ten web document quality features, which are: NumVisTerms, NumTitleTerms, AvgTermLen, FracAnchorText, FracVisText, Entropy (Entropy of the document content), FracStops, StopCover, UrlDepth (depth of the URL path), and FracTableText.
External Expansion (EE). External Expansion [12] is a standard PRF approach for expanding queries using external corpora based on the Relevance Model [23]. It is generally considered as a strong and effective expansion method when external resources are available.
5.3 Parameter Estimation
Parameters for individual baseline methods are tuned as follows:
· For passage-based language models (MSP and SUM), the mixture weights are optimized via a grid search over the range {0.00, 0.05, 0.10, . . . , 0.95, 1.00} using cross validation.
· For external expansion (EE) the procedure followed closely to the original paper Diaz and Metzler [12]. The number of feedback documents (i.e., CQA answers) was set to ten to align with the data. The number of feedback terms nT , collection model weight C , and the mixture ratio Q with respect to the original query were all learned on the target test collections via 100 rounds of randomized search over randomly re-sampled train/test (50%­50%) query splits.10 In our experiments, (nT , C , Q ) were set to (60, 0.3, 0.2) on GOV2 and to (50, 0.2, 0.2) on CW09B.
Parameters for experimental runs are tuned as follows:
· The passage size K is set to fifty words, to be made consistent with the common setting for query-biased summarization [35]. We set  = 0.1 in the extraction of the ILP representation.
· For QL, we set µ = 100 and for BM25, we set b1 = 1.2 and k1 = 0.75, based on common settings in adhoc retrieval. Both p(t |C) and id f (t) are estimated on the target collection.
· For both embedding based estimates EmbWiki and EmbYA, we set  = 10 and x0 = 0 based on cross validation.
10This optimization procedure allows the resulting retrieval scores to be included as a feature in our ranking model. The resulting scores are comparable to the procedure proposed by Diaz and Metzler [12].

· Our implementation of the DR framework follows the original paper [9]: we encode query and passage vectors using 128 hidden units in three-layer bidirectional LSTMs. The model is trained on the SQuAD dataset [37] using AdaMax [18]. The dropout rate is tuned empirically to 0.5. We use the same set of word embeddings learned from the Y!A data (as with EmbYA), but the effectiveness is roughly comparable to a pre-trained model learned on the Common Crawl data [9].
For all methods tested in our experiments, a Coordinate Ascent learning-to-rank algorithm is employed to learn the model weights using ten-fold cross validation, as is commonly practiced in past work [3, 29].11 We used RankLib12 to estimate parameters, which are essentially the weight of each feature. We chose to optimize NDCG@20 throughout the experiments as it gives the best performance in terms of both precision- and recall-oriented metrics.
6 RESULTS
We describe and analyze the effectiveness of ranking using different answer-passage representations: PSG and ILP, as well as passages derived by using open-domain question answering model (DR).
6.1 Comparisons with Previous Work
Our approach is first compared with prior techniques for both test collections, see Table 4. Ten experiments are reported: two representations PSG and ILP with four relevance estimates EmbWiki, EmbYA, QL, and BM25, and the DR framework are tested using title and description queries.
It can be seen that combining SDM with answer-passage quality using all three representations PSG, ILP, and DR, significantly outperforms SDM and the passage-based baselines SDM+MSP and SDM+SUM. While incorporating MSP and SUM shows only marginal benefits over SDM, combining answer-passage quality has seen the biggest effect across all the other methods involved. This provides an answer to RQ1: answer passages can be used to improve ad hoc retrieval in the presence of a strong retrieval baseline SDM, and they can work better than existing passage-based methods.
The fact that DR does not provide strong indication of document relevance is unexpected. Among all representations ILP is found to be the most effective, while PSG and DR are roughly comparable. For both SDM+PSG and SDM+ILP, BM25 gives the best results and QL the second, followed by embedding based estimates EmbWiki and EmbYA. The SDM+DR framework works the best on description queries, suggesting that further tuning might be needed for such models to handle non-verbose queries. On both test collections, the best effectiveness is achieved by using SDM+ILP with BM25.
6.2 Ad Hoc Retrieval with Passage Quality
The previous experiment shows that SDM+ILP paired with retrieval function BM25 and QL may have some advantages over strong retrieval model QSDM and SDM+EE. This leads to a further investigation
11Note that the comparison between ranking algorithms is beyond the scope of this paper. In preliminary experiments, non-linear ranking models such as Gradient Boosted Decision Trees (GBDT) and LambdaMART were also tested, but were found to consistently perform less effectively than Coordinate Ascent on all metrics by a wide margin, suggesting that the ideal response surface is close to a hyperplane, as non-linear models can struggle with this type of ranking problem. 12https://www.lemurproject.org/ranklib.php (version 2.7)

340

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Comparisons with previous methods. Significant differences with respect to SDM/QSDM/SDM+EE are indicated using // for p < 0.05 (or // for p < 0.01). All differences between SDM+PSG/ILP runs and SDM+MSP are significant for p < 0.05.

GOV2 CW09B

Baseline / Passage Baseline SDM() QSDM() SDM+EE() SDM+MSP SDM+SUM
Answer-Passage Approach
SDM+PSG (EmbWiki) SDM+PSG (EmbYA) SDM+PSG (QL) SDM+PSG (BM25) SDM+ILP (EmbWiki) SDM+ILP (EmbYA) SDM+ILP (QL) SDM+ILP (BM25) SDM+DR (Title) SDM+DR (Desc)
Baseline / Passage Baseline SDM() QSDM() SDM+EE() SDM+MSP SDM+SUM
Answer-Passage Approach
SDM+PSG (EmbWiki) SDM+PSG (EmbYA) SDM+PSG (QL) SDM+PSG (BM25) SDM+ILP (EmbWiki) SDM+ILP (EmbYA) SDM+ILP (QL) SDM+ILP (BM25) SDM+DR (Title) SDM+DR (Desc)

NDCG@10
0.4769 0.5127 0.5189 0.4826 0.4741
0.4999 0.5010 0.5085 0.5174 0.5081 0.4983 0.5131 0.5293 0.4821 0.4999
0.2542 0.2735 0.2880 0.2535 0.2499
0.2693 0.2752 0.2613 0.2811 0.2843 0.2818 0.3090  0.3115  0.2584 0.2833

NDCG@20
0.4751 0.5022 0.5057 0.4745 0.4749
0.4975 0.4957 0.5068 0.5116 0.4967 0.4951 0.5052 0.5171 0.4786 0.4894
0.2462 0.2639 0.2736 0.2469 0.2409
0.2588 0.2644 0.2569 0.2687 0.2652 0.2665 0.2901  0.2955  0.2505 0.2681

P@10
0.5694 0.6197 0.6129 0.5782 0.5680
0.6041 0.6007 0.6102 0.6184 0.6204 0.6075 0.6238 0.6367 0.5735 0.6014
0.3682 0.3938 0.4021 0.3656 0.3631
0.3831 0.3856 0.3805 0.3938 0.3954 0.3990 0.4313  0.4379  0.3662 0.3949

P@20
0.5469 0.5759 0.5738 0.5422 0.5500
0.5745 0.5724 0.5823 0.5847 0.5752 0.5779 0.5844 0.5946 0.5480 0.5612
0.3321 0.3467 0.3590 0.3328 0.3267
0.3421 0.3479 0.3490 0.3521 0.3392 0.3485 0.3736  0.3787  0.3295 0.3441

MRR
0.7763 0.8174 0.8220 0.7696 0.7729
0.8063 0.8024 0.7991 0.8271 0.8098 0.7900 0.7878 0.8234* 0.7817 0.8038
0.5010 0.5224 0.5619 0.4989 0.4952
0.5325 0.5299 0.5222 0.5499 0.5803 0.5579 0.5786  0.5902  0.5298 0.5613

MAP@100
0.1802 0.1919 0.1879 0.1805 0.1805
0.1888 0.1888 0.1929 0.1946 0.1892 0.1876 0.1964 0.2009  0.1811 0.1842 
0.1053 0.1094 0.1136 0.1054 0.1047
0.1058 0.1103 0.1087 0.1113 0.1070 0.1092 0.1164  0.1209  0.1050 0.1094

regarding improvements over strong retrieval models. We next incorporate passage quality features into an expanded set of retrieval models, using the ILP representation together with BM25 and EmbYA relevance estimates. For the choice of base models, we used SDM, QSDM, and QSDM+EE, with the latter being a novel and strong combination of quality-biased ranking and external expansion.
The results (Table 5) show three rows in each collection for each base model. Incorporating ILP significantly improves SDM for all metrics, across collections. On GOV2, BM25 improves over QSDM for NDCG@10, NDCG@20 and MAP@100. On CW09B, using BM25 leads to significant increases over QSDM for all metrics. For QSDM+EE, significant increases were observed on P@10, P@20, and MAP@100 on the GOV2 data using BM25, and CW09B runs show a similar trend but with a more pronounced effect. We conclude that RQ2

is answered: incorporating answer-passage quality can significantly improve ad hoc retrieval in general, but as the base system improves, further gains are likely to get smaller.
6.3 Combining Multiple Representations
Next, two answer-passage representations are involved in the ranking process. Denoted as Combined, this new experimental run effectively leverages passage-level evidence from two representations learned by using different methodologies. The aim is to understand whether quality estimates derived from different representations provide similar effects to document ranking.
For this experiment, we incorporate answer-passage quality estimates from both representations ILP (BM25) and DR (Desc) into the

341

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 5: Retrieval effectiveness of ranking models using quality estimates of answer-biased summaries. Significant differences with respect to baselines SDM/QSDM/QSDM+EE are indicated using // for p < 0.05 (or // for p < 0.01).

GOV2 CW09B

SDM() SDM+ILP (EmbYA) SDM+ILP (BM25)
QSDM() QSDM+ILP (EmbYA) QSDM+ILP (BM25)
QSDM+EE() QSDM+EE+ILP (EmbYA) QSDM+EE+ILP (BM25)
SDM() SDM+ILP (EmbYA) SDM+ILP (BM25)
QSDM() QSDM+ILP (EmbYA) QSDM+ILP (BM25)
QSDM+EE() QSDM+EE+ILP (EmbYA) QSDM+EE+ILP (BM25)

NDCG@10
0.4769 0.4983 0.5293
0.5127 0.5197 0.5412 
0.5339  0.5329  0.5442 
0.2542 0.2818 0.3115 
0.2735 0.2853 0.3107 
0.2985  0.3042  0.3194 

NDCG@20
0.4751 0.4951 0.5171
0.5022 0.5126 0.5245 
0.5213  0.5208  0.5311 
0.2462 0.2665 0.2955 
0.2639 0.2691 0.2959 
0.2819+ 0.2864  0.3015 

P@10
0.5694 0.6075 0.6367
0.6197 0.6238 0.6463
0.6374 0.6429 0.6605 
0.3682 0.3990 0.4379 
0.3938 0.3923 0.4333 
0.4056 0.4174 0.4338 

P@20
0.5469 0.5779 0.5946
0.5759 0.5874 0.5939
0.5901 0.5959  0.6082 
0.3321 0.3485 0.3787 
0.3467 0.3485 0.3774 
0.3610 0.3679  0.3826 

MRR
0.7763 0.7900 0.8234
0.8174 0.8258 0.8338
0.8416 0.8044 0.8407
0.5010 0.5579 0.5902 
0.5224 0.5566 0.6002 
0.5799  0.5881  0.6138 

MAP@100
0.1802 0.1876  0.2009 
0.1919 0.1891 0.2007 
0.1948 0.1947 0.1996 
0.1053 0.1092 0.1209 
0.1094 0.1109 0.1190 
0.1148  0.1169  0.1210 

Table 6: Combining ILP and DR significantly improves QSDM (significant differences are indicated using  for p < 0.05 or  for p < 0.01).

GOV2 CW09B

QSDM() QSDM+Combined
QSDM() QSDM+Combined

N@20
0.5022 0.5280
0.2639 0.2896

P@20
0.5759 0.6007
0.3467 0.3656

MAP@100
0.1919 0.1972
0.1094 0.1166

QSDM run. The results of these experiments are shown in Table 6. The Combined method produces strong retrieval runs, but not significantly better than just incorporating ILP. However, QSDM+Combined significantly outperforms QSDM on both collections, and across all metrics except MRR and MAP@100 on GOV2. Regarding the answer to RQ3, we conclude that there is some evidence to support the claim that the use of multiple representations will lead to a stronger ranking model.
6.4 Feature Importance
An ablation analysis was conducted on the run QSDM+EE+ILP (BM25) with twenty one features in total, to examine the relative feature importance. The top seven features for each collection are shown in Table 8, ordered by decreasing difference of NDCG@20 score after removing a feature. NDCG@20 is used as an ordering criterion following our optimization metric in the main experiment. The letter P in square brackets indicates passage-level quality features.

SDM remains the most important feature across collections. Some differences between collections can be seen based on the relative importance of features. Our passage quality features AvgWordWeight, QueryOverlap, and FracStop[P] appear more effective on GOV2. On CW09B, PassageScore, StopCover[P], and AvgTermLen are among the top-ranked features. Note that PassageScore and EE also show high importance on CW09B, indicating the usefulness of external CQA resources in ad hoc retrieval.
6.5 Lack of CQA Resources
To investigate to what extent the ranking effectiveness changes when the coverage of good related CQA answers is not guaranteed, we conduct an experiment using the related answers obtained from the offline Yahoo! Webscope L6 collection13 using a mixture approach [49].14 Note that this dataset was collected prior to the creation of CW09B, so the respective TREC query topics are less likely to have direct answers in the data.
Our results (Table 9) suggest a decrease of up to 2.6% on GOV2 and 3% on CW09B compared to the result of using the related CQA answers obtained from Yahoo! Answers (Y!A) search engine (see Table 5). The implication is that a good coverage of related answers is crucial in the generation of answer passages, that are primarily supported by the size of collection and the human efforts involved in curating the best answers.
It is worth noting that using an offline resource with limited coverage of related CQA answers still leads to a significant improvement in NDCG@20 and P@20 on the CW09B collection. This
13 http://webscope.sandbox.yahoo.com/ 14The weight of SDM retrieval score for question title, question body, and best answer fields are respectively set to 0.5, 0.2, and 0.3 based [49].

342

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 7: Answer passages extracted from a top-ranked relevant document clueweb09-enwp01-16-17964 for TREC Web Topic 65, " Find information and resources on the Korean language." (query: korean language)

ILP (EmbYA)

For example, different endings are used based on whether the subjects and listeners are friends, parents, or honoured persons. in a similar way European languages borrow from Latin and Greek. Its use limited some cases and the aristocracy prefers Classical Chinese for its writing. "Mortal enemy" and "head of state" are homophones in the South. Learn to read, write and pronounce Korean

ILP (BM25)

Yanbian (People's Republic of China) Given this, it is sometimes hard to tell which actual phonemes are present in a certain word. Unlike most of the European languages, Korean does not conjugate verbs using agreement with the subject, and nouns have no gender. The Korean language used in the North and the South exhibits differences in pronunciation, spelling, grammar and vocabulary.

DR (Desc)

Korean is similar to Altaic languages in that they both lack certain grammatical elements, including number, gender, articles, fusional morphology, voice, and relative pronouns (Kim Namkil). Korean especially bears some morphological resemblance to some languages of the Northern Turkic group, namely Sakha (Yakut).

Table 8: Results of ablation study to determine feature importance for both test collections.

GOV2

CW09B

Feature

Diff. Feature

Diff.

SDM FracStop AvgWordWeight UrlDepth QueryOverlap FracAnchorText FracStop[P]

0.0306 0.0101 0.0076 0.0063 0.0052 0.0049 0.0048

SDM StopCover PassageScore FracVisText EE StopCover[P] AvgTermLen

0.0223 0.0092 0.0086 0.0077 0.0076 0.0046 0.0038

Table 9: An investigation of using external CQA resources from offline collection. Significant differences with respect to QSDM are indicated using  for p < 0.05 (or  for p < 0.01).

GOV2 CW09B

QSDM() QSDM+ILP (BM25)
QSDM() QSDM+ILP (BM25)

N@20
0.5022 0.5083
0.2639 0.2804

P@20
0.5759 0.5759
0.3467 0.3679

MAP@100
0.1919 0.1926
0.1094 0.1136

is in line with one previous study [51] on exploiting CQA resources for non-factoid question answering, which shows modest improvements in the quality of produced answers even when no CQA answer exactly matches the queries. We note that, in the case where CQA answers are not available for a particular query, a live system can simply back off to a retrieval mode that does not incorporate such evidence, as the lack of appropriate CQA resources is clearly indicated through an empty results list.
7 DISCUSSION
The results in the previous sections provide strong empirical evidence to support the validity of the answer-bearingness hypothesis, and also directly support the recurring argument in previous work [4, 8, 14, 22, 40, 46] that passage-level evidence can benefit retrieval

effectiveness. It is however surprising that, the open-domain question answering model shows little benefit in extracting answerbearing passages for document ranking. This may be due to task mismatch (i.e. model trained to detect factoids) or the lack of appropriate training instances. Word embeddings learned on the CQA data are arguably useful for the task, but simpler methodologies appear to win on the overall efficacy.
The ILP representation benefits the most from the inclusion of passage quality estimates, which we suspect is due to the fact that summaries are more likely to cover broken sentences on nonrelevant documents. The compressive nature forces summaries to include all textual evidence that seems relevant, but when such evidence is scarce the quality can be poor.
For illustrative purposes, some example answer passages produced by using ILP and DR are also given in Table 7. The answer passages are extracted from a top-ranked relevant document for a randomly sampled query topic. We note that the extraction algorithms tend to capture a broader range of answers when the underlying document is relevant, as is shown in the example. While the captured evidence may differ in terms of efficacy for document ranking, it remains an open question how this difference correlates with users' perception towards the answer-passage quality.
8 CONCLUSIONS AND FUTURE WORK
In this paper, we proposed quality-biased ranking that incorporates signals from passages that are likely to bear answers. A new approach that exploits external resources in the creation of such passages is developed to induce high-quality sub-document representations, called answer passages, from the retrieved documents. We developed a set of methodologies to improve term relevance estimates and extract answer passages. A range of quality features is extracted from the generated passages, and blended into the ranking model, which leads to improved effectiveness: our experiments on two web collections showed that this approach is more effective than passage-based methods and external expansion, and can significantly improve on state-of-the-art ranking models SDM and QSDM. Signals from multiple representations can also be combined to improve ranking effectiveness. A final ranking model that combines all these quality estimates achieved significant effectiveness improvements on GOV2 and ClueWeb09B.

343

Session 3C: Question Answering

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

In future work we plan to conduct a user study to gain an under-
standing of human perceptions of the quality of answer passages,
and of the improvement in document ranking. A promising new
approach to entity representation has recently been published [48],
which approaches the problem of ranking from an angle orthogonal
to our work. We plan to explore a combination of such approaches
in future. We will look to examine possible improvements to our
ranking model, such as using link-based features [20], user behavior
signals [1], and filtering CQA answers based on their quality [41].
9 ACKNOWLEDGMENTS
This research is supported in part by the Australian Research Coun-
cil (DP140102655 and DP170102726) and the Indonesia Endowment
Fund for Education (LPDP). The authors want to thank Marwan
Torki for kindly sharing his CQA research data.
REFERENCES
[1] Eugene Agichtein, Eric Brill, and Susan Dumais. 2006. Improving web search ranking by incorporating user behavior information. In Proc. of SIGIR. ACM, 19­26.
[2] Gianni Amati and Cornelis Joost van Rijsbergen. 2002. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357­389.
[3] Michael Bendersky, W. Bruce Croft, and Yanlei Diao. 2011. Quality-biased Ranking of Web Documents. In Proc. of WSDM. ACM, 95­104.
[4] Michael Bendersky and Oren Kurland. 2008. Utilizing passage-based language models for document retrieval. In Proc. of ECIR. Springer, 162­174.
[5] Michael Bendersky, Donald Metzler, and W. Bruce Croft. 2010. Learning Concept Importance Using a Weighted Dependence Model. In Proc. of WSDM. ACM, 31­40.
[6] Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the right facts in the crowd: factoid question answering over social media. In Proc. of WWW. ACM, 467­476.
[7] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching Word Vectors with Subword Information. arXiv preprint arXiv:1607.04606 (2016).
[8] James P. Callan. 1994. Passage-level Evidence in Document Retrieval. In Proc. of SIGIR. Springer-Verlag New York, Inc., 302­310.
[9] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain Questions. In Proc. of ACL. Association for Computational Linguistics, 1870­1879.
[10] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (Oct. 2011), 441­465.
[11] W Bruce Croft. 2002. Combining approaches to information retrieval. In Proc. of ECIR. Springer, 1­36.
[12] Fernando Diaz and Donald Metzler. 2006. Improving the Estimation of Relevance Models Using Large External Corpora. In Proc. of SIGIR. ACM, 154­161.
[13] Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing. Association for Computational Linguistics, 10­18.
[14] Jing He, Pablo Duboue, and Jian-Yun Nie. 2012. Bridging the Gap between Intrinsic and Perceived Relevance in Snippet Generation. In Proc. of COLING. 1129­1146.
[15] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems. 1693­1701.
[16] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Trans. Inf. Syst. 20, 4 (2002), 422­446.
[17] Mostafa Keikha, Jae Hyun Park, and W Bruce Croft. 2014. Evaluating answer passages using summarization measures. In Proc. of SIGIR. ACM, 963­966.
[18] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).
[19] Eyal Krikon and Oren Kurland. 2011. A study of the integration of passage-, document-, and cluster-based information for re-ranking search results. Information Retrieval 14, 6 (2011), 593­616.
[20] Oren Kurland and Lillian Lee. 2010. PageRank without hyperlinks: Structural reranking using links induced by language models. ACM TOIS 28, 4 (2010), 18.
[21] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. Query expansion using word embeddings. In Proc. of CIKM. ACM, 1929­1932.

[22] Adenike M. Lam-Adesina and Gareth J. F. Jones. 2001. Applying Summarization Techniques for Term Selection in Relevance Feedback. In Proc. of SIGIR. ACM, 1­9.
[23] Victor Lavrenko and W Bruce Croft. 2001. Relevance based language models. In Proc. of SIGIR. ACM, 120­127.
[24] Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. In Proc. of HLT/NAACL. Association for Computational Linguistics, 912­920.
[25] Qiaoling Liu, Eugene Agichtein, Gideon Dror, Evgeniy Gabrilovich, Yoelle Maarek, Dan Pelleg, and Idan Szpektor. 2011. Predicting web searcher satisfaction with existing community-based answers. In Proc. of SIGIR. ACM, 415­424.
[26] Yandong Liu, Jiang Bian, and Eugene Agichtein. 2008. Predicting information seeker satisfaction in community question answering. In Proc. of SIGIR. ACM, 483­490.
[27] Craig Macdonald, Rodrygo L.T. Santos, and Iadh Ounis. 2012. On the Usefulness of Query Features for Learning to Rank. In Proc. of CIKM. ACM, 2559­2562.
[28] Edgar Meij and Maarten de Rijke. 2010. Supervised query modeling using wikipedia. In Proc. of SIGIR. ACM, 875­876.
[29] Donald Metzler and W. Bruce Croft. 2005. A Markov Random Field Model for Term Dependencies. In Proc. of SIGIR. ACM, 472­479.
[30] Donald Metzler and Tapas Kanungo. 2008. Machine Learned Sentence Selection Strategies for Query-Biased Summarization. In SIGIR Learning to Rank Workshop.
[31] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv preprint arXiv:1705.01509 (2017).
[32] Alistair Moffat and Justin Zobel. 2008. Rank-biased precision for measurement of retrieval effectiveness. ACM Trans. Inf. Syst. 27, 1 (2008), 2.
[33] John O'Connor. 1980. Answer-passage retrieval by text searching. Journal of the Association for Information Science and Technology 31, 4 (1980), 227­239.
[34] Jay M Ponte and W Bruce Croft. 1998. A language modeling approach to information retrieval. In Proc. of SIGIR. ACM, 275­281.
[35] Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Çelebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam Winkel, and Zhu Zhang. 2004. MEAD -- A platform for multidocument multilingual text summarization. In Proc. of LREC.
[36] Fiana Raiber and Oren Kurland. 2013. Ranking document clusters using markov random fields. In Proc. of SIGIR. ACM, 333­342.
[37] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 (2016).
[38] Stephen E Robertson. 1997. Overview of the okapi projects. Journal of Documentation 53, 1 (1997), 3­7.
[39] Joseph John Rocchio. 1971. Relevance feedback in information retrieval. The SMART retrieval system: experiments in automatic document processing (1971), 313­323.
[40] Tetsuya Sakai and Karen Sparck-Jones. 2001. Generic Summaries for Indexing in Information Retrieval. In Proc. of SIGIR. ACM, 190­198.
[41] Chirag Shah and Jefferey Pomerantz. 2010. Evaluating and predicting answer quality in community QA. In Proc. of SIGIR. ACM, 411­418.
[42] Hiroya Takamura and Manabu Okumura. 2009. Text summarization model based on maximum coverage problem and its variant. In Proc. of EACL. Association for Computational Linguistics, 781­789.
[43] Anastasios Tombros and Mark Sanderson. 1998. Advantages of Query Biased Summaries in Information Retrieval. In Proc. of SIGIR. ACM, 2­10.
[44] Ingmar Weber, Antti Ukkonen, and Aris Gionis. 2012. Answers, not links: extracting tips from yahoo! answers to address how-to web queries. In Proc. of WSDM. ACM, 613­622.
[45] Wouter Weerkamp, Krisztian Balog, and Maarten de Rijke. 2012. Exploiting External Collections for Query Expansion. ACM Trans. Web 6, 4 (2012), 1­29.
[46] Ross Wilkinson. 1994. Effective Retrieval of Structured Documents. In Proc. of SIGIR. Springer-Verlag New York, Inc., 311­317.
[47] Kristian Woodsend and Mirella Lapata. 2012. Multiple aspect summarization using integer linear programming. In Proc. of EMNLP. Association for Computational Linguistics, 233­243.
[48] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2017. Word-Entity Duet Representations for Document Ranking. In Proc. of SIGIR. ACM, 763­772.
[49] Xiaobing Xue, Jiwoon Jeon, and W Bruce Croft. 2008. Retrieval models for question and answer archives. In Proc. of SIGIR. ACM, 475­482.
[50] Zi Yang, Keke Cai, Jie Tang, Li Zhang, Zhong Su, and Juanzi Li. 2011. Social context summarization. In Proc. of SIGIR. ACM, 255­264.
[51] Evi Yulianti, Ruey-Cheng Chen, Falk Scholer, W. Bruce Croft, and Mark Sanderson. 2018. Document summarization for answering non-factoid queries. IEEE Trans. Knowl. Data Eng. 30, 1 (2018), 15­28.
[52] Hamed Zamani and W Bruce Croft. 2016. Embedding-based query language models. In Proc. of ICTIR. ACM, 147­156.
[53] Chengxiang Zhai and John Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (2004), 179­214.

344

Session 7A: Crowdsourcing & Assessment

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement?

Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease

Qatar University

University of Texas at Austin

{mucahidkutlu,yassmine.barkallah,telsayed}@qu.edu.qa

{tmcdonnell,ml}@utexas.edu

ABSTRACT
While crowdsourcing offers a low-cost, scalable way to collect relevance judgments, lack of transparency with remote crowd work has limited understanding about the quality of collected judgments. In prior work, we showed a variety of benefits from asking crowd workers to provide rationales for each relevance judgment [21]. In this work, we scale up our rationale-based judging design to assess its reliability on the 2014 TREC Web Track, collecting roughly 25K crowd judgments for 5K document-topic pairs. We also study having crowd judges perform topic-focused judging, rather than across topics, finding this improves quality. Overall, we show that crowd judgments can be used to reliably rank IR systems for evaluation.
We further explore the potential of rationales to shed new light on reasons for judging disagreement between experts and crowd workers. Our qualitative and quantitative analysis distinguishes subjective vs. objective forms of disagreement, as well as the relative importance of each disagreement cause, and we present a new taxonomy for organizing the different types of disagreement we observe. We show that many crowd disagreements seem valid and plausible, with disagreement in many cases due to judging errors by the original TREC assessors. We also share our WebCrowd25k dataset, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed.
KEYWORDS
Crowdsourcing, Relevance Assessment, Evaluation, Disagreement
ACM Reference Format: Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease. 2018. Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement?. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210033
1 INTRODUCTION
Crowdsourcing platforms such as Amazon's Mechanical Turk provide a low-cost and scalable way of collecting relevance judgments [2, 14]. While crowdsourcing is most often motivated by improved
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210033

scalability, it offers other potential benefits as well. Instead of relying on a single expert judgment for each document, a set of crowd judgments can be collected and aggregated to guard against human error (even trusted judges are fallible), or again human bias, by reflecting average opinion in what is an inherently subjective judging task. It may even be easier to find a crowd judge with relevant expertise than personnel available in one's local area [7, 22].
While crowdsourced judgments have been used in developing several test collections [5, 16], crowd task designs require special attention to ensure the quality of the collected judgments. Therefore, understanding reasons for crowd disagreements with trusted assessors is important for designing better crowd tasks.
Despite many studies reporting high disagreement in relevance judging between trusted assessors [26, 27], disagreements with crowd workers are sometimes attributed to workers being lazy, stupid, or deceitful. While much prior work has sought to improve the quality of collected crowd data, relatively less work has sought to better understand and characterize the types of judging disagreement the crowd tends to exhibit. Moreover, most work has assumed crowd disagreement constitutes error rather than trying to distinguish valid disagreement from actual error.
Understanding the reasons behind judging disagreement is difficult without having insights into the judges' thought-processes. Consequently, prior work studying relevance judgments of primary vs. secondary assessors has sometimes relied on research methods involving interaction with participant judges, such as think-aloud [1] and interviewing [26]. However, it can be challenging to apply these methods on the current crowdsourcing platforms.
Our earlier work [20, 21] proposed a Rationale Task (RT) design for collecting crowdsourced relevance judgments. In particular, simply asking judges to provide short excerpts from each document to explain their judgment for it was shown to yield a multitude of benefits. In this work, we investigate how we can further exploit these rationales to gain new insights into reasons for judging disagreement, especially with remote crowd work. Largely following our original RT design, we collect roughly 25K crowd judgments for 5K document-topic pairs sampled from the 2014 TREC Web Track [11]. As a refinement, we show that having judges focus on judging within a topic, rather than across topics, improves label quality. Overall, we find that crowd judgments are good enough for ranking information retrieval (IR) systems reliably.
Next, we conduct a qualitative analysis using rationales to understand the disagreements and present a novel taxonomy of types of disagreement. In our analysis, we manually inspect 1K crowd judgments for 200 documents (5 judgments per document) in which the aggregated crowd judgment differs from the original TREC judgment, and we assess the relative importance of each disagreement

805

Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease

cause. Our analysis distinguishes between valid disagreement due to subjective considerations (e.g., relevance thresholds) from consistently recognizable human error (e.g., clearly missed evidence of relevance). Among the 200 documents we inspected, we agreed with TREC assessors in only 51.5% of the cases, disagreeing otherwise due to perceived human error (in 21.5%), subjective considerations (20%), and other miscellaneous reasons (7%). Similarly, we agree with 51% of the 1K crowd judgments while disagreeing due to perceived human error (37% of cases), subjective considerations (6%), and other miscellaneous reasons (6%).
Contributions of our work are as follows:
· We show that topic-focused crowd judging improves quality vs. our earlier design judging across topics [21].
· We show that crowd judgments we collect are largely valid and plausible, and that they enable reliable ranking of participant IR systems in the 2014 TREC Web Track.
· We demonstrate the value of assessor rationales for helping to explain disagreements in relevance judging.
· We present a novel taxonomy over types of disagreement, qualitatively and quantitatively distinguish objective vs. subjective disagreements, and estimate the extent of human error in trusted judgments.
· We share our WebCrowd25k dataset1, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed. In separate work [13], we also describe and share (3) crowd judging behavioral data.
The remainder of this paper is organized as follows. We first present related work in Section 2. We then describe our prior Rationale Task design [21] in Section 3. Section 4 explains our crowd task design and collection, and evaluates the quality of the judgments. In Section 5, we discuss our qualitative analysis and outline our novel taxonomy of disagreement types, presenting the results of the analysis in Section 6. Finally, we conclude in Section 7.
2 RELATED WORK
2.1 Reasons behind Disagreements
Al-Harbi and Smucker [1] categorized the reasons for disagreement into four groups: difficulty in applying the search topic, difficulty in processing the document, secondary assessor mistakes, and primary assessor mistakes. Sormunen [26] found that disagreement between primary and secondary assessors become more likely with ambiguous topic descriptions. Through examining documents from the TREC 2009 Legal Track, Grossman and Cormak [15] showed that disagreement is mainly caused by human error. Chandar et al. [9] found that longer, less coherent and easily comprehended documents provoke more disagreements. Our analysis includes most of these disagreement reasons and adds subjective considerations and technical judging issues, including four types of crowd errors.
1 http://qufaculty.qu.edu.qa/telsayed/datasets/webcrowd25k/

2.2 Understanding Disagreements
Al-Harbi and Smucker [1] conducted a think-a-loud study to better understand secondary assessors' thought processes during relevance judging. Sormunen [26] interviewed secondary assessors to learn about reasons for disagreement. Wakeling et al. [28] recorded the behavioral data of assessors (i.e., mouse movements, screenshots and others) and then conducted interview in order to compare primary and secondary relevance judgments. While these methods are effective to understand the disagreement reasons, it is challenging to implement them with current crowdsourcing platforms.
Alonso and Mizzaro [2, 3] asked crowd workers to provide optional justifications for their judgments in free text format. In contrast, our RT design [21] requires rationales be provided in the form of document excerpts. While free text gives workers more flexibility, there is no easy quality check. On the other hand, requiring excerpts can be used to detect spammers, as shown in our analysis.
2.3 Assessing NIST Judgments
Scholer et al. [25] showed that NIST assessors judge similar documents differently, suggesting that their judgments can be inconsistent. Kazai et al. [18] found that there is a strong bias to Wikipedia pages (i.e., overrating them) in NIST judgments. Various studies also report that NIST assessors made mistakes in relevance judging [1­3, 8]. In our analysis, we also found that NIST assessors made mistakes but we also distinguish subjective vs. objective forms of our disagreement with NIST judgments.
2.4 Crowd Judgment Quality
Prior work has a mixture of findings about the quality of crowd judgments. Alonso and Mizzaro [2, 3] claim that crowd judgments can be a reliable alternative for relevance assessment. Blanco et al. [6] show that the ranking of IR systems do not change significantly when crowd judgments are replaced with NIST judgments. Kazai et al. [18] found that crowd workers perform as well as professional judges untrained in web search judging. On the other hand, there are also studies opposing using judgments of non-trained secondary assessors. Bailey et al. [4] show that judgments of primary or trained secondary assessors' judgments cannot be replaced by non-trained secondary assessors' judgments. Kinney et al. [19] report that judgments of non-expert assessors for domain-specific queries cause significant errors affecting system evaluation. Clough et al. [10] compare crowd judgments with expert judgments for domain-specific search tasks. They report that while crowd workers are able to rank systems correctly, they are less capable of differentiating levels of high accurate results than expert assessors. In our work, we show that crowd judgments do not yield significant changes in system ranking for general knowledge topics, establishing that they are reliable enough for practical use.
3 ORIGINAL RATIONALE TASK DESIGN
In this section, we describe the Rationale Task (RT) design from our prior work [21], which we adopt and modify in this work.
Instructions: We found in [21] that providing overly-specific instructions, examples, or corner-case notes ultimately left workers frustrated, both because of the length of instructions and because it made them feel more unsure about their final answer. As a result,

806

Session 7A: Crowdsourcing & Assessment

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA

we omitted all such specific instructions and provided a minimal task description, relying on a simple and intuitive judging scale to guide crowd workers (described below).
Judging Scale: Our RT design adopts a simple 4-point relevance scale: {Definitely Not Relevant, Probably Not Relevant, Probably Relevant, Definitely Relevant}. There are several proposed benefits of this scale: the relevance categories are evenly spaced across the spectrum of relevance, which makes conversion to binary judgments straightforward and offers flexibility to judges without overcomplicating the scale; each of the relevance categories features colloquial language, rather than jargon common in the IR space; and the categories are symmetric with regard to adjectival descriptors (e.g., Probably Relevant vs. Probably Not Relevant).
Rationales: We asked workers in [21] to provide a rationale to support their labeling decision by selecting 2-3 sentences from documents. These excerpts enable new avenues of automated analysis and also provide expressive freedom for workers to argue in favor of the quality of their work and a new level of transparency which judgments alone do not provide.
Serving Tasks To Workers: In our earlier work [21], judging tasks were distributed to crowd workers uniformly at random from among the pool of all available judging tasks. As a result, workers might jump back and forth between different topics during judging.
4 MODIFIED DESIGN, DATA, & ANALYSIS
In this section, we present and evaluate our modified rationale task design at a much larger scale than in our earlier work [21]. We summarize the main differences in our experimental design vs. our prior study [21] as follows. Firstly, whereas our prior study used 700 NIST-judged documents from the 2009 Web Track, here we use 5K documents from the 2014 Web Track (Section 4.1). Secondly, we ask assessors to judge crawled webpages in the test collection, since this is what NIST does, rather than live versions of pages we used before. Thirdly, whereas our prior study judged a convenient, balanced, and largely random sample of documents, here we strategically sample documents to support ranking of participant IR systems. Fourthly, we employ the topic-focused judging order vs. judging across topics (Section 4.2). Fifthly, we decrease the price of relevance judging from $0.10 to $0.05 as a means of further exploring the stability of RT. Finally, in separate work [13], we describe, analyze, and share additional behavioral data we also collected during crowd judging.
In Section 4.3, we measure accuracy of crowd judges wrt. TREC assessors under varying aggregation methods. Section 4.4 reports the impact of crowd judging disagreement on the ranking of IR systems.
4.1 Test Collection & Document Sampling
We focus on the 2014 TREC WebTrack [11] (WT2014), which is the most recent TREC Web Track. WT2014 uses ClueWeb122 as document collection and contains 50 topics. The topics are a mixture of broad and specific queries, including navigational, informational and multi-faceted topics. For each topic, only a title and a one-sentence description were recorded. We focused on the ad-hoc search task, for which there are 14,432 relevance judgments. In comparison with our 4-point scale (Section 3), NIST's six-point judging
2 http://www.lemurproject.org/clueweb12.php/

scale slightly differs: 1) spam or junk; not useful for any reasonable purpose, 2) does not provide useful information, 3) provides some information, 4) provides substantial information, 5) dedicated to the topic, and 6) the home page of an entity named in the topic. To induce binary judgments, we map the first two categories to "nonrelevant" and the remaining four to "relevant". While this six-point scale may valuably support evaluation metrics using graded judgments, it may have side effects when collecting crowd judgments because of its complexity, as mentioned in Section 3. Additionally, we focus on binary disagreements (Section 5) and use mean average precision (MAP) in our evaluation (Section 4.4).
We sampled 100 documents to re-judge for each topic (i.e., 50 × 100 = 5, 000 documents in total) using statAP [23] weighted sampling. According to the original TREC judgments, 45.4% of these documents are relevant (Table 3). Next, we collected 5 relevance judgments for each document using our NIST-Style Rationale Task on Mechanical Turk. In post-analysis, we found 9 documents having only 4 crowd judgments, so we removed them for consistency, leaving 24,955 judgments for 4,991 documents across 50 topics in our shared WebCrowd25k dataset.
4.2 Topic-Focused Judging with Rationales
Our prior study [21] reported 92% agreement with NIST judgments in Web Track 2009 using the RT design. We adopt this design with a slight modification in the order of documents presented to the crowd workers, intended to better reflect the traditional judging approach employed by NIST. In the original RT design, a crowd worker might first judge a document for Topic X, followed by a document for Topic Y, followed by a Topic Z, before finally returning to Topic X. This varies significantly from the traditional TREC paradigm in which each topic is judged by a single (primary) assessor. To more closely mirror the TREC style of collecting relevance judgments, we propose a topic-focused, NIST-Style Rationale Task Design in which workers continue to judge documents from the same topic until it is exhausted, and only then move on to a fresh topic. This allows assessors to calibrate their internal topic definitions and relevance thresholds [24]. We will show that this yields higher judgment agreement than the original RT design.
To investigate the effect of topic-focused judging, we randomly selected 370 documents (out of the 4,991 sample above) across 27 topics and collected 5 relevance judgments per document (1850 judgments in total) using both the original RT design (random ordering) and the topic-focused judging described above. Overall, the topic-focused design produced 10.8% higher absolute accuracy (78.1% vs. 67.3% accuracy) over the original randomized ordering (aggregating crowd judgments by majority voting.). In analyzing the randomized ordering judgments, we identified several individual cases where workers were over-rating the relevance of documents, suggesting that they were unable to build an effective relevance threshold [24] while constantly oscillating between topics.
4.3 Accuracy of Crowd Judgments
We next discuss the quality of the relevance judgments we collected with respect to NIST judgments using the large sample of 4,991 documents. We used varying aggregation methods such as Majority Voting (MV) and Dawid-Skene (DS) [12]. We also considered a

807

Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease

threshold filtering (TF) method we proposed earlier [21] which filters crowd judgments based on overlap between rationales and then applies a given aggregation method to remaining judgments.

Table 1: NIST-Style RT Agreement Results wrt. NIST.

Aggregation Method Majority Voting (MV) Dawid-Skene (DS) Threshold Filtering & MV Threshold Filtering & DS

Accuracy 0.799 0.798 0.779 0.749

Results in Table 1 report simple accuracy of aggregated crowd judgments vs. TREC judgments. MV and DS appear effectively indistinguishable, while filtering performs slightly worse. Most notable, however, is that all of the accuracies are far lower than what reported on WT2009 in our prior study [21]: 0.92. One can imagine a variety of reasons for this; the start of Section 4 discusses a number of differences in experimental design and setup. We analyze the impact on ranking IR systems in Section 4.4, and further analyze disagreements in Section 5.
4.4 Effect on Ranking IR Systems
Next, we assess the impact of judging disagreement on ranking of IR systems participating in WT2014. Following typical TREC evaluation, we induce the ground-truth ranking of systems by using all (14,432) NIST judgments and evaluating systems by mean average precision (MAP). We refer to this ranking as MAP-NIST. In addition to this ground-truth ranking, we also rank the systems based only on the reduced set of 100 documents per topic sampled via statAP (Section 4.1), using either NIST judgments (StatAP-NIST) or crowd judgments (StatAP-Crowd). We calculate the correlation between these three rankings using Kendall's  and AP [30], a variant of Kendall's  giving higher weight to swaps at higher ranks.

Table 2: Correlation of IR system rankings on WT2014.

Correlation Measures
STATAP-NIST STATAP-Crowd

MAP-NIST



AP

0.905 0.876

0.937 0.921

STATAP-NIST



AP

1.0 1.0

0.947 0.939

Results are shown in Table 2. Remarkably, we see a higher ranking correlation score wrt. ground-truth ranking using crowd judgments than using NIST judgments (0.905 vs. 0.937 for  and 0.876 vs. 0.921 for AP ). While this does not mean that crowd workers provide better judgments than NIST, it does indicate that disagreements between NIST and crowd judgments are not hurting IR system rankings according to either rank correlation metric.
In order to further explore this, we conduct an additional experiment. We simulate crowd error on the subset of statAP-sampled documents by randomly introducing errors on 20% of the documents (i.e., 1 - the 0.799 accuracy of MV-aggregated crowd judgments vs. NIST judgments). Next, we rank the systems using this judgment set and calculate Kendall's  and AP wrt. using the real NIST judgments for the subset. We repeat this process 100 times

and calculate average  and AP across trials. Results of this simulation are lower ( = 0.882, with  = 0.032, and AP = 0.861, with  = 0.039) than when using the real crowd disagreements (Table 2). This suggests that crowd workers tend to disagree with NIST on documents that do not greatly impact the ranking (something Voorhees [27] reported earlier in analyzing disagreements among NIST judges).
Thus, despite the lower accuracy of crowd judgments seen in Section 4.3 vs. our prior study [21], we still see that using crowd judgments easily surpasses the traditionally established  = 0.9 threshold for reliable ranking of IR systems [27].
5 UNDERSTANDING DISAGREEMENT
In this section, we explain our qualitative analysis into reasons for disagreement using the rationales provided by the crowd judges. We first present the methodology of our analysis (Section 5.1), followed by the reasons for disagreement we identified (Section 5.2).
5.1 Methodology
To investigate the reasons for judging disagreements, we manually inspected a sample of topic-document pairs (See Section 6.1) in which the aggregated judgment of the crowd disagrees with the NIST judgment. Two authors of this paper judged the relevance of sampled documents independently and with no prior knowledge of other judgments by NIST or the crowd. Next, each of the two authors examined the respective NIST and crowd judgments and assigned one of four stance labels: 1) Strongly Agree with NIST; 2) Slightly Agree with NIST; 3) Slightly Disagree with NIST; or 4) Strongly Disagree with NIST. In "strong" cases, we perceive clear evidence in the document for our own judgment. With "slight" cases, we agree with one of the judgments but believe that the other is also reasonable, given a different interpretation of the topic or perception of relevance (e.g., relevance threshold). Finally, the two authors met in person to compare their labels and discuss their reasoning, ultimately arriving at a single, reconciled label for each case. Note that our stance labels (individual or reconciled) can be easily converted to graded relevance judgments.
Next, we sought to understand why there is disagreement by performing the following analysis for each document we judged:
· Understanding disagreements with NIST: For each case where we disagree with NIST, we carefully inspect the document and NIST judgment.
· Understanding disagreements with the crowd: We follow a similar process for each case in which we disagree with a crowd judgment. We consider each individual crowd judgments and rationale. Even when our judgment matches a crowd judgment, we still consult the rationale provided to verify it is reasonable.
· Additional Categories: Independent of our stance labels, we also annotated each document-topic pair to note if (1) the document is very long; (2) the document has low readability (e.g., poor web design or writing); or (3) expert knowledge appears necessary to judge the document-topic pair (e.g., knowledge of the American Revolution needed to judge a topic about the war). Note that these labels are not mutually exclusive; i.e., a document may have more than one.

808

Session 7A: Crowdsourcing & Assessment

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA

In some cases, we identified more than one possible cause for disagreement: ambiguity in the topic description, and/or a crowd worker may have misunderstood the topic or have deemed the relevant content insufficient. In such cases, we determined the reason we believe to be most likely, yielding a single best reason for every case. We repeated this reasoning process over two full passes in order to further increase the consistency of our analysis. The entire process took around 30 hours.
Despite careful inspection, our method has certain limitations:
(1) Our understanding of each topic is limited to what the original NIST assessor recorded, via topic definition and judgments. Some more nuanced understanding of an intended topic may have eluded us as secondary assessors.
(2) While we carefully analyzed a small set of disagreements, we are certainly fallible and susceptible to human error.
(3) To further understand the thought process of crowd workers, we rely on (our interpretation of) their provided rationales, which are limited to text excerpts from the documents. Rationales appear most useful to show why a document is relevant and less helpful for showing why a document is not relevant. In the latter case, we do our best to guess the reason for disagreement using all evidence we have at our disposal.
5.2 Reasons for Disagreement
In this section, we discuss in detail the reasons we observed for judging disagreements. We further induce a novel taxonomy over those reasons, presented in Figure 1.
5.2.1 Human Error. We sometimes observed seemingly unambiguous evidence for document relevance. For instance, in judging the document clueweb12-0310wb-50-29927 for topic 273 ("Find Wilson's Disease Association website"), the NIST assessor judged it as relevant, but it is neither the website nor has the URL for it. Therefore, in cases where we strongly disagree (with either NIST or crowd), we designate the reason for the disagreement as human error. Due to lack of any insight into NIST judgments beyond topic descriptions and judgments, we are unable to further understand this NIST judgment. However, inspecting crowd rationales led us to categorize 4 types of crowd errors:
(1) Topic Misunderstanding. In this category of mistakes, we believe the crowd worker made a good faith effort to judge the document but misunderstood the topic. Ideally, the rationale for a document judged relevant should indicate a part of the document making it relevant to a particular topic. Judging a document as relevant but providing a rationale that is not closely matched to the topic suggests that the crowd worker misunderstood the topic. For instance, in judging document clueweb12-0204wb-61-01007 for topic 273 ("Find Wilson's Disease Association website"), one crowd worker judged it "Definitely Relevant" with the following rationale:
Wilson's disease is an inherited condition which causes copper to build up in the body. This excess copper tends to collect in the brain and liver but can also be found in the corneas (in the eyes) and the kidneys. If . . . not treated properly, it can cause very serious symptoms.
The chosen excerpt from the document explains Wilson's disease, which indicates an over-interpretation of the topic definitions, such

that the crowd worker might have thought that this information about Wilson's disease would be useful for a person who searches for WDA's website. This is also similar to Wakeling et al. [28]'s finding: Secondary assessors can sometimes judge documents as relevant because of thinking that the information on that page can be also useful for a person who makes that search.
(2) Missing Relevant Content. Another reason for disagreement could be the lack of concentration or other unknown human error that causes missing a relevant content in a document. We used this label when crowd workers judged a document as "definitely not relevant" or "probably not relevant" with a rationale not relevant to the topic while there is a clear evidence in the document to be relevant.
(3) Spammers. Rationales are also useful in detecting workers who clearly do not follow task instructions, acting as "spammers". We observed 4 types of behavior in this category:
· Providing rationales from other documents · Providing off-topic rationales for documents judged relevant · Reporting a page load error3 yet providing a rationale · Judging a document with only textual content as relevant yet
using our pre-defined rationale text for non-textual pages4
While we typically assume that crowd workers' labels are correct when they match our own, when the label space is small (e.g., binary), we must also account for accidental agreement. We found rationales to be useful in identifying such cases. For example, a judge providing a rationale from the wrong document indicates that the judgment is likely spam, even if the label seems correct.
(4) Relevant Rationale for Not-Relevant Judgment. During our analysis, we also observed that some crowd workers judged a document as "Definitely Not Relevant" but provided a rationale which is a definitely relevant statement to the topic, i.e., a conflicting rationale. For instance, in judging the document clueweb12-1611wb-41-22823 for topic 270 (i.e., "Find quotes from Sun Tzu"), a worker provided the following rationale:
sun tzu said, "the good fighters of old first put themselves beyond the possibility of defeat, and then waited for an opportunity of defeating the enemy."
Though this rationale is a perfect example to judge the document as relevant, the worker judged the document as "Definitely Not Relevant". Our earlier study [21] noted such behavior as well, which seems to be most likely due to clicking too quickly or misunderstanding the topic or the judging scale.
5.2.2 Ambiguous Topic Definition. While a primary assessor formulates a clear definition of the information need in their head, secondary assessors (including ourselves) are reliant upon the primary assessor's written topic description to understand the information need. Overly terse, incomplete topic descriptions may introduce ambiguity in the information need and relevance judging.
In such cases, while we disagree with the NIST or crowd judgments, we think that his/her judgment is also reasonable based on
3In addition to our 4-point judging scale, workers were also provided a fifth option to indicate that a given web page did not load and so could not be judged. 4For cases in which a relevance judgment depends on non-textual content, we asked workers to enter a particular pre-defined text string.

809

Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease

Reasons for Disagreement

NIST Error

Crowd Error
Misunderstanding
Missing Relevant Content
Spammer Relevant Rationale for Not-Relevant
Judgment

Different Perception of Relevance
Relevance Threshold
Indirect Relevance

Ambiguous Topic Definition

Figure 1: Reasons for Disagreement in Relevance Judging.

Technical Issues
Page Load Error
Redirecting Page Missing Multimedia

the topic description (given another interpretation of the topic). For example, in judging the documents for topic 261 (i.e., "What folk remedies are there for soothing a sore throat"), it seems that the NIST assessor did not consider home-remedies nor herbal-remedies as folk remedies and thus did not judge such documents as relevant.
5.2.3 Different Perception of Relevance. Assessors may disagree also due to the inherently subjective nature of relevance judging. Many such factors impact relevance judgments, such as novelty of the document [29], reliability of the source [31] and others. We may have disagreed with crowd workers or NIST assessors due to such perception. We identified two types of cases:
(1) Relevance Threshold. The amount of relevant content in a document has a large impact on our relevance judgment, as expected. We see this impact more obvious by checking the rationales. For instance, in judging the document clueweb12-1601wb-52-10051 for topic 290 (i.e., "How do you identify a Norway Spruce?"), one of the crowd workers provided the following rationale:
The Norway Spruce (Picea abies) is a large evergreen coniferous tree, growing to 35-55 m tall and with a trunk diameter of up to 1-1.5 m. The shoots are orange-brown and hairless.
While showing that the document provides some relevant information for the topic, the crowd worker still judged it as "Probably Not Relevant", indicating that although s/he found some relevant content, s/he did not find that the relevant material was sufficient to satisfy the information need. Interestingly, three crowd workers also provided rationales covering these sentences but judged differently (i.e., two as "Definitely Relevant," and one as "Probably Relevant"), further suggesting different thresholds for relevance.
We also found cases in which we disagreed with NIST likely due to different relevance thresholds. For example, for topic 278 (i.e., "What are the lyrics to the theme song for 'Mister Rogers' Neighborhood"), the NIST assessor judged document clueweb121006wb-74-08027 as "Not Relevant", likely due to it only containing the first four lines of the lyrics, rather than the full lyrics. We believe this should be sufficient to judge the document as relevant.

(2) Indirect Relevance. According to track judging instructions [11], a page should be judged as "Not Relevant" if it does not contain a relevant content but only provides a link or mentions a resource name (e.g., a book or a course given by a university) directly related to the topic of interest. However, we found that NIST assessors and crowd workers consider the links or resource names as relevant information in many cases we inspected.
For example, the NIST assessor judged document clueweb120700wb-28-32790 as relevant to topic 267 (i.e., "What are the lyrics to the song Feliz Navidad") though the page does not contain the lyrics, only a link named "View Feliz Navidad Lyrics". Also in judging the document clueweb12-1509wb-34-18722 for topic 276 (i.e., "How has African American music influenced history, including cultural history"), we observed that the NIST assessor judged the document as relevant even though it only lists courses offered by a university. While some courses seem relevant to the topic, there is no relevant information for the topic in the page. Similarly, we also observed that some crowd workers give the link as their rationale, suggesting that they consider links in their judging process.
5.2.4 Technical Issues. Correctly rendering crawled web pages is challenging due to the complex structure of web pages (e.g., containing multimedia files from the host server and also from other web addresses). Therefore, relevance judging of crawled web pages imposes many technical challenges which may have an impact on the relevance judgments. We identified the following issues:
(1) Page Loading Error. Crowd workers explicitly indicated some pages did not load to be judged.
(2) Missing Multimedia. Rendering web pages using crawled pages is challenging because many pages use varying multimedia files from other web addresses (e.g., YouTube) which may not be available at the time of collecting relevance judgments. Even if the multimedia files are captured during the crawling, it can be challenging to modify the web page source to get the images to display correctly. Missing multimedia becomes a big problem especially with topics such as topic 258 (i.e., "Find pictures of a hip roof").

810

Session 7A: Crowdsourcing & Assessment

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA

(3) Redirecting. In our analysis, we noticed that some pages are redirecting to other pages, causing judging of different documents than we believe were viewed by NIST assessors. This problem was potentially more severe in our earlier study using live pages [21].
6 RESULTS & DISCUSSION
In this section, we present and discuss the results of our analysis. We first explain how we sampled the documents to be manually inspected (Section 6.1) and then show our own judgments for the sampled documents (Section 6.2). Finally, we present the distribution of disagreement reasons (Section 6.3).
6.1 Sampling Documents to be Inspected
Table 3 shows the distribution of documents at varying agreement levels (AL) (i.e., the percentage of crowd workers agreeing with the NIST assessor in judging a particular document when graded judgments are collapsed to binary judgments). To select documents to be analyzed, we opt for stratified sampling. In our sampling method, we consider 3 different agreement levels of crowd workers that cause disagreement with NIST judgments (i.e., 0%, 20%. 40%) and two possible NIST-judged binary relevance judgments (i.e., relevant and not relevant) separately, yielding 6 (=3x2) different combinations. We sample 25 documents from each case. In order to have a better representation of the disagreements, we also randomly sample 50 more documents from the remaining disagreement cases, resulting in 200 documents in total. The distribution of the 6 cases in our sample is given in Table 4.

Table 3: Distribution of Documents at Varying Agreement Levels between NIST and Crowd Workers. The shaded rows represent disagreement between crowd and NIST based on majority voting.

Agreement Level 0% 20% 40% 60% 80% 100% Total

NIST-Judged Not Relevant 1.4% 4.7% 8% 11.7% 14% 14.8% 54.6%

NIST-Judged Relevant 0.6% 1.6% 3.7% 7.8% 10.9% 20.6% 45.4%

Total
2% 6.3% 11.7% 19.6% 24.9 35.5% 100%

Table 4: Document Distribution of the Manually Inspected Sample.

Agreement Level 0% 20% 40% Total

NIST-Judged Not Relevant 28 39 48 115

NIST-Judged Relevant 25 29 31 85

Total
53 68 79 200

6.2 Our Own Relevance Judgments
In this section, we discuss our own relevance judgments for the sampled documents. We discuss the results in two different ways: 1)

results within the stratified sample of documents (Section 6.1); and

2) results projected to each of the six cases NIST and aggregated

crowd judgment differ, based on the frequency of each case. See

the first three rows of Table 3 to do this projection.

The summary of our judgments is given in Table 5. In 23% and

25.5% of the cases, we slightly and strongly disagree with NIST,

respectively. This means that we disagree with NIST assessors in

48.5% (=23% + 25.5%) of the cases we inspected, projected to 47% of

all disagreement cases. This suggests that the quality of the crowd

judgments is much higher than we earlier calculated (Section 4.3).

Considering only NIST-judged relevant documents, as expected,

we agree with NIST in many more cases than we disagree: 51

(=7+10+12+4+6+12) vs. 34 (=7+5+3+7+8+4). However, we see the

opposite pattern over the NIST-judged non-relevant documents: 52

(=10+9+21+1+6+5) vs. 63 (=8+11+12+9+13+10). This suggests that

our judgments are even more liberal than NIST assessors [26].

We also observe that as fewer crowd workers agree with NIST,

our own agreement with NIST similarly decreases, as expected.

Specifically, among documents with crowd AL of 40% (i.e., both

NIST-judged relevant and non-relevant documents), we disagree

with

NIST

assessors

in

36.7% (=

12+10+3+4 48+31

)

of the

cases.

Our

dis-

agreement

(=

8+9+7+7 28+25

with NIST ) when the

increases crowd AL

to is

54.4% (=

11+13+5+8 39+29

)

and

58.5%

20% and 0%, respectively.

6.3 Distribution of Disagreement Reasons
In this section, we first discuss the results of our qualitative analysis in the all sampled documents (Section 6.3.1). Next, we focus on only hard-to-process documents (Section 6.3.2) and the problematic topics where there is a high disagreement between NIST and crowd workers (Section 6.3.3).

6.3.1 Distribution across the whole sample. In our analysis, we assessed 200 NIST judgments and 1000 (= 200x5) crowd judgments. The distribution of each category of agreement/disagreement is shown in Figure 2.
There are several observations we can make from these results. Firstly, our agreement ratios with NIST and crowd workers are similar (51.5% vs. 51.2%). However, among the agreements with crowd judgments, 27.1% (= 1% + 13.6% + 12.5%) of them appear to be accidental agreement (Section 5.2).
Secondly, the human error ratio of NIST assessors is lower than human error ratio of crowd workers, as expected (21.5% vs. 36.7%). Among crowd errors, missing relevant content and misunderstanding are the main problems, contributing to about 80% of all crowd errors. Judgments from spammers are just 16.3% of the crowd errors, that is, 6% (= 36.7% x 16.3%) of all cases we inspected. We also noticed that among spam judgments, 78% of them use rationales from other documents. Therefore, most of such spam can be easily detected by checking whether each rationale actually exists in the document being judged. We automatically identified that 2,878 crowd judgments in the entire collection (i.e., 11.5% of all crowd judgments) use rationales from other documents.
Misunderstanding and conflicting judgments might be resolved by providing better topic descriptions and task instructions, suggesting that 42.8% (= 39.5% + 3.3%) of crowd errors (i.e., 15.7% (= 42.8% x 36.7%) of the sample and 18.6% (projected) of the all disagreements) could be eliminated by the "perfect" task design. However, missing

811

Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease

Table 5: Our Relevance Judgments. The ratio of each case with respect to the total sample size is given in parentheses.

NIST Binary Judgment Crowd Agreement Level
Strongly agree with NIST Slightly agree with NIST Slightly disagree with NIST Strongly disagree with NIST
Total

Not Relevant

0%

20%

40%

10 (5%) 1 (0.5%) 8 (4%) 9 (4.5%)

9 (4.5%) 6 (3%) 11 (5.5%) 13 (6.5%)

21 (10.5%) 5 (2.5%) 12 (6%) 10 (5%)

28 (14%) 39 (19.5%) 48 (24%)

0%
7 (3.5%) 4 (2%) 7 (3.5%) 7 (2.5%)
25 (12.5%)

Relevant 20%
10 (5%) 6 (3%) 5 (2.5%) 8 (4%)
29 (14.5%)

40%
12 (6%) 12 (6%) 3 (1.5%) 4 (2%)
31 (15.5%)

Total
69 (34.5%) 34 (17%) 46 (23%) 51 (25.5%)
200 (100%)

Figure 2: Distribution of Agreement & Disagreement Reasons.

relevant content, which contributes to 15% of the sample and 12% (projected) of the all disagreements, may be a harder problem to fix. On the other hand, simply paying more could incentivize higher quality work given a more complex task [17].
Thirdly, the ratio of different perceptions of relevance for NIST judgments is much higher than its ratio for crowd workers (20% vs. 5.6%). Relevance threshold constitutes to 65% and 80% of those cases for NIST and crowd workers, respectively.
Interestingly, we also found that even expert NIST assessors did not always follow track judging instructions [11] wrt. indirect relevance, sometimes judging a document as relevant even when it only pointed to a resource (a web page or a course/book name) that is potentially relevant (7% of all cases we inspected).
6.3.2 Distribution across Hard-to-Process Documents. As noted earlier, during our inspection, we labeled the documents that are hard to process using three different labels. Figure 3 shows the distribution of reasons for the documents with these labels. In all three cases, the human error ratio for crowd workers increases compared to its ratio among all sampled documents (36.7% vs. 45%, 45.2%, and 45.6%). On the other hand, human error ratio for NIST is lower than its ratio on all sampled documents (21.5% vs. 12.5%, 16.1%, and 5.6%) suggesting that NIST assessors are not affected by the difficulties in processing the documents. Among crowd errors on document-topic pairs requiring expert knowledge, we found that 55% of the disagreement reasons is misunderstanding. On the other hand, we found that 69% and 83% of the crowd errors are due to missing relevant content for long documents and documents with low readability, respectively.

(a) Document-Topic Pairs needing Expert Knowledge (16). Top Bar: NIST, Lower Bar: Crowd.
(b) Long Documents (21 cases). Top Bar: NIST, Lower Bar: Crowd.
(c) Documents having Low Readability (31). Top Bar: NIST, Lower Bar: Crowd. Figure 3: Distribution of Agreement & Disagreement Reasons for Hard-To-Process Documents. The number of documents for each case is given in parentheses.

812

Session 7A: Crowdsourcing & Assessment

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Crowd vs. Expert: What Can Relevance Judgment Rationales Teach Us About Assessor Disagreement? SIGIR '18, Ann Arbor, MI, USA

Figure 4: Agreement between NIST and Crowd Workers. The horizontal line represents the average agreement among all topics.

Table 6: Topics with the Lowest Agreement between NIST and Crowd Workers.

Topic ID
261 273 260 288 265

Topic Description
What folk remedies are there for soothing a sore throat Find Wilson (Wilson's) Disease Association web site Find a list of the major battles of the American Revolution Find quotes from Fidel Castro What were the ten worst tornadoes in the USA

Sample Size
9 16 8 13 7

Our Judgment

Agree w/ NIST Disagree w/ NIST

1

8

12

4

7

1

9

4

5

2

6.3.3 Topic Specific Disagreement Reasons. In our analysis, we noticed very low agreement between NIST and crowd workers for a few particular topics. Figure 4 shows the agreement between NIST and aggregated crowd judgment for each topic. We investigate 5 topics where the agreement is lower than 50% to better understand such topic-specific problems. Descriptions for these topics and our stance labels (collapsed to binary) are shown in Table 6.
Topic 261. We realized that the NIST assessor did not think any remedies named as home, herbal or even grandma's remedy (in the document with id clueweb12-1911wb-01-10721) as a folk remedy. We disagree with NIST (i.e., agree with the aggregated crowd judgment) in 8 cases out of 9 in our inspected sample, where 7 of them are due to topic ambiguity. We also determined that, for this topic, 19 crowd judgments, out of 45 (= 9 x 5), in our inspected sample and 184 judgments (among 100 x 5 = 500) in the whole collection are actually spam.
Topic 273. We mostly agree with NIST assessors (in 12 cases out of 16). In 46 crowd judgments, out of 80 (= 16 x 5), we noticed that the crowd workers misunderstood the topic and usually provided a text that describes Wilson's disease.
Topic 260. We determined that 14 judgments in our inspected sample and 194 judgments in the whole collection for this topic are actually spam. Misunderstanding is the second most important disagreement reason we found (25% of the judgments we inspected). We observed that the crowd workers were very liberal in judging documents as relevant if the document is somehow related to American Revolution even though it does not mention any battle name. This appears consistent with past work [19] finding that laymen often fall back on simple query term matching in assessing relevance for topics which exceed their level of topical expertise.

Topic 288. In our inspected sample of this topic, 40% of our disagreements with crowd workers appear to be due to workers missing relevant content. We labeled 5 out of 13 documents as either "too long" or "low readability" and found that 60% of the disagreement reasons for these 5 documents are missing relevant content, suggesting these factors may have negatively impacted judging. We also found that 25% of the judgments suffered from technical issues (20% page load error and 5% redirecting).
Topic 265. The main problem appears to be the high ratio of spam (51% of the crowd judgments we inspected and 46% of all crowd judgments over the entire collection for this topic).
We automatically detected spammers who provide rationales that do not exist in the documents to be judged and noticed that automatically-detected spam ratios are particularly high for a few specific topics, and that the agreement ratios for these topics were generally lower than others. A likely explanation is that our topicfocused judging design (Section 4.2) had the unintended side-effect of also concentrating spam within a topic instead of distributing it evenly across topics.
7 CONCLUSION & FUTURE WORK
While crowdsourcing offers a low-cost, scalable way to collect relevance judgments [2, 14], lack of transparency with remote workers has limited understanding about the quality of collected judgments. In prior work [20, 21], we investigated the value of asking crowd workers to provide rationales explaining each relevance judgment. In this work, we scaled up this rationale-based judging design to assess its reliability in practice to support a real TREC track evaluation: the 2014 WebTrack [11]. We investigated having crowd

813

Session 7A: Crowdsourcing & Assessment SIGIR '18, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Mucahid Kutlu, Tyler McDonnell, Yassmine Barkallah, Tamer Elsayed, and Matthew Lease

judges focus on judging within a topic, rather than across topics and showed this improved the quality of collected judgments. Overall, we showed that we were able to reliably rank IR systems using crowd judgments.
To investigate the potential of rationales to provide new insight into judging disagreements between expert and crowd assessors, we then analyzed 200 disagreements between TREC and crowd judges. We found that rationales for judging relevant do provide useful insights into crowd workers' thought process and can be used to better understand disagreement reasons. However, negative rationales (for judging a document non-relevant) were usually not helpful for disagreement analysis. In total, we disagreed with NIST assessors in 48.5% of the cases we inspected, finding that many crowd disagreements appear valid and plausible. We presented a novel taxonomy over reasons for disagreement, and we share our WebCrowd25k dataset, including: (1) crowd judgments with rationales, and (2) taxonomy category labels for each judging disagreement analyzed. In a separate work [13], we also describe, analyze, and share (3) behavioral data collected during crowd judging.
Overall, we believe that forming a rationale is critical to forming a coherent relevance judgment, whether or not judging instructions explicitly require it. Our earlier results [21] showed that requiring annotators to provide rationales incurs almost no additional time, suggesting that annotators might be already doing so implicitly. While we have investigated collecting judgment rationales for crowd work, as we have earlier argued [20, 21], we believe that asking (traditional) expert judges to also provide rationales could provide a myriad of benefits, enriching both the quality and value of collected relevance judgments.
ACKNOWLEDGMENTS
We thank the many talented crowd contributors and NIST relevance assessors who provided the data for our study, and the reviewers for their valuable feedback. This work was made possible by NPRP grant# NPRP 7-1313-1-245 from the Qatar National Research Fund (a member of Qatar Foundation). The statements made herein are solely the responsibility of the authors.
REFERENCES
[1] Aiman L Al-Harbi and Mark D Smucker. 2014. A qualitative exploration of secondary assessor relevance judging behavior. In Proceedings of the 5th Information Interaction in Context Symposium. ACM, 195­204.
[2] Omar Alonso and Stefano Mizzaro. 2009. Can we get rid of TREC assessors? Using Mechanical Turk for relevance assessment. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, Vol. 15. 16.
[3] Omar Alonso and Stefano Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Information Processing & Management 48, 6 (2012), 1053­1066.
[4] Peter Bailey, Nick Craswell, Ian Soboroff, Paul Thomas, Arjen P de Vries, and Emine Yilmaz. 2008. Relevance assessment: are judges exchangeable and does it matter. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 667­674.
[5] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A test collection with query variability. In Proceedings of the 39th ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 725­728.
[6] Roi Blanco, Harry Halpin, Daniel M Herzig, Peter Mika, Jeffrey Pound, Henry S Thompson, and Thanh Tran Duc. 2011. Repeatable and reliable search system evaluation using crowdsourcing. In Proceedings of the 34th ACM SIGIR conference on Research and development in Information Retrieval. ACM, 923­932.
[7] Muhammed Fatih Bulut, Yavuz Selim Yilmaz, and Murat Demirbas. 2011. Crowdsourcing location-based queries. In Pervasive Computing and Communications Workshops (PERCOM Workshops), 2011 IEEE International Conference on. IEEE, 513­518.

[8] Ben Carterette and Ian Soboroff. 2010. The effect of assessor error on IR system evaluation. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 539­546.
[9] Praveen Chandar, William Webber, and Ben Carterette. 2013. Document features predicting assessor disagreement. In Proceedings of the 36th ACM SIGIR conference on Research and development in information retrieval. ACM, 745­748.
[10] Paul Clough, Mark Sanderson, Jiayu Tang, Tim Gollins, and Amy Warner. 2013. Examining the limits of crowdsourcing for relevance assessment. IEEE Internet Computing 17, 4 (2013), 32­38.
[11] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and Ellen M Voorhees. 2015. TREC 2014 Web Track Overview. In Proceedings of the Twenty-Third NIST Text REtrieval Conference (TREC).
[12] Alexander Dawid and Allan Skene. 1979. Maximum likelihood estimation of observer error-rates using the EM algorithm. Applied statistics (1979), 20­28.
[13] Tanya Goyal, Tyler McDonnell, Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2018. Your Behavior Signals Your Reliability: Modeling Crowd Behavioral Traces to Ensure Quality Relevance Annotations. In 6th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). 10 pages.
[14] Catherine Grady and Matthew Lease. 2010. Crowdsourcing Document Relevance Assessment with Mechanical Turk. In Proc. of the NAACL-HLT 2010 Workshop on Creating Speech and Language Data with Amazon's Mechanical Turk. 172­179.
[15] Maura R Grossman and Gordon V Cormak. 2012. Inconsistent responsiveness determination in document review: Difference of opinion or human error. Pace L. Rev. 32 (2012), 267.
[16] Maram Hasanain, Reem Suwaileh, Tamer Elsayed, Mucahid Kutlu, and Hind Almerekhi. 2017. EveTAR: building a large-scale multi-task test collection over Arabic tweets. Information Retrieval Journal (21 Dec 2017).
[17] Chien-Ju Ho, Aleksandrs Slivkins, Siddharth Suri, and Jennifer Wortman Vaughan. 2015. Incentivizing high quality crowdwork. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 419­429.
[18] Gabriella Kazai, Nick Craswell, Emine Yilmaz, and Seyed MM Tahaghoghi. 2012. An analysis of systematic judging errors in information retrieval. In Proceedings of the 21st ACM conference on Information and knowledge management. 105­114.
[19] Kenneth A. Kinney, Scott B. Huffman, and Juting Zhai. 2008. How Evaluator Domain Expertise Affects Search Result Relevance Judgments. In Proceedings of the 17th ACM Conference on Information and Knowledge Management (CIKM '08). ACM, New York, NY, USA, 591­598. https://doi.org/10.1145/1458082.1458160
[20] Tyler McDonnell, Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2017. The Many Benefits of Annotator Rationales for Relevance Judgments. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17). AAAI, 4909­4913.
[21] Tyler McDonnell, Matthew Lease, Mucahid Kutlu, and Tamer Elsayed. 2016. Why is That Relevant? Collecting Annotator Rationales for Relevance Judgments. In Proceedings of the 4th AAAI Conference on Human Computation and Crowdsourcing (HCOMP). AAAI, 139­148. Best Paper Award.
[22] Jean-Francois Paiement, James G Shanahan, and Remi Zajac. 2010. Crowdsourcing local search relevance. In Proceedings of CrowdConf.
[23] V Pavlu and J Aslam. 2007. A practical sampling strategy for efficient retrieval evaluation. Technical Report. Technical report, Northeastern University.
[24] Falk Scholer, Diane Kelly, Wan-Ching Wu, Hanseul S Lee, and William Webber. 2013. The effect of threshold priming and need for cognition on relevance calibration and assessment. In Proceedings of the 36th ACM SIGIR conference on Research and development in information retrieval. 623­632.
[25] Falk Scholer, Andrew Turpin, and Mark Sanderson. 2011. Quantifying test collection quality based on the consistency of relevance judgements. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 1063­1072.
[26] Eero Sormunen. 2002. Liberal relevance criteria of TREC-: Counting on negligible documents?. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 324­330.
[27] Ellen M Voorhees. 2000. Variations in relevance judgments and the measurement of retrieval effectiveness. Information processing & mgmt. 36, 5 (2000), 697­716.
[28] Simon Wakeling, Martin Halvey, Robert Villa, and Laura Hasler. 2016. A comparison of primary and secondary relevance judgements for real-life topics. In Proc. of the ACM on Conf. on Human Information Interaction and Retrieval. 173­182.
[29] Yunjie Xu and Zhiwei Chen. 2006. Relevance Judgment: What Do Information Users Consider Beyond Topicality? Journal of the American Society for Information Science and Technology (JASIS&T) 57, 7 (May 2006), 961­973.
[30] Emine Yilmaz, Javed A Aslam, and Stephen Robertson. 2008. A new rank correlation coefficient for information retrieval. In Proceedings of the 31st annual ACM SIGIR conference on Research & development in information retrieval. 587­594.
[31] Yinglong Zhang, Jin Zhang, Matthew Lease, and Jacek Gwizdka. 2014. Multidimensional relevance modeling via psychometrics and crowdsourcing. In Proc. of the 37th ACM SIGIR conference on R&D in information retrieval. 435­444.

814

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Automatic Ground Truth Expansion for Timeline Evaluation

Richard McCreadie, Craig Macdonald and Iadh Ounis
University of Glasgow, Scotland, UK (firstname.lastname)@glasgow.ac.uk

ABSTRACT
The development of automatic systems that can produce timeline summaries by filtering high-volume streams of text documents, retaining only those that are relevant to a particular information need (e.g. topic or event), remains a very challenging task. To advance the field of automatic timeline generation, robust and reproducible evaluation methodologies are needed. To this end, several evaluation metrics and labeling methodologies have recently been developed - focusing on information nugget or cluster-based ground truth representations, respectively. These methodologies rely on human assessors manually mapping timeline items (e.g. tweets) to an explicit representation of what information a `good' summary should contain. However, while these evaluation methodologies produce reusable ground truth labels, prior works have reported cases where such labels fail to accurately estimate the performance of new timeline generation systems due to label incompleteness. In this paper, we first quantify the extent to which timeline summary ground truth labels fail to generalize to new summarization systems, then we propose and evaluate new automatic solutions to this issue. In particular, using a depooling methodology over 21 systems and across three high-volume datasets, we quantify the degree of system ranking error caused by excluding those systems when labeling. We show that when considering lower-effectiveness systems, the test collections are robust (the likelihood of systems being miss-ranked is low). However, we show that the risk of systems being miss-ranked increases as the effectiveness of systems held-out from the pool increases. To reduce the risk of miss-ranking systems, we also propose two different automatic ground truth label expansion techniques. Our results show that our proposed expansion techniques can be effective for increasing the robustness of the TREC-TS test collections, markedly reducing the number of miss-rankings by up to 50% on average among the scenarios tested.
ACM Reference Format: Richard McCreadie, Craig Macdonald and Iadh Ounis. 2018. Automatic Ground Truth Expansion for Timeline Evaluation. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210034
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210034

Table 1: Example timeline summary extract with nuggets.

Timestamp 01/14/2012, 5:02pm
01/14/2012, 9:38pm
01/15/2012, 5:17pm

Update Text Carrying 3,206 passengers and 1,023 crew members, the Costa Concordia was on its usual route across the Mediterranean Sea and departed Civitavecchia three hours before disaster struck. As the Costa Concordia keeps shifting on its rocky ledge, many have raised the prospect of a possible environmental disaster if the 2,300 tons of fuel on the half-submerged cruise ship leaks into the sea. The Costa Concordia death toll has risen by two - as all British passengers and crew were confirmed to have survived. Two French nationals and a Peruvian died after the cruiser ran aground near the island of Giglio off the Tuscan coast on Friday night.

Information Units Crew and Passenger count, Ship route, Time of departure Fuel oil environmental hazard

People

killed

increased by 2.

Location of event

1 INTRODUCTION
With the increasing usage of social media platforms and online reporting channels, information is produced and disseminated online faster and in larger volumes than ever before. As a result, users expect to have easy access to up-to-date information about topics of interest, resulting in a large number of new real-time information-seeking scenarios. These scenarios require solutions that can identify relevant (topical), non-redundant (avoids repeated information), and timely (up-to-date) content from noisy highvolume text streams. A common class of solutions that require these characteristics are event timeline/real-time summary generation systems. Such systems take as input a topic of interest and a large volume of textual items (e.g. news articles or tweets), most of which are non-relevant and/or redundant, and select a subset of those items to be emitted over time into a timeline or an updating summary [12, 19, 26]. An example extract from the output of such systems is shown in Table 1.
This work is concerned with how to effectively and efficiently evaluate the quality of timeline items produced by such systems. Over the last few years new methodologies to evaluate the quality of timelines have been proposed [4, 17]. These methodologies typically use human annotators to manually identify atomic units of information that form a ground truth representing the information a `good quality' summary about a topic should contain (see Figure 1). Next, textual items (e.g. sentences or tweets) returned by a diverse set of timeline generation systems for the topic are pooled. Finally, the pooled text items are manually checked to see what atomic information units for the topic they cover (if any), forming <text item,information unit> pairs. Metrics such as Expected Latency Gain [12] use the resultant pairs to estimate the degree to which individual text items included in a timeline (and hence the timeline as a whole) contains relevant, non-redundant, and timely information.
The use of atomic information units as a ground truth for evaluating timelines/real-time summaries is generally accepted and has been successfully deployed within the Temporal Summarization and Real-time Summarization tracks at the Text Retrieval Conference (TREC) [4, 17]. However, while these tracks produced test collections that can in theory be used to evaluate any timeline generation system, prior works have reported cases where these

685

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

test collections fail to accurately estimate the performance of new timeline generation systems [19, 20]. In particular, it was observed in these past works that the overlap between items included in the initial pools (i.e. the assessed set) and those returned by their new proposed systems was insufficient to facilitate a robust comparison of systems. As a result, it is unclear to what extent the test collections produced during these tracks can be used to evaluate the quality of new systems that were not included in the initial pooling [5]. In this paper, we investigate to what extent current atomic information unit-based test collections are able to distinguish between timeline summarization systems with different effectiveness levels, as well as propose and evaluate automatic solutions to reduce the likelihood of errors occurring when evaluating such systems.
Contributions. The main contribution of our work is an in-depth analysis of the TREC 2013-2015 Temporal Summarization track test collections that quantifies how robust these collections are when evaluating unpooled systems, as well as an effectiveness evaluation of different automatic <text item,information unit> expansion techniques aimed at increasing the robustness of these test collections. Specifically, we tackle two main research questions:
· RQ1: To what extent can the TREC Temporal Summarization track test collections accurately rank unpooled systems?
· RQ2: If we use automatic methods to generate additional <text item,information unit> pairs can we reduce the likelihood of new systems being miss-ranked?
Our results show that the TREC 2013-2015 Temporal Summarization track test collections do not accurately estimate the effectiveness of unpooled systems. Moreover, the discrepancies observed between actual and estimated performances are sufficient to cause errors when ranking those systems. Furthermore, we found that the likelihood of encountering ranking errors is not uniform across system effectiveness levels ­ the better a system is, the more negatively it is impacted by not being pooled. For this reason, we conclude that it is potentially risky to use the TREC-TS test collections outof-the-box. We then experiment with two types of automatic <text item,information unit> expansion techniques aimed at reducing these discrepancies, namely: item-item similarity expansion and item-item semantic expansion. Our experiments using these two expansion techniques show that automatically adding even a small number of <text item,information unit> pairs can markedly reduce the number of ranking errors observed when using the text collections. In particular, we found that item-item similarity expansion can reduce the number of ranking errors by up to 30% while item-item semantic expansion can reduce the number of ranking errors by up to 50%. We conclude that these expansion techniques improve the robustness of the TREC-TS test collections, reducing the risk of miss-ranking new systems that were not pooled.
2 BACKGROUND AND RELATED WORK
2.1 Classical Summarization Evaluation
In the summarization domain, a range of evaluation methodologies have previously been proposed and examined in the literature. Early works focused on estimating the quality of fixed-length textual summaries produced by either single-document or multi-document summarization systems [22]. This is a type of textual comparative evaluation, where a summary produced by an automatic system is

compared against one or more gold standard summaries authored by humans. The idea underpinning this type of evaluation is that good summaries will be textually similar to the gold-standard summaries. To perform the similarity comparison, the ROUGE [16] suite of metrics have become the defacto standard and were used extensively as part of the Document Understanding Conference (DUC) [9] and Text Analysis Conference (TAC) [10] evaluations.
2.2 Timeline Summaries and Evaluation
Comparative evaluation approaches were used for many years to evaluate multi-document summarization systems [1], however the shift toward real-time information sharing and the associated development of timeline generation and real-time summarization solutions [19, 26, 30, 33] that push updates to users over an extended period of time required new evaluation methodologies. A timeline summary can be defined as a number of (approximately) sentence-length timestamped text items. These text items might be sentences extracted from news articles [4] or tweets [17]. A timeline summary is usually about a topic or event, and hence the text items it contains should be relevant to that topic or event. A timeline is normally visualized as a list of text items in chronological or reverse-chronological order. New text items may be added to the timeline over time, as new information emerges and is found by the summarization system. Classical comparative evaluation approaches that use metrics like ROUGE [16] and its temporal extensions [8, 12] make the assumption that both the summary to be evaluated and the gold-standard summaries are of (roughly) equal length, and that the gold-standard summaries do not change over time. As such, these classical comparative summary evaluation approaches are unsuitable to evaluate timelines.
To solve this issue, atomic information units were introduced as an alternative means to evaluate the quality of a timeline summary [12]. Atomic information units had been used in a wide range of domains prior to their application to timeline evaluation such as Web search diversification [25] and question answering [29], although the terminology used to describe them changes depending on the domain they are applied to. Indeed, atomic information units are equivalent to as sub-topics, aspects, facets, clusters or nuggets [12, 19, 23, 26, 32]. The core concept behind atomic information unit-based evaluations is that all of the units that contribute to the evaluation score for a system should be explicitly defined. In this way, evaluation can be reduced to counting the proportion of all units covered by a system. The more units covered (typically within some range constraint such as the top k documents), the better that system is. This concept maps naturally into a summarization context, where each `unit' represents a piece of information that `good' timeline summary for a topic should contain.
In practice, for evaluating timeline summaries, atomic information units have been implemented in two different manners. First in the form of information nuggets within the TREC Temporal Summarization track during 2013 to 2015. Second as information clusters within the TREC Real-time Summarization track during 2016 and 2017. We choose to use the TREC Temporal Summarization implementation as the basis for the study in this paper as it is the more complex/costly to deploy of the two [5]. We discuss this implementaton below. For those interested in differences between the two tracks we recommend the comparison by Baruah et al. [5].

686

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

2.3 TREC Temporal Summarization Track
In 2013 the Text Retrieval Conference (TREC) introduced the Temporal Summarization (TREC-TS) track that examined how to extract sentences from high volume streams of news and social content to return to the user as updates for large events [4]. TREC-TS is a timeline generation task, as defined above, where each topic is an event (represented by an event query, e.g `costa concordia disaster'), the text items are sentences extracted from a stream containing news articles, blogs and other Web documents. To avoid differences in what might be considered a `sentence', each document in the stream was pre-segmented. For a set of events, TREC-TS systems processed the high volume stream of sentences and emitted a subset of those sentences into a timeline summary.
For evaluation, TREC-TS adopted an atomic information unitbased evaluation methodology, where an information unit was referred to as a `nugget'. This methodology was inspired by earlier work developed for question answering [29] and applied in a series of evaluations in the early 2000s. Nuggets in the TREC-TS context represented atomic facts relevant to an event, represented by short natural language phrases. For example, for the event `Costa Concordia shipping disaster', the ground truth might contain nuggets such as `occurred on Friday 13th January 2012', `ran aground on a reef' and `the hull was punctured'. Under this evaluation methodology a perfect summary is one that covers all of the information nuggets for an event while being as short (contains as few sentences) as possible. Summaries containing redundant (repeated) information are penalized and were also evaluated in terms of timeliness (was the information relevant at the time it was retrieved?).
As a TREC track dedicated to supporting standardized evaluation, TREC-TS produced three test collections for evaluating timeline generation systems, one for each of the years that the track ran (2013, 2014 and 2015). These test collections each contain a number of topics (events), a high-volume stream of sentences for each topic, and a ground truth label set comprised of the information nuggets along with a <text item,information unit> (i.e. <sentence,nugget>) mapping that describes what sentences contain the information represented by each nugget. Creating the ground truth label set for each test collection was a three-step process [2, 3]:
(1) Nugget Extraction (`nuggetization'): Human assessors manually defined the information nuggets for each topic. This was achieved by having TREC assessors read the edit stream from the Wikipedia page for each topic (the page describing the event). The assessors defined new information nuggets as they encountered novel information about the event that they considered important enough to be included in a "good" summary about the topic.
(2) Sentence Pooling: Each participating system submitted a timeline summary comprised of sentences for each topic (event). The systems assign each sentence a priority score indicating how confident they are that those sentences are of high-quality. The top-k sentences by priority score were then selected and added into a pool to be assessed.
(3) Nugget Matching: Given the ground truth nuggets extracted from Wikipedia, assessors then manually checked each sentence in the pool, recording whether those sentences contained any of the information represented by the nuggets. A sentence that contains a nugget's information is referred

Table 2: Statistics of the TREC Temporal Summarization test collections from 2013, 2014 and 2015.

Statistic Number of Events Number of Nuggets Number of Matches Number of Updates

2013 9
1,168 5,071 10,377

Year 2014 15 1,394 13,635 14,652

2015 21 996 24,823 33,483

to as `covering' that nugget. The result of this is a set of <sentence,nugget> pairs, specifying which sentences contain the information represented by each nugget. It is worth noting that nuggets represent concepts, hence the matching process often requires the assessor to do more than match the text of a concept to the text of a sentence, e.g. accounting for synonyms.
The statistics of the resultant TREC-TS test collections for each year are provided in Table 2. Both the nugget extraction and nugget matching steps involved significant human effort (by NIST assessors). According to a study by Baruah et al. [5], the total assessment time spent to create the 2013 and 2014 TREC-TS test collections was around 375 hours, where over 80% of that time was spent on nugget matching.
2.4 Questions on TREC-TS Robustness
The test collections produced by TREC-TS have been used for a range of research papers since their original release [5, 13, 14, 19, 20]. However, a number of these works reported needing to add more nuggets/matches to the provided ground-truth sets to make the test collections usable. In particular, McCreadie et al. [19] reported in their paper that there was very low overlap between the sentences included in the TREC-TS pool and the top sentences selected by their system, i.e. assessment completeness [6] was low. To tackle this, they performed additional pooling and matching based on the TREC-TS guidelines, adding 22,424 sentences to the pool at a significant cost. This was then echoed in their later study [20] where they found almost no overlap between their diversification-focused system and the TREC-TS pool (see Annex A from [20] for details), again requiring the pooling and assessment of the new summaries. On the other hand, Ekstrand-Abueg et al. [11] performed a correlation study examining whether removing individual systems from the pool adversely affected the system ranking under the official track metrics. They reported high correlations between system rankings pre and post pooling, indicating that the test collections are reusable. However, they also noted that there were outlier systems that were severely affected (i.e. were miss-ranked) when they were removed from the pool.
These prior studies lead us to question to what extent the TRECTS test collections are in fact robust when evaluating unpooled systems. The studies reported in [20] and [19] required significant additional pooling and assessment effort before the collections could be used. Having to perform reassessment for each new system or summary to be evaluated reduces the value that these test collections bring to IR evaluation. Hence, in this paper, we quantify how robust these collections are for evaluating unpooled systems and also propose and evaluate automatic techniques aimed at increasing the robustness of these test collections.

687

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

2.5 Incompleteness of Relevance Judgments
Apart from the initial examination by Ekstrand-Abueg et al. [11], the robustness of timeline generation test collections have not been explored in the literature. However, there have been a number of past works in the wider information retrieval domain (typically for search tasks) examining the effect that relevance assessments (or lack thereof) has on test collection robustness. For instance, early work by Voorhees [28] investigated how different relevance assessment sets for a test collection impacted on the evaluation of retrieval results for the TREC-4 and TREC-6 test collections. That study showed that while the effectiveness metrics were impacted by using assessments created by different groups (e.g. NIST assessors vs. Waterloo assessors), the resultant ranking of the retrieval runs (systems) were highly correlated. Meanwhile, Zobel [34], examined the fairness of top k pooling methods for selecting documents to assess, showing that a pooling depth of 100 appeared to be adequate for search over the TREC-5 test collection. These early studies support the idea that smaller collections are indeed robust in the face of incomplete assessments.
However, over time, the size of test collections used by evaluation campaigns like TREC grew, but the pool depth (the number of judged documents per topic) across years has remained constant, increasing the relative degree of incompleteness (e.g. due to the varied nature of documents retrieved by systems contributing to the pools for these large corpora). Hence, later studies such as that by Buckley and Voorhees [6] examined the effect that further relaxing the completeness assumption has on the Cranfield evaluation methodology in larger test collections. In contrast, they showed that the Cranfield methodology was not robust in the face of massively incomplete relevance judgments. Moreover, works such as [24] have also questioned how robust different IR metrics are when using pooling at different k values. Parallels can be drawn between these works in the search domain and the questions investigated in this paper. The TREC-TS test collections are built on a corpus containing over a billion items (sentences).1 However, only between 60 and 100 (depending on year [3, 4]) of the top k sentences were pooled from participating systems. Hence, assessment completeness is a valid concern when working with collections at this scale.
3 METHODOLOGY AND SETUP
To examine the extent that the TREC-TS test collections are robust, we need a standardized setting and evaluation methodology with which to quantify `robustness'. To create such a setting, we first define what we mean by `robustness' below:
Robustness: In a timeline generation context, a truly robust test collection is one which can be used to accurately estimate the quality of a timeline summary, regardless of whether that summary was included within the initial pool or not. A robust test collection should correctly rank different timeline generation systems in order of the quality of the timeline summaries they produce.
Given this definition, to evaluate the robustness of a test collection we can identify three main requirements: 1) a series of systems that produce summaries of known quality (such that we have a known ordering of systems); 2) an evaluation metric that reflects the quality of a system according to the test collection; and 3) we
1 http://trec- kba.org/kba- stream- corpus- 2014.shtml

need to have the ability to compare systems when included in the pool and when excluded from the pool. Below we discuss how we design our experimental setup to meet these three requirements.
3.1 Synthetic System Generation
The first requirement for our evaluation is to have a series of timeline generation systems that can produce timeline summaries of known quality. This is so that we have a gold standard ranking of systems that reflects their actual performance. Initially, one might consider using the systems originally submitted to the TREC track in each year. However, this has some notable limitations. First, the systems that participated in TREC are different from year-to-year and the sources for those systems are not always available, hence we cannot deploy each TREC system across all years. This is potentially problematic, as there are relatively few topics (`events') in each test collection (between 9 and 21, see Table 2), which is less than the recommended number of topics for an IR experiment [7]. Second, the TREC systems only represent a subset of the range of possible system performances, e.g. in the first year, all participating systems were rather poor in terms of effectiveness. It would be preferable to be able to deploy single set of systems across all years such that we can compare across a larger number of events and have those systems represent the full range of system effectiveness (poor to perfect).
To achieve this, we instead take an alternative approach inspired by prior work in the Web and expert search domains [18, 27], where we generate synthetic systems with known performances. This is possible, since we are using the TREC-TS test collections as the subject of our investigations, which have sentence-level labels that quantify how much value is added by any sentence. Hence, we can define a synthetic system that takes in the sentence-level labels along with a target effectiveness level, and generates summaries with (approximately) that effectiveness level for each topic.
In particular, as discussed in Section 2.3, the TREC test collections contain atomic information items (nuggets) that form a ground truth for measuring summary quality. More precisely, the test collections contain <sentence,nugget> pairs that specify which individual sentences contain the information represented by each nugget. Following the atomic information nugget evaluation paradigm, a simple way to represent the quality of a timeline summary with k sentences is to calculate the proportion of nuggets it covers. As long as all summaries are of the same length k for a topic, then nugget coverage is a fair representation of timeline summary quality (it measures the volume of information contained).
Given the above, we specify a series of target effectiveness levels in terms of nugget coverage from 95% to 5% in 5% increments. For each target effectiveness level, we generate one summary per topic within the three TREC-TS test collections. For a topic, we first randomly select a subset of the information nuggets that matches the target effectiveness level, e.g. for 60% coverage, we select 60% of the nuggets for that topic. We then iterate over all <sentence,nugget> pairs for that topic in the ground truth label set, selecting one sentence that matches each nugget in a greedy manner. For instance, if a topic contained 100 nuggets and our target effectiveness level was 40%, we would first randomly select 40 of those 100 nuggets, and then attempt to select one sentence matching each of those 40 nuggets. When considering real timeline summarization systems, not all sentences are equally likely to be selected (some are easier to

688

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Synthetic Run Statistics.

Synthetic System Synth-C95 Synth-C90 Synth-C85 Synth-C80 Synth-C75 Synth-C70 Synth-C65 Synth-C60 Synth-C55 Synth-C50 Synth-C45 Synth-C40 Synth-C35 Synth-C30 Synth-C25 Synth-C20 Synth-C15 Synth-C10 Synth-C05

Target Coverage
95% 90% 85% 80% 75% 70% 65% 60% 55% 50% 45% 40% 35% 30% 25% 20% 15% 10% 5%

Actual Coverage
72% 68% 64% 61% 57% 53% 49% 46% 41% 38% 34% 30% 25% 22% 18% 14% 10% 7% 3%

TREC-TS Metrics ELG LC H(ELG,LC) 0.3590 0.6989 0.4589 0.3337 0.6474 0.4249 0.3336 0.6277 0.4216 0.3404 0.5901 0.4165 0.3241 0.5676 0.3996 0.3202 0.5289 0.3834 0.3189 0.5105 0.3792 0.3057 0.4466 0.3488 0.2922 0.4233 0.3314 0.2997 0.4181 0.3371 0.2888 0.3596 0.3047 0.2919 0.3398 0.3002 0.2989 0.2961 0.2824 0.2737 0.2515 0.2501 0.2555 0.2012 0.2108 0.2785 0.1523 0.1869 0.3053 0.1199 0.1623 0.3122 0.0755 0.1121 0.2478 0.0274 0.0473

find than others, e.g. because they contain the event query terms) and most nuggets have multiple sentences we might select. To capture this, instead of selecting any of the available sentences that match a nugget randomly, we instead use a probabilistic selection of sentences, based on the likelihood of each sentence having been selected by the original TREC systems (the more TREC systems that selected a sentence the more likely our synthetic systems will similarly select that sentence). As a sentence may cover multiple nuggets, we exclude a sentence from being selected if it covers any nuggets not in our target set. Furthermore, only around 70% of nuggets have associated matching sentences, i.e. in the remaining cases no systems in the pool found sentences that covered that nugget. In all cases we select as many sentences as possible and then `fill' the remaining slots (to maintain a consistent length k) with redundant sentences. In practice, this means that the actual nugget coverage for a synthetic summary is lower than the target coverage, e.g. a 90% coverage target results in 68% actual coverage when averaged across topics. We summarize the statistics of our generated synthetic runs in Table 3. As can be observed from Table 3, this synthetic system generation approach produces a range of systems that span the range of effectiveness levels attainable in terms of nugget coverage.
3.2 Evaluation Metrics
Having produced a set of systems with known performances, we now need to define metrics to capture how effective the summaries produced by those systems are. One possible option would be to simply use nugget coverage averaged across topics as an estimation of summary quality, as we did in the previous section. However, the TREC-TS track also considered factors beyond nugget coverage, such as novelty, brevity and latency [12]. For this reason, as well as to maintain compatibility with the track, we use the official TREC-TS target evaluation metric, which itself is the harmonic mean between two metrics: Expected Latency Gain and Latency

Comprehensiveness. Expected Latency Gain (ELG) is a precisionlike metric, calculated as the sum of the relevance of each nugget that a sentence covered, computed as:

ELG(S)

=

1 |S|

g(u, n)

(1)

u S n M(u)

where S is the stream of sentences returned by the system, M(u) is the set of gold standard nuggets matching sentence u (as determined by an assessor) and g(u, n) measures the utility of matching sentence u with nugget n. Latency Comprehensiveness (LC) is the proportion
of all nuggets matched by the system updates, computed as:

LC(S) = 1

g(u, n)

(2)

|N|

u S n M(u)

where N is the set of nuggets for the current event. For both ELG and LC, the g(u, n) component contains built-in penalties to capture
sentence brevity and latency. We refer the reader to the TREC track metrics documentation2 for a detailed explanation on how these are calculated. To provide a target metric, an F -like measure was also defined, which we denote H(ELG,LC). This is the harmonic
mean of ELG and LC,

H(ELG,

LC )(S )

=

2



ELG(S) ELG(S)

 +

LC (S ) LC (S )

(3)

We report the performance of our synthetic systems under the TREC-TS Metrics ELG, LC and H(ELG,LC) in Table 3. As we can see, the performance as reported by the TREC-TS LC and H(ELG,LC) and metrics are highly correlated with the actual nugget coverage of the systems.

3.3 Depooling Methodology
Finally, to evaluate the robustness of the test collections, we need to be able to evaluate the difference in performance of systems when they are included within the pool and when excluded from it. The core idea is that if a test collection is robust, then the estimated performance (under H(ELG,LC)) of a pooled system with known coverage X should be similar to the estimated performance for that same system when it is not pooled. In this case, an unpooled system represents a hypothetical new system that did not participate in the original TREC track and hence was not pooled.
TREC-TS followed a top k pooling methodology, where the sentences with the k highest confidence scores were added to the pool and later assessed (i.e. they took part in the nugget matching phase resulting in the <sentence,nugget> pairs M(u)). From the TRECTS pool statistics, we know the number of the original TREC-TS participating systems that contributed each sentence. We refer to sentences contributed by multiple systems as common sentences and sentences that were only contributed by a single system as uncommon sentences.
Building on past work examining the effect of unpooled systems on IR system performance [11], we simulate the state of the TRECTS test collections in scenarios where a particular system was not pooled. For ease of reference, we refer to this as depooling. depooling involves removing one copy of each of the top k sentences contributed by that system from the pool, along with any associated <sentence,nugget> pairs that resulted from the subsequent nugget matching phase. By definition, common sentences would
2 http://www.trec- ts.org/metrics- 10242013.pdf

689

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Synthetic Run Performances under H(ELG,LC) when pooled or depooled.  indicates statistically significant decreases in estimated performance (t-test p<0.01) between the run when in the pooled and when depooled.

Synthetic System Synth-C90 Synth-C80 Synth-C70 Synth-C60 Synth-C50 Synth-C40 Synth-C30 Synth-C20 Synth-C10

H(ELG,LC) When Pooled
0.4249 0.4165 0.3834 0.3488 0.3371 0.3002 0.2501 0.1869 0.1121

H(ELG,LC) When Depooled
0.3850 0.3823 0.3480 0.3196 0.3105 0.2617 0.2259 0.1627 0.0687

not be affected by removing only a single system (that system's sentences would still be contributed by some other system). However, uncommon sentences would be at risk from being eliminated from the pool entirely. If a sentence is eliminated from the pool, then that loss will impact the scoring of all systems. As we used the sentences in the TREC-TS pool previously to produce our synthetic systems, those systems behave as though they have been pooled. Hence, by depooling one of the synthetic systems we can investigate whether its estimated performance would have been adversely impacted had it not been pooled (i.e. due to uncommon sentences not being assessed). As such, we create an evaluation scenario for each of our 19 systems, each representing the case where that system was depooled. In the next section, we use these depooling scenarios to answer our first research question, i.e. RQ1 `To what extent can the TREC Temporal Summarization track test collections accurately rank unpooled systems?'.

4 RQ1: TO WHAT EXTENT ARE THE TREC-TS TEST COLLECTIONS ROBUST?
To answer RQ1, we first examine whether the estimated performance scores for systems change when pooled and when depooled. The ideal outcome is that the scores would not change, however this would only occur in cases where the system being depooled was totally comprised of common sentences. Hence, we can expect some score variance due to uncommon sentences being eliminated from the pool, but we would hope such score variance is minimal. Table 4 reports the estimated performance of the synthetic systems under H(ELG,LC) in the pooled and depooled scenarios (for brevity we only list performances for half the systems, the observations are the same for the other systems). As we can observe from Table 4, in all scenarios, depooling a synthetic system causes a statistically significant decrease in its estimated performance under the official TREC metric (H(ELG,LC)). This is a first indication that the test collections may not be as robust as we would like, as the effectiveness scores estimated for a system is shown to vary greatly depending on whether it was included in the pool or not. Hence, it is likely that new systems that were not originally pooled will have their true performance underestimated.
On the other hand, some error when estimating the performance of depooled systems is to be expected, as this is a known issue with pooling-based evaluation scenarios [6, 11]. From an evaluation perspective, what researchers and developers care about is whether the test collection is able to distinguish between systems

Table 5: Effect of depooling a single system in terms of ranking stability.

Synthetic System Synth-C95 Synth-C90 Synth-C85 Synth-C80 Synth-C75 Synth-C70 Synth-C65 Synth-C60 Synth-C55 Synth-C50 Synth-C45 Synth-C40 Synth-C35 Synth-C30 Synth-C25 Synth-C20 Synth-C15 Synth-C10 Synth-C05
Average

# Rank Swaps
3 2 6 0 4 1 3 1 4 1 2 1 1 0 1 0 0 0 0
1.5790

Rankings Pooled Vs. Depooled

Kendall's 

AP

0.9649 0.9766 0.9298 1.0000 0.9532 0.9883 0.9649 0.9883 0.9532 0.9883 0.9766 0.9883 0.9883 1.0000 0.9883 1.0000 1.0000 1.0000 1.0000
0.9815

0.8129 0.8752 0.8771 1.0000 0.9081 0.9914 0.9579 0.9915 0.9614 0.9914 0.9864 0.9870 0.9946 1.0000 0.9006 1.0000 1.0000 1.0000 1.0000
0.9597

with different effectiveness levels, i.e. whether we get the ordering of systems correct (particularly in the top ranks) is more important than whether individual system scores are underestimated [5]. Indeed, while we might expect that underestimations of a system's performance will cause that system to be miss-ranked, evidence from the search domain indicates that IR metrics tend to have some degree of robustness against incompleteness effects [24], i.e. the error in the score for a system may not be sufficient to cause a ranking swap.
As our synthetic systems have known effectiveness levels (based on nugget coverage), we know the correct ordering of systems. We also showed previously in Table 3 that the official TREC-TS metric (H(ELG,LC)) reflects this correct ranking when all systems are pooled. However, given that we know that depooling a system causes statistically significant changes in H(ELG,LC), it is possible that these changes are severe enough to result in that system (and other systems) being miss-ranked. In Table 5, we report the effect that depooling each system individually has on the overall ranking of synthetic systems in terms of number of rank swaps (miss-rankings) and overall rank correlation under Kendall's  . Additionally, as we are often more interested in distinguishing between systems near the top of the ranking than those at the bottom, we also report AP [31] values, which place higher weight on rank correlations occurring in the top ranks. Ideally, we should preserve the original correct ranking, i.e. the number of rank swaps should be 0, while Kendall's  and AP would be 1. From Table 5, we can see that for the majority of scenarios, depooling a single system results in the miss-ranking of at least one pair of systems. Indeed, the average number of system swaps needed to restore the correct ranking across depooling scenarios is 1.579, while the rank correlation on average was around 0.9815. This result is similar to that reported by Ekstrand-Abueg et al. [11], who observed correlation values around 0.97 when holding out individual TREC-TS systems in their study. However, while these rank correlations appear high,

690

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

it is important to remember that systems are still being ranked incorrectly. Moreover, from Table 5, we see that rank swaps are more common when highly effective systems are depooled, i.e. if you have a new (unpooled) system that is very effective, it is likely to be miss-ranked (see the top of Table 5). On the other hand, it appears that systems at the lower end of the effectiveness scale have little impact on the overall ranking of systems when not pooled.
To answer RQ1, we conclude that the TREC-TS test collections are likely robust when evaluating systems that were not pooled at the lower end of the effectiveness scale, i.e. systems equivalent to or worse than Synth-C30, that has an actual nugget coverage level of 22%. On the other hand, systems that were not pooled that push the upper-end of the effectiveness envelope are more likely to be miss-ranked, and hence using the TREC-TS test collections out-of-the-box is subject to more risk. The issue is that a researcher or developer has no way of knowing which case they fall into. As such, it would be advantageous to improve the test collections to reduce the risk of ranking error for unpooled/depooled systems.
5 RQ2: CAN WE USE AUTOMATIC MATCHING TO INCREASE ROBUSTNESS?
In the previous section we observed that systems that are depooled (i.e. representing new systems that did not participate in the original TREC tracks) are at risk from being miss-ranked. In particular, when comparing the ranking of the same 19 systems when all were pooled vs. when only 18 of them were pooled, we observed that ranking errors start to occur (average Kendall's  and AP values of 0.9815 and 0.9597, respectively).
These ranking errors stem from a system identifying sentences that are: 1) highly important (e.g. they cover a nugget that it is very difficult to find sentences for), 2) uncommon (no other system contributed those sentences to the pool) and 3) the system assigned them a high confidence score (so they would have been added to the pool if the system had been part of the initial evaluation). The result of such a system not being included in the pool is that a portion of its top k documents will not have been assessed,3 and unassessed sentences are assumed to not be relevant to any nuggets. Hence, that system's performance estimation is likely to be an underestimation.
From a test collection perspective, such a system not being included in the pool leads to missing <sentence,nugget> pairs. These pairs could be recovered by pooling all new systems and then reassessing, however, this additional cost eliminates much of the value that these test collections bring to IR evaluation. Indeed, if we use the total amount of time it reportedly took the TREC assessors to perform nugget matching [5] then each additional sentence assessed takes around 50 seconds on average. Moreover, this does not factor in time taken to set up the assessment system and recruit assessors. As such, it would be advantageous to have an automatic means to generate the missing <sentence,nugget> pairs without resorting to more human assessment.
In the remainder of this section we examine methods for automatically generating missing <sentence,nugget> pairs using the initial set of <sentence,nugget> pairs from the TREC-TS pool as a base, which we refer to as match expansion. In particular, we
3Recall that the top k pooling methodology guarantees that all pooled systems will have had their top k documents assessed.

first discuss our experimental methodology (Section 5.1) as well as evaluation metrics (Section 5.2). Then we propose two approaches to generate new <sentence,nugget> pairs (Section 5.3). Finally, we report our experimental results in Section 5.4.

5.1 Matching Expansion Methodology
To evaluate matching expansion, we need two sets of sentences for which we know what the correct <sentence,nugget> pairs are. The first set we need represents the sentence pool pre-expansion, while the second set represents the sentences and <sentence,nugget> pairs that are the correct expansions our proposed system should produce. Previously in Section 3.3 we created such a setting via depooling, which we re-use here. In particular, for each of our synthetic systems, by depooling that system we create a scenario where some sentences and associated <sentence,nugget> pairs will be eliminated from the pool. The goal of a matching expansion algorithm in this case is to then to restore as many of those sentences and associated <sentence,nugget> pairs as possible, while avoiding introducing erroneous <sentence,nugget> pairs.

5.2 Matching Expansion Metrics
Next, we need to define metrics that can tell us how effective an expansion attempt is. As we are proposing automatic methods to expand the ground truth, we could do more harm than good by introducing false <sentence,nugget> pairs if not careful. As such, we define three primary metrics to capture expansion effectiveness, namely: Expansion Recall (E-Recall), Avg. Expansion Pair F1 (aEPF1) and Avg.  /AP (a /aAP ).
Expansion Recall (E-Recall): As discussed above, from a test collection perspective a system not being included in the pool leads to missing <sentence,nugget> pairs, i.e. pairs that would have been added if the top k sentences for that system had been assessed. The goal of the match expansion is to recover these missing <sentence,nugget> pairs. We define E-Recall to be the proportion of missing sentences that were correctly matched to one or more nuggets, calculated as:

E-Recall(SpM , SuMp , SuMpe )

=

|(SpM

 SuMp )  (SuMpe |SpM  SuMp |

 SuMp )|

(4)

where SpM is the set of sentences that correctly matched one or more nuggets if system S was pooled, SuMp is the set of sentences that correctly matched one or more nuggets even if system S was
not pooled (i.e. derived from sentences contributed by other pooled systems) and SuMpe is the set of sentences that would correctly match one or more nuggets if system S was not pooled but after a
matching expansion technique (see Section 5.3) has been applied.
This is analogous to recall from a classification perspective, repre-
senting the proportion of all sentences that we were able to correctly
restore through automatic expansion. Note that we are not inter-
ested in a precision-like metric here, as `false positives' represent sentences that do not exist in SpM . Indeed, as SpM results from top k pooling, we know that it is incomplete, meaning that a `false
positive' might represent a relevant sentence that does cover one
or more nuggets, but no other system contributed it to the pool.

691

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Avg. Expansion Pair F1 (aEP-F1): On the other hand, a simple recall estimation is also insufficient to determine the quality of an

expansion, as the goal is to restore all missing <sentence,nugget>

pairs correctly. E-Recall only specifies the proportion of missing

sentences for which expansion generated at least one correct match

(<sentence,nugget> pair). Hence, we need a second metric that for

each restored sentence, which has 1 correct <sentence,nugget>

pairs, measures the proportion of matches for the sentence that are

correct. Extending the idea of precision and recall for this setting,

we start by defining Expansion Pair Precision (EP-P) and Expansion

Pair Recall (EP-R) for a sentence u as follows:

EP-P(Mpu , Muupe ) =

|Mpu  Muupe | |Muupe |

(5)

EP-R(Mpu , Muupe ) =

|Mpu

 Muupe | |Mpu |

(6)

where Mpu is the set of <sentence,nugget> pairs for sentence u that resulted from matching after being pooled and Muupe is the set of <sentence,nugget> pairs for sentence u that were produced by

expansion when u was not pooled. EP-P measures the proportion

of nugget matches for sentence u produced by expansion that were

correct, while EP-R measures the proportion of all nugget matches

for sentence u that were restored. We can average both EP-P and

EP-R across all sentences with matches from the TREC-TS pool (i.e. where we know Mpu ) and that were subject to expansion (i.e. Mpu and Muupe are different), which we denote as aEP-F and aEPR, respectively. In our later experiments, to have a single metric

representing pair generation quality, we report the harmonic mean

of aEP-P and aEP-R across sentences, denoted aEP-F1:

aEP-F1

=

2

·

aEP-P · aEP-R aEP-P + aEP-R

(7)

Using these two metrics, we can express how effective an expan-

sion attempt is. For instance, if an expansion method achieved an

E-Recall of 0.1456 and an aEP-F1 of 0.9621, then we can read this follows. First, the expansion method managed to find 14.6% of the

missing sentences. Second, of the sentences found, matching F1 was high at 0.9621, meaning that nearly all missing <sentence,nugget>

pairs were restored and very few erroneous <sentence,nugget>

pairs were introduced in the process.

Avg.  /AP (a /aAP ): Finally, while the above metrics capture the effectiveness of an expansion attempt from the perspective of the sentences and <sentence,nugget> pairs restored, our end-goal is to reduce the likelihood that systems which are depooled will be miss-ranked. Previously in Table 5 we reported the average correlation between the correct ranking of systems and the ranking of systems after each individual system was removed from the pool. In a similar way, we can also calculate the average correlation between 1) the correct ranking of systems and 2) the ranking of systems produced after an individual system has been removed and then expansion has been attempted. Intuitively, if expansion is successful, then average correlation should increase, i.e. expansion should reduce the number of ranking errors introduced when each system is depooled. Hence, we report the average  and AP both before and after expansion, as well as the average number of rank swaps needed to restore the correct ranking (i.e. the number of miss-rankings).

5.3 Matching Expansion Techniques
There are two ways one might attempt to generate expansions for <sentence,nugget> pairs. Either we start with a sentence and we try to find all related nuggets (i.e. attempt to simulate in an automatic manner the process that the TREC assessors perform during matching). Alternatively, we start with a <sentence,nugget> pair and try to find other good matches in the collection. To achieve either of these approaches we need effective representations for both the sentences and nuggets. In the case of a sentence, the only representation we have for it is its text. However, an open question is how we represent a nugget. For our experiments here, we choose to use existing <sentence,nugget> mappings to form a series of textual representations for each nugget. Here, each previously matched sentence in the pool is a representation for its associated nuggets. To generate new <sentence,nugget> pairs, for each nugget representation (a sentence A), we search for other textually similar sentences. If another sentence B is sufficiently similar to A, then we infer that for all <A,nugget> pairs, there should also be <B,nugget> pairs. To calculate similarity, we experiment with two ways to estimate the distance between texts, namely: raw text similarity and semantic similarity, which we summarize below.
Raw Text Similarity: For this expansion method, we take the simplest of approaches, using textual similarity with Levenshtein distance between each sentence that has one or more matches in the pool and every other sentence in the collection. If a sentence A and a sentence B have a similarity above a threshold  , then for each <A,nugget> pair we add an associated <B,nugget> pair.
Semantic Similarity: As sentences in the TREC-TS test collections are drawn from an article stream comprised of news articles, blogs and forum posts, we can expect significant vocabulary miss-match between different articles discussing the same information. For this reason, it is reasonable to expect raw text similarity will fail to identify some similar sentences due to vocabulary miss-matches. To tackle this issue, we also experiment with the identification of similar sentences based on semantic rather than textual similarity. In this case, we represent each sentence as a high-dimensional sentence embedding and then calculate similarity in terms of that semantic space. Sentence embeddings have previously been shown to be an effective sentence representation when calculating similarity [15]. In particular, given a sentence, for each word in that sentence, we first convert that word into a high-dimensional word embedding. To produce a sentence embedding we calculate a single position per dimension by averaging the word positions per dimension. For reproducibility, we use Word2Vec [21] along with pre-trained word embeddings from the Google News dataset (about 100 billion words).4 We use Cosine similarity for calculating the distance between the sentence embedding vectors. If a sentence A and a sentence B have a similarity above a threshold  , then for each <A,nugget> pair, we then add an associated <B,nugget> pair.
For both similarity metrics we also need to define a similarity threshold, above which a nugget and sentence will be considered a match. The correct similarity threshold will vary between techniques, e.g. what might be considered an acceptable threshold for raw text similarity will differ from semantic similarity. For brevity,
4Available from https://code.google.com/archive/p/word2vec/

692

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 6: Comparison of match expansion and resultant correlation with the correct system ranking on average across depooling scenarios and topics when performing sentence to sentence similarity expansion. Statistically significant changes (paired t-test p<0.05) in a and aAP against no expansion (None) are denoted  and  for increases and decreases respectively.

Expansion Technique None Item-Item Similarity Expansion Item-Item Similarity Expansion Item-Item Similarity Expansion Item-Item Similarity Expansion Item-Item Semantic Expansion Item-Item Semantic Expansion Item-Item Semantic Expansion Item-Item Semantic Expansion Item-Item Semantic Expansion Item-Item Semantic Expansion

Threshold  -
0.99 0.90 0.80 0.70 1.00 0.98 0.96 0.94 0.92 0.90

Expansion Metrics

E-Recall aEP-F1

-

-

0.0714 0.9401

0.1096 0.8336

0.1199 0.8349

0.1199 0.8349

0.0679 0.9662

0.1239 0.8653

0.1666 0.7436

0.2383 0.4819

0.3678 0.2253

0.5034 0.1341

Ranking Metrics

Avg Rank Swaps a

1.5789

0.9815

1.4211

0.9834

1.0526

0.9877

1.0526

0.9876

1.0526

0.9876

1.1579

0.9865

0.8421

0.9902

0.6842

0.9920

1.9473

0.9772

5.8421

0.9317

8.2105

0.9040

aAP 0.9597 0.9597 0.9619 0.9610 0.9610 0.9625 0.9640 0.9878 0.9334 0.8361 0.7359

in the following section we only report performances from around the peak threshold  observed based on experimentation with different  ranges. We refer to expansion with the raw text as item-item similarity expansion and expansion with semantic similarity as item-item semantic expansion.
5.4 Matching Expansion Results
Table 6 reports the effectiveness of our different proposed expansion techniques in terms of the metrics discussed in Section 5.2. We can divide our metrics into two classes, Expansion Metrics that measure how effectively we are restoring missing <sentence,nugget> pairs; and Ranking Metrics that report how well correlated with the correct system ranking we are after expansion has occurred. E-Recall performances indicates the proportion of the missing sentences that were restored during expansion (higher is better), while aEP-F1 indicates how effectively we restored the matches for those sentences (higher is better). Under the ranking metrics, # Rank Swaps is the number of swaps needed on average to re-create the correct ranking (lower is better), while a and aAP indicate the resultant correlation between the ranking produced post-expansion and the correct ranking (higher is better). In the case of the Ranking Metrics a and aAP , we also report statistically significant changes (paired t-test p < 0.05) against no expansion (None).
From Table 6, examining the two approaches that use the sentences texts alone for expansion, we observe different behaviours depending on whether raw text similarity or semantic similarity was applied. In the case of using the raw text for identifying similar sentences, we see that the E-Recall scores range between 7% and 12%. This indicates that regardless of the threshold selected, only a small proportion of the missing sentences could be found by comparing the raw text of the sentences. In turn, this indicates that uncommon sentences predominantly do not use the same language as the more common sentences. On the other hand, we also see high aEP-F1 performances ranging from 0.9401 to 0.8349. This shows that although only a few of the missing sentences are found, the matches derived from those sentences were almost always correct (i.e. all matches for each sentence were restored and we did not introduce many erroneous matches in the process). If we then examine the effect that using sentence text expansion with raw text similarity has on the ranking of systems, we see that this type of expansion results in fewer ranking errors than observed prior to

expansion (None). Indeed, the average number of swaps needed to restore the correct ranking drops by 30% from 1.5789 to 1.0526 (threshold=0.9) and an average Kendall's  correlation against the correct ranking increases by a small but statistically significant margin (0.9815 to 0.9877) across the scenarios tested. Importantly this shows that the restoration of a relatively small proportion of all sentences that should have been pooled (e.g. around 10%) can eliminate around 30% of the ranking errors (1.5789 rank swaps to 1.0526 rank swaps).
Next, we examine how effective sentence expansion is when is when using semantic similarity rather than raw text similarity from Table 6. Examining the expansion metrics first, we observe a much wider range of E-Recall values as we vary the similarity threshold. In particular, exact vector matching (threshold=1.0) results in 6.8% of the missing sentences being found, while a lower similarity threshold of 0.9 results in half (50.34%) of the missing sentences being found. This shows that semantic similarity comparisons are more effective for finding uncommon sentences than raw text matching. However, as we lower the similarity threshold, we also observe a rapid decline in aEP-F1 (0.9662 when the  =1.0 to 0.1341 when the  =0.9). This illustrates that semantic expansion is much more prone to introducing erroneous matches as we relax the selection constraint. If we examine how this affects the system ranking performance, we observe that even though semantic expansion introduces a higher proportion of erroneous matches than raw text expansion, it can be more effective. In particular, we see that when the vector similarity threshold  is set to 0.98 and 0.96, we further reduce the average number of rank swaps (ranking errors) to 0.8421 and 0.6421, respectively. If we compare this to the number of ranking errors prior to expansion, then semantic expansion can reduce the number of errors by up to 50% (1.5789 rank swaps to 0.6842 rank swaps). This also increases the correlation between the system ranking post-expansion and the correct ranking by a statistically significant margin under both a and aAP .
To answer RQ2, we have shown that automatic <sentences,nugget> expansion techniques that find textually or semantically similar sentences and use those sentences to infer matches can be effective for increasing the robustness of the TREC-TS test collections. In particular, we have shown that expansion using raw text similarity can reduce the number of miss-rankings by 30%, while semantic similarity-based expansion can reduce the number of miss-rankings

693

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

by 50%. We conclude that these expansion techniques improve the robustness of the TREC-TS test collections, reducing the risk of miss-ranking new systems that were not pooled.
6 CONCLUSIONS
In this paper, we quantified the extent to which the TREC Temporal Summarization (TREC-TS) test collections are able to robustly rank different timeline generation systems in scenarios where one of those systems was not included in the sampled pool. We also proposed new automatic approaches aimed at improving the robustness of those test collections. Through a leave-one-out (depooling) experiment, we observed a mixed picture in terms of test collection robustness. In particular, the TREC-TS test collections appear to be robust when evaluating systems that were not pooled at the lower end of the effectiveness scale, i.e. the correlation with the correct system ranking is perfect when the system is depooled. On the other hand, when more effective systems were depooled, we started to observe ranking errors and lower correlations, particularly towards the top ranks. Hence, we conclude that using the TREC-TS test collections out-of-the-box is subject to some risk.
To reduce this risk, we studied two approaches that use the available sentence pool and associated labels (<sentence,information unit> matches) to automatically generate new labels that might have been missed due to sentences not being pooled. In particular, we evaluated item-item similarity expansion and item-item semantic expansion approaches. Our experiments using these two proposed expansion techniques showed that automatically adding even a small number of <text item,information unit> pairs can markedly reduce the number of ranking errors observed when using the TREC-TS test collections. In particular, item-item similarity expansion can reduce the number of ranking errors by up to 30% while item-item semantic expansion can reduce the number of ranking errors by up to 50%. Since our results show that matching expansion techniques enhance robustness of the TREC-TS test collections, we recommend the use of expanded match sets when evaluating new systems, particularly in cases where low completeness levels are observed.
Our work highlighted the limitations of shallow pooling for creating test collections from large streaming data, particularly for tasks where item selections are not independent (in this case selection of an item for inclusion within the summary is dependent on past selections of other sentences), resulting in low overlap between systems when pooling summaries for labeling. In these cases, new labeling approaches are needed that provide increased stream coverage, without dramatically increasing labeling time/cost.
7 DATA RELEASE
To support future researchers working with the TREC-TS test collections, we provide both the synthetic systems that we used in this study as potential new baselines for the community, as well as the expanded assessment matches produced by the best performing expansion approach (Item-Item Semantic Expansion   = 0.96), which we recommend that researchers use in scenarios where they are experiencing low completeness levels under the official labels. These can be freely downloaded at:
http://dx.doi.org/10.5525/gla.researchdata.613

REFERENCES
[1] James Allan, Rahul Gupta, and Vikas Khandelwal. 2001. Temporal summaries of new topics. In ACM SIGIR 2001.
[2] Javed Aslam, Fernando Diaz, Matthew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Tetsuya Sakai. 2014. TREC 2014 Temporal Summarization Track Guidelines. In TREC 2014.
[3] Javed Aslam, Fernando Diaz, Matthew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Tetsuya Sakai. 2015. TREC 2015 Temporal Summarization Track Overview. In TREC 2015.
[4] Javed A Aslam, Matthew Ekstrand-Abueg, Virgil Pavlu, Fernando Diaz, and Tetsuya Sakai. 2013. TREC 2013 Temporal Summarization. In TREC 2013.
[5] Gaurav Baruah, Richard McCreadie, and Jimmy Lin. 2017. A Comparison of Nuggets and Clusters for Evaluating Timeline Summaries. In ACM CIKM 2017.
[6] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete Information. In ACM SIGIR 2004.
[7] Chris Buckley and Ellen M. Voorhees. 2017. Evaluating Evaluation Measure Stability. SIGIR Forum 51, 2 (Aug. 2017), 235­242.
[8] John M. Conroy, Judith D. Schlesinger, and Dianne P. O'Leary. 2011. NouveauROUGE: A Novelty Metric for Update Summarization. Computational Linguistics 37 (2011), 1­8.
[9] Hoa Dang. 2005. Overview of DUC 2005. In DUC 2015. [10] Hoa Trang Dang and Karolina Owczarzak. 2008. Overview of the TAC 2008
Opinion Question Answering and Summarization Tasks. In TAC 2008. [11] Matthew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Fernando Diaz.
2016. A Study of Realtime Summarization Metrics. In CIKM 2016. [12] Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013. Updating Users about Time
Critical Events. In ECIR 2013. [13] Chris Kedzie, Fernando Diaz, and Kathleen McKeown. 2016. Real-Time Web
Scale Event Summarization Using Sequential Decision Making. arXiv preprint arXiv:1605.03664 (2016). [14] Chris Kedzie, Kathleen McKeown, and Fernando Diaz. 2015. Predicting Salient Updates for Disaster Summarization.. In ACL 2015. [15] Tom Kenter and Maarten De Rijke. 2015. Short text similarity with word embeddings. In ACM CIKM 2015. [16] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In ACL Workshop On Text Summarization 2004. [17] Jimmy Lin, Miles Efron, Yulu Wang, and Garrick Sherman. 2014. Overview of the TREC-2014 Microblog Track. In TREC 2014. [18] Craig Macdonald and Iadh Ounis. 2011. The influence of the document ranking in expert search. Information Processing & Management 47, 3 (2011), 376­390. [19] Richard McCreadie, Craig Macdonald, and Iadh Ounis. 2014. Incremental Update Summarization: Adaptive Sentence Selection based on Prevalence and Novelty. In CIKM 2014. [20] Richard McCreadie, Rodrygo Santos, Craig Macdonald, and Iadh Ounis. 2017. Explicit diversification of event aspects for temporal summarization. ACM TOIS (2017). [21] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013). [22] Ani Nenkova and Kathleen McKeown. 2011. Automatic Summarization. FnTIR 5, 2­3 (2011), 103­233. [23] Paul Over. 1997. TREC-6 Interactive Report. In TREC 1997. [24] Tetsuya Sakai. 2008. Comparing metrics across TREC and NTCIR:: the robustness to pool depth bias. In ACM SIGIR 2008. [25] Rodrygo LT Santos, Iadh Ounis, and Craig Macdonald. 2015. Search result diversification. Foundations and Trends in Information Retrieval 9, 1 (2015), 1­90. [26] Luchen Tan, Adam Roegiest, Charles L. A. Clarke, and Jimmy Lin. 2016. Simple Dynamic Emission Strategies for Microblog Filtering. In ACM SIGIR 2016. [27] Andrew Turpin and Falk Scholer. 2006. User performance versus precision measures for simple search tasks. In ACM SIGIR 2006. [28] Ellen M. Voorhees. 1998. Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness. In SIGIR 1998. [29] Ellen M. Voorhees. 2003. Overview of the TREC 2003 Question Answering Track. In TREC 2003. [30] Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011. Evolutionary Timeline Summarization: a Balanced Optimization Framework via Iterative Substitution. In ACM SIGIR 2011. [31] Emine Yilmaz, Javed A. Aslam, and Stephen Robertson. 2008. A New Rank Correlation Coefficient for Information Retrieval. In ACM SIGIR 2008. [32] ChengXiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In ACM SIGIR 2003. [33] Chunyun Zhang, Zhanyu Ma, Jiayue Zhang, Weiran Xu, and Jun Guo. 2015. A Multi-level System for Sequential Update Summarization. In QSHINE 2015. [34] Justin Zobel. 1998. How Reliable Are the Results of Large-scale Information Retrieval Experiments?. In ACM SIGIR 1998.

694

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Neural Query Performance Prediction using Weak Supervision from Multiple Signals

Hamed Zamani
University of Massachusetts Amherst Amherst, MA 01003 zamani@cs.umass.edu

W. Bruce Croft
University of Massachusetts Amherst Amherst, MA 01003 croft@cs.umass.edu

J. Shane Culpepper
RMIT University Melbourne, Australia shane.culpepper@rmit.edu.au

ABSTRACT
Predicting the performance of a search engine for a given query is a fundamental and challenging task in information retrieval. Accurate performance predictors can be used in various ways, such as triggering an action, choosing the most effective ranking function per query, or selecting the best variant from multiple query formulations. In this paper, we propose a general end-to-end query performance prediction framework based on neural networks, called NeuralQPP. Our framework consists of multiple components, each learning a representation suitable for performance prediction. These representations are then aggregated and fed into a prediction subnetwork. We train our models with multiple weak supervision signals, which is an unsupervised learning approach that uses the existing unsupervised performance predictors using weak labels. We also propose a simple yet effective component dropout technique to regularize our model. Our experiments on four newswire and web collections demonstrate that NeuralQPP significantly outperforms state-of-the-art baselines, in nearly every case. Furthermore, we thoroughly analyze the effectiveness of each component, each weak supervision signal, and all resulting combinations in our experiments.
ACM Reference Format: Hamed Zamani, W. Bruce Croft, and J. Shane Culpepper. 2018. Neural Query Performance Prediction using Weak Supervision from Multiple Signals. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210041
1 INTRODUCTION
Query performance prediction (QPP) is a well studied problem in information retrieval (IR) due to its potential importance in improving the effectiveness and efficiency of a wide variety of search tasks [5]. The query performance prediction task is defined as predicting the quality of a retrieval model for a given query, when neither explicit nor implicit relevance information is available. Accurate and real-time performance predictors could potentially be used in triggering a specific action in the retrieval system, such as selecting an index traversal algorithm at query time [27], choosing the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210041

correct number of documents to process in a cascaded multistage retrieval system [13], choosing the most effective ranking function per query, selecting the best variant from multiple query reformulations, or requesting more information from users in cases of potential poor retrieval performance, particularly in conversational systems. Query performance prediction models are categorized as pre-retrieval and post-retrieval approaches. Post-retrieval approaches, which are the focus of this paper, analyze the result list returned by the retrieval engine in response to the query. Postretrieval predictors are our focus as they have been proven to be more effective than pre-retrieval predictors [5].
In this paper, we propose a general framework based on neural networks for the query performance prediction task. Our framework, called NeuralQPP, consists of multiple components, each analyzing a distinct aspect useful for performance prediction. Each component learns a high-dimensional dense representation suitable for the QPP task. These representations are then aggregated and fed into a prediction sub-network. The whole framework is trained in an end-to-end fashion.
We introduce three neural components for implementing the NeuralQPP framework. Each is designed with minimal network engineering for simplicity. The first component analyzes the retrieval scores for the top documents retrieved in response to a given query. The retrieval score distribution has been previously used in a variety of QPP models [14, 40, 44, 57]. The second component analyzes the term distribution for the documents appearing in the result list. A term distribution can be a means for measuring the coherence of the top ranked documents, which has been proven to be highly correlated with query performance [11]. The third component analyzes the distributed representation of documents in a semantic space. This component is able to measure the semantic coherence and diversity of the result list.
Recently, Dehghani et al. [16] and Zamani and Croft [50] proposed the training of Neural IR models with weak supervision. Weak supervision is an unsupervised learning approach where a large set of unlabeled data is labeled with an existing unsupervised model as a weak labeler. As it is often very difficult to generate high quality training data, we describe an approach to training NeuralQPP using multiple weak supervision signals. To do so, we are able to benefit from three existing predictors that estimate the query performance based on different intuitions and assumptions. To be exact, our weak labelers include a clarity-based approach by Cronen-Townsend et al. [11], a score-based approach by Shtok et al. [44], and a combining approach by Shtok et al. [42]. Training a generalized model with multiple weak signals led us to develop a component dropout technique that randomly disables at most K - 1 (out of K) components of the NeuralQPP framework. This can

105

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

be also viewed as a regularization that prevents the models from overfitting.
We evaluate our models using four standard TREC collections, including two newswire test collections (AP and Robust) respectively used for the TREC 1-3 Ad-hoc Tracks and the TREC 2004 Robust Track, and two large-scale web collections (GOV2 and ClueWeb) respectively used for the TREC 2004-2006 Terabyte Tracks and the TREC 2009-2012 Web Tracks. Our experiments show that the proposed model significantly outperforms the baselines, in nearly every setting. We also empirically study the influence of each component in the NeuralQPP framework, and the effectiveness of employing multiple weak signals for training. The results demonstrate that NeuralQPP performs remarkably well in predicting the performance of various retrieval models.
In summary, this paper introduces the first neural network architecture for query performance prediction. Furthermore, it not only provides a successful implementation of the weak supervision idea for an additional fundamental IR task, but also provides new insights on how best to learn with multiple weak labelers. NeuralQPP produces state-of-the-art performance on multiple collections.
2 RELATED WORK
In this section, we first present previous work on query performance prediction, and next briefly review the literature on weak supervision for information retrieval.
2.1 Query Performance Prediction
Quality estimation is a fundamental task that can help to improve effectiveness or efficiency in various applications, such as machine translation [45], and automatic speech recognition [4, 31]. When it comes to search engines, the task is called query performance or query difficulty prediction. This task has been widely studied in the IR literature [5, 11, 12, 14, 18, 22, 42­44, 47, 56, 57]. The task of query performance prediction is defined as predicting the retrieval effectiveness of a search engine given an issued query with no implicit or explicit relevance information.
Query performance prediction approaches can be partitioned into two disjoint sets: pre-retrieval and post-retrieval approaches. Pre-retrieval QPP approaches predict the performance of each query based on the content and the context of the query in addition to the corpus statistics. Pre-retrieval predictors are often derived from linguistic or statistical information. Part-of-speech tags, as well as syntactic and morphological features of query terms are among the linguistic features used for query performance prediction [30]. Inverse document frequency [11] and average query term coherence [19] are examples of statistical information used for this task. Hauff et al. [18] provided a through overview of the pre-retrieval QPP approaches.
Alternately, post-retrieval QPP approaches, which are the focus of this paper, estimate the query performance by analyzing the result list returned by the retrieval engine in response to the query. Carmel and Yom-Tov [5] categorized post-retrieval predictors as clarity-based, robustness-based, and score-based approaches:
· Clarity-based approaches [11, 12] estimate the query performance by measuring the coherence (clarity) of the result list

with respect to the collection. These approaches assume that the more focused the result list, the more effective the retrieval. · Robustness-based approaches predict the query performance by estimating the robustness of the result list. Robustness can be measured in various ways. For example, Zhou and Croft [57] measured it based on query perturbation in a Query Feedback (QF) model. In other work, the same authors [56] measured the ranking robustness through document perturbation by injecting noise into the top results. Both query and document perturbations were also studied by Vinay et al. [49]. Aslam and Pavlu [2] studied the ranking robustness based on retrieval engine perturbation. Apart from perturbation approaches, Diaz [17] measured the ranking robustness using the cluster hypothesis [48] by regularizing the retrieval score of each document given its most similar documents. This approach is called spatial autocorrelation. · A variety of post-retrieval approaches predict the query performance by analyzing the retrieval score distribution, and are commonly referred to as score-based approaches. Among these, the Weighted Information Gain (WIG) of Zhou and Croft [57] and the Normalized Query Commitment (NQC) of Shtok et al. [44] are the most popular QPP models, and are considered stateof-the-art. WIG measures the divergence of the mean retrieval score from the collection score and NQC measures the standard deviation of the retrieval scores normalized by the collection score. Retrieval score distribution has been further employed in other models [14, 34] for the QPP task. Most recently, Roitman et al. [40] proposed a bootstrapping approach to provide a robust standard deviation estimator for retrieval scores.
There is also a line of research that combines multiple predictors from multiple categories. The utility estimation framework (UEF) of Shtok et al. [42] is an example of this QPP family, which is based on statistical decision theory. Making use of both pseudo-effective and pseudo-ineffective reference lists was further studied by Kurland et al. [22], Shtok et al. [43], and Roitman [38].
There also exist a set of supervised approaches for query performance prediction. For instance, Raiber and Kurland [36] proposed a learning to rank model based on Markov random fields and observed significant improvements. Most recently, Roitman et al. [39] introduced a supervised combining approach based on coordinate ascent. Our model does not require human-labeled data for training, and thus supervised approaches are outside the scope of this paper.
2.2 Weak Supervision
Limited training data has been a perennial problem in information retrieval, and many machine learning-related domains [52]. This has motivated researchers to explore building models using pseudo-labels. For example, pseudo-relevance feedback (PRF) [9] is one of the long-standing approaches which assumes that the top retrieved documents in response to a given query are relevant to the query, and thus uses these documents to improve the retrieval performance. Although this assumption does not hold in all cases, PRF has been proven to be effective in many retrieval settings [23, 37, 51, 54]. Building pseudo-collections and simulated queries for various IR tasks could be considered as another set of approaches that tackle this issue [1, 3].

106

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

As widely known, deep neural networks often require a large volume of training data. Recently, training neural IR models based on pseudo-labels has shown to produce successful results [16, 50]. This learning approach is called weak supervision. Dehghani et al. [16] proposed training a neural ranking model for the ad-hoc retrieval task based on the labels generated by an existing retrieval model, such as BM25. Zamani and Croft [50] argued that the objective functions of the general-purpose word embedding models, such as word2vec [29], are not necessarily equivalent to IR objectives. These approaches train relevance-based word embeddings using the output of Lavrenko and Croft's relevance models [23] as a type of weak labeling. Following these studies, the idea of training neural IR models with weak supervision has been further employed [7, 15, 26].
There are two key factors that distinguish our work from previous studies. First, since training neural IR models with weak supervision has been recently shown to be effective, its effectiveness in many common IR tasks remains relatively unexplored. This work introduces a successful implementation of weak supervision for another fundamental IR task. Second, all the previously mentioned studies train models using a single weak label. However, in this work, we describe how to train our NeuralQPP model using multiple weak labels. Learning from multiple weak labelers should lead to higher generalization of the learned models.

3 NEURAL PERFORMANCE PREDICTION

In this section, we propose a general query performance prediction

framework based on neural networks. The framework is called

NeuralQPP and is independent of the retrieval engine. NeuralQPP

consists of K components that cover different and complemen-

tary aspects of query performance prediction. Each component ci is a sub-network in NeuralQPP, that produces a di -dimensional real-valued dense representation, denoted as i . The learned repre-
sentations are expected to provide useful information for the query

performance prediction task. The obtained i s are then aggregated using an aggregation function  which outputs a d-dimensional

dense vector. This vector is finally fed into a prediction function 

that returns a real number representing the predicted performance.

All the sub-networks in the NeuralQPP framework are trained

simultaneously in an end-to-end fashion.

In summary, the performance of each query is predicted as fol-

lows:

((1, 2, · · · , K ))

(1)

where i is the output of the component ci .

In order to minimize the number of hyper-parameters and per-

form minimal network engineering, we implement the aggregation

function  as a weighted average:

(1, 2, · · ·

, K )

=

1 K

K
i i
i =1

(2)

where i controls the influence of each component in the final prediction. The network parameters i are trained as part of the NeuralQPP model. Note that this definition of  forces the dimen-
sions of all i s to be equal. We model the function  as a fully connected feed-forward neural
network that takes the output of  and produces a real number

representing the predicted performance. We use rectified linear unit (ReLU) as our activation function for hidden layers to learn non-linear functions, and sigmoid for the output layer. To prevent overfitting, we use dropout in all hidden layers. The number of hidden layers and their sizes are hyper-parameters of the model.
In this paper, we implement three components for NeuralQPP (K = 3). The first component analyzes the retrieval score distribution, while the second component considers the term distributions in pseudo-effective and pseudo-ineffective document sets. The last component analyzes the semantic information obtained from the top retrieved documents. The following subsections describe these components, in detail.

Retrieval Scores Analyzer. Inspired by the score-based approaches described in Section 2.1, such as WIG [57] and NQC [44], our first component, called the retrieval scores analyzer, estimates the query performance given the retrieval scores for the top n documents returned by the search engine in response to a query q. As shown in Figure 1a, this component takes a vector sì with n + 1 dimensions as input, such that:

si =

score(q, C) score(q, Di-1)

if i = 1 o .w .

(3)

where C and Di-1 denote the collection and the (i - 1)th document in the result list returned by the search engine. 'score' denotes the scoring function used by the retrieval engine and score(q, C) is computed as the retrieval score for a document constructed by concatenating all documents in the collection. The order of concatenation does not matter for bag of words models. We feed the constructed vector sì into a fully-connected feed-forward neural network. In summary, this component computes a non-linear abstract representation of the retrieval score distribution, suitable for the query performance prediction task.

Term Distribution Analyzer. Inspired by the clarity-based ap-
proaches [11] described in Section 2.1, a term distribution analyzer
component predicts the query performance using term distribu-
tion information. The component's architecture is presented in Figure 1b. In this component, we first create a matrix A = [aij ] with n + 1 columns where the first column corresponds to the collection (as a pseudo-ineffective document set) and each of the remaining n columns corresponds to each of the top n documents retrieved in response to the query q (as pseudo-effective documents). The matrix A has m rows, each corresponding to a vocabulary term from a set W containing the top m terms with the highest cumulative count in the top n retrieved documents (|W | = m). Each element of the matrix A is calculated as:

aij =

Pr(wi |C ) Pr(wi |Dj-1 )

if j = 1 o .w .

(4)

where wi , C , and Dj-1 respectively denote the ith term in the vocabulary set W , the collection's unigram language model, and the unigram language model of the (i - 1)th retrieved document.
The language models are estimated using maximum likelihood esti-
mation. Since this component is responsible for term distribution
analysis, we can assume that vocabulary terms are independent. Therefore a non-linear mapping function  : Rn+1  Rf is applied on each row of the matrix A. The parameters of this mapping

107

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Retrieval Scores Analyzer

(b) Term Distribution Analyzer

(c) Semantic Analyzer

Figure 1: NeuralQPP consists of the three components depicted above. The representations learned by each of these components are then aggregated using the arithmetic mean and then fed into a fully-connected feed-forward network that produces a single score for query performance prediction.

function are shared for all m terms (rows of the matrix). Indeed, this is similar to applying a convolutional layer with the window size and stride of 1. The input channel size and the filter size are equal to m and f , respectively. Therefore, this layer outputs a f ×m matrix. A sub-sampling phase is further applied. We take the maximum value of the f features learned for each term (max-pooling), which results in a m-dimensional vector. This vector is then fed to a fully-connected feed-forward network for dimension reduction and learning an abstract representation of term distributions, expected to be suitable for query performance prediction.

Semantic Analyzer. The semantic analyzer component, shown

in Figure 1c, takes the documents returned by the retrieval engine

and measures the query performance based on their distributed

representations. For instance, this component can measure how

semantically coherent or diverse the returned documents are. The

intuition behind this is that coherence and diversity in the returned

documents correlate with the ambiguity of the query, since a query

may carry multiple meanings or intents. Previous QPP models

that analyze the coherence of the result list, e.g., clarity [11], are

based on term occurrence (similar to our term distribution analyzer

component). Thus, this component provides a novel way of looking

at the problem.

In this component, we first represent each document in a latent

semantic space, and then learn a set of latent features based on

the learned representations. Our document representation func-

tion  consists of two major functions: (1) an embedding function E : V  Rl that maps each term from the vocabulary set V to a l-

dimensional embedding space, and (2) a global term weighting func-

tion W : V  R that maps each vocabulary term to a real-valued

number showing its global importance. The document representa-

tion function  represents a document D = {w1, w2, · · · , w |D | } as follows:

|D |

 (D; E, W) = W(wi ) · E(wi )

(5)

i =1

which is the weighted element-wise summation over the term embedding vectors. A normalized weight W is learned for each term

using a softmax function as follows:

W(wi ) =

exp(W(wi ))

|D | j =1

exp(W(w

j

))

(6)

This approach of document representation is based on the bag of words assumption. Despite its simplicity, it was shown to perform well for ad-hoc retrieval tasks [16].
We flatten, concatenate, and feed the representations learned for the top n retrieved documents ({ (D1), (D2), · · · , (Dn )}) into a fully-connected feed-forward network in order to obtain a nonlinear abstract representation that demonstrates useful information for query performance prediction extracted from semantic representation of documents in the result list.

4 TRAINING WITH MULTIPLE WEAK SUPERVISION SIGNALS
In this section, we describe how to train the proposed neural query performance prediction model with no labeled training data. Indeed, we first propose to train our model using multiple weak supervision signals in Section 4.1, and later propose a component dropout technique to regularize our model in Section 4.2. Finally, Section 4.3 introduces the weak supervision signals employed to train the NeuralQPP model.

4.1 Training

Let M be a retrieval model that retrieves documents from the

collection C in response to a given query. In this work, we propose

to train a model with multiple weak supervision signals, which is

categorized as an unsupervised learning approach. To do so, we first

obtain a set of queries Q and N weak labelers: N unsupervised query

performance prediction models that can provide us complementary

information. Predicting the performance of each query qi  Q over

the collection C using the weak supervision signals results in a

training set T

=

{(qi

,



n M

(qi

;

C),

Yi

)

:

qi



Q}

where



n M

(qi

;

C)

denotes the list of the top n documents retrieved by M in response

to the query qi , and Yi denotes a list of predicted performances for qi by each of the weak supervision signals (thus, |Yi | = N ).

108

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A straightforward solution for learning from multiple weak labels would be casting the problem to learning from a single weak label by aggregating the N weak labels to end up with a single label for each query. This aggregation can be done, for example, by averaging the labels for pointwise settings, or by majority voting for pairwise settings.
Another simple solution would be training N distinct models each using one of the weak labels and then aggregating their outputs at inference time by summation.
In this paper, we argue that neither of these solutions are optimal, since they both incur information loss (which is also justified in our experiments). Therefore, we aim to train our model by optimizing across all weak labels at the same time. Our proposed solution simultaneously optimizes N loss functions, each corresponding to a weak label. Hence, we define our loss function as a linear interpolation of N loss functions:

N

L = k Lk

(7)

k =1

where  = [1, 2, · · · , N ] is a vector of hyper-parameters controlling the influence of each weak label in the final loss function.

We investigate two learning settings in our experiments: pointwise

and pairwise.

Pointwise learning. In a pointwise setting, we use mean absolute error (MAE) as the loss function.1 The absolute error for a query
qi in the training set is defined as follows:

Lk (qi ) = |Yik - Pk (qi ; M, C,  )|

(8)

where P denotes the query performance score predicted by our model with the parameter set  for the given query.

Pairwise learning. Since the task of query performance prediction is often defined and evaluated as a ranking task [11, 44, 57] (ranking queries with respect to their performances), we can train our model using a pairwise setting. Therefore, each training instance consists of a random pair of queries from the training set T . To this end, we employ hinge loss (max-margin loss function) that has been widely used in the learning to rank literature for pairwise models [24]. Hinge loss is a linear loss function that penalizes examples violating the margin constraint. The hinge loss for a query pair qi and qj is defined as follows:
Lk (qi , qj ) = max{0, 1-sign(Yik - Yjk )
(Pk (qi ; M, C,  ) - Pk (qj ; M, C,  ))} (9)

In the next subsection, we describe how each Pk is computed.

4.2 Component Dropout
In training our model with multiple weak labels, we are faced with two major issues: (1) The predictions P1, P2, · · · , PN should not be equal; otherwise, this would be equivalent to aggregating the weak labels and training the model using the aggregated labels. On the other hand, P1, P2, · · · , PN should be produced by a single model that would be used at inference time. (2) As can be seen in Section 4.3, some of the employed weak supervision signals can be
1In our experiments, MAE resulted in more robust performance when compared to mean squared error (MSE).

exactly computed using a sub-network of NeuralQPP. For instance,
the retrieval scores analyzer component can compute NQC. There-
fore, when we use NQC as a weak supervision signal, the network
tries to predict the output only based on the retrieval scores ana-
lyzer component. This prevents the model from generalizing well.
To overcome these two issues, we propose a component dropout
approach that also regularizes our model. Assume that we aim at training a NeuralQPP model with K
components using N weak supervision signals. Let pdikrop denote the probability of dropping the effect of the ith component for the kth weak supervision signal (1  i  K and 1  k  N ).
For each training instance, we construct a binary matrix B with the dimensionality of K × N , whose elements are sampled from
Bernoulli distributions as follows:

Bik  Bern(1 - pdikr op )

(10)

Each element Bik indicates whether the ith component should be

kept for the kth weak supervision signal or not. We make sure that

at least one component is kept, such that

K i =1

Bik

>

0.

Therefore,

we compute each prediction Pk as (k (1, 2, · · · , K )), where

k at training time is computed as:

k (1, 2, · · · , K ) =

1

K i =1

Bik

K
Biki i
i =1

(11)

This results in different predictions for P1, P2, · · · , PN at training time. At the inference time, no component must be dropped, so the matrix B is filled with 1s. In this case, Equation (11) is equivalent to Equation (2).
The proposed component dropout technique is similar to the field-level dropout approach, recently proposed by Zamani et al. [53] to prevent over-dependence on high-precision fields (e.g., clicked queries) in neural ranking models for semi-structured documents. The presented technique not also avoids overfitting on a weak supervision signal, but also allows us to use multiple weak signals as described earlier in this section.

4.3 Weak Supervision Signals

To train a generalized model, a natural decision would be to select weak labelers based on different intuitions, assumptions, and consumed information. This enables the neural model to observe complementary information in order to improve its generalization. Hence, we select a clarity-based approach, a score-based approach, and a combining approach (see Section 2.1 for more information about these categories) as the weak supervisors for our NeuralQPP model. The chosen weak labelers are described below:

Clarity. Clarity [11] is one of the early methods for query perfor-

mance prediction that is based on the language modeling frame-

work [35]. In more detail, this method estimates the query perfor-

mance as follows:

clarity(q;

C,

M)

=

w

V

p(w

|Rq

)

log

p(w |Rq ) p(w | C)

(12)

where V denotes the vocabulary set, Rq represents the query language model estimated using relevance models [23], and  C repre-
sents the reference language model estimated using a maximum

109

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

likelihood estimation over the whole collection. Intuitively, this model measures the coherence of term distributions in the top retrieved documents with respect to the collection. The term distribution analyzer component (see Figure 1b) is expected to learn such a measurement. To generate this weak label, we set the number of retrieved documents to 200.

Normalized Query Commitment (NQC). NQC [44] measures the query performance by computing the normalized standard deviation of the retrieval scores assigned to the top retrieved documents, as follows:

N QC(q; C, M) =

1 n

D Mn (q;C) (score(q, D) - µ^)2

score(q, C)

(13)

where



n M

(q;

C

)

is

the

result

list

containing

the

top

n

retrieved

documents in response to the query q. µ^ denotes the mean retrieval

scores

in



n M

(q;

C).

The

intuition

behind

this

model

is

that

query

drift can potentially be estimated by measuring the diversity of

the retrieval scores. The retrieval scores analyzer component (see

Figure 1a) also gives us such a measurement. To generate this weak

label, the number of retrieved documents is again set to 200.

Utility Estimation Framework (UEF). UEF [42] is a theoretical

framework by Shtok et al. based on statistical decision theory. UEF

estimates the utility that each retrieved document provides w.r.t.

the initiated query, as follows:

U EF (q; C, M)



sim( M n

(q;

C

),



n M

(Rq

;



n M

(q;

C)))

Pr(Rq |Iq )

(14)

where



n M

(Rq

;



n M

(q;

C

))

is

the

original

result

list

re-ranked

by

the

relevance model's estimation of the query language model (Rq ). The

function 'sim' computes the similarity between two rank lists. We

used Pearson's  coefficient as a ranking similarity measurement,

as is the standard for QPP comparisons. To estimate the represen-

tativeness probability Pr(Rq |Iq ), we used Zhou and Croft's WIG approach [57] for the unigram language model2. It is computed as

follows:

Pr
W IG

(Rq

|Iq

)



11 |q| n

(score(q, D) - score(q, C))
D Mn (q;C)

(15)

Note that the original UEF approach uses multiple samples to produce a relevance model, although, we used a single sampling to produce this weak label. To obtain this weak label, n is set to 100.

5 EXPERIMENTS
In this section, we study the effectiveness of the proposed method experimentally. We first introduce our datasets and then explain how our model is evaluated. We then describe our experimental setup in detail for further reproducibility. We finally discuss our empirical results.

5.1 Data
Collections. We evaluate our models using four TREC collections: The first two collections (AP and Robust) consist of thousands of news articles and are considered homogeneous collections. AP
2The original WIG approach is based on the term dependence model [28]. This bagof-words variant is used as our third weak signal, and has been shown to be highly effective [5].

and Robust were previously used in the TREC 1-3 Ad-Hoc Tracks and the TREC 2004 Robust Track, respectively. The second two collections (GOV2 and ClueWeb) are large-scale web collections containing heterogeneous documents. GOV2 consists of the ".gov" domain web pages, crawled in 2004. ClueWeb (i.e., ClueWeb09Category B) is a common web crawl collection that only contains English web pages. GOV2 and ClueWeb were previously used in TREC 2004-2006 Terabyte Track and TREC 2009-2012 Web Track, respectively. The statistics of these collections as well as the corresponding TREC topics are reported in Table 1. We use only the topic titles as queries.
We cleaned the ClueWeb collection by filtering out the spam documents. The spam filtering phase was done using the Waterloo spam scorer3 [8] with the threshold of 60%. Stopwords were removed from all collections and no stemming was performed.
Training Queries. Similar to prior work on weak supervision for IR [16, 50], we computed all of the weak supervision signals (see Section 4.3) using several million unique queries obtained from the publicly available AOL query logs [32]. This dataset contains a sample of web search queries submitted to the AOL search engine within a three-month period from March 1, 2006 to May 31, 2006. We only used the query strings, and no session and click information was obtained from the query logs. We filtered out the navigational queries containing URL substrings, i.e., "http", "www.", ".com", ".net", ".org", ".edu". All non-alphanumeric characters were removed from the queries. As a sanity check, we made sure that no queries from the training set appear in our evaluation query sets. Applying all of these constraints leads to over 6 million unique queries as our training query set.
5.2 Evaluation
Following prior work on query performance prediction [11, 14, 40, 42, 44, 57], we evaluate our models by computing the correlation between the predicted performance and the actual average precision for the top 1000 documents retrieved per query (AP@1000). In our main experiment, we also report the correlation with the true NDCG values [20] for the top 20 documents. Note that NDCG@20 is a preferred evaluation metric for the ClueWeb collection due to the shallow pooling performed during relevance assessments [6, 25]. We predict the performance of the query likelihood model [35] with Dirichlet prior smoothing (µ = 1500) [55] implemented in the Galago4 search engine [10].
We use the standard measures from previous research to compute the correlation between predictions and actual performance. Pearson's  coefficient as a linear correlation metric and Kendall's  coefficient as a ranking-based correlation metric are used. Statistically significant results are reported for two confidence intervals: 95% (p_value < 0.05) and 99% (p_value < 0.01).
Following prior work [40, 43, 44], to evaluate our models as well as the baselines, we first generate 30 equal-size random splits for each collection. In each split, the first fold is used for hyperparameter optimization using grid search; the hyper-parameter setting that led to the highest Pearson's  correlation on predicting
3 http://plg.uwaterloo.ca/~gvcormac/clueweb09spam/ 4 http://www.lemurproject.org/galago.php

110

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Statistical properties of the four collections used.

ID AP Robust GOV2 ClueWeb

collection Associated Press 88-89 TREC Disks 4 & 5 minus CR 2004 crawl of .gov domain ClueWeb 09 - Category B

queries (title only) TREC 1-3 Ad-Hoc Track, topics 51-200 TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 2004-2006 Terabyte Track, topics 701-850 TREC 2009-2012 Web Track, topics 1-200

#docs 165k 528k 25m 50m

avg doc length 287 254 648 1506

#qrels 15,838 17,412 26,917 18,771

the actual AP@1000 values was selected for evaluation on the second fold. This process was repeated for all 30 splits, and the average performance over the second folds are reported. This enables us to perform the paired t-test with Bonferroni correction to identify statistically significant differences between the performance of two QPP models (p_value < 0.05).
5.3 Experimental Setup
We implemented and trained our models using TensorFlow5. The network parameters were optimized with the Adam optimizer [21] based on the back-propagation algorithm [41]. In our experiments, the learning rate was selected from [1e - 5, 5e - 5, 1e - 4, 5e - 4, 1e - 3, 5e -3] and the batch size was set to 128. Either two or three hidden layers were used for each sub-networks of the NeuralQPP framework. The layer sizes were selected from {100, 300, 500}. We select the parameter vector  (see Equation (7)) from {0.2, 0.4, 0.6, 0.8} and the dropout and the component dropout probabilities (see Equation (10)) from {0, 0.2, 0.4, 0.6, 0.8}. We initialized the embedding matrix E (see Equation (5)) by pre-trained GloVe [33] vectors trained on Wikipedia dump 2014 plus Gigawords 5.6 The embedding dimension was set to 100.
5.4 Results and Discussions
In this section, we first evaluate our model against state-of-theart unsupervised QPP approaches. We then analyze each component of the designed neural network. We further study the influence of incorporating multiple weak supervision signals in the NeuralQPP model. In our final experiments, we explore how NeuralQPP performs in terms of predicting the performance of various retrieval models.
Comparison with the Baselines. In the first set of experiments, we evaluate our models against popular and state-of-the-art query performance prediction baselines, including:
· Clarity [11]: See Section 4.3 for the details of this model. · Query Feedback (QF): a high-performing QPP approach by Zhou
and Croft [57] that measures the intersection of the result lists obtained by the original query and an estimated query from the top retrieved documents. Intuitively, this approach looks at the retrieval engine as a noisy channel and estimates the quality of the channel by measuring the amount of corruption in the result lists. · Weighted Information Gain (WIG): a popular approach introduced by Zhou and Croft [57] that computes the information gain of the top retrieved documents compared to the collection.
5 http://tensorflow.org/ 6 https://nlp.stanford.edu/projects/glove/

WIG predicts the query performance by analyzing the mean retrieval score and the collection's score. · k : a simple yet effective QPP model that computes the standard deviation of the retrieval scores for the top k retrieved documents. This model has been explored by Pérez-Iglesias and Araujo [34]. · n(x%): another approach based on the standard deviation, proposed by Cummins et al. [14], that uses a dynamic number of documents per query. This approach computes the standard deviation of the top retrieved documents whose retrieval scores are at least x% of the one obtained by the highest ranked document. · Normalized Query Commitment (NQC) [44]: See Section 4.3 for the details of this model. · Score Magnitude and Variance (SMV): a more recent QPP approach by Tao and Wu [46] that considers not only the "variance"7 over the retrieval scores, but also the score magnitude. · Robust Standard Deviation (RSD): a recent QPP method proposed by Roitman et al. [40] that computes multiple weighted standard deviations based on a bootstrapping approach. As suggested by the authors, we used WIG [57], as the sample weighting function. · CombSum: a simple aggregation approach applied on top of the predictions generated by all the above baselines. For this model, we first normalize the scores generated by each model. This is a linear combining approach. · Utility Estimation Framework (UEF) [42]: See Section 4.3 for the details of this model. The choice of representativeness probability in UEF is considered as a hyper-parameter and selected from {NQC, QF, WIG}.
As described in Section 5.2, all of the hyper-parameters of the baselines were optimized in the same way as the proposed models. In particular, the number of top retrieved documents is a common hyper-parameter in all of them. We selected this hyper-parameter from {5, 10, 15, 20, 25, 50, 100, 300, 500, 1000}.
The results for the above baselines and the proposed NeuralQPP model with two training settings (pointwise and pairwise) are reported in Table 2. Note that neither the baselines nor the proposed approaches require labeled training data. To have a fair comparison, we do not compare against supervised baselines, such as [36, 39]. The first observation from Table 2 is that there is no clear winner among the baselines. From the baseline results, predicting the query performance on the web collections is generally a much harder task when compared to the newswire collections. This is mostly due to the collection size, the variety of topics it covers, and the amount of noise in the collection. Although previous work mostly focused on predicting the performance of queries in terms of average precision for a deep ranking cut-off, we also provide the results for
7SMV does not exactly compute the variance. Instead, its formulation is more similar to WIG [57].

111

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Performance of query performance prediction models on four collections, in terms of the Pearson's  and the Kendall's  correlations. The results are reported for estimating the performance of each query in terms of two target metrics (AP@1000 and NDCG@20). The highest value in each column is marked in bold, and the superscripts  /  denote statistically significant
improvements compared to all baselines at 95% / 99% confidence intervals.

Method
Clarity QF WIG k n(x % ) NQC SMV RSD CombSum UEF NeuralQPP (Pointwise) NeuralQPP (Pairwise)

AP P- K- 0.556 0.428 0.585 0.438 0.547 0.391 0.514 0.349 0.524 0.289 0.540 0.369 0.505 0.349 0.594 0.406 0.584 0.444 0.647 0.468

Target Metric: AP@1000

Robust

GOV2

P- K- P- K-

0.410 0.292 0.319 0.205

0.418 0.274 0.494 0.324

0.444 0.294 0.462 0.321

0.438 0.271 0.341 0.292

0.380 0.218 0.342 0.255

0.445 0.283 0.424 0.321

0.401 0.274 0.357 0.279

0.455 0.352 0.444 0.276

0.483 0.338 0.486 0.317

0.565 0.364 0.502 0.315

ClueWeb P- K- 0.046 0.068 0.273 0.130 0.238 0.202 0.323 0.183 0.188 0.139 0.308 0.139 0.326 0.156 0.193 0.096 0.313 0.151 0.341 0.195

0.613 0.432 0.582 0.370 0.517 0.322 0.362 0.219

0.697 0.483 0.611 0.408 0.540 0.357 0.367 0.229

AP P- K- 0.437 0.293 0.442 0.296 0.427 0.287 0.428 0.260 0.428 0.210 0.464 0.290 0.438 0.266 0.459 0.315 0.470 0.318 0.435 0.302

Target Metric: NDCG@20

Robust

GOV2

ClueWeb

P- K- P- K- P- K-

0.321 0.221 0.097 0.073 0.040 0.050

0.379 0.263 0.322 0.208 0.205 0.084

0.335 0.207 0.308 0.225 0.255 0.175

0.365 0.240 0.252 0.223 0.300 0.127

0.273 0.158 0.232 0.196 0.185 0.082

0.390 0.255 0.257 0.203 0.270 0.102

0.371 0.256 0.335 0.241 0.282 0.121

0.394 0.286 0.339 0.203 0.199 0.095

0.434 0.334 0.349 0.213 0.286 0.162

0.501 0.332 0.311 0.188 0.300 0.159

0.442 0.321 0.528 0.350 0.346 0.232 0.341 0.201

0.492 0.336 0.539 0.343 0.371 0.239 0.352 0.218

an additional evaluation metric (NDCG@20) that computes the query performance for a shallow ranking cut-off. An interesting observation here is that estimating the query performance in terms of NDCG@20 is a harder task, since the predicted performance of various methods achieve a lower correlation with the actual NDCG@20 values in comparison with the AP@1000 values.
Our second observation from Table 2 is that the pairwise setting in the NeuralQPP model works much better than the pointwise setting. The reason might be related to the nature of the labels we use for training our models. In fact, weak supervision provides a set of noisy labels, and maximizing the likelihood of generating the labels by a neural model is not necessarily a proper choice; instead, optimizing a pairwise loss function gives more freedom to the model to obtain useful features to discriminate two queries. This enables the model to perform much better than the weak labels in almost all cases. A similar observation was made by Dehghani et al. [16] when training neural ranking models with weak supervision signals in the ad-hoc retrieval task.
Our third observation from the results reported in Table 2 is that NeuralQPP outperforms all the baselines, including the combining approaches, for most collections. The improvements achieved by the NeuralQPP model trained with a pairwise loss function are statistically significant in nearly all cases. This indicates the effectiveness of the proposed neural model and training for query performance prediction.
For the sake of space, we, hereafter, only focus on predicting AP@1000 for each query (which is also what previous work does [11, 22, 44, 57]). We also focus on the pairwise setting to train our model, which has superior performance.
Analysis of the NeuralQPP Components. As pointed out earlier, we propose three components to develop the NeuralQPP model

(see Section 3). In this set of experiments, we evaluate the performance of each of these components, individually. We also evaluate the effectiveness of the proposed component dropout technique to regularize the model with multiple components. In this experiment, we train our model with the pairwise setting and with all weak labels.
Table 3 reports the results for this experiment. According to the results, the performance achieved by each of the individual components exhibits high variance. For instance, the term distribution analyzer component achieved the highest performance on the AP collection, however, the performance achieved by the retrieval scores analyzer component on the ClueWeb collection are far higher than those achieved by the other two components. This shows that various components can capture different aspects required for achieving improved performance on different collections. Our results also validate that our model successfully makes use of the information captured by multiple components ­ employing all components together outperforms all individual components. Furthermore, the results indicate that the component dropout technique is effective in all cases and leads to improved performance. All of the improvements obtained by NeuralQPP with all three components and with the component dropout technique are statistically significant when compared to each individual component.
Analysis of the Weak Supervision Signals. In the next set of experiments, we empirically study how employing multiple weak supervision signals (see Section 4.3) affects the NeuralQPP performance. To achieve this aim, we use our model with all three components trained by each of the weak supervision signals, individually. The results obtained by these models are reported in the first section of Table 4. In the second section, we present the results for two simple models that consider all weak signals (see Section 4.1 for more detail). The first model, All-MV, aggregates

112

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Performance of the NeuralQPP's individual components as well as the Component Dropout technique in case of existing multiple components. The Pearson's  and the Kendall's  correlations are reported for the AP of the top 1000 documents per query. The highest value in each column is marked in bold, and the superscripts  denotes statistically significant
improvements compared to all individual components at a 99% confidence interval.

Component(s)
Retrieval score analyzer Term distribution analyzer Semantic Analyzer All without Component Dropout All with Component Dropout

AP P- K-
0.536 0.388
0.541 0.447
0.471 0.353 0.636 0.462 0.697 0.483

Robust P- K-
0.442 0.289
0.419 0.319
0.485 0.307 0.571 0.367 0.611 0.408

GOV2 P- K-
0.351 0.280
0.308 0.212
0.378 0.210 0.485 0.308 0.540 0.357

ClueWeb P- K- 0.346 0.188 0.056 0.073 0.090 0.084
0.349 0.193 0.367 0.229

Table 4: Performance of NeuralQPP trained with different weak labels, in terms of correlation with the actual AP@1000 values. The highest value in each column is marked in bold, and the superscript  /  denote statistically significant improvements compared to all individual weak signals as well as both All-MV and All-Ind methods at 95% / 99% intervals.

Weak

AP

Robust

GOV2

ClueWeb

Label P- K- P- K- P- K- P- K-

Clarity 0.581 0.443 0.437 0.361 0.330 0.268 0.095 0.103

NQC 0.572 0.369 0.461 0.312 0.439 0.336 0.353 0.184

UEF 0.682 0.480 0.597 0.381 0.527 0.334 0.348 0.191

All-MV 0.694 0.477 0.520 0.351 0.454 0.316 0.357 0.187

All-Ind 0.591 0.454 0.447 0.362 0.384 0.316 0.116 0.128 All-CD 0.697 0.483 0.611 0.408 0.540 0.357 0.367 0.229

the outputs for all of the weak labelers. In fact, for each training pair, All-MV selects the label by majority voting over the output of all weak labelers. The second model, All-Ind, learns three separate NeuralQPP models, each by a single weak label, and then produces the final prediction by summing the output of these individually learned models. In the last section of the table, we report the results achieved by our model, All-CD (CD stands for component dropout).
We report the results of this experiment in Table 4. By looking at the results presented in both Tables 2 and 4, we can observe a clear correlation between the performance obtained by each method: Clarity, NQC, and UEF (see Table 2), and those achieved by NeuralQPP trained with each of these models as a weak signal (see Table 4), respectively. For example, NeuralQPP trained with the Clarity model as the weak signal performs well on the newswire collections, compared to the web collections. The Clarity method itself also behaves similarly. Table 4 also demonstrates that training with multiple weak signals leads to a higher generalization, and thus a more accurate performance predictor. These improvements are statistically significant on the Robust, GOV2, and ClueWeb collections.
Our results also demonstrate that the proposed approach for learning from multiple weak labelers is more effective than both AllMV and All-Ind. In particular, All-Ind has poor overall performance, because the models are learned individually and the scale of their

Table 5: Performance of the NeuralQPP model for predicting the average precision of the top 1000 documents for popular retrieval models.

Retr.

AP

Robust

GOV2

ClueWeb

Model P- K- P- K- P- K- P- K-

QL 0.697 0.483 0.611 0.408 0.540 0.357 0.367 0.229

TF-IDF 0.671 0.480 0.619 0.412 0.562 0.386 0.355 0.210

BM25 0.718 0.503 0.624 0.412 0.483 0.322 0.310 0.197

outputs are not necessarily in the same scale.8 Therefore, one of the models may bias the final prediction. As mentioned in Section 4.1 both All-MV and All-Ind suffer from information loss provided by multiple weak labels.
Predicting the Performance of Various Retrieval Models. In the last set of experiments, we study the ability of NeuralQPP to predict the performance of various retrieval models. The results for evaluating the performance of three popular retrieval models: query likelihood (QL) [35], BM25, and TF-IDF. The results reported in Table 5 demonstrate that the NeuralQPP performs well in predicting the performance of all of these retrieval models.
6 CONCLUSIONS
In this paper, we proposed NeuralQPP, a general neural framework for the fundamental task of query performance prediction in adhoc retrieval. We implemented NeuralQPP using three components. The first two components analyze the retrieval score distribution and the term distribution in the result list, respectively. The third component, called the semantic analyzer, learns an abstract representation from the content of the top ranked documents in a semantic space. Due to the lack of training data in various settings, we proposed model training with weak supervision, an unsupervised learning approach that obtains training labels from existing unsupervised performance predictors. We explored how to train our model with multiple weak labels, which also led to the development of a component dropout technique to prevent overfitting on any of the weak supervision signals. We evaluated our models using four standard TREC collections, including two newswire and two
8Normalizing the scores for each split improves the performance for All-Ind, however, it performs worse than All-MV. The reason is that All-Ind components are trained separately.

113

Session 1C: Prediction

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

large-scale web collections. The experiments demonstrated signifi-
cant improvements over the baselines, in nearly every case. We also
studied the contributions from each component in NeuralQPP, the
effectiveness of employing multiple weak signals, and the positive
effect of the component dropout technique on the performance
prediction accuracy.
Acknowledgements. This work was supported in part by the
Center for Intelligent Information Retrieval, in part by NSF grant
#IIS-1160894, in part by NSF grant #IIS-1419693, and in part by
ARC grant DP170102231. Any opinions, findings and conclusions
or recommendations expressed in this material are those of the
authors and do not necessarily reflect those of the sponsors. We
thank Oren Kurland for insights on shallow evaluation limitations
in QPPs.
REFERENCES
[1] N. Asadi, D. Metzler, T. Elsayed, and J. Lin. 2011. Pseudo Test Collections for Learning Web Search Ranking Functions. In Proc. SIGIR. 1073­1082.
[2] J. A. Aslam and V. Pavlu. 2007. Query Hardness Estimation Using Jensen-Shannon Divergence Among Multiple Scoring Functions. In Proc. ECIR. 198­209.
[3] L. Azzopardi, M. de Rijke, and K. Balog. 2007. Building Simulated Queries for Known-item Topics: An Analysis Using Six European Languages. In Proc. SIGIR. 455­462.
[4] J. G. C. de Souza, H. Zamani, M. Negri, M. Turchi, and D. Falavigna. 2015. Multitask Learning for Adaptive Quality Estimation of Automatically Transcribed Utterances. In Proc. NAACL. 714­724.
[5] D. Carmel and E. Yom-Tov. 2010. Estimating the Query Difficulty for Information Retrieval (1st ed.). Morgan and Claypool Publishers.
[6] C. L. A. Clarke, N. Craswell, and E. M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In Proc. TREC.
[7] D. Cohen, J. Foley, H. Zamani, J. Allan, and W. B. Croft. 2018. Universal Approximation Functions for Fast Learning to Rank. In Proc. SIGIR. (to appear).
[8] G. V. Cormack, M. D. Smucker, and C. L. A. Clarke. 2011. Efficient and Effective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (Oct. 2011).
[9] W. B. Croft and D. J. Harper. 1979. Using Probabilistic Models of Document Retrieval Without Relevance Information. J. Doc. 35, 4 (1979), 285­295.
[10] W. B. Croft, D. Metzler, and T. Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company.
[11] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2002. Predicting Query Performance. In Proc. SIGIR. 299­306.
[12] S. Cronen-Townsend, Y. Zhou, and W. B. Croft. 2006. Precision Prediction Based on Ranked List Coherence. Inf. Retr. 9, 6 (Dec. 2006), 723­755.
[13] J. S. Culpepper, C. L. A. Clarke, and J. Lin. 2016. Dynamic Cutoff Prediction in Multi-Stage Retrieval Systems. In Proc. ADCS. 17­24.
[14] R. Cummins, J. Jose, and C. O'Riordan. 2011. Improved Query Performance Prediction Using Standard Deviation. In Proc. SIGIR. 1089­1090.
[15] M. Dehghani, A. Severyn, S. Rothe, and J. Kamps. 2017. Avoiding Your Teacher's Mistakes: Training Neural Networks with Controlled Weak Supervision. CoRR abs/1711.00313 (2017).
[16] M. Dehghani, H. Zamani, A. Severyn, J. Kamps, and W. B. Croft. 2017. Neural Ranking Models with Weak Supervision. In Proc. SIGIR. 65­74.
[17] F. Diaz. 2007. Performance Prediction Using Spatial Autocorrelation. In Proc. SIGIR. 583­590.
[18] C. Hauff, D. Hiemstra, and F. de Jong. 2008. A Survey of Pre-retrieval Query Performance Predictors. In Proc. CIKM. 1419­1420.
[19] J. He, M. Larson, and M. de Rijke. 2008. Using Coherence-based Measures to Predict Query Difficulty. In Proc. ECIR. 689­694.
[20] K. Järvelin and J. Kekäläinen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446.
[21] D. P. Kingma and J. Ba. 2015. Adam: A Method for Stochastic Optimization. In Proc. ICLR.
[22] O. Kurland, A. Shtok, D. Carmel, and S. Hummel. 2011. A Unified Framework for Post-retrieval Query-performance Prediction. In Proc. ICTIR. 15­26.
[23] V. Lavrenko and W. B. Croft. 2001. Relevance Based Language Models. In Proc. SIGIR. 120­127.
[24] H. Li. 2011. Learning to Rank for Information Retrieval and Natural Language Processing. Morgan & Claypool Publishers.

[25] X. Lu, A. Moffat, and J. S. Culpepper. 2016. The effect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445.
[26] C. Luo, Y. Zheng, J. Mao, Y. Liu, M. Zhang, and S. Ma. 2017. Training Deep Ranking Model with Weak Relevance Labels. In Proc. ADC. 205­216.
[27] J. Mackenzie, J. S. Culpepper, R. Blanco, M. Crane, C. L. A. Clarke, and J. Lin. 2018. Query Driven Algorithm Selection in Early Stage Retrieval.. In Proc. WSDM. 396­404.
[28] D. Metzler and W. B. Croft. 2005. A Markov Random Field Model for Term Dependencies. In Proc. SIGIR. 472­479.
[29] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proc. NIPS. 3111­3119.
[30] J. Mothe and L. Tanguy. 2004. Linguistic Features to Predict Query Difficulty. In Proc. SIGIR Workshop on predicting Query Difficulty - Methods and Applications.
[31] M. Negri, M. Turchi, J. G. C. de Souza, and D. Falavigna. 2014. Quality Estimation for Automatic Speech Recognition. In Proc. COLING. 1813­1823.
[32] G. Pass, A. Chowdhury, and C. Torgeson. 2006. A Picture of Search. In Proc. InfoScale.
[33] J. Pennington, R. Socher, and C. D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proc. EMNLP. 1532­1543.
[34] J. Pérez-Iglesias and L. Araujo. 2010. Standard Deviation As a Query Hardness Estimator. In Proc. SPIRE. 207­212.
[35] J. M. Ponte and W. B. Croft. 1998. A Language Modeling Approach to Information Retrieval. In Proc. SIGIR. 275­281.
[36] F. Raiber and O. Kurland. 2014. Query-performance Prediction: Setting the Expectations Straight. In Proc. SIGIR. 13­22.
[37] J. J. Rocchio. 1971. Relevance Feedback in Information Retrieval. In The SMART Retrieval System - Experiments in Automatic Document Processing. Prentice Hall.
[38] H. Roitman. 2017. An Enhanced Approach to Query Performance Prediction Using Reference Lists. In Proc. SIGIR. 869­872.
[39] H. Roitman, S. Erera, O. Sar-Shalom, and B. Weiner. 2017. Enhanced Mean Retrieval Score Estimation for Query Performance Prediction. In Proc. ICTIR. 35­42.
[40] H. Roitman, S. Erera, and B. Weiner. 2017. Robust Standard Deviation Estimation for Query Performance Prediction. In Proc. ICTIR. 245­248.
[41] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning Representations by Back-propagating Errors. Nature 323 (1986), 533­536.
[42] A. Shtok, O. Kurland, and D. Carmel. 2010. Using Statistical Decision Theory and Relevance Models for Query-performance Prediction. In Proc. SIGIR. 259­266.
[43] A. Shtok, O. Kurland, and D. Carmel. 2016. Query Performance Prediction Using Reference Lists. ACM Trans. Inf. Syst. 34, 4 (June 2016), 19:1­19:34.
[44] A. Shtok, O. Kurland, D. Carmel, F. Raiber, and G. Markovits. 2012. Predicting Query Performance by Query-Drift Estimation. ACM Trans. Inf. Syst. 30, 2 (May 2012).
[45] L. Specia, M. Turchi, N. Cancedda, M. Dymetman, and N. Cristianini. 2009. Estimating the Sentence-Level Quality of Machine Translation Systems. In Proc. EAMT. 28­37.
[46] Y. Tao and S. Wu. 2014. Query Performance Prediction By Considering Score Magnitude and Variance Together. In Proc. CIKM. 1891­1894.
[47] P. Thomas, F. Scholer, P. Bailey, and A. Moffat. 2017. Tasks, Queries, and Rankers in Pre-Retrieval Performance Prediction. In Proc. ADCS. 11:1­11:4.
[48] C. J. van Rijsbergen. 1979. Information Retrieval (2nd ed.). ButterworthHeinemann, Newton, MA, USA.
[49] V. Vinay, I. J. Cox, N. Milic-Frayling, and K. Wood. 2006. On Ranking the Effectiveness of Searches. In Proc. SIGIR. 398­404.
[50] H. Zamani and W. B. Croft. 2017. Relevance-based Word Embedding. In Proc. SIGIR. 505­514.
[51] H. Zamani, J. Dadashkarimi, A. Shakery, and W. B. Croft. 2016. Pseudo-Relevance Feedback Based on Matrix Factorization. In Proc. CIKM. 1483­1492.
[52] H. Zamani, M. Dehghani, F. Diaz, H. Li, and N. Craswell. 2018. SIGIR 2018 Workshop on Learning from Limited or Noisy Data for Information Retrieval. In Proc. SIGIR. (to appear).
[53] H. Zamani, B. Mitra, X. Song, N. Craswell, and S. Tiwary. 2018. Neural Ranking Models with Multiple Document Fields. In Proc. WSDM. 700­708.
[54] C. Zhai and J. Lafferty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In Proc. CIKM. 403­410.
[55] C. Zhai and J. Lafferty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (April 2004), 179­214.
[56] Y. Zhou and W. B. Croft. 2006. Ranking Robustness: A Novel Framework to Predict Query Performance. In Proc. CIKM. 567­574.
[57] Y. Zhou and W. B. Croft. 2007. Query Performance Prediction in Web Search Environments. In Proc. SIGIR. 543­550.

114

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Stochastic Simulation of Test Collections: Evaluation Scores

Julián Urbano
Delft University of Technology The Netherlands
urbano.julian@gmail.com
ABSTRACT
Part of Information Retrieval evaluation research is limited by the fact that we do not know the distributions of system effectiveness over the populations of topics and, by extension, their true mean scores. The workaround usually consists in resampling topics from an existing collection and approximating the statistics of interest with the observations made between random subsamples, as if one represented the population and the other a random sample. However, this methodology is clearly limited by the availability of data, the impossibility to control the properties of these data, and the fact that we do not really measure what we intend to. To overcome these limitations, we propose a method based on vine copulas for stochastic simulation of evaluation results where the true system distributions are known upfront. In the basic use case, it takes the scores from an existing collection to build a semi-parametric model representing the set of systems and the population of topics, which can then be used to make realistic simulations of the scores by the same systems but on random new topics. Our ability to simulate this kind of data not only eliminates the current limitations, but also offers new opportunities for research. As an example, we show the benefits of this approach in two sample applications replicating typical experiments found in the literature. We provide a full R package to simulate new data following the proposed method, which can also be used to fully reproduce the results in this paper.
KEYWORDS
Evaluation, Test Collection, Simulation, Distribution, Copula
ACM Reference Format: Julián Urbano and Thomas Nagler. 2018. Stochastic Simulation of Test Collections: Evaluation Scores. In Proceedings of The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '18). ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210043
1 INTRODUCTION
Much research on Information Retrieval (IR) investigates alternative methods to better evaluate systems. Some of these seek a higher correlation between offline system measures and online user measures, more power to discriminative between systems, or reduction of judgment pool incompleteness. In this type of research we typically ask "what if" questions, like what if we use non-expert relevance
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210043

Thomas Nagler
Technical University of Munich Germany
thomas.nagler@tum.de
assessors Bailey et al. [5], what if we use a different parameterization of DCG [20], or what if we evaluate on a different document collection [24]?
One class of such problems is concerned with the reliability of our evaluation experiments and its trade-off with efficiency [29], that is, the extent to which evaluation results can be replicated, maybe under budget restrictions. Typical questions in this class are what if we change the topic set [37], what if we use P@10 instead of AP [8], what if we use the Wilcoxon test instead of the t-test [32], or how many topics will we need to achieve a certain level of confidence [12]? Unfortunately, research questions of this nature pose fundamental problems that make it impossible to find a direct answer or, at the very least, limit our ability to do so: · Finite data. Evaluation research is often of empirical nature and
uses existing data from archives like TREC. However, these data are limited to dozens of systems and topics for a given task, so the precision and generalizability of our results are severely constrained. We usually overcome this limitation by resampling the existing data, as a way to simulate new topic sets. · Unknown truth. In many cases the researcher needs to know some underlying property of the systems, such as their true mean score and variance over the possibly infinite population of topics, which are of course impossible to know. The workaround usually consists in making random splits of the topic set, considering one as representing the population of topics and the other one as a random sample from it. However, and because of the limited amount of data, these splits are not really independent samples. · Lack of control. Very often we want to study systems of predefined characteristics, such as systems with the same mean or variance, or with a certain degree of dependence. But we can not control these properties: the systems and topics in the existing data are what they are; we can not change them. Sometimes we work around this limitation with artificial modifications of the effectiveness scores, but they result in unrealistic data (eg. shifting scores by some quantity). Consider for instance the problem of choosing an appropriate number of topics for a new test collection [30]. The IR literature contains data-based approaches that repeatedly split the topic sets to calculate some statistic like Kendall  [8, 25, 40]. Extrapolating to larger topic sets, we find empirical results on the reliability of various topic set sizes. However, these studies are clearly limited by the small data sources, they calculate statistics between two samples of topics as opposed to between a sample and a population, and in the end we do not have full knowledge of the true system distributions anyway, so we can not really assess whether the extrapolation is accurate or not. There are also statistical approaches that make various assumptions like normal distributions or homoskedasticity, which clearly do not hold in IR evaluation data [12, 23, 31]. Still, the extent to which these assumptions pose

695

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

a practical problem, remains largely unknown because we can not control these properties of the data.
These limitations, however unfortunate, are present in our everyday research. As a consequence, one may wonder the extent to which past and current results really hold in practice. It is our firm believe that the IR community should seek experimentation methods that help us remove these barriers and allow us to study this kind of questions directly and under desired conditions. For this, we propose the use of stochastic simulation for the generation of evaluation data to serve as input in evaluation research. The idea of using simulation in IR research is of course not new. In the early days simulation was attempted to build collections, and in particular simulate judgments [14, 28]. More recent work has been devoted for instance to simulating queries [3], document scores [22] and various aspects of user interaction [4].
For problems pertaining to reliability we find few cases, such as [9, 10] and [30], which do simulate effectiveness scores. However, simulations therein are rather limited. In the former the scores are drawn from Beta and Uniform distributions without adjusting parameters and correlations based on existing data. The latter work does this to some degree, but the resulting simulations are still unrealistic in terms of support (eg. they are only continuous) and still do not allow us to have full control over system properties.
Building upon these works, in this paper we propose a method for the stochastic simulation of effectiveness scores that effectively avoids the three limitations discussed above. The general idea is to build a model for the joint distribution of system scores given by some effectiveness measure, such that we can simulate endlessly from it. It is important to note that we do not aim at creating a model of the systems that generated the given data, but rather a realistic model of a set of systems similar to those. For this model to be realistic, we implement several alternatives to model the marginal system distributions in a way that the underlying properties of the measure are preserved (eg. the support), as well as several alternatives to model the full dependence structure among systems. By fully specifying the effectiveness distributions, the model is also useful because we have complete knowledge of properties of the data such as the expected mean and variance. Furthermore, by separating the modeling of margins from the modeling of dependencies, we have a high level of customization that allows us to control aspects such as the levels of homoskedasticity and correlation. A full implementation of the proposed method is open-sourced as an R package, available at https://github.com/julian-urbano/simIReff. The results of the paper can be fully reproduced with data and code available at https://github.com/julian-urbano/sigir2018-simulation.
Sections 2 and 3 discuss how to model system dependencies and score distributions. Section 4 evaluates the proposed method, and Section 5 presents two application uses cases replicating past experiments. Section 6 presents the conclusions and future work.
2 MODELING SYSTEM DEPENDENCIES
In order to make realistic simulations, we need to build a joint stochastic model for the effectiveness of a set of systems. An appropriate model should reflect the behavior of the individual system

System 2 -3 -2 -1 0 1 2 3
Density 0.0 1.0 2.0 3.0
Mass 0.00 0.10 0.20 0.30

AP

Clayton

Normal



 



-3 -2 -1 0 1 2 3 System 1

Continuous
N B NKS BKS
0.0 0.2 0.4 0.6 0.8 1.0 nDCG@20

Discrete



BB

DKS



DKS-2

 

 





  


 

 



 

 

 

0.0 0.2 0.4 0.6 0.8 1.0 P@10

Figure 1: Sample bivariate copulas fitted to the AP scores of two TREC 2010 Web Ad hoc systems, and sample distributions of nDCG@20 and P@10 scores. Original data in grey.

scores as well as the dependence among them. But classical multivariate models like the multivariate Gaussian distribution are not flexible enough to describe the kind of data we have in IR evaluation (see Figure 1 for examples). A solution to this problem are copula models [19], which allow us to separate the modeling of marginal distributions (i.e. the individual distribution of each system, regardless of the others), and the dependence among systems.
Denote the effectiveness of system s on some topic by Xs , the marginal distribution of system s by Fs , and the joint distribution of all m system scores as

F (x1, . . . , xm ) = P(X1  x1, . . . , Xm  xm ).

(1)

By Sklar's theorem [27], we can decompose this distribution as

F (x1, . . . , xm ) = C F1(x1), . . . , Fm (xm ) .

(2)

The function C is called the copula of F , and it captures the dependence between systems. It is a distribution function of the random vector (U1, . . . , Um ), where Us = Fs (Xs )  Us  Uniform. Taking the derivative on both sides of (2) gives us a decomposition of the corresponding joint density:

f (x1, . . . , xm ) = c F1(x1), . . . , Fm (xm ) · f1(x1) · · · fm (xm ), (3)

where f1, . . . , fm are the marginal densities and c is the density corresponding to the copula C.
A convenient property of copulas is that they can be fitted separately from the margins. The most common procedure is:

(1) Fit the marginal distributions Fs of each system. (2) Use the fitted margins to transform the observed scores Xs to
pseudo-observations of the copula: Us = Fs (Xs ). (3) Fit the copula model to the pseudo-observations.

The procedure to simulate the scores on a new random topic is:
(1) Generate a pseudo-observation (R1, . . . , Rm ) from the copula. (2) Compute Ys = Fs-1(Rs ). By construction then, we have Ys  Fs . In reality, a copula models the dependence between quantiles of the margins (the pseudoobservations), not the actual raw observations. This way, the same copula can be used to describe the dependence among systems, but we retain full control over individual system distributions just by plugging different margins into the copula.

2.1 Gaussian Copulas
There is a variety of parametric models for the copula C, the most popular of which is the Gaussian copula [19]. It is derived from the multivariate Gaussian distribution by inversion of (2):

696

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

d  d 















d 

 

Figure 2: An R-vine tree sequence for four systems.

Cgaussian(u1, . . . , um ) =  -1(u1), . . . , -1(um ) , (4) where  is the univariate standard Gaussian cumulative distribution function, and  is the distribution function of a multivariate Gaussian with mean 0 and correlation matrix . Since the Gaussian copula is parameterized by , it allows to control the strength of dependence between each pair of variables. This is a major advantage over other parametric families such as Archimedean copulas [19], which only have 1 or 2 parameters in total.
2.2 Vine Copulas
Because the Gaussian copula is derived from the multivariate Gaussian distribution, the dependence between each pair of variables is constrained to be highly symmetric. In reality though, dependence is often asymmetric (see Figure 1-left for an example). The dependence may be stronger for small values of the system scores and weaker for large values, or the other way around. An even more flexible class of copula models are vine copulas [1]. A vine copula model is graphically represented as a set of linked trees where the edges of tree i are the nodes of tree i + 1. The full dependence structure is built with individual two-dimensional copulas attached to each of the edges.
A special kind of vine are regular vines or R-vines, where two edges in tree i are joined by a node in tree i + 1 only if they share a common node. An example on four systems is shown in Figure 2. The vertices in the second tree are the edges in the first tree. For instance, the edge {4, 1} encodes the dependence between X4 and X1. The edge {{4, 1}, {1, 2}} encodes the dependence between X4 and X2 conditional on X1.1 This edge is then represented in tree 3 with the vertex {4, 2|1}. The combination of all conditional paircopulas in the vine fully determines the dependence structure of the vector (X1, . . . , X4).
The concept of vine copulas naturally extends to higher dimensions. One merely has to attach an edge with a new vertex to each tree in the R-vine. The trees are not restricted to be paths as in Figure 2 (e.g. vertex {3} could be connected to vertex {1} instead of {2}), but the edge sets have to fulfill certain conditions that ensure that the vine copula results in a valid and fully specified dependence structure [6]. An algorithm for selecting an appropriate structure in a data-driven manner was introduced by Dissmann et al. [16].
The main advantage of vine copulas is that one can select from a large variety of parametric copula families for each pair-copula, symmetric and asymmetric ones. This makes the models very flexible, but also very complex. A vine copula on m systems consists of m(m - 1)/2 pair-copulas, and each of them requires us to select a family and estimate its parameters. Fortunately, it is possible to do
1The conditioning variable is found by intersecting node indexes: {4, 1}{1, 2} = {1}.

this sequentially, one pair-copula at a time (for details, see Aas et al. [1]). In higher dimensions (m  10), it is common to truncate the model [7]: only the pair-copulas in the first few trees are specified, and all subsequent pairs are considered independent. This allows us to fit and select a vine copula model in a matter of seconds, even in high dimensions, using mainstream computing resources.

3 MODELING SYSTEM EFFECTIVENESS DISTRIBUTIONS
As discussed in the previous section, we can model the marginal system effectiveness distributions separately from their dependence. The obvious choice would be to use the empirical distribution of the given data, but then a value that has not occurred in these data would never come up in a simulation. Instead, in this section we describe various parametric and non-parametric alternatives to model the margins. The first distinction we make is between continuous and discrete distributions. In principle, all measures are discrete because they calculate a score based on a finite document list, a finite set of judgments and a discrete relevance scale, so the possible set of outcomes is also finite. This is evident in measures like P@10, where only 11 different values are possible. However, for more fine-grained measures like AP or nDCG, the set of possible outcomes is fairly large and we can comfortably assume they follow a continuous distribution.
In the following, let us change notation and refer to the set of scores for a system over n topics as {X1, . . . , Xn }.

3.1 Continuous Distributions
When assuming a continuous distribution of effectiveness, we have simple and well-known options such as the Normal or Beta distributions. However, they are fairly restricted as to the shape that their density function can take, so we also consider kernel smoothing. Next, we present four alternatives to model a continuous distribution of effectiveness, based on the Normal and Beta.

3.1.1 (Truncated) Normal Distribution (N). One of the simplest
choices to model continuous data is the Gaussian or Normal distribution, parameterized by the mean  and variance  2. However, a Normal distribution is supported on (-, +), while effectiveness scores are typically supported on [0, 1]. To solve this issue we can truncate the distribution. Let f , F and F -1 be the density, distribu-
tion, and quantile functions of some distribution (Gaussian in our
case). The corresponding distribution truncated between a and b is

ftrunc(x )

=

F

f (b)

(x ) -F

(a)

,

(5)

Ftrunc (q)

=

F F

(q) (b)

- -

F F

(a) (a)

,

(6)

Ft-ru1nc(p) = F -1 p (F (b) - F (a)) + F (a) ,

(7)

where a = 0 and b = 1 for our purposes. Note that no assumptions are made about the original distribution, other than it being continuous. Therefore, we can follow the same idea to truncate other distributions and maintain the support of interest. For the Truncated Normal,  and  2 are typically estimated numerically by maximizing the log-likelihood of the given data. Finally, the

697

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

expected value and variance are [17]:

E[X ]

=



+

(a (b

) - (b ) - (a

) )

(8)

Var[X ] =  2

1+

a

(a (b

) - b (b ) - (a )

)

- (E[X ] - )2

(9)

a = (a - )/ , b = (b - )/ .

Figure 1 shows a truncated Normal (N) fitted to real TREC data.

3.1.2 Beta Distribution (B). A natural alternative for bounded data is the Beta distribution, which is already supported on the unit interval. In addition to a bell shape, its density function can also have J and U shapes, with custom degrees of asymmetry. It is typically described with two shape parameters ,  > 0, and its expectation and variance are

E[X ]

=



 +



,

(10)

Var[X ]

=

(

 + )2( + 

+ 1) .

(11)

There is no close form solution for maximum likelihood estimation of the parameters, so they are typically estimated via numerical optimization. Figure 1 shows a Beta (B) fitted to real TREC data.

3.1.3 (Truncated) Normal Kernel Smoothing (NKS). Even though Normal and Beta distributions are simple and well-known, they are restricted to certain shapes. A more flexible alternative is Kernel Smoothing. In its general form, a kernel-smoothed density estimator is parameterized by a bandwidth b > 0 that controls the degree of smoothing, and a kernel k, which is a non-negative function that integrates to one. The kernel-smoothed distribution is defined as

f (x) = 1 k x - Xi ,

(12)

nb i

b

F (x) = 1 K x - Xi .

(13)

ni

b

In our case, the kernel is k =  and K = , that is, the standard

Normal. In general, there is no close form expression for the quantile function F -1. However, because F is continuous and monotonically

increasing, a quantile p can be computed numerically by finding

the solution to F (x) = p.

In order to select the bandwidth parameter b, we use the auto-

matic procedure by Wand and Jones [34]. As with the Normal, we

truncate the distribution between 0 and 1 following (6). The ex-

pected value and variance are calculated via numerical integration

of the quantile function:



1

E[X ] = x f (x) dx = F -1(x) dx,

(14)



0

1

Var[X ] = (x - E[X ])2 f (x) dx = F -1(x)2 dx - E[X ]2. (15)

0

The first expressions are the text-book formulas for a continuous random variable, but because we will ultimately simulate new data through the quantile functions, we use the second expressions for higher numerical precision. Figure 1 shows a truncated Normal Kernel-Smoothed (NKS) fitted to real TREC data.

3.1.4 Beta Kernel Smoothing (BKS). The kernel function can also be based on the Beta distribution to naturally bound the support. Here we use the kernel proposed by Chen [13], which yields the density function

f (x) = 1 n

i

fBeta

Xi ;

b

x +

1

,

1 b

- +

x 1

.

(16)

Note that the above expression is not a valid density because it is not guaranteed to integrate to one, but this is easily solved by normalization over the full support. The distribution and quantile functions are once again calculated numerically, and the expectation and variance are computed following (14) and (15). The bandwidth parameter is set to b = n-2/5 by default [13]. Figure 1 shows a Beta Kernel-Smoothed (BKS) fitted to real TREC data.

3.2 Discrete Distributions
The typical method to fit a discrete distribution is to assign an integer rank to each possible value of the support, fit a well known distribution such as the Binomial or Poisson, and convert back to the original support when simulating new data. Let f be a probability mass function with support {s1, . . . , sz }. The corresponding cumulative distribution and quantile functions, as well as the expectation and variance, are easily calculated from their very definitions for a discrete distribution:

F (q) =

f (s),

(17)

s q

F -1(p) = inf {s : F (s)  p},

(18)

E[X ] = s · f (s),

(19)

Var[X ] = f (s) · (s - E[X ])2.

(20)

Below we propose a parametric and a non-parametric alternative.
3.2.1 Beta-Binomial (BB). The Beta-Binomial distribution is the Binomial distribution in which the probability of success in each trial is not fixed, but distributed according to a Beta. Similarly, its mass function can have several shapes, making it a suitable candidate for discrete data. It is parameterized by the ,  > 0 parameters of the underlying Beta, and the number of trials m > 0 of the underlying Binomial. The support is {0, 1, . . . , m}, so the original effectiveness scores need to be transformed into integers. For instance, a P@k score X would be transformed to k · X . In general,the i-th support value is transformed into i - 1.
Parameters are estimated by numerical optimization of the loglikelihood. However, note that m can be fixed to the size of the original support set minus one. In the case of P@k, there are only k +1 possible outcomes which correspond to the number of successes in m = k trials. Therefore, only  and  need to be estimated. Figure 1 shows a Beta-Binomial (BB) fit to real TREC data.
3.2.2 Discrete Kernel Smoothing (DKS). The kernel-smoothed distributions in (12) and (16) can be adapted to discrete variables by using a kernel function with a discrete support [35]:

698

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

f (x) = 1 n

i

k (x, Xi , b),

(21)

k(x, Xi , b) =

(1 - b)

1 2

(1

-

b)

·

b

|Xi

-x

|

Xi = x otherwise.

The kernel function k is designed for the case where Xi is integer valued. The discrete kernel smoother is not a proper probability mass function because it usually does not sum up to one, but this can easily be corrected by normalization. The bandwidth parameter b can be selected automatically by least-squares cross-validation [35].
The discrete kernel-smoothed distribution is very appealing for measures with non-standard support, such as Reciprocal Rank. In the typical case with an evaluation cutoff k = 1, 000, the support of an RR score is {0, 1/1000, 1/999, . . . , 1/1}. Most values are thus concentrated near 0, so a parametric model like the Beta-Binomial will not provide a good fit. The kernel-smoothed distribution in (21) is flexible enough to adapt, but it can easily overfit with rare supports like this. To alleviate this issue we introduce a bandwidth multiplier h > 1 such that the actual bandwidth is bh  1. Figure 1 shows the Discrete Kernel Smoothing (DKS) fit with the initially selected bandwidth (i.e. h = 1), and the variant with h = 2 (DKS-2).

3.3 Model Selection
Each of the above distribution models can be fitted for a given set of effectiveness scores, so in practice we will need to choose the best distribution from a set of candidates. An obvious criterion for selection is the log-likelihood (LL)

LL = i log f (Xi ),

(22)

which is maximized by the best model. However, the log-likelihood can be made arbitrarily large by making the model more complex, so it favors distributions that overfit the data. The Akaike Information Criterion [2] and the Bayesian Information Criterion [26] are two classical criteria that correct for this by penalizing the number of parameters  of a model. They are defined as

AIC = -2LL + 2,

(23)

BIC = -2LL +  log(n),

(24)

where n is the sample size (the number of topics in our case). The best fitting model minimizes the AIC or BIC. If n > 8, the BIC puts a stronger penalty on the number of parameters, so it favors more parsimonious models.
For the parametric distributions, the number of parameters is straightforward: the Normal, Beta and Beta-Binomial distributions have two parameters each (recall that m is fixed upfront). For nonparametric distributions the number of parameters is not defined, but there is an equivalent concept called effective degrees of freedom that can be used in the formulas of AIC and BIC. The most common definition arises from the concept of linear estimators [18, 21]: for any density estimator that can be written in the form f (x) = i (x, Xi ), the effective degrees of freedom are

edf =
i

(Xi , X j (Xi ,

i) Xj

)

.

(25)

This is possible to calculate for all three kernel-smoothed distributions presented in the previous sections.

3.4 Ensuring Expectations

As mentioned above, there are many situations in which researchers
want to experiment with systems whose expected values differ by
some predefined amount, or systems that have the same expected
value but a test collection says otherwise. In some other cases we
may want a fitted distribution to have the same expected value as
the observed mean score in the know data. Unfortunately, these re-
strictions are now guaranteed by construction in the above models.
Let F be a cumulative distribution function with mean . A natural way to ensure a expected value  is to shift all observations by subtracting  - , but this has two problems: the resulting distribution would not be bounded by [0, 1], and if it is discrete, the shifted
observations are not guaranteed to have a correct support anymore. For instance, a P@10 score of 0.2 could easily become something like 0.17, which is clearly an invalid value for that measure. We
propose an alternative way to modify F that ensures that the new mean is  and the support remains unchanged.
For any increasing function T : [0, 1]  [0, 1], the function F~(x) = T (F (x)) is again a distribution function and has the same
support as F . The goal is to find a transformation T such that the resulting mean of F~ is ~ = . To this end, we first restrict
the transformation to the family of Beta distribution functions B = {FB(· ; ~, ~)} : ~ > 0, ~ > 0}. Then we can solve numerically for a combination of ~ and ~, such that ~ =  according to (14). This gives us a new distribution F~ that has the desired mean ,
and whose distribution functions are:

f~(x) = fB F (x) ; ~, ~ · f (x),

(26)

F~(q) = FB F (q) ; ~, ~ ,

(27)

F~-1(p) = F -1 FB-1 p ; ~, ~ .

(28)

Essentially the same procedure may be used to ensure a predefined variance instead of mean, thus allowing us to control the homoskedasticity of the model. If on the other hand we want to enforce both mean and variance, the parametric models could be instantiated to already meet the constraints because they have precisely two parameters. In the general case, bivariate optimization can be attempted, but this goes beyond the scope of this paper.

4 EVALUATION
In this section we evaluate the proposed simulation method from the perspectives of the effectiveness distributions, the copulas, and the simulated data. In particular, we will build and evaluate distribution and copula models using data from the Ad hoc submissions to the TREC Web track between 2010 and 2014. Each of these collections contains about 50 topics and between 30 and 88 systems, for a total of 12,924 system-topic pairs. In terms of effectiveness measures, we use Average Precision, nDCG@20 and ERR@20 as exemplars of measures with continuous support, and Reciprocal Rank, P@10 and P@20 as exemplars of measures with discrete support.

4.1 System Effectiveness Distributions
The first step towards a successful simulation is to fit a model to each of the marginal system effectiveness distributions, following the principles in Section 3. Therefore, our first point of interest is which of the various alternatives provide the best fit to real

699

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

AP nDCG@20 ERR@20

P@10

P@20

RR

% cases 0 25 50 75 100

% cases 0 25 50 75 100

LL AIC BIC LL AIC BIC LL AIC BIC Selection criterion

Normal (N) Beta (B)

Normal KS (NKS) Beta KS (BKS)

LL AIC BIC LL AIC BIC LL AIC BIC Selection criterion

Beta-Binomial (BB)

Discrete KS (DKS) DKS w/ mult. (DKS-h)

Figure 3: Distributions of models selected for the marginal system distributions, before  transformation.

R-vine copula 1000 4000 7000

LL
       
1000 3000 5000 7000 Gaussian copula
Web 2010

R-vine copula -8000 -4000

AIC

BIC

4000

  







 





-8000

-4000

Gaussian copula

R-vine copula

-4000 0



 

 





-4000

0 2000

Gaussian copula

Web 2011

Web 2012

Web 2013

Web 2014

Figure 5: LL (higher is better), AIC and BIC (lower is better) of the fitted Gaussian vs. R-vine copulas.

AP nDCG@20 ERR@20

P@10

P@20

RR

% cases 0 25 50 75 100

% cases 0 25 50 75 100

LL AIC BIC LL AIC BIC LL AIC BIC Selection criterion

Normal (N) Beta (B)

Normal KS (NKS) Beta KS (BKS)

LL AIC BIC LL AIC BIC LL AIC BIC Selection criterion

Beta-Binomial (BB)

Discrete KS (DKS) DKS w/ mult. (DKS-h)

Figure 4: Distributions of models selected for the marginal system distributions, after  transformation.

evaluation data. For each collection and measure, we start from the n × m matrix with the scores by all m systems on all n topics. For measures with continuous support we fit 4 different models: truncated Normal (N), Beta (B) truncated Normal Kernel Smoothing (NKS), and Bounded Kernel Smoothing (BKS). For measures with discrete support we fit up to 5 different models: Beta-Binomial (BB), Discrete Kernel Smoothing with multiplier h = 1 (DKS), and DKS with multiplier h  {2, 5, 10} (DKS-h)2.
A grand total of 5,425 models were fitted for all 1,572 combinations of measure and system. For each combination, the best model was selected according to LL, AIC and BIC (see Section 3.3). Figure 3 shows the distributions of best models for each effectiveness measure and before -transformation. Across measures we can see that log-likelihood favors the more complex non-parametric models NKS and DKS. With continuous measures, the parametric models N and B are chosen only about 25% of the times, and the discrete BB is in fact never selected by the log-likelihood criterion. As expected though, AIC and BIC penalize this complexity and tend to select the simpler models. The exception is Reciprocal Rank, where the BB model is still never selected because it is not able to adapt to the non-standard support as well as the non-parametric DKS. What we do appreciate in this case is the selection of DKS-h by AIC and BIC, because of the lower effective degrees of freedom.
As discussed in Section 3.4, we often need to transform system distributions to make sure their expectation equals some predefined value. In principle, the best model before transformation is not necessarily the best one after transformation, so we attempted to transform all 5,425 models such that their expected values equal the mean score observed in the given data (within a 10-5 threshold), and then performed model selection again. The transformation was
2Not all DKS-h models can be fitted in all cases. Whether a particular value of h produces a valid fit depends on the pre-selected bandwidth b.

successful in 5,003 cases (92%), and Figure 4 shows the distributions of selected models. Compared to the untransformed case, we can see that non-parametric models are almost always selected because of their flexibility. In fact, the most successful model in the continuous case is BKS, which was seldom selected before.
Although these results do not tell us in and on themselves about the quality of the fitted models, they suggest that the best options are generally those that are neither too complex like kernel smoothing, nor too simple like basic Normal or Beta distributions. Furthermore, they suggest that there is no single best model for all cases, not even within measures, and that if transformations are required, model selection should be performed afterwards.
4.2 Copulas
The second step towards a successful simulation is to fit a copula model to the pseudo-observations, following the principles in Section 2. Therefore, our second point of interest is which alternative provides the best dependence model to real evaluation data.
In order to study whether Gaussian copulas are appropriate to model the dependence among systems, we first fit bivariate copulas between every pair of systems in the same collection, selecting the best candidate according to log-likelihood. From the total of 39,627 system pairs, Gaussian copulas are selected only in 2.7% of the cases, thus evidencing that simple correlation is not enough to model the dependence found among real systems. The most common copula is the BB8 copula, selected 30% of the times, followed by the Tawn 1 (16%), Tawn 2 (16%), BB7 (12%), t (9%), BB1 (4.8%), Clayton (4.2%), Frank (4%) and others. Figure 1-left shows an example.
We then fit full Gaussian and R-vine copulas to all systems and topics in an effectiveness matrix, performing selection based on LL, AIC and BIC. Suggested by the previous results on pair-copulas, we first check whether the extra complexity of vine copulas actually helps us capture the full dependence structure in a better way. Figure 5 shows the goodness of fit of all 30 Gaussian copulas compared to their R-vine counterparts. According to all selection criteria, R-vine copulas do indeed provide a significantly better fit.
Another aspect to analyze is the possibility to truncate the vines in order to speed up the fitting process at the cost of reducing the goodness of fit. One indicator to decide whether to continue to the next vine level or not, is the Kendall  correlation observed in the pair-copulas of the current level: if the correlation is low, we may decide to stop and truncate, and if the correlation is high we may decide to continue because there appears to be some degree of dependence to further incorporate into the vine. Figure 6

700

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

|| 0.05 0.1 0.2 0.5 1
|| 0.05 0.1 0.2 0.5 1

Mean

Maximum

AP nDCG@20 ERR@20 P@10 P@20 RR

Copula

Copula

12

5 10 20 50

Truncation Level

12

5 10 20 50

Truncation Level

Web 2010

Web 2011

Web 2012

Web 2013

Web 2014

Figure 6: Mean and maximum absolute value of the Kendall  (log-scaled) at each truncation level (log-scaled) of the Rvine copulas. Lines represent within-collection means.

shows the mean and maximum absolute value of the  correlation scores at each level of the R-vine copulas. In the first level we can see very high correlations, but already in the second level they drop significantly. The average correlation remains around 0.1 throughout truncation levels, but the maximum decreases very slightly between 0.3 and 0.2 until nearly the end, indicating that some specific pair-copulas are still capturing a relevant amount of dependence. Overall, these results suggest that there is no obvious point to truncate the vines, and that in any case the truncation level of course depends on the number of systems. Even though the number of pair-copulas fitted in a full vine grows quadratically with the number of systems, a mainstream computer fitted the full models in under one minute each, so our suggestion is not to truncate.
4.3 Simulated Scores
The third and final step towards a successful simulation is to generate random pseudo-observations from the copula, and from there the final effectiveness scores via the marginal distributions. The mean and variance of the simulated scores should meet the values predefined in these distributions. This is the ultimate goal of using the proposed method: simulating new data from a model for which we know the full characteristics in advance.
We start with the marginal distributions and copulas fitted in the previous section, and proceed as follows. For each copula we simulate 1,000 random observations, and record the observed mean and variance for each of the m systems. This is repeated 1,000 times, yielding 1, 000 · m sample means and sample variances. For each of these we compute the deviation from the expected values, that is,  - X and  2 - s2. Over repetitions, we expect these deviations to be centered at zero, meaning that the simulated scores are unbiased. Figure 7 shows at the top the distributions of deviations. We can first appreciate that all distributions are indeed centered at zero, indicating that the simulated scores are not biased and therefore the  and  2 scores predefined by the margins can be trusted. We can also observe higher variability in the discrete measures, because their support is much less fine-grained than the continuous measures, leading to less stability [8]. In terms of magnitude of the deviations, we can see that in most cases they are within 0.01 of the mean and 0.002 of the variance.
We note that these are deviations for samples of size 1,000 topics; larger samples have of course smaller deviations. For comparison, the bottom plots in Figure 7 show the distributions of deviations for

-0.02

-0.01

0

0.01

Deviation from 

R

Beta
Normal Beta- Binomial

0.02

-0.006 -0.004 -0.002 0 0.002 0.004 0.006 Deviation from 2 R

-0.02 -0.01

0

0.01 0.02

Deviation from 

-0.006 -0.004 -0.002 0 0.002 0.004 0.006 Deviation from 2

Figure 7: Deviations from the mean (left) and variance (right), in samples of size 1,000 simulated from the R-vines. The bottom plots show similar statistics for simulations of standard distributions as implemented in R.

various random number generators in the statistical software R. In particular, we simulated 100,000 samples of 1,000 observations each, and similarly recorded the deviations from the theoretical mean and variance. The data are simulated from three distributions with a random selection of parameters such that the simulated scores have dispersion similar to the original TREC data: Gaussian with   [0.15, 0.2], Beta with ,   [1, 20], and Beta-Binomial with m = 10 and ,   [1, 6] (normalized to the unit interval)3. As the plots show, the distributions of deviations in base R are indeed similar to the deviations in the simulated evaluation data.
Finally, Figure 8 (left) shows as an example the Spearman correlation matrix among the Web 2010 systems, as per nDCG@20. The second matrix presents the observed correlations in 500 random topics simulated with the Gaussian copula. As expected, they are very similar because the Gaussian copula models linear correlations. The third matrix presents the correlation in 500 topics simulated from the R-vine copula without truncation, which is much more faithful to the original data at least with respect to correlation (recall that vine copulas model dependence structures more complex than simple correlation). The last matrix shows similar results from an R-vine truncated at level 2, showing that even if we truncate this early the bulk of the dependence structure is accounted for.
Overall, the results suggest that the simulated data behaves as expected within reasonable precision bounds, and that it is indeed capable of capturing the dependence underlying the existing data. These models effectively provide an endless supply of realistic evaluation data with known and predefined characteristics, which can prove to be a very valuable resource for IR evaluation research.
5 SAMPLE APPLICATIONS
In this section we describe two sample applications of the proposed simulation method, showing how it can help us overcome the discussed limitations in IR evaluation research. In particular,
3These simulations used the standard functions stats::rnorm, stats::rbeta and extraDistr::rbbinom.

701

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 8: Spearman correlation matrices of the original Web 2010 nDCG@20 scores and 500 observations simulated with the Gaussian copula, the R-vine copula without truncation, and truncated at 2 levels.

we repeat an experiment by Webber et al. [38] on the estimation of the variability of between-system score deltas for the purposes of power analysis, and an experiment by Voorhees [33] on hypothesis testing. The selection of these two works is by no means intended as criticism. On the contrary, they are selected as clear examples of how researchers have to make do with the available data and work around these limitations.

5.1 Statistical Power and Topic Set Size

A typical problem in IR evaluation is deciding an appropriate number of topics to compare two systems with a some degree of confidence. In Section 5.3 of [38] the authors describe an experiment using the TREC 2004 Robust data, emulating a researcher who builds a new test collection by iteratively adding new topics until a certain level of statistical power is achieved. In particular, they use only the 150 topics from TREC 6­8 and the 78 not description-only systems. Next we describe their original design and two alternatives using our simulation method (Figure 9 presents an outline). Original Design (O). Repeat the following experiment 100 times:

(1) Randomly select a system from the second quartile of runs and

another system from the top three quartiles.

(2) Compute the original per-topic system score deltas D1 . . . D150. (3) Compute 150 as the standard deviation observed in D1 . . . D150.

Assume 150 is the true population standard deviation.

(4) Compute the target  equal to the minimum detectable differ-

ence by a t-test, with power 80% and significance level 5%, if

using 100 topics.

(5) Repeat the following iterative process for i = 1 . . . B:

(a) Start with an empty topic set.

(b) Sample a new topic with replacement from the 150 available, compute the standard deviation  (i) with the current topic set, and the detectable difference d assuming  (i).

(c) If d >  , go back to step 5.b. If not, stop and record the current number of topics n(i) and  (i).

(6) Record the mean number of topics and standard deviation over

the B

=

1, 000 trials: nB

=

1 B

n(i )

and B

=

1 B

 (i).

In this experiment, B is intended to estimate the population  , which is set to 150. Similarly, nB is intended to estimate the

required number of topics, set to 100. In their Figure 5 they plot

each B versus the corresponding 150, showing a clear bias. We

repeated their experiment with 200 system pairs; plot O) in Fig-

ure 10 shows the same results. The standard deviations are always

underestimated, by 5.3% on average, and the required number of

K Z

   
     

  

 
     
   

 Z  Z

     
   
     

  


 
     
   

 Z

  









     

      



 

     

Figure 9: Outline of the experimental designs to show the effect of sequential testing: O) original design [38], A1) and A2) alternatives with the proposed simulation method.

topics is also underestimated to 93. Essentially, this experiment shows empirical evidence of the problem of sequential testing [39], further demonstrated in subsequent IR works [11, 36].
However, this is a clear case in which researchers are restricted by the available data and they have to make certain assumptions to approximate the statistics of interest. Indeed, the ultimate goal of this experiment was to see how much bias the iterative sampling of topics introduces in the estimation of the true population  . In step (3), the 150 observed in the available data is taken as the true population  , which is of course unknown in reality. The larger question though, is not how biased B is with respect to 150, because it has some degree of error itself, but with respect to the true  . Alternative Design 1 (A1). The first alternative to overcome this problem using our simulation methodology, modifies step (2) to:
(2) Fit the margins and bivariate copula from D1 . . . D150, thus setting the true distribution F and  . Simulate a new set D1+ . . . D1+50 from the copula, which are used to compute 150.
The A1) plots in Figure 10 show the results. First, we see that B is still always underestimating 150 because of sequential testing, but when compared to the true and known  , the underestimation is not as consistent. Still, on average  is underestimated by 2.9%, and the required number of topics is 97. Alternative Design 2 (A2). However, the best way to make full use of our proposed simulation methodology is to use the original 150 topics just to fit the copula, and always simulate a brand new topic from it in step (5.b). This allows us to compare B with the true and known  over truly independent topic sets, thus eliminating the limitations of the original design. In the A2) plot of Figure 10 we can observe that the bias due to sequential testing is still clearly present,

702

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

B 0.00 0.05 0.10 0.15 0.20
B 0.00 0.05 0.10 0.15 0.20
B 0.00 0.05 0.10 0.15 0.20
B 0.00 0.05 0.10 0.15 0.20

0) Resampling

  





0.00

0.05

0.10 150

0.15

0.20

A1) Copula + Resampling

 

  

0.00

0.05

0.10 150

0.15

0.20

A1) Copula + Resampling

 





 



 

 

0.00 0.05 0.10 0.15 0.20 

 

A2) Copula
    

0.00 0.05 0.10 0.15 0.20 

Figure 10: O) B estimated via resampling vs. assumed 150 from the original 150 topics (replication of Figure 5 in [38]). A1-left) same but from 150 new topics simulated from a copula. A1-right) B estimated via resampling vs. the true  prefixed by the copula. A2) B estimated via true random sampling from the copula vs. true  prefixed by the copula.

with an average underestimation of 2.9% on  and an average of 97 topics required from the expected 100.
In summary, this first use case allows us to pinpoint two limitations in this kind of IR evaluation research, namely that we do not know the true characteristics of systems (population  ), and the limited availability of data (only 150 topics) that forces us to resample and make assumptions. The proposed methodology for stochastic simulation effectively eliminates these two limitations and allows us to study the research question directly and precisely. Indeed, our experiments reproduce the undesired effect of sequential testing.
5.2 Hypothesis Testing and Type I Errors
A recurring problem in IR evaluation concerns hypothesis testing and Type I errors. A sample work on this topic is [33], where the authors describe an experiment using the TREC 2004 Robust data. In particular, they employed the 100 topics from TREC 7­8, and only the top 83 systems according to mean AP score over the entire set. Next we describe their original design and three alternatives using our simulation method. Original Design (O). The following was repeated 1,000 times:
(1) Randomly split the 100 topics in two halves T1 and T2. (2) For each pair of systems A and B, run a t-test on the scores over
T1 and another test on T2, both 2-tailed and at  = 0.05. (3) Record which of the two tests are significant, and mark the pair
as minor conflict if the mean scores have different sign but only one test is significant, major conflict if the means have different signs and both tests are significant, or no conflict otherwise.
Similar experiments appear in the literature in reference to Type I error rates, sometimes incorrectly approximating it with the fraction of significant results that are in a conflict [15, 25, 32, 40]. In [33], authors reported that 2.8% of the significant results where part of a conflict when using AP, and 10.9% when using P@10. Other papers report similar findings for AP, somewhat suggesting that the t-test is too conservative with IR data and it makes about half as many Type I errors as it should, or twice as many in the case of P@10. Here we repeated the original experiment of [33] and found 2.91% of conflicts with AP and 12% with P@10, thus confirming their results to a large extent. Alternative Design 1 (A1). This experiment is similarly constrained by the limited amount of topics, which are repeatedly split in two in order to simulate independent topic sets. This may introduce bias because the two sets are not really independent and always come from the same 100 topics. To assess the effect of this bias we

carried out a similar experiment, but instead of the random split in step (1), we simulated two new random sets of 50 topics each from a bivariate copula. This means that every single comparison between a pair of systems is made with truly random and independent topic sets. The observed conflict rates are 2.67% in AP and 15% in P@10.
Another limitation of the original design is that the conflict rate is only a rough approximation to the Type I error rate, which is the statistic we are really interested in. In fact, there is no way of knowing if a test yielded a false positive or not because the true system mean scores are unknown, that is, whether the null hypothesis is true or not is unknown to begin with. Using our proposed methodology based on simulation, we have two options to avoid this limitation. Alternative Design 2 (A2). The first alternative ensures that both systems have the same marginal distribution and hence the same expected values, making the null hypothesis true by definition. The Type I error rate can now be empirically estimated as the fraction of comparisons that yield a significant result. We proceed as follows. Given the two systems A and B, we fit the bivariate copula just like in the previous example, but use the same marginal distribution FA for both systems. This way we simulate from systems with the same distribution but different dependence structure. For higher precision, the experiment was repeated 10,000 times for each pair of systems, and we found 4.88% of Type I errors with AP at  = 0.05, and 0.9% at  = 0.01. With P@10 we found 4.94% and 0.96% of errors respectively. Alternative Design 3 (A3). The last alternative consists in using different marginal distributions, but transformed such that they have the same expected value (see Section 3.4). This presents a more realistic scenario than the previous alternative. We proceed as follows. Given a system A, we randomly select a system B from the 10 systems whose expected values are closest to A's, and transform FB such that its expected value is A. This way we simulate from systems with different distributions but same expected values, and different dependence structure. The experiment was again repeated 10,000 times for each pair of systems, and we found 4.88% and 0.9% of Type I errors with AP at  = 0.05 and  = 0.01. With P@10 we found 5% and 0.96% of errors respectively.
In summary, this second use case serves us to pinpoint three main limitations in this kind of IR evaluation research, namely the limited availability of data (only 100 topics), the lack of control over these data (truth of the null hypothesis), and inability to measure the actual statistics of interest (Type I error rate). The proposed

703

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

methodology for stochastic simulation avoids these limitations again and allows us to study the research question directly. In this particular case, our experiments show that the empirical error rates are kept at the nominal  level, showing that the t-test is not a conservative test with IR data.
6 CONCLUSIONS
In this paper we make the case for stochastic simulation of evaluation data to support research on IR evaluation without the known limitations of current practice, namely the scarcity of real data for large experimentation, the lack of control and customization over these data, and the lack of full knowledge about certain properties such as the true distributions of system effectiveness over populations of topics. We propose a method based on R-vine copulas that rids of these limitations and allows us to simulate realistic data about new and random topics, with full knowledge and control over the underlying properties. As an example, we replicated two typical experiments of IR evaluation research to show the benefits of our proposal. In the first experiment we obtained empirical results that reproduce and confirm the sequential testing problem, and in the second experiment we carry out the first empirical and direct assessment of Type I error rates in IR experimentation.
However, the proposed method can be used only in a certain class of evaluation problems. For example, we fit and assume a model for the distribution of scores produced by each measure, so research questions involving the comparison of distributional properties of the measures can not be answered. Similarly. our method simulates effectiveness directly, so research questions regarding pooling methods can not be studied with our method either. In both these cases we would need to simulate, not final effectiveness scores, but rather system runs and judgments; note that the content of documents and topics is not needed for this. This is an exciting topic we plan to study as well.
We have several other plans for further research in this line. First, we will implement other alternatives to model the margins, with special care of evaluation cutoffs that produce censored data. Second, we will study optimal structures for the vines to gain new simulation capabilities such as simulating a new random system for a given set of topics. Third, we will study the inclusion of a third factor other than systems and topics. A fixed third factor such as effectiveness measure seems straightforward to model just by adding new dimensions to the model, but a random factor such as assessor is more challenging. Last but not least, we plan on replicating previous evaluation research to confirm results with experiments free of the limitations discussed in this paper.
ACKNOWLEDGMENTS
This work was carried out on the Dutch national e-infrastructure with the support of SURF Cooperative. JU dedicates this work to the brave people of Namek.
REFERENCES
[1] K. Aas, C. Czado, A. Frigessi, and H. Bakken. 2009. Pair-copula Constructions of Multiple Dependence. Insurance: Mathematics and Economics 44, 2 (2009).
[2] H. Akaike. 1974. A new look at the statistical model identification. IEEE Trans. Automat. Control 19, 6 (1974), 716­723.
[3] L. Azzopardi, M. de Rijke, and K. Balog. 2007. Building simulated queries for known-item topics: an analysis using six european languages. In ACM SIGIR.

[4] L. Azzopardi, K. Järvelin, J. Kamps, and M.D. Smucker. 2010. Report on the SIGIR 2010 workshop on the simulation of interaction. SIGIR Forum 44, 2 (2010), 35­47.
[5] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A.P. de Vries, and E. Yilmaz. 2008. Relevance Assessment: Are Judges Exchangeable and Does it Matter?. In ACM SIGIR. 667­674.
[6] T. Bedford and R.M. Cooke. 2002. Vines -- a new graphical model for dependent random variables. The Annals of Statistics 30, 4 (2002), 1031­1068.
[7] E.C. Brechmann, C. Czado, and K. Aas. 2012. Truncated regular vines in high dimensions with application to financial data. Canadian Journal of Statistics 40, 1 (2012), 68­85.
[8] C. Buckley and E.M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In ACM SIGIR. 33­34.
[9] B. Carterette. 2012. Multiple Testing in Statistical Analysis of Systems-Based Information Retrieval Experiments. ACM TOIS 30, 1 (2012).
[10] B. Carterette. 2015. Bayesian Inference for Information Retrieval Evaluation. In ACM ICTIR. 31­40.
[11] B. Carterette. 2015. The Best Published Result is Random: Sequential Testing and Its Effect on Reported Effectiveness. In ACM SIGIR. 747­750.
[12] B. Carterette, V. Pavlu, E. Kanoulas, J.A. Aslam, and J. Allan. 2009. If I Had a Million Queries. In ECIR. 288­300.
[13] S.X. Chen. 1999. Beta kernel estimators for density functions. Computational Statistics & Data Analysis 31, 2 (1999), 131­145.
[14] M.D. Cooper. 1973. A simulation model of an information retrieval system. Information Storage and Retrieval 9, 1 (1973), 13­32.
[15] Gordon V. Cormack and Thomas R. Lynam. 2007. Validity and Power of t-test for Comparing MAP and GMAP. In ACM SIGIR. 753­754.
[16] J. Dissmann, E.C. Brechmann, C. Czado, and D. Kurowicka. 2013. Selecting and estimating regular vine copulae and application to financial returns. Computational Statistics & Data Analysis 59 (2013), 52­69.
[17] C. Forbes, M. Evans, N. Hastings, and B. Peacock. 2011. Statistical Distributions. Wiley.
[18] J. Friedman, T. Hastie, and R. Tibshirani. 2001. The elements of statistical learning. Springer.
[19] H. Joe. 2014. Dependence Modeling with Copulas. Chapman & Hall/CRC. [20] E. Kanoulas and J.A. Aslam. 2009. Empirical Justification of the Gain and Discount
Function for nDCG. In ACM CIKM. 611­620. [21] C. Loader. 2006. Local regression and likelihood. Springer. [22] S. Robertson and E. Kanoulas. 2012. On Per-Topic Variance in IR Evaluation. In
ACM SIGIR. 891­900. [23] Tetsuya Sakai. 2015. Topic Set Size Design. Information Retrieval Journal 19, 3
(2015), 256­283. [24] M. Sanderson, A. Turpin, Y. Zhang, and F. Scholer. 2012. Differences in Effective-
ness Across Sub-collections. In ACM CIKM. 1965­1969. [25] M. Sanderson and J. Zobel. 2005. Information Retrieval System Evaluation: Effort,
Sensitivity, and Reliability. In ACM SIGIR. 162­169. [26] G. Schwarz. 1978. Estimating the Dimension of a Model. The Annals of Statistics
6, 2 (1978), 461­464. [27] A. Sklar. 1959. Fonctions de Répartition à n Dimensions et Leurs Marges. [28] J. Tague, M. Nelson, and H. Wu. 1981. Problems in the Simulation of Bibliographic
Retrieval Systems. In ACM SIGIR. 236­255. [29] J. Tague-Sutcliffe. 1992. The Pragmatics of Information Retrieval Experimentation,
Revisited. Information Processing and Management 28, 4 (1992), 467­490. [30] J. Urbano. 2016. Test Collection Reliability: A Study of Bias and Robustness to
Statistical Assumptions via Stochastic Simulation. Information Retrieval Journal 19, 3 (2016), 313­350. [31] J. Urbano and M. Marrero. 2016. Toward estimating the rank correlation between the test collection results and the true system performance. In ACM SIGIR. 1033­ 1036. [32] J. Urbano, M. Marrero, and D. Martín. 2013. A Comparison of the Optimality of Statistical Significance Tests for Information Retrieval Evaluation. In ACM SIGIR. [33] E.M. Voorhees. 2009. Topic Set Size Redux. In ACM SIGIR. 806­807. [34] M.P. Wand and M.C. Jones. 1994. Multivariate plug-in bandwidth selection. Computational Statistics 9, 2 (1994), 97­116. [35] M.C. Wang and J.V. Ryzing. 1981. A Class of Smooth Estimators for Discrete Distributions. Biometrika 68, 1 (1981), 301­309. [36] W. Webber, M. Bagdouri, D.D. Lewis, and D.W. Oard. 2013. Sequential Testing in Classifier Evaluation Yields Biased Estimates of Effectiveness. In ACM SIGIR. 933­936. [37] W. Webber, A. Moffat, and J. Zobel. 2008. Score Standardization for Intercollection Comparison of Retrieval Systems. In ACM SIGIR. 51­58. [38] W. Webber, A. Moffat, and J. Zobel. 2008. Statistical Power in Retrieval Experimentation. In ACM CIKM. 571­580. [39] G.B. Wetherill and K.D. Glazebrook. 1986. Sequential Methods in Statistics. Chapman and Hill. [40] J. Zobel. 1998. How Reliable are the Results of Large-Scale Information Retrieval Experiments?. In ACM SIGIR. 307­314.

704

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Efficient Exploration of Gradient Space for Online Learning to Rank

Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, Hongning Wang
Department of Computer Science University of Virginia
Charlottesville, VA 22903, USA {hw7ww,rml5tu,sak2m,esm7ky,hw5x}@virginia.edu

ABSTRACT
Online learning to rank (OL2R) optimizes the utility of returned search results based on implicit feedback gathered directly from users. To improve the estimates, OL2R algorithms examine one or more exploratory gradient directions and update the current ranker if a proposed one is preferred by users via an interleaved test.
In this paper, we accelerate the online learning process by efficient exploration in the gradient space. Our algorithm, named as Null Space Gradient Descent, reduces the exploration space to only the null space of recent poorly performing gradients. This prevents the algorithm from repeatedly exploring directions that have been discouraged by the most recent interactions with users. To improve sensitivity of the resulting interleaved test, we selectively construct candidate rankers to maximize the chance that they can be differentiated by candidate ranking documents in the current query; and we use historically difficult queries to identify the best ranker when tie occurs in comparing the rankers. Extensive experimental comparisons with the state-of-the-art OL2R algorithms on several public benchmarks confirmed the effectiveness of our proposal algorithm, especially in its fast learning convergence and promising ranking quality at an early stage.
CCS CONCEPTS
· Information systems  Learning to rank; · Theory of computation  Online learning algorithms;
KEYWORDS
Online learning to rank; Dueling bandit; Null space exploration
ACM Reference Format: Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, Hongning Wang. 2018. Efficient Exploration of Gradient Space for Online Learning to Rank. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3209978. 3210045
1 INTRODUCTION
The goal of learning to rank is to optimize a parameterized ranking function such that documents that are more relevant to a user's
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210045

query are ranked at higher positions [16]. A trained ranker combines hundreds of ranking features to recognize the relevance quality of a document to a query, and shows several advantages over the manually crafted ranking algorithms [4]. Traditionally, such a ranker is optimized in an offline manner over a manually curated search corpus. This learning scheme, however, becomes a main obstacle hampering the application of learning to rank algorithms for a few reasons: 1) it is expensive and time-consuming to obtain reliable annotations in large-scale retrieval systems; 2) editors' annotations do not necessarily align with actual users' preferences [20]; and 3) it is difficult for an offline-trained model to reflect or capture ever-changing users' information needs in an online environment [21].
To overcome these limitations, recent research has focused on learning the rankers on the fly, by directly exploiting implicit feedback from users via their interactions with the system [5, 10, 27]. Fundamentally, online learning to rank (OL2R) algorithms operate in an iterative manner: in every iteration, the algorithm examines one or more exploratory directions, and updates the ranker if a proposed one is preferred by the users via an interleaved test [9, 23, 29, 30]. The essence of this type of OL2R algorithms is to estimate the gradient of an unknown objective function with low bias, such that online gradient descent can be used for optimization with low regret [6]. For example, one eventually finds a close to optimal ranker and seldom shows clearly bad results in the process. In the web search scenario, the objective function is usually considered to be the utility of search results, which can be depicted by ordinal comparisons in user feedback, such as clicks [20]. However, to maintain an unbiased estimation of the gradient, uniform sampling of random vectors in the entire parameter space is performed in these algorithms. As a result, the newly proposed exploratory rankers are independent from not only the past interactions with users, but also the current query being served. This inevitably leads to slow convergence and large variance of ranking quality during the online learning process.
Several lines of works have been proposed to improve the algorithms' online learning efficiency. Hofmann et al. [9] suggested to reduce the step size in gradient descent for better empirical performance. In their follow-up work [8], historical interactions were collected to supplement the interleaved test in the current query and pre-select the candidate rankers. Schuth et al. [23] proposed to explore multiple gradient directions in one multi-interleaved test [24] so as to reduce the number of comparisons needed to evaluate the rankers. Zhao et al. [30] introduced the idea of using two uniformly sampled random vectors with opposite directions as the exploratory directions, with the hope that when they are not orthogonal to the optimal gradient, one of them should be a more effective direction than a simplely uniformly sampled direction. They also developed a contextual interleaving method, which considers

145

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

historical explorations when interleaving the proposed rankers for comparison, to reduce the noise from multi-interleaving.
Nevertheless, all aforementioned solutions still uniformly sample from the entire parameter space for gradient exploration. This results in independent and isolated rankers for comparison. Therefore, less promising directions might be repeatedly tested, as historical interactions are largely ignored when proposing the new rankers. More seriously, as the exploratory rankers are independently proposed for the current query, they might give the same ranking order of the candidate documents for interleaving (this happens when the difference in the feature weight vectors between two rankers are orthogonal to the feature vectors in those candidate documents). In this scenario, no click feedback can differentiate the ranking quality of those rankers in this query. When the interleaved test cannot recognize the best ranker from ordinal comparison in a query, tie will be arbitrarily broken [23, 29]. This again leads to large variance and slow convergence of ranking quality in these types of algorithms.
We propose improving the learning convergence of OL2R algorithms by carefully exploring the gradient space. First, instead of uniformly sampling from the entire parameter space for gradient estimation, we maintain a collection of recently explored gradients that performed relatively poorly in their interleaved tests. We sample proposal directions from the null space of these gradients to avoid repeatedly exploring poorly performing directions. Second, we use the candidate ranking documents associated with the current query to preselect the proposed rankers, with a focus on those that give different ranking orders over the documents. This ensures that the resulting interleaved test will have a better chance of recognizing the difference between those rankers. Third, when an interleaved test fails to recognize the best ranker for a query, e.g., two or more rankers tie, we compare the tied rankers on the most recent worst performing queries (i.e., the difficult queries) with the recorded clicks to differentiate their ranking quality. We name the resulting algorithm Null Space Gradient Descent, or NSGD for short, and extensively compare it with four state-of-the-art algorithms on five public benchmarks. The results confirm greatly improved learning efficiency in NSGD, with a remarkably fast and stable convergence rate at the early stage of the interactive learning process. This means systems equipped with NSGD can provide users with better search results much earlier, which is crucial for any interactive system.
2 RELATED WORK
Online learning to rank has recently attracted increasing attention in the information retrieval community, as it eliminates the heavy dependency on manual relevance judgments for model training and directly estimates the utility of search results from user feedback on the fly. Various algorithms have been proposed, and they can be categorized into two main branches, depending on whether they estimate the utility of individual documents directly [19] or via a parameterized function over the ranking features [29].
The first branch learns the best ranked list for each individual query by modeling user clicks using multi-armed bandit algorithms [1, 2]. Ranked bandits are studied in [19], where a k-armed bandit model is placed on each ranking position of a fixed input query to estimate the utility of candidate documents being in that position. The system's learning is accelerated by assuming similar documents have similar utility for the same query [25]. By assuming

that skipped documents are less attractive than later clicked ones, Kveton et al. [13] develop a cascading bandit model to learn from both positive and negative feedback. To enable learning from multiple clicks in the same result ranking list, they adopt the dependent click model [7] to infer user satisfaction after a sequence of clicks [12], and later further extend to broader types of click models [31]. However, such algorithms estimate the utility of ranked documents on a per-query basis, and no estimation is shared across queries. This causes them to suffer from slow convergence, making them less practical.
Another branch of study leverages ranking features and look for the best ranker in the entire parametric space. Our work falls into this category. The most representative work in this line is dueling bandit gradient descent (DBGD) [28, 29], where the algorithm proposes an exploratory direction in each iteration of interaction and uses an interleaved test to validate the exploration for model updating. As only one exploratory direction is compared in each iteration of DBGD, its learning efficiency is limited. Different solutions have been proposed to address this limitation. Schuth et al. [23] propose the Multileave Gradient Descent algorithm to explore multiple directions in each iteration. To evaluate multiple candidate rankers at once, multi-interleaving comparison [24] is used. Zhao et al. [30] propose the Dual-Point Dueling Bandit Gradient Descent algorithm to sample two stochastic vectors with opposite directions as the candidate gradients. When they are not orthogonal to the optimal gradient, one of the two should be a more effective gradient than a single proposal. However, all of the aforementioned algorithms uniformly sample the exploratory directions from the entire parameter space, which is usually very high-dimensional. More importantly, the uniform sampling makes the proposed rankers independent from past interactions, and thus they cannot avoid repeatedly exploring less promising directions.
Some works have recognized this deficiency and proposed different solutions. Hofmann et al. [8] record historical interactions during the learning process to supplement the interleaved test when comparing the rankers. They also suggest using historical data to preselect the proposed rankers before interleaved test. However, only the most recent interactions are collected in these two solutions, so that they are not necessarily effective in recognizing the quality of different rankers. Oosterhuis et al. [18] create the exploratory directions via a weighted combination over a set of preselected reference documents from an offline training corpus. The reference documents are either uniformly sampled or are the clustering centroids of the corpus. However, the reference documents are fixed beforehand; this limits the quality of learnt rankers, if the offline corpus has a different feature distribution than the incoming online documents. More importantly, none of these solutions consider the feature distribution in the candidate ranking documents of a particular query when proposing exploratory rankers. It is possible that the proposed rankers are not differentiable by any click pattern for a given query, e.g., they rank the documents in the same order. When the best rankers are tied, the winner is arbitrarily chosen. This further slows down the online learning process. In our solution, we preselect the rankers that tend to provide different ranking lists in the current query, so that the resulting interleaved test will have a better chance to tell the difference among those rankers. When a tie occurs, we use the most recent difficult queries to further evaluate the rankers, as those queries are expected to be more discriminative.

146

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

3 METHOD
We improve the learning convergence of OL2R algorithms by carefully exploring the gradient space. In particular, we aim to avoid repeatedly exploring recent poorly performing directions and focus on the rankers that can be best differentiated by the candidate ranking documents associated with the current query. We first give an overview of a basic online learning to rank algorithm, Dueling Bandit Gradient Descent [29], based on which we describe our proposed solution, Null Space Gradient Descent, in details.
3.1 Overview of Dueling Bandit Gradient Descent
Dueling bandit gradient descent (DBGD) [29] is an OL2R algorithm that learns from interleaved comparisons between one exploratory ranker and one current ranker. Each ranker is represented as a feature weight vector w  Rd , and it ranks documents by taking the inner product with their associated ranking features, i.e., a linear ranking model. As shown in Algorithm 1, at the beginning of iteration t, the algorithm receives a a query and its associated candidate ranking documents, represented as a set of query-document pairs Xt = {x1, x2, ..., xs }. We denote wt0 as the weight vector of the current ranker. DBGD proposes an exploratory direction ut uniformly from the unit sphere, and generates a candidate ranker wt1 = wt0 + ut , where  is the step size of exploration. Two ranked lists generated by these two rankers, i.e., l(Xt , wt0) and l(Xt , wt1), are then combined via an interleaving method, such as Team Draft Interleaving [20]. The resultant list is returned to the user for feedback. Based on the feedback and specific interleaving method, a better ranker is determined. If the exploratory ranker wins, the current ranker gets updated with wt0+1 = wt0 + ut , where  is the learning rate; otherwise the current ranker stays intact. Such exploration and comparison lead to a low bias estimation of gradient in terms of expectation [6], i.e., f^(w) = E[f (w + u)u]d/ , in which f (w) is the target utility function to be estimated. This estimation does not require the function f (w) to be differentiable nor even to be explicitly defined; and thus it is the theoretical basis of this family of OL2R algorithms.
However, only one exploration direction ut is proposed for comparison in each iteration, which limits the learning rate of DBGD. To address this limitation, Schuth et al. [24] proposed the Multileaving Gradient Descent algorithm that uniformly explores m directions at the same time, i.e., wt0 + uti where i  {1..m}. Zhao et al. [30] proposed the Dual-Point Dueling Bandit Gradient Descent algorithm to explore two opposite directions each time, i.e., wt0 + ut and wt0 -ut . Although exploring multiple candidates generally improves the learning rate, the expected improvement is still marginal, as the ranking features usually reside in a high dimensional space and uniform sampling is very inefficient. More importantly, the uniform sampling makes the proposed rankers independent from historical interactions and the current query context. Algorithms with history-independent exploration cannot avoid repeatedly exploring less promising directions that have been discouraged by the most recent user feedback. Additionally, context-independent exploration cannot avoid the issue of multiple rankers generating indifferentiable ranking results, such as by ranking the documents in the same order. Both of them further hamper the convergence rate of aforementioned OL2R algorithms.

Algorithm 1 Dueling Bandit Gradient Descent (DBGD) [29]

1: Inputs:  ,  2: Initiate w10 = sample_unit_vector() 3: for t = 1 to T do

4: Receive query Xt = {x1, x2, ..., xs }

5: ut = sample_unit_vector()

6: wt1 = wt0 + ut

7: Generate ranked lists l(Xt , wt0) and l(Xt , wt1) 8: Set L =Interleave l(Xt , wt0), l(Xt , wt1) and present L to user

9: Receive click positions Ct on L, and infer click credit ct0 and ct1 for wt0 and wt1 accordingly

10: if ct1 > ct0 then

11:

wt0+1 = wt0 + ut

12: else

13:

wt0+1 = wt0

14: end if

15: end for

3.2 Null Space Gradient Descent

Our proposed Null Space Gradient Descent (NSGD) algorithm im-

proves over DBGD-type OL2R algorithms by a suite of carefully

designed exploration strategies in the gradient space.

We illustrate the procedure of NSGD in Figure 1. First, to avoid

uniformly testing exploratory directions in the entire parameter

space, we maintain a collection of most recently explored gradients

that performed poorly in their interleaved tests, and sample new

proposal directions from the null space of these gradients. As a

result, we only search in a subspace that is orthogonal to those

less promising directions. This can be intuitively understood from

Figure 1 part 1: since the interleaved tests in iteration t - 2 and t - 1

unveil the ineffectiveness of the directions marked in red, NSGD

prevents the current ranker wt from exploring these less promising

directions again by exploring the null space of them at iteration

t. Second, we prefer the proposed rankers that tend to generate

the most distinct ranking orders from the current ranker for the

current query, so that the resulting interleaved test will have a better

chance of recognizing the best ranker among those being compared.

We show such an example in Figure 1 part 2, where the current

ranker wt0 and a randomly sampled candidate ranker wt1 rank the candidate documents in the same order. As a result, no interleaved

test can differentiate their ranking quality in this query. NSGD

avoids proposing wt1, and favors wt2 as it ranks the documents in a reverse order and would therefore give the interleaved test a better

chance of recognizing the difference between wt0 and wt2. Third, if an interleaved test fails to recognize the best ranker in a query,

e.g., a tie is encountered as shown in Figure 1 part 3, we compare

the tied rankers on the most recent worst performing queries (i.e.,

the difficult queries) with the recorded clicks to differentiate the rankers. Eventually, NSGD aims to reach w with a minimal number

of interactions as shown in Figure 1, i.e., faster convergence. The

detailed procedures in NSGD are shown in Algorithm 2, and we

will discuss the key steps of it in the following.

· Null Space Gradient Exploration. NSGD maintains a fixed size

queue Q of recently explored directions and their corresponding

quality, i.e., Q =

(i , qi )

T i =1

,

which

is

constructed

in

line

32

to 37 in Algorithm 2. We denote the quality qi of an explored

147

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Null space exploration to avoid repeatedly

w t-1

1

exploring less promising directions.

w

w

w*

t-2

t

w w

w2 d t
d d

w0 t

d

d

w1 d
td
d

c 2 =c 0 tt
X X X ... t-1 t-3 t-5

Preselect differentiable candidate by
2
current documents with associated query.

w 2 >w 0

t

t

Tie breaking function using historical difficult queries
3
when multiple winners occur.

Figure 1: Illustration of model update procedure for the Null Space Gradient Descent algorithm.

direction i as the received click credit difference between the
corresponding exploratory ranker and the default ranker by then (i.e., line 33). Intuitively, qi measures the improvement in ranking quality contributed by the update direction i ; when qi is negative, it suggests the direction i cannot improve the current ranker, and
therefore should be discouraged in future. To realize this, after receiving a user query, NSGD first constructs G = [1, ..., k ] by selecting the top k worst performing historical directions from Q (i.e., line 8), and then solves for the null space of G denoted as G = NullSpace(G) (i.e., line 9). The new exploratory directions are sampled from G (i.e., line 15). Because every vector in the space of G is orthogonal to all k selected historical directions, those ineffective directions (and any linear combination of them)
will not be tested in this query.
Our null space exploration strategy is based on two mild assump-
tions: queries are independent and identically distributed (i.i.d.),
and the gradient of the target (unknown) utility function satisfies Lipschitz continuity, i.e., ||f^(w1)-f^(w2)||   ||w1 -w2||, where  is a Lipschitz constant for the target utility function f (w). The
assumption that queries are i.i.d. is studied and widely adopted in
existing learning to rank research [14, 15]. This assumption allows
NSGD to compare gradient performance across queries and select k worst performing gradients from previous queries. Lipschitz continuity assumption suggests similar rankers would share similar
gradient fields for the same query. This assumption is mild and
consistent with most of existing learning to rank algorithms [3, 16].
However, this assumption requires us to construct the null space
from all historically explored directions whose associated rankers have a similar weight vector w as the current ranker's. This is
clearly infeasible in an online learning setting, as we would have to
store the entire updating history and examine it in every iteration. In NSGD, because the learning rate  is set to be small, rankers with
close temporal proximity will have similar feature weight vectors,
and therefore share a similar gradient field. Hence, NSGD only maintains the most recently tested directions in Q, which approximates the Lipschitz continuity. In our empirical evaluation, we
also tested the exhaustive solution, but aside from the significantly
increased storage and time complexity, little ranking performance
improvement was observed. This supports our construction of the
null space in NSGD.

Another benefit of sampling from null space is that the search takes place in a reduced problem space. DBGD-type algorithms have to sample in the whole d-dimensional space, while NSGD only samples from a subspace of it, whose rank is at most d - k, when the top k worst performing historical gradients are orthogonal to each other. This advantage is especially appealing when the dimension of ranking features is high, which is usually the case in practical learning to rank applications [4, 16].
There are two ways to sample from the null space G in NSGD (i.e., line 11): uniformly selecting the basis vectors of the null space or sampling random unit vectors inside the null space. Randomly selecting the basis vectors can maximize the coverage of sampled directions in the null space, as the basis vectors are linearly independent from each other. It improves the exploration efficiency in an early stage. Zhao et al. tested a similar idea in [30], but they performed it over the entire parameter space. However, in the later stage of model update, the true gradients are usually concentrating in a specific region; continuing to select those independent basis vectors becomes less effective. Exploring linear combinations of those basis vectors, i.e., uniformly sampling inside the space, emerges as a better choice then. But directly sampling from the null space at the beginning might be less effective, as it tends to introduce smaller variance in proposing different directions.
To take advantage of these two sampling schemes, we propose a hybrid sampling method in the null space: comparing with the winning ranker wt0-k created in iteration t - k, if ||wt0 - wt0-k || < 1 - , we switch to sample random unit vectors in G; otherwise uniformly select the basis vectors of G. The intuition behind this switching control is that when the consecutive rankers become similar, it indicates the gradients have converged to a local optimal region, and a refined search is needed to identify the true gradient. Otherwise, the gradient direction has not been identified, and larger diversity is needed to accelerate the exploration of null space. Oosterhuis and Rijke [18] also proposed a similar idea to detect model convergence and convert to more complex models when a simpler model has converged. But their conversion might not always be feasible, e.g., when no linear mapping between models exists; we only switch the sampling schemes for exploratory directions, which has no additional assumption about the model space. · Context-Dependent Ranker Preselection. NSGD selectively constructs the candidate rankers to maximize the chance that they

148

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Algorithm 2 Null Space Gradient Descent (NSGD)

1: Inputs:  , , n, m, k, kh,T,Th
2: Initiate w10 = sample_unit_vector() 3: Set Q = queue(T) and Qh = queue(Th ) as fixed size queues 4: for t = 1 to T do

5: Receive query Xt = {x1, x2, ..., xs }

6: 7:

Generate ranked

x¯t =

s i =1

xi

list

l

(Xt

,

wt0)

8: Construct G = [1, ..., k ] by directions selected from Q

with the worst recorded quality q

9: G = NullSpace(G)

10: for i = 1 to n do

11:

ti = sample_unit_vector(G)

12: end for

13: Select top m gradients that maximize x¯tTti from {ti }in=1

14: for i = 1 to m do

15:

wti = wt0 + ti

16:

Generate ranked list l(Xt , wti )

17: end for

18: Set Lt = Multileave {l(Xt , wti )}im=0 , and present Lt to user

19: Receive click positions Ct on Lt , and infer click credits

{cti }im=0 for all rankers 20: Infer winner set Bt from {cti }im=0

21: if |Bt | > 1 then

22:

Select kh worst performing queries

(Xi , Li , Ci )

kh i =1

from Qh by Eval(Li , Ci ).

23:

j = arg maxo Bt

kh i =1

Eval(l

(X

i

,

wo

),

Ci

)

24: else

25:

Set j to the sole winner in Bt

26: end if

27: if j = 0 then

28:

wt0+1 = wt0

29: else

30:

wt0+1 = wt0 + tj

31: end if

32: for i = 1 to m do

33:

qti = cti - ct0

34:

if qti < 0 then

35:

Append (ti , qti ) to Q

36:

end if

37: end for

38: Append (Xt , Lt , Ct ) to Qh

39: end for

can be differentiated from the current best ranker wt in the interleaved tests. A straightforward solution is to select the rankers which give totally distinct ranking orders to that from wt0. But this clearly emphasizes too much the exploration of new directions, but ignores the exploitation of current best ranker. Especially in the later stage of model update when the current ranker can already provide satisfactory ranking results, a very distinct ranking indicates a higher risk of providing worse result quality.
To balance the needs for exploration and exploitation, we propose a Context-Dependent Preselection (CDP) criterion as shown in line 13 of Algorithm 2: after randomly sampling n vectors from G, we select the top m of them that maximize the inner product with the aggregate document feature vector x¯ for query Xt .

This can be understood as a necessary condition for having a proposed ranker that generates a different ranked list in Xt than that from wt0. More specifically, as we are learning a linear ranker, the ranking score of each document is computed by the inner product between document feature vector xi and the feature weight vector wt0; and the ranking scores lead to the ranked list l(Xt , wt0). To generate a different ranked list, there has to be at least one document that has different ranking scores under these two rankers, i.e., j, |xjT(wti - wt0)| > 0. This can be simplified as j, |xjTti | > 0; and by the triangle inequality (i.e., |a| + |b |  |a + b |), we require a differentiable ranker to satisfy | j xjTti | > 0. To choose the candidate rankers that can best satisfy this condition, we select the top m proposal directions that maximize this inner product. · History-Dependent Tie Breaking. NSGD is flexible in selecting the number of rankers for comparison: the hyper-parameter m in line 13 is an input to the algorithm. If multiple rankers are selected for comparison, multi-interleaving [24] can be performed to compare the quality of the proposed rankers, i.e., infer the click credit cti for each ranker wti and determine the winning ranker (i.e., line 19 and 20). However, because of position bias in user clicks [11], very few result documents will be clicked each time. The sparsity in result clicks directly reduces the resolution of interleaved test in recognizing the winning ranker, e.g., multiple rankers might share the same aggregate click credit. The situation becomes even worse when multiple rankers are compared. Existing solutions break the tie arbitrarily [29, 30] or heuristically take the mean vector of rankers in the winner set [23]. No solutions consider the ranking problem at hand, and they are not effective in general.
We propose the idea of leveraging historical queries, especially the most difficult ones, to choose the winner whenever a tie happens. First, in line 38, the 3-tuple comprised of the historical query, its displayed ranking list, and its corresponding click positions are stored in a fixed size queue. In future iterations, they are selected in line 22 to identify the best ranker, whenever a tie happens. Because only click feedback is available in online learning, we use click position Ct in the evaluation function Eval(Lt , Ct ), such as MAP or NDCG by treating clicked documents as relevant, to measure the ranking quality of Lt in query Xt (i.e., line 22 and 23). More importantly, because the ranker is improving on the fly, a poorly served query might be caused by a badly performing ranker, rather than its intrinsic difficulty. Therefore, in NSDG we only collect recent click results to select the most discriminative queries.
4 EXPERIMENTS
In this section, we perform extensive empricial comparisons between our proposed Null Space Gradient Descent (NSGD) algorithm with four state-of-the-art OL2R algorithms on five public learning to rank benchmarks. Both quantitative and qualitative evaluations are performed to examine our proposed gradient space exploration strategies, especially their advantages over the existing solutions in improving online learning efficiency.
4.1 Experiment Setup
· Datasets. We used five benchmark datasets which are part of the LETOR 3.0 and LETOR 4.0 collections [17]: MQ2007, MQ2008, TD2003, NP2003 and HP2003. Among them, NP2003 and HP2003 implement navigational tasks, such as homepage finding and namedpage finding; TD2003 implements topic distillation, which is an informational task; MQ2007 and MQ2008 mix both types of tasks.

149

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Perfect

(b) Navigational

(c) Informational

Figure 2: Offline NDCG@10 on MQ2007 dataset under three click models.

(a) Perfect

(b) Navigational

(c) Informational

Figure 3: Standard deviation of offline NDCG@10 on MQ2007 dataset under three click models.

(a) Perfect

(b) Navigational

(c) Informational

Figure 4: Discounted cumulative NDCG@10 on MQ2007 dataset under three click models.

Documents in TD2003, NP2003 and HP2003 datasets are collected from the .GOV collection, which is crawled from the .gov domain; while the MQ2007 and MQ2008 datasets are collected from 2007 and 2008 Million Query track at TREC [26]. In these datasets, each query-document pair is encoded as a vector of ranking features, including PageRank, TF.IDF, BM25, and language model on different parts of a document. The number of features is 46 in MQ2007 and MQ2008, and 64 for the other three datasets. In the MQ2007 and MQ2008 datasets, every document is marked with a relevance label between 0 and 2, while the other datasets only have binary labels. The MQ2007 and MQ2008 datasets contain 1,700 and 1,800 queries respectively, but with fewer assessments per query; while each of the other three datasets only contain fewer than 150 queries but with 1,000 assessments per query. All of the datasets are split into 5 folds for cross validation. We take the training set for online experiments gathering cumulative performance, and use testing set for offline evaluation. · Simulating User Clicks. To make our reported results comparable to existing literature, we follow the standard offline evaluation scheme proposed in Lerot [22], which simulates user interactions

with an OL2R algorithm. We make use of the Cascade Click Model [7] to simulate user click behavior. The click model simulates user interaction with the system by assuming that as a user scans through the list he/she makes a decision about whether or not to click on a returned document. The probability of a user clicks on a document is conditioned on the relevance label. Likewise, after clicking, the user makes a decision about continuing to look through the documents or to stop. The probability of this decision is also conditioned on the current document's relevance label. Adjusting these probabilities allows us to simulate different types of users.
We use three click model configurations as shown in Table 1, including: 1) perfect user who clicks on all relevant documents and does not stop browsing, which contributes the least noise; 2) navigational user who would stop early once they've found a relevant document; and 3) informational user who sometimes clicks on irrelevant documents in their search for information, which contributes the most noise. The length of resulting list evaluated by the click models is set to 10 as a standard setting in [23, 30]. · Evaluation Metrics. To evaluate an OL2R algorithm, cumulative Normalized Discounted Cumulative Gain (NDCG) and offline

150

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Configurations of simulation click models.

Relevance grade Perfect
Navigational Informational

Click Probability 01 2 0.0 0.5 1.0
0.05 0.5 0.95 0.4 0.7 0.9

Stop Probability 01 2 0.0 0.0 0.0 0.2 0.5 0.9 0.1 0.3 0.5

NDCG are commonly used to assess the learning rate and ranking quality of the algorithm [22]. Cumulative NDCG is calculated with a discount factor of  set to 0.995 for each iteration. To assess model estimation convergence, in each iteration we measure cosine similarity between the weight vector updated by an OL2R algorithm and a reference weight vector, which is estimated by an offline learning to rank algorithm trained on the manual relevance judgments. In our experiment, we used LambdaRank [3] with no hidden layer to obtain such a reference ranker in each dataset, because of its superior empirical performance. For all experiments, we fix the total number of iterations T to 1,000 and randomly sample query Xt from the dataset with replacement accordingly. · Evaluation Questions. We intend to answer the following questions through empirical evaluations, to better understand the advantages of our proposed algorithm.
Q1: How does our proposed NSGD algorithm perform in comparison to various baseline OL2R methods?
Q2: Do candidate directions generated by NSGD explore the gradient space more efficiently than uniform sampling from the entire parameter space?
Q3: How do the different components in NSGD contribute to its final performance?
Q4: How do different settings of hyper-parameters alter the performance of NSGD?
· Baselines. We chose the following four state-of-the-art OL2R algorithms as our baselines for comparison:
- DBGD [29]: A single direction uniformly sampled from the entire parameter space is explored. Team Draft is used to interleave the results of the two rankers for comparison.
- CPS [9]: It proposes a candidate preselection strategy that uses historical data to preselect the proposed rankers before the interleaved test in DBGD.
- DP-DBGD [30]: Two opposite uniformly sampled directions are explored in DBGD. Both Contextual Interleave, which favors the winning direction from the previous iteration, and Team Draft are used in it in our experiment.
- MGD [23]: Multiple uniformly sampled directions are explored in single iteration. Multileave is used to interleave the results. If there is a tie, the model updates towards the mean of all winners.

4.2 Online and offline performance of NSGD
We start with our first evaluation question: how does NSGD perform in comparison with baseline OL2R methods? We run all OL2R algorithms over all 5 datasets and all 3 click models. According to the standard hyper-parameter settings of DBGD [29] and other baselines, we set  to 1 and  to 0.1. For algorithms that can explore multiple candidates, including MGD and NSGD, we set number of candidates explored in one iteration to 4, (i.e., m = 4 in NSGD). For NSGD, we set k = 25, T = 15, kh = 10, and Th = 50. We will discuss the effect of these different hyper-parameters on NSGD in

Section ??. All experiments are repeated 15 times for each fold, and we report the average performance.
Figures 2 and 4 report the offline and online performance of all OL2R methods on MQ2007 dataset under perfect, navigational and informational click models. We also report the standard deviation of offline NDCG in every iteration of model update on this dataset in Figure 3. Due to the space limit, we cannot report the detailed performance over other datasets, but we summarize the final performance in Table 2 and 3 respectively. From Figure 4, we observe that CPS and NSGD, which both apply candidate preselection, perform better than other methods in terms of cumulative NDCG. This confirms that exploring carefully selected candidate directions generally improves the learning speed in the early iterations compared with the uniform sampling strategy used in other baselines. Our proposed NSGD further improves online learning efficiency over CPS by exploring inside the null space rather than the entire parameter space. From Table 2, we can observe the consistent improvement of NSGD for most of the datasets and click models, which proves the accelerated learning speed by performing more efficient gradient exploration during its online learning process.
In Figure 2 we first observe that NSGD improves offline NDCG significantly faster than other baselines, which generally require much more interactions with users to reach the same performance. This further explains our above analysis of the improved learning speed of NSGD shown in Figure 4. For informational users, MGD requires more than 800 iterations to reach performance comparable to NSGD at less than 200 iterations. From Table 3 we observe that algorithms that explore multiple candidate directions in one iteration, including MGD and NSGD, consistently achieve better offline performance than other methods on all 5 datasets and 3 click models. Compared with MGD, NSGD further improves the final offline NDCG on MQ2007, MQ2008 and NP2003 datasets, especially for the informational users. We have discussed in Section 4.1 that MQ2007 and MQ2008 contain more queries with fewer assessments per query. This improvement suggests that NSGD can better identify the most effective exploration directions even under a noisy environment. We have also tested MGD with 9 candidates explored in one iteration (i.e., m = 9) which has achieves best performance according to [23], and observed same consistent improvement of NSGD over MGD with 9 candidates in online performance. Due to space limit we did not report the performance of MGD with 9 candidates in Table2 and 3.
Figure 3 shows the standard deviation of offline NDCG at each iteration. We observe that both NSGD and MGD enjoy a much smaller standard deviation in the perfect and navigational users, suggesting that exploring multiple directions reduce the variance introduced by random exploration. Another reason for the reduced variance in NSGD is the hybrid sampling method mentioned in Section 3.2: the result confirms that first sampling from the basis vectors of null space and then sampling inside the null space provides a more effective exploration, which not only improves learning efficiency but also effectively reduces variance in an early stage. For informational users, who have a lower stop probability and are likely to generate more clicks, they typically contribute nosier clicks and more ties in the comparison. In this case, NSGD reaches a much smaller standard deviation compared with MGD and all other baselines. The reason is that NSGD applies context-dependent candidate preselection to propose the most differentiable directions and use most difficult queries to discern tied candidates. Although

151

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Online score (discounted cumulative NDCG@10) and standard deviation of each algorithm after 1000 queries under each of the three click models. Statistically significant improvements over MGD baseline are indicated by  (p<0.05).

Click Model Perfect
Navigational Informational

Dataset MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003

DBGD 61.931 (5.535) 81.327 (6.224) 110.012 (8.627) 101.004 (8.702) 39.856 (7.770) 57.989 (4.657) 76.411 (5.983) 95.775 (14.394) 84.699 (12.275) 33.954 (8.368) 55.427 (5.639) 73.941 (6.101) 59.376 (23.637) 56.996 (20.547) 23.021 (8.675)

CPS 59.936 (4.875) 77.694 (6.137) 109.279 (8.565) 98.774 (8.884) 38.054 (6.999) 59.669 (4.911) 75.603 (7.230) 95.925 (12.628) 88.240 (13.039) 35.857 (8.729) 57.094 (5.689) 74.825 (5.419) 56.004 (22.101) 54.615 (19.354) 23.826 (7.964)

DP-DBGD 58.995 (4.926) 76.192 (6.452) 92.422 (11.358) 79.636 (13.338) 34.289 (7.703) 57.301 (4.816) 74.984 (5.959) 88.773 (11.518) 74.521 (14.810) 31.468 (7.322) 55.619 (5.066) 72.392 (6.259) 66.295 (16.782) 62.067 (17.667) 24.948 (6.848)

MGD 59.765 (3.015) 77.543 (4.827) 101.675 (4.943) 104.677 (5.399) 38.380 (5.383) 57.884 (3.266) 75.001 (5.085) 82.244 (26.944) 100.581 (8.962) 36.092 (5.616) 55.338 (3.395) 72.757 (4.690) 75.314 (11.281) 74.497 (13.249) 28.482 (5.299)

NSGD 68.639 (3.311)  88.811 (6.022)  113.890 (8.276)  115.145 (6.287)  42.402 (7.654) 66.635 (2.832)  84.091 (4.553)  109.783 (5.634)  109.433 (5.649)  41.274 (7.318)  67.312 (3.438)  84.053 (4.980)  108.592 (5.503)  108.624 (5.831)  39.386 (7.148) 

Table 3: Offline score (NDCG@10) and standard deviation of each algorithm after 1000 queries under each of the three click models. Statistically significant improvements over MGD baseline are indicated by  (p<0.05).

Click Model Perfect
Navigational Informational

Dataset MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003 MQ2007 MQ2008 HP2003 NP2003 TD2003

DBGD 0.369 (0.030) 0.465 (0.042) 0.760 (0.067) 0.704 (0.052) 0.267 (0.082) 0.359 (0.034) 0.459 (0.038) 0.728 (0.063) 0.709 (0.035) 0.276 (0.095) 0.319 (0.047) 0.425 (0.050) 0.500 (0.196) 0.526 (0.190) 0.174 (0.099)

CPS 0.383 (0.026) 0.474 (0.042) 0.764 (0.068) 0.702 (0.050) 0.296 (0.094) 0.365 (0.037) 0.456 (0.037) 0.734 (0.072) 0.661 (0.066) 0.285 (0.093) 0.325 (0.049) 0.434 (0.047) 0.463 (0.191) 0.443 (0.179) 0.178 (0.092)

DP-DBGD 0.361 (0.032) 0.461 (0.041) 0.762 (0.062) 0.682 (0.062) 0.286 (0.091) 0.339 (0.031) 0.445 (0.045) 0.752 (0.061) 0.675 (0.061) 0.269 (0.087) 0.325 (0.037) 0.422 (0.054) 0.669 (0.103) 0.657 (0.118) 0.219 (0.094)

MGD 0.408 (0.018) 0.487 (0.037) 0.771 (0.062) 0.712 (0.048) 0.308 (0.096) 0.393 (0.024) 0.477 (0.036) 0.707 (0.156) 0.707 (0.052) 0.303 (0.098) 0.355 (0.036) 0.450 (0.041) 0.736 (0.063) 0.660 (0.059) 0.271 (0.090)

NSGD 0.411 (0.019) 0.488 (0.043) 0.752 (0.752) 0.714 (0.049) 0.289 (0.092) 0.398 (0.022) 0.478 (0.037) 0.744 (0.073) 0.710 (0.039) 0.274 (0.094) 0.383 (0.020)  0.472 (0.036) 0.713 (0.069) 0.707 (0.044)  0.251 (0.085)

CPS also uses historical interactions to preselect the rankers, it uniformly selects historical interactions, which are not necessarily informative. As a result, the ranking quality of CPS oscillates when the fidelity of user feedback is low.
4.3 Zooming into NSGD
To answer the second and third evaluation questions, we design detailed ablation studies to carefully study NSGD. All the experiments in this section were conducted on MQ2007 under the informational click model, as the dataset has the largest amount of queries and the click model makes the retrieval task the most challenging.
In the first experiment, we trained an offline LambdaRank model [3] without any hidden layer using manual relevance labels. The model obtained the best offline NDCG performance in this dataset (around 0.437 in average). Its model parameter is denoted as w. We compare cosine similarity between the weight vector estimated by NSGD and w, to that between the weight vectors generated by MGD and DBGD and w in each iteration. In Figure 5 (a) we can observe that NSGD moves towards w much faster than both MGD and DBGD, which suggests the update directions explored by NSGD are more effective in recognizing the important ranking features.

However, note that the final converged model in NSGD is not identical to w, and the final offline NDCG of all OL2R algorithms is worse than LambdaRank's. This is expected: LambdaRank is directly trained by manual labels. To improve an online trained model, one possible solution is to pre-train its weight vector with some offline data, and continue training it with online user feedback. This will take advantage of both training schemes.
The second experiment serves to study the utility of gradients proposed by NSGD. We mix uniform exploratory directions from the entire parameter space with directions proposed from null space in the same algorithm. Specifically, we have in total 4 candidate rankers for multileaving, in which we vary the number of candidates created by null space gradients from 4 to 0, and we report the selection ratio, i.e., the frequency of selecting null space proposed rankers over the current ranker, versus the frequency of selecting uniformly proposed rankers over the current ranker. This ratio is also normalized by the number of proposed rankers in each type, to make the results comparable. We also report the online performance of each combination to understand the consequence of selecting different types of rankers. The result is shown in Figure 5 (b). We can clearly observe that comparing with the uniform exploratory

152

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Cosine similarity between online learnt (b) Selection ratio comparing null space and

model and offline best model w

uniform gradients

(c) Ablation analysis of NSGD

Figure 5: Detailed experimental analysis of NSGD on MQ2007 dataset.

rankers, rankers proposed by NSGD are always more likely to be selected as a better ranker in all combinations. We see that with more candidates proposed by NSGD, the online performance also increases. These results clearly show the superior quality of directions explored by NSGD, and explains the performance improvement observed comparing with other baselines that uniformly sample directions to explore.
To better understand the contribution of different components in NSGD, we disable them in turn and experiment on the resulting variants of NSGD. Specifically, we compare the following four models: 1) NSGD; 2) NSGD without tie breaking, denoted as NSGD w/o (TB); 3) NSGD without tie breaking and context-dependent preselection, denoted as NSGD w/o (CDP & TB); and 4) MGD. The result is reported in Figure 5(c). Comparing NSGD w/o (CDP & TB) with MGD, where the difference is exploring in the null space or entire parameter space, confirms the utility of null space gradient exploration, which avoids repeatedly exploring recent and less promising directions. We want to mention that NSGD w/o (CDP & TB) also significantly improves the learning speed and quickly reaches close to its high offline NDCG in less than 200 iterations, but it took MGD more than 800 iterations to achieve its highest performance. Comparing NSGD w/o (CDP & TB) against NSGD w/o (TB), we observe that our context-dependent candidate preselection further improves the performance by selecting candidates that can be best differentiated by the current query in interleaved tests as compared with uniformly exploring inside the null space. Comparing NSGD w/o (TB) with NSGD, we observe that using difficult queries for tie breaking further improves the performance, rather than arbitrarily breaking the tie or taking the average of winners as suggested by [23], which often introduces unexpected variance in online learning.
To answer the fourth evaluation question, we study the effect of different hyper-parameter settings of NSGD and their corresponding online performance in the MQ2007 dataset with the three click models mentioned above. · Number of candidates. We vary the number of proposed candidate rankers m from 1 to 10, from which the best ranker set is chosen through team-draft multileaving [23]. The result is reported in Figure 6 (a). Although each click model has different best-performing candidate size, with more candidate directions proposed the performance generally first increases and then slightly decreases. As more candidates are proposed, even though more directions can be explored, it is also easier to have multiple winners in the interleaved test, which introduces unnecessary complexity in recognizing the

best ranker. For example, we can clearly observe a trend of decreasing performance across all click models when m is larger than 5. Specifically, since the result list length is set to 10, each ranker will on average only receive 2 clicks when 4 new rankers are proposed. This makes it common to have tied winners. This serves as further motivation for having an effective tie-breaking function in NSGD. We do not present results for m larger than 10 as each candidate ranker can only expect less than one click and the feedback from interleaved test is non informative. · Learning rate. In all DBGD-type OL2R algorithms, the exploration step size is decided by  and the learning rate for updating current ranker is decided by the choice of . Here we study the effect of different learning rate , by fixing  to 1. Figure 6 (b) shows the result of varying  from 0 to 0.5. We notice that in most cases  around 0.1 gives the best performance. This suggests that even though we are exploring with a large step size  , we should use relatively small learning rate  to avoid over-exploration. · Number of historical gradients to construct null space. As mentioned in Section 3.2, when using k historical gradients to construct the null space, NSGD only samples from a subspace whose rank is at most d - k. We vary the choice of k from 5 to 40 (as in MQ2007 the feature dimension d is 46). The result is showed in Figure 6 (c). We observe that when we increase k to 20, the performance keeps relatively stable; but when k goes beyond it, the performance decreases significantly. The key reason is the null space overly reduces the search space when k is too large, such that it prevents NSGD from finding a good direction to explore, and forces it to converge to a suboptimal model quickly. · Number of historical queries for tie breaking. When the algorithm receives multiple winning rankers from an interleaved test, we use the most recent difficult queries to identify the best ranker. In this experiment, we vary the number of historical queries kh for tie breaking from 0 (which means disable our tie breaking function) to 40. The result is showed in Figure 6 (d). Evidently, using more historical queries for tie-breaking leads to an increase the algorithm's performance. However, evaluating candidates over a large number of historical queries also increases the time and storage complexity. To balance computational efficiency and final performance, we set kh = 10 for NSGD in all previous experiments.
5 CONCLUSIONS
In this paper, we propose Null Space Gradient Descent (NSGD) to accelerate and improve online learning to rank. To avoid repeatedly exploring less promising directions, NSGD reduces its exploration space to the null space of recently poorly performing directions.

153

Session 1D: Learning to Rank I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Number of candidates

(b) Learning rate 

(c) Number of historical gradients (d) Number of historical queries for

to construct null space

tie breaker

Figure 6: Performance of NSGD under different hyperparameter settings on MQ2007 dataset.

To identify the most effective exploratory rankers, NSGD uses a context-dependent preselection strategy to select candidate rankers that maximize the chance of being differentiated by an interleaved test for the current query. When two or more rankers tie, NSGD uses historically difficult queries to evaluate and identify the most effective ranker. We performed thorough experiments over multiple datasets and show that NSGD outperforms both the standard DBGD algorithm as well as several state-of-the-art OL2R algorithms.
As our future work, it is important to study the theoretical properties of NSGD, including whether the directions proposed guarantee a low-bias estimation of the true gradients. As we observed in our empirical evaluations, the online trained models are generally worse than the offline trained ones, which benefit most from manual annotations. It is meaningful to combine these two types of learning schemes to maximize the utility of learnt models. Lastly, all OL2R algorithms consider consecutive interactions with users as independent; but this is not always true, particularly when users undergo complex search tasks. In this situation, balancing exploration and exploitation with respect to the search context becomes more important. We plan to explore this direction with our NSGD algorithm, as it already incorporates both long-term and short-term interaction history into gradient exploration.
6 ACKNOWLEDGMENTS
We thank the anonymous reviewers for their insightful comments. This work was supported in part by National Science Foundation Grant IIS-1553568 and IIS-1618948.
REFERENCES
[1] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. 2002. Finite-time analysis of the multiarmed bandit problem. Machine learning 47, 2-3 (2002), 235­256.
[2] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 1995. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Foundations of Computer Science, 1995. Proceedings., 36th Annual Symposium on. IEEE, 322­331.
[3] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.
[4] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Proceedings of the Learning to Rank Challenge. 1­24.
[5] Olivier Chapelle and Ya Zhang. 2009. A dynamic bayesian network click model for web search ranking. In Proceedings of the 18th international conference on World wide web. ACM, 1­10.
[6] Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. 2005. Online convex optimization in the bandit setting: gradient descent without a gradient. In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 385­394.
[7] Fan Guo, Chao Liu, and Yi Min Wang. 2009. Efficient multiple-click models in web search. In Proceedings of the Second ACM International Conference on WSDM. ACM, 124­131.
[8] Katja Hofmann, Anne Schuth, Shimon Whiteson, and Maarten de Rijke. 2013. Reusing historical interaction data for faster online learning to rank for IR. In Proceedings of the sixth ACM international conference on WSDM. ACM, 183­192.
[9] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Balancing exploration and exploitation in listwise and pairwise online learning to rank for information retrieval. Information Retrieval 16, 1 (2013), 63­90.

[10] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the eighth ACM SIGKDD. ACM, 133­142.
[11] Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, and Geri Gay. 2017. Accurately interpreting clickthrough data as implicit feedback. In ACM SIGIR Forum, Vol. 51. Acm, 4­11.
[12] Sumeet Katariya, Branislav Kveton, Csaba Szepesvari, and Zheng Wen. 2016. DCM bandits: Learning to rank with multiple clicks. In International Conference on Machine Learning. 1215­1224.
[13] Branislav Kveton, Csaba Szepesvari, Zheng Wen, and Azin Ashkan. 2015. Cascading bandits: Learning to rank in the cascade model. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15). 767­776.
[14] Yanyan Lan, Tie-Yan Liu, Zhiming Ma, and Hang Li. 2009. Generalization analysis of listwise learning-to-rank algorithms. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 577­584.
[15] Yanyan Lan, Tie-Yan Liu, Tao Qin, Zhiming Ma, and Hang Li. 2008. Querylevel stability and generalization in learning to rank. In Proceedings of the 25th international conference on Machine learning. ACM, 512­519.
[16] Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331.
[17] Tie-Yan Liu, Jun Xu, Tao Qin, Wenying Xiong, and Hang Li. 2007. Letor: Benchmark dataset for research on learning to rank for information retrieval. In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,
Vol. 310.
[18] Harrie Oosterhuis and Maarten de Rijke. 2017. Balancing Speed and Quality in Online Learning to Rank for Information Retrieval. In Proceedings of the 2017 ACM CIKM. ACM, 277­286.
[19] Filip Radlinski, Robert Kleinberg, and Thorsten Joachims. 2008. Learning diverse rankings with multi-armed bandits. In Proceedings of the 25th international conference on Machine learning. ACM, 784­791.
[20] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. 2008. How does clickthrough data reflect retrieval quality?. In Proceedings of the 17th ACM CIKM.
ACM, 43­52.
[21] Mark Sanderson et al. 2010. Test collection based evaluation of information retrieval systems. Foundations and Trends® in Information Retrieval 4, 4 (2010),
247­375.
[22] Anne Schuth, Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2013. Lerot: An online learning to rank framework. In Proceedings of the 2013 workshop on Living labs for information retrieval evaluation. ACM, 23­26.
[23] Anne Schuth, Harrie Oosterhuis, Shimon Whiteson, and Maarten de Rijke. 2016. Multileave gradient descent for fast online learning to rank. In Proceedings of the Ninth ACM International Conference on WSDM. ACM, 457­466.
[24] Anne Schuth, Floor Sietsma, Shimon Whiteson, Damien Lefortier, and Maarten de Rijke. 2014. Multileaved comparisons for fast online evaluation. In Proceedings of the 23rd ACM CIKM. ACM, 71­80.
[25] Aleksandrs Slivkins, Filip Radlinski, and Sreenivas Gollapudi. 2013. Ranked ban-
dits in metric spaces: learning diverse rankings over large document collections. Journal of Machine Learning Research 14, Feb (2013), 399­436. [26] Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval. Vol. 1. MIT press Cambridge.
[27] Hongning Wang, Yang Song, Ming-Wei Chang, Xiaodong He, Ahmed Hassan,
and Ryen W White. 2014. Modeling action-level satisfaction for search task satisfaction prediction. In Proceedings of the 37th international ACM SIGIR conference.
ACM, 123­132.
[28] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. 2012. The k-armed dueling bandits problem. J. Comput. System Sci. 78, 5 (2012), 1538­1556.
[29] Yisong Yue and Thorsten Joachims. 2009. Interactively optimizing information retrieval systems as a dueling bandits problem. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM, 1201­1208.
[30] Tong Zhao and Irwin King. 2016. Constructing reliable gradient exploration for online learning to rank. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 1643­1652.
[31] Masrour Zoghi, Tomas Tunys, Mohammad Ghavamzadeh, Branislav Kveton,
Csaba Szepesvari, and Zheng Wen. 2017. Online Learning to Rank in Stochastic Click Models. In International Conference on Machine Learning. 4199­4208.

154

Session 6A: Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

On Fine-Grained Relevance Scales

Kevin Roitero
University of Udine Udine, Italy
roitero.kevin@spes.uniud. it

Eddy Maddalena
University of Southampton Southampton, U.K.
e.maddalena@soton.ac.uk

ABSTRACT
In Information Retrieval evaluation, the classical approach of adopting binary relevance judgments has been replaced by multi-level relevance judgments and by gain-based metrics leveraging such multi-level judgment scales. Recent work has also proposed and evaluated unbounded relevance scales by means of Magnitude Estimation (ME) and compared them with multi-level scales. While ME brings advantages like the ability for assessors to always judge the next document as having higher or lower relevance than any of the documents they have judged so far, it also comes with some drawbacks. For example, it is not a natural approach for human assessors to judge items as they are used to do on the Web (e.g., 5-star rating).
In this work, we propose and experimentally evaluate a bounded and fine-grained relevance scale having many of the advantages and dealing with some of the issues of ME. We collect relevance judgments over a 100-level relevance scale (S100) by means of a large-scale crowdsourcing experiment and compare the results with other relevance scales (binary, 4-level, and ME) showing the benefit of fine-grained scales over both coarse-grained and unbounded scales as well as highlighting some new results on ME.
Our results show that S100 maintains the flexibility of unbounded scales like ME in providing assessors with ample choice when judging document relevance (i.e., assessors can fit relevance judgments in between of previously given judgments). It also allows assessors to judge on a more familiar scale (e.g., on 10 levels) and to perform efficiently since the very first judging task.

CCS CONCEPTS
· Information systems  Information retrieval; Relevance assessment;

KEYWORDS
IR Evaluation, Relevance Scales
ACM Reference Format: Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro. 2018. On Fine-Grained Relevance Scales. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210052

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210052

Gianluca Demartini
University of Queensland Brisbane, Australia
g.demartini@uq.edu.au

Stefano Mizzaro
University of Udine Udine, Italy
mizzaro@uniud.it

1 INTRODUCTION
Relevance assessment is an integral part of Information Retrieval (IR) evaluation, since the Cranfield experiments, through the TREC and TREC-like initiatives. In the recent years the collection of relevance judgments is being studied using crowdsourcing. To gather relevance labels, several scales have been used in the past. The most common are the classical binary scale, or ordered scales with a limited number of categories (usually ranging from 3 to 7). It has recently been proposed [15, 21] to use Magnitude Estimation (ME) to gather relevance assessments on a ]0, +[ scale that has the following advantages:
(1) it is more fine-grained than the above alternatives (and thus, at least potentially, allowing to capture relevance differences that would otherwise be lost);
(2) it is able of always providing to the assessor a smaller or higher relevance value, and even a value in between other two, always allowing to assign to a new document a relevance value unforeseen in advance. This happens in particular at the extremes of the scale, but also for the values internal to the range; and
(3) it can adapt to different assessors' preferences (e.g., those who prefer to use a binary scale can do that and those who prefer to judge in a scale from 1 to 10 can also do that).
ME is not free from disadvantages, though: · it requires a normalization of the collected scores since each assessor is free to use a different "internal" relevance scale. This normalization is not simple, and it is not clear which is the best alternative, although some techniques seem to be reasonably effective [15, 21]; · it does not allow for a direct comparison of scores provided by different judges and/or on different topics as the score normalization is typically performed on a topic-by-topic basis; · it is somehow unnatural, or at least it requires some adaptation for the human assessor as compared to most common rating scales which are bounded; and · it leads to a log-normal distribution of relevance scores.
In this paper we discuss and experimentally evaluate by means of large-scale crowdsourced relevance judgments the use a finegrained scale on 100 levels (S100). Using the proposed 100 levels scale, the human assessor judges the relevance of a document with respect to a query by means of a number in the [0..100] range (extremes included, thus the levels are actually 101; we name it S100 anyway). Such a scale can be seen as a sort of compromise between the classical a-few-categories relevance scales and ME. We run a large scale crowdsourcing experiment to collect more than 50 thousand labels on such a scale, we discuss its advantages and disadvantages with respect to the already proposed alternatives, and we experimentally compare our judgments with judgments on

675

Session 6A: Evaluation SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro

coarse-grained scales (i.e., binary and 4-levels) and with judgments using ME.
More specifically, our research questions are: · Can relevance values be collected in a reliable way using a 100 levels scale in a crowdsourcing setting? ­ How do crowd workers choose to use the proposed scale? ­ Are the collected labels consistent with standard ground truths? · What are the differences between S100 and ME? ­ What are the effects of the relevance scale on IR system evaluation/ranking? ­ Which scale is more robust to decreasing the number of judgments per topic/document pair? What happens when collecting fewer judgments per assessor? ­ ME requires some learning to be used effectively by crowd workers as it is not like rating scales they are already used to. What happens to judgment quality when the number of documents judged by each worker in each HIT1 decreases? ­ ME allows to go beyond the maximum and minimum judgment level previously used, and to always find a judgment in between two previously expressed judgments. Are these properties required and useful in practice when using S100? ­ Are S100 and ME different w.r.t. the time needed to express judgments? Does ME require more adaptation time when used for the first time (i.e., on the first documents)?
Our main findings are: · w.r.t. binary and coarse-grained relevance scales, S100 gives assessors more flexibility in terms of preferential judgments over the documents they are presented during the judging task. Assessors using S100 also have the freedom to judge on a 10-level scale (or 4-level, etc.). It also better aligns with coarse-grained scales as compared to ME (see Section 4). · w.r.t ME, S100 gives assessors a reference point by providing upper and lower scale boundaries (see Section 4). · S100 is more robust than ME to both fewer assessors per document and fewer documents per assessor (see Section 5). · The theoretical problem of running out of values (at the extremes of the scale) does not occur often in practice, at least in our setting. Of course, with more document to judge for each worker, the problem might manifest (see Section 6). · If a fine-grained scale is preferred, using ME in a crowdsourcing setting can provide results faster while S100 enables direct comparison over topics and workers and does not require normalization (see Section 7). · While ME shows a steeper learning curve with more time needed to judge the first few documents, it becomes faster for crowd workers to judge with ME compared to S100 in the long term. Considering that crowd work is long-tail distributed with most workers completing very few HITs, S100 may be a more efficient strategy for crowdsourced relevance judgments (see Section 7).
This paper is structured as follows. Section 2 surveys related work in the area of relevance scales and crowdsourced relevance judgments. Section 3 presents an analysis of the relevance judgments we collected on the newly proposed S100 scale. In Section 4
1Human Intelligence Task, the task that each worker has to perform.

we compare S100 with other commonly used bounded scales with 2 and 4 levels and with the ME unbounded scale. Section 5 compares the robustness of S100 and ME to having few judgments per document and few documents per assessor. Section 6 presents an analysis looking at how S100 may lead to assessors running out of values as compared to ME which enables higher Section 7 compares S100 and ME in terms of the time required for assessors to express their judgments. Section 8 summarizes our main findings and the main benefits and drawbacks of the newly proposed S100 scale as compared to commonly used relevance scales.
2 RELATED WORK
2.1 Binary vs Graded Relevance Judgments
Relevance is a central concept in IR [18] evaluation; IR systems are usually evaluated using test collections, which are composed of (i) a collection of documents, (ii) a set of queries (called topics), and (iii) a set of relevance assessment for each (topic, document) pair in a pooled set of documents; such assessments are made by human experts according to an ordinal scale, which is usually binary.
Test collections can be created by means of a competition: participating systems return a ranked list of n documents (usually 1000), which are then used to compute the judging pool (e.g., the top 100 documents returned by each system, for each topic). The documents in the pool are the ones assessed by human experts. The produced relevance judgments are used together with the ranked lists produced by the systems to compute an effectiveness metric for each (system, topic) pair; a commonly used metric is Average Precision (AP). In order to provide a final rank of participant systems, the effectiveness scores are averaged over the set of topics; for example, the average of AP scores originates Mean AP (MAP).
Historically, relevance judgments were made by assessing whether a document is relevant or not to a topic; then, based on the observation that more than two levels might be needed, a set of novel metrics which incorporate multiple levels relevance scales were developed, such as, for example, Normalized Discounted Cumulative Gain (NDCG) [10], Expected Reciprocal Rank (ERR) [2], and Q-Measure [17].
Concerning the ideal number of relevance levels to be used, over the years many proposal have been made: a three-level scale was used in TREC-Terabyte Track [3], a six-level scale was used in TREC-Web Track [4], a seven-levels scale was proposed by Tang et al. [20] when studying evaluation of bibliographic records by students, using relevance scales with a range of levels from two to eleven. Then, Maddalena et al. [15] proposed an unbounded scale based on Magnitude Estimation, which is described in the following. Despite the many different approaches on relevance scales, the question of how many relevance levels should we use is far from answered. In our work we present a comprehensive study on the effects of relevance scales on IR evaluation proposing a fine-grained scale at 100 levels that incorporates the benefits of both bounded scales as well as the flexibility of an unbounded scale.
2.2 Continuous Relevance and Magnitude Estimation
We provide some more details on the use of ME since we compare against it in the following, and since our experiments rely on reassessing documents on a 100-level scale following the same

676

Session 6A: Evaluation On Fine-Grained Relevance Scales

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

experimental setting. ME is psychophysical technique used to measure the intensity of sensations [15]. The ME technique asks a human subject to give as a first response a number in the range (0; +); the successive numbers are assigned to reflect their relative difference; the outcome of ME are a set of measurements in a ratio scale [7]. Maddalena et al. [15] evaluated, using the CrowdFlower2 platform, 18 TREC-8 topics, for a total of 4,269 documents. The documents are the top 10 documents returned for IR systems competing in the ad-hoc track; some documents (i.e., 3,881) were previously evaluated by TREC assessors using a binary scale, and some of those documents (i.e., 805) have been reassessed in the study by Sormunen [19] using a 4-level scale.
Results from [15] show that: (i) ME aggregated judgments are closely aligned with the ordinal coarse-grained scale, both overall and across topics; (ii) the gathered judgments have shown a high level of agreement with both TREC and Sormunen; (iii) the impact on system evaluation, i.e., the correlation between the system ranking when using ME judgments, has a Kendall's  correlation of 0.677 with the official TREC ranking using NDCG@10.
In this paper we look at the challenges and opportunities of using S100 as compared to ME, binary and 4-level scales by means of comparative experiments using crowdsourcing platforms to collect relevance judgments at scale.
2.3 Relevance Dimensions and Biases
Recently, Jiang et al. [12] looked at the use of a multi-dimensional relevance definition including novelty, understandability, reliability, and effort for contextual judgments that are performed by assessors when looking at the search engine result page. In our work we rather focus on the classic definition of relevance based on topicality and look at the effect of different scales on IR evaluation.
Eickhoff [5] looked at the effect of cognitive biases in crowdsourced relevance judgment tasks. He showed how crowd workers are affected by fellow workers' answer (Bandwagon effect) and by being presented with multiple options (Decoy effect). The existence of the Decoy effect proves that workers judgment is indeed affected by other documents they have seen before judging a given document, thus supporting even more the need for fine-grained relevance scales (as we propose in our work) that enable workers to express slight relevance differences across different documents.
2.4 Crowdsourcing for IR Evaluation
Over the last few years, the increasing size of document collections created the need to scale the gathering of relevance judgments. For this reason, crowdsourcing has become a consolidated methodology to create relevance labels for query-document pairs given a judgment pool. In order to produce crowdsourced relevance labels at a quality level comparable with that of expert assessors a number of techniques have been proposed and evaluated in literature. A common approach is to collect relevance judgments for the same query-document pair from different crowd workers and to aggregate them together [1, 8, 22] thus allowing to remove noise in the labels. Past research also showed that asking for a justification for the judgments [16] and that limiting the time to judge [14] can increase crowdsourced relevance judgment quality. In our work we leverage crowdsourcing to collect relevance judgments over
2 https://www.crowdflower.com/

different scales and build on top of existing crowdsourcing research in terms of quality checks and HIT design best practices.
3 S100: A 100-LEVEL RELEVANCE DATASET
In this section, we present the results of our crowdsourcing effort aimed at collecting judgments on a 100-level scale. To make our dataset comparable with others, we followed the experimental design defined by [15] and reassessed 4,269 documents from 18 topics of TREC-8 ad hoc collection, in a [0, 100] discrete scale. As done in [15], we used the CrowdFlower crowdsourcing platform and rewarded workers $0.2 for each HIT performed (defined as a sequence of 8 documents which required to be judged in relation to one topic).
The main design difference as compared to that used for the ME collection by [15] is in the HIT graphical interface which, in our case, expects the relevance score to be given by using a [0,100] slider, instead than using a text field and an unbounded scale. The adoption of the slider is motivated by the bounded and fine-grained scale and it commonly used for rating items on multi-level scales (see, for example, [9]). In terms of quality checks, we performed the same checks as [15] (i.e., a test question on topic understanding; at least 20 seconds spent on at least 6 of the 8 documents in the HIT; consistency of judgments on two gold documents included in the 8 documents). Additionally, we required workers to move the slider (which was pre-set at 50) for at least 4 of the 8 documents. When failing the quality checks, workers were allowed to restart the HIT and change their previous answers. Up to 3 attempts were allowed. We tracked the times spent by each worker on each document, and these were cumulated over different attempts. We observed that 85.3% of workers completed the HIT after the first attempt, 11.2% after the second, and 3.5% after the third. Workers could not work on a topic more than once, but they were given the chance to repeat the task on different topics.
3.1 Judgment Distribution in S100
Figure 1 (a) shows the distribution of the individual scores gathered for S100: the x-axis represent the score obtained by a document, the y-axis represent its frequency; the red line represent the cumulative distribution. Figures 1 (b) and (c) show the distribution when doing a breakdown on non-relevant documents and on relevant documents, according to TREC assessors, respectively. From the plot using all the judgments (Figure 1 (a)) we see that, as expected, the distribution is clearly skewed towards lower (less relevant) scores; furthermore, there is a clear tendency of giving scores which are a multiple of ten, and the two most frequent scores are 0 and 100. In fact, the scores which are divisible by 10 are 60% of all the judgments in the dataset. The judgments on the scale boundaries (i.e., 0 and 100) are the 41%. If we do not consider the scale boundaries, the number of judgments which are divisible by 10 are the 32%. Due to the large presence of non relevant documents, the total distribution of scores is mainly influenced by and very similar to the distribution of the non-relevant documents according to TREC assessors (Figure 1 (b)), as we can see when comparing the cumulative distribution for the plots of all the documents with the one of non-relevant documents (i.e, the red lines in Figure 1 (a) and (b)).
When comparing Figure 1 (b) and (c), we see that for non-relevant documents (Figure 1 (b)) the majority of S100 scores is in the lower

677

Session 6A: Evaluation SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro

Cumulative Frequency

3000 14514 2250 1500

45000 33750 22500

3000 11898 2250 1500

600 45000

33750

450

22500

300

1401 8000 6000 4000

Cumulative Frequency Frequency

Cumulative Frequency Frequency

Frequency

750

11250

750

11250

150

2000

0 0
200

0

20

40

60

80

100

Score

(a)

4500

150

3375

0 0
200

0

20

40

60

80

100

Score

(b)

4500

150

3375

0 0
20

0

20

40

60

80

100

Score

(c)

900

15

675

Cumulative Frequency

Cumulative Frequency Frequency

Cumulative Frequency Frequency

Frequency

100

2250

100

2250

10

450

50

1125

50

1125

5

225

0 0

0

20

40

60

80

100

Score

(d)

0 0

0

20

40

60

80

100

Score

(e)

0 0

0

20

40

60

80

100

Score

(f )

Figure 1: Individual score distribution in the S100 dataset for all (a), for non-relevant (b), and for relevant (c) documents according to TREC judgments. Aggregated score distribution in the S100 dataset for all (d), for non-relevant (e), and for relevant (f) documents according to TREC judgments.

part of the scale (left on the plot) and for relevant documents (Figure 1 (c)) the majority of S100 scores is in the higher part of the scale (right on the plot). Moreover, we observe that many non-relevant documents obtained the maximum possible score (i.e., 100), and many relevant documents obtained 0 as a score. This may depend on multiple factors: a misclassification by TREC experts, a document/topic ambiguity, or might even be an indicator of low quality crowd judgments, obtained despite the strict quality checks applied to the task. Furthermore, we notice that the "decimal preference" is still present both for relevant and non relevant documents.
3.2 Aggregated Judgments in S100
Next, we proceed with aggregating the raw relevance judgments collected form the crowd for the same topic/document pair as commonly done to increase the quality of the collection. Relevance scores in S100 are in the [0, 100] range, thus a natural aggregation function is represented by the arithmetic mean of the individual scores, with no prior normalization of individual scores as done for ME.3 Figure 1 (d,e,f) show the distribution for the aggregated judgments. We can see that, as compared to the raw judgments given by individual workers (Figure 1 (a,b,c)), the aggregated judgments follow the expected distribution of many non-relevant documents with a long-tail of more relevant documents. The aggregation has
3We experimentally compared different aggregation functions and the use of score normalization functions but observed that the use of the arithmetic mean over nonnormalized score lead to most accurate labels compared to the other datasets.

Cumulative Frequency

400 350 300 250 200 150 100
50 0 0

20

40

60

80

100

Score

Topic 402 Topic 403 Topic 405 Topic 407 Topic 408 Topic 410 Topic 415 Topic 416 Topic 418 Topic 420 Topic 421 Topic 427 Topic 428 Topic 431 Topic 440 Topic 442 Topic 445 Topic 448

Figure 2: Aggregated score cumulative distribution in the S100 dataset; breakdown on individual topics.

also the effect of making the curves smoother, as well as making the tendency of scores to be a multiple of ten less prominent.
Figure 2 shows the aggregated judgment cumulative distributions broken down by topic. We can observe similar trends over all topics with some topics (e.g., 448 and 442) having a cumulative curve growing faster (i.e., having many `not so relevant' documents) and others having a much slower growth (e.g., 403) showing a presence of more relevant documents. A slightly different pattern is shown by topic 421 which grows towards the end of the relevance

678

Session 6A: Evaluation On Fine-Grained Relevance Scales

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

100

80

Relevance S100

60

40

20

0
UNMRH Relevance

U

0

1

Figure 3: Aggregated judgment scores collected for S100 vs TREC labels (right) and vs S4 (left). U indicates unjudged documents in the TREC and Sormunen collections.

score interval. This is explained by the fact that this topic has a small fraction of low relevance documents (Sormunen 0 and 1) and a high fraction of high relevant documents (Sormunen 2 and 3).
4 COMPARISON WITH OTHER SCALES
In this section we compare the judgments collected for S100 with judgments performed on the same documents over different relevance scales. We introduce an agreement measure that allows us to compute agreement across judgment scales and report agreement values for S100 with the TREC binary scale, the Sormunen 4-level scale (S4), and ME.
4.1 Score Distribution as Compared to Other Scales
Figure 3 shows how the judgments performed on S100 compared with the binary labels collected by TREC and the 4-level judgments performed by Sormunen. We can observe that while the median value for documents judged as relevant and non-relevant by TREC is different in S100, the distribution of S100 scores covers the entire score interval for both type of documents. The distribution of S100 scores compared to S4 labels by Sormunen shows the non linearity of the 4-level labels that have been collected on the scale N-MR-H (i.e., not relevant, marginally relevant, relevant, and highly relevant). When comparing to the similar Figure 3 by Maddalena et al. [15] we can notice that in S100 the relevance scores are better distributed across the full scale with highest levels of relevance in S4 and the binary TREC scale having a median score closer to the upper bound of the scale as compared to ME. Such behavior is not observed for ME as the scale is unbounded at the top making scores for highly relevant documents having a wider distribution.
Figure 4, similar to Figure 14 by Maddalena et al. [15], presents a clearer evidence of the difference between ME and S100: whereas in ME the gain profiles seem to be exponential, or at least super-linear, in our case they are clearly sub-linear. We consider this a reason to prefer S100 to ME, since it better reflects the definition of relevance levels introduced for S4 which assumes that already marginally relevant documents should be substantially better than not relevant ones with a sub-linear step increase for the subsequent relevance

Figure 4: Distribution of individual scores in S100 for each Sormunen level.

levels R and H. Such differences (Figures 3 and 4) between ME and S100 scores are likely due to the effect of the end of scale which is unbounded in ME thus making high-relevance scores disperse. This is not the case in S100, a bounded scale that allows assessors to implicitly map their judgments against the scale upper bound.

4.2 Agreement with TREC
First we introduce a new measure that allows us to check assessor agreement across rating scales. We then adopt this measure to evaluate the quality of S100 scores as compared to other datasets.

4.2.1 An agreement measure for ratings given over different scales.
Given two rating vectors X = {x1, . . . , xn } and Y = {y1, . . . , yn } where xi (yi respectively) represents the i-th document in a sequence of relevance judgments (e.g., a HIT), we define X  as the sorted vector X and Y  the re-ordering of Y maintaining the relation to X . That is, X  = {xi | xi  X  xi  xi+1, i  {1, . . . , n}}
and Y  = zi | zi = yindex _of (xi )  yi  Y , i  {1, . . . , n} . Based
on such two lists, we define the following agreement function4

1 pos_agr(A, B, i, j) = 0

if xi xj  yi < yj  x  A  y  B otherwise,

that tells us whether the ordering of two documents is consistent across the two judging sets. Thanks to this we can now define the agreement score as the ratio of consistent document pairs over all possible pairs:

n

n

pos_agr X , Y , i, j

·

n

-1
.

i=1 j=i+1

2

Note that this is not a symmetric measure, but it rather computes agreement of Y ratings as compared to X considered the baseline
judgments. This measure computes the number of agreement pairs between the two datasets. That is, if a document w has a higher relevance judgment score than a document z according to judgments in X , we would like the same order w  z to be maintained in Y . That is, the relevance judgment score of w should be higher than z according to Y judgments.

4.2.2 Comparison with other scales. Figure 5 shows the complementary cumulative distribution function (showing how often

4We consider yi < yj rather than xi  xj as we assume X to use a coarser-grained scale (e.g., binary) as compared to Y (e.g., S100).

679

Session 6A: Evaluation SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro

7000

6000

Freq.

5000

4000

S100

ME

3000

0.0

0.2

0.4

0.6

0.8

1.0

Agreement score

Figure 5: Complementary cumulative distribution function of pairwise agreement as defined in Section 4.2.1 of S100 and ME with TREC.

agreement is above a given value) of pairwise agreement for S100 and ME with respect to TREC binary judgments. The ME series is another representation of the data in [15, Figure 6]. The comparison highlights that agreement levels in S100 are higher than ME.
Figure 6 shows the topics ordered by another standard measure of agreement, Krippendorff's  [13]. We can make the following observations:
· Agreement scores for judgments collected with S100 are substantially higher than those collected with ME.
· There is some consistency across S100 and ME in the sense that topics with high/low agreement tend to be the same.
· Agreement over TREC non-relevant documents is higher as compared to relevant ones.
· Agreement on the non-relevant documents as compared to agreement on all the documents is similar in the two figures, whereas agreement on the relevant documents as compared to agreement on all the documents is higher in S100 than in ME (the green "TREC: 1" series is "pulled up" in the S100 chart). In other terms, S100 improves, w.r.t. ME,  agreement on the relevant documents.
4.3 IR System Ranking Correlation
Finally, we computed Kendall  correlation of IR systems ranked by effectiveness computed using judgments collected over different relevance scales. Figure 7 shows the IR system ranking correlation using NDCG@10 [11] when using binary judgments as compared to S100 (a), using binary judgments as compared to ME (b); and using S100 as compared to ME (c). Each dot is a system and the charts show its NDCG@10 values over two different scales. We can observe that, while all judgments result in high system ranking correlation values, the best correlation is obtained when comparing S100 and ME. This demonstrates how S100 lead to results similar to ME by providing assessors the flexibility to judge document relevance on a fine-grained basis. This is also explained by the fact that S100 and ME have been collected following the same crowdsourcing setup while the TREC and S4 did not use crowdsourcing. Looking at how S100 and ME compare with TREC (Figure 7 a and b) we can see that while correlation values are similar, NDCG@10 scores obtained using S100 are more consistent with those obtained with TREC labels whereas ME judgments tend to result in lower NDCG scores.

0.8

TREC: 0

TREC: 1

0.6

TREC: all

mean  421 427 407 431 402 420 403 440 415 442 418 408 428 416 448 405 445 410

0.4

0.2

mean  440 431 442 427 407 418 402 403 445 428 448 415 420 416 421 408 405 410

0.0

TREC: 0

0.30

TREC: 1

0.25

TREC: all

0.20

0.15

0.10

0.05

0.00

Topic (all) Topic (all)

Figure 6: Agreement () of the individual topics, in S100 (above) and ME (below).

5 ROBUSTNESS TO FEWER JUDGMENTS
In this section we study how different relevance scales behave in terms of robustness to fewer judgments. That is, we look at how crowdsourced relevance label quality decreases as compared to editorial judgments by experts like TREC and Sormunen's S4 assessors. In detail, we study two kinds of robustness:
· Shorter HITs: including fewer documents to be judged in a HIT so that each worker has the option to do less if they wish to.
· Fewer assignments per document: using fewer workers judging the same document, and averaging their judgments.
We measure robustness by observing how pairwise agreement with TREC and S4 decreases.
5.1 Fewer Documents per HIT
In the crowdsourcing setup used to create the S100 and ME collections each worker is required to judge 8 documents in one HIT. When using fewer documents per HIT, we assume we could lose on training effects (i.e., workers becoming proficient in the judging task) with the benefit of work flexibility.
Figure 8 shows how pairwise agreement varies when using shorter HITs (i.e., looking at judgment quality based on the document position in the HIT). For any HIT length, the pairwise agreement of individual judgments is higher for S100 than ME with an increasing gain in agreement the longer the HIT.
5.2 Fewer Judgments per Document
In both S100 and ME, 10 judgments per document have been collected. When using fewer assignments, as it is expected, the quality of the aggregated judgments decreases. We analyze this by showing

680

Session 6A: Evaluation On Fine-Grained Relevance Scales

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

S100

NDCG@10 0.30

0.25

0.20

0.15

0.10

0.05

Pearson: p=0.94 (p=6.02e-63)

0.00

Kendall: p=0.75 (p=7.26e-37)

0.00 0.05 0.10 0.15 0.20 0.25 0.30 TREC

(a)

ME

NDCG@10 0.30

0.25

0.20

0.15

0.10

0.05

Pearson: p=0.94 (p=3.63e-61)

0.00

Kendall: p=0.75 (p=9.18e-37)

0.00 0.05 0.10 0.15 0.20 0.25 0.30 TREC

(b)

S100

NDCG@10 0.30

0.25

0.20

0.15

0.10

0.05

Pearson: p=0.99 (p=1.49e-124)

0.00

Kendall: p=0.89 (p=4.27e-50)

0.00 0.05 0.10 0.15 0.20 0.25 0.30 ME

(c)

Figure 7: NDCG@10 scores for TREC-8 runs and judgments collected over different scales: TREC, ME and S100.

Using first 2 documents per HIT 7000

Using first 3 documents per HIT 7000

Using first 4 documents per HIT 7000

6000

6000

6000

Freq.

Freq.

Freq.

5000

5000

5000

4000

S100

4000

4000

ME

3000

3000

3000

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Agreement score

Agreement score

Agreement score

Using first 5 documents per HIT 7000

Using first 6 documents per HIT 7000

Using first 7 documents per HIT 7000

6000

6000

6000

Freq.

Freq.

Freq.

5000

5000

5000

4000

4000

4000

3000

3000

3000

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

Agreement score

Agreement score

Agreement score

Figure 8: Complementary cumulative distribution function of pairwise agreement as defined in Section 4.2.1 of S100 and ME with TREC, when using the first i  {2, . . . , 7} documents for each HIT (i = 8 shown in Figure 5).

how aggregated pairwise agreement decreases when using a random subsample of the 10 judgments in Figure 9: the figure reports the pairwise agreement average values, for each topic, over 100 random repetitions. For all sizes of the subsample, S100 median pairwise agreement is higher for S100 than ME.5 These results show a higher robustness to fewer assignments of S100 as compared to ME, making it a more economically viable scale to use to collect crowdsourced relevance judgments.
6 RUNNING OUT OF VALUES
As recalled above, the ME scale has the advantage that the assessor never "runs out of values", neither (i) at the scale extremes (which
5We observed the same result when computing pairwise agreement against Sormunen's S4 judgments but we did not include the figure for space limitations.

are unbounded) nor (ii) inside the scale (which is continuous). This advantage is lost when using limited scales as S100, and it is of course further compounded for scales with a lower amount of values like S4 and TREC. In this section we aim at understanding if these potential problems have actually been a practical constraint for the judges using the S100 scale. An initial analysis can be performed comparing the number of "back" actions crowd workers performed which indicate a desire to change or look at their previously judged documents. This value is much higher in S100 than in ME. In ME the number of single "back" actions was 106, and the number of two or more "back" actions was 9 [15, Table I], whereas for S100 these two figures are 113 and 182 respectively: although the numbers are still very limited (more than 95% of S100 workers did not use the "back" button at all), the differences are noticeable and might be ascribed to a higher difficulty in finding the "right" relevance score.

681

Session 6A: Evaluation SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro

1.00

Pairwise Agreement with TREC

0.95

0.90

0.85

0.80

0.75

0.70

0.65

1

2

3

4

5

6

7

8

9

10

No. of judgments per document

Figure 9: Pairwise agreement of S100 (boxplot on the left) and ME (boxplot on the right) with TREC, when sampling randomly (mean over 100 repetitions) i  {1, . . . , 10} judgments for each document. Each dot is a topic.

Table 1: The number of cases with exactly k 0s or 100s in the same HIT, and the corresponding number of potential run-out-of-values cases in the S100 dataset.

k

0 1 2 3 4 5 6 7 Tot

0s 2355 803 694 689 736 786 641 355 100s 3796 1808 846 398 133 55 19 4

k -1

123456

0s* 100s*

0 0 694 1378 2208 3144 3205 2130 12759 0 0 846 796 399 220 95 24 2380

In the next two subsections we present a more detailed analysis, addressing the two issues (i) and (ii).
6.1 Reaching the Scale Boundaries
Table 1 shows in the first part the number of HITs with "boundary judgments", i.e., with exactly k 0s or 100s in the S100 dataset. The HITs without or with only one boundary judgment (i.e., only one 0 or one 100, in italics in the table) do not create any potential problem; instead, the boundary judgments after another boundary judgment (i.e., in the same HIT, one or more 0s after a first 0, or one or more 100s after a first 100) might be cases in which the worker could have used a lower or higher value if available. So, the HITs with at least two boundary values (the following columns) are those in which, at least potentially, the worker "ran out of values" at each extreme of the scale. A lower (higher) value, if available, could have been selected for each of the k - 1 "boundary judgments" after the first one. The numbers of such "boundary judgments following other boundary judgment(s)" are quantified in the lower part of the table: these are obtained multiplying by k - 1 the figures in the previous two rows (for example, the 2'208 value is obtained as 736x(4-1): in 736 HITs the workers used 0 for 4 times, and the last three in each HIT are candidates for "run-out-of-value" cases).
To provide an understanding of the frequency of the problem, let us remember that we had a total of 7'059 HITs, each one containing 8 documents. Since the first expressed judgment in each

unit can not be preceded by another (boundary) judgment, we have 7'059x7=49'413 judgments that could have manifested the problem. Of those, the problem manifests for a total of 12'759+2'380=15'139 cases (31%). Of course this is not negligible: in almost one case out of three a worker might have been restricted in expressing the true intended judgment. However, this also means that in 69% of the expressed judgment we can say that the worker was not restricted by the boundaries of the S100 scale. Moreover, these 31% of cases are only potential problems, as it might well be that the worker intended to express exactly the same judgment and did not actually run out of values. Therefore, we further analyzed these potential run-out-of-values cases in two ways.
First, we looked in our S100 dataset what fraction of the boundary judgments 0 (or 100) expressed by a worker in a HIT after the first boundary judgment corresponds to a document that has a strictly lower (higher) aggregated score. These are cases in which the worker ran out of values, assuming that the intended score corresponds to the aggregated one. This happened 7'523 cases, namely 50% of the 15'139 potential problematic cases, or 15% of the total 49'413 judgments expressed.
Second, we looked in the ME dataset how many of the 15'139 potential run-out-of-value cases received an ME score that was lower (for the 0s), or, respectively, higher (for the 100s) than the first corresponding boundary judgment in the unit. These are cases in which the worker ran out of values assuming that the judgment expressed by the ME worker in the corresponding unit was exact. This happened for 4'309 cases, namely 28% of the 15'139 potential problematic cases or 9% of the total 49'413 judgments expressed.
So, the two analyses roughly agree that in only around one out of ten cases the bounded scale seems to have indeed limited the assessor, and therefore in about 90% of the expressed judgments the S100 scale did not create any obstacle to judgment expression.
6.2 Discrete vs. Continuous Scale
The second situation in which the S100 scale could constrict judgment expression is when contiguous values are selected, thus making impossible for the worker to select another, different, value in between in the following judgments in the same HIT. We counted

682

Fraction

Session 6A: Evaluation
On Fine-Grained Relevance Scales
.15
.1
.05
.0 0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 100 Relevance score
Figure 10: Fraction of expressed judgments, for each value of the S100 scale, that might have been constrained by previously expressed judgments in the same HIT.
in our S100 dataset how many scores x were preceded by both x and x + 1 (or by both x - 1 and x) in the same HIT. These are the cases in which, potentially, the worker could not give a y ]x - 1, x[ (or y ]x, x + 1[) value because of the discrete scale. There were 1'911 such cases, out of the 7'059x6=42'354 possible ones (as we need to count starting from the third judgment in each HIT), which is less than 5%. Notice again that this fraction is consistent with the number of `back' actions. Moreover, the vast majority of these cases (around 1'500) concern judgments between 0 and 10, which could be considered not critical (as it is probably more important to focus on the "relevant" end of the scale rather than on the "not relevant" end). This is confirmed by Figure 10 that shows, for each value between 0 and 100, the fraction of judgments at a given level that may have been affected by previous judgments given at the same level. These values indicate the percentage of cases in which there may be a limitation because of the use of S100 as compared to ME which would allow to give a slightly higher or lower judgment score as compared to previous ones. Note that this is an upper bound of such expressiveness limitation as assessors may have assigned the same score multiple times on purpose. From Figure 10 we can also observe that most problematic cases are, as expected, at the boundaries of the scale but also that such cases are not prevalent (about 5% of judgments are affected). More potentially constrained judgments are present at the lower end of the S100 scale. This should be less problematic than constrains at the upper end of the scale as we can expect more score ties for not relevant documents.
In summary, taking into account that these are only potential problems, since it is possible that the worker indeed intended to express the very same score again, we can conclude that these do not seem worrying problems in practice and that the theoretical constraints imposed by the S100 scale on judgments expression did not significantly obstacle the workers in practice.
7 JUDGMENT TIME
We compared the time required by crowd workers to assess documents using S100 and ME considering that the experimental design was consistent across the two studies with the only difference being the way relevance was expressed by assessors (i.e., a number versus a slider from 0 to 100).
The dotted series in Figure 11 show the mean time taken by crowd workers to assess documents, based on the order in which they were presented. The cumulated time shows that S100 leads to slightly quicker judgments than ME for the first 5 documents.

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

220

800

165

600

Time Cumulative Time

110

400

55

200

Time S100

Time ME

0

0

1

2

3

4

5

6

7

8

Document Position

Figure 11: Mean time (seconds) for expressing relevance judgments, over the 8 positions in each HIT, considering either only the first HIT for each worker (straight lines) or considering all HITs (dotted lines). All times difference are significant (Wilcoxon signed-rank test p < 0.01).

0.0175

0.0150

0.0125

Frequency

0.0100

0.0075

0.0050

0.0025

0.0000 0

50

100

150

200

250

Average time

Figure 12: Judging time distributions, combined for all topics, for S100 and ME. Average (vertical lines) and median (dashed vertical lines) time is also shown.

Starting from the 6th document in the HIT, the use of the ME scale leads to overall faster judgments. This can be explained by the fact that workers are probably at first disoriented by the uncommon ME scale, but they become more efficient in using it as they progress completing more judgments. The time behavior is quite stable across workers and topics (as it can be seen by the small quartile bars in Figure 11). Moreover, time differences are quite small, though always statistically significant, for each document position in the HIT: apart from the first position, the differences are around 5s for a total judgment time of 50-100s.
Figure 12 shows in more detail the distributions of judging time for both S100 and ME: the two are rather similar, with ME having a slightly longer tail. The behavior does not significantly change across individual topics (not shown for brevity). Given the small and constant time difference, a possible explanation is the presence of

683

Session 6A: Evaluation SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Gianluca Demartini, and Stefano Mizzaro

the slider, which might require a small longer amount of time than inserting a number in a text box, especially on mobile devices as shown by Gadiraju et al. [6]. Further analysis is needed to confirm this conjecture, and we leave that to future work.
To further study the learning effects, we repeated the same analysis considering only the first HIT performed by each worker (remember that workers could redo the task, on a different topic). Going back to Figure 11, the straight, not dotted, lines represent only these first HITs, for both S100 and ME. We notice three main variations with respect to the dotted lines: (i) overall, average times are higher, indicating that indeed some learning effect is present and workers become more efficient after the first HIT; (ii) the time difference on the first expressed judgment is larger, thus confirming that starting with the ME scale is somehow more difficult than with the S100 one; and (iii) on this data, the ME cumulative curve stays above the S100 one, meaning that the disadvantage cumulated on the first document by ME can not be recovered even after 8 judgments are judged.
Overall, we can conclude that time does not seem a critical factor when choosing between S100 and ME: differences are small and a longer time due to learning effects on ME tends to be compensated after some documents are judged.
8 CONCLUSIONS
In this paper we presented a systematic study comparing the effects of different relevance scales on IR evaluation. We have shown many advantages of the S100 scale as compared to coarse-grained scales like binary and S4 and to unbounded scales like ME. S100 preserves many of the advantages of ME like, for example, allowing to gather relevance judgments that are much more fine-grained than the usual binary or 4-value scales. Assessors use the full spectrum, although sometime with a preference for scores that are a multiple of ten. S100 has also demonstrated advantages over ME in terms of agreement with judgments collected on a binary and four level scales. This can be explained by the fact that ME requires a step of score normalization which makes judgments less comparable across assessors and topics. On the other hand, S100 leads to more similar judgments (i.e., higher agreement) to the classic binary and four level scales. S100 has shown to be more robust than ME in terms of less assessors per documents (to be aggregated, as typically done for crowdsourced relevance judgments) and to less documents per assessor thus giving the freedom to crowd workers to perform few or many judging tasks. Our results show that the potential constraints in judgment expression that the S100 scale might create with respect to the complete freedom of ME almost do not occur in practice since about 90% of the judgments did not suffer from this problem. The S100 scale also seems easy to learn for the workers and turns out to be faster than ME for short HITs with 5 or less documents to be judged, and of comparable speed for longer HITs. Overall, our results show that S100 is an effective, robust, and usable scale to gather fine-grained relevance labels.
Acknowledgements. This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 732328.

REFERENCES
[1] Omar Alonso and Stefano Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Inf. Process. Manage. 48, 6 (2012), 1053­1066. https: //doi.org/10.1016/j.ipm.2012.01.004
[2] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). ACM, New York, NY, USA, 621­630. https://doi.org/10.1145/1645953.1646033
[3] Charles LA Clarke, Nick Craswell, and Ian Soboroff. 2004. Overview of the TREC 2004 Terabyte Track.. In TREC, Vol. 4. 74.
[4] Kevyn Collins-Thompson, Craig Macdonald, Paul Bennett, Fernando Diaz, and Ellen M Voorhees. 2015. TREC 2014 web track overview. Technical Report. MICHIGAN UNIV ANN ARBOR.
[5] Carsten Eickhoff. 2018. Cognitive Biases in Crowdsourcing. In WSDM 2018. To appear.
[6] Ujwal Gadiraju, Alessandro Checco, Neha Gupta, and Gianluca Demartini. 2017. Modus Operandi of Crowd Workers: The Invisible Role of Microtask Work Environments. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 1, 3, Article 49 (Sept. 2017), 29 pages. https://doi.org/10.1145/3130914
[7] George A Gescheider. 2013. Psychophysics: the fundamentals. Psychology Press. [8] Mehdi Hosseini, Ingemar J. Cox, Natasa Milic-Frayling, Gabriella Kazai, and
Vishwa Vinay. 2012. On Aggregating Labels from Multiple Crowd Workers to Infer Relevance of Documents. In Advances in Information Retrieval - 34th European Conference on IR Research, ECIR 2012, Barcelona, Spain, April 1-5, 2012. Proceedings. 182­194. https://doi.org/10.1007/978-3-642-28997-2_16 [9] Quan Huynh-Thu, Marie-Neige Garcia, Filippo Speranza, Philip Corriveau, and Alexander Raake. 2011. Study of rating scales for subjective quality assessment of high-definition video. IEEE Transactions on Broadcasting 57, 1 (2011), 1­14. [10] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446. https://doi. org/10.1145/582415.582418 [11] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002), 422­446. [12] Jiepu Jiang, Daqing He, and James Allan. 2017. Comparing In Situ and Multidimensional Relevance Judgments. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '17). ACM, New York, NY, USA, 405­414. https://doi.org/10.1145/3077136.3080840 [13] Klaus Krippendorff. 2007. Computing Krippendorff's alpha reliability. Departmental papers (ASC) (2007), 43. [14] Eddy Maddalena, Marco Basaldella, Dario De Nart, Dante Degl'Innocenti, Stefano Mizzaro, and Gianluca Demartini. 2016. Crowdsourcing relevance assessments: The unexpected benefits of limiting the time to judge. In Fourth AAAI Conference on Human Computation and Crowdsourcing. [15] Eddy Maddalena, Stefano Mizzaro, Falk Scholer, and Andrew Turpin. 2017. On Crowdsourcing Relevance Magnitudes for Information Retrieval Evaluation. ACM Transactions on Information Systems (TOIS) 35, 3 (2017), 19. [16] Tyler McDonnell, Matthew Lease, Mucahid Kutlu, and Tamer Elsayed. 2016. Why is that relevant? Collecting annotator rationales for relevance judgments. In Fourth AAAI Conference on Human Computation and Crowdsourcing. [17] Tetsuya Sakai. 2007. On the Reliability of Information Retrieval Metrics Based on Graded Relevance. Inf. Process. Manage. 43, 2 (March 2007), 531­548. https: //doi.org/10.1016/j.ipm.2006.07.020 [18] Tefko Saracevic. 2007. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology 58, 13 (2007), 1915­1933. https://doi.org/10.1002/asi.20682 [19] Eero Sormunen. 2002. Liberal Relevance Criteria of TREC -: Counting on Negligible Documents?. In Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '02). ACM, New York, NY, USA, 324­330. https://doi.org/10.1145/564376.564433 [20] Rong Tang, William M Shaw Jr, and Jack L Vevea. 1999. Towards the identification of the optimal number of relevance categories. Journal of the Association for Information Science and Technology 50, 3 (1999), 254. [21] Andrew Turpin, Falk Scholer, Stefano Mizzaro, and Eddy Maddalena. 2015. The Benefits of Magnitude Estimation Relevance Assessments for Information Retrieval Evaluation. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). ACM, New York, NY, USA, 565­574. https://doi.org/10.1145/2766462.2767760 [22] Matteo Venanzi, John Guiver, Gabriella Kazai, Pushmeet Kohli, and Milad Shokouhi. 2014. Community-based Bayesian Aggregation Models for Crowdsourcing. In Proceedings of the 23rd International Conference on World Wide Web (WWW '14). ACM, 155­164. https://doi.org/10.1145/2566486.2567989

684

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Entity Set Search of Scientific Literature: An Unsupervised Ranking Approach

Jiaming Shen, Jinfeng Xiao, Xinwei He, Jingbo Shang, Saurabh Sinha, Jiawei Han
Department of Computer Science, University of Illinois Urbana-Champaign, IL, USA {js2, jxiao13, xhe17, shang7, sinhas, hanj}@illinois.edu

ABSTRACT
Literature search is critical for any scientific research. Different from Web or general domain search, a large portion of queries in scientific literature search are entity-set queries, that is, multiple entities of possibly different types. Entity-set queries reflect user's need for finding documents that contain multiple entities and reveal inter-entity relationships and thus pose non-trivial challenges to existing search algorithms that model each entity separately. However, entity-set queries are usually sparse (i.e., not so repetitive), which makes ineffective many supervised ranking models that rely heavily on associated click history. To address these challenges, we introduce SetRank, an unsupervised ranking framework that models inter-entity relationships and captures entity type information. Furthermore, we develop a novel unsupervised model selection algorithm, based on the technique of weighted rank aggregation, to automatically choose the parameter settings in SetRank without resorting to a labeled validation set. We evaluate our proposed unsupervised approach using datasets from TREC Genomics Tracks and Semantic Scholar's query log. The experiments demonstrate that SetRank significantly outperforms the baseline unsupervised models, especially on entity-set queries, and our model selection algorithm effectively chooses suitable parameter settings.
KEYWORDS
Entity-Set Aware Search; Unsupervised Ranking Model; Unsupervised Model Selection; Literature Search
ACM Reference Format: Jiaming Shen, Jinfeng Xiao, Xinwei He, Jingbo Shang, Saurabh Sinha, Jiawei Han. 2018. Entity Set Search of Scientific Literature: An Unsupervised Ranking Approach. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­ 12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3209978.3210055
1 INTRODUCTION
Literature search helps a researcher identify relevant papers and summarize essential claims about a topic, forming a critical step in any scientific research. With the fast-growing volume of scientific publications, a good literature search engine is essential to
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210055

Table 1: Ranking performance on 100 benchmark queries of the S2 production system. Entity-set queries (ESQs), marked bold, perform much weaker than non-ESQs do.

Metrics NDCG@5 NDCG@10 NDCG@15 NDCG@20

ESQs
0.3622 0.3653 0.3840 0.4011

non-ESQs 0.6291 0.6286 0.6221 0.6247

Overall 0.5223 0.5233 0.5269 0.5353

researchers, especially in the domains like computer science and biomedical science where the literature collections are so massive, diverse, and rapidly evolving--few people can master the state-ofthe-art comprehensively and in depth.
A large set of literature search queries contain multiple entities which can be either concrete instances (e.g., GABP (a gene)) or abstract concepts (e.g., clustering). We refer these queries as entity-set queries. For example, a computer scientist may want to find out how knowledge base can be used for document retrieval and thus issues a query "knowledge base for document retrieval", which is an entity-set query containing two entities. Similarly, a biologist may want to survey how genes GABP, TERT, and CD11b are associated with cancer and submits a query "GABP TERT CD11b cancer", another entity-set query with one disease and three gene entities.
Compared with typical short keyword queries, a distinctive characteristic of entity-set queries is that they reflect user's need for finding documents containing inter-entity relations. For example, among 50 queries collected from biologists in 2005 as part of TREC Genomics Track [14], 40 of them are explicitly formulated as finding relations among at least two entities. In most cases, a user who submits an entity-set query will expect to get a ranked list of documents that are most relevant to the whole entity set. Therefore, as in the previous examples, returning a paper about only knowledge bases or only one gene GABP is unsatisfactory.
Entity-set queries pose non-trivial challenges to existing search platforms. For example, among the 100 queries1 released by Semantic Scholar (S2), 40 of them are entity-set queries and S2's production ranking system performs poorly on these entity-set queries, as shown in Table 1. The difficulties of handling entity-set queries mainly come from two aspects. First, entity relations within entity sets have not been modeled effectively. The association or cooccurrence of multiple entities has not gained adequate attention from existing ranking models. As a result, those models will rank papers where a single distinct entity appears multiple times higher than those containing many distinct entities. Second, entity-set queries are particularly challenging for supervised ranking models. As manual labeling of document relevance in academic search requires domain expertise, it is too expensive to train a ranking model
1 http://data.allenai.org/esr/Queries/

565

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

based purely on manually labeling. Most systems will first apply an off-the-shelf unsupervised ranking model during their cold-start process and then collect user interaction data (e.g., click information). Unfortunately, entity-set queries are usually sparse (i.e., not so repetitive), and have less associated click information. Furthermore, many off-the-shelf unsupervised models cannot return reasonably good candidate documents for entity-set queries within the top-20 positions. Many highly relevant documents will not be presented to users, which further compromises the usefulness of clicking information.
This paper tackles the new challenge--improving the search quality of scientific literature on entity-set queries and proposes an unsupervised ranking approach. We introduce SetRank, an unsupervised ranking framework that explicitly models inter-entity relations and captures entity type information. SetRank first links entity mentions in query and documents to an external knowledge-base. Then, each document is represented with both bag-of-words and bagof-entities representations [36, 37] and fits two language models respectively. On the query side, a novel heterogeneous graph representation is proposed to model complex entity information (e.g., entity type) and entity relations within the set. This heterogeneous query graph represents all the information need in that query. Finally, the query-document matching is defined as a graph covering process and each document is ranked based on the information need it covers in the query graph.
Although being an unsupervised ranking framework, SetRank still has some parameters that need to be appropriately learned using a labeled validation set. To further automate the process of ranking model development, we develop a novel unsupervised model selection algorithm based on the technique of weighted rank aggregation. Given a set of queries with no labeled documents, and a set of candidate parameter settings, this algorithm automatically learns the most suitable parameter settings for that set of queries.
The significance of our proposed unsupervised ranking approach is two-fold. First, SetRank itself, as an unsupervised ranking model, boosts the literature search performance on entity-set queries. Second, SetRank can be adopted during the cold-start process of a search system, which enables the collection of high-quality click data for training subsequent supervised ranking model. Our experiments on two benchmark datasets2 demonstrate the usefulness of our unsupervised model selection algorithm and the effectiveness of SetRank for searching scientific literature, especially on entity-set queries.
In summary, this work makes the following contributions: (1) A new research problem, effective entity-set search of scientific
literature, is studied. (2) SetRank, an unsupervised ranking framework, is proposed,
which models inter-entity relations and captures entity type information. (3) A novel unsupervised model selection algorithm is developed, which automatically selects SetRank's parameter settings without resorting to a labeled validation set. (4) Extensive experiments are conducted in two scientific domains, demonstrating the effectiveness of SetRank and our unsupervised model selection algorithm.
2 Both benchmark datasets and our model implementations are publicly available at: https://github.
com/mickeystroller/SetRank.

The remaining of the paper is organized as follows. Section 2 discusses related work. Section 3 presents our ranking framework SetRank. Section 4 presents the unsupervised model selection algorithm. Section 5 reports and analyzes the experimental results on two benchmark datasets and shows a case study of SetRank for biomedical literature search. Finally, Section 6 concludes this work with discussions on some future directions.
2 RELATED WORK
We examine related work in three aspects: academic search, entityaware ranking model, and automatic ranking model selection.
2.1 Academic Search
The practical importance of finding highly relevant papers in scientific literature has motivated the development of many academic search systems. Google Scholar is arguably the most widely used system due to its large coverage. However, the ranking result of Google Scholar is still far from satisfactory because of its bias toward highly cited papers [1]. As a result, researchers may choose other academic search platforms, such as CiteSeerX [33], AMiner [30], PubMed [20], Microsoft Academic Search [29] and Semantic Scholar [38]. Research efforts of many such systems focus on the analytical tasks of scholar data such as author name disambiguation [30], paper importance modeling [28], and entity-based distinctive summarization [26]. However, this work focuses on ad-hoc document retrieval and ranking in academic search. The most relevant work to ours is [38] in which entity embeddings are used to obtain "soft match" feature of each query, document pair. However, [38] requires training data to combine word-based and entity-based relevance scores and to select parameter settings, which is rather different from our unsupervised approach.
2.2 Entity-aware Ranking Model
Entities, such as people, locations, or abstract concepts, are natural units for organizing and retrieving information [10]. Previous studies found that over 70% of Bing's query and more than 50% of traffic in Semantic Scholar are related to entities [12, 38]. The recent availability of large-scale knowledge repositories and accurate entity linking tools have further motivated a growing body of work on entity-aware ranking models. These models can be roughly categorized into three classes: expansion-based, projection-based, and representation-based.
The expansion-based methods use entity descriptions from knowledge repositories to enhance query representation. Xu et al. [39] use entity descriptions in Wikipedia as pseudo relevance feedback corpus to obtain cleaner expansion terms; Xiong and Callen [35] utilize the description of Freebase entities related to the query for query expansion; Dalton et al. [7] expand a query using the text fields of the attributes of the query-related entities and generate richer learning-to-rank features based on the expanded texts.
The projection-based methods try to project both query and document onto an entity space for comparison. Liu and Fang [19] use entities from a query and its related documents to construct a latent entity space and then connect the query and documents based on the descriptions of the latent entities. Xiong and Callen [34] use the textual features among query, entities, and documents to model the query-entity and entity-document connections. These additional

566

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

connections between query and document are then utilized in a learning-to-rank model. A fundamental difference of our work from the above methods is that we do not represent query and document using external terms/entities that they do not contain. This is to avoid adding noisy expansion of terms/entities that may not reflect the information need in the original user query.
The representation-based methods, as a recent trend for utilizing entity information, aim to build entity-enhanced text representation and combine it with traditional word-based representation [37]. Xiong et al. [36] propose a bag-of-entities representation and demonstrated its effectiveness for vector space model. Raviv et al. [25] leverage the surface names of entities to build an entity-based language model. Many supervised ranking models are proposed to apply learning-to-rank methods for combining entity-based signals with word-based signals. For example, ESR [38] uses entity embeddings to compute entity-based query-document matching score and then combines it with word-based score using RankSVM. Following the same spirit, Xiong et al. [37] propose a word-entity duet framework that simultaneously models the entity annotation uncertainty and trains the ranking model. Comparing with the above methods, we also use the bag-of-entity representation but combine it with word-based representation in an unsupervised way. Also, to the best of our knowledge, we are the first to capture entity relation and type information in an unsupervised entity-aware ranking model.
2.3 Automatic Ranking Model Selection
Most ranking models need to manually set many parameter values. To automate the process of selecting parameter settings, some AutoML methods [3, 8] are proposed. Nevertheless, these methods still require a validation set which contains queries with labeled documents. In this paper, we develop an unsupervised model selection algorithm, based on rank aggregation, to automatically choose parameter settings without resorting to a labeled validation set. Rank aggregation aims to combine multiple existing rankings into a joint ranking. Fox and Shaw [9] propose some deterministic functions to combine rankings heuristically. Klementiev et al. [17, 18] propose an unsupervised learning algorithm for rank aggregation based on a linear combination of ranking functions. Another related line of work is to model rankings using a statistical model (e.g., Plackett-Luce model) and aggregate them based on statistical inference [11, 21, 41]. Lately, Bhowmik and Ghosh [2] propose to use object attributes to augment some standard rank aggregation framework. Compared with these methods, our proposed algorithm goes beyond just combining multiple rankings and uses aggregated ranking to guide the selection of parameter settings.
3 RANKING FRAMEWORK
This section presents our unsupervised ranking framework for leveraging entity (set) information in search. Our framework provides a principled way to rank a set of documents D for a query q. In this framework, we represent each document using standard bag-ofwords and bag-of-entities representations [36, 37] (Section 3.1) and represent the query using a novel heterogeneous graph (Section 3.2) which naturally model the entity set information. Finally, we model the query-document matching as a "graph covering" process, as described in Section 3.3.

Field

Raw Text

Title

Playing Atari with Deep Reinforcement Learning

Abstract

... learn control policies directly from sensory input using reinforcement learning (RL) ... can apply our RL method to 7 Atari video games ...

/m/0xwj /m/0hjlw BoE in title field

playing atari with deep

reinforcement

learning

BoW in title field

learn control policies directly from

sensory

input

using

reinforcement

learning rl we apply our rl

/m/0h3wrl9 /m/0hjlw

/m/0xwj

/m/020mfr

method to atari video games

BoE in abstract field

BoW in abstract field

p(e|di,j )

p(w|di,j )

(smoothed) Entity Language Model in abstract field (smoothed) Word Language Model in abstract field
Figure 1: An illustrative example showing one document comprised of two fields (i.e., title, abstract) with their corresponding bagof-words and bag-of-entities representations.

3.1 Document Representation
We represent each document using both word and entity information. For words, we use standard bag-of-words representation and treat each unigram as a word. For entities, we adopt an entity linking tool (details described in Section 5.2) that utilizes a knowledge base/graph (e.g., Wikidata or Freebase) where entities have unique IDs. Given an input text, this tool will find the entity mentions (i.e., entity surface names) in the text and link each of them to a disambiguated entity in the knowledge base/graph. For example, given the input document title "Training linear SVMs in linear time", this tool will link the entity mention "SVMs"' to the entity "Support Vector Machine" with Freebase id `/m/0hc2f'. Previous studies [25, 36] show that when the entity linking error is within a reasonable range, the returned entity annotations, though noisy, can still improve the overall search performance, partially due to the following:
(1) Polysemy resolution. Different entities with the same surface name will be resolved by the entity linker. For example, the fruit "Apple" (with id `/m/014j1m') will be disambiguated with the company "Apple" (with id `/m/0k8z').
(2) Synonymy resolution. Different entity surface names corresponding to the same entity will be identified and merged. For example, the entity "United States of America" (with id `/m/09c7w0') can have different surface names including "USA", "United States", and "U.S." [25]. The entity linker can map all these surface names to the same entity.
After linking all the entity mentions in a document to entities in the knowledge base, we can obtain the bag-of-entities representation of this document. Then, we fit two language models (LMs) for this document: one being word-based (i.e., traditional unigram LM) and the other being entity-based. Notice that in the literature search scenario, documents (i.e., papers) usually contain multiple fields, such as title, abstract, and full text. We model each document field using a separate bag-of-words representation and a separate bag-of-entities representation, as shown in Figure 1.

567

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

To exploit such intra-document structures, we generally assume
a document di has k fields di = {di,1, . . . , di,k } and thus the document collection can be separated into k parts: {D1, . . . , Dk }. Following [23], we assign each field a weight j and formulate the generation process of a token t given the document di as follows:

k
p (t |di ) = p (t |di, j )p (di, j |di ),
j =1

p (di, j |di ) =

j
k j  =1

j

.

(1)

Notice the a token t can be either a unigram w or an entity e, and the field weight j can be either manually set based on prior knowledge or automatically learned using the mechanism described in Sec-
tion 4. The token generation probability under each document field p(t |di, j ) can be obtained from the maximum likelihood estimate with Dirichlet prior smoothing [40] as follows:

p (t |di, j )

=

nt,di, j

+ µj

nt , D j LDj

Ldi,j + µj

,

(2)

where nt,di,j and Ldi,j represent the number of token t in di, j and the length of di, j . Similarly, we can define nt,Dj and LDj . Finally, µj is a scale parameter of the Dirichlet distribution for field j. A
concrete example is shown in Figure 1.

3.2 Query Representation
Given an input query q, we first apply the same entity linker used for document representation to extract all the entity information
in the query. Then, we design a novel heterogeneous graph to represent this query q, denoted as Gq . Such a graph representation captures both word and entity information in the query and models
the entity relations. A concrete example is shown in Figure 2. Node representation. In this heterogeneous query graph, each node represents a query token. As a token can be either a word or
an entity, there are two different types of nodes in this graph. Edge representation. We use an edge to represent a latent relation between two query tokens. In this work, we consider two types
of latent relations: word-word relation and entity-entity relation.
For word-word relation, we add an edge for each pair of adjacent
word tokens with equal weight 1. For instance, given an query "Atari video games", we will add two edges, one between word pairs Atari, video and the other between video, game. On the entity side, we aim to emphasize all the possible entity-entity relations,
and thus add an edge between each pair of entity tokens. Modeling entity type. The type information of each query entity can further reveal the user's information need. Therefore, we assign
the weight of each entity-entity relation based on these two entities'
type information. Intuitively, if the types of two entities are distant
from each other in a type hierarchy, then the relation between these
two entities should have a larger weight. A similar idea is exploited
in [10] and found useful for type-aware entity retrieval. Mathematically, we use e to denote the type of entity e; use
LCAu,v to denote the Lowest Common Ancestor (LCA) of two nodes u and v in a given tree (i.e., type hierarchy), and use l (u, v) to denote the length of a path between node u and node v. In Figure 2, for example, the entity tokens `/m/0hjlw' and `/m/0xwj', corresponding to "reinforcement learning" and "Atari", have types `education.field_of_study' and `computer.game', respectively. The Lowest Common Ancestor of these two types in the type hierarchy is `Thing'. Finally, we define the relation strength between entity

Thing

Computer

Business

Type hierarchy obtained from knowledge base
Education ...

Game

Algorithm

Industry Field of Study Department

/m/0xwj

/m/020mfr

/m/0hjlw

/m/01hyh_

Query

Play Atari video games using reinforcement learning and machine learning.

Query with linked entity mentions

Word

Entity

word-word relation entity-entity relation

1

1 atari

video 1 game

play

/m/0xwj

3 3

/m/0hjlw 3

learning 1

1 /m/01hyh_

3 /m/020mfr
3

1 using
1

reinforcement

machine

1

learning

1

and

1

Heterogeneous graph representation of query

Figure 2: An illustrative example showing the heterogeneous graph representation of one query. Word-word relations are marked by dash lines and entity-entity relations are marked by solid lines. Different solid line colors represent different relation strengths based on two entities' types.

e1 and entity e2 as follows:

LC Ae1,e2 = LC A(e1, e2 ),

(3)

e1,e2 = 1 + max l (e1, LC Ae1,e2 ), l (e2, LC Ae1,e2 ) . (4)

Our proposed heterogeneous query graph representation is general

and can be extended. For example, we can apply dependency parsing

for verbose queries, and only add an edge between two word tokens

that have direct dependency relation. Also, if the importance of each

entity-entity relation is given, we can then set the edge weights

correspondingly. We leave these extensions for future works.

3.3 Document Ranking using Query Graph

Our proposed heterogeneous query graph Gq represents all information need in the user-issued query. Such need can be either to

find document discussing one particular entity or to identify papers

studying an important inter-entity relation. Intuitively, a document

that can satisfy more information need should be ranked at a higher

position. To quantify such information need that is explained by a

document, we define the following graph covering process.
Query graph covering. If a query token t  q exists in a document di , we say di covers the node in Gq that corresponds to this token. Similarly, if a pair of query tokens t1 and t2 exists in di , we say di covers the edge in Gq that corresponds to the relation of this token pair t1, t2. The subgraph of Gq that is covered by the document di , denoted as Gq |di , represents the information need in the query q that is explained by the document di .
Furthermore, we follow the same spirit of [22] and view the
subgraph Gq |di as a Markov Network, based on which we define the joint probability of the document di and the query q as follows:

P (di, q)

d=ef

1 Z

 (c ) ra=nk

log  (c ) ra=nk

f (c ),

c Gq |di

c Gq |di

c Gq |di

(5)

where Z is a normalization factor, c indexes the cliques in graph, and

 (c) is the non-negative potential defined on c. The last equation

holds as we let  (c) = exp[f (c)]. Notice that if Gq |d1 is a subgraph of Gq |d2 which means document d1 covers less information than document d2 does, we should have P (d1, q) < P (d2, q). Therefore,
we should design f (·) to satisfy the constraint f (c) > 0, c.

In this work, we focus on modeling each single entity and pairwise relations between two entities. Therefore, each clique c can

be either a node or an edge in the graph. Modeling higher-order

568

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

relations among more than two entities (i.e., cliques with size larger
than 2) is left for future work. We define the potential functions for
a single node and an edge as follows:
Node potential. Node potential quantifies the information need contained in a single node t, which can be either a word token w or an entity token e. To balance the relative weight of a word token and an entity token, we introduce a parameter E  [0, 1], and define the node potential function f (·) as follows:

f (t ) =

E · a(P (t |di )) (1 - E ) · a(P (t |di ))

if token t is an entity token if token t is a word token

(6)

where a(·) is an activation function that transforms a raw probability to a node potential. Here, we set a(x ) = x in order to amplify P (t |di ) which has a relatively small value.

Edge potential. Edge potential quantifies the information need contained in an edge t1, t2 that can be either a word-word (W-W)
relation or and an entity-entity (E-E) relation. In our query graph

representation, all word-word relations have an equal weight of 1, and the weight of each entity-entity relation (i.e., e1,e2 ) is defined by Equation (3). Finally, we calculate the edge potential as follows:

f (t1, t2) = t1,t2 · a (P (t1, t2 |di )),

(7)

t1, t2 =

E · e1,e2 (1 - E )

if t1, t2 is an E-E relation if t1, t2 is a W-W relation

(8)

where t1,t2 measures the edge importance, and a(·) is the same

activation function as defined above. To simplify the calculation of

P (t1, t2|di ), we make an assumption that two tokens t1 and t2 are conditionally independent given a document di . Then, we replace P (t1, t2|di ) with P (t1|di )P (t2|di ) and substitute it in Equation (7).

Putting all together. After defining the node and edge potentials,
we can calculate the joint probability of each document di and query q using Equation (5) as follows:

P (di, q) = (1 - E )

1+

a (P (w |di )) a (P (w |di ))

w Gq |di

w,w Gq |di

+ E

1+

e,e · a(P (e |di )) a(P (e |di )).

e Gq |di

e, eGq |di

(9)

As shown in the above equation, SetRank will explicitly reward

paper capturing inter-entity relations and covering more unique
entities. Also, it uses E to balance the word-based relevance with entity-based relevance, and models entity type information in e,e.

4 UNSUPERVISED MODEL SELECTION
Although being an unsupervised ranking framework, SetRank still has some parameters that need to be appropriately set by ranking model designers, including the weight of title/abstract field and the relative importance of entity token E . Previous study [40] shows that these model parameters have significant influences on the ranking performance and thus we need to choose them carefully. Typically, these parameters are chosen to optimize the performance over a validation set that is manually constructed and contains the relevance label of each query-document pair. Though being useful, the validation set is not always available, especially for those applications (e.g., literature search) where labeling document requires domain expertise.

To address the above problem, we propose an unsupervised
model selection algorithm which automatically chooses the param-
eter settings without resorting to a manually labeled validation set.
The key philosophy is that although people who design the ranking
model do not know the exact "optimal" parameter settings, they
do have prior knowledge about the reasonable range for each of
them. For example, the title field weight should be set larger than the abstract field weight, and the entity token weight E should be set small if the returned entity linking results are noisy. Our model
selection algorithm leverages such prior knowledge by letting the
ranking model designer input the search range of each parameter's
value. It will then return the best value for each parameter within
its corresponding search range. We first describe our notations and
formulate our problem in Section 4.1. Then, we present our model
selection algorithm in Section 4.2.
4.1 Notations and Problem Formulation
Notations. We use SK to denote the collection of rankings over a set of K documents: D = {d1, . . . , dk , . . . , dK }, k  [K] = {1, . . . , K }. We denote by  : [K]  [K] a complete ranking, where  (k ) denotes the position of document dk in the ranking, and  -1 (j) is the index of the document on position j. For example, given the ranking: d3  d1  d2  d4, we will have  = [2, 3, 1, 4] and  -1 = (3, 1, 2, 4). Furthermore, we use the symbol  (instead of  ) to denote an incomplete ranking which includes only some of the documents in D. If document dk does not occur in the ranking, we set  (k ) = 0, otherwise,  (k ) is the rank of document dk . In the corresponding  -1, those missing documents simply do not occur. For example, given the ranking: d4  d2  d1, we have  = [3, 2, 0, 1] and  -1 = (4, 2, 1). Finally, we let I ( )to represent the index of documents that appear in the ranking list  . Problem Formulation. Given a parameterized ranking model M where  denotes the set of all parameters (e.g., {k, b} in BM25, {µ} in query likelihood model with dirichlet prior smoothing), we want to find the best parameter settings   such that the ranking model M  achieves the best ranking performance over the space Q of all queries. In practice, however, the space consisting of all possible values of  can be infinite and we cannot access all queries in Q. Therefore, we assume ranking model designers will input p possible sets of parameter values:  = {1, . . . , p } and a finite subset of queries Q  Q. Finally, we formulate our problem of unsupervised model selection as follows:
Definition 1. (PROBLEM FORMULATION). Given a parameterized ranking model M , p candidate parameter settings , and an unlabeled query subset Q, we aim to find     such that M  achieves the best ranking performance over Q.
4.2 Model Selection Algorithm
Our framework measures the goodness of each parameter settings i   based on its induced ranking model Mi . The key challenge here is how we can evaluate the ranking performance of each Mi over a query q which has no labeled documents. To address this challenge, we first leverage a weighted rank aggregation technique
to obtain an aggregated rank list and then evaluate the quality of each Mi based on the agreement between its generated rank list and the aggregated rank list. The key intuition here is that high-
quality ranking models will rank documents based on a similar

569

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Algorithm 1: Unsupervised Model Selection.

Input: A parameterized ranking model M , p candidate parameter

settings  = {1, · · · , p }, and an unlabeled query subset Q . Output: The best ranking model M  with    .

1 set scor e (M1 ) = scor e (M2 ) = · · · = scor e (Mp ) = 0;

2 for query q  Q do

3

set 1

=

2

=

·

· · p

=

1 p

;

4

set  pr ev = N one ;

5 while True do

6

// Weighted Rank Aggregation ;

7

for document index j from 1 to |D | do

8

scor e (dj ) = 0;

9

for rank list index i from 1 to p do

10

if j  I (i ) (i.e., dj appears in i ) then

11

scor e (dj ) = scor e (dj ) + i ( |i | + 1 - i (dj ));

12

 = argsort(scor e (d1), · · · , scor e (d|D | ));

13

// Confidence Score Adjustment ;

14

for rank list index i from 1 to p do

15

i =

exp(-d is t (i || )) i exp(-d i s t (i ||

)

)

;

16

// Convergence Check ;

17

if  ==  pr ev then

18

Break;

19

else

20

prev  ;

21 for rank list index i from 1 to p do

22

score (Mi ) = score (Mi ) + i ;

23 M  = arg max  scor e (M );

24 Return M  ;

distribution while low-quality ranking models will rank documents

in a uniformly random fashion. Therefore, the agreement between

each rank list with the aggregated rank list serves as a good signal

of its quality.

At a high level, our model selection method is an iterative algo-

rithm which repeatedly aggregates multiple rankings (with their

corresponding weights) and uses the aggregated rank list to esti-

mate the quality of each of them. Given a query q, we first con-

struct p ranking models Mi , i  [1, . . . , p], one for each parameter settings i   and obtain its returned top-k rank list i over a doc-

ument set Di (i.e., |Di | = k). Then, we construct a unified document

pool D =

p i =1

Di .

After

that,

we

use

i

to

denote

the

confidence

score of each ranking model Mi , and initialize all of them with

equal

value

1 p

.

During

each

iteration,

we

first

aggregate

{1,

.

.

.

,

p

},

weighted by {1, . . . , p }, and obtain the aggregated rank list  .

Then, we adjust the confidence score of each ranking model Mi (i.e., i ) based on the distance of two rankings: i and  . Here, we

use  to denote the aggregated rank list because it is a complete

ranking over the document pool D.

Weighted Rank Aggregation. We aggregate multiple rank lists using a variant of Borda counting method [6] which considers the

relative weight of each rank list. We calculate the score of each

document based on its position in each rank list as follows:

p

scor e (dj ) = i |i | + 1 - i (dj ) 1{j  I (i ) },

(10)

i =1

where |i | denotes the length of a rank list i , and 1{x } is an indicator function. When document dj appears in the rank list i , 1{j  I (i )}

title abs E

1 10 2 10

5 0.5 3 0.7

...

... ......

p 15 5 0.7

M1 1 d3 d1 d2

KT (1k) = 1

1

M2 2 d1
.

d3
.

d4

Aggregated Rank List
2  d1 d3 d2 d4

. .

. .

p

d p

Mp

1

d2

d3

posKT (pk)

=

1 log(1 +

2)

1 log(1 + 3)

Figure 3: An illustrative example showing the process of weighted rank aggregation and the calculation of two different ranking distances (i.e., KT and pos KT ).

equals to 1, otherwise, it equals to 0. The above equation will reward
a document ranked at higher position (i.e., small i (dj )) in a highquality rank list (i.e., large i ) a larger score. Finally, we obtain the aggregated rank of these documents based on their corresponding
scores. A concrete example in shown in Figure 3.
Confidence Score Adjustment. After we obtain the aggregated rank list, we will need to adjust the confidence score i of each ranking model Mi based on the distance between i and aggregated rank list  . In order to compare the distance between an incomplete rank list i with a complete rank list  , we extend the classical Kendall Tau distance [16] and define it as follows:

KT (i | | ) =

1{ (a) >  (b ) }.

(11)

i (a)<i (b ) a,b I (i )

The above distance counts the number of pairwise disagreements between i and  . One limitation of this distance is that it does not differentiate the importance of different ranking positions. Usually,
switching two documents in the top part of a rank list should be
penalized more, compared with switching another two documents
in the bottom part of a rank list. To model such intuition, we propose a position-aware Kendall Tau distance and define it as follows:

posKT (i | | ) = i (a)<i (b ) a,bI (i )

1

1

-

log2 (1 +  (b )) log2 (1 +  (a))

1{ (a) >  (b ) }. (12)

With the distance between two rankings defined, we can adjust the confidence score as follows:

i =

exp(-dist (i | | i exp(-dist (i |

)) |

))

,

(13)

where dist (i || ) can be either KT (i || ) or posKT (i || ) and we will study how different this choice can influence the model selec-

tion results in Section 5.4. The key idea of the above equation is

to promote the ranking model which returns a ranked list better

aligned with the aggregated rank list.

Putting all together. Algorithm 1 summarizes our unsupervised model selection process. Given a query q  Q, we can iteratively
apply weighted rank aggregation and confidence score adjustment

until the algorithm converges. Then, we collect the converged
{^1, . . . , ^p }. Specifically, ^i is the confidence score of ranking model Mi on query q. With a slight abuse of notation, we use score (Mi ) to denote its accumulated confidence score. Given a set of queries Q, we run the former procedure for each query and sum over all converged ^i . Finally, we return the ranking model M  which has the largest accumulated confidence score.

570

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

5 EXPERIMENTS
In this section, we evaluate our proposed SetRank framework as well as unsupervised model selection algorithm on two datasets from two scientific domains.
5.1 Datasets
We use two benchmark datasets for the experiments: Semantic Scholar [38] in Computer Science (S2-CS) and TREC 2004&2005 Genomics Track in Biomedical science (TREC-BIO). S2-CS contains 100 queries sampled from Semantic Scholar's query log, in which 40 queries are entity-set queries and the maximum number of entities in a query is 5. Candidate documents are generated by pooling from variations of Semantic Scholar's online production system and all of them are manually labeled on a 5-level scale. Entities in both queries and documents are linked to Freebase using CMNS [13]. As the original dataset does not contain the entity type information, we enhance it by retrieving each entity's most notable type in the latest Freebase dump3 based on its Freebase ID. These types are organized by Freebase type hierarchy. TREC-BIO includes 100 queries designed by biologists and the candidate document pool is constructed based on the top results of all submissions at that time. All candidate documents are labeled on a 3-level scale. In these 100 queries, 86 of them are entity-set queries and the maximum number of entities in a query is 11. The original dataset contains no entity information and therefore we apply PubTator [32], the state-of-the-art biomedical entity linking tool, to obtain 5 types of entities (i.e., Gene, Disease, Chemical, Mutation, and Species) in both queries and documents. We build a simple type hierarchy with root node named `Thing' and each first-level node corresponds to one of the above 5 types.
5.2 Entity Linking Performance
We evaluate the query entity linking using precision and recall at the query level. Specifically, an entity annotation is considered correct if it appears in the gold labeled data (i.e., the strict evaluation in [4]). The original S2-CS dataset provides such gold labeled data. For TREC-BIO dataset, we asked two Master-level students with biomedical science background to label all the linked entities as well as the entities that they could identify in the queries. We also report the entity linking performance on the general domain queries (ClueWeb09 and ClueWeb12) for references [36]. As we can see in Table 2, the overall linking performance of academic queries is better than that of general domain queries, probably because academic queries have less ambiguity. Also, recall of entity linking in TREC-BIO dataset is very high. A possible reason is that the biomedical entities have very distinctive tokens (e.g., "narcolepsy" is a specific disease related to sleep and is seldom used in other contexts) and thus it is relatively easier to recognize them.
5.3 Ranking Performance
5.3.1 Experimental Setup. Evaluation metrics. Since documents in both datasets have multilevel graded relevance, we use NDCG@{5,10,15,20} as our main evaluation metrics. All evaluation is performed using standard
3 https://developers.google.com/freebase/

Table 2: Entity linking performance on scientific domain queries (S2-CS, TREC-BIO) and general domain queries (ClueWeb09, ClueWeb12).

Precision Recall

S2-CS TREC-BIO 0.680 0.678 0.680 0.727

ClueWeb09 ClueWeb12

0.577

0.485

0.596

0.575

pytrec_eval tool [31]. Statistical significances are tested using twotailed t-test with p-value  0.05. Baselines. We compare SetRank with 4 baseline ranking models: Vector Space Model (BM25 [27]), Query Likelihood Model with
Dirichlet Prior smoothing (LM-DIR) or with Jelinek Mercer smooth-
ing (LM-JM) [40], and the Information-Based model (IB) [5]. All
models are applied to the paper's title and abstract fields. Here, we do not compete with Semantic Scholar's production system and ESR model [38] because they are supervised models trained over
user's click information which is not available in our setting.
The parameters of all models, including the field weights, are set
using 5-fold cross validation over the queries in each benchmark
dataset using the same paradigm in [25] as follows. For each hold-
out fold, the other four folds are served as a validation set. A grid
search is applied to choose the optimal parameter settings that
maximize NDCG@20 on the validation set. Specifically, the title
and abstract field weights are selected from {1,5,10,15,20,50}; the Dirichlet smoothing parameter µ and Jelinek Mercer smoothing parameter  are chosen from {500, 1000, 1500, 2000, 2500, 3000} and {0.1, 0.2, . . . , 0.9}, respectively; the relative weight of entity token E used in SetRank is selected from {0, 0.1, . . . , 1}. The best performing parameter settings are then saved for the hold-out evaluation.

5.3.2 Effectiveness of Leveraging Entity Information. As mentioned before, the entity linking process is not perfect and it generates some noisy entity annotations. Therefore, we first study how different ranking models, including our proposed SetRank, can leverage such noisy entity information to improve the ranking performance. We evaluate three variations of each model ­ one using only word information, one using only entity information, and one using both pieces of information. Results are shown in Table 3. We notice that the usefulness of entity information is inconclusive for baseline models. On S2CS dataset, adding entity information can improve the ranking performance, while on TREC-BIO dataset, it will drag down the performance of all baseline methods. This resonates with previous findings in [15] that simply adding entities into queries and posting them to existing ranking models does not work for biomedical literature retrieval. Compared with baseline methods, SetRank successfully combines the word and entity information and effectively leverages such noisy entity information to improve the ranking performance. Furthermore, SetRank can better utilize each single information source, either word or entity, than other baseline models thanks to our proposed query graph representation. Overall, SetRank significantly outperforms all variations of baseline models.

5.3.3 Ranking Performance on Entity-Set Queries. We further study each model's ranking performance on entity-set queries. There are 40 and 86 entity-set queries in S2-CS and TRECBIO, respectively. We denote these subsets of entity-set queries as S2-CS-ESQ and TREC-BIO-ESQ. As shown in Table 4, SetRank significantly outperforms the best variation of all baseline methods

571

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Effectiveness of leveraging (noisy) entity information for ranking. Each method contains three variations and the best variation is labeled bold. The superscript "" means the model significantly outperforms the best variation of all 4 baseline methods (with p-value  0.05).

Dataset Method

S2-CS

NDCG@5 NDCG@10 NDCG@15 NDCG@20

TREC-BIO

NDCG@5 NDCG@10 NDCG@15 NDCG@20

Word
0.3476 0.3785 0.4001 0.4126
0.3189 0.2968 0.2833 0.2739

BM25 Entity 0.3319 0.3520 0.3616 0.3752
0.1542 0.1488 0.1424 0.1419

Both 0.3675 0.4039 0.4160 0.4333
0.2613 0.2472 0.2395 0.2337

LM-DIR Word Entity Both

0.3447 0.3623 0.3781 0.4012

0.3460 0.3579 0.3673 0.3816

0.3563 0.3901 0.4077 0.4205

0.3053 0.2958 0.2852 0.2781

0.1755 0.1601 0.1579 0.1558

0.2669 0.2571 0.2591 0.2547

LM-JM Word Entity

0.3626 0.3394 0.3774 0.3519

0.4051 0.3666

0.4182 0.3804

0.2957 0.2742 0.2642 0.2560

0.1656 0.1588 0.1575 0.1534

Both
0.3625 0.3962 0.4174 0.4362
0.2826 0.2572 0.2437 0.2362

Word 0.3759 0.3903
0.4113
0.4295
0.3045 0.2918 0.2835 0.2722

IB Entity 0.3420 0.3557 0.3699 0.3855
0.1842 0.1715 0.1664 0.1628

Both
0.3729 0.4009 0.4272 0.4421
0.2770 0.2633 0.2541 0.2406

Word 0.3890 0.4168 0.4411 0.4674
0.3417 0.3165 0.3017 0.2900

SetRank

Entity Both

0.3761 0.3885 0.4054 0.4229

0.4207 0.4431 0.4762 0.4950

0.2111 0.1976 0.1931 0.1885

0.3744 0.3522 0.3363 0.3246

Table 4: Ranking performance on entity-set queries. The best variation of each baseline method is selected. The superscript "" means the model significantly outperforms all 4 baseline methods (with p-value  0.05).

Dataset Metric

S2-CS -ESQ

NDCG@5 NDCG@10 NDCG@15 NDCG@20

TREC-BIO -ESQ

NDCG@5 NDCG@10 NDCG@15 NDCG@20

BM25
0.3994 0.4364 0.4454 0.4609
0.3185 0.2968 0.2812 0.2718

LM-DIR
0.3522 0.3973 0.4160 0.4264
0.2934 0.2834 0.2711 0.2644

LM-JM
0.3812 0.4241 0.4431 0.4618
0.2940 0.2746 0.2636 0.2553

IB
0.3956 0.4209 0.4496 0.4664
0.3011 0.2896 0.2832 0.2708

SetRank
0.4983 0.5130 0.5450 0.5629
0.3639 0.3406 0.3251 0.3132

on S2-CS-ESQ and TREC-BIO-ESQ by at least 25% and 14% respectively in terms of NDCG@5. Also, we can see the advantages of SetRank over the baselines on entity-set queries are larger than those on general queries, This further demonstrates SetRank's effectiveness of modeling entity set information.
5.3.4 Effectiveness of Modeling Entity Relation and Entity Type. To study how the inter-entity relation and entity type information can contribute to document ranking, we compare SetRank with two of its variants, SetRank-t and SetRank-ts . The first variant models entity relation among the set but ignores the entity type
information, and the second variant simply neglects both entity
relation and type. Results are shown in Table 5. First, we compare SetRank-t with
SetRank-ts and find that modeling the entity relation in entity sets can significantly improve the ranking results. Such improvement is especially obvious on the entity-set query sets S2-CS-ESQ and TREC-BIO-ESQ. Also, by comparing SetRank with SetRank-t , we can see adding entity type information can further improve
ranking performance. In addition, we present a concrete case study
for one entity-set query in Table 6. The top-2 papers returned by SetRank-ts are focusing on video game without discussing its relation with reinforcement learning. In comparison, SetRank considers the entity relations and returns the paper mentioning both entities.
5.3.5 Analysis of Entity Token Weight E . We introduce the entity token weight E in Eq. (6) to combine the entity-based and word-based relevance scores. In all previous
experiments, we choose its value using cross validation. Here, we
study how this parameter will influence the ranking performance by constructing multiple SetRank models with different E and directly report their performance on all 100 queries.
As shown in Figure 4, for S2-CS dataset, SetRank's ranking performance first increases as E increases until it reaches 0.7 and then starts to decrease when we further increase E . However, for

Table 5: Ranking performance of different variations of SetRank.

Best results are marked bold. The superscript "" means the model significantly outperforms SetRank-ts (with p-value  0.05).

Dataset S2-CS

Metric
NDCG@5 NDCG@10 NDCG@15 NDCG@20

SetRank-t s 0.3847 0.4095 0.4256 0.4443

SetRank-t 0.4157 0.4423 0.4655 0.4813

SetRank
0.4207 0.4431 0.4762 0.4950

TREC-BIO

NDCG@5 NDCG@10 NDCG@15 NDCG@20

S2-CS -ESQ

NDCG@5 NDCG@10 NDCG@15 NDCG@20

TREC-BIO -ESQ

NDCG@5 NDCG@10 NDCG@15 NDCG@20

0.3414 0.3257 0.3140 0.3058
0.4059 0.4311 0.4469 0.4683
0.3257 0.3100 0.2994 0.2903

0.3705
0.3500
0.3335
0.3217 0.4800 0.5004 0.5266 0.5378
0.3594 0.3380 0.3219 0.3100

0.3744 0.3522 0.3363
0.3246
0.4983 0.5130 0.5450 0.5629
0.3639 0.3406 0.3251 0.3132

0.46 0.44

S2-CS TREC-BIO

0.42

NDCG@10

0.4

0.38

0.36

0.34

0.32

0.3

0.28

0

0.2

0.4

0.6

0.8

1

E

Figure 4: Sensitivity of E in S2-CS and TREC-BIO datasets.

TREC-BIO dataset, the optimal value of E is around 0.3, and if we increases E over 0.6, the ranking performance will drop quickly.

5.4 Effectiveness of Model Selection
5.4.1 Experimental Setup. In this experiment, we try to apply our unsupervised model selection algorithm to choose the best parameter settings of SetRank without using a validation set. We select entity token weight E , title field weight title , abstract field weight abs , dirichlet smoothing factors for both fields µtitle & µabs from {0.2, 0.3, . . . , 0.8}, {5, 10, 15, 20}, {1, 3, 5, 10}, and {500, 1000, 1500, 2000}, respectively. This generates totally 7 × 4 × 4 × 4 × 4 = 1, 792 possible parameter settings and for each of them we can construct a ranking model.
We first apply our unsupervised model selection algorithm (with either KT or posKT as the ranking distance) and obtain the most confident parameter settings returned by it. Then, we plug in these parameter settings into SetRank and denote it as AutoSetRank. For reference, we also calculate the average performance of all 1,792
ranking models.

572

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 6: A case study comparing SetRank with SetRank-ts on one entity-set query in S2-CS. Note: Atari is a video game platform.

Query Method
1 2 3

SetRank-t s

reinforcement learning for video game

SetRank

The effects of video game playing on attention, memory, and executive control

A video game description language for model-based or interactive learning

Can training in a real-time strategy video game attenuate cognitive decline in older adults?

Playing Atari with Deep Reinforcement Learning

A video game description language for model-based or interactive learning

Real-time neuroevolution in the NERO video game

Table 7: Effectiveness of ranking model selection. SetRank-V S : parameters are tuned using 5-fold cross validation. AutoSetRank-(KT /posKT ): parameters are obtained based on our unsupervised model selection algorithm, which uses either KT or pos KT as ranking distance. Mean (±

Std): the averaged performance of all ranking models with standard derivation shown.

Dataset S2-CS

Method
SetRank-V S AutoSetRank-KT AutoSetRank-posKT
Mean (± Std)

title abs E µtitle µabs

NDCG@5

20 5 0.7 1000 1000

0.4207

20 7 0.7 1500 2000

0.4174

20 5 0.7 1500 1500

0.4173

­ ­ ­ ­ ­ 0.3898 (± 0.0112)

NDCG@10
0.4431 0.4427 0.4436 0.4128 (± 0.0106)

NDCG@15
0.4762 0.4730 0.4731 0.4411 (± 0.0161 )

NDCG@20
0.4950 0.4929 0.4923 0.4543 (± 0.0163)

SetRank-V S

20 5 0.2 1000 1000

0.3744

0.3522

0.3363

0.3246

TREC-BIO

AutoSetRank-KT AutoSetRank-posKT

20 20

5 0.2 1500 1000 7 0.2 1000 1000

0.3692 0.3748

0.3472 0.3564

0.3305 0.3367

0.3173 0.3253

Mean (± Std)

­ ­ ­ ­ ­ 0.3479 (± 0.0103) 0.3238 (± 0.0079) 0.3199 (± 0.0079) 0.3036 (± 0.0093)

5.4.2 Experimental Result and Analysis. Table 7 shows the results, including the SetRank's performance when a labeled validation set is given. First, we notice that for S2CS dataset, although the parameter settings tuned over validation set do perform better than the ones returned by our unsupervised model selection algorithm, the difference is not significant. For TREC-BIO dataset, it is surprising to find that AutoSetRank-posKT can slightly outperforms SetRank tuned on validation set. Furthermore, the performance of AutoSetRank function is higher than the average performances of all possible ranking models by 2 standard deviations, which demonstrates the effectiveness of our unsupervised model selection algorithm.
5.5 Use Case Study: Bio-Literature Search
In this section we demonstrate the effectiveness of SetRank in a biomedical use case. As preparation, we build a biomedical literature search engine based on over 27 million papers retrieved from PubMed. Entities in all papers are extracted and typed using PubTator. This search system is cold-started with our proposed SetRank model and we show how SetRank can help this search system to accommodate a given entity-set query and returns a highquality rank list of papers relevant to the query. Comparison with PubMed, a widely used search engine for biomedical literature, will also be discussed. A biomedical case. Consider the following case of a biomedical information need. Genomics studies often identify sets of genes as having important roles to play in the processes or conditions under investigation, and the investigators seek to understand better what biological insights such a list of genes might provide. Suppose such a study, having examined brain gene expression patterns in old mice, identifies ten genes as being of potential interest. The investigator forms a query with these 10 genes, submits it to a literature search engine, and examines the top ten returned papers to look for an association between this gene set and a disease. The query consists of symbols of the 10 genes: "APP, APOE, PSEN1, SORL1, PSEN2, ACE, CLU, BDNF, IL1B, MAPT ". Relevance criterion. We choose the above ten genes for our illustration because these are actually top genes associated with Alzheimer's disease according to DisGeNET [24], and it is unlikely

that there is another completely different (and unknown) commonality among them. Therefore, a retrieved paper is relevant if and only if it discusses at least one of the query genes in the context of Alzheimer's disease. Furthermore, among all relevant papers, we prefer those covering more unique genes. Result analysis. The top-5 papers returned by PubMed4 and our system are shown in Table 8. We see that the "Alzheimer's disease" is explicitly mentioned in the title of all the five papers returned by our system, and the top two papers cover 6 unique genes among the total 10 genes. All five papers returned by SetRank are highly relevant, since they all focus on the association between a subset of our query genes and Alzheimer's disease. In contrast, the top-5 papers retrieved by PubMed are dominated by two genes (i.e., APOE4 and BDNF) and contain none of the remaining eight. Only the 1st of the five papers is highly relevant. It focuses on the association between Alzheimer's disease (mentioned explicitly in the title) and our query gene set. Three other papers (ranked 2nd to 4th) are marginally relevant, in the sense that Alzheimer's disease is the context but not the focus of their studies. The paper ranked 5th is irrelevant. Therefore, users will prefer SetRank since it returns papers covering a large-portion of an entity-set query and helps them to find the association between this entity set with Alzheimer's disease.
6 CONCLUSIONS AND FUTURE WORK
In this paper, we study the problem of searching scientific literature using entity-set queries. A distinctive characteristic of entity-set queries is that they reflect user's interest in inter-entity relations. To capture such information need, we propose SetRank, an unsupervised ranking framework which explicitly models entity relations among the entity set. Second, we develop a novel unsupervised model selection algorithm based on weighted rank aggregation to select SetRank's parameters without relying on a labeled validation set. Experimental results on two benchmark datasets corroborate the effectiveness of SetRank and the usefulness of our model selection algorithm. We further discuss the power of SetRankwith a real-world use case of biomedical literature search.
As a future direction, we would like to explore how we can go beyond pairwise entity relations and integrate higher-order
4 Querying PubMed with the exact same query returns 0 document. To get reasonable results,
PubMed users have to insert an OR logic between every pairs of genes, and change the default "sorting by most recent" to "sorting by best match".

573

Session 5B: Entities

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 8: A real-world use case comparing SetRank with PubMed. The input query contains a set of 10 genes and reflects user's information need of finding an association between this gene set and an unknown disease. Entity mentions in returned paper titles are highlighted in brown and the entity mentions of Alzheimer's disease, which are used to judge paper relevance, are marked in red.

Query Method Rank

APP APOE4 PSEN1 SORL1 PSEN2 ACE CLU BDNF IL1B MAPT Paper Title

1 2 PubMed 3 4 5

Apathy and APOE4 are associated with reduced BDNF levels in Alzheimer's disease ApoE4 and A Oligomers Reduce BDNF Expression via HDAC Nuclear Translocation Cognitive deficits and disruption of neurogenesis in a mouse model of apolipoprotein E4 domain interaction APOE-epsilon4 and aging of medial temporal lobe gray matter in healthy adults older than 50 years Influence of BDNF Val66Met on the relationship between physical activity and brain volume

1 2 SetRank 3 4 5

Investigating the role of rare coding variability in Mendelian dementia genes (APP, PSEN1, PSEN2, GRN, MAPT, and PRNP) in late-onset Alzheimer's disease Rare Genetic Variant in SORL1 May Increase Penetrance of Alzheimer's Disease in a Family with Several Generations of APOE- 4 Homozygosity APP, PSEN1, and PSEN2 mutations in early-onset Alzheimer disease: A genetic screening study of familial and sporadic cases Identification and description of three families with familial Alzheimer disease that segregate variants in the SORL1 gene The PSEN1, p.E318G variant increases the risk of Alzheimer's disease in APOE-4 carriers

entity relations into the current SetRank framework. Besides, it would be interesting to explore whether SetRank can effectively model domain expert's prior knowledge about the relative impor-
tance of entity relations. Furthermore, the incorporation of user interaction and and extension of current SetRank framework to weakly-supervised settings are also interesting research problems.
ACKNOWLEDGEMENTS
This research is sponsored in part by U.S. Army Research Lab. under
Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under
Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-
18481, IIS 17-04532, and IIS-17-41317, DTRA HDTRA11810026, and grant
1U54GM114838 awarded by NIGMS through funds provided by the trans-
NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).
REFERENCES
[1] Joran Beel and Bela Gipp. 2009. Google Scholar's ranking algorithm: An Introductory Overview. In ISSI.
[2] Avradeep Bhowmik and Joydeep Ghosh. 2017. LETOR Methods for Unsupervised Rank Aggregation. In WWW.
[3] Pavel Brazdil and Christophe Giraud-Carrier. 2017. Metalearning and Algorithm Selection: progress, state of the art and introduction to the 2018 Special Issue. Machine Learning (2017).
[4] David Carmel, Ming-Wei Chang, Evgeniy Gabrilovich, Bo-June Paul Hsu, and Kuansan Wang. 2014. ERD'14: entity recognition and disambiguation challenge. SIGIR Forum 48 (2014), 63­77.
[5] Stéphane Clinchant and Éric Gaussier. 2010. Information-based models for ad hoc IR. In SIGIR.
[6] Don Coppersmith, Lisa Fleischer, and Atri Rudra. 2006. Ordering by weighted number of wins gives a good ranking for weighted tournaments. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm. Society for Industrial and Applied Mathematics, 776­782.
[7] Jeff Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In SIGIR.
[8] Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, and Frank Hutter. 2015. Efficient and Robust Automated Machine Learning. In NIPS.
[9] Edward A. Fox and Joseph A. Shaw. 1993. Combination of Multiple Searches. In TREC.
[10] Darío Garigliotti and Krisztian Balog. 2017. On Type-Aware Entity Retrieval. In ICTIR.
[11] John Guiver and Edward Snelson. 2009. Bayesian inference for Plackett-Luce ranking models. In ICML.
[12] Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named entity recognition in query. In SIGIR.
[13] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2015. Entity linking in queries: Tasks and evaluation. In Proceedings of the 2015 International Conference on The Theory of Information Retrieval. ACM, 171­180.
[14] William R. Hersh, Aaron Cohen, Jianji Yang, Ravi Teja Bhupatiraju, Phoebe Roberts, and Marti Hearst. 2005. TREC 2005 Genomics Track Overview. In TREC.
[15] Sarvnaz Karimi, Justin Zobel, and Falk Scholer. 2012. Quantifying the impact of concept recognition on biomedical information retrieval. Information Processing & Management 48, 1 (2012), 94­106.
[16] Maurice G Kendall. 1955. Rank correlation methods. (1955).

[17] Alexandre Klementiev, Dan Roth, and Kevin Small. 2007. An Unsupervised Learning Algorithm for Rank Aggregation. In ECML.
[18] Alexandre Klementiev, Dan Roth, and Kevin Small. 2008. A Framework for Unsupervised Rank Aggregation. In SIGIR LR4IR Workshop.
[19] Xitong Liu and Hui Fang. 2015. Latent entity space: a novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18 (2015), 473­503.
[20] Zhiyong Lu. 2011. PubMed and beyond: a survey of web tools for searching biomedical literature. In Database.
[21] Lucas Maystre and Matthias Grossglauser. 2015. Fast and Accurate Inference of Plackett-Luce Models. In NIPS.
[22] Donald Metzler and W Bruce Croft. 2005. A Markov random field model for term dependencies. In SIGIR.
[23] Paul Ogilvie and James P. Callan. 2003. Combining document representations for known-item search. In SIGIR.
[24] Janet Piñero, Àlex Bravo, Núria Queralt-Rosinach, Alba Gutiérrez-Sacristán, Jordi
Deu-Pons, Emilio Centeno, Javier García-García, Ferran Sanz, and Laura I Furlong.
2016. DisGeNET: a comprehensive platform integrating information on human disease-associated genes and variants. Nucleic acids research (2016). [25] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document Retrieval Using Entity-Based Language Models. In SIGIR. [26] Xiang Ren, Jiaming Shen, Meng Qu, Xuan Wang, Zeqiu Wu, Qi Zhu, Meng Jiang,
Fangbo Tao, Saurabh Sinha, David Liem, Peipei Ping, Richard M. Weinshilboum,
and Jiawei Han. 2017. Life-iNet: A Structured Network-Based Knowledge Exploration and Analytics System for Life Sciences. In ACL. [27] Stephen E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends in Information Retrieval (2009).
[28] Jiaming Shen, Zhenyu Song, Shitao Li, Zhaowei Tan, Yuning Mao, Luoyi Fu, Li
Song, and Xinbing Wang. 2016. Modeling Topic-Level Academic Influence in Scientific Literatures. In AAAI Workshop: Scholarly Big Data. [29] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Paul Hsu,
and Kuansan Wang. 2015. An Overview of Microsoft Academic Service (MAS) and Applications. In WWW. [30] Jie Tang, Jing Zhang, Limin Yao, Juan-Zi Li, Li Zhang, and Zhong Su. 2008. ArnetMiner: extraction and mining of academic social networks. In KDD. [31] Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_eval. In SIGIR. ACM. [32] Chih-Hsuan Wei, Hung-Yu Kao, and Zhiyong Lu. 2013. PubTator: a web-based text mining tool for assisting biocuration. In Nucleic Acids Research. [33] Jian Wu, Kyle Williams, Hung-Hsuan Chen, Madian Khabsa, Cornelia Caragea,
Alexander Ororbia, Douglas Jordan, and C. Lee Giles. 2014. CiteSeerX: AI in a Digital Library Search Engine. AI Magazine 36 (2014), 35­48. [34] Chenyan Xiong and James P. Callan. 2015. EsdRank: Connecting Query and Documents through External Semi-Structured Data. In CIKM. [35] Chenyan Xiong and James P. Callan. 2015. Query Expansion with Freebase. In ICTIR. [36] Chenyan Xiong, James P. Callan, and Tie-Yan Liu. 2016. Bag-of-Entities Representation for Ranking. In ICTIR. [37] Chenyan Xiong, James P. Callan, and Tie-Yan Liu. 2017. Word-Entity Duet Representations for Document Ranking. In SIGIR. [38] Chenyan Xiong, Russell Power, and James P. Callan. 2017. Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding. In WWW. [39] Yang Xu, Gareth J. F. Jones, and Bin Wang. 2009. Query dependent pseudorelevance feedback based on wikipedia. In SIGIR. [40] ChengXiang Zhai and John D. Lafferty. 2001. A Study of Smoothing Methods for Language Models Applied to Ad Hoc Information Retrieval. SIGIR Forum 51 (2001), 268­276.
[41] Zhibing Zhao, Peter Piech, and Lirong Xia. 2016. Learning Mixtures of PlackettLuce Models. In ICML.

574

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Predicting User Knowledge Gain in Informational Search Sessions

Ran Yu
L3S Research Center Hannover, Germany
yu@l3s.de

Ujwal Gadiraju
L3S Research Center Hannover, Germany
gadiraju@l3s.de

Peter Holtz
Leibinz Insitut für Wissensmedien Tübingen, Germany
holtz@iwm-tuebingen.de

Markus Rokicki
L3S Research Center Hannover, Germany
rokicki@l3s.de

Philipp Kemkes
L3S Research Center Hannover, Germany
kemkes@l3s.de

Stefan Dietze
L3S Research Center Hannover, Germany
dietze@l3s.de

ABSTRACT
Web search is frequently used by people to acquire new knowledge and to satisfy learning-related objectives. In this context, informational search missions with an intention to obtain knowledge pertaining to a topic are prominent. The importance of learning as an outcome of web search has been recognized. Yet, there is a lack of understanding of the impact of web search on a user's knowledge state. Predicting the knowledge gain of users can be an important step forward if web search engines that are currently optimized for relevance can be molded to serve learning outcomes. In this paper, we introduce a supervised model to predict a user's knowledge state and knowledge gain from features captured during the search sessions. To measure and predict the knowledge gain of users in informational search sessions, we recruited 468 distinct users using crowdsourcing and orchestrated real-world search sessions spanning 11 different topics and information needs. By using scientifically formulated knowledge tests, we calibrated the knowledge of users before and after their search sessions, quantifying their knowledge gain. Our supervised models utilise and derive a comprehensive set of features from the current state of the art and compare performance of a range of feature sets and feature selection strategies. Through our results, we demonstrate the ability to predict and classify the knowledge state and gain using features obtained during search sessions, exhibiting superior performance to an existing baseline in the knowledge state prediction task.
CCS CONCEPTS
· Human-centered computing  User models; · Computing methodologies  Supervised learning; · Applied computing  Interactive learning environments;
ACM Reference Format: Ran Yu, Ujwal Gadiraju, Peter Holtz, Markus Rokicki, Philipp Kemkes, and Stefan Dietze. 2018. Predicting User Knowledge Gain in Informational
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210064

Search Sessions. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/ 3209978.3210064
1 INTRODUCTION
Searching the web for information is among the most frequent online activities. Broder categorized web search queries into having either navigational, transactional or informational intents [4]. In informational web search sessions, the intent of a user is to acquire some information assumed to be present on one or more web pages.
Recent research in the search as learning (SAL) domain has recognized the importance of learning scopes and focused on observing and detecting learning needs during web search. Eickhoff et al. investigated the correlation between several query and search mission-related metrics and learning progress [8]. Wu et al. predicted the difficulty of search tasks from query and mission-related features [21]. Collins-Thompson et al. investigated the effectiveness of user interaction with respect to certain learning outcomes [7]. In addition, [22] has shown that data obtained during the search process provides valuable indicators about the domain knowledge of a user.
Although the importance of learning as an implicit element of web search has been established, there is still only a limited understanding of the impact of search behavior on a user's knowledge state and knowledge gain. Prior work has focused on improving the learning experience and efficiency during search sessions, but the measurement of a user's knowledge gain through the course of an informational search session has not yet been addressed. This is in part due to the difficulty in accurately quantifying knowledge gain through the course of a search session. If web search engines that are currently optimized for relevance can be re-molded to serve learning outcomes, the capability to predict knowledge gain will be a crucial step forward.
In this paper, we aim to address the aforementioned gap. We used crowdsourcing to recruit users who participated in real-world search sessions spanning 11 different topics and information needs. By using scientifically formulated knowledge tests, we calibrated the knowledge of users before and after their search sessions, quantifying their knowledge gain. We introduce a supervised model to predict a user's knowledge state and knowledge gain from features captured during the search sessions.

75

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Original Contributions. Through our work in this paper, we make the following contributions to the current body of literature: · A model for predicting the user's knowledge gain and state during
real-world informational search sessions. · An analysis of the affect of user interactions (ranging from the
queries entered to their browsing behavior) on their knowledge state and knowledge gain. · We release a dataset capturing user behavior and interactions in 468 experimentally orchestrated informational search missions, capturing all features across the aforementioned dimensions as well as knowledge assessments obtained through pre- and posttests. Given the lack of comparable datasets which are both recent and publicly available, we anticipate that this corpus can facilitate SAL research related to different tasks. Implications. The capability to predict a user's knowledge state and gain through the course of an informational search session has the potential to reshape search engines to support learning outcomes as an implicit part of retrieval and ranking. This is of particular importance given that Web search already augments learning processes in a variety of informal as well as formal learning scenarios, such as classrooms, libraries and in work environments. Our contributions advance the current understanding of learning through web search, setting important precedents for further research.
2 RELATED WORK
We discuss two main realms of closely related work ­ studies on the relation between (i) a user's search behavior and knowledge gain, and (ii) a user's search behavior and knowledge state.
2.1 Search Behavior and Knowledge Gain
Eickhoff et al. [8] investigated the correlation between a number of features extracted from search session as well as SERP (Search Engine Results Page) documents with learning needs related to either procedural or declarative knowledge. Results obtained from an analysis of large-scale query logs showed the distinct evolution of particular features throughout search sessions and the correlation of document features with the actual learning intent. The influence of distinct query types on knowledge gain was studied by CollinsThompson et al. [7], finding that intrinsically diverse queries lead to increased knowledge gain. Gadiraju et al. [11] described the use of knowledge tests to calibrate the knowledge of users before and after their search sessions, quantifying their knowledge gain. They investigated the impact of information needs on the search behavior and knowledge gain of users.
Studies on exploratory search have also investigated a similar set of search behaviors that influence the learning outcome. Hagen et al. [14] investigated the relation between the writing behavior and the exploratory search pattern of writers. The authors revealed that query terms can be learned while searching and reading. In addition, Vakkari [19] provided a structured survey of features indicating learning needs as well as user knowledge and knowledge gain throughout the search process. Zhuang et al. [24] investigated the possibility of using 37 user search behavioral features to predict the user engagement with supervised classifiers. As the engagement in the search process usually is correlated with learning outcome, in

our work we have also taken into consideration the set of features have been studied in this work.
The aforementioned prior works have either studied a limited set of features or have addressed only specific learning scenarios and learning types. The generalizability of knowledge gain measures in previous works has not been investigated. In this paper, we extend the current understanding of user knowledge gain in informational search sessions. Using real world information needs and search sessions on the Web, we investigate the possibility of using search activity related features to predict knowledge gain.
2.2 Search Behavior and Knowledge State
By matching the learning tasks into different learning stages of Anderson and Krathwohl's taxonomy [1], Jansen et al. studied the correlation between search behaviors of 72 participants and their learning stage [15]. They showed that information searching is a learning process with unique searching characteristics corresponding to particular learning levels. Gwizdka et al. [12] proposed to assess learning outcomes in search environments by correlating individual search behaviors with corresponding eye-tracking measures. Syed and Collins-Thompson [18] proposed to optimize the learning outcome of the vocabulary learning task by selecting a set of documents while considering keyword density and domain knowledge of the learner.
White et al. [20] investigated the difference between the behavior of domain experts and non-experts in seeking information on the same topic. By analyzing the activity log of experts and non-experts across different domains, the authors found that the distribution of features such as number of queries and query length differed across the levels of expertise. Zhang et al. [22, 23] explored using search behavior as an indicator for the domain knowledge of a user. Through a small study (n = 35), they identified features such as the average query length or the rank of documents consumed from the search results as being predictive. Further, Cole et al. [6], observed that behavioral patterns provide reliable indicators about the domain knowledge of a user, even if the actual content or topics of queries and documents are disregarded entirely.
Other studies have focused on detecting task difficulty in search environments based on user activity data in situations where the subjective assessment of task difficulty is highly correlated to the user's domain knowledge [13, 17]. Gwizdka and Spence [13] showed that a searcher's perception of task difficulty is a subjective factor that depends on the domain knowledge and some other individual traits. Arguello [2] proposed to use logistic regression to predict task difficulty in a search environment. Data was collected through a crowdsourcing platform, and the author used search tasks created by Wu et al. [21], which contain task difficulty assessments on multiple dimensions.
The aforementioned studies focused on investigating the relation between search behavior and a user's knowledge state. Our work leverages these results to derive a comprehensive feature set for our supervised models. In contrast to prior works, we aim at predicting the knowledge state of a user ­ avoiding the need for explicit postsearch knowledge assessments.

2

76

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

3 PROBLEM DEFINITION
In the context of Web search, Broder [4] classified search queries according to their intent into three classes: 1) navigational, 2) informational, and 3) transactional. Herein, informational queries are defined as those queries where `the intent of a user is to acquire some information assumed to be present on one or more web pages' [4]. Thus, informational queries imply a particular learning intent; intentional learning is generally defined as learning that is motivated by intentions and is goal directed [3], in contrast to latent or incidental learning.
Based on the constructs of intentional learning and informational queries, we arrive at the following definition:
Definition 3.1. Intentional Learning-Related Search Session. An intentional learning-related search session comprises of the sequence of a user's actions, with respect to satisfying her learning intent in a web search environment through informational queries. A user's sequence of actions begins with querying the web, and includes browsing through the search results, click and scroll activity, navigation via hyperlinks, query reformulations, and so forth.
For the sake of simplicity, we henceforth refer to informational sessions, i.e. sessions with a particular learning intent, as "sessions".
In this paper, from the observed user interactions in informational search sessions, we aim to predict (i) the knowledge state and (ii) knowledge gain of a user as follows.
Definition 3.2. Predicting a User's Knowledge State and Gain During Search Sessions. Let s be a search session starting at time ti and ending at time tj aimed at satisfying a particular information need, that is, a learning intent  of user u. Based on the user interactions during session s captured in the time period [ti , tj ], we aim to: (1) classify the knowledge state (KS) k(tj ) of u at time point tj with
respect to a particular information need. For the sake of this work, a user's knowledge state with respect to a particular information need is defined by the user's capability to correctly respond to a set of questions about the corresponding information need. We classify a user's knowledge state into 3 classes according to her capability: low knowledge state, moderate knowledge state and high knowledge state (Section 4.5). (2) classify the knowledge state change, i.e. the knowledge gain (KG) k(ti , tj ) of u during time period [ti , tj ] into different degrees. Similarly, a user's knowledge gain with respect to a particular information need is defined as the improvement of user capability (accuracy) to correctly respond to a set of test questions about the corresponding information need. We classify user knowledge gain into 3 classes according to the improvement of user capability: low knowledge gain, moderate knowledge gain and high knowledge gain (Section 4.5).
4 OBTAINING SEARCH SESSION DATA
We adopted a crowdsourcing approach and orchestrated search sessions with varying information needs. All interactions of the users during the search sessions were logged. We analyzed the data to further the understanding of user knowledge evolution in informational search sessions on the Web. In this section, we describe the study design and experimental setup.

4.1 Study Design and Search Environment
We recruited participants from CrowdFlower1, a premier crowdsourcing platform. At the onset, workers were informed that the task entailed `searching the Web for some information'. Workers willing to participate were redirected to our external platform, SearchWell2, a search system built on top of the Bing Web Search API. We logged worker activity on the platform including mouse movements, clicks, and key presses, using PHP/Javascript and the jQuery library. Workers were first asked to respond to a few questions (called `items') corresponding to a particular topic without searching the Web for answers. The questions took the form of statements pertaining to a topic, and workers had to select whether the statement was `TRUE', `FALSE', or `I DON'T KNOW' in case they were not sure. In this way, we calibrated the knowledge of users corresponding to a given topic. To encourage the workers to respond without external consultation, we informed them that their responses to these questions would not affect their pay. We also encouraged workers to avoid guessing. The results of this pre-test were used to calibrate the knowledge of the workers with respect to the topic. We describe the topics and how the knowledge tests were created in the following Section 4.2. On completing the knowledge calibration test, workers were presented with their actual task.
Workers were presented an information need corresponding to the topic of the calibration test they completed. They were told to use the SearchWell platform to search the Web and satisfy their information need. To incentivize workers towards realistic attempts to learn about the topic, we informed them that they will have to complete a final test on the topic to successfully finish the task. Furthermore, workers were conveyed the message that depending on their accuracy on the final test they could earn a bonus payment. We subsequently logged all the activities of the workers (mouse movements, key presses and clicks) within the SearchWell platform. Workers were allowed to begin the final test anytime after a search session, which is when a link to the final test was made available. Workers were encouraged to proceed to the next stage only once they felt that their information need was satisfied and when they were ready for the post-session test. On completing the post-session test, workers received a unique code that they could enter on CrowdFlower to claim their reward.
We restricted the participation to workers from English-speaking countries to ensure that they understood the task and instructions adequately [9, 10]. To ensure reliability of the resulting data, we restricted the participation to Level-3 workers3 on CrowdFlower.
4.2 Topics ­ Defining Information Needs
We constructed a corpus of topics representing varying scopes of information needs (with some relatively broader than others). Topics were selected from the TREC 2014 Web Track dataset4, and corresponding information needs were defined accordingly. In all cases, the knowledge of users before beginning an informational search session was assessed using pre-tested and evaluated knowledge tests.
1 http://www.crowdflower.com/ 2 http://searchwell.l3s.uni-hannover.de/?uid=12345678. 3Level-3 contributors on CrowdFlower comprise workers who completed over 100 test questions across hundreds of different types of tasks, and have a near perfect overall accuracy. They are workers of the highest quality on CrowdFlower. 4 http://www.trec.nist.gov/act_part/tracks/web/web2014.topics.txt

3

77

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Topics and corresponding information needs presented to participants in the informational search sessions, along with the internal reliability of the corresponding knowledge tests. `1', `2' represent Cronbach's  for the pre-session test and post-session test respectively. `N' is the number of reliable participants after filtering.

Topic

Information Need

1 2 N

1. Altitude Sickness 2. American Revolutionary War 3. Carpenter Bees
4. Evolution 5. NASA Interplanetary Missions 6. Orcas Island 7. Sangre de Cristo Mountains 8. Sun Tzu
9. Tornado 10. USS Cole Bombing
11. HIV

In this task you are required to acquire knowledge about the symptoms, causes and prevention of altitude sickness. (20 items) In this task, you are required to acquire knowledge about the `American Revolutionary War'. (10 items) In this task, you are required to acquire knowledge about the biological species `carpenter bees'. How do they look? How do they live? (10 items) In this task, you are required to acquire knowledge about the theory of evolution. (12 items) In this task, you are required to acquire knowledge about the past, present, and possible future of interplanetary missions that are planned by the NASA. (20 items) In this task you are required to acquire knowledge about the Orcas Island. (20 items) In this task, you are required to acquire knowledge about `Sangre de Cristo' mountain range. (10 items) In this task, you are required to acquire knowledge about the Chinese author Sun Tzu - about his life, his writings, and his influence to the present day. (15 items) In this task, you are required to acquire knowledge about the weather phenomenon that is called `tornado' (20 items) In this task, you are required to acquire knowledge about the 2000 terrorist attack that came to be known as the `USS Cole bombing'. (10 items) In this task you are required to acquire knowledge about the transmission, prevention, and consequences of HIV infection. (45 items)

0.59 0.79 44 0.74 0.55 39 0.79 0.58 43
0.55 0.72 42 0.80 0.75 40
0.91 0.85 38 0.70 0.52 38 0.81 0.63 35
0.82 0.62 37 0.83 0.55 38
0.87 0.84 74

Knowledge tests are scientifically formulated tests that measure the knowledge of a participant on a given topic (for example, the HIV knowledge test [5]). Topics and test items are available online5.
Knowledge on all given topics was measured using knowledge tests comprising of between 10 and 45 items. The answer options were in all cases `TRUE', `FALSE', and `I DON'T KNOW'. The differences in the number of items reflects our attempt to feature varying scopes of information needs; relatively narrow (e.g., Carpenter Bees­ 10 items) as well as broad (e.g., NASA Interplanetary Missions­20 items). In the construction of all scales, an item pool comprising of more items than finally used was constructed. After a pilot test with 100 distinct participants recruited via CrowdFlower for each of the 11 topics, items that proved to be either too easy (e.g., more than 80% correct answers) or too hard/ambiguous (e.g., more false than true answers) were discarded. Table 1 presents the topics and corresponding information needs considered for orchestrating the informational search sessions. It also shows the internal reliability (using Cronbach's ) of the pre- and post-session knowledge tests corresponding to each topic. We observe moderate to high values of  in the pre- and post session knowledge tests, suggesting a desirable level of internal consistency.
4.3 Data Collection
To further ensure the reliability of responses and the behavioral data thus produced in the search sessions, we filtered workers using the following criteria. · Workers who entered no queries in the SearchWell system. Since
the aim of our work is to further the understanding of how the knowledge state of a user evolves in informational search sessions, we discard those users who did not enter a search query. · Workers who selected the same option; either `YES' or `NO', for all items in the calibration test or the post-session test. · Workers who did not complete the post-session test. We filtered out 132 workers due to the aforementioned criteria, resulting in 468 workers across the 11 topics. The analysis and results presented hereafter are based on these 468 sessions

alone. For the benefit of further research in this community, the filtered data has been thoroughly anonymized and made publicly available5. We henceforth refer to these filtered workers as users in our experimentally orchestrated information search sessions.
4.4 Descriptive Analysis ­ Important Details
We measure the knowledge gain of users in search sessions corresponding to an information need as the difference between their knowledge calibration score and the post-session test score6. Table 2 presents the average knowledge calibration scores, post-session test scores, and the resulting knowledge gain of users across search sessions with different information needs. Across all topics and search sessions, we found that users exhibited an average knowledge gain of around 19%. Nearly 70% of all the workers exhibited a knowledge gain, while the remaining workers did not. The standard deviation observed in the knowledge gain of users across all topics is notably high, due to the varying domain knowledge of users. This is evident from the average calibration scores in Table 2.

Table 2: The average knowledge gain of users across the different topics. To enhance readability, the rows have been ordered by ascending knowledge gain.

Topic / Information Need
HIV (N=74) Evolution (N=42) NASA Interplanetary Missions (N=40) Altitude Sickness (N=44) Sangre de Cristo Mountains (N=38) Tornados (N=37) Sun Tzu (N=35) American Revolutionary War (N=39) Carpenter Bees (N=43) USS Cole Bombing (N=38) Orcas Island (N=38)
Overall (N=468)

Avg. Calibration Score (in %) 66.25 ± 14.86 34.07 ± 17.99 38.1 ± 20.53
55.88 ± 16.31 33.25 ± 22.40
34.44 ± 21.02 40.54 ± 23.37 34.52 ± 25.65
45.65 ± 27.08 30.95 ± 25.22 34.74 ± 30.08 40.76 ± 22.23

Avg. Post Score (in %) 71.68 ± 14.48 48.15 ± 22.49 52.5 ± 17.43
70.66 ± 19.11 49.75 ± 18.10
53.47 ± 16.28 60.18 ± 17.15 55.95 ± 20.71
67.17 ± 20.29 54.37 ± 16.29 65.51 ± 22.04 59.04 ± 18.58

Knowledge Gain (in %)
5.44 ± 10.02 14.07 ± 18.66 14.40 ± 22.10
14.78 ± 17.76 16.50 ± 22.31
19.03 ± 22.010 19.64 ± 21.59 21.43 ± 27.31
21.52 ± 30.50 23.41 ± 31.30 30.77 ± 30.25 18.27 ± 23.07

5 https://sites.google.com/view/predicting-user-knowledge

6We consider the `I DON'T KNOW' options that were selected, as incorrect responses while computing the knowledge calibration scores and post-session test scores.

4

78

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

We found that on average, the highest knowledge gain was observed through the search sessions corresponding to the topic, `Orcas Island', while the least knowledge gain was observed through those corresponding to the topic, `HIV '. We found a negative linear relationship between topics that workers were familiar with (indicated by their calibration scores) and their knowledge gain; R= -.65, R2 = .42, p < .001. This suggests that the more popular a topic is, or the more familiar that users are with a topic, the lesser they tend to learn about the topic in informational search sessions. Thus, we found that 42% of the variance in the knowledge gain of users can be explained by the topic familiarity.
We found that the average session length of users across the different topics was nearly 5 mins long (M=4.82, SD=5.20). During the search sessions, users navigated to over 5 web pages on average (M=5.46, SD=3.41). We note that on average users entered 2 distinct queries in a search session (M=2.20, SD=2.18), with an average query length of just over 4 terms (M=4.56, SD=2.63). For each query that was entered, users navigated to over 3 web pages on average (M=3.27, SD=1.60). Users spent nearly 2 minutes actively on web pages they navigated to (M=1.97, SD=1.52).

4.5 Knowledge State and Knowledge Gain Classes

Table 3: User groups created based on aver ae ± 0.5S D.

Task Mean SD Low

Moderate High

KG 0.193 0.231 167

179

122

KS 0.618 0.191 145

171

152

We used a Standard Deviation Classification approach to obtain three classes of learners with regard to their level of knowledge. Assuming approximately normal distributions of the respective test scores (X) for the different topics, we transformed the test scores into Z-scores with a mean of 0 and a Standard Deviation (SD) of 1 (standardization). We then used statistically defined intervals (X < 0.5 SD = low; -0.5 SD < X < 0.5 SD = moderate; 0.5 SD < X = high) for the classification of the learners into roughly equal groups with low, moderate, or high knowledge. The same procedure was repeated for knowledge gain. Here as well, the empirical knowledge gain for every test was transformed into corresponding Z-scores and three roughly equal groups (low knowledge gain; moderate knowledge gain; high knowledge gain) were defined accordingly. In view of the substantial variety of different topics, we argue that such a tripartite categorization of knowledge states and knowledge gains respectively allows for the construction of robust models, which are themselves based on a large variety of features. Thus, insights from the learning tasks considered can be generalized to other similar intentional learning activities. This procedure weighs all different knowledge tests equally irrespective of the number of items.

5 FEATURE EXTRACTION AND ANALYSIS
We approach the problem of predicting knowledge state (k(tj )) and knowledge gain (k(ti , tj )) described in Section 3 with supervised models for classification, where details about the applied classification models are given in Section 6.1. To this end, each session s is represented by a feature vector vì = (f1, f2, ..., fn ), where

considered features are described in Section 5.1 and analyzed in Section 5.2.
5.1 Features Considered
We extracted features according to multiple dimensions of a search session, structured into five categories, namely features related to the session, queries, SERP, browsing behavior and mouse movements. The SERP category consists of features extracted from direct interactions with SERP items, while the browsing category consists of features extracted from subsequent user navigation beyond simple SERP clicks. The majority of features is motivated by existing literature, yet none of the features have been used on the inferential tasks of this work.
All considered features fi are listed in Table 4 together with the Pearson Correlation Coefficient scores Corr (fi , k(ti , tj )), Corr (fi , k(tj )) between the respective feature and the knowledge gain (state).
Session Features. The relation between feature s_duration and different stages of learning has been discussed by Jansen et al. [15]. It has been shown that there is a difference in the duration of sessions among the classifications in Anderson and Krathwohl's taxonomy [1]. White et al. [20] also found that the sessions conducted by domain experts were generally longer than non-expert sessions.
Query Features. Several prior works [2, 15, 20] have investigated the correlation between query activities in a search session and learning performance. Based on the study by White et al. [20], the number of queries (q_num) applied by experts and non-experts show big differences across domains: non-expert users usually run significantly more queries than experts. Jansen et al. [15] also found that the number of queries applied on learning tasks classified as applying stage was significantly different from other learning stages.
The length of queries (q_term_max {min, av, total }) has been found to have a strong correlation with learning outcome by Zhang et al. [22]. Their study shows that the average query length and user domain knowledge is correlated with a Pearson correlation score of 0.344.
The complexity of queries (q_complexity_max_di f f ) has been investigated by Eickhoff et al. [8], and has been found to evolve during the learning process. We applied the same query complexity measure as in [8], which is computed based on the dictionary created by Kuperman et al. [16] that contains a listing of more than 30,000 English words along with the age at which native speakers typically learn the term. The maximum age of acquisition across all query terms is used as query complexity.
Furthermore, the investigation from Arguello [2] shows that beside the number of total terms, the number of unique terms (q_uniq_term_{max, min, av, total }, q_uniq_term_ratio) in the session is strongly correlated with knowledge level on the task, while the number and ratio of stop words do not have a big difference when comparing between search sessions with different levels of domain knowledge.
As we aim at predicting knowledge state change during a session, similarly to the features discussed above, we extract the features q_len_{ f irst, last }, q_uniq_term_{ f irst, last }, which potentially

5

79

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Features for prediction of knowledge gain and knowledge state.

CategoryNotation

Session

s_dur at ion s_dur at ion_per _q

Cor r (fi, k (ti, tj ))
-0.020 -0.019

Query

q_num q_t erm_{max, min, av, t ot al } q_uniq_t erm_{max, min, av, t ot al }
q_uniq_t erm_r at io q_l en_{f ir st, l ast } q_uniq_t erm_{f ir st, l ast } q_compl exity_{max, min, av } q_compl exity_max _dif f

0.052 {0.0002,-0.094,-0.042,0.047} {0.016,-0.087,-0.024,0.06}
0.083 {-0.049,0.055} {-0.023,-0.040} {0.097,0.086,0.093} {0.092}

SERP

S ERP _click

-0.009

S ERP _click _r ank _{hihest, lowest, av } {-0.101,-0.021,-0.017}

S ERP _click _int erval

0.036

S ERP _click _per _query

-0.007

S ERP _no_click _query_{num, pct }

{0.041,-0.051}

S ERP _t ime_{tot al, av, max }

{0.039,0.022,0.049}

S ERP _av_t ime_t o_f ir st _cl ick

-0.002

b _nu m b _u ni q _nu m b_num_per _q b_uniq_num_per _q b_t ime_tot al b_t ime_av_per _q Browsing b_t ime_{max, av }_per _pae b_r evisit ed _r at io b_{num, pct }_f r om_S ERP b_{num, pct }_f r om_non_S ERP b_dist inct _domain_num b_t tl _l en_{max, min, av, t ot al } b_pae_size_{max, min, av, t ot al } b_t tl _q_over l ap_{max, min, av, t ot al } b_ur l _q_over l ap_{max, min, av, t ot al }

-0.018 0.029 -0.017 -0.017 0.243 0.236 {0.306,0.291} -0.058 {-0.017,0.058} {-0.056,0.057} -0.033 {-0.08,-0.058,-0.078,-0.109} {0.109,-0.093,0.122,0.086} {0.15,0.089,0.14,0.091} {0.16,0.064,0.133,0.044}

Mouse

m_num m_num_per _q m_r ank _max m_r ank _max _per _q m_scr oll _dist m_scr oll _dist _per _q m_scr oll _max _pos m_scr oll _max _pos_per _q

0.066 0.094 0.091 0.095 0.120 0.120 0.142 0.127

Cor r (fi, k (tj ))
0.066 0.066
0.103 {0.065,0.032,0.051,0.068} {0.104,0.05,0.084,0.089}
-0.002 {0.031,0.105} {0.036,0.087} {0.087,0.078,0.049} {0.077}
0.063 {-0.063,0.047,0.095} 0.022 -0.012 {0.077,0.029} {0.091,-0.008,0.043} -0.027
0.075 0.109 -0.016 -0.016 0.134 0.063 {0.104,0.089} -0.020 {0.074,0.056} {-0.028,0.025} 0.102 {0.146,0.106,0.146,0.082} {-0.055,-0.074,-0.057,-0.01} {0.005,-0.028,-0.018,0.023} {0.041,0.018,0.025,0.028}
0.113 0.053 0.067 0.039 0.058 0.025 0.052 0.021

Feature description

Duration of the search session of a worker on a given topic Session duration per query

Number of queries in session s

Maximum, minimum, average, total number of query terms

Maximum, minimum, average number of unique terms per query

Number

of

query

terms

/

unique

query

terms

(

q_uniq_t e r m_t ot q_t e r m_t ot al

al

)

First, last query length

Number of unique terms of first, last query

Maximum, minimum, average of query complexity

Difference between the maximum and minimum complexity

Total number of click on search result Average, highest, lowest rank of the clicks Average interval between clicks Average number of clicks per query Number, percentage of SERP with no clicks Total, average, maximum time spend on SERPs Time till first click

Total number of pages browsed in session Number of unique pages browsed in session Average number of page browsed per query Average number of unique page viewed per query Total active time on the pages Average active time on the browsed pages per query Maximum, average active time on the browsed pages Ratio of revisited pages Number, percentage of pages visited through SERP Number, percentage of pages visited through pages other than SERP Number of distinct domains of the visited pages Maximum, minimum, average, total page title length Maximum, minimum, average, total page size Maximum, minimum, average and total overlap between query and page title Maximum, minimum, average, total term overlap between query and page URL

total number of mouseovers in the session average number of mouseovers per query max mouseover rank in the session average max mouseover rank per query total scroll distance in session average scroll distance per query max scroll position in session average max scroll position per query

are indicators of the knowledge level at the beginning and end of the session.
SERP Activity Features. Some activities on SERP have also been investigated by previous works. Specifically, CollinsThompson [7] found that the total number of clicks on SERP (SERP_click) is strongly correlated with a user's understanding of the topic. The analysis shows that users tend to click more often when having stronger interest in the topic.
The ranking position of the clicked URL on SERP has also been shown to be a strong indicator of user domain knowledge by Zhang et al. [22]. In [2], the authors discovered that the difficult tasks with which a user is less knowledgeable are associated with more clicks (SERP_click), more clicks on lower ranks (SERP_click_rank_{hihest, lowest, av}), more abandoned queries (SERP_no_click_query_{num, pct }), i.e. queries without clicks, longer time till first click (SERP_av_time_to_f irst_click) and longer time till next click (SERP_click_interval).
Browsing Features. Browsing features such as number of documents viewed (b_num, b_uniq_num) and average number of documents viewed per query (b_num_per _q, b_uniq_num_per _q) were shown by several previous works [2, 8, 13, 15] to be positively correlated with the knowledge improvement. More detailed features corresponding to the browsing behavior have also been studied,

indicating that the more difficult a task is for a user, the higher the ratio of revisited pages (b_revisited_ratio) is.
Despite the number of pages visited, the time spent (corresponds to features b_time_total, b_time_{max, av}_per _q etc.) on the accessed pages are found to vary to a large extent between domain expert and non-expert [20]. Feature SERP_time_{total, av, max } was shown to be effective for predicting the user's assessment of task difficulty [2], which is subject to the user's knowledge state.
We further distinguish the viewed pages into two sets {pages navigated through SERP, pages navigated through non-SERP}, by parsing its ancestor page. Hence we extract the features b_{num, pct }_f rom_SERP and b_{num, pct }_f rom_non_SERP that are motivated by the features introduced above.
The content of the accessed Web documents strongly influence the user's learning outcome. White et al. [20] found that domain experts encountered different and more diverse domains (feature b_distint_domain_num) than domain novices. Several other document content related features: page size (b_pae_size_{max, min, av, total }), title length b_ttl_len_{max, min, av, total } have also been found to evolve during the learning process [8]. Based on the assumption that domain experts and novices have different capabilities of choosing learning resources, for instance, experts are able to recognize

6

80

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

useful documents without query terms presented in the page title, we computed features based on the overlap between page title and query (b_url_q_overlap_{max, min, av, total }). The page URL (b_ttl_q_overlap_{max, min, av, total }) as a complementary source containing hints about a page's content has also been considered in the feature extraction process.
Mouse Features. Features in the Mouse category are indicators of quantity and quality of user interactions with a knowledge source and were also shown to be effective for predicting the user's assessment of task difficulty [2].
5.2 Feature Analysis and Selection
As a basis for feature selection, we analyze the features with respect to their relationship to knowledge gain and knowledge state, as well as their redundancy.
Correlation between feature and KG (KS). In order to select the most influential features for the prediction task, we compute Corr (fi , k(ti , tj )) and Corr (fi , k(tj )), i.e. the Pearson correlation coefficient between each feature and the knowledge gain (knowledge state). The correlation scores are shown in Table 4. Based on the computed score, we select the features fulfilling the condition Corr (fi , k(ti , tj ))   for the knowledge gain prediction task and Corr (fi , k(tj ))   for the knowledge state prediction task. Performance of the prediction model using features selected based on varied  and  has been evaluated and corresponding results are presented in Section 7.
Correlation between features. We compute the Pearson correlation coefficient between each pair of features Corr (fi , fj ). If Corr (fi , fj )   , i.e. features appear to be not independent, we remove the feature from the feature set, that has lower Corr (fi , k(ti , tj )) respectively lower Corr (fi , k(tj )) for knowledge gain (state) prediction. We evaluate the performance of the prediction model for different values of  . The feature selection results are reported in Section 7.

·  - threshold for feature selection based on correlation between features. We also experimented with different  in the range of {1.0, 0.95, 0.9, 0.85, 0.8, 0.75, 0.7}. The number of features in the feature set corresponding to given  ,  and  is reported in Table 5.

Table 5: Number of features of different configurations.

 (KG)

 (KS)

0.0 0.05 0.1 0.15 0.2 0.25 0.3 0.0 0.05 0.1 0.15

 = 1.0 70 43 16 6 4 2 1 70 41 11 0  = 0.95 66 42 16 6 4 2 1 66 38 11 0  = 0.9 56 37 13 6 4 2 1 58 34 11 0  = 0.85 43 29 10 6 4 2 1 44 24 9 0  = 0.8 39 26 7 4 2 1 1 38 19 8 0  = 0.75 37 25 7 4 2 1 1 34 17 7 0  = 0.7 33 24 6 3 1 1 1 32 16 7 0

6.1.2 Baseline. As discussed in Section 2, the tasks addressed in this paper are comparably novel. To the best of our knowledge, there are no existing baselines for the task of knowledge gain prediction during informational Web search missions. Therefore, we compare our approach for a number of configurations (described above), using multiple standard classification models. For the prediction of knowledge state k(tj ), we compare our approach in addition to one existing baseline [23]. KSZhan refers to the linear regression model fitted by Zhang et al. [23] for domain knowledge prediction as shown in Equation 1.
K SZ han = -1.466+0.039 · S aved +0.147 ·Ql en +0.130 · Relmean (1)
Saved represents the number of documents saved by the user, which is an extremely sparse feature in a real search environment and does not appear in our dataset. Qlen is the mean query length and Relmean is the mean rank of documents opened in SERPs. As the output of the baseline regression model is a real number, we convert the result into 3 classes according to the definition given in Section 4.5.

6 EVALUATION - EXPERIMENTAL SETUP
6.1 Approach Configurations & Baselines
6.1.1 Configurations and Parameters. · Classifier. We apply a range of standard models for the classifi-
cation of the knowledge gain and knowledge state, namely, Naive Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forrest (RF), and Multilayer Perceptron (MP). For our experiments, we used the Weka library for Java7. For each of the configurations described below, we perform grid search to tune the hyperparameters of all of the classifiers. In Section 7, we report the result of the best performing hyperparameter configuration for each classifier. ·  ( )- threshold for feature selection based on correlation between feature and KG (KS). We compare prediction performance before and after applying the selection based on featureKG (KS) correlation. We set the threshold  ( ) for selecting the features in the range of {0.0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3} ({0.0, 0.05, 0.1, 0.15}). We omit results for larger  ( ), as the resulting number of features is insufficient for training a classifier.
7 https://www.cs.waikato.ac.nz/ml/weka/

6.2 Evaluation Metrics
For both tasks, we run repeated 10-fold cross-validation with 10 repetitions on all the approaches and configurations described in Section 6.1 and evaluate the results according to the following metrics: · Accuracy (Accu) across all classes: percentage of search ses-
sions that were classified with the correct class label. · Precision (P), Recall (R), F1 (F 1) score of class i: we compute
the standard precision, recall and F1 score on the prediction result of each class i. · Macro average of precision (P), recall (R), and F1 (F 1): the average of the corresponding score across 3 classes. · Runtime: the time consumed for completing the 10-fold crossvalidation on experimental dataset in milliseconds. To analyze the usefulness of individual features, we make use of the Mean Decrease Accuracy (MDA) metric, which is based on the Random Forest model, i.e. a very well performing model for both tasks as shown in Section 7. MDA quantifies the importance of a feature by measuring the change in prediction accuracy of the Random Forest model when the values of the feature are randomly permuted compared to the original observations.

7

81

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Method
NB LR SVM RF MP





0.85 0.25 0.85 0.05 0.90 0.00 0.95 0.00 0.85 0.25

Table 6: Performance in knowledge gain prediction task.

#Features Runtime P

Low

Moderate

High

Macro average All

R F1 P R F1 P R F1 P R F1 Accu

2

19.1

0.450 0.747 0.562 0.483 0.268 0.344 0.513 0.384 0.439 0.482 0.467 0.448 0.469

29

653.9

0.498 0.537 0.516 0.459 0.382 0.416 0.379 0.431 0.403 0.445 0.450 0.445 0.450

56

441.6

0.488 0.595 0.536 0.487 0.340 0.400 0.410 0.469 0.437 0.462 0.468 0.458 0.465

66

3739.3 0.521 0.542 0.531 0.469 0.410 0.437 0.425 0.480 0.450 0.472 0.477 0.473 0.475

2

1919.3 0.452 0.556 0.497 0.421 0.312 0.356 0.425 0.450 0.435 0.433 0.439 0.429 0.435

Figure 1: Feature importance for knowledge gain prediction.

7 RESULTS: PREDICTION PERFORMANCE AND FEATURE ANALYSIS
In this section, we report the evaluation results of the prediction performance as well as an analysis of feature importance.
7.1 Knowledge Gain Prediction
Performance of different Configurations. For each of the 245 distinct configurations described in Section 6, we run repeated cross-validation as described in the previous section.
From all the different combinations of  and  as listed in Table 5, we present the result of the configuration that produces the highest accuracy for each classifier in Table 6 due to space constraints. A complete set of the evaluation results are available online8. We observed that in the knowledge gain prediction task, the highest average F1 score across classes and the highest accuracy always appear in the same configuration for all the classifiers except for LR, where there is a minor difference of .001 in F1 between the two.
The Random Forest classifier achieves the best performance in terms of accuracy and average F1 score, slightly outperforming Naive Bayes and SVM. As shown, Naive Bayes is the most efficient classifier in terms of computation time for feature sets of comparable size. Comparing classification performance for the different classes, the results for each of the classifiers consistently show higher F1 scores for the `Low' and `High' knowledge gain classes than for the `Moderate' knowledge gain class. Overall, prediction accuracy is above 0.435 and the F1 score is above 0.429 for all of the classifiers, which indicates that the set of features we extracted from search activities can provide meaningful evidence for predicting knowledge gain.
8 https://sites.google.com/view/predicting-user-knowledge

Feature Impact. The MDA results of each feature are shown in Figure 1. Based on the result, the most important features are: b_time_max_per _pae, b_time_av_per _pae and b_time_total. Mostly active time related features which reflect the effort users spend on learning. Similarly, these features are immediately followed by m_scroll_dist_per _q, m_scroll_max_pos, and SERP_click_rank_lowest, three features that are indicators for the amount of information a user has seen during the session.
As shown in Table 6, the NB and MP classifier performs best when using 2 features only, namely b_time_max_per _pae and b_time_av_per _pae. These two features are confirmed as the most important features by our feature importance analysis.
Regarding feature categories, the 10 most useful features in terms of MDA belong to the browsing, mouse and SERP categories. Surprisingly, although multiple query features had above average correlation to knowledge gain (see Table 4) ­ in particular, the features related to query complexity (q_complexity_{max, min, av} and q_complexity_max_di f f ) had correlations ranging from .086 to .097, compared to the median of .042 ­ q_uniq_term_total is the only query feature among the 25 highest ranked according to MDA. Analogously, both session features s_duration and s_duration_per _q appear among the 25 highest ranked features despite their relatively low correlations of -.02 and -.019.
7.2 Knowledge State Prediction
Performance of different Configurations. We have experimented with all different combinations of  and  as listed in Table 5 for all considered classifiers. The result of the configuration that produces the highest accuracy for each classifier is shown in Table 7. We observe that in the knowledge state prediction task, the highest average F1 score across classes and the highest accuracy always

8

82

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Method  

NB LR SVM RF MP

0.75 0.1 1.00 0.05 0.95 0.05 1.00 0.00 1.00 0.05

KSZhan -

-

Table 7: Performance in knowledge state prediction task.

#Features Runtime P

Low

Moderate

High

Macro average All

R F1 P R F1 P R F1 P R F1 Accu

7

23.5

0.352 0.712 0.470 0.424 0.218 0.287 0.370 0.211 0.268 0.382 0.380 0.342 0.369

41

797.7

0.338 0.383 0.359 0.402 0.368 0.384 0.372 0.359 0.366 0.370 0.370 0.370 0.370

38

292.3

0.359 0.479 0.409 0.395 0.303 0.342 0.409 0.386 0.397 0.388 0.389 0.383 0.385

70

4023.4 0.443 0.456 0.449 0.394 0.358 0.374 0.418 0.447 0.432 0.418 0.421 0.418 0.418

41

43619 0.380 0.414 0.396 0.398 0.298 0.341 0.385 0.461 0.419 0.388 0.391 0.385 0.387

2

23

0.320 0.428 0.366 0.328 0.240 0.277 0.362 0.355 0.359 0.337 0.341 0.334 0.335

Figure 2: Feature importance for knowledge state prediction.

appear in the same configuration for all the classifiers except Naive Bayes (average F1 of the highest accuracy configuration is 0.006 lower than the maximum average F1).
Among all evaluated classifiers, Random Forest reaches the highest accuracy and F1 score, outperforming the other classifiers.
Comparison to Baseline. We compare the performance of our approach against the baseline method (KSZhan), shown in the last row in Table 7. The result suggests that, the linear regression model fitted in previous work based on data collected through a lab study does not perform well in the knowledge state prediction task and is outperformed by all five classifiers following our approach.
Feature Impact. The MDA results of each feature in the knowledge state prediction tasks are shown in Figure 2. The most important features (q_complexity_av, b_ttl_len_min and b_ttl_len_av) reflect the user's capability of constructing a query and choosing relevant resources. In terms of feature categories, all of the highest ranked features for this task belong to the query and browsing categories.
Compared to the knowledge gain task, query complexity features (q_complexity_{min, max, av}) are considerably more useful, while features related to time and effort invested, like b_time_max_per _pae and b_time_av_per _pae, are among the lowest ranked. Other query features related to the used vocabulary (e.g. q_uniq_term_min, q_uniq_term_av, and q_term_total) are ranked similarly highly. Apparently, while the time taken by users to take in the discovered documents is predictive of their knowledge gain, their capability of using complex queries and selecting relevant resources reveals more about their knowledge state.

8 DISCUSSION
Based on the experimental results, we conclude that: i) knowledge gain (state) can be predicted during informational search sessions with a certain level of accuracy, ii) performance of the knowledge gain prediction appears to be generally better, suggesting that the task is easier given the nature of our data, and iii) the performance of the prediction approach is better for more extreme classes, i.e. for low and high knowledge gain (state) classes, whereas performance on the moderate classes is lowest in both tasks, presumably due to the moderate classes being the most overlapping ones with respect to their characteristics. In this section we discuss some of the reasons behind these observations.
Most of the features we considered were found to correlate rather weakly with knowledge gain (state). Intuitively, this could be due to the limited duration of the search sessions (just over 5 minutes on average). This could potentially reduce the predictive power of certain features, such as the number of queries or the number of accessed documents. This also rendered evolution-oriented features, which would capture the evolution of queries and behavior throughout a session predictively poor. While these would supposedly be highly indicative of the knowledge gain, they require longer sessions than are usually observable in real-world search sessions as well as in our experimental data.
For the prediction of knowledge gain, our feature analysis result shows that the most important features are the ones related to the user's active time. As our experimental dataset contains mostly short sessions, it is understandable that the time spent affects the knowledge gain strongly. However, we believe that in longer search sessions, the learning pattern and the initial knowledge state of a

9

83

Session 1B: Log Analysis

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

user might be more influential for the knowledge gain than in short sessions. Further experiments are required to establish this.
The results suggest that with the presented approach, the knowledge gain prediction is an easier task than the knowledge state prediction. As shown in Figure 2, the most important features for knowledge state prediction are the features related to the content of queries and browsed documents. Intuitively, these features are also central to the knowledge gain prediction task. Yet, we observe that although the topic descriptions that were given to the users typically provided central keywords for the first query, only a very limited set of queries (1-2) are fired by most users. Given the small number of queries in each session, the query features are less distinguishable and hence, less indicative of the knowledge gain. Thus, query evolution is observable only to a very limited extent.
9 CONCLUSIONS AND FUTURE WORK
In this paper, we propose to use classification models to predict user knowledge state and knowledge gain from behavioral data captured during real-world informational search sessions. Given the lack of available datasets and ground truths, we have created an experimental dataset using crowdsourcing, capturing 468 informational search sessions including user interactions and behavioral traces throughout the process, together with calibration and corresponding post-session knowledge test results.
Previous work related to the aforementioned prediction tasks is still very limited. However, existing state of the art was considered when identifying a novel set of 70 features, partially motivated from related work as well as from exploring our gathered data. Classification experiments were conducted with 5 classification models in a variety of configurations and in comparison to a baseline in the knowledge state prediction task.
The experimental results underline that a user's knowledge gain and knowledge state can be modeled based on a user's online interactions observable throughout the search process. Through feature analysis, we provide evidence for an improved understanding between individual user behavior and the corresponding knowledge state and change. Alongside these results, we also make the gathered dataset available. This dataset captures user interactions throughout diverse informational search sessions and corresponding knowledge assessments, and thereby provides a resource which can facilitate further research in this area.
As a part of future work, we aim to reproduce and refine the findings in more varied search sessions, where durations and learning intents are more diverse; involving considerably longer search sessions and, for instance, procedural knowledge rather than intents focused on declarative knowledge only. This would provide the opportunity to observe evolution-oriented features, such as considering the evolution of queries, their length and complexity.
Potential applications of this work include the consideration of user knowledge and the expected learning progress of a user as part of Web search engines and information retrieval approaches, or within informal learning-oriented search settings, such as libraries or knowledge- and resource-centric online platforms.

REFERENCES
[1] L. W. Anderson, D. R. Krathwohl, P. Airasian, K. Cruikshank, R. Mayer, P. Pintrich, J. Raths, and M. Wittrock. A taxonomy for learning, teaching and assessing: A revision of bloom's taxonomy. New York. Longman Publishing. Artz, AF, & Armour-Thomas, E.(1992). Development of a cognitive-metacognitive framework for protocol analysis of mathematical problem solving in small groups. Cognition and Instruction, 9(2):137­175, 2001.
[2] J. Arguello. Predicting search task difficulty. In ECIR, volume 14, pages 88­99, 2014.
[3] P. Blumschein. Intentional learning. In Encyclopedia of the Sciences of Learning, pages 1600­1601. Springer, 2012.
[4] A. Broder. A taxonomy of web search. In ACM Sigir forum, volume 36, pages 3­10. ACM, 2002.
[5] M. P. Carey, D. Morrison-Beedy, and B. T. Johnson. The hiv-knowledge questionnaire: Development and evaluation of a reliable, valid, and practical selfadministered questionnaire. AIDS and Behavior, 1(1):61­74, 1997.
[6] M. J. Cole, J. Gwizdka, C. Liu, N. J. Belkin, and X. Zhang. Inferring user knowledge level from eye movement patterns. Information Processing & Management, 49(5):1075­1091, 2013.
[7] K. Collins-Thompson, S. Y. Rieh, C. C. Haynes, and R. Syed. Assessing learning outcomes in web search: A comparison of tasks and query strategies. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, pages 163­172. ACM, 2016.
[8] C. Eickhoff, J. Teevan, R. White, and S. Dumais. Lessons from the journey: a query log analysis of within-session learning. In Proceedings of the 7th ACM international conference on Web search and data mining, pages 223­232. ACM, 2014.
[9] U. Gadiraju, R. Kawase, S. Dietze, and G. Demartini. Understanding malicious behavior in crowdsourcing platforms: The case of online surveys. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pages 1631­1640. ACM, 2015.
[10] U. Gadiraju, J. Yang, and A. Bozzon. Clarity is a worthwhile quality­on the role of task clarity in microtask crowdsourcing. In Proceedings of the 28th ACM Conference on Hypertext and Social Media, pages 5­14. ACM, 2017.
[11] U. Gadiraju, R. Yu, S. Dietze, and P. Holtz. Analyzing knowledge gain of users in informational search sessions on the web. In 2018 ACM on Conference on Human Information Interaction and Retrieval (CHIIR). ACM, 2018.
[12] J. Gwizdka and X. Chen. Towards observable indicators of learning on search. In SAL@ SIGIR, 2016.
[13] J. Gwizdka and I. Spence. What can searching behavior tell us about the difficulty of information tasks? a study of web navigation. Proceedings of the Association for Information Science and Technology, 43(1):1­22, 2006.
[14] M. Hagen, M. Potthast, M. Völske, J. Gomoll, and B. Stein. How writers search: Analyzing the search and writing logs of non-fictional essays. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval, pages 193­202. ACM, 2016.
[15] B. J. Jansen, D. Booth, and B. Smith. Using the taxonomy of cognitive learning to model online searching. Information Processing & Management, 45(6):643­663, 2009.
[16] V. Kuperman, H. Stadthagen-Gonzalez, and M. Brysbaert. Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 44(4):978­990, 2012.
[17] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Information Processing & Management, 44(6):1822­1837, 2008.
[18] R. Syed and K. Collins-Thompson. Retrieval algorithms optimized for human learning. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 555­564. ACM, 2017.
[19] P. Vakkari. Searching as learning: A systematization based on literature. Journal of Information Science, 42(1):7­18, 2016.
[20] R. W. White, S. T. Dumais, and J. Teevan. Characterizing the influence of domain expertise on web search behavior. In Proceedings of the second ACM international conference on web search and data mining, pages 132­141. ACM, 2009.
[21] W.-C. Wu, D. Kelly, A. Edwards, and J. Arguello. Grannies, tanning beds, tattoos and nascar: Evaluation of search tasks with varying levels of cognitive complexity. In Proceedings of the 4th Information Interaction in Context Symposium, pages 254­257. ACM, 2012.
[22] X. Zhang, M. Cole, and N. Belkin. Predicting users' domain knowledge from search behaviors. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval, pages 1225­1226. ACM, 2011.
[23] X. Zhang, J. Liu, M. Cole, and N. Belkin. Predicting users' domain knowledge in information retrieval using multiple regression analysis of search behaviors. Journal of the Association for Information Science and Technology, 66(5):980­1000, 2015.
[24] M. Zhuang, G. Demartini, and E. G. Toms. Understanding engagement through search behaviour. In International Conference on Information and Knowledge Management, Proceedings, pages 1957­1966. Association for Computing Machinery, 2017.

10

84

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Pytrec_eval: An Extremely Fast Python Interface to trec_eval

Christophe Van Gysel
University of Amsterdam Amsterdam, The Netherlands
chris@stophr.be
ABSTRACT
We introduce pytrec_eval, a Python interface to the trec_eval information retrieval evaluation toolkit. pytrec_eval exposes the reference implementations of trec_eval within Python as a native extension. We show that pytrec_eval is around one order of magnitude faster than invoking trec_eval as a sub process from within Python. Compared to a native Python implementation of NDCG, pytrec_eval is twice as fast for practically-sized rankings. Finally, we demonstrate its effectiveness in an application where pytrec_eval is combined with Pyndri and the OpenAI Gym where query expansion is learned using Q-learning.
CCS CONCEPTS
· Information systems  Evaluation of retrieval results;
KEYWORDS
IR evaluation, toolkits
ACM Reference Format: Christophe Van Gysel and Maarten de Rijke. 2018. Pytrec_eval: An Extremely Fast Python Interface to trec_eval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210065
1 INTRODUCTION
Evaluation is a crucial component of any information retrieval (IR) system [2]. Reusable test collections and off-line evaluation measures [7] have been the dominating paradigm for experimentally validating IR research for the last 30 years. The popularity and ubiquity of off-line IR evaluation measures is partly due to the Text REtrieval Conference (TREC) [5]. TREC led to the development of the trec_eval1 software package that is the standard tool for evaluating a collection of rankings. The trec_eval tool allows IR researchers to easily compute a large number of evaluation measures using standardized input and output formats. For a document collection, a test collection of queries with query/document relevance information (i.e., qrel) and a set of rankings generated by a particular IR system (i.e., a system run) for the test collection queries,
Open-source implementation is available at https://github.com/cvangysel/pytrec_eval. 1 https://github.com/usnistgov/trec_eval
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210065

Maarten de Rijke
University of Amsterdam Amsterdam, The Netherlands
derijke@uva.nl
trec_eval outputs a standardized output format containing evaluation measure values. The adoption of trec_eval as an integral part of IR research has led to the following benefits: (a) standardized formats for system rankings and query relevance information such that different research groups can exchange experimental results with minimal communication, and (b) open-source reference implementations of evaluation measures--provided by a third party (i.e., NIST)--that promotes transparent and consistent evaluation.
While the availability of trec_eval has brought many benefits to the IR community, it has the downside that it is available only as a standalone executable that is interfaced by passing files with rankings and ground truth information. In recent years, the Python programming language has risen in popularity due to its feature richness (i.e., scientific libraries and data structures) and holistic language design [3]. Research progresses at a rate proportional to the time it takes to implement an idea, and consequently, scripting languages (e.g., Python) are preferred over conventional programming languages [6]. Within IR research, retrieval systems are often implemented and optimized using Python (e.g., [4, 9]) and for their evaluation trec_eval is used. However, invoking trec_eval from Python is expensive as it involves (1) serializing the internal ranking structures to disk files, (2) invoking trec_eval through the operating system, and (3) parsing the trec_eval evaluation output from the standard output stream. This workflow is unnecessarily inefficient as it incurs (a) a double I/O cost when the ranking is first serialized by the Python script and subsequently parsed by trec_eval, and (b) a context-switching overhead as the invocation of trec_eval needs to be processed by the operating system.
We introduce pytrec_eval to counter these excessive efficiency costs and avoid a wild growth of ad-hoc Python-based evaluation measure implementations. pytrec_eval builds upon the trec_eval source code and exposes a Python-first interface to the trec_eval evaluation toolkit as a native Python extension. Rankings constructed in Python can directly be passed to the evaluation procedure, without incurring disk I/O costs; evaluation is performed using the original trec_eval implementation. Due to pytrec_eval's implementation as a native Python extension, context-switching overheads are avoided as the evaluation procedure and its invocation reside within the same process. Next to improved efficiency, pytrec_eval brings the following benefits: (a) current and future reference trec_eval implementations of IR evaluation measures are available within Python, and (b) as the evaluation measures are implemented in C, their execution are typically faster than native Python-based alternatives. The main purpose of this paper is to describe pytrec_eval, provide empirical evidence of the speedup that pytrec_eval delivers, and showcase the use of pytrec_eval in a reinforcement learning application. We ask the following questions: (RQ1) What speedup do we obtain when using pytrec_eval over trec_eval (serialize-invoke-parse workflow)? (RQ2) How fast is pytrec_eval compared to native Python implementations of IR evaluation measures? We also present a demo application that combines Pyndri [9] and pytrec_eval in a query formulation

873

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

qrel = {'q1': {'d1': 1, 'd2': 0}, 'q2': {'d2': 1}}
run = {'q1': {'d1': 0.5, 'd2': 2.0}, 'q2': {'d1': 0.5, 'd2': 0.6}}
evaluator = pytrec_eval.RelevanceEvaluator( qrel, {'map', 'ndcg'})
result = evaluator.evaluate(run)
# result equals # {'q1': {'map': 0.5, 'ndcg': 0.6309297535714575}, # 'q2': {'map': 1.0, 'ndcg': 1.0}}
Code snippet 1: Minimal example of how pytrec_eval can be used to compute IR evaluation measures. Evaluation measures (NDCG, MAP) are computed for two queries--q1 and q2--and two documents--d1 and d2--where for q2 we only have partial relevance (d1 is assumed to be non-relevant).
reinforcement learning setting and provide the environment and the reward signal, integrated within the OpenAI Gym [1].
2 EVALUATING USING PYTREC_EVAL
The pytrec_eval library has a minimalistic design. Its main interface is the RelevanceEvaluator class. The RelevanceEvaluator class takes as arguments (1) query relevance ground truth, a dictionary of query identifiers to a dictionary of document identifiers and their integral relevance level, and (2) a set of evaluation measures to compute (e.g., ndcg, map). Code snippet 1 shows a minimal example on how pytrec_eval can be used to evaluate a ranking. Rankings are encoded by a mapping from document identifiers to their retrieval scores. Internally, pytrec_eval sorts the documents in decreasing order of retrieval score. This behavior mimics the implementation of trec_eval, which ignores the order of documents within the user-provided file, and only considers the document scores. Similar to trec_eval, document ties, which occur when two documents are assigned the same score, are broken by secondarily sorting on document identifier. Query relevance ground truth is passed to pytrec_eval in a similar way to document scores, where relevance is encoded as an integer rather than a floating point value.
Beyond measures computed over the full ranking of documents, pytrec_eval also supports measures computed up to a particular rank k. The values of k are the same as the ones used by trec_eval. For example, measures ndcg_cut and P correspond to NDCG@k and precision@k, respectively, with k = 5, 10, 15, 20, 30, 100, 200, 500, 1000. The set of supported evaluation measures is stored in the pytrec_eval.supported_measures property and the identifiers are the same as used by trec_eval (i.e., running trec_eval with arguments -m ndcg_cut --help will show documentation for the NDCG@k measure). To mimic the behavior of trec_eval to compute all known evaluation measures (i.e., passing argument -m all_trec to trec_eval), just instantiate RelevanceEvaluator with pytrec_eval.supported_measures as the second argument.
3 BENCHMARK RESULTS
As demonstrated above, pytrec_eval conveniently exposes popular IR evaluation measures within Python. However, the same functionality could be exposed by invoking trec_eval in a serializeinvoke-parse workflow--or--by implementing the evaluation measure natively in Python. In this section we provide empirical benchmark results that show that pytrec_eval, beyond its convenience,

is also faster at computing evaluation measures than these two alternatives (i.e., invoking trec_eval or native Python).
Experimental setup. For every hyperparameter configuration, the runtime measurement was repeated 20 times and the average runtime is reported. Speedup denotes the ratio of the runtime of the alternative method (i.e., trec_eval or native Python) over the runtime of pytrec_eval and consequently, a speedup of 1.0 means that both methods are equally fast. When invoking trec_eval using the serialize-invoke-parse workflow, rankings are written from Python to storage without sorting, as trec_eval itself sorts the rankings internally. The resulting evaluation output is read from stdout to a Python string and we do not extract the measure values, as different parsing strategies can lead to large variance in runtime. For the native Python implementation, we experimented with different open-source implementations of the NDCG measure and adapted the fastest implementation as our baseline. The implementation does not make use of NumPy or other scientific Python libraries as (a) we wish to compare to native Python directly and (b) the NumPy-based implementations we experimented with were less efficient than the native implementation we settled with, as NumPy-based implementations require that the rankings are encoded in dense arrays before computing evaluation measures. The evaluated rankings and ground-truth were synthesized by assigning every document a distinct ranking score in N and a relevance level of 1. This allows us to evaluate different evaluation measure implementations with rankings and query sets of different sizes. Experiments were run using a single Intel Xeon CPU (E5-2630 v3) clocked at 2.4GHz, DDR4 RAM clocked at 2.4GHz, an Intel SSD (DC S3610) with sequential read/write speeds of 550MB/s and 450MB/s, respectively, and a hard disk drive (Seagate ST2000NX0253) with a rotational speed of 7200 rpm. All code used to run our experiments is available under the MIT open-source license.2
Results. We now answer our research questions by comparing the runtime performance of pytrec_eval to trec_eval (RQ1) and a native Python implementation (RQ2).
RQ1 What speedup do we obtain when using pytrec_eval over trec_eval (serialize-invoke-parse workflow)?
Fig. 1 shows matrices of speedups of pytrec_eval over trec_eval obtained using different storage types (increasing order of throughput capacity): a regular hard disk drive (HDD), a solid state drive (SSD) and a memory-mapped file system (tmpfs). For the degenerate case where we have a single query and a single returned document, we observe that there is a clear difference between the different storages. In particular, we can see that tmpfs is faster than SSD, and in turn, SSD is faster than the HDD. However, for larger configurations (upper right box in every grid; 10,000 queries with 1,000 documents) we see that the difference between the storage types fades away and that pytrec_eval always achieves a speedup of at least 17 over trec_eval. This is because (a) starting the serialization (e.g., disk seek time) is expensive (as can be seen in the left-lower box of every grid), but that cost is quickly overshadowed by (b) the cost of context switching between processes. In the case of pytrec_eval, however, context switching is avoided as all logic runs as part of the same process. Consequently, we can conclude that pytrec_eval is at least one order of magnitude faster than invoking trec_eval using a serialize-invoke-parse workflow.
2The benchmark code can be found in the benchmarks sub-directory of the pytrec_eval repository; see the footnote on the first page.

874

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Queries Queries

10000 34 34 30 25 24 22 18 18

5000 25 20 19 21 21 22 19 17

1000 14 21 23 19 19 21 18 18

250 16 24 28 29 27 22 18 18

50 54 50 49 41 33 25 19 17

10 229 194 140 96 57 40 22 19

1 951 780 627 443 229 125 38 28

1

5

10 20 50 100 500 1000

Documents per query

10000 49 36 31 26 23 23 18 18

5000 31 30 28 23 21 22 19 18

1000 13 21 26 24 24 23 18 18

250 15 22 26 29 27 28 18 17

50 45 44 42 37 30 29 21 18

10 190 142 106 81 46 35 20 19

1 781 653 565 389 196 113 32 24

1

5

10 20 50 100 500 1000

Documents per query

Queries

10000 55 41 31 28 23 23 18 17

5000 34 29 26 23 22 22 19 18

1000 12 21 25 25 24 21 19 17

250 15 22 26 29 29 27 18 17

50 45 43 41 37 31 28 20 18

10 185 144 109 76 47 34 21 19

1 771 511 525 333 192 102 32 24

1

5

10 20 50 100 500 1000

Documents per query

(a) HDD

(b) SSD

(c) Memory-mapped (tmpfs)

Figure 1: Speedup of pytrec_eval (down-rounded speedup in each box; runtime measured as average over 20 repetitions) com-

pared to invoking trec_eval using a serialize-invoke-parse workflow (§1) for different numbers of queries, different numbers

of ranked documents per query, and using different types of storage (hard disk drive, solid state drive and random access

memory) for serializing the rankings and query relevance ground truth.

Speedup

3
2
1
0 1 3 5 10 20 30 40 50 102 103 104 105
Number of documents
Figure 2: Speedup of pytrec_eval over a native Python implementation of the NDCG evaluation measure (we report average speedup and its standard deviation over 20 repetitions). For practically-sized rankings, pytrec_eval is consistently faster than the native Python implementation.
RQ2 How fast is pytrec_eval compared to native Python implementations of IR evaluation measures?
Fig. 2 shows the speedup of pytrec_eval over a Python-native implementation of NDCG for a single query and a varying number of documents. Here we see that for extremely short rankings (1­3 documents), the native implementation outperforms pytrec_eval. However, for rankings consisting of 5 documents or more, we can see that pytrec_eval provides a consistent performance boost over the native implementation. The reason for the sub-native performance of pytrec_eval for very short rankings is because--before pytrec_eval computes evaluation measures--rankings need to be converted into the internal C format used by trec_eval. The Python-native implementation does not require this transformation, and consequently, can thus be slightly faster when rankings are very short. However, it is important to note that short rankings are uncommon in IR and that the average ranking consists of around 100 to 1,000 documents. We conclude that pytrec_eval is faster than native Python implementations for practically-size rankings.
4 EXAMPLE: Q-LEARNING
We showcase the integration of the Pyndri indexing library [9] and pytrec_eval within the OpenAI Gym [1], a reinforcement learning library, for the task of query expansion. In particular, we use Pyndri to rank documents according to a textual query and

subsequently evaluate the obtained ranking using pytrec_eval. The reinforcement learning agent navigates an environment where
actions correspond to adding a term to the query. Rewards are given
by an increase or decrease in evaluation measure (i.e., NDCG). The goal is for the agent to learn a policy   that optimizes the expected
value of the total reward. For the purpose of this demonstration of
software interoperability, we synthesize a test collection in order to
(1) limit the computational complexity that arises from real-world
collections, and (2) to give us the ability to create an unlimited
number of training queries and relevance judgments.
Document collection. We construct a synthetic document collection D, of a given size |D| = 100, following the principles laid out by Tague et al. [8]. For a given vocabulary size |V | = 10,000, we construct vocabulary V consisting of symbolic tokens. We sample collection-wide unigram (|V | parameters) and bigram (|V | 2 parameters) pseudo counts from an exponential distribution ( = 1.0). This incorporates term specificity within our synthetic collection, as
only few term uni- and bigrams will be frequent and most will be in-
frequent. These pseudo counts will then serve as the concentration
parameters of Dirichlet distributions from which we will sample
a uni- and bigram language model for every document. We create |D| documents as follows. For every document d, given the average document length µd = 200, we sample its document size, |d |, from a Poisson with mean µd . We then sample two language models--one for unigrams P (w | d ) and another for bigrams P ((x, y) | d )--from
a Dirichlet distribution where the concentration parameters we
defined earlier for the whole collection. The document is then constructed as follows. Until we have reached |d | tokens, we repeat the following: (a) sample an n-gram size from a predefined probability distribution (P (n = 1) = 0.9, P (n = 2) = 0.1), and subsequently, (b) sample an n-gram from the corresponding language model. We truncate a document if it exceeds its pre-defined length |d |.
Query collection. Once we obtained our synthetic document collection D, we proceed by constructing our query set Q, of a given size |Q | = 100,000, as follows. For every query q to be constructed, we select r = 5 documents uniformly at random from D and denote these as the set of relevant documents Rq  D for query q. Given the average query length µq = 3, the length of query q, q , is then sampled from a Poisson distribution with mean µq . We write
P w | Rq and P (w | D) to denote the empirical language models

875

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Avg. reward (NDCG)

0.10

0.05

0.00

00h 00m 00s

02h 46m 40s

05h 33m 20s

08h 20m 00s 11h 06m 40s Wall-clock time

13h 53m 20s

16h 40m 00s

Figure 3: Average reward (NDCG) obtained by the Q-learning algorithm over time while training the reinforcement learning

agent. The agent learns to select vocabulary terms that improve retrieval effectiveness for the set of 100k training queries.

estimated from concatenating the relevant documents for query q and from concatenating all documents in the collection D (i.e., the collection language model), respectively. The q terms of query
q are sampled with replacement from P w | Rq (1.0 - P (w | D)), such that terms specific to Rq and uncommon in D are selected. Environment. For each query q, the environment is initialized to the state where only the query terms are present. At any given state, the agent can then choose to expand the query terms with any unigram term from the vocabulary V in addition to a null operation action. Rankings are obtained by querying the Indri search engine using Pyndri, using a Dirichlet language model (µ = 2,500), and obtaining a ranking of the top-10 documents. The reward of choosing an action is the NDCG that is obtained by expanding the query with the chosen term. As observation, the agent receives a binary vector indicating which terms of the vocabulary V occur at least once in the current expanded query. After 5 actions--or a perfect NDCG (i.e., 1.0) is achieved--the episode terminates.
Reinforcement learning agent. We learn an optimal policy tabular   using Q-learning where the initial values of the Q (·) are initialized to zero. We set the learning rate  = 0.1 and the discount factor  = 0.95. During learning, we maintain an -greedy strategy with  = 0.05. Fig. 3 shows the average reward obtained while training an agent on the reinforcement learning problem defined above. The average reward obtained by the agent increases over time. In particular, this example showcases that different IR libraries (Pyndri, pytrec_eval) can easily be integrated with machine learning libraries (OpenAI Gym) to quickly prototype ideas. An essential part here is that expensive operations (i.e., ranking and evaluation) are performed in efficient low-level languages, whereas prototyping occurs in the high-level Python scripting language. All code used in this example is available under the MIT open-source license.3
5 CONCLUSIONS
In this paper we introduced pytrec_eval, a Python interface to trec_eval. pytrec_eval builds upon the trec_eval source code and exposes a Python-first interface to the trec_eval evaluation toolkit as a native Python extension. This allows for convenient and fast invocation of IR evaluation measures directly from Python. We showed that pytrec_eval is around one order of magnitude faster than invoking trec_eval in a serialize-invoke-parse workflow as it avoids the costs associated with (1) the serialization of the rankings to storage, and (2) operation system context switching. Compared to a native Python implementation of NDCG, pytrec_eval is approximately twice as fast for practically-sized rankings (100 to 1,000
3The reinforcement learning code can be found in the examples sub-directory of the pytrec_eval repository; see the footnote on the first page.

documents). In addition, we showcased the integration of Pyndri [9] and pytrec_eval within the OpenAI Gym [1] and showed that all three modules can be combined to quickly prototype ideas.
In this paper, we used a tabular function during Q-learning; other functional forms--such as a deep neural network--can also be used. Pyndri and pytrec_eval expose common IR operations through a convenient Python interface. Beyond the convenience that both modules provide, an important design principle is that expensive operations (e.g., indexing, ranking) are performed using efficient low-level languages (e.g., C), while Python takes on the role of an instructor that links the expensive operations. Future work consists of exposing more IR operations as Python libraries and allowing more interoperability amongst modules. For example, currently Pyndri converts its internal Indri structures to Python structures, which are then again converted back to internal trec_eval structures by pytrec_eval. A closer integration of Pyndri and pytrec_eval could result in even faster execution times as both can communicate directly--in cases where one is only interested in the evaluation measures and not the rankings--rather than through Python.
Acknowledgements. This research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the China Scholarship Council, the Criteo Faculty Research Award program, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Google Faculty Research Awards program, the Microsoft Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scientific Research (NWO) under project nrs CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.
REFERENCES
[1] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI gym, 2016.
[2] D. Harman. Information retrieval evaluation. Synthesis Lectures on Information Concepts, Retrieval, and Services, 3(2):1­119, 2011.
[3] H. Koepke. Why python rocks for research. https://www.stat.washington.edu/ ~hoytak/_static/papers/why-python.pdf, 2010. Accessed February 12, 2018.
[4] D. Li and E. Kanoulas. Bayesian optimization for optimizing retrieval systems. In WSDM. ACM, February 2018.
[5] NIST. Text retrieval conference, 1992­2017. [6] L. Prechelt. An empirical comparison of seven programming languages. Computer,
33(10):23­29, Oct. 2000. [7] M. Sanderson. Test collection based evaluation of information retrieval systems.
Foundations and Trends in Information Retrieval, 4(4):247­375, 2010. [8] J. Tague, M. Nelson, and H. Wu. Problems in the simulation of bibliographic
retrieval systems. In SIGIR, pages 236­255. ACM, June 1980. [9] C. Van Gysel, E. Kanoulas, and M. de Rijke. Pyndri: a python interface to the indri
search engine. In ECIR, pages 744­748. Springer, April 2017.

876

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Split-Lists and Initial Thresholds for WAND-based Search

Andrew Kane
University of Waterloo Waterloo, Ontario, Canada
arkane@uwaterloo.ca
ABSTRACT
We examine search engine performance for rank-safe query execution using the WAND and state-of-the-art BMW algorithms. Supported by extensive experiments, we suggest two approaches to improve query performance: initial list thresholds should be used when k values are large, and our split-list WAND approach should be used instead of the normal WAND or BMW approaches. We also recommend that reranking-based distributed systems use smaller k values when selecting the results to return from each partition.
CCS CONCEPTS
· Information systems  Retrieval eciency; Information retrieval query processing; Search engine indexing;
KEYWORDS
Information retrieval, Query performance, Eciency, Optimization
ACM Reference Format: Andrew Kane and Frank Wm. Tompa. 2018. Split-Lists and Initial Thresholds for WAND-based Search. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­ 12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3209978.3210066
1 INTRODUCTION
With huge amounts of data and large query volumes, search engines consume signicant resources, both in terms of computer hardware and energy usage. The end user sees fast queries because the dataset is partitioned across many machines. However, resource costs are still present, so any improvement in eciency will give signicant cost savings. In this paper, we present various optimization techniques for search execution that maintain the ranking eectiveness of the system, so called rank-safe execution.
The details of executing queries in a search system are complex, but the basic idea is simple. At indexing time, the data is inverted to form a postings list for each token containing the locations of that token in the dataset. Each list is stored in a compressed format and often in document identier (docid) order. Within-document locations are often dropped and only <docid, frequency> pairs are stored. At query time, the system nds the lists for the query terms
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210066

Frank Wm. Tompa
University of Waterloo Waterloo, Ontario, Canada
fwtompa@uwaterloo.ca
and combines them to give the top-k best documents according to a specied ranking algorithm. The standard approach merges lists ordered by docid, resulting in document-at-a-time execution that is fast and requires little temporary memory [5].
During list merging, each iteration of the search execution loop will SELECT the next candidate document, SCORE the candidate, and possibly SAVE it in a top-k heap, as depicted in Figure 1 (ignoring the green italic writing for now). The exhaustive-OR method considers all documents in all the query lists (disjunctive merging), resulting in slow queries, but rank-safe results since every candidate is scored. The exhaustive-AND method considers only documents contained in all the query lists (conjunctive merging), resulting in fast queries, but non-rank-safe results since high-scoring documents missing one or more terms are not found.
next

start

state: <query_lists, current_list_pointers, heap, max_list_scores>

SELECT
<docid, max_potential_score>
SCORE
<docid, score>
SAVE

end

Figure 1: Search execution loop used by WAND and BMW.
The WAND (Weak-AND) [4] and state-of-the-art BMW (BlockMax WAND) [10] approaches start executing as an OR query and transition towards an AND as intermediate results improve. The details of this transition produce both fast query execution and rank-safe results. The WAND approach uses the current top-k results stored in the heap to give a threshold allowing more ecient skipping of potential candidates during SELECT processing. This is done by ordering the query lists by their current docid and summing up their maximum list scores --in order-- until it exceeds the threshold at the pivot docid; then the smallest list before the pivot is advanced, and a new pivot is calculated, repeating until enough lists are on the same pivot docid to score the candidate. The BMW approach adds maximum scores for each encoded list block in the index, and uses them to prune candidates on each pivot calculation. Thus, both approaches change SELECT processing to reduce candidates sent to the SCORE stage. The required additions to the basic search execution loop are marked in Figure 1 (green italic writing).
We start with a recent WAND and BMW implementation using quantized scores and incremental scoring for fast execution [6]. We then improve upon this original code in three ways:
First, we apply small code optimizations to both the WAND and BMW implementations that give signicant runtime improvements.
Second, we improve the query runtime of the WAND and BMW algorithms for large k values by starting at an initial threshold generated from indexing time analysis of the query lists.

877

Short Research Papers I
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
Third, we show that split-lists (i.e., 2-layer lists split by score) can signicantly improve WAND performance, making it faster than the state-of-the-art BMW approach for many congurations. While the 2-layer approach has been used with BMW [9, 10], the 2-layer split-list WAND approach has not been previously examined.
2 RELATED WORK
Compression and Reordering. There are many encoding techniques that can make postings lists both fast and compact. For this study we use QMX [16] to compress lists, as this method is used in the original code, but many other approaches are available, such as OptPFD [19], PForDelta (PFD) [21], Simple-9 [1], Simple-16 [20], and partitioned Elias-Fano codes [12]. It is common to encode in blocks and include skips, as we do, to jump over unused portions of the lists. Ranking information can be stored immediately inside the encoding or in parallel on the block encoding level.
Reordering documents (i.e., renumbering internal document identiers) can improve both space and runtime. One common technique that produces superior results reorders by URL [15], but others reorder at random or by clustering [3, 14], document size [5, §6.3.7], or global rank [11]. We examine both URL and random ordering and expect other approaches to lie between these results.
Query Execution. Executing a query typically involves combining postings lists containing document identier and occurrence frequency pairs using a ranking function. This combining is often done using either document-at-a-time or term-at-a-time processing [18]; the former requires little temporary space, while the latter can give better memory access performance. When lists are ordered by document identier this is a simple merge with skipping within lists. Alternatively, lists could be stored in impact order, which allows for score-at-a-time processing and early termination [2].
Rather than storing frequencies in postings lists, partial scores per-term can be stored instead, often as quantized values. Using quantized scores can give runtime performance gains when combining document ordered lists, but the indexes are larger because quantized scores are less compressible than frequencies [6, 9].
3 EXPERIMENTAL SETUP
Our experiments are run on two datasets with associated workloads: the GOV2 dataset is 426GB of data in 25.2 million documents using the TREC '04-'06 query topic workload (701-850) and the ClueWeb09b dataset is 1.39 TB of data in 50.2 million documents using the TREC '10-'12 query topic workload (51-200). Associated user evaluations ensure no degradation of ranking eectiveness.
Our experiments are run on a large 4x22 core Xeon 2.2GHz Linux machine with 1TB of memory. Postings lists are loaded into memory and full query workloads are executed ten times single threaded to avoid CPU caching. We explore various top-k values, namely k 2 {1, 10, 50, 100, 500, 1000, 2000}, to give a broad understanding of query performance. We nd that workload runtimes are stable with small standard deviations of less than 1.3% for k 10.
4 ORIGINAL CODE
The WAND and BMW code with which we started is a state-of-theart publicly available1 implementation developed at RMIT and used
1 https://github.com/jmmackenzie/Quant-BM-WAND/

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Andrew Kane and Frank Wm. Tompa

runtime (ms/query)

40  WAND original code  BMW original code WAND new code BMW new code
30

GOV2
(URL order)

20
10
 
0 1

 









10

50 100


 
 

500 1000 2000

runtime (ms/query)

140  WAND original code

 BMW original code

120

WAND new code

BMW new code

100

80 

60



ClueWeb09b
(URL order)
 


 
  

40 20


 


0 1

10

50 100

500 1000 2000

k value
Figure 2: Query runtime performance using original and new implementations over various top-k values.

for recent research presented in a WSDM 2017 paper [6]. This code rst indexes the data using ATIRE [17], then converts that index to a document ordered encoding with QMX [16] compression on 128 element blocks and skip structures over blocks. We use the faster variant storing precomputed partial BM25 scores in the index quantized as small integers, rather than storing document frequencies and calculating the partial scores at runtime, even though this does increase index size. In the query execution loop, the SCORE portion of the query is done incrementally, which avoids full scoring for many candidate documents and makes execution faster.
We added code to allow reordering of documents during conversion from the ATIRE index into WAND and BMW indexes. We follow the standard approach of reordering our indexes by URL, which produces a signicant performance improvement [15].
We nd that this original implementation of the WAND and BMW algorithms has good performance over a range of k values, as shown for URL ordered versions of GOV2 and ClueWeb09b in Figure 2 (orange lines). Since using smaller k values can give signicant performance gains, reranking systems with multiple partitions should use smaller k values in the partitions and ensure rank-safe results by revisiting a partition for more results as needed [13]. Thus, for example, rather than retrieving 1000 matches from each partition, a highly distributed web search engine could use a very small k value for each partition, and revisit very few partitions, to produce the top-1000 matches needed for reranking.

5 NEW CODE OPTIMIZATIONS
While it is quite common to add assertion type checks in low level methods, these can cause performance slowdowns if they are not

878

Short Research Papers I Split-Lists and Initial Thresholds for WAND-based Search

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

removed or turned o in production code. As such, we removed two assertion type checks from the underlying compressed list access methods to give performance gains.
Standard template classes are highly useful and often well optimized, but using some generalized accessor paradigms may not be the most ecient choice for simple tasks. The original implementation uses std::array objects to store the sorted list pointers and general std::iterators are used to traverse over these objects. We changed these traversals to use operator[] accesses, which is signicantly faster, especially when the arrays are small.
Returning multiple objects from a method can be accomplished by forming std::pair or std::tuple objects to pass back to the caller, but this can be expensive if the method is called a large number of times. We changed such methods to pass objects by reference so that the method can update and the caller can access the object.
The search execution loop must SAVE candidates with high enough scores, as shown in Figure 1. This is done using a heap, so that a candidate is added when the heap size is less than k, or the minimum scored value in the heap is replaced by the candidate when the candidate's score is higher (i.e., a minimum heap). Rather than checking if the heap size is less than k for each candidate, we initialize the heap with k dummy values having a score of zero, and then avoid outputting them at the end of query execution. This improves performance and also allows for easy implementation of the initial list thresholds described in the next section.
Combining these simple code optimizations results in runtime performance gains, as shown in Figure 2 (blue lines). While these optimizations improve performance for both approaches, the WAND algorithm benets somewhat more than does BMW (average speedup of 1.22x vs. 1.16x).
6 INITIAL LIST THRESHOLDS
During indexing, we determine the kth highest scoring value for each list of at least size k and store the values in a separatele on disk. Before we start executing a query, we calculate the initial threshold by simply taking the maximum kth score value for all the query lists (minus one for quantized scoring or minus a small epsilon for exact ranking). Using our pre-populated heap optimization from the last section, we set the score of the dummy values to be this initial list threshold, thus dening a minimum starting threshold for query execution. Since one of the query lists can generate k values higher than this threshold, we know that the nal threshold must be higher than this value, and thus, the query execution will be rank-safe (i.e., it will produce the k highest ranked results). This initial threshold approach was briey explored in recent work [7].
This approach does restrict the choice of k at query time, though any query using a k value smaller than what was used at indexing time to produce the stored scores can be used and still produce ranksafe results. If k is large, the number of lists of at least size k will likely be small and require little space to store their list thresholds (e.g., our indexes add less than 0.01% space overhead at k = 1000). For a multi-partition index, these list thresholds could be calculated globally, giving higher values that would improve performance; thus the larger the system, the bigger the gain.
We nd that using list thresholds as we have described improves performance for large k values and does not noticeably degrade

40 WAND new code BMW new code WAND list thresholds BMW list thresholds
30

GOV2
(URL order)

runtime (ms/query)

20

10

0 1

10

50 100

500 1000 2000

140

WAND new code

ClueWeb09b

BMW new code

120

WAND list thresholds

(URL order)

BMW list thresholds

100

runtime (ms/query)

80

60

40

20

0 1

10

50 100

500 1000 2000

k value
Figure 3: Query runtime performance using list thresholds over various top-k values.

performance in other situations. Using list thresholds, we nd that the GOV2 dataset has signicant gains for a range of large k values, while ClueWeb09b has some gains for large k values, as shown in Figure 3 (red lines). Since list thresholds can produce performance gains without noticeably degrading any conguration and they can be stored in the index with little overhead, we expect that employing list thresholds will be benecial in many search systems.

7 SPLIT-LISTS
During indexing time, we split each list into two parts, where the rst part contains the highest scored documents for that term and the second part contains the remaining documents (i.e., 2-layer lists split by score [10]). There are many ways to decide how many documents to place in each list, but we simply pick a percentage of the list. Since we are using a quantized score value we split along quantum boundaries, but these boundaries could be far apart, so we pick the last boundary before the desired cut point. In addition, we only split lists with more than 10,000 postings.
At query time, the split-lists are both included in the WAND pivot processing, but the maximum list scores used depend on whether the paired split-list has already been included in the pivot process (i.e., both lists cannot contain the same document, so if both are in the pivot calculation, then the maximum score of the paired lists is used). This checking of pairs adds some overhead, but allows for more skipping. We nd that for our split-list WAND implementation, checking pairs is faster than not checking.
Our split-list WAND implementation produces signicant gains over traditional WAND processing, as shown in Figure 4 (green lines). Indeed, split-list WAND is faster than the state-of-the-art

879

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

40 WAND list thresholds BMW list thresholds WAND split-lists
30

GOV2
(URL order)

runtime (ms/query)

20

10

0 1

10

50 100

500 1000 2000

140

WAND list thresholds

ClueWeb09b

BMW list thresholds

120

WAND split-lists

(URL order)

runtime (ms/query)

100

80

60

40

20

0 1

10

50 100

500 1000 2000

k value
Figure 4: Query runtime performance using split-lists over various top-k values.

BMW algorithm in ClueWeb09b for all k values and in GOV2 for small k values. The improvement in the ClueWeb09b dataset is quite signicant, split-list WAND at k = 10 gives a 1.41x speedup over an already highly optimized BMW implementation for URL order (and more for random order). Importantly, compared to using a normal WAND implementation, the improvement from switching to splitlist WAND is large for all k values in both datasets (ranging from 1.20x to 6.39x speedup with a 10% space overhead). We expect that split-list WAND can be combined with other existing approaches for additional gains (e.g., ltering [8] and 3-phase retrieval [7]).
Previous work on 2-layer BMW suggests it is faster than BMW when no pair checking is implemented [10], but we found 2-layer BMW to be slower both with and without pair checking, likely from our fast scoring limiting gains from additional BMW candidate pruning. Our split-list WAND approach can exploit much larger high-score splits than previously recommended for the 2layer BMW approach, in particular, we use a cuto of 10%, which outperforms the 2% split recommended for 2-layer BMW.

8 CONCLUSIONS
A summary of the runtime performance of the algorithms presented in this paper for k 2 {10, 1000} is shown in Table 1. We nd that using smaller k values in partitions and making some simple code changes can greatly improve query execution times. The simple idea of using initial thresholds based on the kth highest score in each query list can also improve performance for both WAND and BMW when k is large. Finally, we nd that our split-list WAND approach signicantly outperforms the basic WAND approach and can also outperform the state-of-the-art BMW approach. As such, split-list

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Andrew Kane and Frank Wm. Tompa

Table 1: Runtime performance for k=10 and k=1000 (ms).

WAND original code WAND original code WAND new code WAND list thresholds WAND split-lists (10%) BMW original code BMW original code BMW new code BMW list thresholds BMW 2-layer (2%) BMW 2-layer (10%)

order rand
url url url url rand url url url url url

GOV2 k=10 k=1000 23.25 56.20 9.91 29.47 8.35 23.60 8.30 19.74 4.81 15.95 18.93 51.71 6.10 19.86 5.29 17.03 5.23 14.74 6.03 17.31 8.28 20.34

ClueWeb09b k=10 k=1000 121.00 179.98 74.94 118.80 60.61 97.35 61.03 87.01 15.35 55.18 54.51 152.59 24.68 78.12 21.69 65.65 21.66 62.10 29.61 70.94 52.92 83.70

WAND should be considered for use in many search systems. Our implementation of these algorithms is publicly available2.
Acknowledgments. The computing resources used in this research were made available by the University of Waterloo.

REFERENCES
[1] Vo Ngoc Anh and Alistair Moat. 2005. Inverted index compression using wordaligned binary codes. Information Retrieval 8, 1 (2005), 151­166.
[2] Vo Ngoc Anh and Alistair Moat. 2005. Simplied similarity scoring using term ranks. In SIGIR. 226­233.
[3] Dan Blandford and Guy Blelloch. 2002. Index compression through document reordering. In DCC. 342­351.
[4] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya Soer, and Jason Zien. 2003. Ecient query evaluation using a two-level retrieval process. In CIKM. 426­434.
[5] Stefan Büttcher, Charles Clarke, and Gordon V. Cormack. 2010. Information retrieval: Implementing and evaluating search engines. The MIT Press.
[6] Matt Crane, J. Shane Culpepper, Jimmy Lin, Joel Mackenzie, and Andrew Trotman. 2017. A Comparison of Document-at-a-Time and Score-at-a-Time Query Evaluation. In WSDM. 201­210.
[7] Caio Moura Daoud, Edleno Silva de Moura, Andre Carvalho, Altigran Soares da Silva, David Fernandes, and Cristian Rossi. 2016. Fast top-k preserving query processing using two-tier indexes. Information processing & management 52, 5 (2016), 855­872.
[8] Constantinos Dimopoulos, Sergey Nepomnyachiy, and Torsten Suel. 2013. A candidate ltering mechanism for fast top-k query processing on modern CPUs. In SIGIR. 723­732.
[9] Constantinos Dimopoulos, Sergey Nepomnyachiy, and Torsten Suel. 2013. Optimizing top-k document retrieval strategies for block-max indexes. In WSDM. 113­122.
[10] Shuai Ding and Torsten Suel. 2011. Faster top-k document retrieval using blockmax indexes. In SIGIR. 993­1002.
[11] Xiaohui Long and Torsten Suel. 2003. Optimized query execution in large search engines with global page ordering. In VLDB. 129­140.
[12] Giuseppe Ottaviano and Rossano Venturini. 2014. Partitioned Elias-Fano indexes. In SIGIR. 273­282.
[13] Oscar Rojas, Veronica Gil-Costa, and Mauricio Marin. 2013. Ecient parallel block-max WAND algorithm. In Euro-Par. 394­405.
[14] Wann-Yun Shieh, Tien-Fu Chen, Jean Jyh-Jiun Shann, and Chung-Ping Chung. 2003. Inverted le compression through document identier reassignment. Information Processing & Management 39, 1 (2003), 117­131.
[15] Fabrizio Silvestri. 2007. Sorting out the document identier assignment problem. Advances in Information Retrieval (2007), 101­112.
[16] Andrew Trotman. 2014. Compression, SIMD, and postings lists. In ADCS. 50­57. [17] Andrew Trotman, Xiangfei Jia, and Matt Crane. 2012. Towards an ecient and
eective search engine. In Workshop on Open Source Information Retrieval. 40­47. [18] Howard Turtle and James Flood. 1995. Query evaluation: strategies and optimiza-
tions. Information Processing & Management 31, 6 (1995), 831­850. [19] Hao Yan, Shuai Ding, and Torsten Suel. 2009. Inverted index compression and
query processing with optimized document ordering. In WWW. 401­410. [20] Jiangong Zhang, Xiaohui Long, and Torsten Suel. 2008. Performance of com-
pressed inverted list caching in search engines. In WWW. 387­396. [21] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. Super-scalar
RAM-CPU cache compression. In ICDE. 59:1­12.

2 https://github.com/andrewrkane/Quant-BM-WAND/

880

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

ery Performance Prediction using Passage Information

Haggai Roitman
IBM Research AI Haifa, Israel
haggai@il.ibm.com

ABSTRACT

2 RELATED WORK

We focus on the post-retrieval query performance prediction (QPP) task. Speci cally, we make a new use of passage information for this task. Using such information we derive a new mean score calibration predictor that provides a more accurate prediction. Using an empirical evaluation over several common TREC benchmarks, we show that, QPP methods that only make use of document-level features are mostly suited for short query prediction tasks; while such methods perform signi cantly worse in verbose query prediction settings. We further demonstrate that, QPP methods that utilize passage-information are much better suited for verbose settings. Moreover, our proposed predictor, which utilizes both documentlevel and passage-level features provides a more accurate and consistent prediction for both types of queries. Finally, we show a connection between our predictor and a recently proposed supervised QPP method, which results in an enhanced prediction.
1 INTRODUCTION
We focus on the post-retrieval query performance prediction (QPP) task [4]. Given a query, a corpus and a retrieval method that evaluates the query, our goal is to predict the query's performance based on its retrieved result list [4].
Motivated by previous works on document retrieval using passage information [1­3], we propose to use such information for the post-retrieval QPP task as well. To this end, we extend Kurland et al.'s probabilistic QPP framework [10] and show how passage information may be utilized for an enhanced prediction.
Using an evaluation with several TREC corpora, we rst demonstrate that, existing state-of-the-art document-level post retrieval QPP methods, are mostly suited for prediction tasks that involve short (and probably more ambiguous) queries; whereas such methods are less suited for prediction tasks that involve verbose (long and probably more informative) queries. We next demonstrate that, our proposed QPP method which makes use of passage information provides a more robust prediction, regardless of query type. We further set a direct connection with Roitman et al.'s mean retrieval score estimation framework [12]. Moreover, by integrating our proposed passage-information QPP signal as an additional calibration feature within Roitman et al.'s framework [12], we are able to achieve the overall best QPP accuracy.

The query performance prediction task has been extensively studied, where two main approaches have been proposed, either preretrieval or post-retrieval prediction [4]. Yet, most of previous QPP research has focused on ad-hoc retrieval prediction tasks that involved short (keyword-based) queries [4]. In this work, we further study QPP in cases with verbose (long) queries [8].
Passage information has been shown to assist in ad-hoc document retrieval [2, 3] and verbose query evaluation tasks [1]. Motivated by these previous works, in this work, we rely on passage information as a strong evidence source for query performance. A couple of previous works [6, 9] have predicted the outcome of passage retrieval for question answering tasks. Yet, as we shall later show, predictors that were suggested for the passage retrieval QPP task are less suited for the document retrieval QPP task.
Our work extends Kurland et al.'s [10] probabilistic QPP framework with passage information. Finally, Roitman et al. [12] have recently proposed an extension to [10], where the authors derived a generic calibrated (discriminative) mean retrieval score estimator for post-retrieval QPP tasks. Using their proposed predictor (dubbed WPM2), the authors achieved the best reported QPP accuracy [12]. In this work, we further show a connection between our proposed passage-based QPP method and Roitman et al.'s [12] framework, where we utilize it to provide overall better prediction.
3 FRAMEWORK
3.1 Preliminaries
Let q denote a query and let C denote a corpus on which the query is evaluated. For a given text x (e.g., a document d  C or a passage
 d), let sq (x) denote the (retrieval) score assigned to x given q. In this work, we estimate each query q's performance using a post-retrieval approach [4]. Accordingly, let D denote the top-k documents in C with the highest retrieval score sq (d) as determined by the underlying retrieval method. The post-retrieval QPP task is to estimate p(D|q, r ) ­ the likelihood that D contains relevant information to query q [4].

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org.
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
© 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210070

3.2 QPP using passage information
Our goal is to predict a given query q's performance as accurate as possible. To this end, we propose to utilize passage information extracted from documents in D as an additional source for QPP. Our main hypothesis is that, relevant passage information obtained in D may provide valuable evidence whether a given retrieval was (overall) e ective or not.

893

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Our prediction framework is built on top of Kurland et al.'s [10]
probabilistic QPP framework. According to [10], p(D|q, r ) may be estimated1 as follows:

p^d

oc

(D

|q,

r

)

def
=

p(D|d, r )p(d |q, r ).

(1)

d D

p(d |q, r ) denotes document d's likelihood of being a relevant re-

sponse to query q. p(D|d, r ) denotes the likelihood that such rele-

vant response will be further included in D, which we assume in

this work to be uniformly distributed. Applying this assumption,

our QPP estimator can be de ned as follows:

p^ps

(D

|q,

r

)

def
=

p(d |q, r ).

(2)

d D

We next show how p^ps (D|q, r ) can be estimated using passage-

information. As a rst step, we note that:

p(d

|q,

r

)

def
=

p(q|d, r )p(r |d)p(d) p(r |q)p(q) .

(3)

The term p(r |d) denotes the likelihood that document d contains relevant information regardless of any speci c query. Using a MaxPs estimation approach [2], we now estimate this term as follows:

p^(r

|d )

def
=

max sq (

)p(r |

)p(

|d ).

(4)

d

sq ( ) is the query score assigned to passage ( d). p(r | ) represents the likelihood that passage contains relevant information.

We estimate this term as a combination of two sub-terms as fol-

lows:

p^(r |

)

def
=

H(

) · posBias(

).

def
H( ) = -

p(w | ) log p(w | ) is the entropy of passage 's

w

unsmoothed language model ­ preferring more diverse passages.

posBias(

)

def
=

1+

1 log(2+

.s) , where

.s denotes the start position

(in character o sets) of passage within its containing document.

Hence, posBias( ) prefers passages that are located as earlier as

possible within their containing documents.

p( |d) in Eq. 4 further captures the relationship between passage

and its containing document d, estimated as the Bhattacharyya

similarity between their unsmoothed language models:

p^(

|d )

def
=

p(w | )p(w |d).

w

We

next

assume

that,

p(q)

is

uniformly

distributed;

p (d )

def
=

1 |D |

is uniformly distributed over D;

and

sq

(d

)

def
=

p(q|d, r ).

Applying these assumptions back into Eq. 3 and using our deriva-

tion of p^(r |d) according to Eq. 4, we obtain our new estimator ac-

cording to Eq. 2 as follows:

p^ps

(D

|q,

r

)

def
=

1 |D|

max sq ( )p^(r | )p^( |d)

d

sq (d) ·

p(r |q)

. (5)

d D

1See Eq. 3 in [10].

Finally, we note that, similarly to many post-retrieval predic-

tors [4], p(r |q) is a query-sensitive normalization term, estimated

in

this

work

according

to

q's

length:

p^(r

|q)

def
=

|q |.

3.3 Connection with the WPM method

We conclude this section by showing that our proposed passageenhanced QPP method shares direct connection with the recently proposed mean retrieval score estimation framework of Roitman et al. [12]. According to [12], many previous post-retrieval predictors (e.g., Clarity [5], WIG [15], etc) share the following general form:

p^(D

|q,

r

)

def
=

1 |D|

d D

sq (d)

·

r , F

(d ),

where r, F (d)

def
=

fj (d) j is a Weighted Product Model

j

discriminative calibrator; with fj (d) is some retrieval feature and

j  0 denotes its relative importance [12].

According to Eq. 5, our predictor is essentially a calibrated mean

retrieval estimator [12]; Our predictor utilizes two calibration fea-

tures, namely

f1 (d )

= p^(r |d) (see Eq. 4) and

f2 (d )

=

1 p^(r |q)

;

both

features are assigned with equal weights of 1 = 2 = 1. Using this

connection in mind, we make two important observations. First,

by calibrating the two feature weights i ; i  {1, 2}, we might improve our own predictor's performance [12]. Second, we can utilize

f1(d) as a new passage-based calibration feature within Roitman et al.'s [12] QPP framework. As we shall shortly demonstrate, such

an extension indeed signi cantly boosts prediction performance;

even in cases where the prediction quality was already relatively

high.

4 EVALUATION 4.1 Datasets

Corpus #documents

Queries

Disks

TREC5

524,929

251-300

2&4

WSJ

173,252

151-200

1-2

AP

242,918

51-150

1-3

ROBUST 528,155 301-450, 601-700 4&5-{CR}

WT10g 1,692,096

451-550

WT10g

GOV2 25,205,179

701-850

GOV2

Table 1: Summary of TREC benchmarks.

The details of the TREC corpora and queries that we used for the evaluation are summarized in Table 1. These benchmarks were used by many previous QPP works [4, 12]. We evaluated two main types of queries, namely: short (keyword) queries and verbose queries. To this end, for short queries evaluation, we used the titles of TREC topics. For verbose queries evaluation, we further used the TREC topic descriptions, following [1, 8]. We used the Apache Lucene2 open source search library for indexing and searching documents. Documents and queries were processed using Lucene's English text analysis (i.e., tokenization, Porter stemming, stopwords, etc.).
2http://lucene.apache.org

894

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

As the underlying retrieval method we used Lucene's Dirichletsmoothed query-likelihood implementation, with its Dirichlet parameter xed to µ = 1000, following [10, 12].
4.2 Baselines
We compared our proposed QPP method (hereinafter referred to as PI3) with several di erent baseline QPP methods, as follows.
As a rst line of baselines, we compared with Clarity [5], WIG [15] and NQC [13], which are commonly used document-level postretrieval QPP methods [4]. The Clarity [5] method estimates query performance proportionally to the divergence between the relevance model [11] induced from D and the background model induced from C. The WIG method [15] estimates query performance according to the di erence between the average retrieval score in D and that of C. The NQC [13] method estimates query performance according to standard deviation of the retrieval scores of documents in D, further normalized by the corpus score sq (C).
As alternative QPP methods that also utilize passage information, we closely followed [6, 9] and implemented the passage-level counterparts of the three former baselines; denoted Clarity(psg) [6, 9], WIG(psg) [9], NQC(psg) [9], respectively.
As a very strong document-level baseline, we further compared with the WPM2 method [12]. WPM2 utilizes 10 di erent document retrieval score calibration features4, whose weights need to be learned [12].
Following the rst observation we made in Section 3.3, we also implemented a calibrated version of our PI method (denoted C-PI). Finally, following the second observation we made in Section 3.3, we further extended the WPM2 method with our new passagelevel calibration feature (denoted WPM2+PI).

4.3 Setup

We evaluated the various methods using two di erent query set-

tings: once using the short (title) queries and once using the ver-

bose (description) queries. On each setting, we predicted the perfor-

mance of each query based on its top-1000 retrieved documents [4].

Following a common practice [4], we assessed prediction over queries

quality according to the Pearson's- correlation between the pre-

dictor's values and the actual average precision (AP@1000) values

calculated using TREC's relevance judgments.

In order to realize our predictor in Eq. 5, according to Eq. 4, for

each document d( D), we need to obtain the passage  d with

the highest likelihood (score) p^(r |d). To this end, given document

d( D), we rst extracted candidate passages from it using a xed

L = 500 characters-windowing approach [14]; and then scored the

candidates according to Eq. 4. We used Okapi-BM25 as our choice

of sq ( ), with k1 = 0.8 and b = 0.3, following [7]. Most of the methods that we evaluated (and among them the PI

and WPM2 variants) required to tune some free parameters. Com-

mon

to

all

methods

is

the

free

parameter k

def
=

|D|, which is the

number of top scored documents (out of a total of 1000 retrieved

documents) to be used for the prediction. To this end, for each

method we selected k  {5, 10, 20, 50, 100, 150, 200, 500, 1000}.

3PI stands for "Passage Information". 4The full list of features is described in Section 5 of [12].

TREC5 WSJ AP WT10g Robust GOV2

Clarity WIG NQC

.490bp .347p .483bp

.607p .677b .718b

.596p .526bp .554bp

.380bp .434bp .486p

.477p .411bp .575b

.407p .535b .432bp

Clarity(psg) WIG(psg) NQC(psg)

.204bp .344p .292bp

.629p .576bp .497b

.622p .397bp .304bp

.289bp .494bp .488p

.477p .435bp .401bp

.395p .468b .220bp

PI C-PI

.567pc .613c

.677 .718c

.684pc .694c

.518pc .532c

.577pc .585c

.465c .501c

WPM2 WPM2+PI

.738w .787w

.725w .738w .743w .764w

.540w .557w

.640w .647w

.655w .661w

Table 2: Results of prediction over short queries. The super-

script b denotes a statistically signi cant di erence between

one of the rst document-level baselines and its passage-

level counterpart. The superscript p denotes a signi cant dif-

ference between PI and the six rst baselines. The subscript

c denotes a signi cant di erence between PI and C-PI. The

subscript w denotes a signi cant di erence between WPM2

and WPM2+PI.

To implement the three passage-level alternatives (i.e., Clarity(psg),WIG(psg)

and NQC(psg)), we rst used the same window-based passage ex-

traction approach [14] and ranked candidate passages extracted

from the various documents in D according to their Okapi-BM25

score with k1 = 0.8 and b = 0.3 [7]. We then used the top-m scored

passages over D for prediction [6, 9], with m  {5, 10, 20, 50, 100, 150, 200}.

For Clarity and Clarity(psg), following [5, 6, 9], we further clipped

the induced relevance model at the top-n terms cuto , with n 

{5, 10, 20, 50, 100, 150, 200}.

To learn the calibration feature weights of C-PI, WPM2 and

WPM2+PI, following [12], we used a Coordinate Ascent approach.

Similar to [12], we selected the feature weights {j }hj=1 in the grid [0, 5]h with a step size of 0.1 within each dimension, with h 

{2, 10, 11} such di erent features implemented within the C-PI,

WPM2 and WPM2+PI methods, respectively. Further following [12],

feature

values

were

smoothed

fj (d; )

def
=

max(fj (d), ), where

 = 10-10 is a hyperparameter.

Following [12, 13], we trained and tested all methods using a

holdout (2-fold cross validation) approach. Accordingly, on each

benchmark, we generated 30 random splits of the query set; each

split had two folds. We used the rst fold as the (query) train set.

We kept the second fold untouched for testing. We recorded the

average prediction quality over the 30 splits. Finally, we measured

statistical signi cant di erences of prediction quality using a two-

tailed paired t-test with (Bonferroni corrected) p < 0.05 computed

over all 30 splits.

4.4 Prediction over short queries
The results of our rst evaluation setting with short queries are summarized in Table 2. First, we observe that, the three rst documentlevel QPP baselines (i.e., Clarity WIG and NQC) and their passagelevel counterparts (i.e., Clarity(psg) WIG(psg) and NQC(psg)) exhibit a mixed relative performance. This serves as a rst evidence that passage-information is an important QPP signal.

895

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

TREC5 WSJ AP WT10g Robust GOV2

Clarity WIG NQC

.254p .011bp .271bp

-.005bp .368bp
.642

.174bp .510p .528bp

.381bp .377p .397bp

.288bp .278bp .461bp

.281bp .303bp .360p

Clarity(psg) WIG(psg) NQC(psg)

.256p .292bp .338bp

.188bp .588bp
.640

.198bp .526p .545bp

.267bp .377p .458bp

.468bp .446bp .499bp

.446bp .343bp .361p

PI C-PI

.512p .528

.655pc .678c

.637pc .689c

.464pc .476c

.559pc .624c

.460pc .482c

WPM2

.732

WPM2+PI .740

.650 .621w .406w .587w .509w .660 .729w .487w .602w .521w

Table 3: Results of prediction over verbose (long) queries.

The superscripts and subscripts notations are identical to

those de ned in Table 2.

verbose query performance prediction settings. Moreover, such a strategy guarantees a more robust QPP, which is less sensitive to query type. Finally, again, we can observe that, by further calibrating PI (i.e., C-PI), even better prediction quality can be achieved. Moreover, the contribution of PI's passage-level calibration feature p^(r |d) to WPM2+PI is even more notable in this setting.
5 CONCLUSIONS
The conclusions of this work are two-fold. First, this work clearly demonstrates that existing post-retrieval QPP methods that only focus on document-level features are not well suited to the prediction of verbose queries performance. Utilizing passage-information for such QPP sub-task is clearly important. As we further demonstrated, a mixed strategy that considers both types of features, such as the one employed by the PI variants, may result in an enhanced prediction, which is less sensitive to query type.

Next, in most cases, our newly proposed method, PI, had a better prediction. We note that, while the former baselines either uti-

REFERENCES
[1] Michael Bendersky and W. Bruce Croft. Modeling higher-order term dependencies in information retrieval using query hypergraphs. In Proceedings of the 35th

lize only document-level or only passage-level features, PI basi-

International ACM SIGIR Conference on Research and Development in Information

cally utilizes both feature types. This, therefore, supports again the importance of passage-level QPP signals for the document-level

Retrieval, SIGIR '12, pages 941­950, New York, NY, USA, 2012. ACM. [2] Michael Bendersky and Oren Kurland. Utilizing passage-based language models
for document retrieval. In Proceedings of the IR Research, 30th European Confer-

QPP task. Furthermore, by calibrating the two "features" of PI ac-

ence on Advances in Information Retrieval, ECIR'08, pages 162­174, Berlin, Hei-

cording to our rst observation in Section 3.3, we obtained an enhanced performance of C-PI over PI.

delberg, 2008. Springer-Verlag. [3] James P. Callan. Passage-level evidence in document retrieval. In Proceedings
of the 17th Annual International ACM SIGIR Conference on Research and Devel-

Overall, WPM2 was the best document-level only QPP method. Yet, further following the second observation we made in Section 3.3,

opment in Information Retrieval, SIGIR '94, pages 302­310, New York, NY, USA, 1994. Springer-Verlag New York, Inc. [4] David Carmel and Oren Kurland. Query performance prediction for ir. In Pro-

we can observe that, by solely adding the single passage-level fea-

ceedings of the 35th International ACM SIGIR Conference on Research and Develop-

ture of p^(r |d) (see again Eq. 4) to WPM2, which resulted in the WPM2+PI extension, has obtained a signi cant boost in predic-

ment in Information Retrieval, SIGIR '12, pages 1196­1197, New York, NY, USA, 2012. ACM. [5] Steve Cronen-Townsend, Yun Zhou, and W. Bruce Croft. Predicting query per-

tion quality. This is yet another strong testimony on the impor-

formance. In Proceedings of the 25th Annual International ACM SIGIR Conference

tance of passage-level QPP signals for the document-level QPP task.

on Research and Development in Information Retrieval, SIGIR '02, pages 299­306, New York, NY, USA, 2002. ACM. [6] Steve Cronen-Townsend, Yun Zhou, and W Bruce Croft. Precision prediction

based on ranked list coherence. Information Retrieval, 9(6):723­755, 2006.

4.5 Prediction over verbose queries

[7] Mathias Géry and Christine Largeron. Bm25t: A bm25 extension for focused information retrieval. Knowl. Inf. Syst., 32(1):217­241, July 2012.

The results of our second evaluation setting with verbose (long) queries are summarized in Table 3. First, comparing these results

[8] Manish Gupta and Michael Bendersky. Information retrieval with verbose queries. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '15, pages 1121­1124,

with the results in Table 2, it becomes clear that those baseline methods that only utilize document-level features, perform signif-

New York, NY, USA, 2015. ACM. [9] Eyal Krikon, David Carmel, and Oren Kurland. Predicting the performance of
passage retrieval for question answering. In Proceedings of the 21st ACM Interna-

icantly worse in this setting compared to their performance over

tional Conference on Information and Knowledge Management, CIKM '12, pages

short queries. Moreover, those QPP methods that utilize passagelevel information (i.e., Clarity(psg), WIG(psg), NQC(psg), PI, C-

2451­2454, New York, NY, USA, 2012. ACM. [10] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri
Rom. Back to the roots: A probabilistic framework for query-performance pre-

PI and WPM2+PI) provide signi cantly better prediction. This

diction. In Proceedings of the 21st ACM International Conference on Information

demonstrates that, utilizing passage-information for QPP becomes even more eminent in verbose query settings. Verbose queries are

and Knowledge Management, CIKM '12, pages 823­832, New York, NY, USA, 2012. ACM. [11] Victor Lavrenko and W. Bruce Croft. Relevance based language models. In

usually more informative than short queries [8]; yet, existing document-

Proceedings of SIGIR '01.

level QPP methods are not well-designed to predict their quality.

[12] Haggai Roitman, Shai Erera, Oren Sar-Shalom, and Bar Weiner. Enhanced mean retrieval score estimation for query performance prediction. In Proceedings of

Verbose queries tend to express more focused information needs [8];

the ACM SIGIR International Conference on Theory of Information Retrieval, ICTIR

hence, passage-information may provide a better "proxy" to such needs satisfaction within retrieved documents [1].

'17, pages 35­42, New York, NY, USA, 2017. ACM. [13] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits.
Predicting query performance by query-drift estimation. ACM Trans. Inf. Syst.,

Further notable is that, compared to the six rst baseline meth-

30(2):11:1­11:35, May 2012.

ods, PI provided signi cantly better prediction quality; and even exceeded in some of the cases that of WPM2 ­ a very strong

[14] Stefanie Tellex, Boris Katz, Jimmy Lin, Aaron Fernandes, and Gregory Marton. Quantitative evaluation of passage retrieval algorithms for question answering. In Proceedings of SIGIR '03.

document-level QPP baseline. This serves as another strong evidence that, a mixed document-level and passage-level prediction

[15] Yun Zhou and W. Bruce Croft. Query performance prediction in web search environments. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '07, pages

strategy, such as the one employed by PI, is a better choice for

543­550, New York, NY, USA, 2007. ACM.

896

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

IRevalOO: An Object Oriented Framework for Retrieval Evaluation

Kevin Roitero
University of Udine Udine, Italy
roitero.kevin@spes.uniud. it

Eddy Maddalena
University of Southampton Southampton, U.K.
e.maddalena@soton.ac.uk

Yannick Ponte
University of Udine Udine, Italy
ponte.yannick@spes.uniud. it

Stefano Mizzaro
University of Udine Udine, Italy
mizzaro@uniud.it

ABSTRACT
We propose IRevalOO, a flexible Object Oriented framework that (i) can be used as-is as a replacement of the widely adopted trec_eval software, and (ii) can be easily extended (or "instantiated", in framework terminology) to implement different scenarios of test collection based retrieval evaluation. Instances of IRevalOO can provide a usable and convenient alternative to the state-of-the-art software commonly used by different initiatives (TREC, NTCIR, CLEF, FIRE, etc.). Also, those instances can be easily adapted to satisfy future customization needs of researchers, as: implementing and experimenting with new metrics, even based on new notions of relevance; using different formats for system output and "qrels"; and in general visualizing, comparing, and managing retrieval evaluation results.
CCS CONCEPTS
· Information systems  Test collections;

most common: it is used to evaluate the results of the participants to TREC competitions, as well as in other initiatives. trec_eval serves well its purpose but it is not free from limitations.
In this paper, we present a more general framework that aims to extend it, as well as similar IR evaluation software and tool-kits. Our system, called IRevalOO, is conceived according to the ObjectOriented (OO) programming paradigm and, more precisely, it is an OO framework. Besides allowing system evaluations as trec_eval, by exploiting the advantages of OO frameworks, IRevalOO offers a set of useful features: the easy implementation of new custom evaluation metrics, the management of multiple types of measurement scales, the handling of different input formats, and the customization of measurement sessions and results visualisation. The paper is structured as follows: Section 2 describes trec_eval and other evaluation tools, as well as object oriented frameworks, Section 3 presents IRevalOO and its evaluation, Section 4 concludes the paper.

KEYWORDS
TREC, evaluation, test collections, trec_eval
ACM Reference Format: Kevin Roitero, Eddy Maddalena, Yannick Ponte, and Stefano Mizzaro. 2018. IRevalOO: An Object Oriented Framework for Retrieval Evaluation. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210084
1 INTRODUCTION
In Information Retrieval (IR), effectiveness evaluation is an essential process. A widely-adopted methodology is evaluation by means of a test collection, which consists of: a set of documents, a test suite of information needs descriptions (called queries or topics), and the ground-truth of a set of relevance judgements made by experts for a subset of the topic-document pairs. Campaigns such as TREC, NTCIR, FIRE, CLEF, INEX, etc. evaluate effectiveness of systems by comparing their output with the ground-truth, using standard or custom evaluation metrics, often several of them working in different configurations. To facilitate the entire evaluation process, ad-hoc software tools have been created. trec_eval is probably the
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210084

2 BACKGROUND
Trec_eval, and Other Evaluation Tools. The need for evaluating the performance of systems has led to the creations of many toolkits and software for facilitating the measurement process. Trec_eval is probably a sort of de facto standard. Born in the early 90s, maintained by NIST, it is considered a milestone of the evaluation software and it inspired most of the other toolkits used in the different initiatives. trec_eval evaluates system effectiveness by comparing the systems results with a ground truth consisting of a set of relevance judgements expressed by human experts. trec_eval takes in input the qrels files, which are the systems output in the TREC format, the and a set of runs where each run consists of an execution of a system over a topic. trec_eval returns as output various measures obtained using different metrics, like Mean Average Precision (MAP), R-Precision, and many others. trec_eval allows to: specify the metric(s) to use; specify the format for the input and for the output files; and compute the evaluation for each topic. It provides some useful parameters to customize the evaluation process (as described in the README file), although these parameters are sometimes difficult to use and configure by the users.
Trec_eval is a valuable tool for the IR research community, but it is not free from limitations: it is written in C, using legacy technologies, and it is not truly cross platform; researchers face some difficulties during its installation, configuration, and customization; customization is far from being simple and agile, and a user who needs to implement new features, or modify those already implemented, has to modify and even re-write several software modules. This can be a time consuming activity which also requires advanced programming skills. These limitations are probably the reasons to develop other evaluation tools. Most of them though adopt a

913

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Yannick Ponte, and Stefano Mizzaro

Framework

Unused library framework classes

Framework library

Framework core

Application extensions

call

call call

call

Framework

Application

Application code

Figure 1: Framework architecture and Inversion of control.

sort of re-implementation approach, like the TREC_Files Evaluator [2], Pytrec eval [10], sometimes adding some functionalities, like Terrier [8], MATTERS [6], or adapting to a different domain like NTCIREVAL [9]. We propose a different approach: to implement an evaluation tool as an Object Oriented (OO) framework. Object-Oriented Frameworks. An OO framework is a reusable, semi-complete application that can be specialized to produce custom applications, and consists in an abstract core representation of a specific domain [3, 4, 7]. A framework is designed to be specialised by the developer, who can extend its core to his/her specific application, which is called instance or extension. Figure 1 (left) shows the general structure of a framework. Differently from the traditional software libraries, which are invoked by the developer's code, the modules written by the developer will be invoked by the framework: this is called "inversion of control" (see Figure 1, right).
OO frameworks provide a set of advantages to both developers (i.e., users of the framework) and end-users (i.e., users of the instances of the framework) like modularity, reusability, extensibility [3]. OO frameworks define generic and high-level components that can be re-used by developers to create new applications; this avoids to waste resources in re-implementing the same solutions to similar problems, which is a quite common practice in many domains.

3 IREVALOO
We now turn to presenting our IRevalOO software. We discuss motivations and aims, usage scenarios, requirements, design, examples of use, and evaluation. IRevalOO (currently in beta release and still undergoing some refactoring, tuning, and optimizations) is implemented in Java 8 and it can be freely downloaded at https://github.com/IRevalOO/IRevalOO_Software. Motivations and Aims. Both individual researchers and organizers of IR evaluation initiatives can obtain multiple benefits by the redesign of their evaluation software as an OO framework. The missing functionalities can be easily implemented by instances of the framework. To extend and customize an OO framework is much easier than modifying the original evaluation software, which is often written in a low-level language that is not as abstract as the OO paradigm. trec_eval is a concrete example that exemplifies these remarks: its limitations, such as the customization of the file loader, the results manager, the possibility to visually compare different metrics, the possibility to define and test new metrics, relevance, etc. can be easily implemented by IRevalOO instances. trec_eval is written in C and, due to the lack of an adaptive design, it is not naturally suitable to be extended to specific evaluation applications.
We propose IRevalOO, an OO framework that: (i) can be used as-is as a replacement of trec_eval, and (ii) can be easily extended/instantiated to implement different scenarios of test collection based retrieval evaluation. IRevalOO, by exploiting several advantages of

OO paradigm like design patterns and OO frameworks in general, aims to be flexible by allowing its own extensions which let users to define new items, such as new metrics, relevance kinds, input and output formats, etc. We also aim at an easily configurable and customizable tool, in terms of, e.g., selecting output visualization, summarization, and export formats, and/or which metric, which topic or run sets should be included in the evaluation. Three Scenarios. A first typical scenario of IRevalOO involves a user who wants to evaluate his/her own system on a test collection of a previous TREC edition by using the standard evaluation metrics. This need can be satisfied by both trec_eval and IRevalOO. The situation changes considering a second more complex scenarios. If the user has more specific needs, as the definition of a new input format, a new summary representation, or a different set of metrics, maybe even based on a new kind of relevance, trec_eval as-is would not be adequate. On the contrary, IRevalOO can easily be adapted to the user needs. A third interesting scenario can involve a user who needs to implement and test a novel metric called Uncertain MAP (UMAP), consisting in a variation of MAP that takes into account uncertainty in the relevance judgements. Requirements and Design. On the basis of the aims and scenarios, we can list the following functional requirements of IRevalOO: import a test collection (i.e., documents, topics, runs), selecting the desired source and the input format; import more than one run, with the option of consider more than one system; allow the creation of new metrics and/or new metric sets; allow a customized management of results visualization and export format; create a summary report and provide a verbose log of the evaluation process; guarantee the compatibility with the original format of TREC commands; and create and manage a different kind of relevance.
The UML diagram shown in Figure 2 summarizes the structure of IRevalOO. It shows the main packages and classes of the software (about 50% of IRevalOO classes are in the diagram). IRevalOO is made up of five main packages (plus an exceptions package not shown). The control package is the controller of the framework. It contains the EvaluationManager class, that embodies the overall workflow: it uses the classes and methods in the other packages and manages the data flow as well. The other packages usually contain abstract classes to be implemented by the specific application. For example, the package testcollection contains the abstract classes Topic and Document that, with Qrel, form the Collection.
The package relevance contains RelevanceType, an abstract class that models the abstract concept for different kinds of relevance, in this case "Binary", "Numeric", and "Category". The relevance defined in the instance of the framework can be modelled as one of these categories. The package run models the concept of a run, or rather an evaluation of a system over a set of topics. The package metric contains two sub-packages: metrics.definition, containing the classes to define new metrics, and metrics.results, that uses the defined metrics to compute the metric values over the runs and export the results of the computation.
The packages run and metrics.results use some abstract classes (e.g., ResultExporter) to provide a set of methods which can be used to customize both the loader and the exporter of the data, adapting them to new formats. For example in this way a developer can easily import the run files from an XML file, a database, etc. Examples of Use. We briefly describe how IRevalOO can be used by application developers. To use the framework, the developer has

914

Short Research Papers I IRevalOO: An Object Oriented Framework for Retrieval Evaluation

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

control

use

use

use

EvaluatorManager compute

«abstract» RunSet

run
«abstract» Run

TopicRun

«abstract» RunLine

AdHocTRECRunSet

AdHocTRECRun

AdHocTRECRunLine

relevance
«interface» RelevanceType

<<use>> Relevance <<use>>

metrics.definitions

0..*

«interface»

MetricComponent

metric

metrics.results

0..*

«abstract»

ResultComponent

manage

MetricSet

«abstract»

ResultSet

Metric save in

«abstract» Result

BinaryRelevanceType CategoryRelevanceType NumericRelevanceType

AdHocTRECCollection
N «abstract» Document

testcollection «abstract» Collection
N «abstract»
Topic

AdHocDocument

AdHocTopic

TopicQrel
N Qrel

Metric1 Metric2 save in

NumericResult NumericArrayResult
<<istantiate>>
«interface» ResultManager

create

«abstract» MetricSetBuilder

StadardTRECMetrics

«abstract» ResultExporter

«abstract» ResultViewer

FileResultExporter CompleteResultViewer

Figure 2: UML diagram of the main components of IRevalOO.

1 public static void main(String[] args) {

2 String qrel = ".../ qrels.txt";

3 String run = ".../run";

4 Collection c = new AdHocTRECCollection(new

5

NumericCategoryRelevanceType (7,2), "","",qrel);

6 RunSet r = new

7

AdHocTRECRunSet(new NumericRelevanceType(),run);

8 MetricSet m = new MetricSet();

9 m.add(new BPref());

10 m . add ( new PatN (10) ) ;

11 m . add ( new R () ) ;

12 EvaluatorManager em = new EvaluatorManager (c ,r , m ) ;

13 em . evaluate () ;

14 em . showResults ( new OverAllResultViewer () ) ;

15 String outFile = ".../ example . out ";

16 em . exportResults ( new FileResultExporter ( outFile ) ) ;

17 }

Figure 3: The Java code for a typical usage of IRevalOO.

to create instances of it. An instance which reflects the first typical scenario described above follows the schema detailed in Figure 3: 1. instantiate the collection (line 4); 2. instantiate the run set (line 6); 3. instantiate the metric set (line 8); 4. instantiate an Evaluation manager, which takes in input the test
collection, the metrics, and the runs (line 12); 5. (optional) set/customize the options for the Evaluation manager; 6. start the retrieval evaluation (line 13); 7. manage the results according to the user preferences (line 15).
IRevalOO is also adequate for the other two scenarios previously described: Figure 4 shows an example of a new category of relevance definition, where the binary relevance judgement is enriched with a degree of uncertainty; and Figure 5 shows the skeleton of a class implementing the possible new metric UMAP. Evaluation: Correctness and Efficiency. Evaluation of a framework is not simple and can usually be done with precise results only at the end of life cycle of the framework, when it has no practical usefulness [1, 3­5]. However, we can provide an evaluation

1 public class UncertaintyRelevanceType

2

implements RelevanceType {

3 public UncertaintyRelevanceType() {}

4 public double readValue(Object obj)

5

throws IRevalOOException {

6

if (obj.value.equals("-")) {return -1;}

7

else if (obj.value.equals("NOT RELEVANT")){

8

return 0;

9

} else if(obj.value.equals("RELEVANT")){

10

return 1;

11

} else {

12

throw new IRevalOORelevanceException(

13

"unreadable category relevance " + obj.value);

14

}

15 }

16 public double readUncertainty ( Object obj )

17

throws IRevalOOException {

18

if(obj.confidence >= 0 && obj.confidence <= 1)

19

return obj.confidence;

20

else {

21

throw new IRevalOORelevanceException(

22

"unexpected confidence value " +

23

obj.confidence);

24

}

25 }

26 public String toString ( Object obj ) {

27

return "The object is " + obj.value +

28

" with a confidence of " +obj.confidence;

29 }

30 }

Figure 4: The code to create a new kind of relevance.

of IRevalOO in terms of some software metrics, correctness and efficiency; as well as exploiting trec_eval as a comparison baseline.
Table 1 shows some software metrics of trec_eval, both complete (in the second column) and the part of it corresponding to IRevalOO implementation (in the third column), and IRevalOO (fourth column). IRevalOO is not very complex (80 classes in total), and it features fewer lines of code than trec_eval, both when considering trec_eval full implementation and when not taking into account the part of trec_eval that has not been implemented in IRevalOO yet. This is mainly due to the fact that Java is a slightly higher level

915

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Eddy Maddalena, Yannick Ponte, and Stefano Mizzaro

1 public class UMAP extends Metric {

2 public UMAP() {

3

acronym = "UMAP";

4

completeName = "Uncertain Mean Average Precision";

5

...

6}

7 public Result computeTopicResult(TopicRun topicRun ,

8

Collection c) {

9

for (int i = 0; i < retrieved; i++) {

10

... // Iterate over retrieved docs

11

RunLine rl = topicRun.getRun().get(i);

12

Qrel q =

13

c.getQrel(rl.getIdDoc(),rl. getIdTopic());

14

... // The core part of the metric

15

}

16

umap = ... // UMAP computation

17

return new NumericResult(topicRun.getIdTopic(),

18

this , umap);

19 }

20 public Result computeOverallResult ( ResultSet res ,

21

Collection c) {

22

return NumericResult.arithmeticMeanResults(res ,

23

this , c);

24 }

25 }

Figure 5: The skeleton of the code to create a new metric.

Table 1: Statistical data

trec_eval (C)trec_eval (impl.)IRevalOO

Lines of code

8030

Lines of comment

1796

Total lines

9826

Classes / Methods

-/-

5087 2475

1158

988

5993 3463

- / - 80 / 260

language than C and allows to create a more compact source code -- that is more easy to understand and maintain as well.
Concerning correctness, IRevalOO has been tested on all trec_eval examples and on many real TREC test collections: the adHoc tracks of TREC2, 3, 5, 6, 7, 8, and TREC2001, the Robust tracks of 2004 and 2005, the Terabyte tracks of 2004­2006, the Web tracks of 2011­ 2014, and the Million Query tracks of 2007­2009. It has been tested with different relevance levels (binary, three-level, etc.), and it always provides exactly the same results as trec_eval. Concerning efficiency, IRevalOO has been compared to trec_eval using data from two real TREC tracks: TREC8 AdHoc (AH99), and TREC2007 Million Query (MQ07). These two datasets are complementary: one features a high number of runs and the other one a high number of topics. Then, to run a sort of stress test, we created an artificial collection featuring 2000 topics and 200 runs. It has been created considering 1000 documents retrieved by each system, and a set of relevance judgements allocated considering a plausible distribution; although artificial, this collection is realistic, and it represents a sort of "worst case" scenario. Datasets details are shown in Table 2.
We measured the execution time on an ordinary laptop: a MacBook Pro, Retina, 13-inch, Mid 2014, 3 GHz Intel Core i7 processor, 16 GB of 1600 MHz DDR3 RAM, SSD drive). IRevalOO turns out to be always slower than trec_eval. However, trec_eval speed-up is much smaller for large datasets: it goes from around 1.7 times for AH99 to around 1.2 times (1 would mean no speed-up) for the larger artificial collection. When the amount of data and the overall execution time grow, trec_eval and IRevalOO time performance are of the same order of magnitude. IRevalOO time performance seems reasonable: the user will not be too harmed when going from 11s to 19s. Indeed, the difference is not too large. Also, it is important to remark that trec_eval has probably undergone multiple efficiency tuning and improvements during its more than twenty

Table 2: Efficiency test: datasets and results.

Dataset no. topics no. systems trec_eval IRevalOO

AH99 MQ07 Artificial

50 1,000
2,000

129

11s

19s

29

235s

490s

200

709s

848s

years lifetime, whereas IRevalOO has not been carefully optimized yet (although we did pay some attention to efficiency by, for example, adopting Hash Maps and relying on memoization techniques). Furthermore, the small lack of efficiency is of course balanced by the new added functionalities offered by IRevalOO, that should hopefully save the, probably more precious, researcher's time.
4 CONCLUSIONS AND FUTURE WORK
We presented IRevalOO, an Object Oriented framework that can replace and extend the well known trec_eval software as well as other evaluation tools. IRevalOO allows the users to easily implement instances and define, when needed, both new main components as relevance or metrics and useful features for evaluation analysis. Instances of the framework can also be easily created to replace the software used in other initiatives of retrieval evaluation. In the future we plan to provide different instances which will emulate and extend the software used in other initiatives (like, e.g., NTCIREVAL). For the future, after defining a proper set of test cases using a unit test suite like JUnit, we plan to do some refactoring and optimization, as well as to extend the framework by modelling other domain aspects like the abstract concepts of "document" and "topic" which will allow the user to customize test collection components, to experiment with new test collections, and to adapt the framework to specific needs of other initiatives. Furthermore, we intend to provide a graphic user interface that will allow less expert users to interact with the framework and its instances. Finally, going back to the efficiency issue, we plan to use profiling and various optimization techniques to fine-tune IRevalOO and improve its efficiency if needed. Given the public and free release of the software we expect feedback and improvements from the community.
REFERENCES
[1] Jan Bosch, Peter Molin, Michael Mattsson, and PerOlof Bengtsson. 2000. Objectoriented Framework-based Software Development: Problems and Experiences. ACM Comput. Surv. 32, 1es, Article 3 (March 2000).
[2] Savvas A. Chatzichristofis, Konstantinos Zagoris, and Avi Arampatzis. 2011. The TREC Files: The (Ground) Truth is out There. In Proc. of the 34th ACM SIGIR. ACM, New York, NY, USA, 1289­1290. https://doi.org/10.1145/2009916.2010164
[3] Mohamed Fayad and Douglas C Schmidt. 1997. Object-oriented application frameworks. Commun. ACM 40, 10 (1997), 32­38.
[4] Garry Froehlich, James Hoover, Ling Liu, and Paul Sorenson. 1998. Designing object-oriented frameworks. University of Alberta, Canada (1998).
[5] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design Patterns: Elements of Reusable Object-oriented Software. Addison-Wesley, Boston, MA, USA.
[6] Information Management Systems Research Group. 2017. MATTERS. http: //matters.dei.unipd.it. Last access: 2017-01-22.
[7] Ralph E Johnson and Brian Foote. 1988. Designing reusable classes. Journal of object-oriented programming 1, 2 (1988), 22­35.
[8] University of Glasgow. 2017. TERRIER homepage. http://terrier.org. Last access: 2017-01-22.
[9] Tetsuya Sakai. 2017. NTCIREVAL home page. http://research.nii.ac.jp/ntcir/ tools/ntcireval-en.html. Last access: 2017-01-08.
[10] Alberto Tonon. 2017. Pytrec Eval download. https://github.com/eXascaleInfolab/ pytrec_eval. Last access: 2017-01-08.

916

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Index Compression for BitFunnel Query Processing

Xinyu Liu, Zhaohua Zhang, Boran Hou, Rebecca J. Stones, Gang Wang, Xiaoguang Liu
College of Computer and Control Engineering, Nankai University, China {liuxy,zhangzhaohua,houbr,becky,wgzwp,liuxg}@nbjl.nankai.edu.cn

ABSTRACT
Large-scale search engines utilize inverted indexes which store ordered lists of document identifies (docIDs) relevant to query terms, which can be queried thousands of times per second. In order to reduce storage requirements, we propose a dictionarybased compression approach for the recently proposed bitwise data-structure BitFunnel, which makes use of a Bloom filter. Compression is achieved through storing frequently occurring blocks in a dictionary. Infrequently occurring blocks (those which are not represented in the dictionary) are instead referenced using similar blocks that are in the dictionary, introducing additional false positive errors. We further introduce a docID reordering strategy to improve compression.
Experimental results indicate an improvement in compression by 27% to 30%, at the expense of increasing the query processing time by 16% to 48% and increasing the false positive rate by around 7.6 to 10.7 percentage points.

introduced BitFunnel, a bitmap-like data structure based on a Bloom filter [7].
The underlying data structure is split into shards according to document length, and each shard comprises of a collection of mapping matrices. Each term maps to a row or a few rows in the mapping matrices in each shard (as determined by Bloom filter multiplexing). Ordinarily, for each i  {0, 1, . . . , 6}, there is a mapping matrix of rank i. A column of the rank-i mapping matrix corresponds to a set of 2i documents, and the columns of the rank-0 mapping matrix corresponds to individual documents. This is illustrated in a toy example in Figure 1.

query
term 1 term 2

0000100000101000010000010000010000000001010000010110110000000011001001110010 1000001011011000100010100101000010010100010000000100100000011010100000100000 0001000000011001100000100000001100000000000111000100010010000101001100110000
. . . 00001000001100000100011101011001001000 10000000000001000000000001001000000101 10010000000100010010000001000000000010 . . .

rank 0
rank 1 rank 2

CCS CONCEPTS
· Information systems  Information retrieval query processing; Search engine indexing; Search index compression;
KEYWORDS
BitFunnel; Bloom filter; compression; query processing
ACM Reference Format: Xinyu Liu, Zhaohua Zhang, Boran Hou, Rebecca J. Stones, Gang Wang, Xiaoguang Liu. 2018. Index Compression for BitFunnel Query Processing. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210086
1 INTRODUCTION
The main index structure in many current search engines[1] is the inverted index. However, motivated by the high efficiency of bitwise operations, predecessors [3, 5] combined bitmaps and inverted indexes to speed up query processing. Recently, Goodwin et al. [2]
Supported by NSF China grant 61602266, the Science and Technology Development Plan of Tianjin grants 17JCYBJC15300 and 16JCYBJC41900 and the Thousand Youth Talents Plan in Tianjin.
Corresponding authors: Gang Wang and Xiaoguang Liu.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210086

0001110000000000100 0001110110010001100 0001000101010010100
. . .
...
Figure 1: A toy figure of the mapping matrices in a shard, and which rows need to be intersected to process a 2-term query.
In each shard, we independently compute the query intersection results. Roughly speaking, we intersect the relevant rows of equal rank, and we concatenate each rank's result with itself (thereby doubling its length), which is intersected with the next-lower rank intersection results. We do this until we reach rank 0, which determines the final intersection results.
To reduce the impact of false positives, we ensure all the mapping matrices are sparse; this is achieved by varying the number of rows (which affects the size of the mapping matrices, and thus the overall storage requirements).
There is some work on speeding up bitwise operations for BitFunnel [6], however BitFunnel's collection of mapping matrices is costly in terms of space, and is typically larger than the corresponding inverted index. Motivated by these observations, we propose a method for compressing BitFunnel mapping matrices. More specifically:
(1) We propose a dictionary-based compression method for BitFunnel mapping matrices, whereby frequently occurring blocks are replaced by indices to their corresponding blocks in a dictionary.
(2) We also design a document reordering strategy to increase the redundancy in the bitmap structure, reduce the additional false positives to facilitate compression.

921

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

2 MAPPING MATRIX COMPRESSION
We propose a dictionary-based compression method, where each mapping matrix of each shard is compressed independently (with each mapping matrix having its own dictionary). We use b-bit indices to represent k-bit blocks as dictionary references. We illustrate this method using a toy example in Figure 2. Specifically:
· We add a reference for the k-bit all-1 block and 2b - 1 most frequently occurring k-bit blocks to the dictionary. We say a k-bit block is represented if it has a dictionary reference; otherwise we say it is unrepresented.
· In the mapping matrix, every k-bit block is replaced by a dictionary index, where:
(1) represented blocks are replaced by their corresponding dictionary index, while
(2) unrepresented blocks b are replaced by a dictionary index which references a sparsest k-bit block b which has a 1 wherever b has a 1.
For example, in Figure 2, the highlighted dictionary reference is
10  1100,
and we have b = 2 and k = 4. The 4-bit block 1100 is represented, so in the mapping matrix, we replace it by its dictionary index (namely 10). However, the block 0010 is not represented, and in the mapping matrix it is replaced by the dictionary index for 1010 (namely 11).

documents A B C D E F G H I J K L MN O P 1 1110001011101110 2 0000110011001010 3 0101001101000011 4 1100101000011010
block
(a) An uncompressed rank-0 mapping matrix, partitioned into k -bit blocks.

pattern ID
00 01 10 11

definition
1111 1110 1100 1010

AD EH I LMP
1 01110101 2 10101011 3 00001000 4 10110011

index

(b) The dictionary (left) and the compressed mapping matrix (right) partitioned into b-bit indices.

Figure 2: A toy example of the proposed compression method where k-bit blocks are compressed into b-bit indices (where k = 4 and b = 2). Unrepresented k-bit blocks have blue-colored b-bit indices.

When performing query processing, we have an additional decompression step whenever a compressed block is encountered. Aside from this, intersection proceeds as in BitFunnel.
In order to reduce false positives, we ensure the matrices are sparse. In a sparse matrix, dense blocks occur less frequently, so we

expect a non-uniform distribution of k-bit blocks, and we expect this imbalance assists the proposed compression method.
2.1 Selective compression
After compressing all the matrices, we find that the false positive rate can exceed 60% while the storage cost is only halved (when k = 32 and b = 16). Therefore, we study selectively compressing rows of the mapping matrices, which is further motivated by Figure 3. To generate Figure 3, we only use rank-0 matrices (Pri in Section 4); we inspect shard 3, which has 4,477 rows and 7,373,033 documents; we experiment with the MillionSet query set (described in Section 4), which has 60,000 queries; and we reorder the rows in descending order of access frequency. We give two plots in Figure 3:
(1) The blue plot is the row access frequency f (n), i.e., how frequently the n-th row is accessed with MillionSet.
(2) For n  {0, 200, . . . , 4200}, we compute the intersection results for the MillionSet query set when the 200 rows {n, n + 1, . . . , n + 199} are compressed. We define the false positive rate as the proportion of incorrectly included documents in the intersection results, which we plot as the red line in Figure 3.
A false 1 may result from either BitFunnel's Bloom filter method or from compression. The original BitFunnel mapping matrix has an overall false positive rate of 8.21% on shard 3, which is a lower bound on the false positive rate for the proposed compression method.

row access frequency f (n) false positive rate

104 103 102
0

16%

row access frequency f (n) false positive rate; rows n, . . . , n + 199 are compressed

14%

12%

10%

8%

6%

4%

2%

400 800 1,200 1,600 2,000 2,400 2,800 3,200 3,600 4,000 0% n-th row

Figure 3: Access frequency (left axis; logarithmic) and the proposed method's false positive error rate (right axis; linear) for each row in the rank-0 mapping matrix for shard 3. The horizontal dashed line marks the lower bound: 8.21% false positive rate.

The main observation we make from Figure 3 is that a small number of rows of the mapping matrix are both accessed frequently and result in a greater proportion of errors--a double impact on the final false positive rate. Thus, we are motivated to store these rows uncompressed, in order to reduce the final false positive rate.
For a parameter   [0, 1], we leave uncompressed the first q rows, where q is the minimum value for which the first q rows are accessed at least  N times in total, where N is the total number of row accesses. The decision process is described in Section 4.1.
To implement this method, we randomly split the query set into two equal parts, with the first part used for determining access

922

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

frequencies (and the other part as a sample of test queries). Thus, the choice of query set affects which rows are compressed, and the overall compression ratio.
3 DOCUMENT REORDERING
We also explore improving the compression through reordering the documents, i.e., permuting the document identifiers. Each shard is reordered independently, and reordering the documents in one shard will affect all the mapping matrices in that shard. So we only use this reordering method for Pri, aimed at decreasing unrepresented blocks in the rank-0 mapping matrix.
We illustrate the process in a toy example in Figure 4. When we compare Figure 2 (before reordering) to Figure 4 (after reordering) we see that the number of unrepresented blocks in the mapping matrix drops from 7 to 1.

documents O B N K J I H L ME C P G A D F 1 1111110010101100 2 1000110011000001 3 1100101000011010 4 1100000111001100
(a) The original mapping matrix.

pattern ID
00 01 10 11

definition
1111 1100 1010 0001

OK J LMPG F
1 00011001 2 01010111 3 01101110 4 01110101

(b) The dictionary (left) and the compressed mapping matrix (right).

Figure 4: By reordering the documents in the example in Figure 2, we reduce the number of unrepresented blocks.

Given an d-column mapping matrix (where k divides d), we initially reorder the d documents in decreasing order of density, which, for the example in Figure 4, is given by

O, J , M, G, B, I , E, A, N , H , C, D, K, L, P, F ,

which we color d/k documents at a time to illustrate the next step. We then interleave them into d/k groups of k documents: the mostdense d/k columns are moved to the 1st position in each group, the next-most-dense d/k columns are moved to the 2nd position in
each group, and so on. In the example in Figure 4, this gives

1st group

(d/k )th group

O, B, N , K, J , I , H , L, M, E, C, P, G, A, D, F .

Motivated by heuristic ideas, we reorder the documents in order to increase block repetition and thereby decrease the number of unrepresented k-bit blocks for reducing false positives. We also try some other reordering strategies, for example, all documents in one mapping matrix or k documents in each block are ordered by density in descending order. However their performances are not as good as the reordering method proposed in this section.

4 EXPERIMENTAL RESULTS
We perform all experiments on the GOV2 collection, consisting of 25,205,183 documents. The content text for each document, including the body and title section in HTML are extracted.
All experiments are carried out on a PC server with two Intel Xeon E5-2650 v4 CPUs and 512GB of memory. The number of physical cores on each CPU is 12 (with 24 threads), and clocked at 2.20GHz. The L1 instruction cache and data cache is 32KB, L2 cache is 256KB, and L3 cache is 30,720KB. The operating system is Linux CentOS 6.5, with kernel version 2.6.32. All programs are implemented in C++11 and are compiled with g++ version 5.4.0, with optimization flag -04.
We use two query sets: (a) MillionSet, consisting of queries from the 2007, 2008, and 2009 TREC Million Query Track, containing 60 thousand queries in total; and (b) TerabyteSet, consisting of 100 thousand queries from the 2006 TREC Terabyte Track.
Using BitFunnel's original code, we generate mapping matrices using 9 shards and density approximately 0.1. BitFunnel provides the mapping matrices with extra information, but we recover the original mapping matrices through the mapping information (which terms map to which rows). We consider two versions of BitFunnel: (a) Pri, where there is only one rank, namely rank 0, and (b) Opt, where there are 7 ranks.
Among the various k and b values we test, the best compression rate, false positive rate, and intersection time is observed when k = 32 and b = 16, so we consistently use these values in the experiments.
4.1 Threshold selection
Different shards have various initial false-positive rates (high  suits those with high initial false-positive rates) and document content, and thus there are different distributions of 1's in the mapping matrices, so we determine  (which determines how many rows are compressed) by some simple experiments. By varying the threshold  we choose how to trade off compression for false positive errors. Figure 5 plots the false positive rate vs. the compression rate as  varies, for shards 0, 3, and 7 (representing the short, middlesized and long documents). The false positive rate is defined as the proportion of false positive documents in the intersection results caused by compression (using TerabyteSet). The compression rate is defined as sizecomp/sizeorig. For short documents (shard 0), we observe the lowest false positive rate and highest compression ratio.
As a result of this experiment, throughout the paper, we use the  values in Table 1 to ensure the false positive rate caused by compression is around 10%.
Table 1: The threshold  we choose for the experiments. We also list the number of documents (×106).
shard 012345678
Pri 0 0.5 0.6 0.7 0.8 0.8 0.8 0.8 0.8 Opt 0 0 0.6 0.8 0.9 0.8 0.8 0.8 0.8
no. docs 2.65 2.61 5.52 7.37 4.50 1.87 0.50 0.16 0.02

923

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

false positive rate

30% Pri shard 0

25%

Pri shard 3

Pri shard 7

Opt shard 0

20%

Opt shard 3

Opt shard 7 15%

10%

5%

0% 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 compression rate

Figure 5: For BitFunnel Pri (red) and Opt (blue), the observed false positive rate and compression rate using the proposed dictionary-based compression method. From left to right, shard 0 has   {0, 0.1, 0.2, 0.3, 0.4}, and shards 3 and 7 have   {0.5, 0.6, 0.7, 0.8, 0.9}. The x-axis starts at 0.5.

4.2 Compression
We first compare the false positive results before and after the reordering process; we use the selective compression method described in Section 2.1. For Pri, after reordering, the false positive rates of MillionSet and TerabyteSet are reduced up to 6.90 and 6.35 percentage points depending on the shard, respectively. Table 2 tabulates the total size of the mapping matrices we encounter. We observe around a 27% and 30% reduction in size for Pri and Opt, respectively.
Table 2: The total size of the mapping matrices, over all shards and ranks.

size (GB) bits per posting

Pri Pri (comp.), MillionSet Pri (comp.), TerabyteSet

14.22 10.45 10.20

19.64 14.44 14.09

Opt Opt (comp.), MillionSet Opt (comp.), TerabyteSet

15.22 10.77 10.72

21.01 14.87 14.80

We also try testing a traditional Bitmap compression method EWAH [4] (run-length), and the mathematical encoding methods Pfor [9] and Vbyte [8]. We see a decrease in size of 4.35% (EWAH), -1.0% (Pfor) and 11.3% (VByte), which is unsurprisingly poor. Due to the uneven distribution of 1's, the 32-bit subsequences in BitFunnel tend to be larger than differences in inverted indices, and all-0 and all-1 subsequences tend to be short, leading to poor compression with these methods.
4.3 Intersection
To give a meaningful comparison, we rewrite the query part of the BitFunnel code, keeping its algorithmic structure, while using the same optimization level and compile options for different methods.1.
1Source code available from https://github.com/BitFunnelComp/dicComp.

In Table 3, we compare the per-query intersection time with and without the proposed dictionary-based compression. We see that the intersection time increases by around 16% to 48% when using dictionary-based compression. While the time for intersection increases due to compression, a 5ms per-query intersection time accounts for a small proportion of the entire query time (including top-k ranking, snippet generation, etc.), which will not have an significant impact on the end user.
In Table 3, we also see that the false positive rate increases by around 7.6 to 10.7 percentage points. Random false positives that arise are likely poorly related to the input query, in which case they would be excluded during, say, the top-k ranking process. As such, we feel this increase in false positives is not as major of a consideration as intersection time.
Table 3: Intersection time per query and the false positive rate.

MillionSet

TerabyteSet

time (ms) false pos. rate time (ms) false pos. rate

Pri

5.34

11.33%

6.18

7.56%

Pri (comp.) 6.44

19.09%

7.15

15.13%

Opt

3.41

6.79%

3.98

4.41%

Opt (comp.) 5.04

17.53%

5.47

13.38%

5 CONCLUSION
In this paper, we propose a dictionary-based method to compress BitFunnel's underlying index structure.
An avenue for future work is to adapt the compression method for use with a GPU-based or multi-threaded intersection method. When limited to a small memory size (e.g., the GPU memory), the proposed compression method would have a more beneficial trade-off. The proposed method could also be adapted to bitmap data structures, where each term corresponds to a unique row. Using bitmaps results in faster intersection, avoids the problem with false positives as a result of the Bloom filter, and may admit better compression, but the initial bitmap is much larger.
REFERENCES
[1] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Comput. Netw. 30, 1-7 (1998), 107­117.
[2] Bob Goodwin, Michael Hopcroft, Dan Luu, et al. 2017. BitFunnel: Revisiting Signatures for Search. In Proc. SIGIR. 605­614.
[3] Andrew Kane and Frank Wm. Tompa. 2014. Skewed Partial Bitvectors for List Intersection. In Proc. SIGIR. 263­272.
[4] Daniel Lemire, Owen Kaser, and Kamel Aouiche. 2010. Sorting improves wordaligned bitmap indexes. Data Knowl. Eng. 69, 1 (2010), 3­28.
[5] Giuseppe Ottaviano and Rossano Venturini. 2014. Partitioned Elias-Fano indexes. In Proc. SIGIR. 273­282.
[6] Vivek Seshadri, Donghyuk Lee, Thomas Mullins, et al. 2017. Ambit: In-memory Accelerator for Bulk Bitwise Operations using Commodity DRAM Technology. In Proc. MICRO. 273­287.
[7] Xiujun Wang, Yusheng Ji, Zhe Dang, Xiao Zheng, and Baohua Zhao. 2015. Improved Weighted Bloom Filter and Space Lower Bound Analysis of Algorithms for Approximated Membership Querying. In Proc. DASFAA. 346­362.
[8] Hugh E. Williams and Justin Zobel. 1999. Compressing Integers for Fast File Access. Comput. J. 42, 3 (1999), 193­201.
[9] Marcin Zukowski, Sándor Héman, Niels Nes, and Peter A. Boncz. 2006. SuperScalar RAM-CPU Cache Compression. In Proc. ICDE. 59.

924

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

On the Volatility of Commercial Search Engines and its Impact on Information Retrieval Research

Jimmy
Queensland University of Technology Brisbane, Australia
University of Surabaya (UBAYA) Surabaya, Indonesia
jimmy@hdr.qut.edu.au

Guido Zuccon
Queensland University of Technology Brisbane, Australia g.zuccon@qut.edu.au

Gianluca Demartini
University of Queensland Brisbane, Australia
g.demartini@uq.edu.au

ABSTRACT
We studied the volatility of commercial search engines and reflected on its impact on research that uses them as basis of algorithmical techniques or for user studies. Search engine volatility refers to the fact that a query posed to a search engine at two different points in time returns different documents.
By comparing search results retrieved every 2 days over a period of 64 days, we found that the considered commercial search engine API consistently presented volatile search results: it both retrieved new documents, and it ranked documents previously retrieved at different ranks throughout time. Moreover, not only results are volatile: we also found that the effectiveness of the search engine in answering a query is volatile. Our findings reaffirmed that results from commercial search engines are volatile and that care should be taken when using these as basis for researching new information retrieval techniques or performing user studies.
ACM Reference Format: Jimmy, Guido Zuccon, and Gianluca Demartini. 2018. On the Volatility of Commercial Search Engines and its Impact on Information Retrieval Research. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978. 3210088
1 INTRODUCTION
On a number of occasions, information retrieval researchers have used commercial search engines and associated APIs to assist with the research of new algorithms and techniques (type A: algorithmical use), or to investigate user search behaviour (type U: user study use). Examples of this practice include, among others: Cilibrasi and Vitanyi [5] defined a word similarity function based on the number of search results retrieved by Google (A); Symonds et al. [10] used Google to perform a first round of retrieval to inform query expansion (A); Maxwell et al. [8] used Bing to retrieve documents and snippets within a user study that explored user behaviour with respect to snippet length and informativeness trade-off (U).
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210088

To help understand the extent of this practice, we systematically surveyed the literature published in the ACM SIGIR conference between 2006 and 2016 (a total of 2,138 full and short papers)1. We found that 158 contributions (7.4%) used commercial search engines in their experiments2.
Commercial search engines are however often volatile: both the results retrieved and their rankings often differ given two points in time. There are multiple reasons for this volatility. On the one hand, volatility may be due to index updates and fresher information being indexed by the search engines [3, 4]. On the other hand, volatility may be due to operational reasons such as index replication, sharding and routing. Differences may also be due to updates to the ranking function used by the search engines. McCown and Nelson [9] also found that commercial search engines' API and their web interfaces often access different indexes: they argued that the API may access a smaller index than the web interface. Our study focuses on measuring the volatility of search results returned by a search engine's API.
Commercial search engine volatility has been investigated by Altingovde et al. [2] who had experimented using a set of 630,000 queries and found that only 10.7% of top 10 results found in 2007 remained as top 10 results in 2010. Bai and Junqueira studied volatility in Yahoo! and reported that of 1.4M search results analysed over 3 weeks,  35% new URLs were added,  1% had modified content, and  0.06% were deleted [3]. Specific to the rate of URLs with modified content, Adar et al. [1] found that 34% of URLs in their 5 weeks study had no change, while the remaining changed on average every 123 hours (average Dice coefficient: 0.794). These changes may had impacted the search quality over time: however that study did not explicitly measure the relevancy of the search results and whether volatility impacted on search engine effectiveness. On the contrary, we also measure changes in results relevance and search engine effectiveness over time.
Search engine volatility may be a problem when commercial search engines are used by researchers as part of their methods or user studies. In other words: if an algorithm or technique is based on the use of a volatile search service, differences in search results and rankings may vary the effectiveness of the method, or render the replication of the experiments impossible. Similarly, if a user study relies on a commercial search engine to investigate user behaviour, volatility may be a confounding factor affecting effectiveness, especially if the user studies are carried out over a
1Note that this practice is well utilised also outside of the SIGIR literature, as demonstrated by the examples cited above that were not published in SIGIR. 2Data available at http://ielab.io/se-volatility

1105

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 1: Example of search result volatility and its impact on an hypothetical query expansion techniques that exploits the retrieved results.
period of time, rather than all being run concurrently. Yet, this aspect is often ignored when analysing the results, e.g., volatility is not considered as factor within an ANOVA analysis of results.
To further exemplify how search engine volatility may affect information retrieval research that relies on such commercial services, consider the case of a user study relying on the Bing search APIs and investigating the capabilities of users in selecting query suggestions automatically generated by techniques that exploit the search results obtained from the initial user's query. In such case, two components of the experimental methodology rely on results from the commercial APIs: (i) the query suggestion mechanism, and (ii) the results that are retrieved (and evaluated for effectiveness) in response to the user's query and the selected query suggestion. Figure 1 shows an example of a query submitted at two different times. At each time, the query retrieved two (sensibly) different sets of results and thus returned different query suggestions.
In this study, we seek to answer the following research questions:
· RQ1: What amount of volatility do commercial search engine APIs present?
· RQ2: How does the volatility of commercial search engines affect information retrieval research?
To answer RQ1, we periodically used a search engine API to retrieve results for a large set of queries which had no specific temporal intent or seasonality effect. We then studied how results changed over time. To answer RQ2 we assessed the relevance of the top search results we collected over time and we analysed the change in search engine effectiveness over time. Details of the methods used in this study are described next.
2 METHODS
To answer RQ1, we acquired the queries used in the TREC 2013 and 2014 Web Track (100 queries in total). While these queries had no explicit temporal nor seasonal intent, a small number may have been influenced by temporal issues. For example, in our experiments, query 202: "uss carl vinson" was affected by the US decision of deploying the aircraft carrier within strike range of North Korea in early January 2018. We further acquired a set of 300 queries from the CLEF 2016 eHealth IR collection. Of the 300 queries, we removed

query 129005 due to a problem with quotation mark characters in the query. These queries related to consumer health search intents, and were unlikely to be affected by temporal or seasonal intents.
We used the Bing Search API 3 to retrieve a maximum of 50 web results in answer to the query sets, setting English US as the market and with safe search turned off. We performed retrieval every two days from 29/11/2017 to 31/01/2018 with exceptions of 13, 27, and 29 December 2017 where the retrieval process was not triggered due to technical problems. Hence, in total we collected 30 data samples for each query set. We used the data samples to investigate the volatility of search API by counting the number of new URLs between search results from different retrieval dates pairs. We then further investigated whether differences in ranking for an URL were also found over time. To determine whether IP address or browser cookies may have affected the results, we repeated some of the crawls using an additional server located in another Australian state; crawls were started at the same time. A 99.98% match was found between the results obtained by the two servers. Thus, we did not systematically acquire results from different locations and servers, as this appeared to have no effect.
To answer RQ2, we pooled the top ten URLs from every sampled date for the WEB2013-2014 query set, and we assessed their relevance4. Relevance assessments were collected using crowdsourcing. We setup tasks on Amazon Mechanical Turk, assigning each query-website pair to 5 workers. Workers, selected among those with a 90% acceptance rate and at least 1000 tasks completed, were presented with the TREC topic title and description fields and a link to the webpage to be assessed. We simplified the TREC 2013 six-point judgment scale [6] into the following four-point scale: Highly relevant (this point included Nav, Key, and HRel as defined in TREC 2013), Relevant, Not Relevant, and Junk. As suggested in [7], assessments that took less than 4 seconds were discarded. Collected assessments were aggregated as the median of the collected labels for each query-document pair. We analysed the results using ERR@10, nDCG@10, and P@10, as used in the TREC 2013.
3 EMPIRICAL EVALUATION
3.1 Volatility of Search Results
Figure 2 shows the percentage of new URLS introduced on average in the top 10 results. Each square in the heatmap corresponds to a pair of dates, and thus the percentage difference between the results obtained in the two dates. The darker the red tone, the higher the percentage of new URLs being returned on the later date: the diagonal is yellow, indicating no difference (as expected, as we are comparing a date with itself), and we removed the lower part of the heatmap for clarity (the heatmap is symmetric).
Results highlighted in blue refer to the percentage of new URLs retrieved compared to the initial date (the start of our data sampling): in the figure we further visualised this trend as a line plot to give the reader a different representation of the trend and aid interpretation. On average, we found that each day had 24.43% new URLs, compared to the initial sampling date.
3 azure.microsoft.com/en-au/services/cognitive-services/bing-web-search-api/ 4We assumed that the content of the linked webpages remained the same throughout our experiments. While this may have not been the case for all results, as reported in [3], only  1% of urls found in the first two weeks had modified content in the third week of their study.

1106

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Day 1 3 5 7 9 11 13 17 19 21 23 25 27 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64

Day

1

3

5

7

9

11

13

17

19

21

23

25

27

32

34

Average % New Documents from Start Date

36 28 38 24 40 20 42 16 44 12























 



46 8

48 4

50 0 

52

1 7 13 21

54 %New

56 0

32 40 48 56 64

Day

Average % New Documents from Previous Date


24

58 10

60

20

62

30

64

20 16 12
8 4

 






 
 

 
 








 











 

0
1 7 13 21 32 40 48 56 64
Figure 2: Percentage of average (Doayver the query set) new

URLs retrieved in the top 10 results for each pair of sam-

pling dates for the WEB2013-2014 query set.

Another important observation from Figure 2 is highlighted in green. This shows the percentage of new urls found on a sampling date compared to the previous sampling date. We further visualised this trend in the corresponding line plot: new urls were found at a comparable percentage overtime. On average, every two days, 10.72% of the URLs retrieved differed from those retrieved on the previous sampling date.5.
We also investigated the volatility of the top 1 to 9 documents and the top 50 documents, for completeness. We found that percentage volatility trends were similar across rank thresholds (also for CLEF 2016 ­ results available at http://ielab.io/se-volatility).
We then further investigated the ranking distance between occurrences of the same URL across different dates. Results were again represented as a heatmap, which is reported in Figure 3. The heatmap shows the percentage of rank distances between the top 10 URLs for each pair of sampling dates using the WEB2013-2014 query set. Blue and green colours were used to represent similar circumstances as for the previous heatmap. When considering rank movements over time with respect to the first sampling day (blue highlighting), we found that on average URLs moved 11.36% up or down the ranking, with lesser movement found in the first few days of the experiment. When considering rank movements over time with respect to the previous sampling day (green highlighting), we found that on average URLs moved by 6.29% up or down the ranking compared to the previous date, though peaks with larger rank movements did occur. Interestingly, the line plots in Figures 2 and 3 suggest that the trends observed for new URLs were similar to those for rank distance over time.
Given these results, we answer RQ1 by reporting that, on average, between two consecutive days, search engine results change by

5Note we missed sampling on the 13, 27, and 29 December 2017: for the sampling date after the ones we missed, rankings were compared to those in the previous available date. In the line plot, we distinguished the data for these dates using red dots.

Day 1 3 5 7 9 11 13 17 19 21 23 25 27 32 34 36 38 40 42 44 46 48 50 52 54 56 58 60 62 64

1

3

5

7

9

11

13

17

19

21

23

25

27

32

Day

34
36 38 12 40 10 42 8

Average % Rank Distance from Start Date











































 

 

44 6

46 4

48 2

50 0 

52

1 7 13 21

54

56

%Distance 0

58 5

60 10

62

64

32 Day

40 48 56 64

Average % Rank Distance from Previous Date

11 10
9 8 7 6 5 4 3 2 1 0











 















 





 




 

 


 

1 7 13 21 32 40 48 56 64
Figure 3: Percentage of average (Doayver the query set) rank

movement in the top 10 results for each pair of sampling

dates for the WEB2013-2014 query set.

Average Performance

0.9

P@10

0.8 nDCG@10
0.7

0.6

0.5

0.4

0.3 ERR@10
0.2
1 5 9 13 19 25 32 38 44 50 56 62 Day

Figure 4: Average effectiveness of the top 10 URLs retrieved for the WEB2013-2014 query set.

10.72% in terms of new URLs retrieved (1.07 new URLs every 10). Furthermore, we found that the difference is even larger if a wider timespan is considered. In addition, we also report that URLs that occur in the results between two dates are likely to exhibit a rank movement of on average by 6.29%.
3.2 Impact of Result Volatility on Search Effectiveness
Figure 4 shows the search effectiveness over time for the WEB20132014 query set, averaged over all queries for each sampling date. The average trends show that search engine volatility had little impact on the average search effectiveness: despite new URLs were retrieved over time, and existing URLs changed rank, effectiveness on average did not vary significantly. Statistical significant differences (t-test p < 0.05) were found only between  6.7% of the results for each pair of days (only unique pairs were considered).

1107

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 5: nDCG@10 for WEB2013-2014 queries over time. Each box plot refers to a query; queries are ordered in decreasing effectiveness as returned on day 1. The red shaded area indicates the effectiveness gap between results for day 1 and those for the day with the biggest average effectiveness gap over the query set.

The previous results analysed the impact of search engine volatility by averaging effectiveness over the query set. We next analyse the impact volatility had on a query-by-query basis; we did this for graded relevance (nDCG@10) ­ similar findings were observed for other settings. Results are reported in Figure 5. Box plots were organised such that queries were ordered in decreasing effectiveness of the results obtained on the initial date of sampling (day 1): each box summarises the effectiveness of a query over time. The box plots show effectiveness did vary over time for each query, with some queries achieving substantially different effectiveness depending on the date. Specifically, we found that 67 out of 100 queries had a change in nDCG@10 that was higher or equal to 0.1 and, on average, individual query effectiveness varied by 0.1431 over the sampling period, with the largest variation recorded being 0.4700. To further provide an intuition of the gap in effectiveness that search result volatility generated, we highlighted the gap between the effectiveness of each query recorded on the first day of our experiment, and the day with the biggest average effectiveness gap over in the query set (day 64). This gap is represented by the red shaded area in Figure 5.
4 DISCUSSION
The findings from our experiments quantified the amount of volatility measured in 2-day time intervals and over longer periods. In addition, they also highlighted that not only are the search engines volatile, but their effectiveness is also volatile, given a query; although we found that for the query set used, average effectiveness remained mostly unchanged.
These findings suggest that search engine result volatility is likely to largely impact the replicability of results obtained by exploiting commercial search engine APIs either for algorithmical advances or within user studies. We also argue that volatility also impacts reproducibility, as the deterioration of results over time for some queries is large and, if results are used algorithmically, is likely to produce different outcomes and thus affecting the actual quality of techniques like query expansion based on the initial search engine results. While not done here, we aim to empirically investigate this in future work.
While our findings suggest that search engine result volatility may affect results of information retrieval studies, it is unclear how researchers could mitigate these issues and yet use commercial search engines and associated APIs within their research. A possible

avenue may be repeating the studies over a sufficiently long period of time, so as to account for search engine volatility as one of the factors affecting results and study this with respect to their results.
5 CONCLUSION
In this paper, we investigated the volatility of commercial search engines and its impact on information retrieval research. By sampling the results returned by the Bing Web Search API every two days for a period of 64 days, we found that, on average, the search engine retrieved 10.72% new URLs in the top 10 ranks. Additionally, we also found that a URL that was retrieved on a previous date was subject to an average rank movement of 6.29%. When examining the possible impact such a volatility may have on information retrieval research that makes use of such search services, we found that on average, nDCG@10 varied by 0.1431 (19.88%) for each query and the biggest nDCG@10 variation for a query was 0.4700 (143.51%). These results suggest that research that uses commercial search engines as part of an algorithmical pipeline or user study should be aware of search engine volatility and its implications.
Acknowledgements. Jimmy is sponsored by the Indonesia Endowment
Fund for Education (Lembaga Pengelola Dana Pendidikan / LPDP). Guido
Zuccon is the recipient of an Australian Research Council DECRA Research
Fellowship (DE180101579). This project has received funding from the Eu-
ropean Union`s Horizon 2020 research and innovation programme under
grant agreement No 732328.
REFERENCES
[1] Eytan Adar, Jaime Teevan, Susan T Dumais, and Jonathan L Elsas. 2009. The Web Changes Everything: Understanding the dynamics of web content. In WSDM'09.
[2] I Altingovde, R Ozcan, and O Ulusoy. 2011. Evolution of Web Search Results within Years. In SIGIR'11.
[3] X Bai and F Junqueira. 2012. Online Result Cache Invalidation for Real-Time Web Search. In SIGIR'12.
[4] R Blanco, E Bortnikov, F Junqueira, R Lempel, L Telloli, and H Zaragoza. 2010. Caching Search Engine Results over Incremental Indices. In SIGIR'10.
[5] R Cilibrasi and P Vitanyi. 2007. The Google Similarity Distance. TKDE 19, 3 (2007).
[6] K Collins-Thompson, P Bennett, F Diaz, C Clarke, and E Voorhees. 2014. TREC 2013 Web Track Overview. In TREC.
[7] E. Maddalena, M. Basaldella, D. De Nart, D. Degl'Innocenti, S. Mizzaro, and G. Demartini. 2016. Crowdsourcing Relevance Assessments: The unexpected benefits of limiting the time to judge. In HCOMP'16.
[8] D Maxwell, L Azzopardi, and Y Moshfeghi. 2017. A Study of Snippet Length and Informativeness: Behaviour, Performance and User Experience. In SIGIR'17.
[9] Frank McCown and Michael L Nelson. 2007. Search Engines and their Public Interfaces: Which APIs are the most synchronized?. In WWW'07.
[10] M Symonds, P Bruza, G Zuccon, B Koopman, L Sitbon, and I Turner. 2014. Automatic Query Expansion: A structural linguistic perspective. JAIST 65, 8 (2014).

1108

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings

Dominic Seyler
IBM Research dseyler2@illinois.edu

Praveen Chandar
IBM Research praveenr@spotify.com

Matthew Davis
IBM Research matthew.davis@invitae.com

ABSTRACT
We present an Information Retrieval framework that leverages Heterogeneous Information Network (HIN) embeddings for contextual suggestion. Our method represents users, documents and other context-related documents as heterogeneous objects in a HIN. Using meta-paths, selected based on domain knowledge, we create graph embeddings from this network, thereby learning a representation of users and objects in the same semantic vector space. This allows inferences of user interest on unseen objects based on distance in the embedding space. These object distances are then incorporated as features in a well-established learning to rank (LTR) framework. We make use of the 2016 TREC Contextual Suggestion (TRECCS) dataset, which contains user profiles in the form of relevance-rated documents, and demonstrate the competitiveness of our approach by comparing our system to the best performing systems of the TRECCS task.
ACM Reference format: Dominic Seyler, Praveen Chandar, and Matthew Davis. 2018. An Information Retrieval Framework for Contextual Suggestion Based on Heterogeneous Information Network Embeddings. In Proceedings of The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, Ann Arbor, MI, USA, July 8­12, 2018 (SIGIR '18), 4 pages. https://doi.org/10.1145/3209978.3210103
1 INTRODUCTION
Recent advances in HIN research [7] allow us to embed a HIN into a multi-dimensional vector space [6]. We present a method that facilitates these embeddings to be used as features in an LTR framework for the problem of contextual suggestion. In this problem domain, we have contextual queries and user profiles that contain preference rankings of objects of interest. In our approach, we model users and objects in the same HIN and define meta-paths specific to the problem domain, then generate graph embeddings by randomly sampling the HIN conditioned on the meta-paths. Once users and their interests are projected into the resulting embedding space,
Current affiliation: University of Illinois at Urbana-Champaign. Current affiliation: Spotify Research. Current affiliation: Invitae.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210103

we derive features that rank the objects of interest according to a contextual query.
Meta-paths enable us to capture human intuition by introducing a set of semantic constrains on the HIN. For example, if user U1 tags a relevant document D1 using word w, then another user U2 that has used w to tag a different document D1 might also find D1 relevant. This information is inherently contained in the graph but hidden. A domain expert can utilize meta-paths to express her knowledge about this latent structure. The generation of the statistical representation of the graph is then guided by conditioning the sampling of the graph along nodes that follow the meta-path.
In this work we show how meta-paths can be utilized in an Information Retrieval setting. By representing users and the objects of interest (i.e. documents) in the same vector space we can make use of the distances between objects as features in a ranking function. The ranking function is then learned from a small sample of relevance judged documents (in our case the relevance judgments from previous years of the TRECCS task). Once trained, the ranking function can be utilized to re-rank a set of retrieved documents, thereby incorporating the latent information of the HIN.
To demonstrate a concrete application of the retrieval framework we have selected the trip recommendation problem of the TRECCS task [3]. The TRECCS dataset provides user profiles composed of a set of objects ranked by relevancy to the user along with the search intent (e.g. planning a trip). The goal is to return a ranked list of attractions that might be interesting to the user. As the objects of interest are represented by text documents, we investigate how document-based nodes can be broken up into fine-grained node types to improve the retrieval performance significantly. Further, we experimentally show that we can reduce sparsity in the graph by limiting the number of nodes prior to training, which results in significant performance improvements.
To summarize, the contributions of this paper are to: (1) Define a general IR framework for the ranking of heterogeneous objects based on HIN embeddings. (2) Identify node types and graph topology specific to the TRECCS task. (3) Specify meta-paths to encode domain knowledge in the embedding space. (4) Show how finegrained modeling of document-based nodes can improve performance. (5) Compare how different feature selection methods can reduce graph size and sparsity, while improving performance.
2 PROBLEM FORMULATION
The task we are addressing is the problem of contextual suggestion. There, a user has previously rated a list of documents, which make up the user profile, and the goal is to recommend a new set of documents to the user for a contextual query. We focus on the TRECCS task [3] where the IR system is assisting a user in planning

953

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

a trip to a target city. The input to the system is a list of requests (R) and user profiles (U ), where user profiles are a list of rated attractions (preferences), gender and age.
input = {R, U = {info, pref }}
R = {roup, season, trip_type, duration, location} in f o = {ender , ae}
pre f = (attraction, ratin, tas)1, ...,k
The output is a ranked list of attractions not in the preference list, ordered by their posterior probability conditioned on the user profile and request:
output = {P(attraction|R, U )|attraction pre f }
Our approach addresses TRECCS's second phase, which is the main phase of the track. There, the system is given a list of attractions that were retrieved in the first phase of the track. Each participating system is then required to re-rank this list and output it to the user. A list of ratings is given by the task providers to determine the optimal ranking. To evaluate performance of the system we follow the TRECCS task's specifications by using normalized discounted cumulative gain (NDCG@5), precision (P@5), and mean reciprocal rank (MRR).
Along with the request and user profile information the TRECCS task provides a collection of all attractions in the task (attractions that are part of the user profile and that are part of the retrieval corpus). The collection is a list of 1,235,844 attractions with metainformation (i.e. attraction id, city, URL and title). In addition, the task provides a web crawl of all attraction's web pages that are part of the TRECCS collection. The web crawls covers 77.93% percent of attractions or 956,437 web pages. These web pages are a mix of travel recommendation portals (20% foursquare1, 16% yelp2, 5% tripadvisor3) and attraction's home pages (59%).
3 APPROACH
In this section, we present our HIN-embedding-based framework for contextual suggestion. Networks which consider type information have been shown to outperform homogeneous network-based approaches when measuring similarity of objects [9]. Choosing meta-paths over these networks facilitates the information propagation between subgraphs composed of heterogeneous node types, which contain inherent contextual information. We propose to model users and their preference context as heterogeneous nodes in the graph. User defined meta-paths guide the creation of an embedding space, where distances between objects can be interpreted as similarity features that inform a ranking function.
3.1 Graph Modeling
When applying the HIN framework to the domain of contextual suggestion we are faced with a number of choices: i) What are the node types in the graph? ii) Which topology will the node types adhere to? iii) Which meta-paths are most effective? We now elaborate on the modeling of the HIN that is used to incorporate the specific user profile information.
1 http://foursquare.com 2 https://www.yelp.com/ 3 https://www.tripadvisor.com

Table 1: Node Types.

Node Type U L A T B W C E

Description User Location Attraction User tags/endorsements Token in attraction's business name Token on attraction's homepage Category tags from attraction's profile page Named entities in attraction's profile page

Figure 1: Network Topology.

L E
U

B

A

W

T

C

Node Types. We break down the contextual suggestion problem into semantic concepts, from which we derive the node types in the HIN. As part of user contexts, the TRECCS task provides information about users, locations, attractions, and user endorsements. We model each of these as different node types. In addition, TRECCS provides a collection of attractions, from which we utilize the business name as another node type. Along with the collection of attractions, TRECCS provides a web crawl of all attraction home pages, from which we derive words and named entities. Categories are extracted from the travel recommendation portals provided in the web crawl. Following Table 1, we eventually identify eight different node types that satisfactorily describe our problem setting.
Network Topology. Only a subset of node interactions are likely to be relevant to the problem, thus criteria for a modeling an interaction between nodes must be carefully selected. For the TRECCS task, we decided that attractions are the "hub" through which all node types connect to each other. Attractions link to users (U) who rated them, to the location (L) they are at, to the endorsements (T) that were given by a user, to their business name (B), to the words (W) and named entities (E) appearing on their web pages, and to the categories (C) they belong to. Furthermore, users link to the location that they plan a trip to and the endorsements they made. Figure 1 depicts the network topology described.
Meta-Paths. To facilitate information propagation and guide the creation of the network embedding, our method requires the specification of meta-paths. Objects that have many path instances following a given meta-path will be closer in the embedding space than objects that have no such connection [6]. Furthermore, since nodes have types, it is possible to embed the similarity of objects of different types into the resulting vector space. Since we are interested in similarities of users and attractions, we decide to use them as start and end node types of all meta-paths. Table 2 lists all identified meta-paths and their semantic meaning. To create the shared embedding space, we leverage [6] as the graph embedding method of choice.

3.2 Learning to Rank Framework
We derive features from the vector representations of graph objects and the LTR framework we employed, to learn a document ranking

954

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Meta-Paths and Their Semantics.

Meta-Path A-U
A-T-U A-T-A-U A-B-A-U A-W-A-U A-C-A-U A-E-A-U

Semantics Attractions were rated by a user. Attractions were tagged/endorsed by a user. Attractions share tags/endorsements with other attractions that were rated by a user. Attractions share business tokens with other attractions that were rated by a user. Attractions share words on web page with other attractions that were rated by a user. Attractions belong to the same category as other attractions that were rated by a user Attractions mentioning the same entities as other attractions that were rated by a user

function. The vector space incorporates latent information about similarities of objects following different meta-paths. We leverage cosine distance as the similarity measure between the objects. We acknowledge that it would be worth investigating other similarity functions as well. However, the focus of this work is centered around investigating the usefulness of HINs for this problem. We therefore argue that using a well established similarity measure, such as cosine, is sufficient at this point. Equation 1 defines the similarity for two nodes n1, n2 with vector representations vnM1 , vnM2 for meta-path M.

similarity(n1, n2 |M) = cos(vnM1 , vnM2 ) =

vnM1  vnM2 ||vnM1 ||2 ||vnM2 ||2

(1)

Since each of the meta-paths capture different semantics (see

Table 2) we decided to learn a parameter for each meta-path separately. Thus, the feature function f is defined over all meta-paths Mi , i  {1...N } for nodes n1, n2 as shown in Equation 2.

f (n1, n2) = {similarity(n1, n2|Mi )}, i  {1...N }

(2)

For the contextual suggestion task, we measure similarity be-
tween users U and attractions A. In particular, the tasks requires ranking a set of candidate attractions ai  Acandidate for a pair of request ri  R and user profile ui  U . We then define our feature vector F for attraction ai as in Equation 3.

F (ai |ri , ui ) = f (ai , ui ), ai  Acandidates

(3)

For training and testing, we calculate all feature vectors for all tuples of candidate, request, and user, and pair them with the relevancy information. We obtain the relevance judgments for the training phase from the 2015 TRECCS task. Then, we employ LambdaMART [10] to learn the ranking function and optimize for NDCG@5. Once the ranking function is learned, we score all candidate attractions of the 2016 TRECCS requests.

4 EVALUATION
4.1 Experimental Setup
As mentioned previously, we employ learning to rank and choose LambdaMART as our ranker. We use the RankLib 2.8 4 implementation of LambdaMART and optimize for NDCG@5. We select 10% of the training data for validation and use the default settings for all other parameters.
4 https://sourceforge.net/p/lemur/wiki/RankLib/

Table 3: Fine-grained Representations of Attractions.

Node Types
{U, L, A} {U, L, A, T} {U, L, A, T, B} {U, L, A, T, B, W} {U, L, A, T, B, W, C} {U, L, A, T, B, W, C, E}

NDCG@5
.2400(±.0005) .2565(±.0010) .2932(±.0006) .2986(±.0003) .3081(±.0004) .3206(±.0003)

Due to the stochastic nature of the algorithm we expect some variance in the results. In order to provide fair and comparable results we report the average performance over ten identical runs. In addition, we report the variance over these runs in parentheses.
4.2 Fine-grained Representation of Attractions
We now experimentally investigate whether a more fine-grained representation of attractions leads to better recommendations of attractions that are not previously rated by a user. This ties directly to how information propagates between different node types through the choice of different meta-paths. In general, one would expect that additional information in the network (i.e. node types) facilitates information propagation and thereby improving the overall retrieval performance. We show this in our experiments by incrementally adding new node types to the HIN. We then train and evaluate the HIN embeddings based on the meta-paths for each node type following Table 2. Table 3 presents the results.
The TRECCS dataset inherently is made up of three node types (users, locations, attractions). Using these node types as the most basic information network, we find that our approach performs with an NDCG@5 of 0.24. Adding user tags to the graphs increases the performance slightly to 0.2565. We can now observe a trend that with each additional node type more information is introduced in the HIN that was not present before, thereby increasing the overall performance. The best performance of 0.3206 is reached when all node types are present. This result might not surprise, but it shows that domain knowledge can guide the integration of additional information sources into the HIN, which greatly enhances the retrieval performance.
4.3 Reduction of Graph Sparsity
Since the number of nodes for certain types is quite large (the total W subgraph has over 1.5 million tokens), we run into various

955

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 4: Comparison of Different Feature Selection Methods and Cut-Off Points for Sparsity Reduction in the HIN.

Top-k
10 50 100 500 1000

TFIDF
.3309(±.0004) .3157(±.0004) .3246(±.0003) .3294(±.0001) .3163(±.0006)

X2
.2900(±.0003) .3183(±.0004) .3105(±.0005) .2937(±.0005) .3135(±.0006)

MI
.3176(±.0005) .2982(±.0003) .3159(±.0005) .2996(±.0007) .3079(±.0002)

sparsity-related issues. For one, a high number of sparsely connected nodes prevents us from achieving a sufficient amount of sampling depth, since our algorithm is limited by an non-infinite number of samples per iteration. Secondly, dealing with web data introduces a certain amount of noise, which misguides the sampling of the space and negatively influences the resulting embeddings.
To counteract sparsity we compare multiple feature selection strategies, which allows us to reduce the graph to the most "expressive" nodes before training the embedding. We specifically investigate the case where nodes are derived from a natural language text, as was done for our W node types. We choose three common features selection strategies, namely (1) average TFIDF[8] weight, (2) chi-squared (X2) statistics between each non-negative feature and class and (3) mutual information (MI).
Results for this experiment are presented in Table 4 for different Top-k cut-off values. From the table we can see that a small Top-k (i.e., 10) is able to reduce the size of the graph significantly and improve performance simultaneously. We also find that unsupervised feature selection methods (i.e., TFIDF) seem to outperform the supervised features selection methods (i.e., X2 and MI) for almost all values of k. This surprising result might be connected to the locality of the negative examples that are used by both methods. Since TFIDF statistics are computed on the entire document collection it might be better at estimating the overall amount of information a token contributes. On the other hand, supervised methods only consider documents with relevance judgments, which make up only a fraction of the collection. We leave the question of how supervised methods can be extended to incorporate more (unlabeled) documents for future work.
4.4 Comparison to other Systems
To put our work into context, We follow the TRECCS evaluation methodology and compare our system to the track's contestants as baselines. [4] addresses the TRECCS task by performing document analysis using a weighted kNN classifier and a query that was expanded using Rated Rocchio. To improve results they crawl additional information from travel portals such as Foursquare and Yelp. [5] makes use of word embeddings for both user profiles and candidate places. Here objects are represented by the sum of their word vectors. Their work ignores the inherent heterogeneity of the network. This work also extracts additional information from travel portals. [1] trains a binary SVM classifier for predicting the appropriateness of a venue. The approach also relies on external data sources which were manually created using crowdsourcing. [11] uses user-provided ratings and collaborative filtering to infer

Table 5: Comparison of TRECCS 2016 Task Runs. The Best Performance for the Five Highest-Ranked Teams are Shown. Systems are Ordered by NDCG@5 (Highest to Lowest).

System DUTH_knn (debugged) [4] This work Laval_batch_3 [5] USI5 [1] bupt_pris_2016_cs.2_.4_max [11] UAmsterdamDL [2]

NDCG@5 .3388 .3309 .3281 .3265 .2936 .2824

P@5 .4690 .4476 .5069 .5069 .4483 .4448

MRR .6697 .6475 .6501 .6796 .6255 .5924

the missing rankings for attractions. This work also extracts additional information from travel portals. [2] uses word embeddings to build a neural document and a neural category preference model to re-rank attractions. This is the only top-performing system that does not utilize additional information sources other than those provided by TRECCS. As we can see our system is only outperformed by the debugged version of [4], which uses additional information from the web that was not provided by the TRECCS task.
5 CONCLUSION AND FUTURE WORK
We presented an IR framework for contextual suggestion using HIN embeddings. Using the TRECCS task, we instantiated this framework and showed how domain knowledge can be encoded using meta-path guided embeddings as semantic and efficient representations. Furthermore, we showed how feature selection reduces graph sparsity and improves embedding quality.
Our method performs among the top of the state-of-the-art methods for the TRECCS task. The framework also proved effective on a person name disambiguation dataset, whose results were not shown due to spacial constraints. For future work we are planning to further investigate the generalization of our method on other datasets (e.g. MovieLens) and how supervised features selection strategies can benefit from training on unlabeled documents.
REFERENCES
[1] Mohammad Aliannejadi et al. 2016. Venue Appropriateness Prediction for Contextual Suggestion. In TREC.
[2] Seyyed Hadi Hashemi et al. 2016. Neural Endorsement Based Contextual Suggestion. In TREC.
[3] Seyyed Hadi Hashemi et al. 2016. Overview of the TREC 2016 contextual suggestion track. In TREC.
[4] Georgios Kalamatianos and Avi Arampatzis. 2016. Recommending Points-ofInterest via Weighted kNN, Rated Rocchio, and Borda Count Fusion. In TREC.
[5] Jian Mo et al. 2016. Word embeddings and Global Preference for Contextual Suggestion. In TREC.
[6] Jingbo Shang et al. 2016. Meta-Path Guided Embedding for Similarity Search in Large-Scale Heterogeneous Information Networks. arXiv preprint arXiv:1610.09769 (2016).
[7] Chuan Shi et al. 2017. A survey of heterogeneous information network analysis. IEEE Trans. Know. and Data Eng. (2017).
[8] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of documentation (1972).
[9] Yizhou Sun et al. 2011. Pathsim: Meta path-based top-k similarity search in heterogeneous information networks. VLDB (2011).
[10] Qiang Wu et al. 2010. Adapting boosting for information retrieval measures. Information Retrieval (2010).
[11] Danping Yin et al. 2016. A rating model based on tags for contextual suggestion. In TREC.

956

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Effectiveness Evaluation with a Subset of Topics: A Practical Approach

Kevin Roitero
University of Udine Udine, Italy
roitero.kevin@spes.uniud.it

Michael Soprano
University of Udine Udine, Italy
soprano.michael@spes.uniud.it

Stefano Mizzaro
University of Udine Udine, Italy
mizzaro@uniud.it

ABSTRACT
Several researchers have proposed to reduce the number of topics used in TREC-like initiatives. One research direction that has been pursued is what is the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way. Such a research direction has been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. We propose such a practical criterion for topic selection: we rely on the methods for automatic system evaluation without relevance judgments, and by running some experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than random topics.

t1 · · · tn

MAP

s1 AP1,1 · · · AP1,n MAP1

...

...

...

...

...

sm APm,1 · · · APm,n MAPm

Figure 1: AP and MAP representation and correlation curves (both adapted from [6]).

CCS CONCEPTS
· Information systems  Test collections;
KEYWORDS
Few topics, test collections, TREC, topic selection
ACM Reference Format: Kevin Roitero, Michael Soprano, and Stefano Mizzaro. 2018. Effectiveness Evaluation with a Subset of Topics: A Practical Approach. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210108
1 INTRODUCTION
In Information retrieval (IR) test collection based evaluation, the number of topics used is a critical issue. Since it is one of the main parameters to determine the overall cost, it is not surprising that several researchers have studied how to reduce such a number in TREC-like initiatives. One research direction that has been pursued is to identify the optimal topic subset of a given cardinality that evaluates the systems/runs in the most accurate way [2, 6, 11]. The main limitation of such an approach is that it requires the full evaluation to be run, since it needs the effectiveness evaluation for each system/topic pair (usually represented as a system/topic matrix of, e.g., average precision values). So the results have been so far mainly theoretical, with almost no indication on how to select the few good topics in practice. In this paper we propose such a
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210108

practical criterion for topic selection. We rely on the methods for automatic effectiveness evaluation without relevance judgments [1, 4, 9, 10, 12­14, 17], and by running some extensive experiments on several TREC collections we show that the topics selected on the basis of those evaluations are indeed more informative than a random selection of topics.
2 RELATED WORK
2.1 A Few Good Topics
We briefly summarize the research on topic subsets and on effectiveness evaluation without human relevance judgments.
The table on the left of Figure 1 is a representation of the results of a TREC-like evaluation. Each row is a system / run and each column is a topic; each cell of the matrix APi, j is the effectiveness value (in this paper we focus on Average Precision, AP) of system i on topic j. When averaging across the n columns (topics) one obtains Mean AP (MAP), a measure of the effectiveness of a system.
The number n of topics in an evaluation initiative has received attention since the first TREC editions (and even before) [3]. The classical and main approach has been to understand what happens when selecting a random subset of topics of a given cardinality (i.e., a by computing MAP on a random subset of the columns in Figure 1). By doing so, the evaluation of systems / runs is in general different than when using the full set of topics. The effect is generally measured using the (linear, rank) correlation between the ground truth of the m original MAP values and the m predicted MAP values, obtained on the basis of the topic subset only.
A different approach, more related to our work, has been to find the best subset of topics of a given cardinality: one does not select a random subset of columns in Figure 1 but the optimal subset, i.e., the one leading to the highest correlation value between the real and the predicted MAP. The first proposal is by Guiver et al. [6], which performed a heuristic search on all possible topic subsets. More in detail, their work focused on the correlation of three series: (i) Best, the subset of topics which has the highest correlation (i.e.,

1145

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Kevin Roitero, Michael Soprano, and Stefano Mizzaro

the predicted MAP of systems is the most similar) with the ground truth; (ii) Worst, the subset of topics which has the which has the smallest correlation (i.e., the predicted MAP of systems is the least similar) with the ground truth; and (iii) Average, the correlation that one might expect when choosing topics randomly. Guiver et al. used both Pearson's linear and Kendall's rank correlations; in this paper we focus on the latter. Their analysis shows that subsets of good and bad topic sets exist with a high / low correlation even at low cardinalities; for example (see the chart on the right of Figure 1), on a ground truth of 50 topics, at cardinality 8 one can identify a Best topic subset with   0.85 and a Worst set with   0.1).
This work has been continued by Robertson [11], who questioned the generality of Guiver et al. results, as well as attempted a first example of a practical topic selection strategy, that however turned out to be ineffective. Berto et al. [2] confirmed the findings of Guiver et al. and extended their work by looking at the number, the distribution, and the stability of the Best and Worst topic sets. Such a research direction has been so far purely theoretical, with no indication on how to select the few good topics in practice. In this paper we propose such a practical approach. In this respect, our work is an attempt to make the methodology proposed in the above described publications [2, 6, 11] more similar to the more recent work by Kutlu et al. [7] who proposed a learning-to-rank based approach for topic selection, and analyzed in detail the role of deep and shallow pooling in the topic selection process, and its impact on the evaluation.
2.2 Evaluation without Relevance Judgments
The first proposal of evaluating IR systems without relevance judgments is by Soboroff et al. [13]; their proposal is simply to random sample documents from the pool and treat such documents as relevant. The intuition is that the random sample is performed on a biased set of documents: if many different systems retrieve the same document (maybe even in the first rank positions) that document is probably relevant. Results show that the estimated final rank of systems correlates decently (Kendall's   0.5) with the official TREC ranking; the method fails in predicting the rank of best systems, which are somehow "peculiar".
Wu and Crestani [17] proposed a method based on data fusion techniques, which are used to merge the ranked lists of documents retrieved by the systems, assigning a popularity score to each document, and using such score to provide an estimated final rank of systems. Wu and Crestani propose five variants to assign the popularity score to each document.
Another approach, proposed by Aslam and Savell [1], measures the similarity of the ranked lists of each pair of systems, and ranks such systems by the computed similarity index. This methodology is strongly correlated with Soboroff et al.'s one. Aslam and Savell also raise the issue that the predicted ranking of systems is based on document popularity rather than relevance.
Nuray and Can [9, 10] proposed a method based on three strategies used in democratic elections to measure popularity of candidates: the "RankPosition", the "Borda", and the "Condorcet" method. Furthermore, such indexes are computed considering either all the system/runs which participated in a given TREC edition (the "normal" method), or selecting just the most "peculiar" systems (the

"bias" method), that are the systems that have a ranked list which deviates more from the norm.
Spoerri [14] proposed an evaluation method which relies on a set of trials between runs; each run is assigned five times to a set with other four runs, thus in this way each system participate in exactly five trials. Then, for each trial, for each system the method computes the percentage of documents retrieved by the system alone ("Single" index), the percentage of documents retrieved by all the systems in the trial ("AllFive"), and the algebraic difference between the Single and the AllFive indexes ("SingleMinusAllFive"); those three indexes are averaged across the trials and then retrieval systems are ranked according to their SingleMinusAllFive index, the lower the index the better.
Sakai and Lin [12] proposed a variation of the Condorcet method proposed in [10], which is more feasible to compute even for deep pools, and it is strongly correlated although statistically different from Nuray and Can's proposal.
All the above methods provide an approximate evaluation matrix; that is a system by topic matrix similar to the one in Figure 1, but with predicted APi, j and MAPi values.
3 OUR APPROACH
The main limitation of the above "few good topics" approaches [2, 6, 11] is that they are only theoretical, not practical. For example, they provide an optimum to aim at (the Best correlation curves) but to actually select the Best subset of topics, the whole TREC evaluation exercise has to be performed, since the matrix in Figure 1 is needed. However, each of the methods for effectiveness evaluation without relevance judgments [1, 4, 9, 10, 12­14, 17] indeed provides an approximation of that matrix, which can possibly be used as input to the approaches that find the optimal subset of a few good topics. This is the main idea of this paper.
Thus, we run the few topics approach used in [2, 6, 11] on the approximate matrices. Then, for each cardinality, we consider the Best subset of topics found on the approximate matrices and we use such topics on the real matrix to produce a predicted evaluation of systems using the selected few topics. We then measure the correlation obtained by such a reduced topic subset: if the approximate matrices are representative of the real matrix, then the obtained correlation should be higher than the correlation found with a random selection of topics (i.e., the Average correlation curve in Figure 1).
Moreover, instead of using the individual methods alone, we combine such methods. The intuition on which we rely is that while a single method can produce a poorly representative matrix on a particular collection and a highly representative matrix on another collection, a combination of all the approximate matrices should provide a more stable and general AP prediction. To combine the methods, we follow the same approach of Mizzaro et al. [8]: we train a machine learning system that, on the basis of the TREC data of the previous years, learns a model that is then applied on a subsequent year TREC test collection. So, previous years test collections are the training set and the new test collection is then the test set; the features are the approximate matrices produced by the individual methods, and the combination function is the learned best combination of them to fit the real AP values. In other terms, the combination function, or the machine learning model, is

1146

Short Research Papers II Effectiveness Evaluation with a Subset of Topics: A Practical Approach

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Table 1: The datasets used in this paper.

Acronym Name

Year Topics Runs Used Topics

TREC5
TREC6
TREC7
TREC8
TREC01
TB04
R05 W111 W121 W131 W141

Ad Hoc 1996

Ad Hoc 1997

Ad Hoc 1998

Ad Hoc 1999

Ad Hoc 2001

TeraByte 2004

Robust

2005

Web Track 2011

Web Track 2012

Web Track 2013

Web Track 2014

50

61 251-300

50

74 301-350

50 103 351-400

50 129 401-450

50

97 501-550

49

69 701-750 (no 703)

50

74 See [15, Figure 1]

50

61 101-150

50

48 151-200

50

55 201-250

50

30 251-300

1 Binarized: collapsed relevance levels {-2, 0} into 0, and {1, 2, 3} into 1.

PPI

0.12 0.10 0.08 0.06 0.04 0.02 0.00 -0.02

1

10

20

30

40

50

Cardinality

Figure 3: The distributions of PPI values for each cardinality.

TREC8 -- RF 1.0

W12 -- M5P 1.0

0.9 0.8
0.8

0.7

0.6

Tau Tau

0.6

0.4

0.5 0.2
0.4
0.0 0.3

0.2

-0.2

1

10

20

30

40

50

Cardinality

1

10

20

30

40

50

Cardinality

Figure 2: Two examples of the obtained correlation curves.

the one that, on the basis of historical data, provides the best prediction of real AP values. We try six machine learning algorithms [16]: Linear Regression (LR), M5P model tree (M5P), Random Forest (RF), Neural Networks (NN), Support Vector Machine with Polykernel (SVM_Poly), and Support Vector Machine with Radial Basis Function Kernel (SVM_RBF).

4 EXPERIMENTS AND RESULTS

To run our experiments we use the ten datasets shown in Table 1. When compared to the experiments by Kutlu et al. [7], we attempt a more general and systematic evaluation, as we consider 10 different test collections instead of the 4 used by them. For each of the test collections, we compute the 22 approximate matrices (the 16 individual methods of Section 2.2 plus their 6 machine learning combinations above described).
Figure 2 shows two examples of the obtained correlation curves, compared to the Best, Average and Worst. The one on the left is obtained by RF on TREC8 (the same data as in Figure 1); the one on the right by M5P on W12. To better understand, as well as objectively quantify, the effectiveness of the methods, we define the following simple index that measures if and how much the obtained correlation curve is above the Average curve. We denote with Mi (k) the Kendall's  correlation of the i-th method at cardinality k, and with A(k) the  correlation of the Average series at cardinality k. If we have C collections, each of them having T topics, then we can define the Predictive Power Index of method i (PPI(i)) as

PPI(i)

=

1 C

CT
(Mi (k)
c=1 k =1

-

A(k ))

.

PPI describes the behavior of a topic selection strategy with respect to the Average series (i.e., a random topic selection). If PPI > 0

PPI

0.3 0.2 0.1 0.0

TREC5 TREC6 TREC7 TREC8 TREC01 TB04 R05 Cardinality

W11

W12

W13

W14

Figure 4: The distributions of PPI values for each collection.

the topic selection strategy is effective, if PPI < 0 it is not, and if PPI = 0 the topic selection strategy is equivalent to the expected value of a random topic selection.
The box-plots in Figure 3 show the distributions of the PPI values (y-axis) for each cardinality (x-axis). Each box-plot is a representation of the 22 PPI values obtained, one for each method. The PPI index is almost always positive, for both the median values and the lowest quantiles. Also the number of negative outliers is very low. However, this positive result depends in part on a skewed distribution of the PPI values for different collections, as demonstrated in Figure 4, that shows a breakdown of the PPI values for each collection separately: PPI values are still on the y-axis, the different collections are on the x-axis, and each box-plot is still obtained combining the single PPI values for the 22 methods. The W14 collection is a clear outlier, and the positive results obtained might depend just on it. We therefore repeat the same analysis without W14. Figure 5 shows that, although to a smaller extent, the positive result still hold: PPI are positive. This is also confirmed by running a statistical significance test: all series have p-value < 0.01 according to the paired Wilcoxon signed rank test.
Thus, when selecting a few good topics using the approximate matrices obtained with the methods for effectiveness evaluation without relevance judgments, one indeed finds topic subsets that are significantly better than a random selection. This is true for most of such methods; however, some methods could be more effective in this respect. In the last part of the paper we address this issue. We start by remarking that in our scenario not all the cardinalities have the same importance, for two main reasons: (i) above a certain cardinality threshold, the Average  correlation is close to 1 (i.e., the higher the cardinality the harder is to beat the Average baseline),

1147

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

0.125

0.100

0.075

0.050

PPI

0.025

0.000

-0.025

-0.050

1

10

20

30

40

50

Cardinality

Figure 5: The distribution of PPI values for each cardinality, having excluded the outlier collection W14.

all

1-5

1-10

0.20

1-15

1-20

0.15

PPI

0.10

0.05

0.00

-0.05

SMV5MP_RBF

RNSFuVNrNMauEyr_BaTPyiaONsLoNCYruomrnaadyLlCoNRroocnermdt3oSa0rOlcReBatNO3nu0kRrPaOoyFsBiFtiaiosnB3o0rdaS3I0NNGurLAaWEySNULoSACrIMmBNAaGlSBLIEoCrMNdIauNW3rUa0UySCBAiVaL1sLRFIaVWnEkUPCoVsi3tionA3L0LFIVWEUCSVA2KAI30

Figure 6: PPI of the methods over cardinality ranges.

0.125 0.100

feature M5P RF

0.075

0.050

PPI

0.025

0.000

-0.025

-0.050

1

10

20

30

40

50

Cardinality

Figure 7: The PPI values obtained by M5P and RF, compared

to the distribution of PPI values of Figure 5.

and (ii) higher cardinalities are less interesting, since the higher the cardinality the less useful the topic set reduction is; in other words, since we are interested in reducing as much as possible the topic set size, the lower the cardinality the better.
We therefore perform a breakdown of the PPI measure over the first 5 cardinalities (1,5], the first 10, the first 15, the first 20, and all cardinalities. Figure 6 shows the result: whereas when considering all cardinalities M5P, SVM_RBF, and RF are the three most effective methods, when focusing on lower cardinalities clearly M5P and RF show consistently higher PPI. We thus focus on this two methods in Figure 7. The plot shows that both RF and M5P have a PPI score greater than zero, especially for lower cardinalities, i.e., the most

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA
Kevin Roitero, Michael Soprano, and Stefano Mizzaro
interesting ones. Also note that the two curves shown in Figure 2 are two examples of those obtained by these two methods: in both cases, using fewer than 10 topic produces ranking of systems with a Kendall's correlation higher than 0.8 with the ranking obtained with all the topics. Overall, the topics selected applying the fewer topics algorithm to the results of the M5P and RF methods are those with the best results.
5 CONCLUSIONS AND FUTURE WORK
We have applied the few topics approach to the outcome of the methods for effectiveness evaluation without relevance judgments. Our experimental results on ten TREC test collections show that such methods (and especially M5P and RF) allow to select a subset of a few topics that evaluate a population of systems / runs in a more accurate way than a random selection of topics of the same cardinality. This practical result is the first successful attempt of showing the practical usefulness of the few topics approaches [2, 6, 11], thus addressing their main limitation. For example, it could be used to select which topics to evaluate when resources are limited. In future work we plan to include TREC collections with a higher number of topics, as well as report on a more detailed comparison with the work by Kutlu et al. [7]. Furthermore, we plan to apply models used in response theory [5] to study the relationship between topic sets.
REFERENCES
[1] Javed A. Aslam and Robert Savell. 2003. On the Effectiveness of Evaluating Retrieval Systems in the Absence of Relevance Judgments. In Proceedings of 26th ACM SIGIR. 361­362.
[2] Andrea Berto, Stefano Mizzaro, and Stephen Robertson. 2013. On Using Fewer Topics in Information Retrieval Evaluations. In Proc. of ACM ICTIR 2013. 9:30­ 9:37.
[3] Chris Buckley and Ellen M. Voorhees. 2000. Evaluating Evaluation Measure Stability. In Proceedings of the 23rd ACM SIGIR. ACM, New York, NY, USA, 33­40.
[4] Fernando Diaz. 2007. Performance Prediction Using Spatial Autocorrelation. In Proceedings of 30th ACM SIGIR. 583­590.
[5] Susan E Embretson and Steven P Reise. 2013. Item response theory. Psychology Press.
[6] John Guiver, Stefano Mizzaro, and Stephen Robertson. 2009. A Few Good Topics: Experiments in Topic Set Reduction for Retrieval Evaluation. ACM Trans. Inf. Syst. 27, 4, Article 21 (Nov. 2009), 26 pages.
[7] Mucahid Kutlu, Tamer Elsayed, and Matthew Lease. 2018. Intelligent topic selection for low-cost information retrieval evaluation: A New perspective on deep vs. shallow judging. Inform. Processing & Management 54, 1 (2018), 37­59.
[8] Stefano Mizzaro, Josiane Mothe, Kevin Roitero, and Md Zia Ullah. 2018. Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin. In Proc. of the 41st ACM SIGIR. In press.
[9] Rabia Nuray and Fazli Can. 2003. Automatic Ranking of Retrieval Systems in Imperfect Environments. In Proceedings of 26th ACM SIGIR. 379­380.
[10] Rabia Nuray and Fazli Can. 2006. Automatic ranking of information retrieval systems using data fusion. Information Processing & Management 42, 3 (May 2006), 595­614.
[11] Stephen Robertson. 2011. On the Contributions of Topics to System Evaluation. In Proceedings of the 33rd ECIR. 129­140.
[12] Tetsuya Sakai and Chin-Yew Lin. 2010. Ranking Retrieval Systems without Relevance Assessments -- Revisited. In Proceeding of 3rd EVIA -- A Satellite Workshop of NTCIR-8. National Institute of Informatics, Tokyo, Japan, 25­33.
[13] Ian Soboroff, Charles Nicholas, and Patrick Cahan. 2001. Ranking Retrieval Systems Without Relevance Judgments. In Proc. of 24th ACM SIGIR. 66­73.
[14] Anselm Spoerri. 2007. Using the structure of overlap between search results to rank retrieval systems without relevance judgments. Information Processing & Management 43, 4 (2007), 1059 ­ 1070.
[15] Ellen M Voorhees. 2003. Overview of the TREC 2003 Robust Retrieval Track.. In Trec. 69­77.
[16] Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal. 2016. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann.
[17] Shengli Wu and Fabio Crestani. 2003. Methods for Ranking Information Retrieval Systems Without Relevance Judgments. In Proceedings of the 2003 ACM Symposium on Applied Computing. 811­816.

1148

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Beyond Pooling

Gordon V. Cormack
University of Waterloo gvcormac@uwaterloo.ca
ABSTRACT
Dynamic Sampling is a novel, non-uniform, statistical sampling strategy in which documents are selected for relevance assessment based on the results of prior assessments. Unlike static and dynamic pooling methods that are commonly used to compile relevance assessments for the creation of information retrieval test collections, Dynamic Sampling yields a statistical sample from which substantially unbiased estimates of effectiveness measures may be derived. In contrast to static sampling strategies, which make no use of relevance assessments, Dynamic Sampling is able to select documents from a much larger universe, yielding superior test collections for a given budget of relevance assessments. These assertions are supported by simulation studies using secondary data from the TREC 2017 Common Core Track.
ACM Reference Format: Gordon V. Cormack and Maura R. Grossman. 2018. Beyond Pooling. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210119
1 INTRODUCTION
This paper argues that the pooling method should be replaced by "Dynamic Sampling" ("DS"), in which active learning is used to select a stratified sample of documents for assessment to form the "gold standard" of relevance in an information retrieval ("IR") test collection. The essential idea is to adapt Scalable Continuous Active Learning ("S-CAL") [4]--a technology-assisted review method that repeatedly draws samples for assessment from strata of exponentially increasing size--for this task.
Each stratum consists of the documents deemed next-most-likely to be relevant by a learning algorithm, based on prior assessments. The feature representation employed by the learning algorithm may be derived from the relevance rankings afforded by the systems to be evaluated, relevance rankings afforded by reference systems, document content, or any combination of these approaches.
Together, the samples comprise a statistical sample of an arbitrarily large portion of the entire corpus--not just the top-ranked documents submitted for evaluation--whose size is bounded by a fixed assessment budget, regardless of the number of relevant documents in the corpus (R). An unbiased Horvitz-Thompson estimate of recall-independent measures, such as precision at rank (P@k),
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210119

Maura R. Grossman
University of Waterloo maura.grossman@uwaterloo.ca
or R, can be derived from this sample [11]. Estimators for recalldependent measures, such as average precision, R-precison (P@R), or NDCG, which employ a non-linear combination of these elementary estimators, entail bias, which can be mitigated by arranging a low-variance estimate of R.
Dynamic Sampling is particularly suitable for creating test collections that may be used to evaluate the effectiveness of methods that are not available at the time of construction. Provided enough relevant documents are found by DS to provide a low-variance estimate of R for each topic, future methods have no systematic advantage or disadvantage relative to those whose rankings were used as input to the DS process.
Using data from the TREC 2017 Common Core Track1 [1], we simulate the use of DS to create test collections using assessment budgets of 100, 200, 300, and 600 documents per topic, which compare favorably to the official Common Core test collection, which was derived using the "max mean" dynamic-pooling strategy [9].
2 THE TREC 2017 COMMON CORE TRACK
The TREC 2017 Common Core Track ("Core Track") offered a strong baseline depth-100 subset pool consisting of 30,030 relevance assessments over 50 topics [9]. The authors employed the AutoTAR active learning method [5] to assess 11,825 documents for the same 50 topics. These two efforts yielded 9,002 and 8,986 positive assessments, respectively, but only 3,715 in common between them. For the purpose of this exposition, we consider ground truth to be the union of these two sets.
The Core Track baseline is atypical because of two factors that enured to its advantage. First, the topics had been previously used for the TREC 2004 Robust Track [16], yielding 76,783 relevance assessments, albeit for a different set of documents. Thirty-three of the topics had also been used for the TREC 2005 Robust Track [17], yielding 23,911 assessments for yet another set of documents. Of the 55 system rankings used to form the Core Track depth-100 pool, 14 were influenced by these 100,694 prior assessments.
A second factor enuring to the advantage of the baseline was the influence of manual assessments conducted by participating teams such as ourselves. At least one other team also employed active learning and assessed many thousands of documents in the course of their participation; a total of 12 system rankings (including three of ours) were influenced by manual effort. The remaining 29 rankings used to form the pool were derived using fully automatic methods, without influence from historical or current relevance assessments.
While 75 runs, each consisting of 10,000 documents per run, were submitted by participating teams, the Core Track assessment effort considered only the top-ranked 100 documents from 55 runs deemed "highest priority" by each of the Track participants.
1See https://trec-core.github.io/2017/.

1169

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

The experiments detailed below evaluate Dynamic Sampling methods that consider all submitted documents; a pool consisting of one-third more runs and one-hundred times the pool depth. We also consider the effect of restricting the pool to the 29 fully automatic methods. Finally, we consider DS methods that use no pool at all, selecting documents for assessment from the universe of documents, based on their tf-idf word representation.
3 DYNAMIC POOLING
Move-to-front pooling ("MTF") [6] is arguably the first application of active learning to the selection of documents for assessment and inclusion in an IR test collection [13]. Unlike the depth-k pooling method, which selects the top-ranked k documents from each of a set of participating runs for assessment, active learning methods repeatedly select documents from among those not yet assessed, based on the relevance assessments rendered for previously selected documents. A number of studies [9, 13] indicate that for k < 100, MTF and other active learning methods yield better test collections than depth-k pooling, for the same number of assessments, relative to a gold-standard test collection constructed using depth-1002 pooling, which has been the de facto standard for nearly three decades.
The assumption that depth-100 pooling constitutes ground truth has influenced current practice, which typically applies active learning methods to select documents only from the depth-100 pool. This "dynamic pooling" approach requires fewer relevance assessments than the depth-100 pool, but yields an inferior test collection that can, at best, approach the quality of the depth-100 pool. Hereafter, we use the term "subset pooling" to refer to any strategy that selects a subset of a fixed-depth pool for assessment, and "dynamic pooling" to refer to a subset-pooling strategy that employs active learning. Thus, depth-k, meta-ranking, and statistical sampling are subset-pooling strategies, while MTF, hedge, and "bandit" methods [9] are dynamic-pooling strategies.
4 SUBSET SAMPLING
The literature describes several methods to employ statistical sampling as a subset-pooling strategy [3, 11, 19]. The general approach is to draw a uniform or stratified random sample from the pool for assessment, and then to measure the effectiveness of a run by applying a statistical estimator to approximate the value of some effectiveness measure (e.g., P@k or average precision), were the entire pool to be judged.
The "infAP" family of methods [19] (generically, "infAP") use a separate statistical estimator for each stratum, and combine the results to form an overall estimate. The "statAP" family of methods [11] (generically, "statAP"), on the other hand, employ a HorvitzThompson estimator [10] to form an overall estimate based on the inclusion probabilities of the relevant documents in the strata, without regard to the strata from which the documents came. The minimal test collections family of methods [3] (generically, "MTC") are not statistical sampling methods, as they provide intentionally biased estimates for the purpose of distinguishing among runs.
2The arguments here apply to depth-K pooling where K > k , but depth-100 is nearly universal, and should be considered synecdoche for depth-K .

Dynamic Sampling differs from subset sampling in that it is not constrained to selecting documents from a pool, and it identifies strata sequentially in response to the relevance assessments for previous strata. Of the methods used for subset sampling, only statAP is amenable for DS, because it provides a reasonably unbiased estimate, even with many sparsely sampled strata. In our empirical work, we use Pavlu's reference implementation of statAP.3
5 TOTAL RECALL
Interactive Search and Judging ("ISJ"), which involves a sustained search effort to identify and label a substantial fraction of all relevant documents in a collection, has been shown to yield test collections of comparable quality to those yielded by depth-100 pooling, with less effort [6, 14]. More recently, a particular active learning approach has been shown to compare favorably to ISJ, while obviating the need for repeated query reformulation by a search expert [5]. The TREC 2002 Filtering Track [15] coordinators used a similar method to construct a substantially complete set of relevance assessments prior to the conduct of the Track, so that the relevance assessments could be used to simulate real-time feedback, as well as to evaluate the effectiveness of the submitted runs. Subsequent application of depth-100 pooling uncovered a number of relevant documents that were not discovered by the prior effort, but those additional documents were found to have an insubstantial impact on the resulting effectiveness estimates.
The current state of the art in active learning for this purpose is AutoTAR [5], as realized by the TREC Total Recall Track Baseline Model Implementation ("BMI").4 AutoTAR was used to label test collections prior to the TREC 2015 and 2016 Total Recall Tracks, and also was provided to Track partcicipants as a baseline method. No submitted run consistently bettered BMI, whether the effectiveness was measured using the official AutoTAR-selected assessments, statistically sampled assessments, or post-hoc assessments by one of the participating teams [7].
The authors used a modified version of BMI to prepare their submission to the Core Track [2]. Although at the time of this writing, only 50 of the Core Track topics had been assessed, 250 topics were provided to participants. For 250 topics, the authors spent 64.1 hours assessing 42,587 documents (on average, 15.4 mins/topic; 5.4 secs/doc), judging 30,124 of them to be relevant (70.7%). Of these judgments, 11,825 pertained to the 50 topics that have, to date, been officially assessed. Given the Core Track's tight timeline, our efforts were necessarily incomplete. Nonetheless, we judged relevant roughly the same number of documents as the Core Track's official assessors. Of these, more than one-third were not in the Core Track's assessment pool. A statistical sample embedded in one of our submissions indicates that the majority of these unassessed documents would have been judged relevant, had they been assessed [2].
The TREC 2015 and 2016 Total Recall results show that AutoTAR can achieve near-perfect recall given 2R +100 assessments per topic, where R is the number of relevant documents to be found [8, 12]. According to this rule of thumb, we would have had to expend
3See http://trec.nist.gov/data/million.query/07/statAP_MQ_eval_v3.pl. This implementation corrects a serious error in an unpublished but commonly cited manuscript [10]. The description in Pavlu's dissertation [11] is correct. 4See http://cormack.uwaterloo.ca/trecvm/.

1170

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

about

2

1 2

times

as

much

assessment

effort

to

achieve

near-perfect

recall, if we assume that there were 14,273 relevant documents, in

accordance with our ground-truth assumption.

Over and above the absolute cost of obtaining relevance assess-

ments is the problem of budgeting and resource allocation. It is

not known in advance how many documents will be relevant to

each topic, and therefore how assessment resources should be ap-

portioned or scheduled over topics. The variance of R in the Core

Track topics is huge, ranging from 9 to 1,377, with an average of

285 and a mean of 191. Topics with large R consume an inordinate

fraction of the budget, as may topics with very small R, where it

could be a challenge to find any relevant documents.

6 DYNAMIC SAMPLING
Dynamic Sampling adapts AutoTAR to identify a sequence of strata with exponentially increasing size, which are sampled with diminishing frequency, so that the total number of documents assessed per topic is equal to a sampling budget that is fixed in advance. A sampling budget of precisely 600 assessments per topic, or 30,000 assessments in total, nearly equals the 30,030 assessments of the Core Track depth-100 subset pool.
The exponentially increasing strata are precisely the exponentially growing batches of documents identified by AutoTAR; however, a uniform random sample of each batch is selected for assessment, and used to train the learning method, which then identifies the next batch. The main idea is derived from Cormack and Grossman's "S-CAL" [4], which is itself derived from AutoTAR. While the purpose of S-CAL is to achieve the best possible classifier over an infinite population, with statistical estimation playing a supporting role, the purpose of Dynamic Sampling is to achieve the best possible statistical estimator, with classification playing a supporting role. In response to this difference in emphasis, we amended the procedure by which the sampling rate decays.

Algorithm 1 Dynamic Sampling Algorithm (from S-CAL [4]).

1: Construct a relevant pseudo-document from topic description.

2: The initial training set contains only the pseudo-document.

3: Set the initial batch size B to 1.

4: Set the initial decay threshold T to hyper-parameter N .

5: Temporarily augment the training set by adding 100 random

documents from the collection, labeled "not relevant."

6: Score all documents using a model induced from training set.

7: Remove the random documents added in step 5.

8: Select the highest-scoring B documents not previously selected.

9: Draw n =

B ·N T

 B random documents from step 8.

10: Render relevance assessments for the n documents.

11: Add the assessed documents to the training set.

12: Increase B by

B 10

.

13: If the number of assessed relevant documents R  T , double T .

14: Repeat 5 through 13 until assessment budget A is reached.

The Dynamic Sampling method is outlined in Algorithm 1. The only input to the method, other than the document collection, topic statement, and assessment budget A, is a hyper-parameter N controlling the decay of the sampling rate. At the outset, and until at least N relevant documents are discovered, every document in

every batch presented by AutoTAR is presented for assessment, and used to train the model. In the event that N documents are never found, documents are examined exhaustively until the assessment budget is met. Once N relevant documents are found, the sampling rate is halved until N more relevant documents are found, and so on, until the assessment budget is reached.
N quantifies a tradeoff between the objectives of selecting the largest possible number of relevant documents for assessment, and sampling a universe containing the largest possible number of relevant documents. In the extreme case where N = A, and also in the extreme case where R  N , DS is exactly AutoTAR. In the opposite extreme where N  R, the sampling frequency will nearly vanish, and the universe will approach the entire document population. We evaluated all twenty combinations of A  {100, 200, 300, 600} and N  {12, 25, 50, 100, 200}.

7 FEATURE ENGINEERING

In one version of DS, we represented each document as a tf-idf

word vector, exactly as calculated by BMI. In a second version,

we represented each document as a reciprocal-rank vector with d

dimensions, corresponding to d runs used to form a depth-10000

pool. For the full pool consisting of all Core Track submissions,

d = 75; for the automatic pool consisting exclusively of automatic

runs, d

=

29.

The value of each feature is set

to

1 d

·

1 50+

,

where



is the rank of the document according to the corresponding run. In

Table 1, rows labeled "DN " denote results using tf-idf and hyper-

parameter N ; rows labeled "RN " denote results using rank features

from the full pool; and rows labeled "AN " denote results using the

automatic pool. Rows labeled "RDN " and "ADN " denote results

that average the scores from two separate models in Algorithm 1,

step 6: one using tf-idf features; the other using rank features from

the full pool or automatic pool, respectively.

8 RESULTS
The quality of a test collection may be characterized by how well it computes effectiveness measures for particular runs, or by how well it ranks the relative effectiveness of the various runs that it may be called upon to evaluate. For this short paper, we focus on how well a test collection ranks the 75 Core Track runs according to MAP, using Kendall's  to compare rankings. We have also computed AP [18], variance, and bias, which show the same effect. Table 1 shows  for three subsets of the Core Track pool, and the variants of our Dynamic Sampling method, sorted by the column labeled A = 600.
"Core" denotes the 30, 030-document pool identified by the Core Track. "Core-Auto" denotes a subset of Core containing assessments only for documents that are among depth-100 pool formed using only the 29 automatic rankings. Core-Auto contains 15,024 assessments--about 300 per topic--and is therefore included under A = 300 in Table 1. "Core-Sample" denotes a two-stratum sample, drawn by the Core Track organizers, of the depth-75 pool of the 55 runs used to form the Core Track pool. The first stratum consists of all documents in the top-10 pool, while the second stratum consists of a 20% sample of all documents in the top-75 pool, excluding the documents in the first stratum. Core-Sample contains 15,024 assessments--also about 300 per topic--and is included under A = 300.

1171

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Kendall  correlation between ground truth and test collections built using pooling and dynamic sampling.

Method
AD50 RD25 R25 R50 R100 RD100 RD50 AD200 A25 RD200 AD100 Core R12 D50 RD12 D12 AD12 D100 AD25 Core-Sample D200 A100 D25 R200 A50 A12 A200 Core-Auto

Assessment Budget Per Topic*

A=100

A=200

A=300

A=600

.835

.932

.971

.986

.881

.956

.969

.982

.734

.936

.970

.982

.531

.887

.941

.981

.431

.843

.914

.980

.813

.908

.953

.978

.837

.933

.963

.977

.814

.895

.940

.977

.735

.912

.951

.975

.813

.897

.940

.974

.815

.918

.945

.972

.971

.873

.947

.962

.967

.794

.918

.956

.967

.930

.939

.956

.966

.873

.943

.966

.965

.892

.954

.962

.964

.802

.890

.931

.962

.882

.946

.949

.961

.944

.796

.886

.926

.960

.458

.807

.888

.960

.838

.925

.958

.959

.436

.794

.881

.958

.550

.853

.918

.957

.839

.921

.942

.955

.450

.775

.850

.941

.420

Core Core-Sample Core-Auto RN AN DN RDN ADN N * 

Key Core Track assessment pool Core Track depth-75 stratified sample Core Track pool, automatic only Dynamic, all 75 rankings Dynamic, 29 automatic rankings only Dynamic, content Dynamic, all 75 rankings + content Dynamic, 29 automatic rankings + content denotes hyper-parameter N (see text) Fixed budget per topic unless indicated by  Variable budget per topic

9 CONCLUSIONS
For assessment budget A = 600, the Core Track collection is near the median of the various Dynamic Sampling methods. Remarkably, the top-scoring system for this budget is AD50, which uses only automatic rankings and document content--along with relevance feedback--to select documents for assessment. While we cannot say that the difference between AD50 and RD50 results is substantial, it does appear that RDN offers no material advantage over ADN .

It also appears that ADN and RDN compare favorably with Core,
except for the smallest N = 12. Of the single-model methods, RN
fares the best, and offers an apparently superior plug-in replacement
to the dynamic-pooling method used by the Core Track. The results
for DN appear to be slightly inferior to those for Core; however,
it should be noted that DN uses no pooling whatsoever, and is
the only technique presented here that could be used to construct
a test collection prior to the conduct of an evaluation task. AN
is remarkably good, considering the narrow set of rankings from
which it is derived, but inferior to the other methods.
For A = 300, the results for ADN , RDN , and RN are nearly as
good as for A = 600, and better than Core-Sample. Perhaps un-
surprisingly, the results for Core-Auto are abysmal, confirming
received wisdom that the pooling method depends on the pres-
ence of manual runs. The extreme difference between between
Core-Auto and AD50 is noteworthy because both methods rely on
information from the same set of automatic runs, and use the same
number of assessments.
The results for A = 100 and A = 200 show an overall degradation
and an increase in variability, but may still reflect useful methods,
as they require one-sixth to one-third as many assessments as the
Core Track effort. An open question remains: Would assessing all
250 topics with DS and a budget of A = 120 have yielded a better
test collection, for the same total effort of 30,000 assessments? We
believe so.
REFERENCES
[1] James Allan, Evangelos Kanoulas, Donna Harman, and Ellen Voorhees. TREC 2017 Common Core Track overview. In TREC 2017.
[2] Anonymized. Participation in the TREC 2017 Core Track. In TREC 2017. [3] Ben Carterette, James Allan, and Ramesh Sitaraman. Minimal test collections for
retrieval evaluation. In SIGIR 2006. [4] Gordon V Cormack and Maura R Grossman. Scalability of continuous active
learning for reliable high-recall text classification. In CIKM 2016. [5] Gordon V Cormack and Maura R Grossman. Autonomy and reliability of contin-
uous active learning for technology-assisted review. arXiv:1504.06868, 2015. [6] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. Efficient
construction of large test collections. In SIGIR 1998. [7] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. Automatic and
semi-automatic document selection for technology-assisted review. In SIGIR 2017. [8] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. TREC 2016 Total Recall Track Overview. In TREC 2016. [9] David E Losada, Javier Parapar, and Álvaro Barreiro. Feeling lucky?: Multi-armed bandits for ordering judgements in pooling-based evaluation. In Proceedings of the 31st Annual ACM Symposium on Applied Computing, pages 1027­1034. ACM, 2016. [10] V Pavlu and J Aslam. A practical sampling strategy for efficient retrieval evaluation. College of Computer and Information Science, Northeastern University, 2007. [11] Virgil Pavlu. Large Scale IR Evaluation. Northeastern University, 2008. [12] Adam Roegiest, Gordon V Cormack, Maura R Grossman, and Charles L A Clarke. TREC 2015 Total Recall Track Overview. In TREC 2015. [13] Mark Sanderson et al. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010. [14] Mark Sanderson and Hideo Joho. Forming test collections with no system pooling. In SIGIR 2004. [15] Ian Soboroff and Stephen Robertson. Building a filtering test collection for TREC 2002. In SIGIR 2003. [16] Ellen M. Voorhees. Overview of the TREC 2004 Robust Track. In Proceedings of the 13th Text REtrieval Conference, Gaithersburg, Maryland, 2004. [17] Ellen M Voorhees. The TREC 2005 Robust Track. In ACM SIGIR Forum, volume 40. ACM, 2006. [18] Emine Yilmaz, Javed A Aslam, and Stephen Robertson. A new rank correlation coefficient for information retrieval. In SIGIR 2008. [19] Emine Yilmaz, Evangelos Kanoulas, and Javed A. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In SIGIR 2008.

1172

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Testing the Cluster Hypothesis with Focused and Graded Relevance Judgments

Eilon Sheetrit
Technion -- Israel Institute of Technology seilon@campus.technion.ac.il
Oren Kurland
Technion -- Israel Institute of Technology kurland@ie.technion.ac.il
ABSTRACT
The cluster hypothesis is a fundamental concept in ad hoc retrieval. Heretofore, cluster hypothesis tests were applied to documents using binary relevance judgments. We present novel tests that utilize graded and focused relevance judgments; the latter are markups of relevant text in relevant documents. Empirical exploration reveals that the cluster hypothesis holds not only for documents, but also for passages, as measured by the proposed tests. Furthermore, the hypothesis holds to a higher extent for highly relevant documents and for those that contain a high fraction of relevant text.
ACM Reference Format: Eilon Sheetrit, Anna Shtok, Oren Kurland, and Igal Shprincis. 2018. Testing the Cluster Hypothesis with Focused and Graded Relevance Judgments. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210120
1 INTRODUCTION
The cluster hypothesis is: "closely associated documents tend to be relevant to the same requests" [9, 22]. Several cluster hypothesis tests were proposed [5, 9, 20, 23]. Most of these measure similarities between relevant documents with respect to their similarities with non-relevant documents using various similarity estimates [18]1.
Cluster hypothesis tests operate on documents and utilize binary relevance judgments2. A relevant document can contain (much) non-relevant information which could affect inter-document similarity estimates, and hence, cluster hypothesis tests3. Furthermore, to address the drawbacks of retrieving relevant documents with much non-relevant information, passages (instead of documents)
1One of these measures [18] is based on a "reversed" cluster hypothesis: documents should be deemed similar if they are co-relevant to the same queries [7]. 2An exception is a cluster hypothesis test applied to entities [19]. 3Some work uses passage-based information to induce inter-document similarity measures so as to address this issue [12, 18].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210120

Anna Shtok
Technion -- Israel Institute of Technology annabel@technion.ac.il
Igal Shprincis
Microsoft, Herzliya, Israel igals@microsoft.com
can be retrieved (e.g., [1­3, 6, 8, 10, 13]). However, cluster hypothesis tests were not reported for passage (focused) retrieval.
We present novel cluster hypothesis tests that utilize graded and focused relevance judgments [1, 8]; the latter are markups of relevant text in relevant documents. We apply the tests for both documents and passages. We set out to perform a finer analysis than that enabled by using binary relevance judgments. Our tests account for the extent to which a text item (passage or document) is deemed relevant as a whole (i.e., relevance grade), or the amount of relevant text it contains (i.e., focused relevance judgments).
The proposed tests consider the nearest neighbors of an item (document or passage) in the similarity space. The motivation to use a nearest-neighbor-based test, following Voorhees' cluster hypothesis test which utilizes binary relevance judgments for documents [23], is two-fold. First, the results of this nearest-neighbor-based test [23] were shown to be correlated to some extent, for certain retrieval settings, with the relative effectiveness of using inter-document similarities for retrieval [14, 17]. Second, using nearest-neighbor-based clusters for document retrieval was shown to be highly effective with respect to using other clustering techniques [11, 16].
Applying the proposed cluster hypothesis tests over INEX and TREC datasets reveals the following. The cluster hypothesis holds, as measured using the tests, not only for documents but also for passages. Furthermore, the higher the fraction of relevant text an item (passage or document) contains, the more similar it is to other relevant items; specifically, those with a high fraction of relevant text. Finally, documents marked as highly relevant are more similar to other relevant documents (specifically, highly relevant documents) than relevant documents not marked as highly relevant.
In summary, our main contributions are cluster hypothesis tests that utilize graded and focused relevance judgments, and showing that the cluster hypothesis holds also for passages.
2 TESTING THE CLUSTER HYPOTHESIS
Our goal is to test the cluster hypothesis for documents and passages, henceforth referred to as items, using, when available, graded or focused relevance judgments. As in past work [15, 17, 21], we perform the test on a list of items retrieved for a query. We use language-model-based retrieval and inter-item similarity estimates.
2.1 Document and Passage Retrieval
Let q, , d, and D denote a query, a passage, a document and a corpus of documents, respectively. We measure textual similarities using

1173

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

cross entropy (CE) [24]: Sim(x, y) d=ef exp(-CE(pxMLE (·) || pyDir (·)); pxMLE (·) is the maximum likelihood estimate induced from x; pyDir (·) is the Dirichlet smoothed language model induced from y.

We use L to denote a list of n items (documents or passages)

most highly ranked with respect to q. The document list, Ldoc , is

retrieved using a standard language-model-based approach [24]:

document d in the corpus is scored by Sim(q, d). The passage list,

Lps, is created by ranking all passages of documents in Ldoc .

Specifically, passage  in document d ( Ldoc ) is scored by [2, 3]:

(1 - )

Sim
 Lp s 

(q,  ) S i m (q,

)

+



Sim(q,d ) dLdoc S im(q,d)

;



is

a

free

parameter.

We apply cluster hypothesis tests either to Ldoc or to Lps. The

tests rely on analyzing the nearest-neighbors of an item x in the

list L it is part of: N N (x, L; k) is the set of k items y in L (y x)

that yield the highest Sim(x, y).

2.2 Nearest-Neighbor-Based Tests
Inspired by Voorhees' nearest-neighbor cluster hypothesis test [23], we present a suite of novel tests. The tests account for the relevance degree of items: their relevance grade (binary or graded relevance judgments), or the fraction of query-pertaining text they contain (focused relevance judgments [1, 8]). Thus, the tests help to explore how items of low and high relevance degrees are situated in the similarity space with respect to each other.
A test is performed for query q with respect to a retrieved item list L. As a first step, we define the seed set S ( L); seed items are those whose relevance degree is at least  which is a free parameter. For each seed x in S we compute the average relevance degree of its nearest neighbors in N N (x, L; k). The values attained for the seeds in S are averaged to yield a test value for query q. We report the average over queries of a per-query test value.
We use A-B to name a cluster hypothesis test where A is the type of relevance degree used for selecting seeds and B is the type used to quantify relevance degrees of the seeds' neighbors. A and B, which can differ, can be: (i) Binary. The relevance degree of an item is 0 (not relevant) or 1 (relevant) based on binary relevance judgments; for seed selection:  = 1. (ii) Focused. The relevance degree of an item is the fraction of relevant text it contains as measured using focused relevance judgments; for seed selection:  = 0+ (i.e., a seed is an item with relevance degree > 0). (iii) Graded. The relevance degree of an item is its graded relevance judgment: 0 (not relevant), 1 (relevant), 2 (highly relevant); for seed selection:  = 1 or  = 2.
The Binary-Binary cluster hypothesis test, where the test value is the average number of relevant items among the nearest neighbors of a relevant item, is Voorhees' original test which was applied to documents [23]. All other tests are novel to this study.

3 EXPERIMENTAL SETUP
The datasets used for experiments are specified in Table 1. The INEX dataset contains English Wikipedia articles which were flattened by removing all XML markups. It was used for the focused retrieval tracks in 2009 and 2010 [1, 8] and includes binary document-level, and focused, relevance judgments. That is, annotators marked every piece of relevant text in a relevant document. The fraction of text, measured in characters, in an item (document or passage) that was marked relevant is the item's focused relevance degree.

For the TREC datasets (GOV2 and ClueWeb) there are graded relevance judgments at the document level, but no passage-level nor focused relevance judgments. The 0/1/2 graded relevance degrees we use (see Section 2.2) were set as follows. For GOV2 queries, and for ClueWeb queries from TREC 2009, we used the given relevance grades 0/1/2 as relevance degrees. For ClueWeb queries from TREC 2010, 2011 and 2012, additional relevance grades are available: "key" and "nav". We assign a relevance degree 1 to documents marked as "nav" (navigation) as the relevance description of "nav" does not directly translate to "highly relevant". Documents marked as "key" are assigned a relevance degree 2 following TREC's description (https://plg.uwaterloo.ca/~trecweb/2011.html). The motivation for "folding" the relevance grades of the years 2010­2012 to 0/1/2 relevance degrees is two-fold. There are relatively very few documents whose relevance labels are "nav" and "key"; hence, using them as is will bias the cluster hypothesis tests. Second, given the differences of labeling schemes for ClueWeb across the years, using the original relevance grades as relevance degrees would have required addressing the query set of each year separately. This would have resulted in sparsifying the data used to perform the cluster hypothesis tests.
Titles of topics were used as (short) queries. (Queries with no relevant documents in the qrels files were removed.) Krovetz stemming was applied to queries and documents. Stopwords on the INQUERY list were removed only from queries. The two-tailed paired permutation test with p  0.05 was used to determine statistically significant differences of cluster hypothesis test results. We apply Bonferroni correction for multiple comparisons where needed. We used non-overlapping fixed-length windows of 50, 150 and 300 terms for passages; using non-overlapping passages was a requirement in INEX's focused retrieval tracks [1, 8]. The Indri toolkit was used for experiments (www.lemurproject.org).
Document and passage retrieval were performed as described in Section 2.1. For passage retrieval, the passages in the 1000 most highly ranked documents were ranked. For ClueWeb we removed from the document ranking documents with a Waterloo's spam classifier score below 50 [4]. The Dirichlet smoothing parameter, µ, was set to 1000 [24] for document retrieval and for measuring inter-item similarities (see Section 2.1); µ was set to values in {500, 1500, 2500} for passage retrieval. The value of  for passage retrieval was in {0.1, 0.2, . . . , 0.9}. To set the values of µ and  which affect passage retrieval, we used leave-one-out cross validation; MAiP@1500, the mean (over 101 standard recall points) interpolated average precision computed for the top-1500 passages, was used to optimize passage retrieval performance over the train folds following the suggested practice in INEX's focused retrieval tracks [1, 8].
We applied the cluster hypothesis tests to the document (Ldoc ) and passage (Lps) lists with the number of nearest neighbors, k, in {4, 9}, and the number of items in a list, n, in {50, 100, 150, 200}.
4 EXPERIMENTAL RESULTS
Binary-Binary tests. We first study the cluster hypothesis for passages. For reference, we report the test results for documents.
We use the INEX dataset, which in contrast to the TREC datasets includes focused relevance judgments, and therefore allows to assign relevance degrees to passages. For both seed selection and quantification of the relevance-degree of nearest neighbors, we use

1174

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Datasets used for experiments.

Corpus # of docs

Data

Avg doc. length

Queries

INEX 2,666,190

2009&2010

552

2009001-2009115, 2010001-2010107

GOV2 25,205,179

GOV2

930

701-850

ClueWeb 50,220,423 ClueWeb09 (Category B)

807

1-200

Table 2: Binary-Binary test ("bb") over INEX for passages of different lengths and documents. "rand": the expected test value if neighbors are randomly selected rather than via inter-item similarities as in "bb". "ratio": the (average over queries) ratio between "bb" and "rand". Underline: the best result in a row. Boldface: the highest ratio per n (list size).

n = 50

n = 100

n = 150

n = 200

item k bb rand ratio bb rand ratio bb rand ratio bb rand ratio

Psg50

4 .680 .520 1.95 .690 .457 2.61 .689 .402 2.74 .699 .377 2.78 9 .646 .520 1.67 .656 .457 2.14 .648 .402 2.39 .659 .377 2.49

Psg150

4 9

.698 .642

.433 .433

2.26 1.85

.710 .664

.377 .377

2.57 2.31

.705 .660

.339 .339

2.97 2.67

.696 .651

.310 .310

3.27 2.95

Psg300

4 9

.699 .652

.408 .408

2.10 1.92

.680 .637

.340 .340

2.65 2.43

.672 .629

.301 .301

3.19 2.93

.659 .612

.270 .270

3.69 3.34

Doc

4 .662 .387 2.28 .625 .306 3.27 .604 .260 4.20 .584 .227 5.07 9 .591 .387 1.90 .563 .306 2.63 .539 .260 3.28 .520 .227 3.93

the Binary regime so as to follow previous work on testing the cluster hypothesis for documents [15, 17, 23]. That is, we use the Binary-Binary test. A passage is considered relevant if it contains at least one character marked as relevant (cf., [1, 8]). For consistency across experimental settings, only queries (95 out of 120) for which all item lists contain at least two relevant items were used.
As a reference comparison to the test result we present the mean, over queries and seeds per query, precision of the n - 1 items in a list excluding the seed at hand. This baseline, denoted rand, is an estimate of the probability of randomly drawing a relevant item as a neighbor of a seed; hence, this is the expected test result if all neighbors of a seed were randomly selected. (Thus, the result is the same for k = 4 and k = 9 neighbors.) Naturally, the higher n, the lower the rand value. We also report the average ratio over queries between the Binary-Binary test value and the random baseline: the higher the ratio, the cluster hypothesis holds to a larger extent. Using the ratio serves to normalize the test result with respect to the number of relevant items in the list being analyzed which can considerably affect test results.
We see in Table 2 that the ratio, for all passage lengths (50, 150, 300), number of nearest-neighbors considered (k) and sizes of the passage list (n), is substantially larger than 1. The ratio is also substantially larger than 1 for documents which is aligned with previous findings [15, 17, 21]. Hence, we conclude that the cluster hypothesis, as measured using a nearest-neighbor test, holds not only for documents, but also for passages.
Table 2 also shows that the ratio for passages and documents is higher for k = 4 neighbors than for k = 9 neighbors. This finding, which is aligned with those in reports for documents [15, 17], can be attributed to the fact that the more distant the neighbors are from a relevant seed, the less likely they are to be relevant.
We also see in Table 2 that for a given number of neighbors, k, it is often the case that the longer the passage, the higher the ratio value. Indeed, the likelihood of vocabulary mismatch between two long relevant passages is lower than that for two short relevant

Table 3: The effect of  for seed selection on the Focused-
Binary and Focused-Focused tests for INEX (n = 50). '1', '2', '3' and '4': statistically significant differences with " = 0+",
" = .10", " = .25" and " = .50", respectively.

Focused-Binary

Focused-Focused

item k  = 0+  = .10  = .25  = .50  = .75  = 0+  = .10  = .25  = .50  = .75

Psg50 4 9

.714 .678

.7191 .72812 .73512 .74012 .6821 .68712 .69112 .697132

.652 .621

.6561 .66212 .66712 .67112 .6241 .62712 .62912 .63412

Psg150 4 9

.733 .670

.740 .6761

.75512 .68812

..766975131322

..777040131322

.624 .569

.6331 .5761

.65012 .58712

..656955131322

..6579691313242

Psg3004 .706 9 .653

.7241 .6671

.73612 .6741

..765856131322

..7679461313242

.549 .502

.5681 .5151

.58712 .52412

..650356131322

..65245413132424

Doc

4 .662 9 .595

.6951 .7061 .7101 .6161 .62612 .6311

.7171 .6321

.369 .328

.3941 .3411

.3971 .3441

..430478131

.4131 .3501

passages. Accordingly, the highest ratio values per list size (n) are attained for documents. (Refer to the boldfaced numbers.)
Focused-Binary and Focused-Focused tests. Table 3 presents the results of the Focused-Binary and Focused-Focused tests over INEX. In both tests, an item is selected as a seed if its fraction of relevant text  . In the Focused-Binary test, we report the average number of relevant neighbors of the seed, while in the FocusedFocused test we report the average fraction of relevant text that the neighbors contain. We set n = 50, and below we study the effect of varying n. Note that posting the constraint  = 0+ for the Focused-Binary test results in the Binary-Binary test reported in Table 2. However, the numbers in Table 3 differ from those in Table 2 as we perform the test only for queries that have in their retrieved item lists at least one seed with respect to  = .75 and an additional relevant item. We do not use the ratio values as in Table 2 since we compare test values only for the same item list; i.e., the type of items and n are fixed. Hence, the test results are comparable.
Table 3 shows that increasing the seed's relevance degree (i.e., increasing ) increases both the average number of its neighbors which are relevant (Focused-Binary) and the average fraction of relevant text they contain (Focused-Focused). Many of the improvements in tests' results when increasing  are statistically significant. (Bonferroni correction was applied for multiple comparisons.) Thus, we see that relevant items with a high fraction of relevant text are more likely to be similar to other relevant items, specifically those which also contain a high fraction, than to non-relevant items.
In Figure 1 we present the effect of varying the list size, n, on the results of the Focused-Focused test for k = 4. (The results for k = 9 are not presented as they do not convey additional insight.) As can be seen, for all item types, and all values of n, the higher , the higher the test result. This further supports the conclusion based on Table 3 about the connection between the fraction of relevant text items contain and inter-item similarities.
For each graph in Figure 1, curves that correspond to a lower  depict milder increase or steeper decline in test values as a function of n, compared to curves that correspond to a higher . Indeed, larger lists (higher n) and/or those containing longer items have lower precision (see the "rand" column in Table 2). For these lists, the higher the fraction of non-relevant text in an item, the higher the likelihood it will be more similar to non-relevant items.
Graded-Binary and Graded-Graded tests. Table 4 presents the results of Graded-Binary and Graded-Graded tests performed over

1175

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

0.71 0.70 0.69 0.68 0.67 0.66 0.65
50
0.64 0.62 0.60 0.58 0.56 0.54 0.52 0.50
50

 = 0+

 = .10

 = .25

 = .50

 = .75

Psg50

100

150

n

0.69 0.68 0.67 0.66 0.65 0.64 0.63 0.62 0.61 0.60 200 50

Psg150

100

150

200

n

Psg300

100

150

n

0.42 0.41 0.40 0.39 0.38 0.37 0.36 0.35 0.34 0.33 0.32 200 50

Doc

100

150

200

n

Figure 1: The effect of n on the Focused-Focused test for INEX with k = 4. Note: graphs are not to the same scale.

Table 4: Graded-Binary and Graded-Graded tests performed for documents. '1' marks statistically significant difference with respect to " = 1".

Graded-Binary

GOV2

ClueWeb

n k  =1  =2  =1  =2

50

4 9

.737 .673

.7711 .7031

.641 .581

.656 .598

100 4 9

.710 .657

.7621 .6911

.606 .540

.631 .561

150 4 9

.709 .650

.7551 .6871

.580 .506

.597 .525

200 4 9

.699 .644

.7521 .6831

.558 .487

.585 .512

Graded-Graded

GOV2

ClueWeb

 =1  =2  =1  =2

1.001 1.1231 .952 1.0171

.908 .9751 .859 .8901

.950 1.1351 .881 .9971

.875 .9871 .784 .8581

.934 1.1301 .829 .9381

.854 .9861 .723 .8001

.911 1.1211 .798 .9301

.837 .9781 .697 .7891

document lists for the TREC datasets. These datasets have no focused relevance judgments; therefore, the tests are performed only for documents. Furthermore, the INEX dataset is not used as it has no graded relevance judgments. The queries used are those whose retrieved list contains at least one relevant seed with  = 2 and an additional relevant document. Since the comparisons we discuss are for the same list (determined by n), the test results are comparable and there is no need to use the ratio values as in Table 2.
Table 4 shows that in all cases, increasing the relevance degree (grade) of the seed document results in its neighborhood containing more relevant documents (Graded-Binary) with higher relevance grades (Graded-Graded). These findings are conceptually reminiscent of those presented above for the Focused-Binary and FocusedFocused tests. We also see that the increase in the tests' values when moving from  = 1 to  = 2 is always statistically significant for GOV2; for ClueWeb, statistically significant increase is observed for the Graded-Graded test.
5 CONCLUSIONS AND FUTURE WORK
We presented novel cluster hypothesis tests that utilize graded and focused relevance judgments. We found that (i) the cluster hypothesis holds for passages; (ii) relevant items (documents or passages)

that contain a high fraction of relevant text are more similar to
other relevant items (specifically, those with a high fraction of
relevant text) than relevant items with low fraction; and, (iii) docu-
ments marked as highly relevant are more similar to other relevant
documents (specifically, highly relevant) than relevant documents
not marked as such. These findings motivate the development of
passage retrieval methods that utilize inter-passage similarities.
Acknowledgments. We thank the reviewers for their comments.
This paper is based upon work supported in part by the German
Research Foundation (DFG) via the German-Israeli Project Cooper-
ation (DIP, grant DA 1600/1-1).
REFERENCES
[1] Paavo Arvola, Shlomo Geva, Jaap Kamps, Ralf Schenkel, Andrew Trotman, and Johanna Vainio. 2011. Overview of the INEX 2010 ad hoc track. In Comparative Evaluation of Focused Retrieval. Springer, 1­32.
[2] James P. Callan. 1994. Passage-Level Evidence in Document Retrieval. In Proc. of SIGIR. 302­301.
[3] David Carmel, Anna Shtok, and Oren Kurland. 2013. Position-based contextualization for passage retrieval. In Proc. of CIKM. 1241­1244.
[4] Gordon V Cormack, Mark D Smucker, and Charles LA Clarke. 2011. Efficient and effective spam filtering and re-ranking for large web datasets. Information retrieval 14, 5 (2011), 441­465.
[5] Abdelmoula El-Hamdouchi and Peter Willett. 1987. Techniques for the measurement of clustering tendency in document retrieval systems. Journal of Information Science 13, 6 (1987), 361­365.
[6] Ronald T. Fernández, David E. Losada, and Leif Azzopardi. 2011. Extending the language modeling framework for sentence retrieval to include local context. Information Retrieval 14, 4 (2011), 355­389.
[7] Norbert Fuhr, Marc Lechtenfeld, Benno Stein, and Tim Gollub. 2012. The optimum clustering framework: implementing the cluster hypothesis. Information Retrieval 15, 2 (2012), 93­115.
[8] Shlomo Geva, Jaap Kamps, Miro Lethonen, Ralf Schenkel, James A Thom, and Andrew Trotman. 2010. Overview of the INEX 2009 ad hoc track. In Focused retrieval and evaluation. Springer, 4­25.
[9] Nick Jardine and C. J. van Rijsbergen. 1971. The use of hierarchic clustering in information retrieval. Information storage and retrieval 7, 5 (1971), 217­240.
[10] Mostafa Keikha, Jae Hyun Park, W Bruce Croft, and Mark Sanderson. 2014. Retrieving passages and finding answers. In Proc. of ADCS. 81.
[11] Oren Kurland. 2009. Re-ranking search results using language models of queryspecific clusters. Information Retrieval 12, 4 (2009), 437­460.
[12] Sylvain Lamprier, Tassadit Amghar, Bernard Levrat, and Frédéric Saubion. 2008. Using text segmentation to enhance the cluster hypothesis. In Proc. of AIMSA. 69­82.
[13] Vanessa Murdock and W Bruce Croft. 2005. A translation model for sentence retrieval. In Proc. of HLT-EMNLP. 684­691.
[14] S.-H. Na, I.-S. Kang, and J.-H. Lee. 2008. Revisit of nearest neighbor test for direct evaluation of inter-document similarities. In Proc. of ECIR. 674­678.
[15] Fiana Raiber and Oren Kurland. 2012. Exploring the cluster hypothesis, and cluster-based retrieval, over the Web. In Proc. of CIKM. 2507­2510.
[16] Fiana Raiber and Oren Kurland. 2013. Ranking document clusters using markov random fields. In Proc. of SIGIR. 333­342.
[17] Fiana Raiber and Oren Kurland. 2014. The correlation between cluster hypothesis tests and the effectiveness of cluster-based retrieval. In Proc. of SIGIR. 1155­1158.
[18] Fiana Raiber, Oren Kurland, Filip Radlinski, and Milad Shokouhi. 2015. Learning asymmetric co-relevance. In Proc. of ICTIR. 281­290.
[19] Hadas Raviv, Oren Kurland, and David Carmel. 2013. The cluster hypothesis for entity oriented search. In Proc. of SIGIR. 841­844.
[20] Mark D Smucker and James Allan. 2009. A new measure of the cluster hypothesis. In Proc. of ICTIR. 281­288.
[21] Anastasios Tombros, Robert Villa, and C. J. Van Rijsbergen. 2002. The effectiveness of query-specific hierarchic clustering in information retrieval. Information processing & management 38, 4 (2002), 559­582.
[22] C. J. van Rijsbergen. 1979. Information Retrieval (second ed.). Butterworths. [23] Ellen M. Voorhees. 1985. The cluster hypothesis revisited. In Proc. of SIGIR.
188­196. [24] Chengxiang Zhai and John Lafferty. 2001. A study of smoothing methods for
language models applied to ad hoc information retrieval. In Proc. of SIGIR. 334­ 342.

1176

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Query Performance Prediction Focused on Summarized Letor Features

Adrian-Gabriel Chifu
Aix Marseille Univ, Université de Toulon, CNRS, LIS Marseille, France
adrian.chifu@univ-amu.fr

Léa Laporte
INSA Lyon, LIRIS, UMR 5205 CNRS Lyon, France
lea.laporte@insa-lyon.fr

Josiane Mothe
ESPE, Université de Toulouse, IRIT, UMR5505 CNRS Toulouse, France
Josiane.Mothe@irit.fr
ABSTRACT
Query performance prediction (QPP) aims at automatically estimating the information retrieval system effectiveness for any user's query. Previous work has investigated several types of pre- and post-retrieval query performance predictors; the latter has been shown to be more effective. In this paper we investigate the use of features that were initially defined for learning to rank in the task of QPP. While these features have been shown to be useful for learning to rank documents, they have never been studied as query performance predictors. We developed more than 350 variants of them based on summary functions. Conducting experiments on four TREC standard collections, we found that Letor-based features appear to be better QPP than predictors from the literature. Moreover, we show that combining the best Letor features outperforms the state of the art query performance predictors. This is the first study that considers such an amount and variety of Letor features for QPP and that demonstrates they are appropriate for this task.
CCS CONCEPTS
· Information systems  Retrieval effectiveness; Information retrieval query processing;
KEYWORDS
Query performance prediction, Query difficulty prediction, Query features, Post retrieval features, Letor features
ACM Reference Format: Adrian-Gabriel Chifu, Léa Laporte, Josiane Mothe, and Md Zia Ullah. 2018. Query Performance Prediction Focused on Summarized Letor Features . In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210121
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210121

Md Zia Ullah
UPS, Université de Toulouse, IRIT, UMR5505 CNRS Toulouse, France mdzia.ullah@irit.fr
1 INTRODUCTION
While Information Retrieval system effectiveness is usually measured as an average effectiveness over queries, it is well-known that a system performs differently on each individual queries. Query performance prediction (QPP) aims at automatically estimating the information retrieval system effectiveness for any user's query difficulty without relevance judgement [7]. Predicting query performance is a first mandatory step to be able to adapt query processing when the query is detected as difficult. System adaptation can consist in selective query expansion [2], or selecting the best system configurations depending on the query features [9], for instance. Thus, having accurate predictors is a hot and very important topic.
The literature of the field has investigated the use of various types of predictors. Pre-retrieval features can be calculated from the query itself. IDF is an example of a pre-retrieval feature which is calculated by extracting the IDF of each query term from the inverted file and then aggregating the values over the query terms [11]. Some pre-retrieval features need external resources to be calculated as for example the number of query term senses based on WordNet [15]. On the other hand, calculating post-retrieval features requires evaluating the query over the whole collection of documents. They are usually based on functions applied to the document scores or retrieved document lists. For example, Query Feedback (QF) measures the overlap between two retrieved document lists for the original query and the expanded query [22]. Although postretrieval features are based on retrieved document scores, they are not Letor features strictly speaking. Indeed, Letor features have been first used in learning to rank applications where they have been shown to be effective [4, 6, 17, 19]. Letor features have also been used in other applications such as learning to rank system configurations [9] or reducing verbose textual queries [3], but have never been used as QPP. A systematic review does not yet exist in the context of QPP; this is the purpose of this short paper.
Recent work has focused on ways of combining predictors rather than finding better predictors [10, 18, 22]. Although combining features has been shown to be effective, we believe that the better the features, the better the QPP. Our paper revisits this perspective by evaluating more than 350 post-retrieval features (based on variants of Letor-based features) as QPP features. The paper focuses on the post-retrieval features since they have been shown to be more effective than pre-retrieval features for QPP [21].

1177

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

2 SUMMARIZED LETOR-BASED FEATURES
Letor features have been widely used in the context of learning to rank [17]. The main goal of learning to rank is to learn a function to better rank retrieved documents given a query; which is a different problem than QPP. Terrier's FAT component [14] implements most of the LETOR feature that can be found in [17]1 as well as a few others2. Most of the features are calculated from a matching score between the query and the document, either considering the document title or the full content, for a total of 39 features.
All the features are associated with query-document pairs. To make the Letor features (LF) usable as query features, we have used different summary functions over the retrieved documents, for a given query. More formally, let q be a query in the set of all queries Q and Dq,n be the set of the top-ranked n retrieved documents for the query q. We compute the i-th summarized Letor feature, SLFi (S, q) as follows:
SLFi (S, q) = S({LFi (d, q)}, q, Dq,n )
where d  Dq,n is a document, {LFi (d, q)} is the set of values for the Letor feature i for each couple (d, q) and S is a summary function with S  {Min, Max, Mean, Q1, Median, Q3, Std, V ar , and Sum}.
We used 9 summary functions for each LF: - Min , Max , Mean , Sum : the minimum (maximum, average, and sum) value of the Letor feature over the retrieved documents; - Q1, meadian , Q3: after calculating the Letor feature for each retrieved document, we sorted the values in increasing order; then the set is divided into quartiles. Q1 (respectively median, Q3) is the value that makes at least one quarter (2 quarters, 3 quarters) of the values having a score lower than Q1 (respectively median, Q3); - Std , V ar : the standard deviation and the variance of the Letor feature values. Each of the summary functions leads to a variant of the Letor feature. Thus, we have a group of summarized Letor features (SLF) for each LF. As opposed to previous research that analyses a few features, in this study, we analysed more than 350 SLF.
3 EVALUATION OF THE FEATURES
The accuracy of a query performance predictor is usually measured in the literature by evaluating the link (e.g. correlation value) between the predictor values (considered as the predicted effectiveness) and the actual system effectiveness values [11, 18]. Pearson correlation is usually considered; it assumes a linear correlation between the two analyzed variables. Since correlation may exist without being linear, we complete the study using Kendall correlation as well.
The results can be biased by the choice of the system used as a reference. What is now a common reference in QPP is Language Modeling with Dirichlet smoothing and µ=1000 [8, 18, 20, 22]. We also focus on this system in this paper and keep the analysis of system dependency for future work. With regard to system effectiveness, we considered AP and NDCG, which is also common practice in IR evaluation. Finally, we use four reference collections from TREC: Robust (528,155 newspaper documents, 250 topics), WT10G (1.6 million web/blog documents, 100 topics), GOV2 (25
1Features name are the ones used in Terrier; they correspond to different implementations of features number 3, 5, 13, 15, 20, 23, 25, 33, and 35 in Table 2 from [17]. 2All the features are detailed at http://www.terrier.org/docs/v4.0/javadoc/index.html? org/terrier/matching/models/WeightingModel.html

million web documents, 150 topics), and ClueWeb12B (52 million web documents, 100 topics).
3.1 Individual effectiveness
We studied the individual effectiveness for QPP of any single summarized Letor feature. We also considered the state of the art QPP features, namely: Clarity [8], QF [22], WIG [22], UQC [21], and NQC [21].
Since we have more than 350 SLF, it was not possible to present all the correlations; we selected the most correlated features. For each collection, we selected the top 4 features and reported their correlation for the 4 collections in Table 1. In the first part of the table, we present AP while the second part reports NDCG (both @1000). The features are ordered according to decreasing Pearson correlation on GOV2.
In Table 1, we can see that many of the SLF are more correlated than state of the art QPP features, at least on one of the collections, but many times across several collections. The most correlated feature (AP and NDCG) is WMODEL.DFIZ_std which represents a weighting model based on the standardized distance from independence in term frequency [16]. WMODEL.ML2_std (second feature for AP) represents a weighting model based on multinomial randomness model, with Laplace after-effect model and normalisation 2 [12]. WMODEL.In_expC2_max (second feature for NDCG) is the Inverse Expected Document Frequency weighting model with Bernoulli after-effect and normalization [1]. When considering the 4 different collections, the 2 correlation measures, the 2 evaluation measures, and all the SLFs, we found out that several of them consistently correlate significantly across collections and measures. This is the case for the first 5 SLFs in Table 1. None of the state of the art features are as robust across collections and measures. The state of the art feature that has a similar robustness is QF, but it is not significantly correlated for AP on ClueWeb12B. We also observed that the highest correlations are with SLF calculated on document content rather than on part of it such as the title only.
3.2 The impact of n
In the previous section, the SLF have been calculated when considering the n (1000) top-ranked retrieved documents. However, the value of n may have an important impact on the correlation [21]. It was not possible to report the effect of n on all the SLF. Since the idea was to observe the influence of n if any, we chose the first SLF from Table 1, namely WMODEL:DFIZ_std. It corresponds to the feature from Terrier calculated using Divergence From Independence model based on Standardization [13], to which we applied standard deviation when summarizing the values across documents and we showed it is robust across collections for QPP.
Table 2 reports the Pearson correlation that we obtained on the four collections. We can see that the best value of n is not consistent across collections while it is consistent for AP and NDCG. GOV2 requires fewer documents (top 500) to get the highest correlation than the other collections. This result holds also for other SLFs we analysed. An interesting track for future work is to analyse whether the optimal number of documents is the same across features or not. The robustness of the number of documents across queries is also an interesting problem to study.

1178

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Pearson (P.) and Kendall (K.) correlations between the features and the actual system effectiveness (AP@1000 and NDCG@1000). The top four correlated features are selected from each collection separately and then all are ordered by decreasing order based on Pearson correlation on GOV2. We also included the features from the literature that are not Letor-based. "*" stands for p-value < 0.05, while "+" stands for p-value < 0.001. Values in bold are higher than QF baseline.

AP WMODEL.DFIZ_std WMODEL.ML2_std WMODEL.In_expC2_max WMODEL.PL2_std WMODEL.In_expB2_max WMODEL.IFB2_max WMODEL.BB2_max WMODEL.IFB2_std WMODEL.DFReeKLIM_max WMODEL.DPH_max WMODEL.Js_KLs_max WMODEL.DFRee_max WMODEL.SFM.DirichletLM_max WMODEL.LemurTF_IDF_max WMODEL.LemurTF_IDF_std QF [22] Clarity [8] WIG [22] UQC [21] NQC [21] NDCG WMODEL.DFIZ_std WMODEL.In_expC2_max WMODEL.DFIC_std WMODEL.ML2_std WMODEL.IFB2_max WMODEL.PL2_max WMODEL.DPH_std WMODEL.Js_KLs_max WMODEL.BM25_std WMODEL.DFR_BM25_std WMODEL.XSqrA_M_Q3 WMODEL.XSqrA_M_mean WMODEL.SFM.DirichletLM.._std WMODEL.LemurTF_IDF_std QF [22] WIG [22] UQC [21] Clarity [8] NQC [21]

Robust P. K. .191* .200+ .251+ .220+ .320+ .359+ .334+ .296+ .316+ .353+ .318+ .353+ .298+ .344+ .393+ .337+ .343+ .287+ .347+ .292+ .305+ .260+ .295+ .251+ .392+ .324+ .414+ .348+ .457+ .356+ .432+ .343+ .435+ .308+ .315+ .253+ .496+ .358+ .125* .203+ P. K. .306+ .235+ .338+ .359+ .291+ .221+ .326+ .248+ .334+ .351+ .434+ .329+ .380+ .296+ .364+ .271+ .428+ .338+ .429+ .339+ -.055 -.040 -.066 -.047 .354+ .280+ .440+ .340+ .504+ .349+ .364+ .262+ .436+ .343+ .452+ .308+ .117 .191+

WT10G P. K. .217* .235+ .216* .290+ .403+ .298+ .211* .297+ .389+ .296+ .399+ .304+ .374+ .277+ .318* .310+ .313* .202* .323* .223* .266* .164* .252* .152* .235* .210* .342+ .282+ .207* .279+ .343+ .169* .239* .189* .181 .122 .311* .192* .010 .066 P. K. .321* .261+ .429+ .279+ .294* .221* .290* .294+ .420+ .282+ .389+ .258+ .432+ .342+ .280* .161* .320* .295+ .324* .298+ -.030 -.032 -.059 -.046 .430+ .253+ .206* .267+ .350+ .223* .228* .149* .304* .226+ .255* .170* .065 .106

GOV2 P. K. .453+ .305+ .453+ .328+ .449+ .306+ .449+ .319+ .438+ .297+ .437+ .293+ .436+ .300+ .394+ .260+ .385+ .277+ .385+ .276+ .367+ .262+ .359+ .255+ .350+ .270+ .298+ .251+ .258* .230+ .407+ .285+ .112 .086 .302+ .239+ .286+ .191+ -.037 .086 P. K. .447+ .334+ .444+ .326+ .441+ .328+ .438+ .359+ .429+ .312+ .412+ .322+ .397+ .308+ .382+ .289+ .344+ .289+ .341+ .290+ .335+ .217+ .308+ .202+ .302+ .215+ .243* .247+ .398+ .297+ .329+ .273+ .285+ .213+ .111 .100 -.004 .091

ClueWeb12B P. K.
.298* .255+ .265* .256+ .303* .225+ .267* .255+ .304* .228+ .298* .219* .308* .228+ .231* .232+ .376+ .259+ .374+ .253+ .379+ .253+ .377+ .249+ .263* .163* .223* .186* .170 .207* .163 .114 -.148 -.108 .366+ .201* .152 .064 -.095 -.047
P. K. .301* .239+ .264* .200* .303* .241+ .264* .230+ .257* .195* .324+ .235+ .238* .195* .361+ .243+ .162 .174* .160 .173* .363+ .245+ .359+ .235+ .200* .172* .114 .151* .210* .152* .342+ .186* .083 .020 -.132 -.099 -.143 -.069

3.3 Combining features for QPP
Assuming that different QPP features capture diverse information about query performance, whether a combination of features is likely to be a better predictor than a single feature. Following this assumption, a few studies have investigated the linear combination of features [5, 10, 20, 22]. These studies combine less than 10 features.
Considering that we have investigated more than 350 SLFs in the individual effectiveness analysis, using all these features in

a single model such as linear regression would not be optimum. In this paper, we choose to analyse the linear combination of the features that correlate the most with the effectiveness measure to predict, for each collection. Thus, for each collection, we plotted the correlations values for the best 20 features in decreasing order, then empirically searched for a gap in the correlation values, by looking at the differences between consecutive correlation values, two by two. We thus selected all the features before this gap. Note that the features may be different depending on whether we consider

1179

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Pearson correlation of the WMODEL:DFIZ_std [13] SLF predictor according to n, the number of top-ranked retrieved documents considered when calculating the feature.

n

5 10 50 100 500 1000

Robust

.056 .065 .089 .081 .146* .191*

AP

WT10G GOV2

.027 -.084 .163 .142 .175 .217* .261* .385+ .409+ .407+ .453+ .453+

ClueWeb12B .320* .255* .266* .243* .269* .298*

Robust

.081 .119 .183* .191* .252+ .306+

NDCG

WT10G GOV2

.069 -.032 .224* .217* .294* .321* .285+ .397+ .426+ .418+ .453+ .447+

ClueWeb12B .246* .234* .253* .224* .265* .301*

correlation on AP or NDCG. For AP, this process selected 5 features per collection, except for ROBUST (6 features). For NDCG, using the same process we selected 6 features for WT10G and ClueWeb12B, 8 features for Robust, and 10 features for GOV2. We leave the investigation of other feature selection methods such as LASSO (more sparse) and other machine learning methods such as random forest for future work since our main topic here is to investigate the effectiveness of new predictors.
Similarly to [22] and [10], we used multiple linear regression to combine the features into a single performance prediction model. We considered five different sets of predictors to be combined. A first set S1 contains the two state of the art predictors WIG and QF. It corresponds to our baseline. The second set S2 is composed of the two best Letor predictors on a given collection, according to the simple linear regression experiment. This set can be easily compared to the baseline since it is also composed of two features (thus the same level of complexity and costs to be calculated). The third set S3 contains all the best LETOR features per collection we selected as explained previously. The fourth set S4 is the union of sets S1 and S2, thus it combines the state of the art and the two best SLF features per collection. Finally, we considered all the best predictors selected per collection together with the two baselines WIG and QF in the fifth set S5. We considered the same collections, system, effectiveness, and evaluation measures as in the individual performance prediction analysis. We performed the leave-one-out cross-validation to predict query performance. As shown in Table 3,

Table 3: Performance of linear regression of combined postretrieval predictors according to Pearson correlation.

Combination

S1: WIG + QF S2: 2 Best SLF

AP S3: Best SLF

S4: S1  S2

S1  S3: All

S1: WIG + QF

S2: 2 Best SLF

NDCG

S3: S4:

Best SLF S1  S2

S5: S1  S3

Robust WT10G GOV2 ClueWeb .459+ .274* .399+ .287* .382+ .404+ .438+ .237* .402+ .339+ .420+ .302* .478+ .420+ .465+ .260* .454+ .427+ .509+ .208* .537+ .303* .405+ .286* .430+ .449+ .469+ .211* .458+ .457+ .464+ .312* .556+ .468+ .514+ .293* .526+ .446+ .487+ .188

for all the collections except ClueWeb12B and NDCG, the best correlation values are obtained when considering a combination of the two best Letor features and the two state of the art features. Considering ClueWeb12B and NDCG, the best correlation value is

obtained when considering a linear combination of the best Letor
features. If we take a fine-grained look at the results, we observe
that the best results are obtained when considering either the Letor
features alone or combined with WIG and QF, whatever the ef-
fectiveness measure is. As an additional analysis, we computed
the correlations between the features from the literature and the
best Letor predictors. The correlations values were not significant,
supporting our finding that Letor features are complementary to
state-of-the-art ones. Finally, with regard to AP, results are not di-
rectly comparable to Raiber's [18] (they found Pearson correlation of .557 for Robust, .346 for WT10G, .570 for GOV2) since we could
not reproduce their results; for example, the Pearson correlation
values they reported for individual state of the art QPPs are higher
than the ones we re-calculated using the same collections and the
same reference system.
4 CONCLUSION
We showed that the Summarized Letor features we defined are
good query performance predictors and that some of them are
more robust than the ones from the literature across collections.
Moreover, we found that they are complementary to the existing
features as when combined we achieved better QPP.
REFERENCES
[1] Giambattista Amati. 2003. Probability Models for IR based on Divergence from Randomness. Department of CS PhD (2003), 182.
[2] G. Amati, C. Carpineto, and G. Romano. 2004. Query difficulty, robustness, and selective application of query expansion. In ECIR. 127­137.
[3] Jaime Arguello, Sandeep Avula, and Fernando Diaz. 2017. Using Query Performance Predictors to Reduce Spoken Queries. In ECIR. Springer, 27­39.
[4] Niranjan Balasubramanian and James Allan. 2010. Learning to select rankers. In ACM SIGIR. 855­856.
[5] Shariq Bashir. 2014. Combining pre-retrieval query quality predictors using genetic programming. Applied Intelligence 40, 3 (2014), 525­535.
[6] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: from pairwise approach to listwise approach. In ICML. 129­136.
[7] David Carmel and Elad Yom-Tov. 2010. Estimating the query difficulty for IR. Syn. Lect. on Inf. Concepts, Retrieval, and Services (2010), 1­89.
[8] Steve Cronen-Townsend and W. Bruce Croft. 2002. Quantifying Query Ambiguity. In Conference on Human Language Technology Research. 104­109.
[9] Romain Deveaud, Josiane Mothe, and Jian-Yun Nie. 2016. Learning to rank system configurations. In CIKM. ACM, 2001­2004.
[10] Claudia Hauff. 2010. Predicting the effectiveness of queries and retrieval systems. SIGIR Forum 44, 1 (2010), 88. https://doi.org/10.1145/1842890.1842906
[11] Claudia Hauff, Djoerd Hiemstra, and Franciska de Jong. 2008. A survey of preretrieval query performance predictors. In ACM CIKM. 1419­1420.
[12] lker Kocaba, Bekir Taner Dinçer, and Bahar Karaolan. 2014. A Nonparametric Term Weighting Method for Information Retrieval Based on Measuring the Divergence from Independence. Inf. Retr. 17, 2 (April 2014), 153­176.
[13] lker Kocaba, Bekir Taner Dinçer, and Bahar Karaolan. 2014. A nonparametric term weighting method for information retrieval based on measuring the divergence from independence. Information retrieval 17, 2 (2014), 153­176.
[14] Craig Macdonald, Rodrygo LT Santos, Iadh Ounis, and Ben He. 2013. About learning models with multiple query-dependent features. TOIS 31, 3 (2013).
[15] Josiane Mothe and Ludovic Tanguy. 2005. Linguistic features to predict query difficulty. In Predicting query difficulty, ACM SIGIR workshop. 7­10.
[16] Vassilis Plachouras and Iadh Ounis. 2007. Multinomial Randomness Models for Retrieval with Document Fields. In ECIR. 28­39.
[17] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2010. LETOR: A benchmark collection for research on learning to rank for IR. Information Retrieval 13, 4 (2010), 346­374.
[18] Fiana Raiber and Oren Kurland. 2014. Query-performance prediction: setting the expectations straight. In ACM SIGIR. ACM, 13­22.
[19] D. Sheldon, M. Shokouhi, M. Szummer, and N. Craswell. 2011. LambdaMerge: merging the results of query reformulations. In ACM WSDM. 795­804.
[20] Anna Shtok, Oren Kurland, and David Carmel. 2010. Using statistical decision theory and relevance models for QPP. In ACM SIGIR. 259­266.
[21] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. 2012. Predicting query performance by query-drift estimation. ACM TOIS 30, 2 (2012).
[22] Yun Zhou and W Bruce Croft. 2007. Query performance prediction in web search environments. In ACM SIGIR. 543­550.

1180

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A Study of Per-Topic Variance on System Comparison

Meng Yang
School of Computer Science and Technology
Tianjin University mirandayang1213@163.com

Peng Zhang 
School of Computer Science and Technology
Tianjin University pzhang@tju.edu.cn

Dawei Song
School of Computer Science and Technology
Beijing Institute of Technology dawei.song2010@gmail.com

ABSTRACT
Under the notion that the document collection is a sample from a population, the observed per-topic metric (e.g., AP) value varies with di erent samples, leading to the per-topic variance. The results of the system comparison, such as comparing the ranking of systems according to the summary metric (e.g., MAP) or testing whether there is signi cant di erence between two systems, are a ected by the variability of per-topic metric values. In this paper, we study the e ect of per-topic variance on the system comparison. To measure such e ects, we employ two ranking-based methods, i.e., Error Rate (ER) and Kendall Rank Correlation Coe cient (KRCC), as well as two signi cance test based methods, namely Achieved Signi cance Level (ASL) and Estimated Di erence (ED). We conduct empirical comparison of TREC participated systems on Robust and Adhoc track, which shows that the e ect of per-topic variance on the ranking of systems is not obvious, while the signi cance test based comparisons are susceptible to the per-topic variance.

However, Cormack and Lynam proposed [1] that the document collection is a sample from a population of a large document collection. For di erent document collection samples, the per-topic metric values can be di erent. On the basis of this notion, the pertopic variance is generated. The per-topic variance may have e ect on system comparison. For example, the ranking of systems or the result of signi cance test between systems varies with di erent samples of the document collection population. We employ two kinds of methods to measure this e ect. The Error Rate (ER) and Kendall Rank Correlation Coe cient (KRCC) are utilized to analyze the consistency of the rankings of systems. On the other hand, Achieved Signi cance Level (ASL) and Estimated Di erence (ED) are used to compute the signi cance test results that re ects the statistical di erence between two systems. In the premise that document collection is a sample, this work seeks to explore the reliability of the evaluation results on the aspects of the ranking performance and the signi cance test.

KEYWORDS
Evaluation; Per-Topic Variance; System Comparison
ACM Reference Format: Meng Yang, Peng Zhang, and Dawei Song. 2018. A Study of Per-Topic Variance on System Comparison. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210122
1 INTRODUCTION
In traditional Cran eld-Style experiment, the evaluation is based on the xed document collection, a set of topics and the relevance judgment. Given a system-topic pair, the value of a per-topic evaluation metric (e.g., Average Precision, AP) is xed. When comparing two systems, the values of summary metric concerning all topics (e.g., Mean Average Precision, MAP) are used to rank systems. The signi cance test is conducted to judge whether there is signi cant di erence between two systems based on all the per-topic metric values.
Corressonding author.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210122

2 RELATED WORK
Under the condition of Cran eld-Style experiment, the variance of a system usually originates from the di erent topics or query variations. Zhang et al. proposed the bias-variance evaluation to study the variance of ranking performance across topics for the query language modeling approaches [13, 14] and the TREC participated systems [12].
Under the notion of document collection being a sample from a population, the variance can be generated from the di erent document collection samples. Cormack and Lynam tested the testcollection variability on AP and MAP [1]. Robertson studied the reverse relationship between recall and precision [4] and re-examined the common metrics in IR from the view of this notion [3]. In addition, Robertson et al. explored the per-topic noise caused by di erent document collections [6] and presented some insights to model score distribution without real scores [5].
However, the focus of this study is the e ect of per-topic variance on performance comparison among systems. The methods we used are inspired by the prior work. Voorhees [10] de ned ER to represent the probability of one experiment that leads to the wrong conclusion. KRCC is used to compare the rankings of systems using di erent metrics [2, 8]. Sakai employed ASL to compare metrics' sensitivity and ED to estimate the performance di erence for guaranteeing ASL <  (signi cance level) [7]. The application of these existing methods was on the Cran eld-Style experiment where document collection is xed. In this paper, however, these methods are not applied into the Cran eld-Style setup. Instead, we modify these methods to study the e ect of per-topic variance when the document collection is a sample.

1181

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

3 SYSTEM COMPARISON BASED ON PER-TOPIC VARIANCE
The implementation of document collection being a sample is through the method of simulation, such as bootstrap (BST), Kernel Density Estimation (KDE) and etc. In Section 3.1, these two models are brie y reviewed. The methods of comparing retrieval systems are introduced in Section 3.2.

3.1 Simulation Models
Given a system-topic pair, suppose that there are 1000 retrieved documents with 1000 scores correspondingly, and a number (denoted as r ) of them are relevant. Following [6], using Poisson distribution with the mean r , we take a sample rs from it. Speci cally, rs simulated scores are taken from the distribution of relevant scores and 1000 - rs simulated scores are from the score distribution of non-relevant ones. These 1000 simulated scores are sorted with a descending order. Then, a binary IR metric can be applied since each score is labeled as relevant or non-relevant.
The BST takes samples from the relevant scores and non-relevant scores with replacement, respectively. The KDE is a non-parametric method to estimate the probability density function. There are different kernels for KDE and the Gaussian kernel is used in our experiment. The relevant and non-relevant probability density functions are derived by tting relevant scores and non-relevant scores to KDE model, respectively. Then, simulated relevant and non-relevant scores are sampled from the relevant and non-relevant probability density functions, independently. Comparisons of these models are:
Non-parametric Distribution These two models assume no distributions for raw relevant and non-relevant scores. Thus, these models have a wider range of adaptability for each system-topic pair simulation.
Use of Real Data or Not The BST employs raw scores directly while the KDE samples scores from a tted score distribution, which indicates the simulated scores according to the BST model may be closer to the raw scores compared with the simulated scores according to the KDE model.

3.2 Methods of Comparing Systems

For two systems, X and Y, let x =(x1, x2, ..., xn ) and y =( 1, 2, ...,

n ) represent the raw metric values on n topics in a topic set Q. In

previous studies, such as [7, 9, 11], the topic in Q is a sample while

the document collection is xed, leading to the xed metric value

for each topic. In this paper, we study the per-topic variance, so that

each topic is xed while the document collection is a sample, where

the per-topic metric values are di erent for di erent samples. For a

simulation model, we take samples of relevant and non-relevant

scores from the corresponding score distributions and the simulated

metric values can be computed, denoted by xi =(x1i , x2i , .., xni ) and

yi =( 1i ,

2i , ..,

i n

),

where

xi

and

yi

represent

the ith

sample.

The methods of comparing systems under the notion of doc-

ument collection being a sample are based on the following two

ideas:

· For each topic, the raw metric value and the simulated metric value can be computed. Systems can be compared using these

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA
Meng Yang, Peng Zhang, and Dawei Song
per-topic raw metric values and simulated metric values, respectively, deriving raw comparison result and simulated comparison result. · The consistency between raw comparison result and simulated comparison result can be measured by methods described below. If two results are consistent, the variance is considered to have less e ect.
Due to the randomness of samples, simulated comparison results may be di erent. Therefore, the comparison can be repeated multiple times.
3.2.1 Error Rate. The assumption of computing ER is that the simulated comparison result about deciding the ranking between two systems should be the same as the counterpart of the raw data, otherwise it is a wrong conclusion. Generally, the larger the simulated summary metric values' di erence is, the smaller the probability of deriving a wrong conclusion will be. Let M(x) and M(y) denote the raw summary metric value for system X and Y, respectively, and the raw performance di erence is denoted as d. M(xi ) and M(yi ) represent the simulated summary metric value for ith sample of system X and Y and the corresponding simulated performance di erence is denoted as di . The performance di erence can be divided into several bins, which is similar with [10]. The algorithm for calculating the ER is shown in Algorithm 1.
ALGORITHM 1: Algorithm for computing error rate.
set Count = 0 and ErrorCount = 0 for each bin; for each system pair (X,Y) do
d = M (x) - M (y); for i = 1 to B do
take the ith sample from the simulation model, respectively, xi and yi ;
compute M (xi ) and M (yi ); di = M (xi ) - M (yi ); Count = Count + 1 for bin corresponding di ; if d  di < 0 then
ErrorCount = ErrorCount + 1 for bin corresponding di ; end end end for each bin do ER = ErrorCount / Count; end
3.2.2 Kendall Rank Correlation Coe icient. KRCC is used to test the similarity of the orderings of two groups of data when the two groups are ranked by each of the quantities. The larger coe cient re ects the more similar rankings of two groups of data. Given a speci c metric, the KRCC between the raw and simulated system ranking is calculated. Suppose that there are m systems, the raw and simulated summary metric values for these systems are denoted as S = (M(x1), M(x2), ..., M(xn )) and Si = (M(xi1), M(xi2), ..., M(xin )) where Si represents the ith sample. The algorithm is shown in Algorithm 2.

1182

Short Research Papers II A Study of Per-Topic Variance on System Comparison

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

ALGORITHM 2: Algorithm for computing KRCC.
compute S; set k[1...B] = 0 ; for i = 1 to B do
take the ith sample from the simulation model for each system; compute Si ; k[i] = kendall(S,Si ); end KRCC = mean(k);

3.2.3 Achieved Significance Level. The previous two methods

are based on the relative ranking of systems. Now, we describe a

signi cance test based method, namely Achieved Signi cance Level

(ASL). Whether there is signi cant di erence between systems can

be re ected by signi cance test. Let z =(z1, z2, ..., zn ) where zi = xi

- i . Under the the null hypothesis, the paired t test statistic is:

t(z) =

z¯
¯

(1)

n

where z¯ is the mean of z and ¯ is the standard deviation of z. Algorithm 3 shows the algorithm for computing ASL. The smaller the

ALGORITHM 3: Algorithm for computing achieved signi cance level.

Count = 0;

for i = 1 to B do

take the ith sample from the simulation model;

t(zi)

=

z¯i

/

( i

/

 n

);

if |t (zi) | > |t12- /2(n - 1) | then

Count = Count + 1;

end

end

ASL = Count/B;

ASL is, the stronger the evidence for that null hypothesis is false is, which means there is signi cant di erence between two systems with a larger probability.
3.2.4 Estimated Di erence. The summary metric values' di erence does not guarantee that there is signi cant di erence between two systems. In order to derive the con dent di erence for guaranteeing the signi cant di erence, the ED was utilized in [7]. The modi ed algorithm is shown in Algorithm 4.
4 EXPERIMENT
4.1 Experimental Set-Up
The data used for experiment are Robust track 2004 and Adhoc track 1999. There are 249 topics and 110 runs for Robust track 2004 and 50 topics and 129 runs for Adhoc track 1999. We use Average Precision (AP) as the per-topic metric. The summary metric is Mean Average Precision (MAP).
4.2 Results
4.2.1 Variability of Estimated Metric Values. The simulated metric values for each system-topic pair on di erent samples are di erent. The per-topic error bar is plotted against the raw metric value

Simulated AP Simulated AP

ALGORITHM 4: Algorithm for estimating the performance di erence at a given signi cance level  .
DIFF = Ø;
for each system pair (X,Y) do sample B samples from the simulation models; compute t(zi ) for i = 1 to B; sort |t (zi ) | for i = 1 to B; if |t (zi ) | is the B largest value then DIFF = DIFF  zi ;
end
end ED = max(di  DIFF)

1

1

0.9

0.9

0.8

0.8

0.7

0.7

0.6

0.6

0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0

0

0

0.2

0.4

0.6

0.8

1

0

0.2

0.4

0.6

0.8

1

Raw AP

Raw AP

(a) BST

(b) KDE

Figure 1: The error bar of BST and KDE for a speci c run of Robust track 2004.

for a speci c run of Robust track 2004 in Figure 1. The standard deviations of per-topic metric values for two models are similar. The per-topic variance is small because of the similarity among samples. The simulated value is close to the raw value when the raw value is small and vice versa. The same pattern exists for other runs of two data sets. Thus, these two models basically simulate the raw values well and have a wider range of adaptability. In comparison with the KDE model, the per-topic metric value of BST simulation is closer to the observed value, which indicates that the BST model simulates the raw data better than KDE model.
4.2.2 Error Rate. We set B = 10000 in Algorithm 1 and the number of bins is 21. In the rst bin, 0 < di < 0.01; In the second bin, 0.01  di < 0.02;..., and di  0.2 in the last bin. The results of ER for each model on two data sets are shown in Figure 2. For Robust track 2004, the ER of the BST is higher than that of KDE in the rst seven bins while the ER of these two models are close to 0 for the last bins. The ER is decreasing when the bin is increasing, which indicates that the smaller the di erence between two sample means is, the larger the probability of deriving the wrong conclusion is. Although the ER on Adhoc 1999 is larger than that on Robust track 2004, the trends of these two ER lines are similar. This further indicates that the smaller the di erence between two simulated summary metric values is, the larger the probability of deriving the wrong conclusion will be.
4.2.3 Kendall Rank Correlation Coe icient. We set B = 1000 in Algorithm 2. The KRCC between the systems' simulated ranking

1183

Error Rate Error Rate

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Meng Yang, Peng Zhang, and Dawei Song

0.3 0.25
0.2 0.15
0.1 0.05
0 0

5

10

15

Bins

(a) Robust 2004

KDE BST

20

25

0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
0 0

KDE BST

5

10

15

20

25

Bins

(b) Adhoc 1999

Figure 2: Results of error rate on Robust 2004 and Adhoc 1999.

Table 1: The KRCC, Per and ED for BST and KDE on Robust 2004 and Adhoc 1999.

KRCC PCT(ACL < 0.05) ED

Robust 2004 BST KDE 0.9176 0.8088 0.1041 0.0896 0.3955 0.3273

Adhoc 1999 BST KDE 0.8509 0.8121 0.1783 0.1756 0.4135 0.4111

overall distribution. That is the reason why that PCT(ACL < 0.05) is not close to the raw result and changes obviously on the e ect of per-topic variance.
5 CONCLUSION
Motivated by the recent development of the per-topic variance by considering the document collection is a sample from a population, we propose to study the e ect of the per-topic variance on comparing systems. Four comparison methods are employed. Two ranking-based methods are Error Rate (ER) and Kendall Rank Correlation Coe cient (KRCC), and two signi cance test based methods are Achieved Signi cance Level (ASL) and Estimated Di erence (ED). We discover that the ranking of systems changes a little with the variability of per-topic metric value and the results concerning the signi cance test are obviously a ected by the per-topic variance. In the future, we will study more simulation models and comparison methods and explore the theory behind these discoveries.
6 ACKNOWLEDGMENTS
This work is supported in part by the state key development program of China (grant No. 2017YFE0111900), Natural Science Foundation of China (grant No. U1636203, 61772363), and the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 721321.

and the raw ranking is listed in the rst row of Table 1. We can see that the coe cient value of BST is larger than that of KDE. All the values are larger than 0.8, which represents that the simulated ranking of systems is similar to the raw ranking.
4.2.4 Achieved Significance Level and Estimated Di erence. Algorithm 3 re ects that whether there is signi cant di erence between two systems. The smaller ASL indicates that the larger probability of null hypothesis false. We assume that if ASL < 0.05 there is signi cant di erence between two systems. For Robust track 2004, there are 110*109*0.5 system comparisons and each system pair comparison is repeated B = 10000 times. The percent of ACL < 0.05 (denoted as PCT(ACL < 0.05)) of system comparisons displays in the second row of Table 1. The power of discriminating di erent systems of two models from high to low is BST and KDE. The gures signify about 10 percent system is signi cantly di erent. The percent value on raw systems' comparison is 0.8148, which is much larger than the counterpart of simulation models. The third row shows the ED for guaranteeing the signi cant di erence between two systems, which is pretty large. Similar results are observed for Adhoc 1999.
4.2.5 Summary. According to the low ER and high KRCC, the variability of ranking of systems is small if the per-topic value
uctuates within a certain range or the per-topic variance is small. The reason is that the ranking is based on the summary metric MAP. When some per-topic metric values become larger and some become smaller, the overall change of MAP is not obvious. The signi cance test makes a hypothesis about the overall distribution in advance and then judges whether the sample is from the overall distribution. The variability of per-topic metric values for all topics causes that the sample distribution may not be consistent with the

REFERENCES
[1] Gordon V Cormack and Thomas R Lynam. 2006. Statistical precision of information retrieval evaluation. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 533­540.
[2] Jaana KekÃlÃinen. 2005. Binary and graded relevance in IR evaluationsâComparison of the e ects on ranking of IR systems. Information Processing & Management 41, 5 (2005), 1019­1033.
[3] Stephen Robertson. 2007. On document populations and measures of IR e ectiveness. In Proceedings of ICTIR 2007. 9­22.
[4] Stephen Robertson. 2007. On score distributions and relevance. Advances in Information Retrieval (2007), 40­51.
[5] Stephen Robertson, Evangelos Kanoulas, and Emine Yilmaz. 2013. Modelling score distributions without actual scores. In Proceedings of the 2013 Conference on the Theory of Information Retrieval. ACM, 20.
[6] Stephen E Robertson and Evangelos Kanoulas. 2012. On per-topic variance in IR evaluation. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. ACM, 891­900.
[7] Tetsuya Sakai. 2006. Evaluating evaluation metrics based on the bootstrap. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 525­532.
[8] Tetsuya Sakai. 2007. On the reliability of information retrieval metrics based on graded relevance. Information Processing & Management 43, 2 (2007), 531­548.
[9] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In International ACM SIGIR Conference on Research and Development in Information Retrieval. 1045­1048.
[10] Ellen M Voorhees and Chris Buckley. 2002. The e ect of topic set size on retrieval experiment error. In Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 316­323.
[11] William Webber, Alistair Mo at, and Justin Zobel. 2008. Score standardization for inter-collection comparison of retrieval systems. (2008), 51­58.
[12] Peng Zhang, Linxue Hao, Dawei Song, Jun Wang, Yuexian Hou, and Bin Hu. 2014. Generalized Bias-Variance Evaluation of TREC Participated Systems. In Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management (CIKM '14). 1911­1914.
[13] Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. 2013. Bias-variance Decomposition of Ir Evaluation. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '13). 1021­1024.
[14] Peng Zhang, Dawei Song, Jun Wang, and Yuexian Hou. 2014. Bias-variance Analysis in Estimating True Query Model for Information Retrieval. Inf. Process. Manage. 50, 1 (Jan. 2014), 199­217.

1184

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Modeling Multidimensional User Relevance in IR using Vector Spaces

Sagar Uprety
The Open University Milton Keynes, United Kingdom
sagar.uprety@open.ac.uk
Dawei Song
The Open University Milton Keynes, United Kingdom
dawei.song@open.ac.uk
ABSTRACT
It has been shown that relevance judgment of documents is influenced by multiple factors beyond topicality. Some multidimensional user relevance models (MURM) proposed in literature have investigated the impact of different dimensions of relevance on user judgment. Our hypothesis is that a user might give more importance to certain relevance dimensions in a session which might change dynamically as the session progresses. This motivates the need to capture the weights of different relevance dimensions using feedback and build a model to rank documents for subsequent queries according to these weights. We propose a geometric model inspired by the mathematical framework of Quantum theory to capture the user's importance given to each dimension of relevance and test our hypothesis on data from a web search engine and TREC Session track.
CCS CONCEPTS
· Information systems  Personalization;
KEYWORDS
Information Retrieval, User modeling, Multidimensional Relevance
ACM Reference Format: Sagar Uprety, Yi Su, Dawei Song, and Jingfei Li. 2018. Modeling Multidimensional User Relevance in IR using Vector Spaces. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210130
Also with Beijing Institute of Technology. Correspondence Author. Also with Tianjin University. Correspondence author.
ACM acknowledges that this contribution was authored or co-authored by an employee, contractor or affiliate of a national government. As such, the Government retains a nonexclusive, royalty-free right to publish or reproduce this article, or to allow others to do so, for Government purposes only. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210130

Yi Su
School of Computer Science and Technology, Tianjin University
Tianjin, China suyi2016@tju.edu.cn
Jingfei Li
National Computer Network Emergency Response Technical Team/Coordination Center of China Beijing, China jingfl@foxmail.com
1 INTRODUCTION
There is a growing body of work investigating different factors which affect user's judgment of relevance [1, 2, 6, 7, 9, 11­13]. A multidimensional user relevance model (MURM) was proposed [12, 13] which defined five dimensions of relevance namely "Novelty", "Reliability", "Scope", "Topicality" and "Understandability". In a recent paper [7] an extended version of the MURM comprising two additional dimensions "Habit" and "Interest" is proposed. The "Interest" dimension refers to the topical preferences of users in the past, while "Habit" refers to their behavioral preferences. For example, accessing specific websites for some particular information or task is considered under the "Habit" dimension. Experiments on real-world data show that certain dimensions, such as "Reliability" and "Interest", are more important for the user than "Topicality", in judging a document.
Our hypothesis is that in a particular search session or search task, there is a particular relevance dimension or a combination of relevance dimensions which the user has in mind before judging documents. For example, if the user wants to get a visa to a country, he or she would prefer documents which are more reliable ("Reliability") for this task, but when looking to book flights to that country, the user might go to his or her preferred websites ("Habit"). Therefore, for next few queries of the session, "Habit" dimension becomes more important. Thus, the importance given to relevance dimensions might change as the session progresses or tasks switch. By capturing the importance assigned to each dimension for a query, we can model the dimensional importance and use it to improve the ranking for the subsequent queries. The relevance dimensions are modeled using the Hilbert space formalism of Quantum theory which unifies the logical, probabilistic and vector space based approaches to IR [8]. We place the user's cognitive state with respect to a document at the center of the IR process. Such a state is modeled as an abstract vector with multiple representations existing at the same time in different basis corresponding to different relevance dimensions. This cognitive state comes into reality only when it is measured in the context of user interactions.

993

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA S. Uprety et al.

(a) Figure 1.a

(b) Figure 1.b

(c) Figure 1.c

2 GEOMETRIC REPRESENTATION OF MULTIDIMENSIONAL RELEVANCE
Consider a real-valued two dimensional Hilbert Space. Relevance with respect to a dimension (e.g. Reliability) is a vector in this Hilbert space. Non-relevance with respect to the same dimension is an orthogonal vector to it. Further, we denote vectors as kets following the Dirac's notation. For example, the vectors for relevance and non-relevance with respect to novelty are denoted as |novelty and novelty . Figure 1.a shows the construction of a two-dimensional Hilbert Space for a relevance dimension.
Next, we model the user's perception of a document with respect to a dimension of relevance also as a vector in this Hilbert space. This vector is a superposition of relevance and non-relevance vectors with respect to a dimension, e.g., |d =  |novelty +  novelty . The coefficient | |2 is the weight (i.e., probability of relevance) the user assigns to document d in term of novelty, and | |2 + | |2 = 1. We will talk about how to calculate these coefficients in the next section. Figure 1.b shows the modeling of user's cognitive state for document d with respect to the Novelty dimension.
Depending on a user's preference of relevance dimensions for a particular query, the user will judge the same document differently. A document might be of interest to the user but may not be novel when the user is looking for latest documents about the query. This phenomena can be modeled in the same Hilbert space by having different basis for different dimensions of relevance. The same document d can be written in terms of another set of basis vectors corresponding to another dimension of relevance. For example:

|d1 = 11 |novelty + 11 novelty

= 12 |habit + 12 habit

= 13 |topic + 13 topic

(1)

and so on in all seven basis. Figure 1.c shows the construction of such a Hilbert space showing two basis for simplicity.
We have represented user's cognitive state with respect to a single document in different basis corresponding to different dimensions of relevance. Similarly, we can do that for all the documents retrieved for a query. Each document will be represented in a separate Hilbert space.
The user's cognitive state for a document d is an abstract vector, because the vector has different representations in different basis.

It does not have a definite state, and a particular representation comes into picture only when we talk of a particular relevance dimension. This is similar to the concept of a state vector in Quantum theory which contains all the information about a quantum system, yet is an abstract entity and has different representations of the same system. We get to see a particular representation of a system depending on how we measure it. A document may look highly relevant in one basis, if it has a high weight in that basis and the user makes judgment from the perspective of that relevance dimension. However, the relevance can change if the user considers a different basis (a different perspective of looking at the document).

3 CAPTURING USER'S CHANGING WEIGHTS TO RELEVANCE DIMENSIONS

Having established the geometric representation of documents, we can make use of it to capture the weights of relevance dimensions for a user in response to a query (Algorithm 1).
In Algorithm 1, the input parameters docsALL and docsSAT correspond to the list of all retrieved documents and SAT-clicked [7] documents for a query respectively. We quantitatively define each of the seven relevance dimensions using some features. For each query-document pair, these features are extracted and computed (Step 3). They are integrated into the LambdaMART [4] Learning to Rank(LTR) algorithm to generate seven relevance scores (one for each dimension) for the query-document pair (Step 4). Please refer to [7] for more details about the features defined for each dimension and the hyper-parameters of the LTR algorithm. Thus, for a query and its set of retrieved documents, we have seven different rankings, one for each relevance dimension. The seven scores assigned to a document for a query are then normalized using the min-max normalization technique across all the documents for the query (Step 5). The normalized score for each dimension forms the coefficient of superposition of the relevance vector for the respective dimension. It quantitatively represents user's perception of the document for that dimension. For example, for a query q, let d1, d2, ..., dn be the ranking order corresponding to the "Interest" dimension. Let relevance scores be 1, 2, ..., n respectively. We construct the vector for document d1 in the 'Interest' basis as:

|d1 = 11 |interest + 11 interest

(2)

where 11 =

m

1 -m i n ( ) ax ()-min(

)

,

where

max ()

is

the

maximum

value among 1, 2, ..., n . Square root is taken in accordance with

994

Short Research Papers I Modeling Multidimensional User Relevance in IR using Vector Spaces

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

the quantum probability framework. Similarly, the second document is represented in its Hilbert space as:

|d2 = 21 |interest + 21 interest

(3)

with 21 =

2 -m i n ( ) max ()-min()

.

As

done

with

the

interest

dimension,

we can represent each document in a Hilbert space in all the differ-

ent basis corresponding to different dimensions of relevance (Steps

6 - 8).

Now in the original list of documents, suppose the user SAT-
clicks on document dx . We have already formed the Hilbert space for each document. The coefficients of superposition for |dx  vector in a basis represent the importance of document dx with respect to
the relevance dimension represented by that basis. We re-capture
it by taking the projection of |dx  onto the relevance vector of
each basis by taking square of their inner products (Step 13), for ex-
ample, | novelty|dx  |2, | reliability|dx  |2, etc. Here the notation A|B is the inner product of the vectors A and B. It denotes the

probability amplitude, which is in general a complex number. The

square of probability amplitude is the probability that document B is relevant with respect to dimension A. For real valued vectors, inner product is same as the dot product. Let x1, ..., x7 be the projections obtained. Note that the values of these projections are

the same normalized scores which we calculated above. If there are

more than one SAT clicked documents for a query, we average over

the projection scores for each dimension (Step 15).

Thus, for a given query in a search session, we have quantita-

tively captured the user's cognitive state. This cognitive state or the

user preference for each dimension is the average relevance score

for that dimension over all SAT-clicked documents of the query.

These weights are used to re-rank documents for the next query in

the session, as explained in the next section.

Algorithm 1 Capturing weights given to relevance dimensions

1: procedure captureWeights(rel, docsALL, docsSAT )

2: for all r in rel do

 rel - list of 7 dimensions

3:

f eatures[r ]  etFeatures(docsALL, r )  Extract

features from all retrieved docs for a given query

4:

scores[r ][d]  reRank(docsALL, f eatures[r ])



re-rank based on each dim and get score

5:

normScores[r ][d]  normalizeScores(scores[r ][d])

6:

for all d in docsALL do

7:

[d][r ]  normScores[d][r ]  construct vectors

8:

[d][r ]  1 - | [d][r ]|2

9: for all r in rel do

10:

totalW eiht  0

11:

avW eiht[r ]  0

12:

for all d in docsSAT do

13:

wdr  | r |d |2  Take projections(| [d][r ]|2)

14:

totalW eiht  totalW eiht + wdr

15:

avW eiht[r ]  totalW eiht/|docsSAT |  Only SAT

clicks considered 16: return avW eiht  User's importance to each dimension

4 EXPERIMENT AND ANALYSIS
We use the same datasets as used in [7]. The first one is the query logs of the Bing search engine and the second one is the combined session tracks of TREC 2013 and TREC 2014. While the Bing query logs contain information about each query in a session, the TREC dataset only contains the data about the last query for each session. The relevance criteria for Bing logs is SAT clicks and for TREC data we consider relevance grades of 1 and above to correspond to relevant documents. In Section 3, we captured the user's dimensional preference for a query in the form of weights. We now use these weights for the next query in the session, to take a weighted combination of the relevance scores of all seven dimensions for each document of the next query. Thus, for the new query, a new relevance score for each document is created based on the weighted dimensional preference for the previous query. We re-rank the documents according to these new scores and perform evaluation. We use the NDCG metric for evaluation and compare the values with those obtained in [7].
We also performed an initial analysis of the data to support our hypothesis that some combination of relevance dimensions are preferred by the user in a search session. For some randomly sampled 4837 sessions of the Bing query logs, we found that in 3910 or 80.84 percent of the sessions, one of the top three dimensions for the first query of the session remains in the top three for all the queries of the session. Figure 2.a is the snapshot of one such session showing that the "Reliability" remains the top dimension throughout. Figure 2.b shows 20 consecutive sessions for TREC data.
5 RESULTS AND DISCUSSION
We summarize the evaluation results for Bing query logs in Table 1. In the paper [7], re-ranking by using the features of "Reliability" dimension gives the best performance for Bing data. However we show that the results obtained by using the weighted combination gives slightly better results (Table 1). The improvement is not significant but the fact is that weighted combination is a general method which will work for other datasets too. Similar to the results reported in [7], for TREC data, it is not "Reliability" but "Interest" which comes out as the best dimension for re-ranking (Table 2). Therefore one cannot use a fixed relevance dimension for ranking the documents. Table 2 shows that our weighted combination approach also gives improved performance for the TREC data. It is to be noted that TREC data contains information about the last query of each session, and not all the queries. Thus our weighted approach uses the captured weights of the last query of a session to re-rank the documents for the last query of the next session. Improvement over the best result (corresponding to Interest) means that the weighted combination method for ranking works across sessions as well. This indicates that dimensional preference is not only dependent upon the task, but user might have an intrinsic preference for some dimensions as well. Also note that the "Topicality" scores correspond to a traditional baseline model as we use tf-idf and BM25 as features for the "Topicality" dimension.

995

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA S. Uprety et al.

(a) Figure 2.a

(b) Figure 2.b

Dimension

NDCG@1 NDCG@5 NDCG@10 NDCG@ALL

Habit Interest Novelty Reliability Scope Topicality Understandability Weighted Combination

0.3772 0.4574 0.4110 0.6457 0.2501 0.2001 0.2782 0.6552

0.5958 0.6178 0.6025 0.7687 0.4692 0.4486 0.4968 0.7814

0.6533 0.6844 0.6688 0.8038 0.56156 0.5352 0.5867 0.8127

0.6645 0.6955 0.6783 0.8110 0.5726 0.5482 0.5971 0.8189

Table 1: Bing logs evaluation

Dimension

NDCG@1 NDCG@5 NDCG@10 NDCG@ALL

Habit Interest Novelty Reliability
Scope Topicality Understandability Weighted Combination

0.0989 0.1981 0.0966 0.1120 0.1318 0.1459 0.1653 0.2364

0.1406 0.2126 0.1180 0.1333 0.1526 0.1520 0.1913 0.2663

0.1418 0.2242 0.1316 0.1431 0.1647 0.1887 0.1878 0.2729

0.1592 0.1831 0.1557 0.1614 0.1671 0.1701 0.1764 0.1944

Table 2: TREC data evaluation

6 CONCLUSION AND FUTURE WORK
We have thus shown that capturing user's weights for relevance dimensions and ranking based on the combination of these weights leads to a better performance than using only one of the dimensions. The need for a Hilbert space framework is inspired by the fact that some relevance dimensions are incompatible for some documents. A document may not have high relevance weights for both "Novelty" and "Habit" dimensions at the same time. The more relevant it is in the "Novelty" dimension, the less relevant it will be in the "Habit" dimension. This is similar to the Uncertainty Principle in Quantum Theory. We therefore model each relevance dimension as a different basis. For some documents, the basis might coincide, but in general there is incompatibility between relevance dimensions which leads to interference and order effects [5, 10]. For example, a user may find a document less reliable due to its source, but when the user considers the "Topicality" dimension and reads it, it might remove the doubts about the reliability. Thus "Topicality" interferes with "Reliability" in relevance judgment. Such order effects were investigated in [3] through user studies. We intend to investigate

such cognitive phenomena in real world data, and the Hilbert space
representation described in this paper forms a solid basis to carry
out such experiments in the future.
ACKNOWLEDGMENTS
This work is funded by the European Union's Horizon 2020 research
and innovation programme under the Marie Sklodowska-Curie
grant agreement No 721321.
REFERENCES
[1] Carol L. Barry. 1998. Document representations and clues to document relevance. Journal of the American Society for Information Science 49, 14 (1998), 1293­ 1303. https://doi.org/10.1002/(SICI)1097-4571(1998)49:14<1293::AID-ASI7>3.0. CO;2- E
[2] Ulises Cerviño Beresi, Yunhyong Kim, Dawei Song, and Ian Ruthven. 2011. Why did you pick that? Visualising relevance criteria in exploratory search. International Journal on Digital Libraries 11, 2 (27 Sep 2011), 59. https://doi.org/10.1007/ s00799- 011- 0067- 7
[3] Peter Bruza and Vivien Chang. 2014. Perceptions of document relevance. Frontiers in Psychology 5 (2014), 612. https://doi.org/10.3389/fpsyg.2014.00612
[4] Christopher J. C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview.
[5] Jerome R. Busemeyer and Peter D. Bruza. 2012. Quantum Models of Cognition and Decision (1st ed.). Cambridge University Press, New York, NY, USA.
[6] Célia da Costa Pereira, Mauro Dragoni, and Gabriella Pasi. 2009. Multidimensional Relevance: A New Aggregation Criterion. In Advances in Information Retrieval, Mohand Boughanem, Catherine Berrut, Josiane Mothe, and Chantal Soule-Dupuy (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 264­275.
[7] Jingfei Li, Peng Zhang, Dawei Song, and Yue Wu. 2017. Understanding an enriched multidimensional user relevance model by analyzing query logs. Journal of the Association for Information Science and Technology 68, 12 (2017), 2743­2754. https://doi.org/10.1002/asi.23868
[8] C. J. van Rijsbergen. 2004. The Geometry of Information Retrieval. Cambridge University Press, New York, NY, USA.
[9] Anastasios Tombros, Ian Ruthven, and Joemon M. Jose. 2005. How users assess Web pages for information seeking. Journal of the American Society for Information Science and Technology 56, 4 (2005), 327­344. https://doi.org/10.1002/asi.20106
[10] Benyou Wang, Peng Zhang, Jingfei Li, Dawei Song, Yuexian Hou, and Zhenguo Shang. 2016. Exploration of Quantum Interference in Document Relevance Judgement Discrepancy. Entropy 18, 12 (Apr 2016), 144. https://doi.org/10.3390/ e18040144
[11] Yunjie Xu and Hainan Yin. 2008. Novelty and topicality in interactive information retrieval. Journal of the American Society for Information Science and Technology 59, 2 (2008), 201­215. https://doi.org/10.1002/asi.20709
[12] Yunjie (Calvin) Xu and Zhiwei Chen. 2006. Relevance judgment: What do information users consider beyond topicality? Journal of the American Society for Information Science and Technology 57, 7 (2006), 961­973. https: //doi.org/10.1002/asi.20361
[13] Yinglong Zhang, Jin Zhang, Matthew Lease, and Jacek Gwizdka. 2014. Multidimensional Relevance Modeling via Psychometrics and Crowdsourcing. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 435­444. https://doi.org/10.1145/2600428.2609577

996

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Characterizing Question Facets for Complex Answer Retrieval

Sean MacAvaney
IRLab, Georgetown University sean@ir.cs.georgetown.edu

Andrew Yates
Max Planck Institute for Informatics ayates@mpi-inf.mpg.de

Arman Cohan, Luca Soldaini
IRLab, Georgetown University {arman,luca}@ir.cs.georgetown.edu

Kai Hui
SAP SE kai.hui@sap.com
ABSTRACT
Complex answer retrieval (CAR) is the process of retrieving answers to questions that have multifaceted or nuanced answers. In this work, we present two novel approaches for CAR based on the observation that question facets can vary in utility: from structural (facets that can apply to many similar topics, such as `History') to topical (facets that are specific to the question's topic, such as the `Westward expansion' of the United States). We first explore a way to incorporate facet utility into ranking models during query term score combination. We then explore a general approach to reform the structure of ranking models to aid in learning of facet utility in the query-document term matching phase. When we use our techniques with a leading neural ranker on the TREC CAR dataset, our methods yield statistically significant improvements over both an unmodified neural architecture and submitted TREC runs.
ACM Reference Format: Sean MacAvaney, Andrew Yates, Arman Cohan, Luca Soldaini, Kai Hui, and Nazli Goharian, Ophir Frieder. 2018. Characterizing Question Facets for Complex Answer Retrieval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210135
1 INTRODUCTION
As people become more comfortable using question answering systems, it is inevitable that they will begin to expect the systems to answer questions with complex answers. For instance, even the seemingly simple question "Is cheese healthy?" cannot be answered with a simple `yes' or `no'. To fully answer the question, positive and negative qualities should be discussed, along with the strength of evidence, and conditions under which the qualities apply--a complex answer. Complex Answer Retrieval (CAR) frames this problem as an information retrieval (IR) task [2]. Given a query that consists of a topic (e.g., `cheese'), and facets of the topic (e.g., `health effects'), a CAR system should be able to retrieve information from a variety of sources to throughly answer the corresponding question.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210135

Nazli Goharian, Ophir Frieder
IRLab, Georgetown University {nazli,ophir}@ir.cs.georgetown.edu
CAR has similarities with existing, yet distinct, areas of research in IR. Although CAR involves passage retrieval, it is distinguishable from passage retrieval because CAR compiles multiple passages together to form complete answers. It is also different than factoid question answering (questions with a simple answer, e.g. "Who wrote Hamlet?"), and complex question answering (questions that themselves require reasoning, e.g. "Which female characters are in the same room as Homer in Act III Scene I?").
We observe that question facets can be structural or topical. Structural facets refer to general categories of information that could apply to other entities of the same type, such as the `History' or `Economy' of a country. Topical facets refer to categories of information that are specific to the entity mentioned in the question, such as the `Westward expansion' or `Banking crisis' of the United States. (Although either facet could be asked about other topics, they are much more specific to details of the topic than structural headings.) We call this distinction facet utility, and explain it in detail in Section 2, along with additional background and related work. We then present and evaluate two novel approaches to CAR based on this observation and the hypothesis that it will affect how terms are matched. The first approach integrates predictors of a facet's utility into the score combination component of an answer ranker. The second approach is a technique to help any model learn to make the distinction itself by treating different facets independently. To predict facet utility, we use the heading structure of CAR queries (described in Section 2) and corpus statistics. We show how our approaches can be integrated with recent neural ranking models, and evaluate on the TREC CAR dataset. Our approaches yield favorable results compared to other known methods, and achieve significant gains over an unmodified neural architecture and other TREC runs.
2 BACKGROUND AND RELATED WORK
The first major work done with CAR frames the task in terms of Wikipedia content generation [1]. CAR fits naturally with this domain because CAR query topics and facets often correspond well with article titles and headings, respectively. Furthermore, Wikipedia itself provides an extensive source of sample queries (paths in the heading hierarchy from the title), partial answers (i.e., paragraphs), and automatic relevance judgments (paragraphs can be assumed relevant to the headings they are under). For simplicity, we use Wikipedia-focused terminology in the remainder of this work. A heading refers to any component of a query, and corresponds to a question topic or facet. The title is the first query component (topic), the main heading is the last component, and intermediate

1205

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Example CAR queries split by heading position.

# Title

Intermediate Heading(s) Main Heading

1 Cheese

» (none)

2 Green sea turtle

» Ecology and behavior

3 History of the United States » 20th Century

4 Disturbance (ecology)

» (none)

5 Medical tourism

» Destinations » Europe

» Nutrition and health » Life cycle » Imperialism » Cyclic disturbance » Finland

Table 2: Contextual vectors for query 2 from Table 1.

green sea turtle ecology and behavior life cycle

position_title position_inter position_main

111 000 000

0

0

1

1

0

0

0

00

1

00

0

11

heading_frequency 0 0 0

3

3

3

33

heading are any headings between the two (if any). The main and intermediate headings represent the facet of interest to the topic. Example queries using this terminology are given in Table 1.
A central challenge of CAR is resolving facet utility. Due to the structure of CAR queries as a list of headings, we generalize the concept to heading utility--the idea that headings (i.e., question topics and facets) can serve a variety of functions in an article. We distinguish between structural and topical headings. We define structural headings as headings that serve a structural purpose for an article--general question facets that could be asked about many similar topics. In contrast, topical headings describe details that are specific to the particular topic. For instance, "cooking and eating" is a structural heading for Cheese (one would expect it to be found in other food-related articles), whereas "cheeseboard" is a topical heading because it relates specifically to the topic of the article. Because the terminology in structural headings is necessarily more generic (they accommodate many topics), we predict that terms found in these headings are less likely to appear verbatim in relevant paragraphs than terms in topical headings. Thus, modeling this behavior should improve performance on CAR because it will be able to learn which terms are less important. Previous work does not model facet utility, treating all headings equally by concatenating their components.
Nanni et al. [8] presents a survey of prominent general domain ranking and query expansion approaches for CAR. They test one deep neural model (Duet [7]), and find that it outperforms the other approaches, including BM25, cosine similarity with TF-IDF and word embeddings, and a learning-to-rank approach. The recent 2017 TREC track focused on CAR [2]. This track yielded both manual relevance judgments for evaluation of CAR systems, and a variety of new CAR approaches (seven teams participated). One prominent approach used a sequential dependence model [5]. They modified the approach for CAR by limiting ordered ngrams to those found within a single heading, and unordered ngrams to only inter-heading pairs. Another approach uses a Siamese attention network [6], including topic features extracted from DBPedia. While this approach does distinguish the title from other headings, it only uses it for query expansion and related entity extraction. Another submission applied a reinforcement learning-based query reformulation approach to CAR [9].
3 METHOD
Since previous work shows that neural-based rankers have potential for CAR, we focus on an approach that can be adapted for various neural rankers. Many leading interaction-focused neural rankers share a similar two-phase architecture, as shown in Figure 1a. Phase 1 performs matching of query terms to document terms, and phase 2 combines the matching results to produce a final relevance score. For instance, DRMM [3] uses a feed-forward histogram matching

network, and a term gating combination network to predict rele-

vance. MatchPyramid [10] uses hierarchal convolution for match-

ing, followed by a dense layer for aggregation. Similarly, PACRR [4]

uses a max-pooled convolution phase for matching, and a recur-

rent or dense combination phase. Finally, DeepRank [11] generates

query contexts and uses a convolutional layer to generate local rel-

evance representations as a matching phase, and uses a term gating

mechanism for combination. We present two approaches to model

facet utility by modifying this generalized neural ranking structure.

The first approach applies contextual vectors in the combination

phase (Figure 1b), and the second approach splits the input into

independent matching phases (Figure 1c).

Contextual vectors. In the combination phase, signals across

query terms are combined to produce a relevance score, so it is

natural to include information here to provide additional context

about each query term when combining the results. For instance,

PACRR includes the inverse document frequency (IDF) in its com-

bination layer, allowing the model to learn how to weight results

based on this statistic [4]. We use this phase to inform the model

about heading utility based on predictions about the distinction

between structural and topical headings. We call these contextual

vectors, since they provide context in the CAR domain. The intu-

ition is that by providing the model with estimators of heading

utility, the model will learn which terms to weight higher. Here we

explore two types of contextual vectors: heading position (HP) and

heading frequency (HF).

When distinguishing between structural and topical headings,

it is important to consider the position itself in the query. For

instance, since the title is the question topic, it is necessarily topical.

Furthermore, it is reasonable to suspect that intermediate headings

will often be structural because they assist in the organization of an

article. Main headings may either be structural or topical, depending

on the question itself. Thus, for heading position contextual vectors,

we use a simple indicator to distinguish whether a term is from the

title, an intermediate, or the main heading. An example is given in

Figure 2.

Another approach to modeling structural and topical headings

using contextual vectors is to examine the prevalence of a given

heading. This is based on the intuition that structural headings

should appear in many similar documents, whereas the usage of top-

ical headings should be less widespread. For instance, the structural

heading "Nutrition and health" in the article Cheese also appears in

articles entitled Beef, Raisin, Miso, and others, whereas the topical

"Cheeseboard" heading only also appears as the title of a disambigua-

tion page. We model this behavior using heading usage frequency:

f rq(h) =

aC I (h |C |

a)

.

That

is,

the

probability

that

a

given

article

a in corpus C contains heading h, given the indicator function I .

Heading usage frequencies very close to 0 include titles and other

content-specific headings like Cheeseboard. Due to the wide variety

1206

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

main inter. title ...
main inter. title ...
main inter. title ...

query
q q q q q q q q q

document
d d d d dn
matching (e.g. CNN)

combination rel. score (e.g. dense)

(a) general neural ranker

query
q q q q q q q q q

document
d d d d dn
matching (e.g. CNN)

contextual vectors

+

combination rel. score (e.g. dense)

(b) contextual vectors

query
q q q
q q q
q q q

document
d d d d dn
matching (e.g. CNN)
matching (e.g. CNN)
matching (e.g. CNN)

combination rel. score (e.g. dense)

(c) heading independence

Figure 1: (a) General interaction-focused ranking architecture, with matching and combination phases (unmodified). (b) Modified architecture, including contextual vectors for combination. (c) Modified architecture, splitting for heading independence.

of Wikipedia articles, most probabilities are very low. Therefore, we stratify the scores by percentile, grouping similarly-common headings together. Based on pilot studies, we found the (1) 60th, (2) 90th, and (3) 99th percentiles to be effective breakpoints. We use complete, case insensitive heading matches. Unknown headings are assumed to be infrequent, and belong to the 0th percentile. An example of this vector is given in Table 2.
Heading independence. Since contextual vectors are applied in the combination phase, they have no effect on the criteria constituting a strong signal from the matching phase. However, we hypothesize that facet utility can also be important when matching. For instance, a structural heading like "History" might have a lower matching threshold, allowing matches of similar words terms such as "early" or "was" (both of which have a lower word2vec cosine similarity score to "history" than functionally-related word pairs, such as "cheese" and "chocolate").
Thus, we propose a method called heading independence. With this approach, we modify the structure of a generic neural IR model by splitting the matching stage into three independent parts: one for the title, one for intermediate headings, and one for the main heading. Each sub-matching phase operates independently as it otherwise would for the combined query. Then, the results are combined using the same combination logic of the original model (e.g., a dense or recurrent layer). This allows the model to learn separated logic for different heading components. The reasoning behind the split by query component is the same as the reasoning behind using heading position vectors: the title is topical, whereas intermediate headings are likely structural, and the main heading could be either. With separate matching logic for each, the model should be able to more easily distinguish between the types.
An added benefit of this approach is that it improves heading alignment in the combination phase. When headings are simply concatenated (even with a symbol to indicate a change in headings), the alignment of each query component will vary among queries. Since the output of each matching stage is fixed in size, the locations of each query component will be consistent among queries. We suspect that this is particularly useful when using dense combination.
4 EXPERIMENTAL SETUP
Dataset. TREC CAR provides several sets of queries based on a recent dump of Wikipedia [1, 2]. Queries in each set are generated from the heading structure of an article, where each query represents a path from the article title down to the main heading. Each query also includes automatic relevance judgments based on the

assumption that paragraphs under a given heading are relevant to the query with that main heading. Half of the dump belongs to the train set, which is split into 5 folds. We use folds 1 and 2 in this work, consisting of 873, 746 queries and 2.2M automatic relevance judgments (more data than this was not required for our models to converge). The test200 set contains 1,860 queries and 4.7k automatic relevance judgments. The benchmarkY1test set contains 2,125 queries and 5.8k automatic relevance judgments. It also includes 30k manual relevance judgments, ranging from Trash (-2) to Must be mentioned (3). The paragraphcorpus is a collection of 30M paragraphs from the Wikipedia dump with no article or heading structure provided, functioning as a source of answers for retrieval.
Model integration. We evaluate our contextual vector and heading independence approaches using the Position-Aware Convolutional Recurrent Relevance neural IR architecture (PACRR) [4], which is a strong neural retrieval model with a structure that naturally lends itself to incorporating contextual vectors and heading independence signals. We refer the reader to Hui et al. [4] for full details about the model, but we give a short description here to provide details about how our approach is integrated. PACRR first processes square convolutional filters over a q × d query-document similarity matrix, where each cell represents similarity scores between the corresponding query and document term. The filters are max-pooled for each cell, and the scores are k-max pooled over each query term (k = 2). Then a dense layer combines the scores (along with term IDF scores) to yield a final relevance score for the query-document pair. For runs that include contextual vectors, we append them to each term (alongside IDF) during combination. For heading independence, we use separate convolution and pooling layers, followed by a dense layer for each heading component. We also explore using the heading frequency contextual vector when using heading independence (included after the pooling layer), and before the independent dense layer.
Training and evaluation. We train the models on samples from train.fold1 and train.fold2. Positive training examples come from the automatic relevance judgments, whereas negative training examples are selected from the top non-relevant BM25 results for the given query. Each model is trained for 80 iterations, and the top training iteration is selected using the R-Prec on test200. Evaluation is conducted with automatic and manual judgments on benchmarkY1test (unjudged treated as non-relevant). The results are based on an initial ranking of the top 100 BM25 results for each query. We report MAP, R-Precision, MRR, and nDCG of each variation (all four official TREC CAR metrics).

1207

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 3: Performance results on benchmarkY1test. The top value is in bold. Records marked with * are based on submitted TREC runs, and had top results included in the manual assessment pool. Significant results compared to the unmodified PACRR model are marked with  and  (paired t-test, 95% confidence). The abbreviations for our methods are as follows: HP is the heading position contextual vector; HF is the heading frequency contextual vector; HI is heading independence.

Automatic

Manual

Approach

MAP R-Prec MRR nDCG MAP R-Prec MRR nDCG

PACRR (no modification) PACRR + HP* PACRR + HP + HF* PACRR + HI PACRR + HI + HF

0.164  0.170  0.170  0.171  0.176

0.131 0.135 0.134 0.139  0.146

0.247  0.258  0.255  0.256  0.263

0.254  0.260  0.259  0.260  0.265

0.208 0.209  0.211 0.205 0.204

0.219 0.218 0.221 0.213 0.214

0.445 0.452 0.453 0.442 0.440

0.403 0.406  0.408 0.403 0.401

Sequential dependence model* [5] Siamese attention network* [6] BM25 baseline*

 0.150  0.121  0.122

 0.116  0.096  0.097

 0.226  0.185  0.183

 0.238  0.175  0.196

 0.172  0.186  0.393  0.137  0.171  0.345  0.138  0.158  0.317

 0.350  0.274  0.296

5 RESULTS
We present system performance in Table 3. Our methods are compared to the unmodified PACRR model, two other top runs to TREC CAR 2017 (sequential dependency model [5] and the Siamese attention network [6]), and a BM25 baseline (which produces the initial result set that our methods re-rank).
Our method outperforms the other TREC runs and the BM25 baseline by all metrics for both manual and automatic relevance judgments (paired t-test, 95% confidence). The method that uses heading independence (HI) and the heading frequency vector (HF) yields up to a 26% improvement over the next best approach (SDM). We note that, as TREC runs, the [5] and [6] baselines are limited by the evaluation environment of TREC.
Our approach also consistently outperforms the unmodified version of PACRR when evaluating using automatic relevance judgments, performing up to 11% better than the unmodified version of PACRR. Our approach occasionally does better than unmodified PACRR when evaluating with manual relevance judgments. Specifically, our approach that uses the heading position (HP) and heading frequency (HF) contextual vectors does the best overall. We acknowledge that this method (and the version with only heading position) were included as TREC runs, yielding an advantage in the manual comparison.
Our methods are based on the distinction between structural and topical headings, and the differences in how they interact with relevant documents. By plotting the term occurrence rate (that is, the probability that any term occurs in a relevant paragraph) for title, intermediate, and main headings, we see clear differences in the distribution (Figure 2). The plot shows that main headings are much more likely to appear in relevant documents than title and intermediate headings. Furthermore, the distributions of intermediate and title headings are roughly opposite each other, with titles (topical) more likely to occur than intermediate headings (structural).
6 CONCLUSION
In this work, we presented an approach to the new and challenging task of complex answer retrieval. Our approach characterizes question facets by modifying a generic neural IR architecture. We

Density

6

5

Main

Intermediate

4

3

2

1

0

0.0

0.2

0.4

0.6

Term occurrence rate

Title

0.8

1.0

Figure 2: Kernel density estimation of heading term occurrence rates, based on automatic judgments in train.fold0.

explored both approaches that focus on matching (heading independence), and score combination (contextual vectors). When evaluating on the TREC CAR dataset, we achieve statistically significant improvements over the other the unmodified neural architecture and other TREC runs. Furthermore, our approach significantly outperforms a leading neural IR model when evaluating with both automatic and manual judgments.

REFERENCES
[1] Laura Dietz and Ben Gamari. 2017. TREC CAR: A Data Set for Complex Answer Retrieval (Version 1.5). (2017). http://trec-car.cs.unh.edu
[2] Laura Dietz, Manisha Verma, Filip Radlinski, and Nick Craswell. 2017. TREC Complex Answer Retrieval Overview. In TREC 2017.
[3] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. 2016. A deep relevance matching model for ad-hoc retrieval. In CIKM 2016.
[4] Kai Hui, Andrew Yates, Klaus Berberich, and Gerard de Melo. 2017. PACRR: A Position-Aware Deep Model for Relevance Matching in Information Retrieval. In EMNLP 2017.
[5] Xinshi Lin and Wai Lam. 2017. CUIS Team for TREC 2017 CAR Track. In TREC 2017.
[6] Ramon Maldonado, Stuart Taylor, and Sanda M. Harabagiu. 2017. UTD HLTRI at TREC 2017: Complex Answer Retrieval Track. In TREC 2017.
[7] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In WWW 2017.
[8] Federico Nanni, Bhaskar Mitra, Matt Magnusson, and Laura Dietz. 2017. Benchmark for Complex Answer Retrieval. In ICTIR 2017.
[9] Rodrigo Nogueira, Kyunghyun Cho, Urjitkumar Patel, and Vincent Chabot. 2017. New York University Submission to TREC-CAR 2017. In TREC 2017.
[10] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, and Xueqi Cheng. 2016. A Study of MatchPyramid Models on Ad-hoc Retrieval. In NeuIR 2016.
[11] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, and Xueqi Cheng. 2017. DeepRank: A New Deep Architecture for Relevance Ranking in Information Retrieval. In CIKM 2017.

1208

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Universal Approximation Functions for Fast Learning to Rank
Replacing Expensive Regression Forests with Simple Feed-Forward Networks

Daniel Cohen, John Foley, Hamed Zamani, James Allan and W. Bruce Croft
Center for Intelligent Information Retrieval University of Massachusetts Amherst
{dcohen,jfoley,zamani,allan,croft}@cs.umass.edu

ABSTRACT
Learning to rank is a key component of modern information retrieval systems. Recently, regression forest models (i.e., random forests, LambdaMART and gradient boosted regression trees) have come to dominate learning to rank systems in practice, as they provide the ability to learn from large scale data while generalizing well to additional test queries. As a result, efficient implementations of these models is a concern in production systems, as evidenced by past work.
We propose an alternate method for optimizing the execution of learned models: converting these expensive ensembles to a feedforward neural network. This simple neural architecture is quite efficient to execute: we show that the resulting chain of matrix multiplies is quite efficient while maintaining the effectiveness of the original, more-expensive forest model. Our neural approach has the advantage of being easier to train than any direct neural models, since it can match the previously-learned regression rather than learn to generalize relevance judgments directly.
We observe CPU document scoring speed improvements of up to 400x over traditional algorithms and up to 10x over state-of-theart algorithms with no measurable loss in mean average precision. With a GPU available, our algorithm is able to score every document in a batch in parallel for another 10-100x improvement. While we are not the first work to observe that neural networks are efficient as well as being effective, our application of this observation to learning to rank is novel and will have large real-world impact.
ACM Reference Format: Daniel Cohen, John Foley, Hamed Zamani, James Allan and W. Bruce Croft. 2018. Universal Approximation Functions for Fast Learning to Rank. In Proceedings of The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, Ann Arbor, MI, USA, July 8­12, 2018 (SIGIR '18), 4 pages. https://doi.org/10.1145/3209978.3210137
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210137

1 INTRODUCTION
Forest-based regression models are the leading approach to train a learning to rank model [9, 19, 22], especially in industry [3, 5, 26]. These models have numerous benefits: they can be trained directly based on observed gradients of traditional IR metrics, unlike other learning to rank approaches [15], and learning is fairly efficient while achieving state-of-the-art results.
The drawback of forest models for production systems is that the cost of prediction is quite high: the naive algorithm for executing a tree is interpretation. This means that for a forest of size T and depth d, an interpreter will visit O(Td) nodes for every point, and at each node it must compare a feature value and branch on the result. Given that pipelining is the ubiquitous strategy to making modern CPU architectures fast, these branch-heavy models are a worst-case scenario. At every decision point, pipeline is flushed, and actual instruction-level parallelism and therefore throughput will be quite low. This limits both query throughput and query latency of a learning to rank server, using a lot of machines and resources.
Efficiency of learning to rank approaches has drawn a greater amount of interest in recent years [19]. Before that work, Asadi and Lin argued that we should train these models to be more runtimeaware [1]. For many years now, researchers have been pursuing the question "How do we minimize the runtime cost of forest-based learning models?" [1, 4, 8, 13, 14, 19, 21, 25]. In this work, we propose answering this question by a key observation about the nature of these ranking ensembles.
Our key observation is that any ranking model will produce scores, given a set of document features as input. Once such a model is trained and validated, our goal for ranking is to output the predictions of that model as fast as possible. Therefore, if we had available to us a black box that could produce the same scores but faster, then we would be effectively executing our learned model. Hornik identifies feed-forward neural networks as valid universal function approximators [11]. In practice, this means that we can take these popular ranking ensembles and fully approximate them.
In this work, we first present analytic arguments for the effectiveness of feed-forward neural networks as full-approximators of regression forests (§3). Next, we provide an empirical demonstration of this technique: showing that we can learn approximations with no loss in mean average precision for LambdaMART ensembles trained on the MSN30k dataset, and for a Random Forest ranker trained on MQ2007 and trained on GOV2 (to demonstrate generalizability under more train/test skew). Simultaneously, we demonstrate the difficulties of directly training the same neural
 These authors contributed equally.

1017

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

model on document judgments which reflects that using a generalized model (LambdaMART) leads to a generalized neural model. Finally, we present a brief sketch of our performance gains and observe a 2-10x improvement (Table 2) over previous published results.
In some sense, the core task we propose is not novel as it was possible and known since the introduction of the XOR-problem in 1969 alongside the perceptron [24], and confirmed for our specific functions later [11, 18]. However, recent works on faster algorithms for learning to rank ensembles (e.g., [19]) suggest that our revisiting of this theoretical work and empirical confirmation is of significant research value and will lead to important discussions in the learning to rank and information retrieval communities. We hope that industrial production systems employing such models can use our techniques to reduce their energy footprint while still satisfying users.

2 RELATED WORK
Our related work section is brief because we address the majority of related literature inline.
The Universal Approximation Theorem (UAT), proven by Hornik, shows that a single layer neural network under certain conditions is capable of fully approximating a continuous function [11]. While neural models did not gain traction in other fields for over a decade, this foundational contribution provides the base of current state-ofthe-art neural architectures. Recently, Kraska et al. propose learning index structures to replace b-tree nodes for static on-disk indexing [17]. they make a case for approximating expensive tree-based functionality with cheaper, feed-forward neural networks, but they do not make the connection to learned trees.
In the realm of IR, Dehghani et al. show that approximating BM25 as a form of weak supervision is an effective way to train a feed-forward model for retrieval [10]. However, their work shows that while relevance is learned, the model only achieves parity with the signal function by including additional information not contained within the domain of BM25.

3 METHODOLOGY
The crux of this work comes from the UAT shown by Hornik [11], where the authors prove that a single layer neural network of sufficient width can approximate any continuous function within a set of conditions. Thus given N such that

N (x) =  (Wx + b)

and

assuming



is

a

Lebesgue

function,

and

b
a

| (x)|pdx

<

,

for

any f  Lp (K) (p  1) where K is a compact set in Rd , then  N

for some  > 0 such that

||N - f ||K,p < 

(1)

This theorem has been applied in [10] where they copy the BM25 function, which satisfies the above conditions on finite collections. However, the same advantage that allows forest-based models to perform well for ranking, dividing the space into splits, also causes the relevance function, s(x), to be piecewise continuous. This violates the UAT assumption, and presents the condition where f fails

to converge pointwise at c. Thus
||N (c-) - s(c+)|| > ||s(c-) - s(c+)|| > 
We leverage work by Llanas et al. [18] that shows a piecewise continuous f can be approximated via a series of single layer neural networks within some . Thus, only each segment must satisfy the constraints proposed in Equation (1), which all current forestbased methods fulfill. Our empirical results demonstrate that  is acceptable in practice.
This leads to N approximating the approximate function of a random forest within some error bound  via a multilayer feedforward neural network. As we can treat the domain of s as finite on a set of finite documents, this results in a compact subset of R .
3.1 Neural Model Architectures
We use a four layer neural network with hidden dimensions [2000, 500, 500, 100] and a two-layer network with hidden dimensions [500,100]. We use ReLU6 as the non-linear activation. While the universal approximation theorem was shown with the sigmoid function, ReLU6 is continuous and bounded, which satisfies the assumptions set forth in [11]. In addition, ReLU6 has been shown to be robust to low precision operations, allowing for greater speedup in production if needed [12].
We use mean squared error as the loss function, with a batch size of 5000. Adam [16] is used to optimize the parameters with a learning rate of 0.001. While Llanas et al. [18] empirically show that a neural model is able to fully approximate s, they do so over the entire domain. We attempt to broaden our learned model from the training data by synthetically generating samples for which to train N . For each batch size of 5000, 2500 samples are created by randomly sampling around the discontinuous points of s in order to fully approximate the L2R function s.
Those familiar with neural networks used for ranking will note that we are using a "pointwise" learning and ranking style, which is often considered to be weaker than pairwise or listwise learning, but since we are learning from a pre-existing function (our existing forest model) rather than relevance, this pointwise model is more than sufficient.
3.2 Generating Random Training Data
In addition to leveraging our learned forest predictions on our training data, we also generate points from the learned model in order to guarantee that our system is matching the shape learned by the forest.
First, we create an empty list of points for each of the D features in our training set. We initialize these lists with the upper and lower bounds of each feature based on the training data. Then, we walk over every branch in our trained trees, and add each of these split points into our list of points for the appropriate features. At this point, we have identified where all of the discontinuities occur in the feature spaces of our piecewise model. We sort these discontinuity points, and replace them with ordered midpoints.
Now, generating a training point x is a simple as selecting random midpoints of interest for each feature, and evaluating the result using the original ensemble.

1018

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

4 EXPERIMENTAL SETUP
We trained our LambdaMART and Random Forest models using the implementation available in RankLib [7]. This toolkit was preferred to others (e.g., XGBoost) because it accepted TREC-formatted relevance judgments and data without modification.
The MSN30k learning to rank dataset1 is a commonly-used benchmark for the efficiency of ranking ensembles. We use this data for ease of comparison to reported numbers in past [19] and future work. However, a few observations we made about this dataset led us to believe that we should create our own secondary dataset rather than use an additional industry-released set of features.
For our second dataset, we explore our own version of the LETOR2 dataset that is built from GOV2 and MQ2007. We do this for a variety of reasons that end up making our evaluation more robust. We naturally provide the extracted features, trained models, and the code used to run our experiments for future work3.
4.1 Our Extracted Features
Although a complete listing of our features and code for extraction is provided in our source release, we give an overview of the features used here. We used web-based quality features [2], and common retrieval models [23] across the title, body, and document fields available to us in the GOV2 collection as parsed by the JSoup Java library. These features are quite similar in spirit to the original features for the LETOR set and similar to the MSN30k features, however we use a wider variety of retrieval models that require more sophisticated combinations.
5 RESULTS

Table 1: Effectiveness of L2R function and N approximation. There is no significant difference in mean average precision scores with p < 0.05. While most works using such datasets focus on early metrics like NDCG@10, we use mean average precision because it is a deeper measure to better show the effectively lossless nature of our approach. Significant differences are marked with an asterisk.

Method

MSN30k GOV2 # Layers MAP MAP

Regression Forest

-

Napprox

4

Napprox

2

Nrelevance

4

0.6004 0.5950 0.5955 0.5639*

0.2995 0.2995 0.3007 0.2531*

5.1 Retrieval Effectiveness
As seen in Table 1, the neural model is able to almost completely approximate LambdaMart on MSN, while achieving parity on the GOV2 evaluation. The small difference in performance can be attributed to the stochastic nature of training deep neural models,
1 https://www.microsoft.com/en- us/research/project/mslr/ 2 https://www.microsoft.com/en- us/research/project/ letor- learning- rank- information- retrieval/ 3 https://github.com/jjfiv/ltr2net

and past work has shown that neural models have numerous local optima that closely approximate the global optimum [6].
Additionally, directly training N on the true relevance labels, referenced as Nrelevance, results in significantly worse effectiveness. This can be attributed to the interpretation that relevance is not a function; for the same query, an identical document may be relevant for one user while non relevant for another. This results in the neural model fitting a new function rather than approximating a true functions, and reflects the success found in [10].
Examining the performance during training, Napprox in fact generalizes better than the forest based model in 93% of epochs prior to convergence. Thus while some accuracy is sacrificed, this results in improved performance on held out samples prior to completely over fitting on the training data and can be viewed as an additional form of regularization on the forest based model if desired.
5.2 Efficiency
While we refrained from re-implementing the Quickscorer algorithm [19], since it is undergoing a patent process4, we present a brief sketch here that demonstrates our claim that feed-forward neural networks are much more efficient than forest based models, even without moving to GPU computation.
We therefore constructed an artificially low-baseline for our approach: Python/Tensorflow-CPU implementation of the same network on laptop hardware. We run these baselines on a Lenovo T430 laptop, with an Intel i5-3230M CPU @ 2.60GHz, and 16GB of RAM. And we compared directly to publication numbers available in the Quickscorer paper [19], which was run on a machine with an i7-4770K clocked at 3.50Ghz, with 32GiB RAM. Although the python-numpy version of the baseline is especially competitive with QuickScorer for very large forests we are able to significantly improve speed in comparison to the heavily engineered QuickScorer for large forests. Our GPU-based implementation is run on via PyTorch on a single NVIDIA TITAN X (Maxwell) GPU.
Our CPU-based implementation achieves a speedup of between 210x on the larger regression forests studied. Larger batches are more efficient, and this is especially true of our GPU implementation, which manages to score documents in under a microsecond on average for either our small or large network case.
Our dramatic improvements have the potential to change the story of production re-ranking, which is likely to be dominated by the branching needed to execute forest ensembles. With GPU scoring, there is essentially no cost to executing expensive models. We note that our GPU timing numbers include the time needed to transfer feature vectors from main memory to GPU memory, which shows the advantage of such a simple model.
We acknowledge that there are newer extensions of QuickScorer that focus on limiting tree count [21], exploting SIMD instructions [20], and potentially other optimizations are always possible. However, these papers present at most 3-5x improvements over the QuickScorer algorithm while being significantly more complex than our approach which provides an order of magnitude improvement for larger ensembles over the blockwise-Quickscorer.
We are confident that our results will carry over to production systems in terms of both latency and throughput. Whether these
4 https://github.com/hpclab/quickscorer

1019

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Efficiency Comparison. All numbers refer to time to score an individual document in µs. Bounds represent the set of previously reported means and 5th and 95th percentiles for our models. Our simple numpy python approaches become competitive with large-tree executions of the QuickScorer algorithm. Production ranking models are likely to gain greatly in efficiency from our approach.

Impl Generated C++ for Forest QuickScorer [19] Blockwise-Quickscorer

Source If-Then-Else
QS BWQS

Documents to re-rank (batch size):

Tensorflow-CPU 4-layer

N

Tensorflow-CPU 2-layer

N

PyTorch GPU 4-layer

N

PyTorch GPU 2-layer

N

1000 Trees

8 Leaves 64 Leaves

8.2-10.3 55.9-55.1

2.2-4.3

9.5-15.1

Unreported

100 64.4-66.5 14.1-16.7 0.528-0.786 0.305-0.324

200 56.9-60.2 9.29-11.25 0.530-0.546 0.308-0.321

20,000 Trees 8 Leaves 64 Leaves 709.0-772.2 4462.0-4809.0 40.5-41.8 343.7-425.1 33.5-40.5 236.0-274.7

500 52.3-54.0 6.49-7.98 0.620-0.662 0.323-0.325

1000 51.1-53.3 5.83-7.04 0.976-1.01 0.323-0.335

scoring systems have GPUs available for high-throughput or lean on the SIMD instructions in modern CPUs, the elimination of branching from our approach is an improvement that will generalize to architectures of the future.
6 CONCLUSIONS AND FUTURE WORK
In this paper, we show that a forest based learning to rank function can successfully be approximated by a series of matrix multiplies in the form of a small neural network. This is supported both by theoretical guarantees and by empirical examination. Furthermore, the approximation results in slightly improved generalization in some cases which suggests that we have proposed a relatively safe method to improve efficiency without negatively impacting performance.
ACKNOWLEDGEMENTS
This work was supported in part by the Center for Intelligent Information Retrieval, in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) via AFRL contact #FA8650-17-C-9116 under subcontract #94671240 from the University of Southern California. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon, and in part by NSF grant #IIS-1617408. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
REFERENCES
[1] Nima Asadi and Jimmy Lin. 2013. Training efficient tree-based models for document ranking (ECIR). 146­157.
[2] Michael Bendersky, W Bruce Croft, and Yanlei Diao. 2011. Quality-biased ranking of web documents (WSDM). 95­104.
[3] Microsoft Research Blog. 2015. RankNet: A ranking retrospective. https://www. microsoft.com/en-us/research/blog/ranknet-a-ranking-retrospective/. (2015).
[4] Gabriele Capannini, Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Nicola Tonellotto. 2016. Quality versus efficiency in document scoring with learning-to-rank models. In Information Processing & Management. Elsevier, 1161­1177.

[5] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge overview. In Proceedings of the Learning to Rank Challenge. 1­24.
[6] Anna Choromanska, Mikael Henaff, Michaël Mathieu, Gérard Ben Arous, and Yann LeCun. 2015. The Loss Surfaces of Multilayer Networks. In AISTATS.
[7] V Dang. 2015. RankLib, v.2.5-SNAPSHOT. https://sourceforge.net/p/lemur/wiki/ RankLib. (2015).
[8] Domenico Dato, Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, and Rossano Venturini. 2016. Fast ranking with additive ensembles of oblivious and non-oblivious regression trees. In ACM Transactions on Information Systems (TOIS).
[9] Clebson CA de Sá, Marcos A Gonçalves, Daniel X Sousa, and Thiago Salles. 2016. Generalized BROOF-L2R: A General Framework for Learning to Rank Based on Boosting and Random Forests. In SIGIR. 95­104.
[10] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. 2017. Neural Ranking Models with Weak Supervision. In SIGIR. 65­74.
[11] Kurt Hornik. 1991. Approximation capabilities of multilayer feedforward networks. In Neural Networks. 251­257.
[12] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. In CoRR. http://arxiv.org/abs/1704.04861
[13] Muhammad Ibrahim. 2017. Scalability and Performance of Random Forest based Learning-to-Rank for Information Retrieval. In ACM SIGIR Forum, Vol. 51. ACM, 73­74.
[14] Xin Jin, Tao Yang, and Xun Tang. 2016. A Comparison of Cache Blocking Methods for Fast Execution of Ensemble-based Score Computation. In SIGIR. 629­638.
[15] Thorsten Joachims. 2002. Optimizing search engines using clickthrough data. In SIGKDD. 133­142.
[16] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. In CoRR. http://arxiv.org/abs/1412.6980
[17] Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. 2017. The Case for Learned Index Structures. arXiv preprint arXiv:1712.01208 (2017).
[18] Bernardo Llanas, Sagrario Lantarón, and Francisco J. Sáinz. 2008. Constructive Approximation of Discontinuous Functions by Neural Networks. Neural Processing Letters (2008), 209­226.
[19] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, and Rossano Venturini. 2015. Quickscorer: A fast algorithm to rank documents with additive ensembles of regression trees. In SIGIR. 73­82.
[20] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, Nicola Tonellotto, and Rossano Venturini. 2016. Exploiting CPU SIMD extensions to speed-up document scoring with tree ensembles. In SIGIR. 833­836.
[21] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Raffaele Perego, and Salvatore Trani. 2017. X-DART: Blending Dropout and Pruning for Efficient Learning to Rank. In SIGIR. 1077­1080.
[22] Joel Mackenzie, J Shane Culpepper, Roi Blanco, Matt Crane, Charles LA Clarke, and Jimmy Lin. 2018. Query Driven Algorithm Selection in Early Stage Retrieval. In WSDM.
[23] Donald Metzler and W Bruce Croft. 2005. A Markov random field model for term dependencies. In SIGIR. 472­479.
[24] Marvin Minsky and Seymour Papert. 1969. Perceptrons.. In MIT Press. [25] Xun Tang, Xin Jin, and Tao Yang. 2014. Cache-conscious runtime optimization
for ranking ensembles (SIGIR). 1123­1126. [26] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017.
Situational Context for Ranking in Personal Search. In WWW. 1531­1540.

1020

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin

Stefano Mizzaro
University of Udine Udine, Italy
mizzaro@uniud.it
Kevin Roitero
University of Udine Udine, Italy
roitero.kevin@spes.uniud.it
ABSTRACT
Some methods have been developed for automatic effectiveness evaluation without relevance judgments. We propose to use those methods, and their combination based on a machine learning approach, for query performance prediction. Moreover, since predicting average precision as it is usually done in query performance prediction literature is sensitive to the reference system that is chosen, we focus on predicting the average of average precision values over several systems. Results of an extensive experimental evaluation on ten TREC collections show that our proposed methods outperform state-of-the-art query performance predictors.
CCS CONCEPTS
· Information systems  Test collections; Retrieval effectiveness;
KEYWORDS
Query difficulty prediction, AAP, Test collections, TREC
ACM Reference Format: Stefano Mizzaro, Josiane Mothe, Kevin Roitero, and Md Zia Ullah. 2018. Query Performance Prediction and Effectiveness Evaluation Without Relevance Judgments: Two Sides of the Same Coin. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210146
1 INTRODUCTION
Query Performance Prediction (QPP) is about predicting the effectiveness of the system for an unknown query [3, 19] while Effectiveness Evaluation without Relevance Judgments (EEwRJ) mainly tackles the problem of the cost of human relevance judgment by considering new methodologies to assess system effectiveness [15].
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210146

Josiane Mothe
ESPE, Université de Toulouse, IRIT, UMR5505 CNRS Toulouse, France
Josiane.Mothe@irit.fr
Md Zia Ullah
UPS, Université de Toulouse, IRIT, UMR5505 CNRS Toulouse, France mdzia.ullah@irit.fr
We consider these problems as the two sides of the same coin and we propose to combine these two research directions that so far have been treated independently. We show by extensive experiments on ten TREC collections that EEwRJ can be exploited to obtain a more accurate QPP than state-of-the-art.
In the following, we briefly review QPP and EEwRJ in Section 2, detail how EEwRJ can be adapted to QPP in Section 3, present our experiments in Section 4, and summarize our findings and sketch future developments in Section 5.
2 BACKGROUND
Query Performance Prediction. QPP aims at estimating system effectiveness for a given query [3, 19]. Current approaches consider either individual features [4, 9, 14, 20] or a combination of them [2, 7, 13, 20] to predict query performance. QPP accuracy is evaluated by means of correlation between the predicted AP and the real AP [3, 11].
The most effective individual predictors are the post-retrieval ones, which are calculated after the query has been submitted to the search engine considering the retrieved document list and document scores [3]. Although some of these features can be quite sophisticated (e.g. Weighted Information Gain which measures the divergence between the mean of the top-retrieved document scores and the mean of the entire set of document scores [20]), they only weakly correlate with actual system effectiveness [7, 11]: Pearson correlation with actual effectiveness is about 0.5 [14].
Since using one single query feature for QPP is not fully effective, combining features looks as a reasonable alternative. Current research mainly investigated linear regression [2, 6, 13, 20]. Thanks to these types of combination, the correlation has been slightly increased but remains well below 0.6. Evaluation Without Relevance Judgments. The objective of all the EEwRJ methods1 is to predict system effectiveness in a TREClike environment. The first proposal was by Soboroff et al. [15], who proposed to randomly sample documents from the pool and treated such documents as relevant; the intuition is that if a document is retrieved by many systems in the top rank positions it will be pooled and thus it is probably a relevant document. Wu and Crestani [18] used data fusion techniques to merge the ranked lists retrieved by the systems and computed a score for each system based on
1To avoid confusion, we speak of QQP approaches and of EEwRJ methods in this paper.

1233

Short Research Papers II
SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA
the popularity of the documents it retrieves. Aslam and Savell [1] proposed an index based on the similarity between the ranked lists of systems; their index is computed simply considering the ratio between the document intersection and the document union of the ranked lists of each pair of systems.
Nuray and Can [10] adapted methods from democratic election strategies to compute the popularity score of each document by treating the documents as candidates and the systems as voters; more in detail, they used the "RankPosition," "Borda," and "Condorcet" methodologies. Spoerri [16] proposed a set of trials between systems and for each trial measures the percentage of documents retrieved by a system alone, by all the systems in the trial, and a combination of the previous percentage scores.
Diaz [5] embedded the retrieved documents in a high-dimensional space and computed spatial correlation values to measure document similarity and derived a predicted retrieval performance. Diaz [5] methodology is the only one which makes use of the collection documents; we leave such technique as future work. Sakai and Lin [12] used a variation of the Condorcet method from [10] which is less computationally demanding.
3 QPP BY MEANS OF EEWRJ
While QPP focuses on individual queries, EEwRJ focuses on average over queries. By focusing on a single effectiveness measure such as Average Precision (AP), we can say that QPP aims at predicting AP, while the EEwRJ aims at predicting Mean AP (MAP) for all the runs in a given TREC edition. Usually, the EEwRJ methods are evaluated by means of correlation, like QPP. However, while QPP approaches are evaluated by the Pearson correlation between predicted and real AP, EEwRJ methods are evaluated by the correlation between predicted and real MAP.
EEwRJ methods can be taken almost off-the-shelf and, with minor adaptations, exploited "as is" for QPP. Indeed EEwRJ methods can predict (by solving some normalization issues) not only MAP but also individual AP values for each system, topic pair. Following Mizzaro and Robertson [8], we can then derive a prediction of Average AP (AAP) which is the average across systems of AP for a given query ("topic" in TREC terminology).
Considering AAP does make sense for QPP since queries (or topics) which get a low AAP are difficult queries most systems failed on, and we should pay attention to and thus predict as difficult, On the other hand, queries which get high AAP are easy queries that any system can treat. In this paper, we thus focus on AAP as the measure to predict (while most of the papers from the literature consider AP [11, 14, 20]).
Moreover, we also combine the individual EEwRJ methods. So far the EEwRJ methods have been proposed individually, without any combination. Instead, we train a Machine Learning (ML) system that, on the basis of the TREC data of the previous years, learns a model that is then applied on a subsequent year TREC test collection (previous years test collections are the training set and the new test collection is then the test set). In other terms, the combination function, or the ML model, is the one that, on the basis of historical data, provides the best prediction of real AAP values given the individual EEwRJ outcomes.

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Stefano Mizzaro, Josiane Mothe, Kevin Roitero, and Md Zia Ullah

Table 1: Name, acronym, and parameters used for EEwRJ.

Citation

Acronym Name

Soboroff et al. [15] SNC

Soboroff et al.

Wu and Crestani [18]

WUCv0 WUCv1 WUCv2 WUCv3 WUCv4

Basic Version 1 Version 2 Version 3 Version 4

Aslam and Savell [1] AS

Aslam & Savell

Pool depth
100
100 100 100 100 100
100

Nuray and Can [10] NC-NRP Normal Rank Position

30

NC-NB Normal Borda

30

NC-NC Normal Condorcet

30

NC-BRP Bias Rank Position

30

NC-BB

Bias Borda

30

NC-BC Bias Condorcet

30

Spoerri [16]

SPO-S

Single

100

SPO-A

All Five

100

SPO-SA Single Minus All Five

100

Sakai and Lin [12] SL

Sakai and Lin

30

Table 2: Short description of the 10 TREC collections used.

Acron. Collections Corpus Size Topics

T6 T7 T01 R04
R05 Tb04 Tb05 Tb06 W13 W14

TREC6 Adhoc ROBUST 528K 50 (301 ­ 350)

TREC7 Adhoc ROBUST 528K 50 (351 ­ 400)

TREC2001 Adhoc WT10G 1.6M 50 (501 ­ 550)

Robust 2004 ROBUST 528K 249 (301 ­ 450,

601 ­ 700)

Robust 2005 ROBUST 528K 50 (301 ­ 700)

Terabyte 2004 GOV2

25M 49 (701 ­ 750)

Terabyte 2005 GOV2

25M 50 (751 ­ 800)

Terabyte 2006 GOV2

25M 150 (701 ­ 850)

Web Track 2013 ClueWeb12B 52M 50 (201 ­ 250)

Web Track 2014 ClueWeb12B 52M 50 (250 ­ 300)

We use six ML algorithms [17]: Linear Regression (LR), M5P model tree (M5P), Random Forest (RF), Neural Networks (NN), Support Vector Machine with Polynomial kernel (SVM_Poly), and SVM with Radial Basis Function Kernel (SVM_RBF). These, in addition to 17 state-of-the-art EEwRJ individual methods from the previous work presented in Section 2 and summarized in Table 1, sum up to 23 methods used in the following experiments.
4 EXPERIMENTS AND RESULTS
Table 2 shows the ten TREC test collections used in our experiment. For a first evaluation of the predictive power of EEwRJ features, we considered each of the systems that participated to the corresponding TREC edition, predicted its AP using any individual features and then calculated the predicted AAP (by averaging the results across system per topic). Finally, we calculated the Pearson correlation between the predicted AAP and the actual AAP.

1234

Short Research Papers II QPP and EEwRJ: Two Sides of the Same Coin

QF - Predicted AS - Predicted

Pearson: p=0.6 (p=4.41e-06)) 100
80 60 40 20
0.0 0.1 0.2 0.3 0.4 0.5 0.6 AAP
(a) QF vs actual AAP

60 Pearson: p=0.74 (p=6.16e-10)) 50 40 30 20 10
0 0.0 0.1 0.2 0.3 0.4 0.5 0.6 AAP
(b) AS vs actual AAP

Figure 1: TREC7 Adhoc collection. Pearson correlation between AAP and (a) QF [20], (b) AS [1]. While dots correspond to actual and predicted AAP for individual topics, the cone represents the confidence interval.

As for comparison, we calculated also the Pearson correlation, between the actual AAP and the value obtained when using state of the art QPP. As baselines to compare with, we consider the state of the art QPP approaches such as Unnormalized Query Commitment (UQC) [14], Query Feedback (QF) [20], Weighted Information Gain (WIG) [20], and Clarity [4].
To calculate the value of the state of the art QPP post-retrieval features, we used Language modeling. Thus while EEwRJ predictors are calculated for any (topic/participant system) pairs, QPP features are calculated only once for each topic.
For comparison purposes, these QPP approaches are also combined using machine learning algorithms including the same algorithms as previously mentioned (LR, M5P, RF, and SVM_RBF); these predictors are later referred to as ML QPP. The algorithms are trained to learn AAP and thus also predict AAP.
We found that EEwRJ individual features have a higher correlation with AAP than QPP individual features. As for example, Figure 1 reports the predicted values and actual AAP we obtained for TREC7 collection (a) QF, one of the best state of the art QPP feature (correlation value 0.599) and (b) ASLAM method, one of the EEwRJ (correlation value 0.744). The plots and the correlation values confirm that the AS method is a better predictor than QF.
To turn to a more systematic and complete analysis, Table 3 reports Pearson correlation of the predicted AAP values with the actual AAP of the participants' system, for each collection.
In the first two parts, on the top of the table, we report the state of the art baseline query performance predictors when calculated as previously mentioned, and their correlation with AAP. We report first individual predictors, second their combination using machine learning algorithms with leave-one-query-out cross-validation on each collection. Leave-one-query-out cross-validation is widely used in the field as in [14, 20].
The following two parts, on the bottom of the table, report the Pearson correlation values between the predicted AAP by the EEwRJ methods and the actual AAP.2 First, the correlation values for the EEwRJ individual features (listed in Table 1) are reported
2AAP is obtained averaging for each topic the AP values of all systems which participated in a given TREC collection.

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Table 3: Pearson correlation, over the ten TREC collections, between the actual AAP and the predicted AAP by the individual QPP, ML QPP, individual EEwRJ, and ML EEwRJ predictors. "", "", and "*" stand for p-value < 0.001, < 0.01, and < 0.05, respectively. Values in bold are the largest in each part of the table for each collection.

Method T6 T7 T01 R04 R05 Tb04 Tb05 Tb06 W13 W14

QPP UQC WIG QF Clarity

.606 .493 .214 .521 .208 .161 .299* .296 .102 .342* .435 .281* .197 .356 .142 .223 .311* .285 .487 .422 .368 .599 .107 .409 .268 .454 .337*.392 .009 -.121 .415 .587 .316* .476 .164 .251 .121 .136 -.430 -.221

ML QPP LR M5P RF SVM-RBF

.490 .538 .152 .569 -.051 .251 .162 .382 .517 .490 .529 .578 -.077 .548 .049 .327* .155 .351 .538 .605 .519 .597 .000 .549 .076 .195 .227 .312 .423 .281* .453 .671 .003 .501 .060 .268 .285*.289 .308* .082

EEwRJ SNC WUCv0 WUCV1 WUCV2 WUCV3 WUCV4 AS NC-NRP NC-NB NC-NC NC-BRP NC-BB NC-BC SPO-S SPO-A SPO-SA SL

.268 .269 .253 .134* .210 .590 .405 .488 .460 .656 .156 -.068 .317* .150* .275 .673 .465 .474 .364 .263 .180 -.039 .331* .163* .287* .687 .477 .492 .372 .293* .175 -.023 .321* .160* .277 .665 .481 .478 .374 .280* .205 .020 .332* .176 .290* .681 .493 .495 .381 .317*
.159 .182 -.066 .0430 -.001 -.178 .238 -.069 -.010 -.089 .671 .744 .647 .683 .466 .460 .476 .474 .460 .601 -.314*-.0160 .395 .018 .261 .464 .282* .139 .366 .261 .384 .311* .484 .398 .379 .765 .610 .592 .430 .542 .399 .341* .453 .402 .373 .761 .603 .585 .420 .590 .358* .287* .448 .415 .352* .761 .581 .533 .411 .608 .344* .281* .473 .389 .357* .761 .585 .534 .437 .590 .513 .597 .584 .566 .464 .636 .550 .525 .446 .657 .244 .112 .360* .181 .259 .625 .509 .501 .369 .346* .288* .243 .308* .231 .336* .744 .504 .540 .245 .366 .265 .186 .332* .219 .313* .697 .518 .534 .316* .361 .504 .475 .516 .502 .41 .712 .620 .58 .419 .629

ML EEwRJ

LR

.668 .761 .636 .607 .457 .644 .555 .568 .439 .201

M5P

.648 .693 .605 .565 .474 .602 .407 .585 .429 .327*

NET

.582 .738 .634 .509 .479 .557 .367 .596 .440 .159

RF

.621 .685 .634 .651 .519 .578 .509 .589 .408 .187

SVM_Poly .656 .760 .620 .614 .465 .662 .549 .583 .448 .204

SVM_RBF .642 .752 .626 .613 .465 .680 .546 .575 .441 .207

then the ones obtained when using the six ML based combination methods.
To help to understand these data, Figure 2 shows a graphical comparison of the Pearson correlation. The figure contains a series of box-plots for the individual EEwRJ methods and for the ML combination of the EEwRJ methods, as well as a series of point-plots for the individual QPP features, and for the ML combination of the QPP features, i.e., our baselines. We can draw several conclusions from this figure and the data from the Table 3. When comparing individual and combined baseline (QPP) predictors in the top part of the Table 3, we can see that in many cases the combination is better than individual features, although for some collections (e.g. TREC2001) the combination fails and single features are better.

1235

Short Research Papers II SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Stefano Mizzaro, Josiane Mothe, Kevin Roitero, and Md Zia Ullah

Pearson's correlation

0.75
0.50
0.25
0.00
EEwRJ ML EEwRJ QPP ML QPP
T6 T7 T01 R04 R05 Tb04 Tb05 Tb06 W13 W14
Figure 2: Comparison over the 10 collections of Pearson correlation between the actual AAP and the predicted AAP by the individual QPP, ML QPP, individual EEwRJ, and ML EEwRJ predictors. Boxplots are for EEwRJ while dots are for QPP.
When considering individual features, EEwRJ (specifically AS method) outperforms QPP baseline. When combining features, the method we propose based on EEwRJ also outperforms the combination of QPP. For example, the best correlation when combining EEwRJ methods is obtained for TREC7 where our combined methods get a correlation from .685 to .761 while ML QPP correlations are from .538 to .671, depending on the ML algorithm.
Turning to comparing individual and combined EEwRJ methods, we can clearly see that overall the combination of EEwRJ methods (ML EEwRJ) is better than the EEwRJ considered individually, although there are a few individual methods that outperform the ML combinations.
For all but one collection (W13) the best EEwRJ individual method outperforms all the baselines, and in all but two cases (W13 and W14) the EEwRJ ML methods outperform all the baselines as well. We also can see that apart for W14 collection, all the correlations are statistically significant which is not the case for QPP approaches that correlate only for some of the collections. Finally, the correlation values are much higher than any reported correlation when considering AP to be predicted [6, 11, 14].
Clearly, EEwRJ is an effective method for QPP: both as an individual predictor and when combined our method outperforms state of the art.
5 CONCLUSIONS AND FUTURE WORK
In this paper, we proposed to apply the methods to effectiveness evaluation without relevance judgments (EEwRJ) to the problem of

query performance prediction (QPP). Our results clearly show that
EEwRJ is an effective approach to QPP. If the AAP of a TREC topic
is a reliable measure of query ease/difficulty, as it seems reasonable
to assume, then it is possible to find specific EEwRJ methods (both
individual and combined by means of ML) that outperform state-
of-the-art query performance predictors.
In the future we plan to add more test collections to the analysis,
for generality and also for a better understanding of the variation
across datasets (e.g., W13 and W14 look different from the other
collections). We will also take into account different correlation
measures and effectiveness metrics. More in general, we believe
that QPP and EEwRJ are "two sides of the same coin". Our approach
clearly shows that they are related, and we plan in the future to
explore and exploit their relationships in a complete way.
REFERENCES
[1] Javed A. Aslam and Robert Savell. 2003. On the Effectiveness of Evaluating Retrieval Systems in the Absence of Relevance Judgments. In Proceedings of 26th ACM SIGIR. 361­362.
[2] Shariq Bashir. 2014. Combining Pre-retrieval Query Quality Predictors Using Genetic Programming. Applied Intelligence 40, 3 (April 2014), 525­535.
[3] David Carmel and Elad Yom-Tov. 2010. Estimating the Query Difficulty for Information Retrieval (1st ed.). Morgan and Claypool Publishers.
[4] Steve Cronen-Townsend and W. Bruce Croft. 2002. Quantifying Query Ambiguity. In Conference on Human Language Technology Research. 104­109.
[5] Fernando Diaz. 2007. Performance Prediction Using Spatial Autocorrelation. In Proceedings of 30th ACM SIGIR. 583­590.
[6] Claudia Hauff. 2010. Predicting the effectiveness of queries and retrieval systems. SIGIR Forum 44, 1 (2010), 88. https://doi.org/10.1145/1842890.1842906
[7] Claudia Hauff, Djoerd Hiemstra, Leif Azzopardi, and Franciska de Jong. 2010. A Case for Automatic System Evaluation. In Proceedings of ECIR (LNCS), Vol. 5993. 153­165.
[8] Stefano Mizzaro and Stephen Robertson. 2007. HITS hits TREC: exploring IR evaluation results with network analysis. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 479­486.
[9] Josiane Mothe and Ludovic Tanguy. 2005. Linguistic features to predict query difficulty. In ACM SIGIR, Predicting query difficulty-methods and applications workshop. 7­10.
[10] Rabia Nuray and Fazli Can. 2006. Automatic ranking of information retrieval systems using data fusion. Information Processing & Management 42, 3 (May 2006), 595­614.
[11] Fiana Raiber and Oren Kurland. 2014. Query-performance prediction: setting the expectations straight. In ACM SIGIR. ACM, 13­22.
[12] Tetsuya Sakai and Chin-Yew Lin. 2010. Ranking Retrieval Systems without Relevance Assessments -- Revisited. In Proceeding of 3rd EVIA -- A Satellite Workshop of NTCIR-8. National Institute of Informatics, Tokyo, Japan, 25­33.
[13] Anna Shtok, Oren Kurland, and David Carmel. 2010. Using statistical decision theory and relevance models for query-performance prediction. In ACM SIGIR. 259­266.
[14] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. 2012. Predicting Query Performance by Query-Drift Estimation. ACM Trans. Inf. Syst. 30, 2, Article 11 (May 2012), 35 pages.
[15] Ian Soboroff, Charles Nicholas, and Patrick Cahan. 2001. Ranking Retrieval Systems Without Relevance Judgments. In Proceedings of 24th ACM SIGIR. 66­73.
[16] Anselm Spoerri. 2007. Using the structure of overlap between search results to rank retrieval systems without relevance judgments. Information Processing & Management 43, 4 (2007), 1059 ­ 1070.
[17] Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal. 2016. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann.
[18] Shengli Wu and Fabio Crestani. 2003. Methods for Ranking Information Retrieval Systems Without Relevance Judgments. In Proceedings of the 2003 ACM Symposium on Applied Computing. 811­816.
[19] Ying Zhao, Falk Scholer, and Yohannes Tsegay. 2008. Effective Pre-retrieval Query Performance Prediction Using Similarity and Variability Evidence. In Proceedings of 30th ECIR. 52­64.
[20] Yun Zhou and W Bruce Croft. 2007. Query performance prediction in web search environments. In ACM SIGIR. 543­550.

1236

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A New Term Frequency Normalization Model for Probabilistic Information Retrieval

Fanghong Jian, Jimmy Xiangji Huang, Jiashu Zhao and Tingting He 
Information Retrieval and Knowledge Management Research Lab 1National Engineering Research Center for E-Learning, 3School of Computer, Central China Normal University, Wuhan,
China; 2School of Information Technology, York University, Toronto, Canada
jfhrecoba@mails.ccnu.edu.cn,jhuang@yorku.ca,zhaojiashu@gmail.com,tthe@mail.ccnu.edu.cn

ABSTRACT
In probabilistic BM25, term frequency normalization is one of the key components. It is often controlled by parameters k1 and b, which need to be optimized for each given data set. In this paper, we assume and show empirically that term frequency normalization should be specific with query length in order to optimize retrieval performance. Following this intuition, we first propose a new term frequency normalization with query length for probabilistic information retrieval, namely BM25QL. Then BM25QL is incorporated into the state-of-the-art models CRTER2 and LDA-BM25, denoted as CRTER2QL and LDA-BM25QL respectively. A series of experiments show that our proposed approaches BM25QL, CRTER2QL and LDA-BM25QL are comparable to BM25, CRTER2 and LDA-BM25 with the optimal b setting in terms of MAP on all the data sets.
KEYWORDS
Term Frequency Normalization, BM25, Probabilistic Model
ACM Reference Format: Fanghong Jian, Jimmy Xiangji Huang, Jiashu Zhao and Tingting He . 2018. A New Term Frequency Normalization Model for Probabilistic Information Retrieval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978. 3210147
1 INTRODUCTION AND RELATED WORK
Term frequency (TF) normalization is very important in information retrieval (IR) models. There are kinds of term frequency normalization achieving success. Sub-linear term frequency normalization in BM25 [10] is one of state-of-the-art approaches in the last two decades. It has two hyper-parameters (k1 and b), which are as term independent constants and often need to be optimized for each given data set [4]. In recent years, much research work started to focus on the automatic tuning of document length normalization. TF normalization approaches in [4, 8, 9, 14] are document and collection dependent, and fixed term-independent parameter
The corresponding author is Jimmy Xiangji Huang. The affiliation 1 is for Fanghong Jian, 2 for Jiashu Zhao and 3 for Tingting He.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210147

setting remained the same as the original BM25. Cummins et al. [1] first investigated the effect of query length on normalization but didn't measure the effect. [2, 6, 7] used query length normalization constraints to estimate term-specific parameters, which may be expensive and overfitted. Chung et al. [15] have incorporated the query-length into vector space model and conducted experiments on Chinese and English corpora, suggesting that the query-length should be incorporated in other existing ranking functions. So it is worth studying how to simply and effectively incorporate query length into probabilistic model.
In this paper, we propose a new term frequency normalization for probabilistic BM25, and integrate it into state-of-the-art BM25based models with proximity and topic modeling. We also present experiments on TREC data sets to investigate the effect of three term frequency normalization functions.
The remainder of this paper is organized as follows. We propose a modified BM25 via a new term frequency normalization method in Section 2. In Section 3, we set up our experimental environment on eight TREC data sets. In Section 4, the experimental results are presented and discussed. Finally, we conclude our work briefly and present future research directions in Section 5.
2 OUR PROPOSED APPROACH
In this section, we first introduce a new term frequency normalization approach, and then describe how to integrate it into probabilistic BM25. For clarification, Table 1 outlines the notations used throughout the paper.
Table 1: Notations

Notations
c d q qi ql bQ L ql 2bQ L ql 2 dl avdl N n tf qt f
IDF
b, k1, k3

Description collection document query query term query length
first order partial derivative bQ L with respect to ql

second order partial derivative bQ L with respect to ql

length of document

average document length

number of indexed documents in collection

number of indexed documents containing a term

within-document term frequency

within-query term frequency

inverse

document

frequency,

equals

to

log2

N -n+0.5 n+0.5

parameters in BM25

2.1 A New Method for TF Normalization

BM25 is a well-known probabilistic IR model, which scores a docu-

ment d with respect to a query q as follows.

BM 25(q, d) =

(k1+1) · T F qi qd k1 + T F

· (k3+1) · qt f k3 + qtf

· IDF

(1)

where T F =

tf

(1-b

)+b

·

dl avdl

is pivoted document length normaliza-

tion, which is proved to be effective for term frequency normal-

ization. b is a parameter used to balance the impact of document

1237

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

length dl. In practice, b is usually set to a default value or opti-

mized for each individual data set. Generally, parameter b should

be optimized for each given collection [4], so it is worth exploring

a modified term frequency normalization.

In previous work [11], the query length, i.e. ql, the number of

terms in a query q, is used to balance two kinds of TF normalization.

From an information theoretic perspective, adding a term to the

query is equivalent to increasing the information provided by the

query. We assume that when query length increases, the effect of TF

Normalization should be boosted, in order to facilitate preference

to shorter documents. Based on this assumption, we propose a new

method for document TF normalization using query length.

T FQL

=

(1

- bQL(ql))

tf + bQL(ql)

·

dl avdl

(2)

where bQL(ql) is a given function of query length ql. Heuristically,

this function bQL(ql) should increase with the growth of query

length, while it must lie between 0 and 1. In addition, when a

term is added to a shorter query, it is more likely to show more

search intent than added to a longer query. Thus, bQL(ql) should be less affected with the change of ql for larger ql. Specifically, we

characterize bQL(ql) as follows.

· Boundedness: bQL(1) = 0, and bQL() = 1

·

Monotonicity: bQL(ql) < bQL(ql + 1) 

bQ L ql

>0

· Convexity: bQL(ql + 1) - bQL(ql) > bQL(ql + 2) - bQL(ql +

1) 

2bQ L ql 2

<0

To satisfy the above characteristics, we propose several different
types of functions as in Formula (3)-(5). These three functions are
proposed to satisfy all the required characteristics for bQL(ql). In addition, the proposed functions grow differently when the query length ql increases: bQLOLG (ql) is based on the logarithm function which grows the slowest; bQRELC (ql) is based on the reciprocal function which grows with a median speed; bQEXL P (ql) is based on the exponential function which grows the fastest. In this paper, we only
consider these three types of functions and more functions will be
evaluated in the future.

bQLOLG (ql)

=

1

-

1

+

2 log2(1

+ ql)

(3)

bQRELC (ql)

=

1

-

3

4 + ql

(4)

bQE XL

P

(ql )

=

1

-

exp(-

ql

- 6

1

)

(5)

2.2 A New Model: BM25QL

We use the query length for term frequency normalization in BM25

and propose a new BM25QL formula as follows.

BM 25Q L (q, d ) =

(k1+1) · T FQ L qi qd k1 + T FQ L

· (k3+1) · qt f k3 + qtf

· IDF

(6)

In this paper, we explore three term frequency normalization

functions in BM25, and the corresponding BM25QL are denoted as

BM25QLLOG, BM25QLREC and BM25QLEXP respectively.

Recent years, there are some state-of-the-art BM25-based models

succeeded in IR. For example, bigram cross term model CRTER2 in [5]

is a well known probabilistic proximity model, and LDA-BM25 in

[3] is a strong topic based hybrid model. We use BM25QL in the same

way as the BM25 in CRTER2 and LDA-BM25, and propose CRTER2QL and LDA-BM25QL respectively. Similarly, we also investigate term frequency normalization functions in CRTER2QL and LDA-BM25QL.
3 EXPERIMENTAL SETTINGS
We conduct experiments on eight standard TREC data sets, which include AP88-89 with queries 51-100, LA with queries 301-400, WSJ(87-92) with queries 151-200, DISK1&2 with queries 51-200, DISK4&5 no CR with queries 301-450, Robust04 with queries 301450 & 601-700, WT2G with queries 401-450 and WT10G with queries 451-550. These data sets are different in sizes and genres, including high-quality newswire collections and Web collections containing many noisy documents. In all the experiments, we only use the title field of the TREC queries for retrieval. Queries without judgments are removed. For all test data sets used, each term is stemmed by using Porter's English stemmer. Standard English stopwords are removed. The official TREC evaluation measure is used in our experiments, namely Mean Average Precision (MAP).
For fair comparisons, we use the following parameter settings for both the baselines and our proposed models, which are popular in the IR domain for building strong baselines. First, in BM25, k1 and k3 are set to be 1.2 and 8. Meanwhile, we sweep the values of b for BM25 from 0 to 1.0 with an interval of 0.05. Second, in CRTER2, we sweep the values of normalization parameter  in a group of different values 2, 5, 10, 20, 25, 50, 75, 100, and triangle kernel was shown in [5] to achieve best MAP for most data sets. Thirdly, in LDA modeling, we use symmetric Dirichlet priors with  = 50/Kt and  = 0.01, which are common settings in the literature and shown in [3, 16] that retrieval results were not very sensitive to the values of these parameters. The number of topics Kt is set to be 400 as recommended in [3, 16]. Finally, we sweep the values of balancing parameter  from 0.1 to 0.9 with an interval of 0.1 in CRTER2, CRTER2QL, LDA-BM25 and LDA-BM25QL.
4 EXPERIMENTAL RESULTS 4.1 Comparison with BM25
We first investigate the performance of our proposed BM25QL compared with BM25. The experimental results are presented in Figure 1. As shown by the results, our proposed BM25QL models are comparable to BM25 with optimal b on almost all data sets in terms of MAP. Moreover, according to the results in Figure 1, each new term frequency normalization function has its advantage on some aspects. There is no single function can outperform others on all the data sets. Without much knowledge of a new data set, logarithmic function is recommended for BM25QL.
4.2 Comparison with CRTER2
In order to test the robustness, we incorporate our proposed BM25QL models into various types of BM25-based models. Firstly, we use BM25QL to tune the parameter b in the state-of-the-art BM25-based proximity approaches. Zhao et al. [5] showed that bigram cross term model CRTER2 is at least comparable to major probabilistic proximity models PPM [12] and BM25TP [13] in BM25-based framework. We compare our proposed CRTER2QL with CRTER2. The results are presented in Figure 2. Figure 2 shows that the proposed CRTER2QL models are also comparable to CRTER2 with optimal b on almost all data sets.

1238

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

MAP

MAP

MAP

AP88-89 0.29

0.285

0.28

0.275

0.27

0.265 0.26
0.255 0
0.23

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
DISK4&5

0.225

0.22

0.215

0.21 0.205
0.2 0

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

LA 0.255

0.25

0.245

0.24

0.235

0.23

0.225 0.22
0.215 0.21 0
0.255

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
ROBUST04

0.25

0.245

0.24

0.235

0.23
0.225
0.22
0.215 0

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

WSJ 0.34

0.33

0.32

0.31

0.3

0.29 0.28 0.27
0
0.33 0.32

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
WT2G

0.3

0.28

0.26

0.24 0.22
0.2 0

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

DISK1&2 0.25

0.24

0.23

0.22

0.21

0.2 0.19 0.18
0 0.21

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
WT10G

0.2

0.19

0.18

0.17

0.16

0.15
0.14
0.13
0.12 0

BM25 BM25 EXP
QL BM25 LOG
QL BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

Figure 1: MAP Comparison between BM25QL and BM25

0.298 0.295

AP88-89

0.29

0.285

0.28

0.275
0.27
0.265 0
0.235

CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
DISK4&5

0.23

0.225

0.22

0.215
0.21 0

CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

LA 0.258
0.255

0.25

0.245

0.24

0.235

0.23 0.225
0.22 0
0.26

CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
ROBUST04

0.255

0.25

0.245

0.24
0.235
0.23 0

CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

0.35 0.345
0.34 0.335
0.33 0.325
0.32 0.315
0.31 0.305
0.3 0
0.35 0.34
0.32

WSJ
CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value WT2G

MAP

0.25 0.24 0.23 0.22 0.21
0.2 0.19
0 0.225
0.22 0.21
0.2

DISK1&2
CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value WT10G

0.3

0.28 0.26 0.24
0

CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

0.19

0.18

0.17 0.16 0.15
0

CRTER 2
CRTEREXP 2
CRTERLOG 2
CRTERREC 2
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

Figure 2: MAP Comparison between CRTER2QL and CRTER2

0.317 0.316

AP88-89

0.314

0.312

0.31

0.308 0.306 0.304
0 0.235

LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
DISK4&5

0.23

0.225
0.22 0

LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

LA 0.268
0.265

0.26

0.255

0.25 0.245
0.24 0
0.266

LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value
ROBUST04

0.264

0.262

0.26

0.258

0.256

0.254 0.252
0.25 0.248
0

LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 b-value

MAP

MAP

0.355

WSJ

0.35

0.345

0.34

0.335
0.33
0.325 0
0.33 0.32 0.31
0.3 0.29 0.28 0.27 0.26 0.25 0.24 0.23
0

LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
b-value WT2G
LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
b-value

MAP

MAP

0.265

DISK1&2

0.26

0.255

0.25

0.245
0.24 0
0.215 0.21
0.205 0.2
0.195 0.19
0.185 0.18
0.175 0.17
0.165 0.16 0

LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
b-value WT10G
LDA-BM25 LDA-BM25 EXP
QL LDA-BM25 LOG
QL LDA-BM25 REC
QL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
b-value

Figure 3: MAP Comparison between LDA-BM25QL and LDA-BM25

MAP

MAP

MAP

1239

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 2: Summary of Comparison with BM25QL and BM25, CRTER2QL and CRTER2, LDA-BM25QL and LDA-BM25. The bold phase style

means that it is the best result in each group. "1, 2, 3, 4" denotes our proposed models outperform the corresponding models

with the settings for b as 0.35, 0.4, 0.75 and optimal respectively.

BM25-b=0.35 BM25-b=0.4 BM25-b=0.75 BM25-optimal b BM25QLEXP BM25QLLOG BM25QLREC

AP88-89
0.2854
0.2838
0.2720
0.2882 0.2871123 0.2867123 0.2865123

LA
0.2513
0.2494
0.2373
0.2519 0.250823 0.25251234 0.251223

WSJ
0.3298
0.3296
0.3090
0.3323 0.3320123 0.33351234 0.33291234

DISK1&2
0.2402
0.2396
0.2245
0.2402 0.24041234 0.24111234 0.24061234

DISK4&5
0.2258
0.2251
0.2163
0.2258 0.225523 0.22621234 0.22641234

ROBUST04
0.2510
0.2504
0.2397
0.2510 0.24973 0.250523 0.250623

WT2G
0.3139
0.3109
0.2632
0.3191 0.32041234 0.31961234 0.32031234

WT10G
0.2037
0.2006
0.1793
0.2050 0.19953 0.202923 0.202923

CRTER2 -b=0.35 CRTER2 -b=0.4 CRTER2 -b=0.75 CRTER2-optimal b
CRTER2EXP
CRTER2LOG
CRTER2REC

0.2923
0.2913
0.2823
0.2954 0.2942123 0.2936123 0.2939123

0.2528
0.2510
0.2462
0.2533 0.25501234 0.25491234 0.25491234

0.3472
0.3458
0.3282
0.3472 0.346123 0.34771234 0.347223

0.2457
0.2453
0.2303
0.2457 0.24721234 0.24731234 0.24721234

0.2320
0.2312
0.2226
0.2332 0.23441234 0.23341234 0.23371234

0.2574
0.2567
0.2453
0.2583 0.25931234 0.25841234 0.25901234

0.3300
0.3261
0.2861
0.3432 0.3416123 0.3369123 0.3377123

0.2137
0.2117
0.1857
0.2189 0.2153123 0.2145123 0.2145123

LDA-BM25-b=0.35 LDA-BM25-b=0.4 LDA-BM25-b=0.75 LDA-BM25-optimal b LDA-BM25QLEXP LDA-BM25QLLOG LDA-BM25QLREC

0.3158
0.3152
0.3105
0.3161 0.315823 0.315823 0.315723

0.2619
0.2605
0.2544
0.2622 0.26401234 0.26531234 0.26521234

0.3504
0.3503
0.3445
0.3504 0.35251234 0.35241234 0.35221234

0.2634
0.2634
0.2583
0.2634 0.26401234 0.26421234 0.26421234

0.2330
0.2326
0.2274
0.2332 0.23243 0.232823 0.232923

0.2640
0.2639
0.2588
0.2642 0.26303 0.26373 0.26373

0.3163
0.3148
0.2763
0.3222 0.32421234 0.32421234 0.32391234

0.2074
0.2042
0.1855
0.2099 0.206423 0.2077123 0.207023

4.3 Comparison with LDA-BM25
Finally, we further incorporate our proposed BM25QL models into state-of-the-art BM25-based model with topic modeling. Jian et al. [3] showed that LDA-BM25 is at least comparable to the state-ofthe-art model CRTER2. The performance of our proposed LDA-BM25QL and LDA-BM25 is presented in Figure 3. From Figure 3, we can find that LDA-BM25QL models are also comparable to LDA-BM25 with optimal b in MAP on almost all data sets. The performance is even better than searching the parameter space on several data sets, such as LA, WSJ, DISK1&2 and WT2G.
4.4 Analysis and Discussion
The experimental results show that our proposed models have consistent good performance in all scenarios on all data sets. In some occasions, the performance is even better than the heuristic best b-value. This is because that the new variable bQL is self-adjusted for each query, while given the heuristic b-value is tested for all queries on an entire collection. bQL is more adaptive, especially in real applications when the queries are quite different from each other. The functions proposed in Formula (3)-(5) perform similarly in terms of MAP. Although more functions could be considered to define the bQL, most of the functions grow faster than the logarithm function and slower compared with the exponential function. According to the experimental results, we can see that the retrieval performance can be guaranteed using any of the proposed functions in Formula (3)-(5).
5 CONCLUSIONS AND FUTURE WORK
In this paper, we propose a new term frequency normalization model BM25QL for probabilistic IR. Specifically, we present three term frequency normalization functions: logarithmic function, reciprocal function and exponential function. We also incorporate BM25QL into two state-of-the-art BM25-based models CRTER2 and LDA-BM25. Experimental results on eight standard TREC data sets show that BM25QL, CRTER2QL and LDA-BM25QL at least comparable to and sometimes even better than BM25, CRTER2 and LDA-BM25 with the optimal b in terms of MAP.
In the future, we will conduct experiments on more large data sets with different types, such as GOV2 and ClueWeb09. There are also several interesting future research directions for us to explore.

First, it is interesting to conduct an in-depth study on complete new term frequency normalization without hyper-parameters k1 and b. Second, we will investigate the optimal term frequency nor-
malization function. Third, we also plan to evaluate our models on
more data sets including some real data sets and apply our models
into real world applications.
ACKNOWLEDGMENTS
This research is supported by a Discovery grant from the Natural Sciences & Engineering Research Council (NSERC) of Canada, an Ontario Research Foundation award and also supported by the National Natural Science Foundation of China under grants number 61572223. We thank anonymous reviewers for their thorough comments, and greatly appreciate Dr. Xinhui Tu's help and support.
REFERENCES
[1] R. Cummins and C. O'Riordan. 2009. The Effect of Query Length on Normalisation in Information Retrieval. In Proc. of the 2009 AICS. 26­32.
[2] R. Cummins and C. O'Riordan. 2012. A Constraint to Automatically Regulate Document-length Normalisation. In Proc. of the 21st ACM CIKM. 2443­2446.
[3] F. Jian, J. X. Huang, J. Zhao, T. He and P. Hu. 2016. A Simple Enhancement for Ad-hoc Information Retrieval via Topic Modelling. In Proc. of the 39th ACM SIGIR. 733­736.
[4] B. He and I. Ounis. 2007. On Setting the Hyper-parameters of Term Frequency Normalization for Information Retrieval. ACM TOIS 25, 3 (2007), 13.
[5] J. X. Huang J. Zhao and B. He. 2011. CRTER: Using Cross Terms to Enhance Probabilistic IR. In Proc. of the 34th ACM SIGIR. 155­164.
[6] Y. Lv. 2015. A Study of Query Length Heuristics in Information Retrieval. In Proc. of the 24th ACM CIKM. 1747­1750.
[7] Y. Lv and C. Zhai. 2011. Adaptive Term Frequency Normalization for BM25. In Proc. of the 20th ACM CIKM. 1985­1988.
[8] Y. Lv and C. Zhai. 2011. Lower-bounding Term Frequency Normalization. In Proc. of the 20th ACM CIKM. 7­16.
[9] Y. Lv and C. Zhai. 2011. When Documents Are Very Long, BM25 Fails!. In Proc. of the 34th ACM SIGIR. 1103­1104.
[10] X. Huang S. Robertson S. Walker M. Beaulieu, M. Gatford and P. Williams. 1996. Okapi at TREC-5. In Proc. of the 5th TREC. 143­166.
[11] Jiaul H. Paik. 2013. A Novel TF-IDF Weighting Scheme for Effective Ranking. In Proc. of the 36th ACM SIGIR. 343­352.
[12] J.R. Wen R. Song, L. Yu and W.H. Hon. 2011. A Proximity Probabilistic Model for Information Retrieval. Tech. Rep., Microsoft Research (2011).
[13] C. Clarke S. Buttcher and B. Lushman. 2006. Term Proximity Scoring for Ad-hoc Retrieval on Very Large Text Collections. In Proc. of the 29th ACM SIGIR. 621 ­ 622.
[14] H. Zaragoza S. Robertson and M. Taylor. 2004. Simple BM25 Extension to Multiple Weighted Fields. In Proc. of the 13th ACM CIKM. 42­49.
[15] K.F. Wong K.L. Kwok T.L. Chung, R.W.P. Luk and D.L. Lee. 2006. Adapting Pivoted Document-length Normalization for Query Size: Experiments in Chinese and English. ACM TALIP 5, 3 (2006), 245­263.
[16] X. Wei and W. B. Croft. 2006. LDA-Based Document Models for Ad-hoc Retrieval. In Proc. of the 29th ACM SIGIR. 178­185.

1240

Short Research Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A User Study on Snippet Generation: Text Reuse vs. Paraphrases

Wei-Fan Chen1

Matthias Hagen2

Benno Stein1

1Bauhaus-Universität Weimar 2Martin Luther University Halle-Wittenberg <first>.<last>@uni-weimar.de matthias.hagen@informatik.uni-halle.de

Martin Potthast3
3Leipzig University martin.potthast@uni-leipzig.de

ABSTRACT
The snippets in the result list of a web search engine are built with sentences from the retrieved web pages that match the query. Reusing a web page's text for snippets has been considered fair use under the copyright laws of most jurisdictions. As of recent, notable exceptions from this arrangement include Germany and Spain, where news publishers are entitled to raise claims under a so-called ancillary copyright. A similar legislation is currently discussed at the European Commission. If this development gains momentum, the reuse of text for snippets will soon incur costs, which in turn will give rise to new solutions for generating truly original snippets. A key question in this regard is whether the users will accept any new approach for snippet generation, or whether they will prefer the current model of "reuse snippets." The paper in hand gives a first answer. A crowdsourcing experiment along with a statistical analysis reveals that our test users exert no significant preference for either kind of snippet. Notwithstanding the technological difficulty, this result opens the door to a new snippet synthesis paradigm.
ACM Reference Format: Wei-Fan Chen, Matthias Hagen, Benno Stein, and Martin Potthast. 2018. A User Study on Snippet Generation: Text Reuse vs. Paraphrases. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210149
1 INTRODUCTION
Snippets are an essential part of a search results page: they incite users to view (to click) or to skip viewing a retrieved document. Already in 1991, Pedersen, Cutting, and Tukey [15] proposed querybiased snippets, and they have proven useful until today [23, 25, 26]. The more surprising appears Google's recent decision to remove snippets altogether from its redesigned news portal, without offering any explanation.1 Recall that Google has notoriously been "questioning the unquestioned," subjecting virtually every detail of its search interfaces to A/B tests. If this policy has not been changed, can the redesigned news portal be interpreted as evidence that snippets are not so useful after all (in the domain of news search)? A more plausible explanation can be found in the changing interpretation of the copyright: publishers from all over the world are now raising claims for compensation for displaying text extracted from
1 www.blog.google/topics/journalism- news/redesigning- google- news- everyone
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210149

their news articles. Especially in Europe their lobbying for political support was successful: an ancillary copyright for news publishers, which basically exempts their intellectual property from fair use, has been passed into law in Germany and Spain, and it is currently discussed as part of an EU-wide copyright reform. In light of these developments, the removal of snippets from Google News appears as an act of anticipatory obedience.
Inasmuch as today's information economy on the web is financed by advertisements, the welfare of publishers partaking in this ecosystem depends on consumers visiting their web pages. This is also true for a large portion of the revenue of news publishers; some well-known publishers even stopped printing newspapers. It is hence no surprise that news publishers protect their online assets more fiercely than they used to do. News publishers form a well-organized business community with traditionally strong ties to politics and public opinion, yet, other online communities may follow. Even Wikipedia's contents are regularly lifted onto the search results pages of many search engines, depriving the encyclopedia of visitors, which may have contributed to the ongoing decline of active Wikipedia editors since 2007 [13, 21]. Commercial search engines, whose operations remained unchallenged in this respect for more than two decades, may therefore face a turn.
Are information scientists forced to pick a side? Probably not, since it is not our business to protect business models, neither that of search engines nor that of publishers. Rather, we should uphold the vision of developing the "perfect" information system, which is what the information society needs. When it comes to snippets, text reuse has been popular because it is easy. Looking forward, Potthast et al. [17] propose deep-learning-based text generation as a promising, yet more difficult alternative. Moreover, users' expectations may include reuse snippets. The paper in hand debunks this notion: we investigated the users' preferences, showing that, with some reservations, the majority of users does not prefer traditional reuse snippets over paraphrased versions, or vice versa.
2 RELATED WORK
Snippet generation is a variant of extractive summarization, where the summaries are biased toward queries. Luhn, the inventor of term frequency weighting, was one of the earliest contributors [2, 11]. Tombros and Sanderson [23] ascertained the importance that snippets relate to a user's query, while Brin and Page [3] implemented query-biased snippets for the first version of Google. White et al. [25, 26] found that snippets should be re-generated based on implicit relevance feedback when a user returns to a search results page. To speed up snippet generation, Turpin et al. [24] evaluate software architectures based on compressed data structures and RAM caching. Bando et al. [1] ask humans to manually create reuse snippets, comparing the results to machine-generated reuse snippets. They observe that in about 73% of cases humans select the same pieces of text as machines. Savenkov et al. [19] survey

1033

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Wei-Fan Chen, Matthias Hagen, Benno Stein, and Martin Potthast

approaches regarding the evaluation of snippet generation, suggesting automated evaluation approaches and A/B testing. Thomaidou et al. [22] consider the special case of snippet generation for ads that are shown on search results pages. Further research has been invested into studying how the length of snippets affects the perceived search result quality on desktops [9, 12] as well as on mobile devices where screen space is limited [10]. Eye-tracking studies have been conducted to determine to what parts of a search results page users pay most attention [5, 7]; unsurprisingly, snippets play a major role. Finally, reuse snippets are also generated in XML retrieval [8] and semantic web search [16].
The companion task to extractive summarization is abstractive summarization [6], where summaries are not restricted to text reuse. Though abstractive summarization is a long-standing task in the natural language generation community, it has not been considered for snippet generation yet. In their user study, Bando et al. [1] come close, using manually written non-reuse snippets as a gold standard to evaluate automatically generated reuse snippets. It was shown that humans pay attention to the same parts of a document when (1) manually composing a snippet compared to when (2) selecting sentences for a snippet. Recently, neural network models have made great progress toward the task of abstractive summary generation [4, 14, 18, 20], which renders snippet synthesis feasible if the lack of large-scale training data can be overcome.
3 USER STUDY DESIGN
Within three crowdsourcing tasks, we first acquired paraphrases of reuse snippets, and then experimentally determined which kind workers prefer when given a pair, and which kind is more useful to spot relevant results on a search results page.
3.1 Crowdsourcing Paraphrase Snippets
Given Bando et al.'s [1] insight of the high overlap of human reuse / paraphrase snippets to machine-generated reuse snippets, paraphrase snippets represent a sufficient substitute for true original snippets for our user study. To maximize diversity of the set of pairs of reuse snippets and corresponding paraphrase snippets, we resort to crowdsourcing. Using the 150 topics provided for the TREC Web tracks 2009­2011, each topic's query has been submitted to Google's custom search API2 to obtain high-quality search results. The top-5 search results of each query were collected, including title, URL, and Google's reuse snippet for a total of 750 snippets. Since Google's snippet generator sometimes shortens sentences to enforce a maximum snippet length (indicated by ellipses), we recovered the complete sentences from the linked pages. For crowdsourcing we relied on Amazon's Mechanical Turk (AMT), where we offered the task to manually paraphrase the reuse snippets collected. The worker instructions were to significantly rewrite a given snippet while maintaining its length and without removing important named entities, phrases, or quotes (e.g., "to be or not to be"). To foreclose easy cheating, copy and paste was disabled in the AMT interface. Each of the 750 reuse snippets was assigned to two different workers for paraphrasing (i.e., we repeated our experiment twice in a row to test its reliability). Submitted paraphrases were reviewed, rejecting those with lacking changes or poor grammar, resulting in 1,500 pairs of reuse and paraphrase snippets.
2 https://developers.google.com/custom-search/

3.2 Snippet Preference
To assess snippet preference, we recruit 5 workers for each pair of reuse / paraphrase snippets to judge which of the two they would prefer for the given query and web page. The task interface showed instructions, a search box with the topic's query, the pairs of snippets side by side, formatted like standard search results, the associated web page in a frame below, and a text field to enter an assessment justification. Workers were asked to assign one of four labels: snippet 1 or 2 is better, both are good, or both are bad. To avoid order bias, the positions of reuse and paraphrase snippets were randomized. After collecting all judgments, we tallied the scores as follows: a reuse or paraphrase snippet gets 1 point if a worker judged it to be better, both get a point if a worker judged that both are good, neither get a point otherwise.
The worker pool of AMT has been known to comprise dishonest workers, threatening the reliability of our study. We took several precautions: each worker judged at most two pairs of snippets ensuring diversity, and submissions were rejected if workers spent insufficient time, too much time, or if they failed to provide sensible explanations for their judgments, resulting in 4,235 individual workers and 7,500 accepted annotations. Only workers having at least 80% acceptance rate and at least 100 successful assignments were invited. Furthermore, we conducted control experiments with respect to the variables snippet source, preference bias, snippet length, and random pairings to check how workers are affected.
3.3 Snippet Usefulness
To obtain implicit feedback on a snippet's usefulness for spotting relevant search results, another group of workers judged the relevance of a search result to a query given different page configurations. The queries, corresponding web pages, and relevance scores were obtained from the topics used at the TREC Web tracks 2013­2014, which were based on the ClueWeb12. For each topic, we tried to collect 3 web pages judged as relevant and 3 judged as irrelevant that are still available on the live web and whose contents correspond to that found in the ClueWeb12.3 For 29 topics, we were able to collect the desired set of 6 web pages. Following the aforementioned procedures, we collected reuse snippets using Google's custom search API, and paraphrases of them via crowdsourcing.
Workers were then exposed to search results pages comprising 3 results (1) with reuse snippets, (2) with paraphrase snippets, (3) without snippets (only titles and URLs), (4) with reuse snippets only (no titles or URLs), or (5) with captcha-style snippets to ensure workers read the snippets. In the latter case, the snippets just stated whether a result was supposed to be relevant or irrelevant. A search results page could contain 0 up to 3 relevant web pages. For mixtures of relevant and irrelevant pages, we tested all permutations of search result orderings. For each ordering, three workers provided labels, yielding a total of 10,440 annotations (29 topics, 8 relevance settings (0­3 results relevant), 5 snippet conditions (reuse, paraphrase, etc.), 3 results per search results page, and 3 annotators each). Each worker judged search results pages of 5 different topics based on the given information. To ensure annotation quality, we rejected results from workers who did not pass the captcha snippets, resulting in 546 individual workers in this experiment.
3Topics from the previous TREC Web tracks were omitted, since they are based on the ClueWeb09 which is insufficiently represented on today's web.

1034

Short Research Papers I A User Study on Snippet Generation: Text Reuse vs. Paraphrases

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

Table 1: Left: Distribution of judgments; 1,500 pairs of (reuse, paraphrase) snippets at 5 assessors each yield 7,500 judgments. Middle: Average scores (number of votes) of reuse and paraphrase snippets with p values for a paired t-test, bold font indicating significance (p < 0.05); the two row groups correspond to two repetitions of the experiment. Right: Average scores of pairs of snippets grouped by different aspects, with associated p values and one row group per experiment repetition.

Assessment

Judgments

absolute relative

Reuse better Paraphrase better Both good Both bad

2,731 2,652 1,537
580

36.41% 35.36% 20.49% 7.74%

Total

7,500 100.00%

Experiment Reuse Paraphrase p-value

all

3.06

2.97

0.51

Wikipedia

3.31

2.58

0.00

Non-Wikipedia 2.75

2.85

0.31

all

3.05

2.94

0.43

Wikipedia

3.18

2.64

0.01

Non-Wikipedia 2.77

2.82

0.58

Experiment (par. = paraphrase, re. = reuse) (·, ) ( ,·) p-value

(better, worse)

3.91 1.71 0.00

((better-par., worse-re.), (better-re., worse-par.)) 2.85 2.78 0.41

(long, short) ((long-par., short-re.), (long-re., short-par.))

2.91 2.70 0.01 2.82 2.81 0.90

(better, worse)

3.89 1.75 0.00

((better-par., worse-re.), (better-re., worse-par.)) 2.87 2.78 0.23

(long, short) ((long-par., short-re.), (long-re., short-par.))

2.99 2.61 0.00 2.79 2.83 0.60

4 USER STUDY ANALYSIS
We conducted a careful statistical analysis of the crowdsourced snippet judgments. The snippet preference experiment rests on the hypothesis that users do not consciously care whether or not snippets reuse content from linked web pages as long as they are semantically equivalent. If true, there should be no statistically significant difference in terms of user preference. The snippet usefulness experiment rests on the hypothesis that users are not unconsciously negatively affected by paraphrase snippets. If true, users identify relevant web pages either way and there should be no statistically significant differences between the two kinds of snippets.
4.1 Descriptive Statistics
The reuse snippets collected comprise an average of 1.9 sentences and 41.1 words; the longest snippet has 6 sentences and 122 words, the shortest one 1 sentence and 14 words. The paraphrase snippets comprise an average 2.2 sentences and 40.5 words; the longest snippet has 6 sentences and 132 words, the shortest one 1 sentence and 9 words. The paraphrase snippets are significantly longer at the sentence level (p < 0.05), but neither are significantly shorter nor longer at the word level (p > 0.05). A reason might be that workers tended to split long sentences while paraphrasing. The workers spent an average 220.5 seconds to paraphrase a snippet at a maximum of 895 seconds (38 words) and a minimum of 14 seconds (also 38 words), an average of 49 seconds to judge a pair of snippets, and of 17 seconds to judge a web page's relevance when viewing a search results page. The inter-annotator agreement Fleiss  for the snippet preference experiment, presuming the four labels to be independent, was 0.37, indicating a fair agreement, and 0.77 for the snippet usefulness experiment, indicating a substantial agreement.
4.2 Snippet Preference
Judgment distribution. Table 1 (left) shows the distribution of judgments. Recall that workers were unaware which snippet is which; their judgments were mapped to the ground truth afterwards. The amounts of judgments for Reuse better and Paraphrase better are roughly equal and about a quarter of workers had no preference (Both good plus Both bad). Only 580 pairs of snippets (7.74%) were judged Both bad, showing a high overall snippet quality.
Reuse snippets vs. paraphrase snippets. To check for snippet preferences, we performed a paired t-test on the pairs of reuse and paraphrase snippets. Since we repeated our experiment, collecting two different paraphrase snippets for each of the 750 reuse snippets,

we can attest that our results can be replicated under the same conditions: the rows all in Table 1 (middle) show the results for the two repetitions. While the absolute average scores (number of votes) achieved by paraphrase snippets are slightly smaller than those of reuse snippets, no statistically significant difference was measured, given pretty high p values of 0.51 and 0.43, respectively.
Wikipedia snippets vs. non-Wikipedia snippets. From the 750 search results, 260 refer to Wikipedia articles. Considering only this subset, the rows Wikipedia in Table 1 (middle) show that users significantly prefer reuse snippets over paraphrases, which is not the case for non-Wikipedia results. The effect sizes under Cohen's d are small to medium (0.51 and 0.31, respectively). Upon review of Wikipedia snippet pairs, many of the reuse snippets have an a-priori high writing quality, and it may have been difficult for the average AMT worker to compete with that.
Preferred snippets vs. unpreferred snippets. To quantify the difference between snippets preferred by users (better) to those not preferred (worse), we reordered the snippet pairs accordingly, disregarding ties, and then applied a paired t-test. The rows (better, worse) in Table 1 (right) show the results for each repetition of our experiment. As can be seen, there is an average 2.2 score difference between them, rendering the differences significant. However, when comparing the groups of snippet pairs (better-reuse, worseparaphrase) with (better-paraphrase, worse-reuse), p-values of 0.41 and 0.23 indicate that snippet preference is independent of whether they are reused or paraphrased.
Long snippets vs. short snippets. We further investigated if snippet length affects preference (rows (long, short) of Table 1 (right). A snippet belongs to the "long" snippets if it is the longer one of a pair, and to "short" snippets otherwise. On average, the long snippets have 44.7 words and the short snippets have 36.7 words. Our findings corroborate those of Maxwell et al. [12], namely that users prefer longer snippets. In fact, many of the assessment justifications from our workers support this finding. Again, when comparing the groups of snippet pairs (long-paraphrase, short-reuse) with (long-reuse, short-paraphrase), p-values of 0.90 and 0.60 indicate that the dimensions length and reuse are independent.
Reuse snippets vs. unrelated snippets. As a control experiment to ascertain worker diligence, pairs of reuse snippets and unrelated snippets were shown to workers, where a given reuse snippet was paired with a random reuse snippet of a different web page of a different query. Of 1,500 judgments collected, workers preferred the snippet matching the query in 85% of the cases, confirming this experiment's setup validity.

1035

Short Research Papers I SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA Wei-Fan Chen, Matthias Hagen, Benno Stein, and Martin Potthast

Table 2: F-scores of the snippet usefulness experiment indicating whether annotators correctly spot relevant search results under different search results page snippet conditions.

Reuse Paraphrase No snippet Snippet only Random

F-score 67.64

64.61

63.65

60.16

50.00

4.3 Snippet Usefulness
This experiment questioned whether users can identify relevant pages given different kinds of snippets--one of the key tasks snippets should support. A search result is labeled as relevant if more than half of the workers label it as relevant, and irrelevant otherwise. In Table 2 we show the F-score of the crowdsourced judgments based on snippets compared with the ground truth relevance labels obtained from the TREC assessors. The numbers of overall shown relevant and irrelevant web pages were balanced, such that random guessing yields a baseline F-score of 50%. In the captcha-style setting where the snippets just explicitly state that a web page is relevant / irrelevant, the workers achieved an F-score of 100% (since we excluded those who did not succeed in these check instances). As for the other snippet conditions, we find that although reuse snippets achieve the highest F-score (helping users best to judge a result's relevance), the performance of paraphrase snippets is not significantly worse (p = 0.28). Showing only reuse snippets (without titles or URLs) achieves the lowest F-score; no snippets (only title and URL) is better than showing only snippets, confirming that title and URL do play an important role. All settings are significantly better than random guessing. Otherwise, only reuse snippets are significantly better than showing only snippets (p < 0.05). The remaining pairings are not significantly different, corroborating that paraphrase snippets are no worse than reuse snippets.
We conclude that the combination of snippet, title, and URL is crucial to identify relevant web pages on search results pages, regardless of whether snippets are reused or paraphrased. Nevertheless, there is room for improvement given the results obtained from showing the "perfect" snippet, which reveals the relevance of a web page (our captcha setup). The finding that both reuse and paraphrase snippets are useful supports our claim that paraphrase snippets can replace reuse snippets in future information systems.
4.4 Reproducibility
User studies need to be reproduced in order to determine whether the results of previous studies on a given problem of interest generalize and that they were not due to unidentified confounding variables or accidental flaws in the study setup. This pertains particularly to first-time studies, since only an independent reproduction will provide for sufficient confidence that the results obtained are valid and that they may generalize. Since our study is the first of its kind that provides an answer to the question whether reusing text for snippet generation is a necessity, or whether also paraphrased snippets are sufficient, we expect that sooner or later it will have to be reproduced for its results to be corroborated. The reproducibility of a user study rests with a clear description of its setup, which we tried our best to provide. But maybe even more so it rests with access to data, code, and supplementary material that was gathered throughout the study. To ensure the reproducibility of our results, we provide all data collected and the associated code open source.4
4 https://github.com/webis- de/SIGIR- 18

5 CONCLUSION AND FUTURE WORK
Our user study shows that reuse snippets have no significant ad-
vantage over paraphrase snippets, so that, if done right, snippet
synthesis without relying on text reuse will not result in a worse
user experience. However, generating coherent snippets, even if
"just" paraphrasing reuse snippets, is still beyond today's text gen-
eration capabilities. Even deep learning cannot yet be applied out
of the box, since the substantial amounts of training data required
are still missing. In future work, we will focus on compiling a suit-
able training dataset. Regarding the political debate on whether
reuse snippets should be regulated, it can already be said that if
the pressure on commercial search engines is further increased, the
demand for snippet synthesis technology may soon outweigh the
costs of developing it. By the time new copyright laws are enacted,
new technologies may have already superseded them.
REFERENCES
[1] L. L. Bando, F. Scholer, and A. Turpin. 2010. Constructing Query-biased Summaries: A Comparison of Human and System Generated Snippets. In Proc. of IIiX. 195­204.
[2] P. B. Baxendale. 1958. Machine-Made Index for Technical Literature - An Experiment. IBM Journal of Research and Development 2, 4 (1958), 354­361.
[3] S. Brin and L. Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks 30, 1-7 (1998), 107­117.
[4] S. Chopra, M. Auli, and A. M. Rush. 2016. Abstractive Sentence Summarization with Attentive Recurrent Neural Networks. In Proc. of NAACL/HLT.
[5] E. Cutrell and Z. Guan. 2007. What are you Looking for?: An Eye-tracking Study of Information Usage in Web Search. In Proc. of CHI. 407­416.
[6] M. Gambhir and V. Gupta. 2017. Recent Automatic Text Summarization Techniques: A Survey. Artificial Intelligence Review 47, 1 (2017), 1­66.
[7] L. A. Granka, T. Joachims, and G. Gay. 2004. Eye-tracking Analysis of User Behavior in WWW Search. In Proc. of SIGIR. 478­479.
[8] Y. Huang, Z. Liu, and Y. Chen. 2008. Query biased Snippet Generation in XML Search. In Proc. of SIGMOD. 315­326.
[9] M. Kaisser, M. A. Hearst, and J. B. Lowe. 2008. Improving Search Results Quality by Customizing Summary Lengths. In Proc. of ACL. 701­709.
[10] J. Kim, P. Thomas, R. Sankaranarayana, T. Gedeon, and H. Yoon. 2017. What Snippet Size is Needed in Mobile Web Search?. In Proc. of CHIIR 2017. 97­106.
[11] H. P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2, 2 (1958), 159­165.
[12] D. Maxwell, L. Azzopardi, and Y. Moshfeghi. 2017. A Study of Snippet Length and Informativeness: Behaviour, Performance and User Experience. In Proc. of SIGIR. 135­144.
[13] C. McMahon, I. Johnson, and B. Hecht. 2017. The Substantial Interdependence of Wikipedia and Google: A Case Study on the Relationship Between Peer Production Communities and Information Technologies. In Proc. of ICWSM.
[14] R. Nallapati, B. Zhou, C. Nogueira dos Santos, Ç. Gülçehre, and B. Xiang. 2016. Abstractive Text Summarization using Sequence-to-Sequence RNNs and Beyond. In Proc. of CoNLL.
[15] J. Pedersen, D. Cutting, and J. Tukey. 1991. Snippet Search: A single phrase approach to text access. In Proc. of the 1991 Joint Statistical Meetings.
[16] T. Penin, H. Wang, T. Tran, and Y. Yu. 2008. Snippet Generation for Semantic Web Search Engines. In Proc. of ASWC. 493­507.
[17] M. Potthast, W. Chen, M. Hagen, and B. Stein. 2018. A Plan for Ancillary Copyright: Original Snippets. In Proc. of NewsIR, Vol. 2079. CEUR-WS.org, 3­5.
[18] A. M. Rush, S. Chopra, and J. Weston. 2015. A Neural Attention Model for Abstractive Sentence Summarization. In Proc. of EMNLP.
[19] D. Savenkov, P. Braslavski, and M. Lebedev. 2011. Search Snippet Evaluation at Yandex: Lessons Learned and Future Directions. In Proc. of CLEF 2011. 14­25.
[20] A. See, P. J. Liu, and C. D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proc. of ACL.
[21] B. Suh, G. Convertino, E. H. Chi, and P. Pirolli. 2009. The singularity is not near: slowing growth of Wikipedia. In Proc. of WikiSym.
[22] S. Thomaidou, I. Lourentzou, P. Katsivelis-Perakis, and M. Vazirgiannis. 2013. Automated Snippet Generation for Online Advertising. In Proc. of CIKM. 1841­1844.
[23] A. Tombros and M. Sanderson. 1998. Advantages of Query Biased Summaries in Information Retrieval. In Proc. of SIGIR. 2­10.
[24] A. Turpin, Y. Tsegay, D. Hawking, and H. E. Williams. 2007. Fast Generation of Result Snippets in Web Search. In Proc. of SIGIR. 127­134.
[25] R. White, I. Ruthven, and J. M. Jose. 2002. Finding Relevant Documents Using Top Ranking Sentences: An Evaluation of Two Alternative Schemes. In Proc. of SIGIR. 57­64.
[26] R. White, I. Ruthven, and J. M. Jose. 2002. The Use of Implicit Evidence for Relevance Feedback in Web Retrieval. In Proc. of ECIR. 93­109.

1036

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Theoretical Analysis of Interdependent Constraints in Pseudo-Relevance Feedback

Ali Montazeralghaem
Center for Intelligent Information Retrieval,
University of Massachusetts Amherst montazer@cs.umass.edu

Hamed Zamani
Center for Intelligent Information Retrieval,
University of Massachusetts Amherst zamani@cs.umass.edu

Azadeh Shakery
School of ECE, College of Engineering University of Tehran, and
School of Computer Science, Institute for Research in Fundamental Sciences
shakery@ut.ac.ir

ABSTRACT
Axiomatic analysis is a well-defined theoretical framework for analytical evaluation of information retrieval models. The current studies in axiomatic analysis implicitly assume that the constraints (axioms) are independent. In this paper, we revisit this assumption and hypothesize that there might be interdependence relationships between the existing constraints. As a preliminary study, we focus on the pseudo-relevance feedback (PRF) models that have been theoretically studied using the axiomatic analysis approach. In this paper, we introduce two novel interdependent PRF constraints which emphasize on the effect of existing constraints on each other. We further modify two state-of-the-art PRF models, log-logistic and relevance models, in order to satisfy the proposed constraints. Experiments on three TREC newswire and web collections demonstrate that the proposed modifications significantly outperform the baselines, in all cases.
ACM Reference Format: Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery. 2018. Theoretical Analysis of Interdependent Constraints in Pseudo-Relevance Feedback. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210156
1 INTRODUCTION
Many information retrieval (IR) models consist of heuristic components and/or make several simplifying assumptions that are not necessarily correct. Axiomatic analysis [1, 6, 11] provides a welldefined theoretic structure in order to study IR models analytically, which often lead to empirical improvements. In the axiomatic analysis framework, a number of constraints, also called axioms, are defined and IR models are designed or modified to satisfy these constraints. Previous work on axiomatic analysis literature assumes that axioms are independent of each other, and thus the models are studied given each axiom, separately.
A part of this work was done while Ali Montazeralghaem was a Master student at the University of Tehran.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5657-2/18/07. . . $15.00 https://doi.org/10.1145/3209978.3210156

In this paper, we revisit this assumption and hypothesize that there might be an interdependence relationship between different axioms. As a preliminary study, we focus on analyzing the pseudorelevance feedback (PRF) models. PRF is a well-known strategy to address the vocabulary mismatch problem in information retrieval (IR). PRF assumes that the top retrieved documents in response to an initial query are relevant to the query and uses these documents for estimating a more accurate query model. Previous works have shown that PRF models can benefit from axiomatic analysis. For instance, Clinchant and Gaussier [4] proposed five constraints for PRF models and showed that even state-of-the-art PRF models do not satisfy all of the constraints. Following their work, a number of other constraints have been proposed and several modifications have been suggested for PRF models, e.g., see [2, 8­10].
In this paper, we propose two interdependent constraints. The first constraint takes the interdependence relationship of the term frequency (TF) and the inverse document frequency (IDF) into account. "TF effect" and "IDF effect", that are two existing constraints respectively corresponding to TF and IDF, have been previously considered independently [4]. Our first constraint shows the effect of IDF on the TF constraint. With a similar idea, our second constraint focuses on the relationship between term frequency and document relevance scores. In fact, this constraint indicates that the influence of term frequency on the feedback weight should depend on the relevance score of the documents containing the term.
Furthermore, we study two state-of-the-art PRF models, the loglogistic feedback model [3] and the relevance model [7], and modify these two models in order to satisfy the proposed interdependent constraints. The empirical evaluation on three TREC collections demonstrates the effectiveness of each of the proposed constraints. Our modifications to the log-logistic and relevance models lead to significant improvements in all collections. We believe that, our findings open up a new research direction by considering the interdependence relationship between constraints when studying IR models in different tasks.
2 METHODOLOGY
Several theoretic constraints have been proposed for pseudo-relevance feedback. Previous work [4, 8, 10] demonstrated that satisfying these constraints leads to significant improvements in retrieval performance. In this section, we propose two novel constraints for PRF models, which focus on the interdependence of the existing constraints. In the following subsections, we first propose two interdependent constraints and further modify two state-of-the-art PRF models in order to satisfy the proposed constraints.

1249

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Notation. Let FW (w; F , Pw , q) be a real-valued feedback weight function that assigns a weight to each candidate term w for a given query q. F and Pw denote the set of feedback documents for the query q and a set of term-dependent parameters, respectively. Let RS(d, q) denote the relevance score of document d to query q and SRS(w, F ) denote the sum of relevance scores of the feedback documents containing the term w. For simplicity, we use FW (w), RS(d) and SRS(w), hereafter. In the following equations, t f (w, d) and id f (w), respectively denote term frequency and inverse document frequency. The notation | · | represents the length of the given
query/document or the size of the given set.

2.1 Interdependent Constraints

In this subsection, we introduce two interdependent constraints for
pseudo-relevance feedback models. [TF-IDF effect] Let w1 and w2 be two vocabulary terms, such
that id f (w1) > id f (w2) and SRS(w1) = SRS(w2). Assume that there exists a document d  F , where t f (w1, d) = t f (w2, d) > 0 and FW (w1; F \{d }) = FW (w2; F \{d }). In this case, if we add both terms to this document such that t f (w1, d) = t f (w2, d) = t f (w1, d) + 1 and FW (w1) and FW (w1) be updated weights, then we will have:

FW (w1) - FW (w2) > FW (w1) - FW (w2)

(1)

Formally writing, for all candidate feedback terms w, the following constraints should be satisfied:

2FW (w) t f (w, d).id f

(w )

>

0

The intuition behind this constraint is that increasing the term frequency of common terms (low id f ) should have less impact on the feedback weight, compared to the rare terms (high id f ). From the information theory perspective, if we add a rare term to a document, we provide more information, compared to the scenario of adding a common term. Consequently, the difference of feedback weight between two terms caused by discrimination value of them, should increase when these two terms are added to the feedback document. This shows the interdependence between term frequency and inverse document frequency that have been considered as independent axioms in prior work [4].
[TF-SRS effect] Let w1 and w2 be two vocabulary terms, such that id f (w1) = id f (w2) and SRS(w1) > SRS(w2). Let d  F be a feedback document, where t f (w1, d) = t f (w2, d) > 0. In this case, if we add both terms to the document, i.e., t f (w1, d) = t f (w2, d) = t f (w1, d) + 1, then we will have:

FW (w1) - FW (w2) > FW (w1) - FW (w2)

(2)

which can be formally formulated as:

2FW (w) t f (w, d).SRS(w)

>

0

where w is a candidate feedback term. In other words, according to the relevance effect constraint [2, 10], the terms appearing in the documents with higher relevance scores should get higher feedback weights. The TF-SRS constraint indicates that increase in frequency of the words with higher SRS(w) values should lead to higher feedback weights. The intuition behind this idea can be mapped to the one proposed for the previous constraint.

2.2 Modifying the Log-Logistic Model

The log-logistic (LL) feedback model [3] is a state-of-the-art PRF

model that has been shown to outperform several PRF models, in-

cluding the geometric relevance model [12] and the mixture model

[13]. The feedback weight function in the log-logistic model is

defined as follows:

FWLL(w) =

1 |F |

FWLL(w, d) =

d F

1 |F |

d F

t(w, d) + log( w

w

)

(3)

where w

=

Nw N

is the fraction of the documents that contain the

term w in the whole collection. Also, t(w, d) = t f (w, d) log(1 +

c

av l |d |

)

represents

a

term

frequency

function

normalized

by

the

document length, where avl denotes the average document length

and c is a free hyper-parameter. A modification of the log-logistic

model has been proposed recently [8], called LLR, in order to satisfy

the "relevance effect" constraint [10]:

FWLLR (w)

=

1 |F |

FWLL(w, d)  RS(Q, d)

(4)

d F

where RS denotes the relevance score function. We focus on this

model since it significantly outperforms the original log-logistic

model [8]. This model satisfies "TF-IDF effect" because:

2FWLLR (w) t f (w, d).id f (w)

=

(

1 w

log(1

+

c

av l |d |

)

log(1

+

c

av l |d |

)t

f

(w,

d

)

+

1)2

>

0

(5)

Although this approach satisfies the first proposed constraint,

there is only a light interdependence relationship between t f and

id f . In other words, the derivative of the LLR weight function with

respect to t f (w, d), which is given as follows, does not have a strong

dependence to id f :

FWLLR (w) t f (w, d)

=

1 w

1 w

log(1

+

c

av l |d |

)

log(1

+

c

av l |d |

)t

f

(w

,

d

)

+

1

(6)

As shown in the above equation, if we omit 1 from the denom-

inator, the derivative becomes independent from id f . We believe

that increasing this derivative with respect to the id f could lead

to higher performance. To overcome this issue, we re-write this

function as follows:

FWT F -I DF (w)

=

1 |F |

d F

t (w, d)A(w)

log(

w

+ w

)

(7)

where

A(w )

=

log(

1 w

)

=

log(

N Nw

)

(8)

Now the derivative of this function is:

FWT F -I DF (w) t f (w, d)

=

1 w

log(1

+

c

av l |d |

)A(w)t f (w, d)(A(w)-1)

1 w

log(1

+

c

av l |d |

)t f (w, d)A(w)

+1

(9)

According to the above equation, the derivative of FWT F -I DF is more dependent to id f , and when we omit 1 from the denominator

the derivative nearly will be A(w) that is stronger from Equation (6)

for satisfying TF-IDF constraint. Figure 1 illustrates the correlation

between id f and derivative of LLR and LLR+TF-IDF with respect

to term frequency (Equations (6) and (9)). Based on this plot, LLR

weight function has a light interdependence relationship between

t f and id f whereas LLR+TF-IDF improves it.

1250

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

It can be shown that

2 FWLLR (w ) t f (w,d).S RS (w )

=

0, which means that

the log-logistic models (both LL and LLR) do not satisfy the "TF-SRS

effect" constraint. Hence, we modify the weight function as follows

in order to satisfy this constraint:

FWT

F -S RS (w)

=

Com(w ,

F)

1 |F |

d

F

t(w, d) + log( w

w

)

(10)

where Com(w, F ) is defined as follows:

Com(w, F ) =

d F RS(d)  I (w, d) d F RS(d)

(11)

where I (w, d) denotes the occurrence of term w in document d. Hence, the modified log-logistic model is:

FWAl l

(w )

=

Com(w ,

F)

1 |F |

d

F

t (w, d)A(w)

log(

w

+

w

)

(12)

2.3 Modifying the Relevance Model

In this subsection, we focus on RM3, a well-known and state-ofthe-art variant of the relevance models proposed by Lavrenko and Croft [7]. The feedback weight of each term w for a given query Q in RM3 is computed as follows:

FWRM3(w) = p(w |Q) =

p(w, Q) p (Q )

 p(w, Q) = p(w, q1, ..., qk )

(13)

where qi denotes the ith query term. Clinchant and Gaussier [4] showed that relevance model does
not satisfy IDF effect, and this means that this model cannot satisfy TF-IDF effect. This model also does not have any component of SRS and therefore cannot satisfy TF-SRS effect.
We re-write this function by bringing the feedback set into the feedback weight formula as follows:

p(w |Q, F )  p(w, Q |F ) = p(w, q1, ..., qk |F )

= p(D)p(w, q1, ..., qk |D, F ) (14)
D F
The conditional independence assumption of terms leads to the following equation:

k

p(w, q1, ..., qk |D, F ) = p(w |D, F ) p(qi |D, F )

(15)

i =1

We estimate each of the conditional as follows:

p(w

|D,

F

)

=

p(D|w, F )p(w |F )p(F ) p(D|F )p(F )

(16)

Under the assumption of independence of a feedback document given a term from the whole feedback set, we have:

p(w |D, F ) =

p(D|w)p(w |F ) wi D p(D |wi )p(wi |F )

(17)

where p(D|w) can be

computed by Bayes rule p(D|w)

=

p(w |D)p(D) p(w )

where we consider p(D) uniform for all documents and p(w |D) is

computed using the maximum likelihood estimation: p(w |D) =

t

f

(w, D |D |

)

.

Figure 1: Correlation between id f and the derivative of LLR and LLR+TF-IDF formulas with respect to T F (Eq. (6) and (9)).
To satisfy the proposed constraints, we estimate p(w |F ) as follows:
p(w |F )  RS(D)  I (w, D)
D F
which shows how this term appears in the relevant documents. p(w) can also be estimated using the document frequency of the term in the whole collection, which indicates how common the term is. Other computation details are the same as the original relevance models [7].
It can be proved that the proposed modification to RM3 satisfies both TF-IDF and TF-SRS constraints as well as the IDF effect constraint proposed in [4]. We call this model RM3+ALL.
3 EXPERIMENTS
3.1 Collections and Experimental Setup
We used three standard TREC collections in our experiments: AP (Associated Press 1988-89, TREC topics 51-200), Robust (TREC Robust Track 2004 collection, TREC topics 301-450 & 601-700) and WT10g (TREC Web Track 2001-2002, TREC topics 451-550). The first two collections are homogeneous collections containing news articles, while the third collection is a heterogeneous collection containing web pages. The WT10g collection is noisier than the newswire collections.
All documents are stemmed using the Porter stemmer and stopped using the standard INQUERY stopword list. 1
3.1.1 Parameter Setting. The number of feedback documents, the number of feedback terms, the feedback coefficient and the parameter c in the Log-logistic model are set using 2-fold crossvalidation over the queries of each collection. We sweeped the number of feedback documents between {10, 25, 50, 75, 100}, feedback terms between {50, 100, · · · , 300}, the feedback coefficient between {0, 0.1, · · · , 1}, and the parameters c between {2, 4, 6, 8, 10}.
3.1.2 Evaluation Metrics. We use mean average precision (MAP) of the 1000 top-ranked documents as the main metric to evaluate the retrieval effectiveness. We also report the precision of the top 10 retrieved documents (P@10). Furthermore, we consider the robustness index (RI) [5] to evaluate the robustness of methods. Statistically significant differences of performances are determined using the two-tailed paired t-test computed at a 95% confidence level over average precision per query.
3.2 Results and Discussion
3.2.1 Evaluating the Modified Log-Logistic Model. Baselines: (1) the document retrieval model without feedback (NoPRF) computed
1The experiments were carried out using the Lemur toolkit (http://lemurproject.org/)

1251

Short Research Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: Performance of the proposed modifications and the baselines. Superscripts 0/1/2/3/4/6 denote that the MAP improvements over NoPRF/LL/LLR/LLR+TF-IDF/LLR+TF-SRS/RM3 are statistically significant.

Method

AP MAP P@10 RI

Robust MAP P@10 RI

WT10g MAP P@10 RI

NoPRF
LL LLR LLR+TF-IDF LLR+TF-SRS LLR+ALL
RM3 RM3+ALL

0.2642
0.33790 0.34090 0.34530,1,2 0.34700,1,2 0.34900,1,2,4
0.33920 0.34600,6

0.4260 ­
0.4648 0.17 0.4668 0.19 0.4675 0.20 0.4702 0.21 0.4735 0.20
0.4561 0.17 0.4695 0.19

0.2490
0.28160 0.29090,1 0.29560,1,2 0.29650,1,2 0.29860,1,2
0.29190 0.29670,6

0.4237 ­
0.4385 0.32 0.4442 0.36 0.4490 0.37 0.4418 0.39 0.4478 0.37
0.4322 0.24 0.4462 0.25

0.2080
0.2127 0.22990,1 0.23700,1 0.23710,1 0.24010,1,2
0.22130 0.22970,6

0.3030 ­
0.3156 0.11 0.3339 0.19 0.3269 0.19 0.3298 0.24 0.3359 0.17
0.3166 0.12 0.3177 0.20

by query likelihood, (2) the original log-logistic feedback model (LL) [3], and (3) the modified log-logistic model (LLR) [8] that satisfies the relevance effect constraint.
To study the effect of each constraint in the retrieval performance, we modify the log-logistic model based on each constraint, separately. Equations (7) and (10) are the feedback weight functions that satisfy the TF-IDF and the TF-SRS constraints, respectively. We also modify the log-logistic model by considering all of these constraints, called LL+ALL (see Equation (12)). The results obtained by the baselines and those achieved by the proposed modifications are reported in Table 1. LL outperforms the NoPRF baseline in all cases, which indicates the effectiveness of the log-logistic model and validates the findings presented in prior work [3, 8]. Both LLR+TFIDF and LLR+TF-SRS outperform all three baselines (NoPRF, LL, and LLR), which demonstrates the effectiveness of the proposed constraints. The improvements achieved by LLR+TF-SRS is higher than those obtained by LLR+TF-IDF. This indicates that the interdependence between t f and SRS which models the local importance of the term is more effective than the interaction between t f and id f which models the global importance of the term. In addition, LLR+TF-SRS and LLR+TF-IDF are shown to be more robust than the baselines in all the collections. LLR+TF-SRS performs more robust compared to LLR+TF-IDF, in all the collections, especially the web collection. The MAP improvements for both LLR+TF-IDF and LLR+TF-SRS methods are close to each other in all the collections specially in the noisy collection (WT10g) which shows that both of these two constraints are important for PRF methods.
LLR+All that satisfies both constraints outperforms the baselines. These improvements are statistically significant in all cases.
3.2.2 Evaluating the Modified Relevance Model. In this set of experiments, we consider two baselines: (1) the document retrieval method without feedback (NoPRF) computed by query likelihood, and (2) the RM3 method [7]. Since the proposed approach satisfies both constraints simultaneously (we do not propose different solutions for satisfying different axioms), we only report the results for RM3+ALL. As shown in Table 1, RM3+ALL significantly outperforms both baselines (NoPRF and RM3) in all collections, in terms of MAP. The P@10 values achieved by the proposed method is also higher than those achieved by the baselines, in all cases. This shows that our modification to RM3 leads to better performance by satisfying the proposed constraints. Our modification also makes the relevance model more robust, especially in the web collection.

4 CONCLUSIONS AND FUTURE WORK
In this paper, we proposed two novel constraints for pseudo-relevance feedback models, which focus on the interdependence relation of the existing constraints that have been previously considered as independent. To show the importance of these interdependent constraints, we studied and modified two state-of-the-art pseudorelevance feedback models; the log-logistic and the relevance models. Our evaluation on three standard newswire and web collections investigates the effect of each of these constraints on the overall effectiveness and robustness of the models. Our modifications lead to significant improvements over the baselines in all the collections. These observations suggest that taking the interdependence relationship of the constraints into account might lead to designing more accurate IR models, which could be an interesting research direction for future work.
Acknowledgements. This work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.
REFERENCES
[1] M. Ariannezhad, A. Montazeralghaem, H. Zamani, and A. Shakery. Improving retrieval performance for verbose queries via axiomatic analysis of term discrimination heuristic. In SIGIR '17, pages 1201­1204. ACM, 2017.
[2] M. Ariannezhad, A. Montazeralghaem, H. Zamani, and A. Shakery. Iterative Estimation of Document Relevance Score for Pseudo-Relevance Feedback. In ECIR '17, 2017.
[3] S. Clinchant and E. Gaussier. Information-based Models for Ad Hoc IR. In SIGIR '10, pages 234­241, 2010.
[4] S. Clinchant and E. Gaussier. A Theoretical Analysis of Pseudo-Relevance Feedback Models. In ICTIR '13, pages 6­13, 2013.
[5] K. Collins-Thompson. Reducing the Risk of Query Expansion via Robust Constrained Optimization. In CIKM '09, pages 837­846, 2009.
[6] H. Fang, T. Tao, and C. Zhai. A formal study of information retrieval heuristics. In SIGIR '04, pages 49­56, 2004.
[7] V. Lavrenko and W. B. Croft. Relevance Based Language Models. In SIGIR '01, pages 120­127, 2001.
[8] A. Montazeralghaem, H. Zamani, and A. Shakery. Axiomatic Analysis for Improving the Log-Logistic Feedback Model. In SIGIR '16, pages 765­768, 2016.
[9] A. Montazeralghaem, H. Zamani, and A. Shakery. Term proximity constraints for pseudo-relevance feedback. In SIGIR '17, pages 1085­1088. ACM, 2017.
[10] D. Pal, M. Mitra, and S. Bhattacharya. Improving Pseudo Relevance Feedback in the Divergence from Randomness Model. In ICTIR '15, pages 325­328, 2015.
[11] R. Rahimi, A. Shakery, and I. King. Axiomatic analysis of cross-language information retrieval. In CIKM '14, pages 1875­1878. ACM, 2014.
[12] J. Seo and W. B. Croft. Geometric Representations for Multiple Documents. In SIGIR '10, pages 251­258, 2010.
[13] C. Zhai and J. Lafferty. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01, pages 403­410, 2001.

1252

Demonstration Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A2A: Benchmark Your Clinical Decision Support Search

Sarvnaz Karimi
CSIRO Data61 Marsfield, NSW, Australia sarvnaz.karimi@csiro.au

Vincent Nguyen
CSIRO Data61 Marsfield, NSW, Australia vngu4919@uni.sydney.edu.au

Falk Scholer
RMIT University Melbourne, VIC, Australia falk.scholer@rmit.edu.au

Brian Jin
CSIRO Data61 Marsfield, NSW, Australia
brian.jin@csiro.au
ABSTRACT
Clinical Decision Support (CDS) systems aim to assist clinicians in their daily decision-making related to diagnosis, tests, and treatments of patients by providing relevant evidence from the scientific literature. This promise however is yet to be fulfilled, with search for relevant literature for a given patient condition still being an active research topic. The TREC CDS track was designed to address this research gap. We developed a platform to facilitate experimentation and hypothesis testing for information retrieval researchers working on this topic. It provides a large range of query and document processing techniques that are explored in the biomedical search domain.
CCS CONCEPTS
· Information systems  Computing platforms; Decision support systems; Evaluation of retrieval results; · Applied computing  Health informatics;
KEYWORDS
Clinical Decision Support; Experimentation; Search; Reproducibility
ACM Reference Format: Sarvnaz Karimi, Vincent Nguyen, Falk Scholer, Brian Jin, and Sara Falamaki. 2018. A2A: Benchmark Your Clinical Decision Support Search. In SIGIR '18: The 41st International ACM SIGIR Conference on Research and Development in Information Retrieval, July 8­12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3209978.3210166
1 INTRODUCTION
While there are a number of search engines for biomedical literature-- such as PubMed, askMEDLINE, PICO1[12], or OVID2--such search
1 https://pubmedhh.nlm.nih.gov/nlmd/pico/piconew.php 2 http://www.ovid.com
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210166

Sara Falamaki
CSIRO Data61 Marsfield, NSW, Australia
sara.falamaki@csiro.au
in a clinical setting is much less advanced [10]. Specific to the clinical setting is the need for finding evidence that answers questions on testing, diagnosis, and treatment given a patient's conditions.
For three years, from 2014 to 2016, TREC challenged the Information Retrieval (IR) community with the Clinical Decision Support (CDS) track [9, 11, 13]. Similar to many other tasks, the experimental setup included indexing a large set of documents, in this case PubMed Central (PMC) articles; processing the topics to create queries that potentially retrieve relevant articles; running the search; and evaluating the results when relevance judgements were released. Teams often start with typical query and document processing techniques from within the biomedical search community, including concept extraction using Metamap, negation detection and removal, or query expansion using in-domain and generic resources such as Wikipedia. All the setup could easily take weeks, before even a simple experiment can be executed. Our system, Apples to Apples (A2A), provides a user-friendly platform for IR researchers to run many of the most common experiments in a very short amount of time, with flexibility to alter different search parameters. This in turn will translate to (1) reduce the need to re-implement the most common query and document processing methods; and (2) allow fair comparison of these methods on a unified platform. These hopefully lead to advances in the field of search for clinical decision support and ultimately evidence-based medicine, by reducing the need for every researcher to set up similar experiments in their own lab.
2 DEMONSTRATION SYSTEM
Our platform enables researchers to perform experiments within the TREC CDS framework, providing an index of PubMed Central (PMC) full-text articles. It allows experiments that include one or more of following methods or parameters to be defined (Figure 2):
Topics: Users may choose to use the TREC Clinical Decision Support topics from 2014-16 (already in the system), or upload their own topics in the format of the TREC Clinical Decision Support topics. Similar to the TREC CDS settings, they can construct queries using the description, summary or note3 section of each topic. A sample topic from 2016 is shown in Figure 1.
Query Expansion: Users can expand the queries using (1) UMLS concepts; (2) word embeddings created using MEDLINE, Wikipedia or PMC; and (3) Pseudo Relevance Feedback
3Notes were only introduced in 2016.

1277

Demonstration Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

<topic number="27" type="treatment"> <note> 96F found unresponsive on ground at nursing home. Pt was in dining room and found by staff. Unresponsive for 1 min after found. Pt cannot recollect events preceding fall but with some c/o HA and some neck/shoulder discomfort. Taken to [**Hospital1 1218**] where NCHCT preformed at 18:32 showed 9mm L parietal SDH. C-spine negative. Family / Social history: dementia, HTN, afib, CAD SURGICAL Hx: unknown. SOCIAL Hx: Daughter serves as HCP; Pt currently DNR/DNI except for elective procedure (****SEE CLARIFICATIOIN BELOW****).. ALLERGIES: NKDA Physical Examination General Appearance: No acute distress, Thin Eyes / Conjunctiva: PERRL, Conjunctiva pale Head, Ears, Nose, Throat: Normocephalic, Poor dentition Lymphatic: Cervical WNL, Supraclavicular WNL Cardiovascular: (S1: Normal), (S2: Normal) Peripheral Vascular: (Right radial pulse: Present), (Left radial pulse: Present), (Right DP pulse: Diminished), (Left DP pulse: Diminished) Respiratory / Chest: (Expansion: Symmetric), (Breath Sounds: Clear : bialterally) Abdominal: Soft, Non-tender, Bowel sounds present Extremities: Right: Absent, Left: Absent Skin: Warm Neurologic: Attentive, Follows simple commands, Responds to: Verbal stimuli, Oriented (to): A+O x 2, Movement: Not assessed, Tone: Not assessed Imaging: CT head w/o contrast Acute left subdural hematoma measuring 1.5 cm maximal dimensions with leftward subfalcine herniation of 8 mm, downward transtentorial herniation with obliteration of the left suprasellar cistern, and uncal herniation. No fx, destructive infiltrative lesion involving the skull base </note> <description> A 96 y/o female found unresponsive on ground at nursing home. Pt was in dining room and found by staff. Unresponsive for 1 min after found. Pt cannot recollect events preceding fall but with some c/o HA and some neck/shoulder discomfort. NCHCT showed 9mm L parietal SDH. C-spine negative. Imaging: CT head w/o contrast Acute left subdural hematoma measuring 1.5 cm maximal dimensions with leftward subfalcine herniation of 8 mm, downward transtentorial herniation with obliteration of the left suprasellar cistern, and uncal herniation. No fx, destructive infiltrative lesion involving the skull base. </description> <summary> A 96 y/o female found unresponsive on ground at nursing home pressents with headache, herniation, and some neck/shoulder discomfort. CT head shows acute left subdural hematoma. </summary> </topic>
Figure 1: A sample topic (clinical note, description, and summary) from TREC CDS 2016. Clinical note is long and it contains abbreviations and incomplete sentences.
(PRF). They can decide on the weight of the expanded terms (default is 1.0). For PRF, the number of documents to be considered for expansion, and number of top terms to be added, can be specified. Negation Detection in Topics: We provide two negation handling options: (1) removal of negated words from topics only, and (2) hyphenating negations through the Solr hyphenation filter. Hyphenation ensures that documents that contain the negated terms are completely irrelevant and cannot be in the final retrieved list. There are two approaches implemented for negation detection: NegEx [2] or CLPsych [3]. NegEx is the most popular negation detection method and is integrated with Metamap. Originally, it was proposed to detect negated findings or diseases mentioned within narrative medical reports. More recently, CLPsych was introduced for identifying mentions of suicidality in mental health records and it was shown to perform more effectively than NegEx. Demographic Normalisation in Topics: Using some heuristics, the system can normalise the mentions of demographic attributes of patients. For example, 86 y/o m is replaced with elderly male (similar to Karimi et al. [5, 6]). This aims to increase the chances of matching queries with what is typically written in scientific articles.

Figure 2: Register a search request page.
Ranking Methods: The system uses the Okapi BM25 ranking function as provided by Solr 6.6.0. Users can adjust the parameters of the BM25 ranking function. The default values are set to b = 0.75 and k1 = 1.2.
Facet Search: Users can run queries on particular facets of the scientific articles (title, abstract, their UMLS concepts, MeSH headings or body). This option allows filtering facets in the index. Users can also set a weight for these facets in the positive decimal range (minimum of zero). The default weight on all facets is 1.0.
UMLS concepts of topics or document titles and abstracts extracted using Metamap4 can also be optionally included. We set Metamap to extract concepts with the following semantic types: Disease or Syndrome, Sign or Symptom, Pathologic Function, Diagnostic Procedure, Anatomical Abnormality, Laboratory Procedure, Pharmacologic Substance, Neoplastic Process, and Therapeutic or Preventive Procedure. These are chosen to match the query types (diagnosis, treatment, or test).
Once users choose their experimental setting, they can submit the job. These jobs will be assigned a job identifier, queued in the system, and executed in turn. Since some of the settings such as running negation detection could take time, the system is equipped with email notification (Figure 4). Upon completion of an experiment, the user is notified. For each experiment, the results package includes the following files: (1) queries as submitted to the search engine (one could potentially modify these queries as needed and resubmit to the system for further investigation); (2) search results in
4MetaMapLite 3.6, https://metamap.nlm.nih.gov/MetaMapLite.shtml

1278

Demonstration Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 3: Results page listing all the logged jobs and their status.
Figure 4: An example of email notification.
Solr format; (3) search results in TREC format (only includes document ids); (4) TREC evaluation results of trec_eval5 (includes R-Prec, B-Pref, and P@10); and (5) TREC evaluation results of sample-eval (includes infNDCG and infAP). If users choose to use their own topics, they have the option to pick which year's relevance judgements (qrels) they would like their job to be evaluated on.
Figure 5 shows the A2A system in its software architecture level. It consists of a user interface module, job controller on SpringMVC, restful API on Python Tornado, and a search and evaluation core. An embedded SQLite is used for a job repository, and the user interfaces are on Bootstrap V4 and jQuery. In the search and evaluation core, Metamap is used to extract UMLS concepts from queries (base query plus expanded query terms (for example Wikipedia word embeddings). EdisMax Parser is an Extended Dismax Parser from Solr which handles boosting factors as weights. We used the tokeniser and porter stemmer from NLTK.6 The output from the Solr search engine is then evaluated by trec_eval and sample-eval. Since the order of execution for these modules is decided by what options are chosen by the users, we do not show any arrows inside the search and evaluation module. A rough process is that a raw topic is passed to Metamap, NLTK, and query expansion (PRF and Word embeddings) modules; then back to NLTK and Metamap; and finally to EdisMax Parser. The resulting topic is queried over the index using the Solr search engine and then retrieved results are evaluated.
5 http://trec.nist.gov/trec_eval/ 6 http://www.nltk.org/

3 COMPARISON AND EVALUATION
There exist a number of public search engines, such as PubMed, JournalWatch, ClinicalTrials.gov and eMedicine, that facilitate search over biomedical literature, clinical trials, or clinical articles. They often allow for different types of queries, from simple keyword search to advanced search which is mostly Boolean. These search engines however are for public use and do not allow alterations to the search settings for IR research. That is, they control how user queries are reformulated (or not) and how the index is searched. Our system however, is designed for researchers, providing a number of options for query reformulations and filtering for its users. In particular, we were interested in re-implementing most popular methods utilised by the TREC CDS participants as stated in their reports, especially for higher scoring systems such as MerckKGaA [4].
There are also some research-oriented prototypes that are proposed in academic papers, however sometimes they are not made available to the community. The most relevant system proposed recently is by Koopman et al. [7] where a task-based search engine is proposed to assist in clinical search. It indexes the documents together with their UMLS concepts extracted using Metamap, with their mapping then being related to one of the three types of tasks: diagnosis, treatment or test. Their search results were tailored to these three types of tasks as well allowing their users to explore the retrieved documents based on their potential need. Our system however does not focus on search result representation at this stage.
Another existing platform is EvALL [1] which allows outputs of different systems to be evaluated against each other, allowing a fair comparison amongst systems. It also provides an evaluation platform where, once output of a system is uploaded, an evaluation report using different metrics is generated.
Our system, allows researchers to systematically examine the effect of different query processing methods through turning on and off various techniques. For example, one experiment could be evaluating the effect of negation detection in queries. Results for such experiments are listed in Table 1. This is for running 2016 summary and clinical notes topics with the following settings:
Baseline: No negation handling. Plain BM25 on all the articles. Negex, Remove: Negated terms identified using NegEx and
then removed. Negex, Hyphenate: Negated terms identified using NegEx
and then hyphenated. CLPpsych, Remove: Negated terms identified using CLPpsych
and then removed. CLPpsych, Hyphenate: Negated terms identified using CLP-
psych and then hyphenated.
The results in Table 1 show the positive effect that handling negation had on the summarised topics and how the four alterations also had different effects on four metrics. On clinical notes we observed the opposite effect of the summaries, given they are much harder text to process for any negation detection tool. One could then experiment with adding more topic or document processing options, such as query expansion using word embeddings created using MEDLINE. A more complete set of experiments that our system facilitates can be found in Nguyen et al. [8].7
7Note that some of experimental setup, such as learning-to-rank, is yet to be added to the online system.

1279

Demonstration Papers I

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Figure 5: A2A system architecture.

Run

infNDCG infAP P@10 R-Prec

Summary No negation Negex, Remove Negex, Hyphenate CLPpsych, Remove CLPpsych, Hyphenate

0.1721 0.1726 0.1754 0.1787 0.1822

0.0158 0.0159 0.0162 0.0163 0.0168

0.2067 0.2067 0.2100 0.2233 0.2200

0.1167 0.1171 0.1162 0.1174 0.1148

Clinical notes No negation Negex, Remove Negex, Hyphenate CLPpsych, Remove CLPpsych, Hyphenate

0.1080 0.1102 0.1151 0.1087 0.0818

0.0079 0.0088 0.0084 0.0082 0.0048

0.1400 0.1467 0.1500 0.1200 0.1100

0.0749 0.0772 0.0654 0.0748 0.0447

Table 1: A usecase of the platform: examining the effect of negation detection.

4 SUMMARY AND FUTURE DIRECTION
Our experimentation framework, A2A, creates an opportunity for researchers to run a large set of experiments in a short amount of time. Most importantly, it provides a uniform platform to run many of the existing methods explored independently by different researchers all in one setting, and therefore enables reproducibility. This ensures a fair comparison amongst the methods from the literature as well as novel settings not reported in past work. We therefore expect this system to help expedite research in clinical information retrieval.
The choice of implemented methods in this version of A2A was largely based on the popularity and success of the techniques reported in the TREC CDS. There are a number of extensions underway for this system, including the addition of learning-to-rank methods which incorporate query types (diagnosis, test, and treatment) as their features. Adding the option of choosing UMLS concepts, which should be included in the index or query expansion, is also under consideration. Processing clinical notes to normalise the text to what would be found in the literature is another avenue to explore. That is, for example, to make available the option of expanding abbreviations. Another extension for the platform is including statistical analysis over users' selected runs (e.g., paired

t-test for statistical significance). We also intend to extend this platform to allow for experiments on the TREC Precision Medicine track which started on 2017.
ACCESSING THE SYSTEM
The A2A system is available to the public at the following hyperlink: https://www.vizie.csiro.au/trec-eval. Users can create an account or use guest as username and password. If the guest account is used, no confirmation email can be issued.
Acknowledgement. The funding for this project is provided through Decision Sciences Program at CSIRO Data61. The authors would like to thank Cecile Paris and Stephen Wan for their support.
REFERENCES
[1] E. Amigó, J. Carrillo-de Albornoz, M. Almagro-Cádiz, J. Gonzalo, J. RodríguezVidal, and F. Verdejo. 2017. EvALL: Open Access Evaluation for Information Access Systems. In SIGIR. 1301­1304.
[2] W. Chapman, W. Bridewell, P. Hanbury, G. Coopera, and B. Buchanan. 2001. A Simple Algorithm for Identifying Negated Findings and Diseases in Discharge Summaries. J Biomed Inform. 34, 5 (2001), 301­310.
[3] G. Gkotsis, S. Velupillai, A. Oellrich, H. Dean, M. Liakata, and R. Dutta. 2016. Don't Let Notes Be Misunderstood: A Negation Detection Method for Assessing Risk of Suicide in Mental Health Records. In CLPsych at NAACL. San Diego, CA, 95­105.
[4] H. Gurulingappa, A. Bauer, L. Toldo, C. Schepers, and G. Megaro. 2016. SemiSupervised Information Retrieval System for Clinical Decision Support. In TREC. Gaithersburg, MD.
[5] S. Karimi, S. Falamaki, and V. Nguyen. 2016. CSIRO at TREC Clinical Decision Support Track. In TREC. Gaithersburg, MD.
[6] S. Karimi, D. Martinez, S. Ghodke, L. Zhang, H. Suominen, and L. Cavedon. 2011. Search for Medical Records: NICTA at TREC 2011 Medical Track. In TREC. Gaithersburg, MD.
[7] B. Koopman, G. Zuccon, and J. Russell. 2017. A Task-oriented Search Engine for Evidence-based Medicine. In SIGIR. Shinjuku, Tokyo, Japan, 1329­1332.
[8] V. Nguyen, S. Karimi, S. Falamaki, and C. Paris. 2018. Benchmarking Clinical Decision Support Search. (2018). https://arxiv.org/abs/1801.09322
[9] K. Roberts, D. Demner-Fushman, E. Voorhees, and W. Hersh. 2016. Overview of the TREC 2016 Clinical Decision Support Track. In TREC. Gaithersburg, MD.
[10] K. Roberts, M. Simpson, D. Demner-Fushman, E. Voorhees, and W. Hersh. 2016. State-of-the-art in Biomedical Literature Retrieval for Clinical Cases: A Survey of the TREC 2014 CDS Track. Inf Retr. 19, 1-2 (2016), 113­148.
[11] K. Roberts, M. Simpson, E. Voorhees, and W. Hersh. 2015. Overview of the TREC 2015 Clinical Decision Support Track. In TREC. Gaithersburg, MD.
[12] C. Schardt, M. Adams, T. Owens, S. Keitz, and P. Fontelo. 2007. Utilization of the PICO framework to improve searching PubMed for clinical questions. BMC Med Inform Decis Mak. 7, 16 (2007).
[13] M. Simpson, E. Voorhees, and W. Hersh. 2014. Overview of the TREC 2014 Clinical Decision Support Track. In TREC. Gaithersburg, MD.

1280

Demonstration Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

A System for Efficient High-Recall Retrieval

Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D. Smucker, Gordon V. Cormack, and Maura R. Grossman
University of Waterloo, Ontario, Canada {m2abuals,nghelani,haotian.zhang,mark.smucker,gvcormac,maura.grossman}@uwaterloo.ca

ABSTRACT
The goal of high-recall information retrieval (HRIR) is to find all or nearly all relevant documents for a search topic. In this paper, we present the design of our system that affords efficient highrecall retrieval. HRIR systems commonly rely on iterative relevance feedback. Our system uses a state-of-the-art implementation of continuous active learning (CAL), and is designed to allow other feedback systems to be attached with little work. Our system allows users to judge documents as fast as possible with no perceptible interface lag. We also support the integration of a search engine for users who would like to interactively search and judge documents. In addition to detailing the design of our system, we report on user feedback collected as part of a 50 participants user study. While we have found that users find the most relevant documents when we restrict user interaction, a majority of participants prefer having flexibility in user interaction. Our work has implications on how to build effective assessment systems and what features of the system are believed to be useful by users.
CCS CONCEPTS
· Information systems  Information retrieval;
KEYWORDS
High-Recall; Electronic Discovery; Systematic Review
ACM Reference Format: Mustafa Abualsaud, Nimesh Ghelani, Haotian Zhang, Mark D. Smucker, Gordon V. Cormack, and Maura R. Grossman. 2018. A System for Efficient High-Recall Retrieval. In SIGIR '18: The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, July 8­ 12, 2018, Ann Arbor, MI, USA. ACM, New York, NY, USA, 4 pages. https: //doi.org/10.1145/3209978.3210176
1 INTRODUCTION
The objective of high-recall information retrieval (HRIR) is to find all or nearly all relevant documents. Example applications of HRIR include electronic discovery (eDiscovery), systematic review and construction of information retrieval test collections.
Baseline Model Implementation (BMI) ­ a version of CAL (AutoTAR) ­ is the state-of-the-art approach to high-recall retrieval [2, 3, 5]. CAL uses a machine learning model with an iterative feedback process to retrieve documents. Our work builds on BMI.
Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '18, July 8­12, 2018, Ann Arbor, MI, USA © 2018 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5657-2/18/07. https://doi.org/10.1145/3209978.3210176

Figure 1: A high-level view of our platform architecture.
We demonstrate a platform for retrieving and assessing relevant documents that provides high data processing performance and a user-friendly document assessment interface. The platform is designed to increase assessors' assessment performance and thus reduce their review effort.
The performance of the platform was evaluated by 50 participants in a user experiment. In another work [10], we report on the design of user study and show preliminary results from the first 10 participants. In this paper, we show the data-processing performance of our platform, how it works, and how it is used and experienced by users.
2 SYSTEM ARCHITECTURE
Figure 1 shows the architecture of our platform. Our platform is composed of two different retrieval methods, CAL and Search. CAL automatically presents the user with documents based on a machine learning model, whereas Search allows users to enter queries and retrieve lists of documents. The platform server, as shown in the Figure 1, is responsible for displaying the document assessment interfaces to the user, configuring the system, and communicating requests and responses to and from each component. The types of user interactions that the server allows are:
· Set or change the topic of interest; · Set seed query for building CAL's initial machine learning
model; · Search for specific documents using Search; · Assess documents retrieved by either Search or CAL; and

1317

Demonstration Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Topic Description
Clicking on the text will display a pop-up of the topic statement of what is considered relevant. Users can also create their own topics and their statements.
(b) Navigation Buttons
Users can click on the magnifying icon to switch to the Search component or the light bulb icon to move to the CAL component. The archive icon takes the user to a page that contains a list of all judgments made, where users can export their judgments to different formats.
(c) Document Title
The title of the document from the data collection
(d) Paragraph Excerpt
A selected paragraph by the modified CAL model. The model selects the most-likely relevant paragraph from the document as a summary of the document.
(e) Show full document
Users can view the full document content if they wish. The content will be displayed below the paragraph.

(f) Highlight keywords
Users can highlight keywords by entering them in the search bar.
(g) Judging buttons
Three buttons corresponding to 3-level relevance scale, not relevant, relevant and highly relevant
(h) Keyboard shortcuts
Users can make their judgments using any of the pre-defined keyboard shortcuts.
(i) Undo Judgments
A list of all previously made judgments from the CAL interface. Users can modify their existing judgments by clicking on any of document titles. Once clicked, the document will be displayed again to the user.

Figure 2: The continuous active learning (CAL) user interface.

· Export judgments sets.
All components in the architecture are stand-alone and interact with each other via HTTP API. For example, any search engine can be added to our system with minimal effort. This design also adds flexibility to these components and permits their use in different applications. For instance, the platform server was also used in the TREC 2017 RTS track evaluation for collecting judgments of tweets from the assessors [6]. The CAL component also has a command line interface through which various simulation experiments can be conducted.
The platform server is built using Django, a Python web framework. CAL and Search are written in C++. The source code is publicly available1.
3 PLATFORM COMPONENTS & FEATURES
Our implementation of continuous active learning (CAL) is based on the Baseline Model Implementation (BMI)2, which served as the baseline method in the TREC Total Recall Track 2015 and 2016 [4, 7]. BMI provides an iterative feedback process and uses a logistic regression classifier trained on relevance judgments from assessors. BMI uses this classifier to present the assessor with top scoring unjudged documents in the collection. After each iteration of judging and re-training, the learning model improves and returns the next most likely relevant documents to the user. We modified BMI as follows:
1 https://cs.uwaterloo.ca/~m2abuals/code/SIGIR2018/ 2 http://cormack.uwaterloo.ca/trecvm/

· Paragraph-based: A single paragraph is usually short and can contain enough material for a document to be assessed as relevant. Assessing the relevance of a document using a single paragraph can increase the total number of relevant documents found within a limited time and reduce review effort [10, 11]. Our CAL implementation retrieves the next likely relevant paragraph and presents it to the user for assessment, along with the option to view the full document content.
· Frequency of model retraining: The original algorithm processes relevance feedback in batches of size k, where k increases exponentially. Although having an increasing batch size reduces computation cost, it can delay the classifier from exploiting the potential benefits of newer relevance judgments. We made several improvements to the original algorithm to allow relevance feedback to be efficiently processed after each single judgment.
To meet the performance requirements of our modified algorithm, we implemented BMI in C++. In addition to using efficient data structures and parallel operations, we store all document and paragraph representations in memory. This enables quick training and scoring of documents/paragraphs. For the training of our logistic regression classifier, we used Sofia-ML3. We modified the Sofia-ML library to remove unnecessary features and the performance costs associated with them.
We compared our implementation by simulating the original BMI algorithm on the TREC 2017 Common Core test collection [1]. With around 1.8 million documents in the collection, the original
3 https://code.google.com/archive/p/sofia-ml/

1318

Demonstration Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

(a) Topic Description
Clicking on the text will display a pop-up of the topic statement of what is considered relevant.
(b) Navigation buttons
To navigate between Search, CAL, or archive to export all judgments.
(c) Search bar
The title of the document from the data collection. Users are also able to specify phrases ("new york") or require words (+france) in their search result.
(d) Relevance Indicator
An indicator of the relevance judgment made by the user for the document. Any document that has been judged by either Search or CAL will have this vertical bar indicator.

(e) Number of Results
The number of search results returned by the search engine. Users can choose to return 10, 20, 50, or 100 documents per query.
(f) Judging buttons
Three buttons corresponding to 3-level relevance scale, not relevant, relevant and highly relevant.

(g) Show full document
Users can view the full document content by clicking on the search result. The full document content will be displayed in a popup.
Figure 3: The search engine user interface.

(h) Highlight keywords
Users can highlight keywords by entering them in the search bar.
(i) Keyboard shortcuts
Users can make their judgments using any of the pre-defined keyboard shortcuts.

implementation took an average of 294.3 seconds to train the model and score all the documents. In contrast, our implementation took an average of 3.5 seconds (1 thread) and 1.6 seconds (8 threads). It should be noted that the original implementation loads all the document features from disk every time the classifier is trained, while our implementation loads it once and keeps it in memory for subsequent use. This one-time loading cost in our implementation was 31.1 seconds, which is still significantly faster than the original implementation.
We also measured the performance of our implementation by simulating the modified algorithm on the same collection. The training and scoring of around 30 million paragraphs took an average of 2.1 seconds after every relevance judgment. Since this process causes a noticeable delay for users, we immediately present the next paragraph based on the previous scores while the new scores are being generated by the model. To reduce latency for the user, the browser front-end caches the top 10 highest-scoring paragraphs received from the CAL component. After a user makes a judgment, we immediately present the user with the next paragraph in the cache. The cache is flushed and updated every time it receives a new batch of paragraphs from the CAL component.
The modifications and the improvements we made increased the responsiveness of our system and removed any perceptible interface lag.
Figure 3 shows the interfaces of the Search component. We implemented the search component using Indri [9], but our platform is designed such that any search engine can be easily integrated. Our

platform interacts with the search engine via HTTP API. Provided a search query, the platform expects the search engine to respond with a ranked list of documents with their summaries.
In order to help assessors find relevant documents easily, we included features that we found helpful as part of our own usage with a prototype version of our system. The features and their description are shown in Table 1. Screenshots of the CAL interface and features are shown in Figures 2 and 3.
4 USER STUDY DETAILS
We conducted a controlled user study to measure the performance our system. After receiving ethics approval from our university, we recruited 50 participants. The primary purpose of the study was to measure user performance with four variants of the system. A secondary purpose was to collect user feedback regarding the system and its features.
4.1 Corpus and Topics
We used the 50 NIST topics and the corpus provided by the TREC 2017 Common Core Track [1]. The corpus is The New York Times Annotated Corpus [8], which contains over 1.8 million news articles.
4.2 System Variants
The participants experienced all variations of our system during the study. For each variation, the participant spent 1 hour using

1319

Demonstration Papers II

SIGIR'18, July 8-12, 2018, Ann Arbor, MI, USA

Table 1: A list of features participants rated.

Feature Keyword Highlighting Judgment Shortcuts Search Interface Topic Description Undo Judgments Full Document Content Advance Search

Description Keyword search within a document or paragraph Keyboard shortcuts for submitting relevance judgments Ability to use a search engine to find documents in addition to the learning interface Display of topic statement of what is considered relevant Ability to review recent judgments and change judgment Ability to view a full document rather than merely a paragraph summary For the search engine, the ability to specify phrases ("new york") or require words (+france)

Example Figure 2f, 3h Figure 2h, 3i Figure 3 Figure 2a, 3a Figure 2i Figure 2e Figure 3c

Percentage

70

Very Useless

Somewhat Useless

60

Neutral

Somewhat Useful

50

Very Useful

40

30

20

10

0 Keyword HighJluigdhgtminegnt ShorStceuatrsch InterTfaocpeic DescripUtinodno JudgmFenutllsDoc ContAednvtance Search

Figure 4: Percentage of user preference for different system features.

Table 2: Percentage of participants preferring a given system variant.

Treatments
CAL-P CAL-D CAL-P&Search CAL-D&Search

Percentage of participants
16% 26% 10% 48%

our system to find as many relevant documents as they could for a given topic. The system variations were:
· CAL-P: CAL component with just paragraph summary. Search component is not enabled.
· CAL-D: CAL component with paragraph summary and option to view the full document. Search component is not enabled.
· CAL-P&Search: CAL-P with Search component enabled. · CAL-D&Search: CAL-D with Search component enabled.
5 DISCUSSION
In this section, we compare the user performance and user feedback on different variants of our systems. At the conclusion of the study, we asked participants which system variant they preferred the most. 48% of study participants preferred CAL-D&Search over the more restrictive variants. Our participants want full control of a highly interactive system, but we found that performance is highest when their interactions are limited to producing relevance judgments on paragraph length excerpts.

Finally, we asked participants for their feedback on each of the system features in Table 1. We used a 5-point scale to rate each feature. The results are shown in Figure 4. The keyword highlighting feature is the most popular among all features, with 86% of users indicating that it was somewhat useful or very useful.
6 CONCLUSION
In this paper, we described the design of an efficient high-recall information retrieval system. The system allows for both iterative relevance feedback and search, and these components can be easily replaced with different implementations. We found that while participants preferred a system that gives them the flexibility to view full documents and interactively search, actual user performance was maximized when we limited interaction to submitting relevance judgments on paragraph length excerpts.
ACKNOWLEDGMENTS
This work was supported in part by the Natural Sciences and Engineering Research Council of Canada (Grants CRDPJ 468812-14, RGPIN-2017-04239, and RGPIN-2014-03642), in part by a Google Founders' Award, and in part by the University of Waterloo.
REFERENCES
[1] James Allan, Evangelos Kanoulas, Dan Li, Christophe Van Gysel, Donna Harman, and Ellen Voorhees. 2017. TREC 2017 Common Core Track Overview. In TREC.
[2] Gordon V Cormack and Maura R Grossman. 2014. Evaluation of machine-learning protocols for technology-assisted review in electronic discovery. In SIGIR.
[3] Gordon V. Cormack and Maura R. Grossman. 2015. Autonomy and Reliability of Continuous Active Learning for Technology-Assisted Review. CoRR abs/1504.06868 (2015).
[4] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. 2016. TREC 2016 Total Recall Track Overview. In TREC.
[5] Maura R Grossman, Gordon V Cormack, and Adam Roegiest. 2017. Automatic and Semi-Automatic Document Selection for Technology-Assisted Review. In SIGIR.
[6] Jimmy Lin, Salman Mohammed, Royal Sequiera, Luchen Tan, Nimesh Ghelani, Mustafa Abualsaud, Richard McCreadie, Dmitrijs Milajevs, and Ellen Voorhees. 2017. Overview of the TREC 2017 Real-Time Summarization Track. In TREC.
[7] Adam Roegiest, Gordon V Cormack, Maura R Grossman, and C. L. A. Clarke. 2015. TREC 2015 Total Recall Track Overview. In TREC.
[8] Evan Sandhaus. 2008. The New York Times Annotated Corpus. (October 2008). LDC Catalog No.: LDC2008T19, https://catalog.ldc.upenn.edu/ldc2008t19.
[9] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Croft. 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. Amherst, MA, USA, 2­6.
[10] Haotian Zhang, Mustafa Abualsaud, Nimesh Ghelani, Angshuman Ghosh, Mark D. Smucker, Gordon V. Cormack, and Maura R. Grossman. 2017. UWaterlooMDS at the TREC 2017 Common Core Track. In TREC.
[11] Haotian Zhang, Gordon V Cormack, Maura R Grossman, and Mark D Smucker. 2018. Evaluating Sentence-Level Relevance Feedback for High-Recall Information Retrieval. arXiv (2018).

1320


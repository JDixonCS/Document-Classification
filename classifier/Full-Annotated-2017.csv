sentence,label,data,regex
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
On the Benefit of Incorporating External Features in a Neural Architecture for Answer Sentence Selection,1,corpora,True
"Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, W. Bruce Cro ",0,,False
"RMIT University, Melbourne, Australia University of Massachuse s, Amherst, MA, USA",0,,False
"{ruey-cheng.chen,evi.yulianti,mark.sanderson}@rmit.edu.au,cro @cs.umass.edu",0,,False
ABSTRACT,0,,False
"Incorporating conventional, unsupervised features into a neural architecture has the potential to improve modeling e ectiveness, but this aspect is o en overlooked in the research of deep learning models for information retrieval. We investigate this incorporation in the context of answer sentence selection, and show that combining a set of query matching, readability, and query focus features into a simple convolutional neural network can lead to markedly increased e ectiveness. Our results on two standard question-answering datasets show the e ectiveness of the combined model.",1,corpora,True
CCS CONCEPTS,0,,False
·Information systems  Information retrieval; estion answering; ·Computing methodologies  Neural networks;,0,,False
KEYWORDS,0,,False
"Answer sentence selection, external features, convolutional neural networks",0,,False
"ACM Reference format: Ruey-Cheng Chen, Evi Yulianti, Mark Sanderson, W. Bruce Cro . 2017. On the Bene t of Incorporating External Features in a Neural Architecture for Answer Sentence Selection. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080705",1,corpora,True
1 INTRODUCTION,1,DUC,True
"Deep learning approaches have recently become a central methodology in the research of question answering (QA). Many recent a empts in this area have focused on utilizing neural architectures, such as convolutional neural networks (CNN) [8, 19], long shortterm memory networks [12], or a ention mechanisms [15, 18], to explicitly model high-level question-answer structures. ese advances outperform conventional approaches, which are based on engineered heuristics. However, whether deep learning will completely remove the need for such features remains an open question.",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080705",1,ad,True
"e lack of understanding as to whether the features are needed in a neural architecture is what we address in this paper. While the development of new models for question answering has been moving rapidly ahead in the past few years, a wealth of proven useful results from prior art [2, 9, 10, 16] were usually ignored. Recently, there has been some evidence pointing a value in using features [11, 19] in neural network models. Commonly used neural network substructures, such as multilayer perceptrons [3], have the capability to combine a large number of external features, so incorporating all available signals in a neural network could improve e ectiveness and provide robust measurement of any e ect [1].",1,ad,True
"In this paper, we expand on past work using an extensive set of experiments. We demonstrate that, by incorporating a list of 21 common text features into a state-of-the-art CNN model, one can achieve an e ectiveness comparable to the currently best reported results on the TREC QA dataset and the WikiQA data.",1,corpora,True
2 EXTERNAL FEATURES,0,,False
"Our hypothesis is that one can assist relevance modeling with a set of external features to capture aspects of the data that are different to those captured in a neural network model. e chosen set of features should also cover basic signals that can be easily reproduced and implemented. In our experiments, we se le on a set of simple features known to be useful for question answering. We focus on retrieval and readability features. ere are two motivations for this approach: 1) such features are be er understood by information retrieval practitioners, and 2) they are relatively cheap to implement. us, we precluded the use of some sophisticated NLP features in our experiments, such as convolutional tree kernel [11] or syntactic similarity [10].",1,ad,True
"e full list of features is given in Table 1, they are divided into three categories.",0,,False
"Lexical and semantic matching. e rst group of features address non-factoid question answering, described Yang et al. [16], were rst selected, which cover topical relevance and semantic relatedness measures. e reference package released by Yang et al. was used in our implementation. For computing the language model and BM25 scores, we empirically set both µ in the language model and the average document length in BM25 to a xed value of ten.",1,ad,True
"Readability. e second group of features focus on text readability [5], which is a set of seven common surface readability features, such as number of words/syllables per sentence or the complexword ratio, plus one feature representing the notable Dale-Chall readability formula. We did not include other readability indices as",1,ad,True
1017,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: List of unsupervised features used in this study,0,,False
Lexical and semantic matching,0,,False
Length,0,,False
Number of terms in the sentence,0,,False
ExactMatch Whether query is a substring,0,,False
Overlap,0,,False
Fraction of query terms covered,0,,False
OverlapSyn Fraction of query synonyms covered,0,,False
LM,1,LM,True
Language model score,0,,False
BM25,0,,False
BM25 score,0,,False
ESA,0,,False
Cosine similarity with the query ESA vector,0,,False
TAGME,0,,False
Overlap between query/sentence entities,0,,False
Word2Vec,0,,False
Cosine similarity with query word vectors,0,,False
Readability CPW SPW WPS CWPS CWR LWPS LWR DaleChall,1,ad,True
Number of characters per word Number of syllables per word Number of words per sentence Number of complex words per sentence Fraction of complex words Number of long words per sentence Fraction of long words (> 7 chars),0,,False
e Dale-Chall readability index,1,ad,True
Focus MatchedNGram,0,,False
"Maximum semantic relatedness between head question k-gram and any answer n-gram. See (1); 4 variants of (k, n) were used",1,ad,True
most of these indices can be represented as a linear combination of the surface features.,0,,False
"Focus. e third group of features used four parameterized variants of a newly proposed feature MatchedNGram to account for the matching between question head words and the potential answer n-gram. e feature takes the maximum of the semantic similarity between the rst k question words and any n-grams in the answer, using the cosine similarity between question/answer word vectors as the similarity measure. Given k and n, the feature is de ned as follows:",1,ad,True
"MatchedNGram(Q, A) , max cos",0,,False
l,0,,False
"k i ,1",0,,False
qi,0,,False
",",0,,False
l +n-1,0,,False
"j,l aj",0,,False
.,0,,False
(1),0,,False
"is simple feature explicitly looks for best matching answer n-grams with respect to question head phrases, such as ""who invented"", ""how many"", or ""what year did"". Word embeddings are leveraged in the computation of the similarity measure. Also, not all combinations of (k, n) are found e ective. In our experiments, we empirically chose four best con gurations of (k, n), which are {(k, n) | k  {2, 3}, n  {2, 3}}, based on their e ectiveness within a learning-to-rank model.",1,ad,True
3 EXPERIMENTS,0,,False
"Our evaluation was based on two widely used question answering benchmarks: the TREC QA and the WikiQA datasets. e rst benchmark was originally developed for the task of identifying correct answer factoids in retrieved passages. We used the version prepared by Wang et al. [13], with 1,229 questions in the larger",1,TREC,True
"training set, 82 in the dev set, and 100 in the test set. No further ltering was performed on the data (the ""raw"" se ing [7]).1 e second benchmark, WikiQA, was created by Yang et al.",1,Wiki,True
"over the English Wikipedia summary passages and the Bing query logs [17], with crowdsourced annotations. is new benchmark is developed to counter biases introduced in the creation TREC QA: the reliance on using lexical overlap with the question as the sole indication of a candidate answer. Hence, this dataset is by design made more challenging for retrieval based methods. Some major follow-up works [7, 18] used a split that includes questions with all positive labels, a version slightly di erent from the split distributed in the original data. We used the same split as in Rao et al. and used 873 questions in the training set, 126 in the dev set, and 243 in the test set [7].",1,Wiki,True
3.1 Neural Network Con guration,0,,False
"For the choice of a base system, which would serve as the experimental control, we chose to implement a bi-CNN architecture as proposed in Severyn and Moschi i [8]. is state-of-the-art model is preferred over other candidates, i.e., a ention-based CNN [18], for the ease of implementation and parameter optimization. is architecture is fairly robust and in most cases overly excessive parameter tuning is not required.",0,,False
"Convolutional Neural Networks. Our implementation follows closely to the experimental se ing in the original paper. Two sets of word embeddings were used: one with 50 dimensions, developed on top of English Wikipedia and the AQUAINT corpus [8], and the other a 300-dimension pre-trained model released by the word2vec project, using 100-billion words from Google News. e sparse word overlapping indicator features were also used in the convolutional layer [11]. e proposed 21 features were incorporated in the fully-connected layer which also combines pooled representations for the question and the answer sentences. e size of the kernel is set to 100 throughout the experiments. We used hyperbolic tangent tanh as the activation function and max pooling in the pooling layer. e network is trained by using stochastic gradient descent with mini batches. e batch size is set to 50 and AdaDelta update [20] was used with  ,"" 0.95. Early stopping was deployed tracking the change of dev set e ectiveness, and as a result the training almost always stopped in 5 to 10 epochs. We also experimented with dropout in two experimental runs by sweeping through a small set of dropout rates {0.1, 0.2, . . . , 0.9}.""",1,Wiki,True
"e a ention mechanism can also a ect the e ectiveness of the neural network model. To control for this variable, we implemented a simple a ention layer in the base CNN model to approximate the ABCNN-1 model, which is the simplest form of a ention mechanism proposed in Yin et al [18]. In mathematical terms, an a ention layer takes a question-side feature map Fq  Rnq ×d and an answerside feature map Fa  Rna ×d as input. Here, nq and na denote the maximum question/answer sentence length, respectively, and d denotes the dimension of the word embeddings.",0,,False
"1Some previous work chose to remove questions that contain no answers and led to two inconsistent data splits ""raw"" an ""clean"", so results on one split are not directly comparable to those on the other [7].",0,,False
1018,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 2: E ectiveness results on TREC QA and WikiQA datasets. Best-performing runs in each word-embedding group are un-,1,TREC,True
derlined and the overall best result on individual benchmarks printed in boldface. Relative improvements (+/-%) are measured,0,,False
"against the group control (base system). Signi cant di erences with respect to bagged LambdaMART and the group control are indicated by /and /, respectively, for p < 0.05/p < 0.01 using the paired t-test.",0,,False
TREC QA,1,TREC,True
WikiQA,1,Wiki,True
System,0,,False
A n? Drop? MAP,1,MAP,True
MRR,0,,False
S@1,0,,False
MAP,1,MAP,True
MRR,0,,False
S@1,0,,False
Runs (AQUAINT/Wikipedia),1,AQUAINT,True
CNN,0,,False
×,0,,False
× 76.2,0,,False
80.9,0,,False
73.7,0,,False
66.0,0,,False
67.4,0,,False
52.3,0,,False
Combined Model,0,,False
×,0,,False
× 77.9 (+2.2%) 82.2 (+1.6%) 74.7 (+1.4%) 67.2 (+1.8%) 68.5 (+1.6%) 53.9 (+3.1%),0,,False
Combined Model,0,,False
×,0,,False
78.2 (+2.6%) 83.7 (+3.5%) 76.8 (+4.2%) 64.7 (-2.0%) 65.7 (-2.5%) 48.6 (-7.1%),0,,False
CNN Combined Model Combined Model,0,,False
× 75.4,0,,False
79.9,0,,False
71.6,0,,False
65.3,0,,False
66.8,0,,False
52.7,0,,False
× 77.2 (+2.4%) 81.1 (+1.5%) 72.6 (+1.4%) 70.0 (+7.2%) 71.4 (+6.9%) 58.4 (+10.8%),0,,False
77.3 (+2.5%) 82.0 (+2.6%) 74.7 (+4.3%) 69.0 (+5.7%) 70.9 (+6.1%) 58.4 (+10.8%),0,,False
Runs (Google News),0,,False
CNN,0,,False
×,0,,False
Combined Model,0,,False
×,0,,False
Combined Model,0,,False
×,0,,False
CNN Combined Model Combined Model,0,,False
× 76.1,0,,False
82.3,0,,False
75.8,0,,False
67.3,0,,False
69.1,0,,False
57.2,0,,False
× 73.8 (-3.0%) 79.2 (-3.8%) 70.5 (-7.0%) 69.2 (+2.8%) 70.2 (+1.6%) 56.0 (-2.1%),0,,False
74.8 (-1.7%) 80.1 (-2.7%) 71.6 (-5.5%) 69.2 (+2.8%) 70.7 (+2.3%) 56.4 (-1.4%),0,,False
× 75.0,0,,False
81.1,0,,False
73.7,0,,False
66.3,0,,False
68.3,0,,False
54.7,0,,False
× 76.5 (+2.0%) 82.5 (+1.7%) 74.7 (+1.4%) 69.4 (+4.7%) 71.2 (+4.2%) 57.6 (+5.3%),0,,False
76.3 (+1.7%) 82.5 (+1.7%) 74.7 (+1.4%) 67.9 (+2.4%) 69.7 (+2.0%) 56.0 (+2.4%),0,,False
Reference methods Bagged LambdaMART LSTM [12] CNN [8] aNMM [15] ABCNN-3 [18] PairwiseRank + SentLevel [7],0,,False
75.7,0,,False
81.3,0,,False
72.6,0,,False
63.0,0,,False
63.8,0,,False
46.5,0,,False
71.3,0,,False
79.1,0,,False
--,0,,False
--,0,,False
74.6,0,,False
80.8,0,,False
--,0,,False
--,0,,False
75.0,0,,False
81.1,0,,False
--,0,,False
--,0,,False
--,0,,False
--,0,,False
69.2,0,,False
71.1,0,,False
78.0,0,,False
83.4,0,,False
70.1,0,,False
71.8,0,,False
"e matrix A  Rnq ×na representing the ""a ention"" is computed internally to the layer as follows:",0,,False
"Ai, j , 1 +",0,,False
"1 Fq [i, :] - Fa [j, :]",0,,False
",",0,,False
(2),0,,False
"with · being the euclidean distance function. en, the layer",0,,False
"generates two new a ention-based feature maps, Fq and Fa , which are to be combined in the follow-up convolutional layers:",0,,False
"Fq , AWq Fa ,"" AT Wa ,""",0,,False
(3),0,,False
where Wq  Rna ×d and Wa  Rnq ×d denote model weights which are to be learned from the data.,0,,False
3.2 Baselines,0,,False
"A number of published results were included as reference runs in the experiments [8, 12, 15, 18], including a recently proposed neural model PairwiseRank [7]. For comparing with learning-to-rank systems, a Bagged LambdaMART model trained using RankLib2 over the 21 sentence features is included. e bagged LambdaMART was trained by optimizing NDCG@20 with subsampling rate 0.7, feature sampling rate 0.3, using 300 bags.",0,,False
2h ps://www.lemurproject.org/ranklib.php,0,,False
3.3 Results,0,,False
"Table 2 gives the e ectiveness results for the TREC QA and the WikiQA datasets. e e ectiveness of answer selection is measured by Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), and Success at 1 (S@1), using the trec eval package following Severyn and Moschi i [8]. e experimental runs are divided into four groups, according to the word embeddings in use (AQUAINT/Wikipedia or Google News) and whether the a ention mechanism is enabled. In each group, the original CNN model is the experimental control, and runs with external features combined (denoted as Combined Model) are the treatments.",1,TREC,True
"On both sets of data, the PairwiseRank model gives the best results. e baseline bagged LambdaMART model appears to be strong on the TREC QA data, beating a number of neural network models, but it does not appear particularly e ective on the more challenging WikiQA dataset.",1,TREC,True
"Our base CNN model is found to be superior to the learning-torank model, and it gave be er results than the original implementation [8] on the TREC QA benchmark. In general, the Combined Model reliably improves the base CNN model. All experimental con gurations appear to bene t from the inclusion of the 21 extra features, with two exceptions: runs using GoogleNews embeddings on the TREC QA benchmark, and one of the dropout runs on the",1,TREC,True
1019,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"WikiQA data. On the TREC QA benchmark, we saw an increase of 1.3%­4.3% in the three evaluation metrics. On WikiQA, the increase is around 1.6%­7.2% in MAP/MRR and 2.4%­10.8% in S@1.",1,Wiki,True
"ese increases are in most cases consistent with each other, except that in one particular con guration a decreased S@1 is observed alongside improved MAP and MRR scores.",1,MAP,True
"e AQUAINT/Wikipedia embeddings appear to have a slight advantage over the Google News embeddings. e best performing runs on both benchmarks using this embedding achieved a marked increased in e ectiveness compared to the best known results. On the TREC QA benchmark, however, the Combined Model with dropout surpassed the PairwiseRank model [7] in both MAP and MRR. On WikiQA, the Combined Model with the a ention mechanism outperformed ABCNN-3 (i.e., a stronger variant of ABCNN-1), with the achieved e ectiveness only marginally below the e ectiveness of the PairwiseRank model. In most cases, we found that the Combined Model works the best when the a ention mechanism is used together without dropout. We conjecture that in this case the a entional CNN model works di erently to the data, as external features on their own tend to t certain aspects well enough.",1,AQUAINT,True
"Based on these results, we conclude that, for answer sentence selection, combining the proposed external features into a convolutional neural architecture has a bene t of improving overall modeling e ectiveness. is improvement is evident even when the neural architecture is slightly altered to perform advanced neural functions such as a ention mechanism or dropout. e evidence is that the highly tuned convolutional neural architecture failed to model certain aspects in the data, which can be captured with a set of simple features. is points to a limitation of neural network methodology previously not mentioned in research on question answering.",1,ad,True
4 RELATED WORK,0,,False
"ere is a rich body of work in question answering focused on answer sentence selection [7, 8, 11, 12, 14, 15, 17­19]. Most of these e orts address the architectural issues in neural network models. Yu et al. [19] utilize a CNN architecture to model question-answer pairs, and this approach was taken by Yang et al [17] and Severyn and Moschi i [8], who later expanded the network architecture into a bi-CNN model. Wang et al. [14] decomposed vectors into similar/dissimilar components and used a two-channel CNN to capture the signals. e a ention mechanism was investigated in Yin et al. [18] and Yang et al. [15]. He and Lin [4] used a Bidirectional LSTM to model the context of input sentences. Rao et al. [7] proposed a pairwise ranking method that uses two bi-CNN architectures to perform sentence-level pairwise ranking.",1,ad,True
"Feature engineering has been a popular methodology for modeling question-answer structure, and is still actively used in nonfactoid question answering or answer re-ranking [10, 16]. One commonality between these specialized tasks is a focus on retrieving or ranking passage-level answers [6]. Surdeanu et al. [10] proposed using a translation model to capture the mapping between the high-level linguistic representations of the question and the answer. Yang et al. [16] proposed using query matching, semantic, and context features to select answer sentences for non-factoid questions.",0,,False
5 CONCLUSIONS,0,,False
We provide empirical evidence to support the use of conventional,0,,False
features in deep learning models on the task of answer sentence,0,,False
selection. We show that a convolutional neural network model ben-,0,,False
e ts from a group of commonly used text features and outperforms,0,,False
the best published result on a commonly used question answering,0,,False
benchmark. e fact that neural networks can still bene t from,0,,False
these conventional features may point to new possibilities in the,0,,False
"evolution of new neural architectures. In future work, we will seek",0,,False
"to expand this analysis to other neural architectures, such as LSTM-",0,,False
"CNN or recurrent neural networks, and other question answering",0,,False
benchmarks that are more recent and larger.,0,,False
REFERENCES,0,,False
"[1] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don'T Add Up: Ad-hoc Retrieval Results Since 1998. In Proceedings of CIKM '09. ACM, New York, NY, USA, 601­610.",1,hoc,True
"[2] Ma hew W. Bilo i, Jonathan Elsas, Jaime Carbonell, and Eric Nyberg. 2010. Rank Learning for Factoid estion Answering with Linguistic and Semantic Constraints. In Proceedings of CIKM '10. ACM, New York, NY, USA, 459­468.",0,,False
"[3] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Ma Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to Rank Using Gradient Descent. In Proceedings of ICML '05. ACM, New York, NY, USA, 89­96.",1,ad,True
[4] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement. In Proceedings of NAACL '16. 937­948.,0,,False
"[5] Tapas Kanungo and David Orr. 2009. Predicting the Readability of Short Web Summaries. In Proceedings of WSDM '09. ACM, New York, NY, USA, 202­211.",1,ad,True
"[6] Mostafa Keikha, Jae Hyun Park, W. Bruce Cro , and Mark Sanderson. 2014. Retrieving Passages and Finding Answers. In Proceedings of ADCS '14. ACM, New York, NY, USA, Article 81, 4 pages.",0,,False
"[7] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In Proceedings of CIKM '16. ACM, 1913­1916.",0,,False
"[8] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of SIGIR '15. ACM, 373­382.",0,,False
"[9] Huan Sun, Hao Ma, Wen-tau Yih, Chen-Tse Tsai, Jingjing Liu, and Ming-Wei Chang. 2015. Open Domain estion Answering via Semantic Enrichment. In Proceedings of WWW '15. 1045­1055.",0,,False
"[10] Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid estions from Web Collections. Computational Linguistics 37, 2 (April 2011), 351­383.",0,,False
"[11] Kateryna Tymoshenko, Daniele Bonadiman, and Alessandro Moschi i. 2016. Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking. In Proceedings of NAACL '16. 1268­1278.",1,ad,True
[12] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In Proceedings of ACL '15. 707­712.,0,,False
"[13] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In Proceedings of EMNLP '07. 22­32.",0,,False
"[14] Zhiguo Wang, Haitao Mi, and Abraham I ycheriah. 2016. Sentence Similarity Learning by Lexical Decomposition and Composition. In Proceedings of COLING 2016. Osaka, Japan, 1340­1349.",0,,False
"[15] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Cro . 2016. aNMM: Ranking Short Answer Texts with A ention-Based Neural Matching Model. In Proceedings of CIKM '16. ACM, 287­296.",0,,False
"[16] Liu Yang, Qingyao Ai, Damiano Spina, Ruey-Cheng Chen, Liang Pang, W. Bruce Cro , Jiafeng Guo, and Falk Scholer. 2016. Beyond Factoid QA: E ective Methods for Non-factoid Answer Sentence Retrieval. In Proceedings of ECIR '16. Springer International Publishing, 115­128.",0,,False
"[17] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain estion Answering. In Proceedings of EMNLP '15. Lisbon, Portugal, 2013­2018.",1,Wiki,True
"[18] Wenpeng Yin, Hinrich Schtze, Bing Xiang, and Bowen Zhou. 2015. ABCNN: A ention-Based Convolutional Neural Network for Modeling Sentence Pairs. arXiv:1512.05193 [cs] (Dec. 2015).",0,,False
"[19] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. arXiv:1412.1632 [cs] (Dec. 2014).",0,,False
[20] Ma hew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012).,1,ad,True
1020,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Document Expansion Using External Collections,0,,False
Garrick Sherman,0,,False
School of Information Sciences University of Illinois at Urbana-Champaign,0,,False
gsherma2@illinois.edu,0,,False
ABSTRACT,0,,False
"Document expansion has been shown to improve the effectiveness of information retrieval systems by augmenting documents' term probability estimates with those of similar documents, producing higher quality document representations. We propose a method to further improve document models by utilizing external collections as part of the document expansion process. Our approach is based on relevance modeling, a popular form of pseudo-relevance feedback; however, where relevance modeling is concerned with query expansion, we are concerned with document expansion. Our experiments demonstrate that the proposed model improves ad-hoc document retrieval effectiveness on a variety of corpus types, with a particular benefit on more heterogeneous collections of documents.",1,ad-hoc,True
1 INTRODUCTION,1,DUC,True
"Relevance modeling is an extremely influential pseudo-relevance feedback technique in which we assume that both queries and documents are observations sampled from a relevance model (RM) [8], which is a probability distribution over terms in relevant documents. Because we do not have true relevance feedback, relevance modeling makes use of the query likelihood, P (Q |D), to quantify the degree to which words in each document should contribute to the final model R. However, since no document is perfectly representative of its underlying generative model, we may be reasonably concerned that our estimate of P (Q |D) is the result of chance. That is, there is no guarantee that D is a representative sample from R. The quality of our RM, therefore, may benefit from a higher quality document representation than that which is estimated from the text of D.",0,,False
"We employ two techniques to attempt to improve our document language models: document expansion and the use of external document collections. Expandeded documents are expected to exhibit less random variation in term frequencies, improving probability estimates. We hope that estimates may be further refined by expanding documents using external collections, thereby avoiding any term bias exhibited by relevant documents in an individual collection.",0,,False
Our study differs from prior work in a few important ways. Previous investigations into document expansion have tended to use only,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080716",1,ad,True
Miles Efron,0,,False
School of Information Sciences University of Illinois at Urbana-Champaign,0,,False
mefron@illinois.edu,0,,False
"the target collection to expand documents, while our work explores the use of one or more distinct collections. Conversely, most existing work involving external corpora in ad-hoc information retrieval has focused on query expansion; we are interested in incorporating external collections for purposes of document expansion.",1,corpora,True
2 RELATED WORK 2.1 Document Expansion in IR,0,,False
"Document expansion has been well studied in information retrieval literature, e.g. [10, 11, 13, 16]. For example, Liu & Croft propose a method of retrieval that uses document clusters to smooth document language models [10]. Tao et al. propose a similar approach but place each document at the center of its own cluster; this helps to ensure that the expansion documents are as closely related to the target document as possible [13].",0,,False
"Our approach takes as its starting point that of Efron, Organisciak & Fenlon [4], who issue very short microblog documents as pseudo-queries. They employ a procedure closely related to relevance modeling [8] to expand the original document using those microblog documents retrieved for the pseudo-query. We explore the application and adaptation of their work to different scenarios. First, Efron, Organisciak & Fenlon are concerned with microblog retrieval, in which documents are extremely short--perhaps as small as a keyword query. In contrast, we are interested in performing document expansion with more typical full-length documents, such as those found in news and web corpora. Second, while their work used only the target document collection, we propose an extension of their method that allows for multiple expansion corpora. Finally, we investigate pairing document expansion with query expansion, which their work suggests may be problematic in the microblog domain.",1,blog,True
2.2 Incorporating External Collections,1,corpora,True
"The incorporation of external collections into document retrieval is a similarly common theme in the ad-hoc IR literature, particularly with respect to query expansion [2, 3, 9, 15, 17]. Of particular relevance to our work is that of Diaz & Metzler, whose mixture of relevance models is the basis of our Eq. 5 [3]. Their model simply interpolates RMs built on different collections, weighting each by a query-independent quantity P (c). Though our work bears similarities, Diaz & Metzler are interested in query expansion, whereas we apply the technique as one piece in a document expansion model.",1,corpora,True
1045,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
3 DOCUMENT EXPANSION PROCEDURE,0,,False
3.1 Underlying Retrieval Model,0,,False
"Throughout this paper we rely on the language modeling retrieval framework [7]. More specifically, we employ query likelihood (QL) and relevance modeling for ranking.",0,,False
"3.1.1 Query Likelihood. Given a query Q and a document D, we rank documents on P (Q |D ), where D is the language model (typically a multinomial over the vocabulary V ) that generated the text of document D. Assuming independence among terms and a uniform distribution over documents, each document is scored by",1,Query,True
"P (Q |D) ,",0,,False
"P (w |D )c (w,Q )",0,,False
(1),0,,False
w Q,0,,False
"where c (w,Q ) is the frequency of word w in Q. We follow standard procedures for estimating P (w |D ) in Eq. 1, estimating a smoothed language model by assuming that document language models in a",0,,False
given collection have a Dirichlet prior distribution:,0,,False
P^(w |D ),0,,False
",",0,,False
"c (w,D) + µP^(w |C) |D| + µ",0,,False
(2),0,,False
"where P^(w |C) is the maximum likelihood estimate of the probability of seeing word w in a ""background"" collection C (typically C is the corpus from which D is drawn), and µ  0 is the smoothing hyperparameter.",0,,False
3.1.2 Relevance Modeling. Relevance modeling is a form of pseudo-relevance feedback that uses top ranked documents to estimate a language model representing documents relevant to a query [8].,0,,False
"Assuming uniform document prior probabilities, relevance models take the form of",0,,False
"P (w |R) , P (w |D)P (Q |D)",0,,False
(3),0,,False
D C,0,,False
where P (Q |D) is calculated as in Eq. 1 and essentially weights word w in D by the query likelihood of the document. Relevance models,0,,False
are most efficient and robust when calculated over only the top ranked documents and limited to the top terms. These parameters are referred to as f bDocs and f bT erms respectively in Table 1,0,,False
below.,0,,False
"Because relevance models are prone to query drift, it is often",0,,False
desirable to linearly interpolate an RM with the original query,0,,False
model to improve effectiveness:,0,,False
"P (w |Q ) , (1 -  )P (w |R) + P (w |Q ).",0,,False
(4),0,,False
" is a mixing parameter controlling the influence of the original query. This form of relevance model is known as ""RM3.""",0,,False
3.2 Expanding with Document Pseudo-Queries,0,,False
"To expand a document D, we begin by treating the text of D as a pseudo-query which we pose against a collection of documents CE . To transform a document into a pseudo-query we apply two transformations. First we remove all terms from D that appear",0,,False
"in the standard Indri stoplist1. Next, we prune our pseudo-query by retaining only the 0 < k  K most frequent words in the stopped text of D, where K is the total number of unique terms in D. The integer variable k is a parameter that we choose empirically.",0,,False
These are the non-stop words with the highest probabilities in a maximum likelihood estimate of D's language model and are,0,,False
therefore a reasonable representation of the topic of the document.,0,,False
"Though some information may be lost with stopping, with a large enough k we hope to nevertheless capture the general topic of a",0,,False
"document; for example, a document about Hamlet's famous speech",0,,False
"may not be represented by the terms ""to be or not to be,"" but the",0,,False
"terms ""Shakespeare,"" ""Hamlet,"" ""speech,"" etc. will likely represent the document's subject sufficiently. Let QD be the pseudo-query for D, consisting of the text of D after our two transformations are",0,,False
applied.,0,,False
"We rank related documents, called expansion documents, by running QD over a collection CE . More formally, we rank the documents in CE against D using Eq. 1, substituting QD for the query and Ei --the ith expansion document--for the document. Let i be the log-probability for expansion document Ei with respect to D given by Eq. 1.",0,,False
"We now have a ranked list of tuples {(E1,1), (E2,2), ..., (EN ,N )} relating expansion document Ei to D with log-probability i . We take the top n documents where 0  n  N . We call these top documents ED and designate them as our expansion documents for D. Finally, we exponentiate each i and normalize our retrieval scores so they sum to 1 over the n retained documents. Assum-",0,,False
"ing a uniform prior over documents, we now have a probability distribution over our n retained documents: P (E|D).",0,,False
"Since this procedure does not depend on the query, we may compute ED once at indexing time and reuse our expansion documents across queries.",0,,False
4 DOCUMENT EXPANSION RETRIEVAL MODEL,0,,False
We would now like to incorporate our expansion documents into,1,corpora,True
a retrieval model over documents. We assume that a query is gen-,0,,False
"erated by a mixture of the original document language model D and language models E j representing the expansion documents in each corpus Cj  {C1,C2, ...,Cn }. We assume that E j can be estimated using the text of the expansion documents ED j in corpus Cj . This mixture model may be expressed as:",0,,False
|Q |,0,,False
n,0,,False
n,0,,False
"P^ (Q |D) , (1 -  ED j )P (qi |D) +  ED j P (qi |ED j ) (5)",0,,False
"i ,1",0,,False
"j ,1",0,,False
"j ,1",0,,False
where 0 ,0,,False
"n j ,1",0,,False
ED,0,,False
j,0,,False
1. We estimate P (qi |ED j ),0,,False
in expectation:,0,,False
"P (qi |ED j ) ,",0,,False
P (qi |E)P (E|D).,0,,False
(6),0,,False
E ED j,0,,False
"Like P (qi |D), we estimate the probability of qi given expansion document E, P (qi |E), as a Dirichlet-smoothed query likelihood. By virtue of our expansion document scoring and normalization, we also have P (E|D). This general model may be used with any number",0,,False
of expansion corpora.,1,corpora,True
1 http://www.lemurproject.org/stopwords/stoplist.dft,0,,False
1046,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
4.1 Relevance Modeling with Expanded Documents,0,,False
"Given our motivating intuition that document expansion allows for the more accurate estimation of document language models, we would expect that an RM computed using expanded documents should be more accurate than a standard RM. We therefore compute an RM3 as in Eqs. 3 and 4, substituting the expanded document for the original.",0,,False
5 EVALUATION,0,,False
5.1 Data,0,,False
"Although Eq. 5 allows for an arbitrary number of collections, for now we limit ourselves to two: the collection that the document appears in (the ""target"" collection) and Wikipedia2. We expect the latter, as a general encyclopedia, to yield relatively unbiased probability estimates. We build an Indri [12] index over the Wikipedia page text.",1,Wiki,True
We test our approach using TREC datasets:,1,TREC,True
· The AP newswire collection [5] from TREC disks 1 and 2 with topics 101-200.,1,AP,True
"· The robust 2004 [14] topics, numbering 250, from TREC disks 4 and 5.",1,TREC,True
· The wt10g collection [1] with the 100 topics from the 2000 and 2001 TREC Web tracks.,1,TREC,True
"These datasets provide a good range of collection types, from relatively homogeneous with well-formed documents (AP) to heterogeneous with varied document quality (wt10g).",1,AP,True
5.2 Runs,0,,False
"For each collection, we produce eight runs representing a combination of expansion source and query expansion model. Expansion source refers to the collection(s) used for document expansion, while the query expansion model refers to unexpanded queries (QL) or expanded queries (RM3).",0,,False
We produce runs with expansion documents from:,0,,False
"· no expansion, called baseline; · the target collection itself, called self ; · Wikipedia, called wiki; or · a mixture of the previous two, called combined.",1,Wiki,True
"For each source, both the QL and RM3 variations are compared. Stop words are removed from the query. For efficiency, we re-",0,,False
trieve the top 1000 documents using the default Indri QL implementation and re-rank these documents based on their expanded representations as described in Section 4.,0,,False
5.3 Parameters,0,,False
"The parameters required for our approach, their meanings, and the values used in our experiments are shown in Table 1.",0,,False
"For this work, we set k heuristically. In principle, k may equal the length of the document; however, this would increase computation time significantly, so we have set it to a smaller value for efficiency. The parameter n is also set heuristically; see Section 6.1 for a discussion of the sensitivity of our model to the setting of n.",0,,False
2 http://en.wikipedia.org,1,wiki,True
Table 1: Parameter settings for the document expansion procedure and retrieval model,0,,False
Param. k n,0,,False
 ED,0,,False
µ f bDocs f bT erms,0,,False
Meaning,0,,False
The maximum number of document terms to use in constructing QD . The maximum number of expansion documents in ED . One of several related mixing parameters controlling the weights of P (q|D) and P (q|ED ) Used for Dirichlet smoothing of both P (q|D) and P (q|E). The number of feedback documents to use for RM3 runs.,0,,False
The number of terms per document to use for RM3 runs.,0,,False
Mixing parameter controlling the weights of the original query and relevance model for RM3 runs.,0,,False
Value 20 10,0,,False
0.0-1.0,0,,False
2500 20 20 0.0-1.0,0,,False
"The values of  ED and  , are determined using 10-fold crossvalidation. In the training stage, we sweep over parameter values in intervals of 0.1. The concatenated results of each test fold form a complete set of topics.",0,,False
6 RESULTS,0,,False
Retrieval effectiveness of each run is shown in Table 2. We measure effectiveness with mean average precision (MAP) and normalized discounted cumulative gain at 20 (nDCG@20) [6]. Each metric is optimized with 10-fold cross-validation.,1,MAP,True
"The results confirm that document expansion provides benefit over a baseline query likelihood run--no run performs significantly worse than the baseline, and most runs improve over the baseline QL run.",0,,False
"Performance of RM3 runs is more surprising with improvement over the baseline RM3 occurring more rarely compared to improvement over the baseline QL. The data suggest that RM3 runs may be more effective in more heterogeneous collections: there are three RM3 improvements in robust and six in wt10g, compared to only one in AP. This makes intuitive sense since a homogeneous collection would be expected to receive less benefit from query expansion. We can also see that an RM3 run typically improves over its QL counterpart, demonstrating that relevance modeling continues to operate effectively with the introduction of document expansion.",1,AP,True
"In general, wiki runs perform similarly to combined runs. However, the strong performance of combined runs is visible when query expansion is ignored: five out of six combined QL runs show statistically significant improvement over wiki QL runs. In one case (wt10g measuring nDCG@20) the combined QL run even outperforms the wiki RM3 run with statistical significance.",1,wiki,True
6.1 Sensitivity to n,0,,False
"Figure 1 shows sweeps over several values of n, the number of expansion documents, for the self and wiki QL runs using our",1,wiki,True
1047,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
MAP nDCG@20,1,MAP,True
0.26 0.24 0.22 0.20 0.18,0,,False
0,0,,False
Expansion Source self wiki Collection AP wt10g robust 0.45,1,wiki,True
0.40,0,,False
0.35,0,,False
10,0,,False
20,0,,False
30,0,,False
40,0,,False
Number of expansion documents,0,,False
0.30,0,,False
50,0,,False
0,0,,False
10,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
Number of expansion documents,0,,False
"Figure 1: Sweeps over the number of expansion documents, n ,"" {0, 1, 5, 10, 50}, for the self and wiki QL runs.""",1,wiki,True
"established cross-validation procedure with identical folds. The sensitivity to n is not pronounced at n  5, and what little variation exists is not consistent across collections. We therefore set n to 10, an apparently safe value, for all other runs. This is a convenient result since it allows for more efficient document expansion.",0,,False
7 CONCLUSIONS,0,,False
"The results indicate that our approach for document expansion works well in general and especially in concert with traditional relevance modeling. We find that we can improve on traditional document expansion by incorporating external collections into the expansion process. In the future, we plan to investigate how important the choice of external collection is to the retrieval effectiveness of our model.",1,ad,True
REFERENCES,0,,False
"[1] P. Bailey, N. Craswell, and D. Hawking. Engineering a multi-purpose test collection for web retrieval experiments. Information Processing and Management, 39(6):853­871, 2003.",0,,False
"[2] M. Bendersky, D. Metzler, and B. W. Croft. Effective query formulation with multiple information sources. WSDM '12, 2012.",0,,False
"[3] F. Diaz and D. Metzler. Improving the estimation of relevance models using large external corpora. In SIGIR '06, pages 154­161, 2006.",1,corpora,True
"[4] M. Efron, P. Organisciak, and K. Fenlon. Improving retrieval of short texts through document expansion. In SIGIR '12, pages 911­920, 2012.",0,,False
"[5] D. Harman. Overview of the first text retrieval conference (TREC-1). In TREC '92, pages 1­20, 1992.",1,TREC,True
"[6] K. Järvelin and J. Kekäläinen. Cumulated gain-based evaluation of IR techniques. TOIS, 20(4):422­446, 2002.",0,,False
"[7] J. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR '01, pages 111­119, 2001.",0,,False
"[8] V. Lavrenko and W. B. Croft. Relevance based language models. In SIGIR '01, pages 120­127, 2001.",0,,False
"[9] Y. Li, W. Luk, K. Ho, and F. Chung. Improving weak ad-hoc queries using Wikipedia as external corpus. SIGIR '07, pages 797­798, 2007.",1,ad-hoc,True
"[10] X. Liu and W. B. Croft. Cluster-based retrieval using language models. In SIGIR '04, pages 186­193, 2004.",0,,False
"[11] A. Singhal and F. Pereira. Document expansion for speech retrieval. In SIGIR '99, pages 34­41, 1999.",0,,False
"[12] T. Strohman, D. Metzler, H. Turtle, and W. Croft. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, pages 2­6, 2005.",0,,False
"[13] T. Tao, X. Wang, Q. Mei, and C. Zhai. Language model information retrieval with document expansion. In NAACL '06, pages 407­414, 2006.",0,,False
"[14] E. M. Voorhees. Overview of the TREC 2004 robust track. In TREC '132, 2013. [15] W. Weerkamp, K. Balog, and M. de Rijke. A generative blog post retrieval model",1,TREC,True
"that uses query expansion based on external collections. In ACL '09, pages 1057­1065, 2009. [16] X. Wei and W. B. Croft. LDA-based document models for ad-hoc retrieval. In SIGIR '06, pages 178­185, 2006. [17] Y. Xu, G. J. Jones, and B. Wang. Query dependent pseudo-relevance feedback based on Wikipedia. SIGIR '09, 2009.",1,ad-hoc,True
Table 2: Performance of runs using various expansion,0,,False
sources with (RM3) and without (QL) query expansion. Statistical significance at p  0.05 is marked with a variety of symbols; underlining further designates p  0.01:  indicates improvement over the baseline QL run;  and  indicate im-,0,,False
provement and decline respectively with respect to the baseline RM3 run;  indicates improvement over the QL run with the same expansion source; S and W indicate improvement,0,,False
"over the self and wiki sources, respectively, of the same run",1,wiki,True
type. Bolded runs are the highest raw score for an evaluation,0,,False
metric in a given collection.,0,,False
Corpus AP,1,AP,True
Robust wt10g,1,Robust,True
Exp. Source Baseline,0,,False
Self Wiki Combined Baseline Self Wiki Combined Baseline Self Wiki Combined,1,Wiki,True
Run,0,,False
QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3 QL RM3,0,,False
MAP,1,MAP,True
0.2337 0.3310 0.2694 0.3295 0.2644 0.3334 0.2774W S 0.3342S,0,,False
0.2183 0.2639 0.2369 0.2591 0.2326 0.2674S 0.2417W 0.2672S,0,,False
0.1683,0,,False
0.1651,0,,False
0.1660 0.1694 0.1780S 0.2089S 0.1759S 0.2061S,0,,False
nDCG@20,0,,False
0.4170 0.4855 0.4519 0.4876W 0.4582 0.4811 0.4734SW 0.4789,0,,False
0.3867 0.3908 0.4036 0.3894 0.4040 0.4201S  0.4156W S 0.4205S,0,,False
0.2816 0.2834,0,,False
0.2936 0.2758 0.3029 0.3085S  0.3148SW 0.3082S,0,,False
1048,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
A Metric for Sentence Ordering Assessment Based on Topic-Comment Structure,0,,False
Liana Ermakova,0,,False
"LISIS, CNRS-ESIEE-INRA-UPEM Universite´ de Lorraine 5 boulevard Descartes",0,,False
"Champs-sur-Marne, France 77454 liana.ermakova@univ-lorraine.fr",0,,False
Josiane Mothe,0,,False
"IRIT, URM5505 CNRS ESPE, Universite´ de Toulouse",0,,False
"118 route de Narbonne Toulouse, France 31062 josiane.mothe@irit.fr",0,,False
Anton Firsov,0,,False
Perm State University 15 Bukireva st.,0,,False
"Perm, Russia 614990 a rsov@mail.ru",0,,False
ABSTRACT,0,,False
"Sentence ordering (SO) is a key component of verbal ability. It is also crucial for automatic text generation. While numerous researchers developed various methods to automatically evaluate the informativeness of the produced contents, the evaluation of readability is usually performed manually. In contrast to that, we present a selfsu cient metric for SO assessment based on text topic-comment structure. We show that this metric has high accuracy.",1,ad,True
KEYWORDS,0,,False
"Information retrieval, evaluation, text coherence, sentence ordering, topic, comment, information structure, topic-comment structure",0,,False
1 INTRODUCTION,1,DUC,True
Sentence order (SO) has a strong in uence on text perception and understanding [1]. Let consider the following example:,0,,False
"Example 1.1. e Nibelung is the dwarf Alberich, and the ring in question is the one he fashions from the Rhine Gold. Wagner's opera title Der Ring des Nibelungen is most literally rendered in English as e Ring of the Nibelung.",0,,False
"e text is hardly comprehensible. When we are reading the Nibelung or the ring in question, we are asking ourselves what's it all about? which Nibelung? what question? even if in the next sentence it becomes clearer. Let us now reverse the two sentences:",1,ad,True
"Example 1.2. Wagner's opera title Der Ring des Nibelungen is most literally rendered in English as e Ring of the Nibelung. e Nibelung is the dwarf Alberich, and the ring in question is the one he fashions from the Rhine Gold.",0,,False
"Now, it is clear that the Nibelung and the ring in question explain the opera title e Ring of the Nibelung. ese examples illustrate that appropriate SO is crucial for readability. Automatic text generation, particularly multi-document extractive summarization, systems also face SO problem [3, 4]. We distinguish two tasks related to SO: (1) to produce the best order for a given set",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080720",1,ad,True
"of sentences and (2) to measure the order quality, i.e. determining how well a given list of sentences is ordered. In this paper, we focus on the la er task. In order to measure the quality of SO, if a gold standard is available, then correlation between the ground truth SO and the system order (e.g. Kendall or Spearman rank correlation coe cients) can be used [19]. e requirement of a gold standard containing the correct SO limits the usage of such methods. Indeed, gold standard is o en not available or not obvious to build manually. In contrast, in this paper we propose a self-su cient metric for text coherence assessment that does not require additional data. We evaluate the quality of the metric using the framework proposed in [12]. To evaluate the text coherence, we use a linguistic approach based on the topic-comment structure of the text and inter-sentence similarity.",1,ad,True
A clause-level topic is the phrase in a clause that the rest of the,0,,False
"clause is understood to be about, and the comment is what is being",0,,False
"said about the topic. According to [24], the topic does not provide new information",0,,False
"but connects the sentence to the context. us, the topic and the comment are opposed in terms of the given/new information. e contraposition of the given/new information is called information structure or topic-comment structure. Going back to Example 1.1, the Nibelung and the ring in question from the rst sentence are expected to be already known by the reader, i.e. they represent topics. However, only the next sentence provides the necessary information. In contrast, in Example 1.2 the rst mention of the ring and the Nibelung was given at the end of the rst sentence ( e Ring of the Nibelung) and then is detailed in the second sentence. In the rst sentence, Wagner's opera title Der Ring des Nibelungen incarnates the topic and is most literally rendered in English as e Ring of the Nibelung corresponds to the comment. In the second sentence, e Nibelung and the ring in question refers to topic, while the comment parts are presented by is the dwarf Alberich and is the one he fashions from the Rhine Gold.",1,ad,True
"Although, in literature topic-comment structure has been exploited for document re-ranking [13], classi cation [5], and text summarization [11], to our knowledge, it has never been applied for SO. e contribution of this paper is a completely automatic approach for SO evaluation based on topic-comment structure of a text that requires only shallow parsing and has linear complexity. Our metric considers the pairwise term similarities of the topics and the comments of the adjacent sentences in a text since word repetition is one of the formal signs of text coherence [1].",1,ad,True
1061,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 STATE OF THE ART,0,,False
"Current methods to evaluate readability are based on the familiarity of terms and syntax complexity [8]. Word complexity may be estimated by humans [7, 14, 29] or according to its length [30]. Researches also propose to use language models [8, 27]. Usually assessors assign a score to the readability of a text in some range [1]. Syntactical errors, unresolved anaphora, redundant information and coherence in uence readability and therefore the score may depend on the number of these mistakes [26]. BLEU and edit distance may be applied for relevance judgment as well as for readability evaluation. ese metrics are semi-automatic because they require a gold standard. Another set of methods is based on syntax analysis which may be combined with statistics (e.g. sentence length, depth of a parse tree, omission of personal verbs, rate of prepositional phrases, noun and verb groups) [6, 25, 32, 33], but they remain suitable only for the readability evaluation of a particular sentence and, therefore, cannot be used for assessing extracts. Lapata applies the greedy algorithm maximizing the total probability on a text corpus as well as using a speci c ordering to verb tenses [18]. Louis and Nenkova use a hidden Markov model in which the coherence between adjacent sentences is viewed as transition rules between di erent topics [23]. Barzilay and Lapata introduce an entity grid model where sentences are mapped into discourse entities with their grammatical roles [2]. Entity features are used to compute the probability of transitions between adjacent sentences. en machine learning classi ers are applied. Elsner and Charniak add co-reference features [10]. Lin et al. ameliorate the model by discourse relations [21]. e entity grid model and its extensions require syntactical parsing. e disadvantages of these models are data sparsity, domain dependence and computational complexity.",1,ad,True
"e closest work to ours is [12] that proposes an automatic approach for SO assessment where the similarity between adjacent sentences is used as a measure of text coherence. However, it assigns equal scores to initial and inverse SO due to the symmetric similarity measure. In contrast, our topic-comment based method assigns higher score to the text in Example 1.2 than 1.1.",1,ad,True
3 TOPIC-COMMENT STRUCTURE FOR SO,0,,False
"Although it is not the core element of our method, in order to be er understand the topic-comment structure of texts of di erent genres, we manually examined 10 documents randomly chosen from three datasets (30 texts in total): (1) Wikipedia; (2) TREC Robust1; (3) TREC WT10G (for collection details see Section 4). We looked at topic-topic (TT), comment-topic (CT), topic-comment (TC) and comment-comment (CC) inter-sentence relations in the texts, i.e. how frequently a topic (or a comment) of a clause became a topic (or a comment) in posterior clauses. We found that for all collections, the most frequent relation is TT, then follows CT. TT+CT compose more than 65% of the relationships that we found, whatever the collection is; it is more than 80% for Wikipedia. CC is more rare and TC is the most uncommon relation, especially in Wikipedia.",1,Wiki,True
"is preliminary analysis convinced us that using the topiccomment structure could be useful to evaluate readability and that weighting these relations could be a good cue. However, for a scalable method the text structure has to be extracted or annotated",1,ad,True
1 trec.nist.gov,1,trec,True
"automatically. Several parsers have been developed to extract text structure such as HILDA [17] that implements topic changes or SPADE [28] which extracts rhetorical relations and has been used in [22] for example to re-rank documents. ese parsers are based on deep analysis of linguistic features and are hardly usable when large volumes of texts are involved. Moreover, they view the topiccomment relation as a remark on the statement while we consider a topic as the phrase that the rest of the clause is understood to be about as in [13].",0,,False
"e information structure is opposed to formal structure of a clause with grammatical elements as constituents. In contrast to a grammatical subject that is a merely grammatical category, a topic refers to the information or pragmatic structure of a clause and how it is related to other clauses. However, in a simple English clause, a topic usually coincides with a subject. One of the exceptions are expletives (e.g. it is raining) that have only a comment part [15]. Since the unmarked word order in English is Subject - Verb - Object (SVO), we can assume that a topic is usually placed before a verb. As in [13], we also assume that if a subordinate clause provides details on an object, it is rather related to a comment part. us, in our method we split a sentence into two parts by a personal verb (not in nitive nor participle) where the rst part is considered to be a topic while the rest is viewed as a comment. As opposed to other methods from the literature, this method requires only part-of-speech tagging and its computational complexity is linear over the number of words as well as the number of sentences in a text.",0,,False
"e key idea of our method is that in a coherent text there are relations between topic (or comment) parts of the adjacent sentences and these relations are manifested by word repetition. We represent topic and comment parts of a sentence by bag-ofwords. In order to capture the topic-comment relation, we calculate the similarity between them. We propose to use term and noun based similarities. Since the frequencies of TT, TC, CT and CC di er, it seems reasonable to weight the inter-sentence relationship between topic and comment. us, we compute the score between two adjacent sentences si-1 and si as the weighted cosine similarity between them:",1,ad,True
"sc(si-1, si )",0,,False
",",0,,False
|,0,,False
|si,0,,False
1 -1 ||,0,,False
|,0,,False
|si,0,,False
|,0,,False
|,0,,False
[wt,0,,False
t,0,,False
(Ti -1,0,,False
· Ti ),0,,False
+ wct (Ci-1 · Ti ) + wtc (Ti-1 · Ci ) + wcc (Ci-1 · Ci )] (1),0,,False
"where ||·|| is the length of the corresponding vector, Ti and Ci refer to the bag-of-words representations of topic or comment part of the i-th sentence respectively, the scalar product is marked by ·, wtt , wct , wtc , and wcc  [0, 1] indicate the weights of the TT, TC, CT and CC relations within the text. We estimate text coherence as an average score between adjacent sentences in a text S , (si )i|S,1| :",1,ad,True
Coh(S ),0,,False
",",0,,False
1 |S |-1,0,,False
|S |,0,,False
"sc(si-1, si )",0,,False
"i ,2",0,,False
(2),0,,False
4 EVALUATION,0,,False
"We conducted two series of experiments. For the rst evaluation, we used three datasets: (1) Wikipedia dump, (2) TREC Robust, and (3) WT10G. e rst dataset is a cleaned English Wikipedia",1,Wiki,True
1062,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"XML dump of 2012 without notes, history and bibliographic references [3]. We selected 32,211 articles retrieved by the search engine Terrier2 for the queries from INEX/CLEF Tweet Contextualization Track 2012-2013 [3]. TREC (Text Retrieval Conference) Robust dataset is an unspammed collection of news articles from e Financial Times 1991-1994, Federal Register 1994, Foreign Broadcast Information Service, and e LA Times [31]. We used 193,022 documents retrieved for 249 topics from the Robust dataset. In contrast, WT10G is a snapshot of 1997 of Internet Archive with documents in HTML format, some of which are spam [16]. We retrieved 88,879 documents for 98 topics from TREC Web track 2000-2001. Documents from Robust and WT10G may contain spelling or other errors.",1,INEX,True
"As the rst baseline we used a probabilistic graphic model proposed in [18] hereina er referred to as Lapata. Because of page number constraints, we are not detailing this method in this paper.",0,,False
e probabilities were learned from the Wikipedia dataset. For evaluation we calculated text score as the average score between the adjacent sentences. e second baseline TSP is a special case of our approach with equal weights for all relations. We also examined a variant of this method where the similarity is based on noun only (TSPNoun). We estimated the text coherence as the average cosine similarity between the neighboring sentences.,1,Wiki,True
"As in [2, 9, 20, 21, 23], we compare scores assigned to initial documents and the same documents but with randomly permuted sentences. is pairwise evaluation approach was justi ed in [21]. As in previous approaches, we assumed that the best SO is produced by a human and a good metric should re ect that by assigning higher score to initial SO. Besides, we hypothesized that a good metric has small degradation of results provoked by small permutation in SO and greater rate of shu ing provokes larger e ect since the obtained order is remoter from the human-made one.",1,ad,True
"erefore, as in [12], we consider the following types of datasets: (1) Source collection (O), (2) Rn-collection (Rn), (3) R-collection (R). R-collection is derived from the source collection by shu ing all sentences within each document. Rn-collection is generated from the source collection by a random shi of n sentences within each document. We used R1 and R2 collections. e introduction of transitional Rn-collections di ers from the approaches used in [2, 9, 20, 21, 23].",0,,False
We calculated system accuracy which shows the number of times a system prefers the original order over its permutation divided by the total number of test pairs.,0,,False
is approach for metric evaluation is completely automatic and requires only a text corpus.,0,,False
"We conducted the second set of experiments on two corpora that are widely used for SO assessment: (1) airplane Accidents from the National Transportation Safety Board and (2) articles about Earthquakes from the North American News Corpus [9, 21, 23]. Each of these corpora has 100 original texts and for each document 20 permutations (2000 in total). We compared our accuracy results with those reported in the literature, namely entity grid models (Content + Egrid, Content + HMM-prodn, Content + HMM-d-seq, Egrid + HMM-prodn, Egrid + HMM-d-seq,",1,corpora,True
2terrier.org is a search engine platform developed by the University of Glasgow,0,,False
Robust Wikipedia,1,Robust,True
"Egrid + Content + HMM-prodn, Egrid + Content + HMM-dseq, Egrid + Content + HMM-prodn + HMM-d-seq)3, discourse relation based approaches (Type+Arg+Sal,Arg+Sal, Type+Sal, Type+Arg, Baseline+Type+Arg+Sal)4, probabilistic content model (Probabilistic content)5 and topic based model (Topic-relaxed)5.",0,,False
Table 1: % of times where initial order is scored higher/low-,0,,False
er/equally than/to permuted text,0,,False
"Data Method O>R1 R1>O O,R1 O>R2 R2>O O,R2 O>R R>O O,R Lapata 38.80 44.07 17.14 38.33 49.26 12.42 30.25 58.96 10.79 TSP 58.04 26.20 15.75 67.13 25.10 7.77 81.43 13.86 4.71",0,,False
TSPNoun 40.64 19.16 40.21 52.96 21.70 25.35 73.17 14.58 12.25 TopCom 58.86 25.99 15.16 68.12 24.72 7.16 83.64 12.39 3.96 TCNoun 41.04 19.89 39.08 53.21 22.53 24.25 73.82 14.83 11.35 Lapata 40.85 50.42 8.73 41.45 55.00 3.55 35.02 63.09 1.89,0,,False
TSP 57.15 29.09 13.76 65.85 28.58 5.57 81.77 15.47 2.76 TSPNoun 44.68 23.67 31.66 55.94 26.87 17.20 75.46 18.56 5.98 TopCom 57.66 29.23 13.11 66.18 28.91 4.92 82.63 15.45 1.92 TCNoun 45.14 24.45 30.41 56.07 27.85 16.07 75.57 19.36 5.07,0,,False
Lapata 42.78 51.30 5.92 42.37 55.57 2.06 32.33 66.66 1.01 TSP 54.35 24.02 21.62 65.42 24.81 9.78 84.99 12.07 2.95,0,,False
TSPNoun 36.22 15.78 48.00 49.00 19.38 31.62 76.69 13.41 9.90 TopCom 54.31 24.46 21.22 65.24 25.37 9.38 85.72 11.69 2.59 TCNoun 36.42 16.21 47.38 48.91 20.04 31.06 76.84 13.69 9.47,0,,False
WT10G,1,WT,True
24500,0,,False
24400,0,,False
24300,0,,False
7000 24200,0,,False
24100 6000,0,,False
24000,0,,False
"TT (Wct,.5; Wtc,.75; Wcc,1)",0,,False
5000 23900 4000 23800,0,,False
"CT (Wtt,.25; Wtc,.75T; TWcc,1)",0,,False
"TC (Wtt,.25; Wct,.5; CWTcc,1)",1,CW,True
3000 23700 23600,0,,False
"CC (Wtt,.25; Wct,.5;TWCtc,.75)",0,,False
2000,0,,False
0.25,0,,False
0.5,0,,False
0.75,0,,False
1,0,,False
CC 24500,0,,False
"Figure 1: Correlation between wtt , wct , wtc , wcc & accuracy",0,,False
"In Table 1, O, R, R1 and R2 refer to the initial sentence order and the permutations described above and O > /< /, R· shows the proportion of times where initial order was scored higher/lower/equally than/to permuted text for the best set of parameter values wtt ,"" 0.25, wct "","" 0.5, wtc "","" 0.75, and wcc "","" 1. Topic-comment term based method is denoted by TopCom. For all collections according to the number of times where the original order was ranked higher than the shu ed one O > R, the topic-comment approach outperformed the simple similarity-based metrics and Lapata's baseline. Smaller permutations in sentence order provoke smaller changes in the score. In general noun-based similarity is less accurate than all term based methods. It could be caused by lower probability of non-zero similarity between the adjacent sentences. However, both topic-comment based methods showed be er results than their analogues that do not consider text information structure. We varied the coe cients (wtt , wct , wtc , wcc )  {0.25, 0.5, 0.75, 1}4 on the Wikipedia collection. Figure 1 visualizes the correlation between the number of times where the initial document is preferred to shu ed one O > R and each coe cient with the xed values of others. Smaller values of wtt , and wct refer to higher O > R, while be er results correspond to higher wtc and wcc .""",1,ad,True
Table 2 presents the results of accuracy on articles about Earthquakes and airplane Accidents reports. On the Accidents dataset,0,,False
3reported as in [23] 4reported as in [21] 5reported as in [9],0,,False
1063,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 2: Accuracy (%),0,,False
Method,0,,False
Accidents Earthquakes,0,,False
TSP,0,,False
88.7,0,,False
73.5,0,,False
TSPNoun,0,,False
89,0,,False
60.5,0,,False
TopCom,0,,False
86.3,0,,False
75.1,0,,False
TCNoun,0,,False
87,0,,False
60.4,0,,False
Content + Egrid,0,,False
76.8,0,,False
90.7,0,,False
Content + HMM-prodn,0,,False
74.2,0,,False
95.3,0,,False
Content + HMM-d-seq,0,,False
82.1,0,,False
90.3,0,,False
Egrid + HMM-prodn,0,,False
79.6,0,,False
93.9,0,,False
Egrid + HMM-d-seq,0,,False
84.2,0,,False
91.1,0,,False
Egrid + Content + HMM-prodn,0,,False
79.5,0,,False
95.0,0,,False
Egrid + Content + HMM-d-seq,0,,False
84.1,0,,False
92.3,0,,False
Egrid + Content + HMM-prodn + HMM-d-seq 83.6,0,,False
95.7,0,,False
Probabilistic content,0,,False
74,0,,False
-,0,,False
Topic-relaxed,0,,False
94,0,,False
-,0,,False
Baseline,0,,False
89.93,0,,False
83.59,0,,False
Type+Arg+Sal,0,,False
89.38,0,,False
86.50,0,,False
Arg+Sal,0,,False
87.06,0,,False
85.89,0,,False
Type+Sal,0,,False
86.05,0,,False
82.98,0,,False
Type+Arg,0,,False
87.87,0,,False
82.67,0,,False
Baseline+Type+Arg+Sal,0,,False
91.64,0,,False
89.72,0,,False
"we obtained the results comparable with the state of the art. For the Earthquakes articles, the accuracy of our system is slightly lower. It can be explained by the following facts: (1) models are trained and tested separately for each dataset [9, 21, 23]; (2) datasets are very homogeneous (some articles are similar up to 90% of words) and, as noted in [9], very constrained in terms of subject and style. In contrast, the coe cients for our method were learned from the Wikipedia collection. is proves that our metric is general and not restricted by a collection but it demonstrates the results comparable with the state of the art machine learning based approaches.",1,Wiki,True
5 CONCLUSIONS,0,,False
"We introduced a novel self-su cient metric for SO assessment based on topic-comment structure. It has linear complexity and requires only POS-tagging. We evaluated our method on three test collections where it demonstrated high accuracy and signi cantly outperformed similarity-based baselines as well as a transition probability based approach. e evaluation results allow drawing conclusions that (1) topic-comment methods are more e ective than simple similarity based approaches; (2) in general, noun-based similarity is less accurate. In contrast to the state of the art approaches, our method is general and not restricted by a collection but it demonstrates comparable results. One of the promising direction of the future work is the integration of co-reference resolution, synonyms and IDF. Another possible improvement is applying syntactic parsing and linguistic templates for topic-comment structure extraction.",0,,False
REFERENCES,0,,False
"[1] Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring Strategies for Sentence Ordering in Multidocument News Summarization. Journal of Arti cial Intelligence Research (2002), 35­55. 17.",1,adad,True
"[2] Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entitybased approach. Computational Linguistics 34, 1 (2008), 1­34.",0,,False
"[3] Patrice Bellot, Ve´ronique Moriceau, Josiane Mothe, Eric SanJuan, and Xavier Tannier. 2013. Overview of INEX 2013. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization. LNCS, Vol. 8138. 269­281. DOI: h p://dx.doi.org/10.1007/978-3-642-40802-1 27",1,INEX,True
"[4] Danushka Bollegala, Naoaki Okazaki, and Mitsuru Ishizuka. 2010. A bo om-up approach to sentence ordering for multi-document summarization. Information processing & management 46, 1 (2010), 89­109.",0,,False
[5] Abdelhamid Bouchachia and R Mi ermeir. 2003. A neural cascade architecture for document retrieval. In Proc. of the International Joint Conference on Neural,1,ad,True
"Networks, 2003., Vol. 3. IEEE, 1915­1920. [6] Jieun Chae and Ani Nenkova. 2009. Predicting the uency of text with shallow",0,,False
"structural features: case studies of machine translation and human­wri en text. Proc. of the 12th Conference of the European Chapter of the ACL (2009), 139­147. [7] J. S. Chall and E. Dale. 1995. Readability revisited: e new Dale­Chall readability. MA: Brookline Books, Cambridge. [8] Kevyn Collins- ompson and Jamie Callan. 2004. A Language Modeling Approach to Predicting Reading Di culty. Proc. of HLT/NAACL 4 (2004). [9] Micha Elsner, Joseph L. Austerweil, and Eugene Charniak. 2007. A Uni ed Local and Global Model for Discourse Coherence.. In HLT-NAACL. 436­443. [10] Micha Elsner and Eugene Charniak. 2008. Coreference-inspired Coherence Modeling. In Proc. of the 46th Annual Meeting of the ACL on Human Language Technologies: Short Papers (HLT-Short '08). ACL, Stroudsburg, PA, USA, 41­44. [11] Liana Ermakova. 2015. A Method for Short Message Contextualization: Experiments at CLEF/INEX. In Experimental IR Meets Multilinguality, Multimodality,",1,ad,True
"and Interaction: 6th International Conference of the CLEF Association, CLEF'15, Toulouse, France, September 8-11, 2015, Proceedings. Springer International Publishing, Cham, 352­363. DOI:h p://dx.doi.org/10.1007/978-3-319-24027-5 38 [12] Liana Ermakova. 2016. Automatic Sentence Ordering Assessment Based on Similarity. In Proc. of EVIA 2016, Tokyo, Japan, 07/06/2016. NII. [13] Liana Ermakova and Josiane Mothe. 2016. Document re-ranking based on topiccomment structure. In X IEEE International Conference RCIS, Grenoble, France, June 1-3, 2016. 1­10. [14] E. Fry. 1990. A readability formula for short passages. Journal of Reading 8 (1990), 594­597. 33. [15] Michael Go¨tze, Stephanie Dipper, and Stavros Skopeteas. 2007. Information",1,CLEF,True
"Structure in Cross-Linguistic Corpora: Annotation Guidelines for Phonology, Morphology, Syntax, Semantics, and Information Structure. Interdisciplinary Studies on Information Structure (ISIS), Working papers of the SFB 632, Vol. 7. [16] David Hawking and Nick Craswell. 2002. Overview of the TREC-2001 web track. NIST special publication (2002), 61­67. [17] Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, and others. 2010. HILDA: a discourse parser using support vector machine classi cation. Dialogue & Discourse 1, 3 (2010). [18] Mirella Lapata. 2003. Probabilistic Text Structuring: Experiments with Sentence Ordering. Proc. of ACL (2003), 542­552. [19] Guy Lebanon and John La erty. 2002. Cranking: Combining rankings using conditional probability models on permutations. Machine Learning: Proc. of the Nineteenth International Conference (2002), 363­370. [20] Jiwei Li and Eduard H. Hovy. 2014. A Model of Coherence Based on Distributed Sentence Representation.. In EMNLP, Alessandro Moschi i, Bo Pang, and Walter Daelemans (Eds.). ACL, 2039­2048. [21] Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011. Automatically Evaluating Text Coherence Using Discourse Relations. In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies - vol. 1. ACL, Stroudsburg, PA, USA, 997­1006. [22] Christina Lioma, Birger Larsen, and Wei Lu. 2012. Rhetorical Relations for Information Retrieval. In Proc. of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval. 931­940. [23] Annie Louis and Ani Nenkova. 2012. A Coherence Model Based on Syntactic Pa erns. In Proc. of EMNLP-CoNLL '12. ACL, Stroudsburg, PA, USA, 1157­1168. [24] V. Mathesius and J. Vachek. 1975. A Functional Analysis of Present Day English on a General Linguistic Basis. Mouton. [25] A. Mu on, M. Dras, S. Wan, and R. Dale. 2007. Gleu: Automatic evaluation of sentence­level uency. ACL'07 (2007), 344­351. [26] Eric SanJuan, Ve´ronique Moriceau, Xavier Tannier, Patrice Bellot, and Josiane Mothe. 2012. Overview of the INEX 2011 estion Answering Track (QA@INEX). In Focused Retrieval of Content and Structure, Shlomo Geva, Jaap Kamps, and Ralf Schenkel (Eds.). Lecture Notes in Computer Science, Vol. 7424. Springer Berlin Heidelberg, 188­206. [27] L. Si and J. Callan. 2001. A statistical model for scienti c readability. Proc. of the tenth international conference on Information and knowledge management (2001), 574­576. [28] Radu Soricut and Daniel Marcu. 2003. Sentence Level Discourse Parsing Using Syntactic and Lexical Information. In Proc. of NAACL '03 on Human Language Technology - vol. 1. ACL, 149­156. [29] AJ Stenner, Ivan Horabin, Dean R Smith, and Malbert Smith. 1988. e lexile framework. Durham, NC: MetaMetrics (1988). [30] Jade Tavernier and Patrice Bellot. 2011. Combining relevance and readability for INEX 2011 estion­Answering track. (2011), 185­195. [31] Ellen M. Voorhees and Donna Harman. 2000. Overview of the Sixth Text REtrieval Conference (TREC­6). [32] S. Wan, R. Dale, and M. Dras. 2005. Searching for grammaticality: Propagating dependencies in the viterbi algorithm. Proc. of the Tenth European Workshop on Natural Language Generation (2005). [33] S. Zwarts and M. Dras. 2008. Choosing the right translation: A syntactically informed classi cation approach. Proc. of the 22nd International Conference on Computational Linguistics (2008), 1153­1160.",1,TREC,True
1064,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Term Proximity Constraints for Pseudo-Relevance Feedback,0,,False
Ali Montazeralghaem,0,,False
"School of ECE, College of Engineering University of Tehran, Iran ali.montazer@ut.ac.ir",0,,False
Hamed Zamani,0,,False
"Center for Intelligent Information Retrieval,",0,,False
University of Massachuse s Amherst zamani@cs.umass.edu,0,,False
Azadeh Shakery,1,ad,True
"School of ECE, College of Engineering University of Tehran, Iran",0,,False
"School of Computer Science, Institute for Research in Fundamental Sciences",0,,False
shakery@ut.ac.ir,0,,False
ABSTRACT,0,,False
"Pseudo-relevance feedback (PRF) refers to a query expansion strategy based on top-retrieved documents, which has been shown to be highly e ective in many retrieval models. Previous work has introduced a set of constraints (axioms) that should be satis ed by any PRF model. In this paper, we propose three additional constraints based on the proximity of feedback terms to the query terms in the feedback documents. As a case study, we consider the log-logistic model, a state-of-the-art PRF model that has been proven to be a successful method in satisfying the existing PRF constraints, and show that it does not satisfy the proposed constraints. We further modify the log-logistic model based on the proposed proximity-based constraints. Experiments on four TREC collections demonstrate the e ectiveness of the proposed constraints. Our modi cation to the log-logistic model leads to signi cant and substantial (up to 15%) improvements. Furthermore, we show that the proposed proximity-based function outperforms the well-known Gaussian kernel which does not satisfy all the proposed constraints.",1,ad,True
KEYWORDS,0,,False
"Term proximity, term position, axiomatic analysis, pseudo-relevance feedback, query expansion",0,,False
"ACM Reference format: Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery. 2017. Term Proximity Constraints for Pseudo-Relevance Feedback. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: 10.1145/3077136.3080728",1,ad,True
1 INTRODUCTION,1,DUC,True
"Pseudo-relevance feedback (PRF) is a query expansion strategy to address the vocabulary mismatch problem in information retrieval (IR). In PRF, a small set of top-retrieved documents (i.e., pseudo-relevant documents) are assumed to be relevant to the initial query. ese pseudo-relevant documents are further used for updating the query model in order to improve the retrieval performance. PRF has been proven to be highly e ective in many retrieval models [2, 10, 13, 14].",1,ad,True
eoretical analysis of PRF models has shown that there are several constraints (axioms) that every PRF model should satisfy. Based,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080728",1,ad,True
"on these theoretical studies, several modi cations, e.g., [1, 3, 9, 10], have been made to the existing PRF models which lead to signi cant improvements in the retrieval performance. Although term proximity has been shown to be a strong evidence for improving the retrieval performance [5, 12], especially in the PRF task [6­8], none of the existing constraints for PRF takes term proximity into account.1",1,ad,True
"In this paper, we provide a theoretical analysis for the use of term proximity in PRF models. To do so, we introduce three PRF constraints based on the proximity of candidate feedback terms and the query terms in the feedback documents. According to the rst constraint (""proximity e ect""), the candidate feedback terms that are positionally closer to the query terms in the feedback documents should be given higher weights in the feedback model. e second constraint (""convexity e ect"") decreases the e ect of term proximity when the distance between terms increases. e third constraint indicates that proximity to the less common query terms is more important than proximity to the query terms that are general.",0,,False
"Furthermore, previous work on leveraging term proximity for IR tasks, including the positional relevance model, showed that the Gaussian kernel is an e ective way for enhancing IR models with term proximity information [5, 6, 8]. In this paper, we show that the Gaussian kernel does not satisfy all the proposed constraints, and thus it could not be the best way for applying term proximity to PRF models.",0,,False
e primary contributions of this work can be summarized as follows:,0,,False
· Introducing three proximity-based constraints for theoretical analysis of PRF models.,0,,False
"· Studying and modifying the log-logistic feedback model [2], a state-of-the-art PRF model that outperforms many existing models, including the mixture model [14] and the geometric relevance model [11] (see [3] for more details).",0,,False
· Introducing a variant of the Exponential kernel that satis es all the proposed constraints.,0,,False
"· Evaluating our models using four TREC collections which demonstrates signi cant improvements over the original log-logistic model as well as the model enriched with the Gaussian kernel, a widely used weighting function for enhancing IR models using term proximity [5, 6, 8].",1,TREC,True
2 METHODOLOGY,0,,False
"In this section, we rst introduce three proximity-based constraints that (pseudo-) relevance feedback methods should satisfy. We further analyze the log-logistic model [2], and show that this model",0,,False
"1Tao and Zhai [12] proposed two proximity-based constraints for retrieval models, but not for PRF models.",0,,False
1085,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
does not satisfy the proposed constraints. We nally modify the log-logistic feedback model in order to satisfy all the constraints.,0,,False
"We rst introduce our notation. Let FW (w; F , Pw , Q) denote the feedback weight function that assigns a real-valued weight to each feedback term w for a given query Q based on the feedback document set F . Pw is a set of term-dependent parameters. For simplicity, FW (w) is henceforth used as the feedback weight function. In the following equations, T F and I DF are term frequency and inverse document frequency, respectively. | · | represents the size of the given set.",0,,False
2.1 Constraints,0,,False
"In this subsection, we introduce three proximity-based constraints for PRF methods.",0,,False
"[Proximity e ect] Let d(w, q, D) denote the proximity weight of a candidate feedback term w and a given query term q in a feedback document D. en, the following constraint should hold:",0,,False
"FW (w) d(w, q, D)",0,,False
<,0,,False
0,0,,False
"According to this constraint, the candidate feedback terms that are closer to the query terms in the feedback documents should have higher weights. Intuitively, the closer terms to the query terms are more likely to be relevant to the query.",0,,False
[Convexity e ect] e feedback weight function should be convex with respect to the distance of candidate feedback terms from the query terms. We can formalize this constraint as follows:,0,,False
"2FW (w) d(w, q, D)2",0,,False
>0,0,,False
e intuition behind this constraint is that decreasing the e ect of,0,,False
the proximity e ect should be less marked in high distance ranges.,0,,False
"[ ery IDF e ect] Let Q ,"" {q1, q2} be a query with two query terms q1 and q2. Let D1 and D2 denote two feedback documents with equal length, such that q1 only appears in D1, and q2 only appears in D2. Let w1 and w2 be two candidate feedback terms, such that T F (w1, D1) "","" T F (w2, D2), and w1 and w2 only appear in D1 and D2 in the feedback set, respectively. Also assume that d(w1, q1, D1) "","" d(w2, q2, D2) where d is the function to compute the proximity between two terms in a given document. We can say""",0,,False
"if I DF (q1) > I DF (q2), then we should have:",0,,False
FW (w1) > FW (w2),0,,False
"Intuitively, this constraint indicates that proximity to the query terms that are general is less important than proximity to the uncommon query terms. For instance, if a query contains a general term (let say a stopword) then proximity to this term should be less important than the discriminative terms that occur in the query.",0,,False
2.2 Modifying the Log-Logistic Model,0,,False
"As a case study, we analyze and modify the log-logistic feedback model [2]. e reason is that this method has been shown to outperform many strong baselines, including the mixture model [14] and the geometric relevance model [11]. It has been also shown that this method successfully satis es all the PRF constraints proposed in [3]. e log-logistic feedback weight function for each term w is",0,,False
computed as:,0,,False
FW (w),0,,False
",",0,,False
1 |F |,0,,False
D F,0,,False
log(1 +,0,,False
"t(w, D) ) w",0,,False
(1),0,,False
where,0,,False
t,0,,False
"(w ,",0,,False
d,0,,False
),0,,False
",",0,,False
T,0,,False
F,0,,False
"(w,",0,,False
D),0,,False
log(1,0,,False
+,0,,False
c,0,,False
a |D,0,,False
l,0,,False
|,0,,False
),0,,False
(a,0,,False
l denotes the average,0,,False
document length and c is a free hyper-parameter that controls,0,,False
the document length e ect). e document frequency term w is calculated as:,0,,False
"w , Nw /N",0,,False
(2),0,,False
"where Nw and N denote the number of documents in the collection that contain w and the total number of documents in the collection,",0,,False
respectively. FW (w) is then interpolated with the original query,0,,False
based on a free parameter (feedback coe cient)[2].,0,,False
It can be easily shown that the log-logistic feedback model does,0,,False
"not satisfy the proximity-based constraints, since its formulation",0,,False
does not contain any proximity-based component. To the best of,0,,False
"our knowledge, this is the rst a empt to enrich the log-logistic",0,,False
feedback model using term proximity information.,0,,False
"Regarding the query term independence assumption, we propose",0,,False
to modify the log-logistic model as follows to satisfy the proximity-,0,,False
based constraints:,0,,False
"FWprox (w) , FW (w) ",0,,False
" (w, q, D)",0,,False
(3),0,,False
D F q Q,0,,False
"where q is a query term and  (w, q, D) is a function that computes the proximity of w and q in document D.",0,,False
"To de ne the function  , we propose to use the Exponential kernel that satis es the ""proximity e ect"" and the ""convexity e ect"" constraints. We modify the Exponential kernel by adding an IDF term to satisfy the "" ery IDF e ect"" constraint, as well. e function  can be computed as follows:",1,ad,True
" (w, q, D) , exp",0,,False
"-d(w, q, D) ",0,,False
. log 1 q,0,,False
(4),0,,False
"where d(w, q, D) denotes the distance function for two given terms w and q in document D. q is the document frequency component for the query term q (see Equation (2)) and  is a free parameter. Several approaches have been proposed to compute d(w, q, D), such as average, minimum, and maximum distances. Tao and Zhai [12] showed that using the minimum distance between the term w and the query term q in the feedback document outperforms other distance functions. We also use the minimum distance as follows:",0,,False
"d(w, q, D) , min |wi - qj |",0,,False
(5),0,,False
wi wì & qj qì,0,,False
"where wì and qì are two vectors containing the positions of term w and query term q in document D, respectively.",0,,False
3 EXPERIMENTS,0,,False
3.1 Experimental Setup,0,,False
"We used four standard TREC collections in our experiments: AP (Associated Press 1988-89), Robust (TREC Robust Track 2004 collection), WT2g (TREC Web Track 2000 collection), and WT10g (TREC Web Track 2001-2002 collection). e rst two are newswire collections, while the next two are web collections containing more noisy documents. e statistics of these collections are reported in Table 1. We considered the title of topics as queries. All documents",1,TREC,True
1086,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Collections statistics.,0,,False
Collection AP,1,AP,True
Robust WT2g WT10g,1,Robust,True
TREC topics 51-200,1,TREC,True
301-450 & 601-700 401-450 451-550,0,,False
#docs 165k 528k 247k 1692k,0,,False
doc length 287 254 645 399,0,,False
#qrels 15838 17412 2279 5931,0,,False
were stemmed using the Porter stemmer and stopped using the standard INQUERY stopword list. We carried out the experiments using the Lemur toolkit2.,0,,False
"3.1.1 Parameter Se ing. e number of feedback documents, the feedback term count, and the feedback coe cient were set using 2-fold cross validation over the queries of each collection. We swept the number of feedback documents between {10, 25, 50, 75, 100}, the feedback term count between {10, 25, 50, 75, 100}, and the feedback coe cient between {0, 0.1, · · · , 1}. e parameters c and  were also selected using the same procedure from {2, 4, · · · , 10} and {25, 50, · · · , 500}, respectively. e parameter  in the Gaussian kernel is also set similarly.",0,,False
"3.1.2 Evaluation Metrics. To evaluate retrieval e ectiveness, we use mean average precision (MAP) of the top-ranked 1000 documents as the main evaluation metric. In addition, we also report the precision of the top 10 retrieved documents (P@10). Statistically signi cant di erences of average precisions are determined using the two-tailed paired t-test computed at a 95% con dence level. To evaluate robustness of methods, we consider the robustness index (RI) introduced in [4].",1,MAP,True
3.2 Results and Discussion,0,,False
"In this subsection, we rst empirically show that satisfying each of the introduced constraints improves the retrieval performance. Our experiments also demonstrate that the Gaussian kernel that has previously been used in the literature [5, 6, 8] is not as e ective as the proposed proximity weighting function, since the Gaussian kernel does not satisfy all the constraints.",0,,False
"3.2.1 Analysis of the Proximity-based Constraints. We consider two baselines: (1) the document retrieval method without feedback (NoPRF), and (2) the original log-logistic feedback model (LL). Although there are several e ective PRF methods, since in this paper we study the e ect of the proposed constraints in the log-logistic model, we do not consider other existing PRF methods.",0,,False
"To study the in uence of each of the proposed constraints on the retrieval performance, we consider three di erent proximity functions: (1) the quadratic function ( ad) that only satis es the ""proximity e ect"" constraint, (2) the exponential function (Exp) that satis es both ""proximity e ect"" and ""convexity e ect"" constraints, and (3) a modi ed version of the exponential function (Exp*) that satis es all three constraints. More detail is reported Table 2.",1,ad,True
"e results of the baselines and the aforementioned methods are reported in Table 3. According to this table, modifying the loglogistic method using each of the proximity functions improves the retrieval performance, in all collections. e MAP improvements are statistically signi cant in nearly all cases. is indicates the necessity of taking term proximity into account for the PRF task.",1,MAP,True
2h p://lemurproject.org/,0,,False
"Table 2: Summary of di erent proximity functions with respect to the proximity-based constraints (x ,"" d(w, q, D)).""",0,,False
"Func.  (w, q, D)",0,,False
Gaus,0,,False
exp[,0,,False
-x 2,0,,False
2 2,0,,False
],0,,False
ad,1,ad,True
-(,0,,False
x ,0,,False
)2,0,,False
+,0,,False
1,0,,False
Exp,0,,False
exp[,0,,False
-x ,0,,False
],0,,False
Exp*,0,,False
exp[,0,,False
-x ,0,,False
].,0,,False
log,0,,False
N Nq,0,,False
Proximity Convexity e ect e ect,0,,False
Yes Partially,0,,False
Yes,0,,False
No,0,,False
Yes,0,,False
Yes,0,,False
Yes,0,,False
Yes,0,,False
ery IDF e ect,0,,False
No,0,,False
No,0,,False
No,0,,False
Yes,0,,False
"e results demonstrate that LL+Exp outperforms LL+ ad and LL+Exp* outperforms LL+Exp, in all collections. e improvements in the web collections are higher than those in the newswire collections. e reason is that these two collections are web crawls and contain more noisy documents compared to the newswire collections. e other reason is that the WT2g and WT10g documents are much longer than the AP and Robust documents on average (see Table 1). e in uence of proximity-based constraints can be highlighted in longer documents. Besides that, in terms of RI, LL+Exp* outperforms all the baselines in AP, WT2g and WT10g which shows the importance and robustness of ery IDF e ect and these improvements are impressive in WT2g and WT10g which shows that this e ect is important in noisy (web) collections.",1,ad,True
"3.2.2 Analysis of the Gaussian Kernel. In this set of experiments, we study the Gaussian kernel for computing the proximity weight, which has been shown to be the most e ective proximity function among the existing ones [5]. As reported in Table 2, employing the Gaussian kernel for PRF satis es the ""proximity e ect"" constraint. e ""convexity e ect"" constraint is only satis ed when d(w, q, D) >  . erefore, it does not satisfy the ""convexity e ect"" for the candidate feedback terms that are close to the query terms. We evaluate the Gaussian kernel by considering it as a term proximity weight function (LL+Gaus). According to the results reported in Table 3, LL+Exp and LL+Gaus perform comparably in the newswire collections, but LL+Exp outperforms LL+Gaus in the web collections (WT2g and WT10g). LL+Exp* also outperforms LL+Gaus in all collections. e improvements in the web collections are statistically signi cant.",1,WT,True
"3.2.3 Parameter Sensitivity. In the last set of experiments, we study the sensitivity of the proposed method to the following hyperparameters: the number of feedback terms added to the query, (2) the feedback interpolation coe cient, and (3) the parameter  (see Equation (4)). To do so, we sweep one of the parameters and x the other ones to their default values: 50 for feedback term count, 0.5 for feedback coe cient, and 25 for . In these experiments, we report the result for LL+Exp* that achieves the best performance in Table 3.",1,ad,True
"e results are plo ed in Figure 1, in terms of MAP. In this gure, the rst plot shows that the performance of LL+Exp* is stable with respect to the changes in the number of feedback terms, when more than 25 terms are added to the query. In other words, 25 terms are enough for expanding the query in most collections. e second plot in Figure 1 demonstrates that the behaviour of LL+Exp* in newswire collections is similar to each other. LL+Exp* also behaves similarly in the web collections. Interestingly, the feedback model estimated by LL+Exp* does not need to be interpolated with the original query",1,MAP,True
1087,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 3: Performance of di erent proximity functions applied to the log-logistic model. Superscripts 0/1/2 denote that the MAP improvements over NoPRF/LL/LL+Gaus are statistically signi cant. e highest value in each column is marked in bold.,1,MAP,True
Method,0,,False
AP MAP P@10 RI,1,AP,True
Robust MAP P@10 RI,1,Robust,True
WT2g MAP P@10 RI,1,WT,True
WT10g MAP P@10 RI,1,WT,True
NoPRF LL,0,,False
LL+Gaus,0,,False
LL+ ad LL+Exp LL+Exp*,1,ad,True
0.2642 0.4260 ­,0,,False
0.3385 0.4622 0.15,0,,False
0.347101 0.4695 0.19,0,,False
0.344101 0.4682 0.18 0.346801 0.4688 0.19 0.347501 0.4702 0.21,0,,False
0.2490 0.4237 ­,0,,False
0.2829 0.4393 0.33,0,,False
0.292601 0.4454 0.27,0,,False
0.292001 0.4530 0.30 0.293601 0.4442 0.28 0.295001 0.4430 0.30,0,,False
0.3033 0.4480 ­,0,,False
0.3276 0.4820 0.36,0,,False
0.33510 0.4920 0.38,0,,False
0.33090 0.4920 0.34 0.3418012 0.4840 0.38 0.3449012 0.4820 0.47,0,,False
0.2080 0.3030 ­,0,,False
0.2127 0.3187 0.08,0,,False
0.239301 0.3157 0.16,0,,False
0.21950 0.3075 0.02 0.243501 0.3247 0.19 0.2461012 0.3278 0.24,0,,False
0.35,0,,False
0.30,0,,False
0.25,0,,False
 AP Robust,1,AP,True
WT2g WT10g,1,WT,True
0.35,0,,False
 AP Robust,1,AP,True
WT2g WT10g,1,WT,True
0.30,0,,False
0.25,0,,False
 AP Robust,1,AP,True
WT2g WT10g,1,WT,True
0.35    ,0,,False
0.30,0,,False
0.25,0,,False
MAP MAP MAP,1,MAP,True
0.20 0,0,,False
25,0,,False
50,0,,False
75,0,,False
100,0,,False
0.20,0,,False
0.0,0,,False
0.2,0,,False
0.4,0,,False
0.6,0,,False
0.8,0,,False
1.0,0,,False
0.20 25 75 150,0,,False
300,0,,False
400,0,,False
500,0,,False
# of feedback terms,0,,False
feedback coefficient,0,,False
"Figure 1: Sensitivity of LL+Exp* to the number of feedback terms, the feedback coe cient, and the parameter .",0,,False
"model in the newswire collections. In the web collections (WT2g and WT10g), giving a small weight (i.e., 0.2) to the original query model can help to improve the retrieval performance. e reason could be related to the noisy nature of the web collections compared to the newswire collections. e last plot in Figure 1 shows that the proposed method is not highly sensitive to the parameter  when it is higher than 100. e results indicate that 25 and 50 would be proper values for this parameter. e results on the web collections are more sensitive to this parameter. e reason is that the documents in the web collections are much longer than those in the newswire collections (see Table 1).",1,WT,True
4 CONCLUSIONS AND FUTURE WORK,0,,False
"In this paper, we proposed three constraints for the pseudo-relevance feedback models, that focus on the proximity of the candidate feedback terms and the query terms in the feedback documents. To show the e ectiveness of the proposed constraints, we considered the log-logistic model, a state-of-the-art feedback model, as a case study. We rst showed that the log-logistic model does not satisfy the proximity-based constraints. We further modi ed it based on the proposed constraints. Our experiments on four standard TREC newswire and web collections demonstrated the e ectiveness of the proposed constraints for the PRF task. e modi ed log-logistic model signi cantly outperforms the original log-logistic model, in all collections. We also showed that the Gaussian kernel that has been used in previous proximity-based methods does not satisfy all the constraints. We show that the performance of the proposed variant of the exponential kernel is superior to those obtained by employing the Gaussian kernel. As a future direction, the other existing PRF models could be analyzed and modi ed based on the introduced constraints.",1,TREC,True
Acknowledgements. is work was supported in part by the Center for,0,,False
Intelligent Information Retrieval and in part by a grant from the Institute,0,,False
"for Research in Fundamental Sciences (No. CS1396-4-51). Any opinions,",0,,False
ndings and conclusions or recommendations expressed in this material,0,,False
are those of the authors and do not necessarily re ect those of the sponsors.,0,,False
REFERENCES,0,,False
"[1] Mozhdeh Ariannezhad, Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery. 2017. Iterative Estimation of Document Relevance Score for PseudoRelevance Feedback. In ECIR '17. 676­683.",1,ad,True
[2] Ste´phane Clinchant and Eric Gaussier. 2010. Information-based Models for Ad Hoc IR. In SIGIR '10. 234­241.,0,,False
[3] Ste´phane Clinchant and Eric Gaussier. 2013. A eoretical Analysis of PseudoRelevance Feedback Models. In ICTIR '13. 6­13.,0,,False
[4] Kevyn Collins- ompson. 2009. Reducing the Risk of ery Expansion via Robust Constrained Optimization. In CIKM '09. 837­846.,1,Robust,True
[5] Yuanhua Lv and ChengXiang Zhai. 2009. Positional Language Models for Information Retrieval. In SIGIR '09. 299­306.,0,,False
[6] Yuanhua Lv and ChengXiang Zhai. 2010. Positional Relevance Model for Pseudorelevance Feedback. In SIGIR '10. 579­586.,0,,False
"[7] Yuanhua Lv, ChengXiang Zhai, and Wan Chen. 2011. A Boosting Approach to Improving Pseudo-relevance Feedback. In SIGIR '11. 165­174.",0,,False
"[8] Jun Miao, Jimmy Xiangji Huang, and Zheng Ye. 2012. Proximity-based Rocchio's Model for Pseudo Relevance. In SIGIR '12. 535­544.",0,,False
"[9] Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery. 2016. Axiomatic Analysis for Improving the Log-Logistic Feedback Model. In SIGIR '16. 765­768.",1,ad,True
"[10] Dipasree Pal, Mandar Mitra, and Samar Bha acharya. 2015. Improving Pseudo Relevance Feedback in the Divergence from Randomness Model. In ICTIR '15. 325­328.",0,,False
[11] Jangwon Seo and W. Bruce Cro . 2010. Geometric Representations for Multiple Documents. In SIGIR '10. 251­258.,0,,False
[12] Tao Tao and ChengXiang Zhai. 2007. An Exploration of Proximity Measures in Information Retrieval. In SIGIR '07. 295­302.,0,,False
"[13] Hamed Zamani, Javid Dadashkarimi, Azadeh Shakery, and W. Bruce Cro . 2016. Pseudo-Relevance Feedback Based on Matrix Factorization. In CIKM '16. 1483­ 1492.",1,ad,True
[14] Chengxiang Zhai and John La erty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01. 403­410.,0,,False
1088,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Gauging the ality of Relevance Assessments using Inter-Rater Agreement,0,,False
Tadele T. Damessie,1,ad,True
"RMIT University Melbourne, Australia",0,,False
Falk Scholer,0,,False
"RMIT University Melbourne, Australia",0,,False
ABSTRACT,0,,False
"In recent years, gathering relevance judgments through non-topic originators has become an increasingly important problem in Information Retrieval. Relevance judgments can be used to measure the eectiveness of a system, and are oen needed to build supervised learning models in learning-to-rank retrieval systems. e two most popular approaches to gathering bronze level judgments ­ where the judge is not the originator of the information need for which relevance is being assessed, and is not a topic expert ­ is through a controlled user study, or through crowdsourcing. However, judging comes at a cost (in time, and usually money) and the quality of the judgments can vary widely. In this work, we directly compare the reliability of judgments using three dierent types of bronze assessor groups. Our rst group is a controlled Lab group; the second and third are two dierent crowdsourcing groups, CF-Document where assessors were free to judge any number of documents for a topic, and CF-Topic where judges were required to judge all of the documents from a single topic, in a manner similar to the Lab group. Our study shows that Lab assessors exhibit a higher level of agreement with a set of ground truth judgments than CF-Topic and CF-Document assessors. Inter-rater agreement rates show analogous trends. ese nding suggests that in the absence of ground truth data, agreement between assessors can be used to reliably gauge the quality of relevance judgments gathered from secondary assessors, and that controlled user studies are more likely to produce reliable judgments despite being more costly.",0,,False
1 INTRODUCTION,1,DUC,True
"Gathering relevance judgments using humans is a key component in building Information Retrieval test collections. However, human interpretation of ""relevance"" is an inherently subjective process [11]. According to Tang and Solomon [16], judging relevance is a dynamic, multidimensional process likely to vary between assessors, and sometimes even with a single assessor at dierent stages of the process. For example, Scholer et al. [13] found that 19% of duplicate",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: hp://dx.doi.org/10.1145/3077136.3080729",1,ad,True
ao P. Nghiem,0,,False
"RMIT University Melbourne, Australia",0,,False
J. Shane Culpepper,0,,False
"RMIT University Melbourne, Australia",0,,False
document pairings were judged inconsistently in the TREC-7 and TREC-8 test collections. Understanding the factors that lead to such variation in relevance assessments is crucial to reliable test collection development.,1,TREC,True
"To address this issue, Bailey et al. [3] proposed three classes of judges ­ gold, silver and bronze ­ based on the expertise of the assessor. Gold judges are topic originators as well as subject experts; whereas silver judges are subject experts but not topic originators. Bronze judges are neither topic originators nor subject experts. But are all judges in a single class really the same? Secondary assessors who are neither topic creators nor experts are all bronze assessors, but there are in fact many dierent types of assessors who fall into this class. As assessment at the bronze level is now becoming a common practice in IR, in particular with the growing popularity of crowdsourcing, we set up an experiment to investigate the homogeneity of assessment quality using three dierent variants of bronze judges. e classes used in this study are:",1,ad,True
"· Lab: is group of assessors carried out a relevance assessment task in a monitored lab environment, with a requirement to assess a pre-determined number of 30 documents in relation to a single search topic.",0,,False
· CF-Topic: is group of assessors are an exact replica of the Lab group task except that the task was administered using the CrowdFlower crowdsourcing platform.,1,ad,True
"· CF-Document: is group of assessors performed the task using CrowdFlower just as the CF-Topic group, but unlike the other two groups, each participant could judge as few (minimum 1) or as many (maximum 30) documents as they liked for a topic.",0,,False
Our main research question can formally be stated as:,0,,False
Research estion: Are there dierences in the quality of relevance judgments gathered from dierent sub-classes of bronze-level judges?,0,,False
2 RELATED WORK,0,,False
"e subjective nature of relevance is likely to result in disagreement between judges [11, 15]. Voorhees [18] was among the rst to study this phenomenon, and quantied agreement in relevance assessment using overlap between primary TREC assessors and two secondary assessors on 48 topics. A total of 30% of the documents judged relevant by the primary assessor were judged non-relevant, and less than 3% of the documents initially judged as non-relevant by the primary assessor were judged relevant by the secondary assessors.",1,TREC,True
1089,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Number of documents judged,0,,False
0,0,,False
20,0,,False
40,0,,False
60,0,,False
80,0,,False
100,0,,False
Assessors,0,,False
Figure 1: Distribution of number of documents judged per assessor by the CF-Document group.,0,,False
"Sormunen [14] also compared judgments from a group of 9 master's students using a 4-point ordinal relevance scale with TREC binary assessments on 38 topics. Around 25% of the documents originally judged as relevant by the TREC assessors were re-assessed as non-relevant, and around 1% of the documents originally judged as non-relevant were re-assessed as relevant. Al-Maskari et al. [1] also ran an experiment on 56 TREC-8 topics using 56 participants in an interactive search task. e study found a 37% dierence between TREC and non-TREC assessors. at is, out of the 2, 262 documents judged relevant by the non-TREC assessors, 834 of the documents were judged non-relevant by the TREC assessors. In both studies, there is a clear dierence between the TREC assessors who are topic originators, and the non-TREC assessors who oen are not.",1,TREC,True
"To address dierences between TREC judges and secondary assessors, Bailey et al. [3] identied three classes of judges ­ gold, silver and bronze ­ as discussed in Section 1. Bailey et al. found that assessments generated by silver judges were oen comparable to gold judges, but that extra care was needed when using bronze level judgments. However, the study did not prescribe exactly how this might be accomplished. In this study, we focus on dierent types of bronze level of assessors, as they now represent the most common class of judges outside of evaluation campaigns such as TREC which are being employed in large scale assessment gathering initiatives.",1,ad,True
3 METHODS AND DATASETS,0,,False
e TREC 7 and 8 datasets are used in this study. We focus on,1,TREC,True
"topics from these collections since they are widely believed to be among the most complete collections available [10], and provide",0,,False
a strong ground truth when aempting to quantify reliability in re-assessment exercises. Our work builds on two previous stud-,0,,False
"ies using the same topic conguration, and which provide further",0,,False
"details about the user study conguration [5, 6]. We use 4 dierent topics: the 2 highest and 2 lowest performing topics from the",0,,False
"dataset were selected using the average precision of each topic, averaged over the 110 runs submied to TREC 2004 Robust track.",1,TREC,True
"is approach, called average-average-precision (AAP), was initially described by Carteree et al. [4], and used to quantify topic",1,AP,True
diculty. Topic #365 (el nino) and #410 (schengen agreement),0,,False
"have the 2 highest AAP scores, and topic #378 (euro opposition)",1,AP,True
and #448 (,0,,False
) are the 2 lowest AAP scoring topics in,1,AP,True
ship losses,0,,False
"the collection. For assessment, 30 documents were chosen for each",0,,False
"topic, in proportion to an existing distribution of graded document relevance judgments made by Sormunen [14].",1,ad,True
"A total of 32, 40 and 43 assessors judged documents in the Lab, CFTopic and CF-Document experimental groups, respectively. For all crowdsourcing experiments, a mandatory explanation of relevance assignment per document was required, and manually checked as a quality control, to ensure that crowdsourcing participants were performing assessments in good faith. A total of 10 assessors, 5 from CF-Topic and 5 from CF-Document failed the sanity check, and their data was removed from the nal evaluation. All crowdsourcing experiments were conducted using the CrowdFlower platform in a manner similar to previously run studies [2].",0,,False
"e setup for the CF-Document group was designed to be as exible as possible, with assessors free to judge any number of the 30 documents for any of the 4 topics which were assigned randomly by the system. is setup introduces challenges during nal data analysis, however, since assessors judged an unequal number of documents, as shown in Figure 1, and a comparison of agreement between assessors with the same level of precision requires an incomplete balanced block design to be constructed as described by Fleiss [7]. is results in a sparse matrix of relevance scores for the maximum number of unique documents (30 per topic in our case) across the 121 unique assessors who contributed judgments.",0,,False
"Krippendor's Alpha ( ) is a chance-corrected measure of agreement, and not aected by dierences in sample sizes or missing values, and therefore appropriate for analysis of our experimental data [8]. Cohen's Kappa ( ) which is more suited for categorical data [17] is also used to quantify assessment quality against a gold standard. e values produced by these metrics is between 1 and 1, where a level of 0 indicates agreement at the level predicted by chance, 1 signies perfect agreement between raters, and a negative score occurs when agreement is less than what is expected by chance alone.",0,,False
4 RESULTS AND DISCUSSION,0,,False
"Assessor reliability ­ measured by the mean pairwise agreement between each assessor and the Sormunen gold standard assessments ­ is used to assess the quality of the assessments from each experimental group. is analysis is then compared with a measure of assessment quality using only inter-rater agreement, in the absence of any ground truth.",0,,False
"Assessor Reliability. e pairwise overall average reliability score of the Lab, CF-Topic and CF-Document groups, measured using [Krippendor's , Cohen's ] is [0.687, 0.581], [0.407, 0.236] and [0.561, 0.522] respectively. e scores are calculated on binary foldings of the 4-level graded relevance levels ­ non-relevant (0), marginally relevant (1), relevant (2) and highly relevant (3). e marginally relevant (1) and non-relevant (0) judgments are binarized as non-relevant and the others as relevant as recommended by Scholer and Turpin [12].",1,ad,True
"e results in Table 1 indicate Lab and CF-Document assessors are more reliable than CF-Topic assessors. e statistical signicance of the dierences is evaluated using an unpaired two-tailed t-test across the individual pairwise agreement scores, and reported in Table 2. For both and , the overall paern from highest to lowest reliability score measured using the Sormunen judgments",0,,False
1090,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Average pairwise agreement between judges and Sormunen gold standard judgments, measured across All and individual topics using Krippendor's Alpha ( ) on a 4-levels of ordinal scale and Cohen's Kappa( ) on a binary scale.",0,,False
Krippendor's Alpha ( ),0,,False
Cohen's Kappa( ),0,,False
Lab CF-Topic CF-Document Lab CF-Topic CF-Document,0,,False
All,0,,False
0.687 0.407,0,,False
0.561,0,,False
0.581 0.236,0,,False
0.522,0,,False
el nino schengen agreement euro opposition ship losses,0,,False
0.843 0.622 0.665 0.617,0,,False
0.531 0.057 0.437 0.561,0,,False
0.725 0.380 0.377 0.704,0,,False
0.761 0.558 0.436 0.565,0,,False
0.277 0.111 0.112 0.416,0,,False
0.599 0.410 0.391 0.666,0,,False
"Table 2: Statistical signicance of Table 1 results, evaluated using an unpaired two-tailed t-test for all bronze assessors. Results for Krippendor's Alpha ( ) are shown below the diagonal line with ratings on a 4-level ordinal scale, while results for Cohen's Kappa ( ) are shown above the diagonal line with ratings on a binary scale, aening 0 and 1 to 0; and 2 and 3 to 1.",0,,False
Lab CF-Topic CF-Document,0,,False
"Lab [ , 0.687/ , 0.581]",0,,False
"95% CI 0.123, 0.435 Lab (M,""0.687, SD"",0.214) CF-Topic (M,""0.407, SD"",0.390) t (65) ,"" 3.583, p < 0.001 95% CI 0.142, 0.266 Lab (M"",""0.687, SD"",0.214) CF-Document (M,""0.561, SD"",0.345) t (68) ,"" 1.793, p "", 0.077",0,,False
"CF-Topic 95% CI 0.211, 0.479 Lab (M,""0.581, SD"",0.308) CF-Topic (M,""0.236, SD"",0.244) t (65) ,"" 5.082, p < 0.001 [ "", 0.407/ , 0.236]",0,,False
"95% CI 0.325, 0.018 CF-Topic (M,""0.407, SD"",0.390) CF-Document (M,""0.561, SD"",0.345) t (71) ,"" 1.781, p "", 0.079",0,,False
CF-Document,0,,False
"95% CI 0.099, 0.216 Lab (M,""0.581, SD"",0.308) CF-Document (M,""0.522, SD"",0.347) t (68) ,"" 0.739, p "", 0.462",0,,False
"95% CI 0.426, 0.144 CF-Document (M,""0.522, SD"",0.347) CF-Topic (M,""0.236, SD"",0.244) t (71) ,"" 4.026, p < 0.001""",0,,False
"[ , 0.561/ , 0.522]",0,,False
"as a baseline is: Lab, CF-Document and CF-Topic respectively. One explanation for this trend might be that the Lab study is a more directed environment, and assessors know that they are being closely monitored the entire time. is could contribute to longer periods of focus, resulting in a higher overall agreement with the gold standard, and therefore a presumed higher overall quality of obtained judgments.",0,,False
"When comparing only the two crowdsourcing groups, the CFDocument assessors show higher reliability. is is a somewhat surprising result, since the judges assess fewer documents and therefore spend less time overall forming a notion of relevance for a particular topic. However, this lack of ""domain knowledge"" might be counteracted by task completion time: an assessor in CF-Topic had to judge all 30 documents to get paid, and when an assessor encounters long or dicult documents at the tail of an assessment list, the likely outcome is that the assessor becomes less motivated to get any single judgment exactly right. Fatigue and motivation are known to inuence relevance judgment outcomes [9, 19], and perhaps contribute to the drop in quality. In contrast, CFDocument assessors may perceive that less eort is required on their behalf to judge a single topic-document pair before geing paid. ese ""micro"" transactions could very well be a strong motivator for crowdsourced assessors, despite having an implicit startup cost in understanding the task at hand that is amortized when judging multiple documents for the same topic. We plan to study this phenomenon in more detail in future work.",1,ad,True
"Figure 2 and Figure 3 give further insight on the reliability levels (agreement with the gold standard) of individual CF-Topic and CFDocument assessors, respectively. Results for the Lab group were omied due to space limitations; the reliability score for this group was consistently well above > 0.2, with no negative scores for any assessors. A number of assessors in CF-Topic showed lower levels of agreement with the gold standard than expected by chance alone for 2 of the topics as shown in Figure 2. Reliability for the other 2 topics in this group is similar to the trend observed for the Lab assessors. Only one assessor's relative performance in the CF-Document setup deviated signicantly from the others, as shown in Figure 3. We plan to further investigate the reasons for why such low reliability scores were observed for some individual assessors in these groups in followup work. Note that all of these assessors passed manual sanity control measures, and appeared to be performing judgments in good faith.",0,,False
"Agreement. As can been seen in Table 3, overall agreement is higher in Lab, followed by CF-Document and CF-Topic, which are in the same relative order as the reliability scores when comparing against a gold standard, suggesting that inter-rater reliability is a reasonable proxy for the quality of judgments.",0,,False
"To further establish our belief of assessor reliability, we computed the median of the multiple assessments made for each document in each experimental group, and computed the Krippendor's Alpha ( ) agreement between individual assessors and this score, shown in Table 3 (right). e overall trend is again consistent with the ndings of Table 1.",1,ad,True
1091,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: Inter-rater agreement (le) and majority vote (right) measured between assessors in the Lab, CF-Topic and CF-Document groups using Krippendor's alpha ( ) across All and individual topics with ratings on a 4-level ordinal scale. e number of assessors for inter-rater agreement is shown in parenthesis next to each value.",0,,False
Topic All,0,,False
el nino schengen agreement euro opposition ship losses,0,,False
Inter-rater agreement,0,,False
Lab,0,,False
CF-Topic CF-Document,0,,False
0.657 (32) 0.426 (35) 0.530 (121),0,,False
0.845 (8) 0.634 (8) 0.565 (8) 0.558 (8),0,,False
0.394 (8) 0.170 (8) 0.464 (9) 0.377 (10),0,,False
0.682 (31) 0.500 (29) 0.431 (29) 0.471 (32),0,,False
Lab,0,,False
0.787,0,,False
0.917 0.691 0.867 0.710,0,,False
Majority vote CF-Topic CF-Document,0,,False
0.544,0,,False
0.663,0,,False
0.608 0.436 0.537 0.605,0,,False
0.771 0.542 0.599 0.799,0,,False
 score  score,0,,False
a,0,,False
1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 -0.6,0,,False
Assessors,0,,False
b,0,,False
1.0 0.8 0.6 0.4 0.2 0.0 -0.2 -0.4 -0.6,0,,False
Assessors,0,,False
 score,0,,False
Figure 2: Reliability of CF-Topic assessors when compared with the Sormunen judgments using Krippendor's Alpha ( ) for the topics: (a) El nino; and (b) Schengen agreement.,0,,False
1,0,,False
0.8,0,,False
0.6,0,,False
0.4,0,,False
0.2,0,,False
0,0,,False
-0.2,0,,False
-0.4,0,,False
-0.6 Assessors,0,,False
Figure 3: Reliability of CF-Document assessors when compared to the Sormunen judments using Krippendor's Alpha ( ).,0,,False
"Geing gold standard relevance labels is rarely possible in a live judging scenario, but it is possible to compute inter-rater agreement between assessors, and use this to establish the quality of assessments. Our experiments conrm that using agreement between judges to gauge the quality of relevance judgments collected is indeed one possible approach to controlling the quality of judgments gathered by bronze level assessors.",0,,False
5 CONCLUSION,0,,False
"is study analyzed the quality of relevance judgments generated in three (of many possible) dierent sub-classes of bronze assessors, using Krippendor's Alpha ( ) and Cohen's Kappa ( ). e results of both metrics conrm the existence of assessment quality dierences among the three sub-classes of bronze assessors, warranting",0,,False
"further study. Nevertheless, inter-rater agreement can be a reliable",0,,False
tool to benchmark the quality of relevance judgments when gold,0,,False
standard judgments are not readily available.,1,ad,True
Acknowledgment. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP170102231 and DP140103256).,0,,False
REFERENCES,0,,False
"[1] A. Al-Maskari, M. Sanderson, and P. Clough. 2008. Relevance judgments between TREC and Non-TREC assessors. In Proc. SIGIR. 683­684.",1,TREC,True
"[2] O. Alonso and S. Mizzaro. 2012. Using crowdsourcing for TREC relevance assessment. Inf. Proc. & Man. 48, 6 (2012), 1053­1066.",1,TREC,True
"[3] P. Bailey, N. Craswell, I. Soboro, P. omas, A.P. de Vries, and E. Yilmaz. 2008. Relevance assessment: are judges exchangeable and does it maer. In Proc. SIGIR. 667­674.",0,,False
"[4] B. Carteree, V. Pavlu, H. Fang, and E. Kanoulas. 2009. Million ery Track 2009 Overview.. In Proc. TREC.",1,Track,True
"[5] T.T. Damessie, F. Scholer, and J.S. Culpepper. 2016. e Inuence of Topic Diculty, Relevance Level, and Document Ordering on Relevance Judging. In Proc. ADCS. 41­48.",0,,False
"[6] T.T. Damessie, F. Scholer, K. Ja¨rvelin, and J.S. Culpepper. 2016. e eect of document order and topic diculty on assessor agreement. In Proc. ICTIR. 73­76.",0,,False
"[7] J.L. Fleiss. 1981. Balanced incomplete block designs for inter-rater reliability studies. Applied Psychological Measurement 5, 1 (1981), 105­112.",0,,False
"[8] A.F. Hayes and K. Krippendor. 2007. Answering the call for a standard reliability measure for coding data. Comm. Methods and Measures 1, 1 (2007), 77­89.",0,,False
"[9] G. Kazai, J. Kamps, and N. Milic-Frayling. 2013. An analysis of human factors and label accuracy in crowdsourcing relevance judgments. Inf. Retr. 16, 2 (2013), 138­178.",0,,False
"[10] X. Lu, A. Moat, and J. S. Culpepper. 2016. e eect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445.",0,,False
"[11] T. Saracevic. 2007. Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. J. Amer. Soc. Inf. Sc. Tech. 58, 13 (2007), 1915­1933.",0,,False
[12] F. Scholer and A. Turpin. 2009. Metric and relevance mismatch in retrieval evaluation. In Proc. AIRS. 50­62.,0,,False
"[13] F. Scholer, A. Turpin, and M. Sanderson. 2011. antifying test collection quality based on the consistency of relevance judgements. In Proc. SIGIR. 1063­1072.",0,,False
[14] E. Sormunen. 2002. Liberal relevance criteria of TREC-: Counting on negligible documents?. In Proc. SIGIR. 324­330.,1,TREC,True
"[15] M. Stefano. 1997. Relevance: e whole history. J. Amer. Soc. Inf. Sc. 48, 9 (1997), 810­832.",0,,False
"[16] R. Tang and P. Solomon. 1998. Toward an understanding of the dynamics of relevance judgment: An analysis of one person's search behavior. Inf. Proc. & Man. 34, 2 (1998), 237­256.",0,,False
"[17] A.J. Viera and J.M. Garre. 2005. Understanding interobserver agreement: the kappa statistic. Fam. Med. 37, 5 (2005), 360­363.",0,,False
"[18] E.M. Voorhees. 2000. Variations in relevance judgments and the measurement of retrieval eectiveness. Inf. Proc. & Man. 36, 5 (2000), 697­716.",0,,False
"[19] J. Wang. 2011. Accuracy, agreement, speed, and perceived diculty of users' relevance judgments for e-discovery. In Proc. of SIGIR Inf. Ret. for E-Discovery Workshop., Vol. 1.",0,,False
1092,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Word Embedding Causes Topic Shi ing; Exploit Global Context!,0,,False
"Navid Rekabsaz, Mihai Lupu, Allan Hanbury",0,,False
Information & So ware Engineering Group TU WIEN,0,,False
rekabsaz/lupu/hanbury@ifs.tuwien.ac.at,0,,False
ABSTRACT,0,,False
"Exploitation of term relatedness provided by word embedding has gained considerable a ention in recent IR literature. However, an emerging question is whether this sort of relatedness ts to the needs of IR with respect to retrieval e ectiveness. While we observe a high potential of word embedding as a resource for related terms, the incidence of several cases of topic shi ing deteriorates the nal performance of the applied retrieval models. To address this issue, we revisit the use of global context (i.e. the term co-occurrence in documents) to measure the term relatedness. We hypothesize that in order to avoid topic shi ing among the terms with high word embedding similarity, they should o en share similar global contexts as well. We therefore study the e ectiveness of post ltering of related terms by various global context relatedness measures. Experimental results show signi cant improvements in two out of three test collections, and support our initial hypothesis regarding the importance of considering global context in retrieval.",1,ad,True
KEYWORDS,0,,False
Word embedding; term relatedness; global context; word2vec; LSI,0,,False
"ACM Reference format: Navid Rekabsaz, Mihai Lupu, Allan Hanbury and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080733",0,,False
1 INTRODUCTION,1,DUC,True
"e e ective choice of related terms to enrich queries has been explored for decades in information retrieval literature and approached using a variety of data resources. Early studies explore the use of collection statistics. ey identify the global context of two terms either by directly measuring term co-occurrence in a context (i.e. document) [9] or a er applying matrix factorization [3]. Later studies show the higher e ectiveness of local approaches (i.e. using pseudo-relevant documents) [15]. More recently, the approaches to exploit the advancement in word embedding for IR",1,ad,True
"Funded by: Self-Optimizer (FFG 852624) in the EUROSTARS programme, funded by EUREKA, BMWFW and European Union, and ADMIRE (P 25905-N23) by FWF. anks to Joni Sayeler and Linus Wretblad for their contributions in SelfOptimizer. was supported in part by the Center for Intelligent Information Retrieval.",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080733",1,ad,True
Hamed Zamani,0,,False
Center for Intelligent Information Retrieval University of Massachuse s Amherst zamani@cs.umass.edu,0,,False
"has shown not only to be competitive to the local approaches but also that combining the approaches brings further improvements in comparison to each of them alone [12, 16, 17].",0,,False
"Word embedding methods provide vector representations of terms by capturing the co-occurrence relations between the terms, based on an approximation on the likelihood of their appearances in similar window-contexts. Word embedding is used in various IR tasks e.g. document retrieval [11, 12, 18], neural network-based retrieval models [4, 6, 8], and query expansion [16].",0,,False
"In all of these studies, the concept of ""term similarity"" is dened as the geometric proximity between their vector representations. However, since this closeness is still a mathematical approximation of meaning, some related terms might not t to the retrieval needs and eventually deteriorate the results. For instance, antonyms (cheap and expensive) or co-hyponyms (schizophrenia and alzheimer, mathematics and physics, countries, months) share common window-context and are therefore considered as related in the word embedding space, but can potentially bias the query to other topics.",0,,False
"Some recent studies aim to be er adapt word embedding methods to the needs of IR. Diaz et al. [5] suggest training separate word embedding models on the top retrieved documents per query, while Rekabsaz et al. [13] explore the similarity space and suggest a general threshold to lter the most e ective related terms. While the mentioned studies rely on the context around the terms, in this work we focus on the e ect of similarity achieved from global context as a complementary to the window-context based similarity.",1,ad,True
"In fact, similar to the earlier studies [9, 14], we assume each document to be a coherent information unit and consider the cooccurrence of terms in documents as a means of measuring their topical relatedness. Based on this assumption, we hypothesize that to mitigate the problem of topic shi ing, the terms with high word embedding similarities also need to share similar global contexts. In other words, if two terms appear in many similar window-contexts, but they share li le global contexts (documents), they probably re ect di erent topics and should be removed from the related terms.",0,,False
"To examine this hypothesis, we analyze the e ectiveness of each related term, when added to the query. Our approach is similar to that of Cao et al. [2] on pseudo-relevance feedback. Our analysis shows that the set of related terms from word embedding has a high potential to improve state-of-the-art retrieval models. Based on this motivating observation, we explore the e ectiveness of using word embedding's similar term when ltered by global context similarity on two state-of-the-art IR models. Our evaluation on three test collections shows the importance of using global context, as combining both the similarities signi cantly improves the results.",1,ad,True
1105,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 BACKGROUND,0,,False
"To use word embedding in document retrieval, recent studies extend the idea of translation models in IR [1] using word embedding similarities. Zuccon et al. [18] use the similarities in the language modeling framework [10] and Rekabsaz et al. [12] extend the concept of translation models to probabilistic relevance framework. In the following, we brie y explain the translation models when combined with word embedding similarity.",0,,False
"In principle, a translation model introduces in the estimation of the relevance of the query term t a translation probability PT , de ned on the set of (extended) terms R(t), always used in its conditional form PT (t |t ) and interpreted as the probability of observing term t, having observed term t . Zuccon et al. [18] integrate word embedding with the translation language modeling by using the set of extended terms from word embedding:",0,,False
"LM(q, d) , P(q|Md ) ,",1,LM,True
PT (t |t )P(t |Md ) (1),0,,False
t q t R(t ),0,,False
Rekabsaz et al. [12] extend the idea into four probabilistic relevance,0,,False
frameworks. eir approach revisits the idea of computing docu-,0,,False
"ment relevance based on the occurrence of concepts. Traditionally,",1,ad,True
"concepts are represented by the words appear in the text, quanti ed",0,,False
by term frequency (t f ). Rekabsaz et al. posit that we can have a,0,,False
"t f value lower than 1 when the term itself is not actually appear,",0,,False
"but another, conceptually similar term occurs in the text. Based on",0,,False
"it, they de ne the extended t f of a query word t in a document as",0,,False
follows:,0,,False
"t fd , t fd +",0,,False
PT (t |t )t fd (t ),0,,False
(2),0,,False
t R(t ),0,,False
"However, in the probabilistic models, a series of other factors are",0,,False
computed based on t f (e.g. document length). ey therefore,0,,False
propagate the above changes to all the other statistics and refer to,0,,False
the nal scoring formulas as Extended Translation model. Among,0,,False
"the extended models, as BM25 is a widely used and established",0,,False
"model in IR, we use the extended BM25 translation model (BM25)",0,,False
"in our experiments. Similar to the original papers in both models,",0,,False
the estimation of PT is based on the Cosine similarity between two embedding vectors.,0,,False
3 EXPERIMENT SETUP,0,,False
"We conduct our experiments on three test collections, shown in Table 1. For word embedding vectors, we train the word2vec skipgram model [7] with 300 dimensions and the tool's default parameters on the Wikipedia dump le for August 2015. We use the Porter stemmer for the Wikipedia corpus as well as retrieval. As suggested by Rekabsaz et al. [13], the extended terms set R(t) is selected from the terms with similarity values of greater than a speci c threshold. Previous studies suggest the threshold value of around 0.7 as an optimum for retrieval [12, 13]. To explore the e ectiveness of less similar terms, we try the threshold values of {0.60, 0.65..., 0.80}.",1,Wiki,True
"Since the parameter µ for Dirichlet prior of the translation language model and also b, k1, and k3 for BM25 are shared between the methods, the choice of these parameters is not explored as part of this study and we use the same set of values as in Rekabsaz et al. [12]. e statistical signi cance tests are done using the two sided paired t-test and signi cance is reported for p < 0.05. e evaluation of retrieval e ectiveness is done with respect to Mean Average Precision (MAP) as a standard measure in ad-hoc IR.",1,MAP,True
Table 1: Test collections used in this paper,0,,False
Name TREC Adhoc 1&2&3 TREC Adhoc 6&7&8 Robust 2005,1,TREC,True
Collection Disc1&2 Disc4&5 AQUAINT,1,AQUAINT,True
# eries 150 150 50,0,,False
# Documents 740449 556028 1033461,0,,False
"Table 2: e percentage of the good, bad and neutral terms.",1,ad,True
#Rel averages the number of related terms per query term.,0,,False
Collection,0,,False
reshold 0.60 #Rel Good Neutral Bad,1,ad,True
TREC 123,1,TREC,True
8.2 7%,0,,False
84%,0,,False
9%,0,,False
TREC 678,1,TREC,True
8.8 9%,0,,False
78% 14%,0,,False
Robust 2005 10.3 8%,1,Robust,True
77% 15%,0,,False
ALL,0,,False
8.1 8%,0,,False
81% 11%,0,,False
reshold 0.80,0,,False
#Rel Good Neutral Bad,1,ad,True
1.3 19%,0,,False
68% 13%,0,,False
1.2 34%,0,,False
48% 18%,0,,False
1.1 39%,0,,False
44% 17%,0,,False
1.2 27%,0,,False
58% 15%,0,,False
4 PRELIMINARY ANALYSIS,0,,False
"We start with an observation on the e ectiveness of each individual related term. To measure it, we use the LM model as it has shown slightly be er results than the BM25 model [12]. Similar to Cao et al. [2], given each query, for all its corresponding related terms, we repeat the evaluation of the IR models where each time R(t) consists of only one of the related terms. For each term, we calculate the di erences between its Average Precision (AP) evaluation result and the result of the original query and refer to this value as the retrieval gain or retrieval loss of the related term.",1,LM,True
"Similar to Cao et al. [2], we de ne good/bad groups as the terms with retrieval gain/loss of more than 0.005, and assume the rest with smaller gain or loss values than 0.005 as neutral terms. Table 2 summarizes the percentage of each group. Due to the lack of space, we only show the statistics for the lowest (0.6) and highest (0.8) threshold. e average number of related terms per query term is shown in the #Rel eld. As expected, the percentage of the good terms is higher for the larger threshold, however--similar to the observation on pseudo-relevance feedback [2]--most of the expanded terms (58% to 81%) have no signi cant e ect on performance.",1,ad,True
Let us imagine that we had a priori knowledge about the e ectiveness of each related term and were able to lter terms with negative e ect on retrieval. We call this approach Oracle post-,1,ad,True
"ltering as it shows us the maximum performance of each retrieval model. Based on the achieved results, we provide an approximation of this approach by ltering the terms with retrieval loss.",0,,False
Figures 1a and 1b show the percentage of relative MAP improve-,1,MAP,True
"ment of the LM and BM25 models with and without post- ltering with respect to the original LM and BM25 models. In the plot, ignore the Gen and Col results as we return to them in Section 6. e results are aggregated over the three collections. In each threshold the statistical signi cance of the improvement with respect to two baselines are computed: (1) against the basic models (BM25 and LM), shown with the b sign and (2) against the translation models without post ltering, shown with the  sign.",1,LM,True
"As reported by Rekabsaz et al. [13], for the thresholds less than 0.7 the retrieval performance of the translation models (without post ltering) decreases as the added terms introduce more noise. However, the models with the Oracle post ltering continue to improve the baselines further for the lower thresholds with high margin. ese demonstrate the high potential of using related terms from word embedding but also show the need to customize the set of terms for IR. We propose an approach to this customization using the global-context of the terms in the following.",1,ad,True
1106,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
% Improvement (MAP) % Improvement (MAP) Word2Vec Similarity,1,MAP,True
20 ½b,0,,False
15,0,,False
10,0,,False
5½ ½,0,,False
0,0,,False
-5,0,,False
-10 0.6,0,,False
½b,0,,False
½b,0,,False
½b,0,,False
b½b,0,,False
LM,1,LM,True
Ld M Gen,0,,False
Ld M,0,,False
Ld M Col,0,,False
0.65,0,,False
0.7,0,,False
Threshold,0,,False
Ld M Oracle 0.75,0,,False
½b,0,,False
20,0,,False
½b,0,,False
15,0,,False
½b,0,,False
10,0,,False
½ 5½,0,,False
½b,0,,False
½b,0,,False
0,0,,False
-5,0,,False
-10 0.6,0,,False
BM25,0,,False
d BM25,0,,False
d BM25 Gen d BM25 Col,0,,False
0.65,0,,False
0.7,0,,False
Threshold,0,,False
d BM25 Oracle,0,,False
0.75,0,,False
0.90,0,,False
0.85,0,,False
0.80,0,,False
0.75,0,,False
0.70,0,,False
0.65,0,,False
0.60 0.5 0.6 0.7 0.8 0.9 1.0 LSI Similarity,0,,False
(a) LM,1,LM,True
(b) BM25,0,,False
(c) Retrieval Gain/Loss,0,,False
"Figure 1: (a,b) e percentage of relative MAP improvement to the basic models, aggregated on all the collections. e b and  signs show the signi cance of the improvement to the basic models and the extended models without post ltering respectively (c) Retrieval gain or loss of the related terms for all the collection. e red (light) color indicate retrieval loss and the green (dark) retrieval gain.",1,MAP,True
5 GLOBAL-CONTEXT POST FILTERING,0,,False
"Looking at some samples of retrieval loss, we can observe many cases of topic shi ing: e.g. Latvia as query term is expanded with Estonia, Ammoniac with Hydrogen, Boeing with Airbus, and Alzheimer with Parkinson. As mentioned before, our hypothesis is that for the terms with high window-context similarity (i.e. word2vec similarity) when they have high global context similarity (i.e. co-occurrence in common documents), they more probably refer to a similar topic (e.g. USSR and Soviet) and with low global context similarity to di erent topics (e.g. Argentina and Nicaragua).",0,,False
"To capture the global context similarities, some older studies use measures like Dice, Tanimoto, and PMI [9]. Cosine similarity has been used as well, considering each term a vector with dimensionality of the number of documents in the collection, with weights given either as simple incidence (i.e. 0/1), or by some variant of TFIDF. Cosine can also be used a er rst applying Singular Value Decomposition on the TFIDF weighted term-document matrix, resulting in the well known Latent Semantic Indexing (LSI) method [3] (300 dimensions in our experiments). To compute these measures, we consider both the collection statistics and Wikipedia statistics, resulting in 12 sets of similarities (Dice, Tanimoto, PMI, Incidence Vectors, TFIDF Vectors, LSI Vectors)×(collection, Wikipedia). We refer to these similarity value lists as global context features.",1,Wiki,True
"Let rst observe the relationship between LSI and word2vec similarities of the terms. Figure 1c plots the retrieval gain/loss of the terms of all the collections based on their word2vec similarities as well as LSI (when using test collection statistics). e size of the circles shows their gain/loss as the red color (the lighter one) show retrieval loss and green (the darker one) retrieval gain. For clarity, we only show the terms with the retrieval gain/loss of more than 0.01. e area with high word2vec and LSI similarity (top-right) contains most of the terms with retrieval gain. On the other hand, regardless of the word2vec similarity, the area with lower LSI tend to contain relatively more cases of retrieval loss. is observation encourages the exploration of a set of thresholds for global context features to post lter the terms retrieved by embedding.",0,,False
"To nd the thresholds for global context features, we explore the highest amount of total retrieval gain a er ltering the related terms with similarities higher than the thresholds. We formulate it",0,,False
by the following optimization problem:,0,,False
N,0,,False
F,0,,False
argmin 1 xj > j i,0,,False
(3),0,,False
" i,1 j,1",0,,False
"where 1 is the indicator function, N and F are the number of terms",0,,False
"and features respectively,  indicates the set of thresholds j , xj the value of the features, and nally refers to the retrieval gain/loss.",0,,False
We consider two approaches to selecting the datasets used to,0,,False
"nd the optimum thresholds: per collection, and general. In the",0,,False
"per collection scenario (Col), for each collection we nd di erent",0,,False
thresholds for the features. We apply 5-fold cross validation by rst,0,,False
using the terms of the training topics to nd the thresholds (solving,0,,False
Eq. 3) and then applying the thresholds to post lter the terms of,0,,False
"the test topics. To avoid over ing, we use the bagging method by",0,,False
40 times bootstrap sampling (random sampling with replacement),0,,False
and aggregate the achieved thresholds.,0,,False
"In the general approach (Gen), we are interested in nding a",0,,False
"`global' threshold for each feature, which is fairly independent of",0,,False
the collections. As in this approach the thresholds are not speci c,0,,False
"for each individual collection, we use all the topics of all the test",0,,False
collections to solve the optimization problem.,0,,False
6 RESULTS AND DISCUSSION,0,,False
"To nd the most e ective set of features, we test all combinations of features using the per collection (Col) post- ltering approach. Given the post- ltered terms with each feature set, we evaluate the",0,,False
"LM and BM25 models. Our results show the superior e ectiveness of the LSI feature when using the test collections as resource in comparison with the other features as well as the ones based on Wikipedia. e results with the LSI feature can be further improved by combining it with the TFIDF feature. However, adding any of the other features does not bring any improvement and therefore, in the following, we only use the combination of LSI and TFIDF features with both using the test collections statistics.",1,LM,True
e evaluation results of the original LM and LM with post ltering with the general (Gen) and per collection (Col) approaches are,1,LM,True
"shown in Figure 2. e general behavior of BM25 is very similar and therefore no longer shown here. As before, statistical significance against the basic models is indicated by b and against the translation models without post ltering, by .",0,,False
1107,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
0.33 b½,0,,False
0.32,0,,False
b½,0,,False
0.31,0,,False
0.30,0,,False
b½,0,,False
b,0,,False
b½,0,,False
0.29 ½½,0,,False
0.29 b½,0,,False
0.28 0.27,0,,False
b½,0,,False
b½,0,,False
0.26,0,,False
½ 0.25 ½,0,,False
0.24,0,,False
0.23 b½,0,,False
0.22,0,,False
b½,0,,False
b½,0,,False
0.21,0,,False
½,0,,False
½,0,,False
½,0,,False
0.20,0,,False
MAP MAP MAP,1,MAP,True
0.28,0,,False
0.24,0,,False
0.19,0,,False
0.27,0,,False
0.23,0,,False
0.18,0,,False
0.26,0,,False
0.22,0,,False
0.17,0,,False
0.6,0,,False
0.65,0,,False
0.7,0,,False
0.75,0,,False
0.6,0,,False
0.65,0,,False
0.7,0,,False
0.75,0,,False
0.6,0,,False
0.65,0,,False
0.7,0,,False
0.75,0,,False
TREC 123,1,TREC,True
TREC 678,1,TREC,True
Robust 2005,1,Robust,True
LM,1,LM,True
Ld M,0,,False
Ld M Gen,0,,False
Ld M Col,0,,False
Ld M Oracle,0,,False
Figure 2: Evaluation results of LM with/without post ltering. LM and LM without post ltering respectively.,1,LM,True
"e results show the improvement of the LM models with postltering in comparison with the original LM. e models with postltering approaches speci cally improve in lower word embedding thresholds, however similar to the original translation models, the best performance is achieved on word embedding threshold of 0.7.",1,LM,True
e results for both LM and BM25 models with word embedding threshold of 0.7 are summarized in Table 3. Comparing the post-,1,LM,True
"ltering approaches, Col shows be er performance than Gen as with the optimum word embedding threshold, it achieves signi cant improvements over both the baselines in two of the collections.",0,,False
"Let us look back to the percentage of relative improvements, aggregated over the collections in Figures 1a and 1b. In both IR models, while the Col approach has be er results than Gen, their results are very similar to the optimum word embedding threshold (0.7). is result suggests to use the Gen approach as a more straightforward and general approach for post ltering. In our experiments, the optimum threshold value for the LSI similarities (as the main feature) is around 0.62 (vertical line in Figure 1c).",0,,False
"As a nal point, comparing the two IR models shows that despite the generally be er performance of the LM models, the BM25 models gain more. We speculate that it is due to the additional modi cation of other statistics (i.e. document length and IDF) in the BM25 model and therefore it is more sensitive to the quality of the related terms. However an in-depth comparison between the models is le for future work.",1,LM,True
7 CONCLUSION,0,,False
"Word embedding methods use (small) window-context of the terms to provide dense vector representations, used to approximate term relatedness. In this paper, we study the e ectiveness of related terms, identi ed by both window-based and global contexts, in document retrieval. We use two state-of-the-art translation models to integrate word embedding information for retrieval. Our analysis shows a great potential to improve retrieval performance, damaged however by topic shi ing. To address it, we propose the use of global context similarity, i.e. the co-occurrence of terms in larger contexts such as entire documents. Among various methods to measure global context, we identify LSI and TFIDF as the most e ective in eliminating related terms that lead to topic shi ing. Evaluating the IR models using the post- ltered set shows a signi cant improvement in comparison with the basic models as well as the translation models with no post- ltering. e results",1,ad,True
e b and  signs show the signi cance of the improvement to,0,,False
Table 3: MAP of the translation models when terms ltered,1,MAP,True
with word embedding threshold of 0.7 and post ltered with,0,,False
the Gen and Col approach.,0,,False
Collection Model Basic Tran. Tran.+Gen Tran.+Col,0,,False
TREC 123,1,TREC,True
LM 0.275 0.283 BM25 0.273 0.285,1,LM,True
0.290 0.288,0,,False
0.295 b 0.290 b,0,,False
TREC 678,1,TREC,True
LM 0.252 0.259 BM25 0.243 0.255,1,LM,True
0.262 0.257,0,,False
0.261 0.256 b,0,,False
Robust 2005,1,Robust,True
LM BM25,1,LM,True
0.183 0.181,0,,False
0.204 0.203,0,,False
0.208 0.207,0,,False
0.209 b 0.209 b,0,,False
demonstrate the importance of global context as a complementary to the window-context similarities.,0,,False
REFERENCES,0,,False
[1] Adam Berger and John La erty. 1999. Information Retrieval As Statistical Translation. In Proc. of SIGIR.,0,,False
"[2] Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen Robertson. 2008. Selecting good expansion terms for pseudo-relevance feedback. In Proc. of SIGIR.",0,,False
"[3] Sco Deerwester, Susan T Dumais, George W Furnas, omas K Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American society for information science (1990).",0,,False
"[4] Mostafa Dehghani, Hamed Zamani, A. Severyn, J. Kamps, and W Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In Proc. of SIGIR.",0,,False
"[5] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery expansion with locally-trained word embeddings. Proc. of ACL (2016).",0,,False
"[6] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A deep relevance matching model for ad-hoc retrieval. In Proc. of CIKM.",1,ad-hoc,True
"[7] Tomas Mikolov, Kai Chen, G. Corrado, and J. Dean. 2013. E cient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).",1,ad,True
"[8] Bhaskar Mitra, F. Diaz, and N. Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In Proc. of WWW.",0,,False
[9] Helen J Peat and Peter Wille . 1991. e limitations of term co-occurrence data for query expansion in document retrieval systems. Journal of the American society for information science (1991).,0,,False
[10] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In Proc. of SIGIR.,0,,False
"[11] Navid Rekabsaz, Ralf Bierig, Bogdan Ionescu, Allan Hanbury, and Mihai Lupu. 2015. On the use of statistical semantics for metadata-based social image retrieval. In Proc. of CBMI Conference.",1,ad,True
"[12] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In Proc. of CIKM.",0,,False
"[13] Navid Rekabsaz, Mihai Lupu, and Allan Hanbury. 2017. Exploration of a threshold for similarity based on uncertainty in word embedding. In Proc. of ECIR.",0,,False
[14] G Salton and MJ MacGill. 1983. Introduction to modern information retrieval. McGraw-Hill (1983).,0,,False
[15] Jinxi Xu and W Bruce Cro . 1996. ery expansion using local and global document analysis. In Proc. of SIGIR.,0,,False
[16] Hamed Zamani and W Bruce Cro . 2016. Embedding-based query language models. In Proc. of ICTIR.,0,,False
[17] Hamed Zamani and W Bruce Cro . 2017. Relevance-based Word Embedding. In Proc. of SIGIR.,0,,False
"[18] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Integrating and evaluating neural word embeddings in information retrieval. In Proc. of Australasian Document Computing Symposium.",0,,False
1108,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Information Retrieval Model using Generalized Pareto Distribution and Its Application to Instance Search,0,,False
Masaya Murata,0,,False
"NTT Communication Science Labs. 3-1 Morinosato Wakamiya, Atsugi-shi,",0,,False
"Kanagawa Pref., Japan masaya.murata1013@gmail.com",0,,False
Kaoru Hiramatsu,0,,False
"NTT Communication Science Labs. 3-1 Morinosato Wakamiya, Atsugi-shi,",0,,False
"Kanagawa Pref., Japan hiramatsu.kaoru@lab.ntt.co.jp",0,,False
Shin'ichi Satoh,0,,False
"National Institute of Informatics 2-1-2 Hitotsubashi, Chiyoda-ku,",0,,False
"Tokyo, Japan satoh@nii.ac.jp",0,,False
ABSTRACT,0,,False
We adopt the generalized Pareto distribution for the informationbased model and show that the parameters can be estimated based on the mean excess function. The proposed information retrieval model corresponds to the extension of the divergence from independence and is designed to be data-driven. The proposed model is then applied to the specific object search called the instance search and the effectiveness is experimentally confirmed.,1,ad,True
CCS CONCEPTS,0,,False
"· Information systems  Information retrieval; Retrieval models and ranking; Probabilistic retrieval models; Multimedia and multimodal retrieval, Video search",1,Video,True
KEYWORDS: Information retrieval; information-based model;,0,,False
divergence from independence; extreme value statistics; generalized Pareto distribution; video retrieval; instance search,0,,False
1 INTRODUCTION,1,DUC,True
"Effective information retrieval (IR) models have been actively studied roughly since 1960s in the IR community. BM25, language model-based IR (LM) and divergence from randomness (DFR) are state-of-the-art. The axiomatic approach is also proposed [1]. Recently, information-based model (I model) [2] and percentilebased model (P model) [3] are proposed. These models are simple compared to the DFR, since only one distribution is necessary (two distributions are required for the DFR). The problem then becomes the adequate setting of the distribution. There are interesting relations such that adopting the log-logistic distribution (LLD) for I model and P model yields LM with JelinekMercer smoothing and sub-linear normalized term frequency (NTF) term in the BM25 model, respectively [4][3]. Although these facts somehow support the effectiveness of I model and P model, the essential problem is whether the data to be searched follow the",1,LM,True
"LLD or not. Setting the suitable distribution to the objective data is important for the successful retrieval. Recently, the distribution is estimated using the extreme value statistics (EVS) [5]. The distribution that the maximum NTF follows according to the EVS is adopted in the P model. To my knowledge, it is the first successful application of the EVS to the IR model. In this paper, we propose the distribution estimation for the I model according to the EVS and this is the main contribution. We also show that the proposed model corresponds to the extension of the divergence from independence (DFI) [6] which is a parameter-free IR model. Since the parameters of the proposed model are estimated according to the data, the retrieval accuracy is expected to be improved. We demonstrate it for the specific object search called the instance search [7] using image-query video retrieval dataset. The rest of this paper is organized as follows. We describe the proposed model and explain the relation to the DFI model in Section 2. We also clarify the difference from the method in [5]. Section 3 shows the experimental justification using the instance search dataset and the detailed discussion is provided. Finally, Section 4 concludes this paper.",1,ad,True
2 EVS AND ITS APPLICATION TO I MODEL,1,AP,True
2.1 Brief Introduction of EVS,0,,False
"The EVS provides us what distributions maximum block data and threshold excess data (TED) asymptotically follow, respectively. In this paper, we focus on the TED, which is the data larger than a pre-specified threshold. Under the EVS basic assumption for the data, it is mathematically shown that as the threshold increases, the TED asymptotically follows the generalized Pareto distribution (GPD). The cumulative distribution function is characterized as follows:",0,,False
"F,",0,,False
",1- 1+",0,,False
(1),0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan",1,ad,True
© 2017 ACM. ISBN 978-1-4503-5022-8/17/08...$15.00 http://dx.doi.org/10.1145/3077136.3080736,0,,False
"Here, is the TED and , are the distribution parameters. The",0,,False
GPD is heavy-tailed for  > 0. The mean excess function (MEF) is,0,,False
defined as follows:,0,,False
"e ,E - | >",0,,False
(2),0,,False
"Then, the MEF for the GPD exists for  < 1 and becomes",0,,False
"e ,1- +1-",0,,False
(3),0,,False
1117,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
It is proved that the MEF is linear in only for the GPD and we exploit this nature for designing a IR model in the next subsection.,0,,False
2.2 I model using GPD,0,,False
I model is defined as follows:,0,,False
"score , ,",0,,False
-,0,,False
|,0,,False
",,",0,,False
(4),0,,False
"Here, , are query and document vectors and is the NTF for the i-th term. P  | is called the information model and it should be heavy-tailed to prevent the document ranking score from becoming diverged. We then replace the information model with that for the GPD. Using Eq. (1), equation (4) becomes",0,,False
"score , ,",0,,False
-,0,,False
1+,0,,False
(5),0,,False
",,",0,,False
"Here, is the within-document term frequency for the i-th term.",0,,False
"We set , ,",0,,False
"- and ,",0,,False
", which is the",0,,False
expected term frequency for the i-th term within the document,0,,False
"length ,  . is the within-corpus (-dataset) frequency for",0,,False
"the i-th term and ,  . Note that  > 0 is assumed since the",0,,False
information model in Eq. (4) should be heavy-tailed.,0,,False
"For the execution of Eq. (5), , , should be specified",0,,False
"according to the data to be searched. For a certain , we estimate",0,,False
the MEF as follows:,0,,False
e,0,,False
",#",0,,False
1 |>,0,,False
-,0,,False
(6),0,,False
"Here, # | > is the number of terms whose NTFs are larger",0,,False
than a threshold .  denotes zero for  and - for,0,,False
"> . Then, when the estimated MEF seems linear in , applying",0,,False
"the least squares method provides the estimation results for , according to Eq. (3). Note that Eq. (3) only holds for  < 1 and therefore, 0 <  < 1 is implicitly assumed for this parameter estimation method.",0,,False
"is the control (tuning) parameter of the proposed model. When the estimated MEF for a certain is clearly deviated from a linear function, the aforementioned parameter estimation method is not applicable. We can vary until the estimated MEF becomes somehow linear, however, we cannot provide the sophisticated selection method of . To summarize, the proposed IR model is",0,,False
"score , ,",0,,False
- 1+,0,,False
(7),0,,False
",,",0,,False
"where, , are the estimated values based on the least squares results.",0,,False
2.3 Relation to DFI,0,,False
The DFI is expressed as follows:,0,,False
"score , ,",0,,False
1+,0,,False
(8),0,,False
",,",0,,False
The DFI simply evaluates the extent the within-document term,0,,False
frequencies diverge from the expected within-document,0,,False
"frequencies. When the divergence for a certain term is large, the",0,,False
corresponding weight is designed to be larger. Although the DFI is,0,,False
"parameter-free, the function form is arbitrary. Indeed, the",0,,False
following form is also possible:,0,,False
"score , ,",0,,False
1+ -,0,,False
(9),0,,False
",,",0,,False
"In this case, the terms whose within-document frequencies are larger than the expected values are only taken into account. Comparing the DFI equations with the model in Eq. (5), it is readily shown that , , ,"" 1,1,0 makes Eq. (5) identical to Eq. (8). Moreover, , , "","" 1,1,1 in Eq. (5) leads to Eq. (9). On""",1,ad,True
"the other hands, Eq. (7) is based on , , ,"" , , . Therefore,""",0,,False
"the proposed model can be regarded as the extension of the DFI in which the parameters are estimated according to the data to be searched. Since ,"" 0 1 for DFI, we can think of setting "","" 0 1 for Eq. (7). Then, the proposed model also becomes parameter-free. The setting of "","" 1 for the GPD corresponds to the setting of the LLD since the LLD is a special case of the GPD. Therefore, the DFI can be regarded as the I model using the LLD with "", 1.",0,,False
"When the is left as a parameter, the resulting model becomes equivalent to the LM with Jelinek Mercer smoothing [4].",1,LM,True
2.4 Difference from the model in [5],0,,False
The IR model in [5] also uses the EVS. It is based on the following P model:,0,,False
"score , ,",0,,False
|,0,,False
",,",0,,False
(10),0,,False
"Then,  | is replaced with the cumulative distribution function that a maximum NTF follows under the EVS basic assumption. is an arbitrary inverse document frequency for the i-th term. Note that the adopted distribution is not the distribution that the actually follows. Therefore, the implicit assumption is that the weight becomes large for the case that the",1,ad,True
"approaches the maximum value. To the contrary, the proposed model estimates the GPD that the",0,,False
"follows under the EVS basic assumption and does not rely on the assumption such as the aforementioned one. This is the primary difference from the method in [5], although the selection of the NTFs is also different.",0,,False
3 EXPERIMENTS,0,,False
3.1 Instance Search Dataset,0,,False
We show the effectiveness of the proposed model for the specific object search called the instance search. It is an image-query video,0,,False
1118,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
retrieval task and the specific object is shown in the image-query. The system is required to search and rank videos in which the objects are shown in the decreasing order of relevance degrees. The following images are object examples.,0,,False
"For all cases of the duplication thresholds 0.999, 0.95, and 0.9, we confirm that when # | > is sufficiently large, the",0,,False
corresponding MEF can be approximately regarded as linear. This fact supports the use of the GPD assumption and the proposed parameter estimation method is performed for the linear region. We then execute Eq. (7) to rank videos. We also vary and the same parameter estimation procedure is performed for the other settings of .,0,,False
Figure 1: Examples of image-queries. Image queries are sets of original and region-of-interest (ROI) images. The white regions in the ROI images specify the objects in the original images.,0,,False
"The dataset is provided in the TRECVID2012 instance search task [7]. It is composed of 76751 short videos (the average duration is about 10 sec.) and 21 objects such as person/object/place. Each object is provided by five original and ROI images on average. The relevance judgement data is binary and the mean average precision (MAP) is adopted as the search accuracy measure. The frames are extracted from each video by 1 frame/sec and key-points are detected by the Harris-Laplace detector. Then, the key-points are described by 128-dimensional SIFT feature vectors. Same key-point detection and description methods are performed for all of the image-queries and the duplicated key-points are removed. This removal procedure is based on the cosine similarity value (CSV) between two SIFT vectors and the pair of key-points whose CSV is larger than a certain threshold is identified as matched. We varied the duplication threshold such as 0.999, 0.95 and 0.9. Then, for each key-point extracted from frames, the nearest key-point extracted from all of the image-queries is matched based on the CSV with a threshold of 0.9.",1,TRECVID,True
3.2 Results,0,,False
"Figures 2, 3, and 4 depict the estimated MEFs ( ,""0) for the duplication thresholds 0.999, 0.95, and 0.9, respectively. The regions specified by the red circles seem linear and the proposed parameter estimation is performed only for these regions. Note that for  200 in Fig. 2, since # | > becomes small, the MEF becomes less trustworthy.""",0,,False
Figure 2: Estimated MEF for the duplication threshold 0.999. The region in the red circle seems linear. For ,0,,False
", since # | > becomes small, the MEF becomes unstable and less trustworthy.",0,,False
"Figure 3: Estimated MEF for the duplication threshold 0.95. For  , the MEF fluctuates since # | > is small.",0,,False
"Figure 4: Estimated MEF for the duplication threshold 0.9. Compared with Figs. 2 and 3, the size of unstable region is decreased because # | > becomes sufficiently large.",0,,False
1119,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1 shows the MAP results for DFI and proposed models with the duplication threshold of 0.999. We only list the results for this setting since they are the best MAP results among ours. As shown in this table, the search accuracy of the proposed models is significantly better than those for the DFI models. As mentioned in Section 2.3, the proposed model is data-driven and we confirmed that using the estimated parameters from the data leads to the improvement in the search accuracy.",1,MAP,True
Table 1: MAP Results,1,MAP,True
"IR model DFI (Eq. (8)) DFI (Eq. (9)) Proposed model with ,0 Proposed model with ,1 Proposed model with ,5 Proposed model with ,10 Proposed model with ,20 Proposed model with ,50 Proposed model with ,100",0,,False
MAP 0.23 0.24 0.29 0.29 0.30 0.31 0.31 0.31 0.31,1,MAP,True
The MAP values scored by the proposed models are comparable with the highest MAP in the TRECVID2012 instance search task,1,MAP,True
"[7]. It is clearly shown that as increases, the MAP value is improved further. We discuss this issue in the next subsection.",1,MAP,True
3.3 Discussion,0,,False
"From Table 1, we confirm that the search accuracy is improved for large . As mentioned in Section 2.1, as increases, the TED, that",0,,False
"is, ,",0,,False
"- , asymptotically follows the GPD when the EVS",0,,False
"basic assumption holds for the NTF, that is, . Since the",0,,False
"proposed model is based on the GPD assumption, there is a possibility that the EVS basic assumption somehow holds for the NTF. Roughly speaking, when the NTF follows a heavy-tailed distribution, such a proposition is true. When large often occurs in a video, the NTF is expected to follow a heavy-tailed distribution. Considering a video which is the time series of frame images, similar or even same key-points often repeatedly occur in the video, which contributes to large . Such key-points are described as ""bursty"" in the image/video retrieval community and it is known that the adequate treatment is essential for the successful retrieval. Therefore, for the video retrieval task such as the instance search, the NTF can be expected to follow a heavy-tailed distribution. The larger supports the GPD assumption for the TED further and we expect that this tendency is shown in Table 1. The results in Table 1 also indicate that the TED with large are sufficient for the successful video retrieval and that taking the whole data into account results in the deteriorated search accuracy. This is also the main finding of this paper. It is interesting to see whether this proposition also holds for the document retrieval task. Since heavy-tailed distributions are often assumed for the NTF in",1,ad,True
"the document retrieval community [5], there is also a possibility that such a proposition holds for the text retrieval task. Then, the inverted index may be dramatically shortened since only the effective TED ( > 0) are sufficient for the successful retrieval. This is our immediate future work.",0,,False
4 CONCLUSIONS,0,,False
"We proposed a IR model based on the GPD in the I model framework. We also proposed the parameter estimation method based on the least squares method. The MEF is estimated from data to be searched and the linear region is processed by the least squares, which provides the estimates of the shape parameters for the GPD model. The proposed IR model corresponds to the extension of the DFI. Since the model is data-driven, its retrieval accuracy is significantly improved. We confirmed it using the image-query video retrieval task called the instance search.",0,,False
REFERENCES,0,,False
"[1] H. Fang and C. X. Zhai. 2005. An Exploration of Axiomatic Approaches to Information Retrieval. In Proceeding of the 28th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'05). ACM, New York, NY, 480­487.",0,,False
"[2] S. Clinchant and E. Gaussier. 2010. Information-Based Models for Ad Hoc IR. In Proceeding of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'10). ACM, New York, NY, 234­ 241.",0,,False
"[3] Y. Lv and C. X. Zhai. 2012. A Log-Logistic Model-Based Interpretation of TF Normalization of BM25. In Proceeding of the 34th European Conference on IR Research (ECIR 2012). Barcelona, Spain, April 1-5, 244­255.",0,,False
"[4] S. Clinchant and E. Gaussier. 2009. Bridging Language Modeling and Divergence from Randomness Models: A Log-Logistic Model for IR. In Proceeding of the Second International Conference on the Theory of Information Retrieval (ICTIR 2009). Cambridge, UK, September 10-12, 54­65.",0,,False
"[5] J. H. Paik. 2015. A Probabilistic Model for Information Retrieval Based on Maximum Value Distribution. In Proceeding of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'15). ACM, New York, NY, 585­594.",0,,False
"[6] I. Kocabas, B. T. Dincer, and B. Karaoglan. 2014. A Nonparametric Term Weighting Method for Information Retrieval Based on Measuring the Divergence from Independence. Information Retrieval, Vol. 17, Issue 2, 153176.",0,,False
"[7] M. Murata, H. Nagano, R. Mukai, K. Kashino, and S. Satoh. 2014. BM25 With Exponential IDF for Instance Search. IEEE Transactions on Multimedia, Vol. 16, Issue 6, 1690-1699.",0,,False
1120,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge,0,,False
Arman Cohan,0,,False
"Information Retrieval Lab, Dept. of Computer Science Georgetown University",0,,False
arman@ir.cs.georgetown.edu,0,,False
ABSTRACT,0,,False
"Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles.",1,ad,True
KEYWORDS,0,,False
"Text Summarization, Scienti c Text, Information Retrieval",0,,False
1 INTRODUCTION,1,DUC,True
"In scienti c literature, related work is often referenced along with a short textual description regarding that work which we call citation text. Citation texts usually highlight certain contributions of the referenced paper and a set of citation texts to a reference paper can provide useful information about that paper. Therefore, citation texts have been previously used to enhance many downstream tasks in IR/NLP such as search and summarization (e.g. [2, 15, 16]).",0,,False
"While useful, citation texts might lack the appropriate context from the reference article [4, 5, 18]. For example, details of the methods, assumptions or conditions for the obtained results are often not mentioned. Furthermore, in many cases the citing author might misunderstand or misquote the referenced paper and ascribe contributions to it that are not intended in that form. Hence, sometimes the citation text is not su ciently informative or in other cases, even inaccurate [17]. This problem is more serious in life sciences where accurate dissemination of knowledge has direct impact on human lives.",0,,False
"We present an approach for addressing such concerns by adding the appropriate context from the reference article to the citation texts. Enriching the citation texts with relevant context from the reference paper helps the reader to better understand the context for the ideas, methods or ndings stated in the citation text.",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080740",1,ad,True
Nazli Goharian,0,,False
"Information Retrieval Lab, Dept. of Computer Science Georgetown University",0,,False
nazli@ir.cs.georgetown.edu,0,,False
"A challenge in citation contextualization is the discourse and terminology variations between the citing and the referenced authors. Hence, traditional IR models that rely on term matching for",1,ad,True
nding the relevant information are ine ective. We propose to address this challenge by a model that utilizes,1,ad,True
"word embeddings and domain speci c knowledge. Speci cally, our approach is a retrieval model for nding the appropriate context of citations, aimed at capturing terminology variations and paraphrasing between the citation text and its relevant reference context.",0,,False
"We perform two sets of experiments to evaluate the performance of our system. First, we evaluate the relevance of extracted contexts intrinsically. Then we evaluate the e ect of citation contextualization on the application of scienti c summarization. Experimental results on TAC 2014 benchmark show that our approach signi cantly outperforms several strong baselines in extracting the relevant contexts. We furthermore, demonstrate that our contextualization models can enhance summarizing scienti c articles.",0,,False
2 CONTEXTUALIZING CITATIONS,0,,False
"Given a citation text, our goal is to extract the most relevant context",0,,False
to it in the reference article. These contexts are essentially certain,0,,False
"textual spans within the reference article. Throughout, colloquially,",0,,False
we refer to the citation text as query and reference spans in the,0,,False
reference article as documents. Our approach extends Language,0,,False
Models for IR (LM) by incorporating word embeddings and domain,1,LM,True
ontology to address shortcomings of LM for this research purpose.,1,ad,True
"The goal in LM is to rank a document d according to the conditional probability p(d |q)  p(q|d ) , qi q p(qi |d ) where qi shows the tokens in the query q. Estimating p(qi |d ) is often achieved by max-",1,LM,True
imum likelihood estimate from term frequencies with some sort of,0,,False
"smoothing. Using Dirichlet smoothing [21], we have:",0,,False
p(qi |d ),0,,False
",",0,,False
"f (qi , d ) + µ p(qi |C) w V f (w, d ) + µ",0,,False
(1),0,,False
"where f (qi , d ) shows the frequency of term qi in document d, C",0,,False
"is the entire collection, V is the vocabulary and µ the Dirichlet",0,,False
"smoothing parameter. In the citation contextualization problem, (i)",0,,False
the target reference sentences are short documents and (ii) there,0,,False
exist terminology variations between the citing author and the,0,,False
"referenced author. Hence, the citation terms usually do not appear",0,,False
"in the documents and relying only on the frequencies of citation terms in the documents (f (qi , d )) for estimating p(qi |d ) yields an almost uniform smoothed distribution that is unable to decisively",0,,False
distinguish between the documents.,0,,False
"Embeddings. Distributed representation (embedding) of a word w in a eld F is a mapping w  Fn where words semantically similar to w will be ideally located close to it. Given a query q, we rank the documents d according to the following scoring function",0,,False
1133,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
normalized dot product,0,,False
normalized dot product,0,,False
1.0,0,,False
normalized logit,0,,False
1.0,0,,False
normalized logit,0,,False
0.8,0,,False
0.8,0,,False
0.6,0,,False
0.6,0,,False
0.4,0,,False
0.4,0,,False
0.2,0,,False
0.0 0,0,,False
500,0,,False
1000,0,,False
1500,0,,False
2000,0,,False
0.2,0,,False
0.0 0,0,,False
200 400 600 800 1000,0,,False
Figure 1: Dot product of embeddings and its logit for a sample word and its top most similar words (top 2000 and 1000).,0,,False
which leverages this property:,0,,False
p(qi |d ),0,,False
",",0,,False
"fsem (qi , d ) + µ p(qi |C ) w V fsem (w, d ) + µ",0,,False
(2),0,,False
where fsem is a function that measures semantic relatedness of the,0,,False
"query term qi to the document d and is de ned as: fsem (qi , d ) ,",0,,False
"dj d s (qi , dj ); where dj 's are document terms and s (qi , dj ) is the",0,,False
relatedness between the the query term and document term which,0,,False
is calculated by applying a similarity function to the distributed,0,,False
representations of qi and dj . We use a transformation () of dot products between the unit vectors e (qi ) and e (dj ) corresponding to the embeddings of the terms qi and dj for the similarity function:,0,,False
"s (qi , dj ) ,  (e (qi ).e (dj )); if e (qi ).e (dj ) > ",0,,False
0;,0,,False
otherwise,0,,False
We rst explain the role of  and then the reason for considering,0,,False
the function  instead of raw dot product.  is a parameter that,1,ad,True
controls the noise introduced by less similar words. Many unrelated,0,,False
word vectors have non-zero similarity scores and adding them up introduces noise to the model and reduces the performance.  's function is to set the similarity between unrelated words to zero,1,ad,True
"instead of a positive number. To identify an appropriate value for  , we select a random set of words from the embedding model and calculate the average and standard deviation of pointwise absolute",1,ad,True
value of similarities between terms from these two samples. We then select the threshold  to be two standard deviations larger than the average to only consider very high similarity values (this,0,,False
choice was empirically justi ed).,0,,False
Examining term similarity values between words shows that,0,,False
there are many terms with high similarities associated with each,0,,False
"term and these values are not highly discriminative. We apply a transfer function  to the dot product e (qi ).e (dj ) to dampen the e ect of less similar words. In other words, we only want highly",0,,False
related words to have high similarity values and similarity should,0,,False
quickly drop as we move to less related words. We use the logit,0,,False
function for  to achieve this dampening e ect:,0,,False
 (x,0,,False
),0,,False
",",0,,False
log(,0,,False
1,0,,False
x -,0,,False
x,0,,False
),0,,False
Figure 1 shows this e ect. The purple line is the normalized dot,0,,False
product of a sample word with the most similar words in the model.,0,,False
"As illustrated, the similarity score di erences among top words",0,,False
"is not very discriminative. However, applying the logit function",0,,False
(green line) causes the less similar words to have lower similarity,0,,False
values to the target word. Domain knowledge. Successful word embedding methods have,0,,False
previously shown to be e ective in capturing syntactic and semantic,0,,False
relatedness between terms. These co-occurrence based models are,0,,False
"data driven. On the other hand, domain ontologies and lexicons",0,,False
that are built by experts include some information that might not,0,,False
"be captured by embedding methods [8]. Therefore, using domain",0,,False
knowledge can further help the embedding based retrieval model;,0,,False
we incorporate it in our model in the following ways: 1) Retro tting: Faruqui et al. [6] proposed a model that uses the,1,corpora,True
constraints on WordNet lexicon to modify the word vectors and,0,,False
pull synonymous words closer to each other. To inject the domain,0,,False
"knowledge in the embeddings, we apply this model on two domain speci c ontologies, namely, M and Protein Ontologies (PO)1.",0,,False
We chose these two biomedical domain ontologies because they,0,,False
are in the same domain as the articles in the TAC dataset. M,0,,False
is a broad ontology that consists of biomedical terms and PO is a,1,ad,True
more focused ontology related to biology of proteins and genes. 2) Interpolating in the LM: We also directly incorporate the do-,1,LM,True
main knowledge in the retrieval model; we modify the LM into the,1,LM,True
following interpolated LM with parameter :,1,LM,True
"p(qi |d ) , p1 (qi |d ) + (1 - )p2 (qi |d ) where p1 is estimated using Eq. 2 and p2 is similar to p1 except that",0,,False
we replace fsem with the function font which considers domain,0,,False
ontology in calculating similarities:,0,,False
"font (qi , d ),",0,,False
"1,",0,,False
"s2 (qi , dj ); s2 (qi , dj ),"" ,""",0,,False
"if qi ,dj if qi dj",0,,False
dj d,0,,False
"0, o.w. ",0,,False
"where   [0, 1] is a parameter and qi  dj shows that there is",0,,False
an is-synonym relation in ontology between qi and dj 2.,0,,False
3 EXPERIMENTS,0,,False
Data. We use the TAC 2014 Biomedical Summarization benchmark3.,0,,False
This dataset contains 220 scienti c biomedical journal articles and,0,,False
313 total citation texts where the relevant contexts for each citation,0,,False
"text are annotated by 4 experts. Baselines. To our knowledge, the only published results on TAC",0,,False
"2014 is [4], where the authors utilized query reformulation (QR)",0,,False
"based on UMLS ontology. In addition to [4], we also implement sev-",1,ad,True
eral other strong baselines to better evaluate the e ectiveness of our,0,,False
model: 1) BM25; 2) VSM: Vector Space Model that was used in [4]; 3) DESM: Dual Embedding Space Model which is a recent embedding based retrieval model [12]; and 4) LMD-LDA: Language modeling,1,LM,True
with LDA smoothing which is a recent extension of the LMD to,1,LM,True
also account for the latent topics [10]. All the baseline parameters,0,,False
"are tuned for the best performance, and the same preprocessing is",0,,False
applied to all the baselines and our methods. Our methods. We rst report results based on training the em-,0,,False
"beddings on Wikipedia (WEWiki). Since TAC dataset is in biomedical domain, many of the biomedical terms might be either out-",1,Wiki,True
of-vocabulary or not captured in the correct context using gen-,0,,False
"eral embeddings, therefore we also train biomedical embeddings (WEBio)4. In addition, we report results for biomedical embeddings with retro tting (WEBio+rtrft), as well as interpolating domain knowledge (WEBio+dmn)",1,ad,True
3.1 Intrinsic Evaluation,0,,False
"First, we analyze the e ectiveness of our proposed approach for contextualization intrinsically. That is, we evaluate the quality of the",0,,False
1https://www.nlm.nih.gov/mesh/; http://pir.georgetown.edu/pro/ 2The values of the parameters  and  were selected empirically by grid search,0,,False
3,0,,False
http://www.nist.gov/tac/2014/BiomedSumm/ 4We train biomedical embeddings on TREC Genomics 2004 and 2006 collections (both,1,TREC,True
Wikipedia and Genomics embeddings were trained using gensim implementation of,1,Wiki,True
"Word2Vec, negative sampling, window size of 5 and 300 dimensions.",0,,False
1134,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Results on TAC 2014 dataset. c-P, c-R, c-F: character o set Precision, Recall and F-1 scores; R : R ; cP@K: character o set precision at K. shows statistical signi cant improvement over the best baseline performance (two-tailed t-test, p<0.05). Values are percentages.",0,,False
Method,0,,False
c-P c-R c-F nDCG R -1 R -2 R -3 c-P@1 c-P@5,0,,False
VSM [4],0,,False
20.5 24.7 21.2 48.1 49.5 26.4 20.0 31.9 26.1,0,,False
BM25,0,,False
19.5 18.6 17.8 38.1 43.6 23.2 16.3 25.5 24.2,0,,False
DESM [12],0,,False
20.3 23.8 22.3 45.6 50.3 26.2 20.6 32.5 26.5,0,,False
LMD-LDA [10] 22.6 24.8 22.3 46.0 48.3 26.4 20.1 31.4 27.7,1,LM,True
QR [4],0,,False
22.2 29.4 23.8 49.8 50.6 27.2 21.8 37.7 28.1,0,,False
WEWiki WEBio WEBio+rtrft WEBio+dmn,1,Wiki,True
21.8 28.5 23.2 52.8 50.0 26.9 20.9 36.5 29.9,0,,False
23.9 31.2 25.5 57.1 51.9 29.2 23.1 46.2 34.1 24.8 33.6 26.4 58.3 52.4 30.7 24.0 55.5 34.9 25.4 33.0 27.0 59.8 53.0 30.6 24.4 56.1 37.1,0,,False
"Table 2: Top relevant words to the word ""expression"" according to embeddings trained on Wikipedia vs. Genomics.",1,Wiki,True
General (Wikipedia),1,Wiki,True
interpretation sense emotion function intension manifestation expressive,0,,False
Biomedical (Genomics),0,,False
upregulation mrna,0,,False
induction protein,0,,False
abundance gene,0,,False
downregulation,0,,False
extracted citation contexts using our contextualization methods in terms of how accurate they are with respect to human annotations.,0,,False
"Evaluation. We consider the following evaluation metrics for assessing the quality of the retrieved contexts for each citation from multiple aspects: (i) Character o set overlaps of the retrieved contexts with human annotations in terms of precision (c-P), recall (c-R) and F-score (c-F). These are the recommended metrics for the task per TAC5. (ii) nDCG: we treat any partial overlaps with the gold standard as a correct context and then calculate the nDCG scores. (iii) R -N scores: To also consider the content similarity of the retrieved contexts with the gold standard, we calculate the R scores between them. (iv) Character precision at K (c-P@K): Since we are usually interested in the top retrieved spans, we consider character o set precision only for the top K spans and we denote it with ""c-P@K"".",0,,False
Results. The results of intrinsic evaluation of contextualization are presented in Table 1. Our models (last 4 rows of table 1) achieve signi cant improvements over the baselines consistently across most of the metrics. This shows the e ectiveness of our models viewed from di erent aspects in comparison with the baselines. The best baseline performance is the query reformulation (QR) method by [4] which improves over other baselines.,0,,False
"We observe that using general domain embeddings does not provide much advantage in comparison with the best baseline (compare WEwiki and QR in the Table). However, using the domain speci c embeddings (WEBio ) results in 10% c-F improvement over the best baseline. This is expected since word relations in the biomedical context are better captured with biomedical embeddings. In Table 2 an illustrative word ""expression"" gives better intuition why is that the case. As shown, using general embeddings (left column in the table), the most similar words to ""expression"" are those related to",1,ad,True
5 https://tac.nist.gov/2014/BiomedSumm/guidelines.html,0,,False
Table 3: Breakdown of our best model's character F-score (cF) by quartiles of human performance measured by c-P.,0,,False
Quartiles (c-P),0,,False
Q1 Q2 Q3 Q4,0,,False
c-F of our model 16.14 25.41 33.72 37.50 (mean ± stdev.) ±20.20 ±7.78 ±5.81 ±5.93,0,,False
"the general meaning of it. However, many of these related words are not relevant in the biomedical context. In the biomedical context, ""expression"" refers to ""the appearance in a phenotype attributed to a particular gene"". As shown on the right column, the domain speci c embeddings (Bio) trained on genomics data are able to capture this meaning. This further con rms the inferior performance of the out-of-domain word embeddings in capturing correct word-level semantics [13]. Last two rows in Table 1 show incorporating the domain knowledge in the model which results in signi cant improvement over the best baseline in terms of most metrics (e.g. 14% and 16% c-F improvements). This shows that domain ontologies provide additional information that the domain trained embeddings may not contain. While both our methods of incorporating domain ontologies prove to be e ective, interpolating domain knowledge directly (WEBio +dmn) has the edge over retro tting (WEBio +rtrft). This is likely due to the direct e ect of ontology on the interpolated language model, whereas in retro tting, the ontology rst a ects the embeddings and then the context extraction model.",1,corpora,True
"To analyze the performance of our system more closely, we took the context identi ed by 1 annotator as the candidate and the other 3 as gold standard and evaluated the precision to obtain an estimate of human performance on each citation. We then divided the citations based on human performance to 4 groups by quartiles. Table 3 shows our system's performance on each of these groups. We observe that, when human precision is higher (upper quartiles in the table), our system also performs better and with more con dence (lower std). Therefore, the system errors correlate well with human disagreement on the correct context for the citations. Averaged over the 4 annotators for each citation, the mean precision was 56.7% (note that this translates to our c-P@1 metric). In Table 1, we observe that our best method (c-P@1 of 56.1%) is comparable with average human precision score (c-P@1 of 56.7%) which further demonstrates the e ectiveness of our model.",0,,False
3.2 External evaluation,0,,False
"Citation-based summarization can e ectively capture various contributions and aspects of the paper by utilizing citation texts [15]. However; as argued in section 1, citation texts do not always accurately re ect the original paper. We show how adding context from the original paper can address this concern, while keeping the bene ts of citation-based summarization. Speci cally, we compare how using no contextualization, versus various proposed contextualization approaches a ect the quality of summarization. We apply the following well-known summarization algorithms on the set of citation texts, and the retrieved citation-contexts: LexRank, LSAbased, SumBasic, and KL-Divergence (For space constraints, we will not explain these approaches here; refer to [14] for details). We then compare the e ect of our proposed contextualization methods using the standard R -N summarization evaluation metrics.",1,ad,True
"Results. The results of external evaluation are illustrated in Table 4. The rst row (""No context"") shows the performance of each",0,,False
1135,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: E ect of contextualization on summarization.,0,,False
Columns are summarization algorithms and rows show ci-,0,,False
tation contextualization approaches. No Context uses only,0,,False
citations without any contextualization. Evaluation metrics,0,,False
are R,0,,False
(R ) scores. () shows statistically signi cant im-,0,,False
provement over the best baseline performance (p<0.05).,0,,False
KLSUM LexRank,0,,False
LSA,0,,False
SumBasic,0,,False
Method,0,,False
R1 R2 R1 R2 R1 R2 R1 R2,0,,False
No Context,0,,False
36.0 8.3 41.3 10.8 34.7 6.5 38.7 8.7,0,,False
VSM [4],0,,False
35.3 7.9 40.0 9.9 33.5 6.2 39.5 9.4,0,,False
BM25,0,,False
35.5 8.0 39.8 9.9 33.7 6.0 38.9 8.6,0,,False
DESM [12],0,,False
36.3 8.7 40.2 10.4 32.6 6.5 38.3 7.9,0,,False
LMD-LDA [10] 38.4 9.1 43.1 11.0 37.8 7.6 40.1 8.9,1,LM,True
QR [4],0,,False
39.9 10.2 43.8 11.7 38.9 8.0 40.1 8.6,0,,False
WEWiki WEBio,1,Wiki,True
39.7 10.2 42.7 11.8 38.0 8.0 40.2 9.2 41.7 11.7 45.6 13.8 40.3 9.1 42.4 12.6,0,,False
WEBio+rtrft 42.9 12.2 46.2 11.6 40.0 8.9 41.3 9.7 WEBio+dmn 44.0 13.4 47.3 13.6 42.3 10.4 44.0 11.7,0,,False
"summarization approach solely on the citations without any contextualization. The next 5 rows show the baselines and last 4 rows are our proposed contextualization methods. As shown, e ective contextualization positively impacts the generated summaries. For example, our best method is ""WEBio + dmn"" which signi cantly improves the quality of generated summaries in terms of R over the ones without any context. We observe that two low-performing baseline methods for contextualization according to Table 1 (""VSM"" and ""BM25"") also do not result in any improvements for summarization. Therefore, the intrinsic quality of citation contextualization has direct impact on the quality of generated summaries. These results further demonstrate that e ective contextualization is helpful for scienti c citation-based summarization.",0,,False
4 RELATED WORK,0,,False
"Related work has mostly focused on extracting the citation text in the citing article (e.g. [1]). In this work, given the citation texts, we focus on extracting its relevant context from the reference paper. Related work have also shown that citation texts can be used in di erent applications such as summarization [2, 3, 9, 11, 15, 20]. Our proposed model utilizes word embeddings and the domain knowledge. Embeddings have been recently used in general information retrieval models. Vuli and Moens [19] proposed an architecture for learning word embeddings in multilingual settings and used them in document and query representation. Mitra et al. [12] proposed dual embedded space model that predicts document aboutness by comparing the centroid of word vectors to query terms. Ganguly et al. [7] used embeddings to transform term weights in a translation model for retrieval. Their model uses embeddings to expand documents and use co-occurrences for estimation. Unlike these works, we directly use embeddings in estimating the likelihood of query given documents; we furthermore incorporate ways to utilize domain speci c knowledge in our model. The most relevant prior work to ours is [4] where the authors approached the problem using a vector space model similarity ranking and query reformulations.",1,corpora,True
5 CONCLUSIONS,0,,False
Citation texts are textual spans in a citing article that explain certain contributions of a reference paper. We presented an e ective model for contextualizing citation texts (associating them with the,0,,False
appropriate context from the reference paper). We obtained statisti-,0,,False
cally signi cant improvements in multiple evaluation metrics over,0,,False
"several strong baseline, and we matched the human annotators",0,,False
precision. We showed that incorporating embeddings and domain,1,corpora,True
knowledge in the language modeling based retrieval is e ective for,0,,False
situations where there are high terminology variations between,0,,False
the source and the target (such as citations and their reference,0,,False
context). Citation contextualization not only can help the readers,1,ad,True
"to better understand the citation texts but also as we demonstrated,",0,,False
they can improve other downstream applications such as scienti c,0,,False
"document summarization. Overall, our results show that citation",0,,False
contextualization enables us to take advantage of the bene ts of,1,ad,True
"citation texts, while ensuring accurate dissemination of the claims,",0,,False
ideas and ndings of the original referenced paper.,0,,False
ACKNOWLEDGEMENTS,0,,False
We thank the three anonymous reviewers for their helpful com-,0,,False
ments and suggestions. This work was partially supported by Na-,0,,False
tional Science Foundation (NSF) through grant CNS-1204347.,0,,False
REFERENCES,0,,False
"[1] Amjad Abu-Jbara and Dragomir Radev. 2012. Reference scope identi cation in citing sentences. In NAACL-HLT. ACL, 80­90.",1,ad,True
[2] Arman Cohan and Nazli Goharian. 2015. Scienti c Article Summarization Using Citation-Context and Article's Discourse Structure. In EMNLP. 390­400.,0,,False
[3] Arman Cohan and Nazli Goharian. 2017. Scienti c document summarization via citation contextualization and scienti c discourse. International Journal on Digital Libraries (2017).,0,,False
"[4] Arman Cohan, Luca Soldaini, and Nazli Goharian. 2015. Matching Citation Text and Cited Spans in Biomedical Literature: a Search-Oriented Approach. In NAACL-HLT. 1042­1048.",0,,False
"[5] Anita de Waard and Henk Pander Maat. 2012. Epistemic modality and knowledge attribution in scienti c discourse: A taxonomy of types and overview of features. In Workshop on Detecting Structure in Scholarly Discourse. ACL, 47­55.",0,,False
"[6] Manaal Faruqui, Jesse Dodge, Kumar Sujay Jauhar, Chris Dyer, Eduard Hovy, and A. Noah Smith. 2015. Retro tting Word Vectors to Semantic Lexicons. In NAACL-HLT. Association for Computational Linguistics, 1606­1615.",0,,False
"[7] Debasis Ganguly, Dwaipayan Roy, Mandar Mitra, and Gareth J.F. Jones. 2015. Word Embedding Based Generalized Language Model for Information Retrieval. In SIGIR. ACM, 795­798.",0,,False
"[8] Felix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with similarity estimation. Computational Linguistics (2015).",0,,False
"[9] Kokil Jaidka, Muthu Kumar Chandrasekaran, Sajal Rustagi, and Min-Yen Kan. 2016. Overview of the CL-SciSumm 2016 Shared Task.. In BIRNDL@ JCDL.",0,,False
"[10] Fanghong Jian, Jimmy Xiangji Huang, Jiashu Zhao, Tingting He, and Po Hu. 2016. A simple enhancement for ad-hoc information retrieval via topic modelling. In SIGIR. ACM, 733­736.",1,ad-hoc,True
"[11] Qiaozhu Mei and ChengXiang Zhai. 2008. Generating Impact-Based Summaries for Scienti c Literature.. In ACL, Vol. 8. 816­824.",0,,False
"[12] Bhaskar Mitra, Eric Nalisnick, Nick Craswell, and Rich Caruana. 2016. A dual embedding space model for document ranking. CoRR arXiv:1602.01137 (2016).",0,,False
"[13] Ramesh Nallapati, Bowen Zhou, and Mingbo Ma. 2016. Classify or Select: Neural Architectures for Extractive Document Summarization. arXiv:1611.04244 (2016).",0,,False
"[14] Ani Nenkova and Kathleen McKeown. 2012. A survey of text summarization techniques. In Mining text data. Springer, 43­76.",0,,False
[15] Vahed Qazvinian and Dragomir R. Radev. 2008. Scienti c Paper Summarization Using Citation Summary Networks (COLING '08). 689­696.,1,ad,True
"[16] Anna Ritchie, Stephen Robertson, and Simone Teufel. 2008. Comparing Citation Contexts for Information Retrieval. In CIKM. ACM, 213­222.",0,,False
"[17] Ágnes Sándor and Anita De Waard. 2012. Identifying claimed knowledge updates in biomedical research articles. In Proceedings of the Workshop on Detecting Structure in Scholarly Discourse. ACL, 10­17.",0,,False
[18] Simone Teufel and Marc Moens. 2002. Summarizing scienti c articles: experiments with relevance and rhetorical status. Computational linguistics 28 (2002).,0,,False
[19] Ivan Vuli and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In SIGIR.,0,,False
"[20] Stephen Wan, Cécile Paris, and Robert Dale. 2009. Whetting the appetite of scientists: Producing summaries tailored to the citation context. In JCDL. 59­68.",0,,False
"[21] Chengxiang Zhai and John La erty. 2004. A study of smoothing methods for language models applied to information retrieval. TOIS 22, 2 (2004), 179­214.",0,,False
1136,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues,0,,False
Amina Kadry,1,ad,True
"Dymatrix Consulting Stu gart, Germany a.kadry@dymatrix.de",1,ad,True
ABSTRACT,0,,False
Our goal is to complement an entity ranking with human-readable explanations of how those retrieved entities are connected to the information need. Relation extraction technology should aid in,1,ad,True
"nding such support passages, especially in combination with entities and query terms. is work explores how the current state of the art in unsupervised relation extraction (OpenIE) contributes to a solution for the task, assessing potential, limitations, and avenues for further investigation.",0,,False
"ACM Reference format: Amina Kadry and Laura Dietz. 2017. Open Relation Extraction for Support Passage Retrieval: Merit and Open Issues. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080744",1,ad,True
1 INTRODUCTION,1,DUC,True
"It seems obvious that technology for extracting the meaning of text, such as relation extraction, should lead to be er text retrieval methods. Yet, so far successes have been rare. is paper studies di erent ways of exploiting open relation extraction technology, assesses the potential for merit as well as open issues that inhibit further success for text-centric information retrieval.",1,ad,True
"Given sentences as input, open relation extraction (OpenIE) algorithms extract information on how knowledge base entities are related by analyzing the grammatical structure of each sentence.",0,,False
"To assess opportunities for future merit, we choose a text ranking task that operates on the sentence level and for which information about entities and relations is clearly pertinent: Retrieving explanations for how/why a knowledge base entity is relevant for an information need. is task is useful whenever entities are displayed along with web search results, such as entity cards [3].",0,,False
"Task (support passage ranking): A user enters information need Q; an external system predicts a ranking of relevant entities E. Our task is to, for every relevant entity ei  E, retrieve and rank K passages sik that explain why this entity ei is relevant for Q.",0,,False
"We postulate and study the following hypothesis. For a given entity ei , passages sik that explain a relevant relationship involving the",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080744",1,ad,True
Laura Dietz,0,,False
"University of New Hampshire Durham, NH, USA dietz@cs.unh.edu",0,,False
"entity ei , are also good human-readable descriptions of why the entity is relevant for the information need Q.",1,ad,True
"Of course, conventional OpenIE algorithms have no knowledge of the information need Q. erefore, we study outcomes of relation extraction in superposition with retrieval models such as query likelihood. is paper studies how much OpenIE contributes to accomplishing this task. While there are many suggested approaches to OpenIE, we focus on the ClausIE system, which has been shown to be one of the best OpenIE methods on three established benchmark datasets [5].",0,,False
"Contributions. is paper features an in-depth study of the utility of a state-of-the-art OpenIE extraction system. We study how relation extraction can help, what are promising avenues for further research, and what are limitation of current relation extraction approaches that need to be overcome.",0,,False
"We demonstrate that OpenIE methods provide signi cantly better indicators for entity-centric passage ranking tasks, in contrast to low-level NLP methods such as part-of-speech tagging, named entity recognition, or dependency parsing. Despite these signi cant improvements, we quantify how limitations of current OpenIE systems are a ecting the quality of downstream information retrieval tasks.",0,,False
Outline. e state-of-the-art is summarized in Section 2. A short introduction to the relation extraction system ClausIE is given in Section 3. Section 4 details the feature-based learning-to-rank approach through which we evaluate the merit of OpenIE technology.,0,,False
antitative experimental results are provided in Section 5.,0,,False
2 RELATED WORK,0,,False
"Relations and retrieval. Given a relationship in a knowledge graph, Voskarides et al. [14] study the problem of nding human readable descriptions of that relationship. e relationship is given in the form ei , r , ej , where ei and ej are given entities, i.e., nodes in the knowledge graph and r is a type of a relationship, such as works for. Given this relationship, the task is to rank text passages sijk by how well they describe the relationship in human-readable form. is is the inverse problem to relation extraction [5, 11] where the task is to, given a textual description sijk , extract relational facts in the form ei , r , ej . None of these approaches take a further information need Q into consideration.",1,ad,True
"In the context of web queries, Schuhmacher et al. [12] apply supervised relation extraction to documents that are relevant for the information need Q and study how many of the extracted relations ei , r , ej are indeed relevant for Q. ey also analyze sentences, such as sijk , from which the relevant relation were extracted.",0,,False
Sentence retrieval. Previous work on retrieving entities and support sentences addresses the sentence retrieval problem. For,1,ad,True
1149,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"example Blanco et al. [4] present a model that ranks entity support sentences with learning-to-rank. eir work focuses on features based on named entity recognition (NER) in combination termbased retrieval models. Many features based on using knowledge graph entities for text retrieval could also be applied here, such as the latent entity space model of Liu et al. [9].",0,,False
"Temporal event summarization. Temporal summarization is the task of identifying short and relevant sentences about a developing news event such as disasters, accidents, etc. [1] in a realtime se ing. Each event can be seen as a textual query that describes the event. For example, Kedzie et al. [8] propose to cluster sentences with salience predictions in the context of a named event within a multi-document summarization system. In line with many featurebased approaches, their system exploits term-based retrieval, query expansion, geographical and temporal relevance features.",0,,False
"estion answering. Given a question in natural language, estion Answering methods focus on providing correct and precise answers [13]. QA systems rst use IR techniques are used to retrieve passages that contain the answer. Next these are analyzed to extract a concise answer. Whenever the question includes an entity, a solution to our task is also applicable to the rst stage of question answering.",0,,False
3 FOUNDATION: CLAUSIE,0,,False
"ClausIE [5] is an OpenIE (unsupervised relation extraction) system designed for high-precision extractions. In contrast to previous OpenIE approaches, such as TextRunner [2] and Reverb [7], ClausIE distinguishes between the discovery of useful information from a given sentence and the representation of this information through multiple propositions. e system identi es di erent types of clauses, such as adverbial, complement, indirect object, and direct object. In contrast to many earlier approaches, ClausIE does not require labeled or unlabeled training data or global post-processing, making it applicable to open-domain retrieval tasks.",1,ad,True
"Example. Given the following sentence with token indices:1 "" e1 rules2 of3 golf4 are5 a6 standard7 set8 of9 regulations10 and11 procedures12 by13 which14 the15 sport16 of17 golf18 should19 be20 played21 .""",0,,False
"Phase 1. Clause types are extracted, representing constituents by their head word with token o set. For example:",1,ad,True
"Complementary clause SVC(C:set8, V:are5, S:rules2 , A?:of9 )",0,,False
Adverbial clause,0,,False
"SVA(V:played21, S:sport16, S:by13)",0,,False
Phase 2. Propositions of relation tuples are derived. For example:,0,,False
e rules2 of golf e rules2 of golf e rules2 of golf,0,,False
are5,0,,False
a standard set8 of9 regulations,0,,False
are5,0,,False
a standard set8 of9 procedures,0,,False
are5,0,,False
a standard set8,0,,False
the sport16 of golf should be played21 by13 a standard set8 of regulations the sport16 of golf should be played21 by13 a standard set8 of procedures,0,,False
"Whenever entity and query terms are contained in the same proposition, this sentence is likely to explain the connection between query and entity.",0,,False
1See demo at h ps://gate.d5.mpi-inf.mpg.de/ClausIEGate/ClausIEGate.,0,,False
4 APPROACH: RANKING SENTENCES FOR EXPLAINING ENTITY RELEVANCE,1,AP,True
"To study the utility of ClausIE for the support passage ranking task, we make use of a common two-step approach of 1) extracting candidate sentences and 2) using learning to rank (LTR) with a rich set of features, some of which are based on ClausIE's extractions.",0,,False
4.1 Extracting Candidate Sentences,0,,False
"In order to create a set of candidate sentences for a given query Q and entity ei , a corpus of documents that is pertinent to the entity is required. Any corpus could be used here, such as the ClueWeb corpus with entity links, as used by Schuhmacher et al. [12]. Assuming that OpenIE works best on grammatically wellformed sentences, we instead follow Voskarides et al. [14] and base this study on sentences from the Wikipedia article of the entity ei .",1,ClueWeb,True
4.2 Machine Learning (LTR),0,,False
Sentences are ranked with a list-wise learning-to-rank (LTR) approach implemented in RankLib.2 e weight parameter is learned by optimizing for the Mean-Average Precision metric (MAP) using coordinate ascent and 20 restarts. e LTR will learn a weighted feature combination to achieve the best possible ranking on the training set. Features of di erent categories are discussed below. We study feature sets for their merit by applying LTR on hold-out test data using cross-validation.,1,MAP,True
4.3 Sentence Ranking Features,0,,False
"Table 1 details the features which fall into these categories: Text features and quality features (Text) (1­8) capture the relevance and quality of the sentence at the term level. NLP features (9­16) are derived from part-of-speech (POS) and named entity recognition (NER) tags. ese have been speculated to not help IR. Dependency parse tree (DP) features (17­19) capture the grammatical structure of the sentence. We use the Stanford dependency parser [10] which is also used by the ClausIE system. Earlier works on relation extraction use the direct path between two entities in the dependency parse tree [15]. ClausIE features (20­43) capture the sentence's relation information about entity and query terms. Features are divided by positions of the relation proposition, i.e., subject, verb, and object. Relation quality indicators are included, such as the proposition length measured in tokens or the maximum constituent length (number of tokens in dependency subtree)--both averaged across all propositions extracted from this sentence.",0,,False
5 EXPERIMENTAL EVALUATION,0,,False
We conduct a series of experiments to determine the utility and issues of an available state-of-the-art OpenIE system. We focus on the task of ranking support sentences by how well they explain the relevance of a given entity ei for a given information need Q.,0,,False
"e study is divided according to three questions: 1) Under ideal conditions, could relation extractions help rank relevant passages? 2) What quality is achieved by a fully-automatic learning-to-rank",0,,False
2 lemurproject.org/ranklib.php,0,,False
1150,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Features used for support passage ranking.,0,,False
Feat. Description,0,,False
Text 1 sentence length measured in number of words 2 sentence position measured as a fraction of the document 3 fraction words that are stop words 4 fraction of query terms covered by sentence 5 sum of ISF of query terms (ISF is inverse sentence frequency) 6 average of ISF of query terms 7 sum of TF-ISF of query terms 8 number of entities mentioned,0,,False
NLP 9-12 13 14-16,0,,False
for nouns/verbs/adjectives/adverbs: fraction of words with POS tag whether sentence contains a named entity for NER types PER/LOC/ORG: whether NER of type is contained,1,ad,True
DP 17 number of edges on the path between two entities in dependency tree 18 indicator whether path goes through root node 19 indicator whether path goes through query term,0,,False
ClausIE 20 whether ClausIE generated an extraction from this sentence,0,,False
21-27 for all seven clause types: whether clause of this type is extracted 28 proposition length measured in tokens 29 maximum constituent length (size of dependency tree) in proposition,0,,False
30-32 for subject/object/both: if another entity is in subject and/or object position of the proposition,0,,False
33-34 for subject/object position: if given entity is in position of proposition 35-36 for subject/object position: if any entity is in position of proposition 37-38 for subject/object position: if an entity link is in position of prop. 39-41 for subject/verb/object position: if a query term (ignoring stopwords),0,,False
is in position of proposition 42-43 for subject/object position: if a named entity (NER) is in position of,0,,False
proposition,0,,False
approach with OpenIE features (cf. Section 4)? 3) Which open issues of OpenIE systems inhibit the application to text ranking tasks?,0,,False
5.1 Test collection,0,,False
"For this study we build a test collection3 for 95 support passage rankings (one per query and entity). We use a subset of ten 2013/2014 TREC Web track queries and (up to) ten relevant entities E for these topics, which are taken from the REWQ gold standard.4 To focus this study on grammatically sound and well-wri en documents, we use Wikipedia articles of each relevant entity as a basis for candidate sentences. ese are taken from the 2012 Wikipedia Wex dump. To obtain a base set for assessment, these sentences are processed by the ClausIE extraction system.",1,TREC,True
"We ask assessors to imagine they were to write a knowledge article on the topic Q, on which they were to include information about the given entity ei . Assessors are asked to mark passages that would be suitable support passages for the article by answering the following question:",0,,False
AQ1) Explanation: Does the sentence explain the relevance of entity ei ?,0,,False
"is way we obtain candidate sentences for 95 query-entity pairs as input topics. We arrive at a total of 31,397 assessed sentences with 2,906 relevant support passages of entity relevance. O en, the relevant aspects of a relevant entity are not noteworthy enough to be described in the entity's article [6]. is leads to 20 query-entity pairs that don't contain any explanations of entity-relevance. ese",1,ad,True
3data set available: www.cs.unh.edu/dietz/appendix/openie4ir 4h p://mschuhma.github.io/rewq/,0,,False
Table 2: Performance of AQ1­5 as predictors for explanations and Pearson correlation .  signi cance over Qterm,0,,False
Prec() Recall ,0,,False
Count,0,,False
Relation,0,,False
0.46 ±0.05 0.28 ±0.03 0.27,0,,False
1767 (8%),0,,False
Rel rel,0,,False
0.52 ±0.05  0.21 ±0.03 0.52,0,,False
935 (4%),0,,False
ClausIE,0,,False
0.45 ±0.05 0.20 ±0.02 0.33,0,,False
1172 (5%),0,,False
ClausIE rel,0,,False
0.49 ±0.05  0.14 ±0.02 0.49,0,,False
636 (3%),0,,False
Qterm (),0,,False
0.38 ±0.04 0.49 ±0.04 0.47,0,,False
4476 (20%),0,,False
Name,0,,False
0.33 ±0.05 0.43 ±0.04 0.35,0,,False
6173 (27%),0,,False
Table 3: Results on ranking of sentences explaining entity relevance with LTR.,0,,False
Method,0,,False
Full Text NLP DP ClausIE,0,,False
MAP (),1,MAP,True
0.44 ±0.03 0.42* ±0.03 0.31* ±0.03 0.33* ±0.03 0.41* ±0.03,0,,False
Hurt,0,,False
­ 23 39 43 25,0,,False
Helped,0,,False
­ 9 11 5 11,0,,False
Ablation,0,,False
MAP,1,MAP,True
Full-TEXT Full-NLP Full-DP Full-ClausIE,0,,False
0.41 ±0.03 0.43 ±0.04 0.43 ±0.04 0.43 ±0.03,0,,False
"are excluded from this study, leaving 75 query-entity pairs and 22,731 support passage annotations of which 2,906 are marked as relevant according to AQ1.",0,,False
"In order to study characteristics of sentences in relation to AQ1, we further ask annotators to assess the following questions for each sentence sik , per query Q and entity ei : AQ2) Relation: Does the sentence mention any relationship involving ei ? AQ3) Rel rel: Is this relationship relevant for the explanation? AQ4) ClausIE: Does ClausIE extract a valid relationship from sentence? AQ5) ClausIE rel: Is ClausIE's extraction relevant for the explanation?",0,,False
We study these annotations in combination with two heuristics: Qterm: Does the sentence include query terms (stopwords ignored)? Name: Does the sentence include the entity's name?,0,,False
5.2 Experiment 1: Relations and Relevance,0,,False
"By casting the result of every annotation question (Relation, Rel rel, ClausIE, ClausIE rel) as well as heuristics (Qterm, Name) as a random variable, we study both the Pearson correlation  of these predictors and the ground truth (Explanation / AQ1) as well as their predictive power as measured by set precision and recall in Table 2.",0,,False
"ese demonstrate that good explanations are found in sentences that express a relevant relation of the entity (Rel rel / AQ3), re ected in the highest Pearson correlation of 0.52, as well as the highest precision of 0.52. Using Rel rel as a predictor is signi cantly5 be er in terms of precision than using the Qterm heuristic (which achieves precision of 0.38). However, the Qterm heuristic achieves a much higher recall of 0.49. is suggests that combining query terms and relation extractions is a worthwhile avenue for investigation.",0,,False
"Of course, this requires an automatic approach for distinguishing relevant from non-relevant relation expressions. On the pessimistic side, only half of all extracted relations are indeed relevant. On optimistic side, macro-avg precision drops only mildly from 0.52 for relevant relations (Rel rel / AQ3) to 0.46 for any relation (AQ2) and 0.45 for ClausIE extractions (AQ4). We speculate that an OpenIE relation extractor can also serve as a quality indicator for passages as it is sensitive towards well-formed sentences.",0,,False
"5Paired-t-test with  , 5%.",0,,False
1151,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
MAP,1,MAP,True
0.5,0,,False
0.4,0,,False
0.3,0,,False
0.2,0,,False
0.1,0,,False
0,0,,False
Full,0,,False
Text,0,,False
NLP,0,,False
DP,0,,False
ClausIE,0,,False
Figure 1: Results on ranking of sentences explaining entity relevance: full vs. subsets,0,,False
5.3 Experiment 2: Evaluation through LTR,0,,False
Next we demonstrate that features derived from ClausIE's extractions can be e ectively used to train a learning-to-rank (LTR) method for ranking support passages as detailed in Section 4.,0,,False
"e Full feature set, given in Table 1, is divided into four feature sets by category: Text, NLP, DP, and ClausIE. We compare our approach using all features (Full) versus each feature set individually. Statistically signi cant improvements of the Full using a paired t-test ( ,"" 5%) are marked with *. We additionally perform an ablation study by removing one feature subset at a time (Full-category) from the Full feature set, to study redundancy in the feature space.""",1,ad,True
"For learning to rank, approaches are evaluated with 5-fold crossvalidation, where all rankings associated with the same query (but di erent entities) are assigned to the same fold. e ranking performance is measured in mean-average precision (MAP) with respect to the ground truth of a sentence explaining the relevance of the entity for the query (AQ1). Results are presented in Table 3 and Figure 1. Unjudged sentences are considered non-relevant. Results given in Table 3 show that the Full method outperforms all other methods signi cantly with a MAP of 0.36. Individually, the strongest feature subsets are ClausIE and Text, and the ablation study con rms that they provide complementary merit.",1,MAP,True
"Despite issues due to precision-orientation of OpenIE systems (more details about this in the next section), we obtain signi cant improvements with respect to the recall-oriented evaluation metric MAP. is demonstrates that there is merit in further investigating high-level NLP extractions based on OpenIE. is is in contrast to other kinds of NLP extractions such as POS tags, NER tags, and dependency parse information which are signi cantly worse indicator for support passages.",1,MAP,True
5.4 Experiment 3: Open Issues,0,,False
"Many NLP-oriented systems are tuned for high precision at the expense of recall. While this is a desirable property in the context of knowledge base population, it may impose limitations for information retrieval tasks.",0,,False
"Among all sentences that express a relation, ClausIE is missing this relation in 32% of the cases. Additionally, only half of the sentences with relation expressions actually actually contain a relation that is relevant for the query-entity pair (con rming ndings of Schuhmacher et al. [12]). Together this results in only 636 sentences with relevant ClausIE extractions (3%) of all 22731 annotated sentences. In contrast, our data set contains 2906 sentences (13%) with explanations of relevance.",0,,False
"While there are ClausIE extractions for 9951 sentences, only",0,,False
1172 constitute a correct extraction. Comparing this to the 2906,0,,False
true relevant sentences demonstrates that a perfect recall is not,0,,False
obtainable. Let us consider an optimistic thought experiment where,0,,False
all sentences with correct ClausIE extractions are relevant. An ideal,0,,False
"ranking, which places all relevant sentences rst, would obtain a",0,,False
MAP,1,MAP,True
value,0,,False
of,0,,False
1172 2906,0,,False
",",0,,False
0.41,0,,False
(theoretical,0,,False
upper,0,,False
bound).,0,,False
is upper,0,,False
bound happens to coincide with the actual MAP achieved by the,1,MAP,True
"ClausIE feature set alone, MAP 0.41, cf. Table 3. We conclude that",1,MAP,True
our approach obtains an optimal ranking under limitations imposed,0,,False
by the o -the-shelf OpenIE system. Improving coverage of OpenIE,0,,False
systems is likely to translate to immediate quality improvements,0,,False
for text-ranking tasks.,0,,False
6 CONCLUSION,0,,False
"We study the utility of OpenIE technology ranking sentences by how well they explain the relevance of a given entity for a query. Based on manual assessments and evaluation through a learning-torank framework, the study demonstrates that signi cant improvements are achieved by combining relation features with query and entity matches. While we demonstrate the merit of an OpenIE extraction system, we also quantify losses through limitations of current OpenIE systems. we hope this study stimulates work on relation extraction systems that are designed of information retrieval tasks.",0,,False
Acknowledgements,0,,False
"is research was performed as part of a Master's thesis at Mannheim University, Germany. We are grateful to Rainer Gemulla for providing access to the ClausIE system. is work was funded in part by a scholarship of the Eliteprogramm for Postdocs of the Baden-Wu¨r emberg Sti ung.",1,ad,True
REFERENCES,0,,False
"[1] J. Aslam, F. Diaz, M. Ekstrand-Abueg, R. McCreadie, V. Pavlu, and T. Sakai. Trec 2014 temporal summarization track overview. Technical report, 2015.",1,ad,True
"[2] M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. Open information extraction from the web. In Proc. of IJCAI, 2007.",1,ad,True
"[3] A. Berntson et al. Providing entity-speci c content in response to a search query, Mar. 8 2012. US Patent App. 12/876,638.",0,,False
"[4] R. Blanco and H. Zaragoza. Finding support sentences for entities. In Proc. of SIGIR, 2010.",0,,False
"[5] L. Del Corro and R. Gemulla. Clausie: clause-based open information extraction. In Proc. of WWW, 2013.",0,,False
"[6] L. Dietz, A. Kotov, and E. Meij. Tutorial on utilizing knowledge graphs in text-centric information retrieval. In Proc. of WSDM, 2017.",0,,False
"[7] A. Fader, S. Soderland, and O. Etzioni. Identifying relations for open information extraction. In Proc. of EMNLP, 2011.",1,ad,True
"[8] C. Kedzie, K. McKeown, and F. Diaz. Predicting salient updates for disaster summarization. In Prof. of ACL, 2015.",0,,False
"[9] X. Liu and H. Fang. Latent entity space: a novel retrieval approach for entitybearing queries. Information Retrieval Journal, 18(6):473­503, 2015.",0,,False
"[10] C. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky. e Stanford CoreNLP natural language processing toolkit. In Proc. of ACL, 2014.",0,,False
"[11] B. Roth, T. Barth, G. Chrupa la, M. Gropp, and D. Klakow. Relationfactory: A fast, modular and e ective system for knowledge base population. In Proc. of EACL, 2014.",0,,False
"[12] M. Schuhmacher, B. Roth, S. P. Ponze o, and L. Dietz. Finding relevant relations in relevant documents. In Proc. of ECIR, 2016.",0,,False
"[13] M. Surdeanu, M. Ciaramita, and H. Zaragoza. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351­383, 2011.",0,,False
"[14] N. Voskarides, E. Meij, M. Tsagkias, M. de Rijke, and W. Weerkamp. Learning to explain entity relationships in knowledge graphs. In Proc. of ACL, 2015.",0,,False
"[15] L. Yao, A. Haghighi, S. Riedel, and A. McCallum. Structured relation discovery using generative models. In Proc. of EMNLP, 2011.",0,,False
1152,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Generating ery Suggestions to Support Task-Based Search,0,,False
Dar´io Gariglio i,0,,False
University of Stavanger dario.gariglio i@uis.no,0,,False
ABSTRACT,0,,False
"We address the problem of generating query suggestions to support users in completing their underlying tasks (which motivated them to search in the rst place). Given an initial query, these query suggestions should provide a coverage of possible subtasks the user might be looking for. We propose a probabilistic modeling framework that obtains keyphrases from multiple sources and generates query suggestions from these keyphrases. Using the test suites of the TREC Tasks track, we evaluate and analyze each component of our model.",1,ad,True
CCS CONCEPTS,0,,False
·Information systems  ery suggestion;,0,,False
KEYWORDS,0,,False
"ery suggestions, task-based search, supporting search tasks",0,,False
"ACM Reference format: Dar´io Gariglio i and Krisztian Balog. 2017. Generating ery Suggestions to Support Task-Based Search. In Proceedings of SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080745",0,,False
1 INTRODUCTION,1,DUC,True
"Search is o en performed in the context of some larger underlying task [11]. ere is a growing stream of research aimed at making search engines more task-aware (i.e., recognizing what task the user is trying to accomplish) and customizing the search experience accordingly (see §2). In this paper, we focus our a ention on one particular tool for supporting task-based search: query suggestions. ery suggestions are an integral part of modern search engines [16]. We envisage an user interface where these suggestions are presented once the user has issued an initial query; see Figure 1. Note that this is di erent from query autocompletion, which tries to recommend various possible completions while the user is still typing the query. e task-aware query suggestions we propose are intended for exploring various aspects (subtasks) of the given task a er inspecting the initial search results. Selecting them would allow the user to narrow down the scope of the search.",0,,False
"e Tasks track at the Text REtrieval Conference (TREC) has introduced an evaluation platform for this very problem, referred to as task understanding [20]. Speci cally, given an initial query,",1,TREC,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080745",1,ad,True
Krisztian Balog,0,,False
University of Stavanger krisztian.balog@uis.no,0,,False
choose bathroom,0,,False
choose bathroom cabinets lightning choose bathroom decoration style bathroom get ideas renew floor bathroom changing furniture bathroom,0,,False
Figure 1: ery suggestions to support task-based search.,0,,False
"the system should return a ranked list of keyphrases ""that represent the set of all tasks a user who submi ed the query may be looking for"" [20]. e goal is to provide a complete coverage of subtasks for an initial query, while avoiding redundancy. We use these keyphrases as query suggestions.",0,,False
Our aim is to generate such suggestions in a se ing where past usage data and query logs are not available or cannot be utilized.,0,,False
"is would be typical for systems that have a smaller user base (e.g., in the enterprise domain) or when a search engine has been newly deployed [4]. One possibility is to use query suggestion APIs, which are o ered by all major web search engines. ese are indeed one main source type we consider. Additionally, we use the initial query to search for relevant documents, using web search engines, and extract keyphrases from search snippets and from full text documents. Finally, given the task-based focus of our work, we lend special treatment to the WikiHow site,1 which is an extensive database of how-to guides.",1,AP,True
"e main contribution of this paper is twofold. First, we propose a transparent architecture, using generative probabilistic modeling, for extracting keyphrases from a variety of sources and generating query suggestions from them. Second, we provide a detailed analysis of the components of our framework using di erent estimation methods. Many systems that participated in the TREC Tasks track have relied on strategic combinations of di erent sources to produce query suggestions, see, e.g., [7­9]. However, no systematic comparison of the di erent source types has been performed yet--we ll this gap. Additional components include estimating a document's importance within a given source, extracting keyphrases from documents, and forming query suggestions from these keyphrases. Finally, we check whether our ndings are consistent across the 2015 and 2016 editions of the TREC Tasks track.",1,TREC,True
2 RELATED WORK,0,,False
"ere is a large body of work on understanding and supporting users in carrying out complex search tasks. Log-based studies have been one main area of focus, including the identi cation of tasks and segmentation of search queries into tasks [2, 14] and mining task-based search sessions in order to understand query",0,,False
1h p://www.wikihow.com/,1,wiki,True
1153,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
q0,0,,False
QS,0,,False
WS,0,,False
WD,0,,False
WH,0,,False
Keyphrases,0,,False
Query suggestions,1,Query,True
Figure 2: High-level overview of our approach.,0,,False
"reformulations [10] or search trails [19]. Another theme is supporting exploratory search, where users pursue an information goal to learn or discover more about a given topic. Recent research in this area has brought the importance of support interfaces into focus [1, 3, 17]. Our main interest is in query suggestions, as a distinguished support mechanism. Most of the related work utilizes large-scale query logs. For example, Craswell and Szummer [6] perform a random walk on a query-click graph. Boldi et al. [5] model the query ow in user search sessions via chains of queries. Scenarios in the absence of query logs have been addressed in [4, 13], where query suggestions are extracted from the document corpus. However, their focus is on query autocompletion, representing the completed and partial terms in a query. Kelly et al. [12] have shown that users prefer query suggestions, rather than term suggestions. We undertake the task of suggesting queries to users, related to the task they are performing, as we shall explain in the next section.",1,ad,True
3 PROBLEM STATEMENT,0,,False
"We adhere to the problem de nition of the task understanding task of the TREC Tasks track. Given an initial query q0, the goal is to return a ranked list of query suggestions q1, . . . qn that cover all the possible subtasks related to the task the user is trying to achieve. In addition to the initial query string, the entities mentioned in it are also made available (identi ed by their Freebase IDs).",1,ad,True
"For example, for the query ""low wedding budget,"" subtasks include (but are not limited to) ""buy a used wedding gown,"" ""cheap wedding cake,"" and ""make your own invitations."" ese subtasks have been manually identi ed by the track organizers based on information extracted from the logs of a commercial search engine.",0,,False
"e suggested keyphrases are judged with respect to each subtask on a three point scale (non-relevant, relevant, and highly relevant). Note that subtasks are only used in the evaluation, these are not available when generating the keyphrases.",0,,False
4 APPROACH,1,AP,True
"We now present our approach for generating query suggestions. As Figure 2 illustrates, we obtain keyphrases from a variety of sources, and then construct a ranked list of query suggestions from these.",0,,False
4.1 Generative Modeling Framework,0,,False
"We introduce a generative probabilistic model for scoring the candidate query suggestions according to P (q|q0), i.e., the probability that a query suggestion q was generated by the initial query q0.",0,,False
Formally:,0,,False
"P (q|q0) ,"" P (q|q0, s)P (s |q0)""",0,,False
s,0,,False
",",0,,False
"P (q|q0, s, d )P (d |q0, s) P (s |q0)",0,,False
sd,0,,False
",",0,,False
"P (q|q0, s, k )P (k |s, d ) P (d |q0, s) P (s |q0) .",0,,False
sdk,0,,False
"is model has four components: (i) P (s |q0) expresses the importance of a particular information source s for the initial query q0; (ii) P (d |q0, s) represents the importance of a document d originating from source s, with respect to the initial query; (iii) P (k |d, s) is",0,,False
the relevance of a keyphrase k extracted from a document d from,0,,False
"source s; and (iv) P (q|q0, s, k ) is the probability of generating query suggestion q, given keyphrase k, source s, and the initial query q0. Below, we detail the estimation of each of these components.",0,,False
4.2 Source Importance,0,,False
"We collect relevant information from four kinds of sources: query suggestions (QS), web search snippets (WS), web search documents (WD), and WikiHow (WH). For the rst three source types, we use three di erent web search engines (Google, Bing, and DuckDuckGo), thereby having a total of 10 individual sources. In this work, we assume conditional independence between a source s and the initial query q0, i.e., set P (s |q0) , P (s).",1,Wiki,True
4.3 Document Importance,0,,False
"From each source s, we obtain the top-K (K ,"" 10) documents for the query q0. We propose two ways of modeling the importance of a document d originating from s: (i) uniform and (ii) inversely proportional to the rank of d among the top-K documents, that is:""",0,,False
"P (d |q0, s) ,",0,,False
K -r +1,0,,False
"K i ,1",0,,False
K,0,,False
-,0,,False
i,0,,False
+,0,,False
1,0,,False
",",0,,False
K -r +1 K (K + 1)/2,0,,False
",",0,,False
where r is the rank position of d (r  [1..K]).,0,,False
4.4 Keyphrase Relevance,0,,False
"We obtain keyphrases from each document, using an automatic keyphrase extraction algorithm. Speci cally, we use the RAKE keyword extraction system [15]. For each keyphrase k, extracted from document d, the associated con dence score is denoted by c (k, d ). Upon a manual inspection of the extraction output, we introduce some data cleansing steps. We only retain keyphrases that: (i) have an extraction con dence above an empirically set threshold of 2; (ii) are at most 5 terms long; (iii) each of the terms has a length between 4 and 15 characters, and is either a meaningful number (i.e., max. 4 digits) or a term (excluding noisy substrings and reserved keywords from mark-up languages). Finally, we set the relevance of k as P (k |d, s) ,"" c (k, d )/ k c (k , d ).""",0,,False
"In case s is of type QS, each returned document actually corresponds to a query suggestion. us, we treat each of these documents d as a single keyphrase k, for which we set P (k |d, s) , 1.",0,,False
1154,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Comparison of query suggestion generators across the di erent types of sources. Statistical signi cance is tested against the corresponding line in the top block.,0,,False
"P (q|q0, s, k ) S",0,,False
2015,0,,False
2016,0,,False
ERR-IA -NDCG ERR-IA -NDCG,0,,False
Using raw keyphrases,0,,False
QS 0.0755 WS 0.2011 WD 0.1716 WH 0.0744,0,,False
0.1186 0.2426 0.2154 0.1044,0,,False
0.4114 0.3492 0.2339 0.1377,0,,False
0.5289 0.4038 0.2886 0.1723,0,,False
Using expanded keyphrases,0,,False
QS 0.0751 WS 0.1901 WD 0.1551 WH 0.0849,0,,False
0.1182 0.2274 0.2097 0.1090,0,,False
0.4046 0.2927 0.1045 0.0789,0,,False
0.5233 0.3467 0.1667 0.0932,0,,False
Table 2: Comparison of document importance estimators across the di erent types of sources. Statistical signi cance is tested against the corresponding line in the top block.,0,,False
"P (d |q0, s) S",0,,False
2015,0,,False
2016,0,,False
ERR-IA -NDCG ERR-IA -NDCG,0,,False
Uniform,0,,False
QS 0.0755 WS 0.2011 WD 0.1716 WH 0.0849,0,,False
0.1186 0.2426 0.2154 0.1090,0,,False
0.4114 0.3492 0.2339 0.1377,0,,False
0.5289 0.4038 0.2886 0.1723,0,,False
Rank-based QS 0.0891 0.1307,0,,False
decay,0,,False
WS 0.1906 0.2315,0,,False
WD 0.1688 0.2119,0,,False
WH 0.0935 0.1225,0,,False
0.4288 0.3386 0.1964 0.1195,0,,False
0.5455 0.4011 0.2608 0.1495,0,,False
4.5 ery Suggestions,0,,False
"As a nal step, we need to generate query suggestions from the extracted keyphrases. As a baseline option, we take each raw keyphrase k as-is, i.e., with q ,"" k we set P (q|q0, s, k ) "", 1.",0,,False
"Alternatively, we can form query suggestions by expanding keyphrases. Here, k is combined with the initial query q0 using a set of expansion rules proposed in [7]: (i) adding k as a suf-",1,ad,True
"x to q0; (ii) adding k as a su x to an entity mentioned in q0; and (iii) using k as-is. Rules (i) and (ii) further involve a custom string concatenation operator; we refer to [7] for details. Each query suggestion q, that is generated from keyword k, has an associated con dence score c (q, q0, s, k ). We then set P (q|q0, s, k ) ,"" c (q, q0, s, k )/ q c (q , q0, s, k ). By conditioning the suggestion probability on s, it is possible to apply a di erent approach for each source. Like in the previous subsection, we treat sources of type QS distinctly, by simply taking q "","" k and se ing P (q|q0, s, k ) "", 1.",1,ad,True
"We note that it is possible that multiple query suggestions have the same nal probability P (q|q0). We resolve ties using a deterministic algorithm, which orders query suggestions by length (favoring short queries) and then sorts them alphabetically.",0,,False
5 RESULTS,0,,False
In this section we present our experimental setup and results.,0,,False
5.1 Experimental Setup,0,,False
"We use the test suites of the TREC 2015 and 2016 Tasks track [18, 20]. ese contain 34 and 50 queries with relevance judgments, respectively. We report on the o cial evaluation metrics used at the TREC Tasks track, which are ERR-IA@20 and -NDCG@20. In accordance with the track's se ings, we use ERR-IA@20 as our primary metric. (For simplicity, we omit mentioning the cut-o rank of 20 in all the table headers.) We noticed that in the ground truth the initial query itself has been judged as a highly relevant suggestion in numerous cases. We removed these cases, as they make li le sense for the envisioned scenario; we note that this leads to a drop in absolute terms of performance. We report on statistical signi cance using a two-tailed paired t-test at p < 0.05 and p < 0.001, denoted by  and , respectively.",1,TREC,True
"In a series of experiments, we evaluate each component of our approach, in a bo om-up fashion. For each query set, we pick the",0,,False
"con guration that performed best on that query set, which is an idealized scenario. Note that our focus is not on absolute performance",0,,False
"gures, but on answering the following research questions:",0,,False
RQ1 What are the most useful information sources? RQ2 What are e ective ways of (i) estimating the importance,0,,False
of documents and (ii) generating query suggestions from keyphrases? RQ3 Are our ndings consistent across the two query sets?,0,,False
5.2 ery Suggestion Generation,0,,False
"We start our experiments by focusing on the generation of query suggestions and compare the two methods described in §4.5. e document importance is set to be uniform. We report performance separately for each of the four source types S (that is, we set P (s) uniformly among sources s  S and set P (s) ,"" 0 for s S). Table 1 presents the results. It is clear that, with a single exception (2015 WH), it is be er to use the raw keyphrases, without any expansion. e di erences are signi cant on the 2016 query set for all source types but QS. Regarding the comparison of di erent source types, we nd that QS > WS > WD > WH on the 2016 query set, meanwhile for 2015, the order is WS > WD > QS, WH.""",0,,False
5.3 Document Importance,0,,False
"Next, we compare the two document importance estimator methods, uniform and rank-based decay (cf. §4.3), for each source type. Table 2 reports the results. We nd that rank-based document importance is bene cial for the query suggestion (QS) source types, for both years, and for WikiHow (WH) on the 2015 topics. However, the di erences are only signi cant for QS 2015. For all other source types, the uniform se ing performs be er.",1,Wiki,True
"We also compare performance across the 10 individual sources. Figure 3 shows the results, in terms of ERR-IA@20, using the uniform estimator. We observe a very similar pa ern using the rankbased estimator (which is not included due to space constraints). On the 2016 query set, the individual sources follow the exact same pa erns as their respective types (i.e., QS > WS > WD > WH), with one exception. e Bing API returned an empty set of search suggestions for many queries, hence the low performance of QSBin . We can observe a similar pa ern on the 2015 topics, with the exception of sources of type QS, which are the least e ective here.",1,AP,True
1155,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
ERR-IA@20,0,,False
0.40 0.35 0.30 0.25 0.20 0.15 0.10 0.05 0.00,0,,False
2015 Query set 2016 Query set,1,Query,True
QS,0,,False
Google,0,,False
QS,0,,False
DDG,0,,False
WS,0,,False
Bing,0,,False
WS,0,,False
Google,0,,False
WS,0,,False
DDG,0,,False
WD,0,,False
Bing,0,,False
WD,0,,False
Google,0,,False
WD,0,,False
DDG,0,,False
WH QS,0,,False
Bing,0,,False
"Figure 3: Performance of individual sources, sorted by performance on the 2016 query set.",0,,False
5.4 Source Importance,0,,False
"Finally, we combine query suggestions across di erent sources; for that, we need to set the importance of each source. We consider three di erent strategies for se ing P (s): (i) uniformly; (ii) proportional to the importance of the corresponding source type (QS, WS, WD, and WH) from the previous step (cf. Table 2); (iii) proportional to the importance of the individual source (cf. Figure 3). e results are presented in Table 3. Firstly, we observe that the combination of sources performs be er than any individual source type on its own. As for se ing source importance, on the 2015 query set we",0,,False
"nd that (iii) delivers the best results, which is in line with our expectations. On the 2016 query set, only minor di erences are observed between the three methods, none of which are signi cant.",0,,False
5.5 Summary of Findings,0,,False
"(RQ1) ery suggestions provided by major web search engines are unequivocally the most useful information source on the 2016 queries. We presume that these search engine suggestions are already diversi ed, which we can directly bene t from for our task.",1,ad,True
"ese are followed, in order, by keyphrases extracted from (i) web search snippets, (ii) web search results, i.e., full documents, and (iii) WikiHow articles. On the 2015 query set, query suggestions proved much less e ective; see RQ3 below. (RQ2) With a single exception, using the raw keyphrases, as-is, performs be er than expanding them by taking the original query into account. For web query suggestions it is bene cial to consider the rank order of suggestions, while for web search snippets and documents the uniform se ing performs be er. For WikiHow, it varies across query sets. (RQ3) Our main observations are consistent across the 2015 and 2016 query sets, regarding documents importance estimation and suggestions generation methods. It is worth noting that some of our methods were o cially submi ed to TREC 2016 [7] and were included in the assessment pools. is is not the case for 2015, where many of our query suggestions are missing relevance assessments (and, thus, are considered irrelevant). is might explain the low performance of QS sources on the 2015 queries.",1,Wiki,True
6 CONCLUSIONS,0,,False
"In this paper, we have addressed the task of generating query suggestions that can assist users in completing their tasks. We have proposed a probabilistic generative framework with four components:",1,ad,True
Table 3: Combination of all sources using di erent source importance estimators. Signi cance is tested against the uniform setting (line 1).,0,,False
P (s),0,,False
Uniform Source-type Individual,0,,False
2015 ERR-IA -NDCG,0,,False
0.2219 0.2835 0.2381 0.2905 0.2518 0.3064,0,,False
2016 ERR-IA -NDCG,0,,False
0.4561 0.4570 0.4562,0,,False
0.5793 0.5832 0.5832,0,,False
"source importance, document importance, keyphrase relevance,",0,,False
and query suggestions. We have proposed and experimentally,0,,False
compared various alternatives for these components.,0,,False
"One important element, missing from our current model, is the",0,,False
"representation of speci c subtasks. As a next step, we plan to cluster",0,,False
query suggestions together that belong to the same subtask. is,0,,False
would naturally enable us to provide diversi ed query suggestions.,0,,False
REFERENCES,0,,False
"[1] Salvatore Andolina, Khalil Klouche, Jaakko Peltonen, Mohammad E. Hoque, Tuukka Ruotsalo, Diogo Cabral, Arto Klami, Dorota Glowacka, Patrik Flore´en, and Giulio Jacucci. 2015. IntentStreams: Smart Parallel Search Streams for Branching Exploratory Search. In Proc. of IUI. 300­305.",1,ad,True
"[2] Ahmed H. Awadallah, Ryen W. White, Patrick Pantel, Susan T. Dumais, and YiMin Wang. 2014. Supporting Complex Search Tasks. In Proc. of CIKM. 829­838.",1,ad,True
[3] Krisztian Balog. 2015. Task-completion Engines: A Vision with a Plan. In Proc. of the 1st International Workshop on Supporting Complex Search Tasks.,0,,False
"[4] Sumit Bhatia, Debapriyo Majumdar, and Prasenjit Mitra. 2011. ery Suggestions in the Absence of ery Logs. In Proc. of SIGIR. 795­804.",0,,False
"[5] Paolo Boldi, Francesco Bonchi, Carlos Castillo, Debora Donato, Aristides Gionis, and Sebastiano Vigna. 2008. e ery- ow Graph: Model and Applications. In Proc. of CIKM. 609­618.",0,,False
[6] Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In Proc. of SIGIR. 239­246.,0,,False
[7] Dar´io Gariglio i and Krisztian Balog. 2016. e University of Stavanger at the TREC 2016 Tasks Track. In Proc. of TREC.,1,TREC,True
"[8] Ma hias Hagen, Steve Go¨ring, Magdalena Keil, Olaoluwa Anifowose, Amir Othman, and Benno Stein. 2015. Webis at TREC 2015: Tasks and Total Recall Tracks. In Proc. of TREC.",1,TREC,True
"[9] Ma hias Hagen, Johannes Kiesel, Payam Adineh, Masoud Alahyari, Ehsan Fatehifar, Arafeh Bahrami, Pia Fichtl, and Benno Stein. 2016. Webis at TREC 2016: Tasks, Total Recall, and Open Search Tracks. In Proc. of TREC.",1,TREC,True
"[10] Jiepu Jiang, Daqing He, Shuguang Han, Zhen Yue, and Chaoqun Ni. 2012. Contextual Evaluation of ery Reformulations in a Search Session by User Simulation. In Proc. of CIKM. 2635­2638.",1,Session,True
"[11] Diane Kelly, Jaime Arguello, and Robert Capra. 2013. NSF Workshop on Taskbased Information Search Systems. SIGIR Forum 47, 2 (2013), 116­127.",0,,False
"[12] Diane Kelly, Karl Gyllstrom, and Earl W. Bailey. 2009. A Comparison of ery and Term Suggestion Features for Interactive Searching. In Proc. of SIGIR. 371­378.",0,,False
"[13] Udo Kruschwitz, Deirdre Lungley, M-Dyaa Albakour, and Dawei Song. 2013. Deriving query suggestions for site search. JASIST 64, 10 (2013), 1975­1994.",0,,False
"[14] Claudio Lucchese, Salvatore Orlando, Ra aele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2013. Discovering Tasks from Search Engine ery Logs. ACM Trans. Inf. Syst. 31, 3, Article 14 (2013), 43 pages.",0,,False
[15] Alyona Medelyan. 2015. Modi ed RAKE algorithm. h ps://github.com/zelandiya/ RAKE-tutorial. (2015). Accessed: 2017-01-23.,0,,False
"[16] Umut Ozertem, Olivier Chapelle, Pinar Donmez, and Emre Velipasaoglu. 2012. Learning to Suggest: A Machine Learning Framework for Ranking ery Suggestions. In Proc. of SIGIR. 25­34.",0,,False
"[17] Tuan A. Tran, Sven Schwarz, Claudia Niedere´e, Heiko Maus, and Na iya Kanhabua. 2016. e Forgo en Needle in My Collections: Task-Aware Ranking of Documents in Semantic Information Space. In Proc. of CHIIR. 13­22.",0,,False
"[18] Manisha Verma, Evangelos Kanoulas, Emine Yilmaz, Rishabh Mehrotra, Ben Cartere e, Nick Craswell, and Peter Bailey. 2016. Overview of the TREC Tasks Track 2016. In Proc. of TREC.",1,TREC,True
[19] Ryen W. White and Je Huang. 2010. Assessing the Scenic Route: Measuring the Value of Search Trails in Web Logs. In Proc. of SIGIR. 587­594.,0,,False
"[20] Emine Yilmaz, Manisha Verma, Rishabh Mehrotra, Evangelos Kanoulas, Ben Cartere e, and Nick Craswell. 2015. Overview of the TREC 2015 Tasks Track. In Proc. of TREC.",1,TREC,True
1156,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Layout and Semantics: Combining Representations for Mathematical Formula Search,0,,False
Kenny Davila,0,,False
Rochester Institute of Technology 1 Lomb Memorial Drive,0,,False
"Rochester, New York 14623 kxd7282@rit.edu",0,,False
ABSTRACT,0,,False
"Math-aware search engines need to support formulae in queries. Mathematical expressions are typically represented as trees de ning their operational semantics or visual layout. We propose searching both formula representations using a three-layer model. e rst layer selects candidates using spectral matching over tree node pairs. e second layer aligns a query with candidates and computes similarity scores based on structural matching. In the third layer, similarity scores are combined using linear regression. e two representations are combined using retrieval in parallel indices and regression over similarity scores. For NTCIR-12 Wikipedia Formula Browsing task relevance rankings, we see each layer increasing ranking quality and improved results when combining representations as measured by Bpref and nDCG scores.",1,Wiki,True
CCS CONCEPTS,0,,False
·Information systems Similarity measures; Rank aggregation; Language models;,0,,False
KEYWORDS,0,,False
"Formula Retrieval; Operator Tree; Symbol Layout Tree ACM Reference format: Kenny Davila and Richard Zanibbi. 2017. Layout and Semantics: Combining Representations for Mathematical Formula Search. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. DOI: 10.1145/3077136.3080748",0,,False
1 INTRODUCTION,1,DUC,True
"Math-aware search engines deal with information needs where documents containing particular math expressions are sought a er, or where document similarity is de ned by text and formulae. An expression can be represented semantically by its operations using an Operator Tree (OPT) or visually by a Symbol Layout Tree (SLT) [16]. Figure 1 shows an SLT and OPT for x - 2 , 0.",0,,False
"Many researchers in Mathematical Information Retrieval (MIR) assume OPTs provide be er formula retrieval results than SLTs, but each has limitations for retrieval. For SLTs, mathematical notation",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080748",1,ad,True
Richard Zanibbi,0,,False
Rochester Institute of Technology 1 Lomb Memorial Drive,0,,False
"Rochester, New York 14623 rlaz@cs.rit.edu",0,,False
(a) Symbol Layout Tree,0,,False
(b) Operator Tree,0,,False
"Figure 1: Tree representations for x - 2 , 0",0,,False
"can change meaning based on context - a symbol may be an operator in one context, and a variable in another, for example. In contrast, well-formed OPTs are mathematically unambiguous. Online, most write math expressions using SLT representations (e.g., LATEX). SLTs can be converted to OPTs using parsers, but semantics are o en unde ned or ambiguous, producing errors [3].",0,,False
"Our previous work (Tangent-31 [1, 17]) uses a two-stage SLT model for formula retrieval. First, top-k candidates are identi ed using a bag-of-words model, using symbol pairs in SLTs as `words.'",0,,False
"en, the top-k candidates are re-ranked a er aligning query and candidate SLTs. Candidates are re-ranked using the harmonic mean of symbol and relationship recall (the Maximum Subtree Similarity) and two tie-breakers: symbol precision a er uni cation, and symbol recall without uni cation.",0,,False
"We present an extended model, Tangent-S, that works with OPTs and uses a stricter uni cation model to avoid matching functions to variable names. A third stage is added using a linear combination of the structure similarity scores for re-ranking. Stronger formula retrieval results are obtained by retrieving SLTs and OPTs independently, and then linearly combining their similarity scores.",1,ad,True
is supports the view of OPTs and SLTs as complementary for formula retrieval.,0,,False
2 BACKGROUND,0,,False
"Approaches to formula search may be classi ed by the primitives used for indexing as text-based, tree-based, and spectral [17]. Detailed analysis of existing methods can be found elsewhere [3].",0,,False
"Text-Based Approaches. Formulae are converted to a sequence of tokens using linearization of formula trees. To increase the likelihood of nding matches, some methods use canonicalization to simplify expressions, and to identify commutative operators and equivalences [8, 12]. It is also common to enumerate identi ers to support generalized variable matching and/or uni cation [2, 12­14].",0,,False
1h ps://cs.rit.edu/ dprl/So ware.html#tangent,0,,False
1165,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Converting math to text allows use of existing optimizations in text search engines, such as ranking by TF-IDF [10], topic modeling, and word embedding [13].",0,,False
"Tree-Based Approaches. ese approaches index formulas as complete SLTs or OPTs. Typically, the hierarchical structures in formulae are mapped directly, and organized within tree-based indexing structures [4, 6, 18]. In these approaches, all subexpressions in formulae are indexed to support partial matching, with common subexpressions labeled and shared to reduce index sizes [6].",0,,False
"Spectral Approaches. Here paths in OPTs/SLTs or features extracted from trees are used as retrieval primitives. Simpler primitives allow more partial matches, increasing recall. Path-based methods store sets of paths from the root to internal nodes [5] and leaves [18]. Paths may re ect operator commutativity by inserting symbols [18] or using unordered paths [7]. Some use hashing to encode subtrees [7, 11]. In Tangent-3 SLT symbol pairs along with their relative paths are used to index math expressions [1, 17].",0,,False
"In this work, we extend the Tangent-3 system [17] to retrieve formulae using both SLTs and OPTs, and make additional improvements detailed below.",1,ad,True
3 METHODOLOGY,0,,False
"Tangent-3 [17] retrieval model has two stages, one for fast candidate selection using spectral matching, and the second for re-ranking top-k candidates. We add a third stage, using a linear combination of similarity scores computed from the second layer to produce a nal re-ranking of the top-k candidates. Rather than use a learning-to-rank technique [9], a simpler model was chosen for be er understanding of the relevance of each similarity factor.",1,ad,True
3.1 Formula Representation,0,,False
"To de ne and constrain the behavior of matching and uni cation algorithms in SLTs and OPTs, we assign each symbol a type. Edges between symbols are labeled by their order in OPTs, or by the visual location of a child symbol with respect to its parent (e.g., for superscript relationships) in SLTs (see Fig. 1).",0,,False
"Common Symbol Types. ese include: Variables, Numbers, Groups (matrices, vectors, sets, lists, ...), Functions, Operators, Text, White Space, ery Wildcard and Error (e.g., for parsing errors). Real data o en provides strong clues for symbol types, but in some cases symbol type can be hard to infer without context, leading to incorrect symbol types and invalid uni cations.",1,ad,True
"Symbol Layout Trees. is representation is built around writing lines (baselines), leading to deep trees with few branches. e children of a node in this representation are assigned to a spatial relationship class (edge label): Next, Above, Pre-above, Below, Pre-Below, Over, Under, Within, and Element.",1,ad,True
"Operator Trees. is representation is built around the hierarchy of operators in a formula, resulting in shallow trees with many branches. We distinguish between commutative operators (e.g., `+') and non-commutative operators (e.g., `-'). We ignore the order of children for commutative operators. In Figure 1.b, all edges to children of the equals sign have the same label.",0,,False
3.2 Pair-based Index Model,0,,False
"A symbol pair is represented by the tuple (A,D,R) where A and D are the ancestor and descendant symbols, and R is the sequence of edge labels in the path from A to D. We use an inverted index, with symbol tuples as keys, and each posting list storing references to formulae containing the tuple. We use independent formula indices for SLTs and OPTs. Two parameters control the symbol tuple generation process: a window size w and an End-of-Baseline (EOB)",0,,False
"ag. Window size w de nes the maximum path length between an indexed symbol pair. If EOB is true, the system creates dummy pairs between the last symbol on each baseline and null, to help with matching small expressions (depth <, 2). Details may be found elsewhere [17].",0,,False
3.3 Formula Retrieval,0,,False
"Applying detailed similarity metrics can be prohibitively expensive. For this reason, a three-layer retrieval process is used.",0,,False
"Layer 1: Initial Candidate Selection. Candidates are selected by matching query symbol tuples in the index. For each candidate, the harmonic mean of precision and recall of matched symbol tuples is used to assign an initial score [17].",0,,False
"Layer 2: Structural Match Scoring. e largest connected match between the query and candidates is obtained using a greedy algorithm, evaluating pairwise alignments between trees. Symbols (nodes) of similar type are uni ed, and query wildcards are matched to subtrees. Connected matches may contain holes (unmatched intermediate nodes) as long as edge labels between tree structures always match. For SLTs, matching works mostly as de ned for Tangent-3 [1], except that we now restrict uni cation to occur between single character identi ers, or within identi ers with two or more characters, but not between these two groups. is mitigates the issue of spurious uni cation matches between variables and functions leading to bad candidates being ranked too high.",1,ad,True
"For OPTs, the order of arguments for commutative operators is ignored to capture matches between equivalent expressions such as x + , 0 and 0 ,"" + x. However, testing all possible permutations of children at matching time has a factorial time complexity. We use a greedy pair-wise matching algorithm that considers all pair-wise alignments between children of matching commutative operators, and greedily chooses 1-to-1 matches between children maximizing the predicted number of matches a er uni cation, breaking ties by preferring alignments with more exact matches. While suboptimal, this greedy approach can still be computed in polynomial time and it allows us to match x + "", 0 and 0 , + x perfectly.",0,,False
"e output of matching is a subtree of the candidate formula that has been successfully aligned to the query. For re-ranking, we use the same three structural matching scores from Tangent-3 [17]: Maximum Subtree Similarity (MSS), negative count of candidate nodes matched with uni cation, and negative count of query nodes matched without uni cation. To be er support linear regression, we replace the negative counts by the equivalent uni ed precision and recall without uni cation, producing the same lexicographic ordering as before using values in the range [0, 1]. e recall and precision computed here di er from those used by other formula retrieval methods, in that they are computed from subtrees a er matching constraints are enforced.",0,,False
1166,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: NTCIR-12 optional MathIR Wikipedia Formula Browsing Task. Average Precision@K per topic,1,Wiki,True
Table 3: Average nDCG@K of ranks per topic for judged NTCIR-12 Wikipedia Formulae.,1,Wiki,True
Method MCAT [7] Tangent-3 [1] Core Matching SLT Core Matching Regression OPT Core Matching Regression Combined,0,,False
P@5 0.4900,0,,False
0.4150 0.4450,0,,False
0.4000 0.4550 0.3900,0,,False
0.3650 0.3550 0.3150 0.4400,0,,False
Relevant P@10 P@15 0.3900 0.3317,0,,False
0.3150 0.2650 0.2925 0.2517,0,,False
0.3025 0.3450 0.2900,0,,False
0.2567 0.2817 0.2450,0,,False
0.2550 0.2475 0.2475 0.3150,0,,False
0.2050 0.2017 0.2000 0.2583,0,,False
P@20 0.2825,0,,False
0.2200 0.2200,0,,False
0.2125 0.2462 0.2000,0,,False
0.1700 0.1787 0.1700 0.2162,0,,False
P@5 0.9100,0,,False
0.8100 0.8250,0,,False
0.7900 0.8350 0.6300,0,,False
0.6250 0.5550 0.6250 0.7000,0,,False
Partially Relevant P@10 P@15 0.8400 0.8067,0,,False
0.7450 0.7117 0.6825 0.6533,0,,False
0.7275 0.7725 0.5525,0,,False
0.6950 0.7400 0.5117,0,,False
0.4825 0.4400 0.4975 0.6075,0,,False
0.4200 0.3800 0.4317 0.5550,0,,False
P@20 0.7687,0,,False
0.6737 0.6100,0,,False
0.6562 0.6913 0.4675,0,,False
0.3662 0.3425 0.3850 0.5112,0,,False
Table 2: NTCIR-12 optional MathIR Wikipedia Formula Browsing Task. Average Bpref per topic.,1,Wiki,True
Matches Relevant Partially Relevant,0,,False
Core SLT OPT 0.4207 0.4227 0.5126 0.4241,0,,False
Matching SLT OPT 0.4786 0.4760 0.5351 0.4206,0,,False
Regression,0,,False
SLT OPT Comb. 0.5240 0.5127 0.5530 0.5569 0.5492 0.5620,0,,False
"Layer 3: Linear Regression. Using relevance judgments data from the NTCIR-12 Wikipedia Formula Retrieval task, we train a least squares linear regressor to combine the three scores from Layer-2 and produce a nal rank score. While more complex functions could have been used in this step, we choose a simple method to avoid over- ing the limited training data available, and to clearly observe which re-ranking scores best predict relevance.",1,Wiki,True
"Combined SLT/OPT Retrieval Approach. We combine results from SLTs and OPTs in a simple way. First, we perform symbol pair-based retrieval within a separate index for each representation.",0,,False
"e top-k candidates obtained from each index are merged into a single list. en, for each candidate we apply the detailed matching and scoring processes using both SLTs and OPTs representations, and we concatenate the similarity scores into a single vector. Finally, a linear regressor assigns a relevance score for the formula. In our experiments, we see that this simple combination obtains be er rankings than using scores from just SLTs or OPTs.",0,,False
4 EXPERIMENTS,0,,False
"To evaluate our approach, we use data from the NTCIR-12 MathIR competition [15]. Speci cally, we use data from the optional MathIR Wikipedia Formula Browsing Task which has a corpus of 319,689 articles from English Wikipedia with more than half a million formulae. e task has 40 topics for isolated formula retrieval: 20 are concrete (without wildcards) and 20 include wildcards. Each wildcard query is a derived from a concrete query, with portions of the concrete query replaced by wildcards.",1,Wiki,True
"At NTCIR-12, the top-20 results for each topic from 8 submissions were evaluated for relevance. Each result was assessed by two human evaluators who scored them from 0 (irrelevant) to 2 (relevant). ese scores were combined and each formula has a",0,,False
nal relevance score between 0 and 4. A total of 2687 relevance assessments were produced by this method.,0,,False
"We evaluated the ranks produced by the model for each representation (SLT, OPT) at each retrieval stage (Core, Matching, Regression).",0,,False
"e combined SLT/OPT approach (Section 3.3) is also considered, for a total of seven conditions. At the rst stage, we select the",0,,False
Condition SLT Core Matching Regression OPT Core Matching Regression Combined,0,,False
All Topics @5 @20,0,,False
0.7109 0.7534 0.7943,0,,False
0.7002 0.7218 0.7723,0,,False
0.6978 0.7459 0.7519 0.8136,0,,False
0.7184 0.7446 0.7331 0.7908,0,,False
Concrete Only @5 @20,0,,False
0.7991 0.8033 0.8031,0,,False
0.7727 0.7776 0.7958,0,,False
0.7889 0.7889 0.8008 0.8131,0,,False
0.7891 0.8018 0.7773 0.8088,0,,False
Wildcard Only @5 @20,0,,False
0.6236 0.7036 0.7855,0,,False
0.6277 0.6659 0.7488,0,,False
0.6066 0.7028 0.7031 0.8141,0,,False
0.6478 0.6874 0.6888 0.7728,0,,False
"top 1000 candidates for each query using w , All (all tuples) and EOB ,"" True. Given the limited number of relevance assessments available, for conditions using Linear Regression we grouped each concrete query with its corresponding wildcard version, and created 20 data folds. We then used cross-validation, repeatedly using one fold to test and the remaining 19 folds to train a linear regressor, until all queries have been processed.""",0,,False
"Table 1 compares our three-stage ranking systems, and systems participating in the NTCIR-12 competition. We used the TREC eval tool to compute the values of Precision@K with K in {5, 10, 15, 20}. Unlike the Tangent-3 submissions at NTCIR-12 [1], we ensured that the lexicographic order used for ranking a er structural matching (the Matching conditions) was preserved by the TREC eval tool, by computing the ranks produced by the xed-order MSS and tie breaker scores, and then using the reciprocal rank (1/r ) of each score vector as the nal score. Formulae with identical score vectors are re-ranked by the TREC eval tool based on document id.",1,TREC,True
"Mainly because of a large number of unrated formulas that are unfairly assumed to be irrelevant, Precision@K scores for the proposed conditions, specially the OPT-based, are lower than those for systems in the competition, with the exception of SLT Matching which are higher than Tangent-3 Matching. However, Tangent-S is simpler and faster than MCAT [1, 7], the best performing system in the competition.",0,,False
"e binary preference (Bpref) metric ignores unrated matches, and quanti es the ability of the ranking method to keep judged relevant matches ranked higher than irrelevant ones. Table 2 shows a comparison of Bpref values across di erent conditions of our own method. Unfortunately, Bpref values are not available for the original systems in the competition.",0,,False
"In terms of Bpref, we can see that in most cases the values increase at each layer of the retrieval model. For partially relevant results, with SLTs Bpref goes from 0.4207 in the initial set of candidates to 0.5240 a er using linear regression, and from 0.4227 to 0.5127 for OPTs. When SLT and OPT scores are combined, Bpref increases to 0.5530. is con rms that each step of the pipeline improves the quality of the produced ranks, and that combining representations helps.",0,,False
"As the number of expressions judged per topic is relatively small, and given that our new conditions produce many unrated results in the top-20, we have used a di erent approach to further analyze the rankings produced. We re-ranked all judged formulas for each topic using each stage of our retrieval model, and we compared these ranks against the ideal rankings using nDCG@K with K ,"" {5, 20} as shown in Table 3. Consistently, the Matching stage improves the""",0,,False
1167,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: Top-5 formula results for ery-12 for each ranking stage for each representation,0,,False
Core,0,,False
1. O (mn log m) 2. O (mn) 3. O (mnp ) 4. O (m + log n),0,,False
 5. O m n log n,0,,False
S,0,,False
L,0,,False
T,0,,False
Matching,0,,False
"O (mn log m) O (V E log V ) O(V ElogV log (V C )) O (T m) , O n2m log n O (nk log k )",0,,False
Regression,0,,False
"O (mn log m) O (mn log p ) , O (n log n)",0,,False
"O (mn) , O n3 log n",0,,False
O d 5n log3 B,0,,False
O,0,,False
mnr 2,0,,False
log,0,,False
1 ,0,,False
12: O (mn log m),0,,False
O,0,,False
T,0,,False
Core,0,,False
O (mn log m) O (mn) O (mnp )  (mnp ) O (mr ),0,,False
Matching,0,,False
O (mn log m) O (nk log (n)) O (V E log V ) O (n log n) O (nk log k ),0,,False
Regression,0,,False
"O (mn log m) O (mn log p ) , O (n log n)",0,,False
O,0,,False
mnr 2,0,,False
log,0,,False
1 ,0,,False
O (pn (m+n log n)),0,,False
"O (T m) , O n2m log n",0,,False
Combined,0,,False
Regression,0,,False
"O (mn log m) O (mn log p ) , O (n log n)",0,,False
"O M (m) log2 m , O (M (n) log n)",0,,False
O,0,,False
mnr 2,0,,False
log,0,,False
1 ,0,,False
"O (mn) , O n3 log n",0,,False
"ranking quality produced by the initial spectral symbol pair matching for both representations. While linear regression consistently increases nDCG for SLTs, it produces a small decrease for OPTs compared to the lexicographic order of the same scores from the Matching stage. is may be due to a greater collinearity between Recall of nodes and MSS in the OPT space. In general, nDCG values for Concrete topics are higher than for the Wildcard topics. In the current matching procedure, we have observed that allowing Wildcards to match subtrees in the presence of poor uni cations can cause bad partial matches to be ranked high.",1,ad,True
"An analysis of relevance against similarity scores reveals that while the means of MSS and node recall increase with the relevance of judged matches on both representations, node precision a er uni cation is not well correlated with relevance and might hurt linear regression predictions and even re-ranking in general. is is not surprising since some bad partial matches have low recall but high precision. For example, a single query wildcard can be matched to an entire arbitrary expression tree with perfect precision.",1,ad,True
"Table 4 illustrates the di erences in the Top-5 ranks for each stage of the model for the query O (mn log m), for both SLTs and OPTs. Di erences in structure for each representation change the initial set of candidates extracted from the collection. e Matching columns show how the uni cation process helps in increasing the rank of partial matches that become exact a er uni cation. is query illustrates how linear regression can sometimes produce less intuitive rankings than the simpler lexicographic match score ordering. It also shows how the OPT representation can give be er rankings to equivalent expressions that have a slightly di erent layout like the candidate O (nk log (n)) a er uni cation.",0,,False
"It is important to acknowledge noise in the NTCIR-12 formula data. Many expressions are incorrectly but consistently converted into Content MathML from LATEX. For example, the sub expression f (x ) is almost always converted to the tree corresponding to f × x. Such errors in the source can lead to many undesirable partial matches for OPTs at retrieval time.",1,ad,True
5 CONCLUSIONS,0,,False
"We have presented a comparison of the performance for two math expression representations using a three-layer retrieval model. We also presented a simple way to combine Symbol Layout Tree (SLT) and Operator Tree (OPT) representations into a single retrieval model. Overall, this combined model produced be er rankings than the individual representations. Our results suggest that additional restrictions are needed for uni cation, to prevent undesirable matches and resulting rankings. In this study, we simply used the similarity scores proposed in the original retrieval model that we",1,ad,True
"extended. However, the method proposed here may be used to in-",0,,False
corporate and study additional similarity scores that may be be er,1,corpora,True
"predictors for relevance (e.g., symbol and structure recall before",0,,False
uni cation). Similarity scores can also be combined with non-linear,0,,False
models for more accurate candidate selection and relevance predic-,0,,False
"tion, while maintaining fast retrieval.",0,,False
ACKNOWLEDGMENTS,0,,False
We thank Frank W. Tompa and the reviewers for their valuable feed-,0,,False
back. is material is based upon work supported by the National,0,,False
Science Foundation (USA) under Grant No. HCC-1218801.,0,,False
REFERENCES,0,,False
"[1] Kenny Davila, Richard Zanibbi, Andrew Kane, and Frank Wm Tompa. 2016. Tangent-3 at the NTCIR-12 MathIR task. In Proc. NTCIR-12. 338­345.",0,,False
"[2] Liangcai Gao, Ke Yuan, Yuehan Wang, Zhuoren Jiang, and Zhi Tang. 2016. e math retrieval system of ICST for NTCIR-12 MathIR task. Proc. NTCIR-12 (2016), 318­322.",0,,False
"[3] Ferruccio Guidi and Claudio Sacerdoti Coen. 2015. A survey on retrieval of mathematical knowledge. In Proc. CICM. Springer, 296­315.",0,,False
"[4] Radu Hambasan, Michael Kohlhase, and Corneliu-Claudiu Prodescu. 2014. MathWebSearch at NTCIR-11. In Proc. NTCIR-11. 114­119.",1,ad,True
[5] H. Hiroya and H. Saito. 2013. Partial-match Retrieval with Structure-re ected Indices at the NTCIR-10 Math Task. In Proc. NTCIR-10. 692­695.,0,,False
"[6] Shahab Kamali and Frank Wm Tompa. 2013. Structural similarity search for mathematics retrieval. In Intelligent Computer Mathematics, LNCS vol. 7961. Springer, 246­262.",0,,False
"[7] Giovanni Yoko Kristianto, G Topic´, and A Aizawa. 2016. e MCAT math retrieval system for NTCIR-12 MathIR task. In Proc. NTCIR-12. 323­330.",0,,False
"[8] Xiaoyan Lin, Liangcai Gao, Xuan Hu, Zhi Tang, Yingnan Xiao, and Xiaozhong Liu. 2014. A Mathematics Retrieval System for Formulae in Layout Presentations. In SIGIR. ACM, New York, NY, USA, 697­706.",0,,False
"[9] Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval vol. 3 (2009), 225­331.",0,,False
"[10] Bruce R Miller and Abdou Youssef. 2003. Technical aspects of the digital library of mathematical functions. Annals of Mathematics and Arti cial Intelligence vol. 38, 1-3 (2003), 121­136.",0,,False
"[11] Shunsuke Ohashi, Giovanni Yoko Kristianto, Goran Topic´, and Akiko Aizawa. 2016. E cient Algorithm for Math Formula Semantic Search. IEICE TRANSACTIONS on Information and Systems 99, 4 (2016), 979­988.",0,,False
"[12] M Ru^zicka, Petr Sojka, and M Liska. 2016. Math indexer and searcher under the hood: Fine-tuning query expansion and uni cation strategies. In Proc. NTCIR-12. 331­337.",0,,False
"[13] Abhinav anda, Ankit Agarwal, Kushal Singla, Aditya Prakash, and Abhishek Gupta. 2016. A document retrieval system for math queries. (2016), 346­353.",0,,False
"[14] Ke Yuan, Liangcai Gao, Yuehan Wang, Xiaohan Yi, and Zhi Tang. 2016. A mathematical information retrieval system based on RankBoost. In Proc. JCDL. IEEE, 259­260.",0,,False
"[15] Richard Zanibbi, Akiko Aizawa, Michael Kohlhase, Iadh Ounis, Goran Topic´, and Kenny Davila. 2016. NTCIR-12 MathIR Task Overview. In Proc. NTCIR-12. 299­308.",1,ad,True
"[16] Richard Zanibbi and Dorothea Blostein. 2012. Recognition and retrieval of mathematical expressions. IJDAR 15, 4 (2012), 331­357.",0,,False
"[17] Richard Zanibbi, Kenny Davila, Andrew Kane, and Frank Tompa. 2016. MultiStage Math Formula Search: Using Appearance-Based Similarity Metrics at Scale. SIGIR (2016), 145­154.",0,,False
"[18] Wei Zhong and Hui Fang. 2016. OPMES: A Similarity Search Engine for Mathematical Content. In ECIR. Springer, 849­852.",0,,False
1168,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Venue Appropriateness Prediction for Personalized Context-Aware Venue Suggestion,0,,False
Mohammad Aliannejadi and Fabio Crestani,1,ad,True
"Faculty of Informatics, Universita` della Svizzera italiana (USI) Lugano, Switzerland",0,,False
"{mohammad.alian.nejadi,fabio.crestani}@usi.ch",1,ad,True
ABSTRACT,0,,False
"Personalized context-aware venue suggestion plays a critical role in satisfying the users' needs on location-based social networks (LBSNs). In this paper, we present a set of novel scores to measure the similarity between a user and a candidate venue in a new city.",0,,False
"e scores are based on user's history of preferences in other cities as well as user's context. We address the data sparsity problem in venue recommendation with the aid of a proposed approach to predict contextually appropriate places. Furthermore, we show how to incorporate di erent scores to improve the performance of recommendation. e experimental results of our participation in the TREC 2016 Contextual Suggestion track show that our approach beats state-of-the-art strategies.",1,ad,True
1 INTRODUCTION,1,DUC,True
"With the availability of location-based social networks (LBSNs), such as Yelp, TripAdvisor, and Foursquare, users can share check-in data using their mobile devices. LBSNs collect valuable information about users' mobility records with check-in data including user context and feedback, such as ratings and reviews. Being able to suggest personalized venues to a user, taking into account the user's context, plays a key role in satisfying the user needs on LBSNs, for example when exploring a new venue or visiting a city [6].",0,,False
"ere are a number of di erent LBSNs that are widely used. However, a single LBSN does not have a comprehensive coverage over all venues and all types of information. Moreover, combining user's current context with multimodal information, e.g., ratings and reviews of previously visited venues from di erent LBSNs, improves the accuracy of venue suggestion [4].",0,,False
"A major challenge for venue suggestion is how to model the user pro le, that should be built based on the user feedback on previously visited places. Relevant studies propose to model user pro les based on the venues content [9]. Other studies leverage the opinions of users about a place based on online reviews [6].",0,,False
"Another challenge in venue suggestion is how to leverage the contextual information about users to improve suggestion. To this end, the main focus of the TREC Contextual Suggestion track in 2015 and 2016 [11] was to improve the venue suggestion with the",1,TREC,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080754",1,ad,True
"aid of contextual suggestion. However, not many successful participants took into account context. is paper describes our a empt to utilize contextual information to enhance the performance of venue suggestion.",0,,False
"In the e ort to face these challenges, our main contribution in this paper can be summarized as follows: (CO1) We propose a novel method to predict contextually appropriate venues given the user's current context. (CO2) We create a dataset to train our model for contextual appropriateness prediction1. (CO3) We present a set of scores to measure the similarity between a candidate venue and a user pro le based on venues contents and reviews. (CO4) We investigate two di erent ways of combining all the proposed context-, content-based similarity scores.",0,,False
"e o cial results of the TREC 2016 Contextual Suggestion track, as well as the experiment we have done on the dataset, show that our proposed approach outperforms state-of-the-art strategies.",1,TREC,True
2 RELATED WORK,0,,False
"Much work has been done to show that user data from LBSNs can signi cantly improve the e ectiveness of a context-aware recommender system. Several rating-based collaborative ltering approaches have been proposed in the literature, which are based on nding common features among users' preferences and recommending venues to people with similar interests. ese models are usually based on matrix factorization, exploiting check-in data for recommending places, such as the studies reported in [7, 10]. Factorization Machines generalize matrix factorization techniques to leverage not only user feedback but also other types of information, such as contextual information in LBSNs [13]. Also, some studies follow a review-based strategy, building enhanced user pro les based on their reviews [1]. When a user writes a review about a venue, there is a wealth of information which reveals the reasons why that particular user is interested in a venue or not. For example, Chen et al. [6] argued that reviews are helpful to deal with the sparsity problem in LBSNs. Our work consists of di erent similarity scores each of which is aimed at capturing a di erent aspect of information. More speci cally, our work combines information from venues content and online reviews.",0,,False
"Another line of research tries to incorporate the contextual data to enhance the performance of a recommender system. Levi et al. [14] developed a weighted context-aware recommendation algorithm to address the cold start problem for hotel recommendation. More in details, they de ned context groups based on hotel reviews and followed a user's preferences in trip intent and hotel aspects as well as the user's similarity with other users (e.g., nationality)",1,corpora,True
"1 e contextual appropriateness dataset, as well as the additional, crawled data are available at h p://inf.usi.ch/phd/aliannejadi/data.html",1,ad,True
1177,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"to recommend hotels. Other works focused more on time as context [9, 18]. Yuan et al. [18] proposed a time-aware collaborative",0,,False
"ltering approach. More speci cally, their proposed approach recommended venues to users at a speci c time of the day by mining historical check-ins of users in LBSNs. Deveaud et al. [9] modeled venues popularity in the immediate future utilizing time series.",0,,False
"ey leveraged the model to make time-aware venue recommendation to users. e limitation in such line of works is that they only consider time and location as the context, whereas in our work we also take into account other aspects of context. Our work distinguishes itself from these approaches by considering not only time and location but also other contextual dimensions.",0,,False
"e TREC Contextual Suggestion track [11] aimed to encourage the research on context-aware venue recommendation. In fact, the task was to produce a ranked list of venue suggestions for each user in a new city, given the user's context and history of preferences in 1-2 other cities. e contextual dimensions were the duration of the trip, the season in which the trip takes place, the type of the trip (i.e., business, holiday, and other), and the type of group with whom the user is traveling (i.e., alone, friends, etc.). ese contextual dimensions were introduced in the track in 2015. Since then, among the top runs, few approaches were trying to leverage such information. Yang and Fang [17] introduced some handcra ed rules for ltering venues based on their appropriateness to a user's current context. As they showed in their paper, applying such",1,TREC,True
"lters degrades the performance of the system. Hence, we conclude that contextual appropriateness is not a simple case of using some deterministic rules to lter venues.",1,ad,True
"Manotumruksa et al. [15] proposed a set of categorical, temporal, and term-based features to train a set of classi ers to predict contextual appropriateness of venues. We believe that such features are very speci c to their dataset and problem and not all of them can be generalized to similar problems. Moreover, similar to our work, they collected the classi cation dataset using crowdsourcing, however, since they asked the workers to assess the appropriateness of a speci c venue to a particular user context, this could result in biased assessments. In contrast, we make sure that the assessments are not biased and our crowdsourced collection is general enough to be used in other similar problems.",0,,False
3 CONTEXTUAL APPROPRIATENESS,1,AP,True
PREDICTION,0,,False
"In this section, we rst de ne the problem of predicting the contextual appropriateness of venues. We then present the set of features on which we train a classi er to predict the appropriate places. Finally, we describe the collection we used to train the classi er.",0,,False
3.1 Problem De nition,0,,False
"Given a set of venues V ,"" { 1, . . . , n } and contextual information Cx "","" {cx1, . . . , cxm }, the task is to predict if a venue i  V is appropriate to be visited by a user with context Cx . Contextual information expresses user's requirements or preferences and is limited to 3 of those introduced in TREC 2015 Contextual Suggestion track: Trip type (business, holiday, other), Trip duration (night out, day trip, weekend trip, longer) and Group type (alone, friends, family, other). Group type expresses the type of group user prefers""",1,TREC,True
"to go out with, while trip duration indicates how long the trip will last. We formulate the problem as a binary classi cation problem. Given a venue i and the set of its corresponding categories C i ,"" {c1, . . . , ck }, we consider each category cj  C i and the aforementioned contextual dimensions as features for classi cation.""",0,,False
e classi er predicts whether the venue category is appropriate to be visited or not.,0,,False
3.2 Contextual Features,0,,False
"In this section, we discuss the contextual features we used to train a classi er to predict contextual appropriateness of venues. As features, we consider the appropriateness of each contextual dimension to a venue category. For example, assume we have a venue with category nightlife-spot, and we want to see if it is appropriate for an example context: trip type: holiday, group type: family, trip duration: weekend. We take the degree of appropriateness of the venue category with each of the contextual dimensions as features. Let Fapp (cat, cxt ) be a function returning the degree of appropriateness of a venue category, cat, to a contextual dimension, cxt, ranging from -1 to +1. Fapp (cat, cxt ) ,"" -1 indicates that a venue with category cat is absolutely inappropriate to be visited by a user with context cxt, whereas Fapp (cat, cxt ) "","" +1 indicates it is absolutely appropriate. erefore, the features in this example would be: Fapp (nightlife-spot, holiday-trip), Fapp (nightlife-spot, with-family), and Fapp (nightlife-spot, weekend-trip).""",0,,False
"Determining such features may seem intuitive, however, we argue that in many cases determining the output of function Fapp (cat, cxt ) can be very challenging. For instance, in the previous example, the features can be de ned intuitively. On the other hand, de ning such features is not as intuitive as other ones: Fapp (o ce, with -friends), Fapp (food-and-drink-shop, business-trip), or Fapp (stadium, night-out-trip). Based on this observation, we classify the features into two classes: objective and subjective. As the terms suggest, objective features are those that are easily determined and are more objective. erefore, they can in uence a user's decision of visiting a venue or not. As in the previous example, supposedly, going to a nightlife spot with a family is not appropriate; hence, the user would not go to a nightlife spot even though he/she might like nightlife spots in general. erefore, objective features have a direct impact on users' decisions. Subjective features, in contrast, are less discriminative for they depend on each user's opinion and personal preferences.",1,ad,True
"In the e ort to classify the features into objective and subjective and determine the degree of their objectivity/subjectivity, we designed a crowdsourcing task. In the task, we asked the workers to assess the features. More in detail, we showed them a venue category and a context dimension (e.g., cat , nightlife spot and cxt ,"" Group type: Family) and asked them to assess whether the pair is appropriate or not. We assigned at least ve assessors for each category-context pair. e outcome of the task was very interesting since we observed that the workers agreed on one answer when the task was objective, whereas they could not agree on subjective tasks. In this context, those pairs with high agreement rate between the assessors could be considered as objective, while those lacking assessors agreement could be seen as subjective. Table 1 lists some subjective and objective features in our dataset.""",0,,False
1178,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Example contextual features,0,,False
Category,0,,False
Beach Zoo Museum Pet Store Medical Center,0,,False
Context,0,,False
Trip type: Holiday Trip type: Business Trip type: Business Trip duration: Weekend Trip type: Other,0,,False
"Fapp (cat, cxt )",0,,False
+1 -1 -0.66 -0.18 0.0,0,,False
"As we can see in Table 1, the lower rows are more subjective pairs, in fact, we assign the 0.0 score to the last row. As we discussed earlier, we cannot determine if a venue is appropriate based on such subjective features and therefore we treat them as neutral. We computed the contextual features for all pairs of 11 contextual dimensions and the 177 most frequent categories of Foursquare category tree2. Overall we generated 1947 contextual features, leading to 11487 judgments. In fact, this dataset is general enough to be applied to other venue recommendation collections since it considers only place categories, including the most frequent category-context pairs. More details can be found in [3].",1,ad,True
3.3 Training the Classi er,0,,False
"As described in Section 3.1, we formulate the problem of contextual appropriateness of venues as a binary classi cation. We described the features that we use in the classi er in Section 3.2. In this section, we describe how we created the training dataset to train the classi er using the features we created. As training set, we randomly picked 10% of the data from TREC 2016 dataset. To annotate the data, we created another crowdsourcing task in which we asked workers to assess if a category (e.g. Bar) is appropriate to be visited by a user with a speci c context (e.g. Holiday, Friends, Weekend). We assigned at least three workers to assess each row in the dataset. Each row was considered appropriate only if at least two of the three assessors agreed on it [3]. erefore, we trained the contextual appropriateness classi er on 10% of the data from TREC 2016 to predict the rest. As classi er, we trained some widely used classi er, but since we got the best results using Support Vector Machines (SVM) [8], we only report the results of this classi er in this work due to space limitations.",1,TREC,True
4 CONTEXT-AWARE VENUE SUGGESTION,0,,False
"In this section, we describe our approach of combining contextbased and user-based similarity scores to produce a ranked list of venues which ts users' preference and context.",0,,False
"Context-Base Score. As context-based score (denoted as ScFxt ), we consider the value of SVM decision function described in Section 3. If a venue has more than one category, we will run the classi cation against each category. en, we consider the minimum value of the decision functions as the context-based similarity score because we observed whenever there are some venue categories from which one is not appropriate to the user's context, it acts as a barrier and leads to the inappropriateness of the venue.",1,ad,True
2h ps://developer.foursquare.com/categorytree,0,,False
"Frequency-Based Scores. e other set of scores is based on the frequencies of venue categories and taste tags. We rst explain how to calculate the score for venue categories. e score for tags is calculated analogously. Given a user u and a her history of rated venues, hu ,"" { 1, . . . , n }, to each venue is assigned a list of categories C ( i ) "","" {c1, . . . , ck }. We de ne the category index of a user as follows:""",0,,False
"De nition 4.1. A Category Index consists of categories of venues a user has visited and their normalized frequency in a particular user's pro le. e category f requency (cf) is divided into two sub-frequencies: positive category frequency, denoted as cf+, and negative category frequency, denoted as cf-, representing the normalized frequency of a speci c category positively rated and negatively rate by the user, respectively.",0,,False
"Given a user u and a candidate venue , the category-based",0,,False
"similarity score Scat (u, ) between them is calculated as follows:",0,,False
"Scat (u, ) ,",0,,False
cfu+ (ci ) - cfu- (ci ) .,0,,False
(1),0,,False
ci C ( ),0,,False
"We calculated the category similarity score from two sources of information, namely, Foursquare (ScFat ) and Yelp (ScYat ).",0,,False
"Venue Tags Score. As another frequency-based score, we in-",0,,False
dex the venue taste tags from Foursquare following De nition 4.1.,0,,False
Venue taste tags are the most salient words extracted from the users',0,,False
reviews. We leveraged them to have a crisper description of the,0,,False
places and improve our suggestions. e tag similarity score is then,0,,False
calculated similarly to Equation 1.,0,,False
Review-Based Score. A further score uses the reviews to un-,0,,False
derstand the motivation of the user behind a positive or negative,0,,False
"rate. Indeed, modeling a user solely on venue's content is very",0,,False
general and does not allow to understand the reasons why the user,0,,False
enjoyed or disliked a venue. Our intuition is that a user's opinion,0,,False
regarding an a raction could be learned based on the opinions,0,,False
of other users who gave the same or similar rating to the same,0,,False
a raction [1]. We created two TF-IDF indexes of reviews per user,0,,False
of venues a user has visited per user: 1) positive review index con-,0,,False
taining only positively rated reviews of venues that a particular,0,,False
"user likes, 2) negative review index containing only negatively rated",0,,False
reviews of places that a particular user does not like. For each,0,,False
"user, we trained a binary classi er considering the positive and",0,,False
"negative review indexes as positive and negative training examples, respectively3. As classi er, we used SVM with linear kernel and",0,,False
consider the value of the SVM's decision function as the score since,0,,False
it gives us an idea on how close and relevant a venue is to a user pro le4. We used the reviews from Yelp for this score and refer to it as SrYe .,0,,False
Venue Ranking. We investigated two possible methods of com-,0,,False
bining these similarity scores: linear interpolation and learning to,0,,False
rank. Linear interpolation is an e ective yet simple way of combin-,0,,False
ing multiple scores into one. We linearly combined all the similarity,0,,False
scores into one [2]. We also adopted learning to rank techniques,1,ad,True
as they have proved to be e ective in similar problems [4]. We,0,,False
"3An alternative to binary classi cation would be a regression model, but we believe it is inappropriate since when users read online reviews, they make their minds by taking a binary decision (like/dislike). 4Note that we used other well known classi er but do not report the results due to space limitation.",1,ad,True
1179,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 2: Performance evaluation on TREC 2016 Contextual Suggestion track. Bold values denote the best scores.,1,TREC,True
P@5 nDCG@5 MAP MRR P@10,1,MAP,True
BASE1 BASE2 CS-Linear CS-L2Rank,0,,False
0.4724 0.5069 0.5069 0.5379,0,,False
0.3306 0.3281 0.3265 0.3603,0,,False
0.4497 0.4536 0.4590 0.4682,0,,False
0.6801 0.6501 0.6796 0.7054,0,,False
0.4552 0.4500 0.4603 0.4828,0,,False
"adopted ListNet [5] to rank the venues using non-contextual scores as features of ListNet. e contextual score was then used to rerank the initial ranked list to produce the nal list of venues. To nd the optimum se ing for the parameters of our models, we conducted a 5-fold cross validation, training the model using four folds and tuning the parameters using one fold. We denote our linear ranking model as CS-Linear and the one based on learning to rank as CS-L2Rank.",1,ad,True
5 EXPERIMENTAL RESULTS,0,,False
In this section we report the experimental results to demonstrate the e ectiveness of our approach.,0,,False
Dataset. We report the result of our participation [2] in the TREC 2016 Contextual Suggestion track [11] as well as additional experiments carried out on our crawled dataset5 [3] and ground truth labels as released by the coordinators.,1,TREC,True
"Evaluation protocol. We follow the same evaluation metrics as in TREC 2016 to report the results, namely, P@5, nDCG@5, MAP, MRR, and P@10.",1,TREC,True
"Compared methods. We compare our approach with top performing systems in TREC 2016. In particular, BASE1 adopts a modi-",1,TREC,True
"ed Rocchio classi er to rank the venues given a query which is created by Rocchio relevance feedback method from places' descriptions and metadata [12]. BASE2, on the other hand, considers both the global trend and personal preference to recommend venues.",1,ad,True
"e former is a regressor trained using the most visited category in the 2015 TREC dataset, while the la er adopts word embedding to capture individual user preferences [16].",1,TREC,True
"Results. Table 2 demonstrates the performance of our models against competitors for the TREC 2016. Table 2 shows that CS-L2Rank outperforms the competitors w.r.t. the ve evaluation metrics. is indicates that the proposed approach for joint personal-contextual venue suggestion improves the performance of venue suggestion. is happens because our model predicts the contextual appropriateness of venues e ectively. At the same time, it improves the ranking technique by capturing user preferences more accurately, thus addressing the data sparsity problem for venue suggestion. CS-Linear, however, beats the baselines w.r.t. MAP and P@10. It exhibits a comparable performance in terms of other evaluation metrics. It also con rms that the proposed similarity scores are able to capture contextual appropriateness and user interest. However, it indicates that combining multimodal information is a complex problem and thus more sophisticated techniques, such as learning to rank, perform be er.",1,TREC,True
5Available at h p://inf.usi.ch/phd/aliannejadi/data.html,1,ad,True
6 CONCLUSION,0,,False
"In this study, we presented an approach to predicting contextually appropriate venues as well as other similarity scores to model the personal preferences of users for venue suggestion. For contextual appropriateness prediction, we proposed a set of novel relevance features with which we trained a classi er. e features as well as the training data was created using crowdsourcing and is freely available on request. We studied two directions to combine the scores: linear interpolation and learning to rank. e proposed CS-L2Rank model exhibited the best performance beating stateof-the-art approaches in terms of all ve evaluation metrics. is con rms that the proposed approach, CS-L2Rank, solves the data sparsity problem and captures user context and preferences more accurately. As future work, we plan to extend our model to capture the time dimension and perform time-aware venue suggestion.",0,,False
ACKNOWLEDGMENTS,0,,False
is research was partially funded by the RelMobIR project of the Swiss National Science Foundation (SNSF).,0,,False
REFERENCES,0,,False
"[1] Mohammad Aliannejadi, Ida Mele, and Fabio Crestani. 2016. User Model Enrichment for Venue Recommendation. In AIRS. Springer, 212­223.",1,ad,True
"[2] Mohammad Aliannejadi, Ida Mele, and Fabio Crestani. 2016. Venue Appropriateness Prediction for Contextual Suggestion. In TREC. NIST.",1,ad,True
"[3] Mohammad Aliannejadi, Ida Mele, and Fabio Crestani. 2017. A Cross-Platform Collection for Contextual Suggestion. In SIGIR 2017. ACM.",1,ad,True
"[4] Mohammad Aliannejadi, Dimitrios Rafailidis, and Fabio Crestani. 2017. Personalized Keyword Boosting for Venue Suggestion Based on Multiple LBSNs. In ECIR. Springer, 291­303.",1,ad,True
"[5] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to Rank: From Pairwise Approach to Listwise Approach. In ICML. ACM, 129­136.",0,,False
"[6] Li Chen, Guanliang Chen, and Feng Wang. 2015. Recommender systems based on user reviews: the state of the art. User Modeling and User-Adapted Interaction 25, 2 (2015), 99­154.",0,,False
"[7] Chen Cheng, Haiqin Yang, Irwin King, and Michael R. Lyu. 2012. Fused Matrix Factorization with Geographical and Social In uence in Location-Based Social Networks. In AAAI. AAAI Press, 17­23.",0,,False
"[8] Corinna Cortes and Vladimir Vapnik. 1995. Support-Vector Networks. Machine Learning 20, 3 (1995), 273­297.",1,ad,True
"[9] Romain Deveaud, M-Dyaa Albakour, Craig Macdonald, and Iadh Ounis. 2015. Experiments with a Venue-Centric Model for Personalised and Time-Aware Venue Suggestion. In CIKM. ACM, 53­62.",1,ad,True
"[10] Jean-Beno^it Griesner, Talel Abdessalem, and Hubert Naacke. 2015. POI Recommendation: Towards Fused Matrix Factorization with Geographical and Temporal In uences. In RecSys. ACM, 301­304.",0,,False
"[11] Seyyed Hadi Hashemi, Charles L. A. Clarke, Jaap Kamps, Julia Kiseleva, and Ellen M. Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track. In TREC. NIST.",1,ad,True
"[12] Georgios Kalamatianos and Avi Arampatzis. 2016. Recommending Points-ofInterest via Weighted kNN, Rated Rocchio, and Borda Count Fusion. In TREC. NIST.",1,TREC,True
"[13] Yehuda Koren, Robert M. Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. IEEE Computer 42, 8 (2009), 30­37.",0,,False
"[14] Asher Levi, Osnat Mokryn, Christophe Diot, and Nina Ta . 2012. Finding a needle in a haystack of reviews: cold start context-based hotel recommender system. In RecSys. ACM, 115­122.",0,,False
"[15] Jarana Manotumruksa, Craig MacDonald, and Iadh Ounis. 2016. Predicting Contextually Appropriate Venues in Location-Based Social Networks. In CLEF. Springer, 96­109.",1,ad,True
"[16] Jian Mo, Luc Lamontagne, and Richard Khoury. 2016. Word embeddings and Global Preference for Contextual Suggestion. In TREC. NIST.",1,TREC,True
[17] Peilin Yang and Hui Fang. 2015. University of Delaware at TREC 2015: Combining Opinion Pro le Modeling with Complex Context Filtering for Contextual Suggestion. In TREC. NIST.,1,TREC,True
"[18] an Yuan, Gao Cong, Zongyang Ma, Aixin Sun, and Nadia Magnenat- almann. 2013. Time-aware point-of-interest recommendation. In SIGIR. ACM, 363­372.",1,ad,True
1180,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Building Bridges across Social Platforms: Answering Twi er estions with Yahoo! Answers,1,Yahoo,True
Mossaab Bagdouri,0,,False
Department of Computer Science University of Maryland,0,,False
"College Park, MD 20742, USA mossaab@umd.edu",0,,False
ABSTRACT,0,,False
"is paper investigates techniques for answering microblog questions by searching in a large community question answering website. Some question transformations are considered, some proprieties of the answering platform are examined, how to select among the various available con gurations in a learning-to-rank framework is studied.",1,blog,True
CCS CONCEPTS,0,,False
·Information systems  estion answering; Test collections;,0,,False
KEYWORDS,0,,False
Microblogs; CQA; Cross-platform question answering,1,blog,True
1 INTRODUCTION,1,DUC,True
"Over 81% of the questions asked on the microblogging service Twitter that are not addressed to a speci c user receive no response [7]. For questions that express a true information need, any useful answer might be highly appreciated. Unanswered questions can be handled by suggesting answers to similar prior questions [9] or by routing the new question to some relevant expert who might be willing to provide an answer [5]. is approach has been extensively investigated using questions previously posted to the same platform where the new question has been posted. Well known techniques leverage features that can be extracted from old questions and answers, as well as the social graph between the users, the questions and the answers.",1,blog,True
"Sometimes, however, it might be be er to look elsewhere for the answer. Community estion Answering (CQA) websites such as ora and Yahoo! Answers have became very popular in the last decade, gathering hundreds of millions of questions with their answers. is makes them a suitable place to nd answers for questions that have been posed elsewhere. In this paper we use a large crawl of Yahoo! Answers to search for threads that are potentially useful for a tweet question (Section 3), we compare the importance of di erent elds in which we can search (Section 2),",1,Yahoo,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080755",1,ad,True
Douglas W. Oard,0,,False
"iSchool and UMIACS University of Maryland College Park, MD 20742, USA",0,,False
oard@umd.edu,0,,False
"and we study some approaches for adapting the language of Twi er questions to that of Yahoo! Answers (Section 2.1). We present our results in Section 4 before concluding with an overview of future directions that can bene t from our release of the annotations (Section 5). To the best of our knowledge, this is the rst work to examine the usefulness of a CQA service for answering questions posted on a microblogging service.",1,ad,True
2 METHODS,0,,False
"In our search task, we want to retrieve a ""thread"" (i.e., an old question with its answers) from Yahoo! Answers that would be useful for answering the question newly posted to Twi er. A thread has several elds in which we can search. A reasonable baseline is the concatenation of the title and body of the question, together with all of its answers. is approximates a simple search for a web page in a search engine. Alternatively, we can index each eld separately.",1,ad,True
"is allows us to study the importance of each eld independently from the others, and to examine di erent combination possibilities. We implement this alternative using BM25 [8].",0,,False
"ere are two possibilities for indexing the elds of a thread. In the rst, we index each eld of the question, and the concatenation of all of its answers. We call this indexing setup estion-perDocument (QpD). In the second, the indexed document contains the two question elds and a single answer. at is, we index as many documents for a given thread as there are answers. We call this indexing setup Answer-per-Document (ApD). We refer to the indexed elds as question title (T), question body (B), title and body concatenation (C), and answer(s) (A).",1,ad,True
"We experiment with various combinations of these four indexed elds. e weight of each eld is by default set to 1, but we also perform a two dimensional grid search on the weights of the B and A elds in the QpD-TBA con guration (i.e., estion-Per-Document, indexing the Title and Body separately along with a single Answer). For each con guration, we score the the top-1 thread, breaking ties (which is o en needed when searching only the T eld) by selecting the most recent thread.",1,TB,True
2.1 estion Rewriting,0,,False
"Tweets have characteristics that are less common in some other platforms, some of which we address in this section.",1,Tweet,True
"2.1.1 Hashtag Segmentation. Twi er users o en use hashtags to highlight some notion. Since Twi er hashtags don't contain spaces, it is common to concatenate the terms of a multi-words expression. Sometimes a CamelCase convention is used, as in ""i wonder if # eBible is or will be on Net ix?"" In other instances,",0,,False
1181,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"no capitalization cues are present, as in ""Is she Ilona or Elona? #thearchers."" We expect hashtag segmentation to improve retrieval e ectiveness. Our segmentation approach has three stages. In the",0,,False
"rst, we remove the # symbol, and use Google's cloud natural language API1 to see if the resulting term is classi ed as an entity (although we ignore entities of type OTHER). is stage aims to avoid segmenting single-word proper names such as ""Washington."" In the second stage, we generate one or more candidate segmentations. If a hashtag follows the CamelCase convention (detected with a regular expression), we segment at capital le ers. Otherwise, we use the vocabulary of our Twi er index (Section 3.3) to extract all possible segmentations (deleting any candidates containing 4 or more words).2 Since some segmentations may be unreasonable (e.g., segmenting #iPhone into ""i phone""), in the third stage, we remove segmentations that appear (in order) less frequently in our Twi er index than the hashtag (without the #). If no segmentation passes this lter, we maintain the hashtag (without the #). Otherwise, we replace it with the segmentation that has the highest frequency (breaking ties arbitrarily).",1,AP,True
"2.1.2 Spelling Correction. Twi er is mostly accessed from mobile devices3 on which small keyboards increase the chance of misspellings. Consider for example ""Why did the great awaking happen?"" We have li le hope for nding an answer unless the spelling of awaking is corrected to awakening. is problem is particularly critical when the misspelled word is a key term in the question. Another impact appears when a high frequency word (e.g., a stop word) is misspelled, typically resulting in a rare word with high IDF. For example, because we lowercase everything before performing a search, ""should igo to school tomorrow?"" leads to the undesirable retrieval of threads about Inter-Governmental Organizations (IGO).",1,ad,True
"We perform spelling correction in three steps. As with hashtag segmentation, we rst exclude terms that are classi ed as entities (of a type other than OTHER). We then generate a list of (up to) the 1,000 closest words by Levenstein distance, using a model trained on character n-grams from our Twi er index.4 Finally, we keep only alternatives for which both their document frequency and the document frequencies of the terms to their le and right (up to the rst stopword) are greater than those of the original word. If any alternatives pass this lter, we return the alternative with the highest document frequency as the possible correction. To limit the e ect of correction mistakes, we treat the possible correction as a synonym of the original word, computing the BM25 score for each",0,,False
eld a er summing the term frequencies of the original term and its possible correction and approximating the combined document frequency with the maximum of the two document frequencies (which is the document frequency of the possible correction).,0,,False
"2.1.3 Synonyms. e informal language of tweets encourages the adoption of some writing conventions that are less frequent in other platforms. For example, you and conversations would be synonyms to u and convos in ""Should u read your kids convos on the Internet?"" We nd synonyms in three stages. e rst and",1,ad,True
"1h ps://cloud.google.com/natural-language 2We use the WordBreakSpellChecker.suggestWordBreaks() method of Lucene 6.3. 3h p://venturebeat.com/?p,2014007 4We use the SpellChecker.suggestSimilar() method of Lucene 6.3.",0,,False
Is duplicate? Softmax Cosine,0,,False
Question 1,0,,False
Pooling BLSTM Embedding,0,,False
Pooling BLSTM Embedding,0,,False
Question 2,0,,False
Figure 1: An architecture for detecting duplicate questions.,0,,False
"the third stages are identical to what we do for spelling correction. For the second stage (suggesting a candidate synonym), we use a word2vec [6] model trained on our Twi er corpus to suggest the nearest word to the original one, but only if the cosine similarity of their vectors exceeds an arbitrary threshold of 0.5.",0,,False
2.2 Term Statistics,0,,False
"e importance of a term is indicated, in the BM25 scoring function, by its IDF. As a result, the same term might have di erent IDF values in di erent corpora. For the question ""What am I gunna do with this dog for the night?"" we observe that night has a high IDF in Yahoo! Answers compared to dog e opposite (and we think more desirable) relative IDF rank is true for Twi er, however. Some words seem to su er from a ""cost of fame"" in which they are so important that many questions are asked about them in Yahoo! Answers, (where there is an entire subcategory for dogs), thus diminishing their IDF. To mitigate this e ect, we can use the IDF statistics from our Twi er index.",1,corpora,True
2.3 estion/ estion Similarity,0,,False
"Similar questions might be phrased in di erent ways, so we need some way of measuring the extent to which a Twi er question is similar to a question in Yahoo! Answers. ora has recently released a corpus of 404,351 pairs of questions, among which 149,306 are indicated to be duplicates.5 We use 90% of those pairs to train the neural network depicted in Figure 1, and the remaining validation subset to stop training when the accuracy does not improve over the best prior results in the previous 10 epochs. We return the model that has the best accuracy (85.5%) on that validation set, a er optimizing it with ADAM [4], using mean squared error as a loss function as implemented in Keras.6",1,Yahoo,True
2.4 Selecting Con gurations and Answers,0,,False
"e approaches we have introduced so far aim to nd con gurations that work the best on average. However, it is possible to use the features of the questions and the answers to select the thread to be retrieved given the votes of di erent con gurations, and our prior knowledge of their average performance. Here we present a three-stage process to select the best thread among those returned by several con gurations.",1,ad,True
5h p://qim.ec.quoracdn.net/quora duplicate questions.tsv 6h p://keras.io,0,,False
1182,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2.4.1 Ordering the Configurations. Let N be the number of avail-,0,,False
"able con gurations, C1, ...Cn N be a subset of con gurations, and",0,,False
"T1, ...Tm be the number of training questions. Every pair (Ci ,Tj ) cor-",0,,False
"responds to a retrieved thread with a score Si, j . e maximum av-",1,ad,True
"erage score, over the training questions, that can be achieved given",0,,False
this,0,,False
combination,0,,False
(with,0,,False
an,0,,False
oracle),0,,False
would,0,,False
be:,0,,False
S^,0,,False
",",0,,False
1 m,0,,False
"Ti maxCj Si, j .",0,,False
Our goal is to nd the subset of con gurations that maximizes this,0,,False
value given n. With as many as 74 con gurations in our experi-,0,,False
"ments, a greedy search is considerably more e cient than exhaus-",0,,False
tively trying every possible combination. We start by nding the,0,,False
best single con guration. We then repeatedly use the best com-,0,,False
"bination we obtained at iteration n - 1, which gave us potential",0,,False
"average score S^n-1, and iterate over the remaining con gurations to maximize S^n . is process yields an ordered list of con gurations",0,,False
"that can be added, one at a time, to form several combinations.",1,ad,True
"2.4.2 Learning to Rank Threads. With some combination of congurations that, collectively, retrieves a set of threads, we want to learn to rank those threads. For every pair of a question and a retrieved thread, we extract the following vector of features:",1,ad,True
"· e BM25 scores of the title, the body, and the answer(s). · e neural similarity scores between the question, and",0,,False
"each of the three elds above. · e number of answers in the thread (log scaled). · e min, max, mean and standard deviation of the scores",1,ad,True
"of each answer, both for BM25 and neural similarity. · e number of threads with the same BM25 score as the",1,ad,True
"candidate (log scaled) · All the same features, using the rewri en question (with",0,,False
the three rewriting operations). · Binary indicators of whether each con guration returned,0,,False
that thread.,1,ad,True
"Given a training question with several threads, we integerize the ground truth score for each thread (Section 3) using 0.5, 1.5 and 2.5 as cuto points to produce scores of 0 (bad), 1( fair), 2 (good) or 3 (excellent). Finally, we train a learning-to-rank (L2R) model based on those threads using the SVMr ank so ware [3].",1,ad,True
"2.4.3 Selecting the Best Combination of Configurations. Given an ordered list of con gurations (Section 2.4.1) and a model for ranking the threads of a particular combination of con gurations (Section 2.4.2), we can select the best combination. To do so, we start with the best single con guration, and record its e ectiveness on the training and validation questions, considering it to be the best combination so far. en, we iterate over the ordered con gurations, one at a time, adding each to the pool of con gurations, and training its L2R model. We record the average score of the predictions on the training and validation questions using the actual ground truth scores (not the integerized versions). If the e ectiveness increases in both sets, we consider the actual combination to be the best one. We stop when we nish our enumeration, and return the most recent best combination.",1,ad,True
3 TEST COLLECTION,0,,False
"We present a set of Twi er questions, a crawl of Yahoo! Answers, and a collection of tweets used to build a language model.",1,Yahoo,True
3.1 estions and Answers,0,,False
"Among questions with real information needs, only a small fraction could reasonably be answered by an automated system. Consider, for example, ""@user hey, when u coming back?"" Clearly, the asker would want an answer to this question, but probably only the mentioned user could provide it. It would be advisable for an answering system to skip such questions. We have collected a set of 5,000 questions posted on Twi er in February 2016 and asked annotators on the crowdsourcing platform CrowdFlower to indicate whether some stranger probably exists who could read the question and o er a useful answer. In this paper, we use the resulting set of 362 tweets deemed to be answerable questions, with a 177/85/100 split between training, validation and test.",1,ad,True
"With the large base of old questions and answers available in Yahoo! Answers, we hope to successfully nd useful answers to a substantial number of questions asked on Twi er. One option would be to issue the question as a query and rely on the questions and answers retrieved by its""black box"" internal search engine. However, this would prevent us from studying the di erent options for building and using the inverted index. us, we obtain a crawl of 123M questions and 673M answers from [2]. We exclude questions and answers that contain any term from a ""dirty word"" list,7 and index all of the remaining questions and answers posted prior to 2016 (to avoid ""leaking"" future information to new answers).",1,Yahoo,True
3.2 Ground Truth,0,,False
"We also collect annotations for the results of our experiments using CrowdFlower. Because our task is similar, we adopt the same 4level relevance scale (bad, fair, good, excellent) as the the TREC LiveQA track [1]. We assign the weighted average score over three annotators (where the weight is computed from annotator accuracy on a set of questions with known answers) as the ground-truth relevance score of the thread. Annotators with accuracy scores below 85% were removed and replaced.",1,ad,True
"As assessing all answers to a question might be impractical when many answers exist, we present only the question title, body, and a what we expect to be the best few answers. We select these answers in part based on metadata from Yahoo! Answers and in part based on whichever of our systems found the answer. We rst select the best answer, as designated by the asker, if one exists. Otherwise, we select the answer with the highest di erence between thumbsup and thumbs-down votes, breaking ties by the score assigned by the system that found that answer. We also include whatever answer our system scored highest. In both cases, if multiple systems retrieve the same thread but disagree on which is the best answer, we include the best answers from each.",1,ad,True
"We obtained the annotations in several batches. In each batch we gathered the annotations for all threads that had not been previously assessed for all 362 questions. is allowed us to use the results of prior annotations to incrementally improve our systems, thus generating a richer test set, akin to the way systems from one year are used to guide the development of test collections in subsequent years at shared-task evaluations such as TREC. We note, however, that di erent annotators assessed di erent batches.",1,ad,True
7Downloaded from h ps://gist.github.com/roxlu/1769577,1,ad,True
1183,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
3.3 Twitter Language Model,1,Twitter,True
"For language modeling, we obtained the Twi er random 1% public sample stream between January 2012 and December 2015 from the Internet Archive.8 We keep only English tweets and index them with Lucene (without stemming or stopword removal), recording the positions of each indexed term in its tweet. We use this positional index as a language model to guide our question transformations (Section 2.1).",0,,False
4 RESULTS,0,,False
"We experiment with a combinations of 74 con gurations. Table 1 shows the average top-1 accuracy (on a [0-3] scale) for some of the combinations. First, we observe that the single best eld is the Title (line 1). It is signi cantly (p < 0.05, two-sided paired t-test) be er than the Body and the Answer elds (lines 2 and 7) in all sets (i.e., training, validation and test). Searching using this eld is also be er than searching using the entire page as a single eld (line 8), with a signi cance observed in the training and test sets. It also appears that question-per-document indexing may be a bit be er than answer-per-document indexing (compare lines 5, 6 and 7 to 9, 10 and 11), but weak signi cance is observed only in the training set (p < 0.1). Tuning the weights of the elds (line 12) seems to over t to the training set, where it is signi cantly (p < 0.05) be er than all combinations other than QpD-T (i.e., lines 2 through 11). On the validation and test sets, this tuning is not be er than some of those combinations. None of the query rewriting methods, individually or in combination, improve the results signi cantly, and the same is true for using the IDF of the Twi er index. Insigni cant positive di erences, when observed, are restricted to the training set. e statistically signi cant improvements we observe (p < 0.01) with the L2R model over all con gurations appears to be an instance of over ing. In fact, the results over the validation set decrease slightly (from 1.48 of line 5 to 1.45 in line 26), and the gain we get in the test set (from 1.32 to 1.37) is not statistically signi cant.",0,,False
5 CONCLUSION,0,,False
"We studied the possibility of answering the questions asked on Twi er using Yahoo! Answers, and found that, on average, two thirds of the answerable questions do have an excellent answer there. We found that searching in the title eld of the old questions yields a signi cant improvement over search in the concatenation of all the elds of a CQA thread. Small improvements are sometimes observed using various techniques, such as the tuning the weights of the indexed elds, rewriting the tweet question, and using the IDF of an index of tweets. While none of these techniques is particularly be er than the others, the pool of diverse threads they retrieve suggests that a failure analysis might help to identify techniques that can be employed for speci c question types. We have released our test collection to encourage further investigation.9",1,Yahoo,True
6 ACKNOWLEDGMENT,0,,False
is work was made possible in part by NPRP grant# NPRP 6-13771-257 from the Qatar National Research Fund (a member of Qatar,1,ad,True
8h p://archive.org/details/twi erstream 9h p://cs.umd.edu/mossaab/ les/aqweet-answering.tgz,0,,False
Table 1: E ectiveness of con gurations over the scale [0-3].,0,,False
# Con guration,0,,False
Fields,0,,False
Average score Train. Valid. Test,0,,False
1 BM25 2 BM25 3 BM25 4 BM25 5 BM25 6 BM25 7 BM25 8 BM25,0,,False
QpD-T,0,,False
1.22,0,,False
QpD-B,0,,False
0.80,0,,False
QpD-TB 1.10,1,TB,True
QpD-C,0,,False
1.13,0,,False
QpD-TA 1.20,0,,False
QpD-TBA 1.11,1,TB,True
QpD-A,0,,False
0.79,0,,False
QpD-P,0,,False
0.85,0,,False
1.19 1.32 0.89 0.75 1.14 1.05 1.16 1.20 1.48 1.21 1.22 1.14 0.82 0.74 1.09 0.88,0,,False
9 BM25 10 BM25 11 BM25,0,,False
ApD-TA 1.10,0,,False
ApD-TBA 1.02,1,TB,True
ApD-A,0,,False
0.54,0,,False
1.27 1.12 1.15 1.04 0.68 0.41,0,,False
12 Weighted BM25,0,,False
QpD-TBA 1.32 1.28 1.32,1,TB,True
13 BM25 + Hashtag Split 14 BM25 + Hashtag Split,0,,False
QpD-T QpD-TA,0,,False
1.24 1.19 1.31 1.22 1.46 1.20,0,,False
15 BM25 + Spell Correction QpD-T 16 BM25 + Spell Correction QpD-TB 17 BM25 + Spell Correction QpD-TA,1,TB,True
1.23 1.19 1.30 1.13 1.17 1.03 1.21 1.48 1.19,0,,False
18 BM25 + Synonyms 19 BM25 + Synonyms,0,,False
QpD-T QpD-TA,0,,False
1.22 1.24 1.29 1.21 1.43 1.22,0,,False
20 BM25 + 3 Rewriters 21 BM25 + 3 Rewriters 22 BM25 + 3 Rewriters,0,,False
QpD-T QpD-TA QpD-C,0,,False
1.25 1.24 1.27 1.23 1.41 1.19 1.12 1.17 1.10,0,,False
23 BM25 + Twi er IDF 24 BM25 + Twi er IDF 25 BM25 + Twi er IDF,0,,False
QpD-T QpD-TA QpD-P,0,,False
1.21 1.08 1.32 1.09 1.38 0.96 0.82 1.15 0.97,0,,False
"26 L2R , (12) + (20) + (25) + (22)",0,,False
1.43 1.45 1.37,0,,False
27 Oracle,0,,False
1.90 2.03 1.86,0,,False
Foundation) and by an IBM Ph.D. Fellowship. e statements made herein are solely the responsibility of the authors.,1,ad,True
REFERENCES,0,,False
"[1] Eugene Agichtein, David Carmel, Dan Pelleg, Yuval Pinter, and Donna Harman. 2015. Overview of the TREC 2015 LiveQA Track. In TREC. Gaithersburg, MD.",1,TREC,True
"[2] Mossaab Bagdouri and Douglas W. Oard. 2015. CLIP at TREC 2015: Microblog and LiveQA. In TREC. Gaithersburg, MD, USA.",1,TREC,True
"[3] orsten Joachims. 2006. Training Linear SVMs in Linear Time. In KDD'06. Philadelphia, PA, USA, 217­226.",1,ad,True
"[4] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR'15. San Diego, CA, USA.",0,,False
"[5] Baichuan Li and Irwin King. 2010. Routing estions to Appropriate Answerers in Community estion Answering Services. In CIKM'10. Toronto, ON, Canada, 1585­1588.",1,ad,True
"[6] Tomas Mikolov, Kai Chen, Greg Corrado, and Je rey Dean. 2013. E cient Estimation of Word Representations in Vector Space. In Workshop at ICLR'13.",1,ad,True
"[7] Sharoda A. Paul, Lichan Hong, and Ed H. Chi. 2011. Is Twi er a Good Place for Asking estions? A Characterization Study. In ICWSM'11.",1,CW,True
"[8] Stephen Robertson and Hugo Zaragoza. 2009. e Probabilistic Relevance Framework: BM25 and Beyond. FnTIR 3, 4 (April 2009), 333­389.",0,,False
"[9] Anna Shtok, Gideon Dror, Yoelle Maarek, and Idan Szpektor. 2012. Learning from the Past: Answering New estions with Past Answers. In WWW'12. Lyon, France, 759­768.",0,,False
1184,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Improving Retrieval Performance for Verbose eries via Axiomatic Analysis of Term Discrimination Heuristic,0,,False
Mozhdeh Ariannezhad,1,ad,True
"School of Eelectrical and Computer Engineering College of Engineering, University of Tehran m.ariannezhad@ut.ac.ir",1,ad,True
Hamed Zamani,0,,False
Center for Intelligent Information Retrieval University of Massachuse s Amherst zamani@cs.umass.edu,0,,False
ABSTRACT,0,,False
"Number of terms in a query is a query-speci c constant that is typically ignored in retrieval functions. However, previous studies have shown that the performance of retrieval models varies for di erent query lengths, and it usually degrades when query length increases. A possible reason for this issue can be the extraneous terms in longer queries that makes it a challenge for the retrieval models to distinguish between the key and complementary concepts of the query. As a signal to understand the importance of a term, inverse document frequency (IDF) can be used to discriminate query terms. In this paper, we propose a constraint to model the interaction between query length and IDF. Our theoretical analysis shows that current state-of-the-art retrieval models, such as BM25, do not satisfy the proposed constraint. We further analyze the BM25 model and suggest a modi cation to adapt BM25 so that it adheres to the new constraint. Our experiments on three TREC collections demonstrate that the proposed modi cation outperforms the baselines, especially for verbose queries.",1,ad,True
KEYWORDS,0,,False
"Verbose queries, query length, axiomatic analysis, theoretical analysis, term discrimination",0,,False
"ACM Reference format: Mozhdeh Ariannezhad, Ali Montazeralghaem, Hamed Zamani, and Azadeh Shakery. 2017. Improving Retrieval Performance for Verbose eries via Axiomatic Analysis of Term Discrimination Heuristic. In Proceedings of SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: 10.1145/3077136.3080761",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080761",1,ad,True
Ali Montazeralghaem,0,,False
"School of Eelectrical and Computer Engineering College of Engineering, University of Tehran ali.montazer@ut.ac.ir",0,,False
Azadeh Shakery,1,ad,True
"School of Eelectrical and Computer Engineering College of Engineering, University of Tehran School of Computer Science",0,,False
Institute for Research in Fundamental Sciences (IPM) shakery@ut.ac.ir,0,,False
1 INTRODUCTION,1,DUC,True
"Modern retrieval models use di erent query-based, documentbased, and corpus-based properties to compute the relevance score of a document with respect to a query. Term frequency (TF), inverse document frequency (IDF), and document length are the main factors that are typically present in a retrieval function. When optimizing a retrieval model, the main focus is generally on the document-side of the function that weighs the query terms with respect to a document. e query-side part of retrieval models, however, is usually a simple term-weighting function based on the count of a term in the query. As a result, query-speci c properties, such as query length, have been widely assumed as constants that do not a ect document ranking, henceforth ignored in the retrieval models [9].",0,,False
"While most of the existing retrieval models are targeted mainly at short keyword queries, their poor performance on longer queries led a large number of e orts that try to understand the properties of verbose queries [1, 8, 11]. Recently, several studies revealed that query length a ects the performance of retrieval models through interaction with TF and document length normalization [2, 4, 9]. Chung et al. [2] proposed an adaptive method to estimate the parameters of pivoted document length normalization based on query length. Lv [9] proposed a formal constraint to model the relation between query length and the TF decay speed. He further modi ed the BM25 retrieval function to satisfy the proposed constraint and showed that the modi ed version improves the standard BM25 model. e work of Cummins and Riordan [4] is also based on constraint analysis. Similarly, they formalized a constraint to capture the interaction of query length and document length normalization. However, their method only performs comparably to the baseline retrieval models.",1,ad,True
"In this paper, we focus on the interaction of the discrimination value of query terms with the length of query. Usually, verbose queries contain some unessential terms, while short queries consist of keywords that are almost equally important. We argue that the e ect of term discrimination value, which is generally modeled with IDF, should di er in verbose and short keyword queries. We hypothesize that when query length increases, the e ect of IDF should be highlighted, in order to facilitate distinguishing more",0,,False
1201,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"important terms. We propose a formal constraint to model this hypothesis mathematically and use axiomatic analysis to examine BM25 [12], a state-of-the-art retrieval model, and nd that it does not satisfy the proposed constraint. We further modify BM25 so that it adheres to the new constraint. More speci cally, we learn that the constraint requires the di erence of IDF values to increase with query length and propose a simple function to adapt BM25 to the constraint, which can be computed as e cient as the BM25 model.",1,ad,True
"We evaluate the new version of BM25 using three TREC collections: AP (Associated Press 1988-89), Robust (TREC 2004 Robust track), and WT10g (TREC 9-10 Web track). To further study the e ect of query length, we test two types of queries, short queries and verbose queries, which are borrowed from the title and the description elds of the TREC topics, respectively. Our experimental results demonstrate that our proposed method outperforms the baselines, especially in verbose queries.",1,TREC,True
2 DEFINITION OF THE FORMAL,0,,False
CONSTRAINT,0,,False
"Axiomatic analysis or constraint analysis of retrieval models provides a formal framework to evaluate existing retrieval models and diagnose their de ciencies. It can be further employed to improve the retrieval models by introducing new developed versions that address the previously found shortcomings [3, 5­7, 10]. is goal is usually achieved by describing a set of desirable properties that a retrieval model should have, and characterizing a reasonable retrieval formula by listing formal constraints that it must satisfy.",1,ad,True
"Inspired by previous work [5, 9], we propose a formal constraint that retrieval models should satisfy. is constraint, namely QLNIDF, captures the interaction between IDF and query length. We then provide an analytical analysis of BM25, a state-of-the-art retrieval model, to show that it does not satisfy the proposed constraint. We then propose a modi cation to BM25 so that the modi-",0,,False
ed version adheres to the constraint.,1,ad,True
"Notation. We rst introduce our notation. S(Q, D) denotes the relevance score of document D for a given query Q, computed by a retrieval model. |Q | and |D| denote the length of Q and D, respectively. dt f (q, D) weighs query term q with respect to the document D based on the count of q in D, i.e., c(q, D). id f (w) denotes the inverse document frequency for a given term w.",0,,False
"In the following, we introduce our formal constraint, named QLN-IDF.",0,,False
"QLN-IDF: Let Q ,"" {q1, q2} be a query with two terms where id f (q1) > id f (q2). Assume that D1 and D2 are two documents such that |D1| "","" |D2|, c(q1, D1) "","" c(q2, D2) > 0 and c(q1, D2) "","" c(q2, D1) "","" 0 with the document relevance scores S(Q, D1) and""",0,,False
"S(Q, D2). If we reformulate the query Q ,"" Q  {q3} by adding a term to the query, such that c(q3, D1) "","" c(q3, D2) "","" 0, then:""",1,ad,True
"S(Q, D1) - S(Q, D2) < S(Q , D1) - S(Q , D2)",0,,False
(1),0,,False
"Term discrimination constraint [5] states that when two documents contain the same number of occurrences of query terms, the document that has more discriminative terms should get a higher relevancy score. According to QLN-IDF, the di erence of scores between such documents should increase when query gets longer.",0,,False
"From an information theoretic perspective, adding a term to the query is equivalent to increasing the information provided by the query. We hypothesize that when query length increases, the e ect of IDF should be highlighted, in order to facilitate distinguishing more important terms. Suppose that two documents, say D and D , contain q1 and q2, respectively. When additional information is given by inclusion of a new query term q3, which does not occur in either of the mentioned documents, D should not be penalized as much as D , because the highlighted e ect of IDF means that D contains more information compared to D .",1,ad,True
3 A MODIFICATION TO BM25,0,,False
"We now analyze the BM25 retrieval model to examine whether it satis es the proposed constraint or not. Following previous work [9], we use the BM25 formula presented in [5]. e score of a document D with respect to a query Q is calculated as follows:",0,,False
"S (Q, D) ,",0,,False
"qt f (q, Q ) × dt f (q, D) × id f (q)",0,,False
(2),0,,False
q Q D,0,,False
",",0,,False
qQ D,0,,False
"(k3 + 1) × c(q, Q ) k3 + c(q, Q )",0,,False
×,0,,False
k1((1,0,,False
"(k1 + 1) × c(q, D)",0,,False
-,0,,False
b,0,,False
),0,,False
+,0,,False
b,0,,False
a,0,,False
|d | d,0,,False
l,0,,False
),0,,False
+,0,,False
c,0,,False
"(q,",0,,False
D),0,,False
×,0,,False
log,0,,False
N df,0,,False
+1 (q),0,,False
",",0,,False
"where k1, k3 and b are free hyper-parameters. a dl and N respectively denote the average document length and total number of documents in the collection.",0,,False
"It can be easily proved that the BM25 model does not satisfy the QLN-IDF constraint, since there is no query length related property in the BM25 model. In more details, it can be shown that",0,,False
"S(Q, D1) - S(Q, D2) < S(Q , D1) - S(Q , D2) implies the following inequality:",0,,False
id f (q1) - id f (q2) |Q | < id f (q1) - id f (q2) |Q+1| .,0,,False
"e above inequality states that the e ect of id f (q) should change with respect to the query length. In this regard, we propose the following modi cation to the BM25 formula:",0,,False
"S(Q, D) ,",0,,False
"qt f (q, Q) × dt f (q, D) × (id f (q) + 1)log(|Q |+1).",0,,False
q QD,0,,False
"e 1 that is added to the log in the power is to prevent it from becoming zero when |Q | is one. e 1 that is added to the idf formula is to ensure that the value of idf is greater than 1, which is necessary for satisfaction of the QLN-IDF constraint. In order to prove that the new BM25 formula results in the satisfaction of QLN-IDF, the following condition should always be true:",1,ad,True
(id f (q2) + 1)log(|Q+1|+1) - (id f (q2) + 1)log(|Q |+1) < (id f (q1) + 1)log(|Q+1|+1) - (id f (q1) + 1)log(|Q |+1).,0,,False
"With some basic mathematical derivations, it can be shown that this is always the case when id f (q2) < id f (q1), which is implied by the constraint. We refer to this modi ed version of BM25 as BM25-QI.",0,,False
4 EXPERIMENTS,0,,False
"In this section, we rst introduce our collections, experimental setup, and evaluation metrics. We further report and discuss the experimental results.",0,,False
1202,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Summary of TREC collections and topics.,1,TREC,True
ID,0,,False
Collection,0,,False
eries,0,,False
#docs,0,,False
#qrels,0,,False
average query length title (short) description,0,,False
"AP TREC 1-3 Ad-hoc Track, Associated Press 88-89",1,AP,True
topics 51-200,0,,False
"165k 15,838 4.17",0,,False
11.31,0,,False
Robust,1,Robust,True
TREC 2004 Robust Track Collection,1,TREC,True
"topics 301-450 & 601-700 528k 17,412 2.59",0,,False
8.43,0,,False
WT10g,1,WT,True
TREC 9-10 Web Track Collection,1,TREC,True
topics 451-550,0,,False
1692k 5931 2.48,0,,False
6.47,0,,False
Table 2: Comparison of the proposed modi cation to the BM25 model compared to the baselines. Superscripts 1/2 indicate that the improvements over BM25/BM25-QL are statistically signi cant.,0,,False
Short eries,0,,False
Method,0,,False
AP,1,AP,True
Robust,1,Robust,True
WT10g,1,WT,True
MAP P@10 MAP P@10 MAP P@10,1,MAP,True
BM25 0.2717 0.4275 0.2540 0.4353 0.1938 0.2768,0,,False
BM25-QL 0.2729 0.4262 0.2496 0.4353 0.1959 0.3071,0,,False
BM25-QI 0.2731 0.4188 0.25502 0.4353 0.2015 0.2980,0,,False
Verbose eries,0,,False
Method,0,,False
BM25 BM25-QL BM25-QI,0,,False
AP MAP P@10 0.2468 0.4154 0.24951 0.4242 0.269112 0.4201,1,AP,True
Robust MAP P@10 0.2367 0.4100 0.2340 0.4129 0.253012 0.4157,1,Robust,True
WT10g MAP P@10 0.1876 0.3180 0.1854 0.3330 0.210912 0.3280,1,WT,True
4.1 Experimental Design,0,,False
"Collections. We use three standard TREC collections in our experiments: AP, Robust04 and WT10g. AP and Robust are newswire collections, whereas WT10g is a Web collection containing more noisy documents. Statistics of the collections are shown in Table 1. Experimental Setup. We use two types of queries, short queries and verbose queries, which are taken from the title and the description elds of the TREC topics, respectively. Average query length of title and description elds for all collections are shown in Table 1. All documents are stemmed using the Porter stemmer and stopped using the standard INQUERY stopword list. e experiments were carried out using the Lemur toolkit1. Parameter Setting. e parameters b and k1 of BM25 and BM25QI are set using 2-fold cross-validation over the queries of each collection. We changed the parameter b from 0 to 1, and the parameter k1 from 0 to 5 in increments of 0.1. k3 has no e ect in our experiments, because for almost all of the query terms c(q, Q) is equal to 1. erefore, qt f (q, Q) will be equal to c(q, Q). Evaluation Metrics. We use two metrics to measure the retrieval quality: (1) mean average precision (MAP) of the top ranked 1000 documents, and (2) the precision of the top 10 retrieved documents (P@10). MAP also serves as the objective function for parameter tuning. Statistically signi cant di erences of performances are determined using the two-tailed paired t-test at a 95% con dence level.",1,TREC,True
4.2 Results and Discussion,0,,False
"In this subsection, we evaluate the performance of our proposed method (BM25-QI) and compare its performance to those obtained by the baselines. We further study the sensitivity of our method to the input parameters.",0,,False
4.2.1 Evaluation of the Proposed Method,0,,False
"We consider two baselines: (1) the original BM25 method, and (2) the enhanced version of BM25 (BM25-QL) which satis es QLNTFC constraint proposed in [9]. is model computes the retrieval score as follows:",0,,False
"S (Q,",0,,False
D),0,,False
",",0,,False
qQ D,0,,False
"c (q,",0,,False
Q),0,,False
×,0,,False
"( . log( |Q |) +  + 1)c (q, D)  . log( |Q |) +  + c (q, D)",0,,False
×,0,,False
log,0,,False
N df,0,,False
+1 (q),0,,False
",",0,,False
where,0,,False
c,0,,False
"(q, D)",0,,False
",",0,,False
1 - ((,0,,False
. log(|Q |) + ,0,,False
"c(q, D) )) + ((",0,,False
. log(|Q |) + ,0,,False
),0,,False
|D | a dl,0,,False
",",0,,False
"and , ,  , and  are free parameters that are estimated using supervised learning to nd the optimal k1 and b parameters in the original BM25 retrieval model [9]. e parameters are optimized using the linear regression model implemented in the scikit-learn toolkit2.",0,,False
Table 2 summarizes the results achieved by the proposed method and the baselines. We separate title and description queries to demonstrate the behavior of methods for di erent query lengths.,0,,False
"e results show that BM25-QI outperforms BM25 and BM25-QL consistently in terms of MAP and also achieves be er or comparable P@10 scores compared to BM25. e MAP improvements of BM25-QI over the baselines are much larger on verbose queries. In particular, the MAP improvements on all collections are statistically signi cant for verbose queries. is is likely because when the number of terms in a query is high, the range of IDF values is wide. In this situation, IDF can be a good signal of term discrimination value. However, short queries usually consist of keywords and the variance of their IDF values are smaller, which degrades the e ect of our proposed constraint and modi cation.",1,MAP,True
"Another interesting observation is that, while the performance of the baselines is substantially be er on short queries in all collections, the performance of BM25-QI on verbose queries is comparable to its performance on short queries on AP and Robust, and has improved on WT10g. is result empirically con rms that the proposed method be er adapts to di erent query lengths. In other words, while existing retrieval models [9, 12] are targeted mainly at short keyword queries and perform poorly on longer queries, our proposed model signi cantly improves the performance for verbose queries while achieving comparable and in some cases be er results on short queries.",1,AP,True
1h p://lemurproject.org/,0,,False
2h p://scikit-learn.org/,0,,False
1203,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
0.28 0.27 0.26,0,,False
AP,1,AP,True
title desc,0,,False
0.26 0.25 0.24 0.23,0,,False
Robust,1,Robust,True
title desc,0,,False
0.22 0.21 0.20,0,,False
WT10g,1,WT,True
title desc,0,,False
MAP,1,MAP,True
MAP,1,MAP,True
MAP,1,MAP,True
0.25 0.24,0,,False
0.22 0.21 0.20,0,,False
0.19 0.18,0,,False
0.23,0,,False
0.19,0,,False
0.17,0,,False
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5,0,,False
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5,0,,False
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5,0,,False
k1,0,,False
k1,0,,False
k1,0,,False
"Figure 1: Sensitivity of the proposed method to the parameter k1 in di erent collections, for title and description queries.",0,,False
MAP,1,MAP,True
0.27 0.26 0.25 0.24 0.23,0,,False
AP,1,AP,True
title desc,0,,False
MAP,1,MAP,True
0.26 0.25 0.24 0.23 0.22 0.21 0.20 0.19 0.18 0.17,0,,False
Robust,1,Robust,True
title desc,0,,False
MAP,1,MAP,True
0.21 0.20 0.19 0.18 0.17 0.16 0.15 0.14 0.13 0.12 0.11 0.10 0.09 0.08,0,,False
WT10g,1,WT,True
title desc,0,,False
0 0.1,0,,False
0.3,0,,False
0.5,0,,False
0.7,0,,False
0.9 1,0,,False
0 0.1 0.3 0.5 0.7 0.9 1,0,,False
0 0.1,0,,False
0.3,0,,False
0.5,0,,False
0.7,0,,False
0.9 1,0,,False
b,0,,False
b,0,,False
b,0,,False
"Figure 2: Sensitivity of the proposed method to the parameter b in di erent collections, for title and description queries.",0,,False
4.2.2 Parameter Sensitivity,0,,False
"In this set of experiments, we study the performance of BM25QI in terms of MAP with respect to the parameters k1 and b for both query sets on all of the collections. e results are shown in Figures 1 and 2. According to these gures, large values of k1 hurt the performance of BM25-QI on both title and description queries, and description queries almost always achieve higher MAP values compared to title queries. ese results emphasize the advantage of our model for verbose queries. e performance of BM25-QI is more stable with respect to k1 on AP, and higher values of k1 seem to be more suitable for longer queries. A similar behavior on Robust and WT10g is again observed with respect to b. For lower values of b, the performance is be er on title queries and as value of b goes up from 0.5, description queries reach to higher values of MAP. However, the overall performance is stable in the [0.1, 0.5] interval for title queries and in the [0.3, 0.7] interval for description queries.",1,MAP,True
5 CONCLUSIONS AND FUTURE WORK,0,,False
"In this paper, we analyzed the interaction between query length and IDF, a term discrimination heuristic that can be thought as a signal that measures the relative importance of query terms. We proposed the idea that the e ect of discrimination value of query terms should not be the same in verbose queries and short queries. We hypothesized that when query length increases, the e ect of IDF should be highlighted, in order to facilitate distinguishing more important terms. To formalize this idea, we proposed a new constraint that any reasonable retrieval function should satisfy. We have then studied the BM25 model and revealed that it does not adhere to the constraint. We proposed a modi cation to BM25 based on our axiomatic analysis. Our experimental results showed that the modi ed version of BM25 outperforms the original one, in particular for the cases where queries are long. In the future, we intend to study other retrieval models, such as query likelihood, and",1,ad,True
analyze their adherence to the proposed constraint. Investigating a more e ective approach to model the interaction between query length and IDF is also an interesting research direction for future work. Acknowledgements. is work was supported in part by the Center for,1,ad,True
Intelligent Information Retrieval and in part by a grant from the Institute,0,,False
"for Research in Fundamental Sciences (No. CS1396-4-51). Any opinions,",0,,False
ndings and conclusions or recommendations expressed in this material,0,,False
are those of the authors and do not necessarily re ect those of the sponsors.,0,,False
REFERENCES,0,,False
"[1] Michael Bendersky and W. Bruce Cro . 2008. Discovering Key Concepts in Verbose eries. In SIGIR '08. Singapore, Singapore, 491­498.",0,,False
"[2] Tze Leung Chung, Robert Wing Pong Luk, Kam Fai Wong, Kui Lam Kwok, and Dik Lun Lee. 2006. Adapting Pivoted Document-length Normalization for ery Size: Experiments in Chinese and English. ACM Transactions on Asian Language Information Processing (TALIP) 5, 3 (2006), 245­263.",0,,False
"[3] Ronan Cummins. 2016. A Study of Retrieval Models for Long Documents and eries in Information Retrieval. In WWW '16. Montreal, ebec, Canada,",1,ad,True
795­805. [4] Ronan Cummins and Colm O'Riordan. 2012. A Constraint to Automatically,0,,False
"Regulate Document-length Normalisation. In CIKM '12. Maui, Hawaii, USA, 2443­2446. [5] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A Formal Study of Information Retrieval Heuristics. In SIGIR '04. She eld, United Kingdom, 49­56. [6] Hui Fang and ChengXiang Zhai. 2005. An Exploration of Axiomatic Approaches to Information Retrieval. In SIGIR '05. Salvador, Brazil, 480­487. [7] Hui Fang and ChengXiang Zhai. 2006. Semantic Term Matching in Axiomatic Approaches to Information Retrieval. In SIGIR '06. Sea le, Washington, USA, 115­122. [8] Manish Gupta and Michael Bendersky. 2015. Information Retrieval with Verbose",1,ad,True
"eries. Foundations and Trends in Information Retrieval 9, 3-4 (2015), 209­354. [9] Yuanhua Lv. 2015. A Study of ery Length Heuristics in Information Retrieval.",0,,False
"In CIKM '15. Melbourne, Australia, 1747­1750. [10] Yuanhua Lv and ChengXiang Zhai. 2011. Lower-bounding Term Frequency",0,,False
"Normalization. In CIKM '11. Glasgow, Scotland, UK, 7­16. [11] Jiaul H. Paik and Douglas W. Oard. 2014. A Fixed-Point Method for Weighting",0,,False
"Terms in Verbose Informational eries. In CIKM '14. Shanghai, China, 131­140. [12] S. E. Robertson and S. Walker. 1994. Some Simple E ective Approximations to",0,,False
"the 2-Poisson Model for Probabilistic Weighted Retrieval. In SIGIR '94. Dublin, Ireland, 232­241.",0,,False
1204,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Embedding-based ery Expansion for Weighted Sequential Dependence Retrieval Model,0,,False
Saeid Balaneshin-kordan,0,,False
Wayne State University saeid@wayne.edu,0,,False
ABSTRACT,0,,False
"Although information retrieval models based on Markov Random Fields (MRF), such as Sequential Dependence Model and Weighted Sequential Dependence Model (WSDM), have been shown to outperform bag-of-words probabilistic and language modeling retrieval models by taking into account term dependencies, it is not known how to e ectively account for term dependencies in query expansion methods based on pseudo-relevance feedback (PRF) for retrieval models of this type. In this paper, we propose Semantic Weighted Dependence Model (SWDM), a PRF based query expansion method for WSDM, which utilizes distributed low-dimensional word representations (i.e., word embeddings). Our method nds the closest unigrams to each query term in the embedding space and top retrieved documents and directly incorporates them into the retrieval function of WSDM. Experiments on TREC datasets indicate statistically signi cant improvement of SWDM over stateof-the-art MRF retrieval models, PRF methods for MRF retrieval models and embedding based query expansion methods for bagof-words retrieval models.",1,corpora,True
CCS CONCEPTS,0,,False
·Information systems  ery reformulation;,0,,False
KEYWORDS,0,,False
Weighted Sequential Dependence Model; Term Dependencies; Word Embeddings; ery Expansion; Pseudo-Relevance Feedback,0,,False
1 INTRODUCTION,1,DUC,True
"Designing retrieval models and addressing the problem of vocabulary mismatch via query and document expansion have traditionally been two orthogonal directions of information retrieval (IR) research. In particular, separate methods for query [4] or document [3] expansion are typically employed in conjunction with bag-of-words probabilistic, such as BM25 [15], and language modeling based, such as ery Likelihood [14], retrieval models. ese methods typically identify query expansion terms in the collection itself using statistical measures of semantic similarity between pairs of terms pre-computed in advance [2, 9], top-retrieved documents [11], external resources [10] or their combination [1].",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR 17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978­1­4503­5022­8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080764",1,ad,True
Alexander Kotov,0,,False
Wayne State University kotov@wayne.edu,0,,False
"Markov Random Fields (MRF) retrieval models [12], such as Sequential Dependence Model (SDM) and Weighted Sequential Dependence Model (WSDM), consider retrieval function as a graph of dependencies between the query terms and a document and calculate document retrieval score as a linear combination of the potential functions de ned on the cliques in this graph. Although these retrieval models have been shown to outperform probabilistic and language modeling retrieval models by going beyond bagof-words assumption and taking into account term dependencies, it is not known how to e ectively incorporate term dependencies into query expansion methods based on pseudo-relevance feedback (PRF) for this type of retrieval models. Due to sparsity of n-grams, accounting for term dependencies in query expansion based only on term co-occurrence statistics within the collection itself is quite challenging. For this reason, only unigrams have been utilized for query expansion in Latent Concept Expansion (LCE) [13] and Parameterized ery Expansion (PQE) [6], state-ofthe-art PRF methods for SDM and WSDM, respectively.",1,corpora,True
"Word embeddings are distributed low-dimensional vector representations, which have been successfully utilized in di erent IR contexts, such as estimation of translation models [3, 21] and query expansion for bag-of-words retrieval models [19], as well as in retrieval models based on neural architectures [7, 16] and proximity search [8]. Since n-grams typically appear in a limited number of contexts in a collection, the utility of n-gram embeddings for IR is limited. For example, based on the word embeddings trained on a Google News corpus with 100 billion words1, the most semantically similar words to ""human"" are ""humankind"", ""mankind"" and ""humanity"", all of which can be good query expansion terms.",0,,False
"e most semantically similar n-grams to ""human"", however, are ""human beings"", ""impertinent amboyant endearingly"", ""employee Laura Althouse"", and ""Christine Gaugler head""2. It is obvious that these n-grams would only cause topic dri and degrade the retrieval results, if used for query expansion. Furthermore, due to sparsity, bigram embeddings are also ine ective for computing their importance weight in SDM [20].",1,ad,True
"Our proposed model, Semantic Weighted Dependence Model (SWDM), mitigates the potential vocabulary mismatch between queries and documents in WSDM via query expansion. Similar to SDM and WSDM, the retrieval score of a document, according to SWDM, depends on the matching query unigrams, ordered and unordered bigrams. However, unlike SDM, SWDM nds the closest unigrams and bigrams to query terms in embedding space and directly incorporates them into the retrieval function of WSDM.",1,corpora,True
"1h ps://drive.google.com/ le/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/ 2 e similarity of trigrams ""employee Laura Althouse"" and ""Christine Gaugler head"" was deduced from the fragments "". . . said Christine Gaugler, head of human resources . . . "" and "". . . and human resources employee Laura Althouse . . . "", which appear in multiple places within this corpus.",1,ad,True
1213,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"To overcome the n-gram sparsity problem, SWDM takes into account only the dependencies between those pairs of terms, in which at least one term is semantically similar to one of the query terms and both terms have appeared within multiple windows in the collection and top retrieved documents. For example, for the query ""human smuggling"", SWDM identi es ""tra cking"" as one of the expansion unigrams and ""human tra cking"" as one of the expansion bigrams, since the unigram ""tra cking"" is semantically similar to ""smuggling"". Proximity to query terms in the embedding space as well as their frequency in the collection and top-retrieved documents are used as features for weighting the importance of original and expansion concepts (unigrams or bigrams).",0,,False
2 RELATED WORK,0,,False
"Word embeddings are typically utilized in retrieval models to calculate distributional similarity between terms, quantify relevance of documents to queries or as an input to neural architectures for relevance matching.",0,,False
"Calculation of distributional similarity: cosine similarity between word embeddings is used in two scenarios. In the rst scenario, distributional similarity between word vectors is used to",0,,False
"nd semantically similar words to expand queries [19] or documents [3, 21]. Components of a di erence vector between embeddings of query unigrams and embeddings of the entire query have been utilized as features to estimate the weights of query unigrams in SDM [20]. Relevance of documents to queries can be quanti ed by aggregating cosine similarity scores between pairs of the most similar query and document term embedding vectors [8]. An alternative approach to quantifying relevance of documents to queries involves aggregating the embeddings of all document and query words into a single embedding vector for the entire document and a single embedding vector for the entire query and calculating their relevance score as a cosine similarity between these vectors [17].",0,,False
"e proposed method also falls into this category, since it relies on cosine similarity between word embeddings to nd the most semantically similar unigrams to query terms and uses these unigrams for query expansion.",0,,False
"Input to neural architectures: distributed representations of query and document terms have also been used as input to neural architectures based on Convolutional Neural Network [16] or Recurrent Neural Network [18], which directly calculate the relevance score of documents to queries. In [7], a histogram of cosine similarities between embeddings of a query and documents terms is used an input to a feed-forward neural network with term gating, which directly computes the relevance score.",0,,False
3 METHOD,0,,False
Retrieval function of SDM calculates the relevance score of document D to query Q as follows:,0,,False
P,0,,False
(D,0,,False
|Q,0,,False
),0,,False
r,0,,False
ank,0,,False
",",0,,False
"T fT (qi , D) +",0,,False
"B fB (qiqi+1, D)+",0,,False
qi Q,0,,False
"qi ,qi+1 Q",0,,False
"U fU (qiqi+1, D)",0,,False
(1),0,,False
"qi ,qi+1 Q",0,,False
"where qi is a query unigram and qiqi+1 is a query bigram, and fT (qi , D), fB (qiqi+1, D) and fU (qiqi+1, D) are the potential (i.e.,",0,,False
"Figure 1: Graphical representation of SWDM. q1, q2 and q3 are the query terms and D is a collection document. e words in dashed circles are the nearest neighbors of the query terms in the embedding space (only two most semantically similar words to each query term are shown for illustration).",0,,False
"matching) functions for query concept types (unigrams, ordered",0,,False
"and unordered bigrams), respectively, and T , B and U are the weights of these potential functions, which determine the relative",0,,False
"importance of query concept types. e potential function fT (qi , D) for unigrams is de ned as:",0,,False
fT,0,,False
"(qi ,",0,,False
D),0,,False
",",0,,False
n(qi,0,,False
",",0,,False
D),0,,False
+,0,,False
µ,0,,False
"n (qi , C |C |",0,,False
),0,,False
|D| + µ,0,,False
(2),0,,False
"where n(qi , D) and n(qi , C) are the counts of unigram qi in D and collection C, |D| and |C | are the numbers of words in document and",0,,False
"collection, and µ is the Dirichlet smoothing prior. fB (qiqi+1, D) and fU (qiqi+1, D) are obtained in a similar way by counting cooccurrences of qi and qi+1 in D and C in sequential order or within a window of a given size.",0,,False
WSDM provides a more exible parametrization of relevance,0,,False
than SDM by calculating the importance weight of each individual,0,,False
query concept rather than a concept type. e importance weight,0,,False
of each unigram and bigram is calculated as a linear combination,0,,False
of ku unigram feature functions,0,,False
u j,0,,False
(qi,0,,False
),0,,False
and,0,,False
kb,0,,False
bigram,0,,False
feature,0,,False
func-,0,,False
tions,0,,False
b j,0,,False
(qi,0,,False
",",0,,False
qi,0,,False
+1,0,,False
),0,,False
as,0,,False
follows:,0,,False
P,0,,False
(D,0,,False
|Q,0,,False
),0,,False
r,0,,False
ank,0,,False
",",0,,False
ku,0,,False
wuj,0,,False
u j,0,,False
(qi,0,,False
),0,,False
fT,0,,False
(qi,0,,False
",",0,,False
D,0,,False
)+,0,,False
"qi Q j,1",0,,False
kb,0,,False
w,0,,False
b j,0,,False
b j,0,,False
(qi,0,,False
",",0,,False
qi,0,,False
+1),0,,False
fB,0,,False
(qi,0,,False
qi,0,,False
"+1,",0,,False
D,0,,False
)+,0,,False
"qi ,qi+1 Q j,1",0,,False
kb,0,,False
w,0,,False
b j,0,,False
b j,0,,False
(qi,0,,False
",",0,,False
qi,0,,False
+1),0,,False
fU,0,,False
(qi,0,,False
qi +1,0,,False
",",0,,False
D),0,,False
(3),0,,False
"qi ,qi+1 Q j,1",0,,False
3.1 Semantic Weighted Dependence Model,0,,False
"In SWDM, the relevance score of D to Q also takes into account the words that are semantically similar to query terms in the embedding space. We de ne qij as the jth most similar term to qi in the embedding space, according to cosine similarity. We also de ne Eqi ,"" {qi0, qi1, qi2, · · · } as a set of words, whose cosine similarity with qi in the embedding space exceeds a threshold s (including qi0 "","" qi itself). Unlike SDM and WSDM, the potential functions""",0,,False
1214,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Performance of the proposed method with and without unigrams from the top retrieved documents and the baselines. 1 and 2 indicate statistically signi cant improvements of SWDM over WSDM and EQE1, respectively, while 3, 4 and 5 indicate statistically signi cant improvements of SWDM+ over EQE1+RM1, PQE, and SWDM+RM1, respectively, according to the Fisher's randomization test (p < 0.05). Percentage improvements of SWDM over WSDM and EQE1 as well as percentage improvements of SWDM+ over PQE and SWM+RM1 are shown in parenthesis.",0,,False
Method,0,,False
SDM WSDM EQE1,0,,False
SWDM,0,,False
LCE PQE EQE1+RM1 SWDM+RM1,0,,False
SWDM+,0,,False
ROBUST04,0,,False
MAP,1,MAP,True
P@10,0,,False
0.2583,0,,False
0.4278,0,,False
0.2689,0,,False
0.4563,0,,False
"0.2597 0.28021, 2",0,,False
"0.4336 0.46761, 2",0,,False
(+4.20%/+7.89%) (+2.48%/+7.84%),0,,False
0.2886,0,,False
0.4697,0,,False
0.2921,0,,False
0.4726,0,,False
0.2872,0,,False
0.4672,0,,False
"0.2991 0.30343, 4, 5",0,,False
"0.4828 0.49093, 4, 5",0,,False
(+3.87%/+1.44%) (+3.87%/+1.68%),0,,False
GOV2,0,,False
MAP,1,MAP,True
P@10,0,,False
0.3156,0,,False
0.5457,0,,False
0.3232,0,,False
0.5533,0,,False
"0.3172 0.33191, 2",0,,False
"0.5466 0.55981, 2",0,,False
(+2.69%/+4.63%) (+1.17%/+2.41%),0,,False
0.3408,0,,False
0.5667,0,,False
0.3526,0,,False
0.5858,0,,False
0.3315,0,,False
0.5459,0,,False
"0.3557 0.36863, 4",0,,False
"0.5872 0.59973, 4",0,,False
(+4.54%/+3.63%) (+2.37%/+2.13%),0,,False
ClueWeb09B,1,ClueWeb,True
MAP,1,MAP,True
P@10,0,,False
0.0783,0,,False
0.2777,0,,False
0.0762,0,,False
0.2797,0,,False
"0.0742 0.08271, 2",0,,False
"0.2778 0.28121, 2",0,,False
(+8.53%/+11.46%) (+0.54%/+1.22%),0,,False
0.0738,0,,False
0.2693,0,,False
0.0749,0,,False
0.2751,0,,False
0.0731,0,,False
0.2695,0,,False
0.0756,0,,False
0.2716,0,,False
0.0787,0,,False
0.2778,0,,False
(+5.07%/+4.10%) (+0.98%/+2.28%),0,,False
"fT (qi , D), fB (qiqi+1, D) and fU (qiqi+1, D) in SWDM are calculated using all the terms in E and not just the query terms:",0,,False
ku,0,,False
P (D |Q ),0,,False
r ank,0,,False
",",0,,False
w,0,,False
u j,0,,False
u j,0,,False
(qmi,0,,False
),0,,False
fT,0,,False
(qmi,0,,False
",",0,,False
D)+,0,,False
"qim  Eqi j,1",0,,False
kb,0,,False
w,0,,False
b j,0,,False
b j,0,,False
(qmi,0,,False
",",0,,False
qmi +1 ),0,,False
fB,0,,False
"(qmi qmi+1,",0,,False
D,0,,False
)+,0,,False
"qim,qim+1  Eqi , Eqi+1 j ,1",0,,False
kb,0,,False
w,0,,False
b j,0,,False
b j,0,,False
(qmi,0,,False
",",0,,False
qmi +1 ),0,,False
fU,0,,False
"(qmi qmi+1,",0,,False
D,0,,False
),0,,False
"qim,qim+1  Eqi , Eqi+1 j ,1",0,,False
(4),0,,False
"erefore, besides pair-wise dependencies between adjacent query",1,ad,True
"terms, SWDM also captures pair-wise dependencies between query",0,,False
terms and the words semantically similar to them in the embed-,0,,False
"ding space. For an example query in Figure 1, besides the query",0,,False
"unigram q1, retrieval score of D, according to SWDM, also includes the weighted matching scores for unigrams q11 and q12 that are semantically similar to q1. Similarly, besides the query bigram q1q2, SWDM also includes the weighted matching scores for: (1) the bigrams q1q21, q1q22, q11q2, and q12q2, which have only one of their constituent terms not from the original query (2) the bigrams q11q21, q11q22, q12q21, and q12q22, in which both constituent terms are not from the original query. If Eqi , {qi0} ,"" {qi } (i.e., in the case when no semantically similar unigrams are in the neighborhood of orig-""",0,,False
"inal query terms), SWDM only takes into account the unigrams and",0,,False
bigrams in the original query (i.e. is identical to WSDM).,0,,False
"e importance weight of each query concept is computed based on several features. For an expansion unigram qij , we use its cosine similarity with qi , frequency in the collection and top retrieved documents, document frequency in the collection and top documents as features. For an expansion bigram qijqij+1, we use an average cosine similarity of the terms qij and qij+1, sequential and window based co-occurrence frequency in the collection and top",0,,False
"Figure 2: Graphical representation of SWDM+, a variant of SWDM, which besides the neighbors of the query terms in the embedding space (q11, . . . , q32), also incorporates the unigrams from the top retrieved documents ranked by RM1 [11] (q^11, . . . , q31).",1,corpora,True
documents as well as sequential and window based document frequency in the collection and top documents.,0,,False
4 EXPERIMENTS,0,,False
"Experimental results reported below were obtained using word2vec word embeddings with 300 dimensions that were pre-trained on the Google News corpus3, Indri-5.10 IR toolkit4 and GenSim library5 for all word embedding computations.",0,,False
"Retrieval accuracy of SWDM was evaluated with respect to Mean Average Precision (MAP) and Precision@10 (P@10) on ROBUST04, GOV2 and ClueWeb09B (excluding the documents for which Waterloo Fusion spam score was greater than 70) collections and compared with the state-of-the-art MRF retrieval models (SDM [12] and WSDM [5]), PRF methods for MRF retrieval models (LCE [13] and",1,MAP,True
3h ps://drive.google.com/ le/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/ 4h ps://www.lemurproject.org/indri/ 5h ps://radimrehurek.com/gensim/,1,ad,True
1215,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
PQE [6]) and embedding based query expansion method for bag-ofwords retrieval models (EEQ1 [19]). Collection and document frequencies were used as features to calculate the weights of query concepts in WSDM and PQE.,0,,False
"We also experimented with SWDM+, a variant of SWDM, which, besides the neighbors of query terms in the embedding space, also incorporates E^Q ,"" {q^1, q^2, . . . , q^k }, top k unigrams from the top retrieved documents according to the relevance model (RM1) [11] scores, as illustrated in Figure 2. SWDM+ uses the same set of features for weighting query concepts as SWDM. e similarity features to determine the importance of query concepts involving unigrams from top retrieved documents are calculated with respect to the closest query term in the embedding space. SWDM+RM1 and EEQ1+RM1 are a linear interpolation of RM1 with SWDM and EEQ1, respectively. Unigrams from the top retrieved documents in SWDM+RM1, EEQ1+RM1 and SWDM+ are ranked using RM1 and the top k are selected to be incorporated into the query.""",1,corpora,True
"e parameters of all models have been optimized using threefold cross-validation and coordinate ascent to maximize MAP. e range of continuous and discrete model parameters has been examined with the step sizes of 0.02 and 1, respectively.",1,MAP,True
4.1 Results,0,,False
"Table 1 provides a summary of retrieval accuracy for SWDM, its variants and the baselines6. As follows from the rst half of Table 1, SWDM outperforms SDM, WSDM and EQE1 in terms of both MAP and P@10, which indicates the utility of incorporating semantically related terms into WSDM. It also follows from Table 1 that the retrieval accuracy of EQE1 is close to that of SDM, which indicates that utilizing word embeddings for query expansion in conjunction with bag-of-words retrieval model results in similar improvements of retrieval accuracy as accounting for sequential dependencies between query terms. Our results also indicate that SWDM has be er retrieval accuracy than EQE1, since besides incorporating similar words from the embedding space into a query, it also takes into account the dependencies between the expansion terms as well as between the expansion terms and the query terms.",1,MAP,True
"e results in the second half of Table 1 indicate that incorporating unigrams from the top retrieved documents translates into a signi cant increase in retrieval accuracy of SWDM on ROBUST04 and GOV2 collections. In particular, SWDM+ outperforms both LCE and PQE, state-of-the-art PRF methods for MRF retrieval models, which include a separate potential function for expansion terms, but do not take into account neither dependencies between the expansion terms nor between the expansion terms and the original query terms, and EQE1+RM1, which is designed for bag-of-words retrieval models. SWDM+, however, has inferior performance to both SWDM and SWDM+RM1 on ClueWeb09B, which is due to relatively low accuracy of all retrieval models on this collection and, as a result, noisy unigrams from the top retrieved documents that are used for query expansion. is result suggests that the relative in uence of query term neighbors and the expansion terms from the top retrieved documents on retrieval accuracy depends on a collection and the quality of the initial retrieval results. SWDM+ also demonstrated a signi cantly statistical improvement in retrieval",1,corpora,True
6code and runs are available at h p://github.com/teanalab/SWDM,0,,False
"accuracy over SWDM+RM1 on ROBUST04 and GOV2 collections, in-",0,,False
dicating that the features based on similarity of expansion and the,0,,False
original query terms in the embedding space have a positive e ect,0,,False
on retrieval accuracy.,0,,False
5 CONCLUSION,0,,False
"In this paper, we proposed Semantic Weighted Dependence Model,",0,,False
which allows to address the vocabulary gap in Weighted Sequen-,1,ad,True
"tial Dependence Model, by leveraging distributed word represen-",0,,False
"tations (i.e. word embeddings) in two di erent ways. On one hand,",0,,False
word embeddings are used for calculating distributional similarity,0,,False
to nd the terms that are semantically similar to query terms for,0,,False
"query expansion. On the other hand, they are used as features",0,,False
to calculate the importance of query concepts. We also proposed,0,,False
"an extension of SWDM, which besides semantically similar terms,",0,,False
also incorporates the terms from the top retrieved documents.,1,corpora,True
REFERENCES,0,,False
[1] Saeid Balaneshin-kordan and Alexander Kotov. 2016. Optimization method for weighting explicit and latent concepts in clinical decision support queries. In Proceedings of ACM ICTIR. 241­250.,0,,False
[2] Saeid Balaneshin-kordan and Alexander Kotov. 2016. Sequential query expansion using concept graph. In Proceedings of ACM CIKM. 155­164.,0,,False
[3] Saeid Balaneshin-kordan and Alexander Kotov. 2016. A study of document expansion using translation models and dimensionality reduction methods. In Proceedings of ACM ICTIR. 233­236.,0,,False
[4] Saeid Balaneshinkordan and Alexander Kotov. 2016. An Empirical comparison of term association and knowledge graphs for query expansion. In Proceedings of ECIR. 761­767.,0,,False
"[5] Michael Bendersky, Donald Metzler, and W Bruce Cro . 2010. Learning concept importance using a weighted dependence model. In Proceedings of ACM WSDM. 31­40.",0,,False
"[6] Michael Bendersky, Donald Metzler, and W Bruce Cro . 2011. Parameterized concept weighting in verbose queries. In Proceedings of ACM SIGIR. 605­614.",0,,False
"[7] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In Proceedings of ACM CIKM. 55­ 64.",1,hoc,True
"[8] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. Semantic Matching by Non-Linear Word Transportation for Information Retrieval. In Proceedings of ACM CIKM. 701­710.",0,,False
[9] Alexander Kotov and ChengXiang Zhai. 2011. Interactive sense feedback for di cult queries. In Proceedings of ACM CIKM. 163­172.,0,,False
[10] Alexander Kotov and ChengXiang Zhai. 2012. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for di cult queries. In Proceedings of ACM WSDM. 403­412.,0,,False
[11] Victor Lavrenko and W Bruce Cro . 2001. Relevance based language models. In Proceedings of ACM SIGIR. 120­127.,0,,False
[12] Donald Metzler and W Bruce Cro . 2005. A Markov random eld model for term dependencies. In Proceedings of ACM SIGIR. 472­479.,0,,False
[13] Donald Metzler and W Bruce Cro . 2007. Latent concept expansion using markov random elds. In Proceedings of ACM SIGIR. 311­318.,0,,False
[14] Jay M Ponte and W Bruce Cro . 1998. A language modeling approach to information retrieval. In Proceedings of ACM SIGIR. 275­281.,0,,False
"[15] Stephen E Robertson, Steve Walker, Susan Jones, Micheline M HancockBeaulieu, Mike Gatford, and others. 1995. Okapi at TREC-3. NIST 109 (1995).",1,TREC,True
[16] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to rank short text pairs with convolutional deep neural networks. In Proceedings of ACM SIGIR. 373­382.,0,,False
[17] Ivan Vulic´ and Marie-Francine Moens. 2015. Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings. In Proceedings of ACM SIGIR. 363­372.,0,,False
"[18] Di Wang and Eric Nyberg. 2015. A long short-term memory model for answer sentence selection in question answering. Proceedings of ACL (2015), 707­712.",0,,False
[19] Hamed Zamani and W Bruce Cro . 2016. Embedding-based ery Language Models. In Proceedings of ACM ICTIR. 147­156.,0,,False
[20] Guoqing Zheng and Jamie Callan. 2015. Learning to reweight terms with distributed representations. In Proceedings of ACM SIGIR. 575­584.,0,,False
"[21] Guido Zuccon, Bevan Koopman, Peter Bruza, and Leif Azzopardi. 2015. Integrating and evaluating neural word embeddings in information retrieval. In Proceedings of ADCS. 12.",0,,False
1216,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Experiments with Convolutional Neural Network Models for Answer Selection,0,,False
"Jinfeng Rao, Hua He",0,,False
Department of Computer Science University of Maryland,0,,False
"jinfeng@cs.umd.edu,huah@umd.edu",0,,False
Jimmy Lin,0,,False
David R. Cheriton School of Computer Science University of Waterloo jimmylin@uwaterloo.ca,0,,False
ABSTRACT,0,,False
"In recent years, neural networks have been applied to many text processing problems. One example is learning a similarity function between pairs of text, which has applications to paraphrase extraction, plagiarism detection, question answering, and ad hoc retrieval. Within the information retrieval community, the convolutional neural network model proposed by Severyn and Moschi i in a SIGIR 2015 paper has gained prominence. is paper focuses on the problem of answer selection for question answering: we a empt to replicate the results of Severyn and Moschi i using their open-source code as well as to reproduce their results via a de novo (i.e., from scratch) implementation using a completely di erent deep learning toolkit. Our de novo implementation is instructive in ascertaining whether reported results generalize across toolkits, each of which have their idiosyncrasies. We were able to successfully replicate and reproduce the reported results of Severyn and Moschi i, albeit with minor di erences in e ectiveness, but a rming the overall design of their model. Additional ablation experiments break down the components of the model to show their contributions to overall e ectiveness. Interestingly, we nd that removing one component actually increases e ectiveness and that a simpli ed model with only four word overlap features performs surprisingly well, even be er than convolution feature maps alone.",1,ad,True
1 INTRODUCTION,1,DUC,True
"e problem of learning a similarity function between pairs of texts has broad applicability to a variety of natural language processing tasks, including paraphrase extraction, plagiarism detection, question answering, and ad hoc retrieval. e natural language processing community has made substantial strides in tackling this problem with approaches based on neural networks. In contrast to previous work that relies heavily on hand-cra ed features, models based on neural networks are not only more e ective, but also obviate the need for feature engineering.",1,ad,True
"Recently, Severyn and Moschi i [14] (henceforth, SM for short) proposed a convolutional neural network model for learning similarities between pairs of short texts, which was applied to answer selection for question answering and to tweet reranking in an ad",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080648",1,ad,True
"hoc retrieval task. Because this work has gained prominence in the information retrieval community (garnering, for example, a large number of citations in a short amount of time), in this paper we a empted to replicate and reproduce the authors' results.",1,hoc,True
"Our e orts proceeded in two steps: First, SM released their source code on GitHub,1 which uses the eano deep learning toolkit,2 and from this we a empted to replicate the results in their paper--by running their code directly. Second, we set aside their code and a empted to reproduce their model de novo (i.e., from scratch) using the Torch deep learning toolkit.3 Both e orts are instructive for di erent reasons: Replicating the original results using the authors' code veri es that the code does indeed correspond to the models described in the paper, and that there are no ""hidden dependencies"" missing in the code. Reproducing the model from scratch, using a completely di erent toolkit, represents a higher standard of veri ability--that the authors have adequately described their model, and that their results are robust with respect to idiosyncrasies in the underlying deep learning toolkits.",1,ad,True
Contributions. We view our work as having three contributions:,0,,False
"· We were able to successfully replicate and reproduce the results of Severyn and Moschi i, contributing to the community's growing interests in reproducibility and related issues [2, 10, 13].",0,,False
"· Our ablation studies reveal new insights about the inner workings of the SM model in terms of understanding the contribution of di erent components: convolution feature maps, a similarity modeling component, and ""extra features"" that derive from traditional retrieval measures. Interestingly, we nd that removing the similarity modeling component actually increases e ectiveness in answer selection and that a simpli ed model with only four word overlap features performs surprisingly well, even be er than the complete SM model without the extra features.",1,ad,True
"· We make our source code publicly available4 so that others can build on our work. Having publicly-available implementations of the same model in two di erent deep learning toolkits is instructive for developers trying to understand the intricacies of neural network models and the primitive ""building blocks"" o ered by competing toolkits.",0,,False
2 MODEL OVERVIEW,0,,False
2.1 Network Architecture,0,,False
"As this paper primarily focuses on the reproducibility of results, we introduce the model architecture brie y and refer the reader to the",1,ad,True
1h ps://github.com/aseveryn/deep-qa 2h p://deeplearning.net/so ware/theano/ 3h p://torch.ch/ 4h ps://github.com/castorini/SM-CNN-Torch,0,,False
1217,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
sentence matrix,0,,False
convolution feature maps !,0,,False
pooled representation!,0,,False
similarity matching!,0,,False
join ! layer!,0,,False
hidden ! layer!,0,,False
softmax!,0,,False
document!,0,,False
Fd,0,,False
The! cat! sat! on! the! mat!,0,,False
xd,0,,False
xsim M,0,,False
quer !y,0,,False
Fq,0,,False
xq,0,,False
Where! was! the! cat! ?!,0,,False
additional ! features xfeat,1,ad,True
"Figure 1: e SM convolutional neural network model, comprised of three main components: convolution feature maps with pooling, a similarity matrix, and extra features.",0,,False
"original paper for more details. e overall architecture of the SM model is shown in Figure 1, taken from the original paper for clarity.",0,,False
"e model has a general ""Siamese"" structure [3] with two subnetworks processing the ""query"" and the ""document"" in parallel. is general architecture is fairly common and used in a variety of other models as well [6­8]. Implicit in this architecture is the assumption that both inputs are relatively short (i.e., sentences), since they are ultimately converted into xed-length vector representations.",0,,False
"e input to each ""arm"" in the neural network is a sequence of words [w1, w2, ...w |S |], each of which is translated into its corresponding distributional vector (i.e., from a word embedding), yielding a sentence matrix. Convolution feature maps are applied to this sentence matrix, followed by ReLU activation and simple max-pooling, to arrive at a representation vector xq for the query and xd for the document.",0,,False
"At the join layer (see Figure 1), all intermediate representations are concatenated into a single vector:",0,,False
"xjoin , [xTq ; xsim; xTd ; xTfeat]",0,,False
(1),0,,False
where xsim de nes the bilinear similarity between xq and xd as,0,,False
follows:,0,,False
"sim(xq , xd ) , xTq Mxd",0,,False
(2),0,,False
is similarity matrix M is a parameter of the network that is,0,,False
optimized during training.,0,,False
e bilinear similarity matrix can be viewed as an adaptation,1,ad,True
"of the noisy-channel model from machine translation, which has",0,,False
"previously been used for question answering [4]. Basically, it cap-",0,,False
"tures a transformation from the source embedding xd to the target embedding xd , Mxd where similarity to the input query xq is maximized. e parameters are initialized randomly and optimized",0,,False
"through back-propagation during training. Finally, xfeat represents additional features that are task speci c--these are signi cant, as",1,ad,True
we discuss later.,0,,False
e original SM paper examined two speci c tasks: answer se-,0,,False
lection for question answering and tweet reranking in an ad hoc,1,ad,True
"retrieval task. In this paper, we only focus on the rst task for a",0,,False
"number of reasons. Primarily, there are aspects of the SM model",0,,False
that make its setup somewhat unrealistic for tweet reranking (as,0,,False
"described in their paper): speci cally, the model uses as one of the",0,,False
extra features the normalized rank across all systems that partici-,0,,False
pated in the TREC Microblog evaluations. is setup is closer to an,1,TREC,True
Set,0,,False
#,0,,False
TRAIN TRAIN-ALL,0,,False
Dev Test,0,,False
estion,0,,False
"94 1,229",0,,False
84 100,0,,False
# Answers,0,,False
"4,718 53,417 1,148 1,517",0,,False
% Correct,0,,False
7.4% 12.0% 19.3% 18.7%,0,,False
"Table 1: Statistics of the TrecQA dataset for answer selection, which contains two distinct training sets.",0,,False
"oracle run fusion task than an actual tweet reranking task, since a system would not have access to scores from all other systems that participated in the evaluation. We have begun to examine the SM model on the tweet reranking task, but the page limitations of a short paper preclude us from diving into the full details.",0,,False
2.2 Answer Selection,0,,False
"Answer selection is an important component of an overall question answering system: given a question q and a candidate set of sentences {c1, c2, . . . cn }, the task is to identify sentences that contain the answer. In a standard pipeline architecture [15], answer selection is applied to the output of a module that performs passage retrieval based on some form of term matching. Selected sentences can then be directly presented to users or serve as input to subsequent stages that identify ""exact"" answers [16]. e experiments in this paper evaluate the answer selection task for question answering. Although nominally a classi cation task, system output is usually evaluated in terms of ranked retrieval metrics.",0,,False
"For answer selection, the SM model uses ""extra features"" xfeat comprised of four word overlap measures between each sentence pair: word overlap and IDF-weighted word overlap computed between all words and only non-stopwords. e use of these external features aims to mitigate two issues associated with word embeddings. First, about 15% of words in the vocabulary are not found in the word embeddings; those word vectors are randomly initialized during training, which could result in suboptimal similarity learning. Second, even for those words found in the word embeddings, distributional representations sometimes can not capture the relatedness of numbers and proper nouns. However, such information is important, especially for factoid questions where many answers are either numbers or proper nouns.",0,,False
3 EXPERIMENTS,0,,False
"e SM paper evaluates their convolutional neural network on the popular TrecQA dataset, rst introduced by Wang et al. [19] and further elaborated by Yao et al. [20]. Basic statistics are shown in Table 1. e dataset contains a number of factoid questions, each of which is associated with candidate sentences that either contain or do not contain the answer (i.e., positive and negative examples).",0,,False
"e questions are taken from the estion Answering Tracks from TREC 8­13, and the candidate answers derive from the output of track participants. Note that there are two distinct training sets, known as TRAIN and TRAIN-ALL. e TRAIN-ALL set is much larger but also contains more noise. Following previous work, the task is evaluated in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR).",1,Track,True
We rst a empted to replicate the results of SM using the authors' open-source eano code. Results are reported in Table 2: e,0,,False
1218,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Condition,0,,False
TRAIN (full model) TRAIN (-xfeat) TRAIN-ALL (full model) TRAIN-ALL (-xfeat),0,,False
MAP Reported,1,MAP,True
0.7329 0.6258,0,,False
0.7459 0.6709,0,,False
eano,0,,False
0.7325 0.6271,0,,False
0.7538 0.6817,0,,False
Torch,0,,False
0.7318 0.6408,0,,False
0.7428 0.6841,0,,False
MRR Reported,0,,False
0.7962 0.6591,0,,False
0.8078 0.7280,0,,False
eano,0,,False
0.8018 0.6570,0,,False
0.8078 0.7249,0,,False
Torch,0,,False
0.7950 0.6780,0,,False
0.8079 0.7398,0,,False
"Table 2: Attempts to replicate and reproduce SM results. ""Reported"" columns list results in the original paper. "" eano"" columns list results from running the eano code provided by SM. ""Torch"" columns list results from our de novo Torch implementation of the model. Table also presents results of removing the ""extra features"" from the joined representation.",0,,False
"columns marked ""Reported"" contain results copied directly from their SIGIR paper. e columns marked "" eano"" contain results from our replication e orts, following the original experimental conditions. e original paper conducted an ablation analysis removing the extra features, which we also a empt to replicate (the -xfeat condition in the table). Although we do not obtain exactly the same numbers for the di erent conditions, the results are quite close. Overall, we consider this replication a empt successful.",0,,False
"Next, we a empted to reproduce the SM model from scratch using the Torch toolkit, based as close as possible to the description in the SIGIR paper. In our e orts to reproduce the results, we a empted to match the se ings described in the paper as closely as possible: choice of word embeddings, the width of the convolution",0,,False
"lters, the number of feature maps, the optimization objective, the training regime, etc. ese results are also reported in Table 2 under the columns marked ""Torch"". Following the original paper and our replication study, we also performed an experimental run removing the extra features (the -xfeat condition in the table). In all cases, our numbers are quite close to both the eano replication results and the original reported numbers in the paper--despite a completely di erent codebase and the use of a di erent deep learning toolkit. We consider this reproduction e ort successful and can conclude that the model is robust to di erences in the underlying deep learning toolkits.",0,,False
"Comparing the two implementations, we believe that our Torch code is more succinct and readable than the original eano implementation. Our Torch implementation consists of about 50 lines of model code and about 200 lines of training code, while the eano implementation has around 1000 lines of code for constructing the model and another 500 lines for training the model. is is because the Torch framework provides lots of modular pieces (e.g., convolution feature maps and a bilinear similarity modeling module) that are easy to combine, while the eano version implements all these components from scratch. Although eano o ers the nice abstraction of a computation graph that supports automatic gradient computation, writing the forward computations of complex modules remains burdensome. In terms of performance, both the Torch and eano implementations are very e cient, usually taking no more than 2­3 minutes per epoch when running on ve CPU threads on a standard commodity server. We typically reach convergence in 10­20 epochs.",1,ad,True
"In Tables 3a and 3b, we report a number of results on the same experimental conditions from the literature: these are taken from an ACL wiki page that nicely summarizes the state of the art in this answer selection task [1]. We see that, without the extra features,",1,wiki,True
Reference,0,,False
MAP,1,MAP,True
Yih et al. [21] (2013) 0.709,0,,False
He et al. [6] (2015) 0.717,0,,False
SM (full model),0,,False
0.732,0,,False
(a) TRAIN,0,,False
MRR,0,,False
0.770 0.800 0.795,0,,False
Reference,0,,False
MAP,1,MAP,True
Wang et al. [17] (2015) Miao et al. [11] (2015) He et al. [6] (2015) He and Lin [7] (2016) Rao et al. [12] (2016) SM (full model),0,,False
0.713 0.734 0.762 0.755 0.780 0.743,0,,False
(b) TRAIN-ALL,0,,False
MRR,0,,False
0.791 0.812 0.830 0.825 0.834 0.808,0,,False
"Table 3: E ectiveness of the SM model on the TrecQA dataset under the TRAIN and TRAIN-ALL conditions, compared to other results from the literature.",0,,False
"the SM model performs well below the state of the art. With the complete model, we can see that its e ectiveness is reasonably competitive but still below the best system today.",0,,False
3.1 Ablation Study,0,,False
"At a high-level, the SM model consists of three distinct components: the output of the convolution feature maps a er pooling xq and xd, the xsim similarity matrix, and the ""extra features"" xfeat. From a philosophical perspective, it can be argued that the ""extra"" word overlap features go against the ""spirit"" of neural network modeling, in that one of the major advantages of neural networks is the avoidance of manual feature engineering (which is what these extra features represent). Instead, one hopes that the network would discover correlates of these features automatically. As a point of comparison, the work of He et al. [6] requires no additional input features other than the original sentences.",1,ad,True
"To be er understand the contributions of the various components to e ectiveness, we build on the ablation experiments in the previous section, removing each of the components in turn and in combination. For these experiments, we used our Torch implementation. Results are shown in Table 4.",0,,False
"e rst three columns are the three categories of features used in the fully-connected layer, with a ""-"" and ""+"" entry denoting the removal or inclusion of each type of feature. e remaining columns show MAP and MRR scores on the TRAIN and TRAIN-ALL conditions. Note that in addition to the bilinear similarity modeling",1,MAP,True
1219,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
TRAIN,0,,False
TRAIN-ALL,0,,False
"xq, xd xfeat xsim MAP MRR MAP MRR",1,MAP,True
- 0.638 0.669 0.631 0.690,0,,False
+,0,,False
-,0,,False
bilinear 0.641 0.678 0.684 0.740 cosine 0.661 0.720 0.644 0.727,0,,False
dot 0.636 0.681 0.655 0.707,0,,False
-+,0,,False
- 0.648 0.716 0.649 0.709,0,,False
- 0.747 0.812 0.762 0.815,0,,False
+,0,,False
+,0,,False
bilinear 0.732 0.795 0.743 0.808 cosine 0.735 0.774 0.742 0.804,0,,False
dot 0.684 0.740 0.735 0.794,0,,False
Table 4: Results of an ablation analysis removing di erent components of the SM model.,0,,False
"(xsim ,"" xTq Mxd ) used in the original SM model, we also replaced it with a simple cosine similarity and dot product between the query and document feature maps. ese alternative formulations of xsim are also shown in the third column.""",0,,False
"Looking at the rst block of rows where the extra features xfeat are discarded, we see that adding the similarity feature xsim consistently improves e ectiveness. is is no surprise as answer selection is basically a matching task, and thus the joint representation of xsim contributes additional signal beyond the query and document features. Comparing the TRAIN and TRAIN-ALL conditions, we observe a larger improvement in the data-rich condition.",1,ad,True
"In the second block of rows, we show results with only the four word overlap features. is condition essentially uses the fullyconnected layer as a learning-to-rank module, which reduces the number of model parameters from 100K to 1200. Surprisingly, this simple feature engineering baseline works well, and actually be er than the SM model without the extra features (Row ""+ - bilinear""). Note that this simple model still contains two layers with non-linear activation between them, so it is learning useful nonlinear transformations of the input features for the task. is nding suggests that the lexical matching signals captured in word overlaps are more e ective than the convolution feature maps alone (given the amount of training data we have), which seems consistent with the ndings of Guo et al. [5]. Note that this simple baseline is more e ective than all approaches prior to around 2013 (e.g., [9, 18, 19]), according to the ACL Wiki [1].",1,Wiki,True
"e third block of rows shows the results of varying the xsim component while retaining the other two. An interesting nding is that the model achieves the best e ectiveness in both the TRAIN and TRAIN-ALL conditions when the similarity feature is removed. In other words, removing xsim actually makes the model be er than the full model described by SM! Furthermore, looking at the rst and third blocks of rows, we see that cosine similarity is generally comparable to bilinear similarity for the xsim component. Cosine similarity, however, is far simpler in requiring no parameter tuning. Dot product similarity is consistently less e ective due to its lack of normalization.",0,,False
"e above nding, combined with the ""+ - -"" and ""- + -"" conditions, suggests that the e ectiveness of the lexical word overlap features and the convolution feature maps are additive--these two components together explain the e ectiveness of the SM model.",1,ad,True
4 CONCLUSIONS,0,,False
ere is no doubt that deep learning applied to information retrieval,0,,False
"is a ""hot"" emerging area, which makes it even more important to",0,,False
verify published results and to ensure that we as a community are,0,,False
"building on a solid foundation. In this work, we have successfully",0,,False
replicated and reproduced a recent SIGIR paper that has gained,0,,False
"prominence: for answer selection in question answering, we have",0,,False
"not only validated the design of the SM model, but also deepened",0,,False
our understanding of how its various components interact and,0,,False
contribute to overall e ectiveness.,0,,False
REFERENCES,0,,False
"[1] ACL. 2017. estion Answering (State of the art). h p://www.aclweb.org/ aclwiki/index.php?title, estion Answering (State of the art). Accessed: 201705-01.",1,wiki,True
"[2] Jaime Arguello, Ma Crane, Fernando Diaz, Jimmy Lin, and Andrew Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR). SIGIR Forum 49, 2 (2015), 107­116.",0,,False
"[3] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sa¨ckinger, and Roopak Shah. 1993. Signature Veri cation Using a ""Siamese"" Time Delay Neural Network. In NIPS. 737­744.",0,,False
[4] Abdessamad Echihabi and Daniel Marcu. 2003. A Noisy-Channel Approach to estion Answering. In ACL. 16­23.,1,ad,True
"[5] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM. 55­64.",1,hoc,True
"[6] Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks. In EMNLP. 1576­1586.",0,,False
[7] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement. In NAACL. 937­948.,0,,False
"[8] Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, and Jimmy Lin. 2016. UMDTTIC-UW at SemEval-2016 Task 1: A ention-Based Multi-Perspective Convolutional Neural Networks for Textual Similarity Measurement. In SemEval. 662­667.",0,,False
"[9] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to estions. In HLT-NAACL. 1011­1019.",0,,False
"[10] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",1,ad,True
"[11] Yishu Miao, Lei Yu, and Phil Blunsom. 2015. Neural Variational Inference for Text Processing. arXiv:1511.06038.",0,,False
"[12] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In CIKM. 1913­1916.",0,,False
"[13] Jinfeng Rao, Jimmy Lin, and Miles Efron. 2015. Reproducible Experiments on Lexical and Temporal Feedback for Tweet Search. In ECIR. 755­767.",1,Tweet,True
[14] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. 373­382.,0,,False
"[15] Stefanie Tellex, Boris Katz, Jimmy Lin, Gregory Marton, and Aaron Fernandes. 2003. antitative Evaluation of Passage Retrieval Algorithms for estion Answering. In SIGIR. 41­47.",0,,False
[16] Ellen M. Voorhees. 2002. Overview of the TREC 2002 estion Answering Track. In TREC.,1,TREC,True
[17] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In ACL. 707­712.,0,,False
[18] Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and estion Answering. In COLING. 1164­1172.,0,,False
"[19] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32.",0,,False
"[20] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In HLT-NAACL. 858­867.",0,,False
"[21] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. estion Answering Using Enhanced Lexical Semantic Models. In ACL. 1744­",0,,False
1753.,0,,False
Acknowledgments. is research was supported by the Natural Sciences,0,,False
"and Engineering Research Council (NSERC) of Canada, with additional con-",1,ad,True
tributions from the U.S. National Science Foundation under CNS-1405688.,0,,False
We'd like to thank Aliaksei Severyn for sharing additional details about the,1,ad,True
SM model and for commenting on an earlier dra of this paper.,0,,False
1220,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Luandri: A Clean Lua Interface to the Indri Search Engine,0,,False
Bhaskar Mitra,0,,False
"Microso , University College London Cambridge, UK",0,,False
bmitra@microso .com,0,,False
Fernando Diaz,0,,False
"Spotify New York, USA diazf@acm.org",0,,False
Nick Craswell,0,,False
"Microso Bellevue, USA nickcr@microso .com",0,,False
ABSTRACT,0,,False
"In recent years, the information retrieval (IR) community has witnessed the rst successful applications of deep neural network models to short-text matching and ad-hoc retrieval tasks. However, the two communities--focused on deep neural networks and on IR-- have less in common when it comes to the choice of programming languages. Indri, an indexing framework popularly used by the IR community, is wri en in C++, while Torch, a popular machine learning library for deep learning, is wri en in the light-weight scripting language Lua. To bridge this gap, we introduce Luandri (pronounced ""laundry""), a simple interface for exposing the search capabilities of Indri to Torch models implemented in Lua.",1,ad-hoc,True
CCS CONCEPTS,0,,False
·Information systems Information retrieval; Web searching and information discovery; ·Computing methodologies Neural networks;,0,,False
KEYWORDS,0,,False
Information retrieval; application programming interface; neural networks,0,,False
1 INTRODUCTION,1,DUC,True
"In recent years, deep neural networks (DNNs) have demonstrated early positive results on a variety of standard information retrieval (IR) tasks, including on short-text matching [10, 11, 14, 19, 21, 22] and ad-hoc retrieval [9, 17], and shown promising performances on other emerging retrieval tasks such as multi-modal retrieval [15] and conversational IR [28, 30]. is work occurs at the intersection of the machine learning and information retrieval communities, who have di erent research tools that are implemented in di erent programming languages. Popular neural network toolkits are o en implemented in (or have bindings for) scripting languages, such as Python1 (e.g., TensorFlow [1], eano [2], CNTK [29], Ca e [13], MXNet [3], Chainer [25], and PyTorch2) or Lua [12] (e.g., Torch [4])",1,ad-hoc,True
 e author is a part-time PhD student at UCL. Work done while at Microso . 1h ps://www.python.org/ 2h ps://github.com/pytorch/pytorch,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080650",1,ad,True
"because of their rapid prototyping capabilities. In contrast, many popular indexing frameworks for IR are implemented in C++ (e.g., Indri [24]) or Java (e.g., Terrier [18] and Apache Lucene [16]). e open-source community has developed Python wrappers over the Indri [26] and the Apache Lucene [20] programming interfaces to expose the functionalities of these rich IR libraries to the programming language. However, there is still a gap that remains to be bridged for non-Python based deep learning toolkits, such as Torch.",0,,False
"Torch3 is a numeric computing framework popular among the deep neural network community. It has been shown to be significantly faster compared to other toolkits such as TensorFlow on convolutional neural networks in multi-GPU environment [23]. It is implemented using the light-weight scripting language Lua.4 In this paper, we introduce Luandri (pronounced ""laundry"") ­ a Lua wrapper over the Indri search engine. In particular, Luandri exposes parts of the Indri query environment application programming interface (API) for document retrieval including support for the rich Indri query language.",1,AP,True
2 MOTIVATION,0,,False
"ere are a variety of scenarios in which a DNN model can bene t from having access to a search engine during training and/or evaluation. Existing DNN models for ad-hoc retrieval [9, 17], for example, operate on query-document pairs to predict relevance. Running these models on the full corpus is prohibitively costly ­ therefore the evaluation of these models is o en limited to re-ranking topN candidate documents retrieved by a traditional IR model or a search engine. Typically, these candidate sets are retrieved o ine in a process separate from the one in which the DNN is evaluated. However, if the search engine is accessible in the same language as the one in which the DNN is implemented, then the candidate generation step and the DNN-based re-ranking step can follow each other within the same process ­ removing the requirement to store large quantity of intermediate datasets containing the candidates to be ranked.",1,ad-hoc,True
"DNN models train on labelled data, although in some cases labels can be inferred rather than explicit. For example, many DNN models for IR [9­11, 14, 22] use negative training examples that are sampled uniformly from the corpus. Recently Mitra et al. [17] reported that training with judged negative documents can yield be er NDCG performance than training with uniform random negatives. Having access to a search engine during training could enable additional methods for generating negative samples, such as using documents that are retrieved by the engine but at lower ranks.",1,ad,True
3h ps://github.com/torch/torch7 4h ps://www.lua.org,0,,False
1221,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Code snippet 1: Sample Lua code for searching an Indri index using the Luandri API. e Indri index builder application is used for generating the index beforehand. e search query is written using the popular INQUERY structured operators that are supported natively by Indri for specifying matching constraints. e run ery method in the Luandri API accepts the request as a Lua table and automatically converts it into the appropriate C++ request object that Indri natively expects. Similarly, the result object returned by Indri in C++ is automatically converted to a Lua table.",1,AP,True
1,0,,False
"local luandri , paths.dofile( luandri.lua )",0,,False
2,0,,False
"local query_environment , QueryEnvironment()",1,Query,True
3,0,,False
query_environment:addIndex( path_to_index_file ),1,ad,True
4,0,,False
5,0,,False
"local request , {",0,,False
6,0,,False
"query ,"" #syn( #od1(neural networks) #od1(deep learning)) #greater(year 2009) ,""",0,,False
7,0,,False
"resultsRequested , 10",0,,False
8,0,,False
},0,,False
9,0,,False
"local results , query_environment:runQuery(request).results",1,Query,True
10,0,,False
11,0,,False
"for k, v in pairs(results) do",0,,False
12,0,,False
print(v.docid .. ,0,,False
 .. v.documentName .. ,0,,False
 .. v.snippet .. ,0,,False
 ),0,,False
13,0,,False
end,0,,False
e lack of adequate labelled data available for training DNN models for ad-hoc retrieval has been a focus for the neural IR community [5]. It is possible that alternate strategies for supervision may be considered for training these deep models ­ including reinforcement learning [27] and training under adversarial se ings [8] ­ which could also make use of retrieval from a full corpus during the model training.,1,ad,True
"Diaz et al. [6] demonstrated a di erent application of the traditional retrieval step in the neural IR model. Given a query, they retrieve a set of documents using Indri and use that to train a brand new distributed representation of words speci c to that query at run time. Such models, with query-speci c representation learning, can be implemented and deployed more easily if the machine learning framework has access to a search engine.",1,ad,True
"Finally, Ghazvininejad et al. [7] proposed to ""lookup"" external repositories of facts as part of solving larger tasks using neural network models. Empowering DNN models with access to a search engine may be an exciting area for future exploration.",1,ad,True
"In all these scenarios, it is useful for a search engine, such as Indri, to be accessible from the same programming language used to implement the DNN. erefore, we are optimistic that by publicly releasing the Luandri API we will stimulate novel explorations from IR researchers already familiar with Torch.",1,AP,True
3 QUERYING INDRI FROM LUA,0,,False
Indri is an open-source search engine available with the Lemur toolkit.5 Indri consists of two primary components ­ an application that builds an index from a raw document collection and another application that can perform searches using this index. e Indri index builder can deal with several di erent document formats for,0,,False
5h p://www.lemurproject.org/indri/,0,,False
"indexing. is includes TREC (text and Web), HTML, XML, PDF, and plain text among many others.",1,TREC,True
"Searching using Indri involves specifying one or more indices and querying them by either interactively calling the API or by running an application in batch-mode. e Indri query language supports a rich set of operators for specifying phrasal matching conditions, synonymy relationships, document ltering criteria, and other complex constraints. e full query language grammar is available online for reference.6",1,AP,True
Invoking a search on an Indri index using the Luandri API is like how one may use the native C++ Indri API. Code snippet 1 shows a minimal example of a typical Indri-based search using the Luandri API. We observe that the search is performed by invoking very few lines of Lua code.,1,AP,True
"e example also demonstrates the use of Indri structured queries. A search is performed using a structured query that constraints the matching to either of the two ordered phrases ­ ""neural networks"" or ""deep learning"". e query directs Indri to treat both phrases as synonyms. In addition, a numeric lter is speci ed to limit matches to only documents whose value corresponding to the year eld is greater than 2009.",1,ad,True
"is example shows searching on the full document index. However, Luandri also allows users to specify a list of document identi ers in the request object to limit the search to only those set of documents. A xed list of stop words can also be speci ed for retrieval using the Luandri API.",1,AP,True
e full Luandri implementation is available on GitHub7 under the MIT license. We direct interested readers to the source code for exact API speci cations.,1,ad,True
6h ps://www.lemurproject.org/lemur/Indri eryLanguage.php 7h ps://github.com/bmitra-ms /Luandri,0,,False
1222,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
4 UNDER THE HOOD,0,,False
"e implementation of Lua as a programming language puts a strong emphasis on extensibility [12]. Lua is an extension language because any Lua code can be relatively easily embedded as libraries into code wri en in other languages. It is also an extensible language because of its ability to call functions wri en in other languages, such as C. e implementation of the Luandri API bene ts from the la er property of the language.",1,AP,True
Lua comes with a fast Just In Time (JIT) compiler called LuaJIT.8 LuaJIT exposes a foreign-function interface9 (FFI) that makes it easy to call external C functions and manipulate C data structures from Lua. e Luandri API is wri en using the LuaJIT FFI library.,1,AP,True
"Luandri API wraps Indri's query environment data types and methods by extern C functions. en using the LuaJIT's FFI library these C methods are exposed to any code wri en in Lua. Luandri automatically handles any conversions necessary between Lua tables and Indri's C++ objects, and vice versa. e ""Luandri.cpp"" and ""luandri.lua"" les contain all the wrapper logic on the C++ and the Lua side of our API code, respectively.",1,AP,True
"e current Luandri API exposes only some of the data structures and methods from Indri's query environment. In future, we hope to expose more of Indri's retrieval functionalities prioritizing based on the need of the broader research community.",1,AP,True
5 CONCLUSIONS,0,,False
"We introduced Luandri, a Lua API to the Indri search engine. Luandri brings to DNN models, implemented on Torch, the retrieval capabilities of Indri, including its powerful query language grammar. We posit that the capabilities of a search engine may be useful for training future DNN models for IR ­ for sampling negative examples, or for training under reinforcement or adversarial se ings. We hope that the release of Luandri will not only help researchers working on Torch models for IR, but also stimulate new research in novel DNN models that incorporate retrieval from an external knowledge base as an intermediate step towards solving larger tasks.",1,AP,True
REFERENCES,0,,False
"[1] Mart´in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Je rey Dean, Ma hieu Devin, and others. 2016. Tensor ow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).",1,ad,True
"[2] James Bergstra, Olivier Breuleux, Fre´de´ric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. eano: A CPU and GPU math compiler in Python. In Proc. 9th Python in Science Conf. 1­7.",0,,False
"[3] Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, and Zheng Zhang. 2015. Mxnet: A exible and e cient machine learning library for heterogeneous distributed systems. arXiv preprint arXiv:1512.01274 (2015).",0,,False
"[4] Ronan Collobert, Koray Kavukcuoglu, and Cle´ment Farabet. 2011. Torch7: A matlab-like environment for machine learning. In BigLearn, NIPS Workshop.",0,,False
"[5] Nick Craswell, W Bruce Cro , Jiafeng Guo, Bhaskar Mitra, and Maarten de Rijke. 2016. Report on the SIGIR 2016 Workshop on Neural Information Retrieval (Neu-IR). 50, 2 (2016), 96­103.",0,,False
"[6] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with Locally-Trained Word Embeddings. arXiv preprint arXiv:1605.07891 (2016).",0,,False
"[7] Marjan Ghazvininejad, Chris Brocke , Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2017. A Knowledge-Grounded Neural Conversation Model. arXiv preprint arXiv:1702.01932 (2017).",1,ad,True
8h p://luajit.org 9h p://luajit.org/ext .html,0,,False
"[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. 2672­2680.",1,ad,True
"[9] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In Proc. CIKM. ACM, 55­64.",1,hoc,True
"[10] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Proc. NIPS. 2042­2050.",0,,False
"[11] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proc. CIKM. ACM, 2333­2338.",0,,False
"[12] Roberto Ierusalimschy, Luiz Henrique De Figueiredo, and Waldemar Celes Filho. 1996. Lua-an extensible extension language. So w., Pract. Exper. 26, 6 (1996), 635­652.",0,,False
"[13] Yangqing Jia, Evan Shelhamer, Je Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Ca e: Convolutional architecture for fast feature embedding. In Proceedings of the 22nd ACM international conference on Multimedia. ACM, 675­678.",1,ad,True
[14] Zhengdong Lu and Hang Li. 2013. A deep architecture for matching short texts. In Advances in Neural Information Processing Systems. 1367­1375.,0,,False
"[15] Lin Ma, Zhengdong Lu, Lifeng Shang, and Hang Li. 2015. Multimodal convolutional neural networks for matching image and sentence. In Proceedings of the IEEE International Conference on Computer Vision. 2623­2631.",0,,False
"[16] Michael McCandless, Erik Hatcher, and Otis Gospodnetic. 2010. Lucene in Action: Covers Apache Lucene 3.0. Manning Publications Co.",0,,False
"[17] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In Proc. WWW. 1291­1299.",0,,False
"[18] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Christina Lioma. 2006. Terrier: A high performance and scalable information retrieval platform. In Proceedings of the OSIR Workshop. 18­25.",1,ad,True
"[19] Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Shengxian Wan, and Xueqi Cheng. 2016. Text Matching as Image Recognition. In Proc. AAAI.",0,,False
[20] Andreas Schreiber. 2009. Mixing Python and Java. (2009). [21] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to rank short text,0,,False
"pairs with convolutional deep neural networks. In Proc. SIGIR. ACM, 373­382. [22] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014.",0,,False
"A latent semantic model with convolutional-pooling structure for information retrieval. In Proc. CIKM. ACM, 101­110. [23] Shaohuai Shi, Qiang Wang, Pengfei Xu, and Xiaowen Chu. 2016. Benchmarking State-of-the-Art Deep Learning So ware Tools. arXiv preprint arXiv:1608.07249 (2016). [24] Trevor Strohman, Donald Metzler, Howard Turtle, and W Bruce Cro . 2005. Indri: A language model-based search engine for complex queries. In Proceedings of the International Conference on Intelligent Analysis, Vol. 2. Citeseer, 2­6. [25] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. 2015. Chainer: a next-generation open source framework for deep learning. In Proceedings of",0,,False
"workshop on machine learning systems (LearningSys) in the twenty-ninth annual conference on neural information processing systems (NIPS). [26] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. 2017. Pyndri: a Python Interface to the Indri Search Engine. arXiv preprint arXiv:1701.00749 (2017). [27] Ronald J Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning 8, 3-4 (1992), 229­256. [28] Rui Yan, Yiping Song, and Hua Wu. 2016. Learning to respond with deep neural networks for retrieval-based human-computer conversation system. In",1,ad,True
"Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. ACM, 55­64. [29] Dong Yu, Adam Eversole, Mike Seltzer, Kaisheng Yao, Zhiheng Huang, Brian Guenter, Oleksii Kuchaiev, Yu Zhang, Frank Seide, Huaming Wang, and others. 2014. An introduction to computational networks and the computational network toolkit. Technical Report. Tech. Rep. MSR, Microso Research, 2014, h p://codebox/cntk. [30] Xiangyang Zhou, Daxiang Dong, Hua Wu, Shiqi Zhao, R Yan, D Yu, Xuan Liu, and H Tian. 2016. Multi-view response selection for human-computer conversation. EMNLP 16 (2016).",0,,False
1223,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Finally, a Downloadable Test Collection of Tweets",1,ad,True
Royal Sequiera and Jimmy Lin,0,,False
"David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada {rdsequie,jimmylin}@uwaterloo.ca",1,ad,True
ABSTRACT,0,,False
"Due to Twi er's terms of service that forbid redistribution of content, creating publicly downloadable collections of tweets for research purposes has been a perpetual problem for the research community. Some collections are distributed by making available the ids of the tweets that comprise the collection and providing tools to fetch the actual content; this approach has scalability limitations. In other cases, evaluation organizers have set up APIs that provide access to collections for speci c tasks, without exposing the underlying content. is is a workable solution, but di cult to sustain over the long term since someone has to maintain the APIs. We have noticed that the non-pro t Internet Archive has been making available for public download captures of the so-called Twi er ""spritzer"" stream, which is the same source as the Tweets2013 collection used in the TREC 2013 and 2014 Microblog Tracks. We analyzed both datasets in terms of content overlap and retrieval baselines to show that the Internet Archive data can serve as a dropin replacement for the Tweets2013 collection, thereby providing the research community with, nally, a downloadable collection of tweets. Beyond this nding, we also study the impact of tweet deletions over time and how they a ect the test collections.",1,ad,True
1 INTRODUCTION,1,DUC,True
"Test collections--comprised of a corpus of documents, a set of information needs, and associated relevance judgments--lie at the heart of the Cran eld Paradigm [2] for information retrieval research. In most cases, researchers can acquire the document collection under study: in the 1990s, these were on physical CD-ROMs or DVDs delivered via postal mail; today, hard drives are shipped instead. What if it were not possible to distribute document collections for research use? One example is a collection of tweets: Twi er's terms of service forbid redistribution of such data. is is not a Twi erspeci c problem, as similar challenges exist with electronic medical records, emails, and a host of other sensitive collections researchers may wish to study.",1,ad,True
"Over the past several years, the community has experimented with and developed alternative evaluation approaches for cases where the distribution of documents is challenging, collectively known as ""Evaluation as a Service"" (EaaS) [1, 3]. Speci cally for",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080667",1,ad,True
"tweets, TREC organizers have built a search API for researchers to perform evaluation tasks without bulk access to the raw collection [4]; this approach was deployed in both the TREC 2013 and TREC 2014 Microblog Track evaluations.",1,TREC,True
"e Internet Archive1--a nonpro t digital library with the mission of providing ""universal access to all knowledge""--has been making available captures of the so-called Twi er ""spritzer"" stream (an approximately 1% sample of public posts) for download. Putatively, this is the same source that was used to construct the Tweets2013 collection used in the TREC 2013 and 2014 Microblog Tracks. A natural question, therefore, is how this dataset compares to the o cial Tweets2013 collection and if it can be used as a dropin replacement for evaluation purposes. e main contribution of this paper is in answering these questions: we nd that, yes, the publicly downloadable Internet Archive data is substantially similar to the o cial Tweets2013 collection. We observe around 95% overlap in terms of tweet content, and retrieval baselines on the Internet Archive data yield e ectiveness that is statistically indistinguishable from the o cial API. us, the information retrieval community nally has access to a downloadable collection of tweets for research, obviating the need for the service API.",1,ad,True
"Beyond the contribution of validating a downloadable Twi er test collection, this paper also takes a closer look at deleted tweets.",1,ad,True
"e fact that users can delete their tweets means that any collection is constantly changing, and the size of the collection monotonically decreases over time (since there is no ""undelete"" option). We present an analysis of deleted tweets in the Tweets2013 collection over the past several years to quantitatively characterize the delete process and to examine e ects on retrieval e ectiveness. We nd that although the collection indeed degrades over time, and almost a h of tweets from the raw Tweets2013 collection have been deleted as of December 31, 2016, these deletes appear to have minimal impact on the integrity of the test collections built on the tweets.",1,Tweet,True
2 BACKGROUND AND RELATED WORK,0,,False
"Restrictions on the redistribution of tweets have long been a hurdle to building test collections for information seeking on social media streams. e TREC Microblog Tracks, which ran from 2011 to 2015, have wrestled with this issue and experimented with two di erent solutions. e track organizers built the Tweets2011 collection that was used in TREC 2011 and 2012 [6]. To circumvent the no-redistribution limitation, the organizers devised a process whereby NIST distributed the ids of the tweets (rather than the tweets themselves). Given these ids and a downloading program developed by the organizers (essentially, a crawler), a participant could ""recreate"" the collection [5]. Since the downloading program",1,TREC,True
1h ps://archive.org/,0,,False
1225,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"accessed the twi er.com site directly, the tweets were delivered in accordance with Twi er's terms of service.",0,,False
"e ""download it yourself"" approach successfully addressed the no-redistribution issue for the purposes of a shared evaluation, as evidenced by 59 participating groups in the TREC 2011 Microblog Track (one of the largest ever in the history of TREC). Beyond TREC, this approach has been adopted by other communities for sharing collections of tweets. However, distribution via re-downloading exhibits scalability limitations. In particular, the speed of the downloading program, which has built-in rate limiting (imposed voluntarily for robotic ""politeness""), sets a practical upper bound on collection size. e Tweets2011 collection originally contained 16 million tweets, which is small by modern standards, especially considering that tweets are short.",1,ad,True
"e Tweets2011 collection also identi ed another issue with tweet collections in general, explored by McCreadie et al. [5]: they degrade over time due to deletes. Based on recrawls of the collection several months a er its original release, the authors concluded that the deletes did not impact the relative e ectiveness of runs submi ed to TREC 2011. However, to our knowledge, the e ects of deletes over much longer periods of time have not be studied.",1,Tweet,True
"In order to tackle the scalability challenges associated with the ""download it yourself"" approach, for the TREC 2013 Microblog Track the organizers implemented an evaluation-as-a-service solution. ey gathered a collection of tweets centrally, but instead of distributing the tweets, the organizers provided a service API through which participants could access the tweets to complete the evaluation task. To build the o cial collection, organizers developed an open-source crawler using the twi er4j Java library2 to gather tweets from Twi er's public sample stream,3 colloquially known as the ""spritzer"" stream. is level of access is available to anyone with a Twi er account and does not require special authorization. e organizers crawled all tweets between February 1 and March 31, 2013, UTC (inclusive). According to the TREC 2013 Microblog Track overview: e collection was gathered from two separate virtual machine instances on Amazon's EC2 service, one on the east coast of the US, and the other on the west coast of the US. e redundant setup guarded against network outages and other operational issues during the collection period. Fortunately, no downtime was experienced, so one of the copies was simply designated as the o cial collection. In total, the organizers reported gathering 259 million tweets, although at the time of the evaluation, the collection behind the API was reduced to 243 million tweets a er the removal of deleted tweets.",1,ad,True
"e API for accessing the Tweets2013 collection provided basic search capabilities using the open-source Lucene search engine. In addition to returning the text of the retrieved tweets, the API returned associated metadata about the time of the post, the user making the post, and other properties such as the number of retweets, whether the tweet was a reply, etc. Although the setup essentially limited participants to reranking tweets, this is not unlike multistage ranking architectures that are common today [9]. Additional meta-evaluations have shown that using the API does not appear to a ect the diversity of the submi ed runs [8] and a retrievability",1,AP,True
2h p://twi er4j.org/en/index.html 3h ps://dev.twi er.com/docs/streaming-apis,0,,False
Source,0,,False
|T | |A| |T  A| |T  A| |T - A| |A - T |,0,,False
Count,0,,False
"259,035,603 246,615,368 260,382,756 245,268,215 13,767,388",0,,False
"1,347,153",0,,False
"Table 1: Collection statistics, where T represents the raw Tweets2013 collection and A represents the Tweets2013-IA collection from the Internet Archive.",1,Tweet,True
analysis does not reveal any substantive issues that arise from not having access to the entire collection [7].,0,,False
"Although the evaluation-as-a-service approach is a workable solution for some tasks, the biggest challenge of the approach is sustainability over the long term, since someone must ultimately devote resources to the service, manage access, troubleshoot issues, etc. is is an open-ended commitment for the life of the collection: as a point of comparison, TREC test collections from the 1990s are still being used today. It is di cult to imagine anyone supporting the API for two decades. For one, the so ware behind the service will have long become obsolete.",1,TREC,True
3 COLLECTION STATISTICS,0,,False
"In this paper, we examine two tweet datasets available from the Internet Archive for public download:",1,ad,True
· ArchiveTeam JSON Download of Twi er Stream 2013-02: h ps://archive.org/details/archiveteam-twi er-stream-2013-02,1,ad,True
· ArchiveTeam JSON Download of Twi er Stream 2013-03: h ps://archive.org/details/archiveteam-twi er-stream-2013-03,1,ad,True
"According to the Internet Archive, the above datasets are:",0,,False
"A simple collection of JSON grabbed from the general Twi er stream, for the purposes of research, history, testing and memory. is is the ""Spritzer"" version, the most light and shallow of Twi er grabs.",0,,False
"Putatively, this is the same source that the Tweets2013 collection was created from. We downloaded these tweets and compared them against the o cial Tweets2013 collection (collected by the organizers). In Table 1 we present some basic collection statistics for the raw Tweets2013 collection, which we denote as T , and the above datasets downloaded from the Internet Archive, which we refer to as Tweets2013-IA and denote as A for short. Note that T is not the collection exposed via the o cial API, since deletes were applied to it before the TREC evaluations.",1,Tweet,True
"Twi er's streaming API is forma ed in JSON and comprises messages of two types: actual tweet content and delete messages. ese statistics consider tweet JSON messages only. Due to transient network issues, some messages are delivered more than once, and therefore all reported statistics in this paper are on unique counts. For all experiments in this paper, data manipulation is performed using Spark on our Hadoop cluster since the datasets are large; for reference, the raw Tweets2013 collection (including all tweets and deletes) is 107 GB compressed.",1,AP,True
"Overlap statistics between T and A are shown in Table 2. Most importantly, we see that there is approximately 95% overlap in tweet content between the publicly downloadable datasets from",1,ad,True
1226,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Collection,0,,False
Overlap,0,,False
1 - |(T - A)|/|T | 1 - |(A - T )|/|A|,0,,False
94.69% 99.45%,0,,False
Table 2: Overlap analysis between the Tweets2013 (T ) and Tweets2013-IA (A) collections.,1,Tweet,True
"the Internet Archive and the raw Tweets2013 collection. It seems that the la er is nearly a superset of the former, as there are very few tweets in A that are not in T .",1,Tweet,True
4 DELETION ANALYSIS,0,,False
"Per the Twi er Developer Agreement, one must ""delete content that Twi er reports as deleted or expired"".4 Alongside the tweet content, Twi er's streaming API also delivers delete messages. is means that, in order to precisely follow the agreement, gathering any Twi er content from the API also incurs an open-ended liability to monitor the stream inde nitely for delete messages.",1,AP,True
"From the perspective of IR evaluation, this means that any collection of tweets is unstable and will degrade over time--the collection size will monotonically decrease, since there is no ""undelete"" feature. Although McCreadie et al. [5] have previously examined this issue, their analysis was over a much smaller collection and a much shorter time span. Here, we characterize deletes on the raw Tweets2013 collection over a much longer period of time and examine its impact on associated test collections.",1,ad,True
"e deletion data in our analysis come from two long-term crawls of the Twi er spritzer stream from April 2013 through December 2016 (inclusive). Due to occasional crawler failures, we take the union of delete messages observed across both crawls as the ""ground truth"". e notation D (YY/MM-YY/MM) refers to deletes observed between the speci ed years and months, inclusive. D (13/02-13/03) is observed directly in the raw Tweets2013 collection, while all other deletes come from the sources described above.",1,Tweet,True
"Deletion statistics are shown in Table 3, where we provide numbers for a few noteworthy periods: We show the count of deletes that are directly observed as part of the collection (in February and March of 2013). e period from February to June 2013 (inclusive) captures the deletes that were applied for the service API made available for TREC 2013 and TREC 2014. Also of interest are the delete aggregates at yearly intervals, i.e., the counts of deletes through the end of 2013, 2014, 2015, and 2016.",1,AP,True
"In Table 3 we also show the e ects of removing the deleted tweets from the raw Tweets2013 collection T and also the Tweets2013-IA collection A. From the table, we see that by the end of 2016, deletes have reduced the collection to 211m for T and 199m for A, down from the original sizes of 259m and 247m, respectively. Figure 1 plots the number of deletes by month on both the raw Tweets2013 collection and the Internet Archive data. Although we do see that the number of deletes drops o a er the initial few months, there is still a substantial number of deletes even years a er the tweets were originally posted. e total size a er applying all deletes is shown in Figure 2; as expected, we see a steady degradation of the collection over time.",1,Tweet,True
e next obvious question is how these deletes a ect test collections from the TREC 2013 and 2014 Microblog Tracks that have,1,TREC,True
4h ps://dev.twi er.com/overview/terms/agreement-and-policy,0,,False
Source,0,,False
|T |,0,,False
|A| |D (13/02-13/03)| |D (13/04-13/06)| |D (13/07-13/12)| |D (14/01-14/12)| |D (15/01-15/12)| |D (16/01-16/12)| |T - D (13/02-13/03)| |A - D (13/02-13/03)| |T - D (13/02-13/06)| |A - D (13/02-13/06)| |T - D (13/02-13/12)| |A - D (13/02-13/12)| |T - D (13/02-14/12)| |A - D (13/02-14/12)| |T - D (13/02-15/12)| |A - D (13/02-15/12)| |T - D (13/02-16/12)| |A - D (13/02-16/12)|,0,,False
Count,0,,False
"259,035,603 246,615,368 10,631,099",0,,False
"5,091,183 7,197,460 96,98,613 7,928,857 7,496,871 248,404,504 234,337,730 243,313,321 230,893,086 236,115,861 223,695,626 226,417,248 213,997,013 218,488,391 206,068,156 210,991,520 198,571,285",0,,False
"Table 3: Deletion statistics, applying deletes observed in the Twitter ""spritzer"" stream over time.",1,Twitter,True
Figure 1: Number of tweets deleted from Tweets2013 and Tweets2013-IA over time.,1,Tweet,True
"been built on the Tweets2013 data. e answer is shown in Table 4, which lists for various conditions the number of relevant documents and qrels (all judgments in the pool, regardless of relevance) that would have disappeared as a result of the deletes. We see that by the end of 2016, a li le over 5% of the relevant documents would have been deleted. is is a smaller value than the fraction of the entire collection that is deleted, which means that deletes are more likely to a ect non-relevant documents.",1,Tweet,True
5 RETRIEVAL EXPERIMENTS,0,,False
"In our nal set of experiments, we examined the e ectiveness of baseline retrieval techniques on some of the variant collections",0,,False
1227,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 2: Size of the Tweets2013 and Tweets2013-IA collection over time a er applying observed deletes.,1,Tweet,True
Source,0,,False
|T - D (13/02-13/12)| |A - D (13/02-13/12)| |T - D (13/02-14/12)| |A - D (13/02-14/12)| |T - D (13/02-15/12)| |A - D (13/02-15/12)| |T - D (13/02-16/12)| |A - D (13/02-16/12)|,0,,False
missing reldocs,0,,False
"220 (1.12%) 209 (1.06%) 539 (2.74%) 513 (2.61%) 816 (4.15%) 776 (3.95%) 1,095 (5.57%) 1,042 (5.30%)",0,,False
missing qrels,0,,False
"1,820 (1.41%) 1,707 (1.32%) 4,456 (3.45%) 4,190 (3.24%) 6,576 (5.09%) 6,193 (4.79%) 8,500 (6.58%) 7,997 (6.19%)",0,,False
Table 4: Deletion statistics over relevance judgments; percentage of total is shown in parentheses.,0,,False
"explored above. e reference point for comparison is the o cial ri API, which served a collection of 243 million tweets (taking",1,AP,True
"into account deletions up until the time the API was deployed for the evaluation). For our experiments, we used exactly the same code base5 as the API, which was built on top of the open-source Lucene search engine (although in our case, we had direct access to the Lucene indexes).",1,AP,True
"For evaluation, we used 60 topics from TREC 2013 and 55 topics from TREC 2014. Ranking was performed using Lucene's implementation of query-likelihood, just as with the API. Following standard practice, we retrieved up to 1000 hits per topic and measured e ectiveness in terms of average precision (AP) and precision at 30 (P30), the two o cial metrics used in the evaluations. We report results with the o cial original NIST qrels. However, it would certainly be reasonable to remove deleted tweets from the judgments. We do so and report results under the modi ed qrels condition.",1,TREC,True
Experimental results are shown in Table 5 for both the original and modi ed qrels. e condition denoted T - D (13/02-13/06) attempts to replicate the data conditions of the o cial API; our results are very close but not exactly the same because we only consider deletes at monthly increments. e condition A - D (13/02-13/06),1,AP,True
5h p://twi ertools.cc/,0,,False
Track,1,Track,True
O cial ri API T - D (13/02-13/06) A - D (13/02-13/06) T - D (13/02-16/12) A - D (13/02-16/12),1,AP,True
Original AP P30,1,AP,True
0.3198 0.5278 0.3120 0.5278 0.2951 0.5130 0.2996 0.5220 0.2864 0.5130,0,,False
Modi ed,0,,False
AP P30,1,AP,True
-,0,,False
-,0,,False
0.3120 0.5278 0.2951 0.5130,0,,False
0.3158 0.5220 0.3013 0.5130,0,,False
Table 5: E ectiveness measures on TREC 2013 and 2014 Microblog Track topics over di erent data conditions.,1,TREC,True
"represents the best that a researcher can obtain in replicating the ofcial API using publicly available resources. Based on paired t-tests,",1,AP,True
"we do not nd any signi cant di erences (at p < 0.01) between the o cial ri API and these two data conditions, for both metrics (AP and P30), with either the original qrels or the modi ed qrels.",1,AP,True
"e last two rows in Table 5 show the state of the collection as of December 31, 2016 if all deletes were applied. We also do not nd any signi cant di erences between these two data conditions and the o cial ri API, for both metrics and both qrel conditions.",1,AP,True
6 CONCLUSIONS,0,,False
"e Tweets2013 collection, used in the TREC 2013 and TREC 2014 Microblog Tracks, serves as the basis of the most comprehensive evaluation resource for ad hoc retrieval on social media to date. Hampering its availability, however, is the API-based access mechanism. However, courtesy of the Internet Archive, researchers now can directly download tweets from the same period and same source as the o cial Tweets2013 collection. Our analyses con rm that, indeed, the Internet Archive data can serve as a drop-in replacement for evaluation purposes. We share with the community all code and data associated with analyses in this paper as well as instructions for replicating reported data conditions to serve as the basis of future work.6 Finally, researchers now have a downloadable test collection of tweets!",1,Tweet,True
REFERENCES,0,,False
"[1] Allan Hanbury, Henning Mu¨ller, Krisztian Balog, Torben Brodt, Gordon V. Cormack, Ivan Eggel, Tim Gollub, Frank Hopfgartner, Jayashree Kalpathy-Cramer, Noriko Kando, Anastasia Krithara, Jimmy Lin, Simon Mercer, and Martin Potthast. 2015. Evaluation-as-a-Service: Overview and Outlook. arXiv:1512.07454.",0,,False
[2] Donna Harman. 2011. Information Retrieval Evaluation. Morgan & Claypool Publishers.,0,,False
"[3] Jimmy Lin and Miles Efron. 2013. Evaluation as a Service for Information Retrieval. SIGIR Forum 47, 2 (2013), 8­14.",0,,False
[4] Jimmy Lin and Miles Efron. 2013. Overview of the TREC-2013 Microblog Track. In TREC.,1,TREC,True
"[5] Richard McCreadie, Ian Soboro , Jimmy Lin, Craig Macdonald, Iadh Ounis, and Dean McCullough. 2012. On Building a Reusable Twi er Corpus. In SIGIR. 1113­1114.",1,ad,True
"[6] Iadh Ounis, Craig Macdonald, Jimmy Lin, and Ian Soboro . 2011. Overview of the TREC-2011 Microblog Track. In TREC.",1,ad,True
"[7] Jiaul H. Paik and Jimmy Lin. 2016. Retrievability in API-Based ""Evaluation as a Service"". In ICTIR. 91­94.",1,AP,True
"[8] Ellen M. Voorhees, Jimmy Lin, and Miles Efron. 2014. On Run Diversity in ""Evaluation as a Service"". In SIGIR. 959­962.",0,,False
"[9] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A Cascade Ranking Model for E cient Ranked Retrieval. In SIGIR. 105­114.",1,ad,True
Acknowledgments. is work was supported by the Natural Sciences,0,,False
"and Engineering Research Council (NSERC) of Canada, with additional",1,ad,True
contributions from the U.S. National Science Foundation under IIS-1218043,0,,False
and CNS-1405688.,0,,False
6h ps://github.com/castorini/Tweets2013-IA,1,Tweet,True
1228,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Anserini: Enabling the Use of Lucene for Information Retrieval Research,0,,False
"Peilin Yang, Hui Fang",0,,False
Department of Electrical and Computer Engineering University of Delaware,0,,False
"{franklyn,hfang}@udel.edu",0,,False
ABSTRACT,0,,False
"So ware toolkits play an essential role in information retrieval research. Most open-source toolkits developed by academics are designed to facilitate the evaluation of retrieval models over standard test collections. E orts are generally directed toward be er ranking and less a ention is usually given to scalability and other operational considerations. On the other hand, Lucene has become the de facto platform in industry for building search applications (outside a small number of companies that deploy custom infrastructure). Compared to academic IR toolkits, Lucene can handle heterogeneous web collections at scale, but lacks systematic support for evaluation over standard test collections. is paper introduces Anserini, a new information retrieval toolkit that aims to provide the best of both worlds, to be er align information retrieval practice and research. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial e orts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web-scale collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for many TREC test collections, providing a convenient way to replicate competitive baselines right out of the box. Experiments verify that our system is both e cient and e ective, providing a solid foundation to support future research.",1,ad,True
1 INTRODUCTION,1,DUC,True
"Information retrieval researchers have a long history of developing, sharing, and using so ware toolkits to support their work. Over the past several decades, various IR toolkits have been built to aid in the development of new retrieval models, to test hypotheses about information seeking, and to validate new evaluation methodologies. As the eld moves forward, IR toolkits are expected to keep up with emerging requirements such as the ability to handle large web collections and new data formats. e growing complexity of modern so ware ecosystems and the resource constraints most academic research groups operate under make maintaining opensource toolkits a constant struggle.",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080721",1,ad,True
Jimmy Lin,0,,False
David R. Cheriton School of Computer Science University of Waterloo jimmylin@uwaterloo.ca,0,,False
"Most IR toolkits developed by academics, such as Indri,1 Galago,2 and Terrier3 were primarily designed to facilitate evaluation over standard test collections from evaluation forums such as TREC, CLEF, NTCIR, etc. In many cases, scalability took a back seat to e orts around improving retrieval models, and thus these systems o en struggle to scale to modern web collection. As an example, the ClueWeb12 collection4 contains 733 million web pages, totaling 5.54 TB compressed (or 27.3 TB uncompressed). e standard practice for working with this collection, as exempli ed by the infrastructure built for the TREC 2014 Session Track [4], is to separately index partitions of the collection and then build a distributed broker architecture that integrates results from each partition. In general, working with web-scale collections using existing academic IR toolkits is time- and resource-intensive, even for basic tasks.",1,ad,True
"With the exception of a small number of companies (e.g., commercial web search engines), the open-source Lucene system5 and its derivatives such as Solr and Elasticsearch (for convenience, we simply refer to as ""Lucene"" collectively in this paper) have become the de facto platform for deploying search applications in industry. Examples include LinkedIn, Twi er, Bloomberg, as well as a number of online retailers and many large companies in the nancial services space. Despite its undeniable operational success, a large user base, and a vibrant community of contributors, Lucene is not well suited to information retrieval research. For many reasons, including poor documentation of system internals and a number of unintuitive abstractions, Lucene is not as widely used for research as academic toolkits such as Indri or Terrier.",1,ad,True
"In this paper, we describe our e orts in developing a new opensource information retrieval toolkit called Anserini that builds on Lucene.6 We aim to bridge the gap described above that separates information retrieval research from the practice of building real-world search applications. Anserini provides wrappers and extensions on top of core Lucene libraries that allow researchers to use more intuitive APIs to accomplish common research tasks. Our initial e orts have focused on three functionalities: scalable, multi-threaded inverted indexing to handle modern web collections, streamlined IR evaluation for ad hoc retrieval on standard test collections, and an extensible architecture for multi-stage ranking. Anserini ships with support for standard TREC test collections, providing a convenient way to replicate competitive baselines ""right out of the box"", supporting the community's aspirations toward reproducible research [1, 7, 8, 10, 16, 18].",1,AP,True
1h p://www.lemurproject.org/indri/ 2h p://www.lemurproject.org/galago.php 3h p://terrier.org/ 4h p://www.lemurproject.org/clueweb12/ 5h ps://lucene.apache.org/ 6h p://anserini.io/,0,,False
1253,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"We experimentally evaluate the e ciency and e ectiveness of Anserini on a number of standard test collections. In terms of indexing performance, it is able to handle the largest research web collection available today with ease on a single modern server. We observe be er indexing performance compared to Indri, a popular choice among researchers today. In terms of retrieval, we also nd that Anserini is not only faster than Indri, but returns rankings that are comparable in quality. In other words, Anserini is faster and just as good. We present the case that Anserini should be adopted as the toolkit of choice for information retrieval researchers.",1,ad,True
2 ANSERINI OVERVIEW,0,,False
2.1 Motivation,0,,False
"Despite its popularity in industry and broad adoption for operational search deployments, Lucene remains under-utilized in information retrieval research. We begin with some high-level discussions of why we believe this might be so to motivate our e orts in building Anserini.",1,ad,True
"From the very beginning, Lucene was wri en for ""real world"" search applications, not with researchers in mind. For the most part, its developers targeted an audience that mostly used search engines as black boxes, as opposed to researchers that required access to ranking internals such as scoring models, mechanisms for postings traversal, etc. Because of the target user population, documentation for Lucene internals has always been quite poor, especially in keeping up with the relatively rapid pace at which the developer community has been releasing improved versions of the so ware. Access to these internals is exactly what information retrieval researchers need for their studies, and therefore poor documentation has been a barrier to entry.",0,,False
"To further compound this issue, the internal APIs in Lucene are not organized in a way that would be intuitive to most IR researchers, with class names that are not indicative of functionality and many levels of indirection. is is not an issue for ""black box"" users of Lucene, but presents a hurdle for information retrieval researchers who desire access to system internals. As an example, the code to open up a Lucene index and to traverse postings programmatically (without invoking the scoring function) is unnecessarily complex and involves dispatching to several intermediate classes along the way. Some researchers have the impression that Lucene is di cult to use, and indeed there is some truth to this, especially with respect to low-level abstractions.",1,AP,True
"Another side e ect of Lucene's focus on ""black box"" search is that it has severely lagged behind in the implementation of modern ranking functions. For the longest time, the default scoring model was an ad hoc variant of tf-idf. Okapi BM25 was not added to Lucene until 2011,7 more than a decade a er it gained widespread adoption in the research community as being more e ective than tf-idf variants. is lag in adopting ""research best practices"" has contributed to the perception that Lucene is not e ective and illsuited for information retrieval research. However, this perception is no longer accurate today. Lucene comes with implementations of modern baseline retrieval models, and we show that the e ectiveness of Lucene's implementations is at least as good as those o ered by academic IR toolkits (see Section 3).",1,ad,True
7h ps://issues.apache.org/jira/browse/LUCENE-2959,0,,False
"Finally, because Lucene is wri en in Java, there is sometimes the perception that it is slow and ine cient, particularly when scaling up to modern web collections. Developers o en point to the managed memory environment of the Java Virtual Machine (JVM) as not being conducive to e cient low-level implementations of search engine internals. We experimentally show that this is de nitely not true (see Section 3). e open-source community has devoted substantial e ort to optimizing the performance of Lucene and taking advantage of today's multi-core processors. It is capable of handling large web collections on a single server with ease.",1,ad,True
"e goal of Anserini is to align the practice of building search applications with research in information retrieval. Colloquially speaking, our toolkit aims to smooth the ""rough edges"" around Lucene for the purposes of information retrieval research. It is not our goal to replace or to reimplement Lucene, but rather to facilitate its use for research by presenting as gentle a learning curve as possible to newcomers.",0,,False
2.2 Main Components,0,,False
"Anserini components fall into two categories: wrappers and extensions. Wrappers provide APIs that leverage core Lucene library components to accomplish speci c tasks. ey are tightly integrated with ""core"" Lucene and in some cases, represent custom implementations of existing Lucene APIs. Extensions, on the other hand, are components that are distinct from Lucene and more loosely coupled: these may represent our own implementations or connectors to third-party libraries.",1,AP,True
"Multi-threaded indexing (wrapper). Inverted indexing is one of the most fundamental tasks in information retrieval and the starting point of many research studies. In working with large web collections, it is imperative that indexing operations are e cient and scalable. While academic researchers have a empted to address this issue via MapReduce and related frameworks [5, 11], these solutions impose the burden of requiring clusters and additional so ware infrastructure.",1,ad,True
"Lucene supports multi-threaded indexing, and as we experimentally show (Section 3), it is able to scale up to large web collections on a single commodity server. e biggest issue, however, is that Lucene itself only provides access to a collection of indexing components that researchers need to assemble together to build an end-to-end indexer. For example, the developer would need to write from scratch custom document processing pipelines, code for managing individual indexing threads, and implementations of load balancing and synchronization procedures.",1,ad,True
"We address these issues in Anserini by providing abstractions for document collections that an IR researcher would be comfortable with, as well as the implementation of an e cient, high-throughput, multi-threaded indexer that takes advantage of these abstractions. Anserini models collections as comprised of individual segments (for example, the ClueWeb12 collection is comprised of a number of compressed WARC les) and provides implementations for common document formats--for parsing TREC-style XML documents, web pages stored in WARCs, tweets in JSON format, etc. In fact, Anserini ships with the ability to index many TREC collections ""right out of the box"". is greatly reduces the learning curve for researchers to get started with Lucene.",1,ad,True
1254,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Indexing performance of Anserini and Indri on smaller collections using 16 threads on a modest commodity server.,1,ad,True
Collection Disk12 Disk45 AQUAINT WT2G WT10G Gov2,1,AQUAINT,True
docs 742k 528k 1.03m 246k 1.69m 25.2m,0,,False
terms 219m 175m 318m 182m 752m 17.3b,0,,False
Anserini (count) time size,0,,False
00:01:24 199MB 00:01:13 166MB 00:01:53 305MB 00:02:21 143MB 00:04:55 708MB 01:16:32 11GB,0,,False
Anserini (pos) time size,0,,False
00:01:44 512MB 00:01:33 423MB 00:02:10 734MB 00:02:55 437MB 00:05:05 2.9GB 02:32:43 38GB,0,,False
Anserini (doc) time size,0,,False
00:03:09 2.5GB 00:02:51 2.1GB 00:04:32 3.8GB 00:04:24 2.3GB 00:09:51 12GB 06:52:35 331GB,0,,False
Indri time size 00:12:28 2.5GB 00:06:55 1.9GB 00:17:36 3.9GB 00:07:25 2.2GB 00:42:51 9.6GB 14:51:12 215GB,0,,False
Table 2: Indexing performance of Anserini on web collections using 88 threads on a high-end server.,1,ad,True
Collection CW09b CW09 CW12b13 CW12,1,CW,True
docs 50m 504m 52m 733m,0,,False
terms 31b 268b 31b 429b,0,,False
Anserini (count),0,,False
time,0,,False
size,0,,False
00:42 28GB,0,,False
07:32 254GB,0,,False
00:57 29GB,0,,False
17:01 376GB,0,,False
Anserini (pos) time size 01:13 75GB 12:18 649GB 01:25 76GB 22:21 1.1TB,1,TB,True
"Streamlined IR evaluation (extension). Test collections play an important role in information retrieval research, and a substantial amount of research activity in improving ranking models is focused around ad hoc retrieval runs. A research toolkit should make this ""inner loop"" of IR research as easy as possible. Since Lucene was not originally designed for researchers, support for running experiments on standard test collections is largely missing. Anserini lls this gap by implementing missing features: parsers for di erent query formats, a uni ed driver program for ad hoc experiments that outputs standard trec eval format, etc. For convenience, existing TREC topics and qrels are included directly in our code repository--once again, reducing the learning curve for researchers to get started with Lucene.",1,ad,True
"ere are two main uses for this feature in Anserini: First, our toolkit provides an easy way for researchers to replicate baselines of standard retrieval models such as BM25 and query likelihood. Armstrong et al. [2] previously identi ed the prevalent problem of weak baselines in experimental IR papers. Lin et al. [10] further observed that authors are o en vague about the baseline parameter se ings and the implementations they use. For example, Mu¨hleisen et al. [13] reported large di erences in e ectiveness across four systems that all purport to implement BM25. Trotman et al. [15] pointed out that BM25 and query likelihood with Dirichlet priors can actually refer to at least half a dozen variants, and in some cases, di erences in e ectiveness are statistically signi cant. ere is substantial community interest in engaging with reproducibilityrelated issues [1, 8], and Anserini contributes to this discussion. Our proposed solution is to have widely-available baselines that are both competitive in e ectiveness and easy to replicate. It is our hope that Anserini can ll this role.",0,,False
"Second, an easy-to-use baseline retrieval component in Anserini provides the starting point for additional ranking extensions. In particular, we advocate a multi-stage ranking architecture [3, 6, 14, 17] so that researchers will not need to directly work with native Lucene scoring APIs. at is, researchers should take advantage of Anserini APIs that generate an initial document ranking and hooks",1,ad,True
"for feature extraction to build subsequent reranking stages. is, in fact, is the common architecture used in commercial web search engines today to support learning to rank [14].",0,,False
"Relevance feedback (extension). Relevance feedback techniques provide robust solutions to the vocabulary mismatch problem between expressions of user information needs and relevant documents. Anserini provides a reference implementation of the RM3 variant of relevance models [9], built as a reranking module in the multi-stage architecture described above. us, our implementation is useful not only as a baseline for comparing query expansion techniques, but provides an example of how reranking extensions can be implemented in Anserini.",0,,False
3 EVALUATION,0,,False
"We describe experiments to support three claims about Anserini and the use of Lucene for information retrieval research. First, that Anserini is highly scalable and able to e ciently index large web collections. Second, that Anserini is similarly e cient in searching these collections and ranking documents using standard baseline models. Finally, Anserini is able to achieve scalable indexing and e cient retrieval without compromising ranking e ectiveness.",0,,False
"e indexing performance of Anserini on a number of smaller and older collections is shown in Table 1. ese experiments were conducted on a server with dual AMD Opteron 6128 processors (2.0GHz, 8 cores) with 40GB RAM running CentOS 6.8. is machine can be characterized as an old, modest commodity server. All experiments were run on an otherwise idle machine. With Anserini, we used 16 threads for indexing and we report results from three di erent index con gurations: count indexes where only term frequency information is stored (count), positional indexes that also store term positions (pos), and positional indexes that also store the raw documents and parsed document vectors (doc). For each condition, we report the indexing time in HH:MM:SS (averaged over two trials) as well as the index size. e size of each collection is also shown for reference. As a comparison condition, we indexed the same collections using Indri 5.9 on the same machine.",1,ad,True
"In Table 2, we report indexing performance for larger web collections on a server with dual Intel Xeon E5-2699 v4 processors (2.2GHz, 22 cores) and 1 TB RAM running Ubuntu 16.04. e table rows indicate di erent collections: CW09b refers to the ClueWeb09 (category B) web crawl, CW09 refers to all English pages in the ClueWeb09 web crawl, CW12b13 refers to the smaller ClueWeb12B13 web crawl, and CW12 refers to the complete ClueWeb12 web crawl. Due to the size of the collections, we only report the count and positional index con gurations. For these experiments, we used",1,TB,True
1255,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: Retrieval e ciency for Terabyte 06 e ciency queries on Gov2, using a single thread.",1,Terabyte,True
Latency (ms),0,,False
Indri,0,,False
2403,0,,False
Anserini,0,,False
382,0,,False
roughput (qps) 0.42 2.61,0,,False
Table 4: E ectiveness comparisons between Anserini and Indri on standard TREC test collections.,1,TREC,True
Collection eries,0,,False
BM25 (I) BM25 (A) LM (I) LM (A),1,LM,True
Disk12 51-200,0,,False
0.2040 0.2267 0.2269 0.2232,0,,False
Disk45 301-450 601-700,0,,False
0.2478 0.2500 0.2516 0.2465,0,,False
WT2G 401-450,1,WT,True
0.3152 0.3015 0.3116 0.2922,0,,False
WT10G 451-550,1,WT,True
0.1955 0.1981 0.1915 0.2015,0,,False
Gov2 701-850,1,Gov,True
0.2970 0.3030 0.2995 0.2951,0,,False
"88 threads on an otherwise idle machine; indexing time is reported in HH:MM (averaged over two trials). On this server, we are able to index all of ClueWeb12, one of the largest collections available to researchers today, in less than a day! As seen from Table 1, even on an older server, the indexing performance of Lucene is impressive. Compared to academic toolkits, Lucene does not appear to have any trouble scaling to large modern web collections.",1,ad,True
"Our next set of experiments were conducted on the Gov2 collection with Terabyte 06 e ciency queries. We issued all 100,000 queries sequentially against both the Anserini and Indri indexes on the slower AMD Opteron server. Results are shown in Table 3, which reports latency (ms) and throughput (queries per second, or qps). In this experiment, we used only a single query thread, and therefore do not take advantage of Lucene's ability to execute queries in parallel on multiple threads (so in our case, throughput is simply the inverse of latency). We see from these experiments that Lucene is roughly six times faster than Indri.",1,Gov,True
"Finally, we compared the retrieval e ectiveness of Anserini and Indri. For Indri we refer to the RISE work of Yang and Fang [18], as they ne-tuned model parameters to achieve optimal e ectiveness. We considered two baseline ranking models: Okapi BM25 (BM25) and query likelihood with Dirichlet priors (LM). For Anserini, we removed stopwords (the default) and tuned parameters as follows: for BM25, k ,"" 0.9 and b  [0, 1] in increments of 0.1; for LM, µ  [0, 5000] in increments of 500. Results on standard TREC collections and queries are shown in Table 4, where (I) refers to Indri and (A) refers to Anserini. We see that e ectiveness results are comparable between the two systems.""",1,LM,True
"In summary, our experiments show that Anserini is at least as good as Indri in terms of e ectiveness, and much faster in both indexing and retrieval. ese results are consistent with ndings from the recent Open-Source IR Reproducibility Challenge [10]. Together, empirical evidence presents a compelling case for adopting Lucene for information retrieval research.",1,ad,True
4 CONCLUSIONS AND FUTURE WORK,0,,False
"Our message to the information retrieval community is that Lucene is e cient and scalable without compromising e ectiveness. Furthermore, Lucene has the bene t of a large user community and",0,,False
"broad adoption in industry. Anserini smooths over the ""rough",1,ad,True
"edges"" of using Lucene for information retrieval research by pro-",0,,False
viding wrappers and extensions that simplify common tasks such,0,,False
as indexing large research web collections and performing standard,0,,False
ad hoc retrieval runs. We hope that our toolkit will help to be er,1,ad,True
align the research and practice of information retrieval.,0,,False
"Broadly characterized, Anserini provides the foundation for an",1,ad,True
"IR research toolkit, but currently lacks features that one would",0,,False
associate with cu ing-edge research. Ongoing work is focused on,0,,False
"addressing this issue, as we are actively exploring retrieval models",1,ad,True
based on deep learning [12]. E orts include a empts to replicate,0,,False
existing neural retrieval models within our framework. Given the,0,,False
"existence of many deep learning toolkits (Torch, TensorFlow, etc.),",0,,False
"it does not make sense to reinvent the wheel. In this spirit, we",0,,False
have been building connectors between Lucene and the PyTorch,0,,False
"deep learning toolkit. Moving forward, we anticipate substantial",0,,False
continued interest at the intersection of deep learning and informa-,0,,False
"tion retrieval, and the multi-stage ranking architecture of Anserini",0,,False
provides a natural integration point for future explorations.,0,,False
REFERENCES,0,,False
"[1] Jaime Arguello, Ma Crane, Fernando Diaz, Jimmy Lin, and Andrew Trotman. 2015. Report on the SIGIR 2015 Workshop on Reproducibility, Inexplicability, and Generalizability of Results (RIGOR). SIGIR Forum 49, 2 (2015), 107­116.",0,,False
"[2] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don't Add Up: Ad-Hoc Retrieval Results Since 1998. In CIKM. 601­610.",0,,False
[3] Nima Asadi and Jimmy Lin. 2013. E ectiveness/E ciency Tradeo s for Candidate Generation in Multi-Stage Retrieval Architectures. In SIGIR. 997­1000.,1,ad,True
"[4] Ben Cartere e, Evangelos Kanoulas, Mark Hall, and Paul Clough. 2014. Overview of the TREC 2014 Session Track. In TREC.",1,TREC,True
"[5] Marc-Allen Cartright, Samuel Huston, and Henry Feild. 2012. Galago: A Modular Distributed Processing and Retrieval System. In SIGIR 2012 Workshop on Open Source Information Retrieval. 25­31.",0,,False
"[6] Charles L. A. Clarke, J. Shane Culpepper, and Alistair Mo at. 2016. Assessing E ciency--E ectiveness Tradeo s in Multi-stage Retrieval Systems Without Using Relevance Judgments. IRJ 19, 4 (2016), 351­377.",1,ad,True
"[7] Hui Fang, Hao Wu, Peilin Yang, and ChengXiang Zhai. 2014. VIRLab: A Webbased Virtual Lab for Learning and Studying Information Retrieval Models. In SIGIR. 1249­1250.",0,,False
"[8] Nicola Ferro, Norbert Fuhr, Kalervo Ja¨rvelin, Noriko Kando, Ma hias Lippold, and Justin Zobel. 2016. Increasing Reproducibility in IR: Findings from the Dagstuhl Seminar on ""Reproducibility of Data-Oriented Experiments in e-Science"". SIGIR Forum 50, 1 (2016), 68­82.",0,,False
[9] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR. 120­127.,0,,False
"[10] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",1,ad,True
"[11] Jimmy Lin, Donald Metzler, Tamer Elsayed, and Lidan Wang. 2009. Of Ivory and Smurfs: Loxodontan MapReduce Experiments for Web Search. In TREC.",1,TREC,True
[12] Bhaskar Mitra and Nick Craswell. 2017. Neural Models for Information Retrieval. arXiv:1705.01509.,0,,False
"[13] Hannes Mu¨hleisen, aer Samar, Jimmy Lin, and Arjen de Vries. 2014. Old Dogs Are Great at New Tricks: Column Stores for IR Prototyping. In SIGIR. 863­866.",0,,False
"[14] Jan Pedersen. 2010. ery Understanding at Bing. Invited Talk at SIGIR. [15] Andrew Trotman, An i Puurula, and Blake Burgess. 2014. Improvements to",0,,False
"BM25 and Language Models Examined. In ADCS. 58­65. [16] Ellen M. Voorhees, Shahzad Rajput, and Ian Soboro . 2016. Promoting Repeata-",1,ad,True
"bility rough Open Runs. In EVIA. 17­20. [17] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. A Cascade Ranking Model",1,ad,True
for E cient Ranked Retrieval. In SIGIR. 105­114. [18] Peilin Yang and Hui Fang. 2016. A Reproducibility Study of Information Retrieval,0,,False
Models. In ICITR. 77­86.,0,,False
Acknowledgments. is research was supported by the Natural Sciences,0,,False
and Engineering Research Council (NSERC) of Canada and the U.S. National,1,ad,True
Science Foundation under IIS-1423002 and CNS-1405688.,0,,False
1256,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
A Large-Scale ery Spelling Correction Corpus,0,,False
Ma hias Hagen,0,,False
Martin Po hast Marcel Gohsen Anja Rathgeber,0,,False
"Bauhaus-Universita¨t Weimar 99421 Weimar, Germany",0,,False
rstname . lastname @uni-weimar.de,0,,False
Benno Stein,0,,False
ABSTRACT,0,,False
"We present a new large-scale collection of 54,772 queries with manually annotated spelling corrections. For 9,170 of the queries (16.74%), spelling variants that are di erent to the original query are proposed. With its size, our new corpus is an order of magnitude larger than other publicly available query spelling corpora. In addition to releasing the new large-scale corpus, we also provide an implementation of the winner of the Microso Speller Challenge from 2011 and compare it on the di erent publicly available corpora to spelling corrections mined from Google and Bing. is way, we also shed some light on the spelling correction performance of state-of-the-art commercial search systems.",1,corpora,True
1 INTRODUCTION,1,DUC,True
"ery spelling correction is an important step of the query understanding process at search engine side. When a query is submi ed, it is usually rst tokenized and ""normalized"" (e.g., lowercasing), directly followed by a spelling correction. A er that, the query might be lemmatized/stemmed, entities might be detected, etc. However, these subsequent steps of understanding a user's query heavily rely on good spelling (e.g., entities with wrong spelling can be very di cult to accurately detect). us, spelling correction for queries a racted a lot of a ention, both within the Microso Speller Challenge 2011 [22] and in subsequent publications on participating approaches as well as improved versions thereof.",0,,False
"Today, commercial search engines typically o er corrections even while the user is typing [5], and they correct misspelled queries very reliably, asking ""Did you mean [alternative spelling],"" or even directly ""Showing results for"" their best guess. However, not too many details about the underlying systems are published. Instead, academic research on improved spelling correction algorithms still has to rely on only two publicly available corpora with about 6,000 annotated queries each (16­19% with spelling variants di erent to the original query), one being a training set of the mentioned Microso Speller Challenge 2011 [22], the other being published by the third-placed team who used it as an additional training set for the challenge [7]. To o er an alternative, largescale resource, we release a corpus of 54,772 web search queries, out of which 16.74% come with spelling variants di erent to the",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080749",1,ad,True
"original query.1 Along the corpus, we also release the code2 of a re-implementation of the best-performing approach from the Microso Speller Challenge [14] and compare it on our new corpus and the two other publicly available ones against spelling corrections mined from Google's and Bing's search engines.",0,,False
"e analysis of the results shows that our new corpus is a li le harder for the spelling correctors, with Precision@1 scores dropping by about 5­10%. Only the Google spelling correction performs be er than a baseline that does not change the input query at all.",0,,False
"us, our new corpus o ers a challenging alternative to the two existing corpora. Our re-implementation of Lueck's approach [14], who achieved the best performance within the Microso Speller Challenge, also struggles to beat the baseline. is indicates that the version that participated in the challenge probably heavily relied on not fully documented optimizations against the challenge's evaluation metrics that might not help in real-world situations.",1,corpora,True
2 RELATED WORK,0,,False
"ery spelling correction has been a lively research topic since the mid 2000's, especially in the NLP community [1, 4, 11]. Back in that time an (in)famous slide from some Google presentation presented literally hundreds of misspellings of the then-celebrity Britney Spears (or Bri any Spiers ).",0,,False
"Most systems for spelling correction from that time (and still today) are based on language models for the a priori probabilities of words and an error model (e.g., noisy channel) to estimate probabilities of misspellings [16]. Especially due to the error models trained on the input of billions of users, today's commercial search engines can provide a spelling performance that seems to ""magically"" second guess the intended query for most misspellings. In particular, today's search engines go as far as to suggest corrections even while the user is still typing [5], or they try to avoid user misspellings at all by sensible query auto-completions without errors [2], which is also still an ongoing research problem [10].",0,,False
"e problem of query spelling correction a racted a lot of attention around 2010, with the Microso Speller Challenge organized in the year 2011 [22] having more than 300 teams participating. With this challenge, a large public set of 5,892 spellcorrected queries sampled from the TREC Million ery track was released for training. e best-performing approach of Gord Lueck [14], based on combining Hunspell3 suggestions, was followed by Cloudspeller [12] and qSpell [7]. Also the ideas of the other top-performing participants [15, 17, 20] in uenced approaches published later [3, 6, 9, 13, 19, 21], indicating that query spelling is not ""solved"" yet.",1,TREC,True
1h ps://www.uni-weimar.de/medien/webis/corpora/ 2h ps://github.com/webis-de/SIGIR-17 3h p://hunspell.github.io/,1,corpora,True
1261,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Still, there are only two publicly available larger query corpora with annotations of potential alternative spellings. First, the aforementioned set of 5,892 queries published by the Microso Speller Challenge [22], and a set of 6,000 queries which the qSpell team released as their additional training set [7]. To further add to the publicly available corpora, we publish a set of 54,772 queries and possible alternative spellings for 9,170 of them.",1,corpora,True
3 OUR NEW CORPUS,0,,False
"We detail the sampling and annotation process of our corpus and compare it to the mentioned other two publicly available ones with regard to error types, error frequencies, etc.",0,,False
3.1 ery Sampling,0,,False
"For the creation of a previous query segmentation corpus [8], we had sampled 55,555 queries with 3 up to 10 words (i.e., with 2 up to 9 whitespaces) from the AOL query log [18] in three steps: (1) the raw query log was ltered in order to remove ill-formed queries, (2) from the remainder, queries were sampled at random respecting the query length distribution, and (3) the sampled queries were manually checked for anonymity-breaking words, languages other than English, containing child porn intents, etc.",1,ad,True
"In the rst step ( ltering), queries were discarded according to the following exclusion criteria:",0,,False
"· eries comprising remnants of URLs (e.g., .com or h p) or URL character encodings (to exclude strictly ""navigational"" queries caused by confusing the search box with the address bar).",1,ad,True
"· eries from searchers having more than 10,000 queries in the logged 3-month period (to exclude some query bots).",0,,False
· eries from searchers whose average time between consecutive queries is less than one second (to further exclude query bots).,0,,False
· eries from searchers whose median number of le ers per query is more than 100 (probably also bots).,0,,False
· eries that contain non-alphanumeric characters except for dashes and apostrophes in-between characters.,0,,False
· eries from searchers that duplicate preceding queries of themselves (to exclude result page interaction from the query frequency calculation).,0,,False
· eries with less than three or more than ten words.,0,,False
"We had a corpus size of more than 50,000 queries in mind and anticipated that the necessary manual cleansing (third step) could reduce the size of any query sample--thus, initially 55,555 queries were drawn to account for up to a potential 10% reduction.",1,ad,True
"To accomplish the query length distribution sampling (second step), the ltered log was divided into query length classes, where the i-th class contains all queries with i words (i.e., i-1 whitespaces), keeping duplicate queries from di erent searchers. en, the query length distribution was computed and the amount of each length class to be expected in a 55,555 query sample was determined. Based on these expectations, for each length class, queries were sampled without replacement until the expected amount of distinct queries was reached. Hence, our sample represents the query length distribution of the ltered log. And since each length class in the ltered log contained duplicate entries of queries according to their frequency, our sample also represents the query frequency distribution in the ltered query log. One might argue that our",0,,False
"sampling may miss many rare spelling errors but on the other hand, one might also argue that we just favor the more frequent errors whose correction could help many users. Either way, our later analyses of the amount of errors will show that they are similar to the previous corpora.",1,corpora,True
"In the nal manual cleansing (third step), we had one annotator go through all the 55,555 queries, labeling those that are non-English (the target language of our corpus), containing child porn intents (to be excluded from our corpus), or containing any potentially anonymity-breaking information (e.g., social security numbers, etc.). A er the cleansing, 54,772 queries remained such that our goal of sampling more than 50,000 queries was easily reached. ese 54,772 queries then went into manual spelling variant annotation.",1,ad,True
"Parenthesis: A Word on Anonymity. e AOL query log has been released without proper anonymization (only replacing the searchers' IP addresses with numerical IDs) [18]. is raised a lot of concerns among researchers as well as in the media, since some AOL users could be personally identi ed by analyzing their queries. We address this problem in our corpus by removing searcher IDs entirely and only publishing query strings without submission times or surrounding interactions. is way, queries from our sample could only be reliably mapped back to some original searcher if they contain user-identifying information or if they were submi ed by only one user in the AOL log. With our cleansing step described above, we try to avoid the former potential anonymity breach, while, against the la er, someone would have to actually trace a query back in the AOL log and then be able to de-anonymize the respective user(s).",1,ad,True
3.2 ery Spelling Correction,0,,False
"As for the spelling correction, 2 independent annotators went through all the 54,772 queries; allowed to use any tool they wanted to support their work (e.g., Hunspell, aspell, search engines, dictionaries, Wikipedia). For each query, potential alternative spellings (also possibly more than one) had to be annotated. A er two months of working on the spelling corrections (not necessarily full-time), both annotators discussed the cases where they disagreed. is typically resulted in di erent reasonable spelling variants being fed into the nal corpus. A er this step, three annotators each independently checked one third of the queries that contained alternative spellings from the rst iteration and could further add or remove variants if need be--also using tools of their choice. Finally, for 9,170 queries (16.74%) some variant di erent to the original spelling was annotated in the process.",1,Wiki,True
"Of course, this annotation process is not perfect and some spelling errors might have been missed or even been introduced. Hence, correcting the queries will remain an ongoing task with potential future corpus updates. For instance, a er the corpus release, the community working with the corpus may submit further spelling variants that will then be included and also made publicly available.",1,ad,True
3.3 Corpora Analysis and Comparison,0,,False
Table 1 contains the characteristics of the two previously available spelling correction corpora and our new corpus. e typical spelling error types reported in the table are deletion (entertaner,1,corpora,True
1262,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Corpora characteristics (MS ,"" Microso Speller Challenge, JDB "","" qSpell corpus, Ours "", our new corpus).",0,,False
MS,0,,False
JDB,0,,False
Ours,0,,False
Corpus size,0,,False
eries,0,,False
"5,892",0,,False
"6,000",0,,False
"54,772",0,,False
"w/ alternative spellings 1,121 (19.04%) 983 (16.38%) 9,170 (16.74%)",0,,False
Error type frequency (percentage of all queries with alternative spellings),0,,False
Deletion Insertion Space Special character Substitution Transposition,0,,False
308 (27.45%) 163 (14.53%) 625 (55.70%),0,,False
0 ( 0.00%) 135 (12.03%) 31 ( 2.76%),0,,False
226 (22.99%) 235 (23.91%) 497 (50.56%),0,,False
0 ( 0.00%) 118 (12.00%) 27 ( 2.75%),0,,False
"3,054 (33.30%) 1,688 (18.41%) 2,821 (30.76%) 3,229 (35.21%) 1,751 (19.09%)",0,,False
386 ( 4.21%),0,,False
" entertainer), insertion (baseballl  baseball), missing or added spaces (e.g., sponge bob  spongebob), missing or wrong special characters (e.g., noahs ark  noah's ark), substitution (canfederate  confederate), and transposition (chevorlet  chevrolet). Note that the numbers per error type do not necessarily add up to the number of queries with alternative spellings since some queries might contain more than one error type (the percentages indicate the ratio of queries with spelling variants that have a particular error in some variant).",1,ad,True
"As can be seen, the overall ratio of queries with alternative spellings is similar in all corpora. However, per error type, it is obvious that our annotators were the only ones who also annotated special characters as possible spelling variants; although we did not instruct them to do so. Since spelling correction o en takes place a er query normalization (i.e., a er removal of special characters), we added respective variants without special characters in a postprocessing. is ensures compatibility of our corpus with any ordering of the query understanding pipeline (i.e., normalization before or a er spelling correction). On average, the number of spelling variants per query is around 1.05­2.36 for di erent query classes in the corpora (i.e., most corrected queries have just one or two spelling variants) while the average Levenshtein distance from the original query to its closest variant is around 0.3­1.5 for queries with alternative spellings (especially in the Microso Speller Challenge corpus, the original spelling o en is among the alternative spellings).",1,corpora,True
"Altogether, our new corpus has similar error characteristics as the smaller previous corpora with the potential additional bonus of also including corrections with special characters.",1,corpora,True
4 EVALUATION,0,,False
"To compare the di erent corpora not just based on the annotated errors but also with respect to how hard it is for state-of-the-art query spelling correction to handle the ones contained, we conduct a pilot experiment on all three corpora. As a baseline, we choose the approach that does nothing, which turns out to be a rather strong competitor due to the large number of queries not containing any error (>80%) or having the original spelling as one variant. is baseline is contrasted with a re-implementation of Gord Lueck's approach that won the Microso Speller Challenge;",1,corpora,True
Table 2: ery spelling correction performance.,0,,False
Prec@1,0,,False
Microso Corpus,0,,False
Google Bing Lueck Baseline,0,,False
0.962 0.948 0.650 0.947,0,,False
JDB Corpus,0,,False
Google Bing Lueck Baseline,0,,False
0.947 0.929 0.619 0.906,0,,False
Our Corpus,0,,False
Google Bing Lueck Baseline,0,,False
0.912 0.851 0.541 0.851,0,,False
EF1,0,,False
0.892 0.865 0.854 0.873,0,,False
0.914 0.888 0.877 0.870,0,,False
0.904 0.833 0.836 0.842,0,,False
EP,0,,False
0.961 0.928 0.887 0.947,0,,False
0.941 0.918 0.900 0.906,0,,False
0.905 0.833 0.812 0.851,0,,False
ER,0,,False
0.833 0.810 0.823 0.810,0,,False
0.888 0.860 0.855 0.836,0,,False
0.903 0.833 0.863 0.833,0,,False
basing the re-implementation solely on Lueck's publication for the,0,,False
challenge to also conduct a small-scale reproducibility study. To,0,,False
"also include current search systems, we submi ed all the queries",0,,False
from the three corpora to the Google and Bing search engines and,1,corpora,True
"checked whether they suggested corrections (""Showing results for,""",0,,False
"Containing results for, ""Did you mean,"" etc.). Table 2 contains the",0,,False
"results of the three aforementioned approaches, and the baseline",0,,False
that does nothing.,0,,False
"As evaluation measures, we employ the ones established in the",0,,False
"Microso Speller Challenge (EF1, EP, ER), and additionally Precision@1 to check how good an approach's candidate with the",1,ad,True
"highest con dence actually is. For the Microso Speller Challenge,",0,,False
the spell correction approaches could submit a set C of potential,0,,False
correction candidates for each query q from the query set Q of the,0,,False
corpus that also contains the gold standard corrections G (q) for ev-,0,,False
ery query. A correction candidate c from the derived correction set,0,,False
C (q) of query q has to come with a likelihood or con dence P (c |q),0,,False
that c actually is a valid spelling for q; the P (c |q) values have to sum,0,,False
"up to 1 for each query. e ""expected precision"" EP and ""expected",0,,False
"recall"" ER of a spelling correction approach then are de ned as",0,,False
follows:,0,,False
"EP , 1",0,,False
"P (c |q), and",0,,False
|Q | q Q c C (q)G (q),0,,False
"ER , 1",0,,False
|C (q)  G (q)| .,0,,False
|Q | q Q |G (q)|,0,,False
"e combined EF1 score is de ned as 0.5 · (1/EP + 1/ER). Note that with the above de nitions, a successful strategy can be to include many potential corrections with low con dence scores in order to increase ER without harming EP too much. To somewhat counter this possibility, we also report Precision@1, which is simply the average over all queries of the precision at the rst rank given the con dence scores (i.e., ""simulating"" the real-world scenario that a search system has to actually decide whether to correct a query or not, whereas giving tens of possible candidates is not supporting a user). In case of ties at the rst rank (i.e., same con dence scores),",0,,False
1263,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"we checked whether one of these top-ranked corrections is in the gold standard (i.e., in doubt, favor the approach). To compute the con dence scores in case of Google and Bing, we resorted to a simple heuristic: (a) if just results for an alternative spelling are shown, this variant gets a con dence of 1 (""Showing results for""), (b) if results for an alternative and the original spelling are shown, the alternative gets 0.55 and the original 0.45 (""Containing results for""), (c) if only results for the original spelling are shown but an alternative is suggested, the original gets a con dence of 0.75 and the alternative of 0.25 (""Did you mean"").",0,,False
"As can be seen from Table 2, only Google reliably outperforms the do-nothing baseline. It is also particularly striking how low the Prec@1 scores of Lueck's approach are. In fact, we also could not really reproduce the performance of EF1 > 0.9 that was reported for Lueck's approach on the Microso Corpus [6]. We tried to follow Lueck's description of his approach [14] as closely as possible but some parts of the scoring scheme might not have been described and also some ""optimizations"" targeting the Speller Challenge's evaluation measures might not have been reported. Still, even the Bing system struggles to improve upon the baseline.",0,,False
"To further analyze the problems of Bing and Lueck's approach, we take a closer look on the error classes and on queries without spelling problems. While an in-depth analysis is beyond the scope of this paper, we summarize some particularly interesting insights with a focus on Prec@1 since the top rank would probably be the basis for retrieving search results. On queries with no errors, only Google and Bing achieve Prec@1 close to 0.99 while Lueck's approach for about every second or third such query suggests a topvariant that is not in the ground truth. e only approach achieving Prec@1 above 0.5 for most classes (error types and without error) is the Google system (except space and special character). Bing and Lueck's approach for many error classes like insertion or deletion only perform around 0.1­0.2 for Prec@1 (only at most one to two out of ten rank 1 suggestions actually are in the gold standard). On such cases, Lueck's approach rather achieves be er EF1 scores than Bing on our corpus. is is probably due to the many reported possible candidates (our scraping of the Bing suggestions can just report one or two candidates). However, on the Microso and the JDB corpora, the EF1 scores of Bing on error classes are about twice as large as the ones from Lueck; both still being below 0.4, though.",1,corpora,True
Our brief experimental study shows that Google actually seems to have the most useful spelling corrections (high Prec@1 for almost all classes and also highest EF1 scores) while Bing is somewhat behind and the many suggestions produced by Lueck's approach do not help in the practically important Prec@1 category.,0,,False
5 CONCLUSION AND OUTLOOK,0,,False
"Our new freely available corpus of query spelling corrections is about an order of magnitude larger than the two previously available corpora. As future work, we plan to include entity linking and maybe related queries to provide a large-scale corpus that supports research on several components of the query understanding pipeline. In fact, as a rst step, we will link the spelling corrections to our previously collected query segmentations [8].",1,corpora,True
"e portion of queries with alternative spellings in our new corpus is similar to the previous corpora (16.74%). However, our",1,corpora,True
corpus is the only one containing spelling variants with special,0,,False
characters--providing a testbed for query spelling before or a er,0,,False
"normalization (i.e., before or a er treating special characters).",0,,False
"In a rst study, we have compared the spelling corrections",0,,False
from the commercial search engines Google and Bing to a re-,0,,False
implementation of the best performing approach from the Microso,0,,False
Speller Challenge 2011. Our results on all corpora indicate that only,1,corpora,True
Google is able to substantially improve upon a simple do-nothing,0,,False
"baseline, while the other two approaches o en perform worse. But",0,,False
even the Google system is not able to always correct a typo and,0,,False
for some of the queries without errors suggests di erent spellings.,0,,False
"Hence, query spelling correction is still not a ""solved"" problem.",0,,False
REFERENCES,0,,False
[1] Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of HLT/EMNLP 2005.,1,ad,True
"[2] Fei Cai and Maarten de Rijke. 2016. A survey of query auto completion in information retrieval. Foundations and Trends in Information Retrieval 10 (2016), 273­363.",0,,False
"[3] Ishan Cha opadhyaya, Kannappan Sirchabesan, and Krishanu Seal. 2013. A fast generative spell corrector based on edit distance. In Proceedings of ECIR 2013, 404­410.",1,ad,True
"[4] Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proceedings of EMNLP 2004, 293­300.",0,,False
"[5] Huizhong Duan and Bo-June Paul Hsu. 2011. Online spelling correction for query completion. In Proceedings of WWW 2011, 117­126.",0,,False
"[6] Huizhong Duan, Yanen Li, ChengXiang Zhai, and Dan Roth. 2012. A discriminative model for query spelling correction with latent structural SVM. In Proceedings of EMNLP-CoNLL 2012, 1511­1521.",0,,False
"[7] Yasser Ganjisa ar, Andrea Zilio, Sara Javanmardi, Inci Cetindil, Manik Sikka, Sandeep Paul Katumalla, Narges Khatib-Astaneh, Chen Li, and Cristina Lopes. 2011. qSpell: Spelling correction of web search queries using ranking models and iterative correction. In Spelling Alteration for Web Search Workshop 2011.",0,,False
"[8] Ma hias Hagen, Martin Po hast, Benno Stein, and Christof Bra¨utigam. 2011. ery segmentation revisited. In Proceedings of WWW 2011, 97­106.",0,,False
"[9] Sasa Hasan, Carmen Heger, and Saab Mansour. 2015. Spelling correction of user search queries through statistical machine translation. In Proceedings of EMNLP 2015, 451­460.",0,,False
"[10] Liangda Li, Hongbo Deng, Jianhui Chen, and Yi Chang. 2017. Learning parametric models for context-aware query auto-completion via Hawkes processes. In Proceedings of WSDM 2017, 131­139.",0,,False
"[11] Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou. 2006. Exploring distributional similarity based models for query spelling correction. In Proceedings of ACL 2006.",0,,False
"[12] Yanen Li, Huizhong Duan, and ChengXiang Zhai. 2012. CloudSpeller: ery spelling correction by using a uni ed hidden Markov model with web-scale resources. In Proceedings of WWW 2012, 561­562.",0,,False
"[13] Yanen Li, Huizhong Duan, and ChengXiang Zhai. 2012. A generalized hidden Markov model with discriminative training for query spelling correction. In Proceedings of SIGIR 2012, 611­620.",0,,False
[14] Gord Lueck. 2011. A data-driven approach for correcting search queries. In Spelling Alteration for Web Search Workshop 2011.,0,,False
"[15] Peter Nalyvyko. 2011. A REST-based online English spelling checker ""Pythia"". In Spelling Alteration for Web Search Workshop 2011.",0,,False
[16] Peter Norvig. 2007. How to write a spelling corrector. h p://norvig.com/spell-correct.html. (2007).,0,,False
[17] Yoh Okuno. 2011. Spelling generation based on edit distance. In Spelling Alteration for Web Search Workshop 2011.,0,,False
"[18] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A picture of search. In Proceedings of Infoscale 2006, 1.",0,,False
"[19] Jason J. Soo. 2013. A non-learning approach to spelling correction in web queries. In Proceedings of WWW 2013, 101­102.",0,,False
"[20] Dan Stefanescu, Radu Ion, and Tiberiu Boros. 2011. TiradeAI: An ensemble of spellcheckers. In Spelling Alteration for Web Search Workshop 2011.",1,ad,True
"[21] Xu Sun, Anshumali Shrivastava, and Ping Li. 2012. Fast multi-task learning for query spelling correction. In Proceedings of CIKM 2012, 285­294.",0,,False
"[22] Kuansan Wang and Jan Pedersen. 2011. Review of MSR-Bing web scale speller challenge. In Proceedings of SIGIR 2011, 1339­1340.",0,,False
1264,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
DBpedia-Entity v2: A Test Collection for Entity Search,0,,False
Faegheh Hasibi,0,,False
Norwegian University of Science and Technology,0,,False
faegheh.hasibi@ntnu.no,0,,False
Fedor Nikolaev,0,,False
Wayne State University fedor@wayne.edu,0,,False
Chenyan Xiong,0,,False
Carnegie Mellon University cx@cs.cmu.edu,0,,False
Krisztian Balog,0,,False
University of Stavanger krisztian.balog@uis.no,0,,False
Svein Erik Bratsberg,0,,False
Norwegian University of Science and Technology,0,,False
sveinbra@ntnu.no,0,,False
Alexander Kotov,0,,False
Wayne State University kotov@wayne.edu,0,,False
Jamie Callan,0,,False
Carnegie Mellon University callan@cs.cmu.edu,0,,False
ABSTRACT,0,,False
"e DBpedia-entity collection [2] has been used as a standard test collection for entity search in recent years. We develop and release a new version of this test collection, DBpedia-Entity v2, which uses a more recent DBpedia dump and a uni ed candidate result pool from the same set of retrieval models. Relevance judgments are also collected in a uniform way, using the same group of crowdsourcing workers, following the same assessment guidelines. e result is an up-to-date and consistent test collection.To facilitate further research, we also provide details about the pre-processing and indexing steps, and include baseline results from both classical and recently developed entity search methods.",0,,False
KEYWORDS,0,,False
Entity retrieval; Test collection; Semantic search; DBpedia,0,,False
"ACM Reference format: Faegheh Hasibi, Fedor Nikolaev, Chenyan Xiong, Krisztian Balog, Svein Erik Bratsberg, Alexander Kotov, and Jamie Callan. 2017. DBpedia-Entity v2: A Test Collection for Entity Search. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: 10.1145/3077136.3080751",0,,False
1 INTRODUCTION,1,DUC,True
"Entities are meaningful units of retrieval, as many information needs are centered around them [17]. For example, it has been found that more than 70% of Bing's query volume is related to entities [9]; in the Allen Institute's scholar search engine, more than half of the tra c is about research concepts (i.e., abstract entities) and another one third is about person names [26]. Over the course of the past decade, entity search has drawn a lot of a ention from both academia and industry. Entities have also grown to be",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080751",1,ad,True
"rst-class citizens in web search, o en featured as entity cards. Much of this success can be a ributed to the availability of largescale knowledge repositories, which can provide rich semantic information organized around entities. Recently, there has been a shi of focus from semi-structured data sources (in particular, Wikipedia) to structured ones (DBpedia, YAGO, Freebase, etc.). To further research and development in this area, there is a need for a standard test collection--this is exactly what the resource we introduce in this paper, the DBpedia-Entity v2 collection, aims to be.",1,Wiki,True
"Balog and Neumayer [2] introduced the DBpedia-Entity test collection, by assembling search queries from a number of entityoriented benchmarking campaigns and mapping relevant results to DBpedia. Over the past years, this has become a standard test collection for evaluating entity search research, see [5, 11, 16, 20, 27].",0,,False
"e main objective of this work is to create a new, updated version of this test collection. We shall refer to the original collection in [2] as DBpedia-Entity v1 and to our updated version as DBpedia-Entity v2. e new version's improvements are manyfold. (1) e original collection contains only relevant results and relevance is binary for most of the queries; we use graded relevance judgments for all queries and also include all judged items, relevant or not. (2)",1,ad,True
"e DBpedia knowledge base has grown signi cantly over the past years. Many new relevant entities were not judged in the old version; we use a recent DBpedia version and judge the relevance of new entities. (3) Judgments in the original collection have been assembled from multiple campaigns, each with its own setup; we obtain relevance labels under the same conditions for all queries in the collection.",0,,False
"We also present details about how the DBpedia dump is processed and indexed, reducing the inconsistency in preprocessing. We provide rankings using both traditional and recently-developed entity search methods, making future comparison with prior work much easier. All resources, including queries, relevance assessments (qrels), base runs, their evaluation results, and further details on indexing and preprocessing are made publicly available at h p://tiny.cc/dbpedia-entity. We note that annotations for the target type identi cation [8] and entity summarization [12] tasks, using the same queries, are also available in this repository.",1,ad,True
1265,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 THE TEST COLLECTION,0,,False
"is section describes the test collection, including the knowledge base, queries, and process of collecting of relevance assessments.",0,,False
2.1 Knowledge Base,0,,False
"We use DBpedia as our knowledge base; it is o en referred to as ""the database version of Wikipedia."" DBpedia is a community e ort, where a set of rules (""mappings"") are collaboratively created to extract structured information from Wikipedia. Since its inception in 2007, there have been regular data releases; it also has a live extraction component that processes Wikipedia updates real-time. DBpedia is a central hub in the Linking Open Data cloud, and has been widely used in various semantic search tasks [1, 12, 13, 18].1",1,Wiki,True
"We use the English part of the 2015-10 version of DBpedia. It contains 6.2 million entities, 1.1 billion facts, and an ontology of 739 types. In comparison, version 3.7, that has been used in DBpediaEntity v1, contains 3.64 million entities, over 400 million facts, and an ontology of 358 types. DBpedia 2015-10 is also believed to be much cleaner due to be er extraction techniques developed by the DBpedia community.",0,,False
"Preprocessing. We require entities to have both a title and abstract (i.e., rdfs:label and rdfs:comment predicates)--this e ectively",0,,False
"lters out category, redirect, and disambiguation pages. Note that list pages, on the other hand, are retained. In the end, we are le with a total of 4.6 million entities. Each entity is uniquely identi ed by its URI.",0,,False
2.2 Test eries,0,,False
e queries in DBpedia-Entity v2 are the same as in v1. We distinguish between four categories of queries:,0,,False
"· SemSearch ES queries are from the ad-hoc entity search task of the Semantic Search Challenge series [4, 10]. ese are short and ambiguous queries, searching for one particular entity, like ""brooklyn bridge"" or ""08 toyota tundra.""",1,ad-hoc,True
"· INEX-LD queries are from the ad-hoc search task at the INEX 2012 Linked Data track [25]. ey are IR-style keyword queries, e.g., ""electronic music genres.""",1,INEX,True
"· List Search comprises queries from the list search task of the 2011 Semantic Search Challenge (SemSearch LS) [4], from the INEX 2009 Entity Ranking track (INEX-XER) [6], and from the Related Entity Finding task at the TREC 2009 Entity track [3]. ese queries seek a particular list of entities, e.g., ""Professional sports teams in Philadelphia.""",1,INEX,True
"· QALD-2 queries are from the estion Answering over Linked Data challenge [15]. ese are natural language questions that can be answered by DBpedia entities, for example, ""Who is the mayor of Berlin?""",0,,False
"Originally, the SemSearch queries were evaluated using crowdsourcing on a 3-point relevance scale. All other benchmarks employed expert evaluators (trained assessors or benchmark organizers/participants) and have binary judgments.",0,,False
"1DBpedia is not the only general-purpose knowledge base available, but arguably the most suitable one. Alternatives include YAGO [24] (not updated regularly), Freebase (discontinued), and WikiData (still in its infancy).",1,Wiki,True
Figure 1: Crowdsourcing task design.,0,,False
2.3 Relevance Assessments,0,,False
"In DBpedia-Entity v1, the relevance judgments (""qrels"") are assembled from several di erent benchmarks. ese assessments were created using di erent annotation guidelines, judges (trained assessors vs. crowdsourcing), pooling methods, and even di erent corpora (various versions of DBpedia or Wikipedia). For DBpediaEntity v2, we generate new relevance judgments for all queries using the same setup. We pool candidate results from the same set of systems, and use the same annotation procedure and guidelines.",1,corpora,True
"2.3.1 Pooling. Following standard practice of IR test collection building, we employ a pooling approach, and combine retrieval results from four main sources:",0,,False
"· Original qrels. All relevant entities from DBpedia-Entity v1 are included, to ensure that results that have previously been identi ed as relevant get re-assessed.",0,,False
"· Previous runs. We consider 37 di erent retrieval methods (""runs"") that have been evaluated on DBpedia-Entity v1 in prior work [11, 20, 26, 27]. All entity URIs returned by these runs are mapped to DBpedia version 2015-10; entities not present in DBpedia 2015-10 are discarded. e pool depth is 20, i.e., we take the top 20 ranked entities from each run.",0,,False
"· New runs. We obtained retrieval results for DBpedia 2015-10 from 13 di erent systems, by three independent research groups; see Sect. 3 for the description of these methods. Results are pooled from these runs up to depth 20.",0,,False
"· SPARQL results. For QALD-2 queries, the ground truth is obtained by executing a SPARQL query (manually constructed by the campaign organizers [15]) over the knowledge base. We re-ran these queries against the DBpedia API endpoint to obtain up-to-date results, as the answers to some questions might have changed since (e.g., ""Who is the mayor of Berlin?"").",1,AP,True
"e nal assessment pool contains 50,516 query-entity pairs (104 entities per query on average).",0,,False
"2.3.2 Collecting Relevance Judgments. We collected the relevance judgments using the CrowdFlower crowdsourcing platform. For each record (i.e., query-entity pair) in our pool, we provided the workers with the query, the name and short description (DBpedia abstract) of the entity, as well as the link to the entity's Wikipedia page; see Figure 1. Since narratives are only available for a small number of queries in our query set (those from TREC and INEX), we decided to keep the setup uniform across all queries, and present assessors only with the query text. To avoid positional bias, records were presented in a random order. Workers were then asked to",1,Wiki,True
1266,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: ery categories in DBpedia-Entity v2. R1 and R2 refer to the average number of relevant and highly relevant entities per query, respectively.",0,,False
Category,0,,False
#queries Type,0,,False
R1 R2,0,,False
SemSearch ES 113 named entities 12.5 3.0,0,,False
INEX-LD,1,INEX,True
99 keyword queries 23.5 9.2,0,,False
ListSearch,0,,False
115 list of entities 18.1 12.7,0,,False
QALD-2,0,,False
140 NL questions 28.4 29.8,0,,False
Total,0,,False
467,0,,False
21.0 14.7,0,,False
"judge relevance on a 3-point Likert scale: highly relevant, relevant, or irrelevant. We educated workers about the notion of entities and provided them with the following working de nitions for each scale (each further illustrated with examples):",0,,False
"· Highly relevant (2): e entity is a direct answer to the query (i.e., the entity should be among the top answers).",0,,False
"· Relevant (1): e entity helps one to nd the answer to the query (i.e., the entity can be shown as an answer to the query, but not among the top results).",0,,False
"· Irrelevant (0): e entity has no relation to the intent of the query (i.e., the entity should not be shown as an answer).",0,,False
"We have taken quality control very seriously, which was a nontrivial task for a pool size of over 50K. During the course of the assessment, the accuracy of workers was regularly examined with hidden test questions. 400 query-entity pairs were randomly selected as test cases and judged by three authors of the paper; 373 of these were then used as test questions (where at least two of the experts agreed on the relevance label). Only workers with quali cation level 2 (medium) or 3 (high) on CrowdFlower were allowed to participate. ey were then required to maintain at least 70% accuracy throughout the job; those falling below this threshold were not allowed to continue the job and their previous assessments were excluded. We collected 5 judgments for each record and paid workers a reasonable price of ¢1 per judgment. e nal cost was over 3,500 USD, which makes this a very valuable test collection, also in the literal sense of the word. e Fleiss' Kappa inter-annotator agreement was 0.32, which is considered fair agreement. To determine the relevance level for a query-entity pair, we took the majority vote among the assessors. In case of a tie, the rounded average of relevance scores is taken as the nal judgment.",0,,False
"Further inspection of the obtained results revealed that crowd workers are less likely to nd answers to complex information needs. ey are less patient and make judgments primarily based on the provided snippets and Wikipedia pages. When it would be required to read the Wikipedia article more carefully, or to consult additional sources, users are less likely to label them as a entively as expert annotators would. To further the quality of the test collections, we collected expert annotations for cases with ""extreme disagreements,"" i.e., cases without majority vote, or cases that are found irrelevant by crowd workers, but are highly relevant according to the original qrels.2 is resulted in the annotation of 8K query-entity pairs, each by two experts, with a Fleiss' Kappa agreement of 0.48, which is considered moderate. e nal label for",1,Wiki,True
"2 is includes SPARQL query results for QALD queries, highly relevant judgments for SemSearch queries, and all TREC and INEX judgments.",1,TREC,True
"Table 2: Comparison of methods for DBpedia-entity v1 vs. v2 qrels. e supervised methods (bottom block) are trained on v1; for methods trained on v2, we refer the reader to the online repository.",1,ad,True
Method,0,,False
Index,0,,False
v1 MAP P@10,1,MAP,True
MAP,1,MAP,True
v2 nDCG@10,0,,False
BM25 PRMS MLM-all LM SDM,1,LM,True
A 0.0884 0.0971 0.1893 0.2558 B 0.1571 0.1682 0.2895 0.3905 B 0.1618 0.1705 0.3031 0.4021 B 0.1709 0.1837 0.3144 0.4182 A 0.1860 0.1880 0.3259 0.4185,0,,False
LTR,0,,False
A 0.1723 0.1831 0.2446,0,,False
LM+ELR,1,LM,True
B+ 0.1772 0.1895 0.3103,0,,False
SDM+ELR A+ 0.1901 0.1986 0.3284,0,,False
MLM-CA,1,LM,True
A 0.1905 0.2008 0.3061,0,,False
BM25-CA,0,,False
A 0.2067 0.2056 0.3265,0,,False
FSDM,0,,False
A 0.2069 0.2039 0.3279,0,,False
BM25F-CA A 0.2088 0.2126 0.3361,0,,False
FSDM+ELR A+ 0.2210 0.2089 0.3295,0,,False
0.3464 0.4123 0.4200 0.4117 0.4231 0.4267,0,,False
0.4378 0.4335,0,,False
"the extreme disagreement cases was taken to be the expert-agreed label. If such a label did not exist, we took the rounded average between the two expert labels and the crowdsourcing decision (as a third label). Finally, queries that no longer have relevant results were removed (18 in total). Table 1 shows the statistics for the nal v2 collection.",0,,False
3 RETRIEVAL METHODS,0,,False
"We employ a range of entity retrieval approaches to obtain results for pooling. Below, we provide a brief overview of these methods, along with the details of indexing and pre-processing techniques.",0,,False
3.1 Indexing and ery Processing,0,,False
"Retrieval results were obtained using two indices (Index A and Index B), built from the DBpedia 2015-10 dump, following the general approach outlined in [27]. In particular, we used the same entity representation scheme with ve elds (names, categories, similar entity names, a ributes, and related entity names) as in [27]. Index A was constructed using Galago, while Index B was created using Elasticsearch. ey use slightly di erent methods for converting entity URIs to texts. Index B also contains an extra catchall eld, concatenating the contents of all other elds. An extra URI-only index was built according to [11], which is used for the ELR-based methods; we write `+' to denote when this index is used. All the runs were generated using preprocessed queries; i.e., removing the stop pa erns provided in [11] and punctuation marks. Further details are provided in the collection's GitHub repository.",0,,False
3.2 Retrieval Methods,0,,False
"We consider various entity retrieval methods that have been published over the recent years [2, 5, 11, 20, 27]. Unless stated otherwise, the parameters of methods are trained for each of the four query subsets, using cross-validation (with the same folds across all methods). Table 2 shows the particular index version that was used for each method.",0,,False
1267,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Unstructured retrieval models. is group of methods uses a attened entity representation. Speci cally, we report on LM (Language Modeling) [22], SDM (Sequential Dependence Model) [19], and BM25 [23]. All LM-based methods use Dirichlet prior smoothing with µ ,"" 1500 for index A, and µ "", 2000 for index B. e BM25 parameters are k1 , 1.2 and b , 0.8. We also report on BM25-CA with parameters trained using Coordinate Ascent.",1,LM,True
"Fielded retrieval models. is category of methods employs a elded entity representation (cf. Sect. 3.1). We report on MLMCA (Mixture of Language Models) [21], FSDM (Fielded Sequential Dependence Model) [27], and BM25F-CA [23] (the -CA su xes refer to training using Coordinate Ascent). We also report on MLMall, with equal eld weights, and on PRMS (Probabilistic Model for Semistructured Data) [14], which has no free parameters.",1,LM,True
"Other models. e LTR (Learning-to-Rank) approach [5] employs 25 features from various retrieval models and is trained using the RankSVM algorithm. e ELR methods [11] employ TAGME [7] for annotating queries with entities, and use the URI-only index (with a single catchall eld) for computing the ELR component.",0,,False
4 RESULTS AND ANALYSIS,0,,False
"In Table 2 we report on the performance of the di erent retrieval methods using both the original (v1) and new (v2) relevance judgments. (In case of the v1 qrels, we removed entities that are not present in DBpedia 2015-10.) Methods in the top block of the table do not involve any training and use default parameter se ings, while systems in the bo om block are trained for each query category using cross-validation. Training is done using the v1 qrels. Since we have graded relevance judgments for v2, the ""o cial"" evaluation metric for the new collection is NDCG@10. However, to facilitate comparison with the v1 results, we also report on MAP (at rank 100, accepting both levels 1 and 2 as relevant). At rst glance, we observe that the absolute MAP values for v2 are higher than for v1; this is expected, as there are more relevant entities according to the new judgments. We also nd that the relative ranking of methods in the top block remains the same when moving from v1 to v2. On the other hand, methods that involve training (bo om block) show much smaller relative improvements over the models without training (top block) in v2 as for v1. is is explained by the fact that training was done on v1. We note that we are not elaborating on the performance of individual methods as that is not the focus of this paper. One issue we wish to point out, nevertheless, is that default parameter se ings may be un ing for entity retrieval; in particular, observe the large di erence between BM25 with default parameters vs. BM25-CA with trained parameters (which are b  0.05 and k1 in the range 2..6, depending on the query subtype). In the online repository, we further report on the supervised models trained on the new (v2) qrels, and break down evaluation results into di erent query subsets.",1,ad,True
5 CONCLUSION,0,,False
"is paper has introduced an updated version of a standard entity search test collection, by using a more recent DBpedia dump, a more consistent candidate document pool, and a uni ed relevance",0,,False
assessment procedure. We have also provided details about process-,0,,False
"ing and indexing, together with retrieval results for both traditional",1,ad,True
and more recent entity retrieval models. It is our hope that this new,0,,False
test collection will serve as the de facto testbed for entity search,0,,False
"over structured data, and will foster future research.",0,,False
Acknowledgements. is research was partially supported by,0,,False
the National Science Foundation (NSF) grant IIS-1422676. Any,0,,False
"opinions, ndings, and conclusions in this paper do not necessarily",0,,False
re ect those of the sponsors.,0,,False
REFERENCES,0,,False
[1] Krisztian Balog and Robert Neumayer. 2012. Hierarchical target type identi cation for entity-oriented queries. In Proc. of CIKM '12. 2391­2394.,0,,False
[2] Krisztian Balog and Robert Neumayer. 2013. A Test Collection for Entity Search in DBpedia. In Proc. of SIGIR '13. 737­740.,0,,False
"[3] Krisztian Balog, Pavel Serdyukov, Arjen De Vries, Paul omas, and ijs Westerveld. 2010. Overview of the TREC 2009 Entity Track. In Proc. of TREC '09.",1,TREC,True
"[4] Roi Blanco, Harry Halpin, Daniel M Herzig, Peter Mika, Je rey Pound, and Henry S ompson. 2011. Entity Search Evaluation over Structured Web Data. In Proc. of the 1st International Workshop on Entity-Oriented Search. 65­71.",0,,False
"[5] Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An Empirical Study of Learning to Rank for Entity Search. In Proc. of SIGIR '16. 737­740.",0,,False
"[6] Gianluca Demartini, Tereza Iofciu, and Arjen P De Vries. 2009. Overview of the INEX 2009 Entity Ranking Track. In INEX. 254­264.",1,INEX,True
[7] Paolo Ferragina and Ugo Scaiella. 2010. TAGME: On-the- y Annotation of Short Text Fragments (by Wikipedia Entities). In Proc. of CIKM '10. 1625­1628.,1,Wiki,True
"[8] Dario Gariglio i, Faegheh Hasibi, and Krisztian Balog. 2017. Target Type Identication for Entity-Bearing eries. In Proc. of SIGIR '17.",0,,False
"[9] Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2009. Named Entity Recognition in ery. In Proc. of SIGIR '09. 267­274.",0,,False
"[10] Harry Halpin, Daniel M Herzig, Peter Mika, Roi Blanco, Je rey Pound, Henry S ompson, and Duc anh Tran. 2010. Evaluating Ad-hoc Object Retrieval. In",1,hoc,True
"Proc. of the International Workshop on Evaluation of Semantic Technologies. [11] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2016. Exploiting",0,,False
"Entity Linking in eries for Entity Retrieval. In Proc. of ICTIR '16. 171­180. [12] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2017. Dynamic Factual",0,,False
"Summaries for Entity Cards. In Proc. of SIGIR '17. [13] Faegheh Hasibi, Krisztian Balog, and Svein Erik Bratsberg. 2017. Entity Linking",0,,False
"in eries: E ciency vs. E ectiveness. In Proc. of ECIR '17. 40­53. [14] Jinyoung Kim, Xiaobing Xue, and W Bruce Cro . 2009. A Probabilistic Retrieval",0,,False
"Model for Semistructured Data. In Proc. of ECIR '09. 228­239. [15] Vanessa Lopez, Christina Unger, Philipp Cimiano, and Enrico Mo a. 2013. Eval-",0,,False
"uating estion Answering over Linked Data. Web Semantics: Science, Services and Agents on the World Wide Web 21, 0 (2013), 3­13. [16] Chunliang Lu, Wai Lam, and Yi Liao. 2015. Entity Retrieval via Entity Factoid Hierarchy. In Proc. of ACL '15. 514­523. [17] Edgar Meij, Krisztian Balog, and Daan Odijk. 2014. Entity Linking and Retrieval for Semantic Search. In Proc of. WSDM '14. 683­684. [18] Edgar Meij, Marc Bron, Laura Hollink, Bouke Huurnink, and Maarten de Rijke. 2011. Mapping eries to the Linking Open Data Cloud: A Case Study Using DBpedia. Web Semant. 9, 4 (Dec. 2011), 418­433. [19] Donald Metzler and W Bruce Cro . 2005. A Markov Random Field Model for Term Dependencies. In Proc. of SIGIR '05. 472­479. [20] Fedor Nikolaev, Alexander Kotov, and Nikita Zhiltsov. 2016. Parameterized Fielded Term Dependence Models for Ad-hoc Entity Retrieval from Knowledge Graph. In Proc. of SIGIR '16. 435­444. [21] Paul Ogilvie and Jamie Callan. 2003. Combining Document Representations for Known-item Search. In Proc. of SIGIR '03. 143­150. [22] Jay M Ponte and W Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In Proc. of SIGIR '98. 275­281. [23] Stephen E. Robertson and Hugo Zaragoza. 2009. e Probabilistic Relevance Framework: BM25 and Beyond. Found. and Trends in IR 3, 4 (2009), 333­389. [24] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: A Core of Semantic Knowledge. In Proc. of WWW '07. 697­706. [25] Qiuyue Wang, Jaap Kamps, Georgina Ram´irez Camps, Maarten Marx, Anne Schuth, Martin eobald, Sairam Gurajada, and Arunav Mishra. 2012. Overview of the INEX 2012 Linked Data Track. In CLEF Online Working Notes. [26] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding. In Proc. of WWW '17. 1271­1279. [27] Nikita Zhiltsov, Alexander Kotov, and Fedor Nikolaev. 2015. Fielded Sequential Dependence Model for Ad-Hoc Entity Retrieval in the Web of Data. In Proc. of SIGIR '15. 253­262.",1,hoc,True
1268,0,,False
,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
A Cross-Platform Collection for Contextual Suggestion,0,,False
"Mohammad Aliannejadi, Ida Mele, and Fabio Crestani",1,ad,True
"Faculty of Informatics, Universita` della Svizzera italiana (USI) Lugano, Switzerland",0,,False
"{mohammad.alian.nejadi,ida.mele,fabio.crestani}@usi.ch",1,ad,True
ABSTRACT,0,,False
"Suggesting personalized venues helps users to nd interesting places on location-based social networks (LBSNs). Although there are many LBSNs online, none of them is known to have thorough information about all venues. e Contextual Suggestion track at TREC aimed at providing a collection consisting of places as well as user context to enable researchers to examine and compare di erent approaches, under the same evaluation se ing. However, the o cially released collection of the track did not meet many participants' needs related to venue content, online reviews, and user context. at is why almost all successful systems chose to crawl information from di erent LBSNs. For example, one of the best proposed systems in the TREC 2016 Contextual Suggestion track crawled data from multiple LBSNs and enriched it with venuecontext appropriateness ratings, collected using a crowdsourcing platform. Such collection enabled the system to be er predict a venue's appropriateness to a given user's context. In this paper, we release both collections that were used by the system above. We believe that these datasets give other researchers the opportunity to compare their approaches with the top systems in the track. Also, it provides the opportunity to explore di erent methods to predicting contextually appropriate venues.",1,TREC,True
CCS CONCEPTS,0,,False
·Information systems Test collections; Recommender systems;,0,,False
KEYWORDS,0,,False
Collection; Venue Suggestion; Context-Awareness,0,,False
1 INTRODUCTION,1,DUC,True
"Location-based social networks (LBSNs), such as Yelp, TripAdvisor, and Foursquare, allow users to share check-in data using their mobile devices. Such platforms also collect valuable information about users' mobility records such as check-in data and users' feedback (e.g., ratings, tags, and reviews). One important related task consists in suggesting personalized venues to a user who is exploring a new venue or visiting a new city [3]. e Contextual Suggestion track in the Text REtrieval Conference (TREC) aimed at providing a standard evaluation setup for researchers in which they can compare",1,TREC,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 7-11,2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080752",1,ad,True
"their proposed approaches [5]. For each user, the participants had to produce a ranked list of venues to recommend to a user visiting a new city. ey were given the users' history of preferences expressed in one or two previously visited cities (30-60 venues per user) and had to consider both preferences and context of the users for making their suggestions.",1,ad,True
"In the last two years of the track, the organizers a empted to create reusable datasets. Consequently, the track consisted of two phases. Phase 11: participants were free to suggest any venue in the target cities as long as they existed in a reference venue collection (see Table 1). Phase 22: participants had to rerank a given list of venues in the target city for each user. erefore, the coordinators were able to release the ground truth for Phase 2, that is an essential step toward making a collection reusable.",1,ad,True
"In spite of such a empts, there are still some drawbacks with the current collection that can limit its reusability. First, even though the organizers released a crawl of the collection in 2016, it is unstructured and does not introduce a homogeneous set of data (see Table 1). Hence most top-ranked systems ignored it and crawled their collections. Second, the collection has a set of contextual data such as type and duration of the trip. However, the contextual data was neglected by most of the participants. In particular, many of them just ignored the contextual information or used it with handcra ed rule-based methods. It could be due to the current structure of the collection which does not give the researchers many options concerning context-aware recommendation.",0,,False
"In an a empt to address these limitations, we present a collection that was crawled very carefully to be homogeneous, cross platform, and context aware. More speci cally, we release the collection that we used for our participation at the TREC Contextual Suggestion track performing best in both phases of the track. e collection was crawled from two major LBSNs: Foursquare3 and Yelp4. We searched for the venues present in the TREC dataset on the LBSNs to nd their corresponding pro les and veri ed the retrieved data very carefully to prevent adding any noise to the dataset. It is worth noting that more than half of the submi ed systems to TREC 2016 had crawled data from either Yelp or Foursquare or both. More speci cally, we observed that for 12 tasks, namely, the last 2 years with 2 phases each and taking into account the top 3 systems, 11/12 (,92%) of the systems had crawled data from one or both sources and 7/12 (,""58%) of the systems crawled data from more than one LBSN. Hence, it is clear that there is the need for a uni ed, comprehensive dataset of this information which is available publicly. As for the contextual information, we created a secondary dataset""",1,ad,True
1Phase 1 is the 2016 equivalent of Live Experiment in 2015. For simplicity we refer to both of them as Phase 1. 2Phase 2 is the 2016 equivalent of Batch Experiment in 2015. For simplicity we refer to both of them as Phase 2. 3h p://www.foursquare.com 4h p://www.yelp.com,0,,False
1269,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Sample rows from the collection of venues used in TREC Contextual Suggestion 2015 and 2016.,1,TREC,True
ID,0,,False
TRECCS-00001768-394 TRECCS-00001776-394 TRECCS-00001777-394 TRECCS-00001927-182 TRECCS-00001934-182,1,TREC,True
Context,0,,False
394 394 394 182 182,0,,False
URL,0,,False
h p://shop.hobbylobby.com h p://dominos.com h p://www.arbys.com h p://www.milwaukeepublicmarket.org h p://www.botanasrestaurant.com,0,,False
Title,0,,False
Hobby Lobby Dominos Pizza Arbys Milwaukee Public Market Botanas Restaurant,0,,False
of contextual information that enables researchers to investigate di erent approaches to predict the contextual appropriateness of venues and study the in uence of context on user behavior.,0,,False
e released collection5 comprises:,0,,False
· more than 300K crawled venues from Foursquare providing enough information for venue recommendation research;,0,,False
· more than 15K crawled venues from Foursquare and Yelp providing information for cross-platform recommendation;,0,,False
· a human-annotated contextual appropriateness dataset containing human-tailored features for almost 2K context example features.,0,,False
e release of this new collection will provide researchers with a unique opportunity to develop context-aware venue recommender systems under the same se ing and data as the one of the bestsubmi ed systems in the TREC 2016. is will enable them to compare their work with state-of-the-art approaches and explore the brand new venue-context appropriateness dataset.,1,TREC,True
"e remainder of the paper is organized as follows: Section 2 brie y describes the TREC Contextual Suggestion track. Section 3 describes the method we used for the dataset construction. In Section 4 we provide more details on the dataset. Finally, Section 5 concludes the paper.",1,TREC,True
2 TREC CONTEXTUAL SUGGESTION TRACK,1,TREC,True
"In 2012 TREC introduced the task of Contextual Suggestion6 which provided a common evaluation framework for participants who wanted to deal with the challenging problem of improving contextual suggestions. More in details, given a set of example venues as user's preferences (pro le) and some contextual information, such as geographical, temporal, and personal contexts, the task consisted in returning a ranked list of candidate venues ing the user's pro le and context.",1,TREC,True
"e crawled dataset of this paper covers the places that were part of the last two years of the track, that is 2015 and 2016. For 2015 the dataset includes the venues for Phase 2, while for 2016 the dataset covers the venues for both Phase 1 and Phase 2. e additional contextual appropriateness dataset can also be used for 2015 and 2016 collections. In these tasks, there is a number of users, and for each user there is a list of 30 to 60 venues that a particular user has previously rated. Additionally, there is a set of contextual information for each user. Given such information, the task is to return a ranked list of candidate suggestions of venues based on their relevance to the user's needs and context. e collection has also a ground-truth made of relevance assessments which indicate",1,ad,True
5Available at h p://inf.usi.ch/phd/aliannejadi/data.html 6h ps://sites.google.com/site/treccontext/,1,ad,True
"whether a candidate suggestion is relevant to the user or not (i.e., whether the user likes the candidate suggestion or not).",0,,False
"For each venue the pro le includes a rating in the following range 4: very interested, 3: interested, 2: neutral, 1: uninterested, 0: very uninterested, and -1: not rated. Venues may also have tags that indicate why the user liked the particular venue. Contextual information is represented by: location, time, type of trip (Business, Holiday, or Other), duration of trip (Night out, Day trip, Weekend trip, or Longer), group of people involved (Alone, Friends, Family, or Other), and season (Winter, Summer, Autumn, or Spring). Moreover, user's age and gender are optionally included. is information can be exploited to have a be er understanding of user's context in order to recommend appropriate venues. For example, you know that a user liked or disliked some venues in ""New York City"", then she goes to ""Boston"" for a weekend trip, she is alone, and it is winter. Given such information, the recommendation system should be able to rank the candidate suggestions so that the top-ranked places are the most appropriate venues for the user to visit in ""Boston"".",0,,False
3 METHODOLOGY,0,,False
"We chose Foursquare and Yelp not only because they are two of the most popular LBSNs where many users leave their check-in data, but also because the type of information provided by Yelp is a perfect complement for the type of information on Foursquare. Moreover, as we will show in the statistics of the dataset, there is a considerable overlap of venues that have a pro le on both LBSNs. However, there are places in the TREC dataset that appear only on one of the two crawled LBSNs, hence crawling data from both of them allows making the data gathering more complete.",1,TREC,True
"Deveaud et al. [4] showed that venue-centric features which were extracted from Foursquare play a key role in venue recommendation. On the other hand, Chen et al. [3] argued that user reviews on venues provide a wealth of information that can be leveraged to address the data-sparsity and the cold-start problems for venue recommendation. Also, Yang et al. [6] showed that the accuracy of a recommender system can be signi cantly improved by extracting opinions from user reviews in Yelp. Also, almost all of the best performing systems in the TREC Contextual Suggestion track 2015 and 2016 [5] crawled data from these LBSNs. In particular, our previous work which were among the best runs in both 2015 [2] and 2016 [1] bene ted from a comprehensive crawled dataset from Yelp and Foursquare. We also showed that a system can bene t from multiple LBSNs, and systems using reviews from Yelp and venue tags from Foursquare had the best performance.",1,ad,True
1270,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 2: Sample rows from the collection on venues contextual appropriateness. e numbers in parentheses show the degree of agreement between di erent assessors and ranges from -1 (full agreement on no) to +1 (full agreement on yes). Hence, 0 means that there is no agreement between the assessors and so the task is subjective.",0,,False
Category,0,,False
Candy Store Library Pharmacy BBQ Joint Sandwich Place Lounge,0,,False
Trip Type,0,,False
Features Trip Duration,0,,False
Business (-0.60) Holiday (-1.00) Holiday (-0.60) Holiday (+1.00) Business (-0.60) Holiday (+0.58),0,,False
Day Trip (+0.60) Weekend Trip (-0.58) Night out (-0.60) Night out (-0.18) Longer trip (+0.20) Weekend trip (+1.00),0,,False
Group Type,0,,False
Friends (+1.00) Family (+0.62) Family (-0.18) Friends (+1.00) Alone (+0.60) Friends (+1.00),0,,False
Output Appropriateness,0,,False
No (-1.00) No (-1.00) No (-1.00) Yes (+1.00) Yes (+0.68) Yes (+1.00),0,,False
3.1 Data Crawling,0,,False
"For our collection, we crawled data from two LBSNs: Foursquare and Yelp. Foursquare provides an easy-to-access API7 which makes crawling quite easy. We used Foursquare's API to crawl a large number of venues and scraped a very smaller fraction of venues for additional information on the website. Yelp's API8, on the other hand, has more restrictions and therefore we were able to crawl much fewer venues on Yelp. For TREC 2016 Phase 1, we only crawled data using the Foursquare's API since there were virtually 630K venues to crawl in a very limited time and thus the only option was using the Foursquare's API. For Phase 2 of TREC 2015 and 2016, there was more time and much fewer venues to crawl so that we could crawl data from both LBSNs.",1,AP,True
"e data was crawled in two time periods: July - August 2015 and July - August 2016. To nd the corresponding pro les of venues on LBSNs, we used two search engines: 1) Foursquare venue search engine and 2) Google Custom Search. For TREC 2015 there were 8,794 venues from which we crawled 6,427 places from Yelp and 5,639 from Foursquare, with a considerable overlap between data crawled from Yelp and Foursquare. For TREC 2016 there were 18,808 venues from which we crawled 13,868 places from Yelp and 13,417 from Foursquare, again emerging a big overlap between the two sources. As all the venues are in the US cities, we expected that most of the users who reviewed the venues were English speakers.",1,TREC,True
"ery Structure. For each venue in the collection, we created a query to search on the LBSNs. e query consisted of a venue's name and its location. We cleaned the venues' names in the TREC collection since many contained unrelated terms such as the host service (e.g., Facebook, Wikipedia). Finally, the query we used to search for venues was in the form:",1,TREC,True
"query , venue's name + venue's city + venue's state .",0,,False
"Search Result Validation. Since we could not trust the results of search and in order to minimize the noise, we validated the returned results from the search engine following these steps:",0,,False
(1) We rst validated the city and state of the returned venue. (2) We then measured the similarity between the name of the,0,,False
venue and the name of the returned place using Levenshtein distance.,0,,False
7h ps://developer.foursquare.com/docs/ 8h ps://www.yelp.com/developers/documentation/v2/overview,0,,False
"(3) If the similarity between the two names (calculated in Step 2) was more than a threshold (70%), we considered that result as a match. If not, we continued steps 1-3 for other returned results up to the 5th result.",0,,False
Note that the high similarity threshold (70%) was set to prevent adding possible noise to the collection.,1,ad,True
3.2 Crowdsourcing,0,,False
"We used the CrowdFlower9 crowdsourcing platform to collect judgments of contextual appropriateness of venues and create the additional contextual-appropriateness dataset. We asked a number of crowdworkers to judge if a venue category is appropriate for a trip description. For instance, if a trip was described in the collection as trip type: business, trip duration: one day, and group type family, for a venue with category Pizza Place then we asked crowdworkers to judge if the venue was appropriate to the trip. In particular, we asked them: ""Is a Pizza Place appropriate for a business trip?"", ""Is it appropriate to go to a Pizza Place on a one-day trip?"", ""Is it appropriate to go to a Pizza Place with family?"" While assessing such tasks could seem trivial and objective, in fact it is subjective in many cases (e.g., going to a pharmacy with family). erefore, we asked at least 5 crowdworkers to provide their judgment to each row. If we found no agreement among the assessors, we considered the task as subjective. We considered the answer ""appropriate"" as a +1 score and ""inappropriate"" as a -1 score. us, the assessment agreement is the average of assessment scores. We asked workers to judge the context/category pairs for almost all possible pairs regardless of their existence in the TREC collections. is makes this dataset general enough to be used for other purposes.",1,ad,True
"We made sure to explain the task clearly to the workers and asked them to assess such appropriateness regardless of their personal preferences over categories. Also, we performed a training step and allowed only top-quality workers to do the task.",1,ad,True
4 COLLECTION,0,,False
"e released collection contains more than 330K venues from Foursquare for TREC 2016 Phase 1 and 15,765 venues from both Foursquare and Yelp for TREC 2016 Phase 2. As we can see in Table 4 there were many broken or unrelated links in the the TREC collection (300K out of 600K), however, there were much fewer unrelated links",1,TREC,True
9h p://www.crowd ower.com,0,,False
1271,0,,False
Short Resource Papers,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 3: Statistics on the crawled collection,0,,False
Phase 1 TREC'16,1,TREC,True
# venues in TREC collection # venues crawled: Yelp # venues crawled: Foursquare # Yelp and Foursquare overlap avg. reviews per venue avg. categories per venue avg. tags per venue avg. user tags per user # distinct user tags,1,TREC,True
"633,009 336,080 1.35 3.61 150",0,,False
Phase 2 TREC'15 TREC'16,1,TREC,True
"8,794 6,427 5,639 4,844 117.34 1.63 8.73 1.46 186",0,,False
"18,808 13,868 13,417 11,520 66.82 1.57 7.89 3.61 150",0,,False
Table 4: Statistics on the crowdsourced contextual appropriateness collection,0,,False
Number of categories Number of category-context pairs Number of assessments Average assessments per pair Average assessment agreement,0,,False
"179 1969 11,487 5.83 85%",0,,False
Number of full travel annotations 760,0,,False
"for Phase 2 (3K out of 18K). For each venue we release all available information: venue name, address, category, tags, ratings, reviews, check-in count, menu, opening hours, parking availability, etc.",1,ad,True
"e contextual-appropriateness collection consists of 1,969 pairs of trip descriptors and venue categories as features. In order to enable researchers to train their models using the contextual appropriateness of venues, we created another collection providing ground truth assessments for the contextual appropriateness of the venue categories. It completes the contextual information (i.e., trip type, group type, trip duration) for 10% of the whole TREC collection.",1,TREC,True
"is collection contains 760 rows including the features we already created using crowdsourcing and the context-appropriateness labels for venues. e 10% of labeled data allows to model the venues' contextual appropriateness given the users' context and to make prediction for the remaining 90% of the data, as we did in [1].",1,ad,True
"In Table 2 we report some rows of the collection, and Table 4 lists some statistics of the crawled collection. Figure 1 shows the histogram of venue-appropriateness features assessed by the workers. We divided the assessments in three groups based on appropriateness scores: [-1.00, -0.40): not appropriate (objective), [-0.40, +0.40]: no agreement (subjective), (+0.40, +1.00]: appropriate (objective). To categorize the tasks as subjective and objective, we assumed that those tasks for which there was a high agreement between the assessors could be considered objective since everybody agreed on their (in)appropriateness. While we assumed that those tasks with relatively lower agreement between the assessors could be considered subjective.",0,,False
Figure 1: Histogram of venue-context appropriateness score ranges. We partition the histogram into 3 parts based on the scores range. Scores below -0.4 represent inappropriateness and score higher than +0.4 represent appropriateness. Scores between -0.4 and +0.4 do not provide much information and show no agreement among assessors (subjective task).,0,,False
5 CONCLUSIONS,0,,False
"In this paper we present the dataset we used for our participation to the TREC Contextual Suggestion tracks. We crawled information of venues used in the TREC dataset from two popular LBSNs. Also we collected, using crowdsourcing, ratings on the venue-context appropriateness which allows to make predictions on the appropriateness of a recommended venue to a given user's context. Such collection can be helpful for other researchers interested in comparing their venue-recommendation techniques against state-of-the-art approaches. It could also foster further research on contextual suggestions of venues.",1,TREC,True
ACKNOWLEDGMENTS,0,,False
is research was partially funded by the RelMobIR project of the Swiss National Science Foundation (SNSF).,0,,False
REFERENCES,0,,False
[1] Mohammad Aliannejadi and Fabio Crestani. 2017. Venue Appropriateness Prediction for Personalized Context-Aware Venue Suggestion. In SIGIR 2017. ACM.,1,ad,True
"[2] Mohammad Aliannejadi, Ida Mele, and Fabio Crestani. 2016. User Model Enrichment for Venue Recommendation. In AIRS 2016. Springer, 212­223.",1,ad,True
"[3] Li Chen, Guanliang Chen, and Feng Wang. 2015. Recommender systems based on user reviews: the state of the art. UMUAI 25, 2 (2015), 99­154.",0,,False
"[4] Romain Deveaud, M-Dyaa Albakour, Craig Macdonald, and Iadh Ounis. 2015. Experiments with a Venue-Centric Model for Personalised and Time-Aware Venue Suggestion. In CIKM 2015. ACM, 53­62.",1,ad,True
"[5] Seyyed Hadi Hashemi, Charles L. A. Clarke, Jaap Kamps, Julia Kiseleva, and Ellen M. Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track. In TREC 2016. NIST.",1,ad,True
"[6] Peilin Yang, Hongning Wang, Hui Fang, and Deng Cai. 2015. Opinions ma er: a general approach to user pro le modeling for contextual suggestion. Information Retrieval Journal 18, 6 (2015), 586­610.",0,,False
1272,0,,False
,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
EvALL: Open Access Evaluation for Information Access Systems,0,,False
"Enrique Amigo´, Jorge Carrillo-de-Albornoz, Mario Almagro-Ca´diz, Julio Gonzalo, Javier Rodr´iguez-Vidal and Felisa Verdejo nlp.uned.es Universidad Nacional de Educacio´n a Distancia (UNED) Calle Juan del Rosal 16, Madrid, Spain",1,ad,True
"{enrique,jcalbornoz,malmagro,julio,jrodriguez,felisa}@lsi.uned.es",0,,False
ABSTRACT,0,,False
"e EvALL online evaluation service aims to provide a uni ed evaluation framework for Information Access systems that makes results completely comparable and publicly available for the whole research community. For researchers working on a given test collection, the framework allows to: (i) evaluate results in a way compliant with measurement theory and with state-of-the-art evaluation practices in the eld; (ii) quantitatively and qualitatively compare their results with the state of the art; (iii) provide their results as reusable data to the scienti c community; (iv) automatically generate evaluation gures and (low-level) interpretation of the results, both as a pdf report and as a latex source. For researchers running a challenge (a comparative evaluation campaign on shared data), the framework helps them to manage, store and evaluate submissions, and to preserve ground truth and system output data for future use by the research community.",0,,False
EvALL can be tested at h p://evall.uned.es.,0,,False
KEYWORDS,0,,False
"Information Retrieval, Information Access, Evaluation, Evaluation infrastructure, Evaluation Metrics",0,,False
"ACM Reference format: Enrique Amigo´, Jorge Carrillo-de-Albornoz, Mario Almagro-Ca´diz, Julio Gonzalo, Javier Rodr´iguez-Vidal and Felisa Verdejo nlp.uned.es Universidad Nacional de Educacio´n a Distancia (UNED) Calle Juan del Rosal 16, Madrid, Spain {enrique,jcalbornoz,malmagro,julio,jrodriguez,felisa}@lsi.uned.es. 2017. EvALL: Open Access Evaluation for Information Access Systems. In Proceedings of SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan., , 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080822",1,ad,True
1 INTRODUCTION,1,DUC,True
"In spite of the strong focus of Information Retrieval (and Information Access in general) on comparative evaluation and replicability, researchers still face many challenges at the time of assessing the quality of their systems with respect to the state of the art. For",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-4887-4/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080822",1,ad,True
"instance: system descriptions are o en not detailed enough and prevent replication of results, datasets are sometimes di cult to obtain or subject to privacy issues, etc. We focus here on a particular set of issues regarding evaluation:",0,,False
"· Finding all relevant state of the art results is costly. Once a suitable test collection is selected (and acquired), locating state of the art results on the test collection takes time and e ort. If the test collection was created as part of a shared evaluation campaign, it is usually easy to nd descriptions and results of the participating systems, but anything published a er the campaign (where algorithms and results are usually optimized) takes time and e ort to locate.",0,,False
"· Unavailability of system outputs prevents full comparison with state of the art systems. Usually, system performance is reported in terms of a few evaluation metrics, and therefore comparison with previous systems can only be established in a very limited way. When comparing with state of the art systems, no alternative metrics can be used, and no qualitative or per-test-case analysis can be performed. A detailed comparison between systems would only be possible if system outputs were available, and this is not generally the case.",0,,False
"· Proper choice, use and interpretation of adequate evaluation metrics is not straightforward. Because comparison with state of the art is essential, researchers tend to focus on popular evaluation methodologies and metrics, even if the state of the art on evaluation has moved on. As a result, metrics with preferable formal and empirical properties are o en dismissed in favor of legacy metrics that warrant backwards comparability. In Document Retrieval, for instance, the adoption of state-of-the-art metrics is remarkably slow compared with the pace of innovation in the area. In fact, selecting appropriate evaluation metrics as a function of the scenario and task at hand, and understanding what they say about system performance, what they do not say, and how they complement each other, is still challenging for most general problems (such as retrieval, clustering and classi cation, which are the three main document organization task families). Researchers tend to focus on system development, and spend li le time in selecting and understanding evaluation metrics. Again, resorting to the most popular metrics is a safe alternative, at the cost of a suboptimal (and sometimes misleading) interpretation of the experimental results.",1,ad,True
1301,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 1: EvALL default work ow using a stored benchmark,0,,False
"· Sharing system outputs is not trivial. Once results are published as scholar articles, there is still no generalized standard procedure to release system outputs together with them, to be used as a reference for future research on the same test collection.",0,,False
"e EvALL system (h p:/evall.uned.es) has been designed as an online service for the Information Retrieval and Natural Language Processing research communities that addresses all the above issues, for most of the relevant task families in both elds. In essence, EvALL stores system outputs and gold-standard references, and lets researchers evaluate exhaustively their systems with respect to stored state-of-the-art systems and references, and also upload and publish gold-standard annotations for new collections and new system outputs.",1,ad,True
"With respect to other online evaluation services in the areas of Information Retrieval and Natural Language Processing, EvALL has two distinctive features: (i) it aspires to be a universal evaluation service for the IR & NLP communities, while current evaluation platforms are focused on speci c tasks. To make this feasible and cost-e ective, it is focused on the storage of system outputs and gold-standard solutions, leaving aside test collections, systems, executions, work ows, etc.; and (ii) it makes a strong emphasis on assisting users in the choice and interpretation of evaluation metrics, based on a formal analysis and typi cation of metric properties [2, 3].",0,,False
2 RELATED WORK,0,,False
"One of the pillars of scienti c and technological process is an easily accesible state of the art. In the broad eld of Collaborative Science, di erent initiatives promoting transparency have recently emerged in relation to the preservation of research resources, such as data repositories ­ such as FigShare (h ps:// gshare.com) or",1,ad,True
"Dryad (h p://datadryad.org) ­ or code repositories such as GitHub (h ps://github.com) and Zenodo (h ps://zenodo.org/). Also, so ware journals, such as Data in Brief, have started to store datasets as publications. Nevertheless, none of these initiatives focus on improving communication between algorithmic methods, and thus replicability can be di cult in spite of total access to so ware and datasets. Scienti c WorkFlows [9] and Science Gateways [1] have been proposed to address the excessive complexity of experiments over the last decade.",1,ad,True
"Focusing on evaluation in the areas of Information Retrieval and Natural Language Processing, evaluation campaigns have usually been the creators and promoters of assessment tools such as trec eval [5] for Document Retrieval tasks; and general-purpose Machine-Learning enviroments such as Weka [6] usually integrate evaluation modules for classi cation tasks. In general, such tools do not have a preservation layer, and do not allow to store and share system outputs and evaluation results with the research community.",1,trec,True
"On the other hand, there have also been notable e orts to provide online evaluation services, usually for speci c tasks. In the context of Information Retrieval, [4] highlighted the lack of consistency reporting progress with respect to the state of the art in IR, and developed an online evaluation service, EvaluatIR, which provided a repository for IR system runs and evaluation results to ""allow comparison between results submi ed by di erent research groups at di erent times"". More recently, [10] presented RISE (Reproducible Information Retrieval System Evaluation), a Web-based service (built on top of a modi ed version of the Indri toolkit) that implements more than 20 state of the art retrieval functions, and evaluates them over 16 standard TREC collections. RISE is designed to facilitate the implementation and evaluation of retrieval functions, and the system hosts the data collections instead of shipping the data collections to researchers, which can ensure the privacy of the collections. In this sense, RISE implements the Evaluation as a Service (EaaS)",1,TREC,True
1302,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"philosophy [7], which aims at providing infrastructure support for evaluation such that datasets are stored centrally and accessed (usually via an API) for evaluation purposes by researchers, which circumvents legal restrictions on dataset distribution, facilitates scalability for system developers, and improves system comparability (at the cost of reducing system diversity). In the context of Natural Language Processing, a remarkable example is Gerbil [8], an evaluation framework for entity annotators that stores and archives systems, datasets, evaluation tools and experimental results.",1,AP,True
"While similar to EvALL with respect to the goals of preserving evaluation data and facilitating and standardizing the evaluation process, the di erence with respect all the above services is in scope and depth:",0,,False
"(i) All previous online evaluation services are focused on a speci c task or set of related tasks (around entity annotation, for instance, in the case of Gerbil), and they store everything that is needed to carry on meaningful evaluation for such tasks (including centralized datasets and/or code). EvALL, on the other hand, aims to maximize the coverage of Information Access tasks at the most abstract level (starting with classi cation, ranking and clustering problems) by minimizing what is preserved and stored: system outputs, gold standards, evaluation metrics and procedures, and experimental results.",0,,False
"(ii) EvALL also focuses on documentation and a careful choice and explanation of metrics. O en, users ignore how to correctly interpret metrics and what do they say di erently about system behavior. In EvALL, explanatory reports are generated with all the required formal and mathematical background to gure out what evaluation metrics have been applied and how they have been used.",0,,False
"(iii) Finally, EvALL has been designed to have the lowest possible cost of entry; unlike other evaluation facilities, it can be used even with no prior knowledge of evaluation procedures. And registration is only needed if persistent storage of system outputs and/or gold standards is required. As a result of its simplicity, we expect to foster its take-up by the research community, which is one of the major challenges for this type of services. And we also expect EvALL to be particularly useful in master courses in the eld as a gentle introduction to evaluation metrics and system performance analysis.",0,,False
3 EVALL,0,,False
3.1 System Description,0,,False
"For researchers working on a given benchmark / test collection, the EvALL online evaluation service (http://evall.uned.es) lets users:",0,,False
"(i) evaluate their results in a way compliant with measurement theory and with state-of-the-art evaluation practices in the eld. Researchers only have to indicate either the reference collection (if it is already stored in EvALL) or the type of task (otherwise). Depending on the type of task, EvALL selects all suitable measures, checks their preconditions (in the system outputs and the gold standard), and generates the evaluation results. e EvaLL output includes a brief description of the selected measures that summarizes their motivation and properties, indicates their limitations, and provides relevant associated bibliography for further investigation. EvALL exploits the fact that most Information Access tasks belong to a",1,ad,True
"few abstract problems (classi cation, ranking and clustering being the ones currently handled by the system). e organization and description of measures follows the axiomatic approach introduced in [2, 3].",0,,False
"(ii) quantitatively and qualitatively compare their results with the state-of-the-art. If the gold standard is already stored in EvALL, the system provides a repository of baselines and stateof-the-art systems, so that the new system can be compared in detail (rather than just in overall performance) with state-of-the-art approaches. Note that, if the gold standard is not stored in EvALL yet, users can still evaluate their systems by providing a reference gold standard and specifying the type of task.",1,ad,True
"(iii) provide their results as reusable data to the scienti c community. No logging is necessary to use EvALL; but upon login, researchers can choose to share their system outputs and evaluation results with the community. Also, they can choose to upload a new test collection by providing a description of the task, a gold standard, and suitable system outputs (possibly including baseline approaches). Here, EvALL exploits the fact that system outputs and gold standards (reference annotations) are considerably lighter than datasets and code, and sharing them has signi cantly less Intellectual Property issues. For researchers running a challenge (a comparative evaluation campaign on shared data), the framework helps them to manage, store and evaluate submissions, and to preserve ground truth and system output data for future use by the research community.",1,ad,True
"(iv) automatically generate evaluation gures and (low-level) interpretation of the results. EvALL produces as output a pdf report and its Latex source les, that allow researchers to easily copy and paste tables, descriptions, or results analysis into their publications. In addition, EvALL also generates a set of TSV reports containing all data in a more ne-grained way, which allows researchers to do further experimentation and analysis. EvALL is used via a web interface (h p://evall.uned.es) which also includes other features such as statistical signi cance tests, metric smoothing where appropriate, personalization of evaluation reports, default versus manual measure selection and parameterization, standardization of the input format across tasks, and warnings and statistics about the system outputs.",1,ad,True
"In summary, with a single click the user obtains LaTeX-forma ed information with results in terms of multiple measures, statistical signi cance tests, and system output data veri cation, as well as information about the properties and limitations of the measures. And also, upon login, users can share their system outputs and share new test collections (as represented by a gold standard, baselines, assorted system outputs and speci cations for the evaluation, such as, for instance, evaluation campaign guidelines).",0,,False
"So ware-wise, EvALL is designed in two independent components: the web service (EvALL web), and the evaluation library (EvALL Toolkit). e rst one is implemented on the Liferay framework in combination with a MySQL database. e web service manages user requests, stores and interacts with the repositories, and handles users management. It also interacts with the EvALL toolkit via its API to serve user requests. In the EvALL toolkit, all evaluation metrics have been re-implemented by the authors in Java. In order to minimize potential errors, every metric has been implemented twice, and its output has been cross-checked with",1,AP,True
1303,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
third party implementations whenever possible (e.g. some ranking measures have been cross-checked with trec eval so ware).,1,trec,True
3.2 Work ows,0,,False
"Once they enter the system, users are requested to select one out of four main actions:",0,,False
"· Evaluate using an existing benchmark. In this case, users provide one or more system outputs and choose the details of the evaluation procedure.",0,,False
"· Evaluate using their own benchmark. In this case, users are guided to de ne the type of task, and to upload a gold standard in addition to system outputs and baselines.",1,ad,True
"· Publish a new system output. Upon registration, users can add a system output to a dataset that is already stored in EvALL.",1,ad,True
"· Publish a new benchmark. Upon registration, it is also possible to de ne and upload materials to include a new benchmark in EvALL and share it with the research community. Note that the test collection itself is not uploaded (which would complicate ma ers legally from the point of view of distribution); only the gold standard and basic evaluation speci cations (type of task, o cial metrics, etc.) are stored in the system.",1,ad,True
"Let us seen, for instance, the system work ow when the user selects the rst option (evaluate using an existing benchmark). e user is then requested to (a) choose a benchmark by browsing or searching the repository of tasks already included in EvALL, via a faceted search interface which allows to lter results by year, conference and/or keyword; and (b) select a con guration for the evaluation procedure, which can be by default or customized.",1,ad,True
"If the default option is chosen, users are asked to select or upload their system outputs, and EvALL directly produces the results of the evaluation process: (i) a pdf report that includes latex sources (ii) a tsv le with the evaluation output, and (iii) the result of a consistency check on system outputs, with warnings in case of inconsistent formats. In the default report, EvALL makes its own selection of appropriate metrics and reference systems (which include, for instance, the best stored system as the state-of-the-art reference), and provides a full report with all the theoretical explanations needed to interpret metric results. e default report is verbose in order to be self-contained: even with no previous knowledge of evaluation metrics, the reader has all the information needed to understand and interpret the results. Figure 1 displays screen captures for this default work ow.",1,ad,True
"If the customized option is chosen, the user is requested to make additional choices:",1,ad,True
"(1) e system lets the user select metrics by choosing one of three options: (a) o cial set of metrics (as prescribed in the corresponding evaluation campaign, if there is one); (b) full set of metrics (all appropriate metrics implemented in EvALL), or (c) a fully customized set of metrics. In the last option, the user makes a multiple-selection choice of metrics, and may optionally set metric parameters (such as the  parameter that sets the relative weight between metrics in the F-measure).",0,,False
"(2) e user is then asked to select systems to be included in the comparison. It might select options such as ""best stored system"" according to the selected metrics, or go to a completely manual selection among stored systems.",0,,False
"(3) Finally, the user is asked to customize the evaluation report, with the possibility of removing the metric descriptions, the latex sources, the tsv le, the output veri cation, etc.",0,,False
3.3 EvALL coverage: tasks and measures,0,,False
"One of the main goals of EvALL is becoming a universal evaluation tool for any Information Access problem. We have inspected all tasks proposed in TREC, CLEF and SemEval in 2016, and our evaluation service could be currently used in 47 out of 63 tasks (74%).",1,TREC,True
"us, potentially, gold standards and system outputs for 74% of all these tasks could be stored in EvALL for evaluation purposes. And, even without storing them, researchers can evaluate their systems by entering their outputs and the gold standard for any of these tasks. Coverage would increase to 84% by incorporating text similarity metrics (such as ROUGE or WER) and value prediction estimators (such as MAE or Pearson correlation).",1,corpora,True
"A key issue for the success of EvALL is how to overcome the cold-start problem. Even without a dataset of tasks, gold standards and system outputs, it still provides a fast and low-cost way of evaluating systems in a wide range of Information Access tasks. But, ultimately, its true success lies in its adoption by the research community, which will lead to a growing database of gold standard and system outputs. Our plan is to start working with shared task organizers to facilitate its uptake by the community.",1,ad,True
Acknowledgements: is work has been partially funded by the Span-,0,,False
"ish Government (grants Vemodalen, TIN2015-71785-R, and Vox-Populi,",1,Gov,True
TIN2013-47090-C3-1-P),0,,False
REFERENCES,0,,False
[1] Robert N Allan. 2009. Virtual research environments: From portals to science gateways. Elsevier.,0,,False
"[2] Enrique Amigo´, Julio Gonzalo, Javier Artiles, and Felisa Verdejo. 2009. A comparison of extrinsic clustering evaluation metrics based on formal constraints. Information retrieval 12, 4 (2009), 461­486.",0,,False
"[3] Enrique Amigo´, Julio Gonzalo, and Felisa Verdejo. 2013. A general evaluation measure for document organization tasks. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, 643­652.",0,,False
"[4] Timothy G. Armstrong, Alistair Mo at, William Webber, and Justin Zobel. 2009. Improvements at Don'T Add Up: Ad-hoc Retrieval Results Since 1998. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM '09). ACM, New York, NY, USA, 601­610. DOI: h p://dx.doi.org/10.1145/1645953.1646031",1,hoc,True
"[5] C Buckley and others. 2004. e trec eval evaluation package. (2004). [6] Mark Hall, Eibe Frank, Geo rey Holmes, Bernhard Pfahringer, Peter Reutemann,",1,trec,True
"and Ian H Wi en. 2009. e WEKA data mining so ware: an update. ACM SIGKDD explorations newsle er 11, 1 (2009), 10­18. [7] Jimmy Lin and Miles Efron. 2014. Infrastructure Support for Evaluation As a Service. In Proceedings of the 23rd International Conference on World Wide Web (WWW '14 Companion). ACM, New York, NY, USA, 79­82. DOI:h p://dx.doi. org/10.1145/2567948.2577014 [8] R. Usbeck, M. Roder, and A. Ngonga. 2015. GERBIL ­ General Entity Annotator Benchmarking Framework. In Proc. 25th World Wide Web Conference. ACM. [9] WFMC. 1994. Work ow reference model. Technical Report. Work ow Management Coalition, Brussels. [10] Peilin Yang and Hui Fang. 2016. A Reproducibility Study of Information Retrieval Models. In Proceedings of the 2016 ACM International Conference on the eory of Information Retrieval (ICTIR '16). ACM, New York, NY, USA, 77­86. DOI: h p://dx.doi.org/10.1145/2970398.2970415",0,,False
1304,0,,False
,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
LSTM vs. BM25 for Open-domain QA: A Hands-on Comparison of Effectiveness and Efficiency,0,,False
Sosuke Kato,0,,False
Waseda University sow@suou.waseda.jp,0,,False
Riku Togashi,0,,False
Yahoo Japan Corporation rtogashi@yahoo-corp.jp,1,Yahoo,True
Hideyuki Maeda,0,,False
Yahoo Japan Corporation hidmaeda@yahoo-corp.jp,1,Yahoo,True
Sumio Fujita,0,,False
Yahoo Japan Corporation sufujita@yahoo-corp.jp,1,Yahoo,True
ABSTRACT,0,,False
"Recent advances in neural networks, along with the growth of rich and diverse community question answering (cQA) data, have enabled researchers to construct robust open-domain question answering (QA) systems. It is often claimed that such state-of-theart QA systems far outperform traditional IR baselines such as BM25. However, most such studies rely on relatively small data sets, e.g., those extracted from the old TREC QA tracks. Given massive training data plus a separate corpus of Q&A pairs as the target knowledge source, how well would such a system really perform? How fast would it respond? In this demonstration, we provide the attendees of SIGIR 2017 an opportunity to experience a live comparison of two open-domain QA systems, one based on a long short-term memory (LSTM) architecture with over 11 million Yahoo! Chiebukuro (i.e., Japanese Yahoo! Answers) questions and over 27.4 million answers for training, and the other based on BM25. Both systems use the same Q&A knowledge source for answer retrieval. Our core demonstration system is a pair of Japanese monolingual QA systems, but we leverage machine translation for letting the SIGIR attendees enter English questions and compare the Japanese responses from the two systems after translating them into English.",1,ad,True
CCS CONCEPTS,0,,False
· Information systems  Question answering;,0,,False
KEYWORDS,0,,False
community question answering; long short-term memory; question answering,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 http://dx.doi.org/10.1145/3077136.3084147",1,ad,True
Tetsuya Sakai,0,,False
Waseda University tetsuyasakai@acm.org,0,,False
1 INTRODUCTION,1,DUC,True
"Recent advances in neural networks, along with the growth of rich and diverse community question answering (cQA) data, have enabled researchers to construct robust open-domain question answering (QA) systems. It is often claimed that such state-of-theart QA systems far outperform traditional IR baselines such as BM25 [8] (See Section 2). However, most such studies rely on relatively small data sets, e.g., those extracted from the old TREC QA tracks. Given massive training data plus a separate corpus of Q&A pairs as the target knowledge source, how well would such a system really perform? How fast would it respond?",1,ad,True
"In this demonstration, we provide the attendees of SIGIR 2017 an opportunity to experience a live comparison of two open-domain QA systems, one based on a long short-term memory (LSTM) [10] architecture with over 11 million Yahoo! Chiebukuro (i.e., Japanese Yahoo! Answers) questions and over 27.4 million answers for training, and the other based on BM25. Both systems use the same Q&A knowledge source for answer retrieval. Our core demonstration system is a pair of Japanese monolingual QA systems, but we leverage machine translation for letting the SIGIR attendees enter English questions and compare the Japanese responses from the two systems after translating them into English.",1,Yahoo,True
"Yahoo! Chiebukuro is the major cQA service in Japan, which claimed 714 million pageviews in October 2014. As of December 2014, it had over 84 million questions. While we only have 100,000 questions with 241,994 answers in the target Q&A knowledge source at the time of this writing, we plan to deliver a demonstration with a much larger data set for answer retrieval.",1,Yahoo,True
2 RELATED WORK,0,,False
"TREC (Text Retrieval Conference) ran the QA Track from 1999 to 2007; over the years, the task evolved from the relatively simple factoid QA to question series ending with others questions, which meant ""Tell me other interesting things about this target I don't know enough to ask directly"" [2] and therefore solicited complex answers. Other evaluation tasks have also tackled the open-domain QA problem: examples include the QAC (Question Answering Challenge) [3] and the ACLIA (Advanced Crosslingual Information Access) [7] tasks of NTCIR, which were run from 2002 to 2010 when taken together.",1,TREC,True
"While the above efforts in QA aim at automatic extraction of good answers from corpora such as the web and news, leveraging cQA as the knowledge source has also become a promising way to",1,corpora,True
1309,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"tackle the QA problem, as both questions and answers in cQA data are diverse and rich in content. In 2012, Liu et al. reported that unsuccessful web searchers often turn to cQA [5]. The NTCIR-8 cQA task [4] tackled the problem of ranking cQA answers given a question, by leveraging the best answers data available in the Yahoo! Chiebukuro data. However, the entire Yahoo! Chiebukuro data set from that task contained only about 3.1 million resolved questions from 2004 and 2005, which is substantially smaller compared to the data used in our demonstration.",1,Yahoo,True
"More recently, the TREC LiveQA track was launched [1], where unresolved cQA questions are sampled in real time and the systems are expected to respond effectively to them. Wang and Nyberg [10], the top performer in that track, first searched the web and Yahoo! Answers to obtain answer clues, which are texts similar to the question string, and answer passages, which are likely to contain sentences that serve as the answer to the question. Then they ranked candidate sentences based on answer clue scores that leverage BM25 as well as answer passage scores that leverage a multilayer stacked bidirectional LSTM (BLSTM) where the input words are pre-trained by skip-gram-based word embedding [6].",1,TREC,True
"Wang and Nyberg [11] also tackled the problem of answer selection for QA using a combination of a BLSTM approach and BM25. Based on an experiment that utilised topics from the QA tracks of TREC-8 through 13, they reported that their combination of BLSTM and BM25 outperformed BM25 as well as other previously reported methods that were evaluated on the same data. However, it remains to be seen whether their conclusion will generalise to more realistic situations where we have millions of questions and answers. Tan et al. [9] proposed QA-LSTM, Convolutional-pooling and Convolution-based LSTM, and Attentive LSTM, and reported further improvements on the aforementioned data set.",1,TREC,True
3 LSTM-BASED QA,0,,False
"This section describes the LSTM-based algorithm that we use for our QA demonstration system. LSTM-based answer selection consists of two phases: training and retrieval. In the training phase, given a question and an answer, the trainer converts each of them into a representation vector as described below and calculates the distance between these vectors. Figure 1 provides an overview of how we calculate the distance between a question and an answer .",0,,False
Figure 1: Computation of the distance between questions and answers with LSTM.,0,,False
"Using Mecab1, a widely-used Japanese morphological analysis tool, we tokenise both the given question and the answer, and then convert them into a word embedding vector pre-trained by a skipgram method with the Japanese Wikipedia data2. Similar to the system of Tan et al. [9], our BLSTM takes a sequence of word embedding vectors as input and yields an encoded vector as output. Given a sequence of word embedding vectors X ,"" x1, . . . , xt , . . . , xn , a hidden vector ht at time step t is updated as follows:""",1,Wiki,True
"it ,  (Wi xt + Uiht -1 + bi ) ft ,  (Wf xt + Uf ht -1 + bf ) ot ,  (Woxt + Uoht -1 + bo ) jt , tanh(Wj xt + Ujht -1 + bj ) Ct , it  jt + ft  Ct -1 ht ,"" ot tanh(Ct ),""",0,,False
"where an LSTM has three gates, namely input i, forget f and output o, and a cell memory vector Ct .  is the sigmoid function. W  RH ×E , U  RH ×H and b  RH ×1 are the network parameters. E is the dimension of word embedding vectors, and H is that of hidden vectors; we let E , 600 and H ,"" 600. A BLSTM processes the sequence from forward and backward directions, generating two sequences of output vectors, which are then concatenated. We use 1,200-dimensional output vectors at each time step. Given the concatenated output vectors from BLSTM, max pooling over the concatenated vectors is performed in order to generate fixed-sized distributed vector representations of a sequence of tokens. Given token sequences of a question-answer pair, the question and answer BLSTMs generate a vector representation of the question and the answer, respectively. The distance between the question and the answer is calculated as the euclidean distance between thus generated vectors.""",0,,False
"We train the parameters of two BLSTMs and a embedding matrix to minimize triplet loss, which is formulated as follows:",0,,False
"L ,"" max{0, M + d(q, a+) - d(q, a-)}""",0,,False
"where d denotes a euclidean distance function and q denotes a question representation vector. a+ and a- denote positive and negative example answers, respectively. M denotes the margin of the separation; we let M , 0.2.",0,,False
"In the retrieval phase, given a question, we rank answers by euclidean distance by the same mechanism depicted in Figure 1.",0,,False
4 DEMONSTRATION SYSTEM,0,,False
4.1 Overview,0,,False
"Figure 2 shows an overview of our demonstration system. We let SIGIR 2017 attendees enter arbitrary questions in either Japanese or English; English questions are automatically translated into Japanese. As can be seen, the Japanese question is fed to the LSTM component and the BM25 component simultaneously, so that retrieved answers obtained with each method can be viewed side by side. The retrieved Japanese answers are also translated into English.",0,,False
1 http://taku910.github.io/mecab/ 2 https://dumps.wikimedia.org/jawiki/,1,wiki,True
1310,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 2: Overview of the demonstration system.,0,,False
4.2 Yahoo! Chiebukuro Data,1,Yahoo,True
Table 1: Number of questions and answers in our test and training dataset.,0,,False
(a) training data (b) Q&A knowledge source,0,,False
"# of questions 11,074,960",0,,False
"100,000",0,,False
# of answers,0,,False
"27,429,741",0,,False
"241,994",0,,False
"Table 1 shows the statistics of the Yahoo! Chiebukuro data we use. Column (a) shows the number of questions and the number of corresponding answers for training our LSTM component. Column (b) shows the scale of the target knowledge source used by both LSTM and BM25; for the actual demonstration, we plan to expand this data set so that we can cover as diverse questions as possible with the answers that we have for retrieval.",1,Yahoo,True
4.3 Ranking Components,0,,False
"Our LSTM component trains the parameters of BLSTM and an embedding matrix using the aforementioned training data, where we use only posts with 50 tokens or less. After the training is done, our LSTM-component indexes answers in the Q&A knowledge source, by converting answer sentences into representation vectors using the BLSTM for answers, where the dimension of representation vectors is 1,200, which are then indexed with NGT3, a spatial indexing tool. Due to the memory limitation of GPU boards, we only indexed answers shorter than 50 tokens. Table 2 shows the elapsed",0,,False
3 https://github.com/yahoojapan/NGT,0,,False
"Table 2: Elapsed time of the LSTM component to retrieve the top n answers (n ,"" 10, 1000).""",0,,False
retrieving top n,0,,False
"n , 10 n , 1000",0,,False
average elapsed time [msec] 5.76 41.31,0,,False
"time of the LSTM component for retrieving the top 10 and 1,000 answers from 155,648 candidates.",0,,False
"On the other hand, our BM25 component uses Elasticsearch (v5.2.1) 4 with default settings. We indexed all answers in the Q&A knowledge source for retrieval. Search queries are formulated by extracting only nouns, verbs and adjectives from the input question after performing morphological analysis with Mecab.",1,ad,True
4.4 Examples,0,,False
"Figure 3 shows an example question from our target Q&A knowledge source, and the answers retrieved by LSTM and BM25, with rough English translations. Note that in the actual demonstration, attendees can enter arbitrary questions, whose topics may be outside the target Q&A knowledge source. How will LSTM and BM25 compare in terms of effectiveness and efficiency?",0,,False
5 SUMMARY,0,,False
"Given the abundance of data, are neural network-based QA systems going to completely replace traditional IR-based systems in the near future? Please come to the demo, enter a question, compare the answers, and let us know what you think.",1,ad,True
REFERENCES,0,,False
"[1] Eugene Agichtein, David Carmel, Dan Pelleg, Yuval Pinter, and Donna Harman. 2016. Overview of the TREC 2015 LiveQA Track. In Proceedings of TREC 2015.",1,TREC,True
"[2] Hoa Trang Dang, Diane Kelly, and Jimmy Lin. 2008. Overview of the TREC 2007 Question Answering Track. In Proceedings of TREC 2007.",1,TREC,True
"[3] Junichi Fukumoto, Tsuneaki Kato, Fumito Masui, and Tatsunori Mori. 2007. An Overview of the 4th Question Answering Challenge (QAC-4) at NTCIR Workshop 6. In Proceedings of NTCIR-6.",0,,False
"[4] Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando. 2010. Overview of the NTCIR-8 Community QA Pilot Task (Part I): The Test Collection and the Task. In Proceedings of NTCIR-8.",0,,False
"[5] Qiaoling Liu, Eugene Agichtein, Gideon Dror, Yoelle Maarek, and Idan Szpektor. 2012. When Web Search Fails, Searchers Become Askers: Understanding the Transition. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval. 801­810.",0,,False
"[6] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Proceedings of NIPS 2013. 3111­3119.",1,ad,True
"[7] Teruko Mitamura, Hideki Shima, Tetsuya Sakai, Noriko Kando, Tatsunori Mori, Koichi Takeda, Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, and Cheng-Wei Lee. 2010. Overview of the NTCIR-8 ACLIA Tasks: Advanced Cross-Lingual Information Access. In Proceedings of NTCIR-3.",0,,False
"[8] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Mike Gatford, and A. Payne. 1996. Okapi at TREC-3. In Proceedings of TREC-3.",1,TREC,True
"[9] Ming Tan, Cícero Nogueira dos Santos, Bing Xiang, and Bowen Zhou. 2016. Improved Representation Learning for Question Answer Matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. 464­ 473.",0,,False
[10] Di Wang and Eric Nyberg. 2015. CMU OAQA at TREC 2015 LiveQA: Discovering the Right Answer with Clues. In Proceedings of TREC 2015.,1,TREC,True
[11] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers). 707­712.,0,,False
4 https://www.elastic.co/jp/products/elasticsearch,0,,False
1311,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Figure 3: An example question, with answers returned by the LSTM-based system (left) and the BM25-based system (right). Rough English translations are provided by one of the authors; in the actual demonstrations, machine translation will be used.",0,,False
1312,0,,False
,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Visual Pool: A Tool to Visualize and Interact with the Pooling Method,0,,False
Aldo Lipani,0,,False
"TU Wien Institute of So ware Technology & Interactive Systems, Vienna, Austria",0,,False
aldo.lipani@tuwien.ac.at,0,,False
Mihai Lupu,0,,False
"TU Wien Institute of So ware Technology & Interactive Systems, Vienna, Austria",0,,False
mihai.lupu@tuwien.ac.at,0,,False
Allan Hanbury,0,,False
"TU Wien Institute of So ware Technology & Interactive Systems, Vienna, Austria",0,,False
allan.hanbury@tuwien.ac.at,0,,False
ABSTRACT,0,,False
"Every year more than 25 test collections are built among the main Information Retrieval (IR) evaluation campaigns. ey are extremely important in IR because they become the evaluation praxis for the forthcoming years. Test collections are built mostly using the pooling method. e main advantage of this method is that it drastically reduces the number of documents to be judged. It does so at the cost of introducing biases, which are sometimes aggravated by non optimal con guration. In this paper we develop a novel visualization technique for the pooling method, and integrate it in a demo application named Visual Pool. is demo application enables the user to interact with the pooling method with ease, and develops visual hints in order to analyze existing test collections, and build be er ones.",1,ad,True
CCS CONCEPTS,0,,False
·Information systems  Test collections; Relevance assessment; ·Human-centered computing  Visualization techniques;,0,,False
KEYWORDS,0,,False
Visualization; Test Collections; Pooling Method; Pooling Strategies,0,,False
1 INTRODUCTION,1,DUC,True
"Test collection based evaluation in IR is a cornerstone of the IR experimentation. Most o en, test collections are built using the pooling method. is method refers to a sampling procedure, according to a given strategy, of documents to be judged. is demo aims to visualize this procedure, allowing the user deeper insights.",0,,False
"A test collection is composed of a collection of documents, a set of topics, and a set of relevance judgements. A relevance judgment (or qrel) expresses the relevance of a document for a given topic. Due to the size of the modern collection of documents, to produce a complete set of relevance judgements is impossible. For example, if we examine what today would be considered a small test collection, with 500,000 documents and 50 topics (approximately the size of the TREC Ad Hoc 8 test collection [16]), the total relevance judgments",1,TREC,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3084146",1,ad,True
"to be made would be 25 × 106. At an optimistic rate of 120 seconds per judgment, this represents the equivalent of around 400 years of work for one person [4]. To solve this problem, early in the modern IR history, a sampling method was developed, the pooling method [14].",1,ad,True
"e pooling method consists in building a test collection by using the results provided by a set of search engines. ese are usually systems designed by participants of challenges organized by IR evaluation campaigns such as: TREC, CLEF, NTCIR, or FIRE. In these challenges, every participant is provided a collection of documents and a set of topics. eir task is to develop a search engine to produce a result that maximizes the goal de ned by the challenge. is result is then sent to the organizers, who now have everything they need to implement a pooling strategy.",1,TREC,True
"e most common pooling strategy is the Depth@K strategy. is consists of creating a pool by selecting the top K documents from the results submi ed by each system of each participant. e pool is given to the relevance assessors, who will produce a set of relevance assessments, which are then used in combination with an IR evaluation measure to rank the performance of the systems of the participants. ese test collections are then used later by researchers to evaluate their systems. However, when comparing a new system with the search engines that participated in the challenge, the pooled systems have an advantage given by the guarantee that at least their top K documents have been judged, while for the new system this guarantee does not exist. is e ect goes under the name of pool bias, which manifests itself when the evaluated system retrieves documents that will never be considered relevant [5] because they had never been seen by the human assessors.",1,ad,True
"is bias can be mitigated by increasing: 1) the depth of the pool, which decreases the probability of retrieving a non-judged document; 2) the number of topics, which reduces the variability of the bias making it easier to correct; and, 3) the number (assumed to be proportional to the variety) of the submi ed results by the participants, which leads to a be er exploration of the information space. However, all of these solutions result in a mere increase of the number of documents to be judged and therefore in an increase of the cost of the test collection. e research in the IR community to reduce the pool bias has branched out into two directions: (a) identifying a pooling strategy and a set of parameters that manifests a lower bias, and (b) estimating the bias to correct the score obtained by the search engine. e former direction has lead to the development of new pooling strategies [7, 10, 11], the la er instead to the development of new pool bias estimators [6, 8, 9]. Moreover, a hybrid approach has been also explored developing",1,ad,True
1321,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"IR evaluation measures in combination with pooling strategies in order to minimize the pool bias [15, 17].",0,,False
"In this paper we present a demo that enables the user to visualize and interact with the pooling method. is demo addresses the needs of four classes of users: test collection builders, researchers, lecturers, and students. is solution aims to, by exploiting the users' sight, develop visual cues to guide the development of more sophisticated analyses. is solution is open source (MIT licensed) and is available on the website of the rst author.",1,ad,True
"e reminder of the paper goes as follows. We rst present our solution in Section 2. en we present the three use cases in Section 3. In Section 4 we present the technology used. Finally, we discuss and conclude in Section 5.",0,,False
2 VISUAL POOL,0,,False
"Visual Pool gives its users a new perspective over the pooling method, integrating a novel information visualization technique.",0,,False
"is section is divided into three parts: we rst present our pooling visualization technique, then we explain how this is integrated into the demo application, and we conclude listing the features of the demo. e authors have not found any solution that addresses a similar issue, which makes this solution unique in its kind.",1,ad,True
"In Figure 1 we see an example of the pool visualization technique. In this case we have applied a Depth@K pooling strategy. On the le , the run view highlights how the documents are distributed among the runs. On the center, the unique documents runs view",0,,False
r,0,,False
ab cde,0,,False
112345 263789 3 10 11 12 1 13 4 3 14 4 15 16 5 14 17 8 18 11  6 19 20 14 6 21 7 7 22 23 24 3 8 25 8 26 20 14 9 11 27 15 28 2 10 29 6 30 21 4,0,,False
r,0,,False
ab cde,0,,False
12345,0,,False
6,0,,False
789,0,,False
10 11 12,0,,False
13,0,,False
14,0,,False
15 16,0,,False
17,0,,False
18,0,,False
19 20,0,,False
21,0,,False
22 23 24,0,,False
25,0,,False
26,0,,False
27,0,,False
28,0,,False
29,0,,False
30,0,,False
f 12 345,0,,False
12345 6789 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30,0,,False
"Figure 1: Example of three types of visualization of the same pool a er the application of the pooling strategy Depth@5. Every square represents a document identi ed by its unique id. e documents' color means, if gray that is not judged, if green that is relevant, and if red that is not relevant. e y-axis is always the rank  at which a document has been retrieved. On the le the run view, where on the x-axis we nd the list of runs. On the center the unique documents runs view, where, w.r.t. to the previous view, all the duplicate documents have been removed starting from the top le corner. On the right the pool view, where, w.r.t. to the previous view, the documents have been pushed to the le , and the x-axis is now instead the frequency of unique documents at rank .",1,ad,True
"where all the duplicated documents retrieved at a lower rank are removed. On the right, the pool view shows the distribution of unique documents at varying of the rank.",0,,False
In Figure 2 we show a screen-shot of the Visual Pool application. In this user interface we identify the following sections (the numbers correspond to those in the gure):,0,,False
"(1) Pooling Strategy Selection and Con guration. We can select the pooling strategy among the 22 implemented. Every pooling strategy is con gurable, if needed.",0,,False
"(2) Visualization Control. We can select which topic to visualize, and we can control which pool visualization view to display: run, unique documents runs, or pool.",0,,False
"(3) Pool Strategy Control. We can control the progress of the pooling strategy. We can here decide if to step the pooling strategy forward by one document or till the end, for the current topic, or for all the topics.",0,,False
(4) Visualization. We visualize the pool using the previously described visualization technique.,0,,False
(5) Analytics. We have a set of analytics that show statistics about the pool and display the current status of the pooling strategy.,0,,False
"(6) Log. e log of the pooling strategy is displayed, where we show the status of the processed documents.",0,,False
"(7) Run/QRels upload. We can upload the set of runs to be analyzed. It is possible also to indicate at which size to cut the runs. When an existing test collection is to be analyzed, we can also upload the set of relevance assessments, which will be used to visualize the process of assessment.",1,ad,True
"(8) QRels download. We can download the current qrels le, e.g. the current set of relevance assessments as generated by the pooling strategy.",1,ad,True
"In summary, here we list all the features implemented in the version of the demo presented at SIGIR:",0,,False
"· Load runs les in TREC format with a given size; · Load a qrels le in TREC format; · Select a pooling strategy and con gure its parameters; · Select which topic to visualize; · Control the progress of the pooling strategy; · Visualize the pool in three views: runs, unique documents",1,ad,True
"runs, or pool; · Visualize the log of the progress of the pooling strategy; · Visualize the statistics about the pool and the status of the",0,,False
pooling strategy. · Save the progress of the pooling strategy as a qrels le in,0,,False
TREC format; · If required by the pooling strategy ask the user to judge a,1,TREC,True
document; · O er API for controlling the pooling strategy in order to,1,AP,True
perform the judgment with an external application.,0,,False
In Table 1 are listed all the pooling strategies already implemented in the demo.,1,ad,True
3 USE CASES,0,,False
In this section we present three use cases that cover the main user needs expressed by the four classes of users we aim to address. e,1,ad,True
rst use case is about the visualization of an existing test collection.,0,,False
1322,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Figure 2: Screen-shot of the Visual Pool application taken a er having: uploaded the runs with run size 100, uploaded the qrels, and executed the evaluation procedure as dictated by the selected pooling strategy Depth@10. In addition to the colors' meanings presented in Fig. 1, the color black indicates a document that has been pooled but it is not contained in the provided qrels.",1,ad,True
Pooling Strategy,0,,False
Depth@K,0,,False
[14] RRFTake@N,0,,False
[2],0,,False
Take@N,0,,False
[7] RBPTake@N,0,,False
[13],0,,False
BordaTake@N,0,,False
[1] RBPAdaptiveTake@N [13],0,,False
CondorcetTake@N [9] RBPAdaptive*Take@N [13],0,,False
CombMAXTake@N [12] MTFTake@N,0,,False
[3],0,,False
CombMINTake@N [12] HedgeTake@N,0,,False
[11],0,,False
CombMEDTake@N [12] MABRandomTake@N [11],0,,False
CombSUMTake@N [12] MABGreedyTake@N [11],0,,False
CombANZTake@N [12] MABUCBTake@N,0,,False
[11],0,,False
CombMNZTake@N [12] MABBetaTake@N,0,,False
[11],0,,False
DCGTake@N,0,,False
[10] MABMaxMeanTake@N [11],0,,False
Table 1: List of the implemented pooling strategies and their respective references.,0,,False
"e second use case is about the analysis of a pooling strategy. Finally, the third use case is about building a test collection.",0,,False
3.1 Visualizing a Test Collection,0,,False
"is use case addresses the needs of researchers when (a) interested in checking the properties of a test collection, e.g. visualize the",1,ad,True
"pooled runs, assess the behavior of each topic, bias of the nonpooled or new systems, or (b) interested in juxtaposing two or more test collections to compare their properties.",0,,False
"For this use case, it is required from the user to provide as input both the runs les and the qrels le. en, select the pooling strategy used to build the test collection, select the appropriate parameters, and execute the pooling strategy. Now, the application will display a visualization similar to Figure 1, where the user can select dynamically which view, and topic to visualize. When multiple test collections are to be compared, the user can repeat the process with a new instance of the application for each test collection.",0,,False
3.2 Analyzing of a Pooling Strategy,0,,False
"is use case addresses the needs of lecturers to help them explain the pooling method to students, and to address the needs of students to be er understand the algorithm. However, also researchers bene t from this use case, e.g. when interested in juxtaposing the results obtained with di erent pooling strategies.",1,ad,True
"For this use case, it is required from the user to provide as input both the runs les and the qrels le. en, the user can select a pooling strategy to be analyzed, and con gure it. Now, the application of the pooling strategy can be controlled by the controllers in the pooling controller section that allows the user to follow the pooling strategy at her/his own pace. To compare di erent pooling",0,,False
1323,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Name topic_id iteration_id document_id,0,,False
rank score run_name,0,,False
Type String+ String+ String+ Integer Float32 String,0,,False
Ignored? No Yes No Yes No No,0,,False
Table 2: Space separated elds of a runs le and their type. e column `Ignored?' indicates if the eld is ignored or not,0,,False
by the application.,0,,False
Name topic_id iteration_id document_id,0,,False
score,0,,False
Type String+ String+ String+ Integer,0,,False
Ignored? No Yes No No,0,,False
Table 3: Space separated elds of a qrels le and their type. e column `Ignored?' indicates if the eld is ignored or not,0,,False
by the application.,0,,False
"strategies, the user can repeat the process with a new instance of the application for each pooling strategy.",0,,False
3.3 Building a Test Collection,0,,False
"is use case addresses the needs of a test collection builder to help them control the assessments of the selected documents using the application as a dashboard. is is achieved by making use of the API o ered by the application, which allows a third party application to query the application about which document should be judged, and send a response back with the label.",1,ad,True
"For this use case, it is required from the user to provide as input the runs les, select a pooling strategy to be used, and con gure it. en, generate a unique key that will be used by the third party application to communicate with the application. At this point the user is able to follow the judgment process on-line. e application allows the user to change strategy if required, by downloading the current qrels and giving them as input to a new instance of the application.",1,ad,True
4 TECHNOLOGY,0,,False
"is demo has been developed as a modern web application in JavaScript for the front-end and Scala for the back-end. e frontend is based on the web framework Ember.js1, and on the visualization library p5.js2, which is based on the Processing3 language.",0,,False
"e back-end is based on the Play Framework4 and for in-memory storage on Redis5, which is required only to support the API module.",1,AP,True
1h ps://emberjs.com 2h ps://p5js.org 3h ps://processing.org 4h ps://www.playframework.com 5h ps://redis.io,0,,False
"e input les to be provided to the application are based on the de facto standard format of trec eval6. e format is a nonbreakable space separated le. In Table 2 we show the elds in the correct order as they should be contained by a runs le, and in Table 3 we show the same but for a qrels le. As indicated in the tables, some of the elds are ignored because they are redundant.",1,trec,True
e type String+ is a String type that does not contain spaces.,0,,False
5 DISCUSSION & CONCLUSION,0,,False
"In this demo paper we have presented Visual Pool, an application to help test collection builders, researchers, lecturers, and students to visualize the pooling method. We believe that this technology will have a commercial impact because it allows the building of more e cient test collections but at the same cost, through the application of more e cient pooling strategies. We also believe it will have a research impact because it enables the analysis of new pooling strategies. Finally, it will have an educational impact because it supports lecturers in explaining and students in understanding the pooling method.",0,,False
6 ACKNOWLEDGMENTS,0,,False
"is research was partly supported by the Austrian Science Fund (FWF) project number P25905-N23 (ADmIRE). is work has been supported by the Self-Optimizer project (FFG 852624) in the EUROSTARS programme, funded by EUREKA, the BMWFW and the European Union.",0,,False
REFERENCES,0,,False
[1] Javed A. Aslam and Mark Montague. 2001. Models for Metasearch. In Proc. of SIGIR.,0,,False
"[2] Gordon V. Cormack, Charles L A Clarke, and Stefan Bue cher. 2009. Reciprocal Rank Fusion Outperforms Condorcet and Individual Rank Learning Methods. In Proc. of SIGIR.",0,,False
"[3] Gordon V. Cormack, Christopher R. Palmer, and Charles L. A. Clarke. 1998. E cient Construction of Large Test Collections. In Proc. of SIGIR.",0,,False
[4] Bevan Koopman and Guido Zuccon. 2014. Why assessing relevance in medical IR is demanding. In Medical Information Retrieval (MedIR) Workshop.,0,,False
"[5] Aldo Lipani. 2016. Fairness in Information Retrieval. In Proc. of SIGIR. [6] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2015. Spli ing Water: Precision",0,,False
"and Anti-Precision to Reduce Pool Bias. In Proc. of SIGIR. [7] Aldo Lipani, Mihai Lupu, and Allan Hanbury. 2016. e Curious Incidence of",0,,False
"Bias Corrections in the Pool. In Proc. of ECIR. [8] Aldo Lipani, Mihai Lupu, Evangelos Kanoulas, and Allan Hanbury. 2016. e",0,,False
"Solitude of Relevant Documents in the Pool. In Proc. of CIKM. [9] Aldo Lipani, Mihai Lupu, Joao Palo i, Guido Zuccon, and Allan Hanbury. 2017.",0,,False
"Fixed Budget Pooling Strategies Based on Fusion Methods. In Proc. of SAC. [10] Aldo Lipani, Joao Palo i, Mihai Lupu, Florina Piroi, Guido Zuccon, and Allan",0,,False
"Hanbury. 2017. Fixed-Cost Pooling Strategies Based on IR Evaluation Measures. [11] David E. Losada, Javier Parapar, and Alvaro Barreiro. 2017. Multi-armed bandits",1,ad,True
"for adjudicating documents in pooling-based evaluation of information retrieval systems. Information Processing & Management 53, 5 (2017). [12] Craig Macdonald and Iadh Ounis. 2006. Voting for Candidates: Adapting Data Fusion Techniques for an Expert Search Task. In Proc. of CIKM. [13] Alistair Mo at, William Webber, and Justin Zobel. 2007. Strategic System Comparisons via Targeted Relevance Judgments. In Proc. of SIGIR. [14] K. Spa¨rck Jones and C. J. van Rijsbergen. 1975. Report on the need for and provision of an `ideal' information retrieval test collection. British Library Research and Development Report No. 5266 (1975). [15] Ellen M. Voorhees. 2014. e E ect of Sampling Strategy on Inferred Measures. In Proc. of SIGIR. [16] E Voorhes and Donna Harman. 1999. Overview of the eighth text retrieval conference. In Proc. of TREC. [17] Emine Yilmaz and Javed A. Aslam. 2006. Estimating Average Precision with Incomplete and Imperfect Judgments. In Proc. of CIKM.",1,ad,True
6h ps://github.com/usnistgov/trec eval,1,trec,True
1324,0,,False
,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
A Task-oriented Search Engine for Evidence-based Medicine,0,,False
Bevan Koopman,0,,False
"CSIRO Brisbane, Australia bevan.koopman@csiro.au",1,CSIRO,True
Guido Zuccon,0,,False
"eensland University of Technology Brisbane, Australia g.zuccon@qut.edu.au",0,,False
Jack Russell,0,,False
"CSIRO Brisbane, Australia jackrussell1996@gmail.com",1,CSIRO,True
ABSTRACT,0,,False
Evidence-based medicine (EBM) is the practice of making clinical decisions based on rigorous scienti c evidence. EBM relies on e ective access to peer-reviewed literature -- a task hampered by both the exponential growth of medical literature and a lack of e cient and e ective means of searching and presenting this literature. is paper describes a search engine speci cally designed for searching medical literature for the purpose of EBM and in a clinical decision support se ing.,0,,False
CCS CONCEPTS,0,,False
·Information systems  Expert search;,0,,False
1 PROBLEM AND TARGET USERS,0,,False
"While there are mature resources for searching medical literature (PubMed being a widely used example), these are primarily focused on retrieving literature for research purposes, not for clinical decision support. Research on how clinicians (doctors, nurses or other health professionals) search in a clinical decision support se ing [2] has shown that clinicians pose queries within three common clinical tasks: i) searching for diagnoses given a list of symptoms; ii) searching for relevant tests given a patient's situation; and iii) searching for the e ective treatments given a particular condition. An e ective search engine should facilitate interactions that support these three tasks. Doing so would lead to improved retrieval e ectiveness and a more economic interaction with the search engine and, ultimately, improved clinical decisions for patients.",1,ad,True
2 TASK-BASED SEARCH ENGINE FOR EVIDENCE BASED MEDICINE,1,VID,True
"A task-oriented approach is at the core of our proposed search engine. Document representation, the retrieval method, and how results are presented to the user are all centred around the three tasks of diagnosis, test and treatment. Figure 1 shows the overall architecture of the system, which was developed using Elasticsearch. We detail the indexing, retrieval and visualisation components in the following sub-sections.",0,,False
Student internship while studying at Southern Cross University.,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3084136",1,ad,True
2.1 Task-oriented indexing,0,,False
"In the indexing phase, medical articles are fed to a task extraction process that annotates mentions of diagnoses, tests and treatments. Task extraction is achieved by rst identifying mentions of medical concepts using a medical information extraction system [13]. e identi ed medical concepts are then mapped to higher level semantic types (e.g., the concept ""Headache"" belongs to the semantic type ""Sign or Symptom""). Each semantic type can then be mapped to one of the three clinical tasks, diagnosis, treatment or test, by consulting the i2b2 challenge guidelines [15] which de ne a mapping between semantic types and clinical tasks. Once the tasks are identi ed, the original span of text from the article is annotated with details of the task type. A sample text, with annotated spans, is shown in Figure 2.",1,ad,True
"e resulting annotated articles are indexed into an inverted index with separate elds for diagnoses, tests and treatments.",0,,False
2.2 Task-oriented retrieval,0,,False
"When a clinician poses a clinical query, they would typically be provided with a long list of search results. In the task-oriented approach, it is desirable to provide the clinician with a summary of the signi cant diagnoses, tests and treatments. is allows them to quickly gain an understanding of what they might expect to",0,,False
"nd when examining the search results. In addition, when these summaries are interactive (e.g., the searcher can drill-down on speci c tests or treatments) then they are provided with an easy mechanism to navigate the information space. To facilitate such interactions we implement the following retrieval strategy. Given a set of search results, we estimate signi cant diagnoses, tests and treatments. is is done by scoring each mention of a diagnosis, test or treatment (which can comprise of more than one terms) according to its frequency of appearance within the set of search results (foreground probability) vs. the frequency it appears within the collection as a whole (background probability).1 e top ve diagnoses, tests and treatments are displayed to the searcher (along with the regular search results for that query). Given an individual document within the search results, we also estimate the signi cant diagnoses, tests and treatments in that document according to IDF and display those to the user. As the underlying retrieval model, we adopted the default Elasticsearch BM25 model.",1,ad,True
2.3 Task-oriented visualisation of results,0,,False
"A web-based interface provides the clinician with a means to search and interact with results. A screenshot of the user interface, presenting the results of a search for `malaria', is shown in Figure 3. e interface provides a single input box where clinicians can enter a free text, keyword query. Retrieval results are displayed as a ranked",0,,False
1Details on this scoring method can be found in [6].,0,,False
1329,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Indexing,0,,False
Medical articles,0,,False
Task extraction,0,,False
Annotated medical articles,0,,False
Task-oriented indexing of articles,0,,False
Diagnosseess,0,,False
Tests,0,,False
User Interface,0,,False
Treatments,0,,False
Retrieval,0,,False
Clinician searcher,0,,False
Task-oriented retrieval Significant concept estimation,0,,False
Figure 1: Overview of the task-oriented search approach.,0,,False
Field-based inverted file index,0,,False
"Patients with a <test UMLSid, C2238079 title, blood smear >blood smear</test> found to be positive for <diagnosis UMLSid, C0024530 title, malaria [disease/finding] >malaria</diagnosis> were often administered <treatment UMLSid, C0034414 title, quinidine [chemical/ingredient] >quinidine</treatment>.",1,ad,True
"Figure 2: Text containing three task annotations: a test (""blood smear""), a diagnosis (""malaria"") and a treatment (""quinidine"").",0,,False
"list in decreasing order of relevance. Each result is comprised of the article title, journal title, publication date and a snippet.2",0,,False
"ree barplots provide an overview of the signi cant diagnoses (red), tests (orange) and treatments (green). ese plots are interactive: clinicians can click on a particular diagnosis, for example, and the set of search results would be ltered to include only articles mentioning that diagnosis; multiple lters can be applied.",0,,False
"e purpose of this interface was to allow the clinicians to, rstly, easily get an overview of the search results by inspecting the plots and, secondly, easily navigate the set of search results by applying various lters.",0,,False
e clinician can view an article by clicking on its title. is opens a dialog displaying the full-text of the article with appropriate annotation displayed. A screenshot for a sample article showing the annotated diagnoses and treatments is shown in Figure 4.,0,,False
3 COMPARISON WITH EXISTING METHODS,0,,False
3.1 alitative comparison to other systems,0,,False
"While research on how clinicians search indicates that they pose queries according to three mains tasks (diagnoses, tests and treatment) [2], most systems for searching EBM resources do not take these tasks into account. However, structuring IR systems around di erent categories of information is a common approach in IR -- generally referred to as faceted retrieval [3]. Faceted retrieval reduces mental workload by promoting recognition over recall and by suggesting logical yet unexpected navigation paths to the user [16]. Meaningful facets have been found to support learning, re ection, discovery and information nding [8, 12, 16]. EBM-based search can be viewed as a complex search task [7]: clinicians have complex information needs and are o en time pressured. us, an IR approach such as faceted retrieval, which reduces mental overhead, is desirable. In this paper, we test the hypothesis that",1,ad,True
2We used the default snippet generation provided by Elasticsearch.,0,,False
"faceted retrieval, which has shown bene ts in general web search, can improve search for EBM (Section 3.2).",0,,False
"e importance of access to biomedical literature has resulted in many biomedical-speci c retrieval systems [4]. While some systems mention di erent types of clinical queries (e.g., therapy, diagnosis, harm and prognosis) they typically did not integrate these into the retrieval method or in the way the searcher was presented with or interacted with the search results. Our system uses the clinical tasks as the bases for both retrieval and interaction. Finally, most methods for searching EBM resources were for research purposes, rather than clinical decision support. As such, recall was an important factor (i.e., nding all the relevant articles for a particular information need). In contrast, for clinical decision support, precision can be more important (i.e., nding the article that helps with the clinical task without reading many irrelevant articles). Our system bases the design of the IR system around improving precision via task-based ltering.",1,ad,True
"Some IR systems use diagnosis, test and treatment information as features in a retrieval model. A common approach here is to map all queries and documents being searched to medical concepts according to an external domain-knowledge resource; matching is then done at the concept level, comparing a query concept with a document concept [9, 10, 14]. Although concept retrieval using tasks has proved e ective, the tasks were simply used as features within the retrieval model and never exposed to the clinician [9, 10]. In this study, we a empt to make the task-based information explicit in the way the clinician interacts with the system, as well as the basis for the underlying retrieval model.",0,,False
"In summary, while other studies a empt to extract detailed, structured information from medical articles, we adopt a lightweight approach by considering only diagnoses, tests and treatments. ese three tasks were treated in a facet-based approach, which has proved e ective in improving search interactions in other domains. e tasks-oriented information is used not only as",1,ad,True
1330,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Figure 3: Screenshot showing the results of a search for `malaria'. ree barplots provide an overview of the signi cant diagnoses (red), tests (orange) and treatments (green). Individual search results are shown below the barplots.",0,,False
Figure 4: An sample medical article from the user interface showing annotated diagnoses (red) and treatments (green).,0,,False
1331,0,,False
Demonstration Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Retrieval results for task-oriented search. All results showed statistical signi cance over `No lter' baseline (paired t test, p < 0.01).",0,,False
Task-oriented lter- Prec.@10 (%) Recip. rank (%) ing,0,,False
No lter Diagnoses Tests Treatments,0,,False
0.2867 0.3250 (+13%) 0.3283 (+15%) 0.3167 (+10%),0,,False
0.4349 0.5271 (+21%) 0.5324 (+22%) 0.5113 (+16%),0,,False
a feature in retrieval but also as a means for improving the way clinicians might interact with the system.,0,,False
3.2 Empirical evaluation of task-based lters,0,,False
e task-oriented system was evaluated using the TREC Clinical Decision Support (CDS) test collection. Retrieval e ectiveness of the system without ltering was compared with that of the system with a speci c task lter. To evaluate the e ectiveness of task-based,1,TREC,True
"ltering we conducted the following experiment. First, we issued each TREC CDS query topic to the retrieval system and, with no",1,TREC,True
"ltering, evaluated the corresponding precision @ 10 and mean reciprocal rank. We then simulated the clinician interacting with the results by selecting individual diagnoses, tests and treatments as lters. Speci cally, we ltered the search results, one at a time, by each of the top- ve diagnoses, tests and treatments; for example,",0,,False
"lter with only the rst treatment and evaluate the results, then lter with only the second treatment and evaluate the results, etc. Evaluation measures were calculated a er each lter had been applied. us, the change in e ectiveness between the rst (`no lter') search and each of the subsequent task-oriented searches could be calculated. e retrieval e ectiveness of the three di erent task types could be compared and contrasted.",1,ad,True
"e retrieval e ectiveness are shown in Table 1. e results show that task-oriented ltering led to a statistically signi cant improvement in precision @ 10 and mean reciprocal rank. Filtering on tests exhibited the greatest improvement, followed by ltering on diagnosis and, nally, ltering on treatments.",0,,False
4 IMPACT AND OUTLOOK,0,,False
"An important consideration for clinicians, who are o en timepressured, is any labour saving bene ts that a system can provide. As well as improving retrieval e ectiveness, our system can help reduce work load. Speci cally, task-based ltering reduces the number of documents the clinician needs to view. A more detailed economic analysis, simulating a user applying various task-based",1,ad,True
"lters, revealed cost savings when compared to not ltering [6]. Plainly put, the cost of choosing and applying a task-based lter is far less than reading even a single non relevant document. us, even for the same retrieval e ectiveness, viewing less document o ers bene ts to clinicians.",1,ad,True
"Search engines for evidence-based medicine may particularly bene t junior doctors, who are still coming to grips with a large and evolving body of medical literature. It is this cohort of users that we hope to recruit as users of the system. While the empirical evaluation has shown improvements in retrieval e ectiveness and cost savings in using the task-based system, the ultimate evaluation",0,,False
"of the system is with real users, especially given the specialist",0,,False
domain of medical search. An A/B test with and without task-,0,,False
oriented ltering is planned to evaluate the system with real users.,0,,False
"In the current system, users explicitly initiate a search by enter-",0,,False
"ing ad-hoc queries via a free-text input box. However, in clinical",1,ad-hoc,True
practice there are situations where a search may be implicitly ini-,0,,False
tiated by a user. A common scenario for this is when a clinician,0,,False
opens an electronic patient record -- an e ective system would,0,,False
"retrieve relevant diagnosis, test or treatment oriented results based",0,,False
on the contents of the patient record. While the current system sup-,0,,False
"ports retrieval of such results, the process of generating an e ective",0,,False
query from a verbose patient record is needed. Initial research on,0,,False
automatically generating clinical queries is underway [5].,0,,False
Clinical practice that is informed by scienti c evidence is known,0,,False
to improve quality of care [1]. A common means of integrating this,0,,False
evidence-based approach into clinical practice is through clinical,0,,False
"decision support systems, which are also known to improve qual-",0,,False
ity of care [11]. e system we describe in this paper provides a,0,,False
means for clinicians to access evidence-based literature in a clinical,0,,False
decision support se ing. Improvements in retrieval e ectiveness us-,0,,False
ing task-based lters equate to improved access to evidence-based,0,,False
medicine resources. Coupled with the cost savings of using the,0,,False
"system, there are good indications that the use of the system can",0,,False
"lead to improved clinical decisions and, ultimately, patient care.",1,ad,True
REFERENCES,0,,False
"[1] David M Eddy and John Billings. 1988. e quality of medical evidence: implications for quality of care. Health A airs 7, 1 (1988), 19­32.",0,,False
"[2] J.W. Ely, J.A. Oshero , P.N. Gorman, M.H. Ebell, M.L. Chambliss, E.A. Pifer, and P.Z. Stavri. 2000. A taxonomy of generic clinical questions: classi cation study. British Medical Journal 321, 7258 (2000), 429­432.",0,,False
"[3] Marti Hearst, Ame Ellio , Jennifer English, Rashmi Sinha, Kirsten Swearingen, and Ka-Ping Yee. 2002. Finding the Flow in Web Site Search. Commun. ACM 45, 9 (2002), 42­49.",0,,False
"[4] William Hersh. 2009. Information retrieval: a health and biomedical perspective (3rd ed.). Springer Verlag, New York.",0,,False
"[5] Bevan Koopman, Liam Cripwell, and Guido Zuccon. 2017. Generating Clinical eries from Patient Narratives: A Comparison between Machines and Humans.",0,,False
"In SIGIR. Tokyo, Japan. [6] Bevan Koopman, Jack Russell, and Guido Zuccon. 2017. Task-oriented search",0,,False
"for evidence-based medicine. Inter. Journal of Digital Libraries (2017), 1­13. [7] Bevan Koopman and Guido Zuccon. 2014. Why Assessing Relevance in Medical",0,,False
"IR is Demanding. In MedIR, SIGIR. Gold Coast, Australia. [8] Barbara H Kwasnik. 2000. e role of classi cation in knowledge representation",0,,False
"and discovery. Library trends 48, 1 (2000). [9] Nut Limsopatham, Craig Macdonald, and Iadh Ounis. 2013. A Task-Speci c",1,ad,True
"ery and Document Representation for Medical Records Search. In ECIR. Moscow, Russia. [10] Zhenyu Liu and Wesley W. Chu. 2007. Knowledge-based query expansion to support scenario-speci c retrieval of medical free text. Information Retrieval 10, 2 (2007), 173­202. [11] E. V. Murphy. 2014. Clinical decision support: e ectiveness in improving quality processes and clinical outcomes and factors that may in uence success. Yale J Biol Med 87, 2 (Jun 2014), 187­197. [12] Dagobert Soergel. 1999. e rise of ontologies or the reinvention of classi cation. JASIST 50, 12 (1999), 1119. [13] Luca Soldaini and Nazli Goharian. 2016. ickUMLS: a fast, unsupervised approach for medical concept extraction. In SIGIR MedIR Workshop. [14] Dolf Trieschnigg, Djoerd Hiemstra, Franciska de Jong, and Wessel Kraaij. 2010. A cross-lingual framework for monolingual biomedical information retrieval. In CIKM. ACM, 169­178. [15] O¨ zlem Uzuner, Bre R South, Shuying Shen, and Sco L DuVall. 2011. 2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text. JASIST 18, 5 (2011), 552­556. [16] Ryen W White. 2016. Interactions with search systems. Cambridge University Press.",0,,False
1332,0,,False
,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
A Study of Snippet Length and Informativeness,0,,False
"Behaviour, Performance and User Experience",0,,False
David Maxwell,0,,False
"School of Computing Science University of Glasgow Glasgow, Scotland",0,,False
d.maxwell.1@research.gla.ac.uk,0,,False
Leif Azzopardi,0,,False
"Computer & Information Sciences University of Strathclyde Glasgow, Scotland leif.azzopardi@strath.ac.uk",0,,False
Yashar Moshfeghi,0,,False
"School of Computing Science University of Glasgow Glasgow, Scotland",0,,False
Yashar.Moshfeghi@glasgow.ac.uk,0,,False
ABSTRACT,0,,False
"e design and presentation of a Search Engine Results Page (SERP) has been subject to much research. With many contemporary aspects of the SERP now under scrutiny, work still remains in investigating more traditional SERP components, such as the result summary. Prior studies have examined a variety of di erent aspects of result summaries, but in this paper we investigate the in uence of result summary length on search behaviour, performance and user experience. To this end, we designed and conducted a withinsubjects experiment using the TREC AQUAINT news collection with 53 participants. Using Kullback-Leibler distance as a measure of information gain, we examined result summaries of di erent lengths and selected four conditions where the change in information gain was the greatest: (i) title only; (ii) title plus one snippet; (iii) title plus two snippets; and (iv) title plus four snippets. Findings show that participants broadly preferred longer result summaries, as they were perceived to be more informative. However, their performance in terms of correctly identifying relevant documents was similar across all four conditions. Furthermore, while the participants felt that longer summaries were more informative, empirical observations suggest otherwise; while participants were more likely to click on relevant items given longer summaries, they also were more likely to click on non-relevant items. is shows that longer is not necessarily be er, though participants perceived that to be the case ­ and second, they reveal a positive relationship between the length and informativeness of summaries and their a ractiveness (i.e. clickthrough rates). ese ndings show that there are tensions between perception and performance when designing result summaries that need to be taken into account.",1,ad,True
CCS CONCEPTS,0,,False
·Information systems Search interfaces;,0,,False
"ACM Reference format: David Maxwell, Leif Azzopardi, and Yashar Moshfeghi. 2017. A Study of Snippet Length and Informativeness. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080824",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080824",1,ad,True
1 INTRODUCTION,1,DUC,True
"Interactive Information Retrieval (IIR) is a complex, non-trivial process where a searcher undertakes a variety of di erent actions during a search session [16]. Core to their experience and success is the Search Engine Results Page (SERP), with its presentation and design over the years having been subject to much research. With more complex components now becoming commonplace in modern day Web search engines (such as the information card [5, 36] or social annotations [35]), much work however still remains on examining how more traditional SERP components (such as result summaries) are designed and presented to end users.",1,ad,True
"Result summaries have traditionally been viewed as the `ten blue links' with the corresponding URL of the associated document, and one or more textual snippets of keywords-in-context from the document itself, approximately 130-150 characters (or two lines) in length [15]. Numerous researchers have explored result summaries in a variety of di erent ways, such as: examining their length [11, 19, 38]; the use of thumbnails [47, 52]; their a ractiveness [9, 14]; and the generation of query-biased snippets [41, 48].",1,ad,True
"e performance of users has broadly been evaluated in a limited fashion (e.g. by examining task completion times). In this work, we are interested in how the length and information content of result summaries a ects SERP interactions and a user's ability to select relevant over non-relevant items. Prior research has demonstrated that longer result summaries tend to lower completion times for informational tasks (where users need to nd one relevant document) [11], but does this hold in other contexts, speci cally for ad-hoc retrieval, where users need to nd several relevant items? Furthermore, how does the length and information associated with longer result summaries a ect the user's ability to discern the relevant from the non-relevant?",1,ad,True
"is work therefore serves as an investigation into the e ects of search behaviour and search performance when we vary (i) result summary snippet lengths, and by doing so (ii) the information content within the summaries. To this end, a within-subjects crowdsourced experiment (n ,"" 53) was designed and conducted. Under ad-hoc topic retrieval, participants used four di erent search interfaces, each with a di erent size of result summary. Findings allow us to address the two main research questions of this study. RQ1 How does the value of information gain represented as snippet length a ect behaviour, performance and user experience? RQ2 Does information gain ­ again represented as snippet length ­ affect the decision making ability and accuracy (identifying relevant documents) of users? We hypothesise that longer and more informative snippets will enable users to make be er quality decisions (i.e. higher degrees of accurately identifying relevant content).""",1,ad-hoc,True
135,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 RELATED WORK,0,,False
"As previously mentioned, the design and presentation of SERPs has been examined in depth. Researchers have examined various aspects of SERPs, and how the designs of such aspects in uence the behaviour of users. Here, we provide a summary of the various aspects that have been investigated. Speci cally, we focus upon: the layout of SERPs; the size of SERPs; how snippet text is generated; and how much text should be presented within each result summary ­ the la er being the main focus of this work.",0,,False
2.1 SERP Layouts and Presentation,0,,False
"Early works regarding the presentation of result summaries [6, 12] examined approaches to automatically categorise result summaries for users, similar to the categorisation approach employed by early search engines. Chen and Dumais [6] developed an experimental system that automatically categorised result summaries on-the- y as they were generated. For a query, associated categories were then listed as verticals, with associated document titles provided underneath each category header. Traditional result summaries were then made available when hovering over a document title. Subjects of a user study found the interface easier to use than the traditional `ten blue links' approach - they were 50% faster at nding information displayed in categories. is work was then extended by Dumais et al. [12], where they explored the use of hover text to present additional details about search results based upon user interaction. Searching was found to be slower with hover text, perhaps due to the fact that explicit decisions about when to seek additional information (or not to) were required.",1,ad,True
"Alternatives to the traditional, linear list of result summaries have also been trialled (like grid-based layouts [8, 20, 40]). For example, Krammerer and Beinhaur [20] examined di erences in user behaviour when interacting with a standard list interface, compared against a tabular interface (title, snippet and URL stacked horizontally in three columns for each result), and a grid-based layout (result summaries placed in three columns). Users of the grid layout spent more time examining result summaries. e approach demonstrated promise in overcoming issues such as position bias [10], as observed by Joachims et al. [17].",1,ad,True
"Marcos et al. [34] performed an eye-tracking user study examining the e ect of user behaviour while interacting with SERPs ­ and whether the richness of result summaries provided on a SERP (i.e. result summaries enriched with metadata from corresponding pages) impacted upon the user's search experience. Enriched summaries were found to help capture a user's a ention. Including both textual and visual representations of a document when presenting results could have a positive e ect on relevance assessment and query reformulation [18]. Enriched summaries were also examined by Ali et al. [2] in the context of navigational tasks. Striking a good balance between textual and visual cues were shown to be er support user tasks, and search completion time.",1,ad,True
2.2 Generating Snippet Text,0,,False
"Users can be provided with an insight by result summaries as to whether a document is likely to be relevant or not [14]. Consequently, research has gone into examining di erent kinds of snippets, and how long a snippet should be. Work initially focused",0,,False
"upon how these summaries should be generated [30, 31, 39, 48, 51]. ese early works proposed the idea of summarising documents",0,,False
"with respect to the query (query-biased summaries) or keywords-incontext ­ as opposed to simply extracting the representative or lead sentences from the document [29]. Tombros and Sanderson [48] showed that subjects of their study were likely to identify relevant documents more accurately when using query-biased summaries, compared to summaries simply generated from the rst few sentences of a given document. ery-biased summaries have also been recently shown to be preferred on mobile devices [45].",1,ad,True
"When constructing snippets using query-biased summaries, Rose et al. [41] found that a user's perceptions of the result's quality were in uenced by the snippets. If snippets contained truncated sentences or many fragmented sentences (text choppiness), users perceived the quality of the results more negatively, regardless of length. Kanungo and Orr [21] found that poor readability also impacts upon how the resultant snippets are perceived. ey maintain that readability is a crucial presentation a ribute that needs to be considered when generating a query-biased summary. Clarke et al. [9] analysed thousands of pairs of snippets where result A appeared before result B, but result B received more clicks than result A. As an example, they found results with snippets which were very short (or missing entirely) had fewer query terms, were not as readable, and a racted fewer clicks. is led to the formulation of several heuristics relating to document surrogate features, designed to emphasise the relationship between the associated page and generated snippet. Heuristics included: (i) ensuring that all query terms in the generated snippet (where possible); (ii) withholding the repeating of query terms in the snippet if they were present in the page's title; and (iii) displaying (shortened) readable URLs.",1,ad,True
"Recent work has examined the generation of snippets from more complex angles ­ from manipulating underlying indexes [4, 49] to language modelling [14, 32], as well as using user search data to improve the generation process [1, 42]. Previous generation approaches also may not consider what parts of a document searchers actually nd useful. Ageev et al. [1] incorporated into a new model post-click searcher behaviour data, such as mouse cursor movements and scrolling over documents, producing behaviour-biased snippets. Results showed a marked improvement over a strong text-based snippet generation baseline. Temporal aspects have also been considered ­ Svore et al. [46] conducted a user study, showing that users preferred snippet text with trending content in snippets when searching for trending queries, but not so for general queries.",1,corpora,True
2.3 Results per Page,0,,False
"Today, a multitude of devices are capable of accessing the World Wide Web (WWW) ­ along with a multitude of di erent screen resolutions and aspect ratios. e question of how many result summaries should be displayed per page ­ or results per page (RPP) ­ therefore becomes hugely important, yet increasingly di cult to answer. Examining behavioural e ects on mobile devices when interacting with SERPs has a racted much research as of late (e.g. [24­ 26]), and with each device capable of displaying a di erent number of results above-the-fold, recent research has shown that the RPP value can in uence the behaviour of searchers [17, 25]. Understanding this behaviour can help guide and inform those charged with designing contemporary user interfaces.",0,,False
136,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"In a Google industry report, Linden [33] however stated that users desired more than 10RPP, despite the fact that increasing the RPP yielded a 20% drop in tra c; it was hypothesised that this was due to the extra time required to dispatch the longer SERPs. is drop however be a ributed to other reasons. Oulasvirta et al. [37] discusses the paradox of choice [43] in the context of search, where more options (results) ­ particularly if highly relevant ­ will lead to poorer choice and degrade user satisfaction. In terms of user satisfaction, modern search engines can therefore be a victim of their own success, presenting users with choice overload. Oulasvirta et al. [37] found that presenting users with a six-item search result list was associated with higher degrees of satisfaction, con dence with choices and perceived carefulness than an a list of 24 items.",1,ad,True
"Kelly and Azzopardi [22] broadly agreed with the ndings by Oulasvirta et al. [37]. Here, the authors conducted a betweensubjects study with three conditions, where subjects were assigned to one of three interfaces - the baseline interface, showing 10RPP (the `ten blue links'), and two interfaces displaying 3RPP and 6RPP respectively. eir ndings showed that individuals using the 3RPP and 6RPP interfaces spent signi cantly longer examining top-ranking results and were more likely to click on higher ranked documents than those on the 10RPP interface. Findings also suggested that subjects using the interfaces showing fewer RPP found it comparatively easier to nd relevant content than those using the 10RPP interface. However, no signi cant di erence was found between the number of relevant items found across the interfaces. Currently, 10RPP is still considered the de-facto standard [15].",1,ad,True
2.4 Snippet Lengths: Longer or Shorter?,0,,False
"Snippet lengths have been examined in a variety of ways. A user study by Paek et al. [38] compared a user's preference and usability against three di erent interfaces for displaying result summaries. With question answering tasks, the interfaces: displayed a normal SERP (i.e. a two line snippet for each summary, with a clickable link); an instant interface, where an expanded snippet was displayed upon clicking it; and a dynamic interface, where hovering the cursor would trigger the expanded snippet. e instant view was shown to allow users to complete the given tasks in less time than the normal baseline, with half of participants preferring this approach.",0,,False
"Seminal work by Cutrell and Guan [11] explored the e ect of di erent snippet lengths (short: 1 line, medium: 2-3 lines; and long: 6-7 lines). ey found that longer snippets signi cantly improved performance for informational tasks (e.g. `Find the address for Newark Airport.'). Users performed be er for informational queries as snippet length increased. is work was followed up by Kaisser et al. [19]. ey conducted two experiments that estimated the preferred snippet length according to answer type (e.g. nding a person, time, or place), and comparing the results of the preferred snippet lengths to users' preferences to see if this could be predicted.",1,ad,True
"e preferred snippet length was shown to depend upon the type of answer expected, with greater user satisfaction shown for the snippet length predicted by their technique.",0,,False
"More contemporary work has begun to examine what snippet sizes are appropriate for mobile devices. Given smaller screen sizes, this is important ­ snippet text considered acceptable on a computer screen may involve considerable scrolling/swiping on",0,,False
"a smaller screen. Kim et al. [27] found that subjects using longer snippets on mobile devices exhibited longer search times and similar search accuracy under informational tasks1. Longer reading times and frequent scrolling/swiping (with more viewport movements) were exhibited. Longer snippets did not therefore appear to be very useful on a small screen ­ an instant or dynamic snippet approach (as per Paek et al. [38]) may be useful for mobile search, too.",1,ad,True
"e presentation of result summaries has a strong e ect on the ability of a user to judge relevancy [14]. Relevant documents may be overlooked due to uninformative summaries ­ but conversely, non-relevant documents may be examined due to a misleading summary. However, longer summaries also increase the examination cost, so there is likely a trade-o between informativeness/accuracy and length/cost. e current, widely accepted standard for result summaries are two query-based snippets/lines [15]. is work examines whether increasing and decreasing the length (and consequently the informativeness) of result summary snippets a ects user accuracy and costs of relevance decisions in the context of ad-hoc topic search, where multiple relevant documents are sought.",1,ad,True
3 EXPERIMENTAL METHOD,0,,False
"To address our two key research questions outlined in Section 1, we conducted a within-subjects experiment. is allowed us to explore the in uence of snippet length and snippet informativeness on search behaviours, performance and user experience. Subjects used four di erent search interfaces, each of which varied the way in which result summaries were presented to them.",1,ad,True
"To decide the length and informativeness of the result summaries, we performed a preliminary analysis to determine the average length (in words) and informativeness (as calculated by the Kullback-Leibler distance [28] to measure information gain, or relative entropy) of result summaries with the title and varying numbers of snippet fragments (0­10). e closer the entropy value is to zero, the more information gained. Figure 1 plots the number of words, the information gain, and the information gain per word2. It is clear from the plot that a higher level of information gain was present in longer snippets. However, as the length increases with each additional snippet fragment added, the informativeness per word decreased. Consequently, for this study, we selected the four di erent interface conditions in the region where informativeness had the highest change, i.e. from zero to four. e conditions we selected for the study were therefore:",1,ad,True
T0 where only the title for each result summary were presented;,0,,False
"T1 where for each result summary, a title and one query-biased snippet fragment were presented;",0,,False
T2 where a title and two query-biased snippet fragments were presented; and,0,,False
"T4 where a title and four query-biased snippet fragments were presented,",0,,False
"where our independent variable is snippet informativeness, controlled by the length. Figure 2 provides an example of the di erent",0,,False
"1 e tasks considered by Kim et al. [27] were similar to those de ned by Cutrell and Guan [11], where a single relevant document was sought. 2To obtain these values, we submi ed over 300 queries from a previous study (refer to Azzopardi et al. [3]) conducted on similar topics and on the same collection to the search system that we used.",0,,False
137,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"result summaries in each condition. e remainder of this section details our methodology for this experiment, including a discussion of: the corpus, topics and system used (Subsection 3.1); how we generated snippets (Subsection 3.2); the behaviours we logged (Subsection 3.3); how we obtained the opinions of subjects regarding their experience (Subsection 3.4); and further details on our study, including measures taken for quality control (Subsection 3.5).",0,,False
"3.1 Corpus, Search Topics and System",0,,False
"For this experiment, we used the TREC AQUAINT test collection. Using a traditional test collection provided us with the ability to easily evaluate the performance of subjects. e collection contains over one million newspaper articles from the period 1996-2000. Articles were gathered from three newswires: the Associated Press (AP); the New York Times (NYT); and Xinhua.",1,TREC,True
"We then selected a total of ve topics from the TREC 2005 Robust Track, as detailed by Voorhees [50]. e topics selected were:  341 (Airport Security);  347 (Wildlife Extinction);  367 (Piracy);  408 (Tropical Storms); and  435 (Curbing Population Growth). We selected topic  367 as a practice topic so that subjects could familiarise themselves with the system. ese topics were chosen based upon evidence from a previous user study with a similar setup, where it was shown that the topics were of similar di culty [23]. For each subject, the remaining four topics were assigned to an interface (one of T0, T1, T2 or T4) using a Latin-square rotation.",1,TREC,True
"To ground the search tasks, subjects of the experiment were instructed to imagine that they were newspaper reporters, and were required to gather documents to write stories about the provided topics. Subjects were told to nd as many relevant documents as they could during the allo ed time, which was 10 minutes per topic ­ herea er referred to as a search session. With the traditional components of a SERP, such as the query box and result summaries present (refer to Figure 3), subjects were instructed to mark documents they considered relevant by clicking on the `Mark as Relevant' bu on within the document view ­ accessed by clicking on a result summary he or she thought was relevant. Coupled with a two minute period to familiarise themselves with the system (using topic  367), subjects spent approximately 4550 minutes undertaking the complete experiment when pre- and post-task surveys were accounted for.",1,ad,True
"For the underlying search engine, we used the Whoosh Information Retrieval (IR) toolkit 3. We used BM25 as the retrieval algorithm (b ,"" 0.75), but with an implicit ANDing of query terms to restrict the set of retrieved documents to only those that contained all the query terms provided. is was chosen as most search systems implicitly AND terms together.""",0,,False
3.2 Snippet Generation,0,,False
"For interfaces T2 and T4, each result summary presented to the subjects required one or more textual snippets from the corresponding document. ese snippet fragments were query-biased [48], and were generated by scoring sentences according to BM25 and selecting fragments from those sentences. Fragments were then extracted",0,,False
3Whoosh can be accessed at h ps://pypi.python.org/pypi/Whoosh/.,0,,False
Words,0,,False
150 100,0,,False
50 00 -2 -3 -4 -5 0,0,,False
Length and Informativeness,0,,False
1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10,0,,False
IG,0,,False
IG / Word,0,,False
0,0,,False
-0.2,0,,False
-0.4 0 1 2 3 4 5 6 7 8 9 10,0,,False
"Number of Snippet Fragments Figure 1: Plots showing the length (in words), informativeness (in information gain, IG) and the information gain (IG) per word for title, plus 0 to 10 snippets. e closer the value is to zero, the more information that is gained.",0,,False
"from the ordered series of sentences, by identifying query terms within those sentences with a window of 40 characters from either side of the term. Figure 2 provides a complete, rendered example of the result summaries generated by each of the four interfaces. Each result summary contains a document title, a newswire source (acting as a replacement for a document URL), and, if required, one or more textual snippets.",0,,False
3.3 Behaviours Logged,0,,False
"In order for us to address our research questions, our experimental system was required to log a variety of behavioural a ributes for each subject as they performed the variety of actions that take place during a search session. Search behaviours were operationalised over three types of measures: (i) interaction, (ii) performance, and (iii) the time spent undertaking various search activities. All behavioural data was extracted from the log data produced by our system, and from the TREC 2005 Robust Track QRELs [50]. All data was recorded with the interface and topic combination used by the subject at the given time.",1,ad,True
"Interaction measures included the number of queries issued, the number of documents viewed, the number of SERPs viewed, and the greatest depths in the SERPs to which subjects clicked on ­ and hovered over ­ result summaries.",0,,False
"Performance measures included a count of the documents marked as relevant by the subject, the number of documents marked that were also TREC relevant ­ as well as TREC non-relevant, and P@k",1,TREC,True
138,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
T0 Venezuela Declares 42 Species in Danger of Extinction Xinhua News Service,0,,False
T1,0,,False
"Venezuela Declares 42 Species in Danger of Extinction ...the mammals in danger of extinction are the giant cachicamo, Margarita and...",0,,False
Xinhua News Service,0,,False
T2,0,,False
Venezuela Declares 42 Species in Danger of Extinction,0,,False
"...of animals in danger of extinction and banned game hunting of another 105...affecting the population of existing wildlife, such as the irrational exploitation...",0,,False
Xinhua News Service,0,,False
T4,0,,False
Venezuela Declares 42 Species in Danger of Extinction,0,,False
"16 (Xinhua) ­ Venezuela declared 42 wildlife species of animals in danger of...of animals in danger of extinction and banned game hunting of another 105...affecting the population of existing wildlife, such as the irrational exploitation...the mammals in danger of extinction are the giant cachicamo, Margarita and...",0,,False
Xinhua News Service,0,,False
"Figure 2: Examples of the result summaries generated by each of the four interfaces used in this study. e same document above is used ­ with the circle denoting what interface is being shown (of T0, T1, T2 or T4). Each of the result summaries consists of a title (in blue, underlined), none, one or more snippet fragments (in black, with fragments separated by ellipses), and a newswire source (in green).",0,,False
measurements for the performance of the subject's issued queries for a range of rankings.,0,,False
"Time-Based measures included the time spent issuing queries, examining SERPs ­ as well as examining result summaries4 ­ and the time spent examining documents. All of these times added together yielded the total search session time, which elapsed once 10 minutes had been reached.",1,ad,True
"From this raw data, we could then produce summaries of a search session, producing summarising measures such as the number of documents examined by searchers per query that they issued. We could also calculate from the log data probabilities of interaction, including a given subject's probability of clicking a result summary link, given that it was TREC relevant (P (C |R)) or TREC nonrelevant (P (C |N )) ­ or the probability of marking a document that was clicked, given it was either TREC relevant (P (M |R)) or TREC non-relevant (P (M |N )). Actions such as hover depth over result summaries were inferred from the movement of the mouse cursor, which in prior studies has been shown to correlate strongly with the user's gaze on the screen [7, 44].",1,TREC,True
3.4 Capturing User Experiences,0,,False
"To capture user experiences, we asked subjects to complete both pre- and post-task surveys for each of the four interface conditions.",0,,False
4Result summary times were approximated by dividing the total recorded SERP time by the number of snippets hovered over with the mouse cursor. We believe this is a reasonable assumption to make ­ the timings of hover events proved to be unreliable due to occasional network latency issues beyond our control.,0,,False
"Figure 3: Screenshot of the experimental search interface, showing the SERP view, complete with query box (with query `wildlife extinction') and the associated result summaries. In this example screenshot, interface T2 ­ presenting two snippets per result summary ­ is shown.",0,,False
"Pre-task surveys consisted of ve questions, each of which was on a seven-point Likert scale (7 ­ strongly agree to 1 ­ strongly disagree). Subjects were sought for their opinions on their: (i) prior knowledge of the topic; (ii) the relevancy of the topic to their lives; (iii) their desire to learn about the topic; (iv) whether they had searched on this topic before; and (v) the perceived di culty to search for information on the topic.",1,ad,True
"e same Likert scale was used for post-task surveys, where subjects were asked to judge the following statements: (clarity) ­ the result summaries were clear and concise; (con dence) ­ the result summaries increased my con dence in my decisions; (informativeness) ­ the result summaries were informative; (relevance) ­ the results summaries help me judge the relevance of the document; (readable) ­ the result summaries were readable; and (size) ­ the result summaries were an appropriate size and length.",1,ad,True
"At the end of the experiment, subjects completed an exit survey. From ve questions, they were asked to pick which of the four interfaces was the closest t to their experience. We sought opinions on what interface: (most informative) ­ yielded the most informative result summaries; (least helpful) ­ provided the most unhelpful summaries; (easiest) ­ provided the easiest to understand summaries; (least useful) ­ provided the least useful result summaries; and (most preferred) ­ the subject's preferred choice for the tasks that they undertook.",0,,False
3.5 Crowdsourced Subjects & ality Control,0,,False
"As highlighted by Zuccon et al. [54], crowdsourcing provides an alternative means for capturing user interactions and search behaviours from traditional lab-based user studies. Greater volumes of data can be obtained from more heterogeneous workers at a lower cost ­ all within a shorter timeframe. Of course, pitfalls of a crowdsourced approach include the possibility of workers completing tasks as e ciently as possible, or submi ing their tasks without performing the requested operations [13]. Despite these issues, it has been shown that there is li le di erence in the quality between crowdsourced and lab-based studies [54]. Nevertheless, quality control is a major component of a well-executed crowdsourced experiment [5]. Here, we detail our subjects and precautions taken.",1,ad,True
139,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"e study was run over the Amazon Mechanical Turk (MTurk) platform. Workers from the platform performed a single Human Intelligence Task (HIT), which corresponded to the entire experiment. Due to the expected length of completion for the study (45-50 minutes), subjects who completed the study in full were reimbursed for their time with US$9; a typically larger sum (and HIT duration) than most crowdsourced experiments. A total of 60 subjects took part in the experiment, which was run between July and August, 2016. However, seven subjects were omi ed due to quality control constraints (see below). In all, of the 53 subjects who satis ed the expected conditions of the experiment, 28 were male, with 25 female.",0,,False
"e average age of our subjects was 33.8 years (min , 22; max , 48; stde ,"" 7.0), with 19 of the subjects possessing a bachelor's degree or higher, and all expressing a high degree of search literacy, with all subjects stating that they conducted at least ve searches for information online per week. With 53 subjects, each searching over four topics, this meant a total of 212 search sessions were logged.""",0,,False
"We examined extra precautionary measures to ensure the integrity of the log data that was recorded. Precautions were taken from several angles. First, workers were only permi ed to begin the experiment on the MTurk platform that: (i) were from the United States, and were native English speakers; (ii) had a HIT acceptance rate of at least 95%; and (iii) had at least 1000 HITs approved. Requiring (ii) and (iii) reduced the likelihood of recruiting individuals who would not complete the study in a satisfactory manner. Recruits were forewarned about the length of the HIT, which was considerably longer than other crowdsourced experiments.",1,ad,True
"We also ensured that the computer the subject was a empting the experiment on had a su ciently large screen resolution (1024x768 or greater) so as to display all of the experimental interface on screen. With the experiment being conducted in a Web browser popup window of a xed size, we wanted to ensure that all subjects would be able to see the same number of results on a SERP within the popup window's viewport. As the experiment was conducted via a Web browser, we wanted to ensure that only the controls provided by the experimental apparatus were used, meaning that the popup window had all other browser controls disabled to the best of our ability (i.e. history navigation, etc.). e experimental system was tested on several major Web browsers, across di erent operating systems. is gave us con dence that a similar experience would be had across di erent system con gurations.",1,ad,True
"We also implemented a series of log post-processing scripts a er completion of the study to further identify and capture individuals who did not perform the tasks as instructed. It was from here that we identi ed the seven subjects that did not complete the search tasks in a satisfactory way ­ spending less than three of the ten minutes searching. ese subjects were excluded from the study, reducing the number of subjects reported from 60 to 53. Finally, results are reported based upon the rst 360 seconds as some of the remaining subjects didn't fully use all 600 seconds.",0,,False
4 RESULTS,0,,False
"Both search behaviour and user experience measures were analysed by each interface. To evaluate these data, ANOVAs were conducted using the interfaces as factors; main e ects were examined with  , 0.05. Bonferroni tests were used for post-hoc analysis. It should",1,hoc,True
"Table 1: Characters, words and Information Gain (IG) across each of the four interface conditions. An ANOVA test reveals signi cant di erences, with follow-up tests (refer to Section 4) showing that each condition is signi cantly different to others. ere are clearly diminishing returns in information gain as snippet length increases. An IG value closer to zero denotes a higher level of IG. In the table, IG/W. denotes IG per word.",0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Words 6.58±0.01* 25.21±0.06* 44.29±0.10* 77.06±0.13* Chars. 37.37±0.05* 103.29±0.17* 168.36±0.23* 284.78±0.31*,0,,False
IG,0,,False
-6.35±0.01* -3.59±0.00* -3.00±0.00* -2.67±0.00*,0,,False
IG/W. -1.17±0.00* -0.18±0.00* -0.08±0.00* -0.04±0.00*,0,,False
Time Per Snippet 3,0,,False
Seconds,0,,False
2,0,,False
1 T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Interface Condition,0,,False
"Figure 4: Plot showing the mean time spent examining result summaries across each of the four interfaces examined. Note the increasing mean examination time as the snippet length increases, from T0T4.",0,,False
be noted that the error bars as shown in the plots for Figures 4 and 5 refer to the standard error.,0,,False
"To check whether the interfaces were di erent with respect to snippet length and information gain, we performed an analysis of the observed result summaries. Table 1 summarises the number of words and characters that result summaries contained on average. As expected, the table shows an increasing trend in words and characters as snippet lengths increase. Information gain for each snippet was then calculated using the KullbackLeibler distance [28] to measure information gain (e.g. relative entropy). Statistical testing showed that the di erences between snippet length (F (3, 208) ,"" 1.2x105, p < 0.001) and information gain (F (3, 208 "","" 2.6x105, p < 0.001)) were signi cant. Follow up tests revealed that this was the case over all four interfaces, indicating that our conditions were di erent on these dimensions. ese""",0,,False
ndings provide some justi cation for our choices for the number of snippet fragments present for each interface ­ a diminishing increase in information gain a er four snippets suggested that there wouldn't be much point generating anything longer.,0,,False
4.1 Search Behaviours,0,,False
"Interactions. Table 2 presents the mean (and standard deviations) of the number of queries issued, the number of SERPs viewed per query, documents clicked per query, and the click depth per query over each of the four interfaces examined. Across the four di erent interfaces, there were no signi cant di erences reported between",0,,False
140,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 2: Summary table of both interaction and performance measures over each of the four interfaces evaluated. For each measure examined, no signi cant di erences are reported across the four interfaces.",0,,False
Number of eries Number of SERP Pages per ery Number of Docs Clicked per ery Depth per ery,0,,False
P@10 Number of Documents Marked Relevant Number of TREC Rels Found Number of Unjudged Docs Marked Relevant,1,TREC,True
T0,0,,False
3.72± 0.34 2.87± 0.29 4.23± 0.55 24.47± 2.96 0.25± 0.02 6.68± 0.66 2.58± 0.34 1.85± 0.32,0,,False
T1,0,,False
3.19± 0.35 2.69± 0.23 4.83± 0.54 22.87± 2.47 0.23± 0.02 7.00± 0.63 2.28± 0.25 2.08± 0.29,0,,False
T2,0,,False
3.30± 0.35 2.43± 0.13 5.14± 0.66 20.02± 1.46 0.27± 0.02 6.49± 0.58 2.47± 0.28 1.98± 0.24,0,,False
T4,0,,False
3.28± 0.31 2.40± 0.20 4.76± 0.62 19.40± 2.04 0.25± 0.03 7.60± 0.79 2.66± 0.32 1.68± 0.32,0,,False
"Table 3: Summary table of times over each of the four interfaces evaluated. Signi cant di erences exist between T0 and T4 (identi ed by the *, where  , 0.05) on a follow-up Bonferroni test.",0,,False
Time per ery Time per Document Time per Result Summary*,0,,False
T0 8.29± 0.57 17.31± 2.12 1.63 ± 0.13*,0,,False
T1 7.99± 0.57 22.82± 6.03 2.21± 0.21,0,,False
T2 9.42± 0.79 17.19± 1.86 2.35± 0.23,0,,False
T4 8.12± 0.48 18.99± 2.13 2.60 ± 0.27*,0,,False
"any of these measures. e number of queries issued follows a slight downward trend as the length of result summaries increases (3.72 ± 0.34 for T0 to 3.28 ± 0.31 for T4), as too does the number of SERPs examined, and the number of documents examined per query. e depth to which subjects went to per query however follows a downward trend ­ as the length of snippets increases, subjects were likely to go to shallower depths when examining result summaries (24.47 ± 2.96 for T0 to 19.4 ± 2.04 for T4).",0,,False
"Interaction probabilities all showed an increasing trend as snippet length increased over the four interfaces, as shown in Table 4. Although no signi cant di erences were observed over the four interfaces and the di erent probabilities examined, trends across all probabilities show an increase as the snippet length increases. An increase of both the probability of clicking result summaries on the SERP (P (C)) and marking the associated documents (P (M )) as relevant were observed. When these probabilities are examined in more detail by separating the result summaries clicked and documents marked by their TREC relevancy (through use of TREC QRELs), we see increasing trends for clicking and marking ­ both for TREC relevant (P (C |R) and P (M |R) for clicking and marking, respectively) and TREC non-relevant documents (P (C |N ) and P (M |N )). is interesting nding shows that an increase in snippet length does not necessarily improve the accuracy of subjects ­ simply the likelihood that they would consider documents as relevant.",1,TREC,True
"Performance. Table 2 also reports a series of performance measures over the four conditions, averaged over the four topics examined. We report the mean performance of the queries issued with P@10, the number of documents marked relevant, and the number of documents marked relevant that were TREC relevant. Like the interaction measures above, no signi cant di erences were observed",1,TREC,True
"over the four interfaces for each of the performance measures examined. e performance of queries issued by subjects was very similar across all four conditions (P@10  0.25), along with the number of documents identi ed by subjects as relevant (6.49 ± 0.58 for T2 to 7.6 ± 0.79 for T4), and the count of documents marked that were actually TREC relevant (2.28 ± 0.25 for T1 to 2.66 ± 0.32 for T4). We also examined the number of documents marked that were not assessed (unjudged) by the TREC assessors, in case one interface surfaced more novel documents. On average, subjects marked two such documents, but again there was no signi cant di erences between interfaces.",1,TREC,True
"Time-Based Measures. Table 3 reports a series of selected interaction times over each of the four evaluated interfaces. We include: the mean total query time per subject, per interface; the mean time per query; the mean time spent examining documents per query; and the mean time spent examining result summaries per query. No signi cant di erences were found between the mean total query time, the time per query and the time per document. However, a signi cant di erence did exist for the time spent per result summary. A clear upward trend in the time spent examining snippets can be seen in Figure 4 as result summaries progressively got longer, from 1.63 ± 0.13 for T0 to 2.6 ± 0.27 for T4, which was signi cantly di erent (F (3, 208) ,"" 3.6, p "","" 0.014). A follow-up Bonferroni test showed that the signi cant di erence existed between T0 and T4. is suggests that as result summary length increases, the amount of time spent examining result summaries also increases (an intuitive result). is also complies with trends observed regarding examination depths. When the length of result summaries increased, subjects were likely to examine result summaries to shallower depths.""",0,,False
141,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: Table illustrating a summary of interaction probabilities over each of the four interfaces evaluated. Note the increasing trends for each probability from T0  T4 (short to long snippets). Refer to Section 4.1 for an explanation of what each probability represents.,0,,False
T0,0,,False
P (C) P (C |R) P (C |N ),0,,False
0.20± 0.02 0.28± 0.03 0.18± 0.02,0,,False
P (M) P (M |R) P (M |N ),0,,False
0.61± 0.04 0.66± 0.06 0.55± 0.04,0,,False
T1 0.25± 0.02 0.34± 0.03 0.23± 0.02 0.68± 0.04 0.69± 0.05 0.65± 0.04,0,,False
T2 0.26± 0.03 0.35± 0.03 0.25± 0.03 0.65± 0.03 0.67± 0.05 0.58± 0.04,0,,False
T4 0.28± 0.03 0.40± 0.04 0.24± 0.03 0.71± 0.03 0.66± 0.05 0.67± 0.04,0,,False
"Table 5: Summary table of the recorded observations for the post-task survey, indicating the preferences of subjects over six criteria and the four interfaces, where  indicates that T0 was signi cantly di erent from the other conditions. In the table, Conf. represents Con dence, Read. represents Readability, Inform. represents Informativeness, and Rel. represents Relevancy.",1,ad,True
T0,0,,False
T1,0,,False
Clarity 4.16± 0.27*,0,,False
Conf. 3.71± 0.26*,0,,False
Read. 5.18± 0.31*,1,ad,True
Inform. 4.20± 0.30*,0,,False
Rel.,0,,False
3.84± 0.28*,0,,False
Size,0,,False
4.00± 0.31*,0,,False
5.00± 0.21 4.66± 0.26 6.32± 0.17 5.38± 0.24 4.89± 0.25 4.94± 0.25,0,,False
T2,0,,False
5.06± 0.24 4.75± 0.24 6.46± 0.14 5.27± 0.24 5.08± 0.24 5.21± 0.22,0,,False
T4,0,,False
5.40± 0.20 5.06± 0.25 6.36± 0.14 5.62± 0.20 5.36± 0.20 5.36± 0.19,0,,False
Table 6: Table presenting responses from the exit survey completed by subjects. e survey is discussed in Section 3.4.,0,,False
Most Informative Least helpful Easiest Least Useful Most Preferred,0,,False
T0 T1 T2 T4,0,,False
1,0,,False
4,0,,False
20 29,0,,False
46,0,,False
5,0,,False
1,0,,False
2,0,,False
4,0,,False
4,0,,False
24 22,0,,False
49,0,,False
4,0,,False
0,0,,False
1,0,,False
3,0,,False
5,0,,False
20 26,0,,False
4.2 User Experience,0,,False
"Task Evaluations. Table 5 presents the mean set of results from subjects across the four interfaces, which were answered upon completion of each search task. e survey questions are detailed in Section 3.4. Using the seven-point Likert scale for their responses (with 7 indicating strongly agree, and 1 indicating strongly disagree), signi cant di erences were found in all question responses (clarity F (3, 208) ,"" 5.22, p "","" 0.001, con dence F (3, 208) "","" 5.3, p "","" 0.001,""",0,,False
"readable F (3, 208) ,"" 9.25, p < 0.001, informative F (3, 208) "","" 5.22, p "","" 0.001, relevance F (3, 208) "","" 6.44, p < 0.001, and size F (3, 208) "","" 7.28, p < 0.001). Follow-up Bonferroni tests however showed that the signi cant di erence existed only between T0 and the remaining three interfaces, T1, T2 and T4. A series of discernible trends can be observed throughout the responses, with subjects regarding longer snippets as more concise, and a higher degree of clarity (4.16±0.27 for T0 to 5.4±0.2 for T4). is perceived clarity also made subjects feel more con dent that the longer result summaries helped them make be er decisions as to whether they were relevant to the given topic ­ interaction results presented above however di er from this, where the overall probability of marking documents increased, regardless of the document/topic TREC relevancy judgement. Other notable trends observed from the results included an increase in how informative subjects perceived the result summaries to be ­ again, with longer summaries proving more informative. Subjects also reported a general increase in satisfaction of the length of the presented result summaries/snippets ­ although, as mentioned, no signi cant di erence existed between the three interfaces that generated snippets (T1, T2 and T4).""",1,ad,True
"System Evaluations. Upon completion of the study, subjects completed the exit survey as detailed in Section 3.4. Responses from the subjects are presented in Table 6. From the results, subjects found result summaries of longer lengths (i.e. those generated by interfaces T2 and T4) to be the most informative, and those generated by T0 ­ without snippets ­ to be the least helpful and useful.",0,,False
"e longer result summaries were also consistently favoured by subjects, who preferred them over the result summaries generated by interfaces T0 and T1. Subjects also found the result summaries of longer length easier to use to satisfy the given information need.",0,,False
"From the results, it is therefore clear that a majority of subjects preferred longer result summaries to be presented on SERPs, generated by interfaces T2 and T4. Figure 5 provides summary plots, showing general trends across the four interfaces, examining observed interactions and reported experiences.",0,,False
5 DISCUSSION AND FUTURE WORK,0,,False
"In this paper, we investigated the in uence of result summary length on search behaviour and performance. Using the KullbackLeibler distance [28] as a measure of information gain, we examined result summaries of di erent lengths, selected a series of snippet lengths where there was a signi cant di erence in information gain between them, which yielded the con gurations for our four experimental conditions, T0, T1, T2 and T4. We conducted a crowdsourced user study comprising of 53 subjects, each of whom undertook four search tasks, using each of the four interfaces.",0,,False
"Our work was focused around addressing our two research questions, which explored RQ1 how the value of information gain (represented by snippet length) a ected search behaviour and user experience; and RQ2 whether information gain a ected the decision making ability and accuracy of users. Addressing RQ1 rst in terms of search behaviour, there was li le di erence ­ but we did observe the following trends: as summary length increases, participants: issued fewer queries; examined fewer pages; but clicked more documents, i.e. they spent more of their time assessing documents at higher ranks. Second, our results show that in terms",1,ad,True
142,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
#,0,,False
#,0,,False
#,0,,False
Actions,0,,False
Query Count 5,1,Query,True
4,0,,False
3,0,,False
2 T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Pages per Query 4,1,Query,True
3,0,,False
2 T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Docs per Query 6,1,Query,True
4,0,,False
2 T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Depth per Query 30,1,Query,True
20,0,,False
10 T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Docs Marked Relevant 10,0,,False
8,0,,False
6,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Rel Count 3,0,,False
2.5,0,,False
2 T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Interface Condition,0,,False
P( M | N ),0,,False
P( M | R ),0,,False
P( M ),0,,False
P( C | N ),0,,False
P( C | R ),0,,False
P( C ),0,,False
Probabilities,0,,False
P( Click ),0,,False
0.4,0,,False
0.3,0,,False
0.2,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
P( Click | Relevant),0,,False
0.4,0,,False
0.3,0,,False
0.2,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
P( Click | Non-Relevant),0,,False
0.4,0,,False
0.3,0,,False
0.2,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
P( Mark ) 0.8,0,,False
0.6,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
P( Mark | Relevant ) 0.8,0,,False
0.6 T0,0,,False
0.8,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
P( Mark | Non-Relevant ),0,,False
0.6,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Interface Condition,0,,False
Post-Task,0,,False
Clarity,0,,False
6,0,,False
4,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Confidence,0,,False
6,0,,False
4,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Readable,1,ad,True
6,0,,False
4,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Informativeness,0,,False
6,0,,False
4,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Relevance,0,,False
6,0,,False
4,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Size,0,,False
6,0,,False
4,0,,False
T0,0,,False
T1,0,,False
T2,0,,False
T4,0,,False
Interface Condition,0,,False
#,0,,False
#,0,,False
#,0,,False
"Figure 5: Plots, showing a variety of measures and survey results from subjects across the four interfaces examined. From le to right: actions associated with the subjects' search behaviours and performance; probabilities of interaction; and post-task survey responses, using a seven-point Likert scale (7 - strongly agree to 1 - strongly disagree).",0,,False
"of experience, subjects broadly preferred longer summaries. e participants felt that longer summaries were more clear, informative, readable ­ and interestingly ­ gave them more con dence in their relevance decisions. With respect to RQ2, we again observed li le di erence in subjects' decision making abilities and accuracy between the four interfaces. While subjects perceived longer snippets to help them infer relevance more accurately, our empirical evidence shows otherwise. In fact, it would appear that longer result summaries were more a ractive, increasing the information scent of the SERP [53]. is may account for the increase in clicks on the early results, without the bene ts, however: accuracy of our subjects did not improve with longer snippets; nor did they nd more relevant documents. Increased con dence in the result summaries (from T0  T4) may have led to a more relaxed approach at marking content as relevant ­ as can be seen by increasing click and mark probabilities for both relevant and non-relevant content. It is also possible that the paradox of choice [37] could play a role in shaping a user's preferences. For example, in the condition with longer result summaries (T4), users viewed fewer results/choices than on other conditions. is may have contributed to their feelings of greater satisfaction and increased con dence in their decisions.",1,ad,True
"ese novel ndings provide new insights into how users interact with result summaries in terms of their experiences and search behaviours. Previous work had only focused upon task completion times and accuracy of the rst result while not considering their experiences (e.g. [11, 19]). Furthermore, these past works were performed in the context of Web search where the goal was to nd one document. However, we acknowledge that our work also has limitations. Here, we examined out research questions ­ with respect to topic search within a news collection ­ to explore how behaviour and performance changes when searching for multiple relevant documents. It would be interesting to examine this in other search contexts, such as product search, for example. News article titles also can be cra ed di erently from documents in other domains. Summaries in this domain may perhaps be more important than in other domains, and so the e ects and in uences are likely to be larger. Furthermore, we only considered how behaviours changed on the desktop, rather than on other devices where users are more likely to be sensitive to such changes (e.g. [25, 27]). For example, during casual leisure search, multiple relevant documents on tablet devices are o en found, and so it would be interesting to perform a follow up study in this area.",1,ad,True
143,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"To conclude, our ndings show that longer result summaries,",0,,False
"while containing a greater amount of information content, are not",0,,False
necessarily be er in terms of decision making ­ although subjects,0,,False
perceived this to be the case. We also show a positive relationship,0,,False
between the length and informativeness of result summaries and,0,,False
their a ractiveness (clickthrough rates). ese results show that the,0,,False
experience and perceptions of users ­ and the actual performance,0,,False
"of those users ­ is di erent, and when designing interfaces, this",0,,False
needs be taken into account.,0,,False
Acknowledgments Our thanks to Alastair Maxwell and Stuart Mackie,0,,False
"for their comments, the 53 participants of this study, and the anonymous",0,,False
reviewers for their feedback. e lead author is nancially supported by,1,ad,True
"the UK Government through the EPSRC, grant . 1367507.",1,Gov,True
REFERENCES,0,,False
"[1] M. Ageev, D. Lagun, and E. Agichtein. Improving search result summaries by using searcher behavior data. In Proc. 35th ACM SIGIR, pages 13­22, 2013.",0,,False
"[2] H. Ali, F. Scholer, J. A. om, and M. Wu. User interaction with novel web search interfaces. In Proc. 21st OZCHI, pages 301­304.",0,,False
"[3] L. Azzopardi, D. Kelly, and K. Brennan. How query cost a ects search behavior. In Proceedings of the 36th International ACM SIGIR Conference on Research and Development in IR, pages 23­32, 2013.",0,,False
"[4] H. Bast and M. Celikik. E cient index-based snippet generation. ACM Trans. Inf. Syst., 32(2):6:1­6:24, Apr. 2014.",0,,False
"[5] H. Bota, K. Zhou, and J. M. Jose. Playing your cards right: e e ect of entity cards on search behaviour and workload. In Proc. 1st ACM CHIIR, pages 131­140, 2016.",1,ad,True
"[6] H. Chen and S. Dumais. Bringing order to the web: Automatically categorizing search results. In Proc. 18th ACM CHI, pages 145­152, 2000.",0,,False
"[7] M. C. Chen, J. R. Anderson, and M. H. Sohn. What can a mouse cursor tell us more?: Correlation of eye/mouse movements on web browsing. In Proc. 19th ACM CHI Extended Abstracts, pages 281­282, 2001.",0,,False
"[8] F. Chieriche i, R. Kumar, and P. Raghavan. Optimizing two-dimensional search results presentation. In Proc. 4th ACM WSDM, pages 257­266, 2011.",0,,False
"[9] C. L. A. Clarke, E. Agichtein, S. Dumais, and R. W. White. e in uence of caption features on clickthrough pa erns in web search. In Proc. 30th ACM SIGIR, pages 135­142, 2007.",0,,False
"[10] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An experimental comparison of click position-bias models. In Proc. 1st ACM WSDM, pages 87­94, 2008.",0,,False
"[11] E. Cutrell and Z. Guan. What are you looking for?: an eye-tracking study of information usage in web search. In Proc. 25th ACM CHI, pages 407­416, 2007.",0,,False
"[12] S. Dumais, E. Cutrell, and H. Chen. Optimizing search by showing results in context. In Proc. 19th ACM CHI, pages 277­284, 2001.",0,,False
"[13] H. Feild, R. Jones, R. Miller, R. Nayak, E. Churchill, and E. Velipasaoglu. Logging the search self-e cacy of amazon mechanical turkers. In Proc. CSE SIGIR Workshop, pages 27­30, 2010.",0,,False
"[14] J. He, P. Duboue, and J.-Y. Nie. Bridging the gap between intrinsic and perceived relevance in snippet generation. In Proc. of COLING 2012, pages 1129­1146, 2012.",0,,False
"[15] M. Hearst. Search user interfaces. Cambridge University Press, 2009. [16] P. Ingwersen and K. Ja¨rvelin. e Turn: Integration of Information Seeking and",0,,False
"Retrieval in Context. 2005. [17] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and G. Gay. Accurately inter-",0,,False
"preting clickthrough data as implicit feedback. In Proc. 28th ACM SIGIR, pages 154­161, 2005. [18] H. Joho and J. M. Jose. A comparative study of the e ectiveness of search result presentation on the web. In Proc. 28th ECIR, pages 302­313, 2006. [19] M. Kaisser, M. A. Hearst, and J. B. Lowe. Improving search results quality by customizing summary lengths. In Proc. 46th ACL, pages 701­709, 2008. [20] Y. Kammerer and P. Gerjets. How the interface design in uences users' spontaneous trustworthiness evaluations of web search results: comparing a list and a grid interface. In Proc. of the Symp. on Eye-Tracking Research & Applications, pages 299­306, 2010. [21] T. Kanungo and D. Orr. Predicting the readability of short web summaries. In Proc. 2nd ACM WSDM, pages 202­211, 2009. [22] D. Kelly and L. Azzopardi. How many results per page?: A study of serp size, search behavior and user experience. In Proc. 38th ACM SIGIR, pages 183­192, 2015. [23] D. Kelly, K. Gyllstrom, and E. W. Bailey. A comparison of query and term suggestion features for interactive searching. In Proc. 32nd ACM SIGIR, pages 371­378, 2009.",1,Track,True
"[24] J. Kim, P. omas, R. Sankaranarayana, and T. Gedeon. Comparing scanning behaviour in web search on small and large screens. In Proc. 17th ADCS, pages 25­30, 2012.",0,,False
"[25] J. Kim, P. omas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. Eye-tracking analysis of user behavior and performance in web search on large and small screens. J. of the Assoc. for Information Science and Technology, 2014.",0,,False
"[26] J. Kim, P. omas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. Pagination versus scrolling in mobile web search. In Proc. 25th ACM CIKM, pages 751­760, 2016.",0,,False
"[27] J. Kim, P. omas, R. Sankaranarayana, T. Gedeon, and H.-J. Yoon. What snippet size is needed in mobile web search? In Proc. 2nd ACM CHIIR, pages 97­106, 2017.",0,,False
"[28] S. Kullback and R. A. Leibler. On information and su ciency. e Annals of Mathematical Statistics, 22:79­86, 1951.",0,,False
"[29] J. Kupiec, J. Pedersen, and F. Chen. A trainable document summarizer. In Proc. 18th ACM SIGIR, pages 68­73, 1995.",0,,False
"[30] T. Landauer, D. Egan, J. Remde, M. Lesk, C. Lochbaum, and D. Ketchum. Enhancing the usability of text through computer delivery and formative evaluation: the superbook project. Hypertext: A psychological perspective, pages 71­136, 1993.",0,,False
"[31] L. Leal-Bando, F. Scholer, and A. Turpin. ery-biased summary generation assisted by query expansion. J. Assoc. for Info. Sci. and Tech., 66(5):961­979, 2015.",0,,False
"[32] Q. Li and Y. P. Chen. Personalized text snippet extraction using statistical language models. Pa ern Recogn., 43(1):378­386, Jan. 2010.",0,,False
"[33] G. Linden. Marissa mayer at web 2.0, November 2006. h p:// glinden.blogspot. com/ 2006/ 11/ marissa-mayer-at-web-20.html.",1,blog,True
"[34] Marcos, M-C. and Gavin, F. and Arapakis, I. E ect of snippets on user experience in web search. In Proc. 16th Intl. Conf. on HCI, pages 47:1­47:8, 2015.",0,,False
"[35] A. Muralidharan, Z. Gyongyi, and E. Chi. Social annotations in web search. In Proc. 30th ACM CHI, pages 1085­1094, 2012.",0,,False
"[36] V. Navalpakkam, L. Jentzsch, R. Sayres, S. Ravi, A. Ahmed, and A. Smola. Measurement and modeling of eye-mouse behavior in the presence of nonlinear page layouts. In Proc. 22nd WWW, pages 953­964, 2013.",0,,False
"[37] A. Oulasvirta, J. Hukkinen, and B. Schwartz. When more is less: e paradox of choice in search engine use. In Proc. 32nd ACM SIGIR, pages 516­523, 2009.",1,ad,True
"[38] T. Paek, S. Dumais, and R. Logan. Wavelens: A new view onto internet search results. In Proc. 22nd ACM CHI, pages 727­734, 2004.",0,,False
"[39] J. Pedersen, D. Cu ing, J. Tukey, et al. Snippet search: A single phrase approach to text access. In Proc. 1991 Joint Statistical Meetings, 1991.",0,,False
"[40] M. L. Resnick, C. Maldonado, J. M. Santos, and R. Lergier. Modeling on-line search behavior using alternative output structures. In Proc. Human Factors and Ergonomics Soc. Annual Meeting, volume 45, pages 1166­1170, 2001.",1,ad,True
"[41] D. E. Rose, D. Orr, and R. G. P. Kantamneni. Summary a ributes and perceived search quality. In Proc. 16th WWW, pages 1201­1202, 2007.",0,,False
"[42] D. Savenkov, P. Braslavski, and M. Lebedev. Search snippet evaluation at yandex: lessons learned and future directions. Multilingual & Multimodal Information Access Evaluation, pages 14­25, 2011.",0,,False
"[43] B. Schwartz. e Paradox of Choice: Why More Is Less. Harper Perennial, 2005. [44] M. Smucker, X. Guo, and A. Toulis. Mouse movement during relevance judging:",1,ad,True
"Implications for determining user a ention. In Proc. 37th ACM SIGIR, pages 979­982, 2014. [45] N. V. Spirin, A. S. Kotov, K. G. Karahalios, V. Mladenov, and P. A. Izhutov. A comparative study of query-biased and non-redundant snippets for structured search on mobile devices. In Proc. 25th ACM CIKM, pages 2389­2394, 2016. [46] K. M. Svore, J. Teevan, S. T. Dumais, and A. Kulkarni. Creating temporally dynamic web search snippets. In Proc. 35th ACM SIGIR, pages 1045­1046, 2012. [47] J. Teevan, E. Cutrell, D. Fisher, S. M. Drucker, G. Ramos, P. Andre´, and C. Hu. Visual snippets: Summarizing web pages for search and revisitation. In Proc. 27th ACM CHI, pages 2023­2032, 2009. [48] A. Tombros and M. Sanderson. Advantages of query biased summaries in information retrieval. In Proc. 21st ACM SIGIR, pages 2­10, 1998. [49] A. Turpin, Y. Tsegay, D. Hawking, and H. E. Williams. Fast generation of result snippets in web search. In Proc. 30th ACM SIGIR, pages 127­134, 2007. [50] E. M. Voorhees. Overview of the trec 2005 robust track. In Proc. TREC-14, 2006. [51] R. W. White, J. M. Jose, and I. Ruthven. A task-oriented study on the in uencing e ects of query-biased summarisation in web searching. Info. Processing & Management, 39(5):707­733, 2003. [52] A. Woodru , R. Rosenholtz, J. B. Morrison, A. Faulring, and P. Pirolli. A comparison of the use of text summaries, plain thumbnails, and enhanced thumbnails for web search tasks. J. Am. Soc. Inf. Sci. Technol., 53(2):172­185, 2002. [53] Wu, W-C. and Kelly, D. and Sud, A. Using information scent and need for cognition to understand online search behavior. In Proceedings of the 37th International ACM SIGIR Conference, SIGIR '14, pages 557­566, 2014. [54] G. Zuccon, T. Leelanupab, S. Whiting, E. Yilmaz, J. Jose, and L. Azzopardi. Crowdsourcing interactions: using crowdsourcing for evaluating interactive information retrieval systems. Information Retrieval, 16(2):267­305, 2013.",1,ad,True
144,0,,False
,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Improving Exploratory Search Experience through Hierarchical Knowledge Graphs,0,,False
Bahareh Sarrafzadeh,1,ad,True
"Cheriton School of Computer Science University of Waterloo, Canada bsarrafz@uwaterloo.ca",1,ad,True
ABSTRACT,0,,False
"In information retrieval and information visualization, hierarchies are a common tool to structure information into topics or facets, and network visualizations such as knowledge graphs link related concepts within a domain. In this paper, we explore a multi-layer extension to knowledge graphs, hierarchical knowledge graphs (HKGs), that combines hierarchical and network visualizations into a uni ed data representation. rough interaction logs, we show that HKGs preserve the bene ts of single-layer knowledge graphs at conveying domain knowledge while incorporating the sensemaking advantages of hierarchies for knowledge seeking tasks. Specially, this paper describes our algorithm to construct these visualizations, analyzes interaction logs to quantitatively demonstrate performance parity with networks and performance advantages over hierarchies, and synthesizes data from interaction logs, interviews, and thinkalouds on a testbed data set to demonstrate the utility of the uni ed hierarchy+network structure in our HKGs.",1,corpora,True
CCS CONCEPTS,0,,False
·Information systems Search interfaces;,0,,False
KEYWORDS,0,,False
"Knowledge Graphs, Hierarchies, Exploratory Search, Information Seeking, Representations of Search Results",0,,False
1 INTRODUCTION,1,DUC,True
"Finding information on the Web is o en di cult. ere are two predominant paradigms for nding information on the Web: Searching (i.e, Search by query) and Browsing (i.e, Search by Navigation) [20, 31]. While current search engines, following a ""search by query"" paradigm, are generally su cient when the information need is well-de ned in the searcher's mind, examining search results remains a necessary step within a larger information seeking process [23, 25]. To elaborate, Searching requires the user to translate an information need into queries, while Browsing accommodates the knowledge gap between what the user is able to communicate and what the system requires to nd the desired information. is",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080829",1,ad,True
Edward Lank,0,,False
"Cheriton School of Computer Science University of Waterloo, Canada",1,ad,True
"CRIStAL, University of Lille, France lank@uwaterloo.ca",0,,False
"knowledge gap (also formalized as an `anomalous state of knowledge' by Belkin [5]) is more evident when information is sought to address broad curiosities, for learning and other complex mental activities [2, 43].",1,ad,True
"is paper focuses on the design of tools to support browsing. Researchers note that, while search interfaces must support query formulation and an e ective ranking algorithm, browsing interfaces require e ective representations of search results to accommodate searcher's `state of knowledge' and provide `information scent' to guide the user during navigation [31, 32]. Related to this observation of `information scent' is the observation that information seekers o en express a desire for a user interface that organizes search results into meaningful structures to facilitate browsing and understanding of the retrieved results [15].",0,,False
"e desire for browsing support via structure has given rise to interfaces that represent the structure of information. ese structures typically exist in one of two forms: hierarchies and networks [11]. First, information can be presented into hierarchies based on categories. Included in this class of search results organizations are techniques such as faceted browsing or automatic clustering. Second, alongside hierarchies, entity relationship or network representations are also used. In these representations, rather than clustering objects into labeled categories, the connections between objects (people, places, things, etc.) in a document corpus represent a relationship between two items. ese network representations include knowledge graphs [18] and concept maps [29].",0,,False
"Recent work has explored the relative bene ts of hierarchies and networks and has noted that the bene ts are largely complementary: hierarchies provide users with some understanding of central topics, allowing them to develop a be er overview of information; whereas networks allow people to glean concrete information from the representation rather than needing to extensively read individual documents [39]. Given the complementary advantages of knowledge graphs and hierarchies, our main research question in this paper is that whether we can algorithmically generate a seamless data structure that combines the advantages of both hierarchies and networks into a single uni ed structure.",1,ad,True
"In this paper, we evaluate the e cacy of hierarchical knowledge graphs (HKGs) as a combined representation of low-level entity relationships and high-level central concepts. We generate these knowledge graphs automatically using a simple parsing algorithm [37], then extract hierarchies using a dynamic thresholding approach. We evaluate these HKGs using a mixed methods approach. antitative data argues that HKGs preserve the transparency advantages of knowledge graphs and structural advantages",1,ad,True
145,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
of hierarchies. alitative data triangulates with quantitative observations and provides additional insight into the advantages and disadvantages of both hierarchical and network visualizations.,1,ad,True
2 RELATED WORK,0,,False
In this section we survey related work in classifying and understanding web search and in techniques to augment search results.,0,,False
2.1 Understanding web search,0,,False
"ere exist several characterizations of web search queries [6­8, 35, 44]. Marchionini [24] focuses speci cally on a process he terms exploratory search. Marchionini de nes three categories of web search: Look-up, Learn, and Investigate and groups Learn and Investigate tasks under the umbrella of exploratory search.",0,,False
"Learn and Investigate tasks involve sensemaking [14, 36]. Sensemaking is the process of searching for a representation and encoding data in that representation to answer task-speci c questions [36]. Russell et al. [36] present four main cognitive stages that are involved in sensemaking: an interaction between a bo om-up `Search for representation' phase and a top-down representation instantiation phase, a phase of shi ing representations to t newly discovered data into the current representation, and an application of the representation to the user's speci c task.",0,,False
"e top-level representation that results from sensemaking has many di erent names: a ""holistic cognitive structure"" [42], a mindmap, a concept map [29]. Regardless, the construction of this representation must be either provided explicitly by the interface or constructed implicitly by the user [36, 42] before the user can fully ""make sense"" of the retrieved information.",0,,False
"Alongside the distinction between ""look-up"" and exploratory search, there are also varying levels of complexity associated with exploratory search tasks [24]. At the simplest level, one might simply wish to read a document to understand a topic. However, consider tasks such as comparing two di erent diets to understand health bene ts (e.g. oriental and Mediterranean), comparing two di erent political systems to understand relative authoritarianism (e.g. President of Russia versus Iran), or comparing two di erent educational systems to understand relative student success (e.g. British versus Canadian). All of these, as examples of exploratory search tasks, require both searching and browsing to identify information, acquire knowledge, and contrast related data across two contexts. As the complexity of comparison increases (three or more alternatives, more abstract concepts), the information seeking task continues to increase in complexity. Searching is highly e ective at identifying relevant documents to guide basic exploratory search, but browsing and reformulating are needed to fully acquire and synthesize knowledge [23, 26]. erefore, understanding both searching and browsing behaviors are of vital importance to design e ective interactive information retrieval systems.",1,ad,True
2.2 Organizing Search Results,0,,False
"Because of the importance of structure in search, there have been e orts to contrast strengths and weaknesses of di erent spatial representations and groupings of search results. A taxonomy of techniques for organizing search results was proposed by Wilson et al. [46]. ey identify two main classes of approaches: (1) Coupling results with additional metadata and classi cations such that",1,ad,True
"searchers can interact and control the presentation of results. (e.g. faceted browsing or categories), or (2) providing alternative or complementary representations of search results (e.g, a network representation). Wilson et al. also present four common approaches to structured classi cation [46]: hierarchical classi cations, faceted classi cations, automated clustering, and social classi cations.",0,,False
"Looking rst at structured classi cation, early forays into the domain of structuring search results contrasted categories with automatic clustering to support search. Hearst [16] showed that categories, because they were more interpretable for the user, captured important information about the document but became unwieldy when the document corpus was too large. Clusters, by comparison, were highly variable with respect to quality and were o en less meaningful for the user.",0,,False
"Given the lack of intuitiveness associated with clustering [17] and a desire for understandable hierarchies in which categories are presented at uniform levels of granularity [33, 34], alongside speci ed hierarchies such as tables-of-contents, researchers have explored faceted categories, i.e. categories that are semantically related to the search task of the user, to organize search results.",0,,False
"ese include systems that de ne faceted categories [48], research that studies the use of facets to support browsing [9], and research that identi es strengths and weaknesses of faceted browsers [45]. In terms of strengths and weaknesses, faceted browsing has proven bene cial for users already clear about their search task [45]; additional information on interactions between facets (e.g. inter-facet relationships) is helpful when users are unfamiliar with a domain and need `sensemaking'. In other words, exploratory tasks (e.g.learning or investigating [24]) are precisely those tasks where interactions between facets are needed.",1,ad,True
"e need to represent relationships between facets or concepts has given rise to the use of network structures to depict relationships between concepts or entities in a corpus. ese network structures include concept maps, knowledge graphs, and other entity-relationship diagrams. Concept mapping has been widely used in education as a method for knowledge examination, sharing, and browsing [10, 29]. Knowledge graphs have been popularized by Google to represent web-based information. One drawback to network structures is it is hard both to get an overview of an information network and to navigate through the network e ectively: users are easily ""lost"" in these systems [12, 22, 31].",0,,False
"A nal question within this space is how competing representations fare in presenting results for exploratory search tasks. While some past research has explored using questionnaires to determine the e cacy of di erent knowledge representations [30], or has evaluated the e cacy of hierarchies, networks, or concept maps with respect to ordered lists (e.g. [1, 9, 10, 27, 38]), we have found li le research that directly compares networks to hierarchies to understand their competing a ordances. e one exception to this is recent work by Sarrafzadeh et al. [39]; they nd that networks eliminate the need for reading documents ­ users can glean information from the networks with statistically signi cantly less time spent reading ­ and that hierarchies particularly bene t low-knowledge participants by giving them an e ective overview of the domain.",1,ad,True
146,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Given Sarrafzadeh et al.'s observation of the complementary bene ts of hierarchies and networks, one question is whether studies have examined the use of hierarchies and networks as combined ­ synchronized and simultaneous ­ representations of search results. We have found li le work that explores combined networks+hierarchies. Part of the challenge may arise from the complexity of seamlessly integrating both hierarchies and networks into a single uni ed structure. For example, hierarchies are typically best when the structure aligns well with the user's task, but, given this alignment, entities in networks may have many multiple `parents' within the structure, yielding a many-to-many relationship within the hierarchy, i.e. a three-dimensional graph.",1,ad,True
3 HIERARCHICAL KNOWLEDGE GRAPHS,1,AP,True
"In this section, we describe hierarchical knowledge graphs, an extension of knowledge graphs that include hierarchical information about the lower level graphical structures. e rationale behind our proposed approach for employing hierarchical knowledge graphs to represent search results is the complementary bene ts [39] of hierarchies [17, 47] and network structures [2, 29, 38] to support exploratory browsing of search results. More speci cally, hierarchies provide a breadth- rst exploration of the information that allows the user to iteratively reduce confusion, obtain an overview, and slowly exploit detail. ey thus provide a structured way to navigate from more general concepts to more ne grained data and are valuable when people feel a need to orient themselves. In contrast network structures allow users to glean more information from the representation (document reading time is reduced), are more engaging, yield more control over exploration at the lower level of inter-concept relationships [39], and are more similar to one's mental model [3, 11, 28, 39].",1,ad,True
"Given the complimentary bene t of networks and hierarchies, the next question is how to design a representation that can seamlessly merge these two representations. We take the approach that a knowledge graph will be an appropriate low-level representation and seek to incorporate a hierarchical view of this low-level representation of corpus content. To incorporate a hierarchical view into a knowledge graph, we need to nd answers to the following three design questions (DQs):",1,corpora,True
"(1) How do we integrate network and hierarchical views into a single, seamless data structure?",0,,False
(2) How can both the global and the local view of a knowledge graph be co-visualized?,0,,False
(3) How can transitions between views be designed to maximize visualization stability?,0,,False
"To answer these DQs, we rst focus on DQ1 and describe the design of our data structure. Next, to address DQ2 and DQ3 we describe an interface that supports interaction with the data structure. Alongside our DQs, we add one additional constraint to our design. We want to ensure that both the low-level knowledge graph and the hierarchies gleaned from that knowledge graph can be automatically generated from a targeted search performed by the user.",1,ad,True
3.1 Visualization Design and Creation,0,,False
"As noted above, given that we take the approach that a knowledge graph will constitute the lower-level visualization of our data, the",0,,False
task becomes creating a knowledge graph and creating a hierarchy that is gleaned from and corresponds directly to the underlying knowledge graph.,0,,False
"Figure 1 depicts the system architecture that supports the process of automatically generating the hierarchical knowledge graph representation. To simplify hierarchy generation, we create a 3level hierarchy for any document corpus. Beyond the base layer knowledge graph, there is an intermediate layer of central concepts gleaned from the knowledge graph. Finally, at the top-level, the documents, themselves, represent the top level of the hierarchical knowledge graphs. In Figure 1, three main steps are depicted to generate hierarchical knowledge graphs: Document Retrieval (yielding the top-level of the hierarchy), Knowledge Graph Generation (yielding the bo om level of the hierarchy), and Hierarchy-from-graph Generation (yielding an intermediate view of an individual knowledge graph, which we dub a minimap1).",0,,False
Document Retrieval,0,,False
User,0,,False
Knowledge Graph Generation Preprocessing,0,,False
Top 50 docs,0,,False
Entity Extraction,0,,False
Relation Extraction,0,,False
Labelling &,0,,False
Ranking,0,,False
"List of Tuples: <ENT1, ENT2, R, Snippet, Text_Anchor>",0,,False
Minimap Generation,0,,False
Higher Layer: Collection View,0,,False
Middle Layer: Central Entities,0,,False
Lower Layer: Flat,0,,False
Knowledge Graphs,0,,False
Figure 1: Generating Hierarchical Knowledge Graphs,0,,False
3.1.1 Document Retrieval. e Document Retrieval component aims at creating an initial document collection based on a user's query. is collection will then be used as an input for the Knowledge Graph Generation component and will represent the top view of the target hierarchy.,0,,False
"To generate a document corpus, we use the Bing Search engine to retrieve the top n documents for a query while a empting to ensure a reasonable quality of information in the retrieved documents. By default, to ensure that retrieved documents are consistent in their credibility and coverage, we specify Wikipedia as the target domain. Furthermore, because it is known that searchers typically view only a few results [19] and rarely stray past the rst page of results [4], we selected n,10 documents to generate collections.",1,Wiki,True
"e target domain from which to glean documents (e.g. a user might specify webmd for medical documents, 'gov' for public policy documents, 'bbc' for news) and the size of the initial collection can be speci ed by the user at the time of query submission. Finally,",0,,False
"1 e term minimap is drawn from the gaming literature. It represents a less detailed overview of a gaming world, allowing the user to orient themselves.",0,,False
147,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"since most exploratory search tasks require multiple queries to retrieve documents for di erent aspects of the information need, this component assigns one partition per query so the user can narrow down the retrieved collection further.",0,,False
"3.1.2 Knowledge Graph Generation. Inspired by past work [37], we create knowledge graphs for an individual document or set of documents as follows. (1) Entity taggers 2 are used to extract entities from text. (2) Sentences that contain at least two entities are selected and parsed using the Stanford Dependency Parser. For each sentence, we extract meaningful relations between the entities by",0,,False
"nding the shortest path in the corresponding parse tree. (3) Finally, labels are automatically generated for the extracted relations. e labeled relations are ranked based on relevance to the query and the informativeness of the extraction [37].",0,,False
"e outcome is a set of tuples in the form of <entity1, entity2, relation, snippet, document anchor>. ese tuples collectively correspond to a knowledge graph representation of retrieved documents where entity is usually a term or a noun phrase in text that corresponds to a concept in the domain, relation corresponds to a simpli ed sentence that is semantically complete and describes how entity1 and entity2 are connected, snippet is a short portion of text from which the corresponding entity pair and the relationship is derived, and text anchor is an HTML anchor that links the extracted tuple to the corresponding portion of the source document in the collection. For example, from a paragraph on powers and responsibilities of a president the following tuple can be extracted: <president, parliament, ""President nominates the Cabinet members to the Parliament"", snippet, [URL][anchor]>.",0,,False
ese tuples are visualized as a knowledge graph where nodes are the entities and edges are the relationships between them. is visualization constitutes the lowest layer of the hierarchy.,0,,False
"3.1.3 Minimap Generation. e nal component of this system generates a hierarchical representation of the search results by extracting a middle layer from the input Knowledge graph tuples and provides bidirectional mappings between all three layers. As noted earlier, we call this layer the minimap layer.",0,,False
"A natural result of the entity-relationship tuples extracted above is that some entities have a higher number of edges, i.e. are of higher degree. A higher edge count implies a larger number of connections to other entities in the graph; in other words, those entities with higher edge counts were more frequently linked with other entities in the document. We call these higher degree vertices central concepts and hypothesize that one alternative to hierarchical faceted structures is to consider a multi-level view of a knowledge graph around central concepts. e multilevel view focusing on central concepts simply introduces information seekers to those entities or objects that are most frequently linked to other entities within the corpus. Generating the hierarchy becomes a thresholding task to appropriately scope the intermediate level of the visualization. Algorithm 1 describes this process more formally.",0,,False
3.2 Prototype Development,0,,False
"Given our hierarchical representation (DQ1), we must support mechanisms for viewing and interacting with the visualization",0,,False
2h ps://cogcomp.cs.illinois.edu/page/so ware view/NETagger,0,,False
Algorithm 1 Extracting Central Concepts,0,,False
"Require: Nodes: array of nodes in the knowledge graph, min de ree: a pre-speci ed threshhold for the minimum de-",0,,False
"gree of node to be considered as a central concept (starting value ,"" 3), max count: an experimentally derived threshold for""",0,,False
the maximum number of Central Concepts to be included in,0,,False
"the middle layer (default value , 15).",0,,False
1: function E,0,,False
"CC(Nodes, min de ree, max count)",0,,False
2: while true do,0,,False
3:,0,,False
CentralNodes  [],0,,False
4:,0,,False
for all node in Nodes do,0,,False
5:,0,,False
if node.de ree  min de ree then,0,,False
6:,0,,False
CentralNodes.add(nod e ),1,ad,True
7:,0,,False
if CentralNodes.size()  max count then,0,,False
8:,0,,False
return CentralNodes,0,,False
9:,0,,False
min de ree++,0,,False
"(DQ2 and DQ3). In information retrieval, it is di cult to separate any visualization for representing search results from the interface that contains that visualization [17]. We iteratively designed an interface to support navigation of our hierarchical knowledge graphs via a series of pilot studies.",0,,False
"Based on established literature and pilot studies we found that knowledge graphs can become overwhelming or confusing for participants [12, 22, 31, 39]. e overwhelming nature of the full knowledge graph leads to a need to create ltered views of our graph. ese ltered views draw inspiration from the ""expandfrom-known"" paradigm in information visualization [41]. Speci cally, at the top level of the full corpus, a user selects a document, then a central concept from the minimap visualization. While preserving the entire knowledge graph, we alpha-blend all nodes in the knowledge graph except those nodes directly related to the central concept from the minimap. Recall that the central concept is simply a high-degree vertex from the knowledge graph; therefore, the central concept and all its linked nodes are shown saturated. As a result, users can identify the central concept, linked entities, and can see closely related additional entities. Together, this focused detailed view seems to e ectively support expand-from-known at the knowledge graph level.",1,ad,True
"As well, for the Hierarchical View, the biggest challenge to address was the disorientation among the participants during transitions between collection, minimap, and knowledge graph views, a common problem in interfaces that show multiple levels of abstraction. To address this disorientation (DQ3), we maintained the connection between the hierarchical view and the graph view in two ways. First, the user can move between the layers of Collection View and the Document View smoothly through a zooming functionality that changes the focus of the UI (see Figure 2). Second, the interplay between the Document View and the Detailed View is designed such that the overview of the document is present at all times, in terms of a callout on the le side of the screen, an actual minimap as in computer gaming, which allows the user to maintain a sense of where he or she is while manipulating the ne-grained nodes and edges in the Detailed View.",1,ad,True
148,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
a (Collection View),0,,False
d (minimap),0,,False
e (Detailed View),0,,False
b (Partition View),0,,False
c (Document View),0,,False
f (Context Window),0,,False
Figure 2: MultiLayer Graph Interface: (a) Collection View; (b) Partition View; (c) Document View; (d) Minimap (i.e. Global View); (e) Detailed View (Local View); (f) Snippet Window,0,,False
e iterative process culminated in the nal prototype shown in Figure 2.,0,,False
"In this interface, we see an initial overview, the Collection View that presents an overview of the underlying documents' structure in the collection (Figure 2-a). e collection view can potentially provide multiple partitions on the documents. Figure 2-b illustrates one partition of a collection. As an information seeker drills down on each document, the view is altered (Figure 2-c) such that an overview of the document is presented. e Document View provides a Global View of the corresponding document in terms of its central concepts. In this overview, the salient concepts in that article are visualized as circles of di erent sizes, where size indicates the frequency of occurrence in that article. We used force and pack layouts (as part of the D3 library3) to visualize the di erent layers of the knowledge graph representation.",0,,False
"e lowest layer of our representation is the Detailed View (Figure 2-e). is view is a knowledge graph that represents entities and relationships between them. e Detailed View, similar to Sarrafzadeh et al.'s graph interface [39], contains labeled nodes and unlabeled links between nodes. Nodes that represent entities with low frequency are hidden in the initial view, and only appear once a higher-frequency, connected node is clicked, ensuring that the graph does not become too clu ered. Once the user hovers over a node, that node and all connected nodes are highlighted, while the remainder of the graph is alpha-blended into the background. Clicking on a node can expand it by adding in its related nodes. Alternatively, clicking on a node can collapse its neighbours if they are expanded already. Nodes can also be dragged and placed at",1,ad,True
3h p://d3js.org/,0,,False
di erent parts of the canvas. is functionality can help with organizing the graph structure in a way that is more meaningful to the user and it can help with minimizing label overlap in the graph.,0,,False
"Edges can similarly be highlighted by hovering. By clicking on any edge, the user can see the relationship(s) between the two corresponding nodes (linked by this edge) in the context window located on the lower le side of the interface (Figure 2-f). For each relationship in the context region, a hyperlink allows users to view the corresponding web page.",0,,False
4 EXPERIMENTAL DESIGN,0,,False
"Given that we have designed hierarchical knowledge graphs, a related question is how hierarchical knowledge graphs compare to hierarchies and/or knowledge graphs with respect to information seeking tasks. To evaluate this question, we need a set of control interfaces (reference interfaces that can be compared to hierarchical knowledge graphs, HKGs) and a reference data set. ese can then be leveraged to design an experiment. As well, experimental design should replicate, as closely as possible, past work to ensure experimental validity.",0,,False
"In recent work, Sarrafzadeh et al. [39] developed two interfaces for exploratory search: one knowledge graph interface and one hierarchical tree interface. To preserve experimental validity, we use identical interfaces as control interfaces. We also leverage the identical data sets, ensuring that topic is eliminated as a confound. Finally, we use exactly the same experimental task, ensuring that performance numbers are representative between experiments.",1,ad,True
149,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
4.1 Control Interfaces,0,,False
"· e rst interface, a knowledge graph interface, functions as follows: As the interface starts, nodes that represent entities with low frequency are hidden in the initial view, and only appear once a higher-frequency, connected node is clicked. Users can also lter the knowledge graph by clicking on a node; when a user clicks on an edge, snippets and links associated with that edge are shown in a preview pane on the le side of the interface.",0,,False
"· e second interface utilized a hierarchy (or a tree) structure to organize headings and sub-headings of the articles, as observed in each page's table-of-contents. When the user launches the application, the user is presented with a fully expanded tree. By clicking on any node within the tree, that portion of the Wikipedia document corresponding to the node is presented in the preview area at the le of the interface.",1,ad,True
Figure 3: Control interfaces for Knowledge graph and Hierarchy. More details in Sarrafzadeh et al. [39].,1,ad,True
Figure 3 depicts these two interfaces. Contrasting these interfaces with Figure 2 shows a similar preview pane for snippets. Links within the snippets function identically across all three interfaces.,0,,False
4.2 Data Set,0,,False
"We leveraged the two data sets from Sarrafzadeh et al.'s previous study: A history data set, speci cally a corpus of Wikipedia articles describing the historical locations of the capital city of Canada; And a global politics data set, a Wikipedia corpus representing governmental structures in Iran and Russia.",1,ad,True
4.3 Search Tasks,0,,False
"We used the same two exploratory search tasks [24], a simple and a complex exploratory search task, as follows: Simple Politics: What governmental body or bodies are involved",0,,False
in the impeachment of the President of Iran and of Russia? (sample question),0,,False
"Complex Politics: Imagine you are a high school student who is going to write an essay on the Political Systems of Iran and Russia. Knowing li le about the presidents of these two countries, you wish to determine which president has more power. Find at least 3 arguments to justify your answer.",0,,False
Simple History: As a result of which act were Upper and Lower Canada formed? (sample question),1,ad,True
"Complex History: Imagine you are a high school student who is going to write an essay on the History of Canada. Knowing li le about Canadian History, you wish to know which cities have served as a capital for Canada. You would also like to understand the reasons behind moving the capital from one city to another.",1,ad,True
4.4 Study Design,0,,False
"Our study design was a 3 × 2 × 2 [interface, topic, complexity] mixed design. For Knowledge graph and hierarchy, we leverage the data set of Sarrafzadeh et al. [39], available from the researchers in anonymized form. We add additional participants for our HKGs to yield our mixed design as follows.",1,ad,True
"For HKGs, each participant performed two di erent tasks, one simple and one complex. e topic area (history or politics) differed for each of these tasks. More formally, for these participants, our design was a 2 × 2 full factorial mixed design, with topic and complexity as within subjects factors and complexity to topic assignment as a between subject factor. We counter-balanced the order in which the tasks were assigned to the participants.",0,,False
"Alongside the HKG participants, leveraging data from Sarrafazdeh et al. [39] adds two additional levels of Interface (hierarchical tree or knowledge graph) as a between subject factor. Combining the data sets yield the 3 × 2 × 2 mixed design [interface, topic, complexity] with interface as a between subjects factor, and topic and complexity as within subjects factors.",1,ad,True
4.5 Participants,0,,False
"In total we analyze data from forty seven participants. Twenty six participants, thirteen female, used hierarchies and knowledge graphs, the control interfaces. An additional twenty-one participants (4 female) used HKGs, the experimental condition, as a between subjects factor. All participants use the Internet on a regular basis to search for information. Participants were aged between 18 and 45 years old (62% were between 20 and 29 years old). Participants received a $15 incentive for their participation.",1,ad,True
4.6 Procedure,0,,False
"A er introducing the study, participants were presented with an experimental interface (populated with an unrelated data set), and were given time to familiarize themselves with the interface and data structure. Once participants had developed some comfort with the features of the interface ( 3 minutes), participants completed a questionnaire assessing their familiarity with the topic used for the",1,ad,True
"rst task. ey were then given the description of their task (see above), and were asked to complete the task using the interface (15 minutes per task). Participants completed a post-task questionnaire that evaluated the experience; we used questionnaires provided by",0,,False
150,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"TREC-9 Interactive Searching track 4 modi ed to t our experiment. e same process was repeated for the second task. At the end of the second task, a semi-structured interview ex-",1,TREC,True
"plored participants' experience using the interface. Interviews explored the conceptual usability of the visualization, the technical usability of the application and the e cacy of the interface for di erent types of search tasks. Feedback on competing interfaces was also collected from participants.",0,,False
4.7 Data Collection,0,,False
"Alongside a mixed design of within subject and between subject factors, we perform a mixed methods analysis of both quantitative and qualitative data [13]. Data was captured as follows:",0,,False
"(a) e interface was instrumented with a logger which monitored movement on the computer screen and participants' interactions with the system. Interactions collected included node or edge clicks, snippets read, articles viewed, and time spent reading the articles. In HKGs, the transition between the layers and switches between the MiniMap and the knowledge graph were captured.",1,ad,True
"(b) Two assessors evaluated the quality of answers provided by the participants for each of the search tasks independently. Simple queries were rated as either correct or incorrect. Complex questions were rated on a scale. Scores for all queries were normalized to re ect a value in the range [0, 1]. Inter-assessor reliability was evaluated using Pearson coe cient and an overall value of 0.97 for simple queries and 0.94 for complex queries was found.",0,,False
"(c) We captured eld notes during participant interactions, audio recorded all sessions, transcribed nal interviews, and collected questionnaire data. is data was analyzed collectively using open coding to extract low-level themes and axial coding to identify thematic connections between elements. Coding was performed incrementally as each participant's data was collected, and saturation was found a er coding qualitative data from eld notes and transcripts for 15 of our 21 participants.",0,,False
4.8 Hypotheses and Research estions,0,,False
antitative data allows us to test the following hypotheses: · Hierarchical knowledge graphs result in fewer document views and less time spent reading documents than do hierarchical trees. · Hierarchical knowledge graphs exhibit statistically similar behaviors to Knowledge Graphs.,1,ad,True
"Alongside hypothesis testing, our log data provides insight into whether hierarchies are used in hierarchical knowledge graphs and on whether task complexity a ects the use of hierarchies. As well, to triangulate quantitative data, we leverage our qualitative data to compare and contrast the nature of the hierarchies between the tree interface and the hierarchical knowledge graphs and to understand whether the hierarchies provide similar a ordances.",0,,False
5 RESULTS 5.1 antitative Analysis,0,,False
Scoring of participant responses by independent evaluators and log le analysis produced the quantitative measures in Table 1 for,0,,False
4 www-nlpir.nist.gov/projects/t9i/qforms.html,0,,False
"Hierarchical Knowledge Graphs (H. Graphs), Hierarchical Trees (H. Trees), and Knowledge Graphs (K. Graphs). Rows represent measures for Marks (MK), Nodes clicked (NK), Edges Clicked (EC), Document Views (V) and Document View Time (VT). We break each measurement out by two query levels, Simple and Complex, as described previously.",0,,False
H. Graphs,0,,False
H. Trees,0,,False
K. Graphs,0,,False
MK 0.43 (0.21),0,,False
0.32 (0.20),0,,False
0.37 (0.14),0,,False
Simple,0,,False
NC 11.4 (8.6),0,,False
19.0 (10.04),0,,False
11.38 (9.4),0,,False
EC 18.3 (8.9),0,,False
,0,,False
27.15 (12.9),0,,False
V 2.38 (1.61),0,,False
6.08 (2.49),0,,False
2.38 (3.00),0,,False
VT 145.6 (153.7) 1430.9 (2302.8) 211.6 (228.0),0,,False
MK 0.62 (0.18),0,,False
0.57 (0.28),0,,False
0.58 (0.16),0,,False
Complex,0,,False
NC 13.38 (9.2),0,,False
20.09 (17.7) 26.23 (19.12),0,,False
EC 23.09 (12.7),0,,False
,0,,False
41.07 (19.4),0,,False
V 2.15 (2.13),0,,False
4.38 (2.24),0,,False
4.38 (2.24),0,,False
VT 103.4 (97.6) 985.38 (1848.3) 78.76 (131.5),0,,False
"Table 1: Hierarchical (H.) Graphs vs. Hierarchical Trees and Knowledge (K.) Graphs: Mean (Standard Deviation) values for marks (MK - average independent evaluator scores), clicks on nodes (NC) and edges (EC), document views (V), and document view time (VT). Bolded dependent variables exhibited signi cant di erences in post-hoc testing.",1,hoc,True
"5.1.1 Hypotheses Testing. Multivariate analysis of variance with respect to interface (tree versus graph versus hierarchical graph), topic (history versus politics), and task (simple versus complex) for Marks (MK), Views (V), and View Time (VT) shows a statistically signi cant e ect of interface (F6,172 ,"" 7.126, p < 0.001, 2 "","" 0.2) and task (F3,86 "","" 12.22, p < 0.001, 2 "","" 0.3) on dependent variables. Post-hoc factor analysis using Tukey correction indicates that the tree interface exhibited statistically signi cantly higher numbers of document views than both hierarchical graphs and knowledge graphs. As well, the tree exhibited statistically longer reading times than hierarchical graphs (p < 0.05), but not than knowledge graphs (p "", 0.064) in our analysis. Hierarchical graphs and knowledge graphs did not di er signi cantly in their e ects on any dependent variables. Task signi cantly impacted the marks but no other variables.",1,hoc,True
"Clicks are not directly comparable between H. Trees, H.Graphs, and K.Graphs, as edges are not clickable in hierarchies (NA value in Table 1). Performing pairwise comparison between H.Graphs and K.Graphs, our analysis showed no statistically signi cant e ect on dependent variables (F3,30 ,"" 0.752, p > 0.5, 2 "","" 0.70), including node click and edge click behavior.""",0,,False
"Given the above analyses, we reject both null hypotheses and conclude that our hypotheses are supported by our data set. Hierarchical Knowledge Graphs preserve the advantages of Knowledge graphs over hierarchical trees in both reading time and in document views. Focusing speci cally on our hierarchical graph, we nd that our hierarchical graph has statistically lower document views (61% fewer document views, on average) and time reading (90% less time reading documents) than does hierarchical trees and that its",1,ad,True
151,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"behavior is statistically indistinguishable from the prior observations of knowledge graph interfaces. Furthermore, the e ect size measures, 2, are signi cantly above the threshold (0.14) typically considered to be a large e ect, lending support to these di erences being su ciently large to be meaningful. In summary, our quantitative results support our hypothesis that our hierarchical knowledge graphs fully preserve the quantitative advantages identi ed by Sarrafzadeh et al [39] for knowledge graphs over hierarchies.",1,ad,True
"5.1.2 Additional antitative Analysis. Given the statistically indistinguishable nature of HKGs and Knowledge Graphs, one question is if (and whether) intermediate hierarchical representations are used. It is possible that Hierarchical Knowledge Graphs are indistinguisable from Knowledge Graphs because users ignore the hierarchy and simply leverage the knowledge graph.",0,,False
GlobalView MiniMap DetailedView,0,,False
Simple Task,0,,False
27.03%,0,,False
14.61%,0,,False
58.0%,0,,False
Complex Task 23.83%,0,,False
17.24%,0,,False
58.90%,0,,False
"Table 2: Percentage of Time spent on each of Global View, Minimap and Detailed View",0,,False
". To speci cally explore this question, we looked at how much time users spent on each of the provided views in our HKG interface. Overall, our data indicated that participants took advantage of all three layers relatively similarly across both Simple and Complex tasks. Further, while the the time spent on detailed view dominates other views (58% for the simple task and 59% for the complex task), over 40% of time was spent on additional views in the hierarchy (Table 2). Looking speci cally at how participants spent their time in di erent layers of the hierarchy (i.e. utilizing di erent views of the data) for di erent tasks we see that the time spent at the detailed view is similar for both levels of complexity. On the other hand, participants seem to spend less time in MiniMap than Global for the simple task (Pairwise t-tests with Tukey correction yields statistical signi cance, p < 0.01). For Complex task, however, time in Global versus mid-level are not statistically di erent (p > 0.1). Essentially, in the complex task, sensemaking is split between global and minimap views of the hierarchy more equitably, i.e., the minimap is particularly useful during our complex tasks.",1,ad,True
Figure 4: Heatmap visualizing the patterns of users navigating views in HKG for intervals of 1% of task length.,0,,False
"We also explored usage pa erns of views. Figure 4 is a heatmap that visualizes use of di erent views for intervals of 1% of task length. Early in the task, we see frequent use of the global view. While di cult to see, MiniMap usage peaks just a er the halfway point in the task, but there is no strong concentration of use. e hierarchy, and particularly the MiniMap visualization, seems to be used throughout the task.",0,,False
5.2 alitative Analysis,0,,False
e next question we explore involves participant perspectives on hierarchical knowledge graphs as a representation of search results. We were particularly interested in the overviews knowledge graphs provide for the information space and their contrast with Table-ofcontent-based hierarchies.,0,,False
"To address these questions, we performed open-coding of observations, transcripts, and questionnaire data. We coded incrementally, and saturation occurred a er een participants were coded. We coded all participants for completeness. Once open coding was complete, axial coding and thematic analysis was performed collaboratively by the researchers. We present three themes arising from our qualitative data analysis: Supporting Exploratory Search Tasks, Imposing a Structure versus Open Exploration and the Self-Orienting nature of HKGs.",1,ad,True
"5.2.1 Supporting Exploratory Search Tasks. As noted in our study design, we incorporate two exploratory information seeking tasks with di erent levels of complexity. In post-experiment interviews the participants were able to compare how di erent task complexities are supported by the assigned interface.",1,corpora,True
"e hierarchical graph representation was found to provide more support for the Complex Task (i.e., more open ended and exploratory tasks such as essay writing or learning) versus Simple tasks (such as question answering and speci c knowledge nding).",0,,False
is observation seems to be true for any multi-level structure which provides an overview and allows a gradual immersion into details: Finding a speci c piece of information to satisfy a simple query is best done using a traditional search engine.,1,ad,True
"Looking speci cally at HKGs and complex tasks, the overview allowed participants to identify the central concepts of a domain at a glance and the size of the circles indicates their prominence in the corresponding article. As many participants noted, `relevance' or `prominence' of a concept with respect to the main topic or the domain they are exploring is an important asset in Complex Search tasks. is qualitative observation may explain the more equitable use of the MiniMap representation for complex search tasks noted in our quantitative analysis. Complex tasks required synthesizing, rationalizing, and comparing, which seem to require more awareness of the entire data set.",0,,False
"is identi cation of central concepts was also linked to a perception of value of the MiniMap as a starting or entry point into the topic of the document being examined. Several participants articulated a belief that the overview provided by Central Concepts helped with ""going from knowing nothing to having a plan"", ""learning terminology"", ""relevance, importance, or prominence"", and ""objectively learning about a domain"". In particular, the objective nature of central concepts was cited by many participants as key to their utility.",0,,False
"As White and Roth [43] point out, exploratory search is motivated by complex information problems, poor understanding of terminology and information space structure, and o en a `desire to learn'. Vakkari [40] also argues ""more support is needed in the initial stages of a task"", when users have an unstructured mental model. Inspired by Kim [21], Sarrafzadeh et al. [39] found that hierarchical trees provide this bene t in unfamiliar domains. A strength of our design of hierarchical knowledge graphs is that it",1,ad,True
152,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
enables the user to engage in two alternative navigation paradigms. Users can exploit overview layers to explore the collection at a higher level followed by targeted immersion in the detailed view.,1,ad,True
"5.2.2 Imposing a Structure versus Open Exploration. While most participants were unanimous that the hierarchical representation imposes a [subjective] [rigid] structure onto the information space, their a itude towards this phenomenon varied. e level of domain knowledge and the complexity of the search tasks were found to be the major factors a ecting their a itude.",0,,False
"When the searcher is dealing with a domain where he has limited knowledge, he is more open to accepting the structure that the representation imposes. Both hierarchical trees and hierarchical knowledge graphs incorporate imposed structures. Participants articulated a variety of advantages to structures: it was ""easier to follow"", ""contained important aspects"" that ""simpli ed focus"", and guided participants in ""where to go"" or ""what steps to follow"". With respect to hierarchical trees, some participants simply ""trusted"" the designer of the hierarchy (e.g. the author of an article) to be ""logical"" or ""rational"" in the way he broke down things. is was particularly true for participants with limited knowledge of a topic domain and replicates ndings by Sarrafzadeh et al. [39] and Amadieu et al. [1] that low knowledge learners bene ted from hierarchical structures in free recall performance and exhibited reduced disorientation.",1,corpora,True
"In the case of higher domain knowledge, our participants were split in their preferences and a itudes. Some still trusted the logic behind the layout of a hierarchical trees and the fact that their knowledge of the domain can guide them to nd what they want using this hierarchy. ey trusted the designer to place items in close proximity to where the item should be. Other participants strongly opposed the rigid structure of a hierarchy, feeling it was ""not the way I think"", ""based on the mindset of the author"", or ""did not match the domain structure"".",0,,False
One interesting perspective of the multi-layer graph representation which presents central concepts of a domain as an overview for each document is that it re ects the knowledge graph concepts.,0,,False
"is re ection made it, for many participants, more exible and exploratory, a window into the knowledge graph. Many participants commented on this phenomenon, noting it was ""guiding but not imposting"", ""more open"", ""sparked interest"" in the lower level structure, or was ""visually appealing"" and ""fun"".",1,ad,True
5.2.3 Self-Orienting or Relative Positioning. One main advantage of a the Hierarchical Tree visualization in Sarrafzadeh et al. [39] was the explicit connections between nodes (categories or headings) in the representation. ese edges help in two ways:,1,ad,True
"(1) At a glance, you can tell why a concept appeared in this overview, or in this domain. To whit, the hierarchical structure exists the way it does because of a human author's decision.",0,,False
(2) e Path from the root to each of these nodes in the Tree Layout can provide useful information on where a concept is positioned relative to the topic.,0,,False
"Sarrafzadeh et al. [39] note that participants may perceive a domain to have a derivative/hierarchical structure or a multi-faceted structure. If salient relationships are viewed as derivative or hierarchical (e.g. `is-a' relationships), then a tree can best capture this",1,ad,True
"view of data, whereas if salient relationships are more heterogeneous and resist structure as a hierarchy, that disadvantages the hierarchies.",1,ad,True
"is is not the case in our MiniMap, where the connection between each of these main concepts and the main topic is unknown at rst glance. Central concepts are simply extracted based on their high connectivity with other concepts within a speci c document within a corpus. However, it is also true that it would be quite surprising if highly linked concepts were not, somehow, important components of any individual document. e more pervasively they link, the more they interconnect with other concepts, the more important it is to understand them and their relationship. In this way, HKGs become self-orienting for out participants.",0,,False
6 LIMITATIONS,0,,False
"Any study has limitations. Because we leverage the research methodology and data sets of Sarrafzadeh et al. [39], we inherit the limitations of that study, including topic and implementation issues which may bias the study. Despite this, there is also a strength in replication: if interfaces are redesigned, data sets di er, and tasks are unique it becomes di cult to ensure a lack of confound in experimental design. We address this by preserving, to the limit possible, all aspects of a similar study within this space contrasting data structures.",1,ad,True
"Our mixed design of within and between subject factors is a particular strength to our study design. Because topic (history/politics) and task complexity are within-subject factors, they are controlled across participants. Because we are most interested in interface and it is a between subject factor, to observe statistical signi cance we need good separation of dependent variables between the two data sets, reducing the likelihood of a type-one error in our analysis.",0,,False
7 CONCLUSION,0,,False
e primary goal of our research was to explore whether we could combine bene ts from both knowledge graphs and hierarchies into one data structure for visualizing search results. We note that our hierarchical graphs signi cantly reduce documents read and reading time as compared to hierarchical trees and perform on par with knowledge graphs. We also provide evidence that the hierarchy is used by participants via analysis of interaction logs.,1,ad,True
"alitative data from our participants does indicate that hierarchies grounded in tables-of-contents are more familiar, easier to follow, and more focused. is in turn helps users orient themselves in the data. e ve ed nature of hierarchical tables-of-contents was also perceived to be an asset absent from our hierarchical knowledge graphs. e hierarchies in our knowledge graph were viewed slightly di erently, as noted above, with a more quantitative perspective giving them a certain cachet with respect to the unbiased nature of topic selection.",0,,False
"A nal issue to consider is whether any hierarchy might provide bene ts. While it may, one advantage of the hierarchy in our HKGs is its tight connection to the entities contained in a knowledge graph and the ease of automatically extracting the hierarchy through thresholding. Another advantage is exibility: while we currently leverage only three levels ­ corpus, central concept, and knowledge graph ­ it is easy to generalize the hierarchy to an arbitrary number",1,ad,True
153,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
of thresholds depending on the complexity of the domain. We,0,,False
"do not generalize the hierarchy in this paper because, for a rst",0,,False
"experimental validation, there are a limited number of factors that",0,,False
"can be assessed. However, future work can address more detailed",1,ad,True
"inquiries into scalability to larger corpora, scalability to multi-level",1,corpora,True
"hierarchies, and contrasts with other hierarchies such as automatic",0,,False
clusters or user-speci ed facets.,0,,False
"In summary, we nd that our hierarchical knowledge graphs",0,,False
preserve many of the previously observed advantages of traditional,1,ad,True
"knowledge graphs, i.e. fewer document views and reduced reading",1,ad,True
"time. Alongside this, hierarchical knowledge graphs introduce an",0,,False
e ective hierarchical representation into knowledge graphs.,0,,False
ACKNOWLEDGEMENTS,0,,False
Funding for this research was provided by the Natural Science,0,,False
and Engineering Research Council of Canada (NSERC). Authors,1,ad,True
also thank Saeed Nejati for his contributions in implementing the,0,,False
interface.,0,,False
REFERENCES,0,,False
"[1] Franck Amadieu, Andre´ Tricot, and Claude e Marine´. 2010. Interaction between prior knowledge and concept-map structure on hypertext comprehension, coherence of reading orders and disorientation. Interacting with computers 22, 2 (2010), 88­97.",1,ad,True
"[2] Anne Aula and Daniel M Russell. 2008. Complex and exploratory web search. In Information Seeking Support Systems Workshop (ISSS 2008), Chapel Hill, NC, USA.",0,,False
"[3] David Paul Ausubel, Joseph Donald Novak, Helen Hanesian, and others. 1968. Educational psychology: A cognitive view. (1968).",0,,False
"[4] Steven M Beitzel, Eric C Jensen, Abdur Chowdhury, David Grossman, and Ophir Frieder. 2004. Hourly analysis of a very large topically categorized web query log. In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 321­328.",0,,False
"[5] Nicholas J Belkin. 1980. Anomalous states of knowledge as a basis for informationretrieval. Canadian Journal of Information Science-Revue Canadienne Des Sciences De L Information 5, MAY (1980), 133­143.",1,ad,True
"[6] Andrei Broder. 2002. A taxonomy of web search. In ACM Sigir forum, Vol. 36. ACM, 3­10.",0,,False
"[7] Katriina Bystr``Om. 2002. Information and information sources in tasks of varying complexity. Journal of the American Society for information Science and Technology 53, 7 (2002), 581­591.",0,,False
"[8] Donald J Campbell. 1988. Task complexity: A review and analysis. Academy of management review 13, 1 (1988), 40­52.",1,ad,True
"[9] Robert Capra, Gary Marchionini, Jung Sun Oh, Fred Stutzman, and Yan Zhang. 2007. E ects of structure and interaction style on distinct search tasks. In Proceedings of the 7th ACM/IEEE-CS joint conference on Digital libraries. ACM, 442­451.",0,,False
"[10] Mary Carnot, Paul Feltovich, Robert Ho man, Joan Feltovich, and Joseph Novak. 2003. A summary of literature pertaining to the use of concept mapping techniques and technologies for education and performance support. (2003).",0,,False
"[11] Hsinchun Chen, Andrea L Houston, Robin R Sewell, and Bruce R Schatz. 1998. Internet browsing and searching: User evaluation of category map and concept space techniques. Journal of the American Society for Information Science, Special Issue on AI Techniques for Emerging Information Systems Applications (1998).",0,,False
"[12] Andy Cockburn and Steve Jones. 1996. Which way now? Analysing and easing inadequacies in WWW navigation. International Journal of Human-Computer Studies 45, 1 (1996), 105­129.",1,ad,True
"[13] John W Creswell. 2013. Research design: alitative, quantitative, and mixed methods approaches. Sage publications.",0,,False
"[14] Brenda Dervin. 1998. Sense-making theory and practice: an overview of user interests in knowledge seeking and use. Journal of knowledge management 2, 2 (1998), 36­46.",0,,False
[15] Marti Hearst. 2009. Search user interfaces. Cambridge University Press. [16] Marti A Hearst. 1999. e use of categories and clusters for organizing retrieval,0,,False
"results. In Natural language information retrieval. Springer, 333­374. [17] Marti A Hearst. 2006. Clustering versus faceted categories for information",0,,False
"exploration. CACM 49, 4 (2006), 59­61. [18] P James. 1991. Knowledge graphs. Order 501 (1991), 6439. [19] Bernard J Jansen and Amanda Spink. 2003. An Analysis of Web Documents Re-",0,,False
"trieved and Viewed.. In International Conference on Internet Computing. Citeseer, 65­69.",0,,False
"[20] Sussane Jul and George W Furnas. 1997. Navigation in electronic worlds: a CHI 97 workshop. SIGCHI bulletin 29 (1997), 44­49.",0,,False
"[21] Kyung-Sun Kim. 1999. Searching the Web: E ects of Problem Solving Style on Information-Seeking Behavior. In World Conference on Educational Multimedia, Hypermedia and Telecommunications, Vol. 1999. 1541­1542.",0,,False
"[22] Anita Komlodi, Gary Marchionini, and Dagobert Soergel. 2007. Search history",0,,False
"support for nding and using information: User interface design recommendations from a user study. Information processing & management 43, 1 (2007), 10­29.",0,,False
"[23] Bill Kules and Robert Capra. 2008. Creating exploratory tasks for a faceted search interface. Proc. of HCIR 2008 (2008), 18­21.",0,,False
"[24] Gary Marchionini. 2006. Exploratory search: from nding to understanding. CACM 49, 4 (2006), 41­46.",0,,False
"[25] Gary Marchionini and Ben Shneiderman. 1988. Finding facts vs. browsing knowledge in hypertext systems. Computer 21, 1 (1988), 70­80.",0,,False
"[26] Gary Marchionini and Ben Shneiderman. 1993. 3.1 Finding facts vs. browsing knowledge in hypertext systems. Sparks of innovation in human-computer interaction (1993), 103.",0,,False
"[27] Kent L Norman and John P Chin. 1988. e e ect of tree structure on search in a hierarchical menu selection system. Behaviour & Information Technology 7, 1 (1988), 51­65.",0,,False
"[28] Joseph D Novak. 1990. Concept mapping: A useful tool for science education. Journal of research in science teaching 27, 10 (1990), 937­949.",0,,False
[29] Joseph D Novak and Alberto J Can~ as. 2008. e theory underlying concept maps and how to construct and use them. FIHM Fl 284 (2008).,0,,False
"[30] Laura R Novick and Sean M Hurley. 2001. To matrix, network, or hierarchy: at is the question. Cognitive Psychology 42, 2 (2001), 158­216.",0,,False
"[31] Christopher Olston and Ed H Chi. 2003. ScentTrails: Integrating browsing and searching on the Web. ACM Transactions on Computer-Human Interaction (TOCHI) 10, 3 (2003), 177­197.",0,,False
"[32] Peter Pirolli, Stuart K Card, and Mija M Van Der Wege. 2000. e e ect of infor-",0,,False
"mation scent on searching information: visualizations of large tree structures. In Proceedings of the working conference on Advanced visual interfaces. ACM, 161­172.",0,,False
"[33] Wanda Pra , Marti A Hearst, and Lawrence M Fagan. 1999. A Knowledge-Based Approach to Organizing Retrieved Documents.. In AAAI/IAAI. 80­85.",0,,False
"[34] Kerry Rodden, Wojciech Basalaj, David Sinclair, and Kenneth Wood. 2001. Does organisation by similarity assist image browsing?. In Proceedings of the SIGCHI conference on Human factors in computing systems. ACM, 190­197.",0,,False
"[35] Daniel E Rose and Danny Levinson. 2004. Understanding user goals in web search. In Proceedings of the 13th international conference on World Wide Web. ACM, 13­19.",0,,False
"[36] Daniel Russell, Mark Ste k, Peter Pirolli, and Stuart Card. 1993. e cost structure of sensemaking. In Proc. of INTERACT'93 and CHI'93. ACM, 269­276.",1,TERA,True
"[37] Bahareh Sarrafzadeh, Rakesh Gu ikonda, Kaheer Suleman, Jack omas, and Olga Vechtomova. March 2013. Automatic discovery of related concepts. Technical Report.",1,ad,True
"[38] Bahareh Sarrafzadeh, Olga Vechtomova, and Vlado Jokic. 2014. Exploring knowledge graphs for exploratory search. In Proc. of IIiX. ACM, 135­144.",1,ad,True
"[39] Bahareh Sarrafzadeh, Alexandra Vtyurina, Edward Lank, and Olga Vechtomova.",1,ad,True
"2016. Knowledge Graphs versus Hierarchies: An Analysis of User Behaviours and Perspectives in Information Seeking. In Proc. of the 2016 ACM on Conference on Human Information Interaction and Retrieval. ACM, 91­100. [40] Per i Vakkari. 2000. Relevance and contributing information types of searched documents in task performance. In Proc. of SIGIR. ACM, 2­9. [41] Frank Van Ham and Adam Perer. 2009. ""Search, Show Context, Expand on Demand"": Supporting Large Graph Exploration with Degree-of-Interest. Visualization and Computer Graphics, IEEE Transactions on 15, 6 (2009), 953­960. [42] Charles K West, James A Farmer, and Phillip M Wol . 1991. Instructional design: Implications from cognitive science. Prentice Hall Englewood Cli s, NJ. [43] Ryen W White and Resa A Roth. 2009. Exploratory search: Beyond the queryresponse paradigm. Synthesis Lectures on Information Concepts, Retrieval, and Services 1, 1 (2009), 1­98. [44] Barbara M Wildemuth and Luanne Freund. 2012. Assigning search tasks designed to elicit exploratory search behaviors. In Proc. of HCIR 2012. ACM, 4. [45] Max L Wilson and others. 2009. e importance of conveying inter-facet rela-",1,ad,True
tionships for making sense of unfamiliar domains. (2009).,0,,False
"[46] Max L Wilson, Bill Kules, Ben Shneiderman, and others. 2010. From keyword search to exploration: Designing future search interfaces for the web. Foundations and Trends in Web Science 2, 1 (2010), 1­97.",0,,False
"[47] Yi-fang Brook Wu, Latha Shankar, and Xin Chen. 2003. Finding more useful information faster from web search results. In Proceedings of the twel h international conference on Information and knowledge management. ACM, 568­571.",0,,False
"[48] Ka-Ping Yee, Kirsten Swearingen, Kevin Li, and Marti Hearst. 2003. Faceted metadata for image search and browsing. In Proc. of CHI. ACM, 401­408.",1,ad,True
154,0,,False
,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Searching on the Go: The E ects of Fragmented A ention on Mobile Web Search Tasks,0,,False
Morgan Harvey,0,,False
"CIS Department, Northumbria University Newcastle upon Tyne, United Kingdom",0,,False
morgan.harvey@northumbria.ac.uk,0,,False
Ma hew Pointon,0,,False
"CIS Department, Northumbria University Newcastle upon Tyne, United Kingdom",0,,False
m.pointon@northumbria.ac.uk,0,,False
ABSTRACT,0,,False
"Smart phones and tablets are rapidly becoming our main method of accessing information and are frequently used to perform on-thego search tasks. Mobile devices are commonly used in situations where a ention must be divided, such as when walking down a street. Research suggests that this increases cognitive load and, therefore, may have an impact on performance. In this work we conducted a laboratory experiment with both device types in which we simulated everyday, common mobile situations that may cause fragmented a ention, impact search performance and a ect user perception.",1,ad,True
"Our results showed that the fragmented a ention induced by the simulated conditions signi cantly a ected both participants' objective and perceived search performance, as well as how hurried they felt and how engaged they were in the tasks. Furthermore, the type of device used also impacted how users felt about the search tasks, how well they performed and the amount of time they spent engaged in the tasks. ese novel insights provide useful information to inform the design of future interfaces for mobile search and give us a greater understanding of how context and device size a ect search behaviour and user experience.",0,,False
CCS CONCEPTS,0,,False
·Information systems Information retrieval; Users and interactive retrieval; ·Human-centered computing Empirical studies in ubiquitous and mobile computing; User studies;,0,,False
KEYWORDS,0,,False
mobile search; fragmented a ention; search experience; cognition; user study; experimentation,0,,False
1 INTRODUCTION,1,DUC,True
"Recent years have seen rapid growth in the sale and use of various mobile computing devices, giving people the ability to access the Internet away from the con nes of a desk, and in many di erent environmental contexts. Over two-thirds of Americans own a smart phone and almost half own a, somewhat larger, tablet device. At the same time, the sales of desktop and laptop computers have begun",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080770",1,ad,True
"to stagnate and even to fall [1]. Almost all smart phone owners (97%) use their devices to access the Internet, many of whom search for information, and to complete fairly complex retrieval tasks: 62% have used them to look up information about a health condition; 57% to do a search for real estate and 40% to look up government services [35].",0,,False
"People use mobile devices to search the web in a variety of di erent contexts - on public transport, while walking from place to place [17, 23, 33] or in social contexts, where the presence of others can cause distraction [8]. Interaction with such devices is achieved via touch screens upon which small ""so bu ons"" are drawn for users to select items and input text. Although these bu ons may be easy to accurately press in an ideal environment, e.g. when seated, such small and non-tactile targets can be signi cantly more di cult to interact with in other situations [4]. While the ability to perform such tasks ""on the go"" can be of real bene t, hazards and other changes in the surroundings do necessitate the user's brain switching a ention between the ambient environment and the device [11].",0,,False
"ese distractions can preoccupy users [30], reducing their effectiveness in interacting with the UI [4, 23] and may even a ect user perceptions of the environment and tasks [9]. e result is a larger number of misspelled queries and an a empt by users to shorten queries when searching [32, 33]. In fact, concentration on a mobile task while walking even has an e ect on how we walk; to compensate the brain subtly (and subconsciously) alters stance and gait [34]. As such, using a mobile device whilst walking requires both cognitive and motor abilities and so users must divide their a ention between the two tasks [21], meaning either an increase in cognitive load, a decrease in pace, a decrease in task performance or a combination of these [22]. e level of di culty experienced may additionally be in uenced by the device size and type and the amount of encumbrance it itself causes [5, 12].",1,ad,True
"Despite the popularity of mobile devices, their ubiquity in everyday life and the ability they give us to engage in complex search tasks, li le is known about how using them on the go impacts upon search behaviour and search performance and whether or not device type and size is an important factor. With this in mind, we investigate whether the small behaviour changes identi ed in the literature for simple tasks (such as tapping on a highlighted bu on) result in signi cant behavioural changes, di erent perceptions of the task, and di erent task performance for relatively complex web search problems on both smart phone and tablet devices. Does the change in context impact on user behaviour, is this something that users themselves are aware of and does the type of device used ma er? To ensure repeatability, we conducted our study in a lab",0,,False
155,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"using simulated contexts - walking on a treadmill, navigating an obstacle course and si ing still at a desk.",1,ad,True
"Our main research questions, therefore, are:",0,,False
· Do common mobile situations that cause fragmented attention have an impact on: ­ RQ1 Users' perceptions of the task and their own performance? ­ RQ2 Objective measures of users' task performance and behaviour?,0,,False
· RQ3 What impact does the device type have on user performance and perception thereof?,0,,False
"e remainder of the paper is structured as follows: In section 2 we consider related work on the topics of mobile device use, fragmented a ention and user distraction; section 3 describes the user studies we performed to investigate searching on the go; sections 4,5 and 6 describe the results of the user studies in detail; section 7 discusses how the results relate to the existing literature and suggests reasons and intuition behind them; and section 8 concludes the paper with suggestions for potential future work.",0,,False
2 RELATED WORK,0,,False
"Improvements in mobile technologies in recent years have led to a dramatic change in how and when people access and use information, and has ""a profound impact on how users address their daily information needs"" [7]. Research shows that as the power of these devices - as well as the amount of screen space they afford - increases, the complexity of tasks people use them for also increases, with mobile search sessions becoming longer and less homogeneous [19]. Many people now use their smart devices in di erent contexts to nd information, keep up to date with news or to alleviate boredom [35] and frequently use them whilst walking or on public transport. is relatively novel situation of interacting with a computing device when non-stationary can be distracting as a ention must be shared (or ""fragmented"") between operating the device and maintaining motility, typically necessitating a change in posture, stance and gait [34].",1,ad,True
"A large body of work has investigated user contexts and how fragmented a ention a ects user input on mobile devices. Early work designed and evaluated forms of human computer interaction in xed, non-fragmented contexts of use, in a single domain such as a lab [16]. As mobile research evolved, studies began to investigate situations in which a ention is diverted from the interface. Oulasvirta et al. found that when following a pre-de ned, but otherwise uncontrolled, route through a city users experienced signi cant impairment when compared with a ""non-social laboratory condition"" [30]. In a more controlled set of experiments, Lin et al. [23] demonstrated that error rates of stylus input significantly increased as the amount of distraction, and thus degree of a ention fragmentation, increased. Similar e ects were later demonstrated for touch-based input, with error rates increasing in line with walking speed [28].",0,,False
"Early investigations of reading comprehension and word search when walking [3] showed that contextual variations can have large e ects on user behaviour, impairs performance and increases task workload. Mizobuchi et al. looked into mobile text entry and found additional workload e ects when walking and identi ed walking",1,ad,True
"speed as a secondary measure of mental workload [24]. ey concluded that texting whilst walking results in either a reduction in input speed (but not accuracy) or a reduction in walking pace. Large-scale analysis of mobile search logs [18] has shown that the increase in time required for mobile searches deters some types of search behaviour, such as exploratory search, and causes search sessions to be considerably shorter than in desktop search. ese lines of investigation concluded that times increased signi cantly when walking compared to a si ing condition, search behaviour altered whilst mobile and walking speed when texting reduces by a xed amount independent of the level of input di culty, which varied between participants. ese types of investigative conditions create situational impairments which fragment a users' a ention, exerting a range of e ects on performance and creating compelling opportunities for research [20].",1,ad,True
"Interaction with such devices is commonly achieved via touch screens upon which relatively small ""so bu ons"" are drawn for users to select items and input text. e examination of so bu ons, hardware bu ons, and surface gestures under conditions of medium and high distraction found that marking menus (i.e. directional gestures) activated along a smartphone's bevel provided the fastest response time [4, 26]. While these bu ons may be easy to accurately press in an ideal environment, such as when seated, such small and non-tactile targets may be much more di cult to interact with in other distracting situations [4]. Other investigations assessed the e ects of walking on performance with so bu ons, a empting to quantify the negative e ects on use due to walking and exploring design changes that may improve a user's experience with a mobile device [20].",0,,False
"Screen real-estate on a mobile device also creates interaction di culties as a user moves, combined with increasing complexity of mobile task, resulting in considerable obstacles [5, 6, 13]. e limited input modalities a orded by mobile devices have a negative e ect on usability [13], a problem compounded by screen size and the device's reduced ability to present information and navigational cues [5, 6]. Small screens can easily become clu ered with information and widgets (bu ons, menus, windows, etc.) and this presents a di cult challenge for interface designers [5]. Use of larger devices, such as tablets, which have correspondingly larger screens, may mitigate some of these issues and result in notably di erent modalities of use [25].",0,,False
"Research shows that smart phones and tablets are o en used for di erent tasks [25, 31] and an analysis of query logs [36] suggests that querying behaviour di ers between tablet and smart phone users. Furthermore, there may be a negative correlation between screen size and perceived task di culty and experienced workload [12], although it has not been investigated when comparing smart phones and tablets and it is unknown what e ect situational context has, if any. In general, li le is known about the impact di erences between the devices has on user behaviour, perceptions and performance on retrieval tasks and under varying mobile conditions.",1,ad,True
"Delays and time pressures, which may be induced by increased levels of distraction and input error rate, also have a signi cant impact on search behaviour and objective performance. A study by Crescenzi et al. [10] compared two groups of users on a number of",0,,False
156,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"search tasks: one group was given a per­task time limit of 5 minutes, while the other was given no limit. e results showed that users faced with time pressures experience increased (perceived) task di culty and less satisfaction with their performance and felt an increased need to work fast and engage in more metacognitive monitoring. Earlier work [9] by the same authors showed that time pressure leads to more queries being issued, fewer documents being viewed and less focus on examination of documents and SERPs. Recent work [15] has demonstrated that users perceive a similar increase in search task di culty and reduction in satisfaction of their own performance when put under more distracting experimental conditions. ese e ects are likely as a result of the increased cost of complex cognitive tasks under such conditions, leading to a modi cation in behaviour as explained by the search models and studies of Azzopardi et al. [2].",1,ad,True
"Indeed, distractions during walking, driving, and other realworld interactions can preoccupy users [30], reducing their e ectiveness in interacting with the UI [4, 23] and resulting in a larger number of misspelled queries and an a empt by users to shorten queries [32, 33]. Walking whilst using a mobile device requires both cognitive and motor abilities and users must divide their attention between the two tasks [21]. is means either an increase in cognitive load, a decrease in pace, a decrease in task performance or a combination of these [22]. ere are many examples of distracted input on smart phones where users must split their a ention between the task of navigating their physical environment and navigating information on the smart phone screen [26]. It could even be interpreted that users are performing tasks inside a bubble, ipping back and forth between the information on the screen and the outside world [17]. Given that today's users are more likely to be mobile when they search for information online, a deeper understanding of their interactions and challenges whilst mobile will help understand situational search behaviour and the in uences of these fragmentations on search.",1,ad,True
"in 8 participants for each. Distraction level was a between-subjects variable, while device type was within-subjects.",0,,False
"Following the procedure of Lin et al. [23], participants on the treadmill were asked to select a comfortable walking pace using the increase and decrease belt speed bu ons, which was then increased by 20% to induce a small amount of ambulatory distraction. e resulting speeds ranged between 2.2 MPH (3.5 KPH) and 3.8 MPH (6.1 KPH) with a mean of 2.9 MPH (4.7 KPH) and men choosing to walk, on average, 0.78 MPH faster than women. e obstacle course group was shown how to navigate a pre-de ned layout (see Figure 1), were asked to maintain a normal walking pace and were prompted to speed up by the researchers if their pace began to noticeably decrease during the task.",1,ad,True
3 METHOD,0,,False
"We conducted a laboratory experiment with 24 participants drawn from a large European University (a mixture of academic sta , support sta and post-graduate students), of whom 13 were male. Although participants were randomly assigned to one of the 3 conditions, there was a very equal spread of genders with no fewer than 3 of each gender assigned to all conditions (X2,""0.59, p-value"",""0.75). Ages ranged from 18 to 60, with 2 modal age ranges of between 25 and 30 and between 31 and 40. Ages were also distributed between the experimental conditions with no signi cant di erences (X2"",""5.13, p-value"",0.74). 18 of the participants were native English speakers and the rest were completely uent in the language.",1,ad,True
"ere were two independent variables: the type of device (tablet or phone; a Huawei MediaPad M2 8"" and Moto X Style respectively, both running Android version 5 with the Google Chrome web browser) and the level of distraction. e distraction level was varied by simulating 2 everyday situations experienced by mobile device users: walking quickly on a treadmill and navigating an environment with obstacles, as well as a baseline condition in which the participant was seated without any distractions. Participants were randomly allocated to one of the three conditions, resulting",1,ad,True
Figure 1: Plan view of obstacle course layout. Participants began at the orange arrow and followed the course in an anticlockwise direction (green circle followed by yellow).,0,,False
"In order to ensure that we could control the search system and record interaction data we developed a simple mobile search interface named zing, shown in Figure 2. e zing interface mimics a standard search engine by showing the titles of 10 links in descending order of relevance together with snippets for each. e interface allowed participants to enter search terms and indicate (via checkboxes) which documents they thought were relevant. It showed the current task (TREC topic) at the bo om of the screen and allowed participants to progress to the next topic at any time. e interface also prompted users to ll in pre- and post-topic questionnaires to survey their perceptions about the task and their self-assessed post-task performance, satisfaction, perceived time pressure and focus/involvement on the task. Half of the participants completed their rst 2 topics on a phone, moving on to the tablet for their nal 2 topics, while the other half began with the tablet.",1,TREC,True
"We used a standard test collection: AQUAINT, and removed duplicate documents in a pre-processing step to provide a be er",1,AQUAINT,True
157,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"in a random order with a per-task time limit of 15 minutes and alternated between the two device conditions by conducting the rst two tasks on one device before switching to the other for the nal 2 topics. e starting device for each user was allocated at random to prevent fatigue and/or learning e ects from confusing the results. Participants were asked to imagine they wanted to learn more about the subject of each topic for a short report and were requested to select between two and four documents they thought were relevant for each topic and were told they could submit multiple queries per topic, if necessary. Participant actions and behaviour were recorded by means of a GoPro camera worn on the head, a wide-angle view of the obstacle course and by recording and logging interactions with the touchscreen and browser interface (Figure 3).",1,ad,True
Figure 2: zing search interface on an Apple iPhone 5. Checkboxes used to indicate relevance.,0,,False
# Title,0,,False
AP Pre Post,1,AP,True
362 Human smuggling 0.29 2.83 2.75,0,,False
367 Modern Piracy,0,,False
0.26 2.79 2.25,0,,False
638 Wrongful convictions 0.23 2.83 3,0,,False
404 Ireland peace talks 0.28 3.25 2.79,0,,False
Table 1: TREC topics used.,1,TREC,True
"and more familiar user experience. To assess performance we made use of pre-de ned TREC topics from the 2005 Robust track [37], of which we chose 4 at random from a subset of those which are neither too di cult nor too easy1. Table 1 shows the topics chosen as well as the average precision (AP) of their titles on the AQUAINT collection and the participants' perceptions of each topic's di culty before (pre) and a er (post) completing it.",1,ad,True
"Indexing, searching and snippet generation was provided by Apache SOLR2. Each participant was given the same 4 topics (tasks)",0,,False
"1A er the method of Harvey et al. [14], whereby the di culty of a topic is determined by the average precision of its title over the document collection. 2h p://lucene.apache.org/solr/",0,,False
Figure 3: Example of data recorded via the cameras and screen recording so ware. Note that information from all 3 sources is temporally synced.,0,,False
4 RESULTS,0,,False
In the following we use t-tests to compare distributions that are normal (as well as results from Likert scales) and Wilcoxon signrank tests in cases of non-normal data (e.g. task duration and number of hits).,0,,False
4.1 Pre-study questionnaire,0,,False
"Before being told anything about the experiment, participants were asked to ll in a short pre-study questionnaire asking them about their use of mobile devices and search engines as well as how di cult they would expect it to be to search on a phone or a tablet in various contexts.",0,,False
"All but two participants use a mobile device several times a day and all but three use a search engine to nd information several times per day and all participants but one said they were either ""con dent"" or""very con dent"" at using a search engine to nd information. 19 use their mobile device at least once per day whilst walking, 9 use it daily on public transport and all but 3 use it to search the web on a daily basis. Participants expected that using both devices whilst walking on a treadmill, navigating an obstacle course or while si ing in a noisy pub or cafe would be signi cantly more di cult than when si ing still (see Figure 4). ey expected using a mobile phone to be signi cantly more di cult when navigating an obstacle course compared with when walking",1,ad,True
158,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"on a treadmill (t,""2.95, p-value"",""0.005) and expected, for both devices, that searching in a noisy pub or cafe would be signi cantly easier than in either of the other two conditions (all tests p-value""",1,ad,True
0.01).,0,,False
4.2 Pre-task perception,0,,False
"Before each task (TREC topic), the zing interface prompted participants to ll in a short questionnaire about their prior knowledge of the topic, their interest in it and how di cult they expected the task to be (overall di culty, di culty in nding relevant documents, and di culty in knowing when to nish; see Figure 5). To aid them in doing so, the topic title and description were presented at the bo om of the screen. ere was li le variation in the responses between the topics with most people stating that they had fairly li le prior knowledge and were moderately interested in the topics. Responses did indicate an expectation that topic 404 (""Ireland peace talks"") would be the most di cult, although the di erence was not signi cant. ere were only two instances where a participant was unsure of how to complete the task and in only 14% of cases was a topic deemed to be either very di cult or very easy. As expected, responses to all 3 questions on perceived task di culty were all signi cantly correlated with each other.",1,TREC,True
Figure 4: Expected di culty of searching on mobile phones and tablets under various conditions.,0,,False
"As participant age increased, the expected di culty of using either a mobile phone or a tablet on a treadmill (R-squared,""0.27, pvalue"",0.005; R-squared, 0.17 p-value,0.028) and when navigating an obstacle course (R-squared,""0.34, p-value"", 0.01; R-squared,"" 0.29, p-value"",""0.004) increased, however this was not the case for use when si ing still or in a noisy pub or cafe. e more con dent people were at using search engines in general, the easier they expected the task to be on the treadmill (R-squared"",""0.24, pvalue"",0.015) and the obstacle course (R-squared,""0.2, p-value: 0.03) on both devices. However, this relationship only held for the tablet when imagining si ing still (R-squared"",""0.28, p-value"",""0.008). ere was no signi cant relationship between search engine con dence and expected di culty in the noisy pub environment. Surprisingly, the participants' familiarity of using mobile devices when walking or in noisy environments was not predictive of their expected di culty of searching under the same conditions.""",1,ad,True
Figure 5: Results of pre-task questionnaire.,0,,False
Condition,0,,False
Sitting Obstacles Treadmill,1,ad,True
Overall di culty 2.21  3.06,0,,False
3.36,0,,False
Finding rel. docs. 2.43  2.59 ,0,,False
3.03,0,,False
When to nish 2.79  3.06,0,,False
3.58,0,,False
"Table 2: Mean responses about task di culty from pre-task questionnaires by condition.  , sig. di . with Obstacles; ",0,,False
", sig. di . with Treadmill",1,ad,True
"It seems that participants took experimental condition into account when estimating the di culty of tasks as there were differences in the perceived di culty of tasks, as shown in Table 2.",0,,False
"ose who knew they would be si ing still expected the tasks to be signi cantly easier than those who were navigating the obstacle course (t,3.95; p-value 0.01) and those who were on the treadmill (t,5.08; p-value 0.01). ose who were si ing still and",1,ad,True
159,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"those on the obstacle course thought nding relevant documents would be equally easy (t,""0.7, p-value"",""0.49), however those on the treadmill expected this to be signi cantly more di cult (t"",""2.58, p-value"",0.012). e treadmill group thought that knowing when to nish the task (i.e. ascertaining when they'd found enough information) would be signi cantly more di cult than the baseline group (t,""3.15, p-value"",""0.002). ere were no signi cant di erences in perceived task clarity between any of the groups, although those in the baseline group did claim to know more about the topics a priori than those in the other groups (compared to treadmill: t"",""2.22, p-value"",0.031 ; compared to obstacle course: t,""2.18, p-value"",0.033).",1,ad,True
4.3 Post-task perception,0,,False
"Figure 6: Perceived post-task di culty by condition. · , sitting; , obstacle course; , treadmill",1,ad,True
#,0,,False
estion,0,,False
"Q1 I felt hurried or rushed when completing this task Q2 It was important to complete this task quickly Q3 Overall, I thought this was a di cult task Q4 I am satis ed with steps I took to nd information Q5 I forgot my immediate surroundings during the task Q6 I was so involved that I ignored everything around me Q7 I was so involved that I lost track of time Q8 I was absorbed in my search task Q9 I found enough info. about the search topic Q10 I am satis ed with the info. I found",0,,False
Table 3: Selected post-task questions.,0,,False
"Immediately a er each task participants lled in a post-task questionnaire, which included items from the focused a ention scale of O'Brien et al. [29] as well as items from Crescenzi et al. [10] (see Table 3 for selected items). e questions were chosen to ascertain the participants' levels of perceived time pressure, selfassessed performance and involvement in the search task. ere were signi cant di erences in terms of perceived di culty between the 4 topics with 2 topics scoring a median Q3 (""Overall, I thought this was a di cult task"") agreement of 2, one at 3 and the most di cult scoring 4. ere were, however, no signi cant di erences between the 4 topics for the other questions. Interestingly, women reported feeling signi cantly less absorbed in the task (Q8; t,2.96; p-value,0.004) than men and felt less like they lost track of time (Q7; t,1.99; p-value,0.049).",0,,False
"As shown in Table 4, the di erent experimental conditions had a number of di erent e ects on the participants' perceptions. ose on the treadmill felt signi cantly more rushed than in the other two conditions (Q1) and those si ing still felt signi cantly less pressure to complete the tasks quickly than the other 2 groups (Q2). It appears that those si ing still generally found the tasks easiest (Q3; see Figure 6) - signi cantly more so than those in the treadmill group - and were more satis ed with the steps they took to nd",1,ad,True
Condition Sitting Obstacles Treadmill,1,ad,True
Q1,0,,False
2.25  2.53 ,0,,False
3.28,0,,False
Q2,0,,False
2.14  2.87 ,0,,False
3.44,0,,False
Q3,0,,False
2.43  3.0,0,,False
3.31,0,,False
Q4,0,,False
3.86  3.47,0,,False
3.03,0,,False
Q5,0,,False
3.64  3.16 ,0,,False
3.42 ,0,,False
Q6,0,,False
3.53 3.03 ,0,,False
3.47 ,0,,False
Q7,0,,False
3.39 3.09,0,,False
3.52,0,,False
Q8,0,,False
3.96 3.56,0,,False
3.81,0,,False
Q9,0,,False
3.39  3.75 ,0,,False
2.91,0,,False
Q10,0,,False
3.46  3.56 ,0,,False
2.8,0,,False
Table 4: Mean responses from post-task questionnaires by,0,,False
"condition.  , sig. di . with Obstacles;  , sig. di . with",0,,False
Treadmill,1,ad,True
"relevant information (Q4). ose si ing and on the treadmill were signi cantly more likely to forget their immediate surroundings than those on the obstacle course (Q5) and felt more involved in the task (Q6). Although di erences were not signi cant, there was a trend that those on the treadmill felt more involved in the task to the point where they lost track of time (Q7) and those on the obstacle course felt less absorbed in the search tasks (Q8). In terms of being able to nd su cient information to ful ll the task, those in the baseline and obstacle course conditions felt there signi cantly more able to nd enough information (Q9) and were signi cantly more satis ed with what they found than those on the treadmill (Q10).",1,ad,True
5 SEARCH PERFORMANCE,0,,False
"In order to objectively evaluate search performance, we rely on three main metrics: the average number of hits (relevant documents) returned per search query; the mean average precision a ained; the number of documents bookmarked; the number of documents read; the ratio of relevant documents bookmarked relative to the total number bookmarked (to give an indication of how accurate users were with their bookmark choices); and the same ratio for documents read. Based on the results of linear models, the number of hits, mean average precision and number of documents read are all signi cant predictors of perceived success (Q9 and Q10 in the post-task questionnaire). We also consider a number of other",1,ad,True
160,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
proxies of overall search and task performance as well as metrics such as query length and search duration.,0,,False
5.1 Performance by experimental condition,0,,False
Condition,0,,False
Sitting Obstacles Treadmill,1,ad,True
# of queries/user 13,0,,False
12,0,,False
14,0,,False
Hits/query,0,,False
3.71  2,0,,False
1.75,0,,False
MAP,1,MAP,True
0.104  0.085,0,,False
0.083,0,,False
Bookmarks/query 1.32  1.74 ,0,,False
1.03,0,,False
(Ratio relevant) 0.55,0,,False
0.47,0,,False
0.49,0,,False
Docs read/query 1.58  1.19,1,ad,True
1.0,0,,False
(Ratio relevant) 0.43,0,,False
0.41,0,,False
0.44,0,,False
# of query terms 3.61  3.17,0,,False
3.38,0,,False
ery duration 39.5s  30.5s,0,,False
35s,0,,False
"Table 5: Objective performance measures by condition.  ,",0,,False
"sig. di . with Obstacles;  , sig. di . with Treadmill",1,ad,True
"Table 5 shows how the objective performance measures varied by experimental condition. Most notably, the average number of hits per query achieved by the baseline users is signi cantly greater than those by either the treadmill (p-value,0.029) or obstacle course (p-value,""0.023) groups, even though all groups submi ed very similar numbers of queries (see Figure 7). is is also true for mean average precision. is suggests that those si ing were able to generate more accurate and precise queries than those in the other two groups. is may be because the queries they submi ed were longer and more detailed (signi cantly longer than the obstacle course group: p-value"",0.002) and because they spent signi cantly more time per query than the others - over 5 seconds longer on average per query (compared to treadmill: p-value,0.023; compared to obstacle course: p-value,0.005).",1,ad,True
"ose si ing and those on the obstacle course bookmarked signi cantly more documents than the treadmill group (p-values,"" 0.01 and 0.001 resp.). e participants on the obstacle course bookmarked the most o en, however, as they bookmarked a larger number of non-relevant documents, they had the lowest ratio of relevant bookmarks. e baseline group read the largest number of documents on average, perhaps partially explaining their increased query durations, and read signi cantly more than those on the treadmill (W"",""7371, p-value"","" 0.015). is may be because si ing at a desk is a more comfortable environment for in-depth tasks such as reading, which requires concentration and may be disrupted by movements of the screen or eyes.""",1,ad,True
6 IMPACT OF DEVICE USED,0,,False
"To determine what impact device type has on search, half of the search tasks were completed on a smart phone and the other half were completed on a larger tablet device. As shown in Table 6, although the objective performance measures recorded for the different devices were almost identical (i.e. no signi cant di erences), there was substantial variation in the participants' perceptions of searching on each device. In general, people found the smart phone to be much less useful for the tasks set than the tablet: ey felt signi cantly more hurried and rushed when using the phone (t,2.25;",0,,False
Figure 7: Number of hits (relevant documents) returned per query.,0,,False
"p-value,0.025) and found the tasks to be signi cantly more di cult (t,2.7; p-value,0.007). Although users felt equally satis ed on both devices about the steps they themselves had taken to nd the necessary information (t,-0.45; p-value,""0.65), when using the smart phone they were signi cantly less satis ed with the information they found (t"",""-3.14; p-value 0.01), suggesting that they placed the blame on the device and not on their own search behaviour.""",1,ad,True
Device type,0,,False
Smart phone Tablet,0,,False
Hits/query,0,,False
2.8,0,,False
2.77,0,,False
Bookmarks/query,0,,False
1.48,0,,False
1.2,0,,False
# of query terms,0,,False
3.39,0,,False
3.4,0,,False
ery duration,0,,False
48.6,0,,False
49.2,0,,False
Q1 felt hurried/rushed,0,,False
2.98 ,0,,False
2.69,0,,False
Q3 di cult task,0,,False
3.49 ,0,,False
3.12,0,,False
Q4 satis ed with step taken 3.18,0,,False
3.25,0,,False
Q10 satis ed with info. found 2.77 ,0,,False
3.22,0,,False
Table 6: Objective and subjective performance measures by,0,,False
"device type.  , sig. di .",0,,False
"It seems the experimental condition had an impact on how users perceived di erences between the devices (Table 7). Users in the baseline condition (si ing at a desk) actually performed be er - in terms of number of hits - on the tablet than on the phone, albeit not signi cantly (W,""977, p-value"",""0.121). is trend was, however, reversed under the other two experimental conditions with those on the phone seemingly performing be er than those on the tablet. is was also re ected in the users' perception of""",1,ad,True
"ow/involvement in the task: ose si ing felt signi cantly less aware of their surroundings when using the tablet (Q5; t ,"" 2.2, p-value"",""0.03) than the phone, while those in the other conditions had the opposite experience (Q5; t"",""-2.11, p-value"",0.036); and those in the non-baseline conditions felt less aware of time passing when",1,ad,True
161,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"using the phone than the tablet (Q7; t,""3.53, p-value"",""0.001). It's also notable that the baseline group spent longer on the tasks (query duration) when using the tablet than the phone, but the other groups actually spent longer when using the phone.""",0,,False
Baseline Other cond.,0,,False
P,0,,False
TP,0,,False
T,0,,False
Hits/query,0,,False
2.67 3.64 2.84 2.37,0,,False
ery duration,0,,False
59.1 64.1 44.2 42.2,0,,False
Q5 forgot surroundings 3.17  3.7 3.53  3.18,0,,False
Q7 lost track of time 2.98  3.42 3.73  3.1,0,,False
"Table 7: Performance and perception by condition and device type (P,""smart phone, T"",tablet).  , sig. di .",0,,False
7 DISCUSSION,0,,False
is research set out three research questions aimed at exploring mobile searching and the e ects of fragmented a ention in common situations. e following discussion will consider each of the research questions in turn.,0,,False
Do common mobile situations impact on users' percep-,0,,False
tions of the task and their own performance? (RQ1) Our results demonstrate that the di erent conditions had a num-,1,ad,True
"ber of fairly profound e ects on user perceptions, both before and a er completing the tasks. e pre-study questionnaire showed that participants expected using both devices whilst walking on a treadmill would be more di cult than si ing still and navigating an obstacle course. is is something that tallies with past research, which shows that situational impairments do exert a range of effects on performance, adding levels of di culty as interaction with the device takes place [20]. e treadmill lessened their feeling of control, or lack of it, which reduced their perceived e ectiveness as they interact with the UI [4, 23]. e older a participant was, the greater the expected di culty of using a tablet on a treadmill, but this was not the case for phones or when si ing, perhaps because younger people are more familiar with such devices and may have more experience using them in mobile situations [1].",1,ad,True
"Post-task perception showed that di erent experimental conditions had a number of di erent e ects on the participants' perceptions. ose on the treadmill felt signi cantly more rushed than in the other two conditions. Oulasvita et al. [30] pointed to the e ect of a situation on the duration of continuous a ention, nding that participants in their laboratory experiments were more focused on the tasks compared with participants on a busy street. In this study, those si ing and on the treadmill were signi cantly more likely to forget their immediate surroundings than those on the obstacle course and more involved in the task. is may be because there is an increased need to a end to the surrounding environment when walking, but with the treadmill this is not the case as the situation does not change [23].",1,ad,True
"Participants seemed to take the experimental conditions into account when estimating task di culty, recording signi cant differences in perceived task di culty. With the frequency of mobile use continuously on the increase, participants were likely to be aware of these potential challenges as they interacted. ey expected these di culties to increase their cognitive workload and",1,ad,True
the changes in mobility (i.e. walking) to in uence not only their walking speed but mental workload during the tasks [24]. ose who knew they would be si ing still expected the tasks to be easier than the other conditions while those who were si ing still and those on the obstacle course thought nding relevant documents would be equally easy.,1,ad,True
"It is interesting that people expected the treadmill to be most di cult, despite the fact that it should require more cognitive e ort to avoid the obstacles. is may be because these participants have control over the pace at which they are walking, while those on the treadmill are kept at a constant speed by the mechanism.",1,ad,True
"ose on the obstacle course have the possibility to slow down while conducting demanding tasks, such as assessing document relevance, thereby reducing their overall cognitive load [21]. is may explain why Mizobuchi et al. [24] observed no reduction in input accuracy when walking and texting - the participants simply reduced their walking speed to prioritise text input.",1,ad,True
"Participants on the obstacle course felt less absorbed in the search tasks. is could be due to the fact that walking while using a smart phone requires both cognitive and motor abilities and appropriate division of a ention to each [20]. e level of absorption in the search tasks is less due to the participant needing to be aware of their surroundings. e participants are walking and using the device, in doing so they take longer to complete a set route and, therefore, walk more slowly. ere are two repercussions to this, they will slow down on the obstacle route (because they have control) and experience increased cognitive load on the treadmill (not being able to adjust their speed) [22].",1,ad,True
Do common mobile situations impact on objective mea-,0,,False
sures of users' task performance and behaviour? (RQ2) Although the e ects on objective performance were perhaps,0,,False
"not quite as numerous or great as they were on perception, the di erent conditions did impact search behaviour and, consequently, performance. e most profound di erence was found in the quality, in terms of number of hits and MAP, of the queries submi ed those si ing were able to generate signi cantly more accurate and precise queries than those in the other two groups. Perhaps this is because si ing evokes an environment more like desktop search, where users feel that they have more time to think carefully about the queries they enter [18]. is was also evidenced by the si ing group's queries being signi cantly longer (i.e. being comprised of more terms) and is in line with the studies of Kamvar et al. [18] and Schaller et al. [32, 33] and also corresponds with the results from the post-task questionnaire, which showed that the users on the treadmill and on the obstacle course felt more hurried and rushed and were more aware of time pressures.",1,MAP,True
"Additionally, it seems the e ects of time pressure on search behaviour highlighted in the studies of Crescenzi et al. [9, 10] are also relevant in this context, even though in the case of our study time pressures were perceived rather than enforced. Interestingly, though, we did not observe the same increase in querying frequency. [9]. A possible way to mitigate these issues might be to detect when users are walking (by using the device's motion sensors and gyroscopes) and to adapt the interface to o er more querying support and to present more concise snippets in such situations.",1,ad,True
Participants on the treadmill bookmarked signi cantly fewer documents than the other two groups. A situation which is again,1,ad,True
162,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"likely because they felt more rushed, meaning they were less likely to explore the search results and to assess potentially relevant documents for relevance [9], tasks that will likely incur a higher ""cost"" [2] when input accuracy [26] and reading comprehension [3] is reduced. Similarly, participants in both of the non-baseline groups spent signi cantly less time on each SERP and, therefore, assessed signi cantly fewer documents for relevance.",1,ad,True
What impact does the device type have on user perfor-,0,,False
mance and perception thereof? (RQ3) ere was substantial variation in the participants' perceptions of,0,,False
"searching on each device, contradicting the objective performance observed on the devices, which were identical. We found that the device used in uenced participants' perceptions of the search tasks and that the tablet was, on the whole, preferred, although this was somewhat dependent on experimental condition. People felt more hurried, found the tasks harder and were less satis ed with the information they had found when using the phone. e increased (perceived) di culty on the phone may be because users have less screen space to work with, making interaction with the various UI controls more di cult, especially when interaction occurs in a distracting environment [4]. Since larger screens appear less clu ered with information, users may have felt less overwhelmed by the amount of information presented on the relatively more spacious tablet screen [5]. ese ndings are in line with those of Hancock et al. [12], however our results are novel as they demonstrate that this e ect holds between smart phone and tablet devices and is in fact more profoundly felt in the context of mobile search.",1,ad,True
"In contrast to the results of Song et al. [36], we didn't nd any di erence in query length or query duration between the two devices, although there was notable interaction between the device type and experimental condition. Users in the baseline (seated) group performed be er on the tablet than the phone, however, those in the other two groups performed rather be er on the phone than the tablet. is may be because phones are more typically used as a handheld device at arm's length, while the larger, heavier tablets are more o en used when propped up on a table or cradled in one arm [31] and rarely used out of the home [25]. is is also evidenced by the di erence in perceived immersion/ ow in the task - when seated, using the tablet resulted in a greater feeling of immersion than the phone, while this was reserved for the other two conditions. e extra he of the tablet when walking may make the device too conspicuous, serving to pull the user out of ow, while the much lighter, less cumbersome phone does not prevent the users from becoming immersed.",1,ad,True
"e variation in the amount of time spent on tasks (baseline users spend longer on the tablet than the phone, with the situation reversed for the other conditions) is interesting and perhaps speaks to the di erence in weight (and therefore experienced encumbrance) between the two devices. Increased encumbrance has been shown to result in reduced input accuracy and increased mental load [27] and may lead to users more rapidly becoming fatigued, which would explain their propensity to give up the tasks earlier on the tablet when walking. When choosing between devices for a given task, it may therefore be useful to consider whether or not the user is likely to be moving or seated.",1,ad,True
8 CONCLUSIONS,0,,False
"e main aim of this study was to investigate how di erent mobile situational contexts and di erent mobile devices (i.e. phones and tablets) a ect user performance and experience when performing web search tasks. We conducted a laboratory experiment with 24 participants in which three di erent conditions were simulated: si ing at a table (the baseline), walking on a treadmill and navigating an obstacle course. Analysis of subjective measures, derived from pre- and post-task questionnaires, as well as objective performance metrics showed that both the context and device variables had a number of e ects on performance, both perceived and measured, as well as participants' feelings of immersion, satisfaction and urgency.",1,ad,True
"Our results provide useful insights to inform the design of future interfaces for mobile search and give us a greater understanding of how context and device size a ect search behaviour and user experience. It is clear that some contexts have negative e ects on user search experience and that this is additionally a ected by device type. When seated, tablets are preferable for complex search tasks, however this is reversed in instances where the advantage of the device's extra screen space is o set by its additional weight (and therefore, the extra encumbrance experience by the user).",1,ad,True
"ese insights suggest the need for more care to be taken when designing mobile search interfaces by considering the context in which the system will be used, as well as the type of device. Interfaces could be developed that adapt when a walking-like motion is detected to aid the user in generating queries and to present information in a terser, more focused manner to reduce mental load and simplify the information space. is work also has potential repercussions for IR and HCI researchers: When designing and evaluating mobile search systems, it is clear that whether the user is in motion and the combination of device size and weight and situational context have signi cant e ects on perception. It is also clear from this work that a treadmill may not always be appropriate for simulating mobile search as in reality users adjust their walking speed to prioritise interaction with the device, something which is not possible under this condition. erefore, practitioners should be aware of these factors to ensure that these insights are incorporated into study design and taken into account when assessing user performance so that results are in fact demonstrative of e ects induced by the experimental conditions and not other unmeasured variables.",1,ad,True
8.1 Future work,0,,False
"As future research in this area we plan to expand on this work by looking into user search behaviour in more detail using the additional qualitative sources of information we captured during the study. As noted earlier, we have recorded GoPro footage of each participant as well as screen recordings of their interactions which we plan to evaluate to identify pa erns and behaviours unique to each experimental condition. Using the data from the GoPro we will be able to evaluate the participants' spatial awareness (especially on the prede ned route) and their ""a ention-switches"" away from the device in di erent situations. Using the 3 everyday situations we will be able to assess the levels of immersion with each task and compare the GoPro data to the pre-task perceptions - does their",1,ad,True
163,0,,False
Session 2A: Search Interaction 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"initial thinking match reality and can we con rm our suspicions that the tablet's weight and bulk is the main cause of the di erences observed in this research? We intend to develop search interfaces that adapt to the user's situation (i.e. walking or not) and the device type and to investigate whether these changes can in fact aid users in fragmented contexts to query as well as those who are seated. We would also like to simulate other situations that induce a ention fragmentation, such as a busy restaurant or bar, and determine whether or not this causes similar changes in user behaviour and performance.",1,ad,True
REFERENCES,0,,False
"[1] Monica Anderson, Technology device ownership, 2015, Pew Research Center, 2015.",0,,False
"[2] Leif Azzopardi, Diane Kelly, and Kathy Brennan, How query cost a ects search be-",0,,False
"havior, Proceedings of the 36th international ACM SIGIR conference on Research",0,,False
"and development in information retrieval, ACM, 2013, pp. 23­32.",0,,False
"[3] Leon Barnard, Ji Soo Yi, Julie A Jacko, and Andrew Sears, Capturing the e ects",0,,False
"of context on human performance in mobile computing systems, Personal and",0,,False
"Ubiquitous Computing 11 (2007), no. 2, 81­96.",0,,False
"[4] Andrew Bragdon, Eugene Nelson, Yang Li, and Ken Hinckley, Experimental",0,,False
"analysis of touch-screen gesture designs in mobile environments, Proceedings of",0,,False
"the SIGCHI Conference on Human Factors in Computing Systems, ACM, 2011,",0,,False
pp. 403­412.,0,,False
"[5] Stephen Brewster, Overcoming the lack of screen space on mobile computers,",0,,False
"Personal and Ubiquitous Computing 6 (2002), no. 3, 188­205.",0,,False
"[6] Minhee Chae and Jinwoo Kim, Do size and structure ma er to mobile users?",0,,False
"an empirical study of the e ects of screen size, information structure, and task",0,,False
"complexity on user activities with standard web phones, Behaviour & information",0,,False
"technology 23 (2004), no. 3, 165­181.",0,,False
"[7] Karen Church, Mauro Cherubini, and Nuria Oliver, A large-scale study of daily",0,,False
"information needs captured in situ, ACM Transactions on Computer-Human",0,,False
"Interaction (TOCHI) 21 (2014), no. 2, 10.",0,,False
"[8] Karen Church and Nuria Oliver, Understanding mobile web and mobile search",0,,False
"use in today's dynamic mobile landscape, Proceedings of the 13th International",0,,False
"Conference on Human Computer Interaction with Mobile Devices and Services,",0,,False
"ACM, 2011, pp. 67­76.",0,,False
"[9] Anita Crescenzi, Diane Kelly, and Leif Azzopardi, Time pressure and system",0,,False
"delays in information search, Proceedings of the 38th International ACM SIGIR",0,,False
"Conference on Research and Development in Information Retrieval, ACM, 2015,",0,,False
pp. 767­770.,0,,False
[10],0,,False
", Impacts of time constraints and system delays on user experience, ACM",0,,False
"CHI, ACM, 2016, pp. 141­150.",0,,False
"[11] Mark Dunlop and Stephen Brewster, e challenge of mobile devices for human",0,,False
"computer interaction, Personal and ubiquitous computing 6 (2002), no. 4, 235­236.",0,,False
"[12] PA Hancock, BD Sawyer, and S Sta ord, e e ects of display size on performance,",0,,False
"Ergonomics 58 (2015), no. 3, 337­354.",0,,False
"[13] Rachel Harrison, Derek Flood, and David Duce, Usability of mobile applications:",0,,False
"literature review and rationale for a new usability model, Journal of Interaction",0,,False
"Science 1 (2013), no. 1, 1.",0,,False
"[14] Morgan Harvey, Claudia Hau , and David Elsweiler, Learning by example: train-",0,,False
"ing users with high-quality query suggestions, Proceedings of the 38th Interna-",0,,False
tional ACM SIGIR Conference on Research and Development in Information,0,,False
"Retrieval, ACM, 2015, pp. 133­142.",0,,False
"[15] Morgan Harvey and Ma hew Pointon, Perceptions of the e ect of fragmented",0,,False
"a ention on mobile web search tasks, ACM SIGIR Conference on Human Infor-",0,,False
"mation Interaction & Retrieval (CHIIR), 2017.",0,,False
"[16] Peter Johnson, Usability and mobility; interactions on the move, Proceedings of",0,,False
"the First Workshop on Human-Computer Interaction with Mobile Devices, 1998.",0,,False
"[17] Anne Kaikkonen, Full or tailored mobile web-where and how do people browse on",0,,False
"their mobiles?, Mobility, ACM, 2008, p. 28.",0,,False
"[18] Maryam Kamvar and Shumeet Baluja, A large scale study of wireless search",0,,False
"behavior: Google mobile search, Proceedings of the SIGCHI conference on Human",0,,False
"Factors in computing systems, ACM, 2006, pp. 701­709.",0,,False
[19],0,,False
", Deciphering trends in mobile search, IEEE Computer 40 (2007), no. 8,",0,,False
58­62.,0,,False
"[20] Shaun K Kane, Jacob O Wobbrock, and Ian E Smith, Ge ing o the treadmill:",1,ad,True
"evaluating walking user interfaces for mobile devices in public spaces, Proceedings",0,,False
of the 10th international conference on Human computer interaction with mobile,0,,False
"devices and services, ACM, 2008, pp. 109­118.",0,,False
"[21] Eric M Lamberg and Lisa M Muratori, Cell phones change the way we walk, Gait",0,,False
"& posture 35 (2012), no. 4, 688­690.",0,,False
"[22] Sammy Licence, Robynne Smith, Miranda P McGuigan, and Conrad P Earnest,",1,ad,True
"Gait pa ern alterations during walking, texting and walking and texting during cognitively distractive tasks while negotiating common pedestrian obstacles, PLoS",0,,False
"one 10 (2015), no. 7, e0133281.",0,,False
"[23] Min Lin, Rich Goldman, Kathleen J Price, Andrew Sears, and Julie Jacko, How",0,,False
"do people tap when walking? an empirical investigation of nomadic data entry,",1,ad,True
"International Journal of human-computer studies 65 (2007), no. 9, 759­769.",0,,False
"[24] Sachi Mizobuchi, Mark Chignell, and David Newton, Mobile text entry: relation-",0,,False
"ship between walking speed and text input task di culty, Proceedings of the 7th",0,,False
international conference on Human computer interaction with mobile devices &,0,,False
"services, ACM, 2005, pp. 122­128.",0,,False
"[25] Hendrik Mu¨ller, Jennifer L. Gove, John S. Webb, and Aaron Cheang, Understand-",1,Gov,True
ing and comparing smartphone and tablet use: Insights from a large-scale diary,0,,False
"study, Proceedings of the Annual Meeting of the Australian Special Interest",0,,False
"Group for Computer Human Interaction (New York, NY, USA), OzCHI '15, ACM,",0,,False
"2015, pp. 427­436.",0,,False
"[26] Matei Negulescu, Jaime Ruiz, Yang Li, and Edward Lank, Tap, swipe, or move:",0,,False
"a entional demands for distracted smartphone input, Proceedings of the Interna-",0,,False
"tional Working Conference on Advanced Visual Interfaces, ACM, 2012, pp. 173­",0,,False
180.,0,,False
"[27] Alexander Ng, John Williamson, and Stephen Brewster, e e ects of encumbrance",0,,False
"and mobility on touch-based gesture interactions for mobile phones, Proceedings of",0,,False
the 17th International Conference on Human-Computer Interaction with Mobile,0,,False
"Devices and Services, ACM, 2015, pp. 536­546.",0,,False
"[28] Hugo Nicolau and Joaquim Jorge, Touch typing using thumbs: Understanding the",0,,False
"e ect of mobility and hand posture, ACM CHI (New York, NY, USA), CHI '12,",0,,False
"ACM, 2012, pp. 2683­2686.",0,,False
"[29] Heather L O'Brien and Elaine G Toms, e development and evaluation of a survey",0,,False
"to measure user engagement, Journal of the American Society for Information",0,,False
"Science and Technology 61 (2010), no. 1, 50­69.",0,,False
"[30] An i Oulasvirta, Sakari Tamminen, Virpi Roto, and Jaana Kuorelahti, Interaction",0,,False
"in 4-second bursts: the fragmented nature of a entional resources in mobile hci,",0,,False
"ACM CHI, ACM, 2005, pp. 919­928.",0,,False
"[31] Tommaso Piazza, Morten Fjeld, Gonzalo Ramos, AsimEvren Yantac, and Sheng-",0,,False
"dong Zhao, Holy smartphones and tablets, batman!: mobile interaction's dynamic",0,,False
"duo, Proceedings of the 11th Asia Paci c Conference on Computer Human",0,,False
"Interaction, ACM, 2013, pp. 63­72.",0,,False
"[32] Richard Schaller, Morgan Harvey, and David Elsweiler, Entertainment on the",0,,False
"go: nding things to do and see while visiting distributed events, IIiX, ACM, 2012,",0,,False
pp. 90­99.,0,,False
[33],0,,False
", Out and about on museums night: Investigating mobile search behaviour",0,,False
"for leisure events, Searching4Fun WS at ECIR, 2012.",0,,False
"[34] Kelly M Seymour, Christopher I Higginson, Kurt M DeGoede, Morgan K Bifano,",0,,False
"Rachel Orr, and Jill S Higginson, Cellular telephone dialing in uences kinematic",0,,False
"and spatiotemporal gait parameters in healthy adults, Journal of Motor Behavior",1,ad,True
"48 (2016), no. 6, 535­541.",0,,False
"[35] Aaron Smith et al., Us smartphone use in 2015, Pew Research Center 1 (2015).",0,,False
"[36] Yang Song, Hao Ma, Hongning Wang, and Kuansan Wang, Exploring and exploit-",0,,False
"ing user search behavior on mobile and tablet devices to improve search relevance,",0,,False
"Proceedings of the 22nd international conference on World Wide Web, ACM,",0,,False
"2013, pp. 1201­1212.",0,,False
"[37] E.M. Voorhees, e trec 2005 robust track, SIGIR Forum 40 (2006), no. 1, 41­48.",1,trec,True
164,0,,False
,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"The Probability That Your Hypothesis Is Correct, Credible Intervals, and E ect Sizes for IR Evaluation",0,,False
Tetsuya Sakai,0,,False
"Waseda University, Tokyo, Japan tetsuyasakai@acm.org",0,,False
ABSTRACT,0,,False
"Using classical statistical signi cance tests, researchers can only discuss P(D+|H ), the probability of observing the data D at hand or something more extreme, under the assumption that the hypothesis H is true (i.e., the p-value). But what we usually want is P(H |D), the probability that a hypothesis is true, given the data. If we use Bayesian statistics with state-of-the-art Markov Chain Monte Carlo (MCMC) methods for obtaining posterior distributions, this is no longer a problem. at is, instead of the classical p-values and 95% con dence intervals, which are o en misinterpreted respectively as ""probability that the hypothesis is (in)correct"" and ""probability that the true parameter value drops within the interval is 95%,"" we can easily obtain P(H |D) and credible intervals which represent exactly the above. Moreover, with Bayesian tests, we can easily handle virtually any hypothesis, not just ""equality of means,"" and obtain an Expected A Posteriori (EAP) value of any statistic that we are interested in. We provide simple tools to encourage the IR community to take up paired and unpaired Bayesian tests for comparing two systems. Using a variety of TREC and NTCIR data, we compare P(H |D) with p-values, credible intervals with con dence intervals, and Bayesian EAP e ect sizes with classical ones. Our results show that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di erence in means but also for the e ect size in terms of Glass's .",1,ad,True
CCS CONCEPTS,0,,False
·Information systems  Retrieval e ectiveness; Presentation of retrieval results;,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080766",1,ad,True
KEYWORDS,0,,False
Bayesian hypothesis tests; con dence intervals; credible intervals; e ect sizes; Hamiltonian Monte Carlo; Markov Chain Monte Carlo; p-values; statistical signi cance,0,,False
1 INTRODUCTION,1,DUC,True
"In March 2016, the American Statistical Association (ASA) published an o cial statement about the limitations of classical significance tests and p-values, in response to their continued misuse and misinterpretations [33]. While ASA's main statement does not contain anything new (e.g., ""A p-value, or statistical signi cance, does not measure the size of an e ect or the importance of a result""), the document mentions some alternatives to classical signi cance tests, including Bayesian methods. It goes on to say: ""All these [alternative] measures and approaches rely on further assumptions, but they may more directly address the size of an e ect (and its associated uncertainty) or whether the hypothesis is correct."" In the IR community, similar warnings against classical signi cance tests have been given by Cartere e [4] and Sakai [22], amongst others.",1,ad,True
"e above quotations from the ASA statement may be paraphrased as follows. Classical signi cance tests can only give us P(D+|H ), the probability of observing the data D at hand or something more extreme under the assumption that the hypothesis H is true (i.e., the p-value). But what we usually want is P(H |D), the probability that a hypothesis is true, given the data. eoretically, the Bayesian framework proposed in the 18th century [2] can give us exactly this, but it was heavily criticised during the 19th and 20th centuries (See Section 2.1). However, with the recent advent of e ective and e cient sampling algorithms for obtaining posterior distributions known as Markov Chain Monte Carlo (MCMC) methods [15], Bayesian approaches to statistical testing are rapidly gaining popularity among statisticians [30, 31]. us, as alternatives to the classical p-values and 95% con dence intervals which are o en misinterpreted respectively as ""probability that the hypothesis is (in)correct"" and ""probability that the true parameter value drops within the interval is 95%,"" we can employ Bayesian tests to easily obtain P(H |D) and credible intervals, which represent exactly the above. Moreover, with Bayesian tests, we can easily handle virtually any hypothesis, not just ""equality of means,"" as we shall demonstrate later.",1,ad,True
We provide simple tools to encourage the IR community to take up paired and unpaired Bayesian tests for comparing two systems; our tools are based on a state-of-the-art MCMC method called Hamiltonian Monte Carlo and a recently-proposed variant called No-U-Turn Sampler [13]. Using a variety of TREC1 and NTCIR2,1,TREC,True
1h p://trec.nist.gov/ 2h p://research.nii.ac.jp/ntcir/,1,trec,True
25,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"data, we compare P(H |D) with p-values, credible intervals with con dence intervals, and Bayesian EAP e ect sizes with classical ones. Our results show that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di erence in means but also for the e ect size in terms of Glass's .",1,AP,True
2 RELATED WORK,0,,False
2.1 Frequentists Versus Bayesians,0,,False
"It is well known that Ronald A. Fisher heavily and persistently criticised the Bayesian statistics since the 1960s: ""the theory of inverse probability [i.e., Bayesian statistics] is founded upon an error, and must be wholly rejected"" [9] (p.9). During the 20th century, the de facto standard in statistical analysis was indeed the ""frequentist"" approach founded upon Fisher's views, and the Bayesian approach was largely neglected.",0,,False
"e Bayesian approach had two weaknesses. e rst, which has not been resolved completely even to this day, is the fact that it relies on prior probabilities which nobody knows and therefore must be set based on researchers' subjective decisions or beliefs. However, given the lack of knowledge about priors, noninformative priors such as those that obey a uniform distribution can always be used, although this too is a subjective decision which may or may not re ect the true nature of the phenomenon under study3. In the present study, we simply follow the standard Bayesian practice of employing uniform distributions for obtaining priors.",1,ad,True
"e second weakness in the original Bayesian approach was that, despite the theoretical beauty of Bayes' eorem, it was o en di cult to obtain the posterior distributions as this o en involves computationally infeasible integrations. However, this second problem has actually been solved, with the recent advent of e ective and e cient sampling algorithms known as Markov Chain Monte Carlo (MCMC) methods [15]. Because of this, Bayesian statistics is now gaining popularity rapidly: for example, according to Toyoda [30], over one-half of Biometrika4 papers published in 2014 utilised Bayesian statistics. In the present study, we utilise a stateof-the-art MCMC method called Hamiltonian Monte Carlo [17] and a recently-proposed variant called No-U-Turn Sampler [13], which come with easy-to-use implementations.",1,ad,True
"Meanwhile, the classical signi cance testing approach of the frequentists have also received many criticisms over the past decades (e.g. [12]). Some even consider signi cance tests to be harmful. First, the practice of using the signi cance criterion  instead of the p-value o en leads to dichotomous thinking: ""Is the di erence statistically signi cant, or not?"" Ziliak and McCloskey [35] hold",1,ad,True
"3 In 2005, Efron remarked: ""What looks uninformative enough o en turns out to subtly force answers in one direction or another"" while arguing that a combination of Bayesian and frequentist ideas is needed to handle modern problems [8]. 4 is is the journal in which William S. Gosset (or ""Student"") published the famous paper on the t -test in 1908 [29]. h p://biomet.oxfordjournals.org/",0,,False
"Fisher responsible for this5. Second, even if the p-value is reported, this is a function not only of the e ect size (the magnitude of the di erence that we are interested in; See Section 3.6) but also the sample size. at is, a small p-value (i.e., a statistically highly signi cant result) may just re ect a large sample size (e.g., number of topics used for computing mean retrieval e ectiveness scores) rather than a large e ect size [22]. Moreover, as was mentioned in Section 1, the outcomes of classical signi cance tests are o en misinterpreted. We believe that it is time for the IR community to start using Bayesian statistics regularly, perhaps along with classical signi cance tests if the community feels reluctant to let the la er go. Since the Bayesian approach is not only highly intuitive and exible but now also computationally feasible, there really is no reason to reject it.",0,,False
"In the eld of psychology, Kruschke [14] argues that the Bayesian approach is superior to the (unpaired) t-test: ""Some people may wonder which approach, Bayesian or NHST,6 is more o en correct.",0,,False
"is question has limited applicability because in real research we never know the ground truth; all we have is a sample of data. [. . .] the relevant question is asking which method provides the richest, most informative, and meaningful results for any set of data. e answer is always Bayesian estimation."" In the present study, we empirically demonstrate the relationships between paired/unpaired t-tests and the corresponding state-of-the-art Bayesian methods using a variety of real IR evaluation data.",0,,False
2.2 Classical Signi cance Tests in IR,0,,False
"e limitations of classical signi cance tests have been pointed out in the eld of IR as well. Cartere e argues: ""we still believe p-values from paired t-tests provide a decent rough indicator that is useful for many of the purposes they are currently used for. We only argue that p-values and signi cance test results in general should be taken with a very large grain of salt, and in particular have an extremely limited e ect on publication decisions and community-wide decisions about ""interesting"" research directions."" Sakai [22] encourages IR researchers to report not only the p-values but also e ect sizes and con dence intervals. However, Sakai's more recent examination of over 1,000 SIGIR and TOIS papers [23] shows that about 30% of the entire papers lack signi cance testing, while about 65% of the papers with signi cance testing neither report p-values nor test statistics.",0,,False
"While computer-based, distribution-free alternatives to classical signi cance tests, namely, the bootstrap [20] and the randomisation test [27], have been advocated for IR evaluation, they have not been used as widely as the classical tests [23]. Moreover, these tests address the same limited question as the classical tests: what is the p-value?",1,ad,True
2.3 Bayesian Inferences for IR,0,,False
"We are already beginning to see Bayesian approaches to IR evaluation. Cartere e [3, 5] have proposed to evaluate IR systems by modelling binary and graded relevance judgments directly instead",1,ad,True
"5 ""Ronald A. Fisher would say, "" e potash manures are not statistically signi cant. Disregard them. [. . .] William S. Gosset would say, ""[. . .] If you want to know about the potash manures you have to consider their pecuniary value compared to the barley you're trying to make money with"" [35]. 6Null Hypothesis Signi cance Testing",0,,False
26,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"of using evaluation measure scores as the atomic unit. Using three di erent TREC data sets, he compared t-test p-values with the posterior probabilities of his four Bayesian models to argue the advantages of the la er. Cartere e uses JAGS (Just Another Gibbs Sampler), an open-source implementation of BUGS (Bayesian inference Using Gibbs Sampling) and its R interface rjags to conduct MCMC simulations.",1,TREC,True
"Our present work is in a sense less ambitious than that of Cartere e in that we are adhering to using evaluation measure scores as the atomic unit (just like Cartere e's ""Model 2""): we would like the IR community to take up the habit of using Bayesian approaches, and to do that, we believe that we need to move cautiously while clarifying how the transition from classical signi cance testing will a ect our research ndings. Cartere e demonstrates that the t-test (i.e., his ""Model 1"") p-values and his Model 2 posterior probabilities are strinkingly similar; we generalise this in several ways, by using diverse data sets from NTCIR and TREC, and by comparing con dence intervals with credible intervals, and classical sample e ect sizes with their Bayesian counterparts.",1,ad,True
"Zhang et al. [34] propose to replace the use of classical signi cance tests with Bayesian tests with probabilistic graphical models tailored to a speci c problem in information retrieval, namely, text classi cation. Following Kruschke [14, 15], they propose to make a decision about two text classifers by comparing the High Density Interval (HDI) and the Region of Practical Importance (ROPE) of the performance di erence  . Zhang et al. use Metropolis-Hastings (MH) sampling for their MCMC simulations.",0,,False
"Regarding the implementation of MCMC, we employ the stateof-the-art Hamiltonian Monte Carlo (HMC) [17] and its variant NoU-Turn Sampler (NUTS) [13] using stan and its R interface rstan, which are gaining popularity. According to Ho man and Gelman [13], HMC's features ""allow it to converge to high-dimentional",0,,False
"target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling""; NUTS automatically sets a required parameter for HMC, namely the number of steps L for the leap-frog method (See Section 3.2). Also, according to Kruschke [15] (pp.399-400), ""HMC can be more e ective than the",0,,False
"various samplers in JAGS and BUGS, especially for large complex models. [. . .] However, Stan is not universally faster or be er (at this stage in its development).""",0,,False
3 BAYESIAN TESTS 3.1 Bayesian Basics,0,,False
"To discuss Bayesian tests, let us start with the famous Bayes' rule:",0,,False
"f ( |x) ,",0,,False
f (x | )f ( ) f (x),0,,False
",",0,,False
f (x | )f ( ),0,,False
 +,0,,False
-,0,,False
f,0,,False
(x,0,,False
| )f,0,,False
(,0,,False
)d,0,,False
",",0,,False
(1),0,,False
"where f ( ) is the prior probability distribution of a random variable  (e.g., a population mean), f (x | ) is the likelihood of data x given  , and f ( |x) is the posterior probability distribution of  having observed x. Note that this view is already strikingly di erent from that of classical statistics: in classical signi cance testing, population parameters ( 's) are constants; in Bayesian statistics, they are random variables that form distributions. Since f ( |x) is a probability distribution, f (x) can be viewed as a normalising constant that",1,ad,True
ensures:,0,,False
 +,0,,False
-,0,,False
f ( |x)d,0,,False
",",0,,False
1 f (x),0,,False
 +,0,,False
-,0,,False
f (x | )f ( )d,0,,False
",1.",0,,False
(2),0,,False
"Hence, Eq. 1 implies that the property of the posterior distribution",0,,False
"f ( |x) is governed by f (x | )f ( ), i.e., the kernel.",0,,False
"If somehow the posterior distribution f ( |x) has been obtained,",0,,False
a point estimate of the population parameter  can be obtained as an expected a posteriori (EAP)7:,1,AP,True
" ^EAP , E[ |x] ,",1,AP,True
"  f ( |x)d ,",0,,False
f,0,,False
(x | )f f (x),0,,False
(,0,,False
),0,,False
d,0,,False
.,0,,False
(3),0,,False
"Moreover, how the random variable  moves around ^EAP can be quanti ed by the posterior variance (or its square root, posterior",1,AP,True
standard deviation):,0,,False
" V [ |x] , E[( - ^EAP )2|x] , ( - ^EAP )2 f ( |x)d . (4)",1,AP,True
"We can also obtain an interval estimate of  by removing an /2% area from either side of f ( |x). is is the 100(1 - )% credible interval (or Bayesian con dence interval), which is highly intuitive: the probability that the random variable  lies within the interval is 100(1 - )%. Recall that con dence intervals (CIs) used in classical statistics do not represent this probability: in the classical paradigm,  is a constant, and when (say) 100 CIs are created from 100 di erent samples, 100(1 -)% of them are expected to contain that particular .",1,ad,True
"As we do not know f ( ), we employ a non-informative prior distribution to avoid subjectivity to the best of our ability. Our choice is to use a uniform distribution, in which case f ( |x) is governed solely by the likelihood f (x | ) in Eq. 1 and therefore the EAP reduces to the maximum likelihood estimate (MLE). at is, in our se ing, the EAP of any parameter  is not directly a ected by our subjective choice of f ( ).",1,AP,True
3.2 HMC and NUTS,0,,False
"In the above discussion, we assumed that f ( |x) can be computed as de ned in Eq. 1. However, in practice, it is usually not feasible to do this analytically. at is where MCMC methods, which try to sample  repeatedly according to f ( |x), come into play. MCMC methods construct Markov Chains of parameter values so that the underlying distribution eventually reaches a stationary distribution: a er a burn-in period (B), all of the values in the chain obey the target distribution f ( |x). us, if we collect T values sequentially and throw away the intial B values, the remaining T , T -B values can be regarded as realisations of  that obey f ( |x). Suppose that we managed to obtain T ,"" 100, 000 realisations of  ; then, ^EAP (Eq. 3) can be obtained by simply averaging the T values. Similarly, the posterior variance (Eq. 4) can be obtained by averaging8 the squared di erence from ^EAP . A 95% credible interval for  can be obtained by sorting the T values in ascending order and then taking the 2,500th and 97,500th values as the lower and upper limits. e EAP values of other statistics such as e ect sizes (See Section 3.6) can be computed similarly. Moreover, for virtually any hypothesis H (e.g., """"mean 1 is higher than mean 2,"""" """"mean 1 is at least 0.2 points""",1,AP,True
"7 Alternatives would be a maximum a posteriori or a posterior median. ese three estimates represent the mean, mode and median of the posterior distribution, respectively. 8 Dividing by T su ces since T is large; there is no need for bias correction.",0,,False
27,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"higher than mean 2,"" or ""the e ect size is greater than 0.5""), the probability that H is correct can be obtained by just counting the instances in which H holds among the T realisations. It is clear that this approach is more versatile than classical signi cance tests.",0,,False
"Hamiltonian Monte Carlo (HMC) [17] and its variant No U-Turn Sampler (NUTS) [13] are state-of-the-art MCMC methods which we utilise for obtaining realisations of  from f ( |x). For details of HMC, we refer the reader to recent books on this topic [15, 17]. However, it would help for us IR researchers to have a general idea about its basic principles. In physics, Hamiltonian is the sum of potential energy and kinetic energy; given a curved surface in an ideal physical world, any object would move on the surface while keeping the Hamiltonian constant. e path of the moving object is governed by Hamilton's equations of motion, which can be solved by the leap-frog method with parameters  (stepsize) and L (leapfrog steps). To achieve sampling from a posterior distribution (which is our ""curved surface""), an intuitive explanation of what HMC does is as follows: put an object somewhere on the surface and give it a push; a er L units of time, let it halt and record its position; give it another push, and so on, until we have recorded T positions.",1,ad,True
"Brie y, for a set of d parameters  ,"" (1, 2, . . . , d ), the HMC algorithm looks like this:""",0,,False
"(1) Set  (1), , L,T , B (where T , T - B); Let t , 1; (2) Generate d independent values p(t ) ,"" (p1t , p2t , . . . , pdt ) from""",0,,False
"a standard normal distribution; (3) Obtain candidates  (a), p(a) using the leap-frog method; (4) Let  (t +1) ,""  (a) (i.e., accept the transition candidate) with""",0,,False
"probability min(1, r ); otherwise  (t +1) ,""  (t ) (i.e., reject the candidate and stay at the current position); (5) End if T "", t; otherwise let t , t + 1 and go to Step 2.",0,,False
"e r in Step 4, which governs how o en we can accept new candidates, is given by [30]:",0,,False
r,0,,False
",",0,,False
"f ( (a), p(a) |x) f ( (t ), p(t ) |x)",0,,False
",",0,,False
(5),0,,False
"where f (, p|x) is a joint distribution of f ( |x) and an independent standard normal distribution f (p). One strength of HMC is that r is o en close to one, and therefore we can achieve e cient sampling through many successful transitions in Step 4 while preserving the Hamiltonian.",0,,False
"NUTS is a variant of HMC that automatically determines an appropriate value of L [13]; both HMC and NUTS utilise an algorithm called dual averaging to automatically set  [18]; hence, from the viewpoint of the users of HMC and NUTS for Bayesian tests, we do not have to worry about se ing these parameters ourselves.",0,,False
3.3 R^ and E ective Sample Size,0,,False
"For MCMC algorithms, methods for checking whether the values have converged to a stationary distribution and for measuring the sampling e ciency are available.",0,,False
"R^, a measure for checking convergence, assumes that the MCMC algorithm produces multiple Markov Chains, and compares the variance across the multiple chains with the variance within the chains9. According to Gelman [10], if R^ is less than 1.1 or 1.2, we",0,,False
"9Zhang et al. [34] remark that ""it is perfectly right to do a single long sampling run and keep all samples."" However, it is very easy to throw away burn-in's that may not yet",0,,False
"can be assured that the chains have reached stationary distributions. MCMC produces Markov Chains and therefore the values in them are correlated to one another. If the chains produce very similar values repeatedly, then that is highly ine cient from the sampling point of view. E ective Sample Size (Ne ) is a measure available in stan for quantifying sampling e ciency, which means ""you have obtained a sample of size T , but that is worth a sample of size (approximately) Ne obtained when there is zero correlation within the chain."" Hence, we should also check that Ne is a large value. Both of the above measures are computed a er removing the burn-in's from the chains. As our Bayesian test tools that we introduce in Section 3.7 rely on stan and its R interface rstan, the tools output R^ and Ne on the R console. Since all experiments reported in the paper use su ciently large sample sizes, namely T ,"" 100, 000, we do not discuss these indicators henceforth.""",0,,False
3.4 Classical versus Bayesian Tests for Comparing Two Means,0,,False
"As was discussed in Section 3.2, Bayesian tests are versatile. However, the present study focusses on the problems of comparing two means from paired and unpaired data, since these are the most common and basic problems in IR evaluation. While two-sided tests that ask ""are the two systems equally e ective or not?"" are o en recommended in classical signi cance tests given lack of prior knowledge as to which system might be be er, this is not a very useful question from the Bayesian point of view, as two di erent systems are, by de nition, di erent. What is more practical to consider is the probability that System 1 is be er than System 2, P(S1 > S2|D) (or, alternatively, P(S1 < S2|D) ,"" 1 - P(S1 > S2|D)). Hence we compare Bayesian tests with classical one-sided tests. To be more speci c, given two systems, we let S2 be the less e ective system according to the sample data and consider P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) based on the Bayesian test, while se ing the classical null and alternative hypotheses as H0 : S1 "","" S2 and H1 : S1 > S2 so that the p-value represents """"P(D+|S1 "","" S2).""""""",0,,False
3.5 Statistical Models,0,,False
"For unpaired data, we assume that S1's scores obey N (µ1, 12), while S2's scores obey N (µ2, 22), for both Bayesian and classical tests. Hence, the classical test we employ is the Welch's t-test, which does not assume homoscedasticity (i.e., equal variances). It is known that Student's and Welch's t-tests yield virtually identical p-values when the two sample sizes are equal [16, 24]; the experiments reported in this paper satisifes this condition and therefore our classical test results can be regarded as representing both types of unpaired t -test.",0,,False
"For paired data, the classical test we apply is the paired t-test, which relies on the same normal assumptions as described above and therefore the score di erences obey N (µ1 - µ2, 12 + 22). As for the Bayesian test, we can easily consider a bivariate normal",0,,False
"obey the desired distribution and we prefer to do so. As for the number of chains, since R^ can be computed even for a single chain, by breaking it into multiple chains, we provide sample scripts for handling both multiple and single chains. e Bayesian test results are almost the same either way, but we report those based on multiple chains.",0,,False
28,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
distribution [31]:,0,,False
f,0,,False
"(x1, x2 |µ1, µ2, 12, 22, )",0,,False
",",0,,False
1 2 12 (1,0,,False
-,0,,False
) e-q/2,0,,False
",",0,,False
(6),0,,False
where,0,,False
"q,",0,,False
1,0,,False
1 -,0,,False
2,0,,False
[(,0,,False
x1,0,,False
- 1,0,,False
µ1,0,,False
)2,0,,False
-,0,,False
2,0,,False
(,0,,False
x1,0,,False
- 1,0,,False
µ1,0,,False
)(,0,,False
x2,0,,False
- 2,0,,False
µ2,0,,False
),0,,False
+,0,,False
(,0,,False
x,0,,False
2,0,,False
- 2,0,,False
µ2,0,,False
)2],0,,False
.,0,,False
(7),0,,False
"Here,  is the population correlation coe cient for x1's and x2's. us, for paired data, we can discuss hypotheses about the corre-",0,,False
lation between the two systems just as well as those about means and e ect sizes if we are interested in that aspect.,0,,False
3.6 E ect Sizes,0,,False
Sakai [22] stresses the importance of reporting e ect sizes and,0,,False
con dence intervals in the context of classical signi cance testing as,0,,False
a small p-value may just re ect a large sample size (See Section 2.2).,0,,False
"When comparing two means, the e ect size is usually given as",0,,False
the di erence between the two measured in standard deviation,0,,False
units. We argue that Bayesian test results in IR should also be,0,,False
accompanied with e ect sizes as well as credible intervals.,0,,False
While there are several choices of e ect sizes for comparing,0,,False
"two means, we choose to avoid relying on the homoscedasticity",0,,False
"assumption, to be consistent with the statistical models described in",0,,False
Section 3.5. One of the simplest e ect size in such a case is Glass's,0,,False
" [19]. at is, if System 1 is taken as the baseline run (or ""control",0,,False
"group,"") then its standard deviation is probably representative of",0,,False
"an ""ordinary world"" before the advent of System 2, and therefore:",1,ad,True
Gl ass 1,0,,False
",",0,,False
µ1 - µ2 1,0,,False
.,0,,False
(8),0,,False
"us, Glass's  measures the absolute di erence in ""ordinary"" stan-",0,,False
"dard deviation units. Alternatively, if System 2 is taken as the",0,,False
baseline:,0,,False
Gl ass 2,0,,False
",",0,,False
µ1 - µ2 2,0,,False
.,0,,False
(9),0,,False
"In our experiments, we focus on Eq. 9 since the second system is",0,,False
"the less e ective one (i.e., ""baseline"") according to the sample data.",0,,False
"For convenience, we will herea er refer to this version of e ect",0,,False
"size simply as ""Glass2.""",0,,False
"We can easily obtain the EAP and credible intervals for Glass2,",1,AP,True
as well as the probability of a hypothesis about the e ect size,0,,False
"being true, in exactly the same way as described in Section 3.2.",0,,False
"For example, the EAP for Glass2 can be obtained by computing",1,AP,True
"Eq. 9 T times using T realisations of µ1, µ2, 2 and then averaging them. Hence we propose that the IR community report the EAP,",1,AP,True
the credible interval and the probablity of hypothesis being true,0,,False
not only for the raw di erence in means but also for e ect sizes.,0,,False
Note that this is applicable to both paired and unpaired tests.,0,,False
3.7 Implementation,0,,False
"Here, we brie y describe our Bayesian test tools for comparing two means. e sample R scripts are based on those developed by Hideki Toyoda [30]10. We have sample scripts for generating both multiple and single chains but discuss only the former here. We have added a few shell scripts for postprocessing the Bayesian",1,ad,True
"10 Toyoda's original scripts are available from h p://www.asakura.co.jp/G 27 2.php? id,200. e present author is solely responsible for the modi cations and any errors introduced thereby.",0,,False
"realisations, to encourage researchers to use credibile intervals not only for the raw di erence in means but also for e ect sizes. e scripts and sample data are available from our website11. R with the rstan package12 and Rtools13 must be installed rst in order to use these scripts.",0,,False
"Figure 1 shows our sample R script for comparing paired data. It reads a stan le which describes the aforementioned bivariate normal distribution as well as generated quantities (the absolute di erence and Glass's 's), and a system score le wri en in R, and generates ve csv les that correspond to ve Markov chains.",1,ad,True
"Figure 2 shows an output of our UNIX shell script that summarises the Bayesian test results by reading the csv les. Here, the",1,ad,True
"rst argument is ""1032"" because this is the line number in each csv le where the realisations start a er the burn-in; the second argument is the number of realisations contained in the le (excluding the burn-in's); the third argument is the number of chains (i.e., number of csv les); the remaining arguments are the aforementioned Markov chain csv les. is script works for both multiple and single chains, by se ing the arguments appropriately14. e screenshot provides the following information about the paired data from run1 and run2:",0,,False
"· e EAP for the di erence in means is 0.042, and the 95% credibile interval is [0.009, 0.074]. e probability that µ1 - µ2 is greater than the speci ed threshold (which is set to 0 by default within the script) is 99.4%;",1,AP,True
"· e EAP for Glass2 (with run2 taken as the baseline) is 0.189 (i.e., about 19% of run2's standard deviation), and the 95% credible interval is [0.043, 0.345]. e probability that this e ect size is greater than the speci ed threshold (which is set to 0.2 by default) is 43.3%. Similar results with run1 taken as the baseline are also presented.",1,AP,True
"· e EAP for the correlation (i.e.,  in Eq. 6) between the scores of run1 and run2 is 0.857, and the 95% credible interval is [0.767, 0.920]. e probability that the correlation is greater than the speci ed threshold (which is set to 0.9 by default) is 12.2%.",1,AP,True
Note that thresholds can be altered arbitrarily within the shell script according to researchers' practical needs.,0,,False
Figures 3 and 4 provide similar screenshots of our scripts for comparing unpaired data.,0,,False
4 EXPERIMENTS,0,,False
"To encourage IR researchers to transition comfortably from the classical signi cance test paradigm to the Bayesian one for comparison of means, we now report on experiments that compare Bayesian results against classi cal test results using actual IR data. More speci cally, we compare the probability that System X is be er than System Y with p-values; credible intervals with classical con dence intervals, as well as e ect sizes computed based on both approaches. We show that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect",1,ad,True
"11 h p://www.f.waseda.jp/tetsuya/tools.html 12 h ps://cran.r-project.org/ 13 h ps://cran.r-project.org/bin/windows/Rtools/ 14By typing the command without arguments, suggested sets of arguments are displayed.",0,,False
29,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: IR data sets and evaluation measures used in this study,0,,False
Data set name NTCIR7IR4QA NTCIR9INTENT NTCIR12STC TREC03robust TREC11webD TREC15TS,1,INTENT,True
year track/task,0,,False
language measure,0,,False
tool,0,,False
#teams #runs #topics,0,,False
2008 IR for question answering [25],0,,False
Chinese Q-measure,0,,False
NTCIREVAL,0,,False
9 40 (20),0,,False
97,0,,False
2011 INTENT [28] (web diversity task),1,INTENT,True
Chinese D -nDCG@10 NTCIREVAL,0,,False
7 24 (20),0,,False
100,0,,False
2016 short text conversation [26] (tweet retrieval) Chinese nERR@10,0,,False
NTCIREVAL,0,,False
16 44 (20),0,,False
100,0,,False
2003 robust track [32],0,,False
English nDCG@1000 NTCIREVAL,0,,False
16 78 (20),0,,False
50,0,,False
2011 web track diversity task [7],0,,False
English  -nDCG@10 ndeval,0,,False
9 25 (20),0,,False
50,0,,False
"2015 temporal summarisation track, task 2 [1]",0,,False
English H,0,,False
-,0,,False
9 22 (20),0,,False
21,0,,False
Figure 1: A sample R script for comparing paired data.,0,,False
Figure 3: A sample R script for comparing unpaired data.,0,,False
"Figure 2: A shell script for obtaining the EAP, credible intervals, and the probability that the hypothesis is correct (paired data) for the absolute di erence, Glass's , and correlation coe cient (paired data).",1,AP,True
"sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes.",1,AP,True
"Table 1 provides a brief summary of the six data sets we used for our experiments. To strengthen the generalisability of our experimental results, we tried to cover diverse IR tasks from both TREC and NTCIR. For each data set (i.e., test collection with its submi ed runs), we chose one particular commonly-used evaluation measure: with the exception of TREC03robust15, we chose from one of the",1,TREC,True
15 We chose nDCG rather than Average Precision (AP) for TREC03robust as AP cannot handle graded relevance even though the data set comes with graded relevance assessments.,1,AP,True
"Figure 4: A shell script for obtaining the EAP, credible intervals, and the probability that the hypothesis is correct (paired data) for the absolute di erence, and Glass's  (unpaired data).",1,AP,True
"o cial measures of that track/task. Also, for each data set, we considered only the top 20 runs for pairwise comparisons as measured by that particular evaluation measure, which gives us 190 run pairs. For TREC11webD, -nDCG was computed using their o cial evaluation script ndeval16; For TREC15TS, the values of H (which is basically like a nugget-based F-measure de ned over a timeline [1]) were obtained from the o cial results of the track; other evaluation measures were computed using NTCIREVAL17, with the exponential gain value se ing (See, for example, [26]).",1,TREC,True
"For each data set, we conducted Bayesian and classical tests for every system pair (where System 1 outperforms System 2 for the sample data), using both paired and unpaired tests. For classical paired and unpaired tests, common sample e ect sizes were obtained by substituting sample means and System 2's sample standard deviations into Eq. 9. For Bayesian paired and unpaired tests, the EAP values of µ1, µ2, 2 were used with Eq. 9, under the two models described in Section 3.5, respectively.",1,AP,True
16 h p://trec.nist.gov/data/web/11/ndeval.c 17 h p://research.nii.ac.jp/ntcir/tools/ntcireval-en.html,1,trec,True
30,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 5: Paired Bayesian vs classical tests: comparisons with NTCIR data.,0,,False
4.1 Paired Test Results,0,,False
"Here we compare the paired Bayesian test (based on NUTS) with the classical paired t-test. We compare: (I) the paired Bayesian P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) with the classical one-sided paired p-value (See Section 3.4); (II) the paired 95% credible interval with the classical paired 95% con dence interval; (III) the paired Bayesian EAP Glass2 with the classical Glass2 based on sample statistics. e margin of error for the classical paired 95% con dence interval is given by t(n - 1; 0.05) V /n, where n is the sample size, V is the sample variance of the score di erences, and t(; ) is the two-sided critical t value18.",1,AP,True
"Figure 5 visualises the results of comparing the Bayesian and classical paradigms for paired data with NTCIR7IR4QA (graphs (A)(D)), NTCIR9INTENT (graphs (E)-(H)), and NTCIR12STC (graphs (I)(L)). Graphs (A), (E) and (I) (i.e., the le most column) compare the Bayesian P(S1 < S2|D) with the p-value; graphs (B), (F), and (J) compare the Bayesian EAP Glass2 with the sample Glass2; graphs (C), (G) and (K) compare the Bayesian credible interval lower limit with the con dence interval lower limit; and graphs (D), (H) and",1,ad,True
"18 T.INV.2T(P,  ) with Microso Excel.",0,,False
"(L) (i.e., the rightmost column) compare the Bayesian credible interval upper limit with the con dence interval upper limit. Figure 6 provides similar information for TREC03robust, TREC11webD, and TREC15TS.",1,TREC,True
"With the exception of Figure 6(J), i.e., the e ect size results for TREC15TS which we shall discuss in Section 4.3, the results in Figures 5 and 6 are highly consistent across the diverse NTCIR and TREC data sets and the messages are clear:",1,TREC,True
"(1) Graphs (A), (E), and (I) in Figures 5 and 6 show that the Bayesian P(S1 < S2|D) and the classical p-values are very highly correlated, echoing an earlier observation by Cartere e [5] (See Section 2.3). us, while P(S1 < S2|D) is what we usually want, it appears that the one-sided pvalue, which represents P(D+|S1 ,"" S2), can be considered as a reasonable approximation of P(S1 < S2|D).""",0,,False
"(2) Graphs (C), (D), (G), (H), (K), (L) in Figures 5 and 6 show that the Bayesian 95% credible intervals and the classical 95% con dence intervals are also very similar, despite the fundamental di erences in what they represent. us, the con dence interval can be considered as an approximation to the credible interval, which is what we really want.",0,,False
31,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 6: Paired Bayesian vs classical tests: comparisons with TREC data.,1,TREC,True
"(3) Graphs (B) and (F) in Figures 5 and 6 suggest that, while the Bayesian EAP e ect sizes generally align with the sample e ect sizes (Glass2), if the sample e ect size is small (e.g., less than 0.2), that may be an underestimation of the population e ect size. For example, Figure 6(B) indicates an instance (with a baloon) where the sample e ect size is 0.070 even though the Bayesian EAP e ect size, which we believe to be more accurate, is 0.113.",1,AP,True
"Observations (1) and (2) suggest that, even though the IR community may have relied on p-values and con dence intervals for decades, sometimes with incorrect interpretations, switching to Bayesian approaches would not turn all the experimental results in the literature upside down. As for Observation 3, even though we lack the ground truth for population e ect sizes, we would like to repeat Kruschke's argument [14]: ""the relevant question is asking which method provides the richest, most informative, and meaningful results for any set of data [. . .]"" (See Section 2.1).",1,ad,True
4.2 Unpaired Test Results,0,,False
Here we compare the unpaired Bayesian test (based on NUTS) with the classical Welch's t-test. We compare: (I) the unpaired Bayesian,0,,False
"P(S1 < S2|D) (i.e., the probability of the less likely hypothesis)",0,,False
with the classical one-sided unpaired p-value (See Section 3.4);,0,,False
(II) the unpaired 95% credible interval with the classical unpaired,0,,False
95% con dence interval; (III) the unpaired Bayesian EAP Glass2,1,AP,True
with the classical Glass2 based on sample statistics. e margin of,0,,False
"error for the classical unpaired 95% con dence interval is given by t(; 0.05) V1/n1 + V2/n2, where n1, n2 are the sample sizes, V1, V2 are the sample variances, and the approximated degrees of freedom  is given by [24]:",0,,False
",",0,,False
( V1 n1,0,,False
+,0,,False
V2 )2/{ (V1/n1)2,0,,False
n2,0,,False
n1 - 1,0,,False
+,0,,False
(V2/n2)2 } n2 - 1,0,,False
.,0,,False
(10),0,,False
"In fact, since our unpaired test experiments merely regard the paired data from NTCIR and TREC (i.e., two systems evaluated with a common topic set) as unpaired data, n1 , n2 holds in our case.",1,TREC,True
"Figure 7 visualises the results of comparing the Bayesian and classical paradigms for unpaired TREC data, similarly to Figure 6.",1,ad,True
"e unpaired results with the NTCIR data, which look very much like the corresponding paired test results with the NTCIR data shown in Figure 5, are omi ed in this paper due to lack of space.",0,,False
32,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 7: Unpaired Bayesian vs classical tests: comparisons with TREC data.,1,TREC,True
"Note that the classical sample e ect sizes used in Graphs (B), (F), and (J) are the same as the ones used in Figure 6. For example, we have observed from Figure 6(B) that while the sample e ect size for a system pair was 0.070, the Bayesian paired model gives us an EAP Glass2 of 0.113; in contrast, Figure 7(B) indicates that the Bayesian unpaired model gives us an EAP Glass2 of 0.161 for the same system pair.",1,AP,True
"It is clear that Observations 1-3 listed up in Section 4.1 also hold for the unpaired tests as well, which further strengthens our",0,,False
"ndings. However, just like Figure 6(J), Figure 7(J) shows and anomalous result for TREC15TS: we therefore discuss these results separately in the next section.",1,TREC,True
4.3 On the Anomalous Behaviour of H,0,,False
"e nugget-based H measure, the primary measure from the TREC 2015 Temporal Summarisation track, demonstrates a strange behaviour in Figure 6(J) and Figure 7(J), where the Bayesian EAP e ect sizes and sample e ect sizes are compared. e graphs look quite di erent from Graphs (B) and (F) of Figures 5-7. More specifically, for a small set of system pairs, EAP values are larger than sample e ect sizes; for a few others, sample e ect sizes are larger",1,TREC,True
"than EAPs. For example, for a system pair whose sample e ect size is 11.39, the paired Bayesian EAP is 12.10 (Figure 6(J)), while the unpaired Bayesian EAP is 12.37 (Figure 7(J)), as indicated by baloons. Similarly, for a system pair whose sample e ect size is 16.19, the paired Bayesian EAP is 12.55, while the unpaired Bayesian EAP is 12.76. is is in contrast to the aforementioned Obsevation 3 (Section 4.1) for the other data sets.",1,AP,True
"While the other evaluation measures used in our experiments have been studied quite extensively (e.g., [6, 20, 21]), we are not aware of any work in the literature that validated H in a similar way, and we believe that an investigation is in order. For example, while the actual nDCG scores from our TREC03robust data range between 0 and 0.9816 (i.e., almost fully covering the theoretical range of [0, 1]), the actual H scores from TREC15TS range between 0 and 0.4021: that is, the actual range width is about 0.4 rather than 1. Moreover, the standard deviations of H for each system are very low compared to the other measures, causing very large e ect sizes (See Eq. 9): hence, note that the axis scales of Figure 6(J) and 7(J) are very di erent from the other e ect size graphs. While studying the properties of a new measure is not the focus of this",1,TREC,True
33,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"study, these anomalous results with H may deserve a ention from other researchers such as the TREC track coordinators.",1,TREC,True
5 CONCLUSIONS AND FUTURE WORK,0,,False
"Using diverse data sets from TREC and NTCIR, we compared, under both paired and unpaired data se ings, (I) the Bayesian P(S1 < S2|D) (i.e., the probability of the less likely hypothesis) with the classical one-sided p-value; (II) the 95% credible interval with the classical 95% con dence interval; (III) the Bayesian EAP Glass2 with the classical Glass2 based on sample statistics. Our results showed that (a) p-values and con dence intervals can respectively be regarded as approximations of what we really want, namely, P(H |D) and credible intervals; and (b) sample e ect sizes from classical signi cance tests can di er considerably from the Bayesian EAP e ect sizes, which suggests that the former can be poor estimates of population e ect sizes. Fortunately, our results suggest that Bayesian statistics will not turn all experimental results in the IR literature upside down; however, we hope that these results, as well as our tools, will help IR researchers to take up Bayesian hypothesis testing approaches. For both paired and unpaired tests, we propose that the IR community report the EAP, the credible interval, and the probability of hypothesis being true, not only for the raw di erence in means but also for Glass's .",1,TREC,True
"e present study focussed on the problem of comparing two systems, and did not address the multiple comparison and familywise error rate problems [11]. If a researcher is interested in the p-value of every system pair, one e ective way to obtain them would be to employ the randomised Tukey HSD test [4, 22], which is completely distribution-free. In future work, we would like to explore Bayesian alternatives to this test and validate them.",1,ad,True
ACKNOWLEDGEMENTS,0,,False
"We thank Professor Hideki Toyoda (Waseda University) for le ing us modify his R code and distribute it, and Dr. Ma hew EkstrandAbueg (Google) for providing the TREC temporal summarisation track results.",1,TREC,True
REFERENCES,0,,False
"[1] Javed Aslam, Fernando Diaz, Ma hew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Tetsuya Sakai. 2016. TREC 2015 Temporal Summarization Track. In Proceedings of TREC 2015.",1,ad,True
"[2] omas Bayes. 1763. An Essay towards Solving a Problem in the Doctrine of Chances. Philosophical Transactions of the Royal Society of London 53 (1763), 370­418.",0,,False
[3] Ben Cartere e. 2011. Model-Based Inference About IR Systems. In Proceedings of ICTIR 2011 (LNCS 6931). 101­112.,0,,False
"[4] Ben Cartere e. 2012. Multiple testing in statistical analysis of systems-based information retrieval experiments. ACM TOIS 30, 1 (2012).",0,,False
[5] Ben Cartere e. 2015. Bayesian Inference for Information Retrieval Evaluation. In Proceedings of ACM ICTIR 2015. 31­40.,0,,False
"[6] Charles L.A. Clarke, Nick Craswell, Ian Soboro , and Azin Ashkan. 2011. A Comparative Analysis of Cascade Measures for Novelty and Diversity. In Proceedings",1,ad,True
"of ACM WSDM 2011. 75­84. [7] Clarles L. A. Clarke, Nick Craswell, Ian Soboro , and Ellen M. Voorhees. 2012.",0,,False
"Overview of the TREC 2011 Web Track. In Proceedings of TREC 2011. [8] Bradley Efron. 2005. Bayesians, Frequentists, and Scientists. J. Amer. Statist.",1,TREC,True
"Assoc. 100, 469 (2005), 1­5. [9] Ronald A. Fisher. 1970. Statistical Methods for Research Workers (14th Edition).",0,,False
Oliver & Boyd. [10] Andrew Gelman. 1996. Inference and Monitoring Convergence. In Markov Chan,0,,False
"Monte Carlo in Practice, W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.). Chapman & Hall/CRC, Chapter 8. [11] Andrew Gelman, Jennifer Hill, and Masanao Yajima. 2008. Why We (Usually) Don't Have to Worry about Multiple Comparisons. Technical Report. [12] Lisa L. Harlow, Stanley A. Mulaik, and James H. Steiger (Eds.). 2016. What If",0,,False
ere Were No Signi cance Tests (Classic Edition). Routledge. [13] Ma hew D. Ho man and Andrew Gelman. 2014. e No-U-Turn Sampler:,0,,False
"Adaptively Se ing Path Lengths in Hamiltonian Monte Carlo. Journal of Machine Learning Research 15 (2014), 1351­1381. [14] John K. Kruschke. 2013. Bayesian Estimation Supersedes the t test. Journal of Experimental Psychology: General 142, 2 (2013), 573­603. [15] John K. Kruschke. 2015. Doing Bayesian Data Analysis (Second Edition). Elsevier. [16] Yasushi Nagata. 1996. How to Understand Statistical Methods (in Japanese). Nikkagiren. [17] Radford M. Neal. 2011. MCMC Using Hamiltonian Dynamics. In Handbook of Markov Chain Monte Carlo, Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng (Eds.). 113­162. [18] Yurii Nesterov. 2009. Primal-Dual Subgradient Methods for Convex Problems. Mathematical Programming 120 (2009), 221­259. [19] Matia Okubo and Kensuke Okada. 2012. Psychological Statistics to Tell Your Story: E ect Size, Con dence Interval, and Power. Keiso Shobo. [20] Tetsuya Sakai. 2006. Evaluating Evaluation Metrics based on the Bootstrap. In Proceedings of ACM SIGIR 2006. 525­532. [21] Tetsuya Sakai. 2011. Evaluating Diversi ed Search Results Using Per-Intent Graded Relevance. In Proceedings of ACM SIGIR 2011. 1043­1052. [22] Tetsuya Sakai. 2014. Statistical Reform in Information Retrieval? SIGIR Forum 48, 1 (2014), 3­12. [23] Tetsuya Sakai. 2016. Statistical Signi cance, Power, and Sample Sizes: A Systematic Review of SIGIR and TOIS, 2006-2015. Proceedings of ACM SIGIR 2016 (2016), 5­14. [24] Tetsuya Sakai. 2016. Two Sample T-tests for IR Evaluation: Student or Welch?. In Proceedings of ACM SIGIR 2016. 1045­1048. [25] Tetsuya Sakai, Noriko Kando, Chuan-Jie Lin, Teruko Mitamura, Hideki Shima, Donghong Ji, Kuang-Hua Chen, and Eric Nyberg. 2008. Overview of the NTCIR-7 ACLIA IR4QA Task. In Proceedings of NTCIR-7. 77­114. [26] Lifeng Shang, Tetsuya Sakai, Zhengdong Lu, Hang Li, Ryuichiro Higashinaka, and Yusuke Miyao. 2016. Overview of the NTCIR-12 Short Text Conversation Task. In Proceedings of NTCIR-12. 473­484. [27] Mark D. Smucker, James Allan, and Ben Cartere e. 2007. A Comparison of Statistical Signi cance Tests for Information Retrieval Evaluation. In Proceedings of ACM CIKM 2007. 623­632. [28] Ruihua Song, Min Zhang, Tetsuya Sakai, Makoto P. Kato, Yiqun Liu, Miho Sugimoto, Qinglei Wang, and Naoki Orii. 2011. Overview of the NTCIR-9 INTENT Task. In Proceedings of NTCIR-9. 82­105. [29] Student. 1908. e Probable Error of a Mean. Biometrika 6, 1 (1908), 1­25. [30] Hideki Toyoda (Ed.). 2015. Fundamentals of Bayesian statistics: Practical Ge ing Started by Hamiltonian Monte Carlo Method (in Japanese). Asakura Shoten. [31] Hideki Toyoda. 2016. An Introduction to Statistical Data Analysis: Bayesian Statistics for `post p-value era' (in Japanese). Asakuha Shoten. [32] Ellen M. Voorhees. 2004. Overview of the TREC 2003 Robust Retrieval Track. In Proceedings of TREC 2003. [33] Ronald L. Wasserstein and Nicole A. Lazar. 2016. e ASA's Statement on P-values: Context, Process, and Purpose. e American Statistician (2016). [34] Dell Zhang, Jun Wang, Emine Yilmaz, Xiaoling Wang, and Yuxin Zhou. 2016. Bayesian Performance Comparison of Text Classi ers. In Proceedings of ACM SIGIR 2016. 15­24. [35] Stephen T. Ziliak and Deirdre N. McCloskey. 2008. e Cult of Statistical Signi cance: How the Standard Error Costs Us Jobs, Justice, and Lives. e University of Michigan Press.",1,ad,True
34,0,,False
,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian Nonparametric Approach,0,,False
Rishabh Mehrotra and Emine Yilmaz,0,,False
"University College London, London, United Kingdom  e Alan Turing Institute, British Library, London, United Kingdom",0,,False
"{r.mehrotra,e.yilmaz}@cs.ucl.ac.uk",0,,False
ABSTRACT,0,,False
"A signi cant amount of search queries originate from some real world information need or tasks [13]. In order to improve the search experience of the end users, it is important to have accurate representations of tasks. As a result, signi cant amount of research has been devoted to extracting proper representations of tasks in order to enable search systems to help users complete their tasks, as well as providing the end user with be er query suggestions [9], for be er recommendations [41], for satisfaction prediction [36] and for improved personalization in terms of tasks [24, 38]. Most existing task extraction methodologies focus on representing tasks as at structures. However, tasks o en tend to have multiple subtasks associated with them and a more naturalistic representation of tasks would be in terms of a hierarchy, where each task can be composed of multiple (sub)tasks. To this end, we propose an e cient Bayesian nonparametric model for extracting hierarchies of such tasks & subtasks. We evaluate our method based on real world query log data both through quantitative and crowdsourced experiments and highlight the importance of considering task/subtask hierarchies.",0,,False
KEYWORDS,0,,False
search tasks; bayesian non-parametrics; hierarchical model,0,,False
"ACM Reference format: Rishabh Mehrotra and Emine Yilmaz. 2017. Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian Nonparametric Approach. In Proceedings of SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080823",0,,False
1 INTRODUCTION,1,DUC,True
"e need for search o en arises from a person's need to achieve a goal, or a task such as booking travels, buying a house, etc., which would lead to search processes that are o en lengthy, iterative, and are characterized by distinct stages and shi ing goals. [13].",1,ad,True
"us, identifying and representing these tasks properly is highly important for devising search systems that can help end users complete their tasks. It has previously been shown that these task representations can be used to provide users with be er query",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080823",1,ad,True
"suggestions [9], o er improved personalization [24, 38], provide be er recommendations [41], help in satisfaction prediction [36] and search result re-ranking. Moreover, accurate representations of tasks could also be highly useful in aptly placing the user in the task-subtask space to contextually target the user in terms of be er recommendations and advertisements, developing task speci c ranking of documents, and developing task based evaluation metrics to model user satisfaction. Given the wide range of applications these tasks representations can be used for, signi cant amount of research has been devoted to task extraction and representation [12, 13, 15, 17, 21].",1,ad,True
"Task extraction is quite a challenging problem as search engines can be used to achieve very di erent tasks, and each task can be de ned at di erent levels of granularity. A major limitation in existing task-extraction methods lies in their treatment of search tasks as at structure-less clusters which inherently lack insights about the presence or demarcation of subtasks associated with individual search tasks. In reality, o en search tasks tend to be hierarchical in nature. For example, a search task like planning a wedding involves subtasks like searching for dresses, browsing di erent hairstyles, looking for invitation card templates, nding planners, among others. Each of these subtasks (1) could themselves be composed of multiple subtasks, and (2) would warrant issuing di erent queries by users to accomplish them. Hence, in order to obtain more accurate representations of tasks, new methodologies for constructing hierarchies of tasks are needed.",0,,False
"As part of the proposed research, we consider the challenge of extracting hierarchies of search tasks and their associated subtasks from a search log given just the log data without the need of any manual annotation of any sort. In a recent poster we showed that Bayesian nonparametrics have the potential to extract a hierarchical representation of tasks [25]; we extend this model further to form more accurate representations of tasks.",0,,False
"We present an e cient Bayesian nonparametric model for discovering hierarchies and propose a tree based nonparametric model to discover this rich hierarchical structure of tasks/subtasks embedded in search logs. Most existing hierarchical clustering techniques result in binary tree structures with each node decomposed into two child nodes. Given that a complex task could be composed of an arbitrary number of subtasks, these techniques cannot directly be used to construct accurate representations of tasks. In contrast, our model is capable of identifying task structures that can be composed of an arbitrary number of children. We make use of a number of evaluation methodologies to evaluate the e cacy of the proposed task extraction methodology, including quantitative and qualitative analyses along with crowdsourced judgment studies",0,,False
285,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
speci cally catered to evaluating the quality of the extracted task hierarchies. We contend that the techniques presented expand the scope for be er recommendations and search personalization and opens up new avenues for recommendations speci cally targeting users based on the tasks they involve in.,0,,False
2 RELATED WORK,0,,False
Web search logs provide explicit clues about the information seeking behavior of users and have been extensively studied to improve search experiences of users. We cover several areas of related work and discuss how our work relates to and extends prior work.,0,,False
2.1 Task Extraction,0,,False
"ere has been a large body of work focused on the problem of segmenting and organizing query logs into semantically coherent structures. Many such methods use the idea of a timeout cuto between queries, where two consecutive queries are considered as two di erent sessions or tasks if the time interval between them exceeds a certain threshold [6, 10, 33]. O en a 30-minute timeout is used to segment sessions. However, experimental results of these methods indicate that the timeouts are of limited utility in predicting whether two queries belong to the same task, and unsuitable for identifying session boundaries.",0,,False
"More recent studies suggest that users o en seek to complete multiple search tasks within a single search session [20, 23] with over 50% of search sessions having more than 2 tasks [23]. At the same time, certain tasks require signi cantly more e ort, time and sessions to complete with almost 60% of complex information gathering tasks continued across sessions [1, 22]. ere have been a empts to extract in-session tasks [13, 20, 35], and cross-session tasks [15, 37] from query sequences based on classi cation and clustering methods, as well as supporting users in accomplishing these tasks [9]. Prior work on identifying search-tasks focuses on task extraction from search sessions with the objective of segmenting a search session into disjoint sets of queries where each set represents a di erent task [12, 21].",0,,False
"Kotov et al. [15] and Agichtein et al. [1] studied the problem of cross-session task extraction via binary same-task classi cation, and found di erent types of tasks demonstrate di erent life spans. While such task extraction methods are good at linking a new query to an on-going task, o en these query links form long chains which result in a task cluster containing queries from many potentially di erent tasks. With the realization that sessions are not enough to represent tasks, recent work has started exploring cross-section task extraction, which o en results in complex non-homogeneous clusters of queries solving a number of related yet di erent tasks. Unfortunately, pairwise predictions alone cannot generate the partition of tasks e ciently and even with post-processing, the nal task partitions obtained are not expressive enough to demarcate subtasks [18]. Finally, authors in [17] model query temporal pa erns using a special class of point process called Hawkes processes, and combine topic model with Hawkes processes for simultaneously identifying and labeling search tasks.",0,,False
Jones et al. [13] was the rst work to consider the fact that there may be multiple subtasks associated with a user's information need and that these subtasks could be interleaved across di erent,0,,False
"sessions. However, their method only focuses on the queries submi ed by a single user and a empts to segment them based on whether they fall under the same information need. Hence, they only consider solving the task boundary identi cation and same task identi cation problem and cannot be used directly for task extraction. Our work alleviates the same user assumption and considers queries across di erent users for task extraction. Finally, in a recent poster [25], we proposed the idea of extracting task hierarchies and presented a basic tree extraction algorithm. Our current work extends the preliminary model in a number of dimensions including novel model of query a nities and task coherence based pruning strategy, which we observe gives substantial improvement in results. Unlike past work, we also present detailed derivation and evaluation of the extracted hierarchy and application on task extraction.",0,,False
2.2 Supporting Complex Search Tasks,0,,False
"ere has been a signi cant amount of work on task continuation assistance [1, 28], building task tours and trails [30, 34], query suggestions [2, 14, 27], predicting next search action [5] and notes taking when accomplishing complex tasks [8]. e quality of most of these methods depends on forming accurate representations of tasks, which is the problem we are addressing in this paper.",1,ad,True
2.3 Hierarchical Models,0,,False
"Rich hierarchies are common in data across many domains, hence quite a few hierarchical clustering techniques have been proposed.",0,,False
"e traditional methods for hierarchically clustering data are bo omup agglomerative algorithms. Probabilistic methods of learning hierarchies have also been proposed [3, 19] along with hierarchical clustering based methods [7, 11]. Most algorithms for hierarchical clustering construct binary tree representations of data, where leaf nodes correspond to data points and internal nodes correspond to clusters. ere are several limitations to existing hierarchy construction algorithms. e algorithms provide no guide to choosing the correct number of clusters or the level at which to prune the tree. It is o en di cult to know which distance metric to choose. Additionally and more importantly, restriction of the hypothesis space to binary trees alone is undesirable in many situations - indeed, a task can have any number of subtasks, not necessarily two. Past work has also considered constructing task-speci c taxonomies from document collections [39], browsing hierarchy construction [40], generating hierarchical summaries [16]. While most of these techniques work in supervised se ings on document collections, our work instead focused on short text queries and o ers an unsupervised method of constructing task hierarchies.",1,ad,True
"Finally, Bayesian Rose Trees and their extensions have been proposed [3, 4, 32] to model arbitrary branching trees. ese algorithms naively cast relationships between objects as binary (0-1) associations while the query-query relationships in general are much richer in content and structure.",0,,False
"We consider a number of such existing methods as baselines and the various advantages of the proposed approach is highlighted in the evaluation section wherein the proposed approach in addition to being more expressive, performs be er than state-of-the-art task extraction and hierarchical methods.",1,ad,True
286,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Symbol,0,,False
nT ab |c,0,,False
ch(T),0,,False
(T ),0,,False
p(Dm |Tm ) Tm f (Dm ) H(T ),0,,False
"f (Q) rqki , q j",0,,False
"Description number of children of tree T partition of set {a, b, c} into disjoint sets {a, b},{c} children of T partition of tree T likelihood of data Dm given the tree Tm mixing proportions of partition of tree T marginal probability of the data Dm set of all partitions of queries Q , lea es(T ) task a nity function for set of queries Q",0,,False
the k-th inter-query a nity between qi & qj,0,,False
Table 1: Table of symbols,0,,False
3 DEFINING SEARCH TASKS,0,,False
"Jones et al. [13] was one of the rst papers to point out the importance of task representations, where they de ned a search task as:",0,,False
De nition 3.1. A search task is an atomic information need resulting in one or more queries.,0,,False
"Ahmed et al. [9] later extended this de nition to a more generic one, which can also capture task structures that could possibly consist of related subtasks, each of which could be complex tasks themselves or may nally split down into simpler tasks or atomic informational needs. Following Ahmed et al. [9], a complex search task can then be de ned as:",0,,False
"De nition 3.2. A complex search task is a multi-aspect or a multistep information need consisting of a set of related subtasks, each of which might recursively be complex.",0,,False
"e de nition of complex tasks is much more generic, and captures all possible search tasks, that can be either complex or atomic (non-complex). roughout this paper we adopt the de nition provided in De nition 3.2 as the de nition for a search task.",1,ad,True
"Hence, by de nition a search task has a hierarchical nature, where each task can consist of an arbitrary number of, possibly complex subtasks. An e ective task extraction system should be capable of accurately identifying and representing such hierarchical structures.",0,,False
4 CONSTRUCTING TASK HIERARCHIES,0,,False
"While hierarchical clustering are widely used for clustering, they construct binary trees which may not be the best model to describe data's intrinsic structure in many applications, for example, the task-subtask structure in our case. To remedy this, multi-branch trees are developed. Currently there are few algorithms which generate multi-branch hierarchies. Blundel et al. [3, 4] adopt a simple, deterministic, agglomerative approach called BRTs (Bayesian Rose Trees) for constructing multi-branch hierarchies. In this work, we adapt BRT as a basic algorithm and extend it for constructing task hierarchies. We next describe the major steps of BRT approach.",1,ad,True
4.1 Bayesian Rose Trees,0,,False
"BRTs [3, 4] are based on a greedy probabilistic agglomerative approach to construct multi-branch hierarchies. In the beginning,",0,,False
Figure 1: e di erent ways of merging trees which allows us to obtain tree structures which best explain the tasksubtask structure.,0,,False
"each data point is regarded as a tree on its own: Ti ,"" {xi } where xi is the feature vector of i-th data. For each step, the algorithm selects two trees Ti and Tj and merges them into a new tree Tm . Unlike binary hierarchical clustering, BRT uses three possible merging operations, as shown in Figure 1:""",0,,False
"· Join: Tm ,"" Ti ,Tj , such that the tree Tm has two children now""",0,,False
"· Absorb: Tm ,"" children(Ti )  Tj , i.e., the children of one tree gets absorbed into the other tree forming an absorbed tree with >2 children""",0,,False
"· Collapse: Tm ,"" children(Ti )  children(Tj ), all the children of both the subtrees get combined together at the same level.""",0,,False
"Speci cally, in each step, the algorithm greedily nds two trees Ti and Tj to merge which maximize the ratio of probability:",0,,False
p(Dm |Tm ) p(Di |Ti )p(Dj |Tj ),0,,False
(1),0,,False
"where p(Dm |Tm ) is the likelihood of data Dm given the tree Tm , Dm is all the leaf data of Tm , and Dm , Di  Dj . e probability p(Dm |Tm ) is recursively de ned on the children of Tm :",0,,False
"p(Dm |Tm ) , Tm f (Dm ) + (1 - Tm )",0,,False
p(Di |Ti ) (2),0,,False
Ti ch(Tm ),0,,False
"where f (Dm ) is the marginal probability of the data Dm and Tm is the ""mixing proportion"". Intuitively, Tm is the prior probability that all the data in Tm is kept in one cluster instead of partitioned",1,ad,True
"into sub-trees. In BRT[4], Tm is de ned as:",0,,False
"Tm , 1 - (1 -  )nTm -1",0,,False
(3),0,,False
"where nTm is the number of children of Tm , and 0    1 is the hyperparameter to control the model. A larger  leads to coarser",1,ad,True
287,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
cosine edit Jac Term,0,,False
Min-edit-U Avg-edit-U Jac-U-min Jac-U-avg,0,,False
Same-U Same-S,0,,False
Embedding,0,,False
ery-Term Based A nity (r 1) cosine similarity between the term sets of the queries,0,,False
norm edit distance between query strings Jaccard coe between the term sets of the queries proportion of common terms between the queries,0,,False
URL Based A nity (r 2) Minimum edit distance between all URL pairs from the queries Average edit distance between all URL pairs from the queries Minimum Jaccard coe cient between all URL pairs from the queries Average Jaccard coe cient between all URL pairs from the queries,0,,False
Session/User Based A nity (r 3) if the two queries belong to the same user,1,Session,True
if the two queries belong to the same session Embedding Based A nity (r 4) cosine distance between embedding vectors of the two queries,0,,False
Table 2: ery- ery A nities.,0,,False
partitions and a smaller  leads to ner partitions. Table 1 provides an overview of notations & symbols used throughout the paper.,1,ad,True
4.2 Building Task Hierarchies,0,,False
"We next describe our task hierarchy construction approach built on top of Bayesian Rose Trees. A tree node in our se ing is comprised of a group of queries which potentially compose a search task, i.e. these are the set of queries that people tend to issue in order to achieve the task represented in the tree node.",0,,False
"We de ne the task-subtask hierarchy recursively: T is a task if either T contains all the queries at its node (an atomic search task) or if T splits into children trees as T ,"" {T1,T2, ...,TnT } where each of the children trees (Ti ) are disjoint set of queries corresponding to the nT subtasks associated with task T . is allows us to consider trees as a nested collection of sets of queries de ning our tasksubtask hierarchical relation.""",0,,False
"To form nested hierarchies, we rst need to model the query data. is corresponds to de ning the marginal distribution of the data f (Dm ) as de ned in Equation 2. e marginal distribution of the query data (f (Dm )) helps us encapsulate insights about task level interdependencies among queries, which aid in constructing be er task representations. e original BRT approach [4] assumes that the data can be modeled by a set of binary features that follow the Bernoulli distribution. In other words, features (that represent the relationship/similarities between data points) are not weighted and can only be binary. Binary (0/1) relationships are too simplistic to model inter-query relationships; as a result, this major assumption fails to capture the semantic relationships between queries and is not suited for modeling query-task relations. To this end, we propose a novel query a nity model and to alleviate the binary feature assumption imposed by BRT, we propose a conjugate model of query a nities, which we describe next.",0,,False
4.3 Conjugate Model of ery A nities,0,,False
A tree node in our se ing is comprised of a group of queries which potentially belong to the same search task. e likelihood of a tree should encapsulate information about the di erent relationships,0,,False
"which exists between queries. Our goal here is to make use of the rich information associated with queries and their result set available to compute the likelihood of a set of queries to belong to the same task. In order to do so, we propose a query a nity model which makes use of a number of di erent inter-query a nities to determine the tree likelihood function.",0,,False
We next describe the technique used to compute four broad categories of inter-query a nity and later describe the Gamma-Poisson conjugate model which makes use of these a nities to compute the marginal distribution of the data.,1,ad,True
"ery-term based A nity (r 1): Search queries catering to the same or similar informational needs tend to have similar query terms. We make use of this insight and capture query level a nities between a pair of queries. We make use of cosine similarity between the query term sets, the normalized edit distances between queries and the Jaccard Coe cient between query term sets.",0,,False
"URL-based A nity (r 2): Users tackling similar tasks tend to issue queries (possibly di erent) which return similar URLs, thus encoding the URL level similarity between pairs of queries into the query a nity model helps in capturing another task-speci c similarity between queries. Any query pair having high URL level similarity increase the possibility of the query pair originating from similar informational needs. We capture a number of URL-based signals including minimum and average edit distances between URL domains and jaccard coe cient between URLs.",0,,False
User/Session based A nity (r 3): It is o en the case that users issue related queries within a session so as to satisfy their informational need. We leverage this insight by making use of session level information (as a 0/1 binary feature) and user-level information (as a 0/1 binary feature) in our a nity model to identify queries issued in the same session and by the same user accordingly.,1,Session,True
288,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"ery Embedding based A nity (r 4): Word embeddings capture lexico-semantic regularities in language, such that words with similar syntactic and semantic properties are found to be close to each other in the embedding space. We leverage this insight and propose a query-query a nity metric based on such embeddings. We train a skip-gram word embeddings model where a query term is used as an input to a log-linear classi er with continuous projection layer and words within a certain window before and a er the words are predicted. To obtain a query's vector representation, we average the vector representations of each of its query terms and compute the cosine similarity between two queries' vector representations to quantify the embedding based a nity (r 4).",0,,False
"Table 2 summarizes all features considered to compute these a nities. Our goal is to capture information from all four a nities when de ning the likelihood of the tree. We assume that the global a nity among a group of queries can be decomposed into a product of independent terms, each of which represent one of the four a nities from the query-group. For each query group Q, we take the normalized sum of the a nities from all pairs of queries in the group Q to form each of the a nity component (rk , k,""1,2,3,4).""",0,,False
"Poisson models have been shown as e ective query generation models for information retrieval tasks [26]. While these a nities could be used with a lot of distributions, in the interest of computational e ciency and to avoid approximate solutions, our model will use a hierarchical Gamma-Poisson distribution to encode the query-query a nities. We incorporate the gamma-Poisson conjugate distribution in our model under the assumptions that the query a nities are discretized and for a group of queries Q, the a nities can be decomposed to a product of independent terms, each of which represents contributions from the four di erent a nity types. Finally, for a tree (Tm ) consisting of the data (Dm ), i.e. the set of queries Q, we de ne the marginal likelihood as:",1,corpora,True
"k ,4",0,,False
"f (Dm ) , f (Q) , p",0,,False
"rqki,qj |k , k (4)",0,,False
"k ,1 i 1··· |Q | j 1··· |Q |",0,,False
where k & k are respectively the shape parameter & the rate parameter of the four di erent a nities. Making use of the Poisson-,0,,False
"Gamma conjugacy, the probability term in the above product can",0,,False
be wri en as:,0,,False
"p(r |, ) ,"" p(r |)p(|, )d""",0,,False
(5),0,,False
r,0,,False
",",0,,False
( + r )  r ! ()  + 1,0,,False
1  +1,0,,False
(6),0,,False
"where  is the Poisson mean rate parameter which gets eliminated from computations because of the Gamma-Poisson conjugacy and where r ,  &  get replaced by a nity class speci c values.",0,,False
4.4 Task Coherence based Pruning,0,,False
"e search task extraction algorithm described above provides us a way of constructing a task hierarchy wherein as we go down the tree, nodes comprising of complex multi-aspect tasks split up to provide ner tasks which ideally should model user's ne grained information needs. One key problem with the hierarchy construction algorithm is the continuous spli ing of nodes which results",0,,False
"in singleton queries occupying the leave nodes. While spli ing of nodes which represent complex tasks is important, the nodes representing simple search task queries corresponding to atomic informational needs should not be further split into children nodes. Our goal in this section is to provide a way of quantifying the task complexity of a particular node so as to prevent spli ing up nodes representing atomic search task into further subsets of query nodes.",0,,False
"4.4.1 Identifying Atomic Tasks. We wish to identify nodes capturing search subtasks which represent atomic informational need. In order to do so, we introduce the notion of Task Coherence:",0,,False
De nition 4.1. Task Coherence is a measure indicating the atomicity of the information need associated with the task. It is captured by the semantic closeness of the queries associated with the task.,0,,False
"By measuring Task Coherence, we intend to capture the semantic variability of queries within this task in an a empt to identify how complex or atomic a task is. For example, a tree node corresponding to a complex task like planning a vacation would involve queries from varied informational needs including ights, hotels, getaways, etc; while a tree node corresponding to a ner task representing an atomic informational need like nding discount coupons would involve less varied queries - all of which would be about discount coupons. Traditional research in topic modelling has looked into automatic evaluation of topic coherence [29] via Pointwise Mutual Information. We leverage the same insights to capture task coherence.",1,ad,True
"4.4.2 Pointwise Mutual Information. PMI has been studied variously in the context of collocation extraction [31] and is one measure of the statistical independence of observing two words in close proximity. We wish to compute PMI scores for each node of the tree. A tree node in our discussion so far has been represented by a collection of search queries. We split queries into terms and obtain a set of terms corresponding to each node, and calculate a node's PMI scores using the node's set of query terms.",0,,False
"More speci cally, the PMI of a given pair of query terms (w1 & w2) is given by:",0,,False
"PMI (w1, w2) , lo",0,,False
"p(w1, w2) p(w1)p(w2)",0,,False
(7),0,,False
where the probabilities are determined from the empirical statistics of some full standard collection. We employ the AOL log query set for this and treat two query terms as co-occurring if both terms,0,,False
"occur in the same session. For a given task node (Q), we measure task coherence as the average of PMI scores for all pairs of the search terms associated with the task node:",0,,False
PMI,0,,False
- Score(Q),0,,False
",",0,,False
1 |w |,0,,False
"|w | i ,1",0,,False
|w |,0,,False
"PMI (wi , wj )",0,,False
"j ,1",0,,False
(8),0,,False
where |w | represents the total number of unique search terms associated with task node Q. e node's PMI-Score is used as the,0,,False
nal measure of task coherence for the task represented via the corresponding node.,0,,False
"4.4.3 Tree Pruning. We use the task coherence score associated with each node of the task hierarchy constructed, and prune lower level nodes of the tree to avoid aggressive node spli ing. e",0,,False
289,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"overall motivation here is to avoid spli ing nodes which represent simple search tasks associated with atomic informational needs. We scan through all levels of the search task hierarchy obtained by the algorithm described above and for each node compute its task coherence score. If the task coherence score exceeds a speci c threshold, it implies that all the queries in this particular node are aimed at solving the same or very similar informational need and hence, we prune o the sub-tree rooted at this particular node and ignore all further splits of this node.",0,,False
4.5 Algorithmic Overview,0,,False
"We summarize the overall algorithm to construct the hierarchy by outlining the steps. e problem is treated as one of greedy model selection: each tree T is a di erent model, and we wish to nd the model that best explains the search log data in terms of task-subtask structure.",0,,False
"Step 1: Forrest Initialization: e tree is built in a bo om-up greedy agglomerative fashion, start-",0,,False
"ing from a forest consisting of n (,""|Q |) trivial trees, each corresponding to exactly one vertex. e algorithm maintains a forest F of trees, the likelihood p(i) "","" p(Di |Ti ) of each tree Ti  F and the di erent query a nities. Each iteration then merges two of the trees in the forest. At each iteration, each vertex in the network is a leaf of exactly one tree in the forest. At each iteration a pair of trees in the forest F is chosen to be merged, resulting in forest F .""",0,,False
"Step 2: Merging Trees: At each iteration, the best potential merge, say of trees X and Y resulting in tree I, is picked o the heap. Binary trees do not t into representing search tasks since a task is likely to be composed of more than two subtasks. As a result, following [3] we consider three possible mergers of two trees Ti and Tj into Tm . Tm may be formed by joining Ti and Tj together using a new node, giving Tm ,"" {Ti ,Tj }. Alternatively Tm may be formed by absorbing Ti as a child of Tj , yielding Tm "","" {Tj } ch(Ti ), or vice-versa, Tm "", {Ti } ch(Tj ). We explain the di erent possible merge operations in Figure 1. We obtain arbitrary shaped sub-trees (without restricting to binary tress) which are be er at representing the varied task-subtask structures as observed in search logs with the structures themselves learnt from log data. Such expressive nature of our approach di erentiates it from traditional agglomerative clustering approaches which necessarily result in binary trees.",1,ad,True
"Step 3: Model Selection: Which pair of trees to merge, and how to merge these trees, is determined by considering which pair and type of merger yields the largest Bayes factor improvement over the current model. If the trees Ti and Tj are merged to form the tree M, then the Bayes factor score is:",0,,False
"SCORE(M; I,",0,,False
),0,,False
",",0,,False
p(DM |F ) p(DM |F ),0,,False
(9),0,,False
",",0,,False
p(DM |M) p(Di |Ti )p(Dj |Tj ),0,,False
(10),0,,False
"where p(Di |Ti ) and p(Dj |Tj ) are given by the dynamic programming equation mentioned above. A er a successful merge, the",0,,False
"statistics associated with the new tree are updated. Finally, potential mergers of the new tree with other trees in the forest are considered and added onto the heap.",1,ad,True
"e algorithm nishes when no further merging results in improvement in the Bayes Factor score. Note that the Bayes factor score is based on data local to the merge - i.e., by considering the probability of the connectivity data only among the leaves of the newly merged tree. is permits e cient local computations and makes the assumption that local community structure should depend only on the local connectivity structure.",0,,False
"Step 4: Tree Pruning: A er constructing the entire hierarchy, we perform the post-hoc tree pruning procedure described in Section 4.4 wherein we identify atomic task nodes via their task coherence estimates and prune all child nodes of the identi ed atomic nodes.",1,hoc,True
5 EXPERIMENTAL EVALUATION,0,,False
"We perform a number of experiments to evaluate the proposed task-subtask extraction method. First, we compare its performance with existing state-of-the-art task extraction systems on a manually labelled ground-truth dataset and report superior performance (5.1). Second, we perform a detailed crowd-sourced evaluation of extracted tasks and additionally validate the hierarchy using human labeled judgments (5.2). ird, we show a direct application of the extracted tasks by using the task hierarchy constructed for term prediction (5.3).",1,ad,True
"Parameter Setting: Unless stated otherwise, we made use of the best performing hyperparameters for the baselines as reported by the authors. e query a nities in the proposed approach were computed from the speci c query collection used in the dataset used for each of the three experiments reported below. While hyperparmeter optimization is beyond the scope of this work, we experimented with a range of the shape and inverse scale hyperparameters (, ) used for the Poison Gamma conjugate model and used the ones which performed best on the validation set for the search task identi cation results reported in the next section. Additionally, for the tree pruning threshold, we empirically found that a threshold of 0.8 gave the best performance on our toy hierarchies, and was used for all future experiments.",1,ad,True
5.1 Search Task Identi cation,0,,False
"To justify the e ectiveness of the proposed model in identifying search tasks in query logs, we employ a commonly used AOL data subset with search tasks annotated which is a standard test dataset for evaluating task extraction systems. We used the task extraction dataset as provided by Lucchese et al.[20]. e dataset comprises of a sample of 1000 user sessions for which human assessors were asked to manually identify the optimal task-based query sessions, thus producing a ground-truth that can be used for evaluating automatic task-based session discovery methods. For further details on the dataset and the dataset access links, readers are directed to Lucchese et al.[20].",1,ad,True
290,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
We compare our performance with a number of search task identi cation approaches:,0,,False
· Bestlink-SVM [37]: is method identi ed search task using a semi-supervised clustering model based on the latent structural SVM framework.,0,,False
"· QC-HTC/QC-WCC [20]: is series of methods viewed search task identi cation as the problem of best approximating the manually annotated tasks, and proposed both clustering and heuristic algorithms to solve the problem.",0,,False
"· LDA-Hawkes [17]: a probabilistic method for identifying and labeling search tasks that model query temporal patterns using a special class of point process called Hawkes processes, and combine topic model with Hawkes processes for simultaneously identifying and labeling search tasks.",0,,False
"· LDA Time-Window(TW): is model assumes queries belong to the same search task only if they lie in a xed or exible time window, and uses LDA to cluster queries into topics based on the query co-occurrences within the same time window. We tested time windows of various sizes and report results on the best performing window size.",0,,False
"5.1.1 Metrics. A commonly used evaluation metric for search task extraction is the pairwise F-measure computed based on pairwise precision/recall [13, 15] de ned as,",0,,False
ppair,0,,False
",",0,,False
i j (,0,,False
"(qi ), (qj )) ( ^(qi ),  ( ^(qi ), ^(qj ))",0,,False
^(qj )),0,,False
(11),0,,False
rpair,0,,False
",",0,,False
i j (,0,,False
"(qi ), (qj )) ( ^(qi ), ^(qj ))  ( (qi ), (qj ))",0,,False
(12),0,,False
where ppair evaluates how many pairs of queries predicted in the,0,,False
"same task, i.e.,  ( ^(qi ), ^(qj ) ,"" 1, are actually annotated as in the""",0,,False
"same task, i.e.,  ( (qi ), (qj )) , 1 and rpair evaluates how many",0,,False
pairs annotated as in the same task are recovered by the algorithm.,0,,False
"us, globally F-measure evaluates the extent to which a task con-",0,,False
tains only queries of a particular annotated task and all queries,0,,False
"of that task. Given ppair and rpair , the F-measure is computed",0,,False
as:F1,0,,False
",",0,,False
2×ppair ×rpair ppair +rpair,0,,False
.,0,,False
"5.1.2 Results & Discussion. Figure 2 compares the proposed model with alternative probabilistic models and state-of-the-art task identi cation approaches by F1 score. To make fair comparisons, we consider the last level of the pruned tree constructed as task clusters when computing pairwise precision/recall values. It is important to note that the labelled dataset has only at tasks extracted on a per user basis; as a result, this dataset is not ideal for making fair comparisons of the proposed hierarchy extraction method with baselines. Nevertheless, the proposed approach manages to outperform existing task extraction baselines while having much greater expressive powers and providing the subdivision of tasks into subtasks. LDA-TW performs the worst since its assumptions on query relationship within the same search task are too strong. e advantage over QC-HTC and QC-WCC demonstrates that appropriate usage of query a nity information can even better re ect the semantic relationship between queries, rather than exploiting it in some collaborative knowledge.",1,ad,True
F Score,0,,False
0.85 0.84 0.83 0.82 0.81,0,,False
0.8 0.79 0.78 0.77 0.76 0.75,0,,False
Proposed,0,,False
LDA-TW,0,,False
Bestlink-SVM,0,,False
LDA-Hawkes,0,,False
QC-HTC,0,,False
QC-WCC,0,,False
Figure 2: F1 score results on AOL tagged dataset,0,,False
5.2 Evaluating the Hierarchy,0,,False
"While there are no gold standard datasets for evaluating hierarchies of tasks, we performed crowd-sourced assessments to assess the performance of our hierarchy extraction method. We separately evaluated the coherence and quality of the extracted hierarchies via two di erent set of judgements obtained via crowdsourcing.",0,,False
"Evaluation Setup For the judgment study, we make use of the AOL search logs and randomly sampled entire query history of frequent users who had more than 1000 search queries. e AOL log is a very large and long-term collection consisting of about 20 million of Web queries issued by more than 657000 users over 3 months. We run the task extraction algorithms on the entire set of queries of the sampled users and collect judgments to assess the quality of the tasks extracted. Judgments were provided by over 40 judges who were recruited from the Amazon Mechanical Turk crowdsourcing service. We restricted annotators to those based in the US because the logs came from searchers based in the US. We also used hidden quality control questions to lter out poor-quality judges. e judges were provided with detailed guidelines describing the notion of search tasks and subtasks and were provided with several examples to help them be er understand the judgement task.",1,ad,True
"Evaluating Task Coherence In the rst study, we evaluated the quality of the tasks extracted by the task extraction algorithms. In an ideal task extraction system, all the queries belonging to the same task cluster should ideally belong to the same task and hence have be er task coherence. To this end, we evaluate the task coherence property of the tasks extracted by the di erent algorithms. For each of the baselines and the proposed algorithm, we select a task at random from the set of tasks extracted and randomly pick up two queries from the selected task. We then ask the human judges the following question:",0,,False
291,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Task Relatedness,0,,False
Proposed LDA-TW QC-WCC LDA-Hawkes QC-HTC,0,,False
Task Related,0,,False
72%*,0,,False
47%,0,,False
60%,0,,False
67%,0,,False
61%,0,,False
Somewhat Related 20%,0,,False
14%,0,,False
15%,0,,False
13%,0,,False
5%,0,,False
Unrelated,0,,False
10%,0,,False
23%,0,,False
25%,0,,False
20%,0,,False
34%,0,,False
Table 3: Performance on Task Relatedness. e results highlighted with * signify statistically signi cant di erence between the proposed approach and best performing baseline using  2 test with p  0.05.,0,,False
Subtask Validity,0,,False
Valid,0,,False
Proposed Jones BHCD BAC,0,,False
81%*,0,,False
69% 51% 49%,0,,False
Somewhat Valid,0,,False
8%,0,,False
19% 17% 21%,0,,False
Not Valid,0,,False
11%,0,,False
12% 32% 30%,0,,False
Subtask Usefulness,0,,False
Useful,0,,False
67%*,0,,False
52% 41% 43%,0,,False
Somewhat Useful,0,,False
8%,0,,False
17% 19% 20%,0,,False
Not Useful,0,,False
25%,0,,False
31% 40% 37%,0,,False
Table 4: Performance on Subtask Validity and Subtask Use-,0,,False
fulness. Results highlighted with * signify statistically sig-,0,,False
ni cant di erence between the proposed framework and best performing baseline using  2 test with p  0.05.,0,,False
"RQ1: Task Relatedness: Are the given pairs of queries related to the same task? e possible options include (i) Task Related, (ii) Somewhat Task Related and (iii) Unrelated.",0,,False
"e task relatedness score provides an estimate of how coherent tasks are. Indeed, a task cluster containing queries from di erent tasks would score less in Task Relatedness score since if the task cluster is impure, there is a greater chance that the 2 randomly picked queries belong to di erent tasks and hence get judged Unrelated.",0,,False
"Evaluating the hierarchy While there are no gold standard dataset to evaluate hierarchies, in our second crowd-sourced judgment study, we evaluate the quality of the hierarchy extracted. A valid task-subtask hierarchy would have the parent task representing a higher level task with its children tasks representing more focused subtasks, each of which help the user achieve the overall task identi ed by the parent task.",0,,False
"We evaluate the correctness of the hierarchy by validating parentchild task-subtask relationships. More speci cally, we randomly select a parent node from the hierarchy and then randomly select a child node from the set of its immediate child nodes. Given such parent-child node pairs, we randomly pick 5 queries from the parent node and randomly pick 2 queries from the child node. We then show the human judges these parent and child queries and ask the following questions:",0,,False
RQ2: Subtask Validity: Consider the set of queries representing the search task and the pair of queries representing the subtask.,0,,False
How valid is this subtask given the overall task?,0,,False
"e possible judge options include (i) Valid Subtask, (ii) Somewhat valid and (iii) Invalid. Answering this question helps us in analyzing the correctness of the parent-child task-subtask pairs.",0,,False
RQ3: Subtask Usefulness: Consider the set of queries representing the search task and the pair of queries representing the subtask. Is the subtask useful in completing the overall search task?,0,,False
"e possible judge options include (i) Useful, (ii) Somewhat Useful and (iii) Not Useful. is helps us in evaluating the usefulness of task-subtask pairs by nding the proportion of subtasks which help users in completing the overall task described by the parent node. Overall, the RQ2 and RQ3 help in evaluating the correctness and usefulness of the hierarchy extracted.",0,,False
"Baselines Since RQ1 evaluates task coherence without any notion of tasksubtask structure, we compare against the top performing baselines from the task extraction setup described in section 5.1. On the other hand, RQ2 & RQ3 help in answering questions about the quality of hierarchy constructed. To make fair comparisons while evaluating the hierarchies, we introduce additional hierarchy extraction baselines:",1,ad,True
· Jones Hierarchies [13]: A supervised learning approach for task boundary detection and same task identi cation. We train the classi er using the supervised Lucchese AOL task dataset and use it to extract tasks on the current dataset used in the judgment study.,0,,False
· BHCD [3]: A state-of-the-art bayesian hierarchical community detection algorithm based on stochastic blockmodels and makes use of Beta-Bernoulli conjugate priors to de ne a network. We build a network of queries and apply BHCD algorithm to extract hierarchies of query communities.,0,,False
· Bayesian Agglomerative Clustering (BAC) [11]: A standard agglomerative hierarchical clustering model based on Dirichlet process mixtures.,0,,False
"Results & Discussion For the rst judgment study, each HIT is composed of 20 query pairs per approach being judged for task relatedness. We had three judges work on every HIT. Overall, per method we obtained judgments for 60 query pairs to evaluate the performance on task-relatedness. From among the three judges judging each query-pair, we followed majority voting mechanism to nalize the label for the instance.",1,ad,True
292,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
avg no of query terms predicted per session,0,,False
1.2,0,,False
QC-WCC Proposed BHCD LDA-TW LDA-Hawkes,0,,False
1.1,0,,False
1,0,,False
0.9,0,,False
0.8,0,,False
0.7,0,,False
0.6,0,,False
0.5,0,,False
0.4,0,,False
0.3 50,0,,False
66,0,,False
75,0,,False
80,0,,False
90,0,,False
% age of user session data tested on,0,,False
Figure 3: Term Prediction performance,0,,False
"Table 3 presents the proportions of query pairs judged as related. About 72% of query pairs were judged task-related for the proposed approach with LDA-Hawkes performing second best with 67%. Task relatedness measures how pure the task clusters obtained are, a higher score indicates that the queries belonging to the same task are indeed used for solving the same search task. e overall results indicate that the tasks extracted by the proposed task-subtask extraction algorithm are indeed be er than those extracted by the baselines.",0,,False
"For the second judgment study used for evaluating the quality of the hierarchy, we show 10 pairs of parent-child questions in each HIT and ask the human annotators to judge the subtask validity and usefulness. Overall, per method we evaluate 300 such judgments resulting in over 1200 judgments and used maximum voting criterion from among the 3 judges to decide the nal label for each instance. Table 4 compares the performance of the proposed hierarchy extraction method against other hierarchical baselines.",0,,False
"e identi ed subtask was found useful in 67% cases with the best performing baseline being useful in 52% of judged instances. is highlights that the extracted hierarchy is indeed composed of be er subtasks which are found to be useful in completing the overall task depicted by the parent task. It is interesting to note that for BHCD and BAC baselines, most o en the subtasks were found to be invalid and not useful.",0,,False
"Since the same parent-child task-subtask was judged for validity and usefulness, it is expected that the proportion of task-subtasks judged useful would be less than the ones judged valid. Indeed, as can be seen from the Table 4, the relative proportions of taskssubtasks found useful is much less than those found valid.",0,,False
5.3 Term Prediction,0,,False
"In addition to task extraction and user study based evaluation, we chose to follow an indirect evaluation approach based on ery Term Prediction wherein given an initial set of queries, we predict future query terms the user may issue later in the session. is is in line with our goal of supporting users tackling complex search tasks since a task identi cation system which is capable of identifying",1,ad,True
good search tasks will indeed perform be er in predicting the set of future query terms.,0,,False
"To evaluate the performance of the proposed task extraction method, we primarily work with the TREC Session Track 2014 [? ] and AOL log data and constructed a new dataset consisting of user sessions from AOL logs concerned with Session Track queries.",1,TREC,True
e session track data consists of over 1200 sessions while AOL logs consists of 20M search queries issued by over 657K users. We,0,,False
"nd the intersection of queries between the Session Track data and AOL logs to identify user sessions in AOL data trying to achieve similar task objectives. e Session Track dataset consists of 60 di erent topics. For each of these 60 topics, we separately nd user sessions from the entire AOL logs which contain query overlaps with these topics. For each topic, we iterate through the entire AOL logs and select any user session which contains query overlap with the current topic. As a result, we obtain a total of 14030 user sessions which contain around 6.4M queries.",1,Session,True
"Given the initial queries from a user session and a set of tasks extracted from Session Track data, we leverage queries from the identi ed task to predict future query terms. For each Session Track topic, we construct a task hierarchy and use the constructed task hierarchy to predict future query terms in the associated user sessions. More speci cally, for each topic, we split each user session into two parts: (i) task matching and (ii) held-out evaluation part. We use queries from the task matching part of user sessions to obtain the right node in the task hierarchy from which we then recommend query terms. We pick the tree node which has the highest cosine similarity score based on all the query terms under consideration. We evaluate based on the absolute recall scores - the average number of recommended query terms which match with the query terms in the held-out evaluation part of user sessions.",1,Session,True
"We baseline against the top performing task extraction baselines from Section 5.1 as well as the top performing hierarchical algorithms from Section 5.2. To make fair comparisons, we consider nodes at the bo om most level of the pruned tree for task matching and term recommendation.",0,,False
Figure 3 compares the performance on term prediction against the considered baselines. We plot the average number of query terms predicted against the proportion of user session data used.,0,,False
e proposed method is able to be er predict future query terms than a standard task extraction baseline as well as a very recent hierarchy construction algorithm.,0,,False
6 CONCLUSION,0,,False
"Search task hierarchies provide us with a more naturalistic view of considering complex tasks and representing the embedded tasksubtask relationships. In this paper we rst motivated the need for considering hierarchies of search tasks & subtasks and presented a novel bayesian nonparametric approach which extracts such hierarchies. We introduced a conjugate query a nity model to capture query a nities to help in task extraction. Finally, we propose the idea of Task Coherence and use it to identify atomic tasks. Our experiments demonstrated the bene ts of considering search task hierarchies. Importantly, we were able to demonstrate competitive performance while at the same time outpu ing a richer and more",0,,False
293,0,,False
Session 3A: Search Interaction 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
expressive model of search tasks. is expands the scope for bet-,0,,False
"ter task recommendation, be er search personalization and opens",0,,False
up new avenues for recommendations speci cally targeting users,0,,False
based on the tasks they are involved in.,0,,False
REFERENCES,0,,False
"[1] Eugene Agichtein, Ryen W White, Susan T Dumais, and Paul N Bennet. 2012. Search, interrupted: understanding and predicting search task continuation. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval. ACM, 315­324.",0,,False
"[2] Ricardo Baeza-Yates, Carlos Hurtado, and Marcelo Mendoza. 2004. ery recommendation using query logs in search engines. In International Conference on Extending Database Technology. Springer, 588­596.",1,ad,True
[3] Charles Blundell and Yee Whye Teh. 2013. Bayesian hierarchical community discovery. In Advances in Neural Information Processing Systems. 1601­1609.,0,,False
"[4] Charles Blundell, Yee Whye Teh, and Katherine A Heller. 2012. Bayesian rose trees. arXiv preprint arXiv:1203.3468 (2012).",0,,False
"[5] Huanhuan Cao, Daxin Jiang, Jian Pei, Enhong Chen, and Hang Li. 2009. Towards context-aware search by learning a very large variable length hidden markov model from search logs. In Proceedings of the 18th international conference on World wide web. ACM, 191­200.",0,,False
"[6] Lara D Catledge and James E Pitkow. 1995. Characterizing browsing strategies in the World-Wide Web. Computer Networks and ISDN systems 27, 6 (1995), 1065­1073.",0,,False
"[7] Shui-Lung Chuang and Lee-Feng Chien. 2002. Towards automatic generation of query taxonomy: A hierarchical query clustering approach. In Data Mining, 2002. ICDM 2003. Proceedings. 2002 IEEE International Conference on. IEEE, 75­82.",0,,False
"[8] Debora Donato, Francesco Bonchi, Tom Chi, and Yoelle Maarek. 2010. Do you want to take notes?: identifying research missions in Yahoo! search pad. In Proceedings of the 19th international conference on World wide web. ACM, 321­ 330.",1,Yahoo,True
"[9] Ahmed Hassan Awadallah, Ryen W White, Patrick Pantel, Susan T Dumais, and Yi-Min Wang. 2014. Supporting complex search tasks. In Proceedings of the",1,ad,True
"23rd ACM International Conference on Conference on Information and Knowledge Management. ACM, 829­838. [10] Daqing He, Ay¸se Go¨ker, and David J Harper. 2002. Combining evidence for automatic web session identi cation. Information Processing & Management 38, 5 (2002), 727­742. [11] Katherine A Heller and Zoubin Ghahramani. 2005. Bayesian hierarchical clustering. In Proceedings of the 22nd international conference on Machine learning. ACM, 297­304. [12] Wen Hua, Yangqiu Song, Haixun Wang, and Xiaofang Zhou. 2013. Identifying users' topical tasks in web search. In Proceedings of the sixth ACM international conference on Web search and data mining. ACM, 93­102. [13] Rosie Jones and Kristina Lisa Klinkner. 2008. Beyond the session timeout: automatic hierarchical segmentation of search topics in query logs. In Proceedings of the 17th ACM conference on Information and knowledge management. ACM, 699­708. [14] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating query substitutions. In Proceedings of the 15th international conference on World Wide Web. ACM, 387­396. [15] Alexander Kotov, Paul N Benne , Ryen W White, Susan T Dumais, and Jaime Teevan. 2011. Modeling and analysis of cross-session search tasks. In Proceedings",1,ad,True
"of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 5­14. [16] Dawn J Lawrie and W Bruce Cro . 2003. Generating hierarchical summaries for web searches. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM, 457­458. [17] Liangda Li, Hongbo Deng, Anlei Dong, Yi Chang, and Hongyuan Zha. 2014. Identifying and labeling search tasks via query-based hawkes processes. In",0,,False
"Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 731­740. [18] Zhen Liao, Yang Song, Li-wei He, and Yalou Huang. 2012. Evaluating the e ectiveness of search task trails. In Proceedings of the 21st international conference on World Wide Web. ACM, 489­498. [19] Xueqing Liu, Yangqiu Song, Shixia Liu, and Haixun Wang. 2012. Automatic taxonomy construction from keywords. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 1433­ 1441. [20] Claudio Lucchese, Salvatore Orlando, Ra aele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2011. Identifying task-based sessions in search engine query logs. In Proceedings of the fourth ACM international conference on Web search and data mining. ACM, 277­286. [21] Claudio Lucchese, Salvatore Orlando, Ra aele Perego, Fabrizio Silvestri, and Gabriele Tolomei. 2013. Discovering tasks from search engine query logs. ACM Transactions on Information Systems (TOIS) 31, 3 (2013), 14.",0,,False
"[22] Bonnie Ma Kay and Carolyn Wa ers. 2008. Exploring multi-session web tasks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1187­1196.",0,,False
"[23] Rishabh Mehrotra, Prasanta Bha acharya, and Emine Yilmaz. 2016. Characterizing users' multi-tasking behavior in web search. In Proceedings of the 2016 ACM on Conference on Human Information Interaction and Retrieval. ACM, 297­300.",0,,False
"[24] Rishabh Mehrotra and Emine Yilmaz. 2015. Terms, topics & tasks: Enhanced user modelling for be er personalization. In Proceedings of the 2015 International Conference on e eory of Information Retrieval. ACM, 131­140.",0,,False
"[25] Rishabh Mehrotra and Emine Yilmaz. 2015. Towards hierarchies of search tasks & subtasks. In Proceedings of the 24th International Conference on World Wide Web. ACM, 73­74.",0,,False
"[26] Qiaozhu Mei, Hui Fang, and ChengXiang Zhai. 2007. A study of Poisson query generation model for information retrieval. In Proceedings of the 30th annual",0,,False
"international ACM SIGIR conference on Research and development in information retrieval. ACM, 319­326. [27] Qiaozhu Mei, Dengyong Zhou, and Kenneth Church. 2008. ery suggestion using hi ing time. In Proceedings of the 17th ACM conference on Information and knowledge management. ACM, 469­478. [28] Dan Morris, Meredith Ringel Morris, and Gina Venolia. 2008. SearchBar: a search-centric web history for task resumption and information re- nding. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. ACM, 1207­1216. [29] David Newman, Jey Han Lau, Karl Grieser, and Timothy Baldwin. 2010. Automatic evaluation of topic coherence. In Human Language Technologies: e",0,,False
"2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 100­108. [30] Brendan O'Connor, Michel Krieger, and David Ahn. 2010. TweetMotif: Exploratory Search and Topic Summarization for Twi er.. In ICWSM. 384­385. [31] Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language resources and evaluation 44, 1-2 (2010), 137­158. [32] Eran Segal and Daphne Koller. 2002. Probabilistic hierarchical clustering for biological data. In Proceedings of the sixth annual international conference on Computational biology. ACM, 273­280. [33] Craig Silverstein, Hannes Marais, Monika Henzinger, and Michael Moricz. 1999. Analysis of a very large web search engine query log. In ACm SIGIR Forum, Vol. 33. ACM, 6­12. [34] Adish Singla, Ryen White, and Je Huang. 2010. Studying trail nding algorithms for enhanced web search. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval. ACM, 443­450. [35] Amanda Spink, Sherry Koshman, Minsoo Park, Chris Field, and Bernard J Jansen. 2005. Multitasking web search on vivisimo. com. In Information Technology: Coding and Computing, 2005. ITCC 2005. International Conference on, Vol. 2. IEEE, 486­490. [36] Hongning Wang, Yang Song, Ming-Wei Chang, Xiaodong He, Ahmed Hassan, and Ryen W White. 2014. Modeling action-level satisfaction for search task satisfaction prediction. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 123­132. [37] Hongning Wang, Yang Song, Ming-Wei Chang, Xiaodong He, Ryen W White, and Wei Chu. 2013. Learning to extract cross-session search tasks. In Proceedings of the 22nd international conference on World Wide Web. ACM, 1353­1364. [38] Ryen W White, Wei Chu, Ahmed Hassan, Xiaodong He, Yang Song, and Hongning Wang. 2013. Enhancing personalized search by mining and modeling task behavior. In Proceedings of the 22nd international conference on World Wide Web. ACM, 1411­1420. [39] Hui Yang. 2012. Constructing task-speci c taxonomies for document collection browsing. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, 1278­1289. [40] Hui Yang. 2015. Browsing hierarchy construction by minimum evolution. ACM Transactions on Information Systems (TOIS) 33, 3 (2015), 13. [41] Yongfeng Zhang, Min Zhang, Yiqun Liu, Chua Tat-Seng, Yi Zhang, and Shaoping Ma. 2015. Task-based recommendation on a web-scale. In Big Data (Big Data), 2015 IEEE International Conference on. IEEE, 827­836.",1,Tweet,True
294,0,,False
,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Can Deep E ectiveness Metrics Be Evaluated Using Shallow Judgment Pools?,0,,False
Xiaolu Lu,0,,False
"RMIT University Melbourne, Australia",0,,False
Alistair Mo at,0,,False
"e University of Melbourne Melbourne, Australia",0,,False
J. Shane Culpepper,0,,False
"RMIT University Melbourne, Australia",0,,False
ABSTRACT,0,,False
"Increasing test collection sizes and limited judgment budgets create measurement challenges for IR batch evaluations, challenges that are greater when using deep e ectiveness metrics than when using shallow metrics, because of the increased likelihood that unjudged documents will be encountered. Here we study the problem of metric score adjustment, with the goal of accurately estimating system performance when using deep metrics and limited judgment sets, assuming that dynamic score adjustment is required per topic due to the variability in the number of relevant documents. We seek to induce system orderings that are as close as is possible to the orderings that would arise if full judgments were available.",1,ad,True
"Starting with depth-based pooling, and no prior knowledge of sampling probabilities, the rst phase of our two-stage process computes a background gain for each document based on rank-level statistics. e second stage then accounts for the distributional variance of relevant documents. We also exploit the frequency statistics of pooled relevant documents in order to determine a threshold for dynamically determining the set of topics to be adjusted. Taken together, our results show that: (i) be er score estimates can be achieved when compared to previous work; (ii) by se ing a global threshold, we are able to adapt our methods to di erent collections; and (iii) the proposed estimation methods reliably approximate the system orderings achieved when many more relevance judgments are available. We also consider pools generated by a two-strata sampling approach.",1,ad,True
KEYWORDS,0,,False
Test collection; relevance assessment; pooling; shallow judgments.,0,,False
1 INTRODUCTION,1,DUC,True
"Batch evaluations are performed by calculating a metric score based on a set of judged documents. Despite ve decades of success, this ""Cran eld/TREC"" paradigm also faces challenges. One of the key issues is that realistic collection sizes now greatly exceed the budget available to perform human judgments. ""Pooling-to-depth-d"" is one widely-used approach [25], in which documents in the union of the top-d lists returned from a set of contributing systems are judged, but other documents are not. e pooling depth d is ideally",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080793",1,ad,True
"determined by the needs of the e ectiveness metric to be used, but in reality is also constrained by the experimental budget. Although pooling has identi ed the majority of relevant documents in earlier collections [30], there is growing evidence that this is not true for the web collections that are now the norm [2, 11].",0,,False
"e uncertainty in e ectiveness measurement in large collections is the key emphasis of our work here, focusing on how to estimate evaluation scores when reduced judgment sets are used.",0,,False
"is is not a new problem, and a range of prediction mechanisms have been proposed [1, 22, 23, 27, 28], mainly focusing on predicting system orderings. We focus on prevailing pool-based test collection construction methods, as these best match our methodology, and on deep evaluation metrics, noting that pool depth has a lesser impact on shallow evaluation metrics such as ERR [6]. Alternative approaches using direct sampling exploit prior knowledge of the probability of each document being judged, and are applied during pool construction, on the assumption that all systems requiring measurement have been identi ed. But that process makes it di cult to infer scores for any new systems that get added later. On the other hand, pooling selects documents based on the assumption that top-ranked documents are both more likely to be relevant, and hence more in uential in computing e ectiveness scores. In this more general se ing there is no a priori knowledge of the system scores, and while that means that regression cannot be applied, new systems can be considered. We also argue that the decision to apply score adjustment should be done on a per topic basis. Robertson [17] notes that topics vary in terms of the number of potential relevant documents, and that this can have a signi cant impact on evaluation scores. Dynamically identifying when to perform score adjustment is thus a second challenge that must be considered.",1,ad,True
"e end objective of an evaluation goes beyond the metric scores, of course; in the end we wish to be able to compare and choose between systems, meaning that it is also important that the score estimations are concordant with the system orderings that would arise if full knowledge were available. Since the la er is measured according to a reference point which may not be known, there is no clear optimization goal, another complication that we address.",1,ad,True
"ese various considerations lead to two questions: Research estion 1: For each topic, how can we estimate the evaluation score of a system using a shallow pooling depth? Research estion 2: Can stable system rankings be achieved using the adjusted scores? In considering these two questions, we perform experiments using several di erent ad-hoc test collections and a range of modeled pool depths. Our results show that: (i) a two-stage optimization framework generates more accurate score estimations than previous approaches; (ii) topic-based adjustment thresholds identi ed using early TREC collections allow additional improvements in",1,ad,True
35,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"estimation accuracy; and (iii) the adjusted evaluation scores yield be er approximations of the ""true"" system rankings than do the unadjusted scores. In addition to standard pooling methods, we also consider two-strata sampling [24].",1,ad,True
2 RELATED WORK,0,,False
"Incomplete Judgments and Evaluation Bias. Two types of bias arise in batch evaluations: pooling depth bias [19] and system bias [20]. e rst is caused by the use of shallow pools, and the second by performance underestimation for systems that did not contribute to the pool. Both are a result of documents appearing in the ranking for which judgments are not available. e simplest response to unjudged documents is to stipulate that anything not examined in the pooling process is not relevant. Zobel [30] challenged this notion using a series of leave-one-out experiments, and showed for several early TREC collections that while it was likely there were indeed further relevant documents that had not been identi ed, system bias was nevertheless within acceptable levels. However, on more recent web collections, there is growing evidence that this situation may not be assumed [2, 11].",1,TREC,True
"Other responses to the issue of unjudged documents have been proposed. Buckley and Voorhees [3] describe BPref, which balances the rank positions of documents judged as non-relevant and relevant, and ignores unjudged documents. In a related approach, Sakai [18] considers condensed lists, which compute scores using a",0,,False
"ltered ranking containing only judged documents, and nds that standard metrics give higher discriminative ratios than achieved by BPref. However, the condensed list methodology has not been shown to be stable when comparing relative system orderings using Kendall's  or discrimination ratios [19, 20]. Score Estimation / Collection Construction. Documents without judgments are not distributed randomly in ranked result lists.",0,,False
"erefore, sample-based collection construction approaches have been suggested to support statistical inference [1, 22, 23, 27, 28]. Yilmaz and Aslam [27] present an inferred Average Precision (AP) metric that uses an expectation model, and can be coupled with a sampling process to select documents to be judged. eir InfAP metric uses uniform random sampling during collection construction. When compared with standard TREC-style pooling, the results produced by InfAP were strongly correlated with AP. However, this sampling process is random, and retrieval systems return documents in rank order, meaning that relevant documents are more likely to be returned at the top of the list if the system is e ective.",1,AP,True
"e use of non-random sampling has also been explored. Yilmaz et al. [28] extended their previous work, proposing metrics XInfAP and XInfNDCG, based on a strati ed sampling process. In contrast, Aslam et al. [1] consider the use of importance sampling for the same task, proposing statAP, which estimates the expectation of AP. e key di erence between InfAP and statAP is that statAP is designed to generate the optimal distribution estimates using all of the contributing systems. Voorhees [24] further examines the e ect of sampling methods on inferred metrics.",1,AP,True
"A recent study by Schnabel et al. [23] also used importance sampling, this time in conjunction with Discounted Cumulative Gain (DCG). e key idea in their approach was to use the probability of",0,,False
"relevance with respect to rank information when determining the sample distribution. ey provide an analysis on how to derive the optimal sampling distributions under di erent system comparison se ings [22]. Using the proposed framework, any metric can be reformulated in the form of expectations and be estimated directly from the sampling process. Mo at et al. [14] had earlier examined targeted pooling and document judgment order in conjunction with the Rank-Biased Precision (RBP) metric.",1,ad,True
"Score Estimation Based on Pooling Methods. Estimation in traditional pooling techniques has also received considerable a ention [4, 7, 9, 10, 16, 26]. Most existing techniques focus on adjusting the bias which exists between pooled and unpooled systems. Webber and Park [26] proposed two methods to perform score adjustment.",1,ad,True
"e rst uses an adjustment factor, which is computed from the contributing systems. Each contributing system has an error value assigned when it is le out of the training process, and the mean of those values is applied to any new system to be measured. e second approach requires a set of common topics with ""complete judgments"". A similar calculation is performed in order to obtain the adjustment factor, but restricted to the subset of common topics. To obtain additional adjustment accuracy, Webber and Park introduced randomization to build an unbiased estimator.",1,ad,True
"Recent work by Lipani et al. [9] using a precision metric outperformed the rst method of Webber and Park. eir ""anti-Precision"" measurement is similar in spirit to the residual computed by RBP [15]. Lipani et al. [9] compute adjustment factors using the leaveone-run out methodology, and then improve their previous approach by computing an average distribution [10].",1,ad,True
"e closest work to our current approach is that of Ravana and Mo at [16]. ey focus on pooling depth bias, proposing three methods to estimate the e ect of unjudged documents, using the residual that can be computed for weighted-precision metrics [15].",0,,False
"eir rst method uses a background estimation based on a static scaling factor; the second assumes that the percentage of relevant but unjudged documents can be derived directly from the known score component; and the third uses a parametric combination of the rst two. Lu et al. [12] subsequently de ne the same problem in terms of the anticipated e ectiveness gain as a function of ranking depth. Based on di erent assumptions derived from the underlying gain distributions, they propose several alternatives, and compare the estimates achieved. ey empirically show that relatively simple models can be used to estimate gain values for unjudged documents.",0,,False
"An approach due to Bu¨ cher et al. [4] directly predicts the relevance of unjudged documents, using two types of classi ers trained with the existing pool to predict the relevance of unjudged document in a new system. Although the e ectiveness of the classi er is low, their results show that classi cation does help maintain similar system orderings when measured via Kendall's  . Jayasinghe et al. [7] take a similar approach, and show that reliably predicting document relevance is o en di cult.",0,,False
3 PRELIMINARIES AND BASELINES,0,,False
"Pools. Figure 1 shows the construction of a pool for one topic, with sj,i (on the le ) corresponding to the j th document in the run for system Si , and with the corresponding documents (on the",0,,False
36,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
T,0,,False
S1 s11,0,,False
S2 s12,0,,False
s21 s31 s41 s51 ... sd1 ...,0,,False
s22 s32 s42 s52 ... sd2 ...,0,,False
sk1 sk2,0,,False
S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,0,,False
...,0,,False
... ... ... ... ... ... ... ... ...,0,,False
Sn s1n s2n s3n s4n s5n ... sdn ... skn,0,,False
Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,0,,False
... sdn+1,0,,False
... skn+1,0,,False
Rank,0,,False
1 2 3 4 .5. . .d. .,0,,False
k,0,,False
S1,0,,False
D1 D3 D2 D7 D6 ... D10 ... D49,0,,False
Complete Set J,0,,False
J,0,,False
S2 S3 . . . Sn Sn+1 ,0,,False
D2 D1 D6 D5 D3 ... D6 ...,0,,False
D3 D7 D2 D8 D5 ... D1 ...,0,,False
... ... ... ... ... ... ... ...,0,,False
D3 D4 D7 D2 D1 ... D5 ...,0,,False
D4 D2 D8 D1 D9 ... D3 ...,0,,False
D50 D30 . . . D18 D6,0,,False
System Matrix: S,0,,False
M@k: M1 M2 M3 . . . Mn Mn+1,0,,False
Figure 1: Pooling process for a topic T . e le matrix is a rankbased representation; the right one shows the equivalent document,0,,False
"identi ers. e two boxes indicate two possible sets of pooled documents, the larger to depth d, and the smaller to some depth d < d. e metric M is evaluated at some depth k, where k may or may not be less than or equal to d or d .",0,,False
"right), each potentially retrieved by multiple systems at di erent rank positions. Hence, a document D can also be represented by its rank-position information, D, (pD,1, pD,2, . . . , pD,n ) , in which pD,i is the rank returned for D by contributing system Si . Metric evaluation to depth d for systems S1 to Sn requires that the documents in the set , {D | minni,""1 pD,i  d } be judged. at is, both matrices can be further mapped to a matrix of relevance Rd×n in which rj,i is a relevance, or gain, value.""",0,,False
"If there is insu cient judgment volume available, a shallower pool might be formed, with documents D for which minni,""1 pD,i > d not judged, and elements in Rd×n le without values. Unknown relevance labels may also arise for a new system Sn+1, regardless of the pooling and evaluation depths. In this framework, the rst of""",0,,False
the two research questions proposed in Section 1 can be split into,0,,False
"two aspects: (1a) for each topic, how do we estimate the scores of",0,,False
a system using a set of shallow pooled judgments; and (1b) which,0,,False
topics may assume that unjudged documents are not relevant and,0,,False
which ones should not.,0,,False
One method for dealing with missing data is to compute expected,0,,False
"gains as a function of retrieval rank [12]. However, modeling rele-",0,,False
vance as a function of rank only considers the LHS representation,0,,False
"in Figure 1, and ignores that documents can have multiple ranks.",0,,False
Addressing that limitation is a key part of our work here.,0,,False
"Metric Residuals. Suppose that for some topic T , a set of docu-",0,,False
ments results from pooling to depth d (Figure 1). Consider the,0,,False
"ranked list returned by some system Si ,"" (s1,i , s2,i , . . . , sk,i ) and let rj,i represent the gain of the document at rank j, normally (but not necessarily) a value in [0, 1]. e e ectiveness Mi of Si when computed to depth k by a weighted-precision metric M is:""",0,,False
k,0,,False
"Mi ,"" M@k (Si , ) "",",0,,False
"rj,i · WM (j) ,",0,,False
(1),0,,False
"j ,1",0,,False
"sj,i ",0,,False
"where WM (j) is the weight assigned by the metric at depth j, with",0,,False
" j ,1",0,,False
WM,0,,False
(j,0,,False
),0,,False
",",0,,False
"1 [13, 15]; and where the restriction sj,i",0,,False
is,0,,False
"required to ensure that only de ned values of rj,i are included.",0,,False
"A corresponding residual i can then be computed, quantifying the",0,,False
metric weighting associated with the unjudged documents [15]:,0,,False
k,0,,False
"i ,",0,,False
rmax · WM (j ) +,0,,False
"rmax · WM (j ) ,",0,,False
(2),0,,False
"j ,""1 sj,i""",0,,False
"j,k +1",0,,False
"where rmax is the maximum possible gain. Either term might be zero, depending on whether Si contributed to the pool, on the relationship between the evaluation depth k and the pooling depth d, and on whether WM (j) ,"" 0 when j > k, as occurs with truncated""",0,,False
metrics.,0,,False
ere is a three-way tension between metric depth (quanti ed as,0,,False
the expected point reached in the ranking in the corresponding user,0,,False
"model [13]); accuracy of measurement, captured by the residual; and the cost | | of performing the judgments. For example, in RBP",0,,False
"the tail residual (the second component in Equation 2) is given by pk , and if p ,"" 0.5, k  10 is su cient. Similar calculations apply""",0,,False
"for ERR [6]. But in either case, the rst term of Equation 2 might be",0,,False
"non-zero for new runs. Furthermore, even the tail residuals might become large for deeper metrics, for example, RBP with p , 0.95.",0,,False
"Truncated (that is, non-in nite) metrics such as Scaled DCG at",0,,False
"depth 100, SDCG@100, also require deep pools if the residual is to be moderately bounded. e same requirement must apply by",0,,False
implication to other deep metrics such as Average Precision.,0,,False
"Problem De nition. Consider a set of n contributing systems {S1, S2, . . . , Sn }. For one topic T , let d be a pooling depth at which",0,,False
it is believed that a majority of the relevant documents occurring,0,,False
in the runs of those systems have been identi ed. We refer to this,0,,False
"set of judgments as the ""complete set"". Let d < d be a shallower pooling depth, with judgments forming an incomplete set  . Given a weighted precision metric M, the e ectiveness score of Si evaluated using M and to depth d is denoted as Mi ,"" M@d (Si , ), with a residual of i . Similarly, an estimated metric score based on judgments to depth d < d, is denoted as M^ i "","" Ed (M@d (Si , )) where Ed (·) is an estimation function for the same metric at depth d. Following Lu et al. [12], the estimation error i is then de ned as:""",0,,False
i,0,,False
",",0,,False
Mi 0,0,,False
-,0,,False
M^ i,0,,False
"if M^ i < Mi ,",0,,False
"if Mi  M^ i  Mi + i ,",0,,False
(3),0,,False
M^ i - (Mi + i ) if M^ i > Mi + i .,0,,False
"is de nition respects the residual range, and only gives non-zero",0,,False
"values if the estimated e ectiveness falls outside the score range arising from the use of at depth d. e challenge is to develop a method Ed (·) that estimates the depth-d e ectiveness score of a contributing system based on a subset of the judgments, and minimizes the average value of i .",0,,False
"In the experiments in Section 6 we report the RMS aggregate of the i values computed, across systems and topics; and, as a ""percentage accurate"", the fraction of those values that are zero.",0,,False
"Lower-Bound Estimation. A simple approach is to take Ed (x ) ,"" x, that is M^ i "","" Mi , where Mi is the score for system Si when evaluated using , and assert that documents outside do not""",0,,False
alter the score. Taking unjudged documents to be not relevant is the,0,,False
"normal default in batch evaluation, and is a valid estimator. But the",0,,False
"estimation quality depends on the breadth of the pool, and whether",1,ad,True
a majority of relevant documents have been identi ed. When there,0,,False
37,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
J,0,,False
Fitted Gain,0,,False
Rank ,0,,False
1,0,,False
S1 s11,0,,False
2 3 4 .5. . .d. .,0,,False
s21 s31 s41 s51 ... sd1 ...,0,,False
k,0,,False
sk1,0,,False
S2 s12 s22 s32 s42 s52 ... sd2 ... sk2,0,,False
S3 s13 s23 s33 s43 s53 ... sd3 ... sk3,0,,False
...,0,,False
... ... ... ... ... ... ... ... ...,0,,False
Sn s1n s2n s3n s4n s5n ... sdn ... skn,0,,False
Sn+1 s1n+1 s2n+1 s3n+1 s4n+1 s5n+1,0,,False
... sdn+1,0,,False
... skn+1,0,,False
Gain Vector:,0,,False
"ni,1 r1i/n ni,1 r2i/n ni,1 r3i/n ni,1 r4i/n",0,,False
"ni,1 r5i/n",0,,False
g,0,,False
 Fitting,0,,False
G1(k),0,,False
g011 g021 g031 g041 g051 ... g0d1 ...,0,,False
g0k1,0,,False
G2(k),0,,False
g012 g022 g032 g042 g052 ... g0d2 ... g0k2,0,,False
G3(k),0,,False
g013 g023 g033 g043 g053 ... g0d3 ... g0k3,0,,False
...,0,,False
... ... ... ... ... ... ...,0,,False
...,0,,False
Figure 2: Overview of rank-based estimation for a single topic. e judgments are used to infer an observed gain vector g; each,0,,False
of a set of m functions G (k ) is then ed to g.,0,,False
"are still many unjudged relevant documents, this estimator results in underestimation of system performance. Reasonably good rank correlation between the estimates and the true score over a set of systems can be obtained, but there is no guarantee that the performance of each of the systems has been accurately measured.",0,,False
"Interpolative Estimation. A second baseline is provided by the RM interpolative estimator proposed by Ravana and Mo at [16], who scale the metric score across the residual, assuming that unjudged documents are relevant at the same rate as judged ones:",0,,False
"M^ i , Mi /(1 - i ) .",0,,False
(4),0,,False
"A collection-based background probability is used when i , 1. is estimator assumes that gain is accrued at the same rate across",0,,False
"all of the documents retrieved by the system, both judged and unjudged. Although more robust than the LB estimator, it does not allow the likelihood of relevance to decrease as the pool is extended from d to d.",0,,False
"Rank-Based Estimators. Lu et al. [12] introduce rank-based estimation, illustrated in Figure 2. e judgments are used to estimate expected gain as a function of rank on a per-topic basis. ose rank-based fractional gain predictions are then used for unjudged documents ­ interpolated at depths up to d , and extrapolated from d to the metric evaluation depth k. Lu et al. explore alternative estimation functions, measuring the prediction error using the mechanism described in Equation 3, and nd that while improvements are possible, no single estimator works consistently well across all collections and topics. Rank-based estimation also has the drawback of ignoring the fact that a document can appear at di erent ranks for di erent systems; and hence potentially assigns di erent gain estimates to the same document in di erent runs, a representational issue that usually leads to a biased estimation [29]. As a further drawback, an entire row in the system matrix S (see Figure 1) must be judged in order to compute the expected gain, limiting construction methods to pooling or sampling by rank, and possibly excluding strati ed sampling processes.",1,ad,True
Sampling-Based Estimation. Other sampling approaches can also be used when forming the judgment set. Voorhees [24] de-,0,,False
"scribes a two-strata sampling method, which consists of shallow pooled judgments to some depth d , and then 10% random sampling to depth d in a second set s . ese judgments allow computation of inferred recall-based metrics, and also inferred versions",0,,False
"of weighted-precision metrics, with M^ i for system i calculated as:",0,,False
k,0,,False
k,0,,False
"M^ i ,",0,,False
"rj,i · WM (j) +  ·",0,,False
"rj,i · WM (j) ,",0,,False
(5),0,,False
"j ,""1 sj,i """,0,,False
"j ,""1 sj,i  s""",0,,False
where,0,,False
-1,0,,False
k,0,,False
k,0,,False
",",0,,False
WM (j) ·,0,,False
WM (j),0,,False
"j ,""1 sj,i""",0,,False
"j ,""1 sj,i  s""",0,,False
and where the second term in Equation 5 estimates the total gain,0,,False
"associated with documents contained in the second stratum. Here,  is the interpolation estimator. Note that Equation 5 only adapts the RM method for sample based judgments.",1,ad,True
4 TWO-STAGE ESTIMATION,0,,False
"Overview of the Framework. To compute score estimates, we propose a two-stage framework, guided by a uni ed optimization goal, and built on a set of m  1 per-topic rank-level estimators.",0,,False
e overall structure of this mechanism is described in Algorithm 1.,0,,False
"We omit the process of obtaining rank-level estimations, discussed",0,,False
"brie y in the previous section, and in detail by Lu et al. [12]. at is, we assume as our starting point here that m di erent rank-based estimators have been generated, each derived from the judged documents D  , and that values for a set of gain functions have been computed, with 0j, the gain associated with an unjudged document that appears in the j th position of any of the n system",0,,False
Algorithm 1 Estimation Framework,0,,False
Input: System matrix Sk×n ; partial relevance judgments with 2[D] the gain associated with document D for D  and,0,,False
unde ned otherwise; and a set of m rank-level background,0,,False
"gain estimates, 0j, for 1  j  k and 1   m, with 0,  0j, | 1  j  k and 1[D]  1 [D] | 1   m .",0,,False
"Output: Values 2[D], gain estimates for the documents D  ",0,,False
1: for D  \ do 2[D]  0,0,,False
2:   C,0,,False
"CV( , S) // compute coe cient of variance",0,,False
3: if  >  then,0,,False
// adjust only if  exceeds threshold,1,ad,True
4: for  1 to m do,0,,False
5:,0,,False
for D  do 1 [D]  0,0,,False
6:,0,,False
"w1opt  arg min L h1 ( 0, , w1) | D ",0,,False
"w1 [0, 1]n",0,,False
7:,0,,False
for D  do,0,,False
8:,0,,False
"1 [D]  h1 0, , w1opt",0,,False
9:,0,,False
end for,0,,False
10: end for,0,,False
"11: w2opt  arg min L h2 ( 1[D], w2) | D  w2 [0, 1]m",0,,False
12: // get nal per-document estimation,0,,False
13: for D  \ do,0,,False
14:,0,,False
"2[D]  h2 1[D], w2opt",0,,False
15: end for,0,,False
16: end if,0,,False
17: return 2,0,,False
38,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"rankings, as predicted by the th of the m di erent estimators.",0,,False
"Prior to forming the new combined estimates, we rst compute the coe cient of covariance  from the judgment set [5], in order to",0,,False
"determine whether to use a background ""unjudged are not relevant"" predictor. Estimation is computed by steps 4 to 15, with h1 (·) and h2 (·) two parametric combining functions, in which the parameters are obtained by minimizing a loss function L(·). We discuss the details of Algorithm 1, including the rationale behind the use of  ,",0,,False
in the next few paragraphs.,0,,False
"First Stage. As noted already, one problem with rank-based estimators is the potential inconsistency across runs of the gain a ached",1,ad,True
"to any particular document. As always, we assume that one topic is being addressed; the goal in the rst stage is to aggregate the m × n per-document estimates across the m estimators and n systems into a smaller set of m estimates per document. at is, the m rank-level estimators are treated separately at rst, in the loop at step 4, to obtain a consistent background gain for each document D for each model, denoted 1 [D]. is is done via a combining function h1 (·) that maps a vector to a single value. Several options for h1 (·) are available, with the choice between them depending on assumptions",1,ad,True
about system quality and the degree to which the systems are cor-,0,,False
"related. For simplicity, we assume that the systems are independent and that they vary in quality. erefore, for each document D, a natural combining function is to compute a weighted average, with h1 (step 6) parameterized by an n-element weighting vector w1 that is speci c to the th estimator:",0,,False
n,0,,False
"D  , h1 ( 0, , w1 | D) ,",0,,False
"0pD,i, · w1i",0,,False
"i ,1",0,,False
n,0,,False
(6),0,,False
"with w1i ,"" 1 and w1i  [0, 1] ,""",0,,False
"i ,1",0,,False
"and where 0pD,i, applies the th estimator to the rank at which document D appears in the i th of the n runs. One practical issue is that a document may not be retrieved by all systems in their top-k ranked lists, where k is the maximum depth of lists returned. In such cases the rank-based background gain of that document for that system is set to the modeled gain at depth k.",0,,False
"To compute a value for w1, we consider the aggregation process as an optimization problem, where the goal is to minimize the",0,,False
estimation error. e estimation error has two granularities: (i) the,0,,False
total error of system e ectiveness score calculated using ; and,0,,False
(ii) the total error of estimating the background gain of the labeled,0,,False
"documents. From either perspective, we can formalize an objective function L and use it at step 6 of Algorithm 1. Consider the rst case, with the system matrix as shown in Figure 1. We de ne L as:",0,,False
2,0,,False
nk,0,,False
"La (·) ,",0,,False
"WM (j) · (h1 (·, w1 | sj,i ) - rj,i ) , (7)",0,,False
"i,1 j,""1 sj,i """,0,,False
"where WM (j) · h1 (·, w1 | sj,i ) is the estimated background gain for document sj,i  , and rj,i is the known relevance value of that same document. As noted, La minimizes the overall estimation",0,,False
error of the evaluation scores for the set of systems.,0,,False
"e second alternative uses the document-position representation (pD,1, pD,2, . . . , pD,n ):",0,,False
"Lb (·) ,",0,,False
D,0,,False
n 2,0,,False
"WM (pD,i ) · (h1 (·, w1 | D) - rD ) , (8)",0,,False
"i ,1",0,,False
"in which rD is the relevance value of document D and is included only once per document, rather than once per document-rank.",0,,False
"When compared to Equation 7, which considers estimation er-",0,,False
"rors at the system level, this loss function is focused at the per-",0,,False
"document level, seeking to minimize the overall estimation error",0,,False
for the weighted gain of each document. Either Equation 7 or Equa-,0,,False
"tion 8 can be used at step 6 of Algorithm 1, with the combination function h1 (·) and constraints de ned in Equation 6. e result is the computation of a sequence of w1opt vectors, one for each of the m di erent rank-level estimators.",0,,False
Second Stage. Multiple ing models have been proposed because di erent assumptions about the underlying relevance distributions,0,,False
"across all systems are plausible, with a risk that no single model",0,,False
"covers the true hypothesis space. Indeed, the limited non-random",0,,False
training data means that we may su er from a high variance if only,0,,False
"one model is considered. erefore, a ""meta"" optimizer is also used,",0,,False
"combining results from the rst stage, as described by steps 11",0,,False
"to 15. A weighted average is used in this role too, considering each document D, together with the estimated background gains generated by the m previous computations, 1[D]. at combiner, h2 (·) (step 11), is de ned via the m-vector w2 as:",0,,False
m,0,,False
"D  , h2 1[D], w2 ,"" 1 [D] · w2 ,""",0,,False
",1",0,,False
m,0,,False
(9),0,,False
"with w2 ,"" 1 and w2  [0, 1] .""",0,,False
",1",0,,False
"Both La and Lb can be used in step 11, but may not necessarily be the same. Note that the m-vector w2opt, computed at step 11 as the minimizing value for Equation 9, provides an indication of",0,,False
the importance of individual optimizers from the previous stage.,0,,False
Previous work has shown that the expected error of combining loss,0,,False
functions is smaller than the average error on results output by,0,,False
each optimizer in isolation from the rst stage [29].,0,,False
"Computing the Coe cient of Variance. e score adjustment and estimation process has been presented on a per-topic basis, with an underlying assumption that a shallow judgment pool cannot identify a majority of the relevant documents. However, some topics may have only a small number of relevant documents, and a shallow depth may be su cient to identify most of them, with adjustment unnecessary. Only if deeper pooling would identify further relevant documents can score adjustment have an e ect on system e ectiveness scores. Hence a coe cient of variance [5] is computed for the relevant documents in the shallow pool and used as an indicator, as described in step 2.",1,ad,True
"Pooling is treated as a sampling with replacement process, with an unknown probability of a relevant document being sampled. Although the nal judgment process considers only the documents in the pool, a document returned by multiple systems has a selection",0,,False
39,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
frequency. e intuition behind  is to make use of that frequency,0,,False
information to describe the sample coverage of relevant documents.,0,,False
"Consider the system matrix S in Figure 1 and a pooling depth d . Each document sj,i (1  j  d , 1  i  n) has a multiplicity in Sd ×n ; we then group them by that frequency count. Let fi be the number of relevant documents appearing i times in Sd ×n , R ,"" i fi the number of relevant documents, and C "", i i · fi be",0,,False
"the total occurrence count of relevant documents. For example, if",0,,False
"only D8 and D1 in Figure 1 are identi ed as relevant documents, then we have f1 ,"" 1, f3 "","" 1, and R "", 2 and C ,"" 4. Based on these elements, the coe cient of variance,  , is estimated via [5]:""",0,,False
2,0,,False
",",0,,False
max,0,,False
|R | 1-f1 /C,0,,False
C,0,,False
i i · (i - 1) · · (C - 1),0,,False
fi,0,,False
-,0,,False
"1,",0,,False
 0,0,,False
.,0,,False
(10),0,,False
"When  ,"" 0, the probability of sampling a relevant document follows a uniform distribution; and when  is high, the distribution""",0,,False
"is skewed, and it is likely that more relevant documents exist due to",0,,False
"the low sampling coverage. Based on this, we have two hypotheses:",0,,False
Hypothesis 1:  tends to decrease as pooling depth increases.,0,,False
"Hypothesis 2: ere is a threshold  , where if  <  , then the existence of unjudged documents will only negligibly a ect the estimate of the system performance, and they can be ignored.",0,,False
"e rst hypothesis is easy to understand, because increasing the pooling depth increases the sample size, and increases the sampling coverage. e second hypothesis assumes that the score can be dynamically adjusted based on a threshold. If this is correct, then a point at which the total estimation error is minimal can be observed. Otherwise, we must conclude that a shallow pool is not su cient for nding relevant documents, and adjustment must be applied to all topics in all evaluations.",1,ad,True
"Discussion. We have described two possible realizations of loss functions, and one option for the combining functions h1 (·) and h2 (·). More sophisticated mechanisms are also possible. For example, the relationship between systems might be leveraged to derive a be er h1 (·) and its constraint.",0,,False
Note also that although our process targets the problem of es-,0,,False
"timating the e ectiveness of runs that contribute to the pool, it is",0,,False
possible to apply the same process to estimate the score of a new,0,,False
"system, and is demonstrated empirically in Section 6. Section 6",0,,False
also shows that the framework can be applied to the judgments,0,,False
"constructed using two-strata sampling [24], incorporating the addi-",1,corpora,True
tional information provided in the second stratum.,0,,False
5 COMPARING SYSTEM RANKINGS,0,,False
"Section 3 already de ned i , a score-based evaluation criterion. But we are also interested in comparing system orderings as a measure",1,ad,True
of usefulness of an estimation regime.,0,,False
"Kendall's Distance. is distance metric is widely used to measure the similarity between ranked lists, and counts the number of inverted pairs between two n-item orderings. Let i, j represent the pairwise relationship between the e ectiveness metric means S¯i and S¯j of systems Si and Sj over a set of topics according to one measurement regime, with i, j  {-1, 0, 1} indicating that S¯i < S¯j ,",0,,False
"that S¯i ,"" S¯j , and that S¯i > S¯j , respectively; and let i, j be the corresponding values for a second measurement regime and the system""",0,,False
"means that it induces, for example, using pooling to a di erent depth. en Kendall's normalized  distance is the number of pairs 1  i < j  n in which i, j · i, j < 0, divided by n(n - 1)/2 to bring it into the range 0    1, with 0 meaning ""identical"".",0,,False
Statistical Weighting. Paired t-tests are o en used to quantify the,0,,False
"strength of the relationship between two systems, and the values i, j and i, j might be thought of as being continuous rather than ternary. Kumar and Vassilvitskii [8] describe a weighted  distance",0,,False
"that counts the strength of each discordant pair, focusing solely on cases where i, j · i, j < 0. In practice we are not only interested in the discordant pairs, but also in pairs that are deemed to be",0,,False
signi cantly di erent according to one of the measurement regimes,0,,False
"but not the other, even if their overall relationship is concordant. Suppose that S¯i > S¯j according to the rst measurement, and",0,,False
"that a paired one-tail statistical test across topics yields pi, j . Values of pi, j near zero indicate a signi cant superiority of Si over Sj ; values close to 0.5 indicate that it is by chance. If we de ne",0,,False
"i, j ,"" 00..05 - pi, j""",0,,False
"pj,i - 0.5 ",0,,False
"if S¯i > S¯j if S¯i ,"" S¯j if S¯i < S¯j ,""",0,,False
"then -0.5  i, j  0.5 is a real-valued quantity that captures",0,,False
both the direction and strength of the relationship between the two,0,,False
systems according to the rst measurement regime. We compute,0,,False
"i, j similarly using a second measurement approach, and then, to compare the alternative rankings of n systems induced by the two",0,,False
"measurement techniques, calculate",0,,False
"dist ,",0,,False
" · |i, j - i, j | ,",0,,False
(11),0,,False
1i <j n,0,,False
"where   0 is an additional scaling factor. For example, if  ,"" |i, j | then the strength of the relationship between Si and Sj according to the rst measurement regime also in uences the measured distance. Overall, if dist  0, then the two measurement regimes agree in terms of both the direction of each pairwise relationship Si versus Sj , and also its strength. If dist is substantially greater then zero, then the two measurement regimes give rise to many system""",1,ad,True
pairs for which there are non-trivial disagreements (including in,0,,False
"both discords and in concords) over the strength of the measured relationships. Compared with Kendall's  distance, Equation 11 operates over continuous values, which makes it both resistant",0,,False
"to inconclusive changes in rank position, and also sensitive to di erences in which the direction of the relationship between Si and Sj stays the same, but the statistical strength varies markedly.",0,,False
6 EXPERIMENTS,0,,False
"e experiments described in this section include: (i) a post-hoc analysis for testing two hypotheses proposed in Section 4, and se ing the threshold  ; (ii) evaluating prediction accuracy using RMSE and Acc% as de ned by Lu et al. [12]; (iii) system ordering stability evaluation using the distance metric de ned in Equation 11 with  ,"" 1, and using normalized  distance; and (iv) a case study covering the ClueWeb 2010 (CW10) task.""",1,hoc,True
40,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Dataset d |S |,0,,False
Judgments per topic,0,,False
2-strata,0,,False
"d , 10 d , 20 d , 30 d , d",0,,False
TREC5 100 76 272 (13) 512 (10) 747 (8) 2298 (4) ­,1,TREC,True
TREC9 100 59 174 (11) 322 (8) 462 (7) 1382 (4) 294 (7),1,TREC,True
TREC10 100 54 182 (13) 335 (10) 480 (9) 1402 (5) 303 (9),1,TREC,True
Rob04 100 42 75 (25) 139 (18) 206 (15) 710 (7) 134 (15),0,,False
TB04 80 33 164 (31) 313 (27) 453 (25) 1121 (19) 270 (25),1,TB,True
TB05 100 34 111 (41) 202 (36) 291 (33) 878 (25) 187 (33),1,TB,True
TB06 50 39 141 (31) 270 (26) 394 (23) 633 (19) ­,1,TB,True
CW10 20 21 98 (30) ­,1,CW,True
­ 187 (28) ­,0,,False
Table 1: Datasets used: d is the original pooling depth and provides the reference point for metric scores; d is a notional pooling depth used our experimentation; and |S | is the number of contributing,0,,False
runs. Only Adhoc Task runs are used. e middle four column pairs,1,hoc,True
"show the number of judgments averaged across topics at each pooling depth d , and the percentage of relevant documents. e last",0,,False
"column shows the statistics when using two-strata sampling [24],",0,,False
averaged over topics and over ten random iterations.,0,,False
"Experimental Setup. e collections and con guration parameters used in our experiments are shown in Table 1. We also measured a range of behavior using the TREC7 and TREC8 collections, but do not include them here because those two collections were used as part of the post-hoc analysis and parameter se ing. Scripts are available to reproduce all of the various results given here1.",1,TREC,True
"Pooling to di erent depths is simulated using the identi ed contributing systems, and the average number of judgments required per topic at di erent pool depths is also shown in Table 1, together with the corresponding percentage of documents identi ed as being relevant. In the experiments measuring rank stability, we also examine the two-strata sampling method described by Voorhees, and averages over ten runs for this randomized approach are included in the table. For the Robust04 task the last 49 topics are used, and judged to a depth of 100; for other tasks, we use all of the original topic set and judgments. Our goal in collection selection was to capture as much variety as possible. TREC5 and Rob04 use the NewsWire document collection, TREC9 and TREC10 use WT10G, a small web collection, and TB04/05/06 use the GOV2 web collection.",1,Robust,True
"e ClueWeb 2010 task (CW10) uses the largest web collection but also has fewer contributing systems and a shallow pool depth. It is representative of newer collections, which are large, and have more uncertainty associated with the judgment coverage ­ the core issue which motivated our investigation. We show results for this dataset as a practical application of our work, noting that a pooling depth of d , 20 cannot provide a ground truth for a deep metric [11].",1,ClueWeb,True
"We use RBP with p ,"" 0.95 for training and for all testing, as a representative weighted-precision metric. RBP supports graded relevance (needed to make use of the estimated background gains we generate); allows residuals to be computed; and with p "","" 0.95 gives similar system orderings to AP and NDCG [15]. e estimated background gain of each document generated via training using RBP0.95 can also be used to compute other weighted-precision measures, such as the truncated metric SDCG@k when k > d.""",1,ad,True
1h ps://github.com/xiaolul/opt est,0,,False
"We consider ve methods for predicting e ectiveness scores,",0,,False
"three of which are baselines. e rst baseline is the lower bound, LB, which assumes unjudged documents are not relevant; the second is the interpolative estimator of Ravana and Mo at [16] (Equation 4), denoted RM; and the third is the linear model Lin. that is the best of the rank-based approaches described by Lu et al. [12]. ey",0,,False
"are compared to the loss functions de ned in Equations 7 and 8, denoted La and Lb respectively, with the same loss function used in both stages, and aggregation via Equations 6 and 9.",0,,False
"We use Linear, Zipf and Discrete Weibull models as initial rankbased estimators [12], and hence have m ,"" 3. Two experiments explore rank stability, categorized by how the judgment set is con-""",0,,False
structed: (i) pooling based judgments; and (ii) two-strata sampling,0,,False
based judgments. Rank stability is measured using the approaches,0,,False
discussed in Section 5. e same baselines are used in the rst rank,0,,False
"stability evaluation. However, for the sample-based judgments, we consider the metrics InfRBP (p ,"" 0.95) de ned by Equation 5, and Yilmaz and Aslam's InfAP [27] as baselines. roughout the ex-""",1,AP,True
"periments, the system scores (plus residuals) and system orderings",0,,False
"computed using the same metric, but evaluated at the full pool depth (that is, at k ,"" d), are taken as the """"gold standard"""". e truncated metric AP@d is computed as described by Sakai [21]. Setting  . We rst test the two hypotheses in Section 4, with  in Equation 10 normalized by the number of systems. Average (over topics)  values are plo ed against pool depth in the le -hand plot in Figure 3, showing that  decreases as the pooling depth increases.""",1,AP,True
"is is as expected, since the increasing pooling depth results in",0,,False
"a more complete judgment set. Among the plo ed datasets the TB06 collection has the largest average  , and corresponds to a high relevance rate (Table 1). TREC5 is a relatively complete test collection, and hence has the lowest  among the datasets plo ed.",1,TB,True
"e center pane in Figure 3 shows the distribution of  across topics for d ,"" 10. Although  is usually low for TREC5, there are still some topics that have high values. e same pa ern is""",1,TREC,True
"also observable for Rob04 and TREC10. Based on our hypothesis,",1,TREC,True
"this observation indicates that, on earlier TREC collections, not all topics necessarily require score adjustment even at d , 10.",1,TREC,True
"To set  we use the earlier datasets TREC5, TREC7 and TREC8 and perform a post-hoc analysis, noting that the majority of relevant",1,TREC,True
"documents have been identi ed in these collections, and hence",0,,False
that the computed RMSE should be close to the true error. e,0,,False
"right-hand pane in Figure 3 shows TREC5 outcomes, with three rank-based models plo ed. Weibull (Wei.) may be an overestimate due to the shaping parameter, and Linear (Lin.) tends to provide low estimates due to the monotonically decreasing nature of the",1,TREC,True
"model [12]. At rst, neither of the score estimation methods works be er than the lower bound LB, but as  increases, fewer topics need to be estimated, and when  ,"" 0.018, both estimation methods outperform LB. Similar cross-overs occur for TREC7 and TREC8.""",1,TREC,True
"Prediction Accuracy. We then employed  ,"" 0.018 for the other datasets, obtaining the results shown in Table 2. When  "","" 0 the Lb method outperforms all three baselines (LB, RM and Lin.) in terms of RMSE and Acc%, while the La approach has a higher RMSE than Lb and on earlier datasets (TREC9, TREC10) is slightly worse than the LB and Lin. baselines. at is, the loss function La provides poorer coverage of the true hypothesis space than does Lb . e RM""",1,TREC,True
41,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Mean  (10-3) Percentage (%) RMSE (10-3),0,,False
30,0,,False
Dataset,0,,False
TREC5 Rob04,1,TREC,True
TREC10 TB06,1,TREC,True
25,0,,False
20,0,,False
15,0,,False
10 10 20 30 40 50 60 70 80 90 100,0,,False
Pooling Depth,0,,False
50 Datasets,0,,False
Rob04 TREC10,1,TREC,True
40,0,,False
TB06 TREC5,1,TB,True
30,0,,False
20,0,,False
10,0,,False
0,0,,False
0 4 8 12 16 20 24 28 32,0,,False
 (10-3),0,,False
50,0,,False
Method,0,,False
LB Lin Wei,0,,False
45,0,,False
40,0,,False
0,0,,False
4,0,,False
8 12 16 20 24,0,,False
 (10-3),0,,False
"Figure 3: Le :  relative to pooling depth d . Middle: distribution of  per topic when d ,"" 10. Right: impact of the threshold on the training set TREC5, with d "", 10.",1,TREC,True
Dataset d,0,,False
LB,0,,False
Lin. RM,0,,False
La,0,,False
Lb,0,,False
" , 0  , 0.018",0,,False
" , 0  , 0.018",0,,False
" , 0  , 0.018",0,,False
10 TREC9 20,1,TREC,True
30,0,,False
0.031 (45) 0.012 (59) 0.006 (67),0,,False
0.056 (22) 0.025 (32) 0.012 (45),0,,False
0.037 (31) 0.031 (44) 0.013 (51) 0.012 (60) 0.006 (66) 0.006 (68),0,,False
0.038 (26) 0.030 (44) 0.013 (42) 0.012 (58) 0.005 (63) 0.006 (68),0,,False
0.031 (41) 0.031 (46) 0.010 (62) 0.012 (61) 0.005 (71) 0.006 (68),0,,False
10 TREC10 20,1,TREC,True
30,0,,False
0.038 (39) 0.016 (53) 0.007 (64),0,,False
0.064 (16) 0.030 (25) 0.015 (37),0,,False
0.034 (25) 0.033 (31) 0.015 (45) 0.014 (51) 0.007 (61) 0.007 (63),0,,False
0.036 (13) 0.031 (27) 0.016 (36) 0.014 (50) 0.007 (55) 0.007 (62),0,,False
0.027 (34) 0.028 (38) 0.012 (53) 0.012 (55) 0.006 (66) 0.006 (66),0,,False
10 Rob04 20,0,,False
30,0,,False
0.046 (21) 0.020 (34) 0.008 (49),0,,False
0.088 (5) 0.040 (9) 0.020 (14),0,,False
0.043 (21) 0.039 (20) 0.015 (33) 0.016 (34) 0.007 (49) 0.007 (52),0,,False
0.045 (9) 0.039 (17) 0.016 (26) 0.015 (32) 0.007 (47) 0.006 (53),0,,False
0.039 (20) 0.035 (20) 0.013 (38) 0.016 (35) 0.005 (59) 0.006 (55),0,,False
10 0.117 (14) 0.082 (14) 0.082 (15) 0.087 (14) 0.072 (15) 0.077 (15) 0.073 (16) 0.077 (16) TB04 20 0.053 (21) 0.039 (23) 0.039 (25) 0.041 (25) 0.035 (28) 0.037 (28) 0.033 (32) 0.036 (31),1,TB,True
30 0.026 (26) 0.020 (39) 0.018 (39) 0.019 (38) 0.015 (45) 0.016 (44) 0.015 (44) 0.016 (43),0,,False
10 0.125 (6),0,,False
0.080 (5),0,,False
0.085 (7) 0.085 (7),0,,False
0.070 (6) 0.070 (8),0,,False
0.067 (7) 0.067 (9),0,,False
TB05 20 0.056 (10) 0.041 (10) 0.039 (13) 0.039 (13) 0.034 (14) 0.034 (14) 0.033 (18) 0.033 (19),1,TB,True
30 0.028 (16) 0.022 (18) 0.021 (24) 0.021 (24) 0.018 (24) 0.018 (24) 0.017 (29) 0.017 (29),0,,False
10 0.089 (24) 0.065 (43) 0.059 (43) 0.059 (43) 0.047 (55) 0.047 (55) 0.053 (51) 0.053 (50) TB06 20 0.033 (40) 0.023 (68) 0.021 (66) 0.021 (66) 0.013 (81) 0.013 (81) 0.017 (73) 0.017 (73),1,TB,True
30 0.013 (58) 0.007 (87) 0.006 (85) 0.006 (85) 0.003 (94) 0.003 (94) 0.005 (89) 0.005 (89),0,,False
"Table 2: RMSE and Acc% scores for RBP0.95 for all estimation methods, with d the depth of the reduced pool, and the reference depth d of",0,,False
each dataset as listed in Table 1. Bold numbers are the lowest RMSE and highest Acc% for that collection at that depth.,0,,False
"approach performs poorly on all of the earlier datasets, for which the assumption that unjudged documents are equivalent to judged ones is inappropriate. On the larger collections such as TB04/05/06, the gain decreases at a slower rate, making the assumptions in RM more appropriate. e LB approach has similar issues, seen in the TB04/05/06 collections. However, for TB06, smaller RMSE (and larger Acc%) values are achieved when compared to the other collections. is is because the reference depth d ,"" 50 is smaller, resulting in larger residuals. As shown in Figure 3, some of the topics may not necessarily require a score adjustment process, especially in the earlier test collections. is explains why the LB estimator works well on those collections. As expected, applying a threshold  improves the estimation for both La and for the Lin. model, on TREC9, TREC10 and Rob04 test collections. Unsurprisingly, on TB04/05/06, only minor score changes are observed when  "","" 0.018 is used, because the computed  values are larger than""",1,TB,True
"the threshold, indicating low coverage of the relevant documents",0,,False
identi ed. e only unexpected observation occurs on the TB04,1,TB,True
"test collection, where the threshold falsely identi es Topic 734 as",0,,False
"having a ""su cient"" sampling of relevant documents, but around",0,,False
"48% in the nal judged set are relevant, which increases the RMSE",0,,False
"value. Table 3 shows the results for a leave-one-group-out experiment at d , 10 (with  ,"" 0), demonstrating the applicability of the framework in adjusting for both system and pooling depth bias.""",1,ad,True
"System Ordering Stability on Pooling-Based Judgments. e system orderings derived from the score estimates when compared against the orderings at the reference depth of k ,"" d are shown in Figure 4. Kendall's  correlation was also computed, but the closely-related  distance is used here since it has a strictly positive value. In the rst row, when normalized  distance is measured, the estimation framework gives orderings close to the reference""",0,,False
42,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
12,0,,False
Method,0,,False
12,0,,False
Method,0,,False
12,0,,False
Method,0,,False
LB RM Lb,0,,False
LB RM Lb,0,,False
LB RM Lb,0,,False
Lin. La,0,,False
Lin. La,0,,False
Lin. La,0,,False
8,0,,False
8,0,,False
8,0,,False
 Distance (10-2),0,,False
 Distance (10-2),0,,False
 Distance (10-2),0,,False
4,0,,False
4,0,,False
4,0,,False
0,0,,False
10,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
60,0,,False
Pooling Depth,0,,False
0,0,,False
10,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
60,0,,False
Pooling Depth,0,,False
0,0,,False
10,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
60,0,,False
Pooling Depth,0,,False
Distance,0,,False
20 16 12,0,,False
8 4 0,0,,False
10,0,,False
Method,0,,False
LB RM Lb Lin. La,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
60,0,,False
Pooling Depth,0,,False
Distance,0,,False
20 16 12,0,,False
8 4 0,0,,False
10,0,,False
Method,0,,False
LB RM Lb Lin. La,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
60,0,,False
Pooling Depth,0,,False
Distance,0,,False
20 16 12,0,,False
8 4 0,0,,False
10,0,,False
Method,0,,False
LB RM Lb Lin. La,0,,False
20,0,,False
30,0,,False
40,0,,False
50,0,,False
60,0,,False
Pooling Depth,0,,False
"Figure 4: System ordering comparisons (RBP0.95) for ve estimators. e rst row uses normalized  distance; the second row uses dist (Equation 11). e columns (from le ) show Rob04, TB04, and TB05, with reference lists using LB at d ,"" 100, d "", 80 and d ,"" 100, respectively.""",1,TB,True
Dataset LB,0,,False
RM,0,,False
Lin.,0,,False
La,0,,False
Lb,0,,False
Rob04 TB04 TB05 TB06,1,TB,True
0.060 (19) 0.126 (4) 0.068 (11) 0.060 (9) 0.050 (19) 0.181 (11) 0.202 (11) 0.131 (9) 0.117 (11) 0.119 (11) 0.170 (6) 0.141 (5) 0.125 (4) 0.110 (4) 0.110 (5) 0.125 (22) 0.185 (32) 0.112 (35) 0.090 (48) 0.086 (46),0,,False
"Table 3: RMSE and Acc% for leave-out-one-group experiments with d ,"" 10 throughout, averages across groups assuming that""",0,,False
each group in turn is omi ed from pool construction (RBP0.95).,0,,False
"ordering across a range of nominal pool depths d . e RM approach performs well on TB04/05, agreeing with the results in Table 2. However, as noted above,  is sensitive to swaps that might be inconclusive. e bo om row of Figure 4 shows the dist measure of Equation 11. Overall, there are situations in which LB performs poorly, and situations in which RM performs poorly. e Lin., La , and Lb methods consistently provide the highest agreements.",1,TB,True
"We also carried out paired t-tests and calculated the discrimination ratio for a signi cance level p ,"" 0.05, and compared against the original discrimination ratios. e Lin., La , and Lb estimation methods used with all have only a small e ect on discrimination""",0,,False
ratio when compared to the use of LB and .,0,,False
System Ordering Stability on Sample-Based Judgments. We,0,,False
also show the applicability of our methods on the judgment set,0,,False
"constructed using a two-strata sampling method [24], which has",0,,False
been empirically shown to assist when computing inferred metrics.,0,,False
"On this set of judgments we compute InfAP using trec eval, and InfRBP as de ned in Equation 5. Figure 6 shows that La , Lb and InfRBP give rise to stable system orderings, with normalized ",1,AP,True
Estimation Methods,0,,False
"d',20 CW10",1,CW,True
"d',20 TB05",1,TB,True
"d',60 TB05",1,TB,True
UB,0,,False
RM,0,,False
dist,0,,False
0.125,0,,False
Lb,0,,False
0.100,0,,False
0.075,0,,False
La,0,,False
0.050,0,,False
0.025,0,,False
Lin,0,,False
0.000,0,,False
LB,0,,False
LB Lin La Lb RM UB LB Lin La Lb RM UB LB Lin La Lb RM UB,0,,False
Estimation Methods,0,,False
Figure 5: Normalized  distance between system orderings gen-,0,,False
erated by di erent estimation methods based on a pool of depth,0,,False
"d ,"" 20, and on TB05 based on pool depths of d "", 20 and d , 60.",1,TB,True
"distance scores below 0.05 across all collections. When dist is measured, La outperforms InfRBP on all collections but TREC9, while Lb outperforms InfRBP except on TB05. e slightly worse outcome for La on TREC9 is a consequence of the increase in the number of signi cantly di erent system pairs. Note the more",1,TREC,True
variable outcomes generated when InfAP is used as the metric,1,AP,True
driving the system orderings.,0,,False
"Predictions in ClueWeb. As a nal test of our approach, we examine the CW10 collection. It has a shallow pool depth (d ,"" 20), meaning that validation is not possible, as there is no deep-pool reference ordering. Instead, we compute the normalized  distance between each pair of estimation methods, and simply record how""",1,ClueWeb,True
"much the rankings di er, as shown in Figure 5. e UB estimator assumes that all unjudged documents are relevant. As a reference",0,,False
43,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Method,0,,False
100,0,,False
10,0,,False
InfAP La,1,AP,True
InfRBP Lb,0,,False
 Distance (10-2) Distance,0,,False
40 4,0,,False
2,0,,False
1,0,,False
TREC9,1,TREC,True
TREC10,1,TREC,True
Rob04,0,,False
Collection,0,,False
TB04,1,TB,True
TB05,1,TB,True
20,0,,False
Method,0,,False
10,0,,False
InfAP La,1,AP,True
InfRBP Lb,0,,False
TREC9,1,TREC,True
TREC10,1,TREC,True
Rob04,0,,False
Collection,0,,False
TB04,1,TB,True
TB05,1,TB,True
"Figure 6: System ordering comparisons on a two-strata sampled judgment set, repeated ten times. Judgments are to depth d ,"" 10, plus a 10% random sample of remaining documents to depth 100 to form the second stratum. Note the logarithmic vertical scales.""",0,,False
"point, we also compute the same values for TB05, at two depths, d , 20 and d ,"" 60. At the la er depth all estimation approaches tend to agree with each other. On TB05, all of the estimation results, including UB, tend to agree on the system ordering. However, on CW10, there is clear uncertainty, con rming that d "", 60 is a more robust pool depth for TB05 than is d , 20 on either TB05 or CW10 when seeking to apply RBP0.95 as an evaluation metric. Great caution should be exercised when the d , 20 CW10 judgments are used for anything other than shallow metrics.",1,TB,True
7 CONCLUSIONS,0,,False
"We have presented new methods to improve system comparisons in batch IR evaluation, with the key idea being to predict a gain value for each unjudged document. We show that estimation is a viable technique to predict scores for deep evaluation metrics when limited judgments are available, including the case when the judgments are obtained using strati ed sampling rather than pooling. One important aspect of our approach is to make decisions on when to adjust topics, instead of treating all topics equally.",1,ad,True
"A secondary contribution is the development of a new technique to more precisely compare system orderings. By focusing on swaps that are conclusive, our weighted rank correlation coe cient dist can be used to measure the stability of a variety of estimation techniques. Using dist, we show that estimation improves our ability to score and compare systems using limited judgments.",0,,False
"It must be noted, however, that the estimation is built on the m rank-based ed models, each of which requires that when constructing the judgment set, documents up to some rank d be fully judged. is means that for some sampling-based judgment approaches, the proposed method is not applicable. Second, while we show that our estimation methods can also account for system bias to some extent, outcomes might be further improved by introducing more randomization into the optimization framework. Hence, in answer to the question posed in the title, our answer remains a somewhat cautious ""be er than before"", rather than a ""yes"".",0,,False
Funding. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP140103256 and DP170102231).,0,,False
REFERENCES,0,,False
"[1] J. A. Aslam, V. Pavlu, and E. Yilmaz. 2006. A statistical method for system evaluation using incomplete judgments. In Proc. SIGIR. 541­548.",0,,False
"[2] C. Buckley, D. Dimmick, I. Soboro , and E. M. Voorhees. 2007. Bias and the limits of pooling for large collections. Inf. Retr. 10, 6 (2007), 491­508.",0,,False
[3] C. Buckley and E. M. Voorhees. 2004. Retrieval evaluation with incomplete information. In Proc. SIGIR. 25­32.,0,,False
"[4] S. Bu¨ cher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboro . 2007. Reliable information retrieval evaluation with incomplete and biased judgements. In Proc. SIGIR. 63­70.",0,,False
"[5] A. Chao and S. Lee. 1992. Estimating the number of classes via sample coverage. J. American Statistical Association 87, 417 (1992), 210­217.",0,,False
"[6] O. Chapelle, D. Metlzer, Y. Zhang, and P. Grinspan. 2009. Expected reciprocal rank for graded relevance. In Proc. CIKM. 621­630.",1,ad,True
"[7] J. K. Jayasinghe, W. Webber, M. Sanderson, and J. S. Culpepper. 2014. Improving test collection pools with machine learning. In Proc. Aust. Doc. Comp. Symp. 2­9.",0,,False
[8] R. Kumar and S. Vassilvitskii. 2010. Generalized distances between rankings. In Proc. WWW. 571­580.,0,,False
"[9] A. Lipani, M. Lupu, and A. Hanbury. 2015. Spli ing water: Precision and antiprecision to reduce pool bias. In Proc. SIGIR. 103­112.",0,,False
"[10] A. Lipani, M. Lupu, E. Kanoulas, and A. Hanbury. 2016. e solitude of relevant documents in the pool. In Proc. CIKM. 1989­1992.",0,,False
"[11] X. Lu, A. Mo at, and J. S. Culpepper. 2016. e e ect of pooling and evaluation depth on IR metrics. Inf. Retr. 19, 4 (2016), 416­445.",0,,False
"[12] X. Lu, A. Mo at, and J. S. Culpepper. 2016. Modeling relevance as a function of retrieval rank. In Proc. AIRS. 3­15.",0,,False
"[13] A. Mo at, P. omas, and F. Scholer. 2013. Users versus models: What observation tells us about e ectiveness metrics. In Proc. CIKM. 659­668.",0,,False
"[14] A. Mo at, W. Webber, and J. Zobel. 2007. Strategic system comparisons via targeted relevance judgments. In Proc. SIGIR. 375­382.",0,,False
"[15] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Information Systems 27, 1 (2008), 2:1­2:27.",0,,False
"[16] S. D. Ravana and A. Mo at. 2010. Score estimation, incomplete judgments, and signi cance testing in IR evaluation. In Proc. AIRS. 97­109.",0,,False
[17] S. E. Robertson. 2007. On document populations and measures of IR e ectiveness. In Proc. ICTIR. 9­22.,0,,False
[18] T. Sakai. 2007. Alternatives to BPref. In Proc. SIGIR. 71­78. [19] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,1,TREC,True
pool depth bias. In Proc. SIGIR. 691­692. [20] T. Sakai. 2008. Comparing metrics across TREC and NTCIR: The robustness to,1,TREC,True
"system bias. In Proc. CIKM. 581­590. [21] T. Sakai. 2014. Metrics, statistics, tests. In Bridging Between Information Retrieval",0,,False
"and Databases, N. Ferro (Ed.). Springer, 116­163. [22] T. Schnabel, A. Swaminathan, P. I. Frazier, and T. Joachims. 2016. Unbiased",0,,False
"comparative evaluation of ranking functions. In Proc. ICTIR. 109­118. [23] T. Schnabel, A. Swaminathan, and T. Joachims. 2015. Unbiased ranking evaluation",0,,False
on a budget. In Proc. WWW. 935­937. [24] E. M. Voorhees. 2014. e e ect of sampling strategy on inferred measures. In,0,,False
Proc. SIGIR. 1119­1122. [25] E. M. Voorhees and D. K. Harman. 2005. TREC: Experiment and Evaluation in,1,TREC,True
Information Retrieval. e MIT Press. [26] W. Webber and L. A. F. Park. 2009. Score adjustment for correction of pooling,1,ad,True
bias. In Proc. SIGIR. 444­451. [27] E. Yilmaz and J. A. Aslam. 2008. Estimating average precision when judgments,0,,False
"are incomplete. Knowledge and Information Systems 16, 2 (2008), 173­211. [28] E. Yilmaz, E. Kanoulas, and J. A. Aslam. 2008. A simple and e cient sampling",0,,False
method for estimating AP and NDCG. In Proc. SIGIR. 603­610. [29] Z. Zhou. 2012. Ensemble Methods: Foundations and Algorithms. CRC press. [30] J. Zobel. 1998. How reliable are the results of large-scale information retrieval,1,AP,True
experiments?. In Proc. SIGIR. 307­314.,0,,False
44,0,,False
,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Retrieval Consistency in the Presence of ery Variations,0,,False
Peter Bailey,0,,False
"Microso Canberra, Australia",0,,False
Falk Scholer,0,,False
"RMIT University Melbourne, Australia",0,,False
ABSTRACT,0,,False
"A search engine that can return the ideal results for a person's information need, independent of the speci c query that is used to express that need, would be preferable to one that is overly swayed by the individual terms used; search engines should be consistent in the presence of syntactic query variations responding to the same information need. In this paper we examine the retrieval consistency of a set of ve systems responding to syntactic query variations over one hundred topics, working with the UQV100 test collection, and using Rank-Biased Overlap (RBO) relative to a centroid ranking over the query variations per topic as a measure of consistency. We also introduce a new data fusion algorithm, Rank-Biased Centroid (RBC), for constructing a centroid ranking over a set of rankings from query variations for a topic. RBC is compared with alternative data fusion algorithms.",1,UQV,True
"Our results indicate that consistency is positively correlated to a moderate degree with ""deep"" relevance measures. However, it is only weakly correlated with ""shallow"" relevance measures, as well as measures of topic complexity and variety in query expression.",0,,False
ese ndings support the notion that consistency is an independent property of a search engine's retrieval e ectiveness.,0,,False
CCS CONCEPTS,0,,False
·Information systems Retrieval e ectiveness; Test collections;,0,,False
KEYWORDS,0,,False
"Test collections, retrieval consistency, semantic e ectiveness",0,,False
1 INTRODUCTION,1,DUC,True
"Evaluating search e ectiveness has several aspects. One aspect that has been especially popular is calculating average scores according to some relevance measure (such as NDCG or AP) over a set of topics and associated queries for some common corpus of information. In the batch evaluation methodology, di erent systems are compared using the same measure, and statistical tests are applied",1,AP,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080839",1,ad,True
Alistair Mo at,0,,False
"e University of Melbourne Melbourne, Australia",0,,False
Paul omas,0,,False
"Microso Canberra, Australia",0,,False
"to determine whether the di erence in performance is likely due to factors other than chance. Alternative methods for determining relevance include user studies, online interleaving, and A/B testing. What is important to note is that retrieval e ectiveness encompasses more than just relevance.",0,,False
"Batch evaluation has typically used only a single query per topic, although a number of researchers working on early test collections advocated and explored the e ect of using multiple queries per topic (see work by Spa¨rck Jones and van Rijsbergen [35], Belkin et al. [5] and Buckley and Walz [7] among others). Recent work by Bailey et al. [3] and Koopman and Zuccon [22] has returned to this theme, resulting in new test collections with large numbers of query variations responding to each topic's information need.",1,ad,True
"e availability of such test collections allows us to consider a new dimension in assessing the retrieval e ectiveness of search engines ­ namely, how consistent they are when returning results in response to query variations that address the same information need. e importance of consistency can be understood when we consider simple examples like mis-spellings. For example ""facebook"", ""facebok"", and ""faecbook"" pre y clearly all want to nd the Facebook home page. Consistency also applies to more complex examples involving synonyms (for example, ""health bene ts of vitamin c"" and ""health bene ts of ascorbic acid"") or entirely rephrased needs (for example, ""how much does a raspberry pi cost"" and ""price of raspberry pi computer""). In each of these cases, we can contemplate that there exists an ideal ranked set of relevant results drawn from the corpus. Test collections are premised on this principle, where the ideal set for a topic is discovered through judging a document pool formed from di erent rankings. An ideal search engine would return (only) this set of results given a query for a topic, and the di erence in relevance from what is actually returned and this ideal ranking is captured by a relevance measure. Equally, given a set of syntactic query variations, an ideal search engine would return this ideal ranking of results, independent of the query variation.",1,ad,True
"Indeed, much research in information retrieval seeks to tackle exactly this problem of nding an ideal set of results without relying solely on the original query's syntactic expression. For instance, query re-writing techniques such as spelling correction [11], term stemming [24], query expansion [33], and query substitutions [19] are used to manipulate the user-entered query and thereby extract a be er set of documents from the index. Stemming and stopword removal [23, 25] may also be used in indexing processes or within the matching algorithms at query execution time.",0,,False
"Two strands of research have investigated how to combine rankings to improve relevance e ectiveness: data fusion (for example,",0,,False
395,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Belkin et al. [6]), which merges rankings from di erent query representations; and distributed information retrieval or meta-search (for example, Callan [8]), which merges rankings from di erent underlying search engines or indexes. ese techniques have been assessed principally from the standpoint of improving relevance overall, as measured by the relevance score of the resulting ranking.",0,,False
"People use many di erent expressions to describe the same information need (see, for example, Furnas et al. [15]). Even when re nding a single information resource, the same person may use di erent queries [37]. Bailey et al. [2] give evidence that the e ect of query variation on relevance scores dwarfs that of system and topic e ects. We believe that these ndings make it important to consider new approaches to characterizing e ectiveness, including ones that address query variation for a single information need.",1,ad,True
"We propose that the consistency in rankings of a system when faced with many di erent query variations for a single topic can be one such measure. e Rank-Biased Overlap measure developed by Webber et al. [38] is adopted to characterize consistency, and used with the UQV100 test collection [3] to investigate consistency across a set of ve systems. Due to the scale of variations within UQV100 (19­101 unique query variations per topic, for 100 topics), each individual topic has a similar number of queries as might be found in an entire query-only processed test collection like the TREC 2014 Web track [10].",1,ad,True
"To determine relative consistency for a single system and topic combination, RBO requires us to declare some reference ranking against which the individual rankings for each query variation can be compared. We develop a new fusion algorithm, the RankBiased Centroid (RBC), drawing inspiration from both RBO and Rank-Biased Precision [27], to determine this reference ranking.",0,,False
We consider these research questions in regard to RBC fusion:,0,,False
RQ-F1 Do RBC rankings outperform the initial query rankings for a system?,0,,False
RQ-F2 How does RBC compare to other data fusion algorithms in relevance e ectiveness?,0,,False
RQ-F3 Does combining both query variations and systems for RBC outperform query variations-only RBC?,0,,False
"en, in connection with consistency, we ask:",0,,False
RQ-C1 Do topics vary in consistency? RQ-C2 Does consistency vary with the number of query variations,0,,False
or with changes in topic complexity? RQ-C3 Do systems vary in consistency? RQ-C4 Are increases in per-topic consistency for a system inde-,0,,False
pendent of increases in relevance for a system?,0,,False
2 RELATED WORK,0,,False
2.1 Data fusion,0,,False
"Data fusion ­ combining evidence from di erent sources ­ is a widely-studied problem. In IR fusion is typically applied when evidence from multiple ranked answer lists needs to be combined into a single ranked list, for example in meta-search, where results from multiple independent search systems are combined into a single ranking [1], and in multi-lingual retrieval, where results from searches across collections in di erent languages are combined into a single answer list [16].",0,,False
"Data fusion approaches can be broadly grouped into those that use the ranker's score (that is, the value assigned by a ranking function) of each document in a results list, and those that make use only of the rank position of each document in the answer list. Perhaps the most well-known approaches in the former category are by Fox and Shaw [13], including CombMAX, where the nal score of a document is the maximum of the ranker scores that it received in any input ranked list; CombSUM, where the nal document score is the total of the ranker's scores that it received in the input lists; and CombMNZ, where the nal document score is calculated as in CombSUM but further multiplied by the number of input lists in which the document appears, thereby promoting those documents that were retrieved in multiple lists. e document ranker scores in the input ranked lists may also be normalized in di erent ways, including linear re-scaling into a chosen range [39], or by controlling an upper bound based on the sum or variance of the input scores [28].",1,ad,True
"For fusion based only on rank information, techniques from social choice theory such as the Borda count and Condorcet criterion have been applied. In the Borda count [1], each candidate (document) receives a score determined by how many other candidates were ranked lower, with these scores summed across all ballots (input lists). e Condorcet criterion instead determines an outcome based on which candidate achieves the highest number of wins based on pairwise comparisons with all other candidates [12, 29]",1,ad,True
"e impact of data fusion techniques on e ectiveness can vary from case to case, leading Wu and McClean [40] to investigate approaches for predicting the performance impact of applying fusion.",1,ad,True
"eir results showed that the selective application of fusion, based on features such as the number of component result lists and the overlap of items in these lists, can further enhance the positive impact on nal retrieval e ectiveness.",0,,False
"Prior work that has speci cally considered data fusion in the context of multiple queries for the same underlying information need was carried out by Belkin et al. [5], who investigated the e ects of combining ve independent Boolean query formulations for ten TREC topics, and demonstrated that fusing results can substantially boost performance. Subsequent work using the TREC-2 collection further demonstrated that good methods for fusing the results of multiple queries can lead to results that are be er than those of the best single query [6]. Pickens et al. [30] also con rmed that combining multiple queries for the same intent boosts e ectiveness.",1,TREC,True
2.2 Measuring consistency,0,,False
"In IR, as in many other domains, it can be important to compare the similarity, or consistency, of groups of things. ese groups may be conjoint (consisting of the same items) or disjoint (one group may include items that do not occur in the other group), and may be setbased (where there is no known or inferred ordering of the items) or ordered (where the sequence in which the items occur ma ers). Typical examples where one might wish to compare groups include measuring the similarity of the answer lists returned by two search engines in response to the same query; or the similarity of the e ectiveness ranking of a set of several di erent retrieval systems, when evaluated over two di erent test collections. A wide range of list similarity measures have been proposed and applied.",0,,False
396,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"One of the similarity measures most commonly used in IR is Kendall's  , a rank correlation coe cient that calculates the normalized number of concordant pairs (items that are ordered the same in two rankings) minus discordant pairs (items that are ordered di erently in the two rankings), resulting in a score between 1 (perfect agreement between the two rankings) and -1 (perfect disagreement) [9]. Kendall's  is a measure that assumes conjoint ranked lists, that is, the lists are permutations of the same set of items; it is also an unweighted measure, where each pair contributes equally to the outcome, wherever it occurs in the ranking. However, when comparing ranked answer lists, items that are higher in the ranking are more important (users pay more a ention to top-ranked results); similarly, when comparing rankings of system e ectiveness scores, di erences between the top systems are generally of greater interest than di erences between systems that perform less well. TauAP [41] addresses a number of these weaknesses, while Rank-Biased Overlap (RBO) [38] addresses even more. RBO is a generalized measure of similarity between rankings based on a probabilistic user model, and readily handles non-conjoint lists. It applies a geometric sequence of weights to items in the lists; with the emphasis of the weighting adjustable via a user persistence parameter , which also determines the probability that the user will reach a certain rank. Inspired by RBO, Tan and Clarke [36] de ne a family of Maximized E ectiveness Di erence (MED) measures, each based on an IR e ectiveness metric (and hence a di erent underlying user model).",1,AP,True
"Jiang et al. [18] examine ranking consistency in web search from the basis of nding similar classes of queries (based on sharing common entity types in a knowledge base), and preserving the relative ordering of URL domains in the rankings for queries belonging to the same class, for example, people who are professional basketballers but also appear in movies. Large scale web log click data is used to derive their models of class similarity, based on URL pa erns. Jiang et al. develop a consistency measure based on Kendall's  across all pairs of queries belonging to the same class.",0,,False
"Finally, Zuccon et al. [43] present an evaluation framework using mean variance analysis over retrieval e ectiveness, for both intertopic and intra-topic sources of variation. Systems are preferred, all other things being equal, when one system is more stable than another in the presence of such variation.",0,,False
3 RANK-BIASED CENTROIDS,0,,False
"As noted in the previous section, a range of methods have been proposed for constructing fused rankings, given an initial set of same-basis source rankings. In this section we introduce a further approach: the rank-biased centroid, or RBC.",0,,False
3.1 User model for Borda fusion,0,,False
"To motivate the discussion, consider the four alternative rankings R1, R2, R3, and R4 shown in the le side of Figure 1, with each of the elements denoted by a le er of the alphabet. One run has ordered all of the seven di erent elements, while the other three are truncated and omit one or more of the items ­ a typical situation. Moreover, note that even if they are all of the same length, the runs might contain di erent subsets of a larger group of elements ­ they need not be permutations of each other. Finally, note that in the",0,,False
"Rank R1 R2 R3 R4  , 0.6  , 0.8  , 0.9",0,,False
1 A B A G A (0.89) D (0.61) D (0.35) 2 D D B D D (0.86) A (0.50) C (0.28) 3 B E D E B (0.78) B (0.49) A (0.27) 4 C C C A G (0.50) C (0.37) B (0.27) 5 G ­ G F E (0.31) G (0.37) G (0.23) 6 F ­ F C C (0.29) E (0.31) E (0.22) 7 ­ ­ E ­ F (0.11) F (0.21) F (0.18),0,,False
"Figure 1: Example of RBC fusion: four example rankings (le ); and three di erent fused orderings (right). Note that the RBC weights are shown to two decimal places only, and there are no score ties.",0,,False
"most general scenario, there are situations in which the provided rankings are pre xes of longer lists, themselves of unknown (and perhaps even in nite) length.",0,,False
"e Borda scoring process assigns a weight to item A of 7+7+4 ,"" 18 (note that A does not appear in ranking R2), tying it with B, and placing it behind item D, which gets 6 + 6 + 6 + 5 "","" 23 points. e overall Borda ordering is D, A"",""B, C, G, E, F. In the Borda regime, swapping the two adjacent items at any pair of consecutive ranks gives one of the items a +1 score change, and the other item a -1 change. is occurs regardless of whether the swap takes place at rank 1, at rank 10, or at rank 100. at is, all binary item preferences as expressed in the visible input rankings are regarded as being of equal merit; and any preferences that may not have been surfaced (in the case that the provided rankings are pre xes) are ignored.""",1,ad,True
"To create a user model that captures this behavior we can imagine a universe of agents, each of whom acts independently of the others, but follows the same simple rule: they randomly pick a depth d according to some probability distribution, they examine all of the input rankings to depth d (at most ­ but less if the rankings are shorter), and they sort the pool of items according to decreasing order of the number of times they saw each item in their set of length-d pre xes. e nal fused ranking is then a probabilistic expectation over all agents of the orderings that were constructed.",0,,False
"Given this overall probabilistic structure, the Borda ordering is derived when the probability distribution used by the agents is taken to be P(d , x) ,"" 1/n; that is, each agent is equally likely to select a pre x of any length between 1 and n, where n is the number of items. e Borda score for an item is then proportional to the expected value of the total number of times it was observed in the individual top-d sets of the probabilistic universe of agents.""",0,,False
3.2 An alternative weighting regime,0,,False
"Because there are many situations in which the supplied rankings are assumed to be pre xes of arbitrary-length ones, we contend that swaps near the heads of each of the rankings are somehow more indicative of preference than swaps deeper in the rankings. In the example shown in the le side of Figure 1, swapping A and D in ranking R1 has the same net e ect on A's Borda score as does swapping A and F in ranking R4, but the la er swap might seem to be somewhat less damaging to A, since in R4 it has already been deprecated by the person or system that generated that ordering.",1,ad,True
397,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Instead of assigning a Borda weight of (n - x + 1)/n (in a normalized sense) to each item at rank 1  x  n when the rankings are over n items, we suggest that a geometrically decaying weight function be employed, with the distribution of d over depths x given not by 1/n, but instead by (1 - )x-1 for some value 0    1 determined by considering the purpose for which the fused ranking is being constructed. e parameter  is the persistence, or patience of the imagined universe of probabilistic agents; and use of a geometric sequence models the same behavior as is embedded in the e ectiveness metric RBP [27] and in the rank correlation coe cient RBO [38] ­ namely, that the person examining the rankings always examines the rst item in each, and therea er proceeds from the i th to the i + 1 st with conditional probability , and ends their search at the i th with conditional probability (1 - ). In an implementation the fused ranking is determined by assigning a weight of (1 - )i-1 to each item at depth i in any of the rankings, and then summing over items and sorting by total weight.",1,ad,True
ere are a number of bene ts of this proposed approach:,0,,False
"· as already motivated, greater emphasis is placed on the earlier preferences than on deeper ones in each ranking;",1,ad,True
"· an upper bound on the lengths of the rankings is not required, nor are the rankings required to be the same length (the Borda method shares this exibility, albeit somewhat awkwardly);",0,,False
"· if further items are added at the tail of any of the rankings, the resultant item scores converge smoothly.",1,ad,True
"As extreme values, consider  , 0 and  , 1. When  ,"" 0, the agents only ever examine the rst item in each of the input rankings, and the fused output is by decreasing score of rst preference; this is somewhat akin to a rst-past-the-post election regime. When  "","" 1, each agent examines the whole of every list, and the fused ordering is determined by the number of lists that contain each item ­ a kind of """"popularity count"""" of each item across the input sets. In between these extremes, the expected depth reached by the agents viewing the rankings is given by 1/(1 - ). For example, when  "","" 0.9, on average the rst 10 items in each ranking are being used to contribute to the fused ordering; of course, in aggregate, across the whole universe of agents, all of the items in every ranking contribute to the overall outcome.""",0,,False
"In practice, what this means is that di erent values of  between 0 and 1 give rise to di erent fused orderings, balancing topweightedness and exhaustivity. e right side of Figure 1 shows the orderings generated for the rankings R1, R2, R3, and R4, discussed earlier, for three di erent values of . e total weight associated with each item (to two decimals) is also shown. Note how item A is top-ranked when the ranking agents are relatively impatient, and (on average) abandon the ranking early ( ,"" 0.6), but that if the fused ranking is assembled on a more patient basis ( "", 0.8 and  ,"" 0.9), items D and C become preferred, and A is demoted.""",0,,False
"As with Borda fusion, unanimous preferences are respected: in the example, because C is below D in all of the four input rankings, it must also fall below D in the fused ranking, regardless of the value of . e di erences that arise as  varies are limited only to the elements where there is disagreement in the input rankings as to their respective ordering. ese are, arguably, exactly the elements that we might be interested in focusing on.",0,,False
3.3 Discussion,0,,False
"We have de ned RBC in terms of a one-state user model [27]. Another way of looking at it is as an estimation of the normalized document scores used in CombMNZ and CombSUM. By assigning decreasingly small weights to documents further down the ranking, RBC can be viewed as seeking to approximate the long tail of document-query similarity scores generated by disjunctive ranked retrieval systems. Functions other than the geometric sequence might also be suitable for use, for example, Zip an weightings.",0,,False
4 FUSION OVER QUERY VARIATIONS,0,,False
"is section explores the practical bene t of fusing over query variations, and also shows that fusion over systems retains some of its power even a er query variations have been incorporated.",1,corpora,True
4.1 e UQV100 collection,1,UQV,True
"e UQV100 test collection is made up of 100 topics and associated information need statements, with approximately 100 individual query variations per topic; 10,835 in total [3]. When spelling correction and normalization are applied there are between 19 and 101 unique query variations per topic; 5,765 in total. ere are also 55,587 relevance judgments available in regard to those 100 topics, covering ClueWeb12-CatB documents pooled from ve systems, spanning three separate search engine code bases, and ve di erent ranking algorithms [26]. We again employ the runs for those ve contributing systems, anonymized here as Systems 1, 2, 3, 4, and 5. Due to some processing anomalies we observed in the run data, the overlapping set of unique query variations processed by all ve systems contains 5,736 queries. Each system run contains a ranking of length 200 for each of those distinct queries. For de niteness we ordered the set of queries for each topic by decreasing frequency according to crowd-based process used to originally collect them [2], with ties broken randomly.",1,UQV,True
4.2 Fusion over query variations,0,,False
"Table 1 provides a detailed evaluation of approaches for fusion as applied to query variations. Each pane of the table gives results for one e ectiveness metric, and within each pane the columns represent increasing numbers of query variations (note that for",0,,False
" 20, the legend , x indicates that as many as x query variations were used ­ some topics had fewer than the listed number of variations). Note that , 1 makes use of the most frequentlysuggested query for each of the UQV100 topics; ,"" 2 adds the second most frequently suggested one; and so on. Stepping across each row thus involves more and more input runs being used to form each output run, and as can be seen, e ectiveness scores (with a few exceptions) increase. Many of the fusion methods give very similar e ectiveness. Even so, there are some notable pa erns:""",1,ad,True
"· using as few as , 2 query variations gives improved e ectiveness (relative to the , 1 baseline) for all metrics and all fusion methods;",0,,False
"· for all of the metrics, RBC with high values of p provides good fused outcomes, comparable with or be er than those achieved by CombMNZ and Borda;",0,,False
"· for the two recall-based metrics, RBC-based fusion provides markedly be er outcomes than Borda and CombMNZ;",0,,False
398,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Fusion,0,,False
"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
"RBC,  ,"" 0.9 0.491 0.490 RBC,  "","" 0.95 0.497 0.504 RBC,  "","" 0.98 0.505 0.511 RBC,  "", 0.99 0.511 0.520",0,,False
0.510 0.516 0.522 0.525,0,,False
0.516 0.526 0.535 0.534,0,,False
0.524 0.529 0.533 0.533,0,,False
Borda,0,,False
0.511 0.522 0.527 0.534 0.532,0,,False
CombMNZ 0.513 0.521 0.527 0.534 0.532,0,,False
"(a) RBP0.85, common baseline 0.474",0,,False
0.522 0.526 0.532 0.534,0,,False
0.535,0,,False
0.531,0,,False
Fusion,0,,False
"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
"RBC,  ,"" 0.9 0.490 0.490 RBC,  "","" 0.95 0.496 0.504 RBC,  "","" 0.98 0.503 0.510 RBC,  "", 0.99 0.508 0.519",0,,False
0.506 0.514 0.519 0.523,0,,False
0.516 0.526 0.533 0.532,0,,False
0.523 0.528 0.533 0.531,0,,False
Borda,0,,False
0.507 0.520 0.525 0.530 0.528,0,,False
CombMNZ 0.509 0.517 0.524 0.531 0.528,0,,False
"(b) INST, common baseline 0.471",0,,False
0.522 0.527 0.532 0.532,0,,False
0.532,0,,False
0.528,0,,False
Fusion,0,,False
"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
Fusion,0,,False
"Number of variations per query , 2 , 4 , 10 , 20 , 40 , all",0,,False
"RBC,  ,"" 0.9 0.222 0.230 0.248 0.265 0.288 0.299 RBC,  "","" 0.95 0.223 0.234 0.256 0.280 0.297 0.303 RBC,  "","" 0.98 0.225 0.239 0.260 0.275 0.281 0.284 RBC,  "", 0.99 0.226 0.241 0.254 0.264 0.266 0.270",0,,False
"RBC,  ,"" 0.9 0.437 0.454 RBC,  "","" 0.95 0.438 0.458 RBC,  "","" 0.98 0.440 0.462 RBC,  "", 0.99 0.442 0.464",0,,False
0.484 0.505 0.539 0.553 0.489 0.519 0.545 0.554 0.492 0.510 0.521 0.525 0.481 0.494 0.499 0.505,0,,False
Borda,0,,False
0.226 0.239 0.251 0.260 0.262 0.267 Borda,0,,False
0.442 0.464 0.478 0.489 0.493 0.502,0,,False
CombMNZ 0.227 0.240 0.252 0.260 0.273 0.266 CombMNZ 0.442 0.463 0.479 0.490 0.494 0.500,0,,False
"(c) AP, common baseline 0.204",1,AP,True
"(d) NDCG, common baseline 0.409",0,,False
"Table 1: Fusion over query variations, average e ectiveness across 100 UQV topics for runs generated from di erent numbers of query variations and according to di erent fusion approaches for System 1: (a) RBP0.85 scores; (b) INST scores; (c) AP scores; (d) NDCG scores.",1,UQV,True
"ery variations are sorted in decreasing order of occurrence frequency in the UQV100 collection. e baseline scores for ,"" 1 (that is, executing the single most popular query variation) are shown under each table. So that pa erns of behavior can be seen, the two largest""",1,UQV,True
"values in each column are highlighted in bold. Daggers indicate arrangements in which the RBC-based system was signi cantly be er than the corresponding Borda run (one-sided paired t-tests with p < 0.05). No signi cant di erences were detected for CombMNZ fusion, or using RBP0.85 or INST. Similar behavior was observed for other combinations of system and metric (not shown here).",0,,False
Fusion,0,,False
RBP0.85,0,,False
Metric,0,,False
INST,0,,False
AP,1,AP,True
NDCG,0,,False
"RBC,  ,"" 0.9 RBC,  "","" 0.95 RBC,  "","" 0.98 RBC,  "", 0.99",0,,False
0.503 0.508 0.506 0.505,0,,False
0.501 0.506 0.504 0.502,0,,False
0.217 0.219 0.220 0.217,0,,False
0.441 0.442 0.443 0.440,0,,False
Borda,0,,False
0.503 0.500 0.215 0.440,0,,False
CombMNZ,0,,False
0.506 0.505 0.219 0.442,0,,False
"Table 2: Fusion over ve di erent retrieval systems, based on one query variation ( , 1). All numbers are average e ectiveness scores over the 100 topics in the UQV100 collection. Single-system scores for the four metrics are shown in the rst two columns of Table 3. e largest two entries in each column are shown in bold. Daggers represent signi cance relative to Borda fusion (p < 0.05).",1,UQV,True
"· moreover, the greater the number of query variations being fused, the smaller the value of  needed to obtain those outcomes.",0,,False
We also explored round-robin fusion and CombSUM fusion; the former was never competitive (and e ectiveness decreased as query variations were added); and CombSUM typically gave performance slightly inferior to CombMNZ.,1,ad,True
"Metric Initial, , 1 mean max",0,,False
"Fused, , all mean max",0,,False
"Fused2, s , 5 mean gain",0,,False
RBP0.85 INST AP NDCG,1,AP,True
0.474 0.470 0.190 0.400,0,,False
0.487 0.481 0.204 0.411,0,,False
0.530 0.532 0.268 0.517,0,,False
0.557 0.558 0.303 0.554,0,,False
0.559 0.563 0.303 0.561,0,,False
+18% +20% +59% +41%,0,,False
"Table 3: Summary of e ectiveness gains achieved by fusing rst over query variations, and then second over systems. e rst four data columns are mean and maximum average scores over ve systems; the ""gain"" is relative to the initial system average in the",0,,False
rst column. All fusion is carried out using RBC0.95.,0,,False
4.3 Fusion over systems,0,,False
"Table 2 shows the outcome of applying fusion across the ve systems used in our experiments. A single query is used in each input run ( ,"" 1), and fusion applied to the ve rankings for each topic. In this se ing, all methods give comparable improvements in e ectiveness, with Borda fusion marginally the worst of them.""",0,,False
4.4 Double fusion,0,,False
"Table 3 provides an overall summary of the e ectiveness gains that can be achieved by fusing over query variations and then over systems, with RBC0.95 used at all fusing steps. Starting on the le ,",0,,False
"ve systems each execute one query variation ( , 1) for each of the",0,,False
399,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Description,0,,False
RBP0.85 INST AP NDCG,1,AP,True
"All queries ( , all) First queries ( , 1) Best query per topic",0,,False
0.405 0.474 0.712,0,,False
0.394 0.151 0.336 0.471 0.204 0.409 0.718 0.271 0.503,0,,False
"Table 4: Average metric scores for all queries per topic; for the most popular query per topic; and for per-metric per-topic ""omniscient"" query selections. System 1 is used throughout. ese scores can be directly compared with those shown in the four panes of Table 1.",0,,False
UQV100 topics; this can be regarded as being the starting baseline condition (no fusion performed) for both this table and Table 2. e,1,UQV,True
"ve-way mean ""average over 100 topics"" and ve-way maximum ""average over 100 topics"" values show typical behavior for ve good retrieval systems when measured using a single query per topic. If each of those ve systems is given more query variations, and generates a single fused run for each of the 100 topics as its output, the values in the middle pair of columns arise. Substantial performance improvements can be observed, and the means of the",0,,False
"ve ""fusion over query variations"" systems handsomely exceeds the best average score of the ve original systems.",0,,False
"e third pair of columns in Table 3 then shows the outcome of fusing the ve system runs generated a er the query variations have been folded in. e mean scores shown (now with just a single ranking for each of the 100 topics) exceed the previous maximum scores in three of four cases, and exceed the middle-column mean scores in all four cases. e nal column shows the end-to-end gain in e ectiveness that has been achieved by the compound fusing (""initial mean"" to ""fused2 mean""). at is, fusing rst over query variations, and then over systems, gives rise to average e ectiveness gains of 18% and higher. In terms of statistical signi cance, and looking at the various relativities summarized in Table 3:",0,,False
"· across the ve systems and four metrics (twenty paired runs in total), the largest p-value computed by a two-tailed paired Student's t-test comparing the corresponding , 1 and , all conditions was less than 0.005;",0,,False
"· when the ve ,"" all fused runs are compared with the nal """"fused2"""" run, each metric yields one relatively large p-value, arising when the system that happens to be the """"max"""" is compared with the nal fused run (p  0.8, 0.7, 0.9, and 0.3 respectively across the four metrics, with two di erent systems represented twice each as the """"max"""" one), and a range of other smaller p-values, the largest of which was 0.059 (INST, comparing System 2 with "","" all against the nal fused run), and the remainder of which were 0.01 or smaller.""",0,,False
"at is, we are highly con dent that fusion over queries helps retrieval e ectiveness regardless of system and regardless of metric; and also con dent that additional fusion across systems is also bene cial, helping ensure that the outcomes are as good as, or be er than, what would have been a ained if by chance we were already working with the best system for that metric.",1,ad,True
4.5 An unrealistic target?,0,,False
"Hindsight is a wonderful guide, a fact that is also true in IR. Table 4 shows the result of a post-hoc evaluation of the runs generated",1,hoc,True
"for System 1. e rst row shows the (unweighted) average metric scores across all of the UQV100 topics, and for each topic across the (approximately, on average) 55 distinct query variations. e second row shows the baseline e ectiveness scores used in Table 1, arrived at by selecting the most popular of the query variations for each topic. e third row then shows results for four ""oracle"" query subsets, one for each metric, each incorporating (based post hoc on the relevance judgments and the computed metric scores) the ""best"" query variation for each of the UQV100 topics.",1,UQV,True
"Comparing the rst and second rows, the most frequently posed query generated by the crowd-workers for each topic obtains notably be er e ectiveness than the average of the variations. at di erence is why we ordered the query variations as we did (Section 4.1). Comparing the second and third rows reveals a substantial further gap ­ for each of the topics and each of the metrics there are highly e ective queries available within the sets created by the crowd-workers. For two of the metrics the single-query oracle runs are outperformed by the best of the fused approaches (Table 1), but for two metrics they are considerably be er. Also worth noting is that the oracle runs have non-trivial di erences, with di erent best queries arising for di erent metrics in many cases. Across the four metrics, a total of 193 best queries were identi ed.",0,,False
5 CONSISTENCY DEFINED,0,,False
5.1 De nition,0,,False
"As discussed in Section 2, Rank-Biased Overlap (RBO) [38] measures the top-weighted rank similarity between two non-conjoint inde nite rankings. As the rankings increase in similarity, especially towards the start of the rankings, the value of RBO trends towards 1.0. Due to the geometric sum of weights, governed by parameter , the total overlap score is bounded, ranging from 0.0 (no overlap) to 1.0 (total overlap).",0,,False
"To use RBO to measure consistency across query variations for a topic, we rst select a common reference or objective ranking (the centroid) for each system-topic pair, making use of the RBC algorithm described in the previous sections. e persistence factor  for RBC is set to 0.90, to mirror a user whose expected depth of examination into a ranked list is 10, a reasonably deep level of examination relative to standard web search; also, the UQV100 pooling ensured at least depth-10 judging for each query from each system. Di erent persistence factors could be selected, which would emphasize shallower or deeper probabilities of inspection of the lists forming the centroid or the depth of overlap. e centroid for all-systems might also have been considered; however we sought to measure a system's self-consistency, rather than with respect to a centroid that requires knowledge of other systems.",1,UQV,True
"Given a centroid, we compute the RBO score for every query variation, using the same value of  ,"" 0.90. Computation of RBO provides both a point-estimate, and a minimum, residual, and maximum value. Since system runs typically report 200 or more documents, when  "","" 0.90 the residual is less than 10-10, and the point value is essentially equivalent to the minimum and maximum.""",0,,False
"Formally, given a set of query variations Vi ,"" { i .1, i .2, . . . , i .k } for a topic ti  T "","" {t1, t2, ..., tn }, given a system S and its rankings for this set of variations Di "","" {d i .1 , d i .2 , ..., d i .k }, and given the corresponding RBC ranking for S and topic ti , denoted di .rbc, we""",0,,False
400,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
measure topic consistency Cti with respect to S as,0,,False
"Cti ,",0,,False
"k q,1",0,,False
RBO(d,0,,False
"i .q , di .rbc) ,",0,,False
k,0,,False
(1),0,,False
"and the collection consistency CT as the average topic consistency over the set of topics T , again with respect to S, as",0,,False
"CT ,",0,,False
"n i ,1",0,,False
Cti,0,,False
.,0,,False
n,0,,False
(2),0,,False
"at is, consistency is the average RBO score relative to the pertopic centroids generated for the system, expressed either as a set of per-topic scores, or aggregated over topics for a per-collection score, but always with regard to a system S. We can also speak of a system's consistency, which is simply topic (or collection) consistency for a particular system. Note that the near-zero RBO residual means that issues of averaging over RBO scores with di erent residuals can be ignored. We choose the average of averages for CT because the number of variations per topic may vary, and topics with large numbers of unique variations should not unduly bias nal scores.",0,,False
"Unlike for RBC, where we explored the consequence of adding more variations and thus needed an ordering, for consistency we use all unique query expressions without repeats (unweighted). In UQV100, each topic typically has a small number of commonly chosen variations which occur multiple times, and a large number of variations that occur only once. We wished to avoid biasing the consistency measure unduly by counting the contribution of the more popular variations multiple times. e choice of unweighted query variations also reinforces the decision to compute average of averages for CT ; in UQV100 the number of unique query variations per topic ranges from 19 to 101, so double averaging helps avoid undue in uence from the topics with more diverse variations.",1,ad,True
5.2 Why this de nition of consistency?,0,,False
"Test collection-based evaluation reduces many sources of variance that occur in information seeking in the wild to a level that is tractable from the standpoint of statistical analysis. Our de nition of consistency is predicated on having test collections that embody some plausible set of query variations per information need. We do not claim that it can address all possible sources of, or needs for, desirable consistency (or inconsistency) in information seeking.",1,ad,True
"Our de nition of consistency might be brought into question by queries that exhibit extrinsic diversity [31] or intrinsic diversity [32] or searching as learning [14]. Extrinsic diversity occurs when a query has many di erent information needs that might be associated with it, while intrinsic diversity addresses cases where there are multiple sub-tasks associated in satisfying the information need. Searching as learning involves evolving query expression throughout a session. For cases involving extrinsic diversity, test collections without query variations typically declare one information need and judge relevance with respect to that information need; other plausible information needs are ignored. From the standpoint of assessing consistency, our approach is the same and has the same aw of ignoring other information needs. For cases involving intrinsic diversity and searching as learning, approaches have been developed that target aspects of such complex evaluation situations, including TREC's Web track's Diversity task [10] and",1,ad,True
"the Session [20] and Tasks [42] tracks. For assessing just consistency, we suggest that test collections should involve more narrow information needs, with an emphasis on developing speci c unambiguous topic statements. Despite this, we believe that consistency is also important for systems that are able to accurately detect and respond to intrinsic diversity queries, and just as with narrower information needs, there will be a wide range of query variations for an information need that is intrinsically diverse.",1,Session,True
"One more question the reader might have is why measure consistency at all, and why not just go straight to average relevance. Two issues arise: rst, relevance judgments are a substantially more expensive resource to accumulate, particularly when dealing with test collections with thousands of query variations. Second, although two rankings may have identical relevance scores, they may be completely di erent. Consider an information need such as ""You are worried about the prevalence of fake news, and decide to nd authoritative newspapers to read instead, just like people did last century."" Now consider two rankings in response to two query variations, one of which lists [theguardian.com, zeit.de, .com, washingtonpost.com] and one of which lists [wsj.com, lemonde.fr, nytimes.com, theglobeandmail.com]. From a relevance standpoint, these rankings are e ectively identical, but from a consistency perspective, they share nothing. If we accept that a searcher cares about re-",1,ad,True
"nding the same information for an information need, even if they forget the precise query variation they used previously [37], then it is clear that being able to quantify consistency in rankings is not captured by relevance equivalence alone.",0,,False
6 ANALYSIS OF CONSISTENCY,0,,False
6.1 Consistency and topics,0,,False
"We address RQ-C1 by assessing Cti over the ve contributed runs described in Section 4.1. RQ-C1 asks whether topics vary in consistency, and to do this we plot the average and standard deviation of Cti against all 100 topics of UQV100, sorted by increasing Cti . We characterize this in two ways: Figure 2 shows the results for System 1 (where standard deviation is of the RBO scores per variation for the topic), while Figure 3 shows the results when aggregating over the Cti scores obtained from the ve systems. Even with the large standard deviations that can be observed in both plots (while being clearly smaller in the second), we can conclude that di erent topics have di erent consistency. For persistence  ,"" 0.9, approximately 25% of topics have consistency under 0.25, while around 12% have consistency greater than 0.5. We also observe that some topics have great variation in their consistency scores, and others much less; and, overall, that consistency does indeed vary across topics.""",1,ad,True
6.2 Consistency and topic attributes,0,,False
"In the rst part of RQ-C2 we ask whether there is a relationship between the number of query variations per topic ti and corresponding consistency scores Cti . Intuitively, we might expect that the more unique query variations there are for a topic, the lower the consistency scores. We address this aspect through a correlation analysis, using Spearman's , a non-parametric rank correlation statistic. Unlike Pearson's product-moment coe cient statistic, this does not rely on the data having equal variance or having few to no outliers. A sca er plot, not shown, demonstrated that these",1,ad,True
401,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
1.00,0,,False
Avg. RBO,0,,False
0.75 0.50 0.25 0.00,0,,False
0,0,,False
25,0,,False
50,0,,False
75,0,,False
100,0,,False
Topics,0,,False
"Figure 2: Consistency scores Cti for System 1, ordered by score, for 100 topics. Bars are ±1 s.d. of the underlying RBO values.",0,,False
1.00,0,,False
System,0,,False
(a) Num. variations,0,,False
(b) Est. docs,0,,False
(c) Est. queries,0,,False
1,0,,False
-0.35,0,,False
-0.30,0,,False
-0.43,0,,False
2,0,,False
-0.43,0,,False
-0.27,0,,False
-0.43,0,,False
3,0,,False
-0.42,0,,False
-0.35,0,,False
-0.45,0,,False
4,0,,False
-0.36,0,,False
-0.25,0,,False
-0.39,0,,False
5,0,,False
-0.37,0,,False
-0.36,0,,False
-0.43,0,,False
"Table 5: Correlation measured using Spearman's  between Cti and: (a) number of unique query variations per topic; (b) average estimated useful documents per topic; and (c) average estimated useful queries per topic, for each (system, topic) pair. In all cases, p < 0.01().",0,,False
1.00,0,,False
0.75,0,,False
Consistency,0,,False
Avg. average RBO,0,,False
0.75,0,,False
0.50 0.25,0,,False
0.00 0,0,,False
25,0,,False
50,0,,False
75,0,,False
100,0,,False
Topics,0,,False
"Figure 3: Average consistency scores Cti from ve systems, ordered by increasing score, for 100 topics. Bars are ±1 s.d. Note that",0,,False
the topics may not be in the same order as in Figure 2.,0,,False
"requirements might not hold. In Table 5(a), we show the results for all systems. As expected, the direction of the association is negative (Cti goes down when the number of query variations goes up). However, the correlations are relatively weak weak magnitude (0.3 <  < 0.5) so although there is an association, it is not something we can reliably anticipate. e corresponding sca er plot of values is not shown for space reasons, but con rms that there is a broad range of consistency scores as the number of query variations per topic changes. ese outcomes are a li le surprising, suggesting that the causes of increased consistency are complex.",1,ad,True
In the second part of RQ-C2 we ask whether there is a relationship between the estimated topic complexity and corresponding consistency scores. Estimated topic complexity is available as the average of the estimates of the number of useful documents (and the number of queries) expected by each person providing a query,0,,False
0.50,0,,False
0.25,0,,False
0.00,0,,False
1,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
Systems,0,,False
"Figure 4: Consistency scores Cti for ve systems and 100 topics. e diamond marks the median, and the horizontal line marks CT .",0,,False
"variation for a topic description in UQV100. More complex topics have higher values for these two averaged estimates. Again, intuitively we might expect that the more complex a topic is, the lower the consistency score. As above, we assess the relationship using Spearman's , for both the average estimated documents and average estimated queries per topic ti and corresponding consistency scores Cti . Results are shown in Table 5(b) and (c), and just as before indicate a negative association, as surmised. However, once again the correlations are weak at best, at best of weak magnitude, meaning that increases in estimated topic complexity are only loosely associated with decreases in consistency. Interestingly, the estimates of the required number of queries all have a stronger correlation than the corresponding estimates of the required number of useful documents. is outcome might in part arise because the complexity estimate providers may be be er at estimating changes in small numbers than in larger ones, but there are other possible explanations.",1,UQV,True
402,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
System,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
1,0,,False
-9.756 -10.497 -14.362 -1.030,0,,False
2,0,,False
-0.930 -6.821 9.107,0,,False
3,0,,False
-8.035 8.490,0,,False
4,0,,False
12.330,0,,False
Table 6: System di erences measured using a paired Student's,0,,False
"t-test between all pairs of systems over the corresponding Cti per topic. Values reported are the t statistic, df ,"" 99 in all cases, and""",0,,False
signi cant di erences at p < 0.05 () and p < 0.01 ().,0,,False
6.3 Consistency and systems,0,,False
"If consistency was identical across di erent retrieval systems, then it would be uninteresting when selecting an e ective system. In RQC3, we examine the relationship between collection consistency CT and the ve systems which contributed to UQV100. In Figure 4 we report on the consistency scores for each system, using boxplots to show the spread of values. From this we can observe that Systems 1 and 5 are very similar to each other (CT 0.32); Systems 2 and 3 are very similar to each other and more consistent (CT 0.40), and System 4 is di erent and yet more consistent (CT ,"" 0.44). Using a two-tailed paired Student's t-test, we also assessed each pair of systems; Table 6 con rms our observations.""",1,UQV,True
"Another way of understanding the relationship between consistency and systems is to treat each topic as an ""assessor"" and each system as the ""subject"" being assessed with regards to its degree of consistency. Since topics in UQV100 contain a similar number of query variations (average of 55 per topic) as many existing test collections have queries, and past practice has been to examine rank order correlation of systems by comparing sets of systems across two or more collections, we will adopt a similar method here. We assess how similar the relative ordering of systems is using one-way Intraclass Correlation [4], Kendall's Coe cient of Concordance [21] (commonly wri en as Kendall's W ), and Krippendor 's  [17]. All of these measures address inter-assessor agreement, for three or more assessors, and can accommodate ordinal data (and for ICC and , interval or ratio data). In Table 7, we report the results for these measures of rank agreement, where the score is the topic consistency Cti . We use these measures rather than pair-wise comparisons of topics using Kendall's  , since we have 100 topic ""assessors"" involved, and the measures allow us to consider multiple ""assessors"" with a single test statistic, while Kendall's  only supports two ""assessors"". Due to the di erences between ICC and Kendall's W , it is expected that ICC scores may be lower than Kendall's W scores over the same data, since it considers not just relative rank order but also the magnitude of di erences, as discussed by Sheskin [34]. In all three measures, 1 indicates perfect agreement among the assessors, and 0 indicates no agreement beyond what would be expected by chance. Both ICC and  can report small negative values, which also signify no agreement.",1,UQV,True
"e values from ICC and Krippendor 's  both indicate there is a very low degree of inter-assessor agreement in rank ordering the systems by consistency; and although Kendall's W is 0.491 for Cti , this is still a relatively low degree of agreement.",0,,False
"From these two analyses, we conclude that although these systems do have di erent overall collection consistency CT , they are",0,,False
Agreement,0,,False
(a) ICC,0,,False
(b),0,,False
(c),0,,False
Kendall's W Krippendor 's ,0,,False
Cti,0,,False
NDCG INST,0,,False
0.111,0,,False
-0.002 -0.005,0,,False
0.491,0,,False
0.169 0.033,0,,False
0.090,0,,False
-0.003 -0.006,0,,False
"Table 7: Rank agreement over all ve systems measured using (a) Intraclass Correlation; (b) Kendall'sW ; and (c) Krippendor 's . For ICC and W , signi cance is reported as p < 0.05(), and p < 0.01(); for  it is not reported. e top row uses Cti as the ranking score for each system; the bo om two rows use NDCG and INST (averaged by topic, as for Cti ).",0,,False
System AP NDCG Q RBP INST,1,AP,True
1,0,,False
0.61 0.68 0.56 0.32 0.35,0,,False
2,0,,False
0.62 0.70 0.58 0.33 0.37,0,,False
3,0,,False
0.55 0.62 0.53 0.32 0.36,0,,False
4,0,,False
0.59 0.65 0.55 0.39 0.40,0,,False
5,0,,False
0.59 0.65 0.53 0.25 0.30,0,,False
"Table 8: Correlation measured using Spearman's  between topic consistency Cti and corresponding relevance measures (AP, NDCG, Q measure, RBP0.85, and INST), for each (system, topic) pair. In all cases, there is a signi cant correlation, with p < 0.01 (), except for System 5 and RBP, signi cant only at p < 0.05 ().",1,AP,True
"not systematically ordered on the basis of topic consistency Cti . at is, for one topic a particular system may have high consistency,",0,,False
"while for another topic a completely di erent system may have high consistency, and the earlier system may have low consistency.",0,,False
6.4 Consistency and relevance,0,,False
"Our last investigation, addressing RQ-C4, concerns the relationship between consistency and relevance. e construction of UQV100 guaranteed relevance judgments to at least depth 10 for all query variations for the ve systems being analyzed. us we are able to explore the degree of correlation between consistency and relevance, across a range of relevance measures, including AP, NDCG, Q measure, RBP0.85, and INST. e results, calculated using Spearman's , are displayed in Table 8. While there is moderate correlation for the ""deep"" relevance measures (AP, NDCG, and Q), there is only weak correlation for the ""shallow"" relevance measures (RBP0.85 and INST). Sca er plots of the data, not shown, indicate considerable spread of scores for all measures as Cti increases.",1,ad,True
"As a comparison with consistency, we repeat the topics-as""assessors"" inter-assessor agreement analysis, with results reported for average-by-topic NDCG and INST scores in rows two and three of Table 7. While any degree of agreement was only just observable for consistency, with these relevance measures, any agreement on the ordering of systems by relevance is e ectively random.",0,,False
7 CONCLUSIONS,0,,False
Consistency ­ the ability to give similar results for a topic even when presented with di erent queries ­ is desirable for search engines in a variety of circumstances. We have de ned a consistency,0,,False
403,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"measure and explored it across a set of 5,736 query variations across 100 topics, using a novel relevance-based centroid algorithm.",0,,False
"e RBC algorithm for fusing rankings has the bene t of incorporating a persistence parameter that allows modeling of di erent depths of a ention into rankings, and adds another strand to the ""RB-"" family. RBC is competitive or be er than existing algorithms, and like Borda count has no reliance on system scores. We con-",1,corpora,True
"rmed previous ndings that data fusion over queries and over systems is bene cial, and fusion over both is even be er. With the oracle runs we have also demonstrated that substantially be er e ectiveness performance is possible, at least hypothetically.",0,,False
"Based on the various analyses, we can also state that the consistency measure informs us about something di erent to existing measures. Consistency varies by topic and by system, tends to decrease as topic complexity and the number of query variations increases, and has weak-to-moderate correlations with several relevance measures. However, in no circumstance is consistency strongly correlated with any of these existing test collection dimensions, con rming that it measures a di erent property altogether. Neither the measures of consistency nor relevance reliably order the",0,,False
"ve systems over the UQV100 topics, indicating there are no clear system winners or losers for this test collection on these dimensions of e ectiveness when examined topic by topic.",1,UQV,True
"More investigation is required into the nature of consistency and its e ect on perceptions of the retrieval e ectiveness of search systems. Such work might include user studies, low-level analysis of the root causes of variable ranking within one or more systems, and broadening the current analysis to similar test collections with multiple query variations per topic.",1,ad,True
"Acknowledgment is work was supported by the Australian Research Council's Discovery Projects Scheme (projects DP110101934 and DP140102655). Ma Crane, Xiaolu Lu, David Maxwell, and Andrew Trotman assisted greatly, providing the system runs that were analyzed. e UQV100 judgments were generated using resources provided by Microso .",1,UQV,True
REFERENCES,0,,False
"[1] J. A. Aslam and M. Montague. 2001. Models for metasearch. In Proc. SIGIR. ACM, 276­284.",0,,False
"[2] P. Bailey, A. Mo at, F. Scholer, and P. omas. 2015. User variability and IR system evaluation. In Proc. SIGIR. 625­634.",0,,False
"[3] P. Bailey, A. Mo at, F. Scholer, and P. omas. 2016. UQV100: A test collection with query variability. In Proc. SIGIR. 725­728. Public data: h p://dx.doi.org/10. 4225/49/5726E597B8376.",1,UQV,True
"[4] J. J. Bartko. 1966. e intraclass correlation coe cient as a measure of reliability. Psychological Reports 19, 1 (1966), 3­11.",0,,False
"[5] N. J. Belkin, C. Cool, W. B. Cro , and J. P. Callan. 1993. e e ect of multiple query representations on information retrieval system performance. In Proc. SIGIR. 339­346.",0,,False
"[6] N. J. Belkin, P. Kantor, E. A. Fox, and J. A. Shaw. 1995. Combining the evidence of multiple query representations for information retrieval. Inf. Proc. & Man. 31, 3 (1995), 431­448.",0,,False
[7] C. Buckley and J. Walz. 1999. e TREC-8 query track. In Proc. TREC.,1,TREC,True
"[8] J. Callan. 2002. Distributed information retrieval. In Advances in Information Retrieval. Springer, 127­150.",0,,False
[9] B. Cartere e. 2009. On rank correlation and the distance between rankings. In Proc. SIGIR. 436­443.,0,,False
"[10] K. Collins- ompson, C. Macdonald, P. N. Benne , F. Diaz, and E. M. Voorhees. 2014. TREC 2014 web track overview. In Proc. TREC.",1,TREC,True
[11] S. Cucerzan and E. Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proc. EMNLP. 293­300.,0,,False
"[12] C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. 2001. Rank aggregation methods for the web. In Proc. WWW. 613­622.",0,,False
[13] E. A. Fox and J. Shaw. 1993. Combination of multiple searches. In Proc. TREC. 243­252.,1,TREC,True
"[14] L. Freund, H. O'Brien, and R. Kopak. 2014. Ge ing the big picture: Supporting comprehension and learning in search. In Proc. Searching As Learning (SAL) Workshop.",0,,False
"[15] G. W. Furnas, T. K. Landauer, L. M. Gomez, and S. T. Dumais. 1987. e vocabulary problem in human-system communication. Comm. ACM 30, 11 (1987), 964­971.",0,,False
"[16] F. C. Gey, N. Kando, and C. Peters. 2005. Cross-language information retrieval: The way ahead. Inf. Proc. & Man. 41, 3 (2005), 415­431.",1,ad,True
"[17] A. F. Hayes and K. Krippendor . 2007. Answering the call for a standard reliability measure for coding data. Commun. Methods and Measures 1, 1 (2007), 77­89.",0,,False
"[18] J.-Y. Jiang, J. Liu, C.-Y. Lin, and P.-J. Cheng. 2015. Improving ranking consistency for web search by leveraging a knowledge base and search logs. In Proc. CIKM. 1441­1450.",0,,False
"[19] R. Jones, B. Rey, O. Madani, and W. Greiner. 2006. Generating query substitutions. In Proc. WWW. 387­396.",1,ad,True
"[20] E. Kanoulas, B. Cartere e, M. Hall, P. Clough, and M. Sanderson. 2011. Overview of the TREC 2011 session track. In Proc. TREC.",1,TREC,True
"[21] M. G. Kendall and B. B. Smith. 1939. e problem of m rankings. Annals of Mathematical Statistics 10, 3 (1939), 275­287.",0,,False
[22] B. Koopman and G. Zuccon. 2016. A test collection for matching patients to clinical trials. In Proc. SIGIR. 669­672.,0,,False
"[23] R. T.-W. Lo, B. He, and I. Ounis. 2005. Automatically building a stopword list for an information retrieval system. J. Dig. Inf. Man. 3, 1 (2005), 3­8.",0,,False
"[24] J. B. Lovins. 1968. Development of a stemming algorithm. MIT Information Processing Group, Electronic Systems Laboratory Cambridge.",0,,False
"[25] H. P. Luhn. 1957. A statistical approach to mechanized encoding and searching of literary information. IBM J. Res. Dev. 1, 4 (1957), 309­317.",0,,False
[26] A. Mo at. 2016. Judgment pool e ects caused by query variations. In Proc. Aust. Doc. Comp. Symp. 65­68.,0,,False
"[27] A. Mo at and J. Zobel. 2008. Rank-biased precision for measurement of retrieval e ectiveness. ACM Trans. Inf. Sys. 27, 1 (2008), 2.1­2.27.",0,,False
[28] M. Montague and J. A. Aslam. 2001. Relevance score normalization for metasearch. In Proc. CIKM. 427­433.,0,,False
[29] M. Montague and J. A. Aslam. 2002. Condorcet fusion for improved retrieval. In Proc. CIKM. 538­548.,0,,False
"[30] J. Pickens, G. Golovchinsky, C. Shah, P. Qvarfordt, and M. Back. 2008. Algorithmic mediation for collaborative exploratory search. In Proc. SIGIR. 315­322.",0,,False
"[31] F. Radlinski, P. N. Benne , B. Cartere e, and T. Joachims. 2009. Redundancy, diversity and interdependent document relevance. SIGIR Forum 43, 2 (2009), 46­52.",1,ad,True
"[32] K. Raman, P. N. Benne , and K. Collins- ompson. 2013. Toward whole-session relevance: Exploring intrinsic diversity in web search. In Proc. SIGIR. 463­472.",0,,False
"[33] S. E. Robertson. 1990. On term selection for query expansion. J. Documentation 46, 4 (1990), 359­364.",0,,False
[34] D. J. Sheskin. 2003. Handbook of Parametric and Nonparametric Statistical Procedures. CRC Press.,0,,False
"[35] K. Spa¨rck Jones and C. J. van Rijsbergen. 1975. Report on the need for and the provision of an ""ideal"" information retrieval test collection. Technical Report 5266. Computer Laboratory, University of Cambridge. British Library Research and Development Report.",0,,False
"[36] L. Tan and C. L. A. Clarke. 2015. A family of rank similarity measures based on maximized e ectiveness di erence. IEEE Trans. Know. Data Eng. 27, 11 (2015), 2865­2877.",0,,False
"[37] J. Teevan, E. Adar, R. Jones, and M. A. S. Po s. 2007. Information re-retrieval: Repeat queries in Yahoo's logs. In Proc. SIGIR. 151­158.",1,Yahoo,True
"[38] W. Webber, A. Mo at, and J. Zobel. 2010. A similarity measure for inde nite rankings. ACM Trans. Inf. Sys. 28, 4 (2010), 20.1­20.38.",0,,False
"[39] M. Wu, D. Hawking, A. Turpin, and F. Scholer. 2012. Using anchor text for homepage and topic distillation search tasks. JASIST 63, 6 (2012), 1235­1255.",0,,False
"[40] S. Wu and S. McClean. 2006. Performance prediction of data fusion for information retrieval. Inf. Proc. & Man. 42 (2006), 899­915.",0,,False
"[41] E. Yilmaz, J. A. Aslam, and S. Robertson. 2008. A new rank correlation coe cient for information retrieval. In Proc. SIGIR. 587­594.",0,,False
"[42] E. Yilmaz, M. Verma, R. Mehrotra, E. Kanoulas, B. Cartere e, and N. Craswell. 2015. Overview of the TREC 2015 tasks track. In Proc. TREC.",1,TREC,True
"[43] G. Zuccon, J. Palo i, and A. Hanbury. 2016. ery variations and their e ect on comparing information retrieval systems. In Proc. CIKM. 691­700.",0,,False
404,0,,False
,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Comparing In Situ and Multidimensional Relevance Judgments,0,,False
Jiepu Jiang,0,,False
"Center for Intelligent Information Retrieval, University of Massachuse s",0,,False
Amherst jpjiang@cs.umass.edu,0,,False
Daqing He,0,,False
"School of Computing and Information, University of Pi sburgh",0,,False
dah44@pi .edu,0,,False
James Allan,0,,False
"Center for Intelligent Information Retrieval, University of Massachuse s",0,,False
Amherst allan@cs.umass.edu,0,,False
ABSTRACT,0,,False
"To address concerns of TREC-style relevance judgments, we explore two improvements. e rst one seeks to make relevance judgments contextual, collecting in situ feedback of users in an interactive search session and embracing usefulness as the primary judgment criterion. e second one collects multidimensional assessments to complement relevance or usefulness judgments, with four distinct alternative aspects examined in this paper--novelty, understandability, reliability, and e ort.",1,ad,True
"We evaluate di erent types of judgments by correlating them with six user experience measures collected from a lab user study. Results show that switching from TREC-style relevance criteria to usefulness is fruitful, but in situ judgments do not exhibit clear bene ts over the judgments collected without context. In contrast, combining relevance or usefulness with the four alternative judgments consistently improves the correlation with user experience measures, suggesting future IR systems should adopt multi-aspect search result judgments in development and evaluation.",1,TREC,True
"We further examine implicit feedback techniques for predicting these judgments. We nd that click dwell time, a popular indicator of search result quality, is able to predict some but not all dimensions of the judgments. We enrich the current implicit feedback methods using post-click user interaction in a search session and achieve be er prediction for all six dimensions of judgments.",0,,False
KEYWORDS,0,,False
Relevance judgment; search experience; implicit feedback.,0,,False
1 INTRODUCTION,1,DUC,True
"Test collection-based IR evaluation relies on human assessments of search result quality. e most popular method is the Cran eldstyle relevance judgments [9], such as the approach used in TREC [10], where assessors (usually trained experts) judge a preassigned set of search results one a er another using criteria that focus on topical relevance. is method had achieved great success but also a racted criticism such as focusing solely on topical relevance and ignoring real users' perceptions of the usefulness of results in a particular search context. We examine two directions to improve this status quo.",1,TREC,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080840",1,ad,True
"One direction is to incorporate context into assessments. at is, the value of a search result depends on the scenario and context of accessing the result. Belkin et al. [5] proposed to evaluate interactive search systems by the usefulness of each interaction for accomplishing a search task. We can apply this model to search result judgments--to assess the usefulness of a click (the perceived usefulness of a clicked result). is intrinsically requires us to switch from relevance to usefulness as the primary judgment criteria, and to collect in situ judgments to take into account the particular time and context of accessing a search result.",1,corpora,True
"Two recent e orts [25, 36] examined this direction. Kim et al. [25] collected users' in situ feedback of clicked results a er they had",1,ad,True
"nished examining the results. However, they restricted the in situ feedback to ""thumbs-up"" or ""thumbs-down"". Mao et al. [36] asked users to assess the usefulness of the clicked results a er a search session without considering the particular context. Both studies reported improved correlations with search experience measures comparing to TREC-style relevance judgments by external assessors. However, neither study excluded the in uence of the di erence between searchers and external assessors on relevance judgments.",1,TREC,True
"e other direction is to use a combination of multiple aspects of judgments. Many previous studies tried to complement relevance with seemingly reasonable dimensions, such as novelty [6, 55], understandability [41, 56], credibility [39, 46, 51, 53], readability [42, 49], e ort [20, 50, 54], freshness [11], etc. Multidimensional judgments are also popular approaches used in user-centric evaluation models [19, 27, 52]. However, most previous IR studies had only examined one particular alternative dimension to relevance, and they had not veri ed the value of multidimensional judgments by correlating with user experience measures.",1,ad,True
"We evaluate and compare these two directions. We collected users' search result judgments from six dimensions (relevance, usefulness, novelty, understandability, reliability, and e ort) in two se ings--an in situ one that happened right a er users had nished examining a clicked search result (called in situ judgments), and a context-independent one collected a er a search session (called post-session judgments). We evaluate the two types of judgments on six dimensions by correlating with six search experience measures collected from a laboratory user study. We also examined implicit feedback methods for predicting these judgments.",1,ad,True
We examine the following questions in the rest of this article:,0,,False
· Do in situ judgments be er correlate with search experience measures than context-independent (post-session) ones? Do multiple dimensions of judgments help relevance/usefulness judgments be er correlate with search experience measures? Which dimensions of judgments should we collect to improve a particular user experience measure? Section 3 seeks answers to these questions.,0,,False
405,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 1: A screenshot of the search interface and the in situ judgments interface.,0,,False
· Can we e ectively predict di erent search result judgments using implicit feedback signals? Section 4 and Section 5 examine techniques for addressing this challenge.,1,ad,True
2 USER STUDY,0,,False
We designed a user study to collect search result judgments. e user study asked participants to work on di erent tasks in an experimental search system. We recorded users' search behavior and collected their in situ and post-session search result judgments.,0,,False
2.1 Experiment Design,0,,False
e user study employed a 2×2 within-subject design to balance di erent types of search tasks. e tasks come from the TREC session tracks [7] and were categorized into four types by the targeted task product and goal based on Li and Belkin's faceted classi cation framework [28]. e targeted task product is either factual (to locate facts) or intellectual (to enhance the user's understanding of a topic). e goal of a task is either speci c (clear and fully developed) or amorphous (an ill-de ned or unclear goal that may evolve along with the user's exploration).,1,TREC,True
We divided participants into groups of four. Participants in the same group worked on the same four tasks (one task for each type) but using a di erent sequence (rotated using a Latin square). We assigned di erent tasks to di erent groups to increase task diversity.,0,,False
"For each task, the participants went through two stages: · Search stage (10 minutes). e participants performed an in-",0,,False
"teractive search session to address the task. ey could submit and reformulate any queries and click on any search results. After clicking on a result's link, the participants switched to the result webpage in a new browser tab. When they had nished examining the result and turned back to the SERP, the participant needed to provide in situ judgments on the clicked results before they could resume the search session. Figure 1 shows the screenshots of the search interface and the in situ judgments. · Judgment stage (about 10 minutes). e participants rated their search experience in the session and nished post-session judgments on each result they visited in the session. Section 2.2 introduces details of the in situ and post-session judgments. As Figure 1 shows, the interface of the experimental system is similar to popular web search engines. e system redirected users' queries to Google and returned ltered Google search results. e system only showed the ""10-blue links"", vertical search results",1,ad,True
"(except image verticals), and related queries. Other SERP elements were removed to simplify the user study. e system displayed results in the same way they would appear on Google. e main di erence between our system and Google in SERP design was that our system showed task description on the top of a SERP (to help participants recall task requirements) and we showed related searches on the right side of a SERP.",0,,False
"e participants spent about 100 minutes to nish an experiment. First, they worked on a training task (including all the steps) for 10 minutes. en, they worked on four formal tasks, spending about 20 minutes on each task. We required the participants to take a 5-minute break a er two formal tasks to reduce fatigue.",0,,False
2.2 Collecting Search Result Judgments,0,,False
We collected search result judgments in two di erent scenarios: · In situ judgments ­ participants assessed a clicked result when,0,,False
they had nished examining it and turned back to the SERP. · Post-session judgments ­ the judgments collected a er a search,1,ad,True
session (in the judgment stage). e in situ judgments measure the participants' perceptions of,0,,False
"the clicked result at (roughly) the same time and contexts they visit the result. e approach is similar to Kim et al. [25], except that we adopted di erent measures to assess search results. In the search stage, we instructed the participants to examine results as they would normally do when using a search engine in their daily lives. For example, they did not need to fully read a result and they could abandon examining. Particularly, they were instructed that during the in situ judgments, they should not revisit the result for the purpose of answering the judgment questions (and we did not o er a link for revisiting in the in situ judgment interface). is is to ensure that the in situ judgments only measure participants' perceptions of the latest click activity.",1,ad,True
"e post-session judgments resemble the TREC-style relevance judgments, where the assessors judge results without a particular search context and in a random order--they are asked to judge a set of results one a er another in detail. In our post-session judgments, the assessors are real searchers. We asked them to judge the set of results they visited in the session. We instructed them to examine the results in a be er detail in the post-session judgments. e system also required participants to revisit each clicked result and spend at least 30 seconds to judge a result.",1,TREC,True
406,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: estions for collecting search result judgments and users' search experience.,0,,False
Search Result Judgments,0,,False
Topical Relevance (TRel),0,,False
Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),1,Novelty,True
"estion & Options How relevant is this webpage? · Key (3): this page or site is dedicated to the topic; authoritative and comprehensive; it is worthy of being a top result. · Highly Relevant (2): the content of this page provides substantial information on the topic. · Relevant (1): the content of this page provides some information on the topic, which may be minimal. · Not Relevant or Spam (0). In Situ: How much useful information did you get from this web page? From 1 (none) to 7 (a lot of). Post-session: How much useful information did this web page provide for the task? From 1 (none) to 7 (a lot of). How much new information did you get from this web page? From 1 (none) to 7 (a lot of). How much e ort did you spend on this web page? From 1 (none) to 7 (a lot of). How di cult was it for you to follow the content of this web page? From 1 (very di cult) to 7 (very easy). How trustworthy is the information in this web page? From 1 (not at all trustworthy) to 7 (very trustworthy).",0,,False
Search Experience Measures Satisfaction (Sat) Frustration (Frus) System Helpfulness (Help) Goal Success (Succ) Session E ort (S.Eff) Di culty (Diff),1,Session,True
estion & Options How satis ed were you with your search experience? From 1 (very unsatis ed) to 7 (very satis ed). How frustrated were you with this task? From 1 (not frustrated) to 7 (very frustrated). How well did the system help you in this task? From 1 (very badly) to 7 (very well). How well did you ful ll the goal of this task? From 1 (very badly) to 7 (very well). How much e ort did this task take? From 1 (minimum) to 7 (a lot of). How di cult was this task? From 1 (very easy) to 7 (very di cult).,1,ad,True
We collected users' in situ and post-session judgments of six different measures. Table 1 shows the detailed questions and options.,0,,False
"· TREC relevance (TRel) ­ the de facto standard of relevance judgments due to the popularity of TREC test collections. We collected TRel using the criteria of the latest TREC web track [10]. As Table 1 shows, the criteria focus on topical relevance. We excluded the relevance level Nav (the correct homepage of a navigational query) from the original TREC criteria because our search tasks do not include navigational search.",1,TREC,True
"· Usefulness (Usef) ­ Following Belkin et al.'s model [5] and Mao et al.'s study [36], we collected users' perceptions regarding the usefulness of the clicked results.",0,,False
"· Novelty (Nov) ­ Novelty was o en assessed algorithmically in previous studies based on sub-topic or ""nugget"" level relevance judgments [8, 40, 43, 55]. In contrast, we collect users' explicit novelty judgments.",1,Novelty,True
· Understandability (Under) ­ the easiness of understanding the content of the result. Recent studies incorporated understandability into search result ranking [41] and evaluation [56].,1,corpora,True
"· Reliability (Relia) ­ the reliability, credibility, and trustworthy of the information presented in the result [39, 46, 51] (here we do not distinguish the three constructs).",0,,False
· E ort ­ Yilmaz et al. [54] and Verma et al. [50] examined e ort as a dimension of evaluating search result.,0,,False
"e following table summarizes the measures collected in in situ and post-session judgments. We only collected TRel in post-session judgments because the TREC criteria do not consider context. We only collected Nov and Effort during in situ judgments because the participants of a pilot study reported confusions assessing the two measures twice. In the rest of this paper, we will use .i and .p su xes to denote in situ and post-session judgments, respectively. For example, Usef.i denotes users' in situ usefulness judgments.",1,TREC,True
"Except for TRel, we collected judgments using a 7-point Likert scale, because a previous study [48] showed that assessors approximate the optimal level of con dence when using a 7-point scale for relevance judgments. TRel used a di erent scale so that it is",0,,False
TREC relevance (TRel) Usefulness (Usef) Novelty (Nov) E ort (Effort) Understandability (Under) Reliability (Relia),1,TREC,True
In Situ (.i),0,,False
Post-session (.p),0,,False
consistent with the TREC web track (as a representative example of the state-of-the-art relevance judgment methods).,1,TREC,True
2.3 Search Experience Measures,0,,False
"In the judgment stage, participants rated their search experience in a session. We collected six representative user experience measures used in previous studies of information retrieval and recommender systems--satisfaction (Sat) [17, 21, 26, 35, 36, 45], goal success (Succ) [1, 18], frustration (Frus) [12, 13], task di culty (Diff) [4, 15, 29, 31, 32], the helpfulness of the system (Help) [19] and the total e ort spent (S.Eff) [27]. Table 1 includes the questions.",0,,False
2.4 Rationale of Experiment Design,0,,False
"e way we balance di erent types of tasks is similar to previous studies [22, 24, 30, 33, 36]. However, we acknowledge that the selected tasks cannot cover all varieties. It is also worth noting that the TREC session track tasks [7] are more complex than regular web search requests such as navigational search.",1,TREC,True
"Our study aims to collect both in situ judgments and user behaviors related to the clicked results. is poses challenges to the experiment design. On the one hand, we hope to collect accurate in situ judgments, which o en requires multi-item measurements [27, 52]. On the other hand, interrupting participants for in situ judgments breaks the ow of search session and can a ect their subsequent search behaviors. To balance between the two purposes, we made a few compromises in experiment design, e.g., we only collected six popular dimensions of judgments, and we simply used one question to measure each dimension.",1,ad,True
407,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 2: Spearman's correlation () matrix of di erent judgments for the 727 unique clicks.,0,,False
In Situ Judgments,0,,False
Post-session Judgments,0,,False
Usef.i Novelty Effort Under.i Relia.i TRel Usef.p Under.p,1,Novelty,True
Novelty,1,Novelty,True
0.67,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
In Situ,0,,False
E ort,0,,False
0.22,0,,False
Understandability,0,,False
0.20,0,,False
0.24,0,,False
-,0,,False
0.14 -0.45,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
Reliability,0,,False
0.42,0,,False
0.37,0,,False
0.05,0,,False
0.26,0,,False
-,0,,False
-,0,,False
-,0,,False
-,0,,False
Topical Relevance,0,,False
0.63,0,,False
0.46,0,,False
0.16,0,,False
0.14,0,,False
0.42,0,,False
-,0,,False
-,0,,False
-,0,,False
Post-session,0,,False
Usefulness Understandability,0,,False
0.72 0.20,0,,False
0.52,0,,False
0.16,0,,False
0.18 -0.36,0,,False
0.18 0.68,0,,False
0.43 0.83 0.29 0.18,0,,False
0.24,0,,False
-,0,,False
Reliability,0,,False
0.43,0,,False
0.38,0,,False
0.04,0,,False
0.22,0,,False
0.82 0.48,0,,False
0.51,0,,False
0.31,0,,False
e reported values are estimated from 1000 bootstrap samples (we used strati ed sampling to balance user and task dependency).,0,,False
"While examining search behaviors, we excluded the time spent on answering in situ judgments from dwell time. On average the participants spent 57.1 seconds on a clicked result and 12.1 seconds to answer the ve in situ judgment questions.",0,,False
2.5 Collected Data,0,,False
We recruited 28 participants (16 are female) through iers posted on the campuses of two universities in the United States. We required participants to be English native speakers to exclude the in uence of language uency on relevance judgments [16]. All the participants were undergraduate or graduate students studying di erent elds.,1,ad,True
"ey were reimbursed $15 per hour. We collected 112 sessions by 28 participants on 28 tasks. Each participant worked on four unique tasks and each task was performed by four unique users. In total, we collected 537 queries (4.8 per session) and 736 clicks (6.6 per session) on 727 unique sessionURL pairs (9 cases of revisiting). We exclude the 9 cases of revisiting from the analysis (about 1% of the data) to simply the analysis.",0,,False
3 IN SITU VS. POST-SESSION JUDGMENTS,0,,False
3.1 Correlation of Di erent Judgments,0,,False
"Table 2 reports the correlation of di erent judgments, which are generally consistent with previous studies. For example, relevance and usefulness positively correlate with novelty and reliability [52], understandability negatively correlates with e ort [50], etc. We examined the relationship of the judgments in another article [23].",0,,False
"Note that Mao et al. [36] reported a weak correlation (0.332) of searchers' post-session usefulness judgments and external assessors' relevance judgments. However, Table 2 shows that TRel and Usef.p are strongly correlated ( ,"" 0.83) when both of them are assessed by searchers. is suggests that the low correlation reported by Mao et al. [36] may be mostly due to the disparity between searchers and external assessors, rather than the di erence between using relevance or usefulness as the judgment criteria.""",0,,False
3.2 Correlating with User Experience,0,,False
"We evaluate di erent search result judgments by correlating with (regressing) users' search experience measures in a session. is is based on the assumption that the ""quality"" of the clicked results in a session can in uence users' search experience in that session--thus, a reasonable search result judgment (assumed to indicate certain ""quality""), or a reasonable set of judgments, should also correlate with users' search experience in a session.",0,,False
3.2.1 Regression Analysis. We use multilevel regression analysis,0,,False
to examine the relationship between the judgments of the clicked,0,,False
results and users' search experience in a session. e dependent,0,,False
variables (DVs) are each of the six search experience measures. e,0,,False
independent variables (IVs) include the statistics of judgments re-,0,,False
"garding the clicked results in a session (such as the mean, maximum,",0,,False
"and minimum ratings). For TRel, Usef.i, and Usef.p, we include",0,,False
"the mean, maximum, and minimum ratings of the clicked results in",0,,False
a session as IVs in the regression analysis. For other search result,0,,False
"judgments, we only include the maximum and minimum ratings of",0,,False
the clicked results as IVs. is is because the mean ratings of the,0,,False
"other measures o en highly correlate with those of TRel and Usef,",0,,False
causing multicollinearity issues for regression analysis.,0,,False
"For each user experience measure (the DV), we examine six",0,,False
di erent models that include di erent judgments as IVs.,0,,False
· Unidimensional & Context-independent ­ Model 1 and 2,0,,False
only include context-independent search result judgments from,0,,False
a single dimension--Model 1 includes the statistics of TRel and,0,,False
Model 2 includes those of Usef.p.,0,,False
· Unidimensional & In Situ ­ Model 3 includes in situ judg-,0,,False
ments from a single dimension (the statistics of Usef.i) as IVs.,0,,False
· Multidimensional & Context-independent ­ Model 4 and,0,,False
5 extend Model 1 and 2 to include other dimensions of judg-,0,,False
"ments (the statistics of Under.p, Relia.p, Nov, and Effort).",0,,False
Note that Model 4 and 5 include two in situ judgments (Nov,0,,False
and Effort) because we did not collect post-session judgments,0,,False
on these two dimensions (as discussed in § 2.2).,0,,False
· Multidimensional & In Situ ­ Model 6 extends Model 3 to,0,,False
"include other dimensions of judgments (the statistics of Under.i,",0,,False
"Relia.i, Nov, and Effort).",0,,False
Contextindependent,0,,False
In Situ,0,,False
Unidimensional,0,,False
1 TRel only 2 Usef.p only,0,,False
3 Usef.i only,0,,False
Multidimensional,0,,False
4 TRel + others 5 Usef.p + others,0,,False
6 Usef + others,0,,False
"All six models also include the same set of control variables, including: gender (Male or Female), age (four levels; 0 for 18­24, 1 for 25­30, 2 for 31­40, and 3 for Over 40), highest degree obtained or expected (Undergraduate or Graduate), the expertise of using web search engines (SE Expertise) rated using a Likert scale from 1 (very badly) to 5 (very well), task product and goal, user's familiarity with the topic of the task (Topic Familiarity) rated using a Likert scale from 1 (very unfamiliar) to 7 (very familiar), and the number of clicks (# clicks) and queries (# queries) in the session.",1,ad,True
408,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 3: e adjusted R2 of di erent regression models.,1,ad,True
Models,0,,False
Sat Frus Succ S.Eff Help,0,,False
Base (control only) 0.12 0.06 0.11 0.06 0.11,0,,False
1 TRel,0,,False
0.23 0.09 0.18 0.10 0.16,0,,False
2 Usef.p,0,,False
0.25 0.15 0.36 0.17 0.18,0,,False
3 Usef.i,0,,False
0.29 0.14 0.35 0.16 0.22,0,,False
4 TRel + others 0.31 0.25 0.33 0.33 0.31,0,,False
4 vs. 1,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
5 Usef.p + others 0.30 0.26 0.42 0.37 0.31,0,,False
5 vs. 2,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
6 Usef.i + others 0.30 0.27 0.34 0.37 0.27,0,,False
6 vs. 3,0,,False
**,0,,False
**,0,,False
*,0,,False
* and ** indicate p < 0.05 and p < 0.01 by F-test.,0,,False
Diff,0,,False
0.03 0.09 0.18 0.22 0.21,0,,False
** 0.26,0,,False
** 0.33,0,,False
**,0,,False
We examine multicollinearity between variables using variance,0,,False
"in ation factor (VIF). e IVs of all models satisfy VIF < 4, the com-",0,,False
monly suggested threshold (4­10) for concerns of multicollinearity issues [37]. Table 3 reports the adjusted R2 of the six models for,1,ad,True
regressing the six dimensions of search experience.,0,,False
"3.2.2 TREC Relevance vs. Usefulness. We rst compare TREC relevance criteria (TRel) and post-session usefulness judgments (Usef.p). is is a revisit of Mao et al.'s study [36], which compared searchers' usefulness judgments and external assessors' relevance judgments. Here we collected both judgments from real searchers, removing the in uence caused by the di erence between searchers and external annotators in relevance judgments. e regression analysis suggest that switching from TREC relevance to usefulness is fruitful, consistently enhancing the ability of the regression models to correlate with user experience (by adjusted R2).",1,TREC,True
"Models 1 and 2 include the mean, maximum, and minimum TRel or Usef.p ratings of the clicked results. Model 2 consistently explains the six search experience measures be er than Model 1 (by adjusted R2). We note that usefulness (Usef.p) seems to be particularly be er than TREC relevance (TRel) in terms of correlating with goal success (Succ), with adjusted R2 , 0.36 vs 0.18.",1,ad,True
"Models 4 and 5 further include other dimensions of judgments as IVs. is helps compare TRel and Usef.p judgments with other search result judgments as controls. Still, we consistently observe that Model 5 explains the six search experience measures be er than or as well as model 4 . ese results verify that usefulness is indeed a be er criteria of relevance judgments than TREC-style relevance (in terms of correlating with users' search experience).",1,TREC,True
"3.2.3 In Situ vs. Context-independent (Post-session) Judgments. We further compare in situ and post-session judgments in both unidimensional and multidimensional se ings. Results suggest in situ usefulness judgments have be er correlations with a few (but not all) user experience measures than post-session usefulness judgments. However, a er combining search result judgments from di erent dimensions, in situ judgments show limited advantages over post-session ones.",1,ad,True
"Models 3 and 2 include the mean, maximum, and minimum Usef.i or Usef.p ratings of the clicked results as IVs. Results show Model 3 explains satisfaction (Sat), helpfulness (Help), and task di culty (Di ) slightly be er than Model 2 , with about 0.04 di erence in adjusted R2.",1,ad,True
"We further compare in situ and post-session judgments in a multidimensional se ing, using a combination of Usef.p/Usef.i",0,,False
"and other four judgments as IVs (Models 5 and 6 ). Results show that the post-session multidimensional model ( 5 ) be er correlates with search success (Succ) than the in situ one (adjusted R2 0.42 vs 0.34), but the la er also be er correlates with task di culty (adjusted R2 0.26 vs. 0.21). Overall, no evidence suggests either model is consistently be er than another in terms of correlating with users' search experience measures.",1,ad,True
"Even though Model 3 (Usef.i only) performs slightly be er than Model 2 (Usef.p only), results suggest limited advantages of in situ judgments over post-session ones in terms of correlating with search experience measures. We suspect a possible reason is that a 10-minute session is not long enough to trigger su cient di erences between in situ and post-session judgments. Although we expect to observe a greater di erence between in situ and post-session judgments in longer sessions, we believe a substantial proportion of web search sessions are no longer than 10 minutes, which may not bene t much from in situ judgments. In addition, it also requires a more complex experiment design to collect in situ judgments.",1,ad,True
"3.2.4 Unidimensional vs. Multidimensional Judgments. We further compare models using a combination of multiple aspects of judgments (Models 4 , 5 , and 6 ) with those using a single dimension (Models 1 , 2 , and 3 ). Results suggest that it is almost always helpful (enhancing the correlation with most of the six search experience measures signi cantly) to complement either relevance or usefulness with the alternative dimensions.",0,,False
"Models 4 and 5 explain all six dimensions of search experience measures signi cantly be er than Models 1 and 2 , suggesting that multidimensional judgments are almost always helpful for TREC-style relevance judgments (TRel) and post-session usefulness judgments (Usef.p). We also note that in situ usefulness judgments (Usef.i) worked particularly well for correlating with users' satisfaction (Sat) and goal success (Succ), such that combining with more dimensions of judgments adds li le to the model.",1,TREC,True
"Results demonstrate that multidimensional search result judgments are helpful, complementing unidimensional judgments and yielding be er correlation with search experience measures. is also suggests the advantages of multidimensional search result judgments over the in situ one--the former can consistently improve relevance/usefulness to be er correlate with almost all user experience measures, while the la er shows limited advantages.",1,ad,True
3.3 Which Dimensions To Judge?,0,,False
"A crucial issue of information retrieval is deciding which criteria to use to rank search results. We come to initial answers by looking into the standardized coe cients () of Model 5 (Table 4) as an example due to its superiority over other models. e standardized coe cient  stands for the magnitude of change in the DV (relative to its standard deviation) caused by one-unit change in the IV (relative to the IV's standard deviation) while other variables being equal. e coe cients of the model indicate how changes in the ""quality"" of the clicked results will (theoretically) a ect users' search experience in a session. Table 4 suggests that: · To enhance user satisfaction, a search system should present",0,,False
useful and novel results--both Usef.p (mean) and Nov (max) show signi cant positive e ects on Sat in Model 5 .,0,,False
409,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: Multilevel regression: standardized coe cients () of independent variables for Model 5 ­ Usef.p + others.,0,,False
Independent,0,,False
DV: session-level search experience,0,,False
Variables,0,,False
Sat Frus Succ S.Eff Help Diff,0,,False
Gender: Male Age Degree: Graduate SE Expertise,1,ad,True
0.10 -0.05 -0.03,0,,False
0.12,0,,False
0.19 0.09 0.06 0.00 0.16 0.00 -0.03 -0.02 -0.11 0.00 0.05 0.01 0.13 -0.08 0.23 0.04 0.08 0.01 0.12 -0.00,0,,False
Product: Factual Goal: Speci c Topic Familiarity,0,,False
0.02 -0.02 -0.06 -0.09 -0.00 0.02 0.02 -0.07 0.04 0.05 0.07 0.01 0.10 -0.23 0.17 -0.20 0.19 -0.24,0,,False
# clicks,0,,False
0.20 -0.12 0.17 -0.07 0.18 -0.01,0,,False
# queries,0,,False
-0.36 0.17 -0.25 0.16 -0.35 -0.02,0,,False
 Usef.p (mean) 0.23 -0.38 0.36 -0.36 0.08 -0.43,0,,False
 Usef.p (max)  Usef.p (min),0,,False
0.16 0.09 0.22 -0.07 0.11 -0.08 0.01 0.19 -0.04 0.19 0.01 0.18,0,,False
 Nov (max)  Nov (min),0,,False
0.24 -0.10 0.18 -0.09 -0.01 -0.20 -0.08 -0.06,0,,False
0.25 -0.11 0.07 -0.00,0,,False
 Under.p (max)  Under.p (min),0,,False
0.09 -0.27 0.16 -0.08,0,,False
0.30 -0.15 0.14 -0.26,0,,False
0.14 -0.22 0.29 -0.27,0,,False
 Relia.p (max) -0.13 -0.08 0.01 0.05 -0.03 0.06,0,,False
 Relia.p (min) 0.06 0.01 -0.05 0.08 -0.07 0.04,0,,False
 Effort (max) -0.12 0.16 0.08 0.28 -0.13 0.02,0,,False
 Effort (min) Adjusted R2,0,,False
0.21 0.04 0.12 0.01 0.25 0.02 0.30 0.26 0.42 0.37 0.31 0.26,0,,False
"Light and dark shadings indicate p < 0.05 and 0.01, respectively.",1,ad,True
"· To reduce user frustration, a search system should o er results that are useful and easy-to-understand--both Usef.p (mean) and Under.p (max) show signi cant negative e ects on Frus.",0,,False
"· To help users successfully reach the goal (Succ), a search system should retrieve useful, novel, and easy-to-understand results-- Usef.p (mean), Nov (max), and Under.p (max) show signi cant positive e ects on Succ.",0,,False
"· To reduce the total e ort of a search session, the system should retrieve easy-to-understand results and avoid those requiring too much e ort--Under.p (min) shows a signi cant negative e ect on S.Eff and Effort (max) shows a positive one.",0,,False
"· To be er help users in a session (enhance the helpfulness of the system), a system should retrieve novel and easy-to-understand results--both Nov (max) and Under.p (max) show signi cant positive e ects on Help.",0,,False
"· To reduce the perceived task di culty, we need to retrieve useful and easy-to-understand results--both Usef.p (mean) and Under.p (min) show signi cant negative e ects on Diff. e coe cients suggest that the mean usefulness of the clicked",0,,False
"results is helpful for explaining all six search experience measures (has statistically signi cant coe cients). In addition, novelty, understandability, and e ort also signi cantly relate to many di erent search experience measures, suggesting they are useful complements to usefulness in search result judgments. In contrast, reliability shows no signi cant e ect on any of the six user experience measures in Model 5 . However, we suspect this is because the top-ranked results returned by Google are mostly reliable ones, which makes reliability a less important judgment measure among the clicked results.",1,ad,True
Table 5: Statistics of the absolute di erence of two users' ratings on the same results (||).,0,,False
Usef.i Effort Nov Relia.i Under.i TRel Usef.p Relia.p Under.p,0,,False
| | mean (SD) 1.55 (1.45) 1.52 (1.25) 1.60 (1.47) 1.23 (1.21) 1.18 (1.23) 0.63 (0.68) 1.53 (1.54) 1.38 (1.35) 1.08 (1.31),0,,False
"|| , 0 25.9% 22.9% 26.4% 31.3% 34.8% 48.3% 29.9% 30.8% 38.8%",0,,False
||  1 58.2% 57.7% 54.2% 67.2% 68.2% 89.1% 60.7% 62.2% 76.6%,0,,False
||  2 79.1% 76.1% 78.1% 86.1% 88.1% 100.0% 77.6% 81.1% 90.5%,0,,False
3.4 Variability of Judgments,0,,False
"We further examine the variability of judgments among di erent searchers, because in many practical scenarios we may have to train and evaluate retrieval systems based on relevance judgments made by external assessors. We suspect di erent users may have a greater degree of inconsistencies in their in situ judgments than their post-session ones (due to the contextual nature of the former). However, results do not support this conjecture well.",1,ad,True
"We examine the absolute di erence of two users' ratings on the same result. Table 5 reports the mean absolute di erence and the distribution. e mean absolute di erence for in situ and postsession usefulness judgments (Usef.i and Usef.p) are very close (1.55 vs. 1.53). e mean absolute di erence of post-session reliability judgments (Relia.p) is slightly higher than that for in situ ones (Relia.i) (1.38 vs. 1.23), but that for post-session understandability judgments (Under.p) is also slightly lower than the in situ ones (Under.i, 1.08 vs. 1.18). Overall, no evidence suggests that either in situ or post-session judgments is more or less consistent than the other across di erent users.",0,,False
"Further, we note that di erent users' reliability and understandability judgments seem more consistent than those for usefulness, e ort, and novelty judgments, regardless of performed in an in situ se ing or a post-session one. is suggests that usefulness, e ort, and novelty judgments may su er from inter-rate consistency by a greater extent, while inter-rate agreement is less likely a concern for understandability and reliability judgments. However, since users judged TRel by a di erent scale, it remains unclear how do the other ve judgments compare with standard TREC relevance judgments in terms of inter-rate consistency.",1,TREC,True
3.5 Summary,0,,False
"To sum up, this section discloses both opportunities and challenges for future search result judgments. · Opportunity ­ Since a combination of multidimensional judg-",0,,False
"ments explains user experience measures be er than using relevance or usefulness alone, we expect that an appropriate ranking of search results by multiple criteria may potentially yield be er user experience as well. e results in Table 4 also help select ranking criteria according to a targeted user experience measure. · Challenge ­ Extending current judgments from a single dimension to multiple aspects largely increases the cost of judgments.",0,,False
is is a crucial issue for the scalability of multidimensional judgments. e following sections address this concern by predicting multidimensional judgments using implicit feedback techniques.,1,ad,True
410,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 6: Implicit feedback features and their correlation with di erent search result quality measures.,0,,False
Pearson's r with search result judgments,0,,False
Click Dwell Time Features,0,,False
Note,0,,False
TRel Usef.p Nov Effort Under.p Relia.p,0,,False
T1 Click dwell time (log).,0,,False
0.38,0,,False
0.43 0.41,0,,False
0.36,0,,False
0.12,0,,False
0.34,0,,False
T2 T3 T4 T5,0,,False
(t - µ )/ . t is the result's dwell time; µ is average click dwell time;  is the standard deviation of click dwell time. T3-5 are based on personalized versions of µ and  .,0,,False
all clicks by user by task by length,0,,False
0.31,0,,False
0.34 0.30,0,,False
0.32,0,,False
0.31,0,,False
0.36 0.38,0,,False
0.32,0,,False
0.31,0,,False
0.35 0.30,0,,False
0.32,0,,False
0.29,0,,False
0.33 0.29,0,,False
0.31,0,,False
0.06 0.09 0.06 0.06,0,,False
0.24 0.24 0.24 0.24,0,,False
Follow-up ery Features,0,,False
Q1,0,,False
Q2,0,,False
e number of terms in the next query found in the URL/title/body,0,,False
Q3 of the result.,0,,False
URL title body,0,,False
TRel,0,,False
-0.04 -0.03 -0.03,0,,False
Usef.p,0,,False
0.03 -0.00 -0.03,0,,False
Nov,0,,False
-0.01 -0.00,0,,False
0.10,0,,False
Effort,0,,False
-0.02 0.01 0.06,0,,False
Under.p,0,,False
-0.02 -0.00,0,,False
0.03,0,,False
Relia.p,0,,False
0.03 -0.02 -0.02,0,,False
Q4,0,,False
e percentage of terms in the next query found in the,0,,False
Q5 URL/title/body of the result.,0,,False
Q6,0,,False
URL title body,0,,False
0.02 0.09 0.02 -0.04,0,,False
0.01,0,,False
0.08,0,,False
0.07 0.10 0.06 -0.00,0,,False
0.05,0,,False
0.07,0,,False
0.17,0,,False
0.18 0.21,0,,False
0.04,0,,False
0.13,0,,False
0.18,0,,False
Q7,0,,False
e number of newly added query terms in the next query refor-,1,ad,True
Q8 mulation found in the URL/title/body of the result.,0,,False
Q9,0,,False
URL title body,0,,False
0.03 -0.01 -0.03 -0.06 0.07 0.04 0.00 -0.07 0.07 0.07 0.13 -0.04,0,,False
0.02,0,,False
0.02,0,,False
0.01 -0.00,0,,False
0.04 -0.02,0,,False
Q10,0,,False
e number of removed query terms in the next query reformula-,0,,False
Q11 tion found in the URL/title/body of the result.,0,,False
Q12,0,,False
URL title body,0,,False
0.01 0.01 -0.00 -0.07 -0.04 -0.12,0,,False
0.06 0.09 0.06 -0.09,0,,False
0.03 -0.06,0,,False
0.08,0,,False
0.07 0.06,0,,False
0.01,0,,False
-0.09,0,,False
-0.04,0,,False
Q13,0,,False
e mean/max/min log likelihood scores between the full content,0,,False
Q14 of the result and follow-up queries.,0,,False
Q15,0,,False
mean max min,0,,False
0.22 0.23 0.22 -0.03,0,,False
0.19,0,,False
0.23,0,,False
0.22 0.23 0.21 -0.03,0,,False
0.17,0,,False
0.18,0,,False
0.15 0.19 0.19 -0.01,0,,False
0.17,0,,False
0.19,0,,False
Follow-up Click Features,0,,False
TRel Usef.p Nov Effort Under.p Relia.p,0,,False
C1,0,,False
e mean/max/min similarity between the title of the result and,0,,False
C2 the titles of clicked results in follow-up searches.,0,,False
C3,0,,False
mean max min,0,,False
0.04,0,,False
0.05 0.12,0,,False
0.02,0,,False
0.04,0,,False
0.01,0,,False
0.05,0,,False
0.04 0.09,0,,False
0.00,0,,False
0.06,0,,False
0.00,0,,False
0.06,0,,False
0.08 0.10,0,,False
0.01,0,,False
-0.01,0,,False
0.03,0,,False
C4,0,,False
e mean/max/min similarity between the snippet of the result,0,,False
C5 and the snippets of clicked results in follow-up searches.,0,,False
C6,0,,False
mean max min,0,,False
-0.00 -0.04,0,,False
0.08,0,,False
0.01 -0.06,0,,False
0.11,0,,False
0.01 -0.06,0,,False
0.10,0,,False
0.04 -0.05,0,,False
0.07,0,,False
0.02,0,,False
0.03,0,,False
0.01 -0.04,0,,False
0.06,0,,False
0.09,0,,False
C7,0,,False
e mean/max/min similarity between the full content of the result,0,,False
C8 and the full contents of SAT clicks (dwell time > 30s) in follow-up,0,,False
C9 searches.,0,,False
C10,0,,False
e mean/max/min similarity between the title of the result and,0,,False
C11 the titles of skipped results in follow-up searches.,0,,False
C12,0,,False
mean max min mean max min,0,,False
0.19,0,,False
0.23 0.09,0,,False
0.01,0,,False
-0.02,0,,False
0.10,0,,False
0.20 0.20 0.05 -0.00,0,,False
0.00,0,,False
0.08,0,,False
0.12 0.17 0.07 -0.00 -0.01,0,,False
0.07,0,,False
0.09,0,,False
0.11 0.11,0,,False
0.02,0,,False
0.05,0,,False
0.05,0,,False
0.11 0.09 0.06 -0.01,0,,False
0.05,0,,False
0.02,0,,False
0.09 0.13 0.13 -0.05,0,,False
0.00,0,,False
0.05,0,,False
C13,0,,False
e mean/max/min similarity between the snippet of the result,0,,False
C14 and the snippets of skipped results in follow-up searches.,0,,False
C15,0,,False
mean max min,0,,False
0.01,0,,False
0.03 0.03,0,,False
0.11,0,,False
-0.05,0,,False
0.03,0,,False
0.02 0.01 -0.01 -0.00,0,,False
0.02 -0.02,0,,False
0.10,0,,False
0.12 0.12,0,,False
0.13,0,,False
-0.05,0,,False
0.11,0,,False
"Light and dark shadings indicate the correlation is signi cant at 0.05 and 0.01 levels, respectively.",1,ad,True
4 PREDICTION,0,,False
"is section introduces our techniques for predicting multidimensional judgments of clicked results from search logs. We model the prediction task as a regression problem--the input is features related to a target click, the output is the predicted judgment score of the clicked result. We use gradient boosted regression trees (GBRT) for prediction. Table 6 lists the prediction features. Due to the limited space, we only report results for predicting TRel and the judgments included in Model 5 --Usef.p, Nov, Effort, Under.p, and Relia.p. However, the described approach can also e ectively predict other search result judgments as well.",1,ad,True
4.1 Click Dwell Time Features,0,,False
"Click dwell time (T1) is one of the most widely used implicit feedback measure. As Table 6 shows, T1 does not correlate much with understandability, but it still has 0.3­0.4 correlations (signi cant at 0.01 level) with other measures.",0,,False
T2­T5 measure the deviation of a click's dwell time from the mean dwell time (µ) of a group of clicks (normalized by the standard deviation  ). T2 computes µ and  based on all clicks in the training sets. T3 is based on clicks by the same user. T4 is based on clicks in sessions with the same task type. T5 is based on clicks on documents with similar length (we divide the clicked results into ten bins by length and compute µ and  of a click based on its bin).,0,,False
4.2 Follow-up ery Features,0,,False
Follow-up query features are based on the intuition that a clicked result may in uence follow-up query reformulation in a session.,0,,False
"us, we can infer the quality of a click from queries issued a er the clicked result in the same session.",0,,False
Q1­Q6 match the terms in the immediate follow-up query with the target click. Q7­Q12 match the newly added and removed terms in the immediate follow-up query reformulation with the target click. Q13­Q15 match the target click with all follow-up queries.,1,ad,True
411,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Many of the follow-up query features (such as Q6 and Q13­Q15) have signi cant correlations with the search result quality measures, con rming that the intuition is reasonable. We also note that Q6 and Q13­Q15 have stronger correlations with understandability than click dwell time features.",0,,False
4.3 Follow-up Click Features,0,,False
"Similar to follow-up query features, we may also infer the quality of a target click based on follow-up clicks in a session.",0,,False
"C1­C6 measure the similarity between the target click and followup clicks. C7­C9 measure the similarity with follow-up satisfactory (SAT) clicks. C10­C15 measure the similarity with follow-up skipped results (unclicked results ranked higher than a clicked result). Some features have signi cant correlations with the search result quality measures, suggesting they may be useful predictors.",0,,False
4.4 Prior-to-click Features (Baseline),0,,False
"Prior-to-click features include the existing techniques that predict search result quality measures using information available before users clicking on the result. In this paper, they serve as the baseline for the implicit feedback features. We include a full list of prior-toclick features in an online appendix1.",0,,False
"We incorporate di erent prior-to-click features for predicting di erent measures. e shared features for all six measures include the rank of the result by Google search, ad hoc search models (QL, BM25, DFR [3], and SDM [38]), and session search models [14, 47].",1,corpora,True
e unique features for predicting each measure are: · TRel ­ a subset of LETOR features [34]. · Usef.p ­ a subset of LETOR features [34] and a subset of the,0,,False
"usefulness features by Mao et al. [36] that do not rely on postclick information. · Nov ­ the similarity of the click with previous clicks and higher ranked results in the same SERP (motivated by previous work on novelty-based search result diversi cation [6, 43, 44, 55]). · Effort ­ Yilmaz et al. [54] and Verma et al. [50]. · Under.p ­ Palo i et al. [41, 42]. · Relia.p ­ Olteanu et al. [39] and Wawer et al. [51]. Our prior-to-click features are representatives of the state-of-theart techniques for predicting each dimension of judgments without using implicit feedback. However, we did not include features that we do not have the resource to calculate, which include link structure based features and social media popularity features such as Twi er mention. Note this may reduce the e ectiveness of predicting reliability since the excluded features take about 1/3 of the features by Olteanu et al. [39] and Wawer et al. [51].",0,,False
5 EVALUATION,0,,False
5.1 Experiment Settings,0,,False
"We evaluate prediction (regression) by the Pearson's correlation between the predicted values and actual judgments (prediction correlation) and the root mean square error (RMSE) of the predicted values. Note that the RMSE for predicting di erent measures is not comparable-- rst, TREC relevance ranges from 0­3 while others from 1­7; second, their distributions vary a lot. Here we only report",1,TREC,True
1 h p://ciir.cs.umass.edu/downloads/mdrel/,1,ad,True
prediction correlation for its easy interpretability. e results of RMSE is highly consistent with that using prediction correlation.,0,,False
"e dataset for evaluation includes multidimensional judgments on the 727 unique clicked results. We use 10-fold cross validation for evaluation (using eight folds for training, one for validation, and one for testing). We randomly shu e the dataset 10 times and apply 10-fold cross-validation for each random shu ing of the whole dataset--this generates prediction results on 10 × 10 ,"" 100 test folds in total (note that we are not using a 100-fold cross validation). We report the mean and standard deviation (SD) of prediction correlation on the test folds. We note that the prediction correlation reported in this section is di erent from and cannot be compared with the correlation in Table 6, which are computed for the whole dataset without cross validation.""",0,,False
5.2 Click Dwell Time Features,0,,False
"Current techniques for inferring search result quality from logs rely on click dwell time. Results ( 1 in Table 7) suggest the click dwell time features work reasonably well for predicting usefulness, novelty, and e ort, but they have di culties inferring the understandability and reliability of results.",0,,False
"e click dwell time features ( 1 ) are e ective predictors for usefulness, novelty, and e ort. For these three measures, the predicted values have about 0.3­0.4 mean Pearson's correlation with the actual judgments, which is comparable to that for predicting TREC relevance (mean r ,"" 0.35). However, the click dwell time features perform much worse for predicting understandability and reliability. On average the predicted and actual judgments have only 0.10 and 0.22 correlation, suggesting it is necessary to incorporate new implicit feedback signals.""",1,TREC,True
5.3 Follow-up ery and Click Features,0,,False
We extend click dwell time to include signals from follow-up search activities. Results suggest the new features are helpful.,0,,False
"e follow-up query ( 2 ) and click features ( 3 ) alone have limited prediction capability. However, combining them with the click dwell time features ( 4 ) consistently produces be er prediction than using click dwell time features alone ( 1 ): except for e ort, the prediction correlation for the other ve measures using feature set 4 is signi cantly be er than that for click dwell time features ( 1 ). is indicates that follow-up queries and clicks indeed provide useful implicit feedback that are complementary to click dwell time.",0,,False
"e follow-up query and click features are particularly helpful for predicting reliability. Combining them with the click dwell time features improves the mean correlation of prediction from 0.22 to 0.36. e new features are also helpful for predicting TREC relevance and usefulness as well. is partly con rms our intuition--the quality of a clicked result may in uence follow-up search activities, making it possible to infer the quality of a clicked result based on what happened a erward in the session.",1,TREC,True
"e new features also improved the mean prediction correlation for understandability from 0.10 to 0.20. However, we note the combination of all implicit feedback features still does not work well for predicting understandability (mean r ,"" 0.20). is suggests that, compared with other judgments, it is more challenging to predict understandability based on the implicit feedback information.""",0,,False
412,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 7: e e ectiveness of di erent features for predicting multidimensional search result judgments.,0,,False
Mean (SD) Pearson's r between true and predicted judgments over the test folds,0,,False
Features,0,,False
TRel,0,,False
Usef.p,0,,False
Nov,0,,False
Effort,0,,False
Under.p,0,,False
Relia.p,0,,False
1 Click Dwell Time,0,,False
0.35 (0.11),0,,False
0.40 (0.11),0,,False
0.42 (0.11),0,,False
0.31 (0.10),0,,False
0.10 (0.14),0,,False
0.22 (0.13),0,,False
2 Follow-up ery,0,,False
0.19 (0.11),0,,False
0.17 (0.14),0,,False
0.13 (0.13),0,,False
0.12 (0.11),0,,False
0.14 (0.12),0,,False
0.19 (0.12),0,,False
3 Follow-up Click,0,,False
0.15 (0.12),0,,False
0.20 (0.11),0,,False
0.11 (0.12),0,,False
0.14 (0.11),0,,False
0.11 (0.12),0,,False
0.17 (0.12),0,,False
4 All ( 1 + 2 + 3 ),0,,False
0.39 (0.09),0,,False
0.46 (0.08),0,,False
0.45 (0.09),0,,False
0.33 (0.11),0,,False
0.20 (0.13),0,,False
0.36 (0.12),0,,False
1 vs. 4,0,,False
**,0,,False
**,0,,False
*,0,,False
**,0,,False
**,0,,False
5 Prior-to-click,0,,False
0.36 (0.10),0,,False
0.29 (0.10),0,,False
0.28 (0.11),0,,False
0.13 (0.12),0,,False
0.20 (0.14),0,,False
0.18 (0.13),0,,False
4 vs. 5,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
6 All+Prior-to-click,0,,False
0.45 (0.08),0,,False
0.49 (0.09),0,,False
0.47 (0.09),0,,False
0.39 (0.09),0,,False
0.26 (0.12),0,,False
0.40 (0.11),0,,False
5 vs. 6,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
**,0,,False
* and ** indicate the di erence is statistically signi cant at 0.05 and 0.01 levels by two-tail paired t -test.,0,,False
5.4 Comparing to Prior-to-click Features,0,,False
An important application of implicit feedback techniques is to infer relevance labels from search logs. Aggregating inferred relevance labels or implicit feedback signals from past search logs may help rank search results in the future [2]. We examine whether or not implicit feedback techniques can serve a similar purpose for multidimensional judgments.,0,,False
"e combination of the implicit feedback features and the priorto-click features ( 6 ) generated signi cantly be er prediction results on all the six judgments than using the prior-to-click features alone ( 5 ). is suggests that the implicit feedback features are indeed helpful and complementary to the prior-to-click features for predicting these judgments. We also note that the improvements in mean prediction correlation can be as large as over 0.2 (such as for predicting reliability and e ort). However, even combining the two sets of features still cannot adequately predict understandability (mean r , 0.26).",1,ad,True
6 DISCUSSION AND CONCLUSION,0,,False
"A crucial issue of information retrieval is deciding which criteria to use to rank search results. We compared two seemingly reasonable directions for improving current TREC-style relevance judgments. One direction is to collect in situ search result judgments. e other one is to complement a single dimension of judgments (such as relevance or usefulness) by combining with other aspects. We found that the la er direction seems more e ective and versatile-- using a combination of di erent dimensions of judgments, we can almost always improve correlation with user experience measures.",1,TREC,True
"We envision future search engines should rank results by multiple aspects. We also o ered initial suggestions on which criteria to adopt and when to adopt them. We further examined and improved implicit feedback techniques for predicting multiple judgments, addressing the scalability concern of applying multidimensional judgments in real web search applications.",1,ad,True
Our study makes the following contributions: · We evaluated and compared in situ usefulness judgments with,0,,False
"regular relevance/usefulness judgments by searchers. We show that using usefulness as the judgment criteria is fruitful, but in situ judgments do not show clear bene ts over regular ones. · We evaluate multidimensional search result judgments considering four alternative aspects other than relevance/usefulness. We show that multidimensional judgments be er correlate with user",0,,False
"experience measures than using relevance/usefulness judgments alone. We also note that multidimensional judgments is a be er direction for improving TREC-style relevance judgments. · Our study also discloses the connections between di erent user experience measures and various dimensions of search result judgments. is o ers practical suggestions for system design, such as the appropriate dimensions to judge search results for the purpose of improving a particular user experience measure. · We successfully generalize implicit feedback signals to include follow-up searches and clicks in a search session to help click dwell time be er predict multidimensional judgments. To the best of our knowledge, we are also the rst to examine the e ectiveness of implicit feedback approaches for predicting novelty, understandability, reliability, and e ort.",1,TREC,True
Our work also sheds lights on a few critical areas for exploration in the future:,0,,False
"An important line of future work is to provide more accurate criteria for search result ranking and evaluation. Based on a regression analysis, we have already o ered initial suggestions on what criteria to use and when to use them, as discussed in Section 3.3. We note that, with a su ciently large dataset, one can possibly learn a prediction model for search experience measures by taking multidimensional judgments of results as input. Such a model can further address issues such as what are the proper weights to put on di erent aspects when ranking search results. It may also solve the discrepancy between o ine evaluation measures and user experience measures, and ultimately serve as a be er objective function for training ranking models.",1,ad,True
"Another important application is to perform multidimensional ranking of search results based on implicit feedback signals and other information. We have already demonstrated that implicit feedback approaches can infer judgments of usefulness, novelty, e ort, and reliability with reasonable accuracy comparing to those for relevance labels. Aggregating such inferred judgments from past search logs may serve as useful features for performing multidimensional search result ranking in the future. However, we also note that our current technique needs to be improved to be er infer understandability of results from search logs.",1,ad,True
"We do admit certain limitations in our current study. First, our analysis and experiments are solely based on data collected from one laboratory user study, which is limited in both scale and representativeness. We suggest that further studies employ larger",1,ad,True
413,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"datasets to verify our ndings. Second, it is worth noting that our way of collecting in situ judgments in uenced users' natural search behaviors. We observed in our log that users spent on average 12.1 seconds to nish the in situ judgments. us some particular user behavior pa erns may vary when applied to another scenario (without interrupting users for in situ judgments). ird, we also note that we only collected search result judgments for the clicked results, while it remains unclear to which extent the ndings can be generalized to the unclicked ones. Last but not least, the collected post-session judgments are more or less in uenced by the search session and the in situ judgments (although we meant to collect context-independent judgments such as to compare with contextual ones). It is also worth noting that our post-session judgments are not fully representative of the existing TREC-style approach.",1,TREC,True
7 ACKNOWLEDGMENT,0,,False
"is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.",0,,False
REFERENCES,0,,False
"[1] M. Ageev, Q. Guo, D. Lagun, and E. Agichtein. Find it if you can: A game for modeling di erent types of web search success using interaction data. In SIGIR '11, pages 345­354, 2011.",0,,False
"[2] E. Agichtein, E. Brill, and S. Dumais. Improving web search ranking by incorporating user behavior information. In SIGIR '06, pages 19­26, 2006.",1,corpora,True
"[3] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.",0,,False
"[4] J. Arguello. Predicting search task di culty. In ECIR '14, pages 88­99, 2014. [5] N. J. Belkin, M. J. Cole, and J. Liu. A model for evaluation of interactive infor-",0,,False
"mation retrieval. In Proceedings of the SIGIR 2009 Workshop on the Future of IR Evaluation, 2009. [6] J. Carbonell and J. Goldstein. e use of MMR, diversity-based reranking for reordering documents and producing summaries. In SIGIR '98, pages 335­336, 1998. [7] B. Cartere e, P. Clough, M. Hall, E. Kanoulas, and M. Sanderson. Evaluating retrieval over sessions: e TREC session track 2011-2014. In SIGIR '16, pages 685­688, 2016. [8] C. L. Clarke, M. Kolla, G. V. Cormack, O. Vechtomova, A. Ashkan, S. Bu¨ cher, and I. MacKinnon. Novelty and diversity in information retrieval evaluation. In SIGIR '08, pages 659­666, 2008. [9] C. W. Cleverdon. e evaluation of systems used in information retrieval. In Proceedings of the International Conference on Scienti c Information, pages 687­ 698, 1959. [10] K. Collins- ompson, C. Macdonald, P. Benne , F. Diaz, and E. Voorhees. TREC 2014 web track overview. In TREC 2014, 2014. [11] N. Dai, M. Shokouhi, and B. D. Davison. Learning to rank for freshness and relevance. In SIGIR '11, pages 95­104, 2011. [12] H. A. Feild and J. Allan. Modeling searcher frustration. In HCIR '09, pages 5­8, 2009. [13] H. A. Feild, J. Allan, and R. Jones. Predicting searcher frustration. In SIGIR '10, pages 34­41, 2010. [14] D. Guan, S. Zhang, and H. Yang. Utilizing query change for session search. In SIGIR '13, pages 453­462, 2013. [15] J. Gwizdka. Revisiting search task di culty: Behavioral and individual di erence measures. In ASIS&T '08, 2008. [16] P. Hansen and J. Karlgren. E ects of foreign language and task scenario on relevance assessment. J. Doc., 61(5):623­639, 2005. [17] A. Hassan. A semi-supervised approach to modeling web search satisfaction. In SIGIR '12, pages 275­284, 2012. [18] A. Hassan, R. Jones, and K. L. Klinkner. Beyond DCG: User behavior as a predictor of a successful search. In WSDM '10, pages 221­230, 2010. [19] R. Hu and P. Pu. A study on user perception of personality-based recommender systems. In UMAP '10, pages 291­302, 2010. [20] J. Jiang and J. Allan. Adaptive e ort for search evaluation metrics. In ECIR '16, pages 187­199, 2016.",1,TREC,True
"[21] J. Jiang, A. Hassan Awadallah, X. Shi, and R. W. White. Understanding and predicting graded search satisfaction. In WSDM '15, pages 57­66, 2015.",1,ad,True
"[22] J. Jiang, D. He, and J. Allan. Searching, browsing, and clicking in a search session: Changes in user behavior by task and over time. In SIGIR '14, pages 607­616, 2014.",0,,False
"[23] J. Jiang, D. He, D. Kelly, and J. Allan. Understanding ephemeral state of relevance. In CHIIR '17, pages 137­146, 2017.",0,,False
"[24] D. Kelly, J. Arguello, A. Edwards, and W.-c. Wu. Development and evaluation of search tasks for IIR experiments using a cognitive complexity framework. In ICTIR '15, pages 101­110, 2015.",0,,False
"[25] J. Y. Kim, J. Teevan, and N. Craswell. Explicit in situ user feedback for web search results. In SIGIR '16, pages 829­832, 2016.",0,,False
"[26] J. Kiseleva, E. Crestan, R. Brigo, and R. Di el. Modelling and detecting changes in user satisfaction. In CIKM '14, pages 1449­1458, 2014.",0,,False
"[27] B. P. Knijnenburg, M. C. Willemsen, Z. Gantner, H. Soncu, and C. Newell. Explaining the user experience of recommender systems. User Modeling and User-Adapted Interaction, 22(4-5):441­504, 2012.",0,,False
"[28] Y. Li and N. J. Belkin. A faceted approach to conceptualizing tasks in information seeking. Inf. Process. Manage., 44(6):1822­1837, 2008.",0,,False
"[29] C. Liu, J. Liu, and N. J. Belkin. Predicting search task di culty at di erent search stages. In CIKM '14, pages 569­578, 2014.",0,,False
"[30] J. Liu, J. Gwizdka, C. Liu, and N. J. Belkin. Predicting task di culty for di erent task types. In ASIS&T '10, 2010.",0,,False
"[31] J. Liu, C. Liu, M. Cole, N. J. Belkin, and X. Zhang. Exploring and predicting search task di culty. In CIKM '12, pages 1313­1322, 2012.",0,,False
"[32] J. Liu, C. Liu, J. Gwizdka, and N. J. Belkin. Can search systems detect users' task di culty?: Some behavioral signals. In SIGIR '10, pages 845­846, 2010.",0,,False
"[33] J. Liu, C. Liu, X. Yuan, and N. J. Belkin. Understanding searchers' perception of task di culty: Relationships with task type. In ASIS&T '11, 2011.",0,,False
"[34] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. LETOR: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 workshop on learning to rank for information retrieval, pages 3­10, 2007.",0,,False
"[35] Y. Liu, Y. Chen, J. Tang, J. Sun, M. Zhang, S. Ma, and X. Zhu. Di erent users, di erent opinions: Predicting search satisfaction with mouse movement information. In SIGIR '15, pages 493­502, 2015.",0,,False
"[36] J. Mao, Y. Liu, K. Zhou, J.-Y. Nie, J. Song, M. Zhang, S. Ma, J. Sun, and H. Luo. When does relevance mean usefulness and user satisfaction in web search? In SIGIR '16, pages 463­472, 2016.",0,,False
"[37] S. Menard. Applied Logistic Regression Analysis. Sage, 1997. [38] D. Metzler and W. B. Cro . A markov random eld model for term dependencies.",0,,False
"In SIGIR '05, pages 472­479, 2005. [39] A. Olteanu, S. Peshterliev, X. Liu, and K. Aberer. Web credibility: Features",0,,False
"exploration and credibility prediction. In ECIR '13, pages 557­568, 2013. [40] P. Over. e TREC interactive track: An annotated bibliography. Inf. Process.",1,TREC,True
"Manage., 37(3):369­381, 2001. [41] J. Palo i, L. Goeuriot, G. Zuccon, and A. Hanbury. Ranking health web pages",0,,False
"with relevance and understandability. In SIGIR '16, pages 965­968, 2016. [42] J. Palo i, G. Zuccon, and A. Hanbury. e in uence of pre-processing on the",0,,False
"estimation of readability of web documents. In CIKM '15, pages 1763­1766, 2015. [43] D. Ra ei, K. Bharat, and A. Shukla. Diversifying web search results. In WWW",1,ad,True
"'10, pages 781­790, 2010. [44] R. L. Santos, C. Macdonald, and I. Ounis. On the role of novelty for search result",0,,False
"diversi cation. Inf. Retr., 15(5):478­502, 2012. [45] A. Schuth, K. Hofmann, and F. Radlinski. Predicting search satisfaction metrics",1,ad,True
"with interleaved comparisons. In SIGIR '15, pages 463­472, 2015. [46] J. Schwarz and M. Morris. Augmenting web pages and search results to support",0,,False
"credibility assessment. In CHI '11, pages 1245­1254, 2011. [47] X. Shen, B. Tan, and C. Zhai. Context-sensitive information retrieval using",0,,False
"implicit feedback. In SIGIR '05, pages 43­50, 2005. [48] R. Tang, W. M. Shaw, Jr., and J. L. Vevea. Towards the identi cation of the optimal",0,,False
"number of relevance categories. J. Am. Soc. Inf. Sci., 50(3):254­264, 1999. [49] J. van Doorn, D. Odijk, D. M. Roijers, and M. de Rijke. Balancing relevance",0,,False
"criteria through multi-objective optimization. In SIGIR '16, pages 769­772, 2016. [50] M. Verma, E. Yilmaz, and N. Craswell. On obtaining e ort based judgements for",0,,False
"information retrieval. In WSDM '16, pages 277­286, 2016. [51] A. Wawer, R. Nielek, and A. Wierzbicki. Predicting webpage credibility using",0,,False
"linguistic features. In WWW '14 Companion, pages 1135­1140, 2014. [52] Y. Xu and Z. Chen. Relevance judgment: What do information users consider",0,,False
"beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57(7):961­973, 2006. [53] Y. Yamamoto and K. Tanaka. Enhancing credibility judgment of web search",0,,False
"results. In CHI '11, pages 1235­1244, 2011. [54] E. Yilmaz, M. Verma, N. Craswell, F. Radlinski, and P. Bailey. Relevance and",1,ad,True
"e ort: An analysis of document utility. In CIKM '14, pages 91­100, 2014. [55] C. X. Zhai, W. W. Cohen, and J. La erty. Beyond independent relevance: Methods",0,,False
"and evaluation metrics for subtopic retrieval. In SIGIR '03, pages 10­17, 2003. [56] G. Zuccon. Understandability biased evaluation for information retrieval. In",0,,False
"ECIR '16, pages 280­292, 2016.",0,,False
414,0,,False
,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Online In-Situ Interleaved Evaluation of Real-Time Push Notification Systems,0,,False
"Adam Roegiest, Luchen Tan, and Jimmy Lin",0,,False
"David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada",1,ad,True
"{aroegies,luchen.tan,jimmylin}@uwaterloo.ca",0,,False
ABSTRACT,0,,False
"Real-time push noti cation systems monitor continuous document streams such as social media posts and alert users to relevant content directly on their mobile devices. We describe a user study of such systems in the context of the TREC 2016 Real-Time Summarization Track, where system updates are immediately delivered as push noti cations to the mobile devices of a cohort of users. Our study represents, to our knowledge, the rst deployment of an interleaved evaluation framework for prospective information needs, and also provides an opportunity to examine user behavior in a realistic se ing. Results of our online in-situ evaluation are correlated against the results a more traditional post-hoc batch evaluation. We observe substantial correlations between many online and batch evaluation metrics, especially for those that share the same basic design (e.g., are utility-based). For some metrics, we observe li le correlation, but are able to identify the volume of messages that a system pushes as one major source of di erences.",1,TREC,True
1 INTRODUCTION,1,DUC,True
"ere is growing interest in systems that address prospective information needs against continuous document streams, exempli ed by social media services such as Twi er. We might imagine a user having some number of ""interest pro les"" representing prospective information needs, and the system's task is to automatically monitor the stream of documents to keep the user up to date on topics of interest. For example, a journalist might be interested in collisions involving autonomous vehicles and wishes to receive updates whenever such an event occurs. Although there are a number of ways such updates can be delivered, we consider the case where they are immediately pushed to the user's mobile device as noti cations. At a high level, these push noti cations must be relevant, novel, and timely.",1,ad,True
"To date, there have been two formal evaluations of the push noti cation problem, at the TREC 2015 Microblog Track [15] and the TREC 2016 Real-Time Summarization (RTS) Track [16]. Despite the obvious real-time nature of this problem, systems have been assessed with a post-hoc batch evaluation methodology. It seems",1,TREC,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080808",1,ad,True
obvious that the push noti cation task should be evaluated in an online manner that be er matches how content is actually delivered in operational se ings.,0,,False
"We describe a user study of real-time push noti cation systems in the context of the TREC 2016 RTS Track, in which systems' noti cations are delivered to users' mobile devices as soon as they are generated. is evaluation is online, in contrast to post-hoc batch evaluations, and in-situ, in that the users are going about their daily activities and are interrupted by the systems' output. Since the RTS Track deployed both this online in-situ methodology and a more traditional batch methodology, the setup provided us with an opportunity to compare the results of both.",1,TREC,True
"Contributions. We view our work as having two main contributions: First, we describe, to our knowledge, the rst user study and actual deployment of an interleaved evaluation for prospective noti-",0,,False
cations. Our work is based on a previously-proposed interleaving framework [18] that has only been examined in simulation. We present an analysis of user behavior in such an evaluation methodology and demonstrate that it is workable in practice.,0,,False
"Second, we compare results of our online methodology to a more traditional batch methodology in the same evaluation. A number of metrics for assessing push noti cation systems have been proposed: we observe substantial correlations between many online and batch metrics, particularly those that share the same basic design (e.g., are utility-based). is is a non-obvious nding, since all judgments in our online methodology are sparse and made locally, with respect to one tweet at a time, whereas the batch evaluation methodology takes into account all relevant tweets via a global clustering process.",1,ad,True
ere are two interpretations of this nding:,0,,False
"· If one believes in the primacy of user-centered evaluations, our results suggest that established batch evaluation metrics are able to capture user preferences.",0,,False
"· On the other hand, our online evaluation methodology is less mature than the batch evaluation methodology, which has been extensively examined over the past several years; its core ideas date back at least a decade. If one takes this perspective and believes in the primacy of the established approach, then our results suggest a cheaper way to conduct evaluation of push noti cations systems that yield similar conclusions.",1,ad,True
"Despite substantial correlations between many online and batch metrics, there are some metrics that exhibit no meaningful correlation. We observe that systems vary widely in the volume of messages they push, and that this is the biggest source of metric disagreement. We do not believe that the proper role of message volume in evaluating push noti cation systems is fully understood, but this paper elucidates key issues as an important rst step.",0,,False
415,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 BACKGROUND AND RELATED WORK,0,,False
"Work on prospective information needs against document streams dates back at least a few decades and is closely related to ad hoc document retrieval [6]. Major initiatives in the 1990s include the TREC Filtering Tracks, which ran from 1995 [13] to 2002 [21], and the research program commonly known as topic detection and tracking (TDT) [2]. e TREC Filtering Tracks are best understood as binary classi cation on every document in the collection with respect to standing queries, and TDT is similarly concerned with identifying all documents related to a particular event--with an intelligence analyst in mind. In contrast, we are focused on identifying a small set of the most relevant updates to deliver to users--any more than a handful of noti cations per day would surely be annoying. Furthermore, in both TREC Filtering and TDT, systems must make online decisions as soon as documents arrive. In the case of push noti cations, systems can choose to push older content, thus giving rise to the possibility of algorithms operating on bounded bu ers. Latency is one aspect of the evaluation, allowing systems to trade o output quality with timeliness.",1,ad,True
"More recently, Guo et al. [8] introduced the temporal summarization task, whose goal is to generate concise update summaries from news sources about unexpected events as they develop. is has been operationalized in the TREC Temporal Summarization (TS) Tracks from 2013 to 2015 [4]. e task is closely related to the push noti cation problem that we study, and in fact the TREC Real-Time Summarization Track, which provides the context for our work, represents a merger of the TS and Microblog Tracks. However, nearly all previous evaluations, including TDT, TREC Filtering, and Temporal Summarization, merely simulated the streaming nature of the document collection, whereas in RTS the participants were required to build working systems that operated on tweets posted in real time (more details in Section 3).",1,TREC,True
"Our online in-situ evaluation framework builds on growing interest in so-called Living Labs [22, 24] and related Evaluation-as-aService (EaaS) [9] approaches that a empt to be er align evaluation methodologies with user task models and real-world constraints to increase the delity of research experiments. In this respect, our comparison between user-oriented and batch evaluations ties into a long history of research that examines the correlation between e ectiveness metrics from system-oriented evaluations and metrics from user-oriented evaluations [1, 3, 10, 23, 26, 29­31]. ere is, however, one important di erence: all of these cited papers, with one exception [31], focus on ad hoc retrieval, which has received much a ention over the years. Although there have been previous user studies on push noti cations from the HCI perspective (e.g., [17]), there is relatively li le empirical work on prospective information needs that we can draw from.",1,ad,True
"e nal thread of relevant work concerns interleaved evaluations [7, 11, 19, 20, 25], which have emerged as the preferred approach to evaluating web search engines over traditional A/B testing [12]. Our work departs from this large body of literature because these papers all focus on web search ranking, whereas we tackle the push noti cation problem: in our task, systems must take into account temporality and redundancy, both of which are less important in web search. e length of system output (i.e., volume of pushed messages) is another major di erence between",1,ad,True
"our task and web ranking. ese issues were explored in a recent paper by Qian et al. [18], who extended the interleaved evaluation methodology to retrospective and prospective information needs on document streams. However, their proposed approach was only validated in simulation. We take the next step by deploying an adapted version of their proposed technique in a live user study.",1,ad,True
3 EVALUATION METHODOLOGY,0,,False
"Although the push noti cation problem is applicable to document streams in general, we focus on social media posts: the public nature of Twi er makes tweets the ideal source for shared evaluations. In particular, Twi er provides a streaming API through which clients can obtain a sample (approximately 1%) of public tweets--this level of access is available to anyone who signs up for an account. In order to evaluate push noti cation systems in a realistic se ing, the TREC 2016 RTS Track de ned an o cial evaluation period during which all participants ""listened"" to the tweet sample stream to identify relevant and novel tweets with respect to users' interest pro les in a timely manner. e evaluation period began Tuesday, August 2, 2016 00:00:00 UTC and lasted until ursday, August 11, 2016 23:59:59 UTC.",1,AP,True
"Interest pro les, which represent users' information needs, followed the standard TREC ad hoc topic format of ""title"", ""description"", and ""narrative"". ese were made available to all participants a few weeks prior to the beginning of the evaluation period. Given the prospective nature of the pro les, it is di cult to anticipate what topics will be discussed during the evaluation period and what events will be ""interesting"". Instead, the organizers adopted the strategy of ""overgenerate and cull"": in total, 203 interest pro les were provided to the participants, more than there were resources available for assessment, with the anticipation of le ing users decide what pro les should be assessed (more details below).",1,TREC,True
3.1 Online Evaluation Setup,0,,False
"e TREC 2016 RTS Track contained two separate tasks: push noti cations (so-called ""Scenario A"") and email digests (so-called ""Scenario B""). In this paper we are only concerned with push noti cations, but for more details we refer the reader to the track overview [16].",1,TREC,True
"e overall evaluation framework is shown in Figure 1. Before the evaluation period, participants ""registered"" their systems with the evaluation broker to request unique tokens (via a REST API), which are used in subsequent requests to associate submi ed tweets with speci c systems.1 During the evaluation period, whenever a system identi ed a relevant tweet with respect to an interest pro le, the system submi ed the tweet id to the evaluation broker (also via a REST API), which recorded the submission time. Each system was allowed to push at most ten tweets per interest pro le per day; this limit represents an a empt to model user fatigue.",1,AP,True
"Once the evaluation broker recorded a system's submission, the tweet was immediately delivered to the mobile devices of a group of users, where it was rendered as a push noti cation containing both the text of the tweet and the corresponding interest pro le.",0,,False
"1As is standard in TREC, each participant was permi ed to submit multiple ""runs"" (usually system variants), but for the purposes of this discussion we refer to them as di erent systems.",1,TREC,True
416,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Stream of Tweets,1,Tweet,True
Participating TREC RTS Systems evaluation broker,1,TREC,True
Twitter API,1,Twitter,True
Assessors,0,,False
Figure 1: Evaluation setup for push noti cations: systems,0,,False
listen to the live Twitter sample stream and send results,1,Twitter,True
"to the evaluation broker, which then delivers push noti ca-",0,,False
tions to users.,0,,False
"e user may choose to a end to the tweet immediately, or if it arrived at an inopportune time, to ignore it. Either way, the tweet is added to a queue in a custom app on the user's mobile device, which she can access at any time to examine the queue of accumulated tweets. For each tweet, the user can make one of three judgments with respect to the associated interest pro le: relevant, if the tweet contains relevant and novel information; redundant, if the tweet contains relevant information, but is substantively similar to another tweet that the user had already seen; not relevant, if the tweet does not contain relevant information. As the user provides judgments, results are relayed back to the evaluation broker and recorded. Users have the option of logging out of the app, at which point they will cease to receive noti cations completely (until they log back in).",1,ad,True
"Our setup has two distinct characteristics: First, judgments happen online as systems generate output, as opposed to traditional batch post-hoc evaluation methodologies, which consider the documents some time (typically, weeks) a er they have been generated by the systems. Note that although the push noti cations are delivered in real-time, it is not necessarily the case that judgments are provided in real time since users can ignore the noti cations and come back to them later. Second, our judgments are in situ, in the sense that the users are going about their daily activities (and are thus interrupted by the noti cations). is aspect of the design accurately mirrors the intended use of push noti cation systems. Furthermore, from the evaluation perspective, we believe that this setup yields more situationally-accurate assessments, particularly for rapidly developing events. With post-hoc batch evaluations, there is always a bit of disconnect as the assessor needs to ""imagine"" herself at the time the update was pushed. With our evaluation framework, we remove this disconnect.",1,ad,True
"Our entire evaluation was framed as a user study (with appropriate ethics review and approval). A few weeks prior to the beginning of the evaluation period, we recruited users from the undergraduate and graduate student population at the University of Waterloo, via posts on various email lists as well as paper yers on bulletin boards.",1,ad,True
e users were compensated $5 to install the mobile assessment app and then $1 per 20 judgments provided.,0,,False
"As part of the training process, users installed the custom app described above on their mobile devices. In addition, they subscribed, using an online form, to receive noti cations for interest pro les they were interested in, selecting from the complete list of",1,ad,True
"203 interest pro les provided to all systems. To encourage diversity, we did not allow more than three users to select the same pro le (on a rst come, rst served basis).",0,,False
"e evaluation broker followed the temporal interleaving strategy proposed by Qian et al. [18], which meant that tweets were pushed to users as soon as the broker received the submi ed tweets from the systems. Although Qian et al. only discussed interleaving the output of two systems, it is straightforward to extend their strategy to multiple systems. e broker made sure that each tweet was only pushed once (per pro le), in the case where the same tweet was submi ed by multiple systems at di erent times. Although one can imagine di erent ""routing"" algorithms for pushing tweets to di erent users that have subscribed to a pro le, we implemented the simplest possible algorithm where the tweet was pushed to all users that had subscribed to the pro le. is meant that the broker might receive more than one judgment per tweet.",1,ad,True
3.2 Online Metrics,0,,False
"e output of our online in-situ evaluation is a sequence of judgments, which need further aggregation before we can use the results to compare the e ectiveness of di erent systems. Note that this aggregation is more complicated than a similar process in interleaved evaluations for web search because systems can vary widely in tweet volume (i.e., how many tweets they push). In standard interleaving techniques for evaluating web search, both variant algorithms being tested contribute to the nal ranking for all queries-- thus, it usually su ces to count the number (or fraction) of clicks to determine the winner (e.g., [7, 24]). However, in our case, there isn't a query that lends itself to a natural paired comparison. Some systems are quite pro igate in dispatching noti cations, while other systems are very quiet.",0,,False
"Another implication of our interleaved evaluation setup is that a user will encounter tweets from di erent systems, which makes the proper interpretation of ""redundant"" judgments more complex. A tweet might only be redundant because the same information was contained in a tweet pushed earlier by another system (and thus not the ""fault"" of the particular system that pushed the tweet). In other words, the interleaving itself was directly responsible for introducing the redundancy. is observation was made by Qian et al. [18], who proposed a heuristic for more accurate credit assignment when interleaving two systems. However, we decided to adopt a much simpler approach (explained below), which is justi ed by our experimental results (more details later).",1,ad,True
"Recognizing the issues discussed above, we computed two aggregate metrics based on user judgments:",0,,False
"Online Precision. A simple and intuitive metric is to measure precision, or the fraction of relevant judgments:",0,,False
relevant relevant + redundant + not relevant,0,,False
(1),0,,False
"We term this ""strict"" precision because systems don't get credit for redundant judgments. As an alternative, we could compute ""lenient"" precision, where the numerator includes redundant judgments. Extending this further, redundant judgments could in principle be assigned fractional credit, but as we discuss later, such schemes do not appear to have any impact on our overall ndings.",0,,False
417,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Two minor details are worth mentioning for the proper interpretation of this metric: First, tweets may be judged multiple times since a tweet is pushed to all users who had subscribed to the pro-",1,ad,True
"le. For simplicity, all judgments are included in our calculation. Second, our precision computation represents a micro-average (and not an average across per-pro le precision). is choice was made due to the sparsity of judgments: macro-averaging would magnify the e ects of pro les with few judgments.",1,ad,True
"Online Utility. As an alternative to online precision, we could take a utility-based perspective and measure the total gain received by the user. e simplest method would be to compute the following:",0,,False
relevant - redundant - not relevant,0,,False
(2),0,,False
"which we refer to as the ""strict"" variant of online utility. Paralleling the precision variants above, we de ne a ""lenient"" version of the metric as follows:",0,,False
(relevant + redundant) - not relevant,0,,False
(3),0,,False
"Of course, we could further generalize with weights for each type of judgment. However, we lack the empirical basis for se ing the weights. Furthermore, experimental analyses show that our",0,,False
ndings are insensitive to weight se ings.,0,,False
"To summarize: from user judgments, we compute two aggregate metrics--online precision and online utility. Note that there is no good way to compute a recall-oriented metric since we have no control over when and how frequently user judgments are provided.",0,,False
is is a fundamental limitation of this type of user study.,0,,False
3.3 Batch Evaluation Setup,0,,False
"In order to mitigate the risk inherent in any new evaluation methodology, the TREC 2016 RTS Track also deployed a more traditional post-hoc batch evaluation methodology--speci cally, the approach developed for the Tweet Timeline Generation (TTG) task at the TREC 2014 Microblog Track [14], which was also used in 2015 [15].",1,TREC,True
"e methodology has been externally validated [31] and can be considered mature due to its deployment in multiple formal evaluations. e assessment work ow proceeded in two major stages: relevance assessment and semantic clustering. Here we provide only a brief overview, referring the reader to the cited papers above for additional details.",1,ad,True
"Tweets returned by participating systems were judged for relevance by NIST assessors via pooling. Note that this occurred a er the live evaluation period ended, so it was possible to gather all tweets pushed by all participating systems. NIST assessors began a few days a er the end of the evaluation period to minimize the ""staleness"" of tweets. Each tweet was assigned one of three judgments: not relevant, relevant, or highly-relevant. A er the relevance assessment process, the NIST assessors proceeded to perform semantic clustering on only the relevant and highly-relevant tweets. Using a custom interface, they grouped tweets into clusters in which tweets share substantively similar content, or more colloquially, ""say the same thing"". e interpretation of what this means operationally was le to the discretion of the assessor. In particular, they were not given a particular target number of clusters to form; rather, they were asked to use their judgment, considering both the interest pro le and the actual tweets. e output of the cluster annotation",1,Tweet,True
process is a list of tweet clusters; each cluster contains tweets that are assumed to convey the same information.,0,,False
3.4 Batch Evaluation Metrics,0,,False
"As previously discussed, push noti cations should be relevant, nonredundant, and timely. One challenge, however, is that there is li le empirical work on how users perceive timeliness. erefore, instead of devising a single-point metric that tries to combine all three characteristics, the organizers decided to separately capture output quality (relevance and redundancy) and timeliness (latency). In this paper, we only focus on output quality metrics. In short, RTS batch evaluation metrics a empt to capture precision, recall, and overall utility. We elaborate below:",1,ad,True
Expected Gain (EG) for an interest pro le on a particular day is,0,,False
de,0,,False
ned as,0,,False
1 N,0,,False
"G(t ), where N is the number of tweets returned and",0,,False
G(t ) is the gain of each tweet: not relevant tweets receive a gain of 0;,0,,False
relevant tweets receive a gain of 0.5; highly-relevant tweets receive,0,,False
"a gain of 1.0. Once a tweet from a cluster is retrieved, all other tweets",0,,False
from the same cluster automatically become not relevant. is,0,,False
penalizes systems for returning redundant information. Expected,0,,False
gain can be interpreted as a precision metric.,0,,False
Normalized Cumulative Gain (nCG) for an interest pro le on,0,,False
a particular day is de,0,,False
ned as,0,,False
1 Z,0,,False
"G(t ), where Z is the maximum",0,,False
possible gain (given the ten tweet per day limit). e gain of each,0,,False
individual tweet is computed in the same way as above. Note that,0,,False
gain is not discounted (as in nDCG) because the notion of document,0,,False
ranks is not meaningful in this context. We can interpret nCG as a,0,,False
recall-like metric.,0,,False
"e score for a run is the average over scores for each day over all interest pro les. An interesting question is how scores should be computed for days in which there are no relevant tweets: for rhetorical convenience, we call days in which there are no relevant tweets for a particular interest pro le (in the pool) ""silent days"", in contrast to ""eventful days"" (when there are relevant tweets). In the EG-1 and nCG-1 variants of the metrics, on a silent day, the system receives a score of one (i.e., a perfect score) if it does not push any tweets, or a score of zero otherwise. In the EG-0 and nCG-0 variants of the metrics, for a silent day, all systems receive a gain of zero no ma er what they do. For more details about this distinction, see Tan et al. [28].",0,,False
"erefore, under EG-1 and nCG-1, systems are rewarded for recognizing that there are no relevant tweets for an interest pro le on a particular day and remaining silent (i.e., the system does not push any tweets). e EG-0 and nCG-0 variants of the metrics do not reward recognizing silent days: that is, it never hurts to push tweets. We show later in our analyses that EG-0 and nCG-0 are poorly-formulated metrics.",0,,False
"Gain Minus Pain (GMP) is de ned as  · G - (1 -  ) · P, where G (gain) is computed in the same manner as above, pain P is the number of non-relevant tweets that the system pushed, and  controls the balance of weights between the two. We investigated three  se ings: 0.33, 0.50, and 0.66. Note that this metric is the same as the linear utility metrics used in the TREC Filtering [13, 21] and Microblog [27] Tracks, although our formulation takes a slightly di erent mathematical form.",1,TREC,True
418,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
,0,,False
,0,,False
,0,,False
 Day   ,0,,False
,0,,False
 ,0,,False
 ,0,,False
 ,0,,False
 ,0,,False
"Figure 2: Distribution of response times over the rst minute (14.3%), rst ten minutes (44.9%), rst hour (71.3%), and across the entire evaluation period. Percentages in parentheses show how many judgments were received in the corresponding period.",0,,False
User Judgments Pro les Messages Response,0,,False
1,0,,False
53,0,,False
4,0,,False
1619,0,,False
3.27%,0,,False
2,0,,False
3305,0,,False
10,0,,False
7141 46.28%,0,,False
3,0,,False
136,0,,False
10,0,,False
5860,0,,False
2.32%,0,,False
4,0,,False
327,0,,False
8,0,,False
3795,0,,False
8.62%,0,,False
5,0,,False
949,0,,False
12,0,,False
6330 14.99%,0,,False
6,0,,False
28,0,,False
12,0,,False
7211,0,,False
0.39%,0,,False
7,0,,False
281,0,,False
10,0,,False
4162,0,,False
6.75%,0,,False
8,0,,False
1908,0,,False
15,0,,False
7754 24.61%,0,,False
9,0,,False
3791,0,,False
33,0,,False
16654 22.76%,0,,False
10,0,,False
680,0,,False
16,0,,False
7257,0,,False
9.37%,0,,False
11,0,,False
107,0,,False
43,0,,False
22676,0,,False
0.47%,0,,False
12,0,,False
324,0,,False
2,0,,False
938 34.54%,0,,False
13,0,,False
226,0,,False
12,0,,False
7058,0,,False
3.20%,0,,False
"Table 1: User statistics. For each user, columns show the number of judgments provided, the number of interest pro-",0,,False
"les subscribed to, the maximum number of push noti cations received, and the response rate.",0,,False
"To summarize: we have multiple batch metrics for evaluating push noti cation systems: EG-1 and EG-0 (both of which measure precision), nCG-1 and nCG-0 (both of which measure recall), and GMP with  ,"" {0.33, 0.50, 0.66} (which capture utility).""",0,,False
4 USER BEHAVIOR,0,,False
"e evaluation methodology for push noti cations detailed above was deployed in the TREC 2016 Real-Time Summarization Track. In total, 18 groups from around the world participated, submi ing a total of 41 systems (runs). Over the evaluation period, these runs pushed a total of 161,726 tweets, or 95,113 unique tweets a er de-duplicating within pro les.",1,TREC,True
"To simplify app development, we only targeted users of Android devices. For our evaluation, we recruited a total of 18 users, 13 of whom ultimately provided judgments. Of these, 11 were either graduates or undergraduate students at the University of Waterloo. In total, we received 12,115 judgments over the assessment period, with a minimum of 28 and a maximum of 3,791 by an individual user. Overall, 122 interest pro les received at least one judgment; 93 received at least 10 judgments; 67 received at least 50 judgments; 44 received at least 100 judgments.",1,ad,True
We begin with descriptive characterizations of user behavior: a breakdown is shown in Table 1. e second column lists the number of judgments each user provided and the third column shows,0,,False
Distribution of Notifications and Judgments,0,,False
8/02,0,,False
8/03,0,,False
8/04,0,,False
8/05,0,,False
8/06,0,,False
8/07,0,,False
8/08,0,,False
8/09,0,,False
8/10,0,,False
8/11 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Hour,0,,False
"Figure 3: Heatmap showing the volume of push noti cations, overlaid with circles whose areas are proportional to the number of received judgments.",0,,False
"the number of pro les that each user subscribed to. e fourth column shows the sum of all push noti cations for the pro les that each user subscribed to: this count captures the maximum number of push noti cations that the user could have received during the evaluation period. Note that we do not have the actual number of noti cations each user received because the user could have logged out during some periods of time or otherwise adjusted the local device se ings (e.g., to disable noti cations). e nal column shows the response rate, computed as the fraction between the second and fourth columns (which is a lower-bound estimate). From this table, we see that some users are quite diligent in providing judgments, while others provide judgments more sporadically.",1,ad,True
"How quickly do users provide judgments? e plots in Figure 2 answer this question, showing the distribution of response times over the rst minute, rst ten minutes, rst hour, and across the entire evaluation period. e bars show bucketed counts, while the line graph shows cumulative counts. Normalizing, we nd that 14.3% of judgments arrive within the rst minute a er the push noti cation has been delivered, 44.9% of judgments arrive in the",0,,False
"rst ten minutes, and 71.3% of judgments arrive in the rst hour. We nd that users are quite responsive to interruptions!",0,,False
"Finally, Figure 3 provides an overview of the entire evaluation period. In the heatmap, each box represents one hour across the ten-day evaluation period: the color re ects the total number of pushed tweets by all systems across all pro les that at least one user subscribed to. A deeper red indicates more tweets pushed. e overlaid circles represent judgments received from all users, where",0,,False
419,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 4: Analyses of online metrics. e le and middle plots show tweet volume vs. online precision and online utility. e right plot shows almost no correlation between online precision and online utility because systems with roughly the same online precision can vary widely in push volume.,0,,False
"the area is proportional to the number of judgments. Note that time is given in UTC; for reference, 00:00:00 UTC translates into 20:00:00 in the local time zone of the users.",0,,False
"A few interesting observations follow: we nd that relatively more tweets are pushed by systems in the rst and nal hours of each day. We believe that this is mostly an evaluation artifact: recall that each system receives a quota of ten tweets per day per pro le. At the beginning of each day (hour 00), the quota resets-- thus allowing systems that have used up their quota the previous day to start pushing noti cations again. At the end of each day (hour 23), we believe that the rise in tweets corresponds to systems ""using up"" the remainder of their quota.",0,,False
"Looking at the circles, which represent the volume of judgments, we see that they mostly line up with the push volume. at is, darker red cells generally have larger circles--the more tweets systems push, the more judgments we receive. However, there are some deviations, which represent delayed judgments--for example, a burst of tweets that wasn't examined until some time later. It is also interesting to note that with the exception of night time when users are asleep, there does not appear to be a consistent diurnal cycle across our population of users. e users are exposed to a pre y constant stream of push noti cations throughout the day (and indeed during sleeping hours also), but there doesn't appear to be a time of the day when we consistently receive more judgments.",0,,False
5 ANALYSIS,0,,False
"By design, the TREC 2016 RTS Track employed both the online in-situ interleaved evaluation methodology as well as the more traditional post-hoc batch evaluation methodology. is means that for the same systems and interest pro les, we have independentlyderived metrics from two very di erent approaches. For the batch metrics, NIST assessors fully judged 56 interest pro les (relevance judgments and clusters). Section 3.3 and Section 3.4 provide an overview, but since this is not the focus of our work, we refer the reader to details provided in the track overview [16].",1,TREC,True
"We begin by presenting separate analyses of online and batch metrics, and then describe results of correlation analyses between them. In particular, comparing online and batch metrics allows us to explore two questions: From the perspective of the user, do user preferences correlate with batch metrics? From the perspective of system-centered evaluations, can unreliable online judgments replace high-quality NIST assessors?",0,,False
"In considering the online metrics, there is a question regarding which metric to use--the ""strict"" or ""lenient"" variant of online precision and online utility (see Section 3.2). We performed analyses with both: All plots look very similar, except for systematic shi s due to the metric variants; for example, all the absolute precision values increase from ""strict"" to ""lenient"" precision, but the overall relationships between the points remain largely unchanged. erefore, we only report the ""strict"" variants here for brevity. is also suggests that the credit assignment heuristic of Qian et al. [18], which lies somewhere between the strict and lenient variants, is also unlikely to alter our ndings.",0,,False
5.1 Online Metrics,0,,False
ree di erent analyses of the online metrics are shown in Figure 4. We organize our ndings around two themes:,0,,False
"Precision is an intrinsic metric of push noti cation quality, while utility is a convenient composite metric. Online precision computes the fraction of relevant user judgments, but does not factor in the volume of tweets that a system pushes. Online utility implies a particular precision target with volume as a scaling factor, and thus serves as a convenient composite metric. To see why this is so, consider a system that achieves a precision of 0.5: se ing aside relevance grades for now, the expected utility per tweet is zero (for  ,"" 0.5) and the overall expected utility is also zero, regardless of how many tweets the system pushes. A system with lower precision has a negative expected utility per tweet, and the total expected utility is simply that value multiplied by the volume of tweets. Since the precision of most systems in the evaluation falls below 0.5, we observe a strong negative correlation between tweet volume and utility: this can be clearly seen in the middle plot in Figure 4, which shows tweet volume against online utility. Here, volume is measured as the number of tweets pushed by the system for all interest pro les that received at least one judgment.""",1,ad,True
"Our argument can be generalized to other ways of computing utility. Of course, one could assign di erent weights to non-relevant tweets, but for every weighting scheme, there is an implied precision at which the expected utility per tweet is zero. Only systems that have higher precision can provide positive utility; otherwise, negative utility is directly proportional to push volume. e same idea can be straightforwardly extended to relevance grades: all utility-based metrics encode (at least implicitly) a breakeven point between ""good"" results and ""bad"" results.",1,ad,True
420,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Figure 5: Analyses of batch metrics. e le and middle plots show tweet volume vs. EG-0 and nCG-0, illustrating the dominant e ect of tweet volume, which is a major aw in those metrics. e right plot shows a strong correlation between EG-1 and nCG-1 (with the exception of a few outliers).",0,,False
"Tweet volume is an independent and important measure of system output. Building on the previous observation, the independence of tweet volume and precision can be clearly seen in the right plot of Figure 4, where we observe almost no relationship between online precision and online utility. is is further reinforced in the le plot, which shows tweet volume vs. online precision. Although we observe a negative correlation overall, the e ect is primarily due to outliers. If we focus only on systems with tweet volume under 2000, there is li le correlation between online precision and volume. In particular, in the band from 0.3 to 0.4 precision, systems vary widely in volume. is leads to the wide spread of precision values for systems that have similar utility in the right plot. us, from the user perspective, we believe that online precision and tweet volume are the two fundamental inputs to metrics for measuring system e ectiveness.",1,Tweet,True
5.2 Batch Metrics,0,,False
Analyses of batch metrics are shown in Figure 5. Our two main ndings are as follows:,0,,False
"EG-0 and nCG-0 are awed metrics. Recall that these variants do not reward systems for recognizing that there is no relevant information and staying silent, and thus it never hurts to push noti cations. As a result, these two metrics reward systems that push a large volume of tweets without necessarily di erentiating the quality of those tweets. is is most evident in the middle plot in Figure 5, which shows tweet volume against nCG-0. Tweet volume here is measured as the total number of tweets pushed across the interest pro les that were evaluated by NIST assessors. Due to the much more involved batch evaluation methodology, the NIST assessors considered a smaller set of interest pro les than users in the online evaluation, and thus the plots report smaller tweet volumes. While it is possible to push a large number of non-relevant tweets (bo om right corner of the middle plot), in general, the more tweets a system pushes, the higher its nCG-0 score.",1,Tweet,True
"We note a similar e ect for EG-0, although less pronounced, from the le plot in Figure 5, which shows tweet volume against EG-0. Once again, disregarding the outliers in the bo om right corner, higher tweet volumes correlate with higher EG-0 scores. Since under EG-0 all systems receive EG scores of zero for silent days when there are no relevant tweets, it never hurts to ""guess"" by pushing tweets. us, we believe that EG-0 and nCG-0 are awed metrics since it is unlikely that users desire high-volume systems",0,,False
"that push tweets of questionable quality. We advocate that these metrics be dropped in future evaluations, and we remove EG-0 and nCG-0 from subsequent analyses in this paper.",1,ad,True
"EG-1 and nCG-1 are highly correlated. is correlation can be seen in the right plot in Figure 5. In contrast to EG-0 and nCG-0, these metrics reward systems for remaining silent on days when there is no relevant content. e plot shows that systems with higher gain (utility) also tend to achieve higher precision.",0,,False
"It is interesting to observe that such a strong correlation exists between EG-1 and nCG-1, since EG is quite similar to precision and nCG is recall-like: in principle, systems could make tradeo s along these two dimensions independently. However, this might simply be a statement about the current state of push noti cation techniques. Nevertheless, we do observe some outliers: the group of runs around 0.06 in EG-1 and around 0.2 in nCG-1 are those that push a high volume of tweets. What they lack in the overall quality of individual tweets, they make up in volume, leading to higher nCG-1 than their EG-1 scores would otherwise suggest (i.e., the points lie above the trend).",1,ad,True
5.3 Online vs. Batch Metrics,0,,False
"Sca er plots correlating various batch metrics against online utility and online precision are shown in Figure 6. In the top row we show correlations between EG-1, nCG-1, and GMP ( ,"" 0.50) against online utility; in the bo om row, the same metrics against online precision. Note that we removed EG-0 and nCG-0 from consideration given the discussion above. For GMP, the choice of  does not change the shape of the plots and does not a ect our conclusions, so for brevity we omit GMP with  "","" {0.33, 0.66}.""",0,,False
"When performing correlational studies on retrieval experiments, outlier runs may have a disproportional in uence on the results. For example, poor performing systems are easy to distinguish, and most metrics can easily identify poor systems. erefore, including such systems tends to increase correlations in ways that are not particularly helpful in discriminating systems that are not outliers.",0,,False
"e outliers in our case are systems that push a large volume of tweets and those that push very few tweets. From Figure 4 and Figure 5 we can identify the outliers as those runs that push more than 4000 tweets in the online evaluation and more than 1500 tweets in the batch evaluation. ere are eight such systems and both criteria identify exactly the same systems. On the whole, these are systems that perform poorly. In the plots, we identify",0,,False
421,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
      ,0,,False
      ,0,,False
      ,0,,False
"       !""",0,,False
       ,0,,False
       !,0,,False
 ,0,,False
 ,0,,False
,0,,False
 ,0,,False
 ,0,,False
,0,,False
 ,0,,False
 ,0,,False
,0,,False
             ,0,,False
 ,0,,False
          !    ,0,,False
 ,0,,False
               ,0,,False
 ,0,,False
"Figure 6: Scatter plots comparing EG-1, nCG-1, and GMP ( , 0.50) to online utility (top row) and to online precision (bottom row). High-volume systems are represented by empty diamonds and low-volume systems are represented by empty circles. Solid lines denote best t lines with all points; dotted lines denote best t lines discarding high- and low-volume systems.",0,,False
"these runs separately as empty diamonds. At the other end of the spectrum, we have runs that push very few tweets. We arbitrarily set this threshold to be less than 100 tweets pushed based on the batch evaluation. Since the batch evaluation considered 56 interest pro les spanning 10 days, this translates into less than two tweets per interest pro le (over the entire span), which is close to a system that basically does nothing. ere are ve such runs, identi ed as empty circles in the plots. is leaves us with 28 systems (runs) whose tweet volumes fall somewhere in the middle, identi ed by the solid squares in the plots.",0,,False
"For each analysis, we considered two separate conditions: First, with all runs. e results of linear regressions are shown as solid lines. Second, we discarded high- and low-volume systems (as described above); the results of linear regressions in these cases are shown as do ed lines. e second condition a empts to remove the in uence of these outliers: in some cases, it a ects the ndings, but in other cases, not. For both conditions, alongside the coe cient of determination for the linear regression shown in the legend, we also report rank correlation in terms of Kendall's  , the standard metric of rank stability in information retrieval experiments.",0,,False
"ere is a lot to unpack in our results, and so we organize our ndings around several major themes:",0,,False
"Online utility is highly correlated with GMP. Our strongest nding is a high correlation between online utility and GMP (see Figure 6, top row, right plot). e correlation weakens slightly if we discard the high-volume and low-volume systems, but is still substantial. Because there are many points packed in the top right corner of the plot, the Kendall's  we observe is a bit lower compared to the coe cient of determination, but still solidly in the range that would be considered good agreement for retrieval experiments.",0,,False
"At rst glance, this might seem like an obvious nding since online utility and GMP are both utility-based metrics, but this is a non-obvious result for several reasons: GMP is computed over",0,,False
"cluster judgments from pooled tweets and therefore represents a global view over tweets from all systems. In particular, tweets are grouped into clusters and systems do not get credit for pushing tweets that say the same thing. In contrast, online utility captures only a local view of content--users are making decisions about each tweet pushed to them, and the redundant judgments are subjected to the fallacies of human memory (i.e., users may have forgo en having seen similar tweets).",0,,False
"In addition, GMP is computed using ""dense"" judgments over a smaller set of pro les gathered by pooling, whereas online utility is computed from sparse judgments over uncontrolled samples, since we have no control over when and how frequently users provide judgments. It is surprising that sporadic, unpredictable, in-situ judgments from a multitude of users yield results that are highly-correlated with the careful deliberations of professional NIST assessors.",1,ad,True
"Online precision exhibits moderate correlations with EG-1 and nCG-1. is is shown in Figure 6, bo om row, le and center plots. Since",0,,False
"the de nition of EG-1 shares similarities with online precision, one might expect this, and since EG-1 and nCG-1 are correlated (right plot, Figure 5), it is no surprise to nd that online precision also correlates with nCG-1. As with GMP above, we emphasize that online precision is computed from tweets evaluated in isolation, whereas EG-1 and nCG-1 are based on cluster annotations, which take into account the global cluster structure of tweets relevant to the interest pro le.",0,,False
"In both cases, the correlation strengthens if we discard high- and low-volume systems (although for nCG-1, Kendall's  is essentially unchanged). An empty run (i.e., a system that does nothing) would receive a score of 0.2339 for EG-1 and nCG-1, which is simply the fraction of silent days when there are no relevant tweets. erefore, low-volume systems receive scores that are close to the score of an empty run, and this throws o the correlation.",0,,False
422,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Online utility exhibits at best a weak correlation with EG-1 and nCG-1. is is shown in Figure 6, top row, le and center plots. In both",0,,False
"these cases, the correlation weakens substantially if we remove the high- and low-volume systems. erefore, outliers are giving the impression of a stronger correlation than one that actually exists. In a sense, it is not surprising that we observe li le correlation between a utility metric vs. precision-oriented and recall-like metrics.",0,,False
"Online precision exhibits almost no correlation with GMP. is is shown in Figure 6, bo om row, right plot. is nding is consistent with what we see in Figure 4. Precision doesn't capture tweet volume, whereas tweet volume has a substantial impact on utility, as previously discussed.",0,,False
6 DISCUSSION AND LIMITATIONS,0,,False
"One potential objection to our evaluation methodology is our reliance on explicit user judgments. is, of course, stands in contrast to web ranking, which bene ts from a tremendous amount of implicit judgments that are collected as a byproduct of users searching. However, we argue that explicit judgments are an important component of push noti cations since they, by de nition, interrupt the user. Given that the interruption has already occurred (if the user has chosen to a end to the noti cation), allowing the user an opportunity to provide feedback seems like good design. us, we argue that our evaluation setup is realistic, mirroring how a production push noti cation system might be deployed. For example, in some implementations today, noti cations already appear with a ""dismiss"" option for users to take explicit action; adding options for quality feedback would incur minimal extra cost.",1,ad,True
"At a high-level, our work supports three ndings: First, that online in-situ interleaved evaluations of push noti cations systems are workable in practice. It is indeed possible to recruit users and they are willing to provide su cient judgments (quite diligently, in fact) to meaningfully evaluate systems. is seems consistent with our arguments above regarding the role of explicit judgments in push noti cation systems. Second, we observe substantial correlations between online and batch evaluation metrics that share the same design (e.g., are precision-oriented or utility-based). ird, the volume of messages that a system pushes is an important aspect of system evaluation, but its role is not fully understood.",0,,False
"With respect to the second nding--the substantial correlation between online and batch metrics--this is by no means obvious, given the large body of literature that has shown divergences between user- and system-oriented metrics (see Section 2). We have touched on some of the main di erences between the online and batch methodologies, but they bear additional emphasis:",1,ad,True
"· e batch evaluation considered all tweets that all systems pushed during the evaluation period. at is, all push noti cations were included in the pool and so the judgments are exhaustive from the perspective of the participants. is stands in contrast to the online judgments, which are best characterized as a small convenience sample by users (see response rates in Table 1). We have no control over when and how many judgments are provided-- and whether there are any systematic biases, for example, a user who only marks relevant tweets but ignores non-relevant tweets (i.e., a bias against explicit negative judgments).",0,,False
"· e batch metrics all operate at the level of semantic clusters, taking into account redundancy. ese clusters are formed from the pool and therefore contain a ""global"" view of tweets pushed by all systems. Accordingly, systems are penalized for retrieving multiple tweets that say that same thing. In contrast, user judgments occur tweet-by-tweet and represent a ""local"" view-- our users assess only the tweets in front of them. Furthermore, redundant judgments are made with respect only to tweets the users had previously assessed, and are subject to the e ects of imperfect memory. Another consequence of this setup is that from batch judgments we can characterize silent days (at least within the limitations of pooling), whereas with online judgments there is no way for the user to obtain this information. us, it is not possible for an online metric to reward systems for ""staying quiet"". Finally, high-volume systems (which tend to have lower precision) are disproportionately represented.",1,ad,True
"· e batch evaluation used professional NIST assessors, many with decades of experience. ey have become the gold standard against which human judgments are compared [5]. Contrast this with our users: since they are simply going about their daily lives (which is indeed the point), we have no idea in what context they are assessing tweets--were they alone in a quiet se ing considering tweets with care or hurriedly skimming tweets while multi-tasking? We assume our users were acting in good faith and judging the tweets to the best of their ability (and we have no reason to suspect otherwise), but the overall delity and quality of judgments are likely to be lower than the NIST assessors who operated in a carefully-controlled environment.",1,ad,True
"ese di erences considered, we nd the correlations between online and batch metrics non-obvious and interesting. ere are two di erent interpretations to these results:",0,,False
"If one believes in the primacy of user-centered evaluations, our ndings suggest that established batch evaluation metrics are able to capture user preferences. at is, the batch metrics are capturing aspects of what users care about in useful systems. is result nicely complements the ndings of Wang et al. [31], who validated batch metrics for the related task of retrospective timeline summarization over tweet streams.",0,,False
"On the other hand, one might put more faith in a mature batch evaluation methodology that has been through the gauntlet of multiple deployments, and whose core ideas date back at least a decade. If one takes this perspective and believes in the primacy of the established approach, then our results suggest a cheaper way to conduct evaluations of push noti cation systems that yield similar conclusions. Of course, these two perspectives are not necessarily con icting. Instead, they point to more work that is necessary to fully align user- and system-oriented perspectives to assessing push noti cation systems.",1,ad,True
"It makes sense to discuss some of the limitations of this work. Our users are compensated for their participation in the study and thus can be assumed to operate under certain social norms. One might argue that their behavior would be di erent had they ""organically"" discovered an app for push noti cations. While this is certainly a legitimate criticism, it could be leveled against any user study that involves compensation--potential di erences between paid subjects and ""real users"" are beyond the scope of this study.",1,ad,True
423,0,,False
Session 4A: Evaluation 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"A closely-related issue is the fact that our users subscribed to interest pro les that were not ""their own"", i.e., they did not come up with the information needs themselves. is concern, however, is mitigated by having users select from a broad range of pro les (a couple hundred) to match their interests. erefore, our evaluation is less likely to have su ered from user indi erence.",1,ad,True
"Another limitation of our study is that it captures only a snapshot of current technology. is, of course, is an implicit quali cation of any evaluation, not just our work. For example, consider a empts to control the volume of push noti cations and to recognize when there is no relevant content: such techniques are nascent at best, since the community is just beginning to understand the nuances of systems ""learning when to shut up"". We have found that these issues are confounding variables when trying to correlate online and batch metrics, but as the technology evolves and matures, the nature of the confound might change. As another example, we empirically observe that EG-1 and nCG-1 are correlated, even though in principle systems can operate in a tradeo space in which the measures are not correlated. Nevertheless, we are unable to speculate on future developments that have yet to happen--we can only draw conclusions based on the data at hand. e only way to address this limitation is a follow-up study that considers systems once push noti cation techniques have substantially progressed.",1,ad,True
7 CONCLUSIONS,0,,False
"is paper describes a formal user study of push noti cation systems with two distinct characteristics: tweets are assessed online and in-situ. As the infrastructure for conducting our evaluation can be reused (all so ware deployed in this study is open source), future iterations will take less e ort. erefore, we hope to see more of these evaluations as the methodology becomes ""just another hammer"" in the toolbox of information retrieval researchers and practitioners.",0,,False
"As an outstanding issue, we believe that the proper role of notication volume in evaluating systems is not yet fully understood. As we have empirically observed, systems with the same precision can vary widely in the volume of noti cations they push. However, the question remains: how much content should a system actually push? Even assuming that systems can achieve high precision--let's say, 90% or greater--are more noti cations really be er? Intuitively, one would expect that, at some point, user fatigue sets in, even for a stream of high-quality tweets. We might imagine the user having access to a ""volume dial"" to provide feedback: ""yes, these are all good tweets, but too many!"" As we have shown, tweet volume is not directly captured in existing metrics, but the problem lies deeper: our understanding of how users perceive noti cations in response to prospective information needs remains quite poor, especially when factoring in the cost of interruptions. More work on fundamental issues along these lines is needed.",0,,False
8 ACKNOWLEDGMENTS,0,,False
"is work was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada, with additional contributions from the U.S. National Science Foundation under IIS-1218043 and CNS-1405688. Any ndings, conclusions, or recommendations expressed do not necessarily re ect the views of the sponsors.",1,ad,True
REFERENCES,0,,False
"[1] Azzah Al-Maskari, Mark Sanderson, Paul Clough, and Eija Airio. 2008. e Good and the Bad System: Does the Test Collection Predict Users' E ectiveness? In SIGIR. 59­66.",1,ad,True
"[2] James Allan. 2002. Topic Detection and Tracking: Event-Based Information Organization. Kluwer Academic Publishers, Dordrecht, e Netherlands.",1,Track,True
"[3] James Allan, Ben Cartere e, and Joshua Lewis. 2005. When Will Information Retrieval Be ""Good Enough""? User E ectiveness as a Function of Retrieval Accuracy. In SIGIR. 433­440.",0,,False
"[4] Javed Aslam, Fernando Diaz, Ma hew Ekstrand-Abueg, Richard McCreadie, Virgil Pavlu, and Tetsuya Sakai. 2015. TREC 2015 Temporal Summarization Track Overview. In TREC.",1,ad,True
"[5] Peter Bailey, Nick Craswell, Ian Soboro , Paul omas, Arjen P. de Vries, and Emine Yilmaz. 2008. Relevance Assessment: Are Judges Exchangeable and Does it Ma er? In SIGIR. 667­674.",0,,False
"[6] Nicholas J. Belkin and W. Bruce Cro . 1992. Information Filtering and Information Retrieval: Two Sides of the Same Coin? CACM 35, 12 (1992), 29­38.",0,,False
"[7] Olivier Chapelle, orsten Joachims, Filip Radlinski, and Yisong Yue. 2012. LargeScale Validation and Analysis of Interleaved Search Evaluation. ACM TOIS 30, 1 (2012), Article 6.",1,ad,True
"[8] Qi Guo, Fernando Diaz, and Elad Yom-Tov. 2013. Updating Users about Time Critical Events. In ECIR. 483­494.",1,ad,True
"[9] Allan Hanbury, Henning Mu¨ller, Krisztian Balog, Torben Brodt, Gordon V. Cormack, Ivan Eggel, Tim Gollub, Frank Hopfgartner, Jayashree Kalpathy-Cramer, Noriko Kando, Anastasia Krithara, Jimmy Lin, Simon Mercer, and Martin Potthast. 2015. Evaluation-as-a-Service: Overview and Outlook. arXiv:1512.07454.",0,,False
"[10] William Hersh, Andrew Turpin, Susan Price, Benjamin Chan, Dale Kramer, Lyne a Sacherek, and Daniel Olson. 2000. Do Batch and User Evaluations Give the Same Results? In SIGIR. 17­24.",0,,False
"[11] Katja Hofmann, Shimon Whiteson, and Maarten de Rijke. 2011. A Probabilistic Method for Inferring Preferences from Clicks. In CIKM. 249­258.",0,,False
"[12] Ron Kohavi, Randal M. Henne, and Dan Sommer eld. 2007. Practical Guide to Controlled Experiments on the Web: Listen to Your Customers not to the HiPPO. In KDD. 959­967.",0,,False
"[13] David D. Lewis. 1995. e TREC-4 Filtering Track. In TREC. 165­180. [14] Jimmy Lin, Miles Efron, Yulu Wang, and Garrick Sherman. 2014. Overview of",1,TREC,True
"the TREC-2014 Microblog Track. In TREC. [15] Jimmy Lin, Miles Efron, Yulu Wang, Garrick Sherman, and Ellen Voorhees. 2015.",1,TREC,True
"Overview of the TREC-2015 Microblog Track. In TREC. [16] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees,",1,TREC,True
"and Fernando Diaz. 2016. Overview of the TREC 2016 Real-Time Summarization Track. In TREC. [17] Abhinav Mehrotra, Veljko Pejovic, Jo Vermeulen, Robert Hendley, and Mirco Musolesi. 2016. My Phone and Me: Understanding People's Receptivity to Mobile Noti cations. In CHI. 1021­1032. [18] Xin Qian, Jimmy Lin, and Adam Roegiest. 2016. Interleaved Evaluation for Retrospective Summarization and Prospective Noti cation on Document Streams. In SIGIR. 175­184. [19] Filip Radlinski and Nick Craswell. 2010. Comparing the Sensitivity of Information Retrieval Metrics. In SIGIR. 667­674. [20] Filip Radlinski and Nick Craswell. 2013. Optimized Interleaving for Online Retrieval Evaluation. In WSDM. 245­254. [21] Stephen Robertson and Ian Soboro . 2002. e TREC 2002 Filtering Track Report. In TREC. [22] Alan Said, Jimmy Lin, Alejandro Bellog´in, and Arjen P. de Vries. 2013. A Month in the Life of a Production News Recommender System. In CIKM Workshop on Living Labs for Information Retrieval Evaluation. 7­10. [23] Mark Sanderson, Monica Paramita, Paul Clough, and Evangelos Kanoulas. 2010. Do User Preferences and Evaluation Measures Line Up? In SIGIR. 555­562. [24] Anne Schuth, Krisztian Balog, and Liadh Kelly. 2015. Overview of the Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab 2015. In CLEF. [25] Anne Schuth, Katja Hofmann, and Filip Radlinski. 2015. Predicting Search Satisfaction Metrics with Interleaved Comparisons. In SIGIR. 463­472. [26] Mark Smucker and Chandra Jethani. 2010. Human Performance and Retrieval Precision Revisited. In SIGIR. 595­602. [27] Ian Soboro , Iadh Ounis, Craig Macdonald, and Jimmy Lin. 2012. Overview of the TREC-2012 Microblog Track. In TREC. [28] Luchen Tan, Adam Roegiest, Jimmy Lin, and Charles L. A. Clarke. 2016. An Exploration of Evaluation Metrics for Mobile Push Noti cations. In SIGIR. 741­ 744. [29] Andrew Turpin and William R. Hersh. 2001. Why Batch and User Evaluations Do Not Give the Same Results. In SIGIR. 225­231. [30] Andrew Turpin and Falk Scholer. 2006. User Performance versus Precision Measures for Simple Search Tasks. In SIGIR. 11­18. [31] Yulu Wang, Garrick Sherman, Jimmy Lin, and Miles Efron. 2015. Assessor Di erences and User Preferences in Tweet Timeline Generation. In SIGIR. 615­ 624.",1,TREC,True
424,0,,False
,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Eicient Cost-Aware Cascade Ranking in Multi-Stage Retrieval,1,ad,True
Ruey-Cheng Chen,0,,False
"RMIT University Melbourne, Australia",0,,False
Roi Blanco,0,,False
"RMIT University Melbourne, Australia",0,,False
ABSTRACT,0,,False
"Complex machine learning models are now an integral part of modern, large-scale retrieval systems. However, collection size growth continues to outpace advances in eciency improvements in the learning models which achieve the highest eectiveness. In this paper, we re-examine the importance of tightly integrating feature costs into multi-stage learning-to-rank (LTR) IR systems. We present a novel approach to optimizing cascaded ranking models which can directly leverage a variety of dierent state-of-the-art LTR rankers such as LambdaMART and Gradient Boosted Decision Trees. Using our cascade model, we conclusively show that feature costs and the number of documents being re-ranked in each stage of the cascade can be balanced to maximize both eciency and eectiveness. Finally, we also demonstrate that our cascade model can easily be deployed on commonly used collections to achieve state-of-the-art eectiveness results while only using a subset of the features required by the full model.",1,ad,True
1 INTRODUCTION,1,DUC,True
"Learning-to-Rank (LTR) systems are now commonly deployed by major search engine companies and they have been repeatedly shown to be highly eective for a variety of search related problems [6, 15, 26, 30]. ere has been a growing body of recent work which focuses on improving the eciency of multi-stage LTR systems using several dierent techniques: improving tree traversal [19], cascaded ranking [36], tree pruning [18, 38, 39], and minimizing sample sizes in stages [11, 22].",1,ad,True
"In this paper we revisit the idea of cascaded ranking in order to provide more control over eciency and eectiveness tradeos in large scale search systems. A cascade ranking model [36] is a sequence of learning-to-rank models (called stages) chained together to collectively rank a set of documents for a query.e main assumption behind cascaded ranking is that full inspection of the content, which would presumably require generating expensive features is not required for every incoming document as only a small fraction of all documents will be relevant. erefore, LTR",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prot or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permied. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specic permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: hp://dx.doi.org/10.1145/3077136.3080819",1,ad,True
Luke Gallagher,0,,False
"RMIT University Melbourne, Australia",0,,False
J. Shane Culpepper,0,,False
"RMIT University Melbourne, Australia",0,,False
"models in a cascade can be deployed in an ascending order of model complexity, and only a fraction of documents in each stage will advance to the next stage. Generally, early-stage rankers are cheaper to run, and usually focus on executing an early-exit strategy, such as ltering out non-relevant documents as quickly as possible. Ranking models in later stages are usually more accurate but require more resources.",1,ad,True
"When discussing system performance, it is important to consider both ranking eectiveness and system throughput within the same framework. Wang et al. [36] used a modied AdaRank algorithm to incorporate the costs of individual rankers, in terms of execution time for each single-feature weak learner used in the training procedure. is cascade model, however, cannot be used with gradient-boosted tree models, which are now widely believed to be state-of-the art for web search ranking algorithms [25, 30].",1,corpora,True
"Conceptually, the making of a tree-based cascade model can be reasonably separated into two steps, which are cascade construction and model deployment. In the rst step, a learning algorithm takes into account the eectiveness of features and the cost of feature extraction, makes the best tradeos by following the direction from cascade designer, and automatically trains a cascade of ranking models. e learned cascade can then be deployed in the second step, focusing on optimizing low-level system performance. In this paper, we develop a new approach to constructing a cost-aware cascade. A considerable amount of recent research eort has been invested in the space of optimizing the run-time performance of gradient-boosted tree models [3, 14, 19, 20], which can be directly leveraged by our new cascading approach.",1,ad,True
"Research Goals. In this work, we revisit the problem of integrating feature costs into learning-to-rank models. In particular, we focus on how best to balance feature importance and feature costs in multi-stage cascade ranking models. Our overarching goal is to devise a generic framework which can be used with any state-ofthe-art LTR algorithm, and allows more control over balancing eciency and eectiveness in the entire re-ranking process. In order to achieve these goals, we focus on two related research problems:",1,ad,True
"Research estion (RQ1): When designing multi-stage retrieval systems, what approaches provide the best balance between extraction/runtime costs and feature importance when using cascaded LTR algorithms?",1,ad,True
"Research estion (RQ2): Can we build multi-stage ranking models that require substantially less costs than a full cost-insensitive model, and still achieve overall eectiveness close to the full model?",0,,False
445,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
relevance,0,,False
Inverted Index,0,,False
Dynamic Features,0,,False
d1,0,,False
d2,0,,False
BOW Run,0,,False
Reorder d3,0,,False
( ) create initial sample,0,,False
d4 d5,0,,False
dk,0,,False
Learning to Rank,0,,False
Pre-Computed Features,0,,False
"Figure 1: A typical learning-to-rank system conguration is composed of an inverted index which is used to generate an initial candidate set (sample) of s documents. is set of documents is then re-ordered using one or more rounds of machine learning algorithms. e number of documents can be pruned in each round, or iteratively smaller subsets of the highest ranking documents in the initial sample s are re-ordered. A nal top-k set of documents are then returned from the system in relevance order.",0,,False
2 BACKGROUND AND RELATED WORK,0,,False
"Learning to rank. A signicant body of prior work exists in the area of learning-to-rank (LTR) [15]. e majority of research advances in LTR have focused on ways to improve the eectiveness of the systems, with several document collections released to test their performance. A recent study by Tax et al. [30] compare 87 learning to rank methods using 20 dierent test collections.",1,ad,True
"However, one common problem with these test collections is that the features used by the models are oen not fully dened, making it very dicult to implement them using commonly used IR test collections. is in turn prevents easily transferring the advances made into working end-to-end search systems. While many dierent publicly available search engines [32] are commonly used by researchers and practitioners, only Terrier 4.x [23] currently supports end-to-end multi-stage retrieval on commonly used IR document collections with lile or no manual intervention. So the chasm between academic research and large search engine companies on provably good system architectures remains relatively wide. Figure 1 shows the architecture of a complete LTR system consisting of at least two stages. Every aspect of this architecture should be considered when building eective and ecient search systems. Macdonald et al. [22] were among the rst to consider all of the dierent angles when building an LTR system for adhoc search.",1,ad,True
Improving Eciency in LTR Algorithms. A critical aspect of LTR must be considered when translating these powerful models,0,,False
"into working search engines which must index internet-scale document collections ­ eciency. Eciency concerns may be strictly algorithmic [3, 7, 14, 19, 20], they may explicitly focus on feature costs [1, 6, 23, 35­37, 39], or they may perform post-learning optimizations to reduce the size of the tree ensembles [18, 38, 39].",0,,False
"Another related line of research is to focus on the importance of balancing eciency and eectiveness in LTR systems, which is directly aligned with our current work. Perhaps the most comprehensive study on this problem is the recent work of Capannini et al. [7]. A less obvious trade-o concern is how to construct the ""sample"" of documents that are used for training and for scoring at runtime for new queries coming into the system [10, 22].is issue can have an important impact on both training and runtime scoring in multi-stage systems, and a problem that we revisit in the context of cascaded ranking. Finally, the cost of model training can also be an important problem [2, 22], but is not explored further in this work.",1,ad,True
"Cascade Ranking. Raykar et al. [28] described an approach to jointly train a cascade of classiers to support clinical decision making, with the expected cost of feature acquisition taken into account. is approach does not aempt to address the issues of cascade design, such as the number of cascade stages and the design of cutos. e closest work to our own are the cascade models previously explored by Wang et al. [36] and Xu et al. [39]. Wang et al. proposed a cascade learning algorithm based on an additive ranking model AdaRank. e algorithm produces a cascade by incrementally incorporating weaker rankers in the ascending order of cost eciency. In each stage only one weak ranker is incorporated. e document scores are accumulative, so conceptually all the previously selected features are involved in the scoring. However, more recent improvements in GBRT-based LTR algorithms has made this approach less competitive than state-of-the-art learning models.",1,ad,True
"Xu et al. [39] proposed an algorithm that takes a trained GBDT model and produces a cascade by reweighting the trees in the full model. e eectiveness of the cascade is roughly the same as a full GBDT model. ey use a monolithic cost function which accounts for several variables such as: model loss, tree evaluation costs, and feature cost. However, optimization of this loss function is quite complex, and their approach do not address the design issues pointed out in this paper, such as the eect of cascade structures on the nal retrieval eectiveness of the cascade model.",1,ad,True
3 APPROACH,1,AP,True
"Stage-wise cascades are exible models that allow for a number of architectural decisions, such as: the number of stages used, the number of documents forwarded to the next stage, and so on.e choice of features involved in each stage is a critical factor in balancing eciency and eectiveness in the end-to-end system.is trade-o is further elucidated by the following observations:",1,ad,True
"· A cascade may choose to defer the use of expensive features to later cascade stages as feature extraction on fewer documents is necessary, and will be more cost ecient.",1,ad,True
"· A cascade may choose to include useful features early on, since features extracted in earlier stages can be re-used in all remaining stages without incurring additional costs.e reusability of key features can make the cascade more eective.",1,ad,True
446,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
d1,0,,False
"C,K,3",0,,False
d2,0,,False
d3,0,,False
"C,K,1",0,,False
dN3,0,,False
d5,0,,False
d6,0,,False
dN2 d8,0,,False
d9,0,,False
"C,K,2",0,,False
d10,0,,False
d11,0,,False
d12,0,,False
dN1,0,,False
Figure 2: A three level cascade which initially takes ds documents,1,ad,True
"as the sample input. In Round 1, C, K ,"" 1 reorders all dN1 documents. In Round 2, a subset of the dN2 documents are reordered by C, K "","" 2. In the nal round, dN3 documents are reordered. Up to dN documents total can be returned from the nal level.""",0,,False
"Our general approach to cascade construction is to rst assign feature sets to dierent stages using a set of predened heuristics (c.f. Sec. 4), and then perform automatic feature selection for every stage of the cascade, while jointly optimizing ranking eectiveness and eciency. Ideally, the procedure should maintain performance comparable to a complete feature set model while at the same time accounting for feature costs. We now describe a theoretical framework for model regularization that reuses well-known solutions in the machine learning eld in order to achieve both objectives. Even though the goal of regularization is to minimize the eect of overing, in this paper we show how it can also be used to produce compact models that are feature extraction cost aware.",1,ad,True
3.1 Cost-Aware Feature Selection,0,,False
"Regularization. Supervised machine learning algorithms are exposed to a training set of pairs {(xi, i )}n with the goal ofnding an approximation to the function h, mapping to x that minimizes the expected value of a predened loss function L( , h(x)) over",0,,False
"the joint distribution of all (x, ) values:",0,,False
"h ,"" argmin E ,x[L( , h(x))] "","" argmin Ex[E [L( , h(x))]|x]. (1)""",0,,False
h 2H,0,,False
h 2H,0,,False
"e choice of loss function depends on the type of problems being learned (classication, regression, pairwise, listwise). It is common practice to incorporate a regularization term R(h) in the loss function to prevent overing. Regularization usually leads to improved eectiveness because sparsity is enforced in model training and, as a result, the learned model is less likely to overt the training set. e most common type of regularizers apply a penalty on the complexity, shrinking the value of the parameters in order to reduce overing. For instance, if h(x) ,"" wT x is a linear model with its parameters represented by a weight vector w 2 Rd , a widely-used regularizer is the L2 norm of the weight vector. Given a training set of n instances, the model would minimize the following""",1,corpora,True
expression:,0,,False
Xn,0,,False
"h ,"" argmin L( , hw (x)) + kwk22 ,""",0,,False
(2),0,,False
"h 2H i,1",0,,False
"where in this case h functionally depends on w. For instance h(x) ,"" wT (x), where is a kernel feature mapping.""",0,,False
"Cost-Aware L1 regularization. One problem with Equation 2 is that the learning algorithm is agnostic to feature costs. In order to minimize costs, one would like to reduce the number of features (covariates) that are used by the model, weighted by their cost, and at the same time maximize the performance. is problem is closely related to feature selection, and has a close connection with regularization. In fact, Equation 2 tries to bring down the contribution (weight) of each feature as much as possible while also minimizing the loss. In our case, however, having a non-zero weight for a particular feature implies that we have to pay the whole cost of extracting it, no maer how small it is.",0,,False
"Let c 2 Rd be the feature-cost vector, in which each entry represents the normalized cost for extracting the feature. In the case of a linear model, we want to minimize cT I>0 (w), where I>0 is the component-wise indicator function, which is 1 if the weight is over zero, and 0 otherwise. is penalty factor would be included in the formulation of Equation 2. In practice, this means we need a procedure for controlling the amount of covariates included in the nal model automatically. To do this, we allow the learner to perform automatic feature selection by adding a L1 penalty to the loss function (Eq. 2). is penalty is the L1 norm of the weight vector weighted by the feature costs c.",1,ad,True
"Conventionally, L1 regularized regression models with a least square loss function are also known as LASSO (least absolute shrinkage and selection operator) and were originally designed to perform covariate selection, and help to make the model more interpretable. Lasso is able to achieve this by forcing the sum of the absolute value of the regression coecients to be less than a xed value, which in practice forces certain coecients to be set to zero, eectively choosing a simpler model that does not include those coecients [31]. In our case, we will exploit this property to generate less expensive models in terms of feature extraction time.",0,,False
"To sum up, the ranker would minimize the expression:",0,,False
"Xn h ,"" argmin L( , hw (x)) + kwk22+ kc wk , (3)""",0,,False
"h 2H i,1",0,,False
where and are parameters that control the trade-o between the,1,ad,True
"loss and regularization penalty, and is the component-wise prod-",0,,False
"uct. erefore, the main idea is to learn a model using Equation 3, and then select the features that have a wi > 0, either directly in a linear model (we would use hw for ranking), or as an input to other LTR methods (which would learn a model using only the subset of",0,,False
parameters selected). ere are several options for learning the parameters w of such,0,,False
"a model. An eective method is to use stochastic gradient descent and update w one example at a time; in this case the training update for a sample (xj, j ) is as follows:",1,ad,True
"wt +1 , wt",0,,False
"t @ *L( @w ,",0,,False
X,0,,False
",hw(x)) + n",0,,False
i,0,,False
ci |wi |+ -,0,,False
",",0,,False
(4),0,,False
447,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"where t is the learning rate, which may depend on t, the number",0,,False
of iterations so far. Note that the L2 regularizer in Eq (3) is omied here for clarity.,0,,False
"Strictly speaking, the L1 norm is not dierentiable (at w ,"" 0). However, methods that rely on subgradients can be used to solve""",1,ad,True
"minimization problems that involve L1 regularized objective functions, which in this particular case, boils down to replacing the",0,,False
"partial derivative of the regularizer with its sign, which for each",0,,False
feature i results in:,0,,False
"wit +1 , wit",0,,False
"t @L( ,hw(x)) @wi",0,,False
t n sign(wki ),0,,False
(5),0,,False
One drawback of this formulation is that it may not produce a,0,,False
"compact model, because the weight of a feature does not become",0,,False
"zero unless it happens to be exactly zero, which is rare in prac-",0,,False
"tice. To overcome this limitation, we use a variant of a proximal",0,,False
method proposed by Tsuruoka et al. [33] which works well with,0,,False
"the Stochastic Gradient Descent (SGD) optimization procedure, and",1,ad,True
has been shown empirically to produce very compact models.e,0,,False
main motivation is to smooth out the uctuation of the gradients,1,ad,True
"through multiple iterations, which can be high when using SGD as",0,,False
"it approximates the true gradient, and is computed using the whole",1,ad,True
"sample, one example at a time. e original method, named SGD-",0,,False
"cumulative, approximates the loss gradient using the following",1,ad,True
update rules:,0,,False
"w^it +1 , wit",0,,False
"t @L( ,hw(x))",0,,False
",",0,,False
@wi,0,,False
"w,wt",0,,False
(6),0,,False
Xt,0,,False
"ut ,",0,,False
"j,",0,,False
(7),0,,False
"n j,1",0,,False
qit,0,,False
",",0,,False
Xt,0,,False
 wi,0,,False
j,0,,False
+1,0,,False
"w^ij+1 ,",0,,False
(8),0,,False
"j ,1",0,,False
andnally,0,,False
"( wit +1 ,",0,,False
"max(0, w^i t +1 (ut + qt min(0, w^i t +1 + (ut qt",0,,False
"1)), 1)),",0,,False
w^i t +1 > 0 w^i t +1  0,0,,False
(9),0,,False
In order to introduce the per-feature,0,,False
variable per feature as uit,0,,False
",",0,,False
ci,0,,False
n,0,,False
Pt,0,,False
"j ,1",0,,False
"cost ci , we j which is",0,,False
create one then used,0,,False
uit to,0,,False
update wit+1. It is important to note that the method is able to select,0,,False
a subset of features that can be used to further retrain any arbitrary,0,,False
"model, and thus it can be used in combination with state of the art",0,,False
non-linear rankers such as the ones commonly used in production,0,,False
"systems (GBRT or LambdaMART for example). Henceforth, we will",0,,False
"use a least squares loss function, and simple linear regression for h:",0,,False
L(,0,,False
",hw(x)) ,",0,,False
1 2,0,,False
wT,0,,False
 x,0,,False
2,0,,False
(10),0,,False
"is proved to be empirically eective in our setup, while also",0,,False
converging quickly in fewer epochs.,0,,False
"ere are several alternative feature selection methods, that in",0,,False
general are based on an optimality criteria metrics such as Bayesian,0,,False
"information criterion, or Minimum Description Length. In this work, we also make further use of GBRT's feature importance [12],1",0,,False
as it intrinsically captures interdependencies between covariates.,0,,False
"In short, the process learns a set of decision trees, where each node",0,,False
1An equivalent process exists for the case of multi-class classication.,0,,False
"splits the data using one feature. With each split, the tree outputs are modied, and the training squared loss varies. en, once an ensemble is learned, the non-terminal nodes of the trees can be iterated through to compute the reduction of squared loss for every feature, and the results aggregated for dierent feature splits. Lastly, the nal importance is computed as the average over all of the trees.",0,,False
3.2 Cascade Construction,1,ad,True
"Constructing a cascade model involves seing a number of parameters, including the number of cascade stages K, the cuto thresholds hc1, c2, . . . , cK i, and the features sets used in each stage hF1, . . . FK i. As one might expect, the design space of a cascade model is humongous. e complexity of exploring the entire space of all possible parameter combinations and feature allocations is prohibitively large, and interdependencies between features can aect both the eectiveness and computational costs signicantly.",1,ad,True
"Randomized Search. To tackle this problem, randomized search [4] is performed in this study to select the cascade conguration. is is done by randomly sampling a large number of cascade congurations from this space, followed by a seletion step that maximizes the cascade eectiveness on validation data. Ideally, this approach can explore any search space fairly eciently within a relatively small number of rounds, but when feature allocation is involved many feature combinations it explores will not be eective. Randomized search does not work well when good congurations are dicult to reach.",1,ad,True
"e cost-aware L1 regularization algorithm, as described in Sec. 3.1, was developed to mitigate this issue and simplify the search. It turns the search problem in a combinatorial space (that covers all possible ways of feature allocation) into a simple line search, making it possible to ""si through"" the feature allocation space eciently by tweaking . In our formulation, the coecient controls the desired level of eectiveness-eciency tradeo, so when a dierent tradeo is given a dierent subset of features that reects this change will be selected. Practically speaking, a small leads to a gently reduced feature set with slightly decreased eectiveness compared to the full model; a large , on the other hand, will prune the feature set fairly aggressively and result in a compact model that uses only couples of features, which is ideal as an early stage model. Using this algorithm, a cascade can then be constructed by feeding in a sequence of decreasing values (from early to late) to generate cascade stages.",1,ad,True
"More details about the use of randomized search will be described in later sections. In the rst two experiments, we use a set of predened cascade congurations to simplify the experimental setup and serve as the experimental control. Further investigations on ne-tuning cascade congurations is carried out using GOV2 with the best-performing cascade methods discussed in Sec. 4.3.",1,ad,True
"Feature Availability. We also experimented with a number of feature availability seings, and assume that the availability of a feature may change across cascade stages. In a production search system, some features might arrive much later than the others for various reasons, such as that they are expensive to run or their generation being deferred due to the design of the feature extraction procedure. To simulate this eect, our approach is to have",1,ad,True
448,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"certain models subdivide the full feature set into K equal-sized partitions and assign feature partitions to the respective cascade stages. e rationale behind this approach is that, by presenting a limited choice of features, which is 1/K of the full set, a feature extraction pipeline can be simulated to work in parallel with the ranking models, serving features in an order based on a pre-dened criteria. Each cascade stage has access to all features extracted in the previous stages without incurring additional costs. In this paper, we explored three dierent feature availability seings:",1,ad,True
(1) Cost-biased allocation (C): Features are rst sorted in ascending order of unit cost and partitioned into K stages.is seing is a close approximation to the scenario where cheap features are available to the ranking model earlier than expensive ones.,0,,False
"(2) Cost-eciency-biased allocation (E): e features arerst sorted in descending order of cost eciency and partitioned into K stages. e cost eciency of a feature is dened as the importance score divided by its unit cost, where importance is computed from a ground-truth tree model as described in Sec. 3.1. is seing simulates having a dedicated extraction pipeline for more cost-ecient features.",0,,False
"(3) Full allocation (F): All features are accessible from individual cascade stages. is seing represents the scenario where the choice of extracted feature is unrestricted, providing the greatest exibility to the underlying cost-aware feature selection algorithm.",1,ad,True
"Aer applying one of these seings, cost-aware L1 regularization is performed to each cascade stage separately with a sequence of decreasing values. e algorithm (Sec. 3.1) will reach the desired level of feature size in 10­20 epochs. Running this procedure for more iterations does not change the results. We also set a constant decaying learning rate , 0.1 across the board.",1,ad,True
4 EXPERIMENTS,0,,False
"We now evaluate the impact of our approaches on reducing costs in cascade learners in two dierent seings, one large but shallow LTR dataset, and a standard TREC benchmark with 150 queries but a large number of documents to be ranked per query.",1,ad,True
Experimental Setup. All experiments were executed on a 24-,0,,False
"core Intel Xeon E5-2630 with 256 GB of RAM hosting RedHat RHEL v7.2, and baselines generated using Indri2, Krovetz stemming,",0,,False
and dependency models generated using Metzler's MRF congu-,0,,False
ration3. All LTR algorithms were implemented in Python using,0,,False
4,0,,False
scikit-learn,0,,False
0.18.1,0,,False
and,0,,False
xgboost,0,,False
5,0,,False
0.6a2.,0,,False
Source,0,,False
"code,",0,,False
con-,0,,False
"guration les, and detailed explanations for all experiments can",0,,False
be found in the GitHub repository for this paper6.,0,,False
Two dierent test collections were used for the experiments.e,0,,False
rst collection is the C14 Webscope Yahoo Learning To Rank dataset 7 [9]. e dataset contains two subsets designed for dierent pur-,1,Yahoo,True
poses and used dierent feature sets. We use only Set 1 (Y!S1) which,0,,False
"2 hp://www.lemurproject.org/indri.php 3 hp://ciir.cs.umass.edu/metzler/dm.pl 4 hp://scikit- learn.org/stable/ 5 hps://github.com/dmlc/xgboost 6hps://github.com/rmit-ir/LTR Cascade 7 hps://webscope.sandbox.yahoo.com/catalog.php?datatype,c",1,ad,True
Table 1: Summary of the key properties of the two benchmark collections used in this study.,0,,False
# eries # Total Docs # Features,0,,False
Y!S1,0,,False
"6,983",0,,False
"165,660",0,,False
519,0,,False
GOV2,0,,False
"150 1,500,000",0,,False
425,0,,False
"contains 519 features (out of 700 in total) with an associated feature cost, and has 19,944 training queries, 1,266 validation queries, and 3,798 test queries. e original cost estimates included with the data were used without modication in our experiments. All features used in Set 1 have extraction costs between 1 and 200. Our second collection is the TREC GOV2 test collection (GOV2) using queries 701­850 in a 5-fold cross validated conguration. We created 425 features for this collection as described next in Section 4.2. For all queries, we created the initial sample by running BM25 with k1 , 0.9 and b ,"" 0.4 to an initial depth of 5,000. A summary of the two benchmark collections are shown in Table 1.""",1,TREC,True
"Learning Algorithms. Table 2 summarizes all of the baselines, as well as all of the new cascade model congurations tested on the two collections. We used a broad range of dierent learning algorithms in our experiments. ese ranking models can be divided into the following three categories:",1,ad,True
"(1) Ground Truth Models: We compare with ranking models executed in a non-cascade seing, where the full set of features is used in training and prediction. ree ranking models are employed: Gradient-Boosted Decision Trees (GBDT) [12], Gradient-Boosted Regression Trees (GBRT) [12], and LambdaMART [5]. Our implementations of these ranking models are based on xgboost.",1,ad,True
"(2) Baselines: We use several baseline methods, such as QL [40], BM25 [29], and SDM [24], on GOV2 to verify the gain in eectiveness relative to a standard retrieval seing. ese baseline methods are however not available for Y!S1.",0,,False
"(3) Cascade Baselines: We implemented the cascade ranking algorithm described in Wang et al. [36], using the suggested seing , 0.1. Note that seing a smaller does not improve its eectiveness. We also implemented early stopping on training eectiveness to avoid explicitly seing the number of cascade iterations.",1,ad,True
"Model hyperparameters (number of trees, depth, learning rate) were trained with the provided independent validation set for Y!S1, and using 5-fold cross-validation on GOV2. For ease of experimentation, some cascade parameters, such as the number of stages K, and cuto thresholds hc1, c2, . . . , cK i, were xed in the rst two experiments. Other parameters, such as , were tuned on the validation data using randomized search. Tree cascade parameters were tuned dierently on the two datasets, as previous parameters for the ground truth models did not always generalize well on the learned cascades. We also empirically found that the linear cascades work beer with the L2 regularization turned o (i.e., ,"" 0), making the SGD optimizations more stable. Further experimental details are described in Sections 4.1 and 4.2.""",1,ad,True
449,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 2: Summary of the baselines and new cascading methods used.,1,ad,True
"Method Name GBDT-BL GBRT-BL LambdaMART-BL QL-BL, BM25-BL, SDM-BL",0,,False
WLM-BL,1,LM,True
LM-C3-X,1,LM,True
GBDT-C3-X GBRT-C3-X LambdaMART-C3-X,0,,False
"Parameters Y!S1/GOV2: 1,000/525 trees, 16/16 nodes Y!S1/GOV2: 1,000/525 trees, 16/16 nodes Y!S1/GOV2: 1,000/525 trees, 16/32 nodes Default Indri seings",0,,False
"Y!S1: ,"" 0.1, GOV2: "", 0.1",0,,False
"Y!S1: 1,000 trees, 16 nodes GOV2: adaptive (Sec. 4.2) Y!S1: 1,000 trees, 16 nodes GOV2: adaptive (Sec. 4.2) Y!S1: 1,000 trees, 16 nodes GOV2: adaptive (Sec. 4.2)",1,ad,True
Description,0,,False
GBDT [12] (,0,,False
"), ,"" 0.05, subsample rate 0.8.""",0,,False
xgboost,0,,False
GBRT[12] (,0,,False
"), ,"" 0.05, subsample rate 0.8.""",0,,False
xgboost,0,,False
LambdaMART [5] (,0,,False
"), ,"" 0.05, subsample rate 0.8.""",0,,False
xgboost,0,,False
"Commonly used single-pass retrieval runs to depth 1,000 using ery likeli-",0,,False
"hood with Dirichlet priors smoothing, BM25, and a Sequential Dependency",0,,False
"Model (SDM). Note that while SDM is a strong eectiveness baseline, it has",0,,False
well-known eciency limitations when used on large document collection [17].,0,,False
"Reimplementation of the linear cascade model by Wang et al. [36], with early stopping on training NDCG.",1,ad,True
"ree level cascade using linear model under the elected feature availability seing X, trained using Stochastic Gradient Descent (SGD) with batch size set to 50 and ,"" 0.1. e seing X could be C/E/F. ree level cascade using GBDT under the elected feature availability seing X, using the same SGD conguration as LM-C3-X. ree level cascade using GBRT under the elected feature availability seing X, using the same SGD conguration as LM-C3-X. ree level cascade using LambdaMART under the elected feature availability seing X, using the same SGD conguration as LM-C3-X.""",1,ad,True
"Evaluation Metrics. For retrieval eectiveness, we used standard",0,,False
"early precision evaluation metrics: Expected Reciprocal Rank (ERR),",0,,False
"Normalized Discounted Cumulative Gain (NDCG), and Precision (P),",0,,False
"with three cutos (5, 10, and 20). We use",0,,False
8 to compute ERR,0,,False
gdeval,0,,False
"and NDCG, and",0,,False
9 to compute the precision to ensure,0,,False
trec eval,1,trec,True
that reported numbers are easily reproducible.,0,,False
"In this work, we focus on early precision improvements only,",0,,False
"but if deeper metrics are desirable, our cascade approach can be",1,ad,True
tuned to support it. e cost of a cascade is given by the following,1,ad,True
formula:,0,,False
1 XK X,0,,False
N,0,,False
"Ni C(f ),",0,,False
"i,1 f 2Fi",0,,False
"where C ( f ) denotes the unit cost of feature f , Ni denotes the number of documents that enter cascade level i, and N denotes the",1,ad,True
total number of documents that enter the cascade.,1,ad,True
4.1 Experiments on the Y!S1 Collection,0,,False
"In the rst experiment, we tested the eectiveness of the proposed cascade ranking algorithm on the Y!S1 collection. As a signicant number of queries in this data have less than 40 retrieved documents, there is relatively lile exibility in the design of cascade stages and cutos. In our initial investigation, we chose to utilize a xed conguration to simplify the experimental design. e cascade is congured to contain only 3 stages, with xed cutos h20, 10i between stages.",1,ad,True
"e values for the linear cascade models were derived using randomized search and NDCG on the validation data (cf. Table 3). For simplicity, all of the tree cascades in this experiment were trained with the same parameter seing as their ground truth counterparts. Note that tuning the number of trees/nodes in the tree",1,ad,True
8 hp://trec.nist.gov/data/web/10/gdeval.pl 9hp://trec.nist.gov/trec eval/,1,trec,True
"cascades can further improve the performance, and this approach is explored further in Sec. 4.2.",1,ad,True
"Main Results. e main results for the Y!S1 experiments are presented in Table 3. In the table, the results are divided into three sections. From top to boom they are: ground truth models, the cascade baseline, and the proposed cascade ranking models. Ground truth models, such as GBDT-BL or GBRT-BL, provide the best effectiveness in general, but the feature extraction costs are also signicantly higher. Interestingly, these models already perform their own kind of feature selection as some of the input features are never used in the nal trees, and therefore incur dierent costs.",1,ad,True
"When comparing cascading models, the cascade baseline WLMBL spends far less (0.62% of the cost incurred by GBDT-BL) on feature extraction than full models, at the cost of degraded eectiveness. Cascade models LM-C3-C, LM-C3-E, and LM-C3-F performed relatively poorly in terms of ERR@k and NDCG@k with respect to ground truth models, but in general their eectiveness is beer than the WLM-BL baseline. e tree-based cascade models are more competitive than their linear model counterparts. Among all cascading models, LambdaMART-based cascades appear to provide the best tradeo. e best-performing cascade LambdaMART-C3-F signicantly outperformed WLM-BL on all 9 tested metrics, but is still less ecient that all three ground truth models, leaving a noticeable gap of 0.01­0.02 in ERR@k, 0.04­0.05 in NDCG@k, and 0.01­0.03 in P@k.",1,ad,True
4.2 Experiments on the GOV2 Collection,0,,False
"In the second experiment, we investigate the use of the cascade ranking models on a commonly used web test collection, GOV2, where documents and features are to be processed and extracted by ourselves. To prepare the data for the cascade ranking experiment, for each query we retrieved 5,000 documents using BM25, and for each retrieved document 425 query or non-query features were",1,ad,True
450,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: Main results on Yahoo! Learning-to-Rank Challenge data. For the proposed cascade models, signicant improvements over WLM-BL are indicated by * for p < 0.05 and ** for p < 0.01 in a paired t-test.",1,Yahoo,True
ERR@k,0,,False
System,0,,False
@5 @10 @20,0,,False
Ground Truth Models,0,,False
GBDT-BL GBRT-BL LambdaMART-BL,0,,False
0.4605 0.4751 0.4598 0.4744 0.4526 0.4674,0,,False
Cascade Models (including Baseline) a,1,ad,True
0.4789 0.4782 0.4712,0,,False
WLM-BL,1,LM,True
0.3679 0.3876 0.3933,0,,False
LM-C3-C LM-C3-E LM-C3-F GBDT-C3-C GBDT-C3-E GBDT-C3-F GBRT-C3-C GBRT-C3-E GBRT-C3-F LambdaMART-C3-C LambdaMART-C3-E LambdaMART-C3-F,1,LM,True
0.3950 0.3871 0.3876 0.4191 0.4264 0.4178 0.4025 0.4100 0.4158 0.4163 0.4183 0.4353,0,,False
0.4127 0.4039 0.4047 0.4357 0.4419 0.4350 0.4203 0.4260 0.4332 0.4332 0.4346 0.4513,0,,False
0.4175 0.4089 0.4093 0.4405 0.4466 0.4395 0.4254 0.4313 0.4378 0.4379 0.4394 0.4557,0,,False
NDCG@k @5 @10 @20,0,,False
0.7448 0.7872 0.8279 0.7420 0.7852 0.8264 0.7314 0.7768 0.8203,0,,False
0.5886 0.6506 0.7088,0,,False
0.6461 0.6503 0.6541 0.6535 0.6721 0.6554 0.6304 0.6380 0.6479 0.6577 0.6629 0.6847,0,,False
0.7067 0.7033 0.7113 0.7100 0.7180 0.7163 0.6931 0.6867 0.7094 0.7145 0.7133 0.7354,0,,False
0.7638 0.7618 0.7666 0.7631 0.7703 0.7672 0.7488 0.7431 0.7612 0.7673 0.7671 0.7851,0,,False
@5,0,,False
0.8323 0.8322 0.8330,0,,False
0.7832 0.8086 0.8192 0.8226 0.7878 0.7942 0.7866 0.7743 0.7697 0.7862 0.7994 0.7968 0.8060,0,,False
P@k @10,0,,False
0.7577 0.7562 0.7564,0,,False
0.7171 0.7364 0.7413 0.7483 0.7245 0.7241 0.7310 0.7168 0.7009 0.7294 0.7328 0.7268 0.7379,0,,False
@20,0,,False
0.5967 0.5962 0.5964,0,,False
0.5673 0.5856 0.5885 0.5915 0.5781 0.5778 0.5819 0.5737 0.5637 0.5802 0.5820 0.5786 0.5847,0,,False
Cost,0,,False
15988 15876 15856,0,,False
99,0,,False
1871 1580 5278 1760 1535 4953 1760 1535 4949 1760 1535 4929,0,,False
"aAll -C models set values h100000, 30000, 500i, -E models use h8000, 8000, 3000i, and -F models use h5000, 800, 300i.",0,,False
"extracted. All 425 of the features implemented depend on either: the query; the query and term statistics from the indexed postings; the query, document and bigram statistics from ephemeral postings; the query and the document; or, the document. Table 5 shows a summary of these features. e majority of these features were derived from prior work within the LTR literature [15, 22, 23].",0,,False
"LTR Features. For all experiments, with GOV2, a total of 425 features were used. For each feature, several timing experiments were ran to compute the relative feature costs. We then normalized the costs based on the cheapest and most expensive features used in the experiments. Table 5 shows the complete feature breakdown based on the two main categories of features used.",0,,False
"e rst set of features are a large collection of pre-retrieval features commonly used for predicting query diculty [8], and more recently within LTR [21, 34] were gathered. ese features draw on statistical information contained within the query alone or on simple scoring methods that require postings list access. As such they are reasonably ecient to compute on-the-y at query time. e most important point about these features is that they are query specic, but must only be computed once using pre-computed unigram scores. is makes it relatively dicult to properly account for their true costs as LTR systems use SVM formaed inputles, which implicitly have a per document feature cost in the model. erefore, we divide these one-o pre-retrieval feature costs by the number of documents produced in the initial retrieval stage, resulting in an amortized unit cost of 1. All other costs are computed relative to this cost.",0,,False
"e second set of features are per document costs. While the cost of a single Document Prior lookup is very fast in practice, it must be done for every document in the current stage, and therefore more expensive than the one-o cost of the aggregate pre-retrieval feature scores. Likewise, all models incorporating bigrams are more expensive than their unigram counterparts. e bigram costs include a one-o cost to generate an ephemeral posting for the bigram [16], that can be reused to compute all of the bigram preretrieval features, and also used on the y for per document bigram scoring. is amortized cost is reected in the nal unit costs used for our experiments. Alternative indexing approaches [13, 27] have been proposed to improve the eciency of n-gram scoring in recent years, but feature-specic performance enhancements are beyond the scope of this work.",1,corpora,True
"Due to space constraints, we cannot describe all of the features or costs. A detailed description for all of the features as well as how costs (both estimated and real) can be found in the GitHub repository for the paper. e main point we want to make is that Table 5 provides realistic relative costs for both one-o and perdocument features, and takes into account the relative complexity of each.",0,,False
"Main Results. In our initial investigation, the cascade is congured to 3 stages with cutos h1000, 100i. In this basic conguration, the cuto thresholds are selected from widely used cutovalues in adhoc retrieval experiments. e experiment is conducted in a 5-fold cross-validated seing, so xing the cascade conguration can considerably speed up the search, with the caveat of achieving",1,ad,True
451,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 4: Main results on the GOV2 collection using 5-fold cross validation. For the proposed cascade models, signicant improvements over QL-BL/WLM-BL are indicated by */ for p < 0.05 (**/ for p < 0.01) in a paired t-test.",1,ad,True
ERR@k,0,,False
System,0,,False
@5,0,,False
@10,0,,False
@20,0,,False
Baseline Bag-of-Words and Term Dependency Models,0,,False
QL-BL BM25-BL SDM-BL,0,,False
0.3937 0.3781 0.4453,0,,False
0.4131 0.3980 0.4632,0,,False
0.4218 0.4062 0.4702,0,,False
Ground Truth LTR Models,0,,False
GBDT-BL GBRT-BL LambdaMART-BL,0,,False
0.4361 0.4501 0.4590,0,,False
0.4590 0.4678 0.4802,0,,False
0.4652 0.4745 0.4849,0,,False
Cascade Models (cost  5000),1,ad,True
WLM-BL,1,LM,True
LM-C3-C LM-C3-E LM-C3-F,1,LM,True
0.4221,0,,False
0.4297 0.4298 0.4366,0,,False
0.4422,0,,False
0.4454 0.4465 0.4537,0,,False
0.4485,0,,False
0.4537 0.4545 0.4608,0,,False
Cascade Models (cost  1/2 full model cost),1,ad,True
LM-C3-F,1,LM,True
0.4332 0.4508,0,,False
LambdaMART-C3-F a 0.4396 0.4578,0,,False
"LM-C3-F, adaptive b 0.4295 0.4469",1,LM,True
0.4566 0.4647 0.4530,0,,False
@5,0,,False
0.3839 0.3796 0.4396,0,,False
0.4441 0.4546 0.4684,0,,False
0.4204 0.4453 0.4418 0.4435,0,,False
0.4419 0.4373 0.4435,0,,False
NDCG@k,0,,False
@10,0,,False
@20,0,,False
0.3826 0.3806 0.4346,0,,False
0.3950 0.3814 0.4345,0,,False
0.4473 0.4446 0.4692,0,,False
0.4411 0.4345 0.4593,0,,False
0.4177,0,,False
0.4328 0.4315 0.4440,0,,False
0.4132,0,,False
0.4314 0.4294 0.4509,0,,False
0.4452 0.4333 0.4492,0,,False
0.4442 0.4208,0,,False
0.4501,0,,False
@5,0,,False
0.5275 0.5114 0.6013,0,,False
0.6255 0.6161 0.6470,0,,False
0.5919 0.5933 0.5946 0.6161,0,,False
0.6174 0.6094 0.6242,0,,False
P@k @10,0,,False
0.5007 0.4893 0.5711,0,,False
0.6027 0.5805 0.6215,0,,False
0.5664 0.5624 0.5624 0.5779,0,,False
0.5872 0.5732 0.5926,0,,False
@20,0,,False
0.5000 0.4705 0.5443,0,,False
0.5487 0.5305 0.5641,0,,False
0.5242 0.5312 0.5285 0.5601,0,,False
0.5517 0.5181 0.5611,0,,False
Cost,0,,False
­ ­ (High),0,,False
213683 211640 213482,0,,False
1249 4013,0,,False
11 4717,0,,False
145693 129529 110473,0,,False
"a With 650 trees and 32 nodes b With values h800, 0.1, 0.05i and cutos h2500, 700i",0,,False
Table 5: Summary of all features used in this work.,0,,False
Description,0,,False
Unit Cost # Features,0,,False
Pre-Retrieval Features,0,,False
ery Dependent (Unigram),0,,False
1,0,,False
159,0,,False
ery Dependent (Bigram),0,,False
100,0,,False
147,0,,False
Document Dependent Features,0,,False
Stage 0 Score Static Document Priors Score (Unigram) Score (Bigram),0,,False
1,0,,False
1,0,,False
500,0,,False
9,0,,False
"2,000",0,,False
107,0,,False
"8,000",0,,False
2,0,,False
Total,0,,False
425,0,,False
"limited improvement on retrieval eectiveness. is issue is briey investigated in this experiment by including a run that also optimizes the cuto thresholds. In Sec 4.3, we explore various cascade congurations and investigate the eect of cascade parameters on retrieval eectiveness.",1,ad,True
"Using a randomized search-based approach, the cascade models LM-C3-C, LM-C3-E, and LM-C3-F are selected by maximizing the unbounded NDCG score on the validation folds. In a 5-fold cross validated seing, this metric is averaged across 5 folds on the respective validation sets. e unbounded NDCG is not specic to any cuto threshold, so essentially it can be used to optimize any",1,ad,True
cascade stage. Similarly behaved recall-oriented metrics (such as Mean Average Precision) could also be used.,1,ad,True
"e main results for GOV2 are presented in Table 4. In contrast to the Y!S1 collection in the previous experiment, more than 72% of the features used on GOV2 are query dependent pre-retrieval features. e presence of query specic features poses a serious challenge to all cascade models. ery features are usually cheaper to compute, and more likely to be selected (by cost-biased strategy, for example) in early cascade stages. GBDT and LambdaMART can eectively use these query features, but for other ranking models the query features are not as useful. As a result, cascading models that do not eectively utilize query features oen see reduced eectiveness in the early cascade stages.",1,ad,True
"Ground truth models, as expected, give the best eectiveness among all baselines but also incurred the most feature extraction cost, around 210, 000­220, 000 unit cost. When compared with GBDT-BL and GBRT-BL, LambdaMART-BL achieves the best eectiveness. e cascade baseline WLM-BL spends far less on feature extraction, requiring only 0.58% of the full model cost, but at the cost of eectiveness.",1,ad,True
"A range of cascade models that spend less than 1/20 of the full model cost are rst selected using the LM-C3-C, LM-C3-E, and LMC3-F approaches. All three linear cascades outperform the WLM-BL baseline in nearly all metrics with the exception of P@10. Compared to WLM-BL, LM-C3-F signicantly improves NDCG@20 by 0.037, and P@20 by 0.035, spending three times more on feature extraction. All three selected models behave dierently than in the",1,ad,True
452,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
ERR@20 NDCG@20,0,,False
P@20,0,,False
0.47,0,,False
0.43,0,,False
0.39,0,,False
SDM,0,,False
BM25,0,,False
1K,0,,False
10K,0,,False
Unit Cost,0,,False
2 stage,0,,False
 3 stage,0,,False
4 stage,0,,False
5 stage,0,,False
100K,0,,False
(a) ERR@20,0,,False
0.46,0,,False
0.43,0,,False
0.40,0,,False
0.37,0,,False
SDM ,0,,False
2 stage ,0,,False
BM25,0,,False
  3 stage,0,,False
4 stage,0,,False
5 stage,0,,False
1K,0,,False
10K,0,,False
100K,0,,False
Unit Cost,0,,False
(b) NDCG@20,0,,False
0.55,0,,False
0.50,0,,False
0.45,0,,False
SDM ,0,,False
2 stage BM25,0,,False
3 stage   ,0,,False
4 stage,0,,False
5 stage,0,,False
1K,0,,False
10K,0,,False
100K,0,,False
Unit Cost,0,,False
(c) P@20,0,,False
"Figure 3: Eectiveness versus Cascade Cost in the GOV2 collection using the LM-C*-F models. e solid line at the boom represents the eectiveness of a BM25 BOW run, the doed line is a Sequential Dependency Model run which represents a competitive baseline on the collection, and the dots represent dierent LTR congurations and their respective trade-os. Based on the validation data, the highlighted dots in black signify the most eective runs overall, while the highlighted dots in blue are the best cost-eective runs.",1,ad,True
"previous experiment. Both LM-C3-F and LM-C3-C are of comparable costs roughly in the range of 4, 000­4, 800. LM-C3-E tends to select extremely compact feature sets and results in a greatly reduced cascade model that uses only the cheapest features. In general, the order of the three models in terms of eectiveness (in descending order) is LM-C3-F, LM-C3-E, and LM-C3-C, despite the fact that LM-C3-E actually costs much less than LM-C3-C. Other cascading models which require 1/2 of the full model cost are also shown in the table. None of the congurations from LM-C3-C and LM-C3E fall into this range. e best-scoring LM-C3-F model (in terms of validation set NDCG) achieves comparable performance to the same model selected in the previous group, but requires much more feature extraction resources. A LambdaMART-C3-F model trained by ing the selected feature sets in LM-C3-F does slightly better on ERR@k but sees degraded performance on NDCG@k and P@k. Note that, unlike the Y!S1 experiment, the parameters used in training LambdaMART-BL do not generalize over LambdaMARTC3-F. Another round of randomized search is needed to nd the conguration that maximizes the tradeo.",1,LM,True
"Finally, a LM-C3-F run that simultaneously optimizes the values and cuto thresholds is also presented. is model is generally the most eective cascade model in terms of NDCG@k and P@k. It signicantly outperforms the WLM-BL model on P@10 by 0.03, on P@20 by 0.035, and on NDCG@20 by 0.035. is result suggests that jointly optimizing feature allocation and cascade conguration can lead to further improvements. is issue is investigated further in the next experiment.",1,LM,True
4.3 Eect of Cascade Conguration,1,ad,True
"In the third experiment, we investigate the eect of cascade congurations on retrieval eectiveness and cascade cost. We relax two variables that were held xed in the previous experiments ­ the number of cascade stages, K, and the cutos hc1, c2, . . . , cK i ­ and jointly optimize these parameters together with values in a combined random search-based framework. As these congurations are more expensive to tune, the exploration was deferred until the inuence of other variables was beer understood. is experiment",1,ad,True
was carried out using the GOV2 collection. Our exploration starts by executing a full-range randomized search over the entire cascade design space.,1,ad,True
"We used predened grids of each variable to ensure that the explored data points were not too densely packed 10. We then iterated from K ,"" 2 to 5, which indicates the number of cascade stages, and for each seing of K, sampled a set of feasible cutos and values from the aforementioned seings. In the experiment, each seing of K produced more than 200 congurations.""",1,ad,True
"e eectiveness versus cascade cost for each explored combination were then ploed and shown in Figure 3, in which points from dierent seings of K are ploed in dierent colors and shapes. For each K seing, the best conguration (with cost < 1/2 full model cost) found by using NDCG validation is ploed as a black dot. ese ""best"" congurations are summarized in Table 6.",1,ad,True
"Figure 3 shows that a wide range of low cost but eective models can be found regardless of the choice of K. For ERR@20, two stage cascades are oen quite eective, but can also be among the most expensive. For NDCG@20 and P@20, three stage cascades consistently provided the most eective congurations. Among all seings of K, three level cascades consistently provided the best trade-o between eectiveness and eciency. We intend to investigate these trade-os further in future work.",1,ad,True
5 CONCLUSION,0,,False
"In this work, we have presented a new approach to cascaded ranking which can be used with any commonly used LTR algorithms. We make direct comparisons to several state-of-the-art approaches, and conclusively show that our approach can consistently achieve beer trade-os than other cascade ranking systems such as WLMBL. In the experiments, we have presented several eective feature allocation strategies that have not previously been explored, and are the rst to directly explore the relationship between the number",1,ad,True
"10e range of searched was {0.01, 0.03, 0.05, 0.08, 0.1, 0.3, 0.5, 0.8, 1, 3, 5, 8, 10, 30, 50, 80, 100, 300, 500, 800 }; the range of the cuto threshold is the union of the three sets: {20, 30, . . . , 100}, {100, 200, . . . , 1000}, and {2000, 2500, 3000, . . . , 5000}.",0,,False
453,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 6: e best conguration for the K-stage LM-C3-F cascade (for K ,"" 2, 3, 4, 5) found by maximizing the unbounded NDCG on the validation data. Signicant improvements over WLM-BL are indicated by */** for p < 0.05/p < 0.01 in a paired t-test.""",1,LM,True
System ( values; cutos),0,,False
"WLM-BL h800, 0.01i; h400i h800, 0.1, 0.05i; h2500, 700i h500, 10, 0.03, 0.01i; h3000, 2000, 700i h800, 0.5, 0.1, 0.08, 0.05i; h2000, 800, 500, 80i",1,LM,True
@5,0,,False
0.4204,0,,False
0.4529 0.4435 0.4446 0.4343,0,,False
NDCG@k,0,,False
@10,0,,False
@20,0,,False
0.4177,0,,False
0.4511 0.4492 0.4446 0.4340,0,,False
0.4132 0.4476 0.4501 0.4435 0.4408,0,,False
@5,0,,False
0.5919,0,,False
0.6094 0.6242 0.6161 0.6040,0,,False
P@k,0,,False
@10,0,,False
0.5664,0,,False
0.5866 0.5926 0.5913 0.5691,0,,False
@20,0,,False
0.5242 0.5517 0.5611 0.5530 0.5466,0,,False
Cost,0,,False
1249 18297 110472 106465 80612,0,,False
of cascades stages and document sample sizes on performance trade-os.,1,ad,True
"In future work we wish to more closely explore the relationship between feature costs and feature importance weighting at dierent levels of the cascade. Our current approach to parameter selection is largely empirical, and is quite costly when using hundreds of features and large scale document collections, resulting in several strong Linear Models, which are currently needed before generalizing to gradient boosted tree models. erefore, an appealing next step in this work is to nd more principled approaches to dynamically select the best cascade conguration on a per-query basis, and to further explore the best congurations for a wider variety of LTR ranking algorithms",1,ad,True
Funding Statement. is work was supported by the Australian Research Council's Discovery Projects Scheme (DP140103256 and DP170102231).,0,,False
REFERENCES,0,,False
"[1] N. Asadi and J. Lin. 2013. Document Vector Representations for Feature Extraction in Multi-Stage Document Ranking. Inf. Retr. 16, 6 (2013), 747­768.",1,ad,True
[2] N. Asadi and J. Lin. 2013. Training ecient tree-based models for document ranking. In Proc. ECIR. 146­157.,1,ad,True
"[3] N. Asadi, J. Lin, and A. P. De Vries. 2014. Runtime optimizations for tree-based machine learning models. Trans. on Know. and Data Eng. 26, 9 (2014), 2281­2292.",1,ad,True
"[4] James Bergstra and Yoshua Bengio. 2012. Random search for hyper-parameter optimization. Journal of Machine Learning Research 13, Feb (2012), 281­305.",0,,False
"[5] C. Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81.",0,,False
"[6] B. B. Cambazoglu, H. Zaragoza, O. Chapelle, J. Chen, C. Liao, Z. Zheng, and J. Degenhardt. 2010. Early Exit Optimizations for Additive Machine Learned Ranking Systems.. In Proc. WSDM. 411­420.",0,,False
"[7] G. Capannini, C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, and N. Tonelloo. 2016. ality versus eciency in document scoring with learning-to-rank models. Inf. Proc. & Man. 52, 6 (2016), 1161­1177.",0,,False
[8] D. Carmel and E. Yom-Tov. 2010. Estimating the ery Diculty for Information Retrieval. Morgan & Claypool.,0,,False
"[9] O. Chapelle and Y. Chang. 2011. Yahoo! Learning to Rank Challenge Overview. 14 (2011), 1­24.",1,Yahoo,True
"[10] C. L. A. Clarke, J. S. Culpepper, and A. Moat. 2016. Assessing eciency­ eectiveness tradeos in multi-stage retrieval systems without using relevance judgments. Inf. Retr. 19, 4 (2016), 351­377.",1,ad,True
"[11] J. S. Culpepper, C. L. A. Clarke, and J. Lin. 2016. Dynamic Cuto Prediction in Multi-Stage Retrieval Systems. In Proc. ADCS. 17­24.",0,,False
"[12] J. Friedman. 2001. Greedy function approximation: a gradient boosting machine. Annals of statistics (2001), 1189­1232.",1,ad,True
"[13] S. Huston, J. S. Culpepper, and W. B. Cro. 2014. Indexing Word-Sequences for Ranked Retrieval. ACM Trans. Information Systems 32, 1 (2014), 3.1­3.26.",0,,False
"[14] X. Jin, T. Yang, and X. Tang. 2016. A Comparison of Cache Blocking Methods for Fast Execution of Ensemble-based Score Computation. In Proc. SIGIR. 629­638.",0,,False
"[15] T.-Y. Liu. 2009. Learning to Rank for Information Retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009), 225­331.",0,,False
"[16] X. Lu, A. Moat, and J. S. Culpepper. 2015. On the Cost of Extracting Proximity Features for Term-Dependency Models. In Proc. CIKM. 293­302.",0,,False
"[17] X. Lu, A. Moat, and J. S. Culpepper. 2016. Ecient and Eective Higher Order Proximity Modeling. In Proc. ICTIR. 21­30.",0,,False
"[18] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, F. Silvestri, and S. Trani. 2016. Post-learning optimization of tree ensembles for ecient ranking. In Proc. SIGIR. 949­952.",0,,False
"[19] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonelloo, and R. Ven-",0,,False
"turini. 2015. ickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees. In Proc. SIGIR. 73­82. [20] C. Lucchese, F. M. Nardini, S. Orlando, R. Perego, N. Tonelloo, and R. Venturini.",0,,False
"2016. Exploiting CPU SIMD extensions to speed-up document scoring with tree ensembles. In Proc. SIGIR. 833­836. [21] C. Macdonald, R. L. T. Santos, and I. Ounis. 2012. On the Usefulness of ery Features for Learning to Rank. In Proc. CIKM. 2559­2562. [22] C. Macdonald, R. L. T. Santos, and I. Ounis. 2013. e whens and hows of learning to rank for web search. Inf. Retr. 16, 5 (2013), 584­628. [23] C. Macdonald, R. L. T. Santos, I. Ounis, and B. He. 2013. About learning models with multiple query-dependent features. ACM Trans. Information Systems 31, 3 (2013), 11:1­11:39.",0,,False
[24] D. Metzler and W. B. Cro. 2005. A Markov random eld model for term dependencies.. In Proc. SIGIR. 472­479.,0,,False
"[25] A. Mohan, Z. Chen, and K. Q. Weinberger. 2011. Web-Search Ranking with Initialized Gradient Boosted Regression Trees. Journal of Machine Learning Research 14 (2011), 77­89.",1,ad,True
"[26] J. Pedersen. 2010. ery understanding at Bing. Invited talk, SIGIR (2010). [27] M. Petri, A. Moat, and J. S. Culpepper. 2014. Score-safe term dependency",0,,False
"processing with hybrid indexes. In Proc. SIGIR. 899­902. [28] V. C. Raykar, B. Krishnapuram, and S. Yu. 2010. Designing ecient cascaded",1,ad,True
"classiers: tradeo between accuracy and cost. In Proc. KDD. 853­860. [29] S. E. Robertson, S. Walker, S. Jones, M. Hancock-Beaulieu, and M. Gatford. 1994.",1,ad,True
"Okapi at TREC-3.. In Proc. TREC-3. [30] N. Tax, S. Bockting, and D. Hiemstra. 2015. A cross-benchmark comparison of",1,TREC,True
"87 learning to rank methods. Inf. Proc. & Man. 51, 6 (2015), 757­772. [31] R. Tibshirani. 1994. Regression Shrinkage and Selection Via the Lasso. Journal",0,,False
"of the Royal Statistical Society, Series B 58 (1994), 267­288. [32] A. Trotman, C. L. A. Clarke, I. Ounis, J. S. Culpepper, M.-A. Cartright, and S. Geva.",0,,False
"2012. Open source information retrieval: a report on the SIGIR 2012 workshop. SIGIR Forum 46, 2 (2012), 95­101. [33] Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty. In Proc. ACL. 477­485. [34] S. Tyree, K. Q. Weinberger, K. Agrawal, and J. Paykin. 2011. Parallel Boosted Regression Trees for Web Search Ranking. In Proc. WWW. 387­396. [35] L. Wang, J. Lin, and D. Metzler. 2010. Learning to eciently rank. In Proc. SIGIR. 138­145.",1,ad,True
"[36] L. Wang, J. Lin, and D. Metzler. 2011. A Cascade Ranking Model for Ecient Ranked Retrieval. In Proc. SIGIR. 105­114.",1,ad,True
"[37] L. Wang, J. Lin, D. Metzler, and J. Han. 2014. Learning to eciently rank on big data. In Proc. WWW (Companion Volume). 209­210.",0,,False
"[38] Z. Xu, M. J. Kusner, K. Q. Weinberger, and M. Chen. 2013. Cost-Sensitive Tree of Classiers.. In Proc. ICML. 133­141.",0,,False
"[39] Z. Xu, M. J. Kusner, K. Q. Weinberger, M. Chen, and O. Chapelle. 2014. Classier Cascades and Trees for Minimizing Feature Evaluation Cost. Journal of Machine Learning Research 15 (2014), 2113­2144.",1,ad,True
"[40] C. Zhai and J. Laerty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Information Systems 22, 2 (April 2004), 179­214.",0,,False
454,0,,False
,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Information Retrieval Meets Game Theory: The Ranking Competition Between Documents' Authors,0,,False
Nimrod Raifer,0,,False
"Technion, Israel nimo@campus.technion.ac.il",0,,False
Fiana Raiber,0,,False
"Yahoo Research, Israel ana@yahoo-inc.com",1,Yahoo,True
Moshe Tennenholtz,0,,False
"Technion, Israel moshet@ie.technion.ac.il",0,,False
ABSTRACT,0,,False
"In competitive search se ings as the Web, there is an ongoing ranking competition between document authors (publishers) for certain queries. e goal is to have documents highly ranked, and the means is document manipulation applied in response to rankings. Existing retrieval models, and their theoretical underpinnings (e.g., the probability ranking principle), do not account for post-ranking corpus dynamics driven by this strategic behavior of publishers. However, the dynamics has major e ect on retrieval e ectiveness since it a ects content availability in the corpus. Furthermore, while manipulation strategies observed over the Web were reported in past literature, they were not analyzed as ongoing, and changing, post-ranking response strategies, nor were they connected to the foundations of classical ad hoc retrieval models (e.g., content-based document-query surface level similarities and document relevance priors). We present a novel theoretical and empirical analysis of the strategic behavior of publishers using these foundations. Empirical analysis of controlled ranking competitions that we organized reveals a key strategy of publishers: making their documents (gradually) become similar to documents ranked the highest in previous rankings. Our theoretical analysis of the ranking competition as a repeated game, and its minmax regret equilibrium, yields a result that supports the merits of this publishing strategy. We further show that it can be predicted with high accuracy, and without explicit knowledge of the ranking function, whether documents will be promoted to the highest rank in our competitions. e prediction utilizes very few features which quantify changes of documents, speci cally with respect to those previously ranked the highest.",1,ad,True
KEYWORDS,0,,False
ad hoc retrieval; game theory; ranking competition,1,ad,True
 e paper is based on work done while the author was at the Technion.,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080785",1,ad,True
Oren Kurland,0,,False
"Technion, Israel kurland@ie.technion.ac.il",0,,False
1 INTRODUCTION,1,DUC,True
"Ad hoc document retrieval models are o en based on the assumption of a xed document corpus -- i.e., corpus dynamics is not accounted for. e core challenge is estimating the relevance of a document to the query. e probability ranking principle (PRP) [25] is the theoretical foundation of this practice: to maximize user utility, documents should be ranked by their relevance probabilities.",1,hoc,True
"In practice, document corpora are not static as documents are changed, created or removed. Some of the corpus dynamics, specifically, in competitive search se ings (e.g., the Web), results from ranking incentives of document authors, henceforth referred to as publishers. at is, publishers might modify documents to promote them in rankings induced for queries of interest. ese modi cations are referred to as search engine optimization (SEO) [16]. Spam ltering, and more generally, using document quality measures as features in learning-to-rank methods [4], are examples of approaches for rank-penalizing documents that have gone through unwarranted modi cations (a.k.a., black-hat SEO [16]).",1,corpora,True
"However, existing retrieval approaches, and their theoretical foundations, do not account for future corpus dynamics driven by rankings. For example, it was recently shown that the PRP is sub-optimal in competitive retrieval se ings [3] as it can lead to decreased content breadth in the corpus, among other issues.",1,ad,True
"To estimate post-ranking corpus dynamics, speci cally, that caused by responses of publishers to rankings (i.e., document modi cations), analysis of the strategic behavior of publishers is called for. While types and techniques of SEO strategies were discussed in past work [16], these were not studied as response strategies with respect to rankings induced for speci c queries. Rather, they were presented as general actions observed on the Web (e.g., keyword stu ng and content copying).",0,,False
"Furthermore, there are no studies, to the best of our knowledge, that analyze publishers' strategies with respect to retrieval models and their foundations; namely, the e ect, over time, on features used for ranking. Such analysis is important for incorporating strategy predictions (estimates) in, and addressing their e ects on, retrieval approaches. A case in point, it was shown that if the actual writing quality of publishers for topics is known, then this information can be used in non-deterministic retrieval models to promote content breadth in the corpus, and therefore improve search e ectiveness along time [3]. More generally, analysis of the strategic behavior of publishers is crucial for se ing theoretical foundations for handling post-ranking corpus dynamics. e same way user modeling is",1,corpora,True
465,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"important for interactive information retrieval models [30], modeling the strategic behavior of publishers in response to induced rankings is important for addressing post-ranking corpus dynamics in retrieval models.",1,ad,True
"We present a novel initial theoretical and empirical analysis of the (temporal) strategic behavior of publishers in terms of changes they introduce to documents in response to induced rankings. e analysis is performed in the context of classical ad hoc retrieval models in two respects. First, we focus on content-based retrieval and accordingly content manipulation. Analyzing post-ranking strategies of changing hypertext, hyperlinks and a ecting clicks or any additional signal that can be used for relevance estimation is outside the scope of this paper. Nevertheless, we note that (i) content-based relevance estimates (e.g., Okapi BM25 and languagemodel-based estimates) are among the most important ones used in learning-to-rank approaches applied over Web data [20]; (ii) content manipulation techniques are quite pervasive, speci cally, over the Web [16]; and, (iii) for experiments we use a state-of-the-art learning-to-rank approach applied with content-based estimates. Second, we empirically study content manipulation in terms of the building blocks of classical, content-based, retrieval methods.",1,ad,True
ese include document-query surface-level similarities [14] and query-independent document relevance priors [4].,0,,False
"Performing empirical analysis of the ""ranking competition"" between publishers whose incentive is to have their documents ranked high, even if assuming the availability of a large-scale log of a search engine, is a major challenge due to the numerous dynamic aspects that a ect this competition. Over the Web, pages emerge and disappear, the search engine's index coverage changes rapidly, the ranking function, as well as estimates it utilizes, change throughout time and across sets of users and queries. Furthermore, di erent publishers cannot necessarily employ the same document modi cations, and many modi cations are not content-based as the ranking function also considers non content-based relevance signals.",0,,False
"Given that our goal, as described above, is to study the strategic behavior of publishers in the scope of the foundations of classical content-based retrieval models, we performed controlled empirical analysis by organizing ranking competitions between students in a course. Two basic conditions were set in these competitions. First, the students were not aware of the ranking function, nor of the actual features it used. Second, the students were incentivized to write quality documents that would be ranked high by the ranking function. As shown below, the dataset allowed to gain interesting and important observations about potential strategic behavior of publishers in a ranking competition.",0,,False
"An important observation that emerged in the competition analysis that we present is that publishers were gradually making their documents become more similar, in several respects, to those most highly ranked in previous rankings1. An interesting fundamental question that follows is whether this competing strategy can be theoretically justi ed given the information available to publishers: observations of past rankings and li le to no knowledge of the ranking function. To address this question, we present a novel game theoretic analysis of the ranking competition as a repeated",1,ad,True
"1 is strategy is conceptually reminiscent of the black-hat weaving and stitching content-based SEO techniques applied over the Web [6, 16] where content from legitimate pages is copied to spam pages so as to promote them.",0,,False
game [1]. Our main theoretical result with respect to the minmax regret equilibrium of the game [17] provides formal support to the merits of this publishing (competing) strategy.,0,,False
"In addition to analyzing the ranking competition theoretically and empirically, we set as a goal to predict whether a document would be ranked the highest given that this was not the case in the previous ranking; the predictor does not have explicit knowledge of the ranking function. Interestingly, relying on very few features that quantify the extent to which the document was changed and became similar to a document previously ranked the highest can yield high accuracy prediction. ese features are inspired by the cluster hypothesis [18], and more speci cally, one of the important operational premises that it gave rise to: ""similar documents should receive similar retrieval scores"" [9]. us, in lack of knowledge of the ranking function, the predictor essentially uses inter-document similarities as proxies for retrieval score similarities.",1,ad,True
Our contributions can be summarized as follows.,0,,False
· We present the rst dataset of query-based ranking competitions between publishers. e focus is on content manipulation.,0,,False
· We present an empirical analysis of publishers' strategies employed in the competitions.,0,,False
· We present a novel game theoretic analysis of the ranking competition as a repeated game. e main result of analyzing the minmax equilibrium of the game provides formal support to the merits of a key strategy employed by publishers in our games.,0,,False
"· We show that, in our se ing, it is possible to predict with high accuracy whether a document will be promoted to the highest rank in the next ranking. e prediction is based on very few features and does not rely on explicit knowledge of the ranking function.",0,,False
2 RELATED WORK,0,,False
"ere is much work on identifying, characterizing and addressing unwarranted (a.k.a. black-hat SEO [16]) actions of publishers [6]. In contrast, we focus on the strategic behavior of publishers when applying legitimate content-based manipulations.",1,ad,True
"Studies of the dynamic aspects of interactive retrieval focus on changes of queries and the ranking function (e.g., [15, 27, 30, 31]). Changes of clickthrough pa erns were also studied [27]. e dynamics of the collection as a result of the ranking competition, which is our focus, was not addressed.",1,ad,True
"ere has been work on studying and predicting the dynamics of the Web collection (e.g., [23, 26]), where the main operational goals were improving crawling policies and personalizing content delivery. Past versions of a Web page were used to improve its representation for ranking [13]. However, in contrast to our work, the dynamics has not been studied with respect to the ranking competition between publishers.",0,,False
"Recently, the publishers' ranking competition was analyzed using a game theoretic approach [3]. In contrast to our work, the assumption was that publishers have full knowledge of the ranking function, the competition was not analyzed as a repeated game, and no empirical analysis was presented.",0,,False
466,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"e merits of non-deterministic ranking functions from [3] were argued using a simulation of a ranking competition between publishers who stu query terms in documents [2]. In contrast with our work, publishers were assumed to know the basic (Okapi BM25) ranking function, there was no theoretical analysis of the competition and no analysis of non-simulated (real) ranking competitions.",0,,False
"A game theoretic approach was used to devise query-based ranking mechanisms that (i) maximize social welfare for ambiguous queries, by diversifying search results that are assumed to be scanned using random sequential search [12]; and (ii) balance relevance and monetization [11]. In contrast to our work, the competition between documents' authors (publishers) was not studied.",0,,False
"Game theoretical analysis has also been applied for adversarial classi cation [8, 10] and for optimizing learning-to-rank functions in non-adversarial retrieval se ings [28]. We address the competitive (adversarial) ad hoc retrieval se ing using di erent theoretical and empirical analyses.",1,ad,True
3 GAME THEORETIC ANALYSIS,0,,False
We analyze the ranking competition as a repeated game [1]. Analyzing the minmax regret equilibrium of the game yields a formal result that helps to explain a key strategy employed by publishers in the ranking competitions we organized as described in Section 5.,0,,False
"In what follows we assume that a query q, and some document ranking function (details below), have been xed. Let N ,"" {1, 2, . . . , n} be a set of n publishers (documents' authors) that would like to have their documents ranked high for q. Let Di be a nite set of documents that publisher i can (or might) write to convey the information she wants to share. For ease of presentation, and to avoid technical tie-breaking issues, assume Di  Dj "",""  for any i, j  N , i j. Let D "", ni,1Di be the set of all documents that can be wri en by the publishers.",0,,False
"We assume a complete linear ordering over D, denoted <. Such ordering can be based, for example, on a single (numeric) feature in a document representation2. Alternatively, the distance, under some representation, to a document which serves as a reference point (e.g., a document ranked the highest at some point) can serve to induce the ordering. us, for ease of exposition we can associate D with elements in the interval [0, 1]. A document ranking function for q is a mapping r : D  +. For simplicity (and avoiding tiebreaking), we assume r (di ) r (dj ) for any di , dj  D.",0,,False
"De nition 3.1. RSP(D1, . . . , Dn ) ,"" RSP(D) denotes the single peak ranking functions. ese are functions r de ned over D, such that for no d  D, there are di , dj  D, di < d < dj such that r (di ) > r (d) and r (dj ) > r (d).""",0,,False
"For example, linear learning-to-rank functions [20] are single peak with respect to each feature. e negative KL divergence used in the language modeling framework [19] is a single peak function over the multinomial distributions in the simplex by the virtue of being a concave function. However, the most e ective ranking functions (e.g., those utilizing non-linear learning-to-rank methods) are not single peak. Nevertheless, it is important to keep in mind that we analyze the dynamics from the point of view of publishers",0,,False
"2In this case, the analysis below applies to each feature in a document representation assuming that the values of others were xed.",0,,False
who have no information about the ranking function except for,0,,False
"that inferred by observing induced rankings. at is, publishers",0,,False
"may assume, and act based on the belief, that the ranking function",0,,False
"is single peak. Indeed, as shown in Section 5, the participants",0,,False
(publishers) in our ranking competitions can be viewed as searching,0,,False
for the structure of a single-peak ranking function for various,0,,False
"features, although the ranking function is not single-peak.",0,,False
Below we care only about the relative ranking of documents in,0,,False
"D; thereby, we consider the possible total ordering induced by the",0,,False
ranking function over D; there are nitely many such orderings.,0,,False
With a slight abuse of notation we will therefore refer to RSP(D),0,,False
as the set of possible single-peak orderings of documents in D.,0,,False
Let D0,0,,False
",",0,,False
"{d 0 ,",0,,False
1,0,,False
.,0,,False
.,0,,False
". , dn0 }",0,,False
be,0,,False
an,0,,False
initial,0,,False
set,0,,False
of,0,,False
documents,0,,False
where,0,,False
di0  Di is the initial document published by publisher i. We assume that each i  N possess no information at the beginning,0,,False
"about the function r  RSP(D), beyond knowing it is a single-peak",0,,False
"ordering. Consider t rounds, l ,"" 1, 2, . . . , t, in each of which each""",0,,False
"player i chooses a document di  Di , and obtains a utility of 1 if di",0,,False
"is ranked rst and 0 otherwise. Herein, a publisher or her document",0,,False
"is called ""winner"" if the document was the highest ranked; all other",0,,False
"publishers and their documents are called ""losers"". Let TO(D) be the set of possible total ordering over D. Notice that selecting di  Di for every i  N determines an ordering over the selected documents by the single-peak function r  RSP(D). e strategy of i at round l is de ned as a function from the history of previously",0,,False
"selected actions and outcomes (i.e., orderings) of all publishers, to",0,,False
"the document selected by publisher i. e outcome at each round can be associated with a subset R  RSP(D) of the possible single peak functions, as it rules out particular orderings. Henceforth, R is referred to as the knowledge state, as it captures the set of currently",0,,False
possible single peak orderings based on the observations received. e publishers ranking game just described is a repeated game [1].,0,,False
"In a repeated game, the same game is repeatedly played in rounds",0,,False
"(iterations). Speci cally, at each round a publisher publishes a",0,,False
"document, but a strategy in each round may relate to all information",0,,False
"observed so far; e.g., the documents published and rankings induced",0,,False
"in previous iterations. Accordingly, given the initial document set",0,,False
"D0, and the total number of rounds t, the set of possible strategies for player i is denoted Si (t, di0).3 e utility Ui (t, di0, s1, . . . , sn ), where sj  Sj (t, dj0) for every j  N , is the sum of utilities of player i in rounds 1, 2, . . . , t given the corresponding strategies.",0,,False
We now introduce a slight modi cation to the utility obtained by player i in a round to capture the cost of modifying documents. is,0,,False
cost re ects both the actual e ort involved in changing a document,0,,False
"and the ""penalty"" incurred by potentially dri ing from the actual",0,,False
"document i planned to publish. Assume there is some negligible cost C, i.e., C |D| < 1, where eC > 0 is the cost for changing document d to document d in distance e (assume standard distance on [0, 1]) in a single round. Formally, the utility of publisher i in round l will be",0,,False
based on its ranking (either rst or not) minus the cost of changing the document wri en in round l - 1.,0,,False
"Given the game described above, a major challenge is to de ne an",0,,False
appropriate solution concept which predicts behavior in the game.,0,,False
e classical solution concept in game theory is the celebrated Nash,0,,False
"3Si (t, d 0) encodes all possible documents published by i at any round of the game given the previous potential orderings.",0,,False
467,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"equilibrium, which is a strategy pro le, one for each player, for which unilateral deviations are not bene cial (i.e., any single player cannot gain by deviating from her strategy assuming the others stick to their strategies). is solution concept always exists in",0,,False
"nite games with complete information if players are allowed to use mixed strategies, and has been also extended to games with incomplete information where there are Bayesian assumptions about the actual game being played. However, our se ing does not exhibit such stylized assumptions, and we need to appeal to other solution concepts. In particular, in a minmax regret equilibrium [17], we consider strategy pro les such that each player (publisher) minimizes her regret when compared to the best response she could have played had she known the exact environment state (e.g., the exact ranking function) assuming others stick to their strategies; and this holds for all players simultaneously.",1,ad,True
"Given a strategy pro le s ,"" (s1, . . . , sn ) the regret of i is maxxUi (t, di0, x, s-i ) - Ui (t, di0, s); s-i denotes the strategy pro le applied by all players except for i. A strategy pro le s "","" (s1, . . . , sn ) is minmax regret equilibrium if for every i, si minimizes regret given s-i [17]. Given the de ned publishers game we can now show that:""",0,,False
T rium.,0,,False
3.2. Any publishers game has a minmax regret equilib-,0,,False
"P . We construct the following equilibrium. Let R be the knowledge state at the beginning of a given round l. At the beginning of round 1 all ranking functions in RSP(D) are possible, while",0,,False
at each following round the knowledge state can only shrink in,0,,False
"terms of the number of ranking functions it contains. Let Vl  [0, 1]",0,,False
be the set of documents which correspond to possible peaks of the,0,,False
functions in the knowledge state R; let dil-1 be the most recent,0,,False
document published by i. Let,0,,False
l i,0,,False
Di,0,,False
 Vl,0,,False
such that |,0,,False
l i,0,,False
- dil -1 |,0,,False
is minimal; if two documents have this minimal distance one is,0,,False
arbitrarily selected. e document published by i in round l would,0,,False
be,0,,False
l i,0,,False
.,0,,False
(In,0,,False
the,0,,False
rst round it is di0.) We now prove that this strategy,0,,False
of i minimizes its regret.,0,,False
"Let Vt be the knowledge state at the beginning of the last round t; Vt may result from arbitrary publishers' behavior in rounds 1, 2, . . . , t - 1. No publisher j i will publish a document not in Vt as",0,,False
"otherwise she cannot win (i.e., this strategy would be dominated). Hence, i's publishing a document out of Vt is dominated by publish-",0,,False
"ing the previous document. ( is has no cost, and publishing out",0,,False
"of Vt cannot result in a win.) On the other hand, since any  Vt",0,,False
"can be a winner, the worst regret would be for not publishing",0,,False
t i,0,,False
as de ned above.,0,,False
is is because,0,,False
t i,0,,False
might,0,,False
be,0,,False
the,0,,False
winner,0,,False
from,0,,False
this,0,,False
"point on, by the virtue of being in Vt , but it incurs minimal cost.",0,,False
"us, minimizing regret in the last round is achieved by selecting",0,,False
t i,0,,False
as,0,,False
prescribed.,0,,False
By,0,,False
"induction,",0,,False
using,0,,False
the,0,,False
argument,0,,False
from,0,,False
above,0,,False
results in i's strategy minimizing regret in every round.,0,,False
Two corollaries follow the proof:,0,,False
C,0,,False
3.3. e above constructed equilibrium is also a sub-,0,,False
game perfect equilibrium.,0,,False
"Namely, if an arbitrary sequence of documents has been selected up to round l < t, then in the remaining game, given the information provided so far on the potential peaks, following each player's",0,,False
strategy in the remaining rounds is still a minmax regret equilibrium.,0,,False
C,0,,False
3.4. Losers at round l - 1 will publish in round l,0,,False
"documents that become closer to (i.e., more similar) to that of the",0,,False
winner from round l - 1.,0,,False
"P . Assume wlog that a publisher who lost round l - 1 published dj  [0, 1] that satis es dj < dw where dw  [0, 1] was the winning document. As selecting any dj < dj is dominated given the knowledge state gathered, and the regret for publishing dj > dw is higher than that of publishing dj such that dw > dj > dj , we",0,,False
get the corresponding phenomenon. Notice that this will also imply,0,,False
that current winners will not change their documents.,0,,False
"us, Corollary 3.4 helps to explain a key strategy employed by publishers in the competitions we organized as we show below; namely, mimicking the winners.",0,,False
4 DATA,0,,False
"As discussed in Section 1, our goal is to analyze content-based ranking competitions so as to shed light on the strategic behavior of publishers. Since there are no publicly available datasets that can be used to that end, we organized repeated ranking competitions. e resulting dataset is available at h ps://github.com/asrcdataset/asrc. (See details in Appendix A.) We next describe the essentials of the competition.",0,,False
"Fi y two senior-undergrad and grad students in an information retrieval course were the publishers. e competition included 31 di erent repeated matches, each of which was with respect to a di erent TREC's ClueWeb09 query. Each student participated in three matches. Five students competed against each other in all matches except for one in which six students competed.",1,ad,True
"e competition was run for eight rounds; i.e., there were eight matches per query. Before the rst round, an example of a relevant document was provided for each match (query). Students were incentivized by course-grade rewards to edit their documents along the rounds so as to have them ranked as high as possible.4 As from the second round, students participating in a match were presented with the ranking of documents submi ed in the previous round by all competitors in the same match.",1,ad,True
"All documents were unstructured plain text of up to 150 terms. e document ranking model was based on the state-of-the-art LambdaMART [29] learning-to-rank approach integrating three classes of features. e rst are query-dependent features, such as eryTermsRatio (ratio of query terms appearing in a document) and LMIR.DIR (language-model-based similarity of a document to the query). e second class of features are query-independent document quality measures [4, 21], including Entropy (entropy of the term distribution in a document) and StopwordsRatio (stopwords to non-stopwords ratio in a document). Increased entropy and occurrence of stopwords a est to content breadth and hence to high prior probability of relevance [4].",1,LM,True
"e feature in the third class, SimInit, was used to incentivize students to write documents that dri from the initial relevant document shared by all students competing in the same match: it is",0,,False
4Students were assigned with unique IDs and all data was anonymized.,0,,False
468,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
# of publishers # of publishers,0,,False
10 8 6 4 2 0 0 1 2 3 4 5 6 7 8 9 10 11 # of matches won,0,,False
50 40 30 20 10,0,,False
0 1234567 # of consecutive matches won,0,,False
Figure 1: e number of (consecutive) matches won (x-axis) by a given number of publishers (y-axis).,0,,False
"based on the language-model similarity of a document to the initial relevant document. We note that in practical scenarios, publishers would rarely change their documents so they will not include the information originally intended for sharing. Indeed, in the theoretical analysis presented in Section 3, a cost was assigned to changes of documents. Yet, as we show below, the conclusions we draw about strategies are aligned with our theoretical results.",0,,False
"Documents (manually) classi ed as keyword stu ed were penalized in the ranking. Information about the ranking function and the features it utilizes was not disclosed to students. e resulting collection contains (i) 1279 documents: 31 initial relevant documents and 1248 documents created by students, 897 of which are unique5; (ii) keyword stu ng annotations; and (iii) exhaustive relevance judgments. Appendix A provides additional details of the collection and ranking model.",1,ad,True
5 EMPIRICAL ANALYSIS,0,,False
"In the following analysis, winner (loser) is a document (or publisher thereof) which was (not) ranked rst in a match.",0,,False
5.1 Analysis of wins,0,,False
"Figure 1 (le ) presents the number of matches won by a given number of publishers (students). e competition included 248 distinct matches (8 rounds × 31 matches per round). Each student was assigned with exactly 3 queries; hence, the maximum number of matches a student could win is 24. We see that only two of the students did not win even a single match, a esting to the students' engagement in the competition. e maximum number of matches won was 11, less than half of the maximal possible number of wins, indicating that the competition was dynamic.",0,,False
Figure 1 (right) presents the number of consecutive matches won by a given number of students; the maximum is the number of rounds (eight). We see that most students could retain the rst rank for at most three rounds. Only a small number of students retained the rst rank in more than four rounds. is nding further a ests to the strong competition held between the students.,0,,False
5.2 Analysis of strategies,0,,False
"By Corollary 3.4, to win matches, losers in previous rounds will publish documents that become similar to that of the winner from the preceding round. Accordingly, we next analyze the similarity",0,,False
"5Several students submi ed the same document over a few rounds; e.g., if the document was the highest ranked in a previous round.",0,,False
of documents that did not win a match (losers) to the winner over a series of rounds in which these documents remained losers. e similarity to the winner is estimated with respect to some of the features used to rank documents which were presented in Section 4.,0,,False
"e eryTermsRatio and LMIR.DIR features quantify the querydocument match; LMIR.DIR is a representative query-document surface-level similarity estimate [14]. e Entropy and StopwordsRatio features are among the most e ective query independent content-based document relevance priors reported in the literature [4]. Hence, the analysis of the strategic behavior of publishers we present next relies on estimates that constitute the foundations of classical content-based ad hoc retrieval approaches.",1,LM,True
Figure 2 depicts the average values of the features for documents that were losers in at least four consecutive rounds before winning a match6. We distinguish between documents whose feature value four rounds before winning a match was lower than or equal to that of the winner (LW) and those whose feature value was higher than that of the winner (L>W). We also present the average feature values of winners (W).,0,,False
"We see in Figure 2 that the average feature values of winners remain relatively stable along the competition; thus, winner documents, o en wri en by di erent publishers, tend to be quite similar along a few dimensions (features).",0,,False
"Figure 2 also shows that, in general, Entropy o en decreases along rounds and eryTermsRatio increases. is a ests to high content repetition in winner documents that might result from high occurrence of query terms. SimInit decreases which is potentially due to our rewarding diversi cation with respect to the initial relevant document.",0,,False
"More generally, we observe a clear trend throughout the competition: feature values of loser documents which became winners were becoming closer, o en gradually converging, to those of winners from previous rounds regardless of their initial values. at is, in lack of knowledge of features used for document ranking, losers were mimicking winners and thereby indirectly a ecting these features. is nding is in accordance with Corollary 3.4.",1,ad,True
6 PREDICTING WINNERS,0,,False
"Given that loser publishers apply the strategy of mimicking the winners, an interesting challenge rises: leveraging aspects of this strategy to predict, without using explicit knowledge of the ranking function, which loser publisher in round l - 1 will win round l assuming that a previous loser indeed wins this round.7",0,,False
"For prediction, we represent each document as a feature vector and de ne two sets of features (details below) that quantify the extent to which the document becomes more similar to the winner of the previous round. e features in the rst set are estimates of this similarity on a macro level, where documents are treated as",0,,False
"6Similar trends were observed for other features used by the ranking model and for losers that lost in at least three or ve consecutive rounds. ese results are omi ed as they convey no further insight. 7Predicting which publisher will win round l regardless if it won round l - 1 is a challenge for future work. As stated in the proof of Corollary 3.4, and as observed in the competitions, winners did not tend to change their documents. is is a fundamental di erence with the dynamics of loser documents which makes this prediction task challenging. For example, many of the dynamics-based features de ned below for predicting whether a loser will turn to a winner are degenerated for winner documents which do not change.",0,,False
469,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
QueryTermsRatio 1.0,1,Query,True
LMIR.DIR 7.0,1,LM,True
Entropy 4.4,0,,False
StopwordsRatio 1.0,0,,False
SimInit 0.8,0,,False
LW,0,,False
0.8,0,,False
5.0,0,,False
4.2,0,,False
0.8,0,,False
0.6,0,,False
L>W,0,,False
W,0,,False
0.6,0,,False
3.0,0,,False
4.0,0,,False
0.6,0,,False
0.4,0,,False
0.4 -4 -3 -2 -1 0,0,,False
1.0 -4 -3 -2 -1 0,0,,False
3.8 -4 -3 -2 -1 0,0,,False
0.4 -4 -3 -2 -1 0,0,,False
0.2 -4 -3 -2 -1 0,0,,False
"Figure 2: Averaged feature values of documents that were losers in at least four consecutive rounds before becoming winners, and whose feature values four rounds before winning were either lower or equal (`LW') or higher (`L>W') than those of the winner. `W': averaged feature value of the corresponding winners. x-axis: (minus) number of rounds before a document won a match. e values of LMIR.DIR are scaled by 100.",1,LM,True
whole units. e features in the second set are micro level similarity estimates that allow to analyze the potential actions taken by publishers to make their documents similar to the winner.,0,,False
6.1 Features,0,,False
"e features in the rst set, henceforth Macro features, are estimates of the bag-of-terms textual similarities (denoted SIM) between the document in round l (D), the document wri en by the same publisher in the previous round l - 1 (PD) and the winner of the previous round l - 1 (PW). e Cosine between tf.idf vector representations of documents is the similarity estimate. ree estimates are used: SIM(D,PD), SIM(D,PW) and SIM(PD,PW).",0,,False
"Using these inter-document similarity measures is inspired by the cluster hypothesis [18] which states that ""closely associated documents tend to be relevant to the same requests"". More speci cally, an important operational manifestation of the cluster hypothesis is the premise that e ective retrieval methods should assign similar documents with similar retrieval scores [9]. Based on the premise, given that the predictor we devise has no explicit knowledge of the ranking method used, inter-document similarities can potentially serve as proxies for similarities between retrieval scores.",0,,False
"e features in the second set, henceforth Micro features, focus on potential actions of publishers to make their documents similar to PW, the winner of the previous round. A document becomes similar to the winner, based on a bag-of-terms representation, if terms from the winner are added and terms not in the winner are removed. Accordingly, given a set S of terms, ADD(PW) and RMV(PW) are the number of unique terms t  S used in PW that were added to, or removed from, the document, respectively. Similarly, ADD( PW) and RMV( PW) are the number of unique terms t  S not used in PW that were added to or removed from the document, respectively. We de ne three term sets S: (i) query terms ( ery), (ii) frequent terms, speci cally, stopwords (Stopwords), and (iii) non-frequent terms not in the query ( ery Stopwords).8",1,ad,True
"Overall, we use 15 features: 3 Macro (SIM(D,PD), SIM(D,PW), SIM(PD,PW)) and 12 Micro ({ADD(PW), RMV(PW), ADD( PW), RMV( PW) } × { ery, Stopwords, ery Stopwords}).",0,,False
"e Macro features, which quantify temporal inter-document similarity changes, are ranking-model agnostic. e Micro features",0,,False
8A term is considered a stopword if it is among the 100 most frequent alphanumeric terms in the ClueWeb09 Category B corpus.,1,ClueWeb,True
"are based on temporal changes of addition/deletion of terms. While term-based information (e.g., query-terms occurrence) would be used by any reasonable ranker, the prediction model uses no explicit knowledge about how this information is used by the non-linear ranker applied in the competition, nor about other features used for ranking.",1,ad,True
6.2 Prediction setup,0,,False
"In each round of the competition, queries for which the winner of the previous round remained the winner were discarded, as our goal is to predict which loser publisher in a previous round will win the current round. us, the number of queries considered in each round ranges from 6 to 26 (out of all 31 queries).",0,,False
"We used the features9 from Section 6.1 for binary classi cation with logistic regression (LReg), linear SVM (LSVM), polynomial SVM (PSVM) and random forests (RForest) via the scikit-learn library [22]; the two classes are winner and loser. To train the classi ers and set hyper-parameter values, we used leave-one-out cross validation over rounds. e documents submi ed by students with respect to all considered queries in a round served for testing; those submi ed in the remaining six rounds, excluding the rst, served for training. Prediction was performed per query: the document in the current round which was wri en by a loser publisher from the previous round and which was assigned the highest classi cation score was predicted the winner; all other documents were predicted to be losers.",0,,False
"Prediction e ectiveness is measured using Accuracy: the percentage of documents correctly predicted as winners or losers, and F1: harmonic mean of Precision and Recall.10 Values are averaged over queries and test folds. Statistically signi cant e ectiveness di erences are determined using the two-tailed paired t-test with p  0.05 applied over queries.",0,,False
"e hyper-parameter values of the classi ers were selected to optimize Accuracy over the train set. For LReg, LSVM and PSVM, the value of the regularization parameter is in {1, 10, 50, 100}.11",0,,False
"e degree of the polynomial SVM (PSVM) was in {2, 3, 4, 5}. e number of trees and leaves for RForest were selected from {10, 50,",0,,False
"9Feature values were min-max normalized per query. 10 Precision is the fraction of correctly predicted winners out of all documents predicted to be winners. Recall is the fraction of winners correctly predicted as winners. 11LReg, LSVM and PSVM were trained with L1 regularization.",0,,False
470,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Prediction e ectiveness of the four classi ers (LReg, LSVM, PSVM, RForest) and the baselines. e performance di erences between each of the classi ers and each of the baselines are statistically signi cant. All the di erences with RForest are statistically signi cant. Bold: the best result in a row. Note: F1 of AllLosers is 0 due to zero Recall.",0,,False
Random Majority AllWinners AllLosers LReg LSVM PSVM RForest,0,,False
Accuracy 0.627,0,,False
F1,0,,False
0.242,0,,False
0.685 0.363,0,,False
0.247 0.396,0,,False
0.753 0.000,0,,False
0.849 0.859 0.867 0.695 0.712 0.730,0,,False
0.878 0.752,0,,False
"100, 500} and {10, 20, 30}, respectively. All other hyper-parameters were set to their default values [22].",0,,False
6.3 Prediction e ectiveness,0,,False
"Main result. We compare the prediction e ectiveness of the aforementioned classi ers with that of four baselines. All prediction algorithms predict as winner(s) documents whose publishers lost the previous round. (i) Random: a single winner is randomly selected; (ii) Majority: the document whose publisher won the majority of past rounds for the query is predicted the winner (ties are broken arbitrarily); (iii) AllWinners: all documents are predicted winners, in which case only one document per query is correctly predicted; and, (iv) AllLosers: all documents are predicted losers, in which case all but one of the documents are correctly predicted as losers. e results are presented in Table 1. Although the four classi ers (LReg, LSVM, PSVM and RForest) utilize no knowledge of the ranking model, they predict with high e ectiveness the winner of the current round. Moreover, the di erences in prediction e ectiveness between each of the classi ers and each of the four baselines are substantial and statistically signi cant. ese ndings a est to the ability to predict winners from previous losers in our competitions based on macro-level and micro-level manipulation strategies of publishers.",0,,False
"Among the four classi ers, the lowest performance is posted by LReg, while the highest is posted by RForest. Hence, in the analysis to follow we focus on RForest.",0,,False
"Feature analysis. We next study the relative e ectiveness of the sets of features used in RForest. Recall that the 15 features belong to two sets: Macro and Micro. e Micro features belong to three subsets: ery, Stopwords and ery Stopwords. In Table 2 we compare the prediction e ectiveness of training RForest using di erent combinations of these (sub)sets of features. We present for reference the e ectiveness of the Majority, AllWinners and AllLosers baselines. We see that using even a single (sub)set of features yields prediction e ectiveness that statistically signi cantly surpasses that of the baselines. Among the three subsets of Micro features, the query-term-based features ( ery) are the most e ective. Integrating all three subsets leads to prediction e ectiveness that always statistically signi cantly surpasses that of using either one or two of the subsets. We also see that using Micro features alone leads to slightly higher e ectiveness than using only Macro features; the di erence is not statistically signi cant. Yet, combining both sets yields the highest prediction e ectiveness. ese",1,ad,True
"ndings suggest that the Micro and Macro features, as well as the three subsets of Micro features, are complementary to some extent.",0,,False
"Table 2: Using subsets of features for prediction. All di erences with respect to Majority, AllWinners, AllLosers and Macro+Micro are statistically signi cant. Bold: best result in a column.",0,,False
Majority AllWinners AllLosers,0,,False
ery Stopwords,0,,False
"ery Stopwords ery+Stopwords ery+ ery Stopwords Stopwords+ ery Stopwords Micro , ery+ Stopwords+ ery Stopwords Macro Macro+ ery Macro+Stopwords Macro+ ery Stopwords",0,,False
Macro+Micro (all features),0,,False
Accuracy,0,,False
0.685 0.247 0.753,0,,False
0.821 0.809 0.796 0.826 0.825 0.813 0.837 0.836 0.851 0.849 0.847,0,,False
0.878,0,,False
F1,0,,False
0.363 0.396 0.000,0,,False
0.635 0.594 0.587 0.650 0.648 0.617 0.673 0.671 0.702 0.694 0.692,0,,False
0.752,0,,False
We next study the e ectiveness of individual features. Table 3 presents the Accuracy of ablation tests performed upon RForest.12 We also report MRR: the mean di erence between the reciprocal ranks of the actual winner when documents are ranked in descending and ascending order of individual feature values. We rst see that removing any single feature statistically signi cantly hurts Accuracy. is a ests to the complementary nature of the features.,0,,False
"e negative MRR of SIM(D,PD) indicates, as expected, that to win a match, a loser publisher should change her document with respect to the previous round. e positive MRR of SIM(PD,PW) and SIM(D,PW) suggest that the document should be similar to the winner (from the previous round) in the previous and current rounds so as to win the match. is nding is aligned with Corollary 3.4.",0,,False
"e MRR of features in the ery and Stopwords subsets indicate that adding (removing) query terms is always good (bad) practice for becoming the winner, regardless of whether these terms were used by the winner. is nding is further supported by the observations about eryTermsRatio in Section 5.2. In contrast, removing (adding) frequent terms, i.e., stopwords, is always good (bad) practice, regardless of the use of stopwords by the winner. e MRR of features in the ery Stopwords subset, which refers to terms that are neither query terms nor stopwords, imply that to",1,ad,True
12Similar pa erns were observed for F1. ese results are omi ed as they convey no additional insight.,1,ad,True
471,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 3: Ablation tests: Accuracy of RForest when trained without one feature. RForest's Accuracy with all features is 0.878. All di erences with RForest are statistically signi cant. MRR: the mean reciprocal ranks di erence of the winner when ranking documents in descending and ascending order of feature values.,0,,False
Macro Features,0,,False
Micro Features,0,,False
Feature,0,,False
"SIM(D,PD) SIM(D,PW) SIM(PD,PW)",0,,False
Ablation,0,,False
0.829 0.837 0.820,0,,False
MRR,0,,False
-0.136 0.168 0.184,0,,False
Feature,0,,False
ADD(PW) RMV(PW) ADD( PW) RMV( PW),0,,False
ery,0,,False
Ablation MRR,0,,False
0.844 0.851 0.840 0.834,0,,False
0.130 -0.043,0,,False
0.104 -0.620,0,,False
Stopwords,0,,False
Ablation MRR,0,,False
0.840 0.843 0.856 0.847,0,,False
-0.219 0.043,0,,False
-0.023 0.060,0,,False
ery Stopwords,0,,False
Ablation MRR,0,,False
0.841 0.857 0.849 0.837,0,,False
0.183 -0.081 -0.053,0,,False
0.029,0,,False
"win a match a document should become more similar to the winner by adding and not removing terms that were used by the winner (positive MRR of ADD(PW) and negative MRR of RMV(PW)), as well as removing and not adding terms that were not used by the winner (positive MRR of RMV( PW) and negative MRR of ADD( PW)). ese manipulations which do not directly a ect the query-document similarity estimates a ect other features used by the ranking model (e.g., Entropy).",1,ad,True
7 CONCLUSIONS,0,,False
"We presented an initial theoretical and empirical study of the strategic behavior of publishers (documents' authors) in query-based ranking competitions. e publishers' goal is promoting their documents in rankings using li le available information, mainly about past rankings. Analysis of ranking competitions that we organized revealed that to achieve their goal, publishers were making their documents similar to those ranked the highest in previous rounds. A game theoretic analysis of the competition yielded a result that provides formal support to the merits of this strategy. We also showed that high accuracy prediction of whether a document will be promoted to the rst rank in our competitions can be achieved using very few features which quantify document changes.",0,,False
Acknowledgments We thank the reviewers for their comments. is work was supported in part by a Google Faculty Research,0,,False
Award.,0,,False
A THE RANKING COMPETITIONS,0,,False
"We next discuss the competition guidelines provided to students (Section A.1), the incentives for participating in the competition (Section A.2), the queries and examples of relevant documents (Section A.3) and the ranking function used (Section A.4).",0,,False
A.1 Guidelines,0,,False
"To alleviate the task for students, and to increase their engagement in the competition, the length of all documents was limited to 150 terms. Students were instructed to write unstructured plain text documents.",0,,False
Duplication of other documents (determined based on a bagof-terms comparison) resulted in the duplicate document being ranked last. e students were permi ed to copy parts of other documents from the competition or the Web. Students were guided,0,,False
"to write documents of the highest quality avoiding slang and informal language. e use of black hat SEO techniques [16], such as keyword stu ng, was discouraged by telling the students that the ranking function will penalize low quality documents, partly based on human annotations. We informed the students that they could use the provided examples of relevant documents, but that the documents they create need not necessarily be relevant.",0,,False
A.2 Incentives,0,,False
"e incentive for participating in the competition was earning extra credit points for the exam. For each query, a student earned two thirds of a point if her document was ranked rst for a query in a match. A third of a point was given to all other students competing with respect to the same query (i.e., the same match).",0,,False
In the rst half of the competition many students did not (signi cantly) update their documents even if these were not ranked,0,,False
"rst. erefore, we further incentivized the students by changing the reward mechanism as from the h round. e student whose document was ranked rst for a query was reworded one point. Students whose documents were ranked second and third were rewarded two thirds and third of a point, respectively. Students whose documents were ranked lower did not receive any credit.",0,,False
A.3 eries and initial relevant documents,0,,False
"We used the titles of 31 topics selected from 1-200 from TREC 20092012 as queries. e preference was selecting queries with clear commercial intent, since they were more likely to stir up competition as is the case on the Web. at is, having a document ranked high ( rst) with respect to these queries should lead to increased (monetary) pro ts to the document's publisher on the Web. e selected queries focused mostly on topics related to products or services. Examples include ""used car parts"", ""cheap internet"" and ""gmat prep classes"". e queries were randomly assigned to students ensuring that two students will not compete against each other in more than two di erent matches; the assignments were not changed throughout the competition.",1,TREC,True
"As already noted, for each query we provided a single example of a relevant document. e goal was to provide the students with information regarding the underlying information need as the queries are very short. To produce these relevant documents, we",1,ad,True
rst used the TREC topic description as a query in a commercial search engine. We extracted from the highly ranked documents,1,TREC,True
472,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
% of keyword stuffed documents % of relevant documents,0,,False
12,0,,False
100,0,,False
3,0,,False
8,0,,False
75,0,,False
4,0,,False
4,0,,False
50,0,,False
5,0,,False
0 12345678 round,0,,False
25 12345678 round,0,,False
"Figure 3: e percentage of documents annotated as keyword stu ed (le ) and relevant (right) by at least 3, 4 or 5 annotators, averaged over queries per each of the eight competition rounds.",0,,False
candidate window passages of up to 150 terms. e passages were annotated for relevance by four annotators. We kept extracting passages for each query until a passage was judged relevant by at least three annotators. is passage then served as the initial relevant document example for all students competing for the query.,0,,False
A.4 Ranking model,0,,False
We next describe the ranking model used for all queries in each round of every match in the competition.,0,,False
"A.4.1 Learning-to-rank. We used a learning-to-rank (LTR) approach with 25 features to rank the documents. Most of the features (22) were all those used in Microso 's learning-to-rank datasets13 for the ""whole document"" except for the Boolean Model, Vector Space Model and LMIR.ABS features. As noted above, the documents in our competition are unstructured plain text. us, all the features are computed only for the entire document. Since documents in our competitive se ing are prone to manipulation, we used three additional features which were shown to be highly e ective for spam classi cation [21] and Web retrieval [4]: (i) the ratio between the number of stopwords and non-stopwords in a document, (ii) the percentage of stopwords in a stopword list that appear in the document, and (iii) the entropy of the term distribution in a document [4]. For the two stopword-based features, the list of stopwords was composed of the 100 most frequent alphanumeric terms in the ClueWeb09 Category B corpus [21].",1,LM,True
"e ClueWeb09 category B dataset with queries 1-200 was used to learn the LTR model. Speci cally, the model was applied upon the 1000 documents most highly ranked by using LMIR.DIR, i.e., the negative cross entropy between the unsmoothed and Dirichletsmoothed (with µ ,"" 1000) unigram language models induced from the query and documents, respectively14. We used LambdaMART [29] via the RankLib library15 to integrate the di erent features. e number of trees and leaves were selected from {100, 250, 500, 750, 1000} and {10, 25, 50}, respectively. NDCG@5 served for optimization when learning the model. In each round of the competition, we added the (unjudged) documents submi ed by students in all matches to the ClueWeb09 Category B corpus to""",1,ClueWeb,True
"13www.research.microso .com/en-us/projects/mslr 14We deliberately did not remove suspected spam documents from the initial document ranking, e.g., using Waterloo's spam classi er [7]. is practice allows learning a model using low quality (e.g., spam) documents. 15 www.lemurproject.org/ranklib.ph",0,,False
"have more updated values of corpus statistics, e.g., inverse document frequency (idf). Yet, we did not re-train the ranker. e Indri toolkit was used for indexing and retrieval16. We applied Krovetz stemming upon queries and documents and removed stopwords on the INQUERY list only from queries. e LMIR.JM feature was used with  ,"" 0.1; for BM25, we set k1 "", 1.2 and b , 0.75.",1,LM,True
"A.4.2 Results diversification. To encourage students to considerably change their documents rather than introduce minor modi cations to the initially provided relevant document, starting from the second round, they were advised to diversify their documents with respect to the relevant document. To further encourage diversi cation, we applied the MMR method [5] with respect to the initial relevant document dinit. Accordingly, the score assigned to document d with respect to query q is score(q, d) d,""ef rank(d, LT R) - (1 - )rank(d, dinit), where  "","" 0.5, rank(d, LT R) is the rank of d in a ranking of all the documents in a match induced by the LTR method and rank(d, dinit) is the rank of d in a ranking created based on the similarity with dinit; here, the rank of the lowest ranked document is 1. e similarity with dinit was computed using LMIR.DIR treating d as the query.""",1,ad,True
"A.4.3 Keyword stu ing. Keyword stu ng [16], speci cally of query terms, is one of the most applicable manipulation approaches the students could employ to promote their unstructured plain text documents in rankings. To avoid rewarding excessive keyword stu ng, and to encourage writing of high quality documents, each document was manually classi ed as keyword stu ed or not17.",0,,False
e annotation was performed via CrowdFlower18; each document was judged by ve annotators from English speaking countries19.,0,,False
"e inter-annotator agreement for keyword stu ng, computed using the free-marginal multi-rater kappa measure [24], is 0.88. A document classi ed as keyword stu ed by at least four annotators was rank-penalized: with probability 0.5 it was swapped with the next document in the ranking. If several consecutively ranked documents were keyword stu ed, then only the lowest ranked document was penalized.",0,,False
"In Figure 3 (le ) we present for each round the percentage of documents classi ed as keyword stu ed by at least three, four or",0,,False
ve annotators averaged over queries. We can see a mostly downward trend until the h round. In the h round we observe the lowest percentage of keyword stu ed documents. Starting from the,0,,False
"h round the percentage of keyword stu ed documents gradually increases. We hypothesis that in the rst half of the competition students' engagement gradually decreased. In the second half, as from the h round in which the rewards for having a document ranked high substantially increased, students started using manipulated texts even more so as to have their documents ranked high. In the h round, there might have been some confusion due to the introduction of a new reward mechanism.",1,ad,True
"16 www.lemurproject.org/indri 17A document was annotated as keyword stu ed if it contained excessive repetition of words which seemed unnatural or arti cially introduced. 18www.crowd ower.com 19Annotators were also instructed to classify documents as spam if they were hard to understand, non-cohesive, did not make any sense or were useless to anyone seeking information. Yet, none of the documents was classi ed as spam.",0,,False
473,0,,False
Session 4B: Retrieval Models and Ranking 2,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
22,0,,False
95,0,,False
1,0,,False
16,0,,False
80,0,,False
MAP@5 NDCG@5,1,MAP,True
2,0,,False
10,0,,False
65,0,,False
3,0,,False
4 12345678,0,,False
round,0,,False
50 12345678 round,0,,False
"Figure 4: e MAP@5 (le ) and NDCG@5 (right) performance of the ranking induced by the retrieval method in each round. Binary relevance judgments were induced for computing MAP@5 by considering a document relevant if its relevance grade was at least 1, 2 or 3.",1,MAP,True
A.5 Ranking e ectiveness,0,,False
"All documents in the collection were judged for relevance. Annotators were presented with both the title and description of each TREC topic, and were asked to classify a document as relevant if it satis ed the information need stated in the description. As was the case with keyword stu ng annotation, each document was judged by ve annotators from English speaking countries via CrowdFlower. e inter-annotator agreement rate, computed using the free-marginal multi-rater kappa measure [24], was 0.67. Four-scale graded relevance judgments were generated using the annotations as follows. A document judged relevant by less than three annotators was labeled as non-relevant (0). Documents judged relevant by at least three, four or ve annotators were labeled as marginally relevant (1), fairly relevant (2) and highly relevant (3), respectively.",1,TREC,True
"As noted above, to address the potential manipulation of documents by students, the retrieval method used in the competition (i) was based on a learning-to-rank approach with multiple features, (ii) incorporated highly e ective document-quality measures and (iii) penalized keyword stu ed documents. Figure 3 (right) presents the percentage of documents classi ed relevant by at least three, four or ve annotators per round averaged over queries. We see that, in general, the percentage of relevant documents decreased over the course of the competition. While many of the documents were judged relevant by at least three annotators, far fewer documents were judged relevant by at least four or ve annotators. is",1,ad,True
nding a ests to the negative e ects of SEO. In Figure 4 we present the MAP@5 and NDCG@5 e ectiveness,1,MAP,True
"of the document ranking induced by the retrieval method in each of the eight competition rounds. We see that the e ectiveness of the ranking has gradually decreased over rounds, which can be partially a ributed to the fact that fewer relevant documents were generated by students as seen in Figure 3. We also see that in the",1,ad,True
"rst two rounds the e ectiveness of the ranking was much higher than that in the rounds to follow. We found that in the rst two rounds students used the initially provided relevant documents without signi cantly changing them. A er the second round, in which the retrieval method was changed by applying diversi cation with respect to the given relevant document (see Section A.4.2), students started diversifying their documents by introducing noise, using non-relevant information and applying content manipulation.",0,,False
REFERENCES,0,,False
[1] R. Aumann and M. Maschler. 1995. Repeated Games with Incomplete Information. MIT Press.,0,,False
[2] Ran Ben-Basat and Elad Kravi. 2016. e ranking game. In Proceedings of the 19th International Workshop on Web and Databases. 7.,1,ad,True
"[3] Ran Ben-Basat, Moshe Tennenholtz, and Oren Kurland. 2015. e Probability Ranking Principle is Not Optimal in Adversarial Retrieval Se ings. In Proceedings of ICTIR. 51­60.",0,,False
"[4] Michael Bendersky, W. Bruce Cro , and Yanlei Diao. 2011. ality-biased ranking of web documents. In Proceedings of WSDM. 95­104.",0,,False
"[5] Jaime G. Carbonell and Jade Goldstein. 1998. e Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries. In Proceedings of SIGIR. 335­336.",1,ad,True
"[6] Carlos Castillo and Brian D. Davison. 2010. Adversarial Web Search. Foundations and Trends in Information Retrieval 4, 5 (2010), 377­486.",0,,False
"[7] Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke. 2011. E cient and e ective spam ltering and re-ranking for large web datasets. Informaltiom Retrieval Journal 14, 5 (2011), 441­465.",0,,False
"[8] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit Sanghai, and Deepak Verma. 2004. Adversarial Classi cation. In Proceedings of KDD. 99­108.",0,,False
[9] Fernando Diaz. 2005. Regularizing Ad Hoc Retrieval Scores. In Proceedings of CIKM. 672­679.,0,,False
[10] Ran El-Yaniv and Mordechai Nisenson. 2010. On the Foundations of Adversarial Single-Class Classi cation. CoRR (2010).,0,,False
"[11] K r Eliaz and Ran Spiegler. 2011. A simple model of search engine pricing. e Economic Journal 121, 556 (2011), F329­F339.",0,,False
"[12] K r Eliaz and Ran Spiegler. 2016. Search design and broad matching. American Economic Review 106, 3 (2016), 563­586.",1,ad,True
[13] Jonathan L. Elsas and Susan T. Dumais. 2010. Leveraging temporal dynamics of document content in relevance ranking. In Proceedings of WSDM. 1­10.,0,,False
"[14] Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A formal study of information retrieval heuristics. In Proceedings of SIGIR. 49­56.",0,,False
"[15] Norbert Fuhr. 2008. A probability ranking principle for interactive information retrieval. Information Retrieval 11, 3 (2008), 251­265.",0,,False
"[16] Zolta´n Gyo¨ngyi and Hector Garcia-Molina. 2005. Web Spam Taxonomy. In Proceedings of AIRWeb 2005, First International Workshop on Adversarial Information Retrieval on the Web. 39­47.",0,,False
[17] Nathanael Hya l and Craig Boutilier. 2004. Regret Minimizing Equilibria and Mechanisms for Games with Strict Type Uncertainty. In Proceedings of UAI. 268­277.,0,,False
"[18] N. Jardine and C. J. van Rijsbergen. 1971. e use of hierarchic clustering in information retrieval. Information Storage and Retrieval 7, 5 (1971), 217­240.",0,,False
"[19] John D. La erty and Chengxiang Zhai. 2001. Document language models, query models, and risk minimization for information retrieval. In Proceedings of SIGIR. 111­119.",0,,False
"[20] Tie-Yan Liu. 2011. Learning to Rank for Information Retrieval. Springer. I­XVII, 1­285 pages.",0,,False
"[21] Alexandros Ntoulas, Marc Najork, Mark Manasse, and Dennis Fe erly. 2006. Detecting spam web pages through content analysis. In Proceedings of WWW. 83­92.",0,,False
"[22] Fabian Pedregosa, Gae¨l Varoquaux, Alexandre Gramfort, Vincent Michel,",0,,False
"Bertrand irion, Olivier Grisel, Mathieu Blondel, Peter Pre enhofer, Ron",0,,False
"Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau,",0,,False
"Ma hieu Brucher, Ma hieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research 12 (2011), 2825­2830.",0,,False
[23] Kira Radinsky and Paul N. Benne . 2013. Predicting content change on the web. In Proceedings of WSDM. 415­424.,1,ad,True
"[24] Justus J. Randolph. 2016. Online Kappa Calculator (2008). Retrieved February 6,",0,,False
h p://justus.randolph.name/kappa. (2016). [25] Stephen E. Robertson. 1977. e Probability Ranking Principle in IR. Journal,0,,False
"of Documentation (1977), 294­304. Reprinted in K. Sparck Jones and P. Wille (eds), Readings in Information Retrieval, pp. 281­286, 1997. [26] Ae´cio S. R. Santos, Bruno Pasini, and Juliana Freire. 2016. A First Study on Temporal Dynamics of Topics on the Web. In Proceedings of WWW. 849­854. [27] Marc Sloan and Jun Wang. 2012. Dynamical information retrieval modelling: a portfolio-armed bandit machine approach. In Proceedings WWW. 603­604. [28] Hong Wang, Wei Xing, Kaiser Asif, and Brian D. Ziebart. 2015. Adversarial Prediction Games for Multivariate Losses. In Proceedings of NIPS. 2728­2736. [29] Qiang Wu, Christopher J. C. Burges, Krysta Marie Svore, and Jianfeng Gao. 2010. Adapting boosting for information retrieval measures. Information Retrieval 13, 3 (2010), 254­270. [30] Grace Hui Yang, Marc Sloan, and Jun Wang. 2016. Dynamic Information Retrieval Modeling. Morgan & Claypool Publishers. [31] Yinan Zhang and Chengxiang Zhai. 2015. Information Retrieval as Card Playing: A Formal Model for Optimizing Interactive Retrieval Interface. In Proceedings of SIGIR. 685­694.",1,ad,True
474,0,,False
,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Intent-Aware Semantic ery Annotation,0,,False
Rafael Glater,0,,False
"CS Dept, UFMG Belo Horizonte, MG, Brazil rafaelglater@dcc.ufmg.br",0,,False
Rodrygo L. T. Santos,0,,False
"CS Dept, UFMG Belo Horizonte, MG, Brazil",0,,False
rodrygo@dcc.ufmg.br,0,,False
Nivio Ziviani,0,,False
"CS Dept, UFMG & Kunumi Belo Horizonte, MG, Brazil",0,,False
nivio@dcc.ufmg.br,0,,False
ABSTRACT,0,,False
"ery understanding is a challenging task primarily due to the inherent ambiguity of natural language. A common strategy for improving the understanding of natural language queries is to annotate them with semantic information mined from a knowledge base. Nevertheless, queries with di erent intents may arguably bene t from specialized annotation strategies. For instance, some queries could be e ectively annotated with a single entity or an entity a ribute, others could be be er represented by a list of entities of a single type or by entities of multiple distinct types, and others may be simply ambiguous. In this paper, we propose a framework for learning semantic query annotations suitable to the target intent of each individual query. orough experiments on a publicly available benchmark show that our proposed approach can signi cantly improve state-of-the-art intent-agnostic approaches based on Markov random elds and learning to rank. Our results further demonstrate the consistent e ectiveness of our approach for queries of various target intents, lengths, and di culty levels, as well as its robustness to noise in intent detection.",0,,False
CCS CONCEPTS,0,,False
·Information systems Retrieval e ectiveness;,0,,False
KEYWORDS,0,,False
Semantic query annotation; Learning to rank; Intent-aware;,0,,False
"ACM Reference format: Rafael Glater, Rodrygo L. T. Santos, and Nivio Ziviani. 2017. Intent-Aware Semantic ery Annotation. In Proceedings of SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080825",0,,False
1 INTRODUCTION,1,DUC,True
"A user's search query has traditionally been treated a short, underspeci ed representation of his or her information need [22]. Despite the trend towards verbosity brought by the popularization of voice queries in modern mobile search and personal assistant interfaces [20], query understanding remains a challenging yet crucial task for the success of search systems. One particularly e ective strategy for improving the understanding of a query is to annotate it with semantic information mined from a knowledge",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080825",1,ad,True
"base, such as DBPedia.1 In particular, previous analysis has shown that over 70% of all queries contain a semantic resource (a named entity, an entity type, relation, or a ribute), whereas almost 60% have a semantic resource as their primary target [32].",0,,False
"State-of-the-art semantic query annotation approaches leverage features extracted from the descriptive content of candidate semantic resources (e.g., the various textual elds in the description of an entity [28, 44]) or their structural properties (e.g., related semantic resources [39]) in a knowledge base. In common, these approaches treat every query uniformly, regardless of its target intent.2 In contrast, we hypothesize that queries with di erent intents may bene t from specialized annotation strategies. For instance, some queries could be e ectively annotated with a single entity (e.g., ""us president"") or an entity a ribute (e.g., ""us president salary""). Other queries could be be er represented by a list of entities of a single type (e.g., ""us presidents"") or of mixed types (e.g., ""us foreign affairs""). Finally, some queries may be simply ambiguous and demand annotations suitable for disambiguation (e.g., ""us"").",0,,False
"In this paper, we propose a framework for learning semantic annotations suitable to the target intent of each individual query. Our framework comprises three main components: (i) intent-speci c learning to rank, aimed to produce ranking models optimized for di erent intents; (ii) query intent classi cation, aimed to estimate the probability of each query conveying each possible intent; and (iii) intent-aware ranking adaptation, aimed to promote the most relevant annotations given the detected intents. To demonstrate the applicability of our framework, we experiment with a state-of-theart learning to rank algorithm for intent-speci c learning, multiple classi cation approaches for intent classi cation, and two adaptive strategies for annotation ranking. orough experiments using a publicly available semantic annotation test collection comprising queries with di erent intents show that our proposed framework is e ective and signi cantly improves state-of-the-art intent-agnostic approaches from the literature. Moreover, a breakdown analysis further reveals the consistency of the observed gains for queries of various target intents, lengths, and di culty levels, as well as the robustness of the framework to noise in intent detection.",1,ad,True
"In summary, our main contributions are three-fold:",0,,False
· An intent-aware framework for learning semantic query annotations from structured knowledge bases.,0,,False
· An analysis of the speci city of several content and structural features for di erent query intents.,0,,False
· A thorough validation of the proposed framework in terms of annotation e ectiveness and robustness.,0,,False
"1h p://wiki.dbpedia.org/ 2While the word ""intent"" has been used as a synonym of ""information need"" in some contexts, here we adopt the more traditional de nition of intent as the type (or class) of an information need, such as informational or navigational [9].",1,wiki,True
485,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"In the remainder of this paper, Section 2 discusses related work on semantic query annotation and intent-aware information retrieval. Section 3 describes the various components of our proposed framework and their instantiation. Sections 4 and 5 describe the setup and the results of our empirical evaluation. Finally, Section 6 provides our conclusions and directions for future research.",0,,False
2 RELATED WORK,0,,False
"In this section, we provide an overview of related work on semantic query annotation using knowledge bases. In addition, we discuss related a empts to exploit query intents in di erent search tasks.",1,ad,True
2.1 Semantic ery Annotation,0,,False
"Semantic search approaches [4] have been extensively researched in recent years, motivated by a series of related workshops and evaluation campaigns [1, 2, 15]. While some research has been devoted to semantic search on the open Web [2, 10, 37], particularly relevant to this paper are approaches focused on ranking semantic resources (e.g., named entities) mined from a structured domain, such as a knowledge base. e top ranked resources can be used directly to enrich a search engine's results page with structured semantic information [5] or indirectly to annotate the user's query for further processing for improved search quality.",0,,False
"Search in knowledge bases is typically performed using structured query languages such as SPARQL.3 However, producing structured queries requires some expertise from the user, which limits the applicability of this approach in a broader scenario. To support unstructured querying, most previous semantic search approaches adapt traditional IR techniques to nd, in the knowledge base, resources that match the user's query. For instance, some related works have used standard bag-of-words models, like BM25 [3, 31, 39] and language models (LM) [16, 17, 21, 27, 43]. Extending traditional bag-of-words models, multi- elded approaches have been proposed to appropriately weight information present in di erent elds describing a semantic resource. For instance, approaches based on BM25F [6, 7, 12, 18, 31, 39] permit the combination of the BM25 scores of di erent elds into the nal retrieval score. Multi- elded approaches based on a mixture of language models have also been proposed [11, 29], which linearly combine query likelihood estimates obtained from multiple elds.",1,ad,True
"Also contrasting with bag-of-words models, recent approaches have exploited dependencies among query term occurrences in the descriptive content of a semantic resource. Building upon the framework of Markov random elds (MRF) [26], these approaches construct a graph of dependencies among the query terms, which is used to estimate the relevance of each retrieved semantic resource. In particular, Zhiltsov et al. [44] introduced a multi- elded extension of MRF, called FSDM, which estimates the weight of each eld with respect to three types of query concept: unigram, ordered bigram, and unordered bigram. FSDM was later extended by Nikolaev et al. [28], who proposed to estimate eld weights with respect to individual query concepts. To cope with the explosive number of concepts (i.e., every possible unigram, ordered, and unordered bigram), they instead learn eld weights with respect to a xed set of concept features (e.g., the probability of occurrence of the concept",1,ad,True
3h ps://www.w3.org/TR/rdf-sparql-query/,0,,False
"in a eld). In contrast to both of these approaches, we propose to learn the appropriateness of intent-speci c feature-based ranking models for each individual query, by automatically predicting the target intent of this query. In Section 5, we compare our approach to FSDM as a representative of the current state-of-the-art.",0,,False
"In addition to exploiting the descriptive content of semantic resources, other researchers have adopted a hybrid approach [11, 17, 21, 34, 39], leveraging structural properties of the knowledge base. In these approaches, an initial ranking of semantic resources is either re-ranked or expanded using the knowledge base structure to nd related resources, which can be done through structured graph traversals [39] or random walks [34]. For instance, Tonon et al. [39] exploited entities initially retrieved using BM25 as seeds in the graph from which related entities could be reached. Bron et al. [11] proposed a method that makes a linear combination of the scores of a content-based approach using language models and a structure-based approach, which captures statistics from candidate entities represented according to their relations with other entities, expressed in RDF triples. Relatedly, Elbassouni et al. [17] proposed a language modeling approach to rank the results of exact, relaxed, and keyword-augmented graph-pa ern queries over RDF triples into multiple subgraphs. e Kullback-Leibler divergence between the query language model and the language models induced by the resulting subgraphs was then used to produce the nal ranking. While our main focus is on learning strategies rather than on speci c features, to demonstrate the exibility of our proposed framework, we exploit multiple structural properties of each semantic resource as additional features. In particular, these features are used for both detecting the intent of a query as well as for ranking semantic resources in response to this query.",1,ad,True
2.2 Exploiting ery Intents,0,,False
"e intent underlying a user's search query has been subject of intense research in the context of web search. Broder [9] proposed a well-known intent taxonomy, classifying web search queries into informational, navigational and transactional. Rose and Levinson [35] later extended this taxonomy to consider more ne-grained classes. In the context of semantic search, Pound et al. [32] categorized queries into four major intents: entity queries, which target a single entity; type queries, which target multiple entities of a single type; a ribute queries, which target values of a particular entity a ribute; and relation queries, which aim to nd how two or more entities or types are related. Entity queries and type queries accounted for more than 50% of a query log sampled in their study, whereas a ribute and relation queries accounted for just over 5%. Other works focused on more speci c intents, such as a question intent [40], which targets answers to the question expressed in the query. In our experiments, we use an intent taxonomy comprising the three major classes described in these studies, namely, entity, type, and question queries, as well as an additional class including less represented intents, such as a ribute and relation queries.",1,ad,True
"In addition to detecting query intents, several approaches have a empted to adapt the ranking produced for a query in light of some identi ed query property, such as its intent. For instance, Yom-Tov et al. [42] proposed to adaptively expand a query depending on its predicted di culty. Kang and Kim [23] proposed to apply",1,ad,True
486,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"di erent hand-cra ed ranking models for queries with a predicted informational, navigational, or transactional intent. However, such a hard intent classi cation may eventually harm the e ectiveness of an adaptive approach, when queries of di erent intents bene t from a single ranking model [14]. To mitigate this e ect, instancebased classi cation approaches have been used to identify similar queries (as opposed to queries with the same predicted intent) for training a ranking model. For example, Geng et al. [19] resorted to nearest neighbor classi cation for building training sets for a given test query. Relatedly, Peng et al. [30] proposed to estimate the bene t of multiple candidate ranking models for a given query by examining training queries that are a ected by these models in a similar manner. In the context of search result diversi cation, Santos et al. proposed adaptive approaches for estimating the coverage of di erent query aspects given their predicted intent [38] as well as for estimating when to diversify given the predicted ambiguity of the query [36]. Our proposed approach resembles these adaptive ranking approaches as we also resort to query intent classi cation as a trigger for ranking adaptation. Nonetheless, to the best of our knowledge, our approach is the rst a empt to produce adaptive learning to rank models for a semantic search task.",1,ad,True
3 INTENT-AWARE RANKING ADAPTATION,1,INTENT,True
FOR SEMANTIC QUERY ANNOTATION,0,,False
"Annotating queries with semantic information is an important step towards an improved query understanding [1]. Given a query, our goal is to automatically annotate it with semantic resources mined from a knowledge base, including named entities, a ributes, relations, etc. For instance, the query ""us president"" could be annotated with arguably relevant semantic resources including ""Donald Trump"", ""Federal Government"", ""White House."" In this paper, we hypothesize that the relevance of a semantic resource given a query depends on the intent underlying this query. For the previous example, knowing that the query ""us president"" targets information around a single entity could promote alternative semantic resources including ""Inauguration"", ""First 100 days"", and ""Controversies"".",1,Gov,True
"In this section, we propose an intent-aware framework for learning to rank semantic query annotations. In particular, we posit that the probability P(r |q) that a given semantic resource r satis es the user's query q should be estimated in light of the possible intents i  I underlying this query. Formally, we de ne:",0,,False
"P(r |q) ,"" P(i |q) P(r |q, i),""",0,,False
(1),0,,False
i I,0,,False
"where P(i |q) is the probability that query q conveys an intent i, with i I P(i |q) ,"" 1, and P(r |q, i) is the probability of observing semantic resource r given the query and this particular intent.""",0,,False
"In Figure 1, we describe the three core components of our framework. In particular, the query intent classi cation and the intentspeci c learning to rank components rely on supervised learning approaches to estimate P(i |q) and P(r |q, i), respectively, for each intent i  I. In turn, the intent-aware ranking adaptation component implements two alternative policies to suit the nal ranking to the detected intents of each individual query.",1,ad,True
i1,0,,False
Li1,0,,False
i1 i2 i3 i4,0,,False
i2,0,,False
Li2,0,,False
query intent classification,0,,False
C,0,,False
q,0,,False
i3,0,,False
Li3,0,,False
i4,0,,False
Li4,0,,False
A,0,,False
intent-specific,0,,False
intent-aware,0,,False
learning to rank ranking adaptation,1,ad,True
Figure 1: Intent-aware semantic query annotation. Each intent-speci c ranking model Li is learned on a query set comprising only queries with intent i. e query intent classi cation model C is learned on a set comprising queries of,0,,False
various intents. e intent-aware ranking adaptation strategy A uses the query intent classi cation outcome to decide,1,ad,True
on how to leverage the intent-speci c ranking models.,0,,False
3.1 ery Intent Classi cation,0,,False
"e rst component of our framework is responsible for predicting the possible intents underlying a query [8]. For this task, we adopt a standard multi-class classi cation approach. In particular, we aim to learn a query classi cation model C : X  Y mapping the input space X into the output space Y. Our input space X comprises m learning instances {xj }mj,""1, where xj "","" (qj ) is a feature vector representation of query qj as produced by a feature extractor . In turn, our output space Y comprises m labels { j }mj"",""1, where j corresponds to one of the target intents i  I assigned to query qj by a human annotator. To learn an e ective classi er C, we experiment with several classi cation algorithms in Section 5.2.""",1,ad,True
"Table 1 presents the features we use to represent a query for intent classi cation. We use a total of 31 simple features, including both lexical as well as semantic ones. Lexical features like number of query terms and mean query term size can help detect, for example, natural language queries, which are usually longer than others. In addition, part-of-speech tags can help identify question queries, indicating the presence of wh-pronouns (e.g., what, where, why, when). Lastly, semantic features include the number of categories and number of ontology classes returned when using the query to search a knowledge base. Our intuition is that queries seeking for a speci c entity will probably return fewer categories or ontology classes than queries seeking for a list of entities. For instance, the query ""ei el"" returns only 5 categories, while the query ""list of lms from the surrealist category"" returns more than 103,000.",1,ad,True
3.2 Intent-Speci c Learning to Rank,0,,False
"e second component of our framework aims to produce multiple ranking models, each one optimized for a speci c query intent i  I. To this end, we resort to learning to rank [24]. Analogously to our query intent classi cation models in Section 3.1, our goal is to learn an intent-speci c ranking model Li : V  W mapping the input space V into the output space W. Our input space",0,,False
487,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: ery features for intent classi cation.,0,,False
# Feature,0,,False
Qty,0,,False
1 No. of query terms,0,,False
1,0,,False
2 Avg. query term size (in characters),0,,False
1,0,,False
3 No. of matched categories in DBPedia,0,,False
1,0,,False
4 No. of matched ontology classes in DBPedia,0,,False
1,0,,False
5 No. of POS tags of di erent types,0,,False
27,0,,False
TOTAL,0,,False
31,0,,False
"includes n learning instances {Vj }nj,""1, where Vj "","" (qj , Rj ) is a feature matrix representation (produced by some feature extractor ) of a sample of semantic resources r  Rj retrieved for query qj annotated with intent i. In our experiments, Rj is produced using BM25 [33], although any unsupervised ranking technique could have been used for this purpose. Our output space W comprises n label vectors {Wj }nj"",""1, where Wj provides relevance labels for each semantic resource r  Rj . To learn an e ective ranking model Li for each intent i  I, we use LambdaMART [41], which represents the current state-of-the-art in learning to rank [13].""",0,,False
"Table 2 lists all 216 features used to represent each semantic resource r  Rj . Features #1-#6 are content-based features commonly used in the learning to rank literature [24], such as number of tokens, BM25, coordination level matching (CLM), TF, and IDF scores.",1,LM,True
"ese are computed in a total of 8 descriptive elds of r , such as name, a ributes, categories (see Section 4.1 for a full description). Since TF and IDF are de ned on a term-level, query-level scores are computed using multiple summary statistics (sum, min, max, avg, var). Finally, CLM, TF, IDF, and TF-IDF are computed for both unigrams and bigrams. Next, features #7-#14 are semantic features derived from a knowledge base. For instance, feature #7 indicates whether r is an entity directly mentioned in the query, while feature #8 considers the number of direct connections between r and all entities mentioned in the query. As an example of the la er feature, in the query ""songs composed by michael jackson"", the candidate resource "" riller"" will be directly related to the entity ""Michael Jackson"" (present in the query). For both features, we use DBPedia Spotlight4 for entity recognition in queries. Features #9-#14 are query-independent features quantifying the connectivity of each candidate resource r with respect to other resources in the knowledge base (e.g., entities, categories, ontology classes).",1,LM,True
"To keep our approach general, instead of handpicking features more likely to be useful for a particular intent, we use the same 216 available features when learning every intent-speci c model Li . To ensure that the learned model Li is indeed optimized to its target intent i, intent-speci c learning is achieved by using one training query set per intent, as illustrated in Figure 1.",1,ad,True
3.3 Intent-Aware Ranking Adaptation,0,,False
"Sections 3.1 and 3.2 described supervised approaches for learning a query intent classi cation model C as well as multiple intentspeci c ranking models Li for all i  I. Importantly, all of these models are learned o ine. When an unseen query q is submi ed online, we must be able to return a ranking of semantic resources",0,,False
4h p://spotlight.dbpedia.org/,0,,False
Table 2: Semantic resource features for learning to rank. Features marked as `Bi' are computed also for bigrams.,0,,False
# Feature,0,,False
"1 No. of tokens (per- eld) 2 BM25 (per- eld) 3 CLM (per- eld) 4 TF (per- eld sum, min, max, avg, var) 5 IDF (per- eld sum) 6 TF-IDF (per- eld sum, min, max, avg, var) 7 Matching entity 8 No. of direct relations with query entities 9 No. of matched relations with query terms 10 No. of inlinks 11 No. of outlinks 12 No. of linked ontology classes 13 No. of linked categories 14 No. of linked entities",1,LM,True
TOTAL,0,,False
Bi Qty,0,,False
8 8  16  80  16  80 1 1 1 1 1 1 1 1,0,,False
216,0,,False
"well suited to the target intent of q. Because we tackle query intent classi cation as a multi-class problem, we can actually estimate the probability P(i |q) of di erent intents i  I given the query q.",0,,False
"To exploit this possibility, we devise two strategies to adapt the ranking produced for a query q to the target intent(s) of this query. Our rst strategy, called intent-aware switching, assigns each query a single intent, namely, the most likely one as predicted by the intent classi cation model C. For instance, for a target set of intents I ,"" {i1, i2, i3} of which i1 is predicted as the most likely for q, we could instantiate Equation (1) with P(i1|q) "","" 1, P(i2|q) "","" 0, and P(i3|q) "","" 0. As a result, only P(r |q, i1) (estimated via ranking model L1) would have an impact on the nal ranking, such that:""",1,ad,True
"P(r |q) ,"" P(r |q, i1).""",0,,False
"Some queries may have no clear winning intent. Other queries may prove simply di cult to classify correctly. To cope with uncertainty in intent classi cation, we propose a second ranking adaptation strategy, called intent-aware mixing. In this strategy, we use the full probability distribution over intents predicted by the classi cation model C to produce the nal ranking for q. In the aforementioned example, suppose the predicted intent distribution is P(i1|q) ,"" 0.7, P(i2|q) "","" 0.2, and P(i3|q) "","" 0.1. Leveraging this distribution directly in Equation (1), we have a mixture of intent-speci c ranking models contributing to the nal ranking:""",1,ad,True
"P(r |q) ,"" 0.7 × P(r |q, i1)""",0,,False
"+ 0.2 × P(r |q, i2)",0,,False
"+ 0.1 × P(r |q, i3).",0,,False
"To assess the e ectiveness of our proposed intent-aware ranking adaptation strategies for semantic query annotation, in the next section, we compare these strategies to each other as well as to state-of-the-art intent-agnostic approaches from the literature.",1,ad,True
4 EXPERIMENTAL SETUP,0,,False
"In this section, we detail the experimental setup that supports the evaluation of our proposed intent-aware semantic query annotation",0,,False
488,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"approach introduced in Section 3. In particular, our experiments aim to answer the following research questions:",0,,False
Q1. Do di erent intents bene t from di erent ranking models? Q2. How accurately can we predict the intent of each query? Q3. How e ective is our semantic query annotation approach? Q4. What queries are improved the most and the least?,0,,False
"In the following, we describe the knowledge base, queries, relevance judgments, and intent taxonomy used in our experiments. We also describe the baselines used for comparison and the procedure undertaken to train and test them as well as our own models.",0,,False
4.1 Knowledge Base,0,,False
"e knowledge base used in our experiments is the English portion of DBPedia 3.7,5 which comprises RDF triples extracted from Wikipedia dumps generated in late July 2011. is version of DBPedia contains information on more than 3.6 million entities organized in over 170,000 categories and 320 ontology classes in a 6-level deep hierarchy. We indexed this knowledge base using Elasticsearch 1.7.56 for textual content and Titan 0.5.47 for the underlying graph structure. RDF triples were parsed to created a elded content representation. Following Zhiltsov et al. [44], we indexed the Names, A ributes, Categories, Similar entity names and Related entity names",1,Wiki,True
"elds. In addition, we included three other elds: Ontology classes, URL and a special eld All, concatenating the available content from all elds. Indexed terms were lower-cased, stemmed using Krovetz stemmer and standard stopwords were removed.",1,ad,True
"4.2 eries, Relevance Judgments, and Intents",0,,False
"We use a publicly available benchmark8 built on top of DBPedia 3.7, which comprises a total of 485 queries from past semantic search evaluation campaigns [3]. In total, there are 13,090 positive relevance judgments available. While some of these include graded labels, for a homogeneous treatment of all queries, we consider relevance as binary. e benchmark covers a wide variety of query intents, including entity, type, relation and a ribute queries, as well as queries with a question intent. Following past research [3, 28, 44], we organize these queries into four intent-speci c query sets, the salient statistics of which are described in Table 3:",1,ad,True
"· E: entity queries (e.g., ""orlando orida""); · T : type queries (e.g., ""continents in the world""); · Q: question queries (e.g., ""who created wikipedia?""); · O: queries with other intents, including less represented",1,wiki,True
"ones, such as relation queries and a ribute queries.",0,,False
4.3 Retrieval Baselines,0,,False
"We compare our approach to multiple intent-agnostic baselines from the literature. As a vanilla ad-hoc search baseline, we consider BM25 with standard parameter se ings (k1 ,"" 1.2, b "","" 0.8). To assess the e ectiveness of our intent-aware ranking adaptation strategies introduced in Section 3.3, we further contrast them to two intent-agnostic strategies, which consistently apply a single ranking model for all queries, regardless of their target intent. As""",1,ad-hoc,True
5h p://wiki.dbpedia.org/data-set-37 6h ps://www.elastic.co/products/elasticsearch 7h p://titan.thinkaurelius.com 8h p://bit.ly/dbpedia-entity,1,wiki,True
Table 3: Statistics of the intent-speci c query sets used in our evaluation. Length and qrels denote per-query averages of query length and positive judgements in each set.,0,,False
Set Campaign [3],0,,False
"E SemSearch ES T INEX-XER, SemSearch LS,",1,INEX,True
TREC Entity Q QALD-2 O INEX-LD,1,TREC,True
TOTAL,0,,False
eries,0,,False
130 115,0,,False
Length,0,,False
2.7 5.8,0,,False
Qrels,0,,False
8.7 18.4,0,,False
140,0,,False
7.9,0,,False
41.5,0,,False
100,0,,False
4.8,0,,False
37.6,0,,False
485,0,,False
5.3 26.55,0,,False
"illustrated in Table 4, the xed strategy applies a model Li learned on one intent-speci c query set, whereas the oblivious strategy applies a model LR learned on a set of random queries. For a fair comparison, both of these baseline strategies as well as our own intent-aware switching and mixing strategies use the same learning algorithm (LambdaMART) and ranking features (all 216 features in Table 2). Lastly, we further contrast our approach to FSDM [44] (see Section 2.1) as a representative of the current state-of-the-art.",0,,False
4.4 Training and Test Procedure,0,,False
"For a fair comparison between our intent-aware semantic query annotation approach and the intent-agnostic baselines described in Section 4.3, we randomly downsample all query sets in Table 3 until they reach 100 queries each (i.e., the number of queries in the smallest query set, namely, O). is procedure ensures the learning process is not biased towards any particular intent. To learn an intent-speci c model Li for each intent i  I ,"" {E,T , Q, O }, we perform a 5-fold cross validation in the corresponding query set from Table 3. For the oblivious strategy, the intent-agnostic model LR is also learned via 5-fold cross validation on a set of 100 queries sampled uniformly at random from the four intent-speci c query sets a er downsampling. is multi-intent query set is also used to tune the parameters of FSDM [44] for di erent concepts (unigrams, ordered, an unordered bigrams) and each of the elds listed in Section 4.1. In each cross-validation round, we use three partitions (60 queries) for training, one partition (20 queries) for validation, and one partition (20 queries) for testing.""",0,,False
"Learning to rank is performed using the LambdaMART implementation in RankLib 2.7,9 optimizing for normalized discounted cumulative gain at the top 100 results (nDCG@100). LambdaMART is deployed with default hyperparameter se ings,10 with 1,000 trees with 10 leaves each, minimum leaf support 1, unlimited threshold candidates for tree spli ing, learning rate 0.1, and early stopping a er 100 non-improving iterations. All results are reported as averages of all test queries across the ve cross-validation rounds. In particular, we report nDCG@10, precision at 10 (P@10), and mean average precision (MAP). All evaluation metrics are calculated on the top 100 results returned by each approach. To check for statistically signi cant di erences among them, we use a two-tailed paired",1,MAP,True
9h ps://sourceforge.net/p/lemur/wiki/RankLib%20How%20to%20use/ 10Hyperparameter tuning on validation data showed no signi cant improvements in our preliminary tests.,1,wiki,True
489,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: Example application of intent-agnostic (baseline) and intent-aware ranking adaptation strategies.,1,ad,True
intent-agnostic,0,,False
intent-aware,0,,False
i xed-E xed-T xed-Q xed-O oblivious switching mixing,0,,False
E LE,0,,False
LT,0,,False
LQ,0,,False
LO,0,,False
LR,0,,False
T LE,0,,False
LT,0,,False
LQ,0,,False
LO,0,,False
LR,0,,False
Q LE,0,,False
LT,0,,False
LQ,0,,False
LO,0,,False
LR,0,,False
O LE,0,,False
LT,0,,False
LQ,0,,False
LO,0,,False
LR,0,,False
LE,0,,False
i wi Li,0,,False
LT,0,,False
i wi Li,0,,False
LQ,0,,False
i wi Li,0,,False
LO,0,,False
i wi Li,0,,False
LO 1.0 0.42 0.7 0.38,0,,False
0.96,0,,False
0.88,0,,False
LQ 0.42 1.0 0.46 0.44,0,,False
0.80,0,,False
0.72,0,,False
LE 0.7 0.46 1.0 0.39,0,,False
0.64,0,,False
0.56,0,,False
LT 0.38 0.44 0.39 1.0,0,,False
0.48,0,,False
0.40,0,,False
LO,0,,False
LQ,0,,False
LE,0,,False
LT,0,,False
Figure 2: Spearman's correlation coe cient for feature importance across pairs of intent-speci c ranking models.,0,,False
"t-test and write ( ) and ( ) to denote signi cant increases (decreases) at the 0.05 and 0.01 levels, respectively. A further symbol  is used to denote no signi cant di erence.",0,,False
5 EXPERIMENTAL EVALUATION,0,,False
"In this section, we empirically evaluate our approach in order to answer the four research questions stated in Section 4. In the following, we address each of these questions in turn.",1,ad,True
5.1 Intent Speci city,0,,False
"e core hypothesis of our proposal is that di erent queries may bene t from a ranking model optimized to their intent. To verify this hypothesis, we address Q1, by assessing the speci city of ranking models optimized to the four intents described in Table 3. To this end, Figure 2 correlates the importance assigned to all 216 features by each intent-speci c ranking model Li , for i  I ,"" {E,T , Q, O }. Feature importance is quanti ed using the least square improvement criterion proposed by Lucchese et al. [25] for gradient boosted regression tree learners, such as LambdaMART. From Figure 2, we observe a generally low correlation ( < 0.5) between models, except for the LE and LO models, with   0.7.""",1,ad,True
"Table 5 lists the ve most important features for each intentspeci c model. e entity-oriented LE model gives importance to features related to the occurrence of bigrams in the name and similar entities elds. For instance, the query ""martin luther king"" expects semantic resources named ""Martin Luther King III"" and ""Martin Luther King High School."" While the type-oriented LT model considers a variety of distinct features, two features related to the categories eld are present in the top 5, which are useful for queries like ""state capitals of the united states of america."" e question-oriented LQ model gives importance to features describing the relation between entities and ontology classes, derived from",0,,False
"both content elds as well as the graph structure underlying the knowledge base. ese can help identify relevant resources linked to an entity in the query through quali ed relations, as in the query ""who was the successor of john f. kennedy?"" Lastly, the LO model, which is optimized on a set comprising queries of various intents, strongly favors content-based features, which are arguably e ective for broad queries such as ""einstein relativity theory."" Recalling question Q1, these results provide a strong indication of the speci city of di erent models to queries of di erent intents.",1,ad,True
Table 5: Top 5 features per ranking model.,0,,False
# Feature,0,,False
1 TF-IDF sum of bigrams in similar entities 2 Matching entity LE 3 TF sum of bigrams in similar entities 4 TF avg of bigrams in similar entities 5 TF-IDF max of bigrams in similar entities,0,,False
1 CLM in categories 2 CLM in all content LT 3 No. of inlinks 4 No. of tokens in similar entities 5 TF-IDF sum of bigrams in categories,1,LM,True
1 BM25 in ontology classes 2 No. of matched relations with query terms LQ 3 No. of direct relations with query entities 4 No. of inlinks 5 TF-IDF max of unigrams in ontology classes,0,,False
1 TF sum of bigrams in name 2 BM25 in name LO 3 TF-IDF max of unigrams in categories 4 TF-IDF max of bigrams in name 5 TF-IDF var of bigrams in all content,0,,False
5.2 Intent Classi cation Accuracy,0,,False
"e results in the previous experiment suggest that exploiting the speci city of di erent query intents may result in more e ective ranking models. Before investigating whether this is indeed the case, in this section, we address Q2, with the aim of establishing what level of query intent detection accuracy can be a ained in practice. To this end, we experiment with a range of traditional classi cation algorithms implemented in Scikit-learn 0.17.1,11 optimized via 5-fold cross validation using the same partitions leveraged for learning to rank, as detailed in Section 4.4. Table 6 reports",1,ad,True
11h p://scikit-learn.org/,0,,False
490,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
nDCG@100,0,,False
-MART (oblivious),0,,False
-MART (switching),0,,False
0.40 0.38 0.36 0.34 0.32 0.30 0.28,0,,False
0,0,,False
20,0,,False
40,0,,False
60,0,,False
80,0,,False
100,0,,False
% Noise amount,0,,False
Figure 3: Semantic query annotation robustness for simulated intent classi ers of a range of accuracy levels.,0,,False
"intent classi cation accuracy averaged across test queries in all cross-validation rounds. As shown in the table, the most accurate intent classi er is learned via stochastic gradient descent with a log loss, which performs an incremental logistic regression. As a result, we will use this classi er in the remainder of our experiments. Other e ective classi cation algorithms include random forest and bagging, while AdaBoost has the lowest accuracy.",1,ad,True
Table 6: ery intent classi cation accuracy.,0,,False
Algorithm,0,,False
AdaBoost Support Vector Machines Gradient Boosting Bagging Random Forest Logistic Regression,1,ad,True
Accuracy,0,,False
67.00% 74.00% 75.75% 76.00% 76.50% 77.00%,0,,False
"e top performing classi er in Table 6 still leaves room for further improvement in intent classi cation accuracy. An interesting question here is whether this level of accuracy is enough for an e ective deployment of our proposed intent-aware semantic query annotation approach. To further investigate the role of the intent classi cation component in our approach, we measure the impact of a range of simulated intent classi ers on the e ectiveness of the produced ranking of semantic annotations. In particular, starting from a perfect intent classi er (i.e., an oracle), we gradually introduce noise in the classi cation outcome by replacing the correct intent with a random one, up to the point where the classi cation itself becomes a random guess of the four available intents (i.e., E, T, Q, and O). As shown in Figure 3, our intent-aware switching strategy can outperform the intent-agnostic oblivious strategy with up to 50% of random noise in intent classi cation, which is a remarkable result. Recalling Q2, the experiments in this section demonstrate that accurate intent classi cation is feasible, and that the overall ranking annotation performance is robust to a considerable amount of noise in the predicted intents.",1,ad,True
5.3 Annotation E ectiveness,0,,False
"Section 5.1 showed the promise of leveraging intent-speci c ranking models, while Section 5.2 demonstrated that achieving this promise is feasible with reasonably accurate query intent classi ers. In this section, we address Q3, by assessing the e ectiveness of our",1,ad,True
"intent-aware semantic query annotation approach in contrast to the various baselines described in Section 4.3. ese include BM25 as a vanilla ad-hoc search baseline, FSDM as a representative of the current state-of-the-art, and multiple deployments of LambaMART using baseline intent-agnostic ranking adaptation strategies ( xed and oblivious) as well as our proposed intent-aware strategies (switching and mixing). Table 7 summarizes the results of this investigation in terms of P@10, nDCG@10, and MAP averaged across all 400 test queries from the four query sets in Table 3.12 In each row describing baseline results (the top half of the table), a rst of the symbols introduced in Section 4.4 denotes a statistically signi cant di erence (or lack thereof) with respect to LambdaMART (switching), whereas a second symbol denotes potential di erences with respect to LambdaMART (mixing). A further symbol is shown alongside LambdaMART (switching) to denote a signi cant di erence (or lack thereof) with respect to LambdaMART (mixing).",1,ad-hoc,True
"Table 7: Comparison of intent-agnostic (BM25, FSDM, LambdaMART xed and oblivious) and intent-aware (LambdaMART switching and mixing) semantic query annotation.",0,,False
BM25 FSDM LambdaMART,0,,False
( xed-E) ( xed-T) ( xed-Q) ( xed-O) (oblivious),0,,False
(switching) (mixing),0,,False
P@10,0,,False
0.181 0.204,0,,False
0.178 0.202 0.152 0.188 0.192,0,,False
0.227 0.243,0,,False
nDCG@10 MAP,1,MAP,True
0.250 0.289,0,,False
0.163 0.195,0,,False
0.244 0.275 0.215 0.260 0.276,0,,False
0.329 0.346,0,,False
0.162 0.172 0.139 0.163 0.178,0,,False
0.219 0.229,0,,False
"From Table 7, we rst observe that FSDM performs strongly, outperforming all intent-agnostic variants deployed with LambdaMART, which con rms its e ectiveness as a representative of the state-of-the-art. Also of note is the fact that a single model trained on a set of multiple intents using the oblivious strategy cannot consistently improve upon the best performing intent-speci c model, produced by the xed-T strategy. In contrast, both of our intent-aware ranking adaptation strategies are able to consistently leverage the best characteristics of each individual intent, signi cantly outperforming all intent-agnostic baselines in all se ings. In particular, compared to FSDM, our switching strategy improves by up to 11% in P@10, 14% in nDCG@10, and 12% in MAP. Compared to the best performing intent-agnostic strategy under LambdaMART ( xed-T), gains are as high as 12% in P@10, 20% in nDCG@10, and 27% in MAP. Lastly, we also note that our mixing strategy further signi cantly improves upon the switching strategy. is result suggests that merging multiple intent-speci c models (the mixing strategy) can be safer than applying a single model associated with the most likely query intent (the switching strategy). Recalling Q3, these results a est the e ectiveness of our intent-aware ranking adaptation for semantic query annotation.",1,ad,True
12E ectiveness breakdown analyses per query intent and various other query characteristics are presented in Section 5.4.,0,,False
491,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
5.4 Breakdown Analyses,0,,False
"e previous analysis demonstrated the e ectiveness of our approach on the entire set of 400 queries. To further shed light on the reasons behind such an e ective performance, we address question Q4, by analyzing the improvements brought by our approach for queries with di erent intents, lengths, and di culty.",1,ad,True
"5.4.1 Analysis by ery Intent. Table 8 breaks down the results in Table 7 according to the target intent of each query. For brevity, only the best among the xed strategy variants is shown. Note that while our approach aims to predict the correct intent of each query, there is no guarantee that a perfect intent classi cation will be achieved, as discussed in Section 5.2. Hence, it is important to understand how well our approach performs on queries of each target intent. From Table 7, as expected, the best xed strategy for each group of queries is that optimized for the group itself (e.g., xed-E is the best xed strategy for entity queries--the E group). Nonetheless, our intent-aware mixing strategy is the most consistent across all groups, with e ectiveness on a par with the best xed strategy for each group. Compared to our switching strategy, the mixing strategy is particularly e ective for type queries (the T group), with statistical ties for all other groups. Regarding performance di erences across the target intents, we note that all approaches achieve their best absolute performance on E queries followed by queries with other intents (the O group), which also includes entity queries. e e ective results a ained even by the simple BM25 baseline suggest that queries with these intents are well handled by content-based approaches.",0,,False
"Compared to the intent-agnostic FSDM baseline, our largest improvements are observed for type queries (the T group) and question queries (the Q group). For T queries, the structure-based features exploited by our learning to rank approach bring only small improvements, as observed by contrasting LambdaMART (oblivious) with FSDM. However, with our proposed intent-aware ranking adaptation strategies, further marked improvements are observed, with the mixing strategy signi cantly improving upon the oblivious strategy by up to 25% in P@10, 35% in nDCG@10, and 44% in MAP. For Q queries, both the extra features exploited via learning to rank as well as our ranking adaptation strategies help, with the switching strategy improving even further compared to the oblivious one by up to 56% in P@10, 51% in nDCG@10, and 50% in MAP. Figure 4 further illustrates the consistent improvements in terms of nDCG@100 a ained by our intent-aware strategies (here represented by the mixing strategy) compared to the intentagnostic oblivious baseline. Indeed, not only does mixing improve more queries than it hurts compared to oblivious, but it also shows larger increases and smaller decreases throughout queries of all four intents. Analyzing each intent separately, the most noticeable di erence can be observed for Q queries, with mixing performing be er for 50% of the queries and losing in only 10%. For E and T queries, the di erences in nDCG are not as high, but mixing is still superior for 60% of the queries. e smallest gap between the two strategies appears in O queries, although once again mixing performs be er for 60% the queries.",1,ad,True
"5.4.2 Analysis by ery Length. Continuing our detailed analysis, Table 9 breaks down the results from Table 7 according to",0,,False
Table 8: E ectiveness breakdown by query intent.,0,,False
P@10,0,,False
nDCG@10 MAP,1,MAP,True
E queries (100 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-E) (oblivious),0,,False
(switching) (mixing),0,,False
0.240 0.286,0,,False
0.293  0.239 0.282 0.297,0,,False
0.416 0.499,0,,False
0.498 0.434 0.486 0.502,0,,False
0.320 0.396,0,,False
0.387 0.329 0.377 0.390,0,,False
T queries (100 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-T) (oblivious),0,,False
0.190 0.211,0,,False
0.289  0.216,0,,False
0.193 0.223,0,,False
0.327  0.225,0,,False
0.141 0.167,0,,False
0.219  0.146,0,,False
(switching) 0.232 (mixing) 0.271,0,,False
0.260 0.303,0,,False
0.185 0.210,0,,False
Q queries (100 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-Q) (oblivious),0,,False
(switching) (mixing),0,,False
0.060 0.061,0,,False
0.143 0.091 0.142 0.141,0,,False
0.108 0.127,0,,False
0.273 0.177 0.267 0.266,0,,False
0.077 0.098,0,,False
0.203 0.132 0.198 0.194,0,,False
BM25 FSDM LambdaMART,0,,False
( xed-O) (oblivious),0,,False
(switching) (mixing),0,,False
O queries (100 queries),0,,False
0.235 0.258,0,,False
0.282 0.308,0,,False
0.259 0.221,0,,False
0.254 0.264,0,,False
0.304 0.267,0,,False
0.305 0.312,0,,False
0.113 0.119,0,,False
0.113 0.105,0,,False
0.116 0.123,0,,False
"the length of each query. In particular, we consider three groups of queries: short queries, with 1 or 2 terms (74 queries); medium queries, with 3 or 4 terms (193 queries); and long queries, with 5 or more terms (133 queries). From Table 9, we observe relatively higher performances of all approaches on short queries compared to those of other lengths. FSDM delivers a particularly strong performance on this group, with only a small gap from our mixing strategy, which is the overall best. is can be explained by FSDM's previously discussed e ectiveness on E queries, which have only 2.7 terms on average. Compared to the oblivious strategy, mixing brings substantial and signi cant improvements, once again demonstrating the bene ts of an intent-aware ranking adaptation. For medium and long queries (5 or more terms), both of our intent-aware strategies bring even more pronounced improvements compared to all intent-agnostic baselines, with the top performing mixing strategy outperforming the oblivious strategy by up to 32% in P@10, 30% in nDCG@10, and 36% in MAP. is tendency is somewhat expected given the e ective performance observed",1,ad,True
492,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
(a) All queries,0,,False
(b) E queries,0,,False
(c) T queries,0,,False
(d) Q queries,0,,False
(e) O queries,0,,False
Figure 4: Di erences in nDCG@100 between LambdaMART (mixing) and LambdaMART (oblivious) across: (a) all queries; (b) E queries; (c) T queries; (d) Q queries; (e) O queries. Positive values indicate mixing is better.,0,,False
"in Table 9 for the proposed intent-aware strategies on Q queries, which are typically longer (8 terms on average).",0,,False
Table 9: E ectiveness breakdown by query length.,0,,False
P@10,0,,False
nDCG@10 MAP,1,MAP,True
1 or 2 terms (74 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-E) (oblivious),0,,False
(switching) (mixing),0,,False
0.253 0.323,0,,False
0.324 0.276 0.324 0.337,0,,False
0.363 0.456,0,,False
0.453 0.394 0.455 0.465,0,,False
0.268 0.331,0,,False
0.327 0.278 0.324 0.332,0,,False
3 or 4 terms (193 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-T) (oblivious),0,,False
(switching) (mixing),0,,False
0.196 0.221,0,,False
0.234 0.208,0,,False
0.240 0.265,0,,False
0.264 0.308,0,,False
0.307 0.296,0,,False
0.349 0.376,0,,False
0.161 0.198,0,,False
0.182 0.183 0.227 0.239,0,,False
5 or more terms (133 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-T) (oblivious),0,,False
(switching) (mixing),0,,False
0.119 0.114,0,,False
0.141 0.121 0.156 0.160,0,,False
0.167 0.171,0,,False
0.206 0.181 0.230 0.236,0,,False
0.106 0.115,0,,False
0.134 0.116,0,,False
0.150 0.158,0,,False
"5.4.3 Analysis by ery Di iculty. To complete our breakdown analysis, we regroup all 400 queries in our investigation according to their di culty. In particular, we consider three groups: di cult queries, with 3 or less relevant results in the ground-truth (108 queries); moderate queries, with 4 to 20 relevant results (184 queries); and easy queries, with more than 20 relevant results (108 queries). e results of this investigation are shown in Table 10. From the table, we note as expected that di cult queries generally incur in reduced precision at early ranks (as measured by both P@10 and nDCG@10), while easy queries tend to penalize recall at",0,,False
"lower ranks (as measured by MAP). Nevertheless, our intent-aware adaptation strategies are once again the most e ective across all groups of queries, with the mixing strategy consistently providing the overall best results. For di cult queries (3 or less relevant results), compared to the oblivious strategy, mixing improves by up to 19% in P@10, 24% in nDCG@10, and 26% in MAP. For easy queries (21 or more relevant results), improvements are as high as 24% in P@10, 27% in nDCG@10, and 39% in MAP.",1,MAP,True
Table 10: E ectiveness breakdown by query di culty.,0,,False
P@10,0,,False
nDCG@10 MAP,1,MAP,True
Di cult: 3 or less relevant results (108 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-T) (oblivious),0,,False
0.059 0.064,0,,False
0.062 0.063,0,,False
0.224 0.256,0,,False
0.230 0.259,0,,False
0.179 0.214,0,,False
0.180 0.213,0,,False
(switching) 0.069 (mixing) 0.075,0,,False
0.308 0.322,0,,False
0.260 0.268,0,,False
Moderate: 4 to 20 relevant results (184 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-O) (oblivious),0,,False
0.210 0.245,0,,False
0.218 0.214,0,,False
0.260 0.308,0,,False
0.274 0.276,0,,False
0.194 0.235,0,,False
0.195 0.202,0,,False
(switching) 0.261 (mixing) 0.279,0,,False
0.329 0.345,0,,False
0.245 0.257,0,,False
Easy: 21 or more relevant results (108 queries),0,,False
BM25 FSDM LambdaMART,0,,False
( xed-T) (oblivious),0,,False
0.255 0.275,0,,False
0.328 0.283,0,,False
0.259 0.292,0,,False
0.338 0.292,0,,False
0.094 0.107,0,,False
0.125 0.103,0,,False
(switching) 0.328 (mixing) 0.350,0,,False
0.352 0.371,0,,False
0.134 0.143,0,,False
"Recalling Q4, the results in this section demonstrate the consistency of our intent-aware ranking adaptation strategies for semantic query annotation. Overall, both the switching and the mixing strategies achieve generally improved results for queries",1,ad,True
493,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"of di erent target intents, lengths, and di culty levels, o en signi cantly. Particularly, question-oriented queries (the Q intent), long queries (queries with 5 or more terms), and moderate to easy queries (queries with 4 or more relevant results) are the ones that bene t the most from our intent-aware approach.",0,,False
6 CONCLUSIONS,0,,False
"We presented a framework for learning to rank semantic annotations suitable to the intent of each individual query. Our approach predicts the intent of a target query and adapts the ranking produced for this query using one of two strategies: switching, which applies a ranking model trained on queries of the same intent as predicted for the target query, or mixing, which combines the results of multiple intent-speci c ranking models according to their predicted likelihood for the target query. Extensive experiments on a publicly available benchmark demonstrated the e ectiveness of our approach for semantic query annotation, with signi cant improvements compared to state-of-the-art intent-agnostic approaches. e results also a ested the consistency of the observed improvements for queries of di erent intents, lengths, and di culty levels.",1,ad,True
"In the future, we plan to assess the impact of intent-aware learning on frameworks other than learning to rank. Preliminary results in this direction show that the FSDM baseline, which is based on the Markov random elds framework, can be improved with an intent-aware approach to hyperparameter tuning, although with less marked gains compared to the ones observed in our experiments with feature-based models using learning to rank. Another direction for future investigation includes evaluating our approach with a larger intent taxonomy, including more queries with less common intents such as a ribute and relation queries.",0,,False
ACKNOWLEDGMENTS,0,,False
"is work was partially funded by projects InWeb (MCT/ CNPq 573871/2008-6) and MASWeb (FAPEMIG/PRONEX APQ-01400-14), and by the authors' individual grants from CNPq and FAPEMIG.",1,NP,True
REFERENCES,0,,False
"[1] Omar Alonso and Hugo Zaragoza. 2008. Exploiting semantic annotations in information retrieval: ESAIR '08. SIGIR Forum 42, 1 (2008), 55­58.",0,,False
"[2] Kriztian Balog, Arien P. de Vries, Pavel Serdyukov, Paul omas, and ijs Westerveld. 2009. Overview of the TREC 2009 Entity track. In TREC.",1,TREC,True
[3] Krisztian Balog and Robert Neumayer. 2013. A Test Collection for Entity Search in DBpedia. In Proc. of SIGIR. 737­740.,0,,False
"[4] Hannah Bast, Bjo¨rn Buchhold, Elmar Haussmann, and others. 2016. Semantic Search on Text and Knowledge Bases. Foundations and Trends in Information Retrieval 10, 2-3 (2016), 119­271.",0,,False
"[5] Bin Bi, Hao Ma, Bo-June (Paul) Hsu, Wei Chu, Kuansan Wang, and Junghoo Cho. 2015. Learning to Recommend Related Entities to Search Users. In Proc. of WSDM. 139­148.",0,,False
"[6] Roi Blanco, Peter Mika, and Sebastian Vigna. 2011. E ective and E cient Entity Search in RDF Data. In Proc. of ISWC. 83­97.",0,,False
"[7] Roi Blanco, Peter Mika, and Hugo Zaragoza. 2010. Entity Search Track Submission by Yahoo! Research Barcelona. In Proc. of Entity Search Track.",1,Track,True
"[8] David J. Brenes, Daniel Gayo-Avello, and Kilian Pe´rez-Gonza´lez. 2009. Survey and Evaluation of ery Intent Detection Methods. In Proc. of WSCD. 1­7.",0,,False
"[9] Andrei Broder. 2002. A Taxonomy of Web Search. SIGIR Forum 36, 2 (2002), 3­10.",0,,False
"[10] Marc Bron, Krisztian Balog, and Maarten de Rijke. 2010. Ranking related entities: components and analyses. In Proc. of CIKM. 1079­1088.",0,,False
"[11] Marc Bron, Krisztian Balog, and Maarten De Rijke. 2013. Example Based Entity Search in the Web of Data. In Proc. of ECIR. 392­403.",0,,False
"[12] Ste´phane Campinas, Renaud Delbru, Nur Aini Rakhmawati, Diego Ceccarelli, and Giovanni Tummarello. 2011. Sindice BM25F at SemSearch 2011. In Proc. of",0,,False
SemSearch. [13] Olivier Chapelle and Yi Chang. 2011. Yahoo! learning to rank challenge,1,Yahoo,True
overview.. In Yahoo! Learning to Rank Challenge. 1­24. [14] Nick Craswell and David Hawking. 2004. Overview of the TREC 2004 Web track.,1,Yahoo,True
"In Proc. of TREC. [15] Arjen P. de Vries, Anne-Marie Vercoustre, James A. om, Nick Craswell, and",1,TREC,True
"Mounia Lalmas. 2007. Overview of the INEX 2007 Entity Ranking track. In Proc. of INEX. 245­251. [16] Shady Elbassuoni and Roi Blanco. 2011. Keyword Search over RDF Graphs. In Proc. of CIKM. 237­242. [17] Shady Elbassuoni, Maya Ramanath, Ralf Schenkel, Marcin Sydow, and Gerhard Weikum. 2009. Language-Model-Based Ranking for eries on RDF-Graphs. In Proc. of CIKM. 977­986. [18] Besnik Fetahu, Ujwal Gadiraju, and Stefan Dietze. 2015. Improving entity retrieval on structured data. In Proc. of ISWC. 474­491. [19] Xiubo Geng, Tie-Yan Liu, Tao Qin, Andrew Arnold, Hang Li, and Heung-Yeung Shum. 2008. ery dependent ranking using k-nearest neighbor. In Proc. of SIGIR. 115­122. [20] Ido Guy. 2016. Searching by Talking: Analysis of Voice eries on Mobile Web Search. In Proc. of SIGIR. 35­44. [21] Daniel M Herzig, Peter Mika, Roi Blanco, and anh Tran. 2013. Federated Entity Search Using On-the- y Consolidation. In Proc. of ISWC. 167­183. [22] Bernard J. Jansen, Amanda Spink, and Te o Saracevic. 2000. Real Life, Real Users, and Real Needs: A Study and Analysis of User eries on the Web. Information Processing and Management 36, 2 (2000), 207­227. [23] In-Ho Kang and GilChang Kim. 2003. ery type classi cation for Web document retrieval. In Proc. of SIGIR. 64­71. [24] Tie-Yan Liu and others. 2009. Learning to rank for information retrieval. Foundations and Trends® in Information Retrieval 3, 3 (2009), 225­331. [25] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego, and Nicola Tonello o. 2015. Speeding Up Document Ranking with Rank-based Features. In Proc. of SIGIR. 895­898. [26] Donald Metzler and W. Bruce Cro . 2005. A Markov Random Field Model for Term Dependencies. In Proc. of SIGIR. 472­479. [27] Robert Neumayer, Krisztian Balog, and Kjetil Nørva°g. 2012. On the Modeling of Entities for Ad-Hoc Entity Search in the Web of Data. In Proc. of ECIR. 133­145. [28] Fedor Nikolaev, Alexander Kotov, and Nikita Zhiltsov. 2016. Parameterized Fielded Term Dependence Models for Ad-hoc Entity Retrieval from Knowledge Graph. In Proc. of SIGIR. 435­444. [29] Paul Ogilvie and Jamie Callan. 2003. Combining Document Representations for Known-item Search. In Proc. of SIGIR. 143­150. [30] Jie Peng, Craig Macdonald, and Iadh Ounis. 2010. Learning to select a ranking function. In Proc. of ECIR. 114­126. [31] Jose´ R Pe´rez-Agu¨era, Javier Arroyo, Jane Greenberg, Joaquin Perez Iglesias, and Victor Fresno. 2010. Using BM25F for semantic search. In Proc. of SemSearch. 2. [32] Je rey Pound, Peter Mika, and Hugo Zaragoza. 2010. Ad-hoc Object Retrieval in the Web of Data. In Proc. of WWW. 771­780. [33] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Mike Gatford, and A. Payne. 1995. Okapi at TREC-4. In Proc. of TREC. [34] Cristiano Rocha, Daniel Schwabe, and Marcus Poggi Aragao. 2004. A Hybrid Approach for Searching in the Semantic Web. In Proc. of WWW. 374­383. [35] Daniel E. Rose and Danny Levinson. 2004. Understanding User Goals in Web Search. In Proc. of WWW. 13­19. [36] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Selectively diversifying web search results. In Proc. of CIKM. 1179­1188. [37] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Voting for related entities. In Proc. of RIAO. 1­8. [38] Rodrygo L. T. Santos, Craig Macdonald, and Iadh Ounis. 2011. Intent-aware Search Result Diversi cation. In Proc. of SIGIR. 595­604. [39] Alberto Tonon, Gianluca Demartini, and Philippe Cudre´-Mauroux. 2012. Combining Inverted Indices and Structured Search for Ad-hoc Object Retrieval. In Proc. of SIGIR. 125­134. [40] Gilad Tsur, Yuval Pinter, Idan Szpektor, and David Carmel. 2016. Identifying Web eries with estion Intent. In Proc. of WWW. 783­793. [41] Qiang Wu, Chris JC Burges, Krysta M Svore, and Jianfeng Gao. 2008. Ranking, boosting, and model adaptation. Technical Report. Technical report, Microso Research. [42] Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow. 2005. Learning to estimate query di culty: including applications to missing content detection and distributed information retrieval. In Proc. of SIGIR. 512­519. [43] Nikita Zhiltsov and Eugene Agichtein. 2013. Improving Entity Search over Linked Data by Modeling Latent Semantics. In Proc. of CIKM. 1253­1256. [44] Nikita Zhiltsov, Alexander Kotov, and Fedor Nikolaev. 2015. Fielded Sequential Dependence Model for Ad-Hoc Entity Retrieval in the Web of Data. In Proc. of SIGIR. 253­262.",1,INEX,True
494,0,,False
,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
E icient & E ective Selective ery Rewriting with E iciency Predictions,0,,False
Craig Macdonald,0,,False
"University of Glasgow Glasgow, Scotland, UK craig.macdonald@glasgow.ac.uk",0,,False
Nicola Tonello o,0,,False
"ISTI-CNR Pisa, Italy nicola.tonello o@isti.cnr.it",0,,False
Iadh Ounis,1,ad,True
"University of Glasgow Glasgow, Scotland, UK iadh.ounis@glasgow.ac.uk",1,ad,True
ABSTRACT,0,,False
"To enhance e ectiveness, a user's query can be rewri en internally by the search engine in many ways, for example by applying proximity, or by expanding the query with related terms. However, approaches that bene t e ectiveness o en have a negative impact on e ciency, which has impacts upon the user satisfaction, if the query is excessively slow. In this paper, we propose a novel framework for using the predicted execution time of various query rewritings to select between alternatives on a per-query basis, in a manner that ensures both e ectiveness and e ciency. In particular, we propose the prediction of the execution time of ephemeral (e.g., proximity) posting lists generated from uni-gram inverted index posting lists, which are used in establishing the permissible query rewriting alternatives that may execute in the allowed time. Experiments examining both the e ectiveness and e ciency of the proposed approach demonstrate that a 49% decrease in mean response time (and 62% decrease in 95th-percentile response time) can be a ained without signi cantly hindering the e ectiveness of the search engine.",0,,False
"ACM Reference format: Craig Macdonald, Nicola Tonello o, and Iadh Ounis. 2017. E cient & E ective Selective ery Rewriting with E ciency Predictions. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080827",1,ad,True
1 INTRODUCTION,1,DUC,True
"Search engines, such as those for the Web, are required to be e ective at answering users' queries but yet also e cient. In particular, while the relevance of the results are important for users' satisfaction, users are in general not willing to wait long for the results to arrive [36]. While search engines are operated in distributed retrieval se ings that can be scaled horizontally to reduce response times, the rami cation of this is increased cost (in terms of both capital outlay and running, e.g., for power) for the search engine infrastructure. is being the case, the e ciency of the search engine is key to providing e ective results without excessive nancial burden. Typically, the infrastructure is designed to maintain a service level, where high percentile response time (the so-called ""tail latencies"" [16, 19]) should not exceed a given target. Indeed, for the Bing search engine, the target is reported to be that 99% percentile response time should not exceed 100 ms [19].",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080827",1,ad,True
Table 1: Example rewrites for the query `poker tournament'.,0,,False
Original query: poker tournament Stemming: poker #syn(tournaments tournament) Proximity: poker tournament #1(poker tournament)0.1,0,,False
#uw8(poker tournament)0.1 Stemming and poker #syn(tournaments tournament),0,,False
Proximity: #1(poker #syn(tournaments tournament))0.1 #uw8(poker #syn(tournaments tournament))0.1,0,,False
"On the other hand, techniques that bene t the e ectiveness of a search engine may hinder e ciency [41], due to their complex nature. For example, in a modern search engine deploying learningto-rank approaches, the number of features to be computed and the learned models both contribute complexity, and have been the subject of recent studies (e.g., [25]). However, the time to traverse the inverted index's posting lists for the query terms, to identify the top K documents -- which are then re-ranked by the learned approach -- takes signi cant time [14].",0,,False
"O en the query submi ed by the user is internally rewri en by the search engine to improve the quality of the search results [20, 33]. For instance, traditional pseudo-relevance feedback approaches typically results in a much larger query, with signi cant negative impact on e ciency. More recently, less aggressive query rewriting approaches such as term proximity [30], query substitutions [20] and query-time stemming [34] have been deployed by search engines. Each query rewriting approaches can lead to a query with additional terms, resulting in prolonged execution times.",1,ad,True
"Table 1 shows three possible rewritings of the query `poker tournament', based on application of combinations of stemming [34] and sequential dependence (proximity) [30]. In the rewri en examples using an Indri-like query language, complex query operators [37] denote additional postings lists that must be traversed during retrieval, namely: #syn, which combines the constituent terms into a single posting list; #1 creates a posting list representing an exact occurrence of an n-gram; and #uw creates a posting list that provides the number of times a n-gram appears in an unordered window of size  [37]. While the search engine may have indexed posting lists for some n-grams, not all possible query operators may have existing posting lists, and hence ephemeral posting lists are required, which need to be created on-the- y from the constituent terms. Moreover, statistics such as the total number of postings in an ephemeral posting list are unknown, and hence the number of postings to be processed and the resulting execution time of a query containing complex operators cannot be known in advance.",1,ad,True
"Indeed, while recent work in query e ciency prediction has shown the possibility of estimating the execution time of a query prior to its processing [19, 21, 29, 38], none of the existing work",0,,False
495,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Query,1,Query,True
Query Rewriting,1,Query,True
Rewritten Query,1,Query,True
Top K Processing,0,,False
Features Lookup and Calculation,0,,False
Documents,0,,False
Learned Ranking Function,0,,False
Unigram Inverted,0,,False
Index,0,,False
Top K Retrieval,0,,False
Features Repository,0,,False
Feature Extraction,0,,False
Learning to Rank Technique,0,,False
Learned Model Application,0,,False
Training Data,0,,False
Figure 1: A pictorial representation of the reference web search engine architecture that we consider in this work.,0,,False
"has considered the execution time of queries containing query operators that generate ephemeral posting lists, such as #syn, or #1.",0,,False
"is makes it di cult to select among query rewriting strategies that use such operators, as their likely execution time is unknown. Hence, in this work, we study the cost of scoring ephemeral posting lists, and use these observations to de ne accurate query e ciency predictions for advanced query operators. Furthermore, we use these query e ciency predictions to instantiate a novel mechanism that selects the best strategy among alternative query rewritings, to improve e ciency, while minimising impact on e ectiveness.",1,ad,True
"Our conducted experiments to measure the e ciency of our proposed selective mechanism upon TREC Web track test collections show that a 49% decrease in mean response time, and 62% decrease in tail (95th-percentile) response time, can be a ained without signi cantly hindering the e ectiveness of the search engine. e contributions of this work are as follows: we show how to make query e ciency predictions for ephemeral posting lists created by complex operators such as #syn and #1; we use these advanced query e ciency predictions to propose a selector mechanism that permits the query to be rewri en in an e ective manner while considering a target response time that the search engine should aim to meet.",1,TREC,True
"e remainder of this paper is structured as follows: Section 2 provides an overview of a reference search engine architecture, describing the necessary background; Section 3 positions our contributions with respect to existing work; Section 4 describes our mechanism for selecting among query rewrites; Section 5 proposes new query e ciency predictors suitable for application to complex query operators. Our experimental setup and results follow in Sections 6 & 7. Finally, we provide concluding remarks in Section 8.",0,,False
2 PRELIMINARIES,0,,False
"In this section, we provide some essential background on index organisation and query processing in search engines. In doing so, we follow the reference architecture for a search engine depicted by Figure 1. e following section summarises and discusses the stateof-the-art query rewriting techniques and approaches addressing e cient but e ective retrieval.",1,ad,True
"Index Organisation. Given a collection D of documents, each document is identi ed by a non-negative integer called document identi er, or docid di . A posting list It is associated to each term t appearing in the collection, containing the list of the docids of all the documents in which the term occurs at least once. e collection of the posting lists for all of the terms is called the inverted index of D,",0,,False
"while the set of the terms is usually referred to as the lexicon. For each term t, the lexicon stores a pointer to its posting list as well as additional information on the statistics of the term in the collection, such as its document frequency Nt , i.e., the length of its posting list, and the total number of occurrences of the term in the collection Ft . Each posting in a posting list typically contains additional information about the term's occurrences in the document, such as the number of occurrences ft,d , and the set of positions, pt,d , where the term t occurs [13]. is position information facilitates phrasal retrieval without resort to large n-gram index data structures.",1,ad,True
"e docids in a posting list can be sorted in increasing order enabling the use of e cient compression algorithms and query processing [31]; or the posting lists can be frequency-sorted [39] or impact-sorted [2], allowing for good compression rates, but also presenting practical disadvantages such as their di culty of use for phrasal queries [22, 37]. As such, in this paper, we focus on the more common search scenario of docid-sorted index layouts [15].",1,ad,True
"ery Processing. e top K ranked retrieval stage identi es the K highest scored documents in the collection, where the relevance score is a function of the query-document pair. Multi-stage retrieval systems have become the dominant model for e cient and e ective web search engines [14]. In such systems (see Figure 1), a rst ""top K"" stage retrieves from the inverted index a relatively small set of K possibly-relevant documents matching the user query, focusing on optimising recall. Subsequent stages compute additional query dependent (e.g., elds [28], proximity) and query independent features, before applying a learning-to-rank technique to re-rank the K documents coming from the rst stage, aiming to maximise measures like NDCG [25]. e inverted index posting lists are processed in the rst stage only, to produce a small set of candidate documents, that will be re-ranked in the subsequent stage(s).",1,ad,True
"Documents are typically scored in the rst stage retrieval by the (weighted) linear combinations of weighting model (e.g., BM25, language models) functions computed for each term-document pair. Such weighting models are usually monotonically increasing in the number of occurrences of the term in the document ft,d . An obvious way to compute the top K scored documents is to exhaustively apply the weighting model to all the documents that match at least one query term in the inverted index. As such an exhaustive method is very expensive for large collections, several dynamic pruning techniques have been proposed in the last few years. Dynamic pruning makes use of the inverted index, augmented with additional data structures, to skip documents that cannot reach a su cient score to enter the top K. us, the nal result is the same as an exhaustive evaluation, but obtained with signi cantly less work. ese techniques include MaxScore [39], WAND [6], and BMW [17]. In this paper, we focus our a ention on the WAND strategy, since we deal with (rewri en) queries that can have a large number of terms (i.e., long queries). Indeed, several previous studies have con rmed that MaxScore performs be er than WAND for short queries while the opposite happens for long queries [18, 31] while, as we will discuss in Sec. 5, the BMW techniques are not suitable for processing rewri en queries.",1,ad,True
"WAND augments the posting list of each term t with an upper bound t on the maximum score of that term among all documents in the list. While processing the query by iterating on the posting lists of its terms, it records the top K scores among the documents",0,,False
496,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"evaluated thus far. To enter the top K, a new document needs to have a larger score than the current K-th score, which we call the min score. WAND maintains the posting list iterators sorted by increasing docid; at every step, it sums up the maximum scores of the lists in increasing order, until the min score is reached. It can be seen that the current docid of the rst list that exceeds the min score is the rst docid that can reach a score higher than the min score, so the other iterators can safely skip all the documents up to that docid. e alignment of the posting lists during WAND processing is achieved by means of a nextt (d) method upon the posting list iterators, which returns the smallest docid in the posting list It that is greater than or equal to d. is functionality signi cantly enhances the retrieval speed exhibited by WAND, by skipping docids that would never be retrieved in the top K, and hence avoiding their decompression and scoring. Indeed, smaller values of K allow for more skipping, since the threshold is in general larger for small values of K than for large values, resulting in smaller query processing times, as reported, for example, in [38].",0,,False
3 RELATED WORK,0,,False
"In the following, we survey existing work in query rewriting and in query e ciency predictions, and position our work accordingly.",0,,False
ery Rewriting. ere are a number of related works across the areas of query rewriting and e cient yet e ective retrieval.,0,,False
"e internal rewriting of a user's query within an IR system has a long history, including pseudo-relevance feedback in the form of automatic query expansion, rst deployed by the SMART system in TREC-3 [7], while others considered the correction of spelling errors or the application of ontologies to identify related concepts [13, Ch. 6]. Indeed, query expansion, which adds additional terms to the query based upon their appearance in the top ranked documents, has been shown to be e ective for adhoc retrieval tasks in evaluation forums such as TREC [42]. For web search, the signi cantly longer generated queries, as well as the need to conduct two retrieval phases, make pseudo-relevance feedback approaches infeasible for e cient retrieval. A further risk is the possibility that the topic of the expanded query can dri from the intent of the initial query.",1,TREC,True
"Instead, some of the techniques widely deployed in web search have focused on rewriting the query based on the large amounts of user interaction data available to a web search engine. For instance Jones et al. [20] describe a way to mine common query reformulation pa erns, based on log likelihood ratio, that can be automatically applied to re ne a new query. Random walks on the query-click graph [12] o er similar possibilities for identifying common paraphrasing queries.",1,ad,True
"Rewriting the query to include common variants of the original query terms can have an important e ectiveness bene t in addressing the word mismatch problem. Indeed, a query-side approach to stemming has a marked advantage of index-time stemming, in that the other words within the query can be taken into account to decide if the stemming is appropriate for a given word. For instance, Peng et al. [34] describe a context-sensitive stemming approach where query segments1 are carefully considered for stemming, by comparing the language model generation probability of both the original and the replacement segments. Naturally, adding additional terms",1,ad,True
"1 N-gram subsequences of queries that demonstrate the underlying grammatical structure, usually determined by dividing longer sequences to maximise n-gram language model probabilities.",0,,False
"to the query can have a marked negative impact on e ciency, hence, as noted by Peng et al., it is not desirable to rewrite queries unnecessarily. In our work in this paper, query rewriting by application of stemming is one of the query rewriting techniques that we consider.",0,,False
"One method of query rewriting that has gained signi cant bene ts in e ectiveness is the application of term dependence (proximity) operators, to boost the retrieval of documents where the query terms occur close together [30, 35]. In particular, Metzler & Cro 's Markov Random Field sequential dependence model makes use of the Indri complex query operators #1 and #uw formed from adjacent pairs of query terms, added to the original query terms with low weights (typically [0.05,0.1]). However, such rewri en queries have a negative e ciency impact, in that more posting lists must be traversed, while if the index only has unigram posting lists, ephemeral posting lists must be created to handle the #1 and #uw operators2. Another variant, the full dependence model ­ which adds complex query operators for each pair of query terms ­ is generally considered too ine cient for common retrieval use [4, 30]. Hence, for large-scale environments, there has to be a perceived bene t in deploying such a term dependence model, due to its inherent negative e ciency impact. For this reason, we note various works that extract term dependence proximity features at the re-ranking stage [28, 38] ­ an approach that we deploy within our baseline retrieval system in this paper.",1,ad,True
"On the other hand, motivated by the ine ciencies in deploying sequential dependence, Wang et al. [41] proposed an e cient variant where the weights for bi-gram operators were adjusted to jointly optimise combinations of e ectiveness and e ciency, and bi-grams predicted not to be useful were eliminated. In this way, the work of Wang et al. is one of the closest to our work. However, their features for estimating the cost of the #1 and #uw bi-gram complex operators assumed the existence of bi-gram index statistics, something that our approach does not require; Moreover, their experiments did not consider deployment under a dynamic pruning strategy, where the e ciency cost of additional complex proximity operators, ­ which have low weighting in the query (see Table 1) ­ may be markedly reduced by the pruning. Finally, as we consider more than just proximity rewriting, our work is more general than that of Wang et al. [41].",1,ad,True
"E ciency Predictions. Using a docid-sorted index layout, the time taken for a query to execute is correlated with the length of the posting lists of the query's constituent terms, as these posting lists must be traversed during execution. Dynamic pruning techniques such as WAND and BMW o er some relief as they o er the potential to safely skip the decompression of postings and the scoring of documents that cannot make the current top K. is makes the exact response time of a query di cult to predict, as not every posting in the postings lists will be decompressed and scored. Nevertheless recent work has considered making accurate predictions on the e ciency of a query, either in terms of absolute response time [29], or in terms of those queries with response times exceeding a threshold [19, 21].",0,,False
"E ciency predictions facilitate a number of applications for ensuring e cient yet e ective retrieval - for instance, routing queries",0,,False
"2 For instance, according to the recent IR system reproducibility e ort [23], Indri's average response time is decreased by a factor of 6 on deploying sequential dependence on the Gov2 corpus.",1,Gov,True
497,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"among busy replicated query shard servers [29]; selectively deploying multiple CPU cores for slow queries [19, 21]; or adjusting the pruning aggressiveness or size of K for di erent queries [5, 14, 38]. Of these, the work of Tonello o et al. [38] is among the most similar to ours, in that they vary the number of documents to be retrieved, K, as well as the pruning aggressiveness, before passing to a learning-to-rank re-ranking phase, based on the predicted execution time of the query. Similarly, in a very recently published work, Culpepper et al. [14] de ned an approach for training the rank cuto in a multi-stage ranking system based on closeness in overlap to a ""reference"" system. However, their approach has a key disadvantage in that they use a simple reference system, and hence would not demonstrate the bene t in going beyond that system, for instance in deploying advanced query rewrites.",1,ad,True
"Indeed, di erently from [14] & [38], in this paper, we go further by considering a prediction of the execution time of possible rewritings of the users' original queries. is is made possible by the novel prediction of the execution time of complex query operators such as #syn and #uw. In the following, we rstly de ne the problem and introduce our selection mechanism (Section 4), before de ning how to obtain query e ciency predictions for queries involving complex operators in Section 5.",1,ad,True
4 SELECTING AMONG QUERY REWRITINGS,0,,False
4.1 Problem Statement,0,,False
"is work considers the e cient yet e ective rewriting of a given query q. Indeed, the search engine may consider several possible ways to reformulate the query into a re ned instance q, for instance, by spli ing compound words, adding alternative words identi ed using stemming algorithms or common query reformulations approaches, or adding proximity terms such as #1. Some such query rewritings can result in a longer query formulation, hindering its e ciency compared to the original query [33].",1,ad,True
"Reformulating the query in a multi-stage ranking system ­ such as one deploying learning to rank ­ can be seen as aiming to improve the recall of the K documents retrieved in the Top K Retrieval stage. is ensures that when re-ranking the documents by the application of the learned model, the search engine has a high chance of identifying the most relevant documents for promotion to the top of the ranked list for presentation to the user. Naturally, improving the formulation of the query may also increase the high precision of these K documents ­ i.e., by retrieving more relevant documents towards the top of that initial list. is suggests that some rewri en queries only need a smaller K  < K. Moreover, as mentioned above, the e ciency of dynamic pruning techniques like WAND is bene ted by smaller K.",0,,False
"Hence, with various di erent rewriting techniques available, a natural question arises: for a given query q, which possible rewritings are appropriate to be applied to the query, q1 . . . qn , such that e ectiveness may be improved, and/or, K reduced to improve e ciency, without signi cantly damaging the overall e ectiveness.",0,,False
4.2 Selection Mechanism,0,,False
"To achieve this, we make use of query e ciency predictions that estimate the execution time of di erent rewritings of the query, before one is selected and executed. A particular challenge in doing so is making accurate estimations of the execution times of",0,,False
"rewri en queries that use operators such as #syn, #1 and #uw. We discuss this further in Section 5.",0,,False
"Firstly, we generate all possible rewritings {q1 . . . qn } of the original query q. We note that the size of this set varies for each query q, as not all rewritings are applicable to each query ­ for instance, no term dependence can be applied to a query with only a single term, or no stemming may be applicable for each query term. At the very least, the original query will always be present.",0,,False
"We also consider m di erent K values for the number of documents to retrieve in the rst retrieval phase, which will then be re-ranked by application of the learned model, namely K1 . . . Km . In doing so, our intuition is that for some queries, simply identifying K ,"" 20 documents will be su cient to identify enough relevant documents, leading to marked e ciency bene ts, particularly if the rewri en query permits higher recall of relevant documents within the top K. A possible plan for executing a query can be denoted as the tuple qi, Kj . All possible query plans, denoted P (q), for executing the query can be generated from the Cartesian product:""",1,ad,True
"P (q) ,"" qi, Kj "", {q1 . . . qn } × {K1 . . . Km }",0,,False
(1),0,,False
"e aim is then to rank and eliminate plans qi, Kj based on their predicted e ciency and expected e ectiveness. While the number of possible rewrites for each query varies, |P (q)|  m.",0,,False
"Consider that the search engine has a service level agreement in place [19], which aims to maximise the number of queries answered in time  . Some plans for queries may exceed  . We use query e ciency predictions, denoted t ( qi, Kj ) to eliminate such plans:",0,,False
"EP (q,  ) ,"" qi, Kj  P (q) | t ( qi, Kj )  """,0,,False
(2),0,,False
"Naturally, plans will vary in e ectiveness. One possible approach to select among the feasible plans EP (q,  ) would be to try to predict the e ectiveness of a given rewriting of a query. However, Tonello o et al. [38] examined the usefulness of query performance (e ectiveness) predictors within their selective pruning mechanism, and found them to have li le correlation with maintaining e ectiveness while enhancing e ciency. Moreover, we are not aware of any existing works that make e ectiveness predictions in the presence of complex operators used by some rewritings.",0,,False
"Instead, to determine the likely e ectiveness of a plan qi, Kj , we measure the expectation of the e ectiveness for some measure µ (e.g., NDCG) of that given rewriting upon a set of training queries Qtr. Denoting with Qitr, Kj the set of query plans for the rewri en queries according to the i-th rewriting, we compute the expected e ectiveness of measure µ over all such query plans, Eµ ( Qitr, Kj ). However, due to excessively long posting lists, some queries may not have any plans that can be executed in time  , i.e., |EP (q,  )| ,"" 0. For this reason, for such queries, we resort to a best a empt, by selecting the plan with the fastest predicted execution time. e""",1,ad,True
"nal selection mechanism to identify the best plan F P (q,  ) for executing the (possibly rewri en) query q in time  is as follows:",0,,False
F,0,,False
"P (q, ",0,,False
),0,,False
",",0,,False
argmax Eµ,0,,False
"qi, Kj  E P (q, )",0,,False
(,0,,False
"Qitr, Kj",0,,False
argmin t (,0,,False
"qi, Kj  E P (q, )",0,,False
"qi, Kj",0,,False
),0,,False
),0,,False
"if |EP (q,  )| > 0 otherwise",0,,False
(3),0,,False
e usefulness of this mechanism is driven by the need to have ac-,0,,False
"curate estimation of the time to execute a given query plan, namely t ( qi, Kj ). Moreover, as mentioned above, the estimation of the",0,,False
498,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 2: Complex operators summary table.,0,,False
Complex operator,0,,False
#syn #1 #uw,0,,False
Complex term,0,,False
"#syn(car, cars) #1(new, york) #uw8(divx, codec)",0,,False
Ephemeral posting list,0,,False
Disjunctive/OR Conjunctive/AND Conjunctive/AND,0,,False
"execution time of complex query operators, particularly under dynamic pruning strategies such as WAND, have not previously been addressed. In the next section, we propose a novel method to address this problem, by using machine-learned models to predict the execution times of query plans that use complex operators.",1,ad,True
5 EFFICIENCY PREDICTION FOR COMPLEX OPERATORS,0,,False
"e complex operators involved in stemming and proximity rewritings are summarised in Table 2. Such complex operators can be applied to two or more uni-gram terms, as well as other complex operators, i.e., complex operators can be nested. Complex operators, such as #1, generate complex terms once instantiated, such as #1(new, york). In the following, we discuss how the posting lists for such complex terms are generated (Section 5.1) and how they can be processed together with the original terms in the WAND strategy (Section 5.2). Section 5.3 presents our approach for predicting the query processing time of queries containing complex terms.",0,,False
5.1 Complex terms and ephemeral posting lists,0,,False
"While posting lists for simple (e.g., uni-gram) terms are stored in the inverted index, it is not feasible to pre-compute statistics and posting lists for complex terms, since their space occupancy will quickly become unmanageable, particularly if complex operators can be nested. Hence, for a complex term #op(t1, . . . , #op1, . . .), its posting list and statistics must be generated on-the- y, such that it can be processed together with other simple and complex terms. Such ephemeral posting lists are materialised as required, depending on the complex operator and the involved terms. We consider two types of ephemeral posting lists: disjunctive/OR-based lists and conjunctive/AND-based lists (see Table 2). OR-based posting lists are used with #syn operators, where the posting lists of two or more terms are merged into a single posting list containing all docids appearing in at least one of the terms' posting lists. ANDbased posting lists are used with #1 and #uw operators, where two or more terms' posting lists must be intersected, i.e., a posting appears in the ephemeral posting list only if it is present in all of the involved posting lists.",0,,False
"e involved complex operator de nes how to merge postings into ephemeral posting lists. In OR-based ephemeral posting lists, when the postings of two di erent terms that refer to the same docid are merged, their term frequencies in a document must be added, and the positions arrays must be merged. Similarly, while the terms' frequencies in the collection (Ft ) must be summed, we cannot know in advance the document frequencies (Nt ) of the resulting ephemeral posting list, as the number of docids in common between the two posting lists is unknown. In AND-based ephemeral posting lists, there is no general way to merge term frequencies in documents, while term positions must be stored separately and processed depending on the semantics of the complex",1,ad,True
"operator [24]. Again, the document frequency Nt of the resulting ephemeral posting list, i.e., its length, cannot be known in advance.",1,ad,True
5.2 ery Processing,0,,False
"Given a user query q composed of simple terms, let us assume it",0,,False
"is rewri en into a query composed of simple terms and complex terms, collectively denoted by q. Complex terms are obtained by",0,,False
applying complex operators ­ summarised in Table 2 ­ to simple or,0,,False
"other complex terms, in a nested fashion. e score of a document d w.r.t. the rewri en query q can be expressed as follows:",0,,False
"s (q, d ) ,",0,,False
"wt st,d +",0,,False
wt,0,,False
st,0,,False
",",0,,False
d,0,,False
",",0,,False
(4),0,,False
t qd,0,,False
t  (q\q)d,0,,False
where wt (resp. wt) denotes the simple (resp. complex) terms,0,,False
weights,0,,False
(see,0,,False
Table,0,,False
"1),",0,,False
while,0,,False
"st,d",0,,False
and,0,,False
st,0,,False
",",0,,False
d,0,,False
are,0,,False
weighting,0,,False
models,0,,False
for simple and complex terms respectively. e linearity of Eq. (4),0,,False
"makes it easily usable in any exhaustive query processing algorithm,",0,,False
by exploiting ephemeral posting lists as normal posting lists.,0,,False
"Conversely, dynamic pruning techniques are not directly usable,",0,,False
"since they rely on the maximum score t of each query term, cal-",0,,False
"culated among all documents in the respective posting lists, i.e.,",0,,False
"t ,"" wt · maxd It st,d . ese maximum scores can be computed o ine by taking the score value of the top document of a single""",0,,False
"term query stored in the lexicon. However, since ephemeral post-",0,,False
"ing lists are materialised on-the- y, such a computation cannot be",0,,False
"performed o ine, hence we must resort to a runtime estimation for",0,,False
"upper bounds on these maximum scores. In [26], the authors pro-",0,,False
posed a general framework to approximate the term upper bounds,0,,False
for proximity weighting models that monotonically increase with,0,,False
"respect to the frequency variable, called MaxTF. In that framework,",0,,False
the upper bound t for a term t is computed by using the maximum,0,,False
term frequency fmax (t ) that appears in the term's posting list as,0,,False
"input for the scoring model, i.e.,",0,,False
"t ,"" wt st,d""",0,,False
"fmax (t ) ,",0,,False
where,0,,False
fmax (t ),0,,False
",",0,,False
max,0,,False
d It,0,,False
"ft,d .",0,,False
(5),0,,False
"In our experiments, we exploit the DLH13 weighting model [1]",0,,False
"for #syn operators and simple terms, and the pBiL dependence weighting model [35] for #uw and #1 operators3. Both models",0,,False
are a generalisation of the parameter-free hypergeometric DFR,0,,False
"model in a binomial case, DLH13 for simple terms while pBiL for",0,,False
"n-grams, and both are monotonically increasing with respect to",0,,False
"the frequency variable ft,d . Hence, given a complex term, we can compute a term upper bound by using the maximum term/n-gram",0,,False
frequency computed from the corresponding ephemeral posting,0,,False
"list, as in the MaxTF framework. Unfortunately, even computing",0,,False
these maximum frequencies is impossible without a complete view,0,,False
"of the posting lists, hence we must resort to a further upper bound",0,,False
"on those frequencies, easily computed as follows:",0,,False
"fmax #syn(t1, t2) ,"" fmax (t1) + fmax (t2),""",0,,False
"fmax #1(t1, t2) ,"" min fmax (t1), fmax (t2) ,""",0,,False
(6),0,,False
"fmax #uw(t1, t2) ,"" min fmax (t1), fmax (t2) .""",0,,False
"3 We use these models as they are parameter free, and their MaxTF upper-bound approximations were proven in [26]; However, the method we describe here is equally applicable for scoring simple and complex terms using Dirichlet language modelling. BM25 cannot be applied, as Nt is not available while scoring complex terms.",0,,False
499,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"For instance, consider an AND-based ephemeral posting list (such as #1): no document can have more occurrences of the n-gram than the minimum of the maximum frequencies of the involved terms that has been observed in their posting lists. For the OR-based #syn operator, the worst-case maximum frequency would occur if a single document had the maximum frequencies of the constituent terms. Hence, the maximum frequency within a #syn ephemeral posting list cannot be greater than the sum of the maximum observed frequencies of the respective constituent posting lists.",1,ad,True
"Such term upper bounds, computed by substituting Equations (6) into Equation (5) as required, can be used in dynamic pruning strategies such as MaxScore and WAND, which leverage global upper bounds on each term, both simple and complex. Conversely, query processing strategies such as BMW leverage local term upper bounds: each posting list is split into consecutive blocks of constant size, e.g., 128 postings per block, and, for each block, a score upper bound is computed and stored, together with largest docid of each block. is cannot be done with ephemeral posting lists, since their sequences of postings is not known in advance. Hence, no block score upper bounds can be computed or stored.",1,ad,True
5.3 Predicting Complex Operator E ciency,0,,False
"e time spent processing a query in dynamic pruning strategies depends on several factors, the most important being: (i) the number of terms to be processed, (ii) the total number of postings to be processed and (iii) the relative importance of terms, i.e., their score contribution to the overall relevance of a document [29]. As shown in [19, 21, 29, 38], by using statistics derived from terms and queries it is possible to estimate the query processing time. However, since we must deal with ephemeral posting lists, most of the statistics used in prior research are not applicable. For example, the mean scores of postings (used in [29]) cannot be precomputed for an ephemeral posting list, since the postings and their statistics for all possible n-grams cannot feasibly be computed o ine. Hence, we must resort to ""estimators"" of the quantities of interest, in particular for the total number to be processed, i.e., the simple and ephemeral posting list lengths, and the corresponding term upper bounds.",0,,False
"To provide an upper bound approximation to the number of postings in an ephemeral posting list, consider a generic complex term #op(t1, t2), where t1 and t2 can be simple or (nested) complex terms. If #op corresponds to an OR-based posting list such as #syn, then the size of its posting list cannot be greater than the sum of the document frequencies of its constituent terms. If #op corresponds to an AND-based posting list, then the total size of its postings list cannot be greater than the minimum of the document frequencies of its constituent terms, i.e., the smallest constituent posting list.",0,,False
"For term scores upper bound approximations, we leverage the MaxTF framework, adapted to complex operators, as summarised in Eq. (6). Upper bound approximations are scaled according to the term-speci c weight resulting from query rewriting (see Table 1).",1,ad,True
"We adopt a machine-learned approach to predict the processing times of complex queries, i.e., t ( qi, Kj ), for di erent values of K. As we use the WAND dynamic pruning strategy (as justi ed in Section 2), the response times heavily depend on the K number of documents to be retrieved. Hence, we train a di erent model for each value of K, but all models share the same set of statistics. All previous works on query processing time prediction use of query statistics and aggregations (max, min, mean etc.) of term-based",1,ad,True
"Table 3: Prediction statistics, projectors and aggregators.",0,,False
Statistics,0,,False
1. Number of terms (query-based) 2. Document frequency (term-based) 3. Score upper bound (term-based),0,,False
Projectors,0,,False
"1. Global 2. Original-only terms 3. #syn-only terms 4. #1-only terms 5-... #uw-only terms, one per di erent ",0,,False
Aggregators,0,,False
"1­2. Minimum, Maximum 3­5. Arithmetic, Harmonic, Geometric Means",0,,False
"statistics across the query terms, as reported in Table 3. However, di erent complex operators acting on the same set of terms can have very di erent impacts on the running time of a query. Hence, we propose an additional, intermediate step between statistics generation and aggregation, namely projection. In this step, all query- and term-based statistics are divided into subsets, whose elements are grouped depending on the nature of the term. us we have several statistics projections, one for every complex operator, one for simple terms and one considering all terms globally. Term-based aggregators are then applied to these subsets of statistics. Note that, due to the di erent nature of AND- and OR-based posting lists, we do not consider the sum and variance operators, while we include the minimum operator, as suggested by [19]. In our experiments, we use #uw8 and #uw12 operators, hence we have 6 query-based features and 2×6×5 term-based features, for a total of 66 features. Section 7.1 provides the details and parameters of the learning algorithm using these feature to predict the processing times of complex queries.",1,ad,True
6 EXPERIMENTAL SETUP,0,,False
6.1 Research estions,0,,False
"In the following, we experiment to address two research questions: RQ1: How accurate are our query e ciency predictions for queries with complex operators? (Section 7.1) RQ2: Can we maintain e ectiveness while reducing response time when selectively rewriting queries? (Section 7.2)",1,ad,True
"In the remainder of this section, we de ne the experimental setup under which our experiments are conducted.",0,,False
6.2 Datasets & Retrieval System,0,,False
"Our experiments address both e ciency and e ectiveness, and hence require diverse setups to ensure accurate conclusions can be drawn for both types of measures. All of our experiments are conducted on the TREC ClueWeb09 category B corpus4, which consists of 50M Web documents. For testing e ectiveness, we use the 197 queries from the TREC Web tracks 2009-2012 that have corresponding relevance assessments on a 4-point scale [9], and denote this as WT. For testing e ciency, we follow best practices in",1,ad,True
4 h p://lemurproject.org/clueweb09/,0,,False
500,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"sampling a signi cant number of queries from a real search engine, namely 1,956 successive queries from the MSN 2006 query log5 [11].",0,,False
"We index all 50M documents of the ClueWeb09 corpus using the Terrier IR platform [32], including also the anchor text of incoming hyperlinks to each document. Position information is recorded for each term. Our index is compressed using Elias-Fano encoding provided in [40], widely considered to be the state-of-the-art in terms of fast decompression.",1,ClueWeb,True
"For retrieval, we conduct e ciency timings using a machine equipped with an AMD Opteron Processor 6276 with 6 MB L3 cache and 128 GB RAM. e entire index is loaded in memory. All experiments are performed on a single core. While the resulting retrieval times using a single machine for retrieval are marginally higher than would be expected for interactive retrieval in a deployed Web search engine, following previous work [41], this does not detract from the generality of the ndings, and avoids the complexities of performing experiments in a distributed retrieval environment.",1,ad,True
"Finally, in our timing experiments, we do not include the time to rewrite the query, calculate e ciency predictions, nor to apply the learned model. Each of these stages is comparatively cheap: for instance, the rewriting approaches discussed below are commonly deployed in search engines; query e ciency predictions can be calculated quickly using only term statistics from the lexicon [19, 29]; and the application of a learned model is also relatively less expensive than the top-K retrieval [38].",0,,False
6.3 ery Plans,0,,False
"Besides the None strategy, where the original queries are not rewritten, we deploy the following rewriting approaches:",0,,False
"MRF. To encapsulate proximity in rewriting a query, we deploy sequential dependence [30] proximity, by considering #1, #uw8 and #uw12 query operators for adjacent query terms.",1,ad,True
"Na¨ive. To address word mismatch, we rewrite the query by adding alternatives to query terms within a #syn query operator6. Inspired by Peng et al. [34], who noted corpus analysis as being a suitable method to determine similar words (based on which words they co-occur with). We select alternative terms for a given term t that each have the same stem as t based on Porter's stemmer, and which are among the M most similar terms to t within a word embedding space. We use Deeplearning4j's word2vec tool and word embeddings vectors trained on Wikipedia and the Gigaword corpus7 and select words with common stems in the top M , 20 for each query term t. is results in a less aggressive stemming than either index-time stemming or the equivalent query-side stemming using all alternatives identi ed by a Porter stemmer.",1,ad,True
"Na¨iveMRF. Finally, we mix Na¨ive and MRF rewritings to create a nal strategy of generating time-expensive query plans.",0,,False
"Table 4 reports the statistics of each query set, incl. the number of simple and complex terms generated by each rewriting approach. Finally, with regards to the value of K, i.e., the number of documents retrieved by WAND during the top K retrieval, we select 4 values, namely 20, 100, 1000 and 5000. is gives us a total of 16 query",0,,False
"5 From a sample of 2000 queries, 44 queries had no matching terms in our collection, so were removed. 6 Initial experiments using the context-sensitive approach of [34] showed no e ectiveness bene t over this simpler Na¨ive stemming. 7 Available from h p://nlp.stanford.edu/projects/glove/",1,ad,True
Table 4: ery sets statistics per rewriting.,0,,False
Dataset Train Test WT,1,WT,True
eries Rewriting simple #syn #1 #uw8 #uw12,0,,False
Na¨iveMRF 1458 1125 1133 1133,0,,False
0,0,,False
978,0,,False
MRF Na¨ive,0,,False
2556 0 1578 1578 781,0,,False
1458 1125 0 0,0,,False
0,0,,False
None,0,,False
2556 0 0 0,0,,False
0,0,,False
Na¨iveMRF 1452 1028 1185 1185,0,,False
0,0,,False
978,0,,False
MRF Na¨ive,0,,False
2468 0 1491 1491 768,0,,False
1452 1028 0 0,0,,False
0,0,,False
None,0,,False
2468 0 0 0,0,,False
0,0,,False
Na¨iveMRF 121 126 113 113,0,,False
0,0,,False
197,0,,False
MRF Na¨ive,0,,False
244 0 146 146 82,0,,False
121 126 0 0,0,,False
0,0,,False
None,0,,False
244 0 0 0,0,,False
0,0,,False
Table 5: ery-dependent (QD) & -independent (QI) ranking features within our experiments.,0,,False
"QD DLH13, Coordinate Level 2",0,,False
QD pBiL (term dependence),0,,False
2,0,,False
"QI Inlinks, Outlinks, PageRank 3",0,,False
QI URL features,0,,False
6,0,,False
QI Content quality [3],0,,False
4,0,,False
QI Spam score,0,,False
1,0,,False
Total,0,,False
18,0,,False
"plans to be plugged into our selective mechanism, and for which we need to train and test our e ciency predictors.",0,,False
6.4 Ranking features,0,,False
"As mentioned in Section 5, in the top K retrieval phase, we use the DLH13 term weighting model [1] from the Divergence from Randomness framework for weighting simple and #syn terms; For #1 and #uw terms, we use the DFR pBiL model [35]. Next, Table 5 lists the 18 features used for re-ranking the top K results during the application of the learned model. Note, that regardless of whether term dependency complex operators are deployed as a rewri en query, we include the score for the term dependency operators in the feature set. is allows the learner to consider the term dependence features separately from the score used in the initial ranking phase.",0,,False
"For learning and ranking, we use the Jforests implementation8 of LambdaMART [8], a gradient-boosted decision tree learning-torank technique. E ectiveness experiments on the 197 TREC Web track queries (denoted WT in Table 4) use a 5-fold cross validation, where each fold has 60% training, 20% validation and 20% test queries. We report NDCG@20.",1,ad,True
7 RESULTS,0,,False
"In the following, we report the results and analysis addressing our two research questions concerning the accuracy of our e ciency predictions for queries with complex operators (Section 7.1), and the application of the selective rewriting mechanism based upon those e ciency predictions (Section 7.2).",1,ad,True
8 h ps://github.com/yasserg/jforests/,0,,False
501,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 6: Pearson correlation on the test query set, per rewriting and K value, of the baseline (Base) and the proposed predictors (Pred). Statistically signi cant improvements over the baseline are denoted in bold.",0,,False
Rewriting,0,,False
20,0,,False
K,0,,False
100,0,,False
1000,0,,False
5000,0,,False
Base Pred Base Pred Base Pred Base Pred,0,,False
Na¨iveMRF 0.639 0.833 0.722 0.840 0.827 0.881 0.861 0.868,0,,False
MRF,0,,False
0.612 0.803 0.697 0.813 0.788 0.870 0.790 0.884,0,,False
Na¨ive 0.620 0.848 0.735 0.878 0.828 0.915 0.886 0.935,0,,False
None,0,,False
0.548 0.738 0.691 0.842 0.802 0.907 0.884 0.952,0,,False
7.1 RQ1: E ciency Prediction,0,,False
"We use the 66 e ciency prediction features derived from statistics, projectors and aggregators summarised in Table 3 to train a machine-learned model on the 978 training queries (see Table 4), for every combination of rewriting and K value. e regression algorithm employed is gradient boosted regression trees, as provided by the scikit-learn toolbox. Denoting the 978 train queries as Qtr, we perform a 5-fold cross validation to train each Qitr, Kj model. Each model was trained with 20 trees, learning rate of 0.1, max depth of 5, and the least square loss function. To evaluate the accuracy of each model, we then used the 978 test queries Qte to compute the Pearson correlation, reported in Table 6. We compare each of our models (denoted Pred) with a linear regressor trained using the sum of number of postings per each simple term in the original terms and complex operators (Base). Our models perform very well, with correlations always higher than 0.8, and almost always signi cantly improving over the Base baseline predictor (according to a Fisher Z-transform, p < 0.05). To further assess the quality of our predictors, in Table 7, we compare the mean actual query times and the mean predicted query times for each model. Our models incur a mean prediction error no greater than 13%, even if they tend to underestimate the actual processing time, in particular for the models using the original queries (None).",1,ad,True
"Moreover, since we use the trained model to compare the predicted execution times of query plans versus a given time  , in Table 8 we report the precision/recall measures of our models when classifying queries with a predicted execution time greater than 0.750 seconds. As can be observed from the table, the recall of our models is close or above 0.9 for Na¨iveMRF, MRF and Na¨ive rewritings. e smaller recall value for None is not problematic, since very few queries exhibit an execution time greater than 0.750 seconds.",0,,False
"Finally, we use the feature importance metric within gradient boosted trees to assess the contribution of the 66 prediction features. To combine the feature importances across various models, for each model we rank features by decreasing importance, and measure mean reciprocal ranks (Mean RR) across models. Table 9 shows the top features across all trained models (reported features a ained Mean RR greater than 0.1). All proposed statistics and aggregators appear in the top features, with the exception of geometric mean (which appears, but it is not reported, as next in the list). Notably, also the minimum document frequency across all terms is in the list, validating the assumption that the minimum aggregator is useful",1,ad,True
"Table 7: Average actual and predicted query processing times on the test query set (in ms), per rewriting and K value, with relative errors (in %).",0,,False
Rewriting 20,0,,False
K,0,,False
100,0,,False
1000,0,,False
5000,0,,False
Actual times,0,,False
Na¨iveMRF 2718,0,,False
3272,0,,False
MRF,0,,False
1438,0,,False
1757,0,,False
Na¨ive,0,,False
992,0,,False
1373,0,,False
None,0,,False
374,0,,False
551,0,,False
4244,0,,False
5090,0,,False
2618,0,,False
3310,0,,False
1893,0,,False
2381,0,,False
802,0,,False
1042,0,,False
Predicted times,0,,False
Na¨iveMRF 2618 (-3.65) 3087 (-5.63) 4109 (-3.18) 4608 (-9.46),0,,False
MRF,0,,False
1511 (5.12) 1762 (0.29) 2655 (1.44) 3101 (-6.31),0,,False
Na¨ive,0,,False
953 (-3.92) 1261 (-8.16) 1866 (-1.42) 2339 (-1.78),0,,False
None,0,,False
369 (-1.19) 485 (-12.13) 829 (-3.38) 1029 (-1.29),0,,False
Table 8: Precision/Recall accuracy to classify queries taking more than 750 milliseconds to process.,0,,False
Rewriting,0,,False
20,0,,False
K,0,,False
100,0,,False
1000,0,,False
5000,0,,False
PRPRPRPR,0,,False
Na¨iveMRF 0.752 0.978 0.790 0.991 0.767 1.000 0.828 1.000,0,,False
MRF,0,,False
0.831 0.936 0.843 0.984 0.819 0.996 0.858 1.000,0,,False
Na¨ive 0.769 0.908 0.813 0.935 0.843 0.979 0.867 0.998,0,,False
None,0,,False
0.800 0.520 0.839 0.719 0.836 0.881 0.783 0.963,0,,False
Table 9: Top features ranked by the mean reciprocal rank of importance across all trained models.,0,,False
Aggregator,0,,False
Arithmetic mean Maximum Arithmetic mean Maximum Maximum Maximum Harmonic mean ­ Maximum Arithmetic mean Minimum,0,,False
Feature,0,,False
Document frequency Document frequency Score upper bound Document frequency Document frequency Document frequency Document frequency Number of terms Document frequency Document frequency Document frequency,0,,False
Projector,0,,False
global #syn global original #uw8 global global global #1 original global,0,,False
Mean RR,0,,False
0.404 0.350 0.306 0.257 0.243 0.215 0.185 0.171 0.152 0.129 0.109,0,,False
"when processing AND-based posting lists. Moreover, all projectors appear in the top features list, with the notable exception of #uw12. We explain this by observing that, according to Table 4, the corresponding complex operator occurs infrequently in the processed queries compared with the frequency of other complex operators.",0,,False
"Hence, in addressing RQ1, we have experimentally evaluated the accuracy of our predictions at estimating the execution times of rewri en queries processed using WAND for di erent K. Overall, with correlations > 0.8 for all tested rewritings and values of K, we conclude that our e ciency predictions are indeed accurate.",1,ad,True
502,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
7.2 RQ2: Selective Rewriting,0,,False
"In this section, we experiment to determine the levels of e ciency and e ectiveness obtainable when using the selective mechanism proposed in Section 4, and using the predictors for complex operators evaluated in the preceding section. In terms of setup, our selective mechanism ranks the query plans by measuring µ , NDCG based on the validation set for each fold of the WT queries.",1,WT,True
"We consider the uniform application of None, K ,"" 5000 to all queries as the baseline that we compare to in terms of e ectiveness. In particular, the use of K "","" 5000 for retrieving on ClueWeb09 has been used by various previous work on the same test collection [10, 28] and empirically veri ed in [27]. We denote the uniform application of this query plan as """"Default"""". In our experiments, we aim to be more e cient than Default, while not experiencing a signi cant decrease in e ectiveness. In addition to Default, we also report the e ciency and e ectiveness of the uniform application of the Fastest, Slowest and Most E ective plans.""",1,ClueWeb,True
"Table 10 provides the main ndings of the e ciency and e ectiveness of the uniform query plans. Note that, as discussed in Section 6, we use di erent query sets for measuring e ciency and e ectiveness; in particular, we report mean and 95th percentile (or ""tail"") response times in milliseconds (MRT & TRT, respectively) on the 978 test queries from the MSN 2006 query log; for e ectiveness, we report NDCG@20 for the 197 TREC Web track queries.",1,TREC,True
"e rst group of rows reports e ciency and e ectiveness for the Uniform plans, while the lower group reports the results for the Selection mechanism as threshold  is varied. Finally, for each row, Rw denotes the percentage of queries rewri en from None.",0,,False
"On inspection of the uniform plan in the top half of Table 10, we note that the MRF and Na¨iveMRF uniform plans result in very high response times (up to 4.86 times slower than Default). In terms of e ectiveness, deploying MRF to increase the recall in the sample results in a marked (but not statistically signi cant) increase in NDCG e ectiveness of the system (0.18770.2001), however, this comes at the cost of retrieval that is 3.2 times slower than Default.",0,,False
"Next, we consider the selective mechanism in the bo om half of Table 10. As expected, as  is varied we note changes in both e ectiveness and e ciency. In particular, for  ,"" 500, we nd that a mean response time of 537 ms can be achieved (49% decrease in mean response time, and 62% decrease in tail response time, compared to Default), without signi cantly degrading e ectiveness (0.1877  0.1751). For this se ing, 8-10% of queries are being rewri en. With higher levels of  , we observe similar increases in e ectiveness and observed response times, and with more queries being rewri en. Finally, we note that the selection mechanism does not strictly observe the  threshold, as can be observed by the tail response times ­ this is expected, as the selection mechanism expressed in Equation (3) will default to the predicted fastest plan available (usually None, K "", 20 ) if no plans can be executed within  .",1,ad,True
"To provide a graphical illustration of the e ciency/e ectiveness tradeo , Figure 2 presents mean response times and mean NDCG. Lines are provided for both our selective mechanism, Full, denoted by a solid line, as well as the same selective mechanism where the candidate plans are restricted only to those involving None (i.e., with K ,"" {20, 100, 1000, 5000}), denoted None-only, and indicated by a dashed line. For most thresholds, the Full selective mechanism can be observed to o er the best tradeo , with points closer to the upper le hand corner. For low mean response times (e.g., < 400""",1,ad,True
"Table 10: E ciency/e ectiveness results using the selective mechanism. * denotes NDCG values that signi cantly di er from that of None, K ,"" 5000 (paired t-test, p < 0.05). TRT denotes the tail response time (95%-th percentile), Rw denotes the % of queries rewritten from None. Times are in ms.""",0,,False
Strategy,0,,False
E iciency (Test) MRT TRT Rw,0,,False
Uniform Plans,0,,False
"Default ( None, K ,"" 5000 ) Fastest ( None, K "","" 20 ) Slowest ( Na¨iveMRF, K "","" 5000 ) Most E ective ( MRF, K "", 5000 )",0,,False
1037 5281 0 376 1779 0 5045 17994 100 3281 12099 100,0,,False
Selective Mechanism,0,,False
" , 200  , 300  , 400  , 500  , 600  , 700  , 750  , 800  , 900  , 1000",0,,False
449 1986 1 472 1986 1 495 1986 4 537 2004 8 577 2023 13 606 2060 19 617 2060 21 639 2128 22 691 2226 27 733 2256 30,0,,False
E ectiveness (WT) NDCG@20 Rw,1,WT,True
0.1877,0,,False
0,0,,False
0.1375*,0,,False
0,0,,False
0.1755 100,0,,False
0.2001 100,0,,False
0.1642*,0,,False
0,0,,False
0.1670*,0,,False
1,0,,False
0.1711*,0,,False
6,0,,False
0.1751 10,0,,False
0.1762 14,0,,False
0.1782 18,0,,False
0.1779 19,0,,False
0.1807 19,0,,False
0.1825 21,0,,False
0.1828 21,0,,False
0.180 0.175,0,,False
NDCG@20,0,,False
0.170,0,,False
0.165,0,,False
500,0,,False
600,0,,False
700,0,,False
MRT,0,,False
Sig. Diff. vs. Default  False True,0,,False
Selection Candidates Full None-only,0,,False
"Figure 2: E ciency/e ectiveness tradeo . e best tradeo occurs for points closest to the upper le corner. Points denoted with  are signi cantly less e ective than None, K ,"" 5000 (paired t-test, p < 0.05).""",1,ad,True
"ms), the None-only line rises above Full, as only None plans can be deployed to achieve such low response times. On the other hand, for larger mean response times (e.g., > 650 ms), the Full line is more e ective; this supports a central tenet of our work, i.e., when time allows, appropriate rewriting of queries results in increased e ectiveness. Moreover, as indicated by the  points of the None-only",0,,False
503,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"line in Figure 2, to achieve the response times savings without selectively rewriting the query, we would be forced to accept signi cant degradations in e ectiveness compared to the Default baseline.",1,ad,True
"Finally, to give a avour of the impact of our selective mechanism, we inspect the queries rewri en for  ,"" 500 and select the query `disneyland hotel'. is query was rewri en to use the plan MRF, K "","" 100 (as per the proximity example in Table 1), which had a predicted execution time of 486 ms, due to the relatively informative term disneyland (which only appears in Nt "","" 78422 documents). Hence, the more e ective MRF rewrite was applied, which resulted in a 14% increase in NDCG@20 compared the Default plan (which had a predicted execution time of 542 ms).""",1,ad,True
"Overall, in addressing RQ2 we have determined that our selective mechanism can achieve a 49% decrease in mean response time, and 62% decrease in tail (95th-percentile) response time without signi cant degradations in e ectiveness.",1,ad,True
8 CONCLUSIONS,0,,False
"is work has considered the selective rewriting of web search queries, with the aim of a aining e cient retrieval without hindering the system's e ectiveness. In particular, we showed that it is possible to accurately measure the response time of a search engine in answering a query with complex operators such as #syn and #1, even when using the WAND dynamic pruning strategy. Our detailed experimental setup involved experiments upon the open TREC ClueWeb09 dataset, using TREC Web track queries for measuring e ectiveness in terms of NDCG@20, and real search engine user queries for measuring e ciency. Moreover, we deployed three strategies to rewrite each query. Our experiments showed not only the accuracy of the newly proposed query e ciency predictions for queries involving complex operators, but also the ability of our proposed selective mechanism to enhance e ciency bene ts (a 49% decrease in mean response time, and 62% decrease in tail, i.e., 95th-percentile, response time) without any signi cant degradation in mean NDCG@20. Overall, our results demonstrate that by selectively rewriting the query (when there is the time to execute the rewri en query), e ectiveness can be at least maintained while markedly bene ting response times. Our proposed selective rewriting mechanism can be further extended, for instance to more query rewriting techniques, such as those based on query reformulation and query-click pa erns [12, 20], and modelling the e ectiveness of rewriting plans through risk rather than mean e ectiveness alone.",1,TREC,True
REFERENCES,0,,False
"[1] Gianni Amati. 2006. Frequentist and Bayesian Approach to IR. In ECIR. 13­24. [2] Vo Ngoc Anh, Owen de Kretser, and Alistair Mo at. 2001. Vector-space ranking",0,,False
"with e ective early termination. In SIGIR. 35­42. [3] Michael Bendersky, W. Bruce Cro , and Yanlei Diao. 2011. ality-biased ranking",0,,False
"of web documents. In WSDM. 95­104. [4] Michael Bendersky, Donald Metzler, and W. Bruce Cro . 2010. Learning Concept",0,,False
"Importance Using a Weighted Dependence Model. In WSDM. 31­40. [5] Daniele Broccolo, Craig Macdonald, Salvatore Orlando, Iadh Ounis, Ra aele",1,ad,True
"Perego, Fabrizio Silvestri, and Nicola Tonello o. 2013. Load-sensitive Selective Pruning for Distributed Search. In CIKM. 379­388. [6] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya So er, and Jason Y. Zien. 2003. E cient query evaluation using a two-level retrieval process. In CIKM. 426­434. [7] Chris Buckley, Gerard Salton, James Allan, and Amit Singhal. 1995. Automatic query expansion using SMART: TREC 3. In TREC 3. 69­80. [8] Christopher J.C. Burges. 2010. From RankNet to LambdaRank to LambdaMART: An Overview. Technical Report MSR-TR-2010-82.",1,ad,True
"[9] Charles L. A. Clarke, Nick Craswell, and Ellen M. Voorhees. 2012. Overview of the TREC 2012 Web Track. In TREC.",1,TREC,True
"[10] Nick Craswell, Dennis Fe erly, Marc Najork, Stephen Robertson, and Emine Yilmaz. 2010. Microso Research at TREC 2009. In TREC.",1,TREC,True
"[11] Nick Craswell, Rosie Jones, Georges Dupret, and Evelyne Viegas (Eds.). 2009. Proceedings of the Web Search Click Data Workshop at WSDM 2009.",0,,False
[12] Nick Craswell and Martin Szummer. 2007. Random walks on the click graph. In SIGIR. 239­246.,0,,False
"[13] W. Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice. Addison-Wesley Publishing Company.",0,,False
"[14] J. Shane Culpepper, Charles L. A. Clarke, and Jimmy Lin. 2016. Dynamic Cuto Prediction in Multi-Stage Retrieval Systems. In ADCS. 17­24.",0,,False
[15] Je rey Dean. 2009. Challenges in building large-scale information retrieval systems: invited talk. In WSDM.,0,,False
"[16] Je rey Dean and Luiz Andr Barroso. 2013. e Tail at Scale. Commun. ACM 56 (2013), 74­80.",0,,False
[17] Shuai Ding and Torsten Suel. 2011. Faster top-k document retrieval using blockmax indexes. In SIGIR. 993­1002.,0,,False
"[18] Marcus Fontoura, Vanja Josifovski, Jinhui Liu, Srihari Venkatesan, Xiangfei Zhu, and Jason Y. Zien. 2011. Evaluation Strategies for Top-k eries over Memory-Resident Inverted Indexes. PVLDB 4, 12 (2011), 1213­1224.",0,,False
"[19] Myeongjae Jeon, Saehoon Kim, Seung-won Hwang, Yuxiong He, Sameh Elnikety, Alan L. Cox, and Sco Rixner. 2014. Predictive Parallelization: Taming Tail Latencies in Web Search. In SIGIR. 253­262.",0,,False
"[20] Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. 2006. Generating ery Substitutions. In WWW. 387­396.",1,ad,True
"[21] Saehoon Kim, Yuxiong He, Seung-won Hwang, Sameh Elnikety, and Seungjin Choi. 2015. Delayed-Dynamic-Selective (DDS) Prediction for Reducing Extreme Tail Latency in Web Search. In WSDM. 7­16.",0,,False
"[22] Nicholas Lester, Alistair Mo at, William Webber, and Justin Zobel. 2005. SpaceLimited Ranked ery Evaluation Using Adaptive Pruning. In WISE. 470­477.",0,,False
"[23] Jimmy Lin, Ma Crane, Andrew Trotman, Jamie Callan, Ishan Cha opadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward Reproducible Baselines: e Open-Source IR Reproducibility Challenge. In ECIR. 408­420.",1,ad,True
"[24] Xiaolu Lu, Alistair Mo at, and J. Shane Culpepper. 2015. On the Cost of Extracting Proximity Features for Term-Dependency Models. In CIKM. 293­302.",0,,False
"[25] Claudio Lucchese, Franco Maria Nardini, Salvatore Orlando, Ra aele Perego, Nicola Tonello o, and Rossano Venturini. 2015. ickScorer: A Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees. In SIGIR. 73­82.",0,,False
"[26] Craig Macdonald, Iadh Ounis, and Nicola Tonello o. 2011. Upper-bound Approximations for Dynamic Pruning. ACM Trans. Inf. Syst. 29, 4 (2011), 17:1­17:28.",1,ad,True
"[27] Craig Macdonald, Rodrygo LT Santos, and Iadh Ounis. 2013. e whens and hows of learning to rank for web search. Information Retrieval 16, 5 (2013), 584­628.",1,ad,True
"[28] Craig Macdonald, Rodrygo L.T. Santos, Iadh Ounis, and Ben He. 2013. About Learning Models with Multiple ery-dependent Features . ACM Trans. Inf. Syst. 31, 3 (2013), 11:1­11:39.",1,ad,True
"[29] Craig Macdonald, Nicola Tonello o, and Iadh Ounis. 2012. Learning to predict response times for online query scheduling. In SIGIR. 621­630.",1,ad,True
[30] Donald Metzler and W. Bruce Cro . 2005. A Markov Random Field Model for Term Dependencies. In SIGIR. 472­479.,0,,False
"[31] Giuseppe O aviano, Nicola Tonello o, and Rossano Venturini. 2015. Optimal Space-time Tradeo s for Inverted Indexes. In WSDM. 47­56.",1,ad,True
"[32] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and Christina Lioma. 2006. Terrier: A High Performance & Scalable IR Platform. In OSIR.",1,ad,True
"[33] Jan Pederson. 2010. ery Understanding at Bing. In Invited Talk, SIGIR 2010 Industry Day.",0,,False
"[34] Fuchun Peng, Nawaaz Ahmed, Xin Li, and Yumao Lu. 2007. Context Sensitive Stemming for Web Search. In SIGIR. 639­646.",0,,False
"[35] Jie Peng, Craig Macdonald, Ben He, Vassilis Plachouras, and Iadh Ounis. 2007. Incorporating Term Dependency in the DFR Framework. In SIGIR. 843­844.",1,ad,True
[36] Eric Shurman and Jake Brutlag. 2009. Performance related changes and their user impacts. In Velocity: Web Performance and Operations Conference.,0,,False
"[37] Trevor Strohman, Howard Turtle, and W. Bruce Cro . 2005. Optimization Strategies for Complex eries. In SIGIR. 219­225.",0,,False
"[38] Nicola Tonello o, Craig Macdonald, and Iadh Ounis. 2013. E cient and E ective Retrieval Using Selective Pruning. In WSDM. 63­72.",1,ad,True
"[39] Howard Turtle and James Flood. 1995. ery evaluation: Strategies and optimizations. Information Processing & Management 31, 6 (1995), 831 ­ 850.",0,,False
"[40] Sebastiano Vigna. 2013. asi-succinct indices. In WSDM. 83­92. [41] Lidan Wang, Jimmy Lin, and Donald Metzler. 2010. Learning to E ciently Rank.",0,,False
In SIGIR. 138­145. [42] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global,0,,False
Document Analysis. In SIGIR. 4­11.,0,,False
504,0,,False
,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Navigating Imprecision in Relevance Assessments on the Road to Total Recall: Roger and Me,1,ad,True
Gordon V. Cormack,0,,False
University of Waterloo gvcormac@uwaterloo.ca,0,,False
ABSTRACT,0,,False
"Technology-assisted review (""TAR"") systems seek to achieve ""total recall""; that is, to approach, as nearly as possible, the ideal of 100% recall and 100% precision, while minimizing human review effort. The literature reports that TAR methods using relevance feedback can achieve considerably greater than the 65% recall and 65% precision reported by Voorhees as the ""practical upper bound on retrieval performance . . . since that is the level at which humans agree with one another"" (Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness, 2000). This work argues that in order to build--as well as to, evaluate--TAR systems that approach 100% recall and 100% precision, it is necessary to model human assessment, not as absolute ground truth, but as an indirect indicator of the amorphous property known as ""relevance."" The choice of model impacts both the evaluation of system effectiveness, as well as the simulation of relevance feedback. Models are presented that better fit available data than the infallible ground-truth model. These models suggest ways to improve TAR-system effectiveness so that hybrid human-computer systems can improve on both the accuracy and efficiency of human review alone. This hypothesis is tested by simulating TAR using two datasets: the TREC 4 AdHoc collection, and a dataset consisting of 401,960 email messages that were manually reviewed and classified by a single individual, Roger, in his official capacity as Senior State Records Archivist. The results using the TREC 4 data show that TAR achieves higher recall and higher precision than the assessments by either of two independent NIST assessors, and blind adjudication of the email dataset, conducted by Roger, more than two years after his original review, shows that he could have achieved the same recall and better precision, while reviewing substantially fewer than 401,960 emails, had he employed TAR in place of exhaustive manual review.",1,TREC,True
1 INTRODUCTION,1,DUC,True
"This study contributes to the body of empirical evidence showing that hybrid human-computer classification systems (known in the legal community as ""technology-assisted review""",0,,False
"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5022-8/17/08. https://doi.org/10.1145/3077136.3080812",1,ad,True
Maura R. Grossman,0,,False
University of Waterloo maura.grossman@uwaterloo.ca,0,,False
"or ""TAR"") can be more effective and more efficient than exhaustive manual review by experts, where effectiveness is measured with respect to an independent gold standard. The results amplify and extend our 2011 work, TechnologyAssisted Review in E-Discovery Can Be More Effective and More Efficient Than Exhaustive Manual Review [14] in the following ways:",0,,False
" We rigorously specify and evaluate a semi-automated process for human-in-the-loop classification in which the only human input is an initial query, followed by assessment of the documents selected for review by the system, until the system determines that high recall and precision have been achieved, and that the review process is complete;",0,,False
" We extend the process with a quality-control (""QC"") mechanism, in which the system suggests a subset of the documents for further adjudication, either by the user or another assessor, to mitigate the fallibility of the user's original assessments;",1,ad,True
" We present a theory of information retrieval (""IR"") system evaluation that extends the Cranfield method [30] to define the end-to-end effectiveness of an interactive IR process, and to model and control for dependencies between the assessments rendered by the human in the loop, and the assessments used to evaluate the result;",0,,False
" Using the TREC 4 AdHoc collection and the alternate assessments used by Voorhees in Variations in Relevance Judgments and the Measurement of Retrieval Effectiveness [29], we provide evidence that our proposed TAR method achieves substantially better recall and precision than the the alternate NIST assessors would have achieved, had they reviewed the entire collection, with a small fraction of the effort;",1,TREC,True
" And finally, using a complete categorization of 401,960 email messages from the administration of Virginia Governor Tim Kaine, which was previously manually reviewed by Senior State Records Archivist Roger Christman (""Roger""), we show, using subsequent assessments rendered by Roger, that Roger could have achieved the same recall and higher precision, for a fraction of the effort, had he employed our TAR method to review the 401,960 email messages.",1,ad,True
"The following sections develop the theory of how to measure the end-to-end effectiveness of high-recall and high-precision IR efforts, how to simulate a human in the loop, our experimental design, and our results on the TREC 4 and Kaine email datasets.",1,TREC,True
5,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 THEORY,0,,False
"It is well understood that the notion of ""relevance"" is imprecise, and that different assessors--or even the same assessor at different times--may provide inconsistent relevance determinations for the same document, regardless of their knowledge and expertise, or the specificity with which ""relevance"" is defined. Nevertheless, it has been observed that relevance determinations by different assessors, while different, are essentially interchangeable as ground truth for the purposes of measuring the relative effectiveness of ad-hoc retrieval systems [3, 29]. In this work, we consider the problem of measuring the end-to-end effectiveness of ""total-recall"" methods, where the goal is to find substantially all relevant documents, and where the overall accuracy in determining relevance rivals that of the user, or any individual assessor. The model for a user is a ""dedicated searcher, not a novice searcher,"" who is ""willing to look at many documents"" in order to find as much relevant information as possible (from TREC-1 [16]). This objective is shared by many critical applications, including electronic discovery in civil litigation, archiving of business or government records, patent search, and systematic review in evidence-based medicine. In 2015 and 2016, the TREC Total Recall Track [10, 15] addressed the total-recall problem, providing to participants a ""Baseline Model Implementation"" (""BMI""),1 simulating a TAR method known as ""continuous active learning"" (""CAL"") (cf. [9, 11]).",1,ad-hoc,True
"The ideal result of a ""total-recall"" IR effort is to identify all and only the relevant documents in a collection; that is, to achieve 100% recall and 100% precision. In practice, the ability to reach this goal is limited by the fallibility of human relevance assessment. Even if it were feasible to assess every document in the collection, a certain number of the resulting assessments would be incorrect, yielding less than 100% recall and less than 100% precision. Relevance assessments generated by a learned classifier would also be fallible, likewise falling short of 100% recall and precision. This article addresses the question: Can hybrid human-computer assessments yield higher recall and precision--with less effort--than human assessments alone?",1,ad,True
"To answer this question, it is necessary to estimate recall and precision, or another measure of how nearly all and only the relevant documents have been identified. The traditional Cranfield method for IR evaluation [30] offers limited insight because it relies on comparison with a ""gold standard"" for relevance, which itself relies on fallible assessments. At high levels of recall and precision, the Cranfield method tends to measure the ability of the method under test to reproduce the flaws in the gold standard, which--if the flaws are random--is impossible, and if the flaws are systematic--is possible only for methods with similar flaws.",1,ad,True
"Measuring the effectiveness of total recall is further complicated by the fact that most high-recall methods involve a human in the loop, and are influenced by that user's fallible assessments. In the simplest ""ranked-retrieval"" scenario, the system orders all documents in the collection by their",0,,False
1 http://cormack.uwaterloo.ca/trecvm/.,1,trec,True
"likelihood of relevance, and the user examines them in order, until a sufficient number of relevant documents have been identified. In the ""relevance-feedback"" scenario, the user's assessment is communicated to the system, which uses this information to revise the ranking of the yet-to-be-examined documents. The ""active-learning"" or ""uncertainty-sampling"" scenario departs from relevance-feedback scenario in that the documents presented to the user are in the order most useful for machine learning, as opposed to likelihood of relevance, with the effect that the user is typically directed to the most marginally relevant documents to examine.",0,,False
"Regardless of the scenario, it is important to define precisely the circumstance under which a document is considered to be ""identified"" by the method. In the ""system-recall "" scenario, a document is deemed to be identified when it is presented to the user, regardless of the user's ultimate relevance assessment. In the ""end-to-end-recall "" scenario, a document is deemed to be identified only when it is presented to the user and the user judges it to be relevant. Where the user is fallible, system recall will generally be higher than end-to-end recall, while system precision will be lower. Which scenario is more apt depends on whether the role of the user is simply to provide guidance to the system, or to make the ultimate determination of whether a document is relevant or not.",0,,False
"Quality-control (""QC"") procedures seek to mitigate the impact of fallible relevance assessments, using one or more supplemental assessments for some or all of the documents. In perhaps the easiest case, a second assessment might be rendered for each document. Where the assessments agree, it would be reasonable to assume that they are likely (but not certainly) both correct; where the assessments disagree, one is likely correct and the other is likely not--but how do we know which is which? One might defer to the second assessment, if it could be ascertained that it was more likely to be correct than the first, perhaps due to the application of additional care, or greater knowledge and skill on the part of the second assessor. One might defer to the ""relevant"" assessment if high recall were particularly important, and to the ""not relevant"" assessment if high precision were important. Alternatively, one might defer to a third assessment, effectively deeming the ""majority vote"" to be correct.",1,ad,True
"Majority-vote QC incurs overhead of (1 + ) additional assessments, where  is the number of documents in the collection, and  is the rate of discord between the first and second assessments. This overhead may be reduced by selecting a subset of    documents for supplemental assessment. If the subset is a statistical sample, it is possible to quantify, but not to substantially mitigate, the fallibility of the first assessment. If a subset can be identified that includes many of the documents with discordant assessments, deferring to a third assessment for those particular documents can provide mitigation approaching that of majority vote, with considerably lower overhead.",1,ad,True
"This work distinguishes between total-recall methods and search tools. At the outset, TREC sought to measure the ease",1,TREC,True
6,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
with which search tools might be used within the context of a total-recall effort [16]:,0,,False
"It should be assumed that the users need the ability to do both high precision and high recall searches, and are willing to look at many documents and repeatedly modify queries in order to get high recall. Obviously they would like a system that makes this as easy as possible.",0,,False
"To this end, relevance-based measures of search-tool effectiveness--notably, rank-based measures such as (mean) average precision (""(M)AP""), precision at a fixed cutoff (""P@k""), and R-precision (""P@R,"" where R is the number of relevant documents in the collection)--were used as proxies for ease of use [18]. With certain exceptions, primarily in the legal, intellectual property, and medical domains, interest within TREC and the IR community has generally shifted to more user-centric contexts, where the goal is to satisfy an ephemeral and user-specific information need, and search-tool effectiveness is quantified by proxy measures for user satisfaction, cf. [1].",1,AP,True
"Regardless of the context or proxy measure, evaluation efforts like those characterized by Voorhees [29, 30] have focused on search-tool (i.e., ""system"") effectiveness, not the overall effectiveness of the user at using the tool to identify as nearly as practicable all and only the relevant documents, where ""relevance"" is defined by extrinsic criteria (i.e., ""endto-end"" effectiveness). The method of repeated ad-hoc search envisioned by TREC is commonly used, but is far from the only--or necessarily the most effective--total-recall method. In some domains, such as the curation of government archival records, exhaustive manual review by an expert constitutes the de facto standard of acceptable practice. In many contexts, a single query is used to identify the subset of the collection for manual review. In Boolean retrieval, the query specifies precisely the subset to be reviewed; in ranked retrieval, the query suggests the nature of relevance, the search tool ranks the documents by their likelihood of relevance, and the user assesses some number of the top-ranked documents. Traditionally, relevance feedback has been construed as a method to automate the query-formulation task envisioned by TREC: The user's assessment of the results from an initial query are provided to the search tool, which reformulates the query and presents a new set of results to the user, and so on. More recently, supervised machine-learning methods have been used to harness relevance feedback, with reported effectiveness apparently exceeding Voorhees' ""practical upper bound,"" see e.g., [6, 14, 25].",1,ad-hoc,True
3 MODELING ASSESSMENT ERROR,0,,False
3.1 Assessment Error in Measurement,0,,False
"For the purposes of this study, we assume that every document  is either ""relevant"" or ""not relevant"" in its own right (()  {, } ), but its relevance can be observed only indirectly by an assessment under conditions  yielding a positive or negative judgment ((, )  {+, -}). We use the abbreviations , , +, and - to denote () ,"" ,""",0,,False
"() ,"" , (, ) "","" +, and (, ) "","" -, respectively. We""",0,,False
"assume that for a random document , a positive judg-",0,,False
ment is evidence of relevance: Pr[|+] > Pr[]. It,0,,False
follows that a negative judgment is evidence of non-relevance:,0,,False
Pr[|-] > Pr[]. One of the principal questions,0,,False
to be addressed by a model is: How strong is this evidence?,1,ad,True
The Cranfield method generally assumes for the pur-,0,,False
pose of evaluation that human assessments are infallible,0,,False
"(Pr[|+] ,"" 1, Pr[|-]) "","" 0), where  is chosen""",0,,False
"carefully, considering the myriad of factors that influence",1,ad,True
relevance assessment.,0,,False
Biased sampling and/or statistical sampling may be used,0,,False
to to reduce the cost of assessment. The pooling method [18],0,,False
is the most prominent of a family of biased sampling methods,0,,False
"that identify a subset of documents for human assessment,",0,,False
and render automatic judgments for the remaining documents.,0,,False
"In the pooling method, each document  is either in the pool",0,,False
"or not (()  {, }); documents in the pool",0,,False
"are assessed, while documents not in the pool are summarily",0,,False
deemed not relevant. The pooling method can be viewed,0,,False
as a semi-automated assessment under conditions  where,0,,False
{,0,,False
"(, ) ,"" (, ) () . Biased sampling methods""",0,,False
-,0,,False
(),0,,False
"place further stress on the Cranfield assumption that (, )",0,,False
is infallible.,0,,False
Statistical sampling may be used to estimate the pro-,0,,False
portion of relevant documents in particular subsets of the,0,,False
"collection, as necessary to compute summary measures of",0,,False
effectiveness [2].,0,,False
Multiple assessments per document may be used in place,0,,False
of a single assessment. The majority judgment of a  as-,0,,False
sessments under conditions 1 ...  will more closely ap-,0,,False
"proximate an infallible assessor, under the assumption that",0,,False
there is greater than 50% conditional probability that each,0,,False
"judgment will be positive for a relevant document, and nega-",0,,False
"tive for a non-relevant document, notwithstanding the other",0,,False
judgments.,0,,False
"Where multiple fallible assessments are available, latent",0,,False
class analysis [21] may be used to infer the true positive rate,0,,False
"Pr[+|] and false positive rate Pr[+|] for each of the assessment conditions, as well as prevalence Pr[],",0,,False
under the assumption of pairwise conditional independence:,0,,False
"Pr[+|()] ,"" Pr[+|(), ( , )] for all  "",  .",0,,False
3.2 Assessment Error in Simulation,0,,False
"Total-recall methods may require relevance assessment for three purposes: (i) To train the system to rank or classify the remaining documents; (ii) to determine the ultimate disposition of each document presented to the user by the system (e.g., produce or withhold in the context of electronic discovery in civil litigation, include or exclude in the context of systematic review in evidence-based medicine); and (iii) to inform the cost-benefit analysis inherent in determining when to stop the review process. For the purposes of this work, we aspire to emulate a user whose fallible assessments are conditionally independent of those used for evaluation. The",0,,False
7,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"fallible assessments used for evaluation, although they may closely emulate those of a user, would confound evaluation were they also to be used to simulate feedback, as they are not conditionally independent. Infallible assessments-- if they existed--would be conditionally independent, but a poor emulation of a real user's feedback. In either case, it is desirable to use a separate set of assessments to emulate user feedback. Even so, it is well known that the order of presentation and the proportion of relevant documents can influence human assessment [24, 27, 28]; such influences are not easily controlled when simulating different total-recall methods. The quest for better models to emulate human assessment is met with a triple challenge: (i) determining the true relevance of a document; (ii) aptly modeling the user's response; and (iii) ensuring the model is conditionally independent of the model used for evaluation.",0,,False
3.3 When to Stop?,0,,False
"An important but rarely studied issue in achieving total recall is when to stop. Blair and Maron [4] reported that users who terminated their searches when they believed they had achieved at least 75% recall, had in fact achieved 20% recall. Eliciting from the user a reliable judgment of when high recall has been achieved remains a vexing problem. Evaluations styled after the TREC AdHoc task have largely finessed this issue, reporting rank-based measures under the assumption that the user would know when to stop, perhaps after reading a fixed number of documents.",1,ad,True
"Automated methods show some promise, but have not previously been evaluated in terms of end-to-end recall with a human in the loop, where user feedback is independent of the assessments used for evaluation. Cormack and Grossman [6] have reported statistical and non-statistical methods for ensuring, with high probability, that their continuous active learning (""CAL"") method achieves very high recall, at the expense of precision. The non-statistical ""knee method"" searches for an inflection point in the recall-effort curve--the ""knee""--and continues well beyond that point. Empirical evidence, based on an assumption of infallible user feedback, suggests that their knee method can achieve system-level recall of over 90%, with more than 95% probability, with precision considerably less than 50%.",0,,False
4 EXPERIMENTAL DESIGN,0,,False
"We conducted two experiments to test the hypothesis that total-recall systems with human assessors in the loop could achieve comparable--or higher--recall and precision, with a small fraction of the effort, than an expert assessor who examined every document in the collection. For this effect to be observable, it is necessary to depart from a model assuming infallible assessment, at least with respect to human assessors in the loop. For evaluation, it is necessary to have a source of reasonably authoritative assessments separate from those used for relevance feedback.",0,,False
"4.1 Datasets, Topics, and Assessments",0,,False
"Our first experiment simulated participation in the TREC 4 AdHoc Task, using the TREC 4 test collection consisting of 567,528 documents, 49 topics, and the official NIST gold standard of relevance [17]. For user feedback and QC, we used two alternate sets of judgments obtained by NIST, for the same topics, using assessors distinct from those who created the gold standard--the same set of alternate assessments studied by Voorhees [29].",1,TREC,True
"Our second experiment reprised the exhaustive manual review of 401,960 email messages from the administration of former Virginia Governor Tim Kaine, which was undertaken by Senior State Archivist Roger Christman, prior to the publication, in 2014, of those he deemed to be ""open records.""2 To simulate user feedback in our experiment, we used Roger's original assessments (the ""Roger I"" assessments). To simulate QC, Roger re-reviewed blind a stratified sample of 2,798 documents (the ""Roger II"" assessments). As the ultimate arbiter of truth, Roger reviewed blind, for a third time, all 901 cases of disagreement between Roger I and Roger II (the ""Roger III"" assessments).",1,ad,True
4.2 Total-Recall Methods,0,,False
"Our simulation used the same TREC Total Recall Baseline Model Implementation (""BMI""), referenced above in Section 2, modified to read the dataset, topics, and simulated relevance assessments from local files, instead of a server, and to implement Cormack and Grossman's ""knee-method"" stopping criterion [6]. BMI runs autonomously and has no tunable parameters: Our input consisted of the datasets, topics, relevance assessments, and the knee method. Output from the BMI runs consisted of: a ranked list of documents, in the order presented to the simulated user, ending where the knee-method stopping criterion was met; and the inflection point (""knee"") in the gain curve, determined retrospectively by the knee method.",1,TREC,True
"The output from BMI was further manipulated to simulate three different result-selection strategies: (i) SystemDetermined, (ii) User-Determined, and (iii) Adjudicated. The end result of the System-Determined strategy was the entire ranked list returned by BMI. The end result of the UserDetermined strategy was the subset of documents in the ranked list that the user judged to be relevant. The end result of the Adjudicated strategy was the subset of the ranked list consisting of those documents that the user and the knee method agreed were relevant, or, where the user and knee method disagreed, a second, auxiliary assessment deemed to be relevant. For the purposes of this work, we deemed the knee method to judge all documents in the ranked list before the knee to be relevant, and all documents after the knee to be non-relevant.",0,,False
2See http://www.virginiamemory.com/collections/kaine/.,0,,False
8,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Strategy Manual System,0,,False
User Adjudicated Adj. - Man.,0,,False
-value,0,,False
Recall 0.57 0.94 0.55 0.64 0.07 0.0001,0,,False
Precision 0.69 0.06 0.81 0.82 0.13,0,,False
0.0002,0,,False
1 0.63 0.10 0.62 0.69 0.06 0.0001,0,,False
"Effort 567,528 22,911 22,911",0,,False
"23,662 543,866",0,,False
-,0,,False
Table 1: Average Effectiveness Measures Over 98 Combinations of 49 TREC 4 Topics and Two Simulated Users.  was computed using a paired t-test.,1,TREC,True
4.3 Evaluation,0,,False
"For all strategies, we report recall, precision, and 1 , as well as effort, as measured by the number of documents presented to the user for assessment. As a baseline, we used an Exhaustive Manual Review (""Manual Review"") strategy, for which effort is simply the number of documents in the collection. For the System-Determined strategy, effort depends on the number of relevant documents in the collection, as well as the recall and precision achieved: For a given topic and recall level, effort is inversely proportional to precision. For the User-Determined and Adjudicated strategies, there is no direct relationship between effort and precision.",0,,False
"Only the System-Determined strategy returns a ranked list from which we can evaluate the recall-precision tradeoff. We can, however, plot recall and precision as a function of effort throughout the progress of a review. These curves illustrate the result that might have been achieved, had a different stopping criterion been applied. They also illustrate how quickly a substantial fraction of the relevant documents can be discovered and forwarded for further analysis or release, while the total-recall effort is still in progress.",1,ad,True
4.4 Prediction and Rationale,0,,False
"Previously published results report recall-precision breakeven scores on the order of 80% for BMI and related methods [7, 8, 11, 25]. With one notable exception (discussed below), these results were derived using simulated feedback from an assumed-infallible user, and evaluated with respect to the same assumed-infallible gold standard. On the one hand, the simulated feedback was conditionally dependent on the evaluation standard, and therefore possibly ""too good to be true."" On the other hand, the evaluation standard was assumed to be perfect, offering BMI no opportunity to better it. The experiments, by design, could not show whether or not BMI could achieve better recall and/or better precision than a fallible user.",0,,False
"There is no basis to assume that a hybrid human-computer system cannot exceed both the recall and precision of its human operator. The literature reports inter-assessor agreement results that are consistent with the hypothesis that a human assessor can achieve on the order of 70% recall and 70% precision [26, 29]. Are the higher results reported for BMI an artifact of too-perfect training, or is a system involving",0,,False
"BMI and human assessment, combined, superior to human assessment alone?",0,,False
"Achieving a high recall-precision break-even score is irrelevant to the success of a total-recall effort, if the point at which this score is achieved is unknown to the user. Cormack and Grossman's knee-method stopping criterion sacrifices (System-Determined) precision to achieve very high recall, under the assumption that an infallible assessor would screen the results, and the only consequence of low precision would be increased effort. The User-Determined strategy has the (fallible) user act in this capacity.",0,,False
"The Adjudicated strategy has BMI and the user share the role of screening, deferring to a second (fallible) assessor the adjudication of cases of disagreement. We assume the inflection point calculated by the knee method to be a good approximation of the recall-precision break-even point; that documents before the knee are more likely to be relevant than not, and that documents after the knee are less likely to be so. This assumption motivates our choice to defer to a second assessor any document before the knee that is judged non-relevant by the user, and any document after the knee that is judged relevant by the user. This strategy makes no assumption that the second assessor is ""better"" than the user. As long as the second assessor is usually correct (as would certainly be the case for an assessor capable of achieving 70% recall and 70% precision), the Adjudicated strategy should achieve higher recall and higher precision than the User-Determined strategy.",1,ad,True
"As noted above, one strand of research has evaluated totalrecall methods in the face of fallible users. The TREC Legal Track Interactive Task (see, e.g., [19, 23]) assigned participating teams the task of finding all and only the relevant documents that were responsive to requests for production in a mock civil litigation. A subject matter expert (the ""Topic Authority"") was made available for consultation while the teams were conducting their reviews; the same Topic Authority adjudicated cases of disagreement, after the fact, between the teams and the human assessors who had created a provisional gold standard for evaluation. Teams were allowed to use any method of their choosing, with no restriction on the nature or quantity of human input. Two TAR methods--one rule-based and one substantially similar to BMI--achieved on the order of 80% recall and 80% precision [19]. In a subsequent analysis, Grossman and Cormack [14] estimated the recall and precision of the human assessments that comprised the provisional gold standard, on average, to have been 59.3% and 31.7%, respectively. While deferring a fuller discussion of these results to Section 5, we note that this work generated some criticism, e.g., [12, 13, 31]; most notably, claims that: (i) the assessors were unskilled, poorly trained, poorly vetted, or poorly supervised; (ii) the assessors had a different ""conception of relevance"" from the Topic Authority; (iii) the participating teams devoted extraordinary skill or extraordinary resources to accomplishing the task; (iv) the gold standard, by virtue of the reconsideration process, was biased in favor of the participants; and (v) the gold standard, by",1,TREC,True
9,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
System-Determined vs. User-Determined Recall and Precision 1,0,,False
System-Determined vs. User-Determined Recall and Precision 1,0,,False
0.8,0,,False
0.8,0,,False
Recall (Precision),0,,False
Recall (Precision),0,,False
0.6,0,,False
0.6,0,,False
System recall System prec.,0,,False
User recall User prec.,0,,False
System recall System prec.,0,,False
User recall User prec.,0,,False
0.4,0,,False
0.4,0,,False
0.2,0,,False
0.2,0,,False
0,0,,False
0,0,,False
0,0,,False
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000,0,,False
0,0,,False
Review Effort (Documents Reviewed),0,,False
User-Determined vs. Adjudicated Recall and Precision,0,,False
1,0,,False
1,0,,False
1000,0,,False
2000,0,,False
3000,0,,False
4000,0,,False
5000,0,,False
Review Effort (Documents Reviewed),0,,False
6000,0,,False
User-Determined vs. Adjudicated Recall and Precision,0,,False
7000,0,,False
8000,0,,False
0.8,0,,False
0.8,0,,False
Recall (Precision),0,,False
Recall (Precision),0,,False
0.6,0,,False
0.6,0,,False
User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
0.4,0,,False
0.4,0,,False
0.2,0,,False
0.2,0,,False
0,0,,False
0,0,,False
0,0,,False
2000,0,,False
4000,0,,False
6000,0,,False
8000,0,,False
10000,0,,False
12000,0,,False
0,0,,False
1000,0,,False
2000,0,,False
3000,0,,False
4000,0,,False
5000,0,,False
6000,0,,False
7000,0,,False
8000,0,,False
9000,0,,False
Review Effort (Documents Reviewed),0,,False
Review Effort (Documents Reviewed),0,,False
"Figure 1: TREC 4 Topic 239 ­ Tradeoff Between Recall, Precision, and Effort. The top panels compare the System-Determined vs. User-Determined strategies; the bottom panels compare the User-Determined vs. Adjudicated strategies. For the left panels, the first alternate TREC assessor was the user, and the second alternate assessor was the adjudicator; for the right panels, the roles were reversed.",1,TREC,True
"virtue of bias on the part of the Topic Authority and the lack of blinding of his or her review, was biased in favor of the participants.",0,,False
"This study controls for the skill, effort, and motivation of both users and assessors. The gold standard for the TREC 4 collection, as well as the alternate assessments, were fixed more than two decades ago. All of the NIST assessors were clearly skilled in their craft; most were former NSA analysts. The alternate assessors had no direct knowledge of the primary assessor's judgments. On the other hand, the simulated assessments for the Kaine email dataset were derived from an assessment of 401,960 documents, by the Virginia Senior State Records Archivist, in his official capacity. In forming the gold standard, the same archivist reviewed and then re-reviewed some of his own previous assessments, after wash-out periods of two years and then two months, respectively.",1,TREC,True
Our rationale predicted that: (i) BMI alone (the SystemDetermined strategy) would achieve superior recall to Manual,0,,False
"Review, but inferior precision, for substantially less effort; (ii) the User-Determined strategy would achieve inferior recall, but superior precision, to the System-Determined strategy, for the same effort; and, (iii) the Adjudicated strategy would achieve superior recall and precision to all other strategies, for moderately higher effort than the System-Determined and User-Determined strategies, but still substantially less than Manual Review.",0,,False
5 RESULTS,0,,False
"Figure 1 plots recall and precision for a representative TREC 4 topic as a function of effort for each of the BMI-derived methods, using each of the alternate assessors as the user, and the other assessor, as occasioned, for adjudication.3 In comparison, the recall and precision of the Manual Review by",1,TREC,True
3Plots and raw results for the other 48 topics are available on request from the authors.,0,,False
10,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Topic Legal Hold,0,,False
Archival Restricted,0,,False
Manual Review,0,,False
Recall Precision 1 0.97 0.91 0.94,0,,False
0.89,0,,False
0.84 0.86,0,,False
0.98 0.75 0.84,0,,False
"Effort 401,960 381,819 146,594",0,,False
Recall 0.96 0.90 0.95,0,,False
Adjudicated Precision 1,0,,False
0.96 0.96 0.89 0.89 0.80 0.87,0,,False
"Effort 40,522 332,410 38,048",0,,False
Table 3: Individual Topic Effectiveness for the Kaine Email Dataset.,0,,False
Roger I,0,,False
Roger II,0,,False
rel,0,,False
nrel,0,,False
Roger I,0,,False
rel,0,,False
"16,640",0,,False
"PP 1,736 1,76P5 PP",0,,False
nrel,0,,False
PP 474 227 PPP,0,,False
P,0,,False
"381,065",0,,False
P,0,,False
Roger II,0,,False
rel,0,,False
nrel,0,,False
rel nrel,0,,False
"33,482115,57717,269 7,193 198,40923,824",0,,False
Roger II,0,,False
rel,0,,False
nrel,0,,False
Roger I,0,,False
rel,0,,False
"23,050",0,,False
"PP 3,661 296 PPP",0,,False
nrel,0,,False
"PP 1,135 7,53P6 PP",0,,False
P,0,,False
"130,821",0,,False
P,0,,False
"Table 2: Agreement Among Roger I, Roger II, and Roger III on the Virginia Tech Legal Hold Emails. In split cells, numbers below the diagonal show agreement between Roger I and Roger III; numbers above the diagonal show agreement between Roger II and Roger III. The top panel shows Virginia Tech legal hold identification; the middle panel shows archival record identification; the bottom panel shows restricted record identification.",0,,False
Strategy Manual Adjud.,0,,False
 -value 95% c.i.,0,,False
"Recall 0.95 0.93 -0.01 0.4 (-.05, .03)",0,,False
Precision 0.83 0.88 +0.05 0.006,0,,False
"(.03, .07)",0,,False
"1 0.88 0.91 +0.02 0.03 (.004, .04)",0,,False
"Effort 310,196 136,993 173,203",0,,False
Table 4: Average Effectiveness Measures Over Three Topics for the Kaine Email Dataset.  was computed using a paired t-test.,0,,False
Roger I & Roger II System & Roger I System & Roger II,0,,False
Legal Hold 80.6% 79.1% 79.9%,0,,False
Archival 60.2% 70.2% 62.1%,0,,False
Restricted 64.2% 67.9% 55.8%,0,,False
"Table 5: Pairwise Overlap (i.e., Jaccard Index) Between the System, Roger I, and Roger II.",0,,False
System-Determined vs. User-Determined Recall and Precision 1,0,,False
0.8,0,,False
Recall (Precision),0,,False
0.6,0,,False
0.4,0,,False
0.2,0,,False
0 0,0,,False
1,0,,False
5000,0,,False
10000,0,,False
15000,0,,False
20000,0,,False
25000,0,,False
Review Effort (Documents Reviewed),0,,False
System recall System prec.,0,,False
User recall User prec.,0,,False
30000,0,,False
35000,0,,False
User-Determined vs. Adjudicated Recall and Precision,0,,False
40000,0,,False
0.8,0,,False
Recall (Precision),0,,False
0.6,0,,False
0.4,0,,False
0.2,0,,False
0 0,0,,False
5000,0,,False
10000,0,,False
User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
15000 20000 25000 30000 Review Effort (Documents Reviewed),0,,False
35000,0,,False
40000,0,,False
45000,0,,False
"Figure 2: Identification of Kaine Administration Email Pertaining to the Virginia Tech Shooting for Legal Hold ­ Tradeoff Among Recall, Precision, and Effort. The top panel compares the SystemDetermined vs. User-Determined strategies; the bottom panel compares the User-Determined vs. Adjudicated strategies.",1,ad,True
"the two assessors were 0.34 and 0.88, and 0.59 and 0.94, respectively. Although the first assessor has substantially lower recall and precision than the second, the System-Determined recall curves are remarkably similar. The User-Determined recall curves are, as predicted, bounded by the recall of the respective users. On the other hand, the Adjudicated recall curves, like the System-Determined curves, are remarkably",0,,False
11,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
System-Determined vs. User-Determined Recall and Precision 1,0,,False
System-Determined vs. User-Determined Recall and Precision 1,0,,False
0.8,0,,False
0.8,0,,False
Recall (Precision),0,,False
Recall (Precision),0,,False
0.6,0,,False
0.6,0,,False
0.4,0,,False
0.4,0,,False
0.2,0,,False
0 0,0,,False
1,0,,False
50000,0,,False
100000,0,,False
150000,0,,False
200000,0,,False
Review Effort (Documents Reviewed),0,,False
System recall System prec.,0,,False
User recall User prec.,0,,False
250000,0,,False
User-Determined vs. Adjudicated Recall and Precision,0,,False
300000,0,,False
0.2,0,,False
0 0,0,,False
1,0,,False
5000,0,,False
System recall System prec.,0,,False
User recall User prec.,0,,False
10000,0,,False
15000,0,,False
20000,0,,False
25000,0,,False
Review Effort (Documents Reviewed),0,,False
30000,0,,False
User-Determined vs. Adjudicated Recall and Precision,0,,False
35000,0,,False
0.8,0,,False
0.8,0,,False
Recall (Precision),0,,False
Recall (Precision),0,,False
0.6,0,,False
0.6,0,,False
0.4,0,,False
0.4,0,,False
0.2,0,,False
0 0,0,,False
50000,0,,False
User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
100000,0,,False
150000,0,,False
200000,0,,False
250000,0,,False
Review Effort (Documents Reviewed),0,,False
300000,0,,False
350000,0,,False
"Figure 3: Identification of Archival Records from the Kaine Administration ­ Tradeoff Among Recall, Precision, and Effort. The top panel compares the System-Determined vs. User-Determined strategies; the bottom panel compares the User-Determined vs. Adjudicated strategies.",1,ad,True
0.2,0,,False
0 0,0,,False
5000,0,,False
10000,0,,False
15000,0,,False
20000,0,,False
25000,0,,False
Review Effort (Documents Reviewed),0,,False
User recall User prec. Adjudicated recall Adjudicated prec.,0,,False
30000,0,,False
35000,0,,False
40000,0,,False
"Figure 4: Identification of Restricted Archival Records from the Kaine Administration ­ Tradeoff Among Recall, Precision, and Effort. The top panel compares the System-Determined vs. UserDetermined strategies; the bottom panel compares the User-Determined vs. Adjudicated strategies.",1,ad,True
"similar, but are superior to both the User-Determined curves. The System-Determined precision curve initially climbs and then declines with increased effort, as expected, while the User-Determined and Adjudicated precision curves are remarkably flat.",0,,False
"Table 1 shows average effectiveness and effort measures over 98 runs, comprising 49 topics and two simulated users. As predicted, the System-Determined strategy achieves very high recall on average, with 4% of the effort of Manual Review. The User-Determined strategy achieves slightly lower recall, but substantially higher precision than Manual Review, also with 4% of the effort. The Adjudicated strategy achieves substantially and significantly higher recall, and precision than Manual Review, with 4.2% of the effort.",0,,False
"The agreement between Roger I and Roger II, and Roger III's adjudication of their disagreements, is shown in Table 2.",1,ad,True
"Overall, Roger I and Roger II have overlap (i.e., Jaccard index) of 80.6%, 60.2%, and 64.2% on each of three topics-- higher than most reported results for separate assessors, but far from perfect. Roger III generally splits the difference between Roger I and Roger II, with a propensity to agree with the negative assessment. Roger, on completing the Roger III assessments, volunteered that ""this was a challenging review,"" suggesting that the adjudication process had identified many hard-to-classify, as opposed to randomly misclassified, documents.",1,ad,True
"Roger I rendered these decisions for each of the three topics seriatim as follows: First, the Virginia Tech documents subject to a legal hold were identified; second, documents not subject to the hold were classified as either archival records or non-records; and finally, documents classified as archival records were categorized as restricted or open records. As a consequence, the document collection diminished for each",0,,False
12,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"subsequent topic. Roger II and Roger III employed the same protocol, resulting in a handful of anomalous judgments. For example, Roger I classified some records as records or non-records not subject to legal hold, while Roger II classified them as subject to legal hold. For these documents, we recorded the disagreement with respect to legal hold, and asked Roger II to specify whether the document would be an archival record or a non-record, were it not subject to legal hold. Roger III was asked in advance to consider all six combinations of: subject to legal hold or not, and open record, restricted record, or non-record. The appropriate hypothetical judgments were used as the gold standard for each topic.",1,ad,True
"The documents reviewed by Roger II formed a stratified sample of the dataset; measures using Roger II or Roger III were estimated using the Horvitz-Thompson estimator [20]. The strata were selected as follows: For each of the three topics and each of the four possible modes of disagreement, 200 documents were selected independently, at random. Because the documents were selected independently, there was some overlap among these strata, and the total number of unique documents was 2,398. After Roger II had commenced his review, it was discovered that one stratum had been repeated and one had been omitted due to a clerical error, so 200 documents from the omitted stratum were added, along with 200 randomly selected documents from outside the stratum, for a total sample size of 2,798. Inclusion probabilities were adjusted to account for the overlapping strata.",1,ad,True
"Figures 2, 3, and 4 show effectiveness versus effort for the Adjudication strategy, while Table 3 shows set-based measures for the three Kaine email topics, and Table 4 shows averages over the topics. The summary measures suggest that the Adjudication strategy achieves significantly better precision and 1 than Roger I ( < 0.05), and no significant difference in recall. The gain in 1 appears to come primarily from balancing recall and precision, which is consistent with the purpose of the Adjudication strategy. None of the differences is larger than 0.5%, suggesting that there is little to choose (in terms of effectiveness) between the Adjudication strategy and Manual Review. On the other hand, the Adjudication strategy offers a huge advantage in terms of efficiency.",1,ad,True
"Table 5 shows the pairwise overlap between the system's assessment (that was compared to Roger I's assessment in the Adjudication strategy), and Roger I and Roger II themselves. Collectively, the results indicate that, for all intents and purposes, there is little to choose between the system's judgments and Roger's, and that a second opinion--whether by a human or a bionic assessor--can be helpful.",0,,False
"The bootstrap method was used to determine the variance due to sampling in the Roger II and Roger III datasets, which showed all per-topic differences between the Adjudication strategy and Manual Review to be significant (with respect to sampling uncertainty).",0,,False
DISCUSSION AND CONCLUSIONS,0,,False
"Effectiveness measures for total recall depend on who you ask, when you ask, and how often you ask. Even a small amount of error in gold-standard assessments can substantially depress recall, as evidenced by the provisional versus final recall estimates of the TREC 2009 Legal Track, where the estimated recall of the best submissions for four topics rose from less than 20% in the Notebook Draft, to about 80% in the Final Overview Paper [19, appendix]. While Webber et al. [32] attribute this difference to bias on the part of the assessors, it is equally well explained by a typical true positive rate Pr[+| > 0.7], and a typical false positive rate Pr[+|]  0.01. The difference between the Legal Track assessment and other TREC efforts is that the assessors reviewed a statistical sample of the entire collection, not just the pool of documents identified by the systems. As a consequence, a sample representing more than 700,000 non-relevant documents was reviewed; it is no surprise that examples representing several thousand of these non-relevant documents were incorrectly judged as relevant. Most of those false positives were identified by a process similar to the Adjudication strategy we evaluated, in which disagreements between the participating systems and the first assessor were adjudicated by a second assessor, the Topic Authority. It is not necessary to assume that the first assessor was incompetent, or that the Topic Authority was more competent than the first assessor, to explain the TREC 2009 results.",1,TREC,True
"Our Adjudication strategy results on the Kaine email dataset are consistent with the superior results reported by Cormack and Mojdeh at TREC 2009 [5]; they ""[re-]examined documents with high scores that were marked `not relevant' and documents with low scores that were marked `relevant.'"" Our results are also entirely consistent with the results reported in our original 2011 study [14]. It is important to bear in mind that results measuring system recall, rather than end-to-end recall, cannot be compared to the results reported here, to those reported in the TREC 2009 Legal Track Overview [19], or to those reported by Grossman and Cormack [14]. Neither can such results be compared when the user assessments are the same as the evaluation assessments.",1,TREC,True
"Our results do not support the mantra of ""garbage in, garbage out,"" or that errors in user feedback are ""amplified"" by the use of a TAR method, as opposed to manual review. To the contrary, our results show that system effectiveness is hardly affected by inferior feedback, and that certain TAR methods can mitigate rather than amplify user error.",0,,False
"This study raises a number of questions that may be addressed by future work. It is well known--and reconfirmed by this study--that humans judge the same document differently under different circumstances, including the order of presentation. The effect of using a dynamically learned ranking on user feedback has yet to be studied. On the one hand, studies suggest that when assessors review a higher proportion of relevant documents, they are less likely to judge them relevant (see, e.g., Roegiest [24]). Is this explained by a higher error rate, or by the general observation that higher",1,ad,True
13,0,,False
Session 1A: Evaluation 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"prevalence or more experience with a review set can lead assessors to become more discriminating? Our results show that Roger III was less likely to judge documents relevant than Roger I or Roger II, perhaps because he exercised greater diligence, or maybe because he had become better informed through the course of examining borderline documents.",1,ad,True
"Roger II and Roger III were blind to the previous Rogers' assessments. While Roger II commented that he recalled several of the themes in the documents, it had been at least two years since he had previously reviewed them. Roger III, on the other hand, had seen the documents two months prior, and in the interim, had reviewed more than ten thousand emails from a different department of the Kaine administration. It is not apparent what effect Roger's memory may have had on the results. The impact of blind review with a wash-out period has yet to be studied; indeed, it is not clear whether the user should be blind, so as to reduce bias, or informed, so as to aid in deliberation, cf. [22]. At TREC 2009, Cormack and Mojdeh [5] appear to have achieved superior results without blinding and no discernible wash-out period, but more study is necessary to arrive at a definitive answer.",1,ad,True
"Overall, our results reconfirm the thesis that hybrid humancomputer classification (i.e., TAR) methods can achieve recall and precision that compare favorably with exhaustive manual review by experts, for much less effort. Where higher recall and precision is desired, additional resources are better spent re-reviewing documents that may have been misjudged by the user, than examining the ranked list to extraordinary depths, or sampling low-ranked documents. When recall and precision values approach 100%, it is essential to consider carefully both the accuracy and independence of the gold standard used for evaluation.",1,ad,True
ACKNOWLEDGEMENT,0,,False
"We are very grateful for the enthusiasm and support we received from our colleagues at the Library of Virginia, most notably, Roger Christman, Susan Gray Page, Rebecca Morgan, and Kathy Jordan; without them, this work would not have been possible.",0,,False
REFERENCES,0,,False
"[1] A. Al-Maskari and M. Sanderson. A review of factors influencing user satisfaction in information retrieval. Journal of the American Society for Information Science and Technology, 61(5):859­868, 2010.",0,,False
"[2] J. A. Aslam, V. Pavlu, and E. Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR 2006.",0,,False
"[3] P. Bailey, N. Craswell, I. Soboroff, P. Thomas, A. de Vries, and E. Yilmaz. Relevance assessment: are judges exchangeable and does it matter? In SIGIR 2008.",0,,False
"[4] D. Blair and M. E. Maron. An evaluation of retrieval effectiveness for a full-text document-retrieval system. Communications of the ACM, 28(3):289­299, 1985.",0,,False
"[5] G. Cormack and M. Mojdeh. Machine learning for information retrieval: TREC 2009 Web, Relevance Feedback and Legal Tracks. In TREC 2009.",1,TREC,True
[6] G. V. Cormack and M. R. Grossman. Engineering quality and reliability in technology-assisted review. In SIGIR 2016.,0,,False
[7] G. V. Cormack and M. R. Grossman. Evaluation of machinelearning protocols for technology-assisted review in electronic discovery. In SIGIR 2014.,0,,False
[8] G. V. Cormack and M. R. Grossman. Multi-faceted recall of continuous active learning for technology-assisted review. In SIGIR 2015.,0,,False
[9] G. V. Cormack and M. R. Grossman. Scalability of continuous active learning for reliable high-recall text classification. In CIKM 2016.,0,,False
[10] G. V. Cormack and M. R. Grossman. Waterloo (Cormack) participation in the TREC 2015 Total Recall Track. In TREC 2015.,1,TREC,True
"[11] G. V. Cormack and M. R. Grossman. Autonomy and reliability of continuous active learning for technology-assisted review. arXiv:1504.06868, 2015.",0,,False
"[12] G. V. Cormack, M. D. Smucker, and C. L. Clarke. Efficient and effective spam filtering and re-ranking for large web datasets. Information Retrieval, 14(5):441­465, 2011.",0,,False
"[13] S. Green and M. Yacano. Computers vs. humans? Putting the TREC 2009 study in perspective. New York Law Journal, (Oct. 1), 2012.",1,TREC,True
"[14] M. R. Grossman and G. V. Cormack. Technology-assisted review in e-discovery can be more effective and more efficient than exhaustive manual review. Richmond Journal of Law & Technology, 17(3), 2011.",0,,False
"[15] M. R. Grossman, G. V. Cormack, and A. Roegiest. TREC 2016 Total Recall Track Overview. In TREC 2016.",1,TREC,True
"[16] D. Harman. Overview of the First Text REtrieval Conference (TREC-1). In TREC 1, 1992.",1,TREC,True
"[17] D. Harman. Overview of the fourth text retrieval conference (trec-4). In TREC 4, 1996.",1,trec,True
"[18] D. K. Harman. The TREC ad hoc experiments. In E. M. Voorhees and D. K. Harman, editors, TREC - Experiment and Evaluation in Information Retrieval, chapter 4. MIT Press, 2005.",1,TREC,True
"[19] B. Hedin, S. Tomlinson, J. R. Baron, and D. W. Oard. Overview of the TREC 2009 Legal Track. In TREC 2009.",1,TREC,True
"[20] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663­685, 1952.",0,,False
"[21] K. Krstovski. Efficient Inference, Search and Evaluation for Latent Variable Models of Text with Applications to Information Retrieval and Machine Translation. University of Massachusetts, 2016.",0,,False
"[22] T. McDonnell, M. Lease, T. Elsayad, and M. Kutlu. Why is that relevant? Collecting annotator rationales for relevance judgments. In 4th HCOMP, 2016.",1,ad,True
"[23] D. W. Oard, B. Hedin, S. Tomlinson, and J. R. Baron. Overview of the TREC 2008 Legal Track. In TREC 2008.",1,TREC,True
[24] A. Roegiest and G. V. Cormack. Impact of review-set selection on human assessment for text classification. In SIGIR 2016.,0,,False
"[25] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. TREC 2015 Total Recall Track Overview. In TREC 2015.",1,TREC,True
"[26] H. L. Roitblat, A. Kershaw, and P. Oot. Document categorization in legal electronic discovery: Computer classification vs. manual review. Journal of the American Society for Information Science and Technology, 61(1):70­80, 2010.",0,,False
"[27] M. Sanderson et al. Test collection based evaluation of information retrieval systems. Foundations and Trends in Information Retrieval, 4(4):247­375, 2010.",0,,False
"[28] L. Schamber. Relevance and information behavior. Annual review of information science and technology (ARIST), 29:3­48, 1994.",0,,False
"[29] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Information Processing & Management, 36(5), 2000.",0,,False
"[30] E. M. Voorhees. The philosophy of information retrieval evaluation. In Workshop of the Cross-Language Evaluation Forum for European Languages, pages 355­370. Springer, 2001.",0,,False
"[31] W. Webber. Re-examining the effectiveness of manual review. In Proc. SIGIR Information Retrieval for E-Discovery Workshop, 2011.",0,,False
"[32] W. Webber, D. W. Oard, F. Scholer, and B. Hedin. Assessor error in stratified evaluation. In CIKM 2010.",0,,False
14,0,,False
,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Relevance-based Word Embedding,0,,False
Hamed Zamani,0,,False
Center for Intelligent Information Retrieval College of Information and Computer Sciences,0,,False
"University of Massachuse s Amherst Amherst, MA 01003 zamani@cs.umass.edu",0,,False
ABSTRACT,0,,False
"Learning a high-dimensional dense representation for vocabulary terms, also known as a word embedding, has recently a racted much a ention in natural language processing and information retrieval tasks. e embedding vectors are typically learned based on term proximity in a large corpus. is means that the objective in well-known word embedding algorithms, e.g., word2vec, is to accurately predict adjacent word(s) for a given word or context. However, this objective is not necessarily equivalent to the goal of many information retrieval (IR) tasks. e primary objective in various IR tasks is to capture relevance instead of term proximity, syntactic, or even semantic similarity. is is the motivation for developing unsupervised relevance-based word embedding models that learn word representations based on query-document relevance information. In this paper, we propose two learning models with di erent objective functions; one learns a relevance distribution over the vocabulary set for each query, and the other classi es each term as belonging to the relevant or non-relevant class for each query. To train our models, we used over six million unique queries and the top ranked documents retrieved in response to each query, which are assumed to be relevant to the query. We extrinsically evaluate our learned word representation models using two IR tasks: query expansion and query classi cation. Both query expansion experiments on four TREC collections and query classi cation experiments on the KDD Cup 2005 dataset suggest that the relevance-based word embedding models signi cantly outperform state-of-the-art proximity-based embedding models, such as word2vec and GloVe.",1,ad,True
KEYWORDS,0,,False
"Word representation, neural network, embedding vector, query expansion, query classi cation",0,,False
"ACM Reference format: Hamed Zamani and W. Bruce Cro . 2017. Relevance-based Word Embedding. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080831",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080831",1,ad,True
W. Bruce Cro,0,,False
Center for Intelligent Information Retrieval College of Information and Computer Sciences,0,,False
"University of Massachuse s Amherst Amherst, MA 01003 cro @cs.umass.edu",0,,False
1 INTRODUCTION,1,DUC,True
"Representation learning is a long-standing problem in natural language processing (NLP) and information retrieval (IR). e main motivation is to abstract away from the surface forms of a piece of text, e.g., words, sentences, and documents, in order to alleviate sparsity and learn meaningful similarities, e.g., semantic or syntactic similarities, between two di erent pieces of text. Learning representations for words as the atomic components of a language, also known as word embedding, has recently a racted much a ention in the NLP and IR communities.",0,,False
"A popular model for learning word representation is neural network-based language models. For instance, the word2vec model proposed by Mikolov et al. [24] is an embedding model that learns word vectors via a neural network with a single hidden layer. Continuous bag of words (CBOW) and skip-gram are two implementations of the word2vec model. Another successful trend in learning semantic word representations is employing global matrix factorization over word-word matrices. GloVe [28] is an example of such methods. A theoretical relation has been discovered between embedding models based on neural network and matrix factorization in [21]. ese models have been demonstrated to be e ective in a number of IR tasks, including query expansion [11, 17, 40], query classi cation [23, 41], short text similarity [15], and document model estimation [2, 31].",0,,False
"e aforementioned embedding models are typically trained based on term proximity in a large corpus. For instance, the word2vec model's objective is to predict adjacent word(s) given a word or context, i.e., a context window around the target word. is idea aims to capture semantic and syntactic similarities between terms, since semantically/syntactically similar words o en share similar contexts. However, this objective is not necessarily equivalent to the main objective of many IR tasks. e primary objective in many IR methods is to model the notion of relevance [20, 34, 43]. In this paper, we revisit the underlying assumption of typical word embedding methods, as follows:",1,ad,True
e objective is to predict the words observed in the documents relevant to a particular information need.,0,,False
"is objective has been previously considered for developing relevance models [20], a state-of-the-art (pseudo-) relevance feedback approach. Relevance models try to optimize this objective given a set of relevant documents for a given query as the indicator of user's information need. In the absence of relevance information, the top ranked documents retrieved in response to the query are assumed to be relevant. erefore, relevance models, and in general all pseudo-relevance feedback models, use an online se ing to obtain training data: retrieving documents for the query and",0,,False
505,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"then using the top retrieved documents in order to estimate the relevance distribution. Although relevance models have been proved to be e ective in many IR tasks [19, 20], having a retrieval run for each query to obtain the training data for estimating the relevance distribution is not always practical in real-world search engines. We, in this paper, optimize a similar objective in an o ine se ing, which enables us to predict the relevance distribution without any retrieval runs during the test time. To do so, we consider the top retrieved documents for millions of training queries as a training set and learn embedding vectors for each term in order to predict the words observed in the top retrieved documents for each query. We develop two relevance-based word embedding models. e rst one, the relevance likelihood maximization model (RLM), aims to model the relevance distribution over the vocabulary terms for each query, while the second one, the relevance posterior estimation model (RPE), classi es each term as relevant or non-relevant to each query. We provide e cient learning algorithms to train these models on large amounts of training data. Note that our models are unsupervised and the training data is generated automatically.",1,LM,True
"To evaluate our models, we performed two sets of extrinsic evaluations. In the rst set, we focus on the query expansion task for ad-hoc retrieval. In this set of experiments, we consider four TREC collections, including two newswire collections (AP and Robust) and two large-scale web collections (GOV2 and ClueWeb09 - Cat. B). Our results suggest that the relevance-based embedding models outperform state-of-the-art word embedding algorithms. e RLM model shows be er performance compared to RPE in the context of query expansion, since the goal is to estimate the probability of each term given a query and this distribution is not directly learned by the RPE model. In the second set of experiments, we focus on the query classi cation task using the KDD Cup 2005 [22] dataset. In this extrinsic evaluation, the relevance-based embedding models again perform be er than the baselines. Interestingly, the query classi cation results demonstrate that the RPE model outperforms the RLM model, for the reason that in this task, unlike the query expansion task, the goal is to compute the similarity between two query vectors, and RPE can learn more accurate embedding vectors with less training data.",1,ad-hoc,True
2 RELATED WORK,0,,False
"Learning a semantic representation for text has been studied for many years. Latent semantic indexing (LSI) [8] can be considered as early work in this area that tries to map each text to a semantic space using singular value decomposition (SVD), a well-known matrix factorization algorithm. Subsequently, Clinchant and Perronnin [5] proposed Fisher Vector (FV), a document representation framework based on continuous word embeddings, which aggregates a non-linear mapping of word vectors into a document-level representation. However, a number of popular IR models, such as BM25 and language models, o en signi cantly outperform the models that are based on semantic similarities. Recently, extremely e cient word embedding algorithms have been proposed to model semantic similarly between words.",0,,False
"Word embedding, also known as distributed representation of words, refers to a set of machine learning algorithms that learn high-dimensional real-valued dense vector representation w  Rd",0,,False
"for each vocabulary term w, where d denotes the embedding dimensionality. GloVe [28] and word2vec [24] are two well-known word embedding algorithms that learn embedding vectors based on the same idea, but using di erent machine learning techniques.",0,,False
"e idea is that the words that o en appear in similar contexts are similar to each other. To do so, these algorithms try to accurately predict the adjacent word(s) given a word or a context (i.e., a few words appeared in the same context window). Recently, Rekabsaz et al. [30] proposed to exploit global context in word embeddings in order to avoid topic shi ing.",1,ad,True
"Word embedding representations can be also learned as a set of parameters in an end-to-end neural network model. For instance, Zamani et al. [39] trained a context-aware ranking model in which the embedding vectors of frequent n-grams are learned using click data. More recently, Dehghani et al. [9] trained neural ranking models with weak supervision data (i.e., a set of noisy training data automatically generated by an existing unsupervised model) that learn word representations in an end-to-end ranking scenario.",0,,False
"Word embedding vectors have been successfully employed in several NLP and IR tasks. Kusner et al. [16] proposed word mover's distance (WMD), a function for calculating semantic distance between two documents, which measures the minimum traveling distance from the embedded vectors of individual words in one document to the other one. Zhou et al. [47] introduced an embeddingbased method for question retrieval in the context of community question answering. Vulic´ and Moens [37] proposed a model to learn bilingual word embedding vectors from document-aligned comparable corpora. Zheng and Callan [46] presented a supervised embedding-based technique to re-weight terms in the existing IR models, e.g., BM25. Based on the well-de ned structure of language modeling framework in information retrieval, a number of methods have been introduced to employ word embedding vectors within this framework in order to improve the performance in IR tasks. For instance, Zamani and Cro [40] presented a set of embedding-based query language models using the query expansion and pseudo-relevance feedback techniques that bene t from the word embedding vectors. ery expansion using word embedding has been also studied in [11, 17, 35]. All of these approaches are based on word embeddings learned based on term proximity information. PhraseFinder [14] is an early work using term proximity information for query expansion. Mapping vocabulary terms to HAL space, a low-dimensional space compared to vocabulary size, has been used in [4] for query modeling.",1,corpora,True
"As is widely known in the information retrieval literature [11, 38], there is a big di erence between the unigram distribution of words on sub-topics of a collection and the unigram distribution estimated from the whole collection. Given this phenomenon, Diaz et al. [11] recently proposed to train word embedding vectors on the top retrieved documents for each query. However, this model, called local embedding, is not always practical in real-word applications, since the embedding vectors need to be trained during the query time. Furthermore, the objective function in local embedding is based on term proximity in pseudo-relevant documents.",0,,False
"In this paper, we propose two models for learning word embedding vectors, that are speci cally designed for information retrieval needs. All the aforementioned tasks in this section can potentially bene t from the vectors learned by the proposed models.",0,,False
506,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
3 RELEVANCE-BASED EMBEDDING,0,,False
"Typical word embedding algorithms, such as word2vec [24] and GloVe [28], learn high-dimensional real-valued embedding vectors based on the proximity of terms in a training corpus, i.e., cooccurrence of terms in the same context window. Although these approaches could be useful for learning the embedding vectors that can capture semantic and syntactic similarities between vocabulary terms and have shown to be useful in many NLP and IR tasks, there is a large gap between their learning objective (i.e., term proximity) and what is needed in many information retrieval tasks. For example, consider the query expansion task and assume that a user submi ed the query ""dangerous vehicles"". One of the most similar terms to this query based on the typical word embedding algorithms (e.g., word2vec and GloVe) is ""safe"", and thus it would get a high weight in the expanded query model. e reason is that the words ""dangerous"" and ""safe"" o en share similar contexts. However, expanding the query with the word ""safe"" could lead to poor retrieval performance, since it changes the meaning and the intent of the query.",1,ad,True
"is example together with many others have motivated us to revisit the objective used in the learning process of word embedding algorithms in order to obtain the word vectors that be er match with the needs in IR tasks. e primary objective in many IR tasks is to model the notion of relevance. Several approaches, such as the relevance models proposed by Lavrenko and Cro [20], have been proposed to model relevance. Given the successes achieved by these models, we propose to learn word embedding vectors based on an objective that ma ers in information retrieval. e objective is to accurately predict the terms that are observed in a set of relevant documents to a particular information need.",0,,False
"In the following subsections, we rst describe our neural network architecture, and then explain how to build a training set for learning relevance-based word embeddings. We further introduce two models, relevance likelihood maximization (RLM) and relevance posterior estimation (RPE), with di erent objectives using the described neural network.",1,LM,True
3.1 Neural Network Architecture,0,,False
We use a simple yet e ective feed-forward neural network with a,0,,False
single linear hidden layer. e architecture of our neural network,0,,False
is shown in Figure 1. e input of the model is a sparse query,0,,False
"vector qs with the length of N , where N denotes the total number of vocabulary terms. is vector can be obtained by a projection",0,,False
function given the vectors corresponding to individual query terms.,0,,False
"In this paper, we simply consider average as the projection function.",0,,False
"Hence, qs",0,,False
",",0,,False
1 |q |,0,,False
"w q ew , where ew and |q| denote the one-hot",0,,False
"vector representation of term w and the query length, respectively.",0,,False
e hidden layer in this network maps the given query sparse vector,0,,False
"to a query embedding vector q, as follows:",0,,False
"q , qs × WQ",0,,False
(1),0,,False
where WQ  RN ×d is a weight matrix for estimating query embedding vectors and d denotes the embedding dimensionality. e,0,,False
output layer of the network is a fully-connected layer given by:,0,,False
 (q × Ww + bw ),0,,False
(2),0,,False
query sparse vector,0,,False
hidden layer,0,,False
qs,0,,False
output layer,0,,False
W1 W2 W3,0,,False
......... .........,0,,False
d neurons,0,,False
WN,0,,False
N neurons,0,,False
Figure 1: e relevance-based word embedding architecture. e objective is to learn d-dimensional distributed represen-,0,,False
"tation for words based on the notion of relevance, instead of term proximity. N denotes the total number of vocabulary terms.",1,ad,True
where Ww  Rd×N and bw  R1×N are the weight and the bias matrices for estimating the probability of each term.  is the activation function which is discussed in Sections 3.3 and 3.4.,0,,False
"To summarize, our network contains two sets of embedding parameters, WQ and Ww . e former aims to map the query into the ""query embedding space"", while the la er is used to estimate the weights of individual terms.",0,,False
3.2 Modeling Relevance for Training,0,,False
"Relevance feedback has been shown to be highly e ective in improving retrieval performance [7, 32]. In relevance feedback, a set of relevant documents to a given query is considered for estimating accurate query models. Since explicit relevance signals for a given query are not always available, pseudo-relevance feedback (PRF) assumes that the top retrieved documents in response to the given query are relevant to the query and uses these documents in order to estimate be er query models. e e ectiveness of PRF in various retrieval scenarios indicates that useful information can be captured from the top retrieved documents [19, 20, 44]. In this paper, we make use of this well-known assumption to train our model. It should be noted that there is a signi cant di erence between PRF and the proposed models: In PRF, the feedback model is estimated from the top retrieved documents of the given query in an online se ing. In other words, PRF retrieves the documents for the initial query and then estimates the feedback model using the top retrieved documents. In this paper, we propose to train the model in an o ine se ing. Moving from the online to the o ine se ing would lead to substantial improvements in e ciency, because an extra retrieval run is not needed in the o ine se ing. To learn a model in an o ine se ing, we consider a xed-length dense vector for each vocabulary term and estimate these vectors based on the information extracted from the top retrieved documents for large numbers of training queries. Note that our models are",1,ad,True
507,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"unsupervised. However, if explicit relevance data is available, such as click data, without loss of generality, both the explicit or implicit relevant documents can be considered for training our models. We leave studying the vectors learned based on supervised signals for future work.",0,,False
"To formally describe our training data, letT ,"" {(q1, R1), (q2, R2), · · · , (qm, Rm )} be a training set with m training queries. e ith element of this set is a pair of query qi and the corresponding pseudo-relevance feedback distribution. ese distributions are estimated based on the top k retrieved documents (in our experiments, we set k to 10) for each query. e distributions can be estimated using any PRF model, such as those proposed in [20, 36, 42, 44]. In this paper, we only focus on the relevance model [20], a state-of-the-art PRF model, that estimates the relevance distribution as:""",0,,False
p(w |Ri )  p(w |d ),0,,False
p(w |d ),0,,False
(3),0,,False
d Fi,0,,False
w qi,0,,False
where Fi denotes a set of top retrieved documents for query qi . Note that the probability of terms that do not appear in the top,0,,False
retrieved documents is equal to zero.,0,,False
3.3 Relevance Likelihood Maximization Model,0,,False
"In this model, the goal is to learn the relevance distribution R.",0,,False
"Given a set of training data, we aim to nd a set of parameters  R",0,,False
in order to maximize the likelihood of generating relevance model,0,,False
probabilities for the whole training set. e likelihood function is,0,,False
de ned as follows:,0,,False
m,0,,False
p (w |qi ;  R )p (w | Ri ),0,,False
(4),0,,False
"i,1 w Vi",0,,False
where p is the relevance distribution that can be obtained given,0,,False
the learning parameters  R and p(w |Ri ) denotes the relevance model distribution estimated for the ith query in the training set,0,,False
(see Section 3.2 for more detail). Vi denotes a subset of vocabulary terms that appeared in the top ranked documents retrieved for the,0,,False
query qi . e reason for iterating over the terms that appeared in this set instead of the whole vocabulary set V is that the probability,1,ad,True
"p(w |Ri ) is equal to zero for all terms w  V - Vi . In this method, we model the probability distribution p using the",0,,False
"so max function (i.e., the function  in Equation (2)) as follows:1",0,,False
"p(w |q;  R ) ,",0,,False
exp (wT q) w V exp (w T q),0,,False
(5),0,,False
"where w denotes the learned embedding vector for term w and q is the query vector came from the output of the hidden layer in our network (see Section 3.1). According to the so max modeling and the log-likelihood function, we have the following objective:",0,,False
m,0,,False
arg max,0,,False
p(w |Ri ) log exp (wT qi ) - log,0,,False
exp (w T qi ),0,,False
"R i,1 w Vi",0,,False
w V,0,,False
(6),0,,False
Computing this objective function and its derivatives would,0,,False
be computationally expensive (due to the presence of the normal-,0,,False
ization factor w V exp (w T q) in the objective function). Since all the word embedding vectors as well as the query vector are,0,,False
"1For simplicity, we drop the bias term in these equations.",0,,False
"changed during the optimization process, we cannot simply omit the normalization term as is done in [41] for estimating query embedding vectors based on pre-trained word embedding vectors. To make the computations more tractable, we consider a hierarchical approximation of the so max function, which was introduced by Morin and Bengio [26] in the context of neural network language models and then successfully employed by Mikolov et al. [24] in the word2vec model.",0,,False
"e hierarchical so max approximation uses a binary tree structure to represent the vocabulary terms, where each leaf corresponds to a unique word. ere exists a unique path from the root to each leaf, and this path is used for estimating the probability of the word representing by the leaf. erefore, the complexity of calculating so max probabilities goes down from O (|V |) to O (log(|V |)) which is the height of the tree. is leads to a huge improvement in computational complexity. We refer the reader to [25, 26] for the details of calculating the hierarchical so max approximation.",1,ad,True
3.4 Relevance Posterior Estimation Model,0,,False
"As an alternative to maximum likelihood estimation, we can estimate the relevance posterior probability. In the context of pseudorelevance feedback, Zhai and La ery [44] assumed that the language model of the top retrieved documents is estimated based on a mixture model. In other words, it is assumed that there are two language models for the feedback set: the relevance language model2 and a background noisy language model. ey used an expectationmaximization algorithm to estimate the relevance language model. In this model, we make use of this assumption in order to cast the problem of estimating the relevance distribution R as a classi cation task: Given a pair of word w and query q, does w come from the relevance distribution of the query q? Instead of p(w |R), this model estimates p(R ,"" 1|w, q;  R ) where R is a Boolean variable and R "","" 1 means that the given term-query pair (w, q) comes from the relevance distribution R.  R is a set of parameters that is going to be learned during the training phase.""",1,ad,True
"erefore, the problem is cast as a binary classi cation task that can be modeled by logistic regression (which means the function  in Equation (2) is the sigmoid function):",0,,False
1,0,,False
"p (R ,"" 1|w, q;  R ) "", 1 + e (-wT q)",0,,False
(7),0,,False
where w is the relevance-based word embedding vector for term w.,0,,False
"Similar to the previous model, q is the output of the hidden layer",0,,False
"of the network, representing the query embedding vector.",0,,False
"In order to address this binary classi cation problem, we consider",1,ad,True
"a cross-entropy loss function. In theory, for each training query,",0,,False
our model should learn to model relevance for the terms appearing,0,,False
in the corresponding pseudo-relevant set and non-relevance for all,0,,False
"the other vocabulary terms, which could be impractical, due to the",0,,False
"large number of vocabulary terms. Similar to [24], we propose to",0,,False
use the noise contrastive estimation (NCE) [12] which hypothesizes,0,,False
that we can achieve a good model by only di erentiating the data,0,,False
from noise via a logistic regression model. e main concept in NCE,0,,False
is similar to those proposed in the divergence from randomness,0,,False
model [3] and the divergence minimization feedback model [44].,0,,False
"2 e phrase ""topical language model"" was used in the original work [44]. We call it ""relevance language model"" to have consistent de nitions in our both models.",0,,False
508,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Based on the NCE hypothesis, we de ne the following negative cross-entropy objective function for training our model:",0,,False
arg,0,,False
max,0,,False
R,0,,False
"m i ,1",0,,False
"+ j ,1",0,,False
Ewj p (w,0,,False
| Ri,0,,False
),0,,False
"log p(R ,"" 1|wj , qi ;  R )""",0,,False
-,0,,False
+ Ewj pn (w ),0,,False
"j ,1",0,,False
"log p(R ,"" 0|wj , qi ;  R ) """,0,,False
(8),0,,False
"where pn (w ) denotes a noise distribution and  ,"" (+, -) is a pair of hyper-parameters to control the number of positive and negative instances per query, respectively. We can easily calculate p(R "","" 0|wj , qi ) "", 1 - p(R ,"" 1|wj , qi ). e noise distribution pn (w ) can be estimated using a function of unigram distribution U (w ) in the whole training set. Similar to [24], we use pn (w )  U (w )3/4 which has been empirically shown to work e ectively for negative sampling.""",0,,False
"It is notable that although this model learns embedding vectors for both queries and words, it is not obvious how to calculate the probability of each term given a query; because Equation 7 only gives us a classi cation probability and we cannot simply use the Bayes rule here (since, not all probability components are known). is model can perform well when computing the similarity between two terms or two queries, but not a query and a term. However, we can use the model presented in [41] to estimate the query model using the word embedding vectors (not the ones learned for query vectors) and then calculate the similarity between a query and a term.",0,,False
4 EXPERIMENTS,0,,False
"In this section, we rst describe how we train the relevance-based word embedding models. We further extrinsically evaluate the learned embeddings using two IR tasks: query expansion and query classi cation. Note that the main aim here is to compare the proposed models with the existing word embedding algorithms, not with the state-of-the-art query expansion and query classi cation models.",0,,False
4.1 Training,0,,False
"In order to train relevance-based word embeddings, we obtained millions of unique queries from the publicly available AOL query logs [27]. is dataset contains a sample of web search queries from real users submi ed to the AOL search engine within a three-month period from March 1, 2006 to May 31, 2006. We only used query strings and no session and click information was obtained from this dataset. We ltered out the navigational queries containing URL substrings, i.e., ""h p"", ""www."", "".com"", "".net"", "".org"", "".edu"". All nonalphanumeric characters were removed from all queries. Applying all these constraints leads to over 6 millions unique queries as our training query set. To estimate the relevance model distributions in the training set, we considered top 10 retrieved documents in a target collection in response to each query using the Galago3 implementation of the query likelihood retrieval model [29] with Dirichlet prior smoothing (µ , 1500) [45].",1,ad,True
3h p://www.lemurproject.org/galago.php,0,,False
"We implemented and trained our models using TensorFlow4. e networks are trained based on the stochastic gradient descent optimizer using the back-propagation algorithm [33] to compute the gradients. All model hyper-parameters were tuned on the training set (the hyper-parameters with the smallest training loss value were selected). For each model, the learning rate and the batch size were selected from [0.001, 0.01, 0.1, 1] and [64, 128, 256], respectively. For RPE , we also tuned the number of positive and negative instances (i.e., + and -). e value of + was swept between [20, 50, 100, 200] and the parameter - was selected from [5+, 10+, 20+]. As suggested in [40], in all the experiments (unless otherwise stated) the embedding dimensionality was set to 300, for all models including the baselines.",1,ad,True
4.2 Evaluation via ery Expansion,0,,False
"In this subsection, we evaluate the embedding models in the context of query expansion for the ad-hoc retrieval task. In the following, we rst describe the retrieval collections used in our experiments. We further explain our experimental setup as well as the evaluation metrics. We nally report and discuss the query expansion results.",1,ad-hoc,True
"4.2.1 Data. We use four standard test collections in our experiments. e rst two collections (AP and Robust) consist of thousands of news articles and are considered as homogeneous collections. AP and Robust were previously used in TREC 1-3 Ad-Hoc Track and TREC 2004 Robust Track, respectively. e second two collections (GOV2 and ClueWeb) are large-scale web collections containing heterogeneous documents. GOV2 consists of the "".gov"" domain web pages, crawled in 2004. ClueWeb (i.e., ClueWeb09Category B) is a common web crawl collection that only contains English web pages. GOV2 and ClueWeb were previously used in TREC 2004-2006 Terabyte Track and TREC 2009-2012 Web Track, respectively. e statistics of these collections as well as the corresponding TREC topics are reported in Table 1. We only used the title of topics as queries.",1,AP,True
4.2.2 Experimental Setup. We cleaned the ClueWeb collection,1,ClueWeb,True
by ltering out the spam documents. e spam ltering phase was done using the Waterloo spam scorer5 [6] with the threshold of 60%.,0,,False
Stopwords were removed from all collections using the standard,0,,False
INQUERY stopword list and no stemming were performed.,0,,False
"For the purpose of query expansion, we consider the language",0,,False
modeling framework [29] and estimate a query language model,0,,False
based on a given set of word embedding vectors. e expanded query language model p(w |q ) is estimated as:,0,,False
"p(w |q ) , pML (w |q) + (1 -  )p(w |q)",0,,False
(9),0,,False
"where pML (w |q) denotes maximum likelihood estimation of the original query and  is a free hyper-parameter that controls the weight of original query model in the expanded model. e probability p(w |q) is calculated based on the trained word embedding vectors. In our rst model, this probability can be estimated using Equation (5); while in the second model, we should simply use the Bayes rule given Equation (7) to estimate this probability. However, since we do not have any information about the probability of each",0,,False
4h p://tensor ow.org/ 5h p://plg.uwaterloo.ca/gvcormac/clueweb09spam/,0,,False
509,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Collections statistics.,0,,False
ID AP Robust,1,AP,True
GOV2,0,,False
ClueWeb,1,ClueWeb,True
collection Associated Press 88-89 TREC Disks 4 & 5 minus Congressional Record,1,TREC,True
2004 crawl of .gov domains,0,,False
ClueWeb 09 - Category B,1,ClueWeb,True
"queries (title only) TREC 1-3 Ad-Hoc Track, topics 51-200",1,TREC,True
"TREC 2004 Robust Track, topics 301-450 & 601-700 TREC 2004-2006 Terabyte Track,",1,TREC,True
topics 701-850 TREC 2009-2012 Web Track,1,TREC,True
topics 1-200,0,,False
#docs 165k 528k,0,,False
25m,0,,False
50m,0,,False
avg doc length 287 254,0,,False
648,0,,False
1506,0,,False
"#qrels 15,838 17,412",0,,False
"26,917",0,,False
"18,771",0,,False
Table 2: Evaluating relevance-based word embeddings in the context of query expansion. e superscripts 0/1/2/3/4 denote that the MAP improvements over MLE/word2vec-external/word2vec-target/GloVe-external/GloVe-target are statistically signi cant. e highest value in each row is marked in bold.,1,MAP,True
Collection AP,1,AP,True
Robust GOV2 ClueWeb,1,Robust,True
Metric,0,,False
MAP P@20 NDCG@20,1,MAP,True
MAP P@20 NDCG@20,1,MAP,True
MAP P@20 NDCG@20,1,MAP,True
MAP P@20 NDCG@20,1,MAP,True
MLE,0,,False
0.2197 0.3503 0.3924,0,,False
0.2149 0.3319 0.3863,0,,False
0.2702 0.5132 0.4482,0,,False
0.1028 0.3025 0.2237,0,,False
word2vec,0,,False
external target,0,,False
0.2399 0.3688 0.4030,0,,False
0.2420 0.3738 0.4181,0,,False
0.2218 0.3357 0.3918,0,,False
0.2215 0.3337 0.3881,0,,False
0.2740 0.5257 0.4571,0,,False
0.2723 0.5172 0.4509,0,,False
0.1033 0.3040 0.2235,0,,False
0.1033 0.3053 0.2252,0,,False
GloVe,0,,False
external target,0,,False
0.2319 0.3581 0.4025,0,,False
0.2389 0.3631 0.4098,0,,False
0.2209 0.3345 0.3918,0,,False
0.2172 0.3281 0.3844,0,,False
0.2718 0.5186 0.4539,0,,False
0.2709 0.5128 0.4485,0,,False
0.1029 0.3033 0.2244,0,,False
0.1026 0.3048 0.2244,0,,False
Rel.-based Embedding,0,,False
RLM,1,LM,True
RPE,0,,False
0.258001234 0.388601234 0.424201234,0,,False
0.245001234 0.347601234 0.398201234,0,,False
0.286701234 0.536701234 0.45760234,0,,False
0.106601234,0,,False
0.3073 0.227301,0,,False
0.254301234 0.3812034 0.422601234,0,,False
0.237201234 0.3409024 0.39550,0,,False
0.285501234 0.535801234 0.4557024,0,,False
0.1031,0,,False
0.3030,0,,False
0.2241,0,,False
"term given a query, we use the uniform distribution. For other word embedding models (i.e., word2vec and GloVe), we use the standard method described in [11]. For all the models, we ignore the terms whose embedding vectors are not available.",0,,False
"We retrieve the documents for the expanded query language model using the KL-divergence formula [18] with Dirichlet prior smoothing (µ , 1500) [45]. All the retrieval experiments were carried out using the Galago toolkit [7].",0,,False
"In all the experiments, the parameters  (the linear interpolation coe cient) and m (the number of expansion terms) were set using 2-fold cross-validation over the queries in each collection. We selected the parameter  from {0.1, . . . , 0.9} and the parameter m from {10, 20, ..., 100}.",0,,False
"4.2.3 Evaluation Metrics. To evaluate the e ectiveness of query expansion models, we report three standard evaluation metrics: mean average precision (MAP) of the top ranked 1000 documents, precision of the top 20 retrieved documents (P@20), and normalized discounted cumulative gain [13] calculated for the top 20 retrieved documents (nDCG@20). Statistically signi cant di erences of MAP, P@20, and nDCG@20 values based on the two-tailed paired t-test are computed at a 95% con dence level (i.e., p alue < 0.05).",1,MAP,True
"4.2.4 Results and Discussion. To evaluate our models, we consider the following baselines: (i) the standard maximum likelihood estimation (MLE) of the query model without query expansion, (ii) two sets of embedding vectors (one trained on Google News as a",0,,False
"large external corpus and one trained on the target retrieval collection) learned by the word2vec model6 [24], and (iii) two sets of embedding vectors (one trained on Wikipedia 2004 plus Gigawords 5 as a large external corpus7 and the other on the target retrieval collection) learned by the GloVe model [28].",1,Wiki,True
"Table 2 reports the results achieved by the proposed models and the baselines. According to this table, all the query expansion models outperform the MLE baseline in nearly all cases, which indicates the e ectiveness of employing high-dimensional word representations for query expansion. Similar observations have been made in [11, 17, 40, 41]. According to the results, although word2vec performs slightly be er than GloVe, no signi cant di erences can be observed between their performances. According to Table 2, both relevance-based embedding models outperform all the baselines in all the collections, which shows the importance of taking relevance into account for training embedding vectors. ese improvements are o en statistically signi cant compared to all the baselines. e relevance likelihood maximization model (RLM) performs be er than the relevance posterior estimation model (RPE) in all cases and the reason is related to their objective function. RLM learns the relevance distribution for all terms, while RPE learns the classi cation probability of being relevance for vocabulary terms (see Equations (5) and (7)).",1,ad,True
6We use the CBOW implementation of the word2vec model.,0,,False
also performs similarly. 7Available at h p://nlp.stanford.edu/projects/glove/.,0,,False
e skip-gram model,0,,False
510,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: Top 10 expansion terms obtained by the word2vec and the relevance-based word embedding models for two sample queries ""indian american museum"" and ""tibet protesters"".",0,,False
"query: ""indian american museum""",0,,False
word2vec,0,,False
Rel.-based Embedding,0,,False
external,0,,False
target,0,,False
RLM,1,LM,True
RPE,0,,False
history,0,,False
powwows,0,,False
chumash,0,,False
heye,0,,False
art,0,,False
smithsonian heye,0,,False
collection,0,,False
culture,0,,False
afro,0,,False
artifacts,0,,False
chumash,0,,False
british,0,,False
mesoamerica smithsonian smithsonian,0,,False
heritage,0,,False
smithsonians collection,0,,False
york,0,,False
society,0,,False
native,0,,False
washington new,0,,False
states,0,,False
heye,0,,False
institution,0,,False
apa,0,,False
contemporary hopi,0,,False
york,0,,False
native,0,,False
part,0,,False
mayas,0,,False
native,0,,False
americans,0,,False
united,0,,False
cimam,0,,False
apa,0,,False
history,0,,False
"To get a sense of what is learned by each of the embedding models8, in Table 3 we report the top 10 expansion terms for two sample queries from the Robust collection. According to this table, the terms added to the query by the word2vec model are syntactically or semantically related to individual query terms, which is expected. For the query ""indian american museum"" as an example, the terms ""history"", ""art"", and ""culture"" are related to the query term ""museum"", while the terms ""united"" and ""states"" are related to the query term ""american"". In contrast, looking at the expansion terms obtained by the relevance-based word embeddings, we can see that some relevant terms to the whole query were selected. For instance, ""chumash"" (a group of native americans)9, ""heye"" (the national museum of the American Indian in New York), ""smithsonian"" (the national museum of the American Indian in Washington DC), and ""apa"" (the American Psychological Association that actively promotes American Indian museums). A similar observation can be made for the other sample query (i.e., ""tibet protesters""). For example, the word ""independence"" is related to the whole query that was only selected by the relevance-based word embedding models, while the terms ""protestors"", ""protests"", ""protest"", and ""protesting"" that are syntactically similar to the query term ""protesters"" were considered by the word2vec model. We believe that these di erences are due to the learning objective of the models. Interestingly, the expansion terms added to each query by the two relevance-based models look very similar, but according to Table 2, their performances are quite di erent. e reason is related to the weights given to each term by the two models. e weights given to the expansion terms by RPE are very close to each other because its objective is to just classify each term and all of these terms are classi ed with a high probability as ""relevant"".",1,Robust,True
"In the next set of experiments, we consider the methods that use the top retrieved documents for query expansion: the relevance model (RM3) [1, 20] as a state-of-the-art pseudo-relevance feedback model, and the local embedding approach recently proposed by Diaz et al. [11] with the general idea of training word embedding models on the top ranked documents retrieved in response to a given query. Similar to [11], we use the word2vec model to train",0,,False
"8For the sake of space, we only report the expanded terms estimated by the word2vec model and the proposed models. 9see h ps://en.wikipedia.org/wiki/Chumash people",1,wiki,True
"query: ""tibet protesters""",0,,False
word2vec,0,,False
Rel.-based Embedding,0,,False
external,0,,False
target,0,,False
RLM,1,LM,True
RPE,0,,False
demonstrators tibetan,0,,False
tibetan,0,,False
tibetan,0,,False
protestors,0,,False
lhasa,0,,False
lama,0,,False
tibetans,0,,False
tibetan,0,,False
demonstrators tibetans,0,,False
lama,0,,False
protests,0,,False
tibetans,0,,False
lhasa,0,,False
independence,0,,False
tibetans,0,,False
marchers,0,,False
dalai,0,,False
lhasa,0,,False
protest,0,,False
lhasas,0,,False
independence dalai,0,,False
activists,0,,False
jokhang,0,,False
protest,0,,False
open,0,,False
protesting,0,,False
demonstrations open,0,,False
protest,0,,False
lhasa,0,,False
dissidents,0,,False
zone,0,,False
zone,0,,False
demonstrations barkhor,0,,False
followers,0,,False
jokhang,0,,False
Table 4: Evaluating relevance-based word embedding in pseudo-relevance feedback scenario. e superscripts 1/2/3 denote that the MAP improvements over RM3/Local Embedding/ERM with Local Embedding are statistically signi cant.,1,MAP,True
e highest value in each row is marked in bold.,0,,False
Collection AP,1,AP,True
Robust GOV2 ClueWeb,1,Robust,True
Metric,0,,False
MAP P@20 NDCG@20,1,MAP,True
MAP P@20 NDCG@20,1,MAP,True
MAP P@20 NDCG@20,1,MAP,True
MAP P@20 NDCG@20,1,MAP,True
RM3,0,,False
0.2927 0.4034 0.4368,0,,False
0.2593 0.3486 0.4011,0,,False
0.2863 0.5318 0.4503,0,,False
0.1079 0.3111 0.2309,0,,False
Local,0,,False
Emb.,0,,False
0.2412 0.3742 0.4173,0,,False
0.2235 0.3366 0.3868,0,,False
0.2748 0.5271 0.4576,0,,False
0.1041 0.3062 0.2261,0,,False
ERM,0,,False
Local RLM,1,LM,True
0.3047 0.4105 0.4411,0,,False
0.2643 0.3498 0.4080,0,,False
0.2924 0.5379 0.4584,0,,False
0.311912 0.423312 0.4495123,0,,False
0.2761123 0.3605123 0.4173123,0,,False
0.2986123 0.541712 0.4603123,0,,False
0.1094 0.112112,0,,False
0.3145 0.3168 0.2328 0.23602,0,,False
"word embedding vectors on top 1000 documents. e results are reported in Table 4. In this table, ERM refers to the embedding-based relevance model recently proposed by Zamani and Cro [40] in order to make use of semantic similarities estimated based on the word embedding vectors in a pseudo-relevance feedback scenario. According to Table 4, the ERM model that uses the relevance-based word embedding (RLM10) outperforms all the other methods. ese improvements are statistically signi cant in most cases. By comparing the results obtained by local embedding and those reported in Table 2, it can be observed that there are no substantial di erences between the results for local embedding and word2vec. is is similar to what is reported by Diaz et al. [11] when the embedding vectors are trained on the top documents in the target collection, similar to our se ing. Note that the relevance-based model was also trained on the target collection.",1,LM,True
"10For the sake of space, we only consider RLM which shows be er performance compared to RPE in query expansion.",1,LM,True
511,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
MAP,1,MAP,True
0.06 0.07 0.08 0.09 0.1 0.23 0.24 0.25 0.26,0,,False
0.30,0,,False
0.25 0.20,0,,False
0.15 ,0,,False
MAP,1,MAP,True
 AP Robust GOV2 ClueWeb,1,AP,True
5,0,,False
10,0,,False
15,0,,False
20,0,,False
25,0,,False
# expansion terms,0,,False
0.10 0.05 0.00,0,,False
0.0,0,,False
 AP Robust GOV2 ClueWeb,1,AP,True
0.2,0,,False
0.4,0,,False
0.6,0,,False
0.8,0,,False
1.0,0,,False
interpolation coefficient,0,,False
(a) # expansion terms,0,,False
(b) interpolation coe cient,0,,False
"Figure 2: Sensitivity of RLM to the number of expansion terms and the interpolation coe cient (), in terms of MAP.",1,LM,True
0.29,0,,False
MAP,1,MAP,True
0.06 0.08 0.1 0.18 0.2 0.22 0.24 0.26 0.28,0,,False
0.27,0,,False
0.25,0,,False
MAP,1,MAP,True
0.09 0.1 0.11,0,,False
0.07,0,,False
 AP Robust GOV2 ClueWeb,1,AP,True
100,0,,False
200,0,,False
300,0,,False
400,0,,False
500,0,,False
embedding dimension,0,,False
"Figure 3: Sensitivity of RLM to the dimension of embedding vectors, in terms of MAP.",1,LM,True
"An interesting observation from Tables 2 and 4 is that the RLM performance (without using pseudo-relevant documents) in Robust and GOV2 is very close to the RM3 performance, and is slightly be er in the GOV2 collection. Note that RM3 needs two retrieval runs11 and uses top retrieved documents, while RLM only needs one retrieval run. is is an important issue in many real-world applications, since the e ciency constraints do not always allow them to have two retrieval runs per query.",1,LM,True
"Parameter Sensitivity. In the next set of experiments, we study the sensitivity of RLM as the best performing word embedding model in Table 2 to the expansion parameters. Figure 2a plots the sensitivity of RLM to the number of expansion terms where the parameter  is set to 0.5. According to this gure, in both newswire collections, the method shows its best performance when the queries are expanded with only 10 words. In the GOV2 collection, 15 words are needed for the method to show its best performance.",1,LM,True
"Figure 2b plots the sensitivity of the methods to the interpolation coe cient  (see Equation 9) where the number of expansion terms is set to 10. According to the curves correspond to AP and Robust, the original query language model needs to be interpolated with the model estimated using relevance-based word embeddings",1,AP,True
"11Diaz [10] showed that for precision-oriented tasks, the second retrieval run can be restricted to the initial rank list for improving the e ciency of PRF models. However, for recall-oriented metrics, e.g., MAP, the second retrieval helps a lot.",1,MAP,True
 AP Robust GOV2 ClueWeb,1,AP,True
1,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
million queries,0,,False
"Figure 4: e Performance of RLM with respect to di erent amount of training data (training queries), in terms of MAP.",1,LM,True
"with equal weights (i.e.,  ,"" 0.5). is shows the quality of the estimated distribution via the learned embedding vectors. In the GOV2 collection, a higher weight should be given to the original query model, which indicates that the original query plays a key role in achieving good retrieval performance in this collection.""",0,,False
"We also study the performance of RLM as the best performing word embedding model for query expansion with respect to the embedding dimensionality. e results are shown in Figure 3, where the query expansion performance generally improves as we increase the embedding dimensionality. e performances become stable when the dimension is larger than 300. is experiment suggests that 400 dimensions would be enough for the relevance-based embedding model.",1,LM,True
"Due to the large number of parameters in the neural networks, they can require large amounts of training data to achieve good performance. In the next set of experiments, we study how much training data is needed for training our best model. e results are plo ed in Figure 4. According to this gure, by increasing the number of training queries from one million to four million queries, the performance signi cantly increases, and becomes more stable a er four million queries.",0,,False
4.3 Evaluation via ery Classi cation,0,,False
"In this subsection, we evaluate the proposed embedding models in the context of query classi cation. In this task, each query is",0,,False
512,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 5: Evaluating embedding algorithms via query classication. e superscripts 1/2 denote that the improvements,0,,False
over word2vec/GloVe are signi cant. e highest value in each column is marked in bold.,0,,False
Method word2vec GloVe Rel.-based Embedding - RLM Rel.-based Embedding - RPE,1,LM,True
Precision,0,,False
0.3712,0,,False
0.3643 0.394312 0.396112,0,,False
F1-measure,0,,False
0.4008,0,,False
0.3912 0.426712 0.429412,0,,False
assigned to a number of labels (categories) which are pre-de ned and a few training queries are available for each label. is is a supervised multi-label classi cation task with li le training data.,0,,False
"4.3.1 Data. We consider the dataset that was introduced in KDD Cup 2005 [22] for the internet user search query categorization task and was previously used in [41] for evaluating query embedding vectors. is dataset contains 800 web queries submi ed by real users randomly collected from the MSN search logs. e queries do not contain ""junk"" text or non-English terms. e queries were labelled by three human editors. 67 categories were pre-de ned and up to 5 labels were selected for each query by each editor.",0,,False
"4.3.2 Experimental Setup. In our experiments, we performed 5-fold cross-validation over the queries and the reported results are the average of those obtained over the test folds. In all experiments, the spelling errors in queries were corrected in a pre-processing phase, the stopwords were removed from queries (using the INQUERY stopword list), and no stemming was performed.",0,,False
"To classify each query, we consider a very simple kNN-based approach proposed in [41]. We rst compute the probability of each category/label given each query q and then select the top t categories with the highest probabilities. e probability p(Ci |q) is computed as follows:",0,,False
"p(Ci |q) ,""  (Ci , q)   (Ci , q)""",0,,False
(10),0,,False
"j  (Cj , q)",0,,False
"where Ci denotes the ith category. Ci is the centroid vector of all query embedding vectors with the label of Ci in the training set. We ignore the query terms whose embedding vectors are not available. e number of labels assigned to each query was tuned on the training set from {1, 2, 3, 4, 5}. In the query classi cation experiments, we trained relevance-based word embedding using Robust as the collection.",1,Robust,True
"4.3.3 Evaluation Metrics. We consider two evaluation metrics that were also used in KDD Cup 2005 [22]: precision and F1measure. Since the labels assigned by the three human editors di er in some cases, all the label sets should be taken into account.",0,,False
ese metrics are computed in the same way as what is described in [22] for evaluating the KDD Cup 2005 submi ed runs. Statistically signi cant di erences are determined using the two-tailed paired t-test computed at a 95% con dence level (p - alue < 0.05).,0,,False
"4.3.4 Results and Discussion. We compare our models against the word2vec and GloVe methods trained on the external collections that are described in the query expansion experiments. e results are reported in Table 5, where the relevance-based embedding",0,,False
0.430,0,,False
F1-measure,0,,False
0.428,0,,False
0.426,0,,False
0.424,0,,False
0.422,0,,False
0.420,0,,False
 RLM RPE,1,LM,True
100,0,,False
200,0,,False
300,0,,False
400,0,,False
500,0,,False
embedding dimension,0,,False
"Figure 5: Sensitivity of the relevance-based embedding models to the embedding dimensionality, in terms of F1measure.",0,,False
0.42 0.40,0,,False
F1-measure,0,,False
0.38 ,0,,False
0.36,0,,False
0.34,0,,False
1,0,,False
 RLM RPE,1,LM,True
2,0,,False
3,0,,False
4,0,,False
5,0,,False
million queries,0,,False
"Figure 6: e Performance of relevance-based embedding models with respect to di erent amount of training data (training queries), in terms of F1-measure.",0,,False
"models signi cantly outperform the baselines in terms of both metrics. An interesting observation here is that contrary to the query expansion experiments, RPE performs be er than RLM in query classi cation. e reason is that in query expansion the weight of each term is considered in order to generate the expanded query language model. erefore, in addition to the order of terms, their weights should be also e ective for improving the retrieval performance with query expansion. In query classi cation, we only assign a few categories to each query, and thus as long as the order of categories is correct, the similarity values between the queries and the categories do not ma er.",1,LM,True
"In the next set of experiments, we study the performance of our relevance-based word embedding models with respect to the embedding dimensionality. e results are plo ed in Figure 5. According to this gure, the performance is generally improved by increasing the embedding dimensionality, and becomes stable when the dimension is greater than 400. is is similar to our observation in the query expansion experiments. We also study the amount of data needed for training our models in Figure 6. According to this gure, at least 4 million queries are needed in order to learn accurate relevance-based word embeddings. It can be seen from Figure 6 that RLM needs more training data compared to RPE in order to perform well, because by increasing the amount of training data the learning curves of these two models get closer.",1,LM,True
513,0,,False
Session 4C: Queries and Query Analysis,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
5 CONCLUSIONS AND FUTURE WORK,0,,False
"In this paper, we revisited the underlying assumption in typical word embedding models, such as word2vec and GloVe. Instead of learning embedding vectors based on term proximity, we proposed learning embeddings based on the notion of relevance, which is the primary objective in many IR tasks. We developed two neural network-based models for learning relevance-based word embeddings. e rst model, the relevance likelihood maximization model, aims to estimate the probability of each word in a relevance distribution for each query, while the second one, the relevance posterior estimation model, classi es each term as belonging to relevant or non-relevant class for each query. We evaluated our models using two sets of extrinsic evaluation: query expansion and query classi cation. e query expansion experiments using four standard TREC collections, two newswire and two large-scale web collections, suggested that the relevance-based word embedding models outperform state-of-the-art word embedding algorithms. We showed that the expansion terms chosen by our models are related to the whole query, while those chosen by typical word embedding models are related to individual query terms. e query classi cation experiments also validated these ndings and investigated the e ectiveness of our models.",1,ad,True
"In the future, we intend to evaluate the learned embedding models in other IR tasks, such as query reformulation, query intent prediction, etc. We can also achieve more accurate relevance-based embedding vectors by considering the clicked documents for training query, instead of or in addition to the top retrieved documents.",1,ad,True
"Acknowledgements. e authors thank Daniel Cohen, Mostafa Dehghani, and Qingyao Ai for their invaluable comments. is work was supported in part by the Center for Intelligent Information Retrieval. Any opinions,",0,,False
ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsor.,0,,False
REFERENCES,0,,False
"[1] Nasreen Abdul-jaleel, James Allan, W. Bruce Cro , Fernando Diaz, Leah Larkey, Xiaoyan Li, Donald Metzler, Mark D. Smucker, Trevor Strohman, Howard Turtle, and Courtney Wade. 2004. UMass at TREC 2004: Novelty and HARD. In TREC '04.",1,ad,True
"[2] Qingyao Ai, Liu Yang, Jiafeng Guo, and W. Bruce Cro . 2016. Analysis of the Paragraph Vector Model for Information Retrieval. In ICTIR '16. 133­142.",0,,False
"[3] Gianni Amati and Cornelis Joost Van Rijsbergen. 2002. Probabilistic Models of Information Retrieval Based on Measuring the Divergence from Randomness. ACM Trans. Inf. Syst. 20, 4 (2002), 357­389.",0,,False
[4] P. D. Bruza and D. Song. 2002. Inferring ery Models by Computing Information Flow. In CIKM '02. 260­269.,0,,False
[5] Stephane Clinchant and Florent Perronnin. 2013. Aggregating Continuous Word Embeddings for Information Retrieval. In CVSC@ACL '13. 100­109.,0,,False
"[6] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. E cient and E ective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (2011), 441­465.",0,,False
"[7] Bruce Cro , Donald Metzler, and Trevor Strohman. 2009. Search Engines: Information Retrieval in Practice (1st ed.). Addison-Wesley Publishing Company.",0,,False
"[8] Sco Deerwester, Susan T. Dumais, George W. Furnas, omas K. Landauer, and Richard Harshman. 1990. Indexing by Latent Semantic Analysis. 41, 6 (1990), 391­407.",0,,False
"[9] Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Cro . 2017. Neural Ranking Models with Weak Supervision. In SIGIR '17.",0,,False
"[10] Fernando Diaz. 2015. Condensed List Relevance Models. In ICTIR '15. 313­316. [11] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with",0,,False
Locally-Trained Word Embeddings. In ACL '16. [12] Michael U. Gutmann and Aapo Hyva¨rinen. 2012. Noise-contrastive Estimation of,0,,False
"Unnormalized Statistical Models, with Applications to Natural Image Statistics. J. Mach. Learn. Res. 13, 1 (2012), 307­361. [13] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446.",0,,False
[14] Yufeng Jing and W. Bruce Cro . 1994. An Association esaurus for Information Retrieval. In RIAO '94. 146­160.,0,,False
[15] Tom Kenter and Maarten de Rijke. 2015. Short Text Similarity with Word Embeddings. In CIKM '15. 1411­1420.,0,,False
"[16] Ma J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kilian Q. Weinberger. 2015. From Word Embeddings to Document Distances. In ICML '15. 957­966.",0,,False
"[17] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. ery Expansion Using Word Embeddings. In CIKM '16. 1929­1932.",0,,False
"[18] John La erty and Chengxiang Zhai. 2001. Document Language Models, ery Models, and Risk Minimization for Information Retrieval. In SIGIR '01. 111­119.",0,,False
"[19] Victor Lavrenko, Martin Choque e, and W. Bruce Cro . 2002. Cross-lingual Relevance Models. In SIGIR '02. 175­182.",0,,False
[20] Victor Lavrenko and W. Bruce Cro . 2001. Relevance Based Language Models. In SIGIR '01. 120­127.,0,,False
[21] Omer Levy and Yoav Goldberg. 2014. Neural Word Embedding as Implicit Matrix Factorization. In NIPS '14. 2177­2185.,0,,False
"[22] Ying Li, Zijian Zheng, and Honghua (Kathy) Dai. 2005. KDD CUP-2005 Report: Facing a Great Challenge. SIGKDD Explor. Newsl. 7, 2 (2005), 91­99.",0,,False
"[23] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang. 2015. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classi cation and Information Retrieval. In NAACL '15. 912­921.",0,,False
"[24] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS '13. 3111­3119.",1,ad,True
[25] Andriy Mnih and Geo rey E Hinton. 2009. A Scalable Hierarchical Distributed Language Model. In NIPS '09. 1081­1088.,0,,False
[26] Frederic Morin and Yoshua Bengio. 2005. Hierarchical Probabilistic Neural Network Language Model. In AISTATS '05. 246­252.,0,,False
"[27] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In InfoScale '06.",0,,False
"[28] Je rey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP '14. 1532­1543.",0,,False
[29] Jay M. Ponte and W. Bruce Cro . 1998. A Language Modeling Approach to Information Retrieval. In SIGIR '98. 275­281.,0,,False
"[30] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In SIGIR '17.",0,,False
"[31] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. Generalizing Translation Models in the Probabilistic Relevance Framework. In CIKM '16. 711­720.",0,,False
[32] J. J. Rocchio. 1971. Relevance Feedback in Information Retrieval. In e SMART Retrieval System: Experiments in Automatic Document Processing. 313­323.,0,,False
"[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning representations by back-propagating errors. Nature 323 (Oct. 1986), 533­536.",0,,False
"[34] T. Saracevic. 2016. e Notion of Relevance in Information Science: Everybody knows what relevance is. But, what is it really? Morgan & Claypool Publishers.",0,,False
"[35] Alessandro Sordoni, Yoshua Bengio, and Jian-Yun Nie. 2014. Learning Concept Embeddings for ery Expansion by antum Entropy Minimization. In AAAI '14. 1586­1592.",0,,False
[36] Tao Tao and ChengXiang Zhai. 2006. Regularized Estimation of Mixture Models for Robust Pseudo-relevance Feedback. In SIGIR '06. 162­169.,1,Robust,True
[37] Ivan Vulic´ and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings. In SIGIR '15. 363­372.,0,,False
[38] Jinxi Xu and W. Bruce Cro . 1996. ery Expansion Using Local and Global Document Analysis. In SIGIR '96. 4­11.,0,,False
"[39] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017. Situational Context for Ranking in Personal Search. In WWW '17. 1531­1540.",0,,False
[40] Hamed Zamani and W. Bruce Cro . 2016. Embedding-based ery Language Models. In ICTIR '16. 147­156.,0,,False
[41] Hamed Zamani and W. Bruce Cro . 2016. Estimating Embedding Vectors for eries. In ICTIR '16. 123­132.,0,,False
"[42] Hamed Zamani, Javid Dadashkarimi, Azadeh Shakery, and W. Bruce Cro . 2016. Pseudo-Relevance Feedback Based on Matrix Factorization. In CIKM '16. 1483­ 1492.",1,ad,True
"[43] ChengXiang Zhai, William W. Cohen, and John La erty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR '03. 10­17.",0,,False
[44] Chengxiang Zhai and John La erty. 2001. Model-based Feedback in the Language Modeling Approach to Information Retrieval. In CIKM '01. 403­410.,0,,False
"[45] Chengxiang Zhai and John La erty. 2004. A Study of Smoothing Methods for Language Models Applied to Information Retrieval. ACM Trans. Inf. Syst. 22, 2 (2004), 179­214.",0,,False
[46] Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. In SIGIR '15. 575­584.,0,,False
"[47] Guangyou Zhou, Tingting He, Jun Zhao, and Po Hu. 2015. Learning Continuous Word Embedding with Metadata for estion Retrieval in Community estion Answering. In ACL '15. 250­259.",1,ad,True
514,0,,False
,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Adapting Markov Decision Process for Search Result Diversification,0,,False
"Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng",0,,False
"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences {xialong,zengwei}@so ware.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn",1,ad,True
ABSTRACT,0,,False
"In this paper we address the issue of learning diverse ranking models for search result diversi cation. Typical methods treat the problem of constructing a diverse ranking as a process of sequential document selection. At each ranking position, the document that can provide the largest amount of additional information to the users is selected, because the search users usually browse the documents in a top-down manner. us, to select an optimal document for a position, it is critical for a diverse ranking model to capture the utility of information the user have perceived from the preceding documents. Existing methods usually calculate the ranking scores (e.g., the marginal relevance) directly based on the query and the selected documents, with heuristic rules or handcra ed features.",1,ad,True
"e utility the user perceived at each of the ranks, however, is not explicitly modeled. In this paper, we present a novel diverse ranking model on the basis of continuous state Markov decision process (MDP) in which the user perceived utility is modeled as a part of the MDP state. Our model, referred to as MDP-DIV, sequentially takes the actions of selecting one document according to current state, and then updates the state for the chosen of the next action. e transition of the states are modeled in a recurrent manner and the model parameters are learned with policy gradient. Experimental results based on the TREC benchmarks showed that MDP-DIV can signi cantly outperform the state-of-the-art baselines.",1,ad,True
KEYWORDS,0,,False
learning to rank; search result diversi cation; Markov decision process,0,,False
"ACM Reference format: Long Xia, Jun Xu , Yanyan Lan, Jiafeng Guo, Wei Zeng, Xueqi Cheng. 2017. Adapting Markov Decision Process for Search Result Diversi cation. In Proceedings of SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 10 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080775",0,,False
* Corresponding author: Jun Xu.,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: h p://dx.doi.org/10.1145/3077136.3080775",1,ad,True
1 INTRODUCTION,1,DUC,True
"In many information retrieval tasks, one important goal involves providing search results that covers a wide range of topics for a search query, called search result diversi cation [1]. One of the key problems in search result diversi cation is ranking, speci cally, how to develop a ranking model that can sort documents based on their relevance to the given query as well as the novelty of the information in the documents.",0,,False
"Typical approaches to search result diversi cation, including the heuristic approaches and the learning approaches, treat the process of constructing a diverse ranking as a problem of sequential document selection. At each ranking position, the additional amount of information (utility) a document can provide is estimated, on the basis of the user query and the documents ranked ahead. e document that can provide maximal additional utility is selected.",1,ad,True
"e sequential document selection matches well with the user activity of browsing the search results: search users usually browse the search results in a top-down manner. us, to accurately select the document at each of the positions, it is critical for a diverse ranking algorithm to model the utility of information the users have already perceived from the preceding documents.",1,ad,True
"Several methods for diverse ranking have been developed and applied to document retrieval. Di erent criteria are adopted in these methods to estimate the new utility a candidate document can provide. For example, in the representative heuristic approach of maximal marginal relevance (MMR) [2], the marginal relevance, which is de ned as a sum of the query-document relevance and the maximal document distance, is used as the utility. In x AD [20], another widely used diverse ranking model, the utility is de ned so as to explicitly account for the relationship between documents retrieved for the original query and the possible aspects underlying this query, in the form of sub-queries. In recent years, machine learning methods have been proposed and applied to search result diversi cation [18, 24­27, 31]. Typical diverse learning models, including the relational learning to rank (R-LTR) [31] and its variations [24­26], de ne the utilities as the linear combinations of the relevance features and the novelty features.",1,ad,True
"All the existing methods on diverse ranking [2, 20, 31] are designed to estimate the utility of a candidate document directly based on the user query and the preceding documents, calculated either by the carefully designed heuristics (e.g., the scoring functions in MMR and x AD) or as a linear combination of the handcra ed relevance features and novelty features (e.g., the scoring function in R-LTR). e utility perceived by the users from the preceding documents, however, is not explicitly modeled and fully utilized.",0,,False
"In this paper we propose to formalize the construction of a diverse ranking as a process of sequential decision making, which can",0,,False
535,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"be modeled with a continuous state Markov decision process (MDP). e new diverse ranking model, referred to as MDP-DIV, model",0,,False
"the user perceived utility of information as a part of its MDP state. Speci cally, in MDP-DIV, a document ranking with M positions is considered as a sequence of M discrete time steps where each time step corresponds to a ranking position. e ranking of documents, thus, is formalized as a sequence of M decisions and each action corresponds to selecting one document from the candidate set. At each time step, the agent receives the environment's state, which models the user's dynamic state on the perceived utility, starting from the rst ranking position. Based on the received state, the agent chooses an action. One time step later, as a consequence of the action, the search users perceive some additional utility from the new selected document, and the system transit to a new state.",1,ad,True
"e transition function, which maps old state and the selected document to a new state, is implemented in a recurrent manner. At each time step, the chosen of the action depends on a policy, which is a function maps from the current state to a probability distribution of selecting each actions.",0,,False
"Reinforcement learning is employed to train the model parameters. Given a set of labeled queries, at each time step, the agent can receive a numerical action-dependent reward which can be de ned upon the diversity evaluation measures. e policy gradient algorithm of REINFORCE [22] is adopted to adjust the model parameters so that expected long-term discounted rewards in terms of the diversity evaluation measure is maximized. In the testing phase, the system fully trusts the learned policy. Given a query and the associated documents, the action with the maximal probability is selected at each ranking position.",1,ad,True
"Advantages of the proposed model include: 1) explicitly modeling the dynamic state on the user perceived utility of information in diverse ranking learning, which uni es the relevance and novelty and can be utilized as the criterion for selecting documents; 2) ability to conduct diverse ranking learning in an end-to-end manner, achieving a diverse ranking model with no need of handcra ing features; 3) ability to learn a ranking model towards to a diversity evaluation measure, via involving the measure in the training.",0,,False
"To evaluate the e ectiveness of MDP-DIV, we conducted experiments on the basis of TREC benchmark datasets. e experimental results showed that MDP-DIV can signi cantly outperform the state-of-the-art diverse ranking approaches including the heuristic methods of MMR, x AD, and the learning methods of R-LTR, PAMM, and PAMM-NTN. We analyzed the results and showed that MDP-DIV improved the performances through 1) optimizing the diversity evaluation measures in training, 2) modeling the dynamic user state on the perceived utility, and 3) utilizing both the immediate rewards and the long-term returns in training phase.",1,TREC,True
2 RELATED WORK,0,,False
2.1 Search result diversi cation,0,,False
It is a common practice to formalize the construction of a diverse ranking list in search as a process of sequential document selection.,0,,False
is is based on the observation that in diverse ranking the additional utility a document can provide depends on not only the document itself but also the preceding documents. Di erent models designed di erent criteria for estimating the utility the search users,1,ad,True
"can perceive from a candidate document. Following the idea, Carbonell and Goldstein [2] proposed the maximal marginal relevance criterion to guide the selection of the documents. At each iteration, the document with the highest marginal relevance score is selected, where the score is a linear combination of the query-document relevance and the maximum distance of the document to the documents in current result set, in other words, novelty. e marginal relevance score is then updated in the next iteration as the number of documents in the result set increases by one. Based on MMR, Guo and Sanner [7] proposed the probabilistic latent MMR model. x AD [19] directly models di erent aspects underlying the original query in the form of sub-queries, and estimates the utility as the relevance of the retrieved documents to each identi ed sub-query. PM-2 [5] treats the problem of nding a diverse search result as nding a proportional representation for the document ranking. Hu et al. [9] proposed a utility function that explicitly leverages the hierarchical intents of queries and selects the documents that maximize diversity in the hierarchical structure. Evaluation methods have also developed based on the intent hierarchies [23]. He et al. [8] proposed to combine the implicit and explicit topic representations for constructing be er diverse rankings. Gollapudi and Sharma [6] proposed an axiomatic approach to result diversi cation.",0,,False
"Machine learning techniques, which automatically learn the ranking models from the human labeled data, have been applied to construct diverse ranking models. Most of learning approaches still adopt sequential document selection as the basic framework, and the additional utility a candidate document can provide is usually modeled as a sum of the relevance score and the novelty score. For example, Zhu et al. [31], Xia et al. [24], and Xu et al. [26] employed a set of handcra ed relevance features and novelty features to calculate the relevance score and the novelty score, respectively. Both of the scores are de ned as linear combinations of the features. Xia et al.[25] proposed to model the novelty score with the deep learning model of neural tensor networks. SVM-DIV [18] propose to construct a diverse ranking with the diversity criterion only. Structured output learning [11] and deep learning models [15] have also been employed to address the problem of learning diverse rankings.",1,ad,True
"Existing methods calculates the ranking scores directly based on the query and the selected documents, with the heuristic rules or the ranking features. ough it is a critical issue for constructing optimal diverse rankings, the dynamic utility the search user perceived from the preceding documents is still not explicitly modeled and fully utilized in current diverse ranking methods.",0,,False
2.2 MDP for information retrieval,0,,False
"In this paper we employ MDP for constructing diverse ranking model, which has been widely used in variant IR applications. For example, in [13], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [30], the log-based document re-ranking is also modeled as a POMDP to improve the re-ranking performances. MDP is also used for building recommender systems. For example, [21] designed an MDP-based recommendation model for taking",0,,False
536,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"both the long-term e ects of each recommendation and the expected value of each recommendation into account. Besides the MDP, researchers also employed the bandits model for constructing diverse ranking [18] and optimizing IR system [28].",0,,False
"Recent advances in deep learning makes it possible to incorporate deep learning methods with sequential decision making. In the literature of vision, Mnih et al. [16] a empts to implement a entional processing in a deep learning framework. Lu and Yang[12] proposes POMDP-Rec, a neural-optimized POMDP algorithm, for building a collaborative ltering recommender system.",1,ad,True
"ough MDP has been applied to various information retrieval tasks, applying it to learning to rank and search result diversi cation is hard. e di culties lie in how to formalize diverse ranking under the MDP framework and how to convert the human labels to the supervision information that can be utilized by MDP. In this paper, we propose to formulate the diverse ranking learning as a problem of learning an MDP model.",0,,False
3 MARKOV DECISION PROCESS,0,,False
"In the paper, we employ continuous state MDP[17, 22], a widely used sequential decision making model, for learning the diverse ranking. An MDP is composed by states, actions, rewards, policy, and transitions, and represented by a tuple S, A,T , R,  :",0,,False
"States S is a set of states. For instance, in this paper we de ne the state as a tuple consisting of preceding document ranking, candidate documents, and the utility the user perceived from the preceding documents.",0,,False
"Actions A is a discrete set of actions that an agent can take. e actions available may depend on the state s, denoted as A(s).",0,,False
"Transition T is the state transition function st+1 ,"" T (st , at ) which speci es a function which maps a state st into a new state st+1 in response to the action selected at .""",0,,False
"Reward r ,"" R(s, a) is the immediate reward, also known as reinforcement. It gives the immediate reward of taking action a at state s.""",0,,False
"Policy  (a|s) describes the behaviors of an agent, which is a probability distribution over the possible actions.  is usually optimized to decide how to move around in the state space to optimize the long term return.",0,,False
"e agent and environment interact at each of a sequence of discrete time steps, t ,"" 0, 1, 2, · · · . At each time step t the agent receives some representation of the environment's state, st  S, and on that basis selects an action at  A(st ), where A(st ) is the set of actions available in state st . One time step later, in part as a consequence of its action, the agent receives a numerical reward, rt +1  R and nds itself in a new state st +1 "","" T (st , at ). Figure 1 illustrates the agent-environment interaction in MDP.""",0,,False
4 MDP FORMULATION OF DIVERSE RANKING,0,,False
"In this paper, we employ the continuous state MDP to model the construction of the diverse ranking.",0,,False
4.1 e basic model,0,,False
"Suppose we are given a query q, which is associated with a set of retrieved documents X ,"" {x1, · · · , xM }  X, where both the""",0,,False
state st,0,,False
"st ,"" [Zt, Xt, ht]""",0,,False
reward rt,0,,False
"rt ,"" R(st, at)""",0,,False
rt+1,0,,False
st+1,0,,False
Agent,0,,False
action at,0,,False
sample at  ,0,,False
Environment,0,,False
Figure 1: e agent-environment interaction in MDP.,0,,False
"query q and the documents xi are represented as L-dimensional preliminary representations, i.e., the vectors learned by the doc2vec",0,,False
"model [10], and X is the set of all possible documents. e goal of",0,,False
diverse ranking is to construct a model that can rank the documents,0,,False
so that the top ranked documents cover a wide range of subtopics,0,,False
for a search query.,0,,False
"Supervised learning approaches can be used to construct the model. Suppose we are given N labeled training queries {(q(n), X (n), (n))}nN,""1, where (n) denotes the human labels on the documents, in the form of a binary matrix. (n)(i, j) "", 1 if document xi(n) contains the j-th subtopic of q(n) and 0 otherwise1. e learning of a",0,,False
"diverse ranking model, thus, can be considered as the learning the",0,,False
parameters in an MDP model in which each time step corresponds,0,,False
"to a ranking position. e states, actions, rewards, transitions, and",0,,False
policy of the MDP are set as:,0,,False
"States S: We design the state at time step t as a triple st ,"" [Zt , Xt , ht ], where Zt "", {x(n)}nt ,""1 is the sequence of t preceding documents, where x(n) is the document ranked at position n. Note that we de ne Z0 "",""  is a empty sequence; Xt  2X is the set of candidate documents; ht  RK is a vector that encodes the user perceived utility from preceding documents in Zt , as well as the information need based on q.""",0,,False
"At the beginning (t ,"" 0), the state is initialized as s0 "", [Z0 ,"" , X0 "","" X , h0]: Z0 is initialized as an empty sequence , the candidate set X0 contains all of the M documents in X , and h0 is initialized as the user's initial information needs, implemented with a""",0,,False
nonlinear transformation of the query:,0,,False
"h0 ,""  (Vq q),""",0,,False
(1),0,,False
"where q  RL is the preliminary representation of the user issued query, Vq  RK×L is the transformation matrix, and  (x) is the",0,,False
nonlinear sigmoid function applied to each of the entries:,0,,False
" (x) ,""  ( x1, · · · , xK ) "",",0,,False
1,0,,False
1 + e-x1,0,,False
",",0,,False
·,0,,False
·,0,,False
·,0,,False
",",0,,False
1,0,,False
+,0,,False
1 e -x K,0,,False
.,0,,False
"Actions A: At each time step t, the A(st ) is the set of actions the agent can choose, each corresponds to selecting a document",0,,False
"from Xt . at is, the action at the time step t, at  A(st ) selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at .",0,,False
"1Some datasets also use graded judgements. In this paper, we assume that all labels are binary.",1,ad,True
537,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"TransitionT : e transition functionT : S ×A  S also consists of three parts, as shown in the following Equation (2):",0,,False
"st +1 ,"" T (st , at )""",0,,False
","" T ([Zt , Xt , ht ], at )""",0,,False
(2),0,,False
","" Zt  {xm(at )}, Xt \ {xm(at )},  (Vxm(at ) + Wht ) ,""",0,,False
"where  concatenates the old sequence Zt with xm(at ), V  RK×L is the document-state transformation matrix, and W  RK×K is the state-state transformation matrix. At each time step t, based on state st the system chooses an action at . en, the system moves to time step t + 1 and the system transits to a new state st+1: First, the system appends the selected document to the end of Zt , generating a new document sequence; Second, the selected document at step t is removed from the candidate set: Xt +1 ,"" Xt \ {xm(at )}. us, the number of actions the agent can choose at step t + 1 is reduced by one. ird, the information from the user's last state and the selected document are combined together to form a new user state.""",0,,False
"Note that in the initialization of h, the parameter Vq is used for transforming the query to state. In the state transformation, another parameter V is used for transforming the selected document to state.",0,,False
e se ing is based on the consideration that they have di erent goals: Vq is for transforming the query q which represents the information needs of the search users; V is for transforming the documents x which contain the utility that can be perceived by the users for ful lling the information needs.,0,,False
"Also note that though the state transition function is implemented in a recurrent fashion, they have striking di erence with recurrent neural networks (RNN): in MDP-DIV the input at time step t depends on the output (action) at the time step t - 1.",0,,False
"Reward R: e reward can be considered as an evaluation of the quality of the selected document. In search result diversi cation, the diversity evaluation measures are used to evaluate the quality of a ranking. Most of these measures are calculated in a sequential manner. us, it is natural to de ne the reward function on the basis of the diversity evaluation measures. For example, based on the diversity evaluation measure of -DCG, we can de ne the reward function as the promotion of -DCG caused by choosing the action at :",0,,False
"R -DCG(st , at ) ,""  -DCG[t + 1] -  -DCG[t],""",0,,False
"where -DCG[t] is the discounted cumulative gain [4] at the t-th position, and the -DCG value at the rank 0 is de ned as zero:  -DCG[0] , 0. 2",0,,False
"Similarly, on the basis of diversity evaluation measure of Srecall [29], we can also de ne another reward which is the promotion of S-recall by the action:",0,,False
"RS-recall(st , at ) ,"" S-recall[t + 1] - S-recall[t],""",0,,False
"where S-recall[t] is the S-recall value at the t-th position, and Srecall[0] , 0.",0,,False
"Since the training algorithm learns the model parameters under the supervision of the rewards, de ning the rewards according to a diversity evaluation measure can guide the training process to achieve an optimal model in terms of that evaluation measure.",0,,False
"2 e calculation of reward is based on the document sequence Zt in st , the selected documents xm(at ), and the relevance labels of these documents. Here we assume that the state st also contains the document labels in the training phase.",0,,False
"Policy  (a|s): e policy  : A × S  [0, 1] de nes the probabi-",0,,False
"lity of selecting each action. Given current state st ,"" [Zt , Xt , ht ] and a possible action at , the policy  is de ned as a normalized so -max function whose input is the bilinear product of the utility""",0,,False
and the selected document:,0,,False
"exp  (at |[Zt , Xt , ht ]) ,",0,,False
xTm(at )Uht Z,0,,False
",",0,,False
(3),0,,False
where U  RL×K is the parameter in the bilinear product and Z is the normalization factor:,0,,False
"Z,",0,,False
exp xTm(a)Uht .,0,,False
a A(st ),0,,False
"Construction of a diverse ranking for the queries in the training data can be formalized as: given a user query q, a set of M candidate documents X , and the corresponding human labels , the system state is initialized as s0 , [Z0 ,"" , X0 "","" X , h0 "",""  (Vq q)]. en, at each of the time steps t "","" 0, · · · , M - 1, the agent receives the state st "","" [Zt , Xt , ht ], chooses an action at which selects the document xm(at ) from the candidate set, and places it to the rank t +1. Moving to the next step t + 1, the state becomes st +1 "","" [Zt +1, Xt +1, ht +1]. On the basis of the human labels for the query, the agent receives immediate reward rt+1 "","" R([Zt , Xt , ht ], at ), which could be used as supervision for training the model parameters. e process is repeated until the candidate set becomes empty.""",0,,False
"Note that in online ranking/testing phase, there is no reward available because the queries are unlabeled. To construct a diverse ranking, we fully trust the learned policy and choose the action with maximal probability at each time step.",0,,False
"Next, we will discuss the o -line training algorithm and online ranking algorithm.",0,,False
4.2 Learning with policy gradient,1,ad,True
"e model has parameters  ,"" {Vq, U, V, W} to learn. Inspired by the REINFORCE [22] algorithm of policy gradient, we devised a novel algorithm which can learn the parameters toward the diversity evaluation measure. e algorithm is referred as MDP-DIV and shown in Algorithm 1. e Algorithm 2 shows the procedure of sampling an episode for Algorithm 1.""",1,ad,True
"e basic idea of Algorithm 1 is updating the parameters via Monte-Carlo stochastic gradient ascent. At each iteration, an episode (consisting a sequence of M states, actions, and rewards) is sampled according to current policy. en, at each time step t of the sampled episode, the model parameters are adjusted according to the gradients of the parameters  log  (at |st ; ), scaled by the step size , discount rate  t , and the long-term return Gt , where Gt is de ned as the discounted sum of the rewards from position t:",1,ad,True
M -1-t,0,,False
"Gt ,",0,,False
" k rt +k+1,",0,,False
(4),0,,False
"k ,0",0,,False
"where M , |X | is the number documents in the candidate set. Note",0,,False
"that if  ,"" 1, G0 is exactly the evaluation measure calculated at the nal rank of the document list, i.e., -DCG@M or S-recall@M.""",0,,False
"Intuitively, the se ing of Gt let the parameters move most in the directions so that the favor actions can yield the highest return.",0,,False
538,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Algorithm 1 MDP-DIV learning,0,,False
Algorithm 2 SampleEpisode,0,,False
"Input: Labeled training set D ,"" {(q(n), X (n), (n))}nN"",""1, learning rate , discount factor  , and reward function R""",0,,False
"Input: Parameters  ,"" {Vq, U, V, W}, q, X , , and R Output: An episode""",0,,False
"Output:  ,"" {Vq, U, V, W}""",0,,False
"1: Initialize s  [, X ,  (Vq q)]{Equation (1)}",0,,False
"1: Initialize  ,"" {Vq, U, V, W}  random values in [-1, 1] 2: repeat""",0,,False
"2: M  |X | 3: E , (){empty episode}",0,,False
"3: for all (q, X , )  D do",0,,False
"4: for t , 0 to M - 1 do",0,,False
4:,0,,False
"(s0, a0, r1, · · · , sM-1, aM-1, rM )  SampleEpisode(, q, X , , R) 5: A  A(s) {Possible actions according to X in state s}",0,,False
"{Algorithm (2), and M , |X |}",0,,False
6: for all a  A do,0,,False
5:,0,,False
"for t , 0 to M - 1 do",0,,False
7:,0,,False
P(a)   (a|s; ),0,,False
6:,0,,False
Gt ,0,,False
"M -1-t k ,0",0,,False
 k rt +k+1,0,,False
{Equation,0,,False
(4)},0,,False
7:,0,,False
   +  t Gt  log  (at |st ; ) {Equation (5)},0,,False
8:,0,,False
end for,0,,False
"8: end for 9: Sample an action a^  A, according to P 10: r  R(s, a^){Calculation on the basis of }",0,,False
9: end for,0,,False
"11: Append (s, a^, r ) to the tail of E",0,,False
10: until converge,0,,False
"12: [Z, X , h]  s",0,,False
11: return ,0,,False
"13: s  Z  {xm(a^)}, X \ {xm(a^)},  (Vxm(a^) + Wh)",0,,False
14: end for,0,,False
"15: return E ,"" (s0, a0, r1, · · · , sM-1, aM-1, rM )""",0,,False
"e gradient of MDP-DIV at time step t is  log  (at |st ; ), which the direction that most increase the probability of repeating",1,ad,True
"the action at on future visits to state st , and is de ned as",0,,False
"where Vht-1 can be unrolled in a similar way. At t ,"" 0, Vh0 is:""",0,,False
" log  (at |st ; ) ,  f (at |st )-",0,,False
"a At  f (a|st ) exp { f (a|st )} , a At exp { f (a|st )}",0,,False
(5),0,,False
"where f (a|st ) ,"" xTm(a)(Uht ), and  f (a|st ) "","" {U f (a|st ),""",0,,False
"Vq f (a|st ), V f (a|st ), W f (a|st ) , where",0,,False
"Vh0 , V (Vq q) ,"" 0K,L,1,K , where 0K,L,1,K  RK ×L×1×K is a tensor of zeros.""",0,,False
"Wht , W (Vxm(at-1) + Wht -1) , diag(ht  (1 - ht )) W(Vxm(at-1) + Wht -1)",0,,False
"U f (a|st ) , xm(a)hTt .",0,,False
","" diag(ht  (1 - ht )) IK,K,K,K ht -1 + (Wht -1) WT ,""",1,WT,True
"As for Vq f (a|st ), V f (a|st ), and W f (a|st ), they can be calculated in a similar way:",0,,False
"Vq f (a|st ) ,"" Vq ht UT xm(a),""",0,,False
"where IK,K,K,K  RK ×K ×K ×K is an identity tensor, and Wht -1 can be unrolled in a similar way. At t ,"" 0, Wh0 is:""",0,,False
"Wh0 , W (Vq q) ,"" 0K,K,1,K ,""",0,,False
"V f (a|st ) ,"" (Vht ) UT xm(a), W f (a|st ) "","" (Wht ) UT xm(a), where Vq ht , Vht , and Wht can be calculated recursively: Vq ht "", Vq  (Vxm(at-1) + Wht -1)",0,,False
"where 0K,K,1,K  RK ×K ×1×K is a tensor of zeros. Compared with conventional REINFORCE algorithm, MDP-DIV",0,,False
"is based on a modi ed MDP model in which the user state of perceived utility is initialized with query and modeled in a recurrent manner. us, in the training phase, MDP-DIV needs to estimate the policy function, as well as the functions for state initialization",0,,False
", diag(ht  (1 - ht )) Vq (Wht -1) ,"" diag(ht  (1 - ht )) Vq ht -1 WT ,""",1,WT,True
"and state transition. In [16], similar idea was presented for extracting information from images. In this paper we adapt the model for the task of search result diversi cation.",1,ad,True
"where 1 is an K-dimensional vector of ones, operator """" denotes the element-wise vector product, operator ""diag"" generates an K ×K diagonal matrix according to the input vector, and Vq ht-1 can be further unrolled in a similar way. At t ,"" 0, Vq h0 is:""",0,,False
"Vq h0 , Vq  (Vq q) ,"" diag(h0  (1 - h0))IK,L,K,Lq, where IK,L,K,L  RK×L×K×L is an identity tensor.""",0,,False
"Vht , V (Vxm(at-1) + Wht -1) , diag(ht  (1 - ht )) V(Vxm(at-1) + Wht -1) ,"" diag(ht  (1 - ht )) IK,L,K,Lxm(at-1) + (Vht -1) WT ,""",1,WT,True
4.3 Online ranking,0,,False
"In the online ranking, the ranking system receives a user query",0,,False
"q and the associated documents X ,"" {x1, · · · , xM }. Since there exists no human label for calculating the immediate rewards, the""",0,,False
system fully relies on the learned policy  for generating the diverse,0,,False
"ranking, as shown in Algorithm 3. A er initializing with q, the",0,,False
algorithm makes a sequence of greedy decisions: at each step the,0,,False
"action with the maximal probability is chosen (line 5 of Alglrithm 3),",0,,False
and the action in return update the state for choosing the next action,0,,False
(line 7 and line 8 of Algorithm 3).,0,,False
e time complexity of the online ranking algorithm is of,0,,False
O,0,,False
"min{K L2 ,",0,,False
LK,0,,False
2,0,,False
},0,,False
M,0,,False
(2+M 4,0,,False
),0,,False
+,0,,False
(M,0,,False
-,0,,False
1)(K 2,0,,False
+,0,,False
KL),0,,False
per query.,0,,False
e rst,0,,False
539,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Algorithm 3 MDP-DIV online ranking,0,,False
"Input: Parameters  ,"" {Vq, U, V, W}, query q, documents X Output: Permutation of documents """,0,,False
"1: Initialize s  [, X ,  (Vq q)]{Equation (1)} 2: M  |X | 3: for t ,"" 0 to M - 1 do 4: A  A(s) {Possible actions according to X in state s} 5: a^  arg maxa A  (a|s; ){Choosing most possible action} 6:  [t + 1]  m(a^){Document xm(a^) is ranked at t + 1} 7: [Z, X , h]  s 8: s  [Z  {xm(a^)}, X \ {xm(a^)},  (Vxm(a^) + Wh)] 9: end for 10: return """,0,,False
"part corresponds to calculating the policy for all of the possible actions at each iteration and the second part corresponds to updating the state for the next iteration. e term min{KL2, LK2} is for calculating the matrix multiplication xTm(at )U ht in the policy with di erent ways. In most cases L is larger than K. Please note that the online ranking algorithm actually runs M - 1 iterations for ranking M documents, because at the last iteration A(sM-1) contains only one action. Usually, K and L are not very large, e.g, we set K , 5 and L ,"" 100 in our experiments. us, the online ranking algorithm is e cient if the candidate set is not very large. In our experiments, on average it takes about 20 milliseconds for ranking about 200 documents, on a server with 24GB memory and two Intel Xeon E5410 2.33GHz ad-Core processors. Note that in the analysis the time for document embedding is not taken into consideration as the document embeddings can be calculated o ine.""",1,ad,True
4.4 eoretical analysis,0,,False
"e learning phase of MDP-DIV tries to optimize general diversity evaluation measures with reinforcement learning. e measures can be -DCG and S-recall, or any other measures that can be calculated at each of the ranking position. We explain why this is the case.",0,,False
"In the training, Monte-Carlo stochastic gradient ascent is used to conduct the optimization. Given a query q in the training set, we want to maximize the value V , which is the expected return of the query:",1,ad,True
"max V (q) ,"" E G0,""",0,,False
"where G0 is the discounted sum of the rewards, starting from position 0, as de ned in Equation (4). Please note G0 is the diversity evaluation measure if  ,"" 1. us, maximizing V (q) is actually maximizing the expected diversity evaluation measure for the query.""",0,,False
"According to the policy gradient theorem presented in [22], chapter 13, the gradient of the performance metric with respect to the parameters  on each query can be calculated as",1,ad,True
"V () ,"" Es,a Q (s, a) (a|s),""",0,,False
"where  is the discounted state distribution given a query q and model parameters, which is de ned as:",0,,False
"(s |q; ) ,""  t -1p(s0  s, t |qn ; ),""",0,,False
"t ,1",0,,False
· · · q,0,,False
"h0 , (Vqq)",0,,False
"T (a0, s0)",0,,False
"s0 ,"" [Z0, X0, h0]""",0,,False
"T (aM 2, sM 2) sM 1 ,"" [ZM 1, XM 1, hM 1]""",0,,False
a0,0,,False
",",0,,False
arg,0,,False
max,0,,False
a,0,,False
(a|s0),0,,False
xm(a0),0,,False
aM,0,,False
1,0,,False
",",0,,False
arg,0,,False
max,0,,False
a,0,,False
(a|sM,0,,False
1),0,,False
xm(aM 1),0,,False
" [1] , m(a0)",0,,False
···,0,,False
" [M ] , m(aM 1)",0,,False
Figure 2: Online document ranking in MDP-DIV.,0,,False
"where p(s0  s, t |qn ; ) is the probability of transitioning from the initial state s0 given the query q in t steps [22]. Q (s, a) is the expected return starting from s, taking the action a and therea er following the policy  :",0,,False
"Q (s, a) , E [Gt |st ,"" s, at "", a].",0,,False
Monte-Carlo method is used to estimate the gradient. Speci -,1,ad,True
"cally, given a sampled episode s0, a0, r1, · · · , sM-1, aM-1, rM and a speci c time step t, the gradient can be estimated as [22]",1,ad,True
"V () ,  sample t",0,,False
" (a|st )Q (st , a)",0,,False
a A(st ),0,,False
", t",0,,False
 (a|st ) ·,0,,False
a A(st ),0,,False
Q,0,,False
(st,0,,False
",",0,,False
a),0,,False
 (a|st  (a|st ),0,,False
),0,,False
",  sample",0,,False
t,0,,False
Q,0,,False
(st,0,,False
",",0,,False
at,0,,False
),0,,False
 (at |st  (at |st ),0,,False
),0,,False
",  sample t Gt  log  (at |st ).",0,,False
"e rst ,"" sample replaces s by its sample st , which is sampled according to ; the second "","" sample replaces a by its sample at , which is sampled according to  ; and the third "", sample replaces the therea er",0,,False
decision process guided by  with the sampled episode. Note that,0,,False
"E [Gt |st , at ] ,"" Q (st , at ) and  log  (at |st ) "",",0,,False
 (at |st (at |st ),0,,False
),0,,False
.,0,,False
We can see that the updating rule in Algorithm 1 exactly follows,0,,False
"the estimated gradients presented above. us, we can conclude",1,ad,True
MDP-DIV tries to optimize general diversity evaluation measures,0,,False
"with Monte-Carlo stochastic gradient ascent when  , 1.",1,ad,True
4.5 Advantages,0,,False
"MDP-DIV provides an elegant approach to modeling user's dynamic state on the perceived utility during the browsing of the diverse ranking results. More importantly, it is a method that can be justi-",0,,False
"ed from the theoretical viewpoint, as discussed above. In addition, MDP-DIV has several other advantages when compared with the existing diverse ranking learning methods such as SVM-DIV, R-LTR and PAMM etc.",1,ad,True
"First, MDP-DIV can conduct an end-to-end learning of the diverse ranking model, which achieves a model with no need of handcra ing relevance features and novelty features. e inputs to the ranking model are the preliminary representations of the queries and the documents, e.g., the distributed representations learned by the doc2vec model. In contrast, all existing diverse ranking learning methods heavily depend on the handcra ed relevance",0,,False
540,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"features and/or novelty features. It has been widely observed that high quality features are critical for constructing diverse ranking, while designing the features, especially designing the novelty features, is very di cult in real applications [25]. MDP-DIV solves the issue via learning a ranking model that needs only the preliminary representations of the queries and the documents.",0,,False
"Second, MDP-DIV utilizes both the immediate rewards and the long-term returns as the supervision information during its training. Speci cally, given an episode, the parameters are updated a er receiving each of the immediate rewards (line 5-8 of Algorithm 1). Meanwhile, the updating rule also utilizes the long-term return Gt , which accumulates all of the future rewards (line 6-7 of Algorithm 1), to re-scale the step size. In contrast, existing methods that directly optimize evaluation measures are only based on a evaluation measure calculated at a xed position [24, 26] on the basis of whole ranking. Our empirical analysis in Section 5.3.2 also showed that training with both the rewards and the returns can achieve be er ranking accuracies.",0,,False
"ird, MDP-DIV makes use of a uni ed criterion, the additional utility a search user can perceive, for selecting documents at each iteration. In contrast, the criterion adopted by most existing methods, e.g., the marginal relevance, consists of two individual factors: the relevance and the novelty. Heuristic diverse ranking model x AD tried to replace these two factors with ""the relevance to the underlying sub-queries"", which has shown to be more reasonable and e ective. In this paper, we also showed that under the MDP framework, the document selection criterion can be uni ed to ""the perceived utility"", which has shown to be simple in concept and be powerful in the real applications.",1,ad,True
5 EXPERIMENTS,0,,False
"We conducted experiments to test the performances of MDP-DIV using a combination of four TREC benchmark datasets: TREC 2009 Web Track (WT2009), TREC 2010 Web Track (WT2010), TREC 2011 Web Track (WT2011), and TREC 2012 Web Track (WT2012).",1,TREC,True
5.1 Experimental settings,0,,False
"e training of MDP-DIV model need lots of labeled queries because it has a large number of parameters. In experiments, for e ective training of the model parameters, we combined four TREC datasets, achieving a new dataset with 200 queries, and in total about 45,000 labeled documents. Each query includes several subtopics identi ed by the TREC assessors. e document relevance labels are made at the subtopic level and the labels are binary3.",1,TREC,True
"All the experiments were carried out on the ClueWeb09 Category B data collection4, which is comprised of 50 million English web documents. Porter stemming, tokenization, and stop-words removal (using the INQUERY list) were applied to the documents as preprocessing. For each query, the initial ranking is generated by",1,ClueWeb,True
"ery-likelihood language model[14]. We conducted 5-fold crossvalidation experiments. We randomly split the queries into ve even subsets. At each fold, three subsets were used for training, one was used for validation, and one was used for testing. e results reported were the average over the ve trials.",0,,False
3WT2011 has graded judgements. In this paper we treat them as binary. 4h p://boston.lti.cs.cmu.edu/data/clueweb09,1,WT,True
e TREC o cial evaluation metrics for the diversity task were,1,TREC,True
"used in the experiments, including the ERR-IA [3] and -NDCG [4].",0,,False
ey measure the diversity of a result list by explicitly rewarding,0,,False
diversity and penalizing redundancy observed at every rank. Follo-,0,,False
"wing the default se ings in o cial TREC evaluation program, the",1,TREC,True
parameter  in these evaluation measures are set to 0.5. We also,0,,False
used traditional diversity measures of subtopic recall (denoted as,1,ad,True
S-recall) [29]. All of the measures are computed over the top-k,0,,False
"search results (k , 5 and k , 10).",0,,False
We compared MDP-DIV with several state-of-the-arts baselines,0,,False
"in search result diversi cation, including the heuristic methods:",0,,False
MMR [2]: a heuristic approach in which the document is se-,0,,False
lected according to maximal marginal relevance.,0,,False
x AD [19]: a representative approach which explicitly models,0,,False
di erent aspects underlying the original query in the form of sub-,0,,False
queries.,0,,False
PM-2 [5]: a method of optimizing proportionality for search,0,,False
result diversi cation.,0,,False
We also compared MDP-DIV with the learning methods:,0,,False
SVM-DIV [27]: a learning approach which utilizes structural,0,,False
SVMs to optimize the subtopic coverage.,0,,False
R-LTR [31]: a state-of-the-art learning approach developed in,0,,False
the relational learning to rank framework. Following the practice,0,,False
"in [31], we used the results of R-LTRmin in which the relation function was de ned as the minimal distance of the candidate",0,,False
document to the selected documents,0,,False
PAMM [24]: another learning algorithm under R-LTR frame-,0,,False
work. PAMM directly optimizes diversity evaluation measure using,0,,False
"structured Perceptron. Following the practice in [24], we con gu-",0,,False
red the PAMM algorithm to directly optimize -NDCG@10 in our,0,,False
"experiments, and set the number of sampled positive rankings per query  + , 5 and the number of sampled negative rankings per",0,,False
"query  - , 20.",0,,False
NTN-DIV: a learning approach which automatically learns no-,0,,False
velty features based on neural tensor networks. Following the,0,,False
"practice in [25], we con gured the learning of NTN-DIV algorithm",0,,False
to directly optimize -NDCG@10 and the number of tensor slices,0,,False
is 7.,0,,False
MDP-DIV and the baseline of NTN-DIV need preliminary re-,0,,False
presentations of the queries and the documents as their inputs. In,0,,False
"the experiments, we used the query vector and document vector",0,,False
generated by the doc2vec [10] to represent the document. Doc2vec,0,,False
model was trained on all of the documents in Web Track dataset,1,Track,True
"and the number of vector dimensions were set to 100. For training the model, we used the distributed bag of words (DBOW) model5.",0,,False
e learning rate is set to 0.025 and the window size is set to 8.,0,,False
e MDP-DIV also has some parameters. e reward function,0,,False
"in MDP-DIV was set as either R -DCG or RS-recall, denoted as MDPDIV(-DCG) and MDP-DIV(S-recall), respectively. In all of the ex-",0,,False
"periments, the learning rate  is tuned on the basis of the validation",0,,False
"set. We set the discounting parameter  ,"" 1, which means that the""",0,,False
"return is the undiscounted sum of the future rewards, which makes",0,,False
"the long term return in Equation (4) becomes Gt ,",0,,False
"M -1-t k ,0",0,,False
rt,0,,False
+k +1 .,0,,False
It makes the training algorithm optimizes the diversity evaluation,0,,False
measure of -DCG and S-recall.,0,,False
5h p://radimrehurek.com/gensim/tutorial.html,1,ad,True
541,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
5.2 Experimental results,0,,False
"Table 1 reports the performances of our approach and all of the baseline methods in terms of the six diversity performance metrics, including -NDCG@5, -NDCG@10, S-recall@5, S-recall@10, ERRIA@5, and ERR-IA@10. Boldface indicates the highest score among all runs. From the results we can see that, in terms of the six diversity evaluation metrics, both MDP-DIV(-DCG) and MDPDIV(S-recall) outperformed all of the baseline methods, including the heuristic method of MMR, x AD, PM-2 and learning methods of R-LTR, PAMM(-NDCG), and NTN-DIV(-NDCG). We conducted signi cance testing (t-test) on the improvements of our approaches over the best baseline NTN-DIV(-NDCG). e results indicate that the improvements are signi cant (p-value < 0.05), in terms of all of the evaluation measures.",0,,False
"Comparing the results of the MDP-DIV(-DCG) and MDP-DIV(Srecall), we can see that MDP-DIV(-DCG) trained with -DCG (se ing -DCG as reward function) performed be er in terms of NDCG@5 and -NDCG@10. Similarly, MDP-DIV(S-recall) trained with S-recall (se ing S-recall as reward function) performed be er in terms of S-recall@5 and S-recall@10. e results indicate that MDP-DIV can indeed enhance diverse ranking performance in terms of a measure by using the measure as reward function in training6. e result agrees well with the theoretical analysis shown in Section 4.4.",0,,False
5.3 Discussion,0,,False
"We conducted experiments to show the reasons that MDP-DIV outperformed the baselines, using the results of MDP-DIV(-DCG) on one trial of the cross validation as examples.",0,,False
"5.3.1 E ects of modeling user perceived utility. We analyzed how the user state on the perceived utility e ects the selection of documents in MDP-DIV. Speci cally, based on the trained MDP-DIV model, we tracked the online ranking process for query number 93 ""ambiguous"", which contains ve subtopics. Figure 3 shows the details of the rst three document selection steps, including the transition of the user dynamic state hi , the ranking score f (at |st ) ,"" xTm(at )Uht for each of the actions7, and the constructed document ranking. Due to the space limitation, we only showed the ve top ranked documents d1, · · · , d5, corresponding to the documents of enwp03-28-04544, en0007-80-16124, en0094-80-42411, en0006-08-03878, and en0010-24-38000 in the Clueweb09 collection, respectively. e subtopics covered by each of the documents are shown in the square brackets.""",1,Clue,True
"From Figure 3, we can see that ht was updated a er choosing each action, indicating the changes of the user state a er perceiving the utility provided by the selected document. At step 0, the selected document d2 covered subtopics 3 and 5. At step 1, as the consequence of the action the user state was updated, and the ranking score of d4 (with the covered subtopic 5) was suppressed from 0.46 to 0.35, while the ranking scores of the other three documents (d1, d3, and d5, with uncovered subtopics) were promoted. e results indicate that the user state h1 captured the utility provided",0,,False
"6Here we consider  -NDCG and  -DCG as ""one"" measure as the only di erence between them is the normalization factor. 7In the online ranking, the selection of actions can be implemented as directly based on the ranking scores instead of based on the probabilities  (at |st ).",1,ad,True
h0,0,,False
h1,0,,False
0.50,0,,False
0.56,0,,False
0.53,0,,False
0.53,0,,False
0.49,0,,False
0.55,0,,False
0.40,0,,False
0.51,0,,False
q,0,,False
0.60,0,,False
doc:subtopics,0,,False
ranking score,0,,False
0.61,0,,False
doc:subtopics,0,,False
ranking score,0,,False
"{ d1 : [2] 0.51 d2 : [3, 5] 1.19 d3 : [1, 4] 1.05 d4 : [5] 0.46",0,,False
"{ d1 : [2] 0.84 d3 : [1, 4] 1.07 d4 : [5] 0.35 d5 : [1, 4] 1.12",0,,False
"d5 : [1, 4] 1.01",0,,False
ranking: h,0,,False
"d2 : [3, 5]",0,,False
"d5 : [1, 4]",0,,False
h2,0,,False
0.53,0,,False
0.48,0,,False
0.51,0,,False
···,0,,False
0.42,0,,False
0.59,0,,False
doc:subtopics,0,,False
ranking score,0,,False
"{ d1 : [2] 0.89 d3 : [1, 4] 0.84",0,,False
d4 : [5] 0.34,0,,False
d1 : [2],0,,False
··· i,0,,False
Figure 3: e online ranking process for query number 93.,0,,False
"by d2, which made the ranking model focusing on documents that can provide the largest amount of new information. As the result, d5, which contains two new subtopics 1 and 4, was selected at step 1. Similarly, at step 2 state vector h2 captured the utility provided by d2 and d5 and making the model to select d1, which contains a new subtopic 2. In contrast, the ranking scores for d3 and d4, whose subtopics had been covered by the preceding documents, were suppressed. e phenomenon clearly indicates MDP-DIV can e ectively capture the user perceived utility of information in its state, and utilize it for generating diverse rankings.",1,ad,True
"5.3.2 E ects of using immediate rewards in training. One advantage of MDP-DIV is that it has the ability of utilizing the immediate rewards as the supervision in training, which makes the training more e ective and e cient. We tried to verify the e ectiveness and e ciency of using the immediate rewards in the training phase. Speci cally, we modi ed the training Algorithm 1 so that the model parameters were updated only at the end of an episode (i.e., se ing the iteration variable t in the line 5 of Algorithm 1 starts from M). In this way, the modi ed algorithm only utilizes the long term return of the whole episode for training, denoted as ""MDPDIV(ReturnOnly)"". Figure 4 shows the performance curves of MDPDIV(-DCG) and MDP-DIV(ReturnOnly) trained with -DCG, on the test data of one trail in the cross validation. e performances of other baseline methods on the same cross validation trail are also shown in the gure.",1,ad,True
"From the results, we can see that MDP-DIV(-DCG) outperformed the MDP-DIV(ReturnOnly) in terms of both convergency rate and the converged performances. e result indicates that utilizing the immediate rewards in MDP-DIV(-DCG) leads to an e ective and e cient training algorithm. Note that in contrast, most existing learning approaches to diverse ranking, including R-LTR, PAMM, and NTN-DIV, can only utilize the accumulated information on the whole ranking as supervision in their training phase. For example, R-LTR uses the likelihood of the whole document rankings, and PAMM uses the prede ned evaluation measure calculated based on the whole ranking. e experimental results showed one reason why MDP-DIV(-DCG) can outperform these baselines.",1,ad,True
"We also noticed that the converged MDP-DIV(ReturnOnly) model still outperformed the baseline methods including SVM-DIV, R-LTR, PAMM, and NTN-DIV, indicating that modeling the user's dynamic state on the perceived utility with MDP is e ective.",0,,False
542,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Performance comparison of all methods on TREC web track datasets.,1,TREC,True
Method,0,,False
MMR x AD PM-2 SVM-DIV R-LTR PAMM( -NDCG) NTN-DIV( -NDCG) MDP-DIV(S-recall) MDP-DIV( -DCG),0,,False
 -NDCG@5,0,,False
0.2753 0.3165 0.3047 0.3030 0.3498 0.3712 0.3962 0.4156 0.4189,0,,False
 -NDCG@10,0,,False
0.2979 0.3941 0.3730 0.3699 0.4132 0.4327 0.4577 0.4734 0.4762,0,,False
S-recall@5,0,,False
0.4388 0.4933 0.4910 0.5122 0.5397 0.5561 0.5817,0,,False
0.6123 0.6102,0,,False
S-recall@10,0,,False
0.5151 0.6043 0.6012 0.6230 0.6511 0.6612 0.6872,0,,False
0.7155 0.7117,0,,False
ERR-IA@5,0,,False
0.2005 0.2314 0.2298 0.2268 0.2521 0.2619 0.2773 0.2963 0.2988,0,,False
ERR-IA@10,0,,False
0.2309 0.2890 0.2814 0.2726 0.3011 0.3029 0.3285 0.3477 0.3494,0,,False
0.55,0,,False
3,0,,False
0.45,0,,False
0.35,0,,False
0.25,0,,False
0.15 0,0,,False
MDP-DIV(ReturnOnly) NTN-DIV PAMM R-LTR SVM-DIV,0,,False
500,0,,False
1000,0,,False
1500,0,,False
2000,0,,False
2500,0,,False
iteration,0,,False
"Figure 4: e performance curves on the test data for MDPDIV(-DCG), and the modi ed MDP-DIV(-DCG) in which the training only involves the long-term returns. e performances of other baselines are shown as horizontal lines.",0,,False
2.5,0,,False
2,0,,False
train(sample) train(arg max) test(arg max),0,,False
1.5,0,,False
0,0,,False
500,0,,False
1000,0,,False
1500,0,,False
2000,0,,False
2500,0,,False
iteration,0,,False
"Figure 5: e performance curves in terms of -DCG on the training data (""train(arg max)"") and the test data (""test(arg max)""). e average performances of the sampled rankings over all training queries are also shown (""train(sample)"").",0,,False
"5.3.3 Analysis of convergence and online ranking criterion. We conducted experiments to test whether the ranking accuracy in terms of the evaluation measure can be continuously improved, as the training of MDP-DIV goes on.",0,,False
"Speci cally, we tested the MDP-DIV(-DCG) models generated at each of the training iteration in one trail of the cross validation.",0,,False
"e performances in terms of -DCG at the last position of the whole document ranking is reported. For each model, the average performances over all of the training queries (or the testing queries) are reported. Figure 5 shows the performance curves on the training data (solid red line and denoted as ""train(arg max)"") and on the test data (dashed yellow line and denoted as ""test(arg max)""). For these two curves, the document rankings for the queries are generated by the online ranking Algorithm 3. e document rankings can also be generated through sampling during the training, via the episode sampling Algorithm 2. e average performances of the sampled rankings for all the training queries are also shown in the",0,,False
"gure (blue dots and denoted as ""train(sample)""). From the results shown in Figure 5, we can see that on both of",0,,False
"the training set and test set, the ranking accuracies of MDP-DIV(DCG) steadily improves, as the training goes on. e experimental",1,ad,True
"results also showed that the ranking accuracies of the sampled rankings (by Algorithm 2) has an obvious trend of steadily improving with some random noise, as the training goes on.",1,ad,True
"Comparing the sampled rankings (""train(sample)"") and the ranking generated by the online ranking algorithm (""train(arg max""), we can see that at the beginning of the training phase, the sampled rankings can achieve be er -DCG values than the rankings generated by the online ranking algorithm, on the basis of the training queries. As the training went on and a er about 200 iterations, the online ranking algorithm outperformed the sampling method, and the trend remains to the end of the training. e phenomenon was repeated in other experiments. We analyzed the reasons.",0,,False
"e online ranking algorithm (Algorithm 3) fully trusts the learned ranking model when generating the document ranking, i.e., a^  arg maxa A  (a|s; ). In contrast, the sampled rankings are generated according to the same ranking model while with some randomness. At the beginning of the training phase, the model parameters are far from their optimal values. In many cases, fully trusting the policy leads to bad decisions and generating rankings with low performances. e sampling method, in contrast, may make be er decisions due to the random natural of sampling. As",1,ad,True
543,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"the training goes on, the model parameters gradually converge to nearly optimal values. Fully trusting the learned policy has the advantages of achieving stable and (nearly) optimal decisions in most cases. e sampling method, however, hurts from unstable results due to the random noise. e results clearly indicate that, fully trusting the learned model (as that of in Algorithm 3) in the online ranking phase is a good criterion, given the model is well trained.",1,ad,True
6 CONCLUSION AND FUTURE WORK,0,,False
"In this paper we have proposed a novel approach to learning diverse ranking model for search result diversi cation, referred to as MDP-DIV. In contrast to existing methods, MDP-DIV explicitly models the dynamic utility the search users perceived during the browsing of the ranking result. e dynamic utility is modeled with a continuous state MDP and the model parameters are estimated with reinforcement learning. MDP-DIV o ers several advantages: no need for handcra ing ranking features, optimizing diversity evaluation measures in training, utilizing both immediate rewards and long-term returns as supervision, and high accuracy in ranking. Experimental results based on the TREC benchmark datasets show that MDP-DIV can signi cantly outperform the baseline methods.",1,ad,True
7 ACKNOWLEDGEMENTS,0,,False
"e work was funded by the National Key R&D Program of China under Grant No. 2016QY02D0405, 973 Program of China under Grant No. 2014CB340401 and 2012CB316303, the National Natural Science Foundation of China (NSFC) under Grants No. 61232010, 61472401, 61433014, 61425016, and 61203298, the Key Research Program of the CAS under Grant No. KGZD-EW-T03-2, and the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102.",0,,False
REFERENCES,0,,False
"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In Proceedings of the Second ACM International Conference on Web Search and Data Mining (WSDM '09). ACM, New York, NY, USA, 5­14.",0,,False
"[2] Jaime Carbonell and Jade Goldstein. 1998. e Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '98). ACM, New York, NY, USA, 335­336.",1,ad,True
"[3] Olivier Chapelle, Shihao Ji, Ciya Liao, Emre Velipasaoglu, Larry Lai, and SuLin Wu. 2011. Intent-based Diversi cation of Web Search Results: Metrics and Algorithms. Inf. Retr. 14, 6 (Dec. 2011), 572­592. DOI:h p://dx.doi.org/10.1007/ s10791- 011- 9167- 7",0,,False
"[4] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ cher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '08). ACM, New York, NY, USA, 659­666.",1,Novelty,True
"[5] Van Dang and W. Bruce Cro . 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversi cation. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 65­74.",0,,False
[6] Sreenivas Gollapudi and Aneesh Sharma. 2009. An Axiomatic Approach for Result Diversi cation. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). 381­390.,0,,False
"[7] Shengbo Guo and Sco Sanner. 2010. Probabilistic Latent Maximal Marginal Relevance. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '10). ACM, New York, NY, USA, 833­834.",0,,False
"[8] Jiyin He, Vera Hollink, and Arjen de Vries. 2012. Combining Implicit and Explicit Topic Representations for Result Diversi cation. In Proceedings of the 35th",0,,False
"International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '12). ACM, New York, NY, USA, 851­860. [9] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversi cation Based on Hierarchical Intents. In Proceedings of the",0,,False
"24th ACM International on Conference on Information and Knowledge Management (CIKM '15). ACM, New York, NY, USA, 63­72. [10] oc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014. 1188­1196. h p://jmlr.org/ proceedings/papers/v32/le14.html [11] Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha, and Yong Yu. 2009. Enhancing Diversity, Coverage and Balance for Summarization rough Structure Learning. In Proceedings of the 18th International Conference on World Wide Web (WWW '09). ACM, New York, NY, USA, 71­80. [12] Zhongqi Lu and Qiang Yang. 2016. Partially Observable Markov Decision Process for Recommender Systems. CoRR abs/1608.07793 (2016). [13] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings of the 37th International ACM",1,Session,True
"SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 587­596. [14] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu¨tze. 2008. Introduction to Information Retrieval. Cambridge University Press, NY, USA. [15] Lilyana Mihalkova and Raymond Mooney. 2009. Learning to Disambiguate Search eries from Short Sessions. In Machine Learning and Knowledge Discovery in Databases. Lecture Notes in Computer Science, Vol. 5782. Springer. [16] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent Models of Visual A ention. In Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS'14). MIT Press, Cambridge, MA, USA, 2204­2212. [17] Martin L. Puterman. 2008. Markov Decision Processes. John Wiley & Sons, Inc. [18] Filip Radlinski, Robert Kleinberg, and orsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 784­791. [19] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting ery Reformulations for Web Search Result Diversi cation. In Proceedings of the 19th International Conference on World Wide Web (WWW '10). 881­890. [20] Rodrygo L. T. Santos, Jie Peng, Craig Macdonald, and Iadh Ounis. 2010. Explicit Search Result Diversi cation through Sub-queries. Springer Berlin Heidelberg, Berlin, Heidelberg, 87­99. [21] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. J. Mach. Learn. Res. 6 (Dec. 2005), 1265­1295. [22] Richard S. Su on and Andrew G. Barto. 2016. Reinforcement Learning: An Introduction (2nd ed.). MIT Press. [23] Xiaojie Wang, Zhicheng Dou, Tetsuya Sakai, and Ji-Rong Wen. 2016. Evaluating Search Result Diversity Using Intent Hierarchies. In Proceedings of the 39th",1,Session,True
"International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). ACM, New York, NY, USA, 415­424. [24] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '15). 113­122. [25] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '16). 395­404. [26] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM Trans. Intell. Syst. Technol. 8, 3, Article 41 (Jan. 2017), 26 pages. [27] Yisong Yue and orsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In Proceedings of the 25th International Conference on Machine Learning (ICML '08). ACM, New York, NY, USA, 1224­1231. [28] Yisong Yue and orsten Joachims. 2009. Interactively Optimizing Information Retrieval Systems As a Dueling Bandits Problem. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML '09). ACM, New York, NY, USA, 1201­1208. [29] Cheng Xiang Zhai, William W. Cohen, and John La erty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In",1,Novelty,True
"Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval (SIGIR '03). 10­17. [30] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Contentfree Document Re-ranking. In Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 1139­1142. [31] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversi cation. In Proceedings of the 37th International",1,ad,True
"ACM SIGIR Conference on Research & Development in Information Retrieval (SIGIR '14). ACM, New York, NY, USA, 293­302.",0,,False
544,0,,False
,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Learning to Diversify Search Results via Subtopic Attention,0,,False
"Zhengbao Jiang1,2, Ji-Rong Wen1,2,3, Zhicheng Dou1,2, Wayne Xin Zhao1,2, Jian-Yun Nie4, Ming Yue1,2",0,,False
"1School of Information, Renmin University of China 2Beijing Key Laboratory of Big Data Management and Analysis Methods, China 3Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China",0,,False
"4DIRO, Universite´ de Montre´al, Que´bec",0,,False
"rucjzb@163.com, jirong.wen@gmail.com, dou@ruc.edu.cn,",0,,False
"batmanfly@gmail.com, nie@iro.umontreal.ca, yomin@ruc.edu.cn",0,,False
ABSTRACT,0,,False
"Search result diversification aims to retrieve diverse results to satisfy as many different information needs as possible. Supervised methods have been proposed recently to learn ranking functions and they have been shown to produce superior results to unsupervised methods. However, these methods use implicit approaches based on the principle of Maximal Marginal Relevance (MMR). In this paper, we propose a learning framework for explicit result diversification where subtopics are explicitly modeled. Based on the information contained in the sequence of selected documents, we use attention mechanism to capture the subtopics to be focused on while selecting the next document, which naturally fits our task of document selection for diversification. The framework is implemented using recurrent neural networks and max-pooling which combine distributed representations and traditional relevance features. Our experiments show that the proposed method significantly outperforms all the existing methods.",1,ad,True
KEYWORDS,0,,False
search result diversification; subtopics; attention,0,,False
1 INTRODUCTION,1,DUC,True
"In real search scenario, queries issued by users are usually ambiguous or multi-faceted. In addition to being relevant to the query, the retrieved documents are expected to be as diverse as possible in order to cover different information needs. For example, when users issue ""apple"", the underlying intents could be the IT company or the fruit. The retrieved documents should cover both topics to increase the chance to satisfy users with different information needs.",1,ad,True
"Traditional approaches to search result diversification are usually unsupervised and adopt manually defined functions with empirically tuned parameters. Depending on whether the underlying intents (or subtopics) are explicitly modeled, they can be categorized into implicit and explicit approaches [28]. Implicit approaches [6] do not model intents explicitly. They emphasize novelty,",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 DOI: http://dx.doi.org/10.1145/3077136.3080805",1,ad,True
"i.e. the following document should be ""different"" from the former ones based on some similarity measures. Instead, explicit approaches [1, 12, 13, 16, 27, 35] model intents (or subtopics) explicitly. They aim to improve intent coverage, i.e. the following document should cover the intents not satisfied by previous ones. Intents or subtopics can be determined by techniques such as query reformulation [2, 14, 34, 38] and query clustering based on query logs and other types of information. Existing studies showed that explicit approaches have better performance [12, 13, 16, 27, 35] than implicit approaches due to several reasons: on the one hand, they provide a more natural way to handle subtopics than implicit approaches; on the other hand, their ranking functions are closer to the diversity evaluation metrics which are mostly based on explicit subtopics. Furthermore, most similarity measures used in the implicit approaches, e.g., those based on language model or vector space model, are determined globally on the whole documents, regardless of possible search intents. This might be problematic for search result diversification: two documents could contain similar words and considered globally similar, but this similar part may be unrelated to underlying search intents.",1,ad,True
"To avoid heuristic and handcrafted functions and parameters, a new family of research work using supervised learning is proposed. They try to learn a ranking function automatically. Their major focus lies in the modeling of diversity, including structural prediction [36], rewarding functions for novel contents [39], measurebased direct optimization [32], and neural network based method [33]. Regardless of diversity modeling and optimization methods, all these solutions inherit the spirit of MMR which is an implicit approach and do not take intents into consideration. Although the learning methods may result in a better similarity measure, they are hindered by the gap between reducing document redundancy and improving intent coverage. They suffer from similar problems with implicit unsupervised approaches. Without modeling subtopics explicitly, they can't directly improve intent coverage. Hence, there is a need to incorporate explicit subtopic modeling into supervised diversification methods.",1,corpora,True
"To address the above issue, we propose to model subtopics in a general supervised learning framework. Our framework combines the strengths of both explicit unsupervised approaches and (implicit) supervised approaches. First, subtopics are explicitly modeled, allowing us to improve intent coverage in a proactive way. Second, it automatically learns the diversification ranking function, and is able to capture complex interaction among documents and",1,ad,True
545,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Subtopic relevance example.,0,,False
doc\subtopic i1 i2 i3,0,,False
d1 d2 d3,0,,False
× × × ×,0,,False
d4,0,,False
×,0,,False
×,0,,False
"subtopics. We call this framework Document Sequence with Subtopic Attention (DSSA). More specifically, to select the next document, we first model the sequence of selected documents in order to capture their contents as well as their relationship with the subtopics. Then based on the information contained by previous documents, attention mechanism is used to determine the undercovered subtopics to which we have to pay attention in selecting the next document. Attention mechanism has been successfully used to deal with various problems in image understanding [24] and NLP [3, 21]. This mechanism corresponds well to the document selection problem in search result diversification: attention on subtopics changes along with the addition of a document in the result list. For example. Assume that we have 3 subtopics and 4 documents whose relevance judgments are shown in Table 1. Given that we have selected d1 and d2, which cover subtopics i1 and i2, the attention for next choice should incline to i3 which is not covered, thus d3 is a better choice than d4 at this position. We will show that the DSSA framework is general enough to cover the ideas of previous unsupervised explicit methods.",1,ad,True
"We then propose a specific implementation of DSSA using recurrent neural networks (RNN) and max-pooling to leverage both distributed representations and traditional relevance features, which we call DSSA-RNNMP. Experimental results on TREC Web Track data show that our method outperforms the existing methods significantly. To our knowledge, this is the first time that a supervised learning framework with attention mechanism is used to model subtopics explicitly for search result diversification.",1,ad,True
2 RELATED WORK,0,,False
2.1 Implicit Diversification Approaches,0,,False
The basic assumption of implicit diversification approaches is that dissimilar documents are more likely to satisfy different information needs. The most representative approach is MMR [6]:,0,,False
"SMMR(q, d,",0,,False
C),0,,False
",",0,,False
(1,0,,False
-,0,,False
")S rel (d ,",0,,False
q),0,,False
-,0,,False
max,0,,False
dj C,0,,False
"S div (d ,",0,,False
dj,0,,False
"),",0,,False
(1),0,,False
"where Srel and Sdiv model document d's relevance to the query q and its similarity to a selected documents dj respectively. To gain high ranking score, a document should not only be relevant, but also be dissimilar from the selected documents. The definition of measures for relevance and document similarity is crucial, which is done manually in this approach.",0,,False
"Recently, machine learning methods have been leveraged to learn score functions. Yue and Joachims [36] proposed SVM-DIV which uses structural SVM to learn to identify a document subset with maximum word coverage. However, word coverage may be different from intent coverage. Optimizing the former may not necessarily lead to optimizing the latter. Similar to MMR, Zhu et",1,ad,True
Table 2: Categorization of diversification approaches.,0,,False
unsupervised,0,,False
supervised,0,,False
implicit MMR,0,,False
"SVM-DIV, R-LTR, PAMM, NTN",0,,False
explicit,0,,False
"IA-Select, xQuAD, PM2, TxQuAD, TPM2, HxQuAD, HPM2, 0-1 MSKP",1,HP,True
DSSA (our approach),0,,False
"al. [39] proposed relational learning-to-rank model (R-LTR) which learns to score a document based on both relevance and novelty automatically, in order to maximize the probability of optimal rankings. Based on R-LTR score function, Xia et al. [32] proposed a perceptron algorithm using measures as margins (PAMM) to directly optimize evaluation metrics by enlarging the score margin of positive and negative rankings. They further proposed to use a neural tensor network (NTN) [33] to measure document similarity automatically from document representations, which avoids the burden to define handcrafted diversity features.",0,,False
"The above supervised approaches are shown to outperform the unsupervised counterparts. However, they are all implicit approaches without using subtopics. In this paper, we propose a learningbased explicit approach which models subtopics explicitly.",0,,False
2.2 Explicit Diversification Approaches,0,,False
"Explicit approaches model subtopics underlying a query, aiming at returning documents covering as many subtopics as possible. These approaches leverage external resources to explicitly represent information needs in subtopics. IA-Select [1] uses classified topical categories based on ODP taxonomy. xQuAD [27] is a probabilistic framework that uses query reformulations as intent representations. PM2 [13] tackles search result diversification problem from the perspective of proportionality. TxQuAD and TPM2 [12] represent intents by terms and transform intent coverage to term coverage. Hu et al. [16] proposed to use a hierarchical structure for subtopics instead of a flat list, which copes with the inherent interaction among subtopics. Two specific models, namely HxQuAD and HPM2, were proposed using hierarchical structure. Yu et al. [35] formulated diversification task as a 0-1 multiple subtopic knapsacks (0-1 MSKP) problem where documents are chosen like filling up multiple subtopic knapsacks. To tackle this NP-hard problem, max-sum belief propagation is used.",1,ODP,True
"As summarized in Table 2, all existing explicit approaches are unsupervised and the functions and parameters are defined heuristically. In this paper, we use supervised learning to model the interaction among documents and subtopics simultaneously.",0,,False
2.3 RNN with Attention Mechanism,0,,False
"RNN can capture the interdependency between elements in a sequence. Attention mechanism, which is usually built on RNN, mimics human attention behavior focusing on different local region of the object (an image, a sentence, etc) at different times. In computer vision, Google DeepMind [24] used RNN with attention to extract information from an image by adaptively selecting a sequence of the most informative regions instead of the whole image. In NLP, attention mechanism is typically used in neural machine translation (NMT). Traditional encoder-decoder models encode the source",1,ad,True
546,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"sentence into a fixed-length vector from which the target sentence is decoded. Such fixed-length vector may not be powerful enough to reflect all the information of the source sentence. An attentionbased model [3] was proposed to automatically pay unequal and varied attention to source words during decoding process. In particular, to decide the next target word, not only the fixed-length vector, but also the hidden states corresponding to source words relevant to the target word are used. Luong et al. [21] generalized the idea and proposed two classes of attention mechanism, namely global and local approaches. In this paper, attention mechanism is used on subtopics, which guides the model to emphasize different intents at different positions.",0,,False
"In the following section, we will first propose a general framework, then instantiate it with a specific implementation.",0,,False
3 DOCUMENT SEQUENCE WITH SUBTOPIC ATTENTION FRAMEWORK,0,,False
"Given a query set Q, a document set Dq and a subtopic set Iq for",0,,False
"each query q  Q, the goal of explicit methods is to learn a ranking",0,,False
"function f (q, Dq, Iq ) which is expected to output a ranking of do-",0,,False
cuments in Dq that is both relevant and diverse. The loss function,0,,False
could be written in the following general form:,0,,False
"L(f (q, Dq , Iq ), Yq ),",0,,False
(2),0,,False
qQ,0,,False
"where L measures the quality gap between the ranking outputted by f and the best ranking Yq . Different from traditional retrieval tasks, diversity has to be considered in the ranking and evaluation",1,ad,True
"process. Theoretically, diversity ranking is NP-hard [1, 7]. Hence,",1,NP,True
"a common strategy is to make greedy selections [6, 27]: at the t-th position, we assume that t - 1 documents have been selected and formed a document sequence Ct-1. The task is to select a locally optimal document dt from the remaining candidate documents based on a score function S(q, dt , Ct-1, Iq ). Note that implicit supervised methods correspond to the case where Iq is an empty set.",0,,False
"To motivate our approach, we start with the ideas of the unsu-",0,,False
"pervised explicit approaches, which can be formulated as the fol-",0,,False
lowing general form:,0,,False
"Sunsupervised(q, dt , Ct -1, Iq ) ,",0,,False
(1,0,,False
- )S ,0,,False
rel(dt,0,,False
",",0,,False
q)+,0,,False
 relevance,0,,False
"Sdiv(dt , ik ) A(Ct -1, Iq )k ,  diversity",0,,False
(3),0,,False
ik Iq,0,,False
subtopic weights,0,,False
where ik  Iq is the k-th subtopic of q and Srel and Sdiv calculate document dt 's relevance to a query and to a subtopic respectively.,0,,False
The essence of diversity lies in the function A which calculates the,0,,False
"weights for subtopics Iq based Ct -1. For xQuAD, A(Ct -1, Iq )k",0,,False
",onPp(irkev|qi)ousddj oCcut-m1 (e1nt-sPe(qduje|inkc)e)",0,,False
"where P(ik |q) is the initial importance of subtopic ik , P(dj |ik ) is",0,,False
the probability that dj is relevant to ik . The weight of a subtopic,0,,False
is determined by the likelihood that previous documents are not,0,,False
relevant to this subtopic. PM2 mimics seats allocation of compe-,0,,False
"ting political parties to adjust subtopic weights after each selection,",1,ad,True
"i.e. A(Ct-1, Iq ) is estimated according to the difference between the subtopic's distributions in Ct-1 and in Iq . All these methods",0,,False
sequence of selected,0,,False
documents d1 d2 d3,0,,False
candidate documents d4 d5 d6,0,,False
hidden state,0,,False
document sequence representation,0,,False
subtopic attention,0,,False
subtopic attention distribution,0,,False
diversity scoring,0,,False
score,0,,False
relevance scoring,0,,False
subtopics i1 i2,0,,False
query q,0,,False
Figure 1: Illustration of DSSA framework.,0,,False
Table 3: Notations in DSSA.,0,,False
Notation Definition,0,,False
"r , dt a ranking, the t-th document.",0,,False
"q, ik",0,,False
"the query, the k-th subtopic.",0,,False
vdt,0,,False
representation of the document at the t-th position.,0,,False
vq,0,,False
representation of the query.,0,,False
vik,0,,False
representation of the k-th subtopic.,0,,False
ht,0,,False
hidden state of previous t documents.,0,,False
"at,k",0,,False
"attkKe,""n1tiaotn,k""",0,,False
"on ,",0,,False
"the k-th 1, at,k ",0,,False
"subtopic at the t-th position. [0, 1] where K is the number",0,,False
of subtopics. A large value means that this subtopic,0,,False
is less satisfied by previous t - 1 documents and thus,0,,False
needs more attention at the t-th position.,0,,False
sdt,0,,False
the final score of the document at the t-th position.,0,,False
"don't model the selected documents as a sequence. In addition, the functions and parameters are heuristically defined, which may not best fit the final goal.",1,ad,True
"To tackle the above problems, we extend Equation (3) to the following general learning framework:",0,,False
"SDSSA(q, dt , Ct -1, Iq ) , sdt ,",0,,False
(1,0,,False
-,0,,False
)Srel (,0,,False
(vdt,0,,False
",",0,,False
vq,0,,False
)+ (,0,,False
)),0,,False
"Sdiv vdt , vi(·) , A H ([vd1 , ..., vdt-1 ]), vi(·) ,",0,,False
 relevance  diversity,0,,False
subtopic attention,0,,False
"(4) where documents, queries, and subtopics are denoted by their representations, as explained in Table 3. In this paper, we focus on learning a ranking function only and assume that these representations are given and will not be modified. There are three main components, namely (1) document sequence representation component H , (2) subtopic attention component A, and (3) scoring component Srel and Sdiv, which are also illustrated in Figure 1. This framework is inspired from the attention models",0,,False
547,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"used in image understanding [24] and neural machine translation [3, 21], however adapted to our diversification task.",1,ad,True
"Next, we briefly describe the three components. The document sequence representation component H encodes the information contained in document sequence Ct-1 into a fixed-length hidden state ht-1, which could consider the interaction and dependency among these documents. ht-1 could be viewed as a comprehensive and high-level representation of Ct-1. The subtopic attention at,(·) is calculated by the subtopic attention component A using ht-1 and subtopic representations vi(·) . The attention evolves from the first to the last ranking position, driving the model to emphasize different subtopics based on previous document sequence. Finally, the scoring components Srel and Sdiv calculate relevance and diversity scores respectively. Notice that Sdiv is not limited to be a weighted sum over all subtopics as Equation (3). It can incorporate more complex interaction among subtopics.",1,corpora,True
"The essence of this framework can be summarized as follows. Along with the selection of more documents, we encode the information of previous document sequence, and the attention mechanism will monitor the degree of satisfaction for each subtopic. High scores are assigned to the documents relevant to less covered subtopics. Finally, multiple subtopics would be well covered by adaptively learning the attention. In this way, our framework builds an intuitive approach to explicitly model subtopics. We name the framework Document Sequence with Subtopic Attention (DSSA). DSSA is a unified architecture that takes both relevance and diversity into consideration, and diversity is achieved by modeling the interaction among documents and subtopics.",1,ad,True
4 RESULT DIVERSIFICATION USING DSSA,0,,False
"In this section, we instantiate DSSA to a concrete form and articulate the training and prediction algorithms. The main idea of DSSA is to dynamically capture accumulative relevance information of previous document sequence, so as to calculate subtopic attention. Inspired by the recent progress on sequence data modeling, we adapt RNN to capture the information of previous document sequence based on distributed representations of documents. However, the effectiveness of distributed representation heavily depends on a large amount of training data. Typically, the representation is built automatically using the data to optimize an objective function [17]. We do not have such large data and we can only use unsupervised methods (e.g. doc2vec) to create representation, of which the effectiveness could be suboptimal. Indeed, our preliminary experiments using only the distributed representation created by unsupervised methods yield low effectiveness. To compensate this weakness, we also use traditional relevance features such as BM25 score, which are proven useful, to calculate subtopic attention and final score. Such a combination of distributed representations and features has been used in several previous works [29, 33]. In addition to RNN, we also adopt the way using max-pooling [33], which has been shown effective, to implement subtopic attention mechanism. We call this model DSSA-RNNMP (DSSA model using RNN and Max-Pooling), as illustrated in Figure 2. In addition, we also propose a list-pairwise approach for optimization, which is different from the existing studies.",1,ad,True
query/subtopic representation,0,,False
document representation,0,,False
max-pooling signals,0,,False
hidden state,0,,False
RNN,0,,False
h1,0,,False
ht -1,0,,False
ed1,0,,False
edt-1,0,,False
max-pooling,0,,False
"at ,(·)",0,,False
subtopic attention,0,,False
edt,0,,False
s div dt,0,,False
e i1 ei2 eq,0,,False
s rel dt,0,,False
diversity score,0,,False
relevance score,0,,False
"xd1 ,i1 xd1 ,i2",0,,False
"xd1 , q",0,,False
"xdt-1 ,i1 xdt-1 ,i2",0,,False
"xdt-1 ,q",0,,False
"xdt ,i1 xdt ,i2 xdt ,q",0,,False
Figure 2: Architecture of DSSA-RNNMP. Previous t - 1 do-,0,,False
"cuments are encoded into ht-1 from distributed representations ed1 , ..., edt-1 . Attention on the k-th subtopic at,k is then calculated based on (1) hidden state ht-1 and subto-",0,,False
"pic representation eik (2) max-pooling on relevance features xd1,ik , ..., xdt-1,ik .",0,,False
Table 4: Parameters in DSSA-RNNMP.,0,,False
"Notation W n, bn W a, wp W s, wr",0,,False
Definition parameters of RNN with vanilla cell. parameters used in subtopic attention. parameters used in scoring.,0,,False
4.1 A Neural Network Implementation,0,,False
"We first describe the constitution of representations, namely vdt , vq , and vik , then elaborate how we implement document sequence representation, subtopic attention, and scoring com-",0,,False
ponents. The parameters to be learned are listed in Table 4. vdt : the representation of a document is composed of two parts:,0,,False
distributed representations and relevance features. Distributed re-,0,,False
"presentation can be constructed in different ways. In this paper,",0,,False
"we consider three methods: SVD, LDA [4], and doc2vec [18]. Rele-",0,,False
"vance features are those used in traditional IR, such as BM25 score",1,ad,True
"etc. Suppose that we have a distributed representation of size Ed , K subtopics, and R relevance features, the total size of vdt would be Ed + R + KR. We use edt  REd , xdt,q and xdt,ik  RR to denote distributed representation, relevance features for a query and",0,,False
"a subtopic respectively. vq , vik : we first retrieve top Z documents using some basic re-",0,,False
trieval model (such as BM25). These documents are concatenated,0,,False
"as a pseudo document, then similar to edt , a distributed representation of size Eq is generated. For consistency, we also use eq and eik  REq to represent these representations.",0,,False
548,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"4.1.1 Document Sequence Representation. H is instantiated using RNN to encode the information of previous document sequence. Several types of RNN cell can be used, ranging from the simple vanilla cell, GRU cell [9], to LSTM cell [15]. For simplicity, we only show the vanilla cell here. At the t-th position, we derive the (accumulative) document sequence representation as follows:",0,,False
"ht ,"" tanh(W n [ht -1; edt ] + bn ),""",0,,False
(5),0,,False
"where W n  RU ×(U +Ed ) (U is the size of the hidden state), bn  RU and [; ] is a concatenation. The cell transforms previous hidden",0,,False
"layer ht -1 and another space,",0,,False
current where a,0,,False
document distributed bias bn is added and a,1,ad,True
representation edt to non-linear activation,0,,False
"(i.e. tanh) then happens, producing the next hidden layer ht . h0",0,,False
is initialized as a vector of zeros. The vanilla cell can be easily,0,,False
"replaced by GRU and LSTM cells, whose results will be report in",0,,False
Section 6.2.,0,,False
"4.1.2 Subtopic Attention. By looking at ht-1 which stores the information of previous t - 1 documents and ei(·) which represents the meaning of each subtopic, we are capable of discovering which",0,,False
"intents are not satisfied and thus need to be emphasized at the tth position. To capture this idea, we use A(ht -1, eik ) to measure the (unnormalized) importance of the k-th subtopic at the t-th po-",0,,False
"sition, which could be implemented in many ways. We consider",0,,False
the following two ways similar to [21]:,0,,False
{,0,,False
"A (ht -1, eik ) ,",0,,False
"ht -1W aeik , -ht -1 · eik ,",0,,False
(general) (dot),0,,False
(6),0,,False
"where W a  RU ×Eq . The ""general"" operation uses bilinear tensor product to relate two vectors multiplicatively through its nonlinearity [30]. The ""dot"" product requires both vectors to be in the same space. Similar ht-1 and eik mean that previous documents are likely to satisfy this subtopic, and thus a lower attention score will be attributed to it. The above way mainly relies on distributed representations, which may not always be effective, especially under limited data.",0,,False
"Hence, we further leverage relevance features to enhance the subtopic attention. xdt,ik directly reflects the degree of satisfaction for a subtopic-document pair and is combined linearly using wp to form an explicit signal. To derive the accumulative information of the document sequence, we adopt commonly used max-pooling to select the most significant signal from previous documents:",1,ad,True
"A (xd1,ik , ..., xdt-1,ik ) ,"" max([xd1,ik · wp , ..., xdt-1,ik · wp ]), (7)""",0,,False
"where A(xd1,ik , ..., xdt-1,ik ) measures the degree of satisfaction of the k-th subtopic based on relevance features through max-pooling. Lower value indicates that the previous documents are more likely to be relevant to this subtopic. Note that if we view the signals produced by max-pooling (i.e. the vectors in ""max-pooling"" section of Figure 2) as a part of the general hidden states, our concrete implementation fit in DSSA framework.",0,,False
We adopt an addictive way to integrate both parts and then use softmax to produce (normalized) attention distribution:,1,ad,True
"at,k",0,,False
",",0,,False
"A (ht -1, eik ) +",0,,False
"A (xd1,ik , ..., xdt-1,ik ),",0,,False
"at,k , Kjw,""i1kweixj pe(xapt(,kat), j )""",0,,False
"(wij  0, j).",0,,False
(8),0,,False
"softmax is modified to include the initial subtopic importance wik , which encodes our intuition that an important subtopic is more",0,,False
likely to gain attention than unimportant ones.,0,,False
"4.1.3 Scoring. The final score consists of relevance score sdretl and diversity score sddtiv, which are combined by a coefficient :",0,,False
"sdt , (1 - )sdretl + sddtiv (0    1).",0,,False
(9),0,,False
The relevance score and diversity score are calculated as follows:,0,,False
"sdretl ,"" S (edt , eq ) + xdt ,q · wr ,""",0,,False
sddtiv,0,,False
",",0,,False
"at , ( ·)",0,,False
·,0,,False
"SS((eeddtt,,eeiiK1",0,,False
) ),0,,False
+ ... +,0,,False
"xdt,i1 · wr xdt,iK · wr",0,,False
",",0,,False
(10),0,,False
"where wr  RR and at,(·) is the attention derived from subtopic attention component. The diversity score is calculated as a weighted",0,,False
combination of the document's relevance to each subtopic by atten-,0,,False
tion distribution. We use the same way to calculate document's re-,0,,False
levance to a query and to its subtopics using both distributional re-,0,,False
"presentations and relevance features, although different ways can",0,,False
"be used. Specifically, dt 's relevance to a query q (or a subtopic ik ) is calculated based on both the similarity between two distributed xrtwedpto,rqerse(eponrretxastdeitno,tniakst)iS.oSn (seidantnt,deenwqd)rs(otliornSpear o(reldydutcc,oeemiakbm)i)naaetnscdhfeirnaeglteuvsrcaeonsr.ceSeibfmeeatiwltauerreetnos A, S could also be implemented as:",0,,False
{,0,,False
S (edt,0,,False
",",0,,False
eik,0,,False
),0,,False
",",0,,False
"edt W seik , edt · eik ,",0,,False
(general) (dot),0,,False
(11),0,,False
where W s  REd ×Eq . Then the score of a ranking r is calculated by summing up all the |r | documents' scores:,0,,False
 |r |,0,,False
"sr , sdt .",0,,False
(12),0,,False
"t ,1",0,,False
"Vector interaction operations A and S could be implemented using more complex models, such as multilayer perceptron (MLP), to model the interaction between two vectors more accurately. We could also use convolutional neural network (CNN) instead of RNN to model the interaction among a sequence of documents and encode their information. We deliberately choose to use simple mechanisms in this implementation in order to show that the general framework is capable of capturing the essence of diversification even without complex operations. More complex implementations will be examined in future work.",1,ad,True
549,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Algorithm 1 A List-pairwise Approach For Optimization,0,,False
d 3,0,,False
d 1,0,,False
d 3,0,,False
d 2,0,,False
d 4,0,,False
rank1,0,,False
1: procedure List-pairwise Training,0,,False
"input: loss function L, learning rate r , epochs V , query set Q,",0,,False
dd,0,,False
1,0,,False
2,0,,False
d 4,0,,False
d 1,0,,False
d 2,0,,False
d 4,0,,False
d rank2 3,0,,False
"document set D, evaluation metric M, random permutation count N output: DSSA with trained parameters  2: initialize ",0,,False
(a) list-pairwise,0,,False
(b) PAMM,0,,False
3: for i from 1 to V do,0,,False
4:,0,,False
"for batch b  GetSamples(Q, D, M, N ) do",0,,False
Figure 3: Pair sample examples of (a) list-pairwise and (b),0,,False
5:,0,,False
"  GetGradient(L(b,  ))",1,ad,True
PAMM. Both samples are positive.,0,,False
6:,0,,False
   - r,0,,False
return DSSA,0,,False
4.2 A List-pairwise Approach for Optimization,0,,False
"Liu [19] classifies LTR approaches into three categories: pointwise, pairwise, and listwise. Search result diversification is naturally a listwise problem because the score of a document depends on the previous documents. Take Table 1 as an example, under no previous documents, d2 is better than d3 because d2 covers one more subtopic (subtopics are of equal weight). However, given that we have selected d1, which is similar to d2 while dissimilar to d3, d3 becomes superior because it provides additional information.",1,ad,True
4.2.1 List-pairwise Training. We propose a list-pairwise training approach. We call it list-pairwise because a sample in our algo-,0,,False
7: procedure GetSamples,0,,False
"input: query set Q, document set Dq for each query q, evaluation metric M, random permutation count N",0,,False
"output: a set of ranking pairs with weight and preference {(q(1), C(1), d1(1), d2(1), w(1), y(1)), (q(2), C(2), d1(2), d2(2), w(2), y(2)), ...} include: GetPerms(Dq, l, N , M) return a best ranking (under metric M) and N random permutations of length l. GetPairs(q, Dq, C, M) samples pairs of documents (d1, d2) from Dq \ C under context C if and only if they lead to different metric scores. Let r1  [C, d1], r2  [C, d2], w , |M(r1) - M(r2)| and y , M(r1) > M(r2) . 8: R  ",1,ad,True
"rithm consists of a pair of rankings (r1, r2): r1 and r2 are totally identical except the last document. The sample can be written as (C, d1, d2), where C is the shared previous document sequence.",0,,False
"The pairwise preference ground-truth is generated based on an evaluation metric M, such as -nDCG. If M(r1) > M(r2), it is positive,",0,,False
9: for query q in Q do,0,,False
10:,0,,False
for l from 0 to |Dq | - 1 do,0,,False
11:,0,,False
"for perm C in GetPerms(Dq, l, N , M) do",0,,False
12:,0,,False
"R  R  GetPairs(q, Dq, C, M)",0,,False
return R,0,,False
"otherwise it is negative. Our approach is similar to pairwise approaches because it aims to compare a pair of documents, but this is done within some context. Similarly to pairwise, the loss function",0,,False
"LPAMM ,",0,,False
"P (rq+) - P (rq-)  M(rq+) - M(rq-) ,",0,,False
"q  Q rq+,rq-",0,,False
(16),0,,False
can be defined as binary classification logarithmic loss:,0,,False
"Llist-pairwise , ",0,,False
w (o ),0,,False
q  Q o  Oq,0,,False
( y(o),0,,False
log,0,,False
"( P (r1(o),",0,,False
) r2(o)),0,,False
+,0,,False
(1,0,,False
-,0,,False
y(o)),0,,False
log,0,,False
( 1,0,,False
-,0,,False
")) P (r1(o), r2(o)) ,",0,,False
(13),0,,False
"where condition is 1 if the condition is satisfied, 0 otherwise, MLE maximizes the probability of positive rankings, and PAMM enlarges the probability margin between positive and negative rankings according to an evaluation metric. For MLE, the number of best rankings is usually small if we only have hundreds of que-",0,,False
"where Oq is all the pair samples of query q, y(o) ,"" 1 indicates positive and 0 for negative, and P(r1(o), r2(o)) is the probability of being""",0,,False
"ries, which may not be enough to train adequately the parameters. PAMM uses preferences between very different rankings that are not comparable (see Figure 3(b)). In contrast, list-pairwise method",1,ad,True
positive,0,,False
calculated,0,,False
by,0,,False
1 1+exp(sr2(o),0,,False
-sr1(o,0,,False
),0,,False
),0,,False
.,0,,False
To,0,,False
enhance,0,,False
"effectiveness,",0,,False
we,0,,False
weight,0,,False
pairs,0,,False
with,0,,False
w (o ),0,,False
",",0,,False
|M,0,,False
(r,0,,False
(o) 1,0,,False
),0,,False
-,0,,False
M,0,,False
"(r2(o))|,",0,,False
which,0,,False
means,0,,False
that,0,,False
only allows the last document to be different (Figure 3(a)). This corresponds better to the decision-making situation in which we have to choose a document under a given context. It is expected that,0,,False
"the bigger the metric score gap, the more important the pair.",0,,False
such a pair sample allows us to better train the ranking function.,0,,False
Because DSSA calculates document d's score sdC based on previ-,0,,False
Experiments will show that our approach works better.,0,,False
"ous document C, we could also use Maximum Likelihood Estima-",0,,False
"As shown in Figure 2, our architecture is a unified neural net-",0,,False
tion (MLE) or PAMM to optimize our model. We use Plackett-Luce,0,,False
"work and the attention function is continuous, so the gradient of",1,ad,True
model [22] to estimate the probability of a ranking r :,0,,False
the loss function can be backpropagated directly to train the model.,0,,False
P(r ),0,,False
",",0,,False
 |r |,0,,False
"i ,1",0,,False
exp(sdr i[:i-1]),0,,False
|r |,0,,False
"j ,i",0,,False
exp(sdr [j:i-1],0,,False
),0,,False
",",0,,False
(14),0,,False
where r [: i - 1] means the top i - 1 documents of ranking r . Then,0,,False
the loss functions could be w ritten as:,0,,False
"LMLE ,"" - log(P (rq+)),""",1,LM,True
(15),0,,False
qQ,0,,False
"We use mini-batch gradient descent to facilitate training process. Unfortunately, it is impossible to acquire all the list-pairwise",1,ad,True
"samples, which has in total |Dq |! (|Dq | is the number of candidate documents) different permutations. So we develop a sampling strategy similar to negative sampling [23] as described in Algorithm 1: for each query q, we sample a large number of pairs of rankings, whose length ranges from 1 to |Dq |. We first obtain some contexts C from both best rankings and randomly sampled negative",0,,False
550,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"rankings (rankings that are not optimal). Then under each C, a pair of documents (d1, d2) are sampled from the remaining documents Dq \ C if and only if they lead to different metric scores.",1,ad,True
"4.2.2 Prediction. In prediction stage, for each query, we sequentially and greedily choose the document with the highest score and append it to the ranking list. Specifically, the first document is selected under initial subtopic importance from the whole candidate set Dq . Once the top t - 1 documents have been selected (i.e. |C| ,"" t - 1), we feed each document in Dq \ C into DSSA at the t-th position one by one and choose the one with the highest sdt . This process continues until all the documents in Dq are ranked.""",0,,False
"4.2.3 Time Complexities. The training time complexity with vanilla cell and ""general"" operation is O(V · |Q| ·  · |Dq | · ) where V is the number of iterations, |Q| is the number of training queries,  ,"" N · |Dq |2 is the number of sampled pairs where N is the number of random permutations, |Dq | is the number of candidate documents, and  is the complexity for each position:""",0,,False
" ,"" U (U + Ed ) + KU Eq + KR + KEd Eq + KR, (17)""",0,,False
document sequence representation,0,,False
subtopic attention,0,,False
scoring,0,,False
where the dominating terms are KU Eq and KEd Eq which are proportional to the number of subtopics K. How to efficiently handle,0,,False
"a large number of subtopics is our future work. The prediction complexity is O(|Dq |2) for each query. We can limit |Dq | to a small number (say 50), so the prediction time can be reasonable.",0,,False
5 EXPERIMENTAL SETTINGS,0,,False
5.1 Data Collections,0,,False
"We use the same dataset as [16] which consists of Web Track dataset from TREC 2009 to 2012. There are 198 queries (query #95 and #100 are dropped because no diversity judgments are made for them), each of which includes 3 to 8 subtopics identified by TREC assessors. The relevance rating is given in a binary form at subtopic level. All experiments are conducted on ClueWeb09 [5] collection.",1,Track,True
"We use query suggestions of Google search engine as subtopics, which are released by Hu et al. [16] on their website1. For DSSA, we only use the first level subtopics and leave the exploration of hierarchical subtopics to future work. Following the existing work [16], we simply use uniform weights for these subtopics.",0,,False
5.2 Evaluation Metrics,0,,False
"We use ERR-IA [8], -nDCG [10], and NRBP [11], which are official diversity evaluation metrics used in Web Track. They measure the diversity by explicitly rewarding novelty and penalizing redundancy. D-measures [26], the primary metric used in NTCIR Intent [25] and IMine task [20], is also included. In addition, we also use traditional diversity measures Precision-IA (denoted as Pre-IA) [1] and Subtopic Recall (denoted as S-rec) [37]. Consistent with existing works [32, 33, 39] and TREC Web Track, all these metrics are computed on top 20 results of a ranking. We use two-tailed paired t-test to conduct significance testing with p-value < 0.05.",1,Track,True
1hierarchical search result diversification: http://www.playbigdata.com/dou/hdiv,0,,False
"Table 5: Relevance features. Each of the first 3 features is applied to body, anchor, title, URL, and the whole documents.",0,,False
Name,0,,False
TF-IDF BM25 LMIR,1,LM,True
PageRank #inlinks #outlinks,0,,False
Description,0,,False
the TF-IDF model BM25 with default parameters LMIR with Dirichlet smoothing,1,LM,True
PageRank score number of inlinks number of outlinks,0,,False
#Features,0,,False
5 5 5,0,,False
1 1 1,0,,False
Table 6: Diversity features. Each feature is extracted over a pair of documents.,0,,False
Name,0,,False
subtopic diversity text diversity title diversity anchor text diversity link-based diversity URL-based diversity,0,,False
Description,0,,False
euclidean distance based on SVD cosine-based distance on term vector text diversity on title text diversity on anchor link similarity of document pair URL similarity of document pair,0,,False
5.3 Baseline Models,0,,False
"We compare DSSA2 to various unsupervised and supervised diversification methods. The non-diversified baseline is denoted as Lemur. We use xQuAD [27], PM2 [13], TxQuAD, TPM2 [12], HxQuAD, and HPM2 [16] as our unsupervised baselines. We use ListMLE [31], R-LTR [39], PAMM [32], and NTN [33] as our supervised baselines. Top 20 results of Lemur are used to train supervised methods. Top 50 (i.e. |Dq |) results of Lemur are used for diversity re-ranking. To construct the representation of a query or a subtopic, we use the top 20 (Z ) documents. We use 5-fold cross validation to tune the parameters in all experiments based on nDCG@20, which is one of the most widely used metrics. A brief introduction to these baselines is as follows:",1,HP,True
Lemur. We use the same non-diversified results as [16] for fair comparison. They are produced by language model and retrieved using the Lemur service3 of which the spams are filtered. These results are released by Hu et al. [16] on the website1.,0,,False
ListMLE. ListMLE is a representative listwise LTR method without considering diversity.,0,,False
"xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2. These are competitive unsupervised explicit diversification methods, as introduced in Section 2.2. All these methods use  to control the importance of relevance and diversity. HxQuAD and HPM2 use an additional parameter  to control the weight of each layer of the hierarchical structure. Both  and  are tuned using cross validation. They all require a prior relevance function to fulfill diversification re-ranking. Following [39], we use ListMLE.",1,HP,True
"R-LTR, PAMM, and NTN. For PAMM, we use -nDCG@20 as the optimization metric. We optimize NTN based on both R-LTR and PAMM, denoted as R-LTR-NTN and PAMM-NTN respectively.",0,,False
2data and code available at: http://www.playbigdata.com/dou/DSSA/ 3Lemur service: http://boston.lti.cs.cmu.edu/Services/clueweb09 batch/,0,,False
551,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"To achieve optimal results, for R-LTR and PAMM, we tune the relational function hS (R) from minimal, maximal, and average. For PAMM, we tune the number of positive rankings  + and negative rankings  - per query. For NTN, the number of tensor slices is tuned from 1 to 10. LDA is used to generate distributed representations of size 100 for NTN and DSSA. For all these supervised methods, the learning rate r is tuned from 10-7 to 10-1. For DSSA, we have different settings possible. In our first set of results, we will use ""general"" as the implementation of vector interaction operations A and S, LSTM with hidden size of 50 as the cell of RNN. We set random permutation count as 10 in list-pairwise sampling. Similarly,  of DSSA is tuned by cross validation. We also test the impact of different model settings and permutation counts on performance in Section 6.2 and Section 6.3 respectively.",0,,False
"Similar to [39], we implement 18 relevance features and 6 diversity features, as listed in Table 5 and 6 respectively. We collect the candidate and retrieved documents of all queries and subtopics to generate the distributed representations.",0,,False
6 EXPERIMENTAL RESULTS,0,,False
6.1 Overall Results,0,,False
"The overall results are shown in Table 7. We find that DSSA significantly outperforms all implicit and explicit baselines, including both unsupervised and supervised. The improvements are statistically significant (two-tailed paired t-test) for all metrics, except S-rec. The results clearly show the superiority of DSSA.",0,,False
"(1) DSSA vs. unsupervised explicit methods. DSSA outperforms unsupervised explicit methods (xQuAD, PM2, TxQuAD, TPM2, HxQuAD, and HPM2) on all the measures. The relative improvement over HxQuAD and HPM2, the best unsupervised explicit approaches, is up to 8.3% and 8.6% respectively in terms of -nDCG. This comparison shows the great advantage of using supervised method for learning the ranking function.",1,HP,True
"(2) DSSA vs. supervised implicit methods. DSSA also outperforms supervised implicit methods (R-LTR, PAMM, R-LTRNTN, and PAMM-NTN) by quite large margins. The improvement over R-LTR-NTN and PAMM-NTN, the best supervised implicit approaches is up to 9.9% and 9.4% respectively on -nDCG. This result demonstrates the utility of taking into account subtopics explicitly in supervised approaches. The improvements are similar to those observed between explicit approaches and implicit approaches in unsupervised framework [12, 13, 16, 27]. The combination of the two observations suggests that explicit modeling of subtopics can improve result diversification, whether it is in a supervised or unsupervised framework.",0,,False
6.2 Effects of Different Settings,0,,False
"We conduct experiments with different settings of DSSA to investigate whether the performance is sensitive to these settings. Different aspects of settings are listed follow. For simplicity, when investigating the impact of each aspect, we keep other aspects the same as the settings specified in Section 5.3.",0,,False
"(1) Representation generation methods: SVD, LDA, and doc2vec with window size of 5.",0,,False
"(2) Implementation of vector interaction operations A and S: ""general"" and ""dot"".",0,,False
Table 7: Performance comparison of all methods. The best result is in bold. Statistically significant differences between DSSA and baselines are marked with various symbols. # indicates significant improvement over all baselines.,0,,False
Methods,0,,False
ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,0,,False
Lemur ListMLE,0,,False
.271 .369 .287 .387,0,,False
.232 .424 .249 .430,0,,False
.153 .621 .157 .619,0,,False
xQuAD .317 .413,0,,False
TxQuAD .308 .410,0,,False
HxQuAD .326 .421,0,,False
PM2,0,,False
.306 .411,0,,False
TPM2,0,,False
.291 .399,0,,False
HPM2,1,HP,True
.317 .420,0,,False
.284 .437 .272 .441 .294 .441 .267 .450 .250 .443 .279 .455,0,,False
.161 .622 .155 .634 .158 .629 .169 .643 .161 .639 .172 .645,0,,False
R-LTR,0,,False
.303,0,,False
PAMM,0,,False
.309,0,,False
R-LTR-NTN .312,0,,False
PAMM-NTN .311,0,,False
DSSA,0,,False
.356#,0,,False
.403 .411 .415 .417 .456#,0,,False
.267 .441 .271 .450 .275 .451 .272 .457 .326# .473#,0,,False
.164 .631 .168 .643 .166 .644 .170 .648 .185# .649,0,,False
Table 8: Effects of different settings.,0,,False
Methods,0,,False
ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,0,,False
SVD LDA doc2vec,0,,False
.348 .450 .315 .356 .456 .326 .351 .452 .318,0,,False
.470 .184 .646 .473 .185 .649 .471 .184 .646,0,,False
general dot,0,,False
.356 .456 .326 .347 .450 .314,0,,False
.473 .185 .649 .470 .184 .647,0,,False
vanilla GRU LSTM,0,,False
.354 .454 .322 .357 .457 .326 .356 .456 .326,0,,False
.471 .184 .649 .473 .185 .649 .473 .185 .649,0,,False
DSSA-RNN,0,,False
.342,0,,False
DSSA-RNNMP .356,0,,False
.445 .306 .456 .326,0,,False
.466 .172 .657 .473 .185 .649,0,,False
"(3) RNN cell: vanilla, GRU, and LSTM cell. (4) Dimensionality: we test several representative settings on",0,,False
"the size of distributed representations Ed and Eq , the size of hidden state U as (25, 10), (50, 25), (100, 50), (200, 100). (5) Max-pooling: we experiment without using max-pooling (denoted as DSSA-RNN) in subtopic attention component.",0,,False
"The results are reported in Table 8. We can observe that DSSA does not heavily rely on specific settings. As for different representation generation methods, LDA has slightly better results. doc2vec could have been more appropriate if we had large datasets with more queries. The ""general"" operation yields slightly better results. A possible reason is that it is bilinear and thus is more powerful than ""dot"" to model the interaction. GRU and LSTM cells yield slightly better results than vanilla cell because of their ability of modeling long-term dependency. The difference is however small. This may be due to that with a limited number of training data, a model is unable to take advantage of its higher complexity to capture the fine-grained subtlety. Results with different size of distributed representation and hidden state shown in Figure 4(a) also",1,ad,True
552,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
-nDCG -nDCG,0,,False
0.47,0,,False
0.47,0,,False
0.46,0,,False
0.46,0,,False
0.45,0,,False
0.45,0,,False
0.44,0,,False
0.44,0,,False
"0.43 (25,10) (50,25) (100,50) (200,100)",0,,False
dimensionality,0,,False
0.43 0,0,,False
5,0,,False
10 15 20,0,,False
#random permutation,0,,False
(a)  -nDCG w.r.t. different size (b)  -nDCG w.r.t. different random permutation count,0,,False
Figure 4: Performance tendency of different settings.,0,,False
DSSA i1 i2 i3 i4 i5 d1 d2 d3 d4 d5,0,,False
PAMM-NTN i1 i2 i3 i4 i5,0,,False
DSSA,0,,False
PAMM-NTN,0,,False
i1 i2 i3 i4 i1 i2 i3 i4,0,,False
(a) ranking of query #58,0,,False
(b) ranking of query #182,0,,False
Figure 5: Case study for DSSA and PAMM-NTN. White means relevant and black means irrelevant.,0,,False
Table 9: Effects of different optimization methods.,0,,False
Methods ERR-IA -nDCG NRBP D-nDCG Pre-IA S-rec,0,,False
MLE,0,,False
.349,0,,False
PAMM,0,,False
.348,0,,False
list-pairwise .356,0,,False
.446 .315 .445 .315 .456 .326,0,,False
.462 .176 .644 .463 .175 .644 .473 .185 .649,0,,False
"indicate no strong correlation between performance and settings. -nDCG remains above 0.45 using different sizes. The best performance is achieved using 100-dimensional representation and 50dimensional hidden state. This suggests that high dimensionality may result in overfitting. Without using max-pooling, -nDCG drops to 0.445, which demonstrates the usefulness of using maxpooling to enhance subtopic attention. The small differences between different settings suggest that DSSA is a stable and robust framework. Note that we use both distributed representations and relevance features, which are complementary to each other. This may be one of the reasons of the stability.",0,,False
6.3 Effects of Different Optimization Methods,0,,False
"Results in Table 9 shows that list-pairwise is more effective than MLE and PAMM. This confirms our earlier intuition that list-pairwise optimization corresponds better to the situation of diversification ranking than the two other methods. Note that even using MLE or PAMM as optimization methods, DSSA could also achieve stateof-the-art performances, which confirms the effectiveness of our explicit learning framework from another perspective.",0,,False
"We vary the number of random permutations used in list-pairwise sampling from 0 to 20 to investigate its effect. As depicted in Figure 4(b), the performance does not heavily rely it. The best performance is achieved around 10. More permutations lead to lower effectiveness, which could be explained by model overfitting.",1,ad,True
6.4 Visualization and Discussion,0,,False
We visualize the ranking results of DSSA and the variation of subtopic attention to better understand why DSSA performs well.,0,,False
"We show the top 5 ranking results of query #58 and #182 in Figure 5 to illustrate why DSSA outperforms implicit learning methods. We choose PAMM-NTN as comparison method, which is the best existing learning method. In Figure 5, white means relevant and black means irrelevant. For query #58, DSSA ranks a document relevant to subtopics i3 and i4 first and a document relevant to i1 and i2 at the second position, while the first two documents of PAMM-NTN cover the same subtopics. Note that there is no",0,,False
d1 d2 d3 d4 d5,0,,False
z1. quit smoking tips (i1),0,,False
z2. quit smoking app (i1),0,,False
z3. quit smoking calculator (i1) subtopics from Google z4. quit smoking help (i1),0,,False
z5. quit smoking benefits (i2),0,,False
z6. quit smoking cold turkey (i3),0,,False
official subtopics,0,,False
z7. quit smoking hypnosis (i4) i1. What are the ways you can quit smoking? i2. What are the benefits of quitting smoking? i3. Can you quit smoking using the cold turkey method? i4. How can hypnosis help someone quit smoking?,0,,False
Figure 6: Subtopic attention variation of query #182. The top part is attention and the bottom part is relevance judgment.,0,,False
"document covering i5 in the candidate set. For query #182, DSSA successively chooses documents that cover i1, i3, i2, and i4. One additional intent is satisfied at every position. PAMM-NTN, however, just covers i1 and i2 by top 5 documents, which is obviously not optimal. We see that the unequal and varied subtopic attention is capable of discovering unsatisfied subtopics at different positions, which eventually leads to more subtopic coverage.",1,ad,True
"To study attention mechanism, we further visualize the variation of subtopic attention of top 5 documents of query #182, namely ""quit smoking"", which has 4 official subtopics (i1 to i4), as shown in Figure 6. The top part is subtopic attention variation and the bottom part is relevance judgment. For attention part, the darker the cell is, the lower the attention (weight) on this subtopic is. Note that we actually leverage query suggestions of Google (z1 to z7) to serve as subtopics, which do not match official ones exactly. We manually align subtopics mined from Google to official ones. At the beginning, all the subtopics have equal attention. The first selected document d1 is relevant to i1, i.e. to the Goggle subtopics z1, z2, z3 and z4. We see that the attention to these latter decreases at second position. Then the document d2 is selected, which is relevant to uncovered i3. We see that the attention to the corresponding z6 begins to diminish from the third position. d3 and d4 satisfy additional i2 and i4 respectively, which leads to the reduction",1,ad,True
553,0,,False
Session 5A: Retrieval Models and Ranking 3,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"of attention on z5 and z7 at the following position. The subtopic attention, initialized as uniform distribution, ends up with more emphasis on z4, z6, and z7. This example illustrates how the unequal and varied attention drives the model to emphasize different subtopics at different positions, which is crucial in explicit diversification. This example also shows a potential problem inherent for any method using automatically discovered subtopics: those topics may be different from the ones defined by human assessors. Equal distribution is assumed on all the subtopics zi . However, this implies an unequal distribution among the manually defined subtopics (more emphasis is put on i1). Assuming an equal distribution at the beginning may not necessarily be the best approach. We will deal with this problem in our future work.",0,,False
7 CONCLUSIONS,0,,False
"In this paper, we propose a general learning framework DSSA to model subtopics explicitly for search result diversification. Based on the sequence of selected documents, unequal and varied subtopic attention is calculated, driving the model to emphasize different subtopics at different positions. This is the first time that attention mechanism is used to model the process. We further instantiate DSSA using RNN and max-pooling to handle both distributed representations and relevance features, which outperforms significantly the existing approaches. The results confirm that modeling subtopics explicitly in a learning framework is beneficial and effective and this also avoids heuristically defined functions and parameters. However, accurately modeling the interaction among documents and subtopics is still challenging. There are many other more complex implementations besides our particular way, which will be investigated in future work. The proposed model contains a number of parameters to be learned. This requires a large number of training data. Collecting more training data to fully unlock the potential of the model is another direction. Finally, this work only deals with the learning of a ranking function, assuming that document and query representations have already been created. In practice, learning these representation is another interesting aspect, which could be incorporated into our framework, provided with sufficient training data.",1,ad,True
ACKNOWLEDGMENTS,0,,False
"Zhicheng Dou is the corresponding author. This work was funded by the National Natural Science Foundation of China under Grant No. 61502501 and 61502502, the National Key Basic Research Program (973 Program) of China under Grant No. 2014CB340403, and the Beijing Natural Science Foundation under Grant No. 4162032.",0,,False
REFERENCES,0,,False
"[1] Rakesh Agrawal, Sreenivas Gollapudi, Alan Halverson, and Samuel Ieong. 2009. Diversifying Search Results. In WSDM.",0,,False
"[2] Ricardo A. Baeza-Yates, Carlos A. Hurtado, and Marcelo Mendoza. 2004. Query Recommendation Using Query Logs in Search Engines. In Current Trends in Database Technology EDBT 2004 Workshops.",1,ad,True
"[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural Machine Translation by Jointly Learning to Align and Translate. CoRR abs/1409.0473 (2014).",0,,False
"[4] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research 3 (2003).",0,,False
"[5] Jamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao. 2009. Clueweb09 data set. http://boston.lti.cs.cmu.edu/Data/clueweb09/. (2009).",1,Clue,True
"[6] Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-based Reranking for Reordering Documents and Producing Summaries. In SIGIR.",1,ad,True
[7] Ben Carterette. 2009. An Analysis of NP-Completeness in Novelty and Diversity Ranking. In ICTIR.,1,NP,True
"[8] Olivier Chapelle, Donald Metlzer, Ya Zhang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In CIKM.",1,ad,True
"[9] Junyoung Chung, C¸aglar Gu¨l¸cehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. CoRR abs/1412.3555 (2014).",0,,False
"[10] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ttcher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In SIGIR.",1,Novelty,True
"[11] Charles L. A. Clarke, Maheedhar Kolla, and Olga Vechtomova. 2009. An Effectiveness Measure for Ambiguous and Underspecified Queries. In ICTIR.",0,,False
[12] Van Dang and Bruce W. Croft. 2013. Term Level Search Result Diversification. In SIGIR.,0,,False
[13] Van Dang and W. Bruce Croft. 2012. Diversity by Proportionality: An Electionbased Approach to Search Result Diversification. In SIGIR.,0,,False
"[14] Amac Herdagdelen, Massimiliano Ciaramita, Daniel Mahler, Maria Holmqvist, Keith Hall, Stefan Riezler, and Enrique Alfonseca. 2010. Generalized Syntactic and Semantic Models of Query Reformulation. In SIGIR.",1,Query,True
"[15] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997).",0,,False
"[16] Sha Hu, Zhicheng Dou, Xiaojie Wang, Tetsuya Sakai, and Ji-Rong Wen. 2015. Search Result Diversification Based on Hierarchical Intents. In CIKM.",0,,False
"[17] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry P. Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM.",0,,False
[18] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML.,0,,False
"[19] Tie-Yan Liu. 2009. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval 3, 3 (2009).",0,,False
"[20] Yiqun Liu, Ruihua Song, Min Zhang, Zhicheng Dou, Takehiro Yamamoto, Makoto P Kato, Hiroaki Ohshima, and Ke Zhou. 2014. Overview of the NTCIR-11 IMine Task.. In NTCIR. Citeseer.",0,,False
"[21] Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention-based Neural Machine Translation. In EMNLP.",0,,False
"[22] John I Marden. 1996. Analyzing and modeling rank data. CRC Press. [23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.",1,ad,True
"2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS. [24] Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent Models of Visual Attention. In NIPS. [25] Tetsuya Sakai, Zhicheng Dou, Takehiro Yamamoto, Yiqun Liu, Min Zhang, Ruihua Song, MP Kato, and M Iwata. 2013. Overview of the NTCIR-10 INTENT-2 Task.. In NTCIR. [26] Tetsuya Sakai and Ruihua Song. 2011. Evaluating Diversified Search Results Using Per-intent Graded Relevance. In SIGIR. [27] Rodrygo L.T. Santos, Craig Macdonald, and Iadh Ounis. 2010. Exploiting Query Reformulations for Web Search Result Diversification. In WWW. [28] Rodrygo L.T. Santos, Craig Macdonald, Iadh Ounis, and others. 2015. Search result diversification. Foundations and Trends® in Information Retrieval (2015). [29] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. [30] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In NIPS. [31] Fen Xia, Tie-Yan Liu, Jue Wang, Wensheng Zhang, and Hang Li. 2008. Listwise Approach to Learning to Rank: Theory and Algorithm. In ICML. [32] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In SIGIR. [33] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversification. In SIGIR. [34] Jeonghee Yi and Farzin Maghoul. 2009. Query Clustering Using Click-through Graph. In WWW. [35] Hai-Tao Yu and Fuji Ren. 2014. Search Result Diversification via Filling Up Multiple Knapsacks. In CIKM. [36] Yisong Yue and Thorsten Joachims. 2008. Predicting Diverse Subsets Using Structural SVMs. In ICML. [37] Cheng Xiang Zhai, William W. Cohen, and John Lafferty. 2003. Beyond Independent Relevance: Methods and Evaluation Metrics for Subtopic Retrieval. In SIGIR. [38] Zhiyong Zhang and Olfa Nasraoui. 2006. Mining Search Engine Query Logs for Query Recommendation. In WWW. [39] Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng, and Shuzi Niu. 2014. Learning for Search Result Diversification. In SIGIR.",1,INTENT,True
554,0,,False
,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
End-to-End Neural Ad-hoc Ranking with Kernel Pooling,1,hoc,True
Chenyan Xiong,0,,False
Carnegie Mellon University cx@cs.cmu.edu,0,,False
Zhuyun Dai,0,,False
Carnegie Mellon University zhuyund@cs.cmu.edu,0,,False
Jamie Callan,0,,False
Carnegie Mellon University callan@cs.cmu.edu,0,,False
Zhiyuan Liu,0,,False
Tsinghua University liuzy@tsinghua.edu.cn,0,,False
Russell Power,0,,False
Allen Institute for AI russellp@allenai.org,0,,False
ABSTRACT,0,,False
"is paper proposes K-NRM, a kernel based neural model for document ranking. Given a query and a set of documents, K-NRM uses a translation matrix that models word-level similarities via word embeddings, a new kernel-pooling technique that uses kernels to extract multi-level so match features, and a learning-to-rank layer that combines those features into the nal ranking score.",0,,False
"e whole model is trained end-to-end. e ranking layer learns desired feature pa erns from the pairwise ranking loss. e kernels transfer the feature pa erns into so -match targets at each similarity level and enforce them on the translation matrix. e word embeddings are tuned accordingly so that they can produce the desired so matches. Experiments on a commercial search engine's query log demonstrate the improvements of K-NRM over prior feature-based and neural-based states-of-the-art, and explain the source of K-NRM's advantage: Its kernel-guided embedding encodes a similarity metric tailored for matching query words to document words, and provides e ective multi-level so matches.",1,ad,True
KEYWORDS,0,,False
"Ranking, Neural IR, Kernel Pooling, Relevance Model, Embedding",0,,False
1 INTRODUCTION,1,DUC,True
"In traditional information retrieval, queries and documents are typically represented by discrete bags-of-words, the ranking is based on exact matches between query and document words, and trained ranking models rely heavily on feature engineering. In comparison, newer neural information retrieval (neural IR) methods use continuous text embeddings, model the query-document relevance via so matches, and aim to learn feature representations automatically. With the successes of deep learning in many related areas, neural IR has the potential to rede ne the boundaries of information retrieval; however, achieving that potential has been di cult so far.",1,ad,True
 e rst two authors contributed equally.,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080809",1,ad,True
"Many neural approaches use distributed representations (e.g., word2vec [17]), but in spite of many e orts, distributed representations have a limited history of success for document ranking. Exact match of query words to document words is a strong signal of relevance [8], whereas so -match is a weaker signal that must be used carefully. Word2vec may consider `pi sburgh' to be similar to `boston', and `hotel' to be similar to `motel'. However, a person searching for `pi sburgh hotel' may accept a document about `pi sburgh motel', but probably will reject a document about `boston hotel'. How to use these so -match signals e ectively and reliably is an open problem.",0,,False
"is work addresses these challenges with a kernel based neural ranking model (K-NRM). K-NRM uses distributed representations to represent query and document words. eir similarities are used to construct a translation model. Word pair similarities are combined by a new kernel-pooling layer that uses kernels to so ly count the frequencies of word pairs at di erent similarity levels (so TF). e so -TF signals are used as features in a ranking layer, which produces the nal ranking score. All of these layers are di erentiable and allow K-NRM to be optimized end-to-end.",1,ad,True
"e kernels are the key to K-NRM's capability. During learning, the kernels convert the learning-to-rank loss to requirements on so -TF pa erns, and adjust the word embeddings to produce a so match that can be er separate the relevant and irrelevant documents. is kernel-guided embedding learning encodes a similarity metric tailored for matching query and document. e tailored similarity metric is conveyed by the learned embeddings, which produces e ective multi-level so -matches for ad-hoc ranking.",1,ad,True
"Extensive experiments on a commercial search engine's query log demonstrate the signi cant and robust advantage of K-NRM. On di erent evaluation scenarios (in-domain, cross-domain and raw user clicks), and on di erent parts of the query log (head, torso, and tail), K-NRM outperforms both feature-based ranking and neural ranking states-of-the-art by as much as 65%. K-NRM's advantage is not from an unexplainable `deep learning magic', but the long-desired so match achieved by its kernel-guided embedding learning. In our analysis, if used without the multi-level so match or the embedding learning, the advantage of K-NRM quickly diminishes; while with the kernel-guided embedding learning, K-NRM successfully learns relevance-focused so matches using its embedding and ranking layers, and the memorized ranking preferences generalize well to di erent testing scenarios.",1,ad,True
e next section discusses related work. Section 3 presents the kernel-based neural ranking model. Experimental methodology is discussed in Section 4 and evaluation results are presented in Section 5. Section 6 concludes.,0,,False
55,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 RELATED WORK,0,,False
"Retrieval models such as query likelihood and BM25 are based on exact matching of query and document words, which limits the information available to the ranking model and may lead to problems such vocabulary mismatch [4]. Statistical translation models were an a empt to overcome this limitation. ey model query-document relevance using a pre-computed translation matrix that describes the similarities between word pairs [1]. At query time, the ranking function considers the similarities of all query and document word pairs, allowing query words to be so -matched to document words.",1,ad,True
e translation matrix can be calculated via mutual information in a corpus [12] or using user clicks [6].,0,,False
"Word pair interactions have also been modeled by word embeddings. Word embeddings trained from surrounding contexts, for example, word2vec [17], are considered to be the factorization of word pairs' PMI matrix [14]. Compared to word pair similarities which are hard to learn, word embeddings provide a smooth low-level approximation of word similarities that may improve translation models [8, 24].",0,,False
"Some research has questioned whether word embeddings based on surrounding context, such as word2vec, are suitable for ad hoc ranking. Instead, it customizes word embeddings for search tasks. Nalisnick et al. propose to match query and documents using both the input and output of the embedding model, instead of only using one side of them [19]. Diaz et al. nd that word embeddings trained locally on pseudo relevance feedback documents are more related to the query's information needs, and can provide be er query expansion terms [5].",1,ad,True
"Current neural ranking models fall into two groups: representation based and interaction based [8]. e earlier focus of neural IR was mainly on representation based models, in which the query and documents are rst embedded into continuous vectors, and the ranking is calculated from their embeddings' similarity. For example, DSSM [11] and its convolutional version CDSSM [22] map words to le er-tri-grams, embed query and documents using neural networks built upon the le er-tri-grams, and rank documents using their embedding similarity with the query.",0,,False
"e interaction based neural models, on the other hand, learn query-document matching pa erns from word-level interactions. For example, ARC-II [10] and MatchPyramid [20] build hierarchical Convolutional Neural Networks (CNN) on the interactions of two texts' word embeddings; they are e ective in matching tweetretweet and question-answers [10]. e Deep Relevance Matching Model (DRMM) uses pyramid pooling (histogram) [7] to summarize the word-level similarities into ranking signals [9]. e word level similarities are calculated from pre-trained word2vec embeddings, and the histogram counts the number of word pairs at di erent similarity levels. e counts are combined by a feed forward network to produce nal ranking scores. Interaction based models and representation based models address the ranking task from di erent perspectives, and can be combined for further improvements [18].",1,ad,True
"is work builds upon the ideas of customizing word embeddings and the interaction based neural models: K-NRM ranks documents using so matches from query-document word interactions, and learns to encode the relevance preferences using customized word embeddings at the same time, which is achieved by the kernels.",0,,False
Query Translation Matrix Kernels,1,Query,True
(! words),0,,False
001322,0,,False
300 300,0,,False
...,0,,False
M!×#,0,,False
0(2,0,,False
300,0,,False
...,0,,False
...,0,,False
Document,0,,False
(# words),0,,False
014,0,,False
300,0,,False
034 064,0,,False
300 300,0,,False
...,0,,False
...,0,,False
...,0,,False
...,0,,False
...,0,,False
054,0,,False
300,0,,False
...,0,,False
Soft-TF,0,,False
Ranking Features,0,,False
012,0,,False
Final,0,,False
Ranking,0,,False
...,0,,False
Score,0,,False
7 ... 0(2,0,,False
"... W,b",0,,False
...,0,,False
Embedding Translation,0,,False
Layer,0,,False
Layer,0,,False
Kernel Pooling,0,,False
Learning-To-Rank,0,,False
"Figure 1: e Architecture of K-NRM. Given input query words and document words, the embedding layer maps them into distributed representations, the translation layer calculates the word-word similarities and forms the translation matrix, the kernel pooling layer generate so -TF counts as ranking features, and the learning to rank layer combines the so -TF to the nal ranking score.",0,,False
3 KERNEL BASED NEURAL RANKING,0,,False
"is section presents K-NRM, our kernel based neural ranking model. We rst discuss how K-NRM produces the ranking score for a querydocument pair with their words as the sole input (ranking from scratch). en we derive how the ranking parameters and word embeddings in K-NRM are trained from ranking labels (learning end-to-end).",0,,False
3.1 Ranking from Scratch,0,,False
"Given a query q and a document d, K-NRM aims to generate a ranking score f (q, d) only using query words q ,"" {t1q , ...tiq ..., tnq } and document words d "","" {t1d , ...tjd ..., tmd }. As shown in Figure 1, K-NRM achieves this goal via three components: translation model, kernel-""",0,,False
"pooling, and learning to rank.",0,,False
Translation Model: K-NRM rst uses an embedding layer to,0,,False
map each word t to an L-dimension embedding ìt :,0,,False
t  ìt .,0,,False
en a translation layer constructs a translation matrix M. Each element in M is the embedding similarity between a query word and a document word:,0,,False
Mi j,0,,False
",",0,,False
"cos( ìtiq ,",0,,False
ìt,0,,False
d j,0,,False
).,0,,False
e translation model in K-NRM uses word embeddings to recover,0,,False
the word similarities instead of trying to learn one for each word,1,ad,True
pair. Doing so requires much fewer parameters to learn. For a,0,,False
"vocabulary of size |V | and the embedding dimension L, K-NRM's",0,,False
"translation model includes |V | × L embedding parameters, much fewer than learning all pairwise similarities (|V |2).",0,,False
56,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
SoftTF SoftTF,0,,False
K1,0,,False
K2,0,,False
K3,0,,False
gradient,1,ad,True
!(#$(%&)),0,,False
K1,0,,False
K2,0,,False
K3,0,,False
Word Pair Similarity (a) Ranking,0,,False
gradient,1,ad,True
!(%&(),0,,False
Word Pair Similarity,0,,False
(b) Learning,0,,False
Figure 2: Illustration of kernels in the ranking (forward) process and learning (backward) process.,0,,False
Kernel-Pooling: K-NRM then uses kernels to convert wordword interactions in the translation matrix M to query-document ranking features (M):,0,,False
n,0,,False
"(M) , log Kì(Mi )",0,,False
"i ,1",0,,False
"Kì(Mi ) ,"" {K1(Mi ), ..., KK (Mi )}""",0,,False
"Kì(Mi ) applies K kernels to the i-th query word's row of the translation matrix, summarizing (pooling) it into a K-dimensional feature vector. e log-sum of each query word's feature vector forms the query-document ranking feature vector .",0,,False
e e ect of Kì depends on the kernel used. is work uses the RBF kernel:,0,,False
"Kk (Mi ) ,",0,,False
j,0,,False
exp(-,0,,False
(Mi,0,,False
j - µk 2k2,0,,False
)2,0,,False
).,0,,False
"As illustrated in Figure 2a, the RBF kernel Kk calculates how word pair similarities are distributed around it: the more word pairs with similarities closer to its mean µk , the higher its value. Kernel pooling with RBF kernels is a generalization of existing pooling techniques. As   , the kernel pooling function devolves to the mean pooling. µ ,"" 1 and   0 results in a kernel that only responds to exact matches, equivalent to the TF value from sparse models. Otherwise, the kernel functions as `so -TF'1. µ de nes the similarity level that `so -TF' focuses on; for example, a kernel with µ "","" 0.5 calculates the number of document words whose similarities to the query word are close to 0.5.  de nes the kernel width, or the range of its `so -TF' count.""",0,,False
Learning to Rank: e ranking features (M) are combined by a ranking layer to produce the nal ranking score:,0,,False
"f (q, d) , tanh(wT (M) + b).",0,,False
w and b are the ranking parameters to learn. tanh() is the activation function. It controls the range of ranking score to facilitate the learning process. It is rank-equivalent to a typical linear learning to rank model.,0,,False
"1 e RBF kernel is one of the most popular choices. Other kernels with similar density estimation e ects can also be used, as long as they are di erentiable. For example, polynomial kernel can be used, but histograms [9] cannot as they are not di erentiable.",0,,False
"Pu ing every together, K-NRM is de ned as:",0,,False
"f (q, d) , tanh(wT (M) + b)",0,,False
Learning to Rank (1),0,,False
n,0,,False
"(M) , log Kì(Mi )",0,,False
"i ,1",0,,False
So -TF Features (2),0,,False
"Kì(Mi ) ,"" {K1(Mi ), ..., KK (Mi )}""",0,,False
Kernel Pooling (3),0,,False
"Kk (Mi ) ,",0,,False
j,0,,False
exp(-,0,,False
(Mi,0,,False
j - µk 2k2,0,,False
)2,0,,False
),0,,False
RBF Kernel (4),0,,False
Mi j,0,,False
",",0,,False
"cos( ìtiq ,",0,,False
ìt,0,,False
d j,0,,False
),0,,False
Translation Matrix (5),0,,False
t  ìt .,0,,False
Word Embedding (6),0,,False
"Eq. 5-6 embed query words and document words, and calculate the translation matrix. e kernels (Eq. 4) count the so matches between query and document's word pairs at multiple levels, and generate K so -TF ranking features (Eq. 2-3). Eq. 1 is the learning to rank model. e ranking of K-NRM requires no manual features.",0,,False
e only input used is the query and document words. e kernels extract so -TF ranking features from word-word interactions automatically.,0,,False
3.2 Learning End-to-End,0,,False
e training of K-NRM uses the pairwise learning to rank loss:,0,,False
"l(w, b, V) ,",0,,False
"max(0, 1 - f (q, d+) + f (q, d-)). (7)",0,,False
"q d +,d - Dq+,-",0,,False
"Dq+,- are the pairwise preferences from the ground truth: d+ ranks higher than d-. e parameters to learn include the ranking parameters w, b, and the word embeddings V.",0,,False
"e parameters are optimized using back propagation (BP) through the neural network. Starting from the ranking loss, the gradients are rst propagated to the learning-to-rank part (Eq. 1) and update the ranking parameters (w, b), the kernels pass the gradients to the word similarities (Eq. 2-4), and then to the embeddings (Eq. 5).",1,ad,True
Back propagations through the kernels: e embeddings contain millions of parameters V and are the main capacity of the model. e learning of the embeddings is guided by the kernels.,0,,False
"e back propagation rst applies gradients from the loss function (Eq. 7) to the ranking score f (q, d), to increase it (for d+) or decrease it (for d-); the gradients are propagated through Eq. 1 to the feature vector (M), and then through Eq. 2 to the the kernel scores Kì(Mi ). e resulted (Kì(Mi )) is a K dimensional vector:",1,ad,True
"(Kì(Mi )) ,"" { (K1(Mi )), ..., (KK (Mi )}.""",0,,False
Its each dimension (Kk (Mi )) is jointly de ned by the ranking score's gradients and the ranking parameters. It adjusts the corre-,1,ad,True
sponding kernel's score up or down to be er separate the relevant document (d+) from the irrelevant one (d-).,0,,False
e kernels spread the gradient to word similarities in the trans-,1,ad,True
"lation matrix Mij , through Eq. 4:",0,,False
(Mi j ),0,,False
",",0,,False
"K k ,1",0,,False
(µk,0,,False
-,0,,False
(Kk (Mi )) Mi j ) exp(,0,,False
× k2,0,,False
(Mij -µk -2k2,0,,False
)2,0,,False
),0,,False
.,0,,False
(8),0,,False
e kernel-guided embedding learning process is illustrated in,0,,False
Figure 2b. A kernel pulls the word similarities closer to its µ to,0,,False
57,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Training and testing dataset characteristics.,0,,False
Training Testing,0,,False
eries,0,,False
"95,229",0,,False
"1,000",0,,False
Documents Per ery,0,,False
12.17,0,,False
30.50,0,,False
Search Sessions,1,Session,True
"31,201,876 4,103,230",0,,False
Vocabulary Size,0,,False
"165,877 19,079",0,,False
"increase its so -TF count, or pushes the word pairs away to reduce it, based on the gradients received in the back-propagation. e strength of the force also depends on the the kernel's width k and the word pair's distance to µk : approximately, the wider the kernel is (bigger k ), and the closer the word pair's similarity to µk , the stronger the force is (Eq. 8). e gradient a word pair's similarity received, (Mij ), is the combination of the forces from all K kernels.",1,ad,True
"e word embedding model receives (Mij ) and updates the embeddings accordingly. Intuitively, the learned word embeddings are aligned to form multi-level so -TF pa erns that can separate the relevant documents from the irrelevant ones in training, and the learned embedding parameters V memorize this information. When testing, K-NRM extracts so -TF features from the learned word embeddings using the kernels and produces the nal ranking score using the ranking layer.",0,,False
4 EXPERIMENTAL METHODOLOGY,0,,False
is section describes our experimental methods and materials.,0,,False
4.1 Dataset,0,,False
"Our experiments use a query log sampled from search logs of Sogou.com, a major Chinese commercial search engine. e sample contains 35 million search sessions with 96, 229 distinct queries.",1,Sogou,True
"e query log includes queries, displayed documents, user clicks, and dwell times. Each query has an average of 12 documents displayed. As the results come from a commercial search engine, the returned documents tend to be of very high quality.",0,,False
"e primary testing queries were 1, 000 queries sampled from head queries that appeared more than 1, 000 times in the query log. Most of our evaluation focuses on the head queries; we use tail query performance to evaluate model robustness. e remaining queries were used to train the neural models. Table 1 provides summary statistics for the training and testing portions of the search log.",1,ad,True
"e query log contains only document titles and URLs. e full texts of testing documents were crawled and parsed using Boilerpipe [13] for our word-based baselines (described in Section 4.3). Chinese text was segmented using the open source so ware ICTCLAS [23]. A er segmentation, documents are treated as sequences of words (as with English documents).",0,,False
4.2 Relevance Labels and Evaluation Scenarios,0,,False
"Neural models like K-NRM and CDSSM require a large amount of training data. Acquiring a su cient number of manual training labels outside of a large organization would be cost-prohibitive. User click data, on the other hand, is easy to acquire and prior research has shown that it can accurately predict manual labels. For our experiments training labels were generated based on user clicks from the training sessions.",0,,False
"Table 2: Testing Scenarios. DCTR Scores are inferred by DCTR click model [3]. TACM Scores are inferred by TACM click model [15]. Raw Clicks use the sole click in a session as the positive label. e label distribution is the number of relevance labels from 0-4 from le to right, if applicable.",0,,False
Condition Testing-SAME Testing-DIFF Testing-RAW,0,,False
Label DCTR Scores TACM Scores Raw Clicks,0,,False
"Label Distribution 70%, 19.6%, 9.8%, 1.3%, 1.1% 79%, 14.7%, 4.6%, 0.9%, 0.9% 2,349,280 clicks",0,,False
"ere is a large amount of prior research on building click models to model user behavior and to infer reliable relevance signals from clicks [3]. is work uses one of the simplest click models, DCTR, to generate relevance scores from user clicks [3]. DCTR calculates the relevance scores of a query-document pair based on their click through rates. Despite being extremely simple, it performs rather well and is a widely used baseline [3]. Relevance scores from DCTR are then used to generate preference pairs to train our models.",0,,False
"e testing labels were also estimated from the click log, as manual relevance judgments were not made available to us. Note",1,ad,True
"that there was no overlap between training queries and testing queries. Testing-SAME infers relevance labels using DCTR, the same",0,,False
click model used for training. is se ing evaluates the ranking model's ability to t user preferences (click through rates).,0,,False
"Testing-DIFF infers relevance scores using TACM [15], a stateof-the-art click model. TACM is a more sophisticated model and uses both clicks and dwell times. On an earlier sample of Sogou's query log, the TACM labels aligned extremely well with expert annotations: when evaluated against manual labels, TACM achieved an NDCG@5 of up to 0.97 [15]. is is substantially higher than the agreement between the manual labels generated by the authors for a sample of queries. is precision makes TACM's inferred scores a good approximation of expert labels, and Testing-DIFF is expected to produce evaluation results similar to expert labels.",1,Sogou,True
"Testing-RAW is the simplest click model. Following the cascade assumption [3], we treat the clicked document in each single-click session as a relevant document, and test whether the model can put it at the top of the ranking. Testing-Raw only uses singleclick sessions ( 57% of the testing sessions are single-click sessions). Testing-RAW is a conservative se ing that uses raw user feedback. It eliminates the in uence of click models in testing, and evaluates the ranking model's ability to overcome possible disturbances from the click models.",1,ad,True
"e three testing scenarios are listed in Table 2. Following TREC methodology, the Testing-SAME and Testing-DIFF's inferred relevance scores were mapped to 5 relevance grades. resholds were chosen so that our relevance grades have the same distribution as TREC Web Track 2009-2012 qrels.",1,TREC,True
"Search quality was measured using NDCG at depths {1, 3, 10} for Testing-SAME and Testing-DIFF. We focused on early ranking positions that are more important for commercial search engines. Testing-RAW was evaluated by mean reciprocal rank (MRR) as there is only one relevant document per query. Statistical signi cance was tested using the permutation test with p < 0.05.",0,,False
58,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: e number of parameters and the word embeddings used by baselines and K-NRM. `­' indicates not applicable, e.g. unsupervised methods have no parameters, and word-based methods do not use embeddings.",0,,False
"Method Lm, BM25 RankSVM Coor-Ascent Trans DRMM CDSSM K-NRM",0,,False
Number of Parameters ­ 21 21 ­ 161,0,,False
"10,877,657 49,763,110",0,,False
Embedding ­ ­ ­,0,,False
word2vec word2vec,0,,False
­ end-to-end,0,,False
4.3 Baselines,0,,False
Our baselines include both traditional word-based ranking models as well as more recent neural ranking models.,1,ad,True
"Word-based baselines include BM25 and language models with Dirichlet smoothing (Lm). ese unsupervised retrieval methods were applied on the full text of candidate documents, and used to re-rank them. We found that these methods performed be er on full text than on titles. Full text default parameters were used.",0,,False
"Feature-based learning to rank baselines include RankSVM2, a state-of-the-art pairwise ranker, and coordinate ascent [16] (CoorAscent3), a state-of-the-art listwise ranker. ey use typical wordbased features: Boolean AND; Boolean OR; Coordinate match; TFIDF; BM25; language models with no smoothing, Dirichlet smoothing, JM smoothing and two-way smoothing; and bias. All features were applied to the document title and body. e parameters of the retrieval models used in feature extraction are kept default.",0,,False
"Neural ranking baselines include DRMM [9], CDSSM [21], and a simple embedding-based translation model, Trans.",0,,False
"DRMM is the state-of-the-art interaction based neural ranking model [9]. It performs histogram pooling on the embedding based translation matrix and uses the binned so -TF as the input to a ranking neural network. e embeddings used are pre-trained via word2vec [17] because the histograms are not di erentiable and prohibit end-to-end learning. We implemented the best variant, DRMMLCH×IDF. e pre-trained embeddings were obtained by applying the skip-gram method from word2vec on our training corpus (document titles displayed in training sessions).",0,,False
"CDSSM [22] is the convolutional version of DSSM [11]. CDSSM maps English words to le er-tri-grams using a word-hashing technique, and uses Convolutional Neural Networks to build representations of the query and document upon the le er-tri-grams. It is a state-of-the-art representation based neural ranking model. We implemented CDSSM in Chinese by convolving over Chinese characters. (Chinese characters can be considered as similar to English le er-tri-grams with respect to word meaning). CDSSM is also an end-to-end model, but uses discrete le er-tri-grams/Chinese characters instead of word embeddings.",1,ad,True
Trans is an unsupervised embedding based translation model. Its translation matrix is calculated by the cosine similarity of word,0,,False
2h ps://www.cs.cornell.edu/people/tj/svm light/svm rank.html 3h ps://sourceforge.net/p/lemur/wiki/RankLib/,1,wiki,True
"embeddings from the same word2vec used in DRMM, and then averaged to the query-document ranking score.",0,,False
"Baseline Settings: RankSVM uses a linear kernel and the hyperparameter C was selected in the development fold of the cross validation from the range [0.0001, 10].",0,,False
"Recommended se ings from RankLib were used for Coor-Ascent. We obtained the body texts of testing documents from the new Sogou-T corpus [2] or crawled them directly. e body texts were used by all word-based baselines. Neural ranking baselines and K-NRM used only titles for training and testing, as the coverage of Sogou-T on the training documents is low and the training documents could not be crawled given resource constraints. For all baselines, the most optimistic choices were made: featurebased methods (RankSVM and Coor-Ascent) were trained using 10fold cross-validation on the testing set and use both document title and body texts. e neural models were trained on the training set with the same se ings as K-NRM, and only use document titles (they still perform be er than only using the testing data). eoretically, this gives the sparse models a slight performance advantage as their training and testing data were drawn from the same distribution.",1,Sogou,True
4.4 Implementation Details,0,,False
"is section describes the con gurations of our K-NRM model. Model training was done on the full training data as in Table 1, with training labels inferred by DCTR, as described in Section 4.2.",0,,False
"e embedding layer used 300 dimensions. e vocabulary size of our training data was 165, 877. e embedding layer was initialized with the word2vec trained on our training corpus.",0,,False
"e kernel pooling layer had K ,"" 11 kernels. One kernel harvests exact matches, using µ0 "", 1.0 and  ,"" 10-3. µ of the other 10 kernels is spaced evenly in the range of [-1, 1], that is µ1 "","" 0.9, µ2 "","" 0.7, ..., µ10 "", -0.9. ese kernels can be viewed as 10 so -TF bins.  is set to 0.1. e e ects of varying  are studied in Section 5.6.",1,ad,True
"Model optimization used the Adam optimizer, with batch size 16, learning rate , 0.001 and  ,"" 1e - 5. Early stopping was used with a patience of 5 epochs. We implemented our model using TensorFlow. e model training took about 50 milliseconds per batch, and converged in 12 hours on an AWS GPU machine.""",0,,False
Table 3 summarizes the number of parameters used by the baselines and K-NRM. Word2vec refers to pre-trained word embeddings using skip-gram on the training corpus. End-to-end means that the embeddings were trained together with the ranking model.,0,,False
"CDSSM learns hundreds of convolution lters on Chinese characters, thus has millions of parameters. K-NRM's parameter space is even larger as it learns an embedding for every Chinese word. Models with more parameters in general are expected to t be er but may also require more training data to avoid over ing.",0,,False
5 EVALUATION RESULTS,0,,False
"Our experiments investigated K-NRM's e ectiveness, as well as its behavior on tail queries, with less training data, and with di erent kernel widths.",0,,False
59,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: Ranking accuracy of K-NRM and baseline methods. Relative performances compared with Coor-Ascent are in percent-,0,,False
"ages. Win/Tie/Loss are the number of queries improved, unchanged, or hurt, compared to Coor-Ascent on NDCG@10. , , §, ¶ indicate statistically signi cant improvements over Coor-Ascent, Trans, DRMM§ and CDSSM¶, respectively.",0,,False
(a) Testing-SAME. Testing labels are inferred by the same click model (DCTR) as the training labels used by neural models.,0,,False
Method Lm BM25 RankSVM Coor-Ascent Trans DRMM CDSSM K-NRM,0,,False
NDCG@1,0,,False
0.1261,0,,False
-20.89%,0,,False
0.1422,0,,False
-10.79%,0,,False
0.1457 0.1594§ ¶,0,,False
-8.59% ­,0,,False
0.1347,0,,False
-15.50%,0,,False
0.1366,0,,False
-14.30%,0,,False
0.1441,0,,False
-9.59%,0,,False
0.2642§¶ +65.75%,0,,False
NDCG@3,0,,False
0.1648,0,,False
-26.46%,0,,False
0.1757,0,,False
-21.60%,0,,False
0.1905,0,,False
-14.99%,0,,False
0.2241§ ¶,0,,False
­,0,,False
0.1852,0,,False
-17.36%,0,,False
0.1902,0,,False
-15.13%,0,,False
0.2014,0,,False
-10.13%,0,,False
0.3210§¶ +43.25%,0,,False
NDCG@10,0,,False
0.2821,0,,False
-20.45%,0,,False
0.2868,0,,False
-10.14%,0,,False
0.3087,0,,False
-12.97%,0,,False
0.3547§ ¶,0,,False
­,0,,False
0.3147,0,,False
-11.28%,0,,False
0.3150,0,,False
-11.20%,0,,False
0.3329§,0,,False
-6.14%,0,,False
0.4277§¶ +20.58%,0,,False
W/T/L 293/116/498 299/125/483 371/151/385,0,,False
­/­/­ 318/140/449 318/132/457 341/149/417 447/153/307,0,,False
"(b) Testing-DIFF. Testing labels are inferred by a di erent click model, TACM, which approximates expert labels very well [15].",0,,False
Method Lm BM25 RankSVM Coor-Ascent Trans DRMM CDSSM K-NRM,0,,False
NDCG@1,0,,False
0.1852,0,,False
-11.34%,0,,False
0.1631,0,,False
-21.92%,0,,False
0.1700 0.2089 ¶,0,,False
-18.62% ­,0,,False
0.1874,0,,False
-10.29%,0,,False
0.2068,0,,False
-1.00%,0,,False
0.1846,0,,False
-10.77%,0,,False
0.2984§¶ +42.84%,0,,False
NDCG@3,0,,False
0.1989,0,,False
-17.23%,0,,False
0.1894,0,,False
-21.18%,0,,False
0.2036 0.2403,0,,False
-15.27% ­,0,,False
0.2127 0.2491 0.2358,0,,False
-11.50% +3.67% -1.86%,0,,False
0.3092§¶ +28.26%,0,,False
NDCG@10,0,,False
0.3270,0,,False
-13.38%,0,,False
0.3254,0,,False
-13.81%,0,,False
0.3519 0.3775 ¶,0,,False
-6.78% ­,0,,False
0.3454 0.3809 ¶,0,,False
-8.51% +0.91%,0,,False
0.3557,0,,False
-5.79%,0,,False
0.4201§¶ +11.28%,0,,False
W/T/L 369/50/513 349/53/530 380/75/477,0,,False
­/­/­ 385/68/479 430/66/436 391/65/476 474/63/395,0,,False
Table 5: Ranking performance on Testing-RAW. MRR eval-,0,,False
uates the mean reciprocal rank of clicked documents in,0,,False
single-click sessions. Relative performance in the percent-,0,,False
"ages and W(in)/T(ie)/L(oss) are compared to Coor-Ascent. , , §, ¶ indicate statistically signi cant improvements over Coor-Ascent, Trans, DRMM§ and CDSSM¶, respectively.",0,,False
Method Lm BM25 RankSVM Coor-Ascent Trans DRMM CDSSM K-NRM,0,,False
MRR,0,,False
0.2193,0,,False
-9.19%,0,,False
0.2280,0,,False
-5.57%,0,,False
0.2241,0,,False
-7.20%,0,,False
0.2415,0,,False
­,0,,False
0.2181,0,,False
-9.67%,0,,False
0.2335,0,,False
-3.29%,0,,False
0.2321,0,,False
-3.90%,0,,False
0.3379§¶ +39.92%,0,,False
W/T/L 416/09/511 456/07/473 450/78/473,0,,False
­/­/­/ 406/08/522 419/12/505 405/11/520 507/05/424,0,,False
5.1 Ranking Accuracy,0,,False
"Tables 4a, 4b and 5 show the ranking accuracy of K-NRM and our baselines under three conditions.",0,,False
"Testing-SAME (Table 4a) evaluates the model's ability to t user preferences when trained and evaluated on labels generated by the same click model (DCTR). K-NRM outperforms word-based baselines by over 65% on NDCG@1, and over 20% on NDCG@10.",0,,False
"e improvements over neural ranking models are even bigger: On NDCG@1 the margin between K-NRM and the next best neural model is 83%, and on NDCG@10 it is 28%.",0,,False
"Testing-DIFF (Table 4b) evaluates the model's relevance matching performance by testing on TACM inferred relevance labels, a good approximation of expert labels. Because the training and testing labels were generated by di erent click models, TestingDIFF challenges each model's ability to t the underlying relevance signals despite perturbations caused by di ering click model biases. Neural models with larger parameter spaces tend to be more vulnerable to this domain di erence: CDSSM actually performs worse than DRMM, despite using thousands times more parameters. However, K-NRM demonstrates its robustness and is able to outperform all baselines by more than 40% on NDCG@1 and 10% on NDCG@10.",0,,False
"Testing-RAW (Table 5) evaluates each model's e ectiveness directly by user clicks. It tests how well the model ranks the most satisfying document (the only one clicked) in each session. K-NRM improves MRR from 0.2415 (Coor-Ascent) to 0.3379. is di erence is equal to moving the clicked document's from rank 4 to rank 3. e MRR and NDCG@1 improvements demonstrate K-NRM's precision oriented property--its biggest advantage is on the earliest ranking positions. is characteristic aligns with K-NRM's potential role in web search engines: as a sophisticate re-ranker, K-NRM is most possibly used at the nal ranking stage, in which the rst relevant document's ranking position is the most important.",1,ad,True
"e two neural ranking baselines DRMM and CDSSM perform similarly in all three testing scenarios. e interaction based model, DRMM, is more robust to click model biases and performs slightly be er on Testing-DIFF, while the representation based model, CDSSM, performs slightly be er on Testing-SAME. However, the featurebased ranking model, Coor-Ascent, performs be er than all neural",0,,False
60,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 6: e ranking performances of several K-NRM variants. Relative performances and statistical signi cances are all,0,,False
"compared with K-NRM's full model. , , §, ¶, and  indicate statistically signi cant improvements over K-NRM's variants of exact-match, word2vec, click2vec§, max-pool¶, and mean-pool, respectively.",0,,False
K-NRM Variant exact-match word2vec click2vec max-pool mean-pool full model,0,,False
Testing-SAME,0,,False
NDCG@1,0,,False
NDCG@10,0,,False
0.1351 0.1529,0,,False
0.1600,0,,False
-49% 0.2943 -42% 0.3223¶ -39% 0.3790¶,0,,False
-31% -24% -11%,0,,False
0.1413 0.2297§ ¶ 0.2642§ ¶,0,,False
-47% -13%,0,,False
­,0,,False
0.2979 0.3614§ ¶ 0.4277§ ¶,0,,False
-30% -16%,0,,False
­,0,,False
Testing-DIFF,0,,False
NDCG@1,0,,False
0.1788,0,,False
-40%,0,,False
0.2160 ¶ 0.2314 ¶,0,,False
-27% -23%,0,,False
NDCG@10,0,,False
0.3460 ¶,0,,False
-18%,0,,False
0.3811¶ -10%,0,,False
0.4002¶ -4%,0,,False
0.1607 0.2424 ¶ 0.2984§ ¶,0,,False
-46% -19%,0,,False
­,0,,False
0.3334 0.3787 ¶ 0.4201 ¶,0,,False
-21% -10%,0,,False
­,0,,False
Testing-RAW,0,,False
MRR,0,,False
0.2147 0.2427 ¶ 0.2667 ¶,0,,False
-37% -28% -21%,0,,False
0.2260 0.2714 ¶ 0.3379§ ¶,0,,False
-33% -20%,0,,False
­,0,,False
baselines on all three testing scenarios. e di erences can be as high as 15% and some are statistically signi cant. is holds even for Testing-SAME which is expected to favor deep models that access more in-domain training data. ese results remind that no `deep learning magic' can instantly provide signi cant gains for information retrieval tasks. e development of neural IR models also requires an understanding of the advantages of neural methods and how their advantages can be incorporated to meet the needs of information retrieval tasks.,1,ad,True
Table 7: Examples of word matches in di erent kernels. Words in bold are those whose similarities with the query word fall into the corresponding kernel's range (µ).,0,,False
"µ ery: `Maserati' "" 1.0 Maserati Ghibli black interior who knows 0.7 Maserati Ghibli black interior who knows 0.3 Maserati Ghibli black interior who knows -0.1 Maserati Ghibli black interior who knows",0,,False
5.2 Source of E ectiveness,0,,False
"K-NRM di ers from previous ranking methods in several ways: multilevel so matches, word embeddings learned directly from ranking labels, and the kernel-guided embedding learning. is experiment studies these e ects by comparing the following variants of K-NRM.",0,,False
"K-NRM (exact-match) only uses the exact match kernel (µ,  ) ,"" (1, 0.001). It is equivalent to TF.""",0,,False
"K-NRM (word2vec) uses pre-trained word2vec, the same as DRMM. Word embedding is xed; only the ranking part is learned.",0,,False
"K-NRM (click2vec) also uses pre-trained word embedding. But its word embeddings are trained on (query word, clicked title word) pairs. e embeddings are trained using skip-gram model with the same se ings used to train word2vec. ese embeddings are xed during ranking.",0,,False
"K-NRM (max-pool) replaces kernel-pooling with max-pooling. Max-pooling nds the maximum similarities between document words and each query word; it is commonly used by neural network architectures. In our case, given the candidate documents' high quality, the maximum is almost always 1, thus it is similar to TF.",0,,False
K-NRM (mean-pool) replaces kernel-pooling with mean-pooling. It is similar to Trans except that the embedding is trained by learning-to-rank.,0,,False
"All other se ings are kept the same as K-NRM. Table 6 shows their evaluation results, together with the full model of K-NRM.",0,,False
"So match is essential. K-NRM (exact-match) performs similarly to Lm and BM25, as does K-NRM (max-pool). is is expected: without so matches, the only signal for K-NRM to work with is e ectively the TF score.",0,,False
Ad-hoc ranking prefers relevance based word embedding. Using click2vec performs about 5-10% be er than using word2vec. User clicks are expected to be a be er t as they represent user search,1,hoc,True
"preferences, instead of word usage in documents. e relevancebased word embedding is essential for neural models to outperform feature-based ranking. K-NRM (click2vec) consistently outperforms Coor-Ascent, but K-NRM (word2vec) does not.",1,ad,True
"Learning-to-rank trains be er word embeddings. K-NRM with mean-pool performs much be er than Trans. ey both use average embedding similarities; the di erence is that K-NRM (mean-pool) uses the ranking labels to tune the word embeddings, while Trans keeps the embeddings xed. e trained embeddings improve the ranking accuracy, especially on top ranking positions.",0,,False
"Kernel-guided embedding learning provides be er so matches. K-NRM stably outperforms all of its variants. K-NRM (click2vec) uses the same ranking model, and its embeddings are trained on click contexts. K-NRM (mean-pool) also learns the word embeddings using learning-to-rank. e main di erence is how the information from relevant labels is used when learning word embeddings. In KNRM (click2vec) and K-NRM (mean-pool), training signals from relevance labels are propagated equally to all querydocument word pairs. In comparison, K-NRM uses kernels to enforce multi-level so matches; query-document word pairs on di erent similarity levels are adjusted di erently (see Section 3.2).",1,ad,True
"Table 7 shows an example of K-NRM's learned embeddings. e bold words in each row are those `activated' by the corresponding kernel: their embedding similarities to the query word `Maserati' fall closest to the kernel's µ. e example illustrates that the kernels recover di erent levels of relevance matching: µ , 1 is exact match; µ , 0.7 matches the car model with the brand; µ , 0.3 is about the car color; µ ,"" -0.1 is background noise. e mean-pool and click2vec's uni-level training loss mix the matches at multiple levels, while the kernels provide more ne-grained training for the embeddings.""",0,,False
61,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
(a) Individual kernel's performance.,0,,False
(b) Kernel sizes before and a er learning.,0,,False
(c) Word pair movements.,0,,False
"Figure 3: Kernel guided word embedding learning in K-NRM. Fig. 3a shows the performance of K-NRM when only one kernel is used in testing. Its X-axis is the µ of the used kernel. Its Y-axis is the MRR results. Fig. 3b shows the log number of word pairs that are closest to each kernel, before K-NRM learning (Word2Vec) and a er. Its X-axis is the µ of kernels. Fig. 3c illustrates",0,,False
"the word pairs' movements in K-NRM's learning. e heat map shows the fraction of word pairs from the row kernel (before learning, µ marked on the le ) to the column kernel (a er learning, µ at the bottom).",0,,False
5.3 Kernel-Guided Word Embedding learning,0,,False
"In K-NRM, word embeddings are initialized by word2vec and trained by the kernels to provide e ective so -match pa erns. is experiment studies how training a ects the word embeddings, showing the responses of kernels in ranking, the word similarity distributions, and the word pair movements during learning.",0,,False
"Figure 3a shows the performance of K-NRM when only a single kernel is used during testing. e x-axis is the µ of the kernel. e results indicate the kernels' importance. e kernels on the far le ( -0.7), the far right ( 0.7), and in the middle ({-0.1, 0.1}) contribute li le; the kernels on the middle le ({-0.3, -0.5}) contribute the most, followed by those on the middle right ({0.3, 0.5}). Higher µ does not necessarily mean higher importance or be er so matching. Each kernel focuses on a group of word pairs that fall into a certain similarity range; the importance of this similarity range is learned by the model.",0,,False
"Figure 3b shows the number of word pairs activated in each kernel before training (Word2Vec) and a er (K-NRM). e X-axis is the kernel's µ. e Y-axis is the log number of word pairs activated (whose similarities are closest to corresponding kernel's µ). Most similarities fall into the range (-0.4, 0.6). ese histograms help explain why the kernels on the far right and far le do not contribute much: because there are fewer word pairs in them.",0,,False
"Figure 3c shows the word movements during the embedding learning. Each cell in the matrix contains the word pairs whose similarities are moved from the kernel in the corresponding row (µ on the le ) to the kernel in the corresponding column (µ at the bo om). e color indicates the fraction of the moved word pairs in the original kernel. Darker indicates a higher fraction. Several examples of word movements are listed in Table 8. Combining Figure 3 and Table 8, the following trends can be observed in the kernel-guided embedding learning process.",0,,False
"Many word pairs are decoupled. Most of the word movements are from other kernels to the `white noise' kernels µ  {-0.1, 0.1}.",0,,False
ese word pairs are considered related by word2vec but not by K-NRM. is is the most frequent e ect in K-NRM's embedding learning. Only about 10% of word pairs with similarities  0.5 are kept.,0,,False
is implies that document ranking requires a stricter measure of,0,,False
"so match. For example, as shown in Table 8's rst row, a person searching for `China-Unicom', one of the major mobile carriers in China, is less likely interested in a document about `China-Mobile', another carrier; in the second row, `Maserati' and `car' are decoupled as `car' appears in almost all candidate documents' titles, so it does not provide much evidence for ranking.",0,,False
"New so match pa erns are discovered. K-NRM moved some word pairs from near zero similarities to important kernels. As shown in the third and fourth rows of Table 8, there are word pairs that less frequently appear in the same surrounding context, but convey possible search tasks, for example, `the search for MH370 '. K-NRM also discovers word pairs that convey strong `irrelevant' signals, for example, people searching for `BMW' are not interested in the `contact us' page.",0,,False
"Di erent levels of so matches are enforced. Some word pairs moved from one important kernel to another. is may re ect the di erent levels of so matches K-NRM learned. Some examples are in the last two rows in Table 8. e -0.3 kernel is the most important one, and received word pairs that encode search tasks; the 0.5 kernel received synonyms, which are useful but not the most important, as exact match is not that important in our se ing.",0,,False
5.4 Required Training Data Size,0,,False
"is experiment studies K-NRM's performance with varying amounts of training data. Results are shown in Figure 4. e X-axis is the number of sessions used for training (e.g. 8K, 32K, . . .), and the coverage of testing vocabulary in the learned embedding (percentages). Sessions were randomly sampled from the training set. e Y-axis is the performance of the corresponding model. e straight and do ed lines are the performances of Coor-Ascent.",1,Session,True
"When only 2K training sessions are available, K-NRM performs worse than Coor-Ascent. Its word embeddings are mostly unchanged from word2vec as only 16% of the testing vocabulary are covered by the training sessions. K-NRM's accuracy grows rapidly with more training sessions. With only 32K (0.1%) training sessions and 50% coverage of the testing vocabulary, K-NRM surpasses Coor-Ascent on Testing-RAW. With 128K (0.4%) training sessions and 69% coverage on the testing vocabularies, K-NRM surpasses",0,,False
62,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 8: Examples of moved word pairs in K-NRM. From and To are the µ of the kernels the word pairs were in before,0,,False
learning (word2vec) and a er (K-NRM). Values in parenthesis,0,,False
"are the individual kernel's MRR on Testing-RAW, indicating the importance of the kernel. `+' and `-' mark the sign of the kernel weight wk in the ranking layer; `+' means word pair appearances in the corresponding kernel are positively correlated with relevance; `-' means negatively correlated.",0,,False
"From µ ,"" 0.9 (0.20, -) µ "","" 0.5 (0.26, -) µ "","" 0.1 (0.23, -) µ "","" 0.1 (0.23, -) µ "","" 0.5 (0.26, -) µ "","" -0.3 (0.30, +)""",0,,False
"To µ ,"" 0.1 (0.23, -) µ "","" 0.1 (0.23, -) µ "","" -0.3 (0.30, +) µ "","" 0.3 (0.26, -) µ "","" -0.3 (0.30, +) µ "","" 0.5 (0.26, -)""",0,,False
"Word Pairs (wife, husband), (son, daughter), (China-Unicom, China-Mobile) (Maserati, car),( rst, time) (website, homepage) (MH370, search), (pdf, reader) (192.168.0.1, router) (BMW, contact-us), (Win7, Ghost-XP) (MH370, truth), (cloud, share) (HongKong, horse-racing) (oppor9, OPPOR), (6080, 6080YY), (10086, www.10086.com)",1,ad,True
0.45,0,,False
0.40,0,,False
0.35,0,,False
0.30,0,,False
0.25,0,,False
0.20 2K,0,,False
8K,0,,False
32K,0,,False
128K 512K,0,,False
2M,0,,False
8M 32M (ALL),0,,False
16% 28% 49% 69% 85% 94% 98% 99%,0,,False
"Testing-SAME, K-NRM Testing-DIFF, K-NRM Testing-RAW, K-NRM",0,,False
"Testing-SAME, Coor-Ascent Testing-DIFF, Coor-Ascent Testing-RAW, Coor-Ascent",0,,False
"Figure 4: K-NRM's performances with di erent amounts of training data. X-axis: Number of sessions used for training, and the percentages of testing vocabulary covered (second row). Y-axis: NDCG@10 for Testing-SAME and TestingDIFF, and MRR for Testing-RAW.",0,,False
"Coor-Ascent on Testing-SAME and Testing-DIFF. e increasing trends against Testing-SAME and Testing-RAW have not yet plateaued even with 31M training sessions, suggesting that K-NRM can utilize more training data. e performance on Testing-DIFF plateaus a er 500K sessions, perhaps because the click models do not perfectly align with each other; more regularization of the K-NRM model might help under this condition.",0,,False
5.5 Performance on Tail eries,0,,False
"is experiment studies how K-NRM performs on less frequent queries. We split the queries in the query log into Tail (less than 50 appearances), Torso (50-1000 appearances), and Head (more than 1000 appearances). For each category, 1000 queries are randomly",1,ad,True
"Table 9: Ranking accuracy on Tail (frequency < 50), Torso (frequency 50 - 1K) and Head (frequency > 1K) queries.  indicates statistically signi cant improvements of K-NRM over Coor-Ascent on Testing-RAW. Frac is the fraction of the corresponding queries in the search tra c. Cov is the fraction of testing query words covered by the training data.",1,ad,True
Frac,0,,False
Cov,0,,False
"Testing-RAW, MRR",0,,False
Coor-Ascent,0,,False
K-NRM,0,,False
Tail 52% 85%,0,,False
0.2977,0,,False
0.3230 +8.49%,0,,False
Torso 20% 91%,0,,False
0.3202,0,,False
0.3768 +17.68%,0,,False
Head 28% 99%,1,ad,True
0.2415,0,,False
0.3379 +39.92%,0,,False
1 0.8 0.6 0.4 0.2,0,,False
0 0.5,0,,False
",0.4 (0.1990)",0,,False
",""0.2 (00..23944027,, ++4212%%)""",0,,False
",0.025 (0.2347)",0,,False
0.6,0,,False
0.7,0,,False
Significantly Better,0,,False
",""0.1 (00..23834779,, ++4108%%)""",0,,False
",0.05",0,,False
"(00..23706128,, ++2154%%)",0,,False
0.8,0,,False
0.9,0,,False
Not Significantly Better,0,,False
bin,0,,False
Figure 5: K-NRM's performance with di erent  . MRR and relative gains over Coor-Ascent are shown in parenthesis. Kernels drawn in solid lines indicate statistically signi cant improvements over Coor-Ascent.,0,,False
"sampled as testing; the remaining queries are used for training. Following the same experimental se ings, the ranking accuracies of K-NRM and Coor-Ascent are evaluated.",0,,False
"e results are shown in Table 9. Evaluation is only done using Testing-RAW as the tail queries do not provide enough clicks for DCTR and TACM to infer reliable relevance scores. e results show an expected decline of K-NRM's performance on rarer queries. K-NRM uses word embeddings to encode the relevance signals, and as tail queries' words appear less frequently in the training data, it is hard to generalize the embedded relevance signals through them. Nevertheless, even on queries that appear less than 50 times, K-NRM still outperforms Coor-Ascent by 8%.",0,,False
5.6 Hyper Parameter Study,0,,False
"is experiment studies the in uence of the kernel width ( ). We varied the  used in K-NRM's kernels, kept everything else unchanged, and evaluated its performance. e shapes of the kernels with 5 di erent  and the corresponding ranking accuracies are shown in Figure 5. Only Testing-RAW is shown due to limited space; the observation is the same on Testing-SAME and Testing-DIFF.",0,,False
"As shown in Figure 5, kernels too sharp or too at either do not cover the similarity space well, or mixed the matches at di erent levels; they cannot provide reliable improvements. With  between 0.05 and 0.2, K-NRM's improvements are stable.",0,,False
"We have also experimented with several other structures for K-NRM, for example, using more learning to rank layers, and using IDF to weight query words when combining their kernel-pooling",0,,False
63,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"results. However we have only observed similar or worse performances. us, we chose to present the simplest successful model to be er illustrate the source of its e ectiveness.",0,,False
6 CONCLUSION,0,,False
"is paper presents K-NRM, a kernel based neural ranking model for ad-hoc search. e model captures word-level interactions using word embeddings, and ranks documents using a learningto-rank layer. e center of the model is the new kernel-pooling technique. It uses kernels to so ly count word matches at di erent similarity levels and provide so -TF ranking features. e kernels are di erentiable and support end-to-end learning. Supervised by ranking labels, the learning of word embeddings is guided by the kernels with the goal of providing so -match signals that be er separate relevant documents from irrelevant ones. e learned embeddings encode the relevance preferences and provide e ective multi-level so matches for ad-hoc ranking.",1,ad-hoc,True
"Our experiments on a commercial search engine's query log demonstrated K-NRM's advantages. On three testing scenarios (indomain, cross-domain, and raw user clicks), K-NRM outperforms both feature based ranking baselines and neural ranking baselines by as much as 65%, and is extremely e ective at the top ranking positions. e improvements are also robust: Stable gains are observed on head and tail queries, with fewer training data, a wide range of kernel widths, and a simple ranking layer.",1,ad,True
"Our analysis revealed that K-NRM's advantage is not from `deep learning magic' but the long-desired so match between query and documents, which is achieved by the kernel-guided embedding learning. Without it, K-NRM's advantage quickly diminishes: its variants with only exact match, pre-trained word2vec, or uni-level embedding training all perform signi cantly worse, and sometimes fail to outperform the simple feature based baselines.",1,ad,True
"Further analysis of the learned embeddings illustrated how K-NRM tailors them for ad-hoc ranking: More than 90% of word pairs that are mapped together in word2vec are decoupled, satisfying the stricter de nition of so match required in ad-hoc search. Word pairs that are less correlated in documents but convey frequent search tasks are discovered and mapped to certain similarity levels.",1,ad-hoc,True
e kernels also moved word pairs from one kernel to another based on their di erent roles in the learned so match.,0,,False
"Our experiments and analysis not only demonstrated the effectiveness of K-NRM, but also provide useful intuitions about the advantages of neural methods and how they can be tailored for IR tasks. We hope our ndings will be explored in many other IR tasks and will inspire more advances of neural IR research in the near future.",1,ad,True
7 ACKNOWLEDGMENTS,0,,False
"is research was supported by National Science Foundation (NSF) grant IIS-1422676, a Google Faculty Research Award, and a fellowship from the Allen Institute for Arti cial Intelligence. We thank Tie-Yan Liu for his insightful comments and Cheng Luo for helping us crawl the testing documents. Any opinions, ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.",0,,False
REFERENCES,0,,False
[1] A. Berger and J. La erty. Information retrieval as statistical translation. In,0,,False
"Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 222­229. ACM, 1999. [2] L. Cheng, Z. Yukun, L. Yiqun, X. Jingfang, Z. Min, and M. Shaoping. SogouT-16: A new web corpus to embrace ir research. In Proceedings of the 40th International",1,Sogou,True
"ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), page To Appear. ACM, 2017. [3] A. Chuklin, I. Markov, and M. d. Rijke. Click models for web search. Synthesis Lectures on Information Concepts, Retrieval, and Services, 7(3):1­115, 2015. [4] W. B. Cro , D. Metzler, and T. Strohman. Search Engines: Information Retrieval in Practice. Addison-Wesley Reading, 2010. [5] F. Diaz, B. Mitra, and N. Craswell. ery expansion with locally-trained word embeddings. In Proceedings of the 54th Annual Meeting of the Association for Computational (ACL). ACL­Association for Computational Linguistics, 2016. [6] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based translation models for web search: from word models to phrase models. In Proceedings of the 19th ACM international conference on Information and knowledge management (CIKM), pages 1139­1148. ACM, 2010. [7] K. Grauman and T. Darrell. e pyramid match kernel: Discriminative classi-",1,ad,True
"cation with sets of image features. In Tenth IEEE International Conference on Computer Vision (ICCV) Volume 1, volume 2, pages 1458­1465. IEEE, 2005. [8] J. Guo, Y. Fan, Q. Ai, and W. B. Cro . Semantic matching by non-linear word transportation for information retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (CIKM), pages 701­710. ACM, 2016. [9] J. Guo, Y. Fan, A. Qingyao, and W. B. Cro . A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (CIKM), pages 55­64, year,""2016, organization"",""ACM. [10] B. Hu, Z. Lu, H. Li, and Q. Chen. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems (NIPS), pages 2042­2050, 2014. [11] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of""",1,ad-hoc,True
"the 22nd ACM international conference on Conference on information & knowledge management (CIKM), pages 2333­2338. ACM, 2013. [12] M. Karimzadehgan and C. Zhai. Estimation of statistical translation models based on mutual information for ad hoc information retrieval. In Proceedings",1,ad,True
"of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), pages 323­330. ACM, 2010. [13] C. Kohlschu¨ er, P. Fankhauser, and W. Nejdl. Boilerplate detection using shallow text features. In Proceedings of the third ACM international conference on Web Search and Data Mining (WSDM), pages 441­450. ACM, 2010. [14] O. Levy, Y. Goldberg, and I. Dagan. Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211­225, 2015. [15] Y. Liu, X. Xie, C. Wang, J.-Y. Nie, M. Zhang, and S. Ma. Time-aware click model. ACM Transactions on Information Systems (TOIS), 35(3):16, 2016. [16] D. Metzler and W. B. Cro . Linear feature-based models for information retrieval. Information Retrieval, 10(3):257­274, 2007. [17] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 2 h Advances in Neural Information Processing Systems 2013 (NIPS), pages 3111­3119, 2013. [18] B. Mitra, F. Diaz, and N. Craswell. Learning to match using local and distributed representations of text for web search. In Proceedings of the 25th International Conference on World Wide Web (WWW), pages 1291­1299. ACM, 2017. [19] E. Nalisnick, B. Mitra, N. Craswell, and R. Caruana. Improving document ranking with dual word embeddings. In Proceedings of the 25th International Conference on World Wide Web (WWW), pages 83­84. ACM, 2016. [20] L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng. Text matching as image recognition. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, AAAI, pages 2793­2799. AAAI Press, 2016. [21] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. A latent semantic model with convolutional-pooling structure for information retrieval. In Proceedings of the",1,ad,True
"23rd ACM International Conference on Conference on Information and Knowledge Management (CIKM), pages 101­110. ACM, 2014. [22] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. Learning semantic representations using convolutional neural networks for web search. In Proceedings of the 23rd International Conference on World Wide Web (WWW), pages 373­374. ACM, 2014. [23] H.-P. Zhang, H.-K. Yu, D.-Y. Xiong, and Q. Liu. HHMM-based chinese lexical analyzer ICTCLAS. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages 184­187. ACL, 2003. [24] G. Zuccon, B. Koopman, P. Bruza, and L. Azzopardi. Integrating and evaluating neural word embeddings in information retrieval. In Proceedings of the 20th Australasian Document Computing Symposium, page 12. ACM, 2015.",0,,False
64,0,,False
,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
BitFunnel: Revisiting Signatures for Search,0,,False
Bob Goodwin,0,,False
Microsoft,0,,False
Alex Clemmer,0,,False
Heptio,0,,False
Michael Hopcroft,0,,False
Microsoft,0,,False
Mihaela Curmei,0,,False
Microsoft,0,,False
Dan Luu,0,,False
Microsoft,0,,False
Sameh Elnikety,0,,False
Microsoft,0,,False
Yuxiong He,0,,False
Microsoft,0,,False
ABSTRACT,0,,False
"Since the mid-90s there has been a widely-held belief that signature files are inferior to inverted files for text indexing. In recent years the Bing search engine has developed and deployed an index based on bit-sliced signatures. This index, known as BitFunnel, replaced an existing production system based on an inverted index. The driving factor behind the shift away from the inverted index was operational cost savings. This paper describes algorithmic innovations and changes in the cloud computing landscape that led us to reconsider and eventually field a technology that was once considered unusable. The BitFunnel algorithm directly addresses four fundamental limitations in bit-sliced block signatures. At the same time, our mapping of the algorithm onto a cluster offers opportunities to avoid other costs associated with signatures. We show these innovations yield a significant efficiency gain versus classic bit-sliced signatures and then compare BitFunnel with Partitioned Elias-Fano Indexes, MG4J, and Lucene.",1,ad,True
CCS CONCEPTS,0,,False
· Information systems  Search engine indexing; Probabilistic retrieval models; Distributed retrieval; · Theory of computation  Bloom filters and hashing;,0,,False
KEYWORDS,0,,False
Signature Files; Search Engines; Inverted Indexes; Intersection; Bitvector; Bloom Filters; Bit-Sliced Signatures; Query Processing,1,Query,True
1 INTRODUCTION,1,DUC,True
"Commercial search engines [2, 5, 19, 24] traditionally employ inverted indexes. In this work, we show how to use signatures, or bit-strings based on Bloom filters [1], in a large-scale commercial search engine for better performance. Prior work comparing inverted files to signature files established that inverted files outperformed signature files by almost every criterion [28]. However, recent software and hardware trends (e.g., large Web corpora with",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080789",1,ad,True
"of billions of documents, large main memory systems) motivated us to reconsider signature files.",0,,False
"In our signature-based approach, known as BitFunnel, we use a Bloom filter to represent the set of terms in each document as a fixed sequence of bits called a signature. Bloom filters are reasonably space efficient and allow for fast set membership, forming the basis for query processing.",0,,False
"Using this approach, however, poses four major challenges. First, determining the matches for a single term requires examining one signature for each document in the corpus. This involves considerably more CPU and memory cycles than the equivalent operation on an inverted index. Second, term frequency follows a Zipfian distribution, implying that signatures must be long to yield an acceptable false positive rate when searching for the rarest terms. Third, the size of web documents varies substantially, implying that signatures must be long to accommodate the longest documents. Fourth, the configuration of signature-based schemes is not a well-understood problem.",0,,False
We develop a set of techniques to address these challenges: (1) we introduce higher rank rows to reduce query execution time; (2) we employ frequency-conscious signatures to reduce the memory footprint; (3) we shard the corpus to reduce the variability in document size; (4) we develop a cost model for system performance; and (5) we use this model to formulate a constrained optimization to configure the system for efficiency.,1,ad,True
"These techniques are used in the Microsoft Bing search engine, which has been running in production for the last four years on thousands of servers. Compared to an earlier production search engine based on inverted lists that it replaced, BitFunnel improved server query capacity by a factor of 10.",0,,False
2 BACKGROUND AND PRIOR WORK,0,,False
We focus on the problem of identifying those documents in a corpus that match a conjunctive query of keywords. We call this the Matching Problem.,0,,False
"Let corpus C be a set of documents, each of which consists of a set of text terms:",0,,False
"C , {documents D} D , {terms t }",0,,False
Define query Q as a set of text terms:,0,,False
"Q , {terms t }",0,,False
605,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Query Q is said to match document D when every term t  Q is also an element of D. This happens when Q  D or Q , Q  D. Define match set M as the set of documents matching Q:",1,Query,True
"M , {D  C | Q , D  Q}",0,,False
"The goal of the Matching Problem is to identify the match set M, given corpus C and query Q.",0,,False
"In Sections 2.2-2.4 we examine conservative probabilistic algorithms that never miss a match, but might falsely report matches. The goal for these algorithms is to identify a conservative filter set M ",0,,False
"M  M  C where the false positive set F , M  \ M is small.",0,,False
2.1 Inverted Indexes,0,,False
"Perhaps the most common approach to the Matching Problem is the inverted index [4, 11]. This approach maintains a mapping from each term in the lexicon to the the set of documents containing the term. In other words,",0,,False
"Postins(t) , {D  C | t  D}",0,,False
"With this approach, M can be formed by intersecting the posting sets associated with the terms in the query:",0,,False
"M , Postins(t)",0,,False
t Q,0,,False
"In practice, the posting sets are usually sorted, allowing fast intersection. They also draw on a large bag of tricks [4, 20] to compress and decompress posting sets [17, 23] while improving intersection time [6, 7]. This is a rich area with ongoing research into novel data structures such as treaps [16] and semi-bitvectors [13].",0,,False
"Inverted indexes find the exact match set, M, every time. Signaturebased approaches [8­10, 15, 25], on the other hand, use probabilistic algorithms, based on superimposed coding [1, 21, 22] and newer approaches, like TopSig [12] to identify a conservative filter set M . BitFunnel is based on classical bit-sliced signatures which are, in turn, based on bit-string signatures.",0,,False
2.2 Bit-String Signatures,0,,False
"The key idea is that each document in the corpus is represented by its signature. In BitFunnel, the signature is essentially the sequence of bits that make up a Bloom filter representing the set of terms in the document. In constructing the Bloom filter, each term in the document is hashed to a few bit positions, each of which is set to 1.",0,,False
"Let n denote the number of bit positions in the Bloom filter. Define H (n, t) as a function that returns the set of bit positions in the range [0..n) corresponding to the hashes of term t. Define s#»t , the signature of term t, as the bit-vector of length-n where bit position i is set to 1 iff i  H (n, t). We can then define the signature of document D as the logical-or of the signatures of its terms:",0,,False
"s#D» , s#»t",0,,False
t D,0,,False
"In a similar manner, we can define the signature of query Q as the logical-or of the signatures of its terms:",0,,False
"s#Q» , s#»t",0,,False
t Q,0,,False
"Document D is said to be a member of M  when s#Q»  s#D» , s#Q»",0,,False
"Given the signatures of the documents in the corpus, one can easily compute M  by identifying those documents whose signatures match the query's signature:",0,,False
"M  , {D  C | s#Q»  s#D» , s#Q»}",0,,False
Here's the pseudocode to search a corpus for documents matching,0,,False
a query:,0,,False
"M ,",0,,False
for,0,,False
all D if s#D»,0,,False
"s#CQ»d,os#Q»",0,,False
then,0,,False
"M  , M   {D}",0,,False
end if end for,0,,False
"Bit-string signatures are elegant, but their uniform encoding of terms, independent of frequency, leads to poor memory utilization. Section 4.2 explains how BitFunnel uses Frequency Conscious Signatures to improve memory efficiency in signatures.",1,ad,True
2.3 Bit-Sliced Signatures,0,,False
If all of the signatures have the same length and share a common,0,,False
"hashing scheme, H (n, t), one can achieve significant performance",0,,False
"gains by using a bit-sliced arrangement [9, 26, 27]. This approach",0,,False
transposes signature vectors from rows to columns in order to,0,,False
allow multiple documents to be searched simultaneously while,0,,False
eliminating the bit masks and shifting necessary to perform Boolean,0,,False
operations on individual bits.,0,,False
"Suppose we have a corpus C , {A..P } and a query Q. The matrix",0,,False
in Figure 1 shows these documents and the query encoded as bit-,0,,False
sliced signatures. Each document corresponds to a column which,0,,False
holds its 16-bit signature. Each row corresponds to a bit position in,0,,False
the document signature.,0,,False
"In this example the signature for document B has bit positions 2,",0,,False
"5, 9, and 13 set. The signature for the query Q has bit positions 2, 5,",0,,False
"and 9 set. Therefore, document B will match the query. It turns out",0,,False
that document F also matches the query.,0,,False
"With the bit-sliced layout, we need only inspect the rows corre-",0,,False
"sponding to bit positions in Q that are set. These rows, which we",0,,False
"call the query's rows, are shaded in Figure 1 and isolated in Figure",1,ad,True
2. Each bit position in the query's rows corresponds to a document.,0,,False
The document matches if its bit position is set in all of the query's,0,,False
rows. We determine which documents match by intersecting the,0,,False
"query's rows and looking for set bits. In Figure 2, columns B and F",0,,False
are the only columns without zeros. Therefore documents B and F,0,,False
are the only matches.,0,,False
Here's the bit-sliced algorithm:,0,,False
"#a» , 0",0,,False
for,0,,False
"all i #a» ,",0,,False
w#a»h&erre#osw#Q»»i[i],0,,False
",,",0,,False
1,0,,False
do,0,,False
"end for M  , {i | #a»[i] 0}",0,,False
606,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Q,0,,False
00 10 21 30 40 51 60 70 80 91 10 0 11 0 12 0 13 0 14 0 15 0,0,,False
0A B 0C D 0E F 0G H 0I J 0K L M0 N 0O P 0 0 0 01 0 0 01 0 01 0 0 0 0 01 01 0 01 1 01 0 0 0 01 0 0 0 0 01 0 0 0 0 0 01 2 0 01 0 0 0 01 0 0 01 0 0 0 01 0 0 0 3 0 0 0 01 0 0 01 0 0 0 01 0 0 0 01 0 4 0 0 0 0 01 0 0 01 0 01 0 0 0 01 0 0 5 01 01 0 0 0 01 0 01 0 0 01 0 0 0 0 01 6 0 0 01 0 0 0 0 0 0 0 0 0 0 01 0 0 7 01 0 0 01 0 0 01 0 01 0 0 01 0 0 01 0 8 0 0 0 0 0 0 0 01 0 01 0 01 0 0 0 0 9 0 01 0 01 0 01 0 0 0 0 0 0 0 01 0 01 10 0 0 01 0 01 0 0 0 01 0 0 0 01 0 0 0 11 01 0 01 0 0 0 01 0 0 0 0 01 0 0 0 0 12 0 0 0 01 0 0 0 01 01 0 0 0 0 0 01 0 13 0 01 0 0 0 01 0 0 0 0 01 0 0 0 0 0 14 0 0 01 0 0 0 01 0 0 0 0 0 01 01 0 01 15 01 0 0 0 0 0 0 0 01 0 0 01 0 0 0 0,0,,False
"Figure 1: Layout with bit-sliced signatures, in which each column is a document signature. Q is the signature of the query.",0,,False
0A B 0C D 0E F 0G H 0I J 0K L M0 N 0O P 2 0 01 0 0 0 01 0 0 01 0 0 0 01 0 0 0 5 01 01 0 0 0 01 0 01 0 0 01 0 0 0 0 01 9 0 01 0 01 0 01 0 0 0 0 0 0 0 01 0 01,0,,False
2  5  9 0 01 0 0 0 01 0 0 0 0 0 0 0 0 0 0 0A B 0C D 0E F 0G H 0I J 0K L M0 N 0O P,0,,False
"Figure 2: Bit-sliced signature layout. Rows 2, 5, and 9 yield documents B and F .",0,,False
2.4 Bit-Sliced Blocked Signatures,0,,False
"While bit-sliced signatures offer a significant performance advantage over bit-string signatures, they still suffer from poor performance when searching for rare terms. The problem is that every document's bit position must be scanned, even in the case where only a handful of documents actually match.",1,ad,True
"The idea behind blocked signatures [14] is to create shorter rows by assigning multiple documents to each column in the signature matrix. The number of documents that share a column is called the blocking factor. Shorter rows improve performance because they can be scanned more quickly, but they introduce noise which increases the false positive rate.",0,,False
"Prior to BitFunnel, bit-sliced block signatures were used primarily as a single-level index into a set of bit-string signature files on disk. At the time the main concern with this approach was reducing the probability of an unsuccessful block match which occurred when a column signature matched the query but none of the documents contained all the terms in the query. Suppose, for example, a column held two documents, one containing the word ""dog"" and the other containing the word ""cat"". This column would match the query {""do"", ""cat""} even though neither document contains both terms. At least one paper proposed a solution to the problem of unsuccessful block matches [14], however [28] argued that blocking increases",0,,False
"complexity while offering little benefit. In Section 4.1, we introduce Higher Rank Rows to address these problems.",1,ad,True
3 THE BITFUNNEL SYSTEM,0,,False
"For the past 4 years, BitFunnel has powered Bing's fresh index of recently crawled documents. During this time the system, which runs on thousands of machines, spread across several data centers, has processed the entire query load sent to Bing.",1,ad,True
3.1 Architectural Overview,0,,False
"Bing maintains multiple, georeplicated copies of the web index, each of which is sharded across a cluster of BitFunnel nodes. Figure 3 shows a single cluster. Queries are distributed, round robin, across the cluster. A node, upon receiving a query, parses it into an abstract syntax tree, rewrites the tree into an execution plan and then compiles the plan locally before broadcasting the compiled plan to the rest of the cluster. The nodes in the cluster run the compiled plan in parallel, returning results to the planning node for aggregation. These results are then passed on to other systems that score the matching documents and generate captions to display on the search results web page.",1,ad,True
Query,1,Query,True
Parse Plan Compile,0,,False
Execute Execute Execute,0,,False
Execute,0,,False
Aggregate,0,,False
Figure 3: BitFunnel cluster.,0,,False
Rank &,0,,False
Capon,0,,False
3.2 The Cost of False Positives,0,,False
"One criticism specific to the signature file approach is the introduction of false positives into the result set. For scenarios like database queries where the identifying exact match set is the goal, the cost of filtering out the false positives can be prohibitive. In the case of web search, the cost of filtering out false positives is negligible. To see why, it is important to understand that the goal of web search is not to find documents matching Boolean expressions of keywords ­ rather it is to find the documents that best match the user's intent when issuing a query. In Bing, we employ a ranking system that, given a document and a query, will generate a score predicting how well the document matches the user's intent for the query. This system relies on many signals beyond keywords and to some extent its inner workings are opaque to us because it is configured by machine learning.",0,,False
"If we had unlimited resources, we could process each query by submitting every single document in the corpus to our ranking oracle and then return the top-n ranked documents. Since we don't have unlimited resources, we insert inexpensive filters upstream of the oracle to discard documents that the oracle would score low. The filters are designed to reject, with high probability, those",1,ad,True
607,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
documents that score low while never rejecting documents that score high. BitFunnel is such a filter.,0,,False
"In this context, the performance of BitFunnel is judged by its impact on the end-to-end system. BitFunnel wins when its time savings in the Boolean matching phase is greater than the time the oracle spends scoring false positives.",0,,False
We turn our attention now to a single BitFunnel node to describe the techniques that enable fast query processing.,0,,False
4 BITFUNNEL INNOVATIONS,0,,False
"In this section, we describe three innovations that address speed and space problems associated with bit-string and bit-sliced signatures.",1,ad,True
4.1 Higher Rank Rows,0,,False
BitFunnel generalizes the idea of blocking so that each term simultaneously hashes to multiple bit-sliced signatures with different blocking factors. The key to making this approach work is the ability to efficiently intersect rows with different blocking factors.,0,,False
"4.1.1 Mapping Columns Across Ranks. In BitFunnel, we restrict",0,,False
"blocking factors to be powers of 2. We define a concept of row rank, where a row of rank r  0 has a blocking factor of 2r .",0,,False
The BitFunnel blocking scheme is specific to the size of the,0,,False
"machine word used for the bit-slice operations. Let w be the log2 of the number of bits in a machine word, so for example, a 64-bit",0,,False
"processor would have w , 6. Then the document in column i0 at rank 0 will be associated with column ir at rank r as follows:",0,,False
ir,0,,False
",",0,,False
i0 2r +w,0,,False
+ (i0 mod 2r ),0,,False
(1),0,,False
"Figure 4 gives a small example for a 4-bit machine word (w , 2) and",0,,False
"ranks 0, 1, and 2. We can see that position 4 at rank 1 is associated with documents {4, 12} while position 0 at rank 2 is associated with documents {0, 4, 8, 12}.",0,,False
Rank 0 0 01 0 0 0 01 0 0 01 0 0 0 01 0 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Rank 1 01 01 0 0 01 01 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Rank 2 01 01 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Figure 4: Forming higher rank equivalents of a single row.,0,,False
"Note that higher rank rows will, in general, magnify the bit density of their lower rank equivalents. This is because the value of each bit at a higher rank is the logical-or of multiple bits at a lower rank. In order to maintain a constant bit density of d across all signatures in BitFunnel, we must use longer signatures at higher ranks. Therefore, a single row at rank 0 will translate into multiple shorter rows at a higher rank. In most cases, a rank zero row and its higher rank equivalents will consume roughly the same amount of",0,,False
memory. We will derive an expression for the memory consumption in higher rank rows in Section 5.4.,0,,False
"Now suppose we have a query, Q, that maps to the three rows shown in Figure 5. To evaluate the query, we need some way of intersecting rows with different ranks. The mapping in Equation (1) is designed to make this operation easy and efficient.",0,,False
Rank 2 01 0 01 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Rank 1 0 01 0 0 01 0 0 01 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Rank 0 01 0 0 0 01 0 0 01 0 0 0 0 01 0 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Figure 5: Intersecting different rows with different ranks.,0,,False
Logically we convert each row to its rank-0 equivalent by concatenating 2r copies of the row as shown in Figure 6. Then we are free to intersect the rank-0 equivalent rows to produce the result vector.,0,,False
0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15 Rank 2 01 0 01 0 01 0 01 0 01 0 01 0 01 0 01 0 Rank 1 0 01 0 0 01 0 0 01 0 1 0 0 1 0 0 1 Rank 0 01 0 0 0 01 0 0 01 0 0 0 0 01 0 0 0,0,,False
Matches 0 0 0 0 01 0 0 0 0 0 0 0 01 0 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Figure 6: Rank-0 equivalent rows.,0,,False
"4.1.2 Optimizing Higher Rank Row Intersection. At a logical level, our approach is to intersect rank-0 equivalent rows. Were we to generate rank-0 equivalents for intersection, we would lose all of the performance gains that come from scanning shorter rows at higher ranks. Mapping (1) was structured specifically to provide opportunities to reuse intermediate results at every rank. As an example, in Figure 6, bits [0..3] of the rank 2 row need only be read once, even though they will be used for positions [4..7], [8..11], and [12..15]. Similarly, the intersection of the first two rows in positions [0..3] will be computed once and used again for positions [8..11]. We will leverage this insight in Section 5.3 where we derive an expression for the expected number of operations required to combine a set of rows with different ranks.",1,ad,True
"In BitFunnel, each term in a query maps to a set of rows that may include higher rank rows.",0,,False
4.2 Frequency Conscious Signatures,0,,False
We saw in Section 2.2 how Bloom filter signatures can be used to encode the set of terms in a document. One shortcoming with this,0,,False
608,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
approach is inefficient memory usage when terms in the lexicon,0,,False
have widely varying frequencies in the corpus.,0,,False
"The problem stems from the fact that, in its classical formulation [1], the Bloom filter is configured with an integer constant, k, which represents the number of hashes for each term1. This value of k is the same for all terms in lexicon L. In other words",0,,False
"|H (n, t)| ,"" k, t L""",0,,False
"To get an intuition for the problem of the one-size-fits-all k, it",0,,False
"helps to think of the quantity of false positives in terms of signalto-noise ratio. Let's consider a single set membership test for t  D. In the context of the test, define the signal s to be the probability that term t is actually a member of document D. This is just the frequency of t in the corpus.",0,,False
Define noise  to be the probability that the Bloom filter will incorrectly report t as a member. Assume the Bloom filter has been configured to have an average bit density of d. Since d is the fraction,0,,False
"of the bits expected to be set, we can treat it as the probability that a random bit is set. A set membership test involves probing k bit positions. If all k probes find bits that are set to one, the algorithm",0,,False
will report a match. Therefore the noise is just the probability that k probes all hit ones when t D:,0,,False
" , (1 - s)dk",0,,False
The signal-to-noise ratio  is then,0,,False
",",0,,False
(1,0,,False
s - s)dk,0,,False
"We can rearrange this and take the ceiling to get an expression for k as a function of d, s, and :",0,,False
"k,",0,,False
lod,0,,False
s (1 - s),0,,False
This is the minimum value of k that will ensure a signal-to-noise ratio of at least . The main take away is that k increases as s,0,,False
"decreases. In other words, rare terms require more hashes to ensure a given signal-to-noise level. The following table shows values of k without the ceiling, for select values of s when d , 0.1 and  , 10:",0,,False
signal (s) 0.1 0.01 0.001 0.0001 0.00001,0,,False
hashes (k) 1.954242509 2.995635195 3.999565488 4.999956568 5.999995657,0,,False
"Now consider a Bloom filter that stores a typical document from the Gov2 corpus2. If we were to configure the Bloom filter with k ,"" 2 we could just barely maintain a signal-to-noise ratio of 10 when testing for the term """"picture"""" which appears with frequency 0.1. To test for the term """"rotisserie"""", which appears with frequency 0.0001, we would need k "", 5 to drive the noise down to a tenth of the signal.",1,Gov,True
"With classical Bloom filters, one must configure for the rarest term in the lexicon, even though the vast majority of common terms could be stored more efficiently. Recent work in Weighted Bloom",0,,False
1In Bloom's original paper [1] this constant was the letter d ; more contemporary,0,,False
descriptions [3] use the letter k . 2Term frequencies are from Corpus D described in Section 6.,0,,False
Filters [3] shows that it is possible to adjust the number of hash functions on a term-by-term basis within the same Bloom filter.,1,ad,True
BitFunnel applies these ideas to reduce memory usage and determine the number of rows needed for each term.,0,,False
4.3 Sharding by Document Length,0,,False
"Bit-sliced signatures have another one-size-fits-all problem resulting from the requirement that all of the document signatures have the same configuration (i.e. their bit lengths, n, must all be the same, and they must all use the same hashing scheme H (n, t)).",0,,False
"The problem is that real world documents vary greatly in length. In Wikipedia, for example, the shortest documents have just a handful of unique terms while the longest ones may have many thousands of terms. The dynamic range of document lengths on the internet is even higher because of files containing DNA sequences, phone numbers, and GUIDs. To avoid overfilling our Bloom filters and generating excessive false positives, it is necessary to configure the Bloom filters for the longest document expected, even if such a document is very rare. Unfortunately, such a configuration would waste enough memory as to offset all of the other benefits of the bit-sliced arrangement.",1,Wiki,True
"A workaround [28] suggested in the late 90s was to shard the index into pieces containing documents with similar lengths. This approach was rejected at the time because, on a single machine, the introduction of length sharding would multiply the number of disk seeks by the number of shards.",0,,False
"This concern is not a factor when the index is many times larger than the capacity of a single machine. As soon as the index is sharded across a large cluster, one must pay for the overhead of sharding. At this point sharding by document length costs the same as sharding by any arbitrary factor.",1,ad,True
"Even on a single machine, the cost of length sharding is greatly reduced on modern hardware where the index can be stored in RAM or on SSD because the access cost is dominated by fixed-sized block transfers (512-bit cache line for RAM, 4096 byte block for SSD), rather than hard disk seeks.",0,,False
"In BitFunnel, we partition the corpus according to the number of unique terms in each document such that each instance of BitFunnel manages a shard in which documents have similar sizes.",0,,False
5 PERFORMANCE MODEL AND OPTIMIZATION,0,,False
"Signature-based approaches have historically been hard to configure because of a large number of design parameters that impact performance [10, 26, 28]. In this section we present an algorithm that optimizes the BitFunnel configuration, given a desired signalto-noise ratio. The algorithm performs a constrained optimization, over relevant configuration parameters, of a cost function that expresses the system efficiency as DQ, the product of the corpus size D and query processing rate Q. The configuration parameters include the mapping from terms with various frequencies to their corresponding number of rows at each rank. The constraint is a lower limit on the allowable signal-to-noise ratio, .",0,,False
"In order to develop the cost function and constraint, we derive expressions for the signal-to-noise ratio, query processing speed, and memory consumption in BitFunnel. We then combine these",0,,False
609,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"expressions into a cost function and constraint used by the algorithm that identifies an optimized set of blocking factors and hash functions for each equivalence class of terms, based on frequency in the lexicon.",0,,False
5.1 Prerequisites,0,,False
"Before deriving these fundamental equations, we discuss the impact of row rank on bit densities and noise. We then characterize two different components of noise in rank-0 equivalent rows. This will form the basis for the noise, speed, and storage equations in Sections 5.2, 5.3, and 5.4.",0,,False
5.1.1 Signal in a Higher Rank Row. Because each bit in a higher,0,,False
"rank row corresponds to multiple documents, the bit density con-",0,,False
tributed by a single term will nearly always be greater in higher,0,,False
rank rows. We can see this in Figure 4 where densities in the rank-0,0,,False
row,0,,False
and,0,,False
its,0,,False
rank,0,,False
1,0,,False
equivalent,0,,False
are,0,,False
4 16,0,,False
and,0,,False
8 16,0,,False
",",0,,False
respectively.,0,,False
Let s0 denote the signal in a rank-0 row and sr denote the signal,0,,False
at rank r . We can express sr as a function of s0 and r . The probability,0,,False
that a bit at rank r is set due to signal is the probability that at least,0,,False
one of the 2r corresponding rank-0 bits is signal. This is just one,0,,False
minus the probability that all of the 2r rank-0 bits are zero:,0,,False
"sr , 1 - (1 - s0)2r",0,,False
(2),0,,False
5.1.2 Noise in a Rank-0 Equivalent Row. Processing a query in,0,,False
BitFunnel is logically equivalent to intersecting the rank-0 equiva-,0,,False
lents of each row associated with the query. Converting a rank-r,0,,False
row to its rank-0 equivalent increases noise. The intuition behind,0,,False
this is simple -- each bit set in a rank-r row means that at least one of 2r documents is a match. It could be one document or all 2r --,0,,False
we can't tell and this is the source of higher noise.,0,,False
Let's look at a simple example. Suppose we have a corpus of,0,,False
16 documents and would like to search for a term that happens,0,,False
to appear in documents 4 and 8. We hash our term to find its,0,,False
"corresponding rows, and we get the set of rows R ,"" {R2, R1, R0} with ranks 2, 1, and 0, respectively. We define the signal, s0 as the""",0,,False
fraction of the bit positions at rank-0 corresponding to a match. In,0,,False
"the case of a term that appears in only 2 documents, s0",0,,False
",",0,,False
2 16,0,,False
.,0,,False
In,0,,False
"Figure 7, the green squares labeled 'S' correspond to the signal.",0,,False
R2 0S 0 0N 0 R1 0S 0N 0 0 0S 0 0 0N R0 0 0N 0N 0 0S 0 0N 0 0S 0N 0 0 0N 0 0N 0,0,,False
S0 0 0 0 0 0S 0 0 0 0S 0 0 0 0 0 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
"Figure 7: A term maps to three rows with different ranks. Since a row is shared with other terms, it contains signal and noise bits but has constant bit density.",0,,False
R2,0,,False
"has one signal bit, so its signal is",0,,False
1 4,0,,False
",",0,,False
4 16,0,,False
.,0,,False
We've,0,,False
arbitrarily,0,,False
"added one noise bit, marked with an 'N' and shaded black. This bit",1,ad,True
is contributed from another term that also maps to R2. The density,0,,False
of R2,0,,False
is,0,,False
2 4,0,,False
",",0,,False
8 16,0,,False
.,0,,False
Row R1 has two signal bits and two arbitrary noise bits so its,0,,False
signal is,0,,False
2 8,0,,False
",",0,,False
4 16,0,,False
and its density is,0,,False
4 8,0,,False
",",0,,False
8 16,0,,False
.,0,,False
"Finally, in row R0, the signal is equal to s0 because each signal",0,,False
"bit maps directly to a single document. As with the other rows, R0",0,,False
contains,0,,False
random,0,,False
noise,0,,False
bits,0,,False
from,0,,False
other,0,,False
"terms,",0,,False
yielding,0,,False
8 16,0,,False
density.,0,,False
"To process our query, we intersect the rank-0 equivalents of",0,,False
rows R2 and R1 with R0. Figure 8 shows how the process of creating,0,,False
rank-0 equivalents increases noise.,0,,False
0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15 R2 0C 0 0U 0 0S 0 0U 0 S0 0 0U 0 0C 0 0U 0 R1 0C 0U 0 0 0S 0 0 0U S0 U 0 0 C 0 0U R0 0 0U 0U 0 0S 0 0U 0 S0 0U 0 0 0U 0 0U 0,0,,False
Results 0 0 0 0 0S 0 0 0 0S 0 0 0 0U 0 0 0 0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15,0,,False
Figure 8: Noise in rank-0 equivalent rows.,0,,False
"Continuing with our example, the signal bit from position 0 in",0,,False
"R2 maps to bit positions 0, 4, 8, and 12 at rank 0. Of these four",0,,False
"positions, only positions 4 and 8 correspond to signal bits. The",0,,False
others are noise bits introduced by the construction of the rank-0,0,,False
"equivalent, and they are colored yellow and marked with the letter",0,,False
"'C'. In a similar manner, R2 bit position 2 introduces noise in rank-0",0,,False
"positions 2, 6, 10, and 14. These bits are colored black and marked",0,,False
"with the letter 'U'. In the case of R2, we went from a rank-2 row",0,,False
with,0,,False
1 4,0,,False
signal,0,,False
and,0,,False
1 4,0,,False
noise,0,,False
to,0,,False
a,0,,False
rank-0,0,,False
row,0,,False
with,0,,False
2 16,0,,False
signal,0,,False
and,0,,False
6 16,0,,False
noise. The noise increase is entirely due to the signal bits in R2. In,0,,False
"contrast, the noise bits in R2 contribute their same density without",0,,False
"amplification, and therefore do not increase noise in the rank-0",0,,False
equivalent row.,0,,False
Now let's look at the rank-0 equivalent of R1. We go from a rank-,0,,False
1 row,0,,False
with,0,,False
2 8,0,,False
signal,0,,False
and,0,,False
2 8,0,,False
noise,0,,False
to a rank-0,0,,False
row,0,,False
with,0,,False
2 16,0,,False
signal,0,,False
and,0,,False
6 16,0,,False
noise.,0,,False
As,0,,False
with,0,,False
"R2,",0,,False
the,0,,False
noise,0,,False
increase,0,,False
is,0,,False
due,0,,False
entirely,0,,False
to,0,,False
signal,0,,False
in,0,,False
rank-1 row.,0,,False
"5.1.3 Correlated and Uncorrelated Noise. We turn to computing the noise resulting from the intersection of a set of the rows. The noise in any rank-0 row is the difference between the row's density and the signal s0. If row R has density d, then its rank-0 equivalent has density d because it consists of the concatenation of 2r copies of the R. Therefore, the noise in R's rank-0 equivalent is d - s0.",0,,False
"Noise is made up of two components, one which is correlated and one which is not. In Figure 8, uncorrelated noise bits are shaded black while correlated noise bits are colored yellow. Row intersections are very effective at reducing uncorrelated noise, but they have less impact on correlated noise.",1,ad,True
"To better illustrate this, let's look at a simple, but extreme example. Suppose our query matches documents 2 and 13 and consists of the three rank-1 rows depicted in Figure 9. In the rank-0 equivalent",0,,False
Ra 0 0 0S 0 0 0S 0N 0 Rb 0 0N 0S 0 0 0S 0 0 Rc 0 0 0S 0N 0 0S 0 0,0,,False
Figure 9: Three rank-2 rows.,0,,False
"rows, shown in Figure 10, noise has two components: correlated and",0,,False
610,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
0 1 02 3 04 5 06 7 08 9 100 11 102 13 104 15 R2 0 0 0S 0 0 0C 0U 0 0 0 0C 0 0 S0 0U 0 R1 0 0U 0S 0 0 0C 0 0 0 U 0C 0 0 S0 0 0 R0 0 0 0S 0U 0 0C 0 0 0 0 0C N0U 0 S0 0 0,0,,False
RaRbRc 0 0 0S 0 0 0C 0 0 0 0 0C 0 0 0S 0 0,0,,False
Figure 10: Correlated and uncorrelated noise in rank-0 equivalent rows.,0,,False
"uncorrelated. The uncorrelated noise, shown in black and marked",0,,False
"with the letter 'U', is completely eliminated in three row intersec-",0,,False
"tions, but the correlated noise, shown in yellow and marked with",0,,False
the letter 'C' remains at the same level despite the intersections.,0,,False
Effectively managing the impact of higher rank rows requires,0,,False
an understanding of the correlated noise in rank-0 equivalent rows.,0,,False
In the following we derive expressions for noise components. Let nr denote noise in a rank-r row R and n0 denote noise in,0,,False
"its rank-0 equivalent. We express n0 as a function of r , s0, and nr . Row R will contribute nr density due to noise already in R and sr in density due to signal in R. A portion of the density in sr corresponds to bonified signal. The remaining density is correlated",1,ad,True
noise introduced by the conversion to rank-0. Thus we compute noise at rank-0 by subtracting s0 from the density contributed by R:,0,,False
"n0 , nr + sr - s0",0,,False
"To compute the correlated noise, we subtract nr from n0 and substitute sr , 1 - (1 - s0)2r :",0,,False
"n0 - nr , sr - s0 , 1 - (1 - s0)2r - s0",0,,False
(3),0,,False
"Note that the number of correlated noise bits in a rank-0 equivalent is a function of the original rank. The higher the row rank, the greather the contribution in correlated noise to its rank-0 equivalent. Also, correlated noise remaining after intersecting a set of rank-0 equivalents is the correlated noise of the lowest rank row in the set. The other correlated noise is converted to uncorrelated noise.",0,,False
It is important to note that the correlated noise bits in a lower rank equivalent always form a subset of the correlated noise bits in a higher rank equivalent. Our equations for noise and speed in Sections 5.2 and 5.3 make use of this fact.,0,,False
5.2 Signal-to-Noise Ratio,0,,False
"We're now ready to write expressions for the noise components after a sequence of row intersections. For this derivation, we will perform the intersections in order from high rank to low rank. We will start with an accumulator, a, which has an initial bit density of 1.0 and then intersect in each row in turn.",1,ad,True
"Let ai denote the total noise in the accumulator at the end of iteration i. Let ci and ui denote the amount of correlated and uncorrelated noise, respectively, on iteration i and let ri denote the rank. The first iteration is effectively loading the first row into the accumulator so, u1 ,"" n1. The correlated noise in the accumulator is always equal to the correlated noise in the last row intersected, so""",1,ad,True
"ci , 1 - (1 - s0)2ri - s0",0,,False
"Since the rows are ordered by non-increasing rank, subsequent rows will never have more correlated noise. In the case where the rank decreases, the amount of correlated noise will decrease. When this happens, some of the correlated noise in the accumulator will become uncorrelated noise, moving forward. This new amount of uncorrelated noise in the accumulator will then be multiplied by the current row's total noise density ni+1:",0,,False
"ui+1 , (ui + ci - ci+1)ni+1",0,,False
"At any given point, the total accumulator noise ai is just the sum of the correlated and uncorrelated noise:",0,,False
"ai , ci + ui",0,,False
"The signal-to-noise ratio, , on iteration i is then",0,,False
i,0,,False
",",0,,False
s0 ai,0,,False
",",0,,False
s0 ci + ui,0,,False
(4),0,,False
5.3 Query Execution Time,1,Query,True
"When modelling running time, we use the number of machine word accesses of unique memory addresses as our proxy for time. On a real computer, row intersections are typically performed in chunks that match the machine register size. As an example, if the machine register size is 64 bits, and the rank-0 rows are 256 bits long, a pairwise row intersection would require 4 register-sized logical-and operations. When intersecting a set of rows, the outer loop is typically over the register-sized chunks in each row and the inner loop is over the set of rows.",1,ad,True
"This ordering of the loops is desirable because intermediate results of row intersections can reside in the accumulator instead of being written to memory. In many cases, the accumulator will become zero in the inner loop before all of the rows have been examined. Since additional intersections cannot change the result, it is possible to break out of the inner loop at this point.",1,ad,True
"In practice, breaking out of the inner loop offers a significant performance improvement. To quantify this impact, we'll focus on the innermost loop which intersects a set of n machine words that reside in memory. Our goal is to write an expression for the expected number of machine words loaded from memory.",1,ad,True
"If we know the probability that a bit remains set after intersecting the first n rows, we can derive a formula for the expected number of machine words accessed when intersecting a set of rows.",0,,False
Let N be a random variable denoting the machine words intersected and define PBZ (N > i) to be the probability that a random bit in the accumulator is zero after iteration i. PBZ (N > i) is the probability that the bit was not set by noise and not set by signal:,0,,False
"PBZ (N > i) , 1 - s0 - ai",0,,False
Define PA(N > i) to be the probability that at least one bit in the accumulator remains set after i intersections If b denotes the number of bits in a machine word then,0,,False
"PA(N > i) , 1 - (PBZ (N > i))b",0,,False
If we were to actually perform intersections on the rank-0 equiv-,0,,False
"alent rows, the expected number of machine words accessed during",0,,False
one iteration of the outer loop would be,0,,False
n,0,,False
n,0,,False
"E(N ) , PA(N > i) , 1 - (1 - s0 - ai )b",0,,False
"i ,1",0,,False
"i ,1",0,,False
611,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"As we saw in Section 4.1.2, the mapping of columns across ranks",0,,False
is structured in such a way that intermediate results from higher,0,,False
"rank intersections can be reused. Since each rank-0 equivalent is just the concatenation of 2r copies of a rank-r original, we need only load the accumulator once for each of the 2r machine word",1,ad,True
positions in the rank-0 equivalent. This reduces the number of machine words accessed in each row by a factor of 2ri :,0,,False
E(N ),0,,False
",",0,,False
"n i ,1",0,,False
1,0,,False
-,0,,False
(1,0,,False
- s0 2ri,0,,False
-,0,,False
ai )b,0,,False
(5),0,,False
"A similar approach can be used to model block devices like CPU cache and SSD block transfers, but it is somewhat more involved than substituting a different value for b.",0,,False
5.4 Space Consumption,0,,False
We express memory consumption as the number of bits per docu-,0,,False
"ment required to store a term. Suppose we have a corpus, C, with",0,,False
"target bit density, d, and we wish to store a term with signal, s0, in some row, q, that has rank r .",0,,False
"Since the corpus has |C| documents, row q must have |C|2-r bit",0,,False
"positions. Equation (2) shows that a term with frequency s0 will set sr of these bits. Therefore the term contributes b1 , sr |C|2-r set bits to row q. Let b0 denote the number of zero bits in row q. By",0,,False
"definition,",0,,False
d,0,,False
",",0,,False
b1 b1 + b0,0,,False
"Rearranging, we get",0,,False
b0,0,,False
",",0,,False
b1 d,0,,False
- b1,0,,False
"Therefore, the total number of bits required in row q to maintain density of d with a signal of s0 is",0,,False
b0,0,,False
+ b1,0,,False
",",0,,False
b1 d,0,,False
",",0,,False
sr |C| d 2r,0,,False
Dividing by the corpus size |C| gives the number of bits per docu-,0,,False
ment signature: sr d 2r,0,,False
"For a set of rows, Q, the total memory consumption per document",0,,False
is therefore,0,,False
sr (q) q Q d2r,0,,False
(6),0,,False
5.5 Choosing Term Configurations,0,,False
"Given expressions for signal-to-noise ratio, machine word reads, and storage consumed, we can now develop an approach for identifying the optimal row configuration for each term. The problem is a constrained optimization over a cost function parameterized by speed and space. Our constraint is that the signal-to-noise ratio, , must exceed some fixed threshold. The cost function is proportional to DQ, the product of the number of documents per unit storage and the number of queries processed per unit of compute.",1,ad,True
D is inversely proportional to the amount of storage required per document. Q is inversely proportional to the number of machine,0,,False
words accessed while processing a query. Therefore,0,,False
DQ ,0,,False
1,0,,False
(7),0,,False
n 1-(1-s0-ai )b,0,,False
"i ,1",0,,False
2ri,0,,False
sr (q) q Q d,0,,False
"Given the small number of possible row configurations, it is easy",0,,False
"to enumerate all configurations and choose the one with the highest DQ where  exceeds the signal-to-noise threshold. For example,",0,,False
"when considering configurations of 0 to 9 rows at each of seven ranks from 0 to 6, we need to examine 107 configurations for each s0 value. If we group s0 values into, say, 100 buckets correspondiong to IDF values from 0.1 to 10.0 in 0.1 increments, the entire optimization involves 109 evaluations of Equation 7. A modern multi-core",0,,False
processor can perform this optimization in a matter of seconds.,0,,False
6 EXPERIMENATAL EVALUATION,0,,False
"Our experiments are based on the TREC Gov2 corpus. Apache Tikka3 was used to extract terms, which were then converted to lower case, but not stemmed. Since BitFunnel shards its index by document term count, we selected five representative shards for our tests. Shard A has relatively short documents with term counts ranging from 64 to 127. Shards B, C, D and E have progressively larger documents.",1,TREC,True
Min terms Max terms Documents (M) Total terms (M) Postings (M) Matches/query Input text (GB),0,,False
Table 1: Corpora. ABC 64 128 256,0,,False
127 255 511 5.870 7.545 3.726 4.181 6.524 6.647,0,,False
"563 1,411 1,268 1,115 3,561 5,124 6.85 25.48 21.02",0,,False
"D 1,024 2,047 0.494 10.109",0,,False
"687 3,728 22.89",0,,False
"E 2,048 4,095 0.157 9.697",0,,False
"432 3,688 20.26",0,,False
Our query log is based the TREC 2006 Efficiency Topics. We removed punctuations from each query and then filtered out those queries that contained terms not in the corpus.4 The resulting query log contains about 98k queries.,1,TREC,True
"BitFunnel was implemented in C++14 and compiled with GCC 5.4.1 with the highest optimization level. Experiments were performed on a 4.0GHz 4-core i7-6700 with 32GB of 3.2GHz DDR4 RAM with Ubuntu 14.04 LTS on Windows Subsystem for Linux. BitFunnel was configured with lower bound signal-to-noise ratio  , 10.",0,,False
The source code to replicate our experiments is available at http://bitfunnel.org/sigir2017.,0,,False
6.1 Match Time vs. Quadwords,1,ad,True
"In Section 5.3 we developed a model for the number of machine words of row data accessed while processing a query. To verify that our model has predictive power, we examined the relationship between row intersection time and the number of quadwords accessed. Since BitFunnel has a significant per-match overhead that",1,ad,True
3 https://tika.apache.org/ 4This filtering was necessary because the Partitioned Elias-Fano index we used requires all query terms be in the index.,0,,False
612,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Median Intersection Time (µs),0,,False
200,0,,False
150,0,,False
IDF,0,,False
3,0,,False
100,0,,False
4,0,,False
5 50,0,,False
0,0,,False
0,0,,False
20000,0,,False
40000,0,,False
60000,0,,False
Quadwords Accessed,1,ad,True
Figure 11: Intersection time increases with quadwords.,1,ad,True
"is not part of the row intersection cost model, we modified the code to perform row intersections, but not report matches. A sample of 5000 queries with I DF > 3 were chosen, at random, from our TREC query log, and these queries were run, single-threaded, against corpus D. To control for system variances not in the model, we ran each query 10 times and recorded the median row intersection time. The scatterplot in Figure 11 shows that row intersection time tends to grow as the number of quadwords increases. The correlation is more pronounced at higher IDF values.",1,TREC,True
6.2 Impact of Frequency Conscious Signatures and Higher Rank Rows,0,,False
"This experiment compares the time and space characteristics of (a) bit-sliced signatures configured with classical Bloom filters (BSS); (b) the same, but with Frequency Conscious Signatures as described in Section 4.2 (BSS-FC); and (c) Higher Ranked Rows as described in Section 4.1 and optimized per Section 5.5 (BTFNL).5",0,,False
"Table 2 examines Corpus D, comparing the three configurations at each of 5 bit densities. The DQ values measure overall system efficiency, expressed as the ratio of QPS to Bits/Posting. We use DQ because it is inversely proportional to the number of servers required, given a particular corpus and a desired QPS",0,,False
Table 2: Impact of BitFunnel Innovations.,0,,False
Treatment Density Bits/Posting kQPS DQ,0,,False
BSS,0,,False
0.05,0,,False
80.0,0,,False
14.0 175,0,,False
BSS,0,,False
0.10,0,,False
50.0,0,,False
11.3 225,0,,False
BSS,0,,False
0.15,0,,False
46.7,0,,False
9.1 194,0,,False
BSS,0,,False
0.20,0,,False
40.0,0,,False
8.2 204,0,,False
BSS,0,,False
0.25,0,,False
36.0,0,,False
6.9 191,0,,False
BSS-FC 0.05,0,,False
23.4,0,,False
"29.5 1,263",0,,False
BSS-FC 0.10,0,,False
16.8,0,,False
"25.5 1,515",0,,False
BSS-FC 0.15,0,,False
14.7,0,,False
"24.0 1,632",0,,False
BSS-FC 0.20,0,,False
13.1,0,,False
"21.4 1,634",0,,False
BSS-FC 0.25,0,,False
12.6,0,,False
"19.4 1,547",0,,False
BTFNL 0.05,0,,False
22.1,0,,False
"65.2 2,954",0,,False
BTFNL 0.10,0,,False
16.0,0,,False
"57.7 3,595",0,,False
BTFNL 0.15,0,,False
13.7,0,,False
"57.0 4,163",0,,False
BTFNL 0.20,0,,False
12.5,0,,False
"46.7 3,746",0,,False
BTFNL 0.25,0,,False
11.9,0,,False
"41.6 3,510",0,,False
"5The BSS Bloom filter targeted  , 0.1 for terms with IDF 4. The BSS-FC and BTFNL configurations set  ,"" 0.1 for all terms, regardless of frequency.""",0,,False
Frequency consciousness reduces storage consumption while,0,,False
"increasing speed. For example, at d ,"" 0.15, the BSS configuration""",0,,False
uses 46.7 bits per posting while the BSS-FC configuration uses only,0,,False
14.7. This 3.2x reduction in storage is achieved while yielding a,0,,False
2.6x increase in speed. The intuition behind the improvement is,0,,False
that frequency consciousness allows each term to have the right,0,,False
"number of rows. With classical Bloom filters, every term has the",0,,False
"same number of rows, meaning that more common terms get excess",0,,False
rows as a side effect of providing sufficient rows to ensure the target,0,,False
signal-to-noise level for rare terms.,0,,False
"Higher Rank Rows mainly impact speed. For example, when",0,,False
"d ,"" 0.15, BSS-FC runs at 24K queries per second, while BTFNL""",0,,False
"runs at 57.0K, a 2.4x improvement. The intuition behind the speed",0,,False
up is that higher rank rows can be scanned more quickly than,0,,False
"rank-0 rows. Generally speaking, processing a rank-r row involves",0,,False
scanning,0,,False
1 2r,0,,False
of the quadwords necessary to process a rank-0 row.,1,ad,True
The DQ column captures the tradeoff between space and speed.,1,ad,True
"BSS-FC has a DQ of 1,632, while BTFNL has a DQ of 4,163, a 2.6x",0,,False
improvement. Combining frequency consciousness with higher,0,,False
rank rows yields a 21x improvement over that BSS DQ of 194.,0,,False
"We found that a density of 0.15 yielded the best DQ for Corpora B,",0,,False
"C, and D, while A and E performed best at 0.05 and 0.20, respectively.",0,,False
6.3 Comparison with Contemporary Indexes,0,,False
"The version of BitFunnel used by Bing includes a forward index with term frequencies used for BM25F ranking. Because this ranking code was not available to us at the time we designed our experiment, we limited our comparison to conjunctive boolean matching.",0,,False
"Our primary comparison system was Partitioned Elias-Fano or PEF[23]. This system is considered state-of-the-art, has excellent performance, and, like BitFunnel, is implemented in C++. We also compared with MG4J's Java implementation of PEF6. This implementation was the second fastest system in the SIGIR 2015 RIGOR workshop[18]. Our final comparison was with Lucene7, a popular Java-based search engine that outperformed MG4J at the RIGOR workshop, in an apples-to-apples comparison using BM25F.",0,,False
"Each of these systems was configured to use a memory-mapped index that was non-positional, with scoring disabled. In this configuration, PEF and MG4J pay no runtime penalty associated with term frequencies because the frequencies are stored in a separate data structure that is never consulted. It is unclear whether Lucene pays a cost associated with stepping past term frequency values.",0,,False
"For each system we used 8 threads to process the entire 98k query log twice, back-to-back, measuring performance on the second pass. This ensured that relevant portions of the index were paged in, as they would be under continuous production load.",1,ad,True
"We can see from Table 3 that BitFunnel is faster than PEF in all cases, but sometimes this comes at a significant cost, for example in Corpus A, BitFunnel uses 5x as many bits per posting while yielding a false positive rate of 1.62%. Across the 5 corpora, MG4J is slower than PEF, as expected since it implements the same algorithm, but in Java. MG4J is faster than Lucene in all but Corpus C.",1,corpora,True
"BitFunnel's overall performance relative to PEF improves as document lengths increase. It first surpasses PEF in Corpus C, where it",0,,False
6 http://mg4j.di.unimi.it/ 7 https://lucene.apache.org/,0,,False
613,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"shows 3.2x the QPS of PEF while using only 2.6x the space. Examining DQ, the ratio of QPS to bits-per-posting, we see that BitFunnel outperforms PEF by factors of 1.3, 3.1, and 4.2 in Corpora C, D, and E, respectively, while PEF outperforms BitFunnel by factors of 3.4 and 1.6 in Corpora A and B.",0,,False
Table 3: Query Processing Performance.,1,Query,True
BitFunnel PEF MG4J Lucene,0,,False
QPS,0,,False
"21,427 14,675 6,866 6,310",0,,False
False positives (%),0,,False
1.62 0.00 0.00 0.00,0,,False
A Bits per posting,0,,False
38.43 7.64 7.85,0,,False
­,0,,False
DQ,0,,False
"558 1,921 875",0,,False
­,0,,False
QPS,0,,False
"8,674 5,049 3,636 3,011",0,,False
False positives (%),0,,False
4.32 0.00 0.00 0.00,0,,False
B Bits per posting,0,,False
20.72 7.33 7.59,0,,False
­,0,,False
DQ,0,,False
419 689 479,0,,False
­,0,,False
QPS,0,,False
"12,722 3,959 3,096 4,120",0,,False
False positives (%),0,,False
3.88 0.00 0.00 0.00,0,,False
C Bits per posting,0,,False
16.91 6.63 6.88,0,,False
­,0,,False
DQ,0,,False
752 598 450,0,,False
­,0,,False
QPS,0,,False
"57,014 8,268 5,900 3,632",0,,False
False positives (%),0,,False
2.43 0.00 0.00 0.00,0,,False
D Bits per posting,0,,False
13.69 6.25 6.28,0,,False
­,0,,False
DQ,0,,False
"4,163 1,322 939",0,,False
­,0,,False
QPS,0,,False
"105,782 13,151 7,349 4,991",0,,False
False positives (%),0,,False
2.64 0.00 0.00 0.00,0,,False
E Bits per posting,0,,False
11.69 6.15 6.15,0,,False
­,0,,False
DQ,0,,False
"9,047 2,139 1,195",0,,False
­,0,,False
"These results are consistent with the interpretation that the biggest factor in BitFunnel performance is row length, which is directly proportional to the number of documents in the corpus. As document lengths increase and the corpus size drops, BitFunnel performance improves relative to PEF.",0,,False
"It is unclear from these results, the extent to which BitFunnel's performance gains are the result of a careful implementation versus actual algorithmic gains. We can see from PEF vs MG4J that choice of implementation language can have a significant impact on performance. Since BitFunnel compiles each query into x64 machine code, it is likely that some of BitFunnel's gains come from highly optimized query code.",0,,False
7 CONCLUSION,0,,False
"This work revisits bit-sliced signatures and describes their use in a commercial search engine, which previously used inverted files. Signature-based approaches introduce several challenges and we develop a set of techniques to reduce the memory footprint and to process queries quickly. Furthermore, we derive a performance model that allows expressing the system configuration as an optimization problem. We evaluate the key techniques behind BitFunnel experimentally, and we provide the source code publicly to accelerate advances in this area.",1,ad,True
8 ACKNOWLEDGMENTS,0,,False
"We thank the following colleagues for their contributions to BitFunnel: Andrija Antonijevic, Tanj Bennett, Denis Deyneko, Utkarsh",0,,False
"Jain, and Fan Wang. We also thank the anonymous reviewers for",0,,False
their feedback which led to an improved experimental section.,0,,False
REFERENCES,0,,False
"[1] Burton H Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM 13, 7 (1970), 422­426.",1,ad,True
"[2] Sergey Brin and Lawrence Page. 1998. The Anatomy of a Large-Scale Hypertextual Web Search Engine. Computer Networks 30, 1-7 (1998), 107­117.",0,,False
"[3] Jehoshua Bruck, Jie Gao, and Anxiao Jiang. 2006. Weighted bloom filter. In 2006 IEEE International Symposium on Information Theory. IEEE.",0,,False
"[4] Stefan Büttcher, Charles LA Clarke, and Gordon V Cormack. 2016. Information retrieval: Implementing and evaluating search engines. Mit Press.",0,,False
[5] Berkant Barla Cambazoglu and Ricardo A. Baeza-Yates. 2015. Scalability Challenges in Web Search Engines. Morgan & Claypool Publishers.,0,,False
"[6] J Shane Culpepper and Alistair Moffat. 2010. Efficient set intersection for inverted indexing. ACM Transactions on Information Systems (TOIS) 29, 1 (2010), 1.",0,,False
"[7] Bolin Ding and Arnd Christian König. 2011. Fast set intersection in memory. Proceedings of the VLDB Endowment 4, 4 (2011), 255­266.",0,,False
"[8] Chris Faloutsos. 1985. Access methods for text. ACM Computing Surveys (CSUR) 17, 1 (1985), 49­74.",0,,False
"[9] Christos Faloutsos. 1992. Information retrieval: data structures and algorithms. Prentice Hall PTR, 44­65.",0,,False
[10] Christos Faloutsos and Stavros Christodoulakis. 1985. Design of a Signature File Method that Accounts for Non-Uniform Occurrence and Query Frequencies.. In VLDB. 165­170.,1,Query,True
"[11] Edward Fox, Donna Harman, w. Lee, and Ricardo Baeza-Yates. 1992. Information retrieval: data structures and algorithms. Prentice Hall PTR, 28­43.",0,,False
"[12] Shlomo Geva and Christopher M De Vries. 2011. Topsig: Topology preserving document signatures. In Proceedings of the 20th ACM international conference on Information and knowledge management. ACM, 333­338.",0,,False
"[13] Andrew Kane and Frank Wm Tompa. 2014. Skewed partial bitvectors for list intersection. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 263­272.",0,,False
"[14] A Kent, Ron Sacks-Davis, and Kotagiri Ramamohanarao. 1990. A signature file scheme based on multiple organizations for indexing very large text databases. Journal of the American Society for Information Science 41, 7 (1990), 508.",0,,False
"[15] Donald E Knuth. 1998. The Art of Computer Programming, Vol. 3, Sorting and Searching (2nd ed.). Vol. 3. Addison-Wesley, 567­573.",0,,False
"[16] Roberto Konow, Gonzalo Navarro, Charles LA Clarke, and Alejandro López-Ortíz. 2013. Faster and smaller inverted indices with treaps. In Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval. ACM, 193­202.",0,,False
"[17] Daniel Lemire and Leonid Boytsov. 2015. Decoding billions of integers per second through vectorization. Software: Practice and Experience 45, 1 (2015), 1­29.",0,,False
"[18] Jimmy Lin, Matt Crane, Andrew Trotman, Jamie Callan, Ishan Chattopadhyaya, John Foley, Grant Ingersoll, Craig Macdonald, and Sebastiano Vigna. 2016. Toward reproducible baselines: The open-source ir reproducibility challenge. In European Conference on Information Retrieval. Springer, 408­420.",1,ad,True
"[19] Sergey Melnik, Sriram Raghavan, Beverly Yang, and Hector Garcia-Molina. 2001. Building a distributed full-text index for the Web. In Proceedings of the Tenth International World Wide Web Conference, WWW 10, Hong Kong, China, May 1-5, 2001. 396­406.",0,,False
"[20] Alistair Moffat and Justin Zobel. 1996. Self-indexing inverted files for fast text retrieval. ACM Transactions on Information Systems (TOIS) 14, 4 (1996), 349­379.",0,,False
[21] Calvin N Mooers. 1948. Application of random codes to the gathering of statistical information. Ph.D. Dissertation. Massachusetts Institute of Technology.,0,,False
"[22] Calvin N Mooers. 1951. Zatocoding applied to mechanical organization of knowledge. American documentation 2, 1 (1951), 20­32.",0,,False
"[23] Giuseppe Ottaviano and Rossano Venturini. 2014. Partitioned elias-fano indexes. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 273­282.",0,,False
"[24] Knut Magne Risvik, Trishul M. Chilimbi, Henry Tan, Karthik Kalyanaraman, and Chris Anderson. 2013. Maguro, a system for indexing and searching over very large text collections. In Sixth ACM International Conference on Web Search and Data Mining, WSDM 2013, Rome, Italy, February 4-8, 2013. 727­736.",0,,False
"[25] Charles S Roberts. 1979. Partial-match retrieval via the method of superimposed codes. Proc. IEEE 67, 12 (1979), 1624­1642.",0,,False
"[26] Ron Sacks-Davis, A Kent, and Kotagiri Ramamohanarao. 1987. Multikey access methods based on superimposed coding techniques. ACM Transactions on Database Systems (TODS) 12, 4 (1987), 655­696.",0,,False
"[27] Harry KT Wong, Hsiu-Fen Liu, Frank Olken, Doron Rotem, and Linda Wong. 1985. Bit Transposed Files.. In VLDB, Vol. 85. Citeseer, 448­457.",0,,False
"[28] Justin Zobel, Alistair Moffat, and Kotagiri Ramamohanarao. 1998. Inverted files versus signature files for text indexing. ACM Transactions on Database Systems (TODS) 23, 4 (1998), 453­490.",0,,False
614,0,,False
,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Faster BlockMax WAND with Variable-sized Blocks,0,,False
Antonio Mallia,0,,False
"University of Pisa, Italy a.mallia@studenti.unipi.it",0,,False
Giuseppe O aviano,0,,False
"ISTI-CNR, Italy giuseppe.o aviano@isti.cnr.it",0,,False
Elia Porciani,0,,False
"University of Pisa, Italy e.porciani1@studenti.unipi.it",0,,False
Nicola Tonello o,0,,False
"ISTI-CNR, Italy nicola.tonello o@isti.cnr.it",0,,False
ABSTRACT,0,,False
"ery processing is one of the main bo lenecks in large-scale search engines. Retrieving the top k most relevant documents for a given query can be extremely expensive, as it involves scoring large amounts of documents. Several dynamic pruning techniques have been introduced in the literature to tackle this problem, such as BlockMaxWAND, which splits the inverted index into constantsized blocks and stores the maximum document-term scores per block; this information can be used during query execution to safely skip low-score documents, producing many-fold speedups over exhaustive methods.",0,,False
"We introduce a re nement for BlockMaxWAND that uses variablesized blocks, rather than constant-sized. We set up the problem of deciding the block partitioning as an optimization problem which maximizes how accurately the block upper bounds represent the underlying scores, and describe an e cient algorithm to nd an approximate solution, with provable approximation guarantees.",0,,False
"rough an extensive experimental analysis we show that our method signi cantly outperforms the state of the art roughly by a factor 2×. We also introduce a compressed data structure to represent the additional block information, providing a compression ratio of roughly 50%, while incurring only a small speed degradation, no more than 10% with respect to its uncompressed counterpart.",1,ad,True
1 INTRODUCTION,1,DUC,True
"Web Search Engines [6, 19] manage an ever-growing amount of Web documents to answer user queries as fast as possible. To keep up with such a tremendous growth, a focus on e ciency is crucial.",0,,False
"ery processing is one of the hardest challenges a search engine has to deal with, since its workload grows with both data size and query load. Although hardware is ge ing less expensive and more powerful every day, the size of the Web and the number of searches is growing at an even faster rate.",1,ad,True
ery processing in search engines is a fairly complex process; queries in a huge collection of documents may return a large set of,0,,False
"Author currently at Facebook, USA.",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: h p://dx.doi.org/10.1145/3077136.3080780",1,ad,True
Rossano Venturini,0,,False
"University of Pisa, Italy",0,,False
rossano.venturini@unipi.it,0,,False
"results, but users are o en interested the most relevant documents, usually a small number (historically, the ten blue links). e relevance of a document can be arbitrarily expensive to compute, which makes it prohibitive to evaluate all the documents that match the queried terms; query processing is thus usually divided in multiple phases. In the rst phase, the query is evaluated over an inverted index data structure [3, 29] using a simple scoring function, producing a medium-sized set of candidate documents, namely the top k scored; these candidates are then re-ranked using more complex algorithms to produce the nal set of documents shown to the user.",0,,False
"In this work we focus on improving the e ciency of the rst query processing phase, which is responsible for a signi cant fraction of the overall work. In such phase, the scoring function is usually a weighted sum of per-term scores over the terms in the document that match the query, where the weights are a function of the query, and the scores a function of the occurrences of the term in the document. An example of such a scoring function is the widely used BM25 [24].",0,,False
"An obvious way to compute the top k scored documents is to retrieve all the documents that match at least one query term using the inverted index, and compute the score on all the retrieved documents. Since exhaustive methods like this can be very expensive for large collections, several dynamic pruning techniques have been proposed in the last few years. Dynamic pruning makes use of the inverted index, augmented with additional data structures, to skip documents during iteration that cannot reach a su cient score to enter the top k. us, the nal result is the same as exhaustive evaluation, but obtained with signi cantly less work.",1,ad,True
"ese techniques include MaxScore [30], WAND [4], and BlockMaxWAND (BMW) [10].",0,,False
"We focus our a ention on the WAND family of techniques. WAND augments the posting list of each term with the maximum score of that term among all documents in the list. While processing the query by iterating on the posting lists of its terms, it maintains the top k scores among the documents evaluated so far; to enter the top k, a new document needs to have score larger than the current k-th score, which we call the threshold. WAND maintains the posting list iterators sorted by current docid; at every step, it adds up the maximum scores of the lists in increasing order, until the threshold is reached. It can be seen that the current docid of the rst list that exceeds the threshold is the rst docid that can reach a score higher than the threshold, so the other iterators can safely skip all the documents up to that docid.",1,ad,True
"e core principle is that if we can upper-bound the score of a range of docids, and that upper bound is lower than the threshold,",0,,False
625,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"then the whole range can be safely skipped. As such, WAND computes the upper bounds of document by using the maximum score of the terms appearing in the document. Nevertheless, it should be clear that the pruning e ectiveness is highly dependent on the accuracy of the upper bound: the more precise the upper bound, the more docids we can skip, and, thus, the faster the query processing.",0,,False
"BMW improves the accuracy of the upper bounds by spli ing the posting lists into constant-sized blocks of postings, and storing the maximum score per block, rather than per list only. is way, the upper bound of a document is the sum of the maximum score of the blocks in which it may belong to. is approach gives more precise upper bounds because the scores of the blocks are usually much smaller than the maximum in their lists. Experiments con rm this intuition, and, indeed, BMW signi cantly outperforms WAND [10].",0,,False
"However, the coarse partitioning strategy of BMW does not take into consideration regularities or variances of the scores that may occur in the posting lists and their blocks. As an example, consider a posting with a very high score surrounded by postings with much lower scores. is posting alone is responsible for a high inaccuracy in the upper bounds of all its neighbors in the same block. Our main observation is that the use of variable-sized blocks would allow to be er adapt to the distribution of the scores in the posting list.",1,ad,True
"e bene ts of variable-sized blocks are apparent in the simple example above, where it is su cient to isolate the highly-scored posting in its own block to improve the upper bounds of several other postings, stored in di erent blocks. More formally, for a block of postings we de ne the block error as the sum of the individual posting errors, i.e., the sum of the di erences between the block maximum score and the actual score of the posting. Our goal is to nd a block partitioning minimizing the sum of block errors among all blocks in the partitioning. Clearly, this corresponds to minimizing the average block error. Na¨ively, the minimum cost partitioning would correspond to blocks containing only a single posting. However, if the blocks are too small, the average skip at query time will be short and, thus, this solution does not carry out any bene t. In this work we introduce the problem of nding a partition of posting lists into variable-sized blocks such that the the sum of block errors is minimized, subject to a constraint on the number of blocks of the partition. en, we will show that an approximately optimal partition can be computed e ciently. Experiments on standard datasets show that our Variable BMW (VBMW) signi cantly outperforms BMW and the other state-ofthe-art strategies.",0,,False
Our Contributions. We list here our main contributions.,0,,False
"(1) We introduce the problem of optimally partitioning the posting lists into variable-sized blocks to minimize the average block error, subject to a constraint on the number of blocks. We then propose a practical optimization algorithm which produces an approximately optimal solution in almost linear time. We remark that existing solutions for this optimization problem run in at least quadratic time, and, thus, they are unfeasible in a practical se ing. Experiments show that this approach is able to reduce the average score error up to 40%, con rming the importance of optimally partitioning posting list into variable-sized blocks.",1,ad,True
"(2) We propose a compression scheme for the block data structures, compressing the block boundary docids with EliasFano and quantizing the block max scores, obtaining a maximum reduction of space usage w.r.t. the uncompressed data structures of roughly 50%, while incurring only a small speed degradation, no more than 10% with respect to its uncompressed counterpart.",1,ad,True
(3) We provide an extensive experimental evaluation to compare our strategy with the state of the art on standard datasets of Web pages and queries. Results show that VBMW outperforms the state-of-the-art BMW by a factor of roughly 2×.,0,,False
2 BACKGROUND AND RELATED WORK,0,,False
"In the following we will provide some background on index organization and query processing in search engines. We will also summarize and discuss the state-of-the-art query processing strategies with a particular focus on the current most e cient strategy, namely BlockMaxWAND, leveraging block-based score upper bound approximations.",0,,False
"Index Organization. Given a collection D of documents, each document is identi ed by a non-negative integer called a document identi er, or docid. A posting list is associated to each term appearing in the collection, containing the list of the docids of all the documents in which the term occurs. e collection of the posting lists for all the terms is called the inverted index of D, while the set of the terms is usually referred to as the dictionary. Posting lists typically contain additional information about each document, such as the number of occurrences of the term in the document, and the set of positions where the term occurs [5, 19, 32].",1,ad,True
"e docids in a posting list can be sorted in increasing order, which enables the use of e cient compression algorithms and document-at-a-time query processing. is is the most common approach in large-scale search engines (see for example [8]). Alternatively, the posting lists can be frequency-sorted [30] or impactsorted [2], still providing a good compression rates as well as good query processing speed. However, there is no evidence of such index layouts in common use within commercial search engines [21].",0,,False
"Inverted index compression is essential to make e cient use of the memory hierarchy, thus maximizing query processing speed. Posting list compression boils down to the problem of representing sequences of integers for both docids and frequencies. Representing such sequences of integers in compressed space is a fundamental problem, studied since the 1950s with applications going beyond inverted indexes. A classical solution is to compute the di erences of consecutive docids (deltas), and encode them with uniquelydecodable variable length binary codes; examples are unary codes, Elias Gamma/Delta codes, and Golomb/Rice codes [25]. More recent approaches encode simultaneously blocks of integers in order to improve both compression ratio and decoding speed. e underlying idea is to partition the sequence of integers into blocks of",0,,False
"xed or variable length and to encode each block separately with di erent strategies (see e.g., [17, 22, 28] and references therein).",0,,False
"More recently, the Elias-Fano representation of monotone sequences [11, 12] has been applied to inverted index compression [31], showing excellent query performance thanks to its e cient random",0,,False
626,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"access and search operations. However, it fails to exploit the local clustering that inverted lists usually exhibit, namely the presence of long subsequences of close identi ers. Recently, O aviano and Venturini [23] described a new representation based on partitioning the list into chunks and encoding both the chunks and their endpoints with Elias-Fano, hence forming a two-level data structure.",0,,False
"is partitioning enables the encoding to be er adapt to the local statistics of the chunk, thus exploiting clustering and improving compression. ey also showed how to minimize the space occupancy of this representation by se ing up the partitioning as an instance of an optimization problem, for which they present a linear time algorithm that is guaranteed to nd a solution at most (1 + ) times larger than the optimal one, for any given   (0, 1). In the following we will use a variation of their algorithm.",1,ad,True
"ery Processing. In Boolean retrieval a query, expressed as a (multi-)set of terms, can be processed in conjunctive (AND) or",0,,False
"disjunctive (OR) modes, retrieving the documents that contain respectively all the terms or at least one of them. Top-k ranked retrieval, instead, retrieves the k highest scored documents in the collection, where the relevance score is a function of the querydocument pair. Since it can be assumed that a document which",1,ad,True
"does not contain any query term has score 0, ranked retrieval can",0,,False
"be implemented by evaluating the query in disjunctive mode, and scoring the results. We call this algorithm RankedOR.",0,,False
"In this work we focus on linear scoring functions, i.e., where the score of a query-document pair can be expressed as follows:",0,,False
"s(q, d) ,",0,,False
"wt st,d",0,,False
t qd,0,,False
"where the wt are query-dependent weights for each query term, and the st,d are scores for each term-document pair. Such scores are usually a monotonic function of the occurrences of the term in the document, which can be stored in the posting list alongside the docid (usually referred to as the term frequency).",0,,False
"It can be easily seen that the widely used BM25 relevance score [24] can be cast in this framework. In BM25, the weights wt are derived from t's inverse document frequency (IDF) to distinguish between common (low value) and uncommon (high value) words, and the scores st,d are a smoothly saturated function of the term frequency. In all our experiments we will use BM25 as the scoring function.",0,,False
"e classical query processing strategies to match documents to a query fall in two categories: in a term-at-a-time (TAAT) strategy, the posting lists of the query terms are processed one at a time, accumulating the score of each document in a separate data structure. In a document-at-a-time (DAAT) strategy, the query term postings lists are processed simultaneously keeping them aligned by docid. In DAAT processing the score of each document is fully computed considering the contributions of all query terms before moving to the next document, thus no auxiliary per-document data structures are necessary. We will focus on the DAAT strategy as it is is more amenable to dynamic pruning techniques.",0,,False
"Solving scored ranked queries exhaustively with DAAT can be very ine cient. Various techniques to enhance retrieval e ciency have been proposed, by dynamically pruning docids that are unlikely to be retrieved. Among them, the most popular are MaxScore [30] and WAND [4]. Both strategies augment the index by",0,,False
"storing for each term its maximum score contribution, thus allow-",0,,False
ing to skip large segments of posting lists if they only contain terms,0,,False
whose sum of maximum scores is smaller than the scores of the top k documents found up to that point.,0,,False
"e alignment of the posting lists during MaxScore and WAND processing can be achieved by means of the NextGEQt (d) operator, which returns the smallest docid in the posting list t that is greater than or equal to d. is operator can signi cantly improve the posting list traversal speed during query processing, by skipping",0,,False
"large amounts of irrelevant docids. e Elias-Fano compression scheme provides an e cient implementation of the NextGEQt (d) operator, which is crucial to obtain the typical subsecond response",0,,False
times of Web search engines. Both MaxScore and WAND rely on upper-bounding the con-,0,,False
"tribution that each term can give to the overall document score,",0,,False
"allowing to skip whole ranges of docids [18]. However, both employ a global per-term upper bound, that is, the",0,,False
"maximum score st,d among all documents d which contain the term t. Such maximum score could be signi cantly larger than the typical score contribution of that term, in fact limiting the opportunities",0,,False
"to skip large amounts of documents. For example, a single outlier",0,,False
for an otherwise low-score term can make it impossible to skip any,0,,False
document that contains that term.,0,,False
"To tackle this problem, Ding and Suel [10] propose to augment",0,,False
the inverted index data structures with additional information to,1,ad,True
store more accurate upper bounds: at indexing time each posting,0,,False
"list is split into consecutive blocks of constant size, e.g., 128 postings",0,,False
per block. For each block the score upper bound is computed and,0,,False
"stored, together with largest docid of each block. ese local term upper bounds can then be exploited by adapting",1,ad,True
"existing algorithms such as MaxScore and WAND to make use of the additional information. e rst of such algorithms is BlockMaxWAND (BMW) [10]. e authors report an average speedup of BMW against WAND of 2.78 ­ 3.04. Experiments in [9] report a speedup of 3.00 and 1.25 of BMW with respect to WAND and MaxScore, respectively. Several versions of Block-Max MaxScore (BMM), the MaxScore variant for block-max indexes, have been proposed in [7, 9, 26]. In [9], the authors implementation of BMM is 1.25 times slower than BMW on average.",1,ad,True
3 VARIABLE BLOCK-MAX WAND,0,,False
"As mentioned in the previous section, BMW leverages per-block upper bound information to skip whole blocks of docids during",0,,False
query processing (we refer to the original paper [10] for a detailed,0,,False
description of the algorithm). e performance of the algorithm,0,,False
highly depends on the size of the blocks: if the blocks are too,0,,False
"large, the likelihood of having at least one large value in each block",0,,False
"increases, causing the upper bounds to be loose. If they are too",0,,False
"small, the average skip will be short. In both cases, the pruning",0,,False
e ectiveness may reduce signi cantly. A sweet spot can thus be,0,,False
determined experimentally. e constant-sized block partitioning of BMW does not take,0,,False
into consideration regularities or variances of the scores that may,0,,False
occur in the posting lists and their blocks. e use of variable-sized,0,,False
blocks allows to be er adapt to the distribution of the scores in the,1,ad,True
posting list.,0,,False
627,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
8 4,0,,False
7 7,0,,False
2,0,,False
2,0,,False
2,0,,False
2 1,0,,False
3,0,,False
"5 blocks, fixed size 3",0,,False
"5 blocks, variable size",0,,False
Figure 1: Block errors in constant (le ) and variable (right) block partitioning.,0,,False
e improvement with this kind of partitioning is apparent from,0,,False
the example in Figure 1. e gure shows a sequence of scores,0,,False
"partitioned in constant-sized blocks and in variable-sized blocks. We de ne the error as the sum of the di erences between each value and its block's upper bound, the shaded area in the gure.",1,ad,True
is example shows that a variable-sized partitioning can produce,0,,False
"a much lower error, e.g., 28 in constant-sized partitioning (with",0,,False
blocks of length 3) versus 10 in variable-sized partitioning.,0,,False
"Problem de nition. To give a more formal de nition, for a partitioning of the sequence of scores in a posting list of n postings let B be the set of its blocks. Each block B  B is a sequence of consecutive postings in the posting list. We use b ,"" |B| and |B| to denote the number of blocks of the partition and the number of postings in B, respectively. e term-document scores are de ned above as st,d ; however, since in the following we will work on one posting list at a time, we can drop the t, so sd will denote the sequence of scores for each document d in the posting list.""",0,,False
We de ne the error of a partitioning B as follows:,0,,False
BB,0,,False
|B|,0,,False
max,0,,False
d B,0,,False
sd,0,,False
-,0,,False
d B,0,,False
sd,0,,False
.,0,,False
(1),0,,False
"Here for each block of postings we are accounting for the the sum of its individual posting errors, i.e., the sum of the di erences between the block maximum score and the score of the posting.",0,,False
"To simplify the formula above we can notice that the right-hand side of the subtraction can be taken out of the sum, since the blocks form a partition of the list, and the resulting term does not depend on B. us, minimizing the error is equivalent to minimizing the following formula, which represents the perimeter of the envelope, for a given number of blocks b , |B|:",0,,False
B,0,,False
B,0,,False
|B,0,,False
|,0,,False
max,0,,False
d B,0,,False
sd,0,,False
.,0,,False
(2),0,,False
"Our goal is to nd a block partitioning that minimizes the sum of block errors among all blocks in the partitioning. Na¨ively, the minimum cost partitioning would correspond to blocks containing only a single posting. Since this solution clearly does not carry out any bene t, we x the number of blocks in the partition to be b. As we will show in Section 5 minimizing the error can signi cantly improve BMW performance over constant-sized blocks.",0,,False
"Existing solutions. e problem of nding a partition that minimizes Equation (2) subject to a constraint b on the number of its blocks can be solved with a standard approach based on dynamic programming. e basic idea is to ll a b × n matrix M where entry M[i][j] stores the minimum error to partition the posting list up to position j with i blocks. is matrix can be lled top-down from le to right. e entry M[i][j] is computed by trying to place the jth posting in the optimal solutions that uses i - 1 blocks. Unfortunately, the time complexity of this solution is (bn2), which is (n3) since, given that the average block size n/b is small (e.g., 32­128), thus, the interesting values of b are (n). is algorithm is clearly unfeasible because n can easily be in the range of millions.",0,,False
is optimization problem is similar in nature to the well-studied,0,,False
problem of computing optimal histograms (see again Figure 1). e,0,,False
complexity of nding the best histogram with a given number of,0,,False
bars is the same as above. Several approximate solutions have,0,,False
been presented. Halim et al. [16] describe several solutions and,0,,False
introduce an algorithm that has good experimental performance,0,,False
"but no theoretical guarantees. All such solutions are polynomial either in n or in b. Some have complexity O(nb). Guha et al. [15] introduce a (1 + ) approximation with O(n + b3 log n + b2/) time. While these techniques can be useful in cases where b is small, in our case b ,"" (n), which makes these algorithms unfeasible for us. Furthermore, the de nition of the objective function in these""",0,,False
"works is di erent from ours, as it minimizes the variance rather",0,,False
than the sum of the di erences.,0,,False
Our solution. We rst present a practical and e cient algorithm,0,,False
with weaker theoretical guarantees regarding the optimal solution,0,,False
"than what would be expected. Indeed, xed the required number",0,,False
"of blocks b and an approximation parameter , with 0 <  < 1,",0,,False
the algorithm nds a partition with b  b blocks whose cost is,0,,False
at most a factor 1 +  larger than the cost of the optimal partition,0,,False
with b edges.,0,,False
is algorithm runs in O(n log1+,0,,False
1 ,0,,False
"log(U n/b)) time,",0,,False
where U is the largest cost of any block. e weakness is due to,0,,False
the fact that there is no guarantee on how much b is close to the,0,,False
"requested number of blocks b. Even with this theoretical gap, in all",0,,False
our experiments the algorithm identi ed a solution with a number,0,,False
"of blocks very close to the desired one. In the last part of the section,",0,,False
we will ll this gap by showing how to re ne the solution to always identify a 1 +  approximated optimal solution with exactly b edges.,0,,False
e rst solution is a variation of the approximate dynamic,0,,False
programming algorithm introduced by O aviano and Venturini [23],0,,False
to optimize the partitioning of Elias-Fano indexes.,0,,False
It is convenient to look at the problem as a shortest path problem,0,,False
over a directed acyclic graph (DAG). e nodes of the graph corre-,0,,False
spond to the postings in the list; the edges connect each ordered,0,,False
"pair i < j of nodes, and represent the possible blocks in the partition. e cost c(i, j) associated to the edge is thus (j - i) maxi d <j sd . In this graph, denoted as G, each path represents a possible",0,,False
"partitioning, and the cost of the path is equal to the cost of the",0,,False
"partitioning as de ned in (2). us, our problem reduces to an instance of constrained shortest path on this graph, that is, nding",0,,False
"the shortest path with a given number of edges [13, 20].",0,,False
We can compute the constrained shortest path with an approach,0,,False
"similar to the one in [1, 13, 20]. e idea is to reduce the problem",0,,False
"to a standard, unconstrained shortest path by using Lagrangian",0,,False
628,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"relaxation: adding a xed cost   0 to every edge. We denote the relaxed graph as G . By varying , the shortest path in G will have a di erent number of edges: if  ,"" 0, the solution is the path of n - 1 edges of length one; at the limit  "","" +, the solution is a single edge of length n. It can be shown that, for any given , if the shortest path in G has edges, then that path is an optimal -constrained shortest path in G. us, our goal is to nd the value of  that give "","" b edges. However, notice that not every b can be""",1,ad,True
"found this way, but in practice we can get close enough. us, our",0,,False
"algorithm performs a binary search to nd the value of  that gives a shortest path with b edges, with b close enough to b. Each step",0,,False
of the binary search requires a shortest-path computation.,0,,False
"Each of these shortest-path computations can be solved in O(|V |+ |E|), where V are the vertices of G and E the edges; for our problem, unfortunately, this is (n2), which is still unfeasible. We can",0,,False
however exploit two properties of our cost function to apply the,0,,False
algorithm in [23] and obtain a linear-time approximate solution for,0,,False
a given value of . ese properties are monotonicity and quasisubadditivity. e monotonicity property is stated as follows.,1,ad,True
P,0,,False
1. (Monotonicity) A function f : V × V  R is said,0,,False
"monotone if for each pair of values i, j  V the following holds:",0,,False
"· f (i, j + 1)  f (i, j), · f (i - 1, j)  f (i, j).",0,,False
"It is easy to verify that our cost function c(i, j) satis es Property 1, because if a block B is contained in a block B , then it follows immediately from the de nition that the cost of B is greater than the cost of B. Monotonicity allows us to perform a rst pruning of G: for any given approximation parameter   (0, 1], we de ne G1 as the graph with the same nodes as G , and all the edges (i, j) of G that satisfy at least one of the following conditions.",0,,False
(1) ere exists an integer h such that,0,,False
"c(i, j)  (1 + )h < c(i, j + 1)",0,,False
"(2) (i, j) is the last outgoing edge from i.",0,,False
e,0,,False
number,0,,False
of,0,,False
edges,0,,False
in G1,0,,False
is,0,,False
n,0,,False
log1+,0,,False
(,0,,False
U ,0,,False
),0,,False
where U,0,,False
is,0,,False
the,0,,False
maxi-,0,,False
mum cost of an edge (which is equal to n maxd sd ).,0,,False
We denote as G the shortest path of the graph G and ex-,0,,False
tend,0,,False
c,0,,False
to,0,,False
denote,0,,False
the,0,,False
cost,0,,False
of,0,,False
a,0,,False
path.,0,,False
It,0,,False
can,0,,False
be,0,,False
shown,0,,False
that,0,,False
c,0,,False
(G,0,,False
1 ,0,,False
),0,,False
"(1 +  )c(G ), that is, the optimal solution in G1 is a (1 +  ) approximation of the optimal solution in G; see [14] for the proof.",0,,False
e complexity to nd the shortest path decreases from O(n2) to,0,,False
O (n,0,,False
log1+,0,,False
(,0,,False
U ,0,,False
)).,0,,False
is would be already applicable in many practical,1,ad,True
"scenarios, but it depends on the value U of the maximum score. We",0,,False
can further re ne the algorithm in order to decrease the complexity,0,,False
"and drop the dependency on U by adding an extra approximation function (1 + ) for any given approximation parameter   (0, 1],",1,ad,True
by leveraging the quasi-subadditivity property.,1,ad,True
P,0,,False
2. ( asi-subadditivity) A function f : V × V  R is,1,ad,True
"said -quasi-subadditive if for any i, k and j  V , with 0  i < l <",1,ad,True
j < |V | the following holds:,0,,False
"f (i, k) + f (k, j)  f (i, j) + .",0,,False
"It is again immediate to show that c(i, j) satis es Property 2: spli ing a block at any point can only lower the upper bound in",0,,False
"the two resulting sub-blocks, so the only extra cost is the additional",1,ad,True
 of the new edge.,0,,False
is property allows us to prune from G1 all the edges with cost,0,,False
higher,0,,False
than,0,,False
L,0,,False
",",0,,False
+,0,,False
2 ,0,,False
;,0,,False
we,0,,False
call,0,,False
the,0,,False
resulting,0,,False
graph,0,,False
G2 .,0,,False
e new,0,,False
graph has O(n log1+,0,,False
1 ,0,,False
),0,,False
",",0,,False
(n),0,,False
"edges,",0,,False
thus,0,,False
shortest,0,,False
paths,0,,False
can,0,,False
be,0,,False
computed in linear time. It can be shown (see [23]) that this pruning,0,,False
incurs an extra (1 + ) approximation; the overall approximation,0,,False
"factor is thus (1 + )(1 + ), which is 1 +  for any   (0, 1] by appropriately xing  ,  ,  .",0,,False
3,0,,False
"Clearly it is not feasible to materialize the graph G and prune it to obtain G2 , since the dominating cost would still be the initial quadratic phase. It is however possible to visit the graph G2 without",1,ad,True
"constructing it explicitly, as described in [23].",0,,False
"By using the above algorithm, every shortest path computation",0,,False
requires O(n log1+,0,,False
1 ,0,,False
),0,,False
", (n) time and linear space.",0,,False
"Since we are binary searching on , the number of required",0,,False
shortest path computations depends on the range of possible values,0,,False
"of . It is easy to see that   0. Indeed, the shortest path in G0 has the largest possible number of edges, n - 1 and the smallest possible cost. We now prove that the shortest path in G with  > U n/(b -1) has less than b edges, where U is the largest cost on G. us, in",0,,False
the binary search we can restrict our a ention to integer values of,0,,False
" in [0, U n/(b - 1)]. e proof is as follows. Consider the optimal path with one edge in G, and let O1 be its cost. By monotonicity, we know that O1 ,"" U . Let Ob be the cost of the best path with b edges in G. For any , the cost of these two paths in G are O1 +  and Ob + b. Observe that if  > U n/b, the former path has a cost""",0,,False
which is smaller than the cost of the la er. is means that we do,0,,False
"not need to explore values of  larger than U n/(b - 1) when we are looking for a path with b edges. us, the rst phase of the algorithm needs O(log(U n/b)) shortest path computations to nd the target value of . us, if we restrict our search to integer values of , the number of shortest path computations is O(log(U n/b)).",0,,False
We can re ne the above solution to nd a provable good approximation of the shortest path with exactly b edges. e re nement,0,,False
"uses the result in [13]. eorem 4.1 in [13] states that, given a DAG G with integer costs which satisfy the monotonicity property, we",0,,False
can compute an additive approximation of the constrained shortest,1,ad,True
"path of G. More precisely, we can compute a path with b edges such that its cost is at most Ob + U , where Ob is the cost of an optimal path with b edges and U is the largest cost on G. e algorithms",0,,False
"works in two phases. In the rst phase, it reduces the problem",0,,False
"to a standard, unconstrained shortest path by using Lagrangian",0,,False
"relaxation as we have done in our rst solution. us, the rst",0,,False
"phase binary searches for the value of  for which the shortest path on G with the least number of edges has at most b edges, while the one with the most edges has at least b edges. If one of these two paths has exactly b edges, this is guaranteed to be an optimal",0,,False
"solution and we are done. Otherwise, we start the second phase",0,,False
of the algorithm. e second phase is called path-swapping and its goal is to combine these two paths to nd a path with b edges,0,,False
"whose cost is worse than the optimal one by at most an additive term A, which equals the largest cost in the graph. We refer to [1]",1,ad,True
and [13] for more details.,0,,False
629,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
We cannot immediately apply the above optimization algorithm,0,,False
because of two important issues. In the following we will introduce,0,,False
and solve both of them.,0,,False
"e rst issue is that the above optimization algorithm assumes that the costs in G are integers, while in our case are not. e",0,,False
idea is to obtain a new graph with integer costs by rescaling and,0,,False
"rounding the original costs of G. More precisely, we can obtain a new graph by replacing any cost c(i, j) with c(i, j)/ , where   (0, 1] is an approximation parameter. We can prove that this",0,,False
"operation slightly a ects the cost of the optimal path. Indeed, let",0,,False
"Ob the cost of the shortest path with b edges in G, the shortest path on the new graph as cost O~b which is Ob  O~b  Ob + b. Due to space limitations, we defer the proof of this inequality to",0,,False
the journal version of the paper. Even if in general we cannot,0,,False
"bound the additive approximation b in terms of Ob , in practice",1,ad,True
the approximation is negligible because Ob is much larger that b.,0,,False
Notice,0,,False
that,0,,False
this,0,,False
approximation,0,,False
increases U,0,,False
to,0,,False
U ,0,,False
.,0,,False
e second issue to address is the fact that additive approxima-,1,ad,True
tion term A in the result of [13] is the largest edge cost U . In our,0,,False
"problem this additive approximation term is the cost of the edge from 1 to n, which equals the cost of the worst possible path. is",1,ad,True
"means that the obtained approximation would be trivial. However,",0,,False
"we observe that, due to the approach of the previous paragraph,",0,,False
the,0,,False
largest,0,,False
cost,0,,False
on,0,,False
the,0,,False
approximated,0,,False
graph,0,,False
G,0,,False
2 ,0,,False
is,0,,False
L,0,,False
",",0,,False
+,0,,False
2 ,0,,False
and,0,,False
"we know that   U n/b. us, the additive approximation term A",1,ad,True
is,0,,False
O(,0,,False
Un b,0,,False
"),",0,,False
which,0,,False
is,0,,False
negligible,0,,False
in,0,,False
practice.,0,,False
"us, we obtained the following theorem.",0,,False
T,0,,False
"3.1. Given a sequence of scores S[1, n] and a xed num-",0,,False
"ber of blocks b, we can compute a partition of S into b blocks whose",0,,False
cost,0,,False
is,0,,False
at,0,,False
most,0,,False
(1+,0,,False
)Ob,0,,False
+O,0,,False
(,0,,False
Un b,0,,False
),0,,False
+b,0,,False
in,0,,False
O,0,,False
(n,0,,False
log1+,0,,False
1 ,0,,False
log(,0,,False
Un b,0,,False
)),0,,False
time,0,,False
"and linear space, where Ob is the cost of the optimal partition with",0,,False
"b blocks, U ,",0,,False
"n i ,1",0,,False
S,0,,False
[i,0,,False
"],",0,,False
"and , ",0,,False
"(0, 1]",0,,False
are,0,,False
the,0,,False
two,0,,False
approximation,0,,False
parameters.,0,,False
4 REPRESENTING THE UPPER BOUNDS,0,,False
"BlockMaxWAND is required to store additional information about the block upper bounds. is additional information must be stored together with the traditional inverted index data structures, and while these upper bounds can improve the time e ciency of query processing, they introduce a serious space overhead problem.",1,ad,True
"e additional information required by BlockMaxWAND can be seen as two aligned sequences: the sequence of block boundaries, that is, the largest docid in each block, and the score upper bound for each block.",1,ad,True
"In the original implementation, the sequences are stored uncompressed, using constant-width encodings (for example, 32-bit integers for the boundaries and 32-bit oats for the upper bounds), and are usually interleaved to favor cache locality. We can however use more e cient encodings to reduce the space overhead.",1,ad,True
"First, we observe that the sequence of block boundaries is monotonic, so it can be e ciently represented with Elias-Fano. In addition to saving space, Elias-Fano provides an e cient NextGEQ operation that can be used to quickly locate the block containing the current docid at query execution time.",1,ad,True
"Second, as far as the upper bounds are concerned, we can reduce space use by approximating their value. e only requirement to",0,,False
preserve the correctness of the algorithm is that each approximate,0,,False
"value is an upper bound for all the scores in its block. us, we",0,,False
"can use the following quantization. First, we partition the score",0,,False
space into xed size buckets. Any score is represented with the,0,,False
"identi er of its bucket. Let us assume that the score space is [0, U ]",0,,False
"and that we partition it into w buckets. en, instead of storing a",1,ad,True
"block upper bound with value s  [0, U ], we store the identi er i",0,,False
such,0,,False
that,0,,False
iU w,0,,False
<s ,0,,False
(i +1)U w,0,,False
.,0,,False
At,0,,False
query,0,,False
"time,",0,,False
the,0,,False
actual,0,,False
score,0,,False
s,0,,False
will,0,,False
"be approximated with the largest possible value in its bucket, i.e.,",0,,False
(i +1)U,0,,False
"w . Clearly, the representation of any score requires",0,,False
log w + 1,0,,False
"bits, a large space saving with respect to the 32 bits of the oat",0,,False
"representation. Obviously, the value of w can be chosen to trade",1,ad,True
o the space usage and the quality of the approximation.,0,,False
A simple optimization to speed up access is to interleave the two,0,,False
"sequences, by modifying of the Elias-Fano data structure. EliasFano stores a monotonic sequence by spli ing each value into its low bits, and the remaining high bits. e value of a constant for",0,,False
"the sequence. While the high bits are encoded with variable-length,",0,,False
"the low bits are encoded verbatim in exactly bits per element, thus the low bits of the i-th element are at the position i of the low",0,,False
"bitvector. We can then interleave the low bits and the quantized score by using a bitvector of ( + w)-bit entries, so that when the",0,,False
"block is located, its quantized upper bound is already in cache.",1,ad,True
5 EXPERIMENTAL RESULTS,0,,False
"In this section we analyze the performance of VBMW with an extensive experimental evaluation in a realistic and reproducible se ing, using state-of-the-art baselines, standard benchmark text collections, and a large query log.",0,,False
Testing details. All the algorithms are implemented in C++11 and compiled with GCC 5.4.0 with the highest optimization se ings.,0,,False
"e tests are performed on a machine with 8 Intel Core i7-4770K Haswell cores clocked at 3.50GHz, with 32GiB RAM, running Linux 4.4.0. e indexes are saved to disk a er construction, and memorymapped to be queried, so that there are no hidden space costs due to loading of additional data structures in memory. Before timing the queries we ensure that the required posting lists are fully loaded in memory. All timings are measured taking the results with minimum value of ve independent runs. All times are reported in milliseconds.",1,ad,True
e source code is available at h ps://github.com/rossanoventurini/ Variable-BMW for the reader interested in further implementation details or in replicating the experiments.,1,ad,True
Datasets. We performed our experiments on the following standard datasets.,0,,False
"· ClueWeb09 is the ClueWeb 2009 TREC Category B collection, consisting of 50 million English web pages crawled between January and February 2009.",1,ClueWeb,True
"· Gov2 is the TREC 2004 Terabyte Track test collection, consisting of 25 million .gov sites crawled in early 2004; the documents are truncated to 256 kB.",1,Gov,True
"For each document in the collection the body text was extracted using Apache Tika1, the words lowercased and stemmed using the",0,,False
1h p://tika.apache.org,0,,False
630,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Basic statistics for the test collections,0,,False
Documents Terms Postings,0,,False
ClueWeb09,1,ClueWeb,True
"50,131,015 92,094,694 15,857,983,641",0,,False
Gov2,1,Gov,True
"24,622,347 35,636,425 5,742,630,292",0,,False
Porter2 stemmer; no stopwords were removed. e docids were,0,,False
assigned according to the lexicographic order of their URLs [27].,0,,False
Table 1 reports the basic statistics for the two collections. If not,0,,False
"di erently speci ed, the inverted index is compressed by using partitioned Elias-Fano (PEF) [23] in the ds2i library2.",0,,False
"eries. To evaluate the speed of query processing we use Trec05 and Trec06 E ciency Track topics, drawing only queries whose terms are all in the collection dictionary and having more than 128",1,Track,True
"postings. ese queries are, respectively, the 90% and 96% of the total Trec05 and Trec06 queries for the Gov2 collection and the 96% and 98% of the total Trec05 and Trec06 queries for the ClueWeb09 collection. From those sets of queries we randomly select 1 000",1,Gov,True
queries for each length.,0,,False
"Processing strategies. To test the performance on query strategies that make use of the docids and the occurrence frequencies we perform BM25 top 10 queries using 5 di erent algorithms: RankedOR, which scores the results of a disjunctive query, WAND [4], MaxScore [30], BlockMaxWAND (BMW) [10], and the proposed Variable BMW (VBMW) in its uncompressed and compressed variants.",0,,False
"We use BMWx to indicate that the xed block size in BMW is x postings, while we use VBMWx to indicate that the average block size in VBMW is x postings. e compressed version of VBMW as described in Section 4 is denoted as C-VBMWx.",0,,False
Validating our BMW implementation. We implemented our version of BMW because the source code of the original implementation was not available. To test the validity of our implementation we,0,,False
"compared its average query time with the ones reported in [10]. We replicated their original se ing by using the same dataset (Gov2), by compressing postings with the same algorithm (PForDelta), by using queries from the same collections (Trec05 and Trec06), and by using BMW64. However, since we are using a di erent faster machine, we cannot directly compare query times, but, instead, we compare the improving factors with respect to RankedOR, which is an easy-to-implement baseline.",1,Gov,True
Table 2 shows the query times reported in the original paper,0,,False
(top) and the ones obtained with our implementation (bo om).,0,,False
"Results show that the two implementations are comparable, with",0,,False
"ours which is generally faster. For example, it is faster by a factor larger than 2.4 on queries with more than three terms in Trec06.",0,,False
"e e ect of the block size in BMW. Although the most commonly used block sizes for BMW are 64 and 128, a more careful experimental evaluation shows that the best performance in terms of",0,,False
query time is obtained with a block size of 40 postings. Table 3 shows the average query time of BMW with respect to,0,,False
"Trec05 and Trec06 on both Gov2 and ClueWeb09, by varying the",1,Gov,True
2h ps://github.com/ot/ds2i,0,,False
"Table 2: ery times (in ms) of RankedOR and BMW64 on Gov2 with queries in Trec05 and Trec06 as reported by Ding and Suel [10] (top) and the ones obtained with our implementation (bottom), for di erent query lengths.",1,Gov,True
Number of query terms,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
6+,0,,False
Trec05 (from [10]),0,,False
"RankedOR 62.1 (x17.7) 238.9 (x18.8) 515.2 (x20.4) 778.3 (x25.9) 1,501.4 (x14.4)",0,,False
BMW64 3.5,0,,False
12.7,0,,False
25.2,0,,False
30.0,0,,False
104.0,0,,False
Trec06 (from [10]),0,,False
RankedOR 60.0 (x14.7) 159.2 (x13.8) 261.4 (x7.8) 376.0 (x6.9),0,,False
BMW64 4.1,0,,False
11.5,0,,False
33.6,0,,False
54.5,0,,False
Trec05,0,,False
646.4 (x5.7) 114.2,0,,False
RankedOR 15.5 (x13.2) 51.3 (x17.3) 100.3 (x22.6) 158.0 (x22.7),0,,False
BMW64 1.2,0,,False
3.0,0,,False
4.5,0,,False
7.0,0,,False
Trec06,0,,False
275.1 (x17.3) 15.9,0,,False
RankedOR 15.5 (x14.7) 57.6 (x16.9) 117.6 (x19.7) 178.0 (x18.5) 311.2 (x13.8),0,,False
BMW64 1.1,0,,False
3.4,0,,False
6.0,0,,False
9.6,0,,False
22.5,0,,False
"block size. We select the block size in the set {32, 40, 48, 64, 96, 128}. It is clear that in all cases, the best average query time is achieved with blocks size 40. BMW40 is 10% faster, on average, than BMW128.",0,,False
"Table 3 also reports the space usage of the (uncompressed) additional information stored by BMW, namely the largest score in the block (as oat) and the last posting in the block (as unsigned",1,ad,True
int). Posting lists with fewer postings than the block size do not,0,,False
"store any additional information. e size of the inverted index of the Gov2 and ClueWeb09 collections (compressed with PEF) is 4.32 GiB and 14.84 GiB respectively. us, the space of the additional information required by BMW is not negligible, since it ranges between 15% and 42% of the compressed inverted index space on both Gov2 and ClueWeb09. As we will see later, this space usage can be reduced signi cantly by compressing the additional information.",1,ad,True
"e e ect of the block size in VBMW. Now, we proceed by analyzing the behavior of VBMW. Instead of adopting the more sophisticated approximation approach detailed in Section 3, we use",1,ad,True
the simpler optimization algorithm which has no theoretical guar-,0,,False
"antees on the nal number of blocks. us, we cannot choose an exact block size for our partitioning but we binary search for the  in the parameter space that gives an average block size close to the values in {32, 40, 48, 64, 96, 128}.",0,,False
"Table 4 reports the average block sizes and score errors for different block sizes w.r.t. BMW and VBMW on Gov2 and ClueWeb09, and optimal values for the Lagrangian relaxation parameter . Note that for BMW, the average block size is not perfectly identical to the desired block size due to the length of the last block in the",1,Gov,True
"posting lists, which may be smaller than the desired block size.",0,,False
"Our optimization algorithm is able to nd an average block size for VBMW within 3% of the average block size for BMW. us, the weaker optimization algorithm of Section 3 su ces in practice",0,,False
"to obtain the desired average block sizes. More importantly, the",0,,False
631,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: Space usage of the additional data required by BMW and average query times with queries in Trec05 and Trec06 on Gov2 and ClueWeb09, by varying the block size.",1,ad,True
"Table 5: Average query times of VBMW with queries in Trec05 and Trec06 on Gov2 and ClueWeb09, by varying the block size.",1,Gov,True
Block size 32 40 48 64 96 128,0,,False
Additional space (GiB),0,,False
Gov2,1,Gov,True
1.83 1.55 1.38 1.15 0.92 0.85,0,,False
ClueWeb09 5.04 4.14 3.62 3.04 2.40 2.24,1,ClueWeb,True
ery time (ms) on Trec05,0,,False
Gov2,1,Gov,True
3.6 3.6 3.7 3.8 3.9 4.2,0,,False
ClueWeb09 12.8 12.6 12.6 12.8 13.3 13.9,1,ClueWeb,True
ery time (ms) on Trec06,0,,False
Gov2,1,Gov,True
8.3 8.2 8.3 8.5 8.9 9.2,0,,False
ClueWeb09 26.4 26.3 26.5 27.0 28.0 29.4,1,ClueWeb,True
"Table 4: Average block sizes and score errors for di erent block sizes w.r.t. BMW and VBMW on Gov2 and ClueWeb09, and optimal values for the Lagrangian relaxation parameter.",1,Gov,True
Block Size,0,,False
32 40 48 64 96 128,0,,False
Gov2,1,Gov,True
Average Block Size,0,,False
BMW 31.94 39.90 47.87 63.74 95.35 127.14 VBMW 31.32 39.63 47.09 63.60 98.40 126.30,0,,False
Average Score Error,0,,False
BMW VBMW,0,,False
1.47 0.82,0,,False
1.55 0.91,0,,False
1.61 0.98,0,,False
1.70 1.09,0,,False
1.83 1.26,0,,False
1.92 1.35,0,,False
VBMW 12.0 15.2 18.0 24.0 35.1 45.9,0,,False
ClueWeb09,1,ClueWeb,True
Average Block Size,0,,False
BMW 31.96 39.94 47.91 63.83 95.65 127.29 VBMW 30.24 39.54 48.03 63.29 97.43 127.72,0,,False
Average Score Error,0,,False
BMW VBMW,0,,False
1.94 1.20,0,,False
2.05 1.34,0,,False
2.15 1.45,0,,False
2.29 1.60,0,,False
2.49 1.83,0,,False
2.63 1.98,0,,False
VBMW 16.0 21.0 25.5 33.4 50.3 64.5,0,,False
"average score error for VBMW is sensibly smaller than the average score error for BMW, with a reduction ranging from 40% for small blocks up to 25% for large blocks. is con rms the importance of",0,,False
partitioning the posting lists with variable-sized blocks. In Table 5 we can see that VBMW reaches the best average query,0,,False
"times with approximatively 32 - 40 elements per block, similar to the best block size for BMW reported in Table 3, i.e., 40 postings per block. As shown in Figure 2, the trade-o in choosing this block",1,ad,True
size w.r.t. average query time is that we use more space to store,0,,False
"block information, as reported in Table 3.",0,,False
e e ect of compression in VBMW. Figure 2 shows how the choice of w a ects both query time and space usage of C-VBMW when the average number of blocks is xed to 40 elements. We,0,,False
xed the number of buckets w to quantize the scores to the powers,0,,False
Block size 32 40 48 64 96 128,0,,False
ery time (ms) on Trec05,0,,False
Gov2,1,Gov,True
2.1 2.1 2.1 2.2 2.5 2.8,0,,False
ClueWeb09 7.2 7.2 7.4 8.1 9.7 11.0,1,ClueWeb,True
ery time (ms) on Trec06,0,,False
Gov2,1,Gov,True
4.6 4.7 4.8 5.3 6.1 6.9,0,,False
ClueWeb09 14.7 15.2 16.1 17.8 21.2 23.7,1,ClueWeb,True
of two from 32 to 512 and we reported the query time and the,0,,False
space of the additional information on both datasets with both set,1,ad,True
"of queries. For comparison, we also plot the results of the plain version of VBMW by varying the average size of the blocks.",0,,False
"e rst conclusion is that the compression approach is very effective. Indeed, C-VBMW improves space usage by roughly a factor 2 with respect to VBMW40. We also notice that the compression approach is more e ective than simply increasing the block size in the uncompressed VBMW. Indeed, for example, C-VBMW with w , 32 uses almost the same space as VBMW128 but is faster by 20% - 40%.",0,,False
e second conclusion is that compression does not decrease,0,,False
"query time which actually sometimes even improves. For example, C-VBMW with w , 512 and w , 256 is faster that its uncompressed version (VBMW40) on both datasets with Trec05. is e ect may be the results to a be er cache usage resulting from the smaller size of additional information in C-VBMW.",1,ad,True
"We observe that there are small di erences (less than 10%) in e ciency between the di erent values of w. us, for the next experiments we will x w to 512 to obtain the best time e ciency.",0,,False
"Overall comparison. To carefully evaluate the performance of C-VBMW w.r.t. other processing strategies, we measured the query times of di erent query processing algorithms for di erent query",0,,False
"lengths, from 2 terms queries to more than 5 terms queries, as well",0,,False
as the overall average processing times and the space use of any,0,,False
required additional data structure with respect the whole inverted indexes represented with PEF.,1,ad,True
"In Table 6, next to each timing is reported in parenthesis the relative speedup of C-VBMW40 with respect to this strategy. Table 6 also reports, in GiB, the additional space usage required by the di erent query processing strategies. Next to each size mea-",1,ad,True
sure is reported in parenthesis the relative percentage against the,0,,False
"data structures used to compress posting lists storing docids and frequencies only, as used by RankedOR.",0,,False
"Not very surprisingly, RankedOR is always at least 34 times slower than C-VBMW40, while both MaxScore and WAND are from 1.4 to 11 times slower than C-VBMW40. e maximum speedup of C-VBMW40 is achieved with queries of two terms where it ranges from 6.5 to 11. Space usage of MaxScore and WAND plainly store the score upper bounds for each term using the 4% - 5% of the inverted index.",0,,False
632,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 2: Space consumed vs. average query times of VBMW with di erent block sizes and C-VBMW with block size 40 by varying w for 32 to 512 with queries in Trec05 and Trec06 on Gov2 and ClueWeb09.,1,Gov,True
All block-based strategies report a minimal variance of query,0,,False
times among di erent query lengths. For both the most common,0,,False
"block size (128 postings per block) and the most e cient one (40 postings per block), VBMW strategies process queries faster than BMW strategies, with the same space occupancies. e corresponding compressed versions, C-VBMW128 and C-VBMW40, sensibly reduce the space occupancies (by 6% and 17% respectively) but while C-VBMW128 never processes queries faster than the corresponding uncompressed VBMW128, C-VBMW40 does not show relevant performance losses with respect to VBMW128, but exhibits some cache-dependent bene ts for short queries.",0,,False
"With respect to the current state-of-the-art processing strategy BMW128, our best strategy in terms of query times is C-VBMW40, able to improve the average query time by a factor of roughly 2×, e ectively halving the query processing times for all query lengths, with a relative 3% - 5% gain in space occupancy. If space occupancy is the main concern, our best strategy is C-VBMW128, able to reduce the space by a relative 30% against BMW128, while still boosting the query times by a factor of roughly 1.5×.",0,,False
6 CONCLUSIONS,0,,False
"We introduced Variable BMW, a new query processing strategy built on top of BlockMaxWAND. Our strategy uses variable-sized blocks, rather than constant-sized. We formulated the problem of",0,,False
partitioning the posting lists of a inverted index into variable-sized,0,,False
"blocks to minimize the average block error, subject to a constraint",0,,False
"on the number of blocks, and described an e cient algorithm to nd",0,,False
"an approximate solution, with provable approximation guarantees.",0,,False
"We also introduced a compressed data structure to represent the additional block information. Variable BMW signi cantly improves the query processing times, by a factor of roughly 2× w.r.t. the best state-of-the-art competitor. Our new compression scheme for the",1,ad,True
"block data structures, compressing the block boundary docids with",0,,False
"Elias-Fano and quantizing the block max score, provides a maximum",0,,False
reduction of space usage w.r.t. the uncompressed data structures of,0,,False
"roughly 50%, while incurring only a small speed degradation, no",1,ad,True
more than 10% with respect to its uncompressed counterpart.,0,,False
Future work will focus on exploring the di erent space-time,0,,False
trade-o s that can be obtained by varying the quantization scheme,1,ad,True
exploited in the compression of the additional data structures.,1,ad,True
ACKNOWLEDGMENTS,0,,False
is work was partially supported by the EU H2020 Program under,0,,False
"the scheme INFRAIA-1-2014-2015: Research Infrastructures, grant agreement #654024 SoBigData: Social Mining & Big Data Ecosystem.",0,,False
REFERENCES,0,,False
"[1] Alok Aggarwal, Baruch Schieber, and Takeshi Tokuyama. 1994. Finding a",0,,False
"Minimum-Weight k-Link Path Graphs with the Concae Monge Property and Applications. Discrete & Computational Geometry 12 (1994), 263­280. [2] Vo Ngoc Anh, Owen de Kretser, and Alistair Mo at. 2001. Vector-space ranking with e ective early termination. In SIGIR. 35­42. [3] Nima Asadi and Jimmy Lin. 2013. E ectiveness/E ciency Tradeo s for Candidate Generation in Multi-stage Retrieval Architectures. In SIGIR. 997­1000. [4] Andrei Z. Broder, David Carmel, Michael Herscovici, Aya So er, and Jason Y.",1,ad,True
"Zien. 2003. E cient query evaluation using a two-level retrieval process. In CIKM. 426­434. [5] Stefan Bu¨ cher, Charles L.A. Clarke, and Gordon V. Cormack. 2010. Information retrieval: implementing and evaluating search engines. MIT Press. [6] Stefan Bu¨ cher and Charles L. A. Clarke. 2007. Index compression is good, especially for random access. In CIKM. 761­770. [7] Kaushik Chakrabarti, Surajit Chaudhuri, and Venkatesh Ganti. 2011. Intervalbased Pruning for Top-k Processing over Compressed Lists. In ICDE. 709­720. [8] Je rey Dean. 2009. Challenges in building large-scale information retrieval systems: invited talk. In WSDM. [9] Constantinos Dimopoulos, Sergey Nepomnyachiy, and Torsten Suel. 2013. Optimizing Top-k Document Retrieval Strategies for Block-max Indexes. In WSDM. 113­122.",0,,False
[10] Shuai Ding and Torsten Suel. 2011. Faster top-k document retrieval using blockmax indexes. In SIGIR. 993­1002.,0,,False
"[11] Peter Elias. 1974. E cient Storage and Retrieval by Content and Address of Static Files. J. ACM 21, 2 (1974), 246­260.",0,,False
"[12] Robert M. Fano. 1971. On the number of bits required to implement an associative memory. Memorandum 61, Computer Structures Group, MIT, Cambridge, MA (1971).",0,,False
"[13] Andrea Farruggia, Paolo Ferragina, Antonio Frangioni, and Rossano Venturini. 2014. Bicriteria data compression. In SODA. 1582­1595.",0,,False
"[14] Paolo Ferragina, Igor Ni o, and Rossano Venturini. 2011. On Optimally Partitioning a Text to Improve Its Compression. Algorithmica 61, 1 (2011), 51­74.",0,,False
"[15] Sudipto Guha, Nick Koudas, and Kyuseok Shim. 2006. Approximation and Streaming Algorithms for Histogram Construction Problems. ACM Trans. Database Syst. 31, 1 (2006), 396­438.",0,,False
"[16] Felix Halim, Panagiotis Karras, and Roland H.C. Yap. 2009. Fast and E ective Histogram Construction. In CIKM. 1167­1176.",0,,False
"[17] Daniel Lemire and Leonid Boytsov. 2015. Decoding Billions of Integers Per Second rough Vectorization. So w. Pract. Exper. 45, 1 (2015), 1­29.",0,,False
"[18] Craig Macdonald, Iadh Ounis, and Nicola Tonello o. 2011. Upper-bound approximations for dynamic pruning. ACM Trans. Inf. Syst. 29, 4 (2011), 17.",1,ad,True
"[19] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schu¨lze. 2008. Introduction to Information Retrieval. Cambridge University Press.",0,,False
[20] Kurt Mehlhorn and Mark Ziegelmann. 2000. Resource Constrained Shortest Paths. In ESA. 326­337.,0,,False
"[21] Alistair Mo at, William Webber, Justin Zobel, and Ricardo Baeza-Yates. 2007. A pipelined architecture for distributed text query evaluation. Inf. Retr. 10, 3 (2007), 205­231.",0,,False
"[22] Giuseppe O aviano, Nicola Tonello o, and Rossano Venturini. 2015. Optimal Space-time Tradeo s for Inverted Indexes. In WSDM. 47­56.",1,ad,True
[23] Giuseppe O aviano and Rossano Venturini. 2014. Partitioned Elias-Fano Indexes. In SIGIR. 273­282.,0,,False
633,0,,False
Session 5C: Efficiency and Scalability,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 6: ery times (in ms) of di erent query processing strategies for di erent query lengths, average query times (Avg, in ms) and additional space (Space, in GiB) w.r.t. Trec05 and Trec06 on Gov2 and ClueWeb09.",1,ad,True
Number of query terms,0,,False
Avg Space,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
6+,0,,False
Gov2 Trec05,1,Gov,True
RankedOR 23.6 (x32.89),0,,False
WAND,0,,False
5.1 (x7.11),0,,False
MaxScore,0,,False
4.7 (x6.61),0,,False
BMW40 VBMW40 C-VBMW40 BMW128 VBMW128 C-VBMW128,0,,False
1.2 (x1.62) 0.8 (x1.10) 0.7 1.4 (x1.99) 1.0 (x1.42) 1.1 (x1.53),0,,False
76.5 (x44.39) 147.9 (x59.74) 235.4 (x60.47),0,,False
5.9 (x3.43) 7.0 (x2.82) 8.8 (x2.27),0,,False
6.0 (x3.45) 7.1 (x2.86) 9.2 (x2.37),0,,False
2.9 (x1.65) 4.3 (x1.72) 6.7 (x1.72),0,,False
1.7 (x1.01) 2.4 (x0.97) 3.8 (x0.97),0,,False
1.7,0,,False
2.5,0,,False
3.9,0,,False
3.5 (x2.01) 4.8 (x1.93) 7.2 (x1.85),0,,False
2.4 (x1.40) 3.2 (x1.30) 4.9 (x1.26),0,,False
2.5 (x1.47) 3.4 (x1.37) 5.1 (x1.32),0,,False
Gov2 Trec06,1,Gov,True
418.7 (x50.18) 106.7 (x50.88) 0.00,0,,False
17.8 (x2.13) 7.0 (x3.36) 0.22 (5%),0,,False
14.2 (x1.70) 6.6 (x3.14) 0.22 (5%),0,,False
14.8 (x1.78),0,,False
3.6,0,,False
1.55 (x1.74),0,,False
(36%),0,,False
8.1 (x0.97),0,,False
2.1,0,,False
1.55 (x1.00),0,,False
(36%),0,,False
8.3,0,,False
2.1,0,,False
0.82 (19%),0,,False
15.9 (x1.90),0,,False
4.2,0,,False
0.85 (x1.98),0,,False
(20%),0,,False
10.7 (x1.28),0,,False
2.8,0,,False
0.85 (x1.34),0,,False
(20%),0,,False
11.3 (x1.36),0,,False
3.0,0,,False
0.58 (x1.42),0,,False
(13%),0,,False
RankedOR 23.1 (x34.72),0,,False
WAND,0,,False
4.7 (x7.12),0,,False
MaxScore,0,,False
4.5 (x6.78),0,,False
BMW40 VBMW40 C-VBMW40 BMW128 VBMW128 C-VBMW128,0,,False
1.1 (x1.58) 0.8 (x1.12) 0.7 1.2 (x1.88) 0.9 (x1.39) 1.0 (x1.48),0,,False
83.3 (x42.20) 169.1 (x52.34) 261.3 (x52.20),0,,False
8.0 (x4.06) 9.3 (x2.86) 12.4 (x2.47),0,,False
7.8 (x3.97) 9.2 (x2.83) 11.7 (x2.34),0,,False
3.2 (x1.62) 5.5 (x1.70) 8.7 (x1.74),0,,False
2.0 (x1.02) 3.2 (x0.98) 4.9 (x0.98),0,,False
2.0,0,,False
3.2,0,,False
5.0,0,,False
3.9 (x1.98) 6.5 (x2.01) 10.1 (x2.03),0,,False
3.1 (x1.56) 4.7 (x1.45) 7.3 (x1.46),0,,False
3.2 (x1.64) 4.8 (x1.50) 7.6 (x1.52),0,,False
ClueWeb09 Trec05,1,ClueWeb,True
470.7 (x37.56) 212.0 (x44.41) 0.00,0,,False
26.3 (x2.10) 12.9 (x2.69) 0.22 (5%),0,,False
19.4 (x1.55) 11.3 (x2.37) 0.22 (5%),0,,False
22.1 (x1.76),0,,False
8.2,0,,False
1.55 (x1.73),0,,False
(36%),0,,False
12.3 (x0.98),0,,False
4.7,0,,False
1.55 (x0.98),0,,False
(36%),0,,False
12.5,0,,False
4.8,0,,False
0.82 (19%),0,,False
23.8 (x1.90),0,,False
9.2,0,,False
0.85 (x1.93),0,,False
(20%),0,,False
17.3 (x1.38),0,,False
6.9,0,,False
0.85 (x1.43),0,,False
(20%),0,,False
18.4 (x1.47),0,,False
7.2,0,,False
0.58 (x1.51),0,,False
(13%),0,,False
"RankedOR 77.9 (x36.01) 228.3 (x42.15) 429.3 (x55.20) 659.7 (x50.14) 1,214.0 (x41.72) 312.6 (x43.76) 0.00",0,,False
WAND,0,,False
23.8 (x10.98) 29.2 (x5.40) 25.7 (x3.31) 29.1 (x2.21) 57.1 (x1.96) 28.7 (x4.01) 0.53 (4%),0,,False
MaxScore 19.3 (x8.91) 22.9 (x4.23) 22.7 (x2.92) 28.1 (x2.14) 42.2 (x1.45) 23.4 (x3.28) 0.53 (4%),0,,False
BMW40 VBMW40 C-VBMW40 BMW128 VBMW128 C-VBMW128,0,,False
4.2 (x1.93) 2.7 (x1.23) 2.2 3.9 (x1.80) 3.1 (x1.43) 3.3 (x1.53),0,,False
10.2 (x1.89) 5.7 (x1.06) 5.4 11.2 (x2.06) 8.9 (x1.63) 9.6 (x1.77),0,,False
14.7 (x1.89) 7.8 (x1.01) 7.8 16.3 (x2.10) 12.0 (x1.55) 12.8 (x1.65),0,,False
22.5 (x1.71) 12.7 (x0.96) 13.2 25.6 (x1.94) 19.2 (x1.46) 20.4 (x1.55),0,,False
49.7 (x1.71) 27.8 (x0.96) 29.1 54.0 (x1.85) 42.2 (x1.45) 45.4 (x1.56),0,,False
12.6,0,,False
4.14 (x1.76),0,,False
(28%),0,,False
7.2,0,,False
4.14 (x1.01),0,,False
(28%),0,,False
7.1,0,,False
2.12 (14%),0,,False
13.9,0,,False
2.24 (x1.94),0,,False
(15%),0,,False
11.0,0,,False
2.24 (x1.54),0,,False
(15%),0,,False
12.0,0,,False
1.48 (x1.67),0,,False
(10%),0,,False
ClueWeb09 Trec06,1,ClueWeb,True
"RankedOR 60.6 (x33.63) 215.9 (x37.04) 439.1 (x41.46) 686.5 (x40.57) 1,270.5 (x32.81) 542.5 (x34.56) 0.00",0,,False
WAND,0,,False
14.2 (x7.86) 23.1 (x3.96) 27.3 (x2.58) 37.3 (x2.20) 73.8 (x1.91) 37.2 (x2.37) 0.53 (4%),0,,False
MaxScore 12.7 (x7.04) 21.3 (x3.66) 27.1 (x2.56) 33.9 (x2.00) 55.0 (x1.42) 32.3 (x2.06) 0.53 (4%),0,,False
BMW40 VBMW40 C-VBMW40 BMW128 VBMW128 C-VBMW128,0,,False
3.2 (x1.77) 2.1 (x1.15) 1.8 3.6 (x1.99) 2.7 (x1.49) 2.9 (x1.59),0,,False
10.0 (x1.72) 6.0 (x1.02) 5.8 12.0 (x2.06) 10.0 (x1.71) 10.6 (x1.83),0,,False
17.5 (x1.65) 10.3 (x0.97) 10.6 20.9 (x1.97) 16.8 (x1.58) 18.0 (x1.69),0,,False
28.1 (x1.66) 16.2 (x0.96) 16.9 32.5 (x1.92) 25.9 (x1.53) 28.0 (x1.65),0,,False
65.9 (x1.70) 37.0 (x0.96) 38.7 71.0 (x1.83) 56.6 (x1.46) 61.0 (x1.58),0,,False
26.3,0,,False
4.14 (x1.68),0,,False
(28%),0,,False
15.2,0,,False
4.14 (x0.96),0,,False
(28%),0,,False
15.7,0,,False
2.12 (14%),0,,False
29.4,0,,False
2.24 (x1.87),0,,False
(15%),0,,False
23.6,0,,False
2.24 (x1.50),0,,False
(15%),0,,False
25.2,0,,False
1.48 (x1.60),0,,False
(10%),0,,False
"[24] Stephen E. Robertson and Karen S. Jones. 1976. Relevance weighting of search terms. Journal of the Am. Soc. for Information science 27, 3 (1976), 129­146.",0,,False
"[25] David Salomon. 2007. Variable-length Codes for Data Compression. Springer. [26] Dongdong Shan, Shuai Ding, Jing He, Hongfei Yan, and Xiaoming Li. 2012.",0,,False
Optimized Top-k Processing with Global Page Scores on Block-max Indexes. In WSDM. 423­432. [27] Fabrizio Silvestri. 2007. Sorting Out the Document Identi er Assignment Problem. In ECIR. 101­112.,0,,False
[28] Fabrizio Silvestri and Rossano Venturini. 2010. VSEncoding: E cient Coding and Fast Decoding of Integer Lists via Dynamic Programming. In CIKM. 35­42.,0,,False
"[29] Nicola Tonello o, Craig Macdonald, and Iadh Ounis. 2013. E cient and E ective Retrieval Using Selective Pruning. In WSDM. 63­72.",1,ad,True
"[30] Howard Turtle and James Flood. 1995. ery evaluation: Strategies and optimizations. Information Processing & Management 31, 6 (1995), 831­850.",0,,False
[31] Sebastiano Vigna. 2013. asi-succinct indices. In WSDM. 83­92. [32] Justin Zobel and Alistair Mo at. 2006. Inverted les for text search engines.,0,,False
"ACM Comput. Surv. 38, 2 (2006).",0,,False
634,0,,False
,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Neural Ranking Models with Weak Supervision,0,,False
Mostafa Dehghani,0,,False
University of Amsterdam dehghani@uva.nl,0,,False
Hamed Zamani,0,,False
University of Massachuse s Amherst zamani@cs.umass.edu,0,,False
Aliaksei Severyn,0,,False
Google Research severyn@google.com,0,,False
Jaap Kamps,0,,False
University of Amsterdam kamps@uva.nl,0,,False
W. Bruce Cro,0,,False
University of Massachuse s Amherst cro @cs.umass.edu,0,,False
ABSTRACT,0,,False
"Despite the impressive improvements achieved by unsupervised deep neural networks in computer vision and NLP tasks, such improvements have not yet been observed in ranking for information retrieval. e reason may be the complexity of the ranking problem, as it is not obvious how to learn from queries and documents when no supervised signal is available. Hence, in this paper, we propose to train a neural ranking model using weak supervision, where labels are obtained automatically without human annotators or any external resources (e.g., click data). To this aim, we use the output of an unsupervised ranking model, such as BM25, as a weak supervision signal. We further train a set of simple yet e ective ranking models based on feed-forward neural networks. We study their e ectiveness under various learning scenarios (point-wise and pair-wise models) and using di erent input representations (i.e., from encoding querydocument pairs into dense/sparse vectors to using word embedding representation). We train our networks using tens of millions of training instances and evaluate it on two standard collections: a homogeneous news collection (Robust) and a heterogeneous large-scale web collection (ClueWeb). Our experiments indicate that employing proper objective functions and le ing the networks to learn the input representation based on weakly supervised data leads to impressive performance, with over 13% and 35% MAP improvements over the BM25 model on the Robust and the ClueWeb collections. Our ndings also suggest that supervised neural ranking models can greatly bene t from pre-training on large amounts of weakly labeled data that can be easily obtained from unsupervised IR models.",1,Robust,True
KEYWORDS Ranking model; weak supervision; deep neural network; deep learning; ad-hoc retrieval,1,ad-hoc,True
1 INTRODUCTION,1,DUC,True
"Learning state-of-the-art deep neural network models requires a large amounts of labeled data, which is not always readily available and can be expensive to obtain. To circumvent the lack of humanlabeled training examples, unsupervised learning methods aim to model the underlying data distribution, thus learning powerful feature representations of the input data, which can be helpful for",1,ad,True
Work done while interning at Google Research.,0,,False
This work is licensed under a Creative Commons Attribution International 4.0 License.,0,,False
building more accurate discriminative models especially when li le or even no supervised data is available.,0,,False
"A large group of unsupervised neural models seeks to exploit the implicit internal structure of the input data, which in turn requires customized formulation of the training objective (loss function), targeted network architectures and o en non-trivial training setups. For example in NLP, various methods for learning distributed word representations, e.g., word2vec [27], GloVe [31], and sentence representations, e.g., paragraph vectors [23] and skip-thought [22] have been shown very useful to pre-train word embeddings that are then used for other tasks such as sentence classi cation, sentiment analysis, etc. Other generative approaches such as language modeling in NLP, and, more recently, various avors of auto-encoders [2] and generative adversarial networks [13] in computer vision have shown a promise in building more accurate models.",1,ad,True
"Despite the advances in computer vision, speech recognition, and NLP tasks using unsupervised deep neural networks, such advances have not been observed in core information retrieval (IR) problems, such as ranking. A plausible explanation is the complexity of the ranking problem in IR, in the sense that it is not obvious how to learn a ranking model from queries and documents when no supervision in form of the relevance information is available. To overcome this issue, in this paper, we propose to leverage large amounts of unsupervised data to infer ""noisy"" or ""weak"" labels and use that signal for learning supervised models as if we had the ground truth labels. In particular, we use classic unsupervised IR models as a weak supervision signal for training deep neural ranking models. Weak supervision here refers to a learning approach that creates its own training data by heuristically retrieving documents for a large query set. is training data is created automatically, and thus it is possible to generate billions of training instances with almost no cost.1 As training deep neural networks is an exceptionally data hungry process, the idea of pre-training on massive amount of weakly supervised data and then ne-tuning the model using a small amount of supervised data could improve the performance [11].",1,ad,True
"e main aim of this paper is to study the impact of weak supervision on neural ranking models, which we break down into the following concrete research questions:",0,,False
RQ1 Can labels from an unsupervised IR model such as BM25 be,0,,False
used as weak supervision signal to train an e ective neural,0,,False
ranker?,0,,False
RQ2 What input representation and learning objective is most suit-,0,,False
able for learning in such a se ing?,0,,False
"SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan",0,,False
© 2017 Copyright held by the owner/author(s). 978-1-4503-5022-8/17/08. DOI: 10.1145/3077136.3080832,0,,False
"1Although weak supervision may refer to using noisy data, in this paper, we assume that no external information, e.g., click-through data, is available.",0,,False
65,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
RQ3 Can a supervised learning model bene t from a weak super-,0,,False
"vision step, especially in cases when labeled data is limited?",0,,False
"We examine various neural ranking models with di erent ranking architectures and objectives, i.e., point-wise and pair-wise, as well as di erent input representations, from encoding query-document pairs into dense/sparse vectors to learning query/document embedding representations. e models are trained on billions of training examples that are annotated by BM25, as the weak supervision signal. Interestingly, we observe that using just training data that are annotated by BM25 as the weak annotator, we can outperform BM25 itself on the test data. Based on our analysis, the achieved performance is generally indebted to three main factors: First, de ning an objective function that aims to learn the ranking instead of calibrated scoring to relax the network from ing to the imperfections in the weakly supervised training data. Second, le ing the neural networks learn optimal query/document representations instead of feeding them with a representation based on prede ned features. is is a key requirement to maximize the bene ts from deep learning models with weak supervision as it enables them to generalize be er. ird and last, the weak supervision se ing makes it possible to train the network on a massive amount of training data.",1,ad,True
"We further thoroughly analyse the behavior of models to understand what they learn, what is the relationship among di erent models, and how much training data is needed to go beyond the weak supervision signal. We also study if employing deep neural networks may help in di erent situations. Finally, we examine the scenario of using the network trained on a weak supervision signal as a pre-training step. We demonstrate that, in the ranking problem, the performance of deep neural networks trained on a limited amount of supervised data signi cantly improves when they are initialized from a model pre-trained on weakly labeled data.",0,,False
"Our results have broad impact as the proposal to use unsupervised traditional methods as weak supervision signals is applicable to variety of IR tasks, such as ltering or classi cation, without the need for supervised data. More generally, our approach uni es the classic IR models with currently emerging data-driven approaches in an elegant way.",1,ad,True
2 RELATED WORK,0,,False
"Deep neural networks have shown impressive performance in many computer vision, natural language processing, and speech recognition tasks [24]. Recently, several a empts have been made to study deep neural networks in IR applications, which can be generally partitioned into two categories [29, 46]. e rst category includes approaches that use the results of trained (deep) neural networks in order to improve the performance in IR applications. Among these, distributed word representations or embeddings [27, 31] have a racted a lot of a ention. Word embedding vectors have been applied to term re-weighting in IR models [32, 47], query expansion [10, 33, 43], query classi cation [25, 44], etc. e main shortcoming of most of the approaches in this category is that the objective of the trained neural network di ers from the objective of these tasks. For instance, the word embedding vectors proposed in [27, 31] are trained based on term proximity in a large corpus, which is di erent from the objective in most IR tasks. To overcome this issue, some approaches try to learn representations in an end-to-end neural model for learning a speci c task like entity ranking for expert nding [39] or product search [38]. Zamani and Cro [45] recently proposed",1,ad,True
relevance-based word embedding models for learning word representations based on the objectives that ma er for IR applications.,0,,False
"e second category, which this paper belongs to, consists of the approaches that design and train a (deep) neural network for a speci c task, e.g., question answering [6, 41], click models [4], context-aware ranking [42], etc. A number of the approaches in this category have been proposed for ranking documents in response to a given query. ese approaches can be generally divided into two groups: late combination models and early combination models (or representation-focused and interaction-focused models according to [14]). e late combination models, following the idea of Siamese networks [5], independently learn a representation for each query and candidate document and then calculate the similarity between the two estimated representations via a similarity function. For example, Huang et al. [18] proposed DSSM, which is a feed forward neural network with a word hashing phase as the rst layer to predict the click probability given a query string and a document title. e DSSM model was further improved by incorporating convolutional neural networks [35].",1,corpora,True
"On the other hand, the early combination models are designed based on the interactions between the query and the candidate document as the input of network. For instance, DeepMatch [26] maps each text to a sequence of terms and trains a feed-forward network for computing the matching score. e deep relevance matching model for ad-hoc retrieval [14] is another example of an early combination model that feeds a neural network with the histogram-based features representing interactions between the query and document. Early combining enables the model to have an opportunity to capture various interactions between query and document(s), while with late combination approach, the model has only the chance of isolated observation of input elements. Recently, Mitra et al. [28] proposed to simultaneously learn local and distributional representations, which are early and late combination models respectively, to capture both exact term matching and semantic term matching.",1,ad-hoc,True
"Until now, all the proposed neural models for ranking are trained on either explicit relevance judgements or clickthrough logs. However, a massive amount of such training data is not always available.",0,,False
"In this paper, we propose to train neural ranking models using weak supervision, which is the most natural way to reuse the existing supervised learning models where the imperfect labels are treated as the ground truth. e basic assumption is that we can cheaply obtain labels (that are of lower quality than human-provided labels) by expressing the prior knowledge we have about the task at hand by specifying a set of heuristics, adapting existing ground truth data for a di erent but related task (this is o en referred to distant supervision2), extracting supervision signal from external knowledge-bases or ontologies, crowd-sourcing partial annotations that are cheaper to get, etc. Weak supervision is a natural way to bene t from unsupervised data and it has been applied in NLP for various tasks including relation extraction [3, 15], knowledge-base completion [17], sentiment analysis [34], etc. ere are also similar a empts in IR for automatically constructing test collections [1] and learning to rank using labeled features, i.e. features that an expert believes they are correlated with relevance [9]. In this paper, we make use of traditional IR models as the weak supervision signal to generate a large",1,ad,True
2We do not distinguish between weak and distant supervision as the di erence is subtle and both terms are o en used interchangeably in the literature.,0,,False
66,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
4.1 Ranking Architectures,0,,False
We de ne three di erent ranking models: one point-wise and two pair-wise models. We introduce the architecture of these models and explain how we train them using weak supervision signals.,0,,False
(a) Score model,0,,False
(b) Rank model,0,,False
(c) RankProb model,0,,False
Figure 1: Di erent Ranking Architectures,0,,False
amount of training data and train e ective neural ranking models that outperform the baseline methods by a signi cant margin.,0,,False
3 WEAK SUPERVISION FOR RANKING,0,,False
"Deep learning techniques have taken o in many elds, as they automate the onerous task of input representation and feature engineering. On the other hand, the more the neural networks become deep and complex, the more it is crucial for them to be trained on massive amounts of training data. In many applications, rich annotations are costly to obtain and task-speci c training data is now a critical bo leneck. Hence, unsupervised learning is considered as a long standing goal for several applications. However, in a number of information retrieval tasks, such as ranking, it is not obvious how to train a model with large numbers of queries and documents with no relevance signal. To address this problem in an unsupervised fashion, we use the idea of ""Pseudo-Labeling"" by taking advantage of existing unsupervised methods for creating a weakly annotated set of training data and we propose to train a neural retrieval model with weak supervision signals we have generated. In general, weak supervision refers to learning from training data in which the labels are imprecise. In this paper, we refer to weak supervision as a learning approach that automatically creates its own training data using an existing unsupervised approach, which di ers from imprecise data coming from external observations (e.g., click-through data) or noisy human-labeled data.",1,ad,True
"We focus on query-dependent ranking as a core IR task. To this aim, we take a well-performing existing unsupervised retrieval model, such as BM25. is model plays the role of ""pseudo-labeler"" in our learning scenario. In more detail, given a target collection and a large set of training queries (without relevance judgments), we make use of the pseudo-labeler to rank/score the documents for each query in the training query set. Note that we can generate as much as training data as we need with almost no cost. e goal is to train a ranking model given the scores/ranking generated by the pseudo-labeler as a weak supervision signal.",0,,False
"In the following section, we formally present a set of neural network-based ranking models that can leverage the given weak supervision signal in order to learn accurate representations and ranking for the ad-hoc retrieval task.",1,ad-hoc,True
4 NEURAL RANKING MODELS,0,,False
"In this section, we rst introduce our ranking models. en, we describe the architecture of the base neural network model shared by di erent ranking models. Finally, we discuss the three input layer architectures used in our neural rankers to encode (query, candidate document) pairs.",0,,False
"Score model : is architecture models a point-wise ranking model that learns to predict retrieval scores for query-document pairs. More formally, the goal in this architecture is to learn a scoring function S(q,d; ) that determines the retrieval score of document d for query q, given a set of model parameters  . In the training stage, we are given a training set comprising of training instances each a triple  ,"" (q,d,sq,d ), where q is a query from training query set Q, d represents a retrieved document for the query q, and sq,d is the relevance score (calculated by a weak supervisor), which is acquired using a retrieval scoring function in our setup. We consider the mean squared error as the loss function for a given batch of training instances:""",0,,False
L,0,,False
(b;,0,,False
),0,,False
",",0,,False
1 |b |,0,,False
"|b | i ,1",0,,False
(S,0,,False
"({q,d",0,,False
}i,0,,False
;,0,,False
),0,,False
-s,0,,False
"{q,d",0,,False
}i,0,,False
)2,0,,False
(1),0,,False
"where {q,d }i denotes the query and the corresponding retrieved document in the ith training instance, i.e. i in the batch b. e",0,,False
conceptual architecture of the model is illustrated in Figure 1a.,0,,False
"Rank model : In this model, similar to the previous one, the goal is to learn a scoring function S(q,d; ) for a given pair of query q and document d with the set of model parameters  . However, unlike the previous model, we do not aim to learn a calibrated scoring function. In this model, as it is depicted in Figure 1b, we use a pair-wise scenario during training in which we have two point-wise networks that share parameters and we update their parameters to minimize a pair-wise loss. In this model, each training instance has ve elements:  ,"" (q,d1,d2,sq,d1 ,sq,d2 ). During the inference, we treat the trained model as a point-wise scoring function to score query-document pairs.""",0,,False
"We have tried di erent pair-wise loss functions and empirically found that the model learned based on the hinge loss (max-margin loss function) performs be er than the others. Hinge loss is a linear loss that penalizes examples that violate the margin constraint. It is widely used in various learning to rank algorithms, such as Ranking SVM [16]. e hinge loss function for a batch of training instances is de ned as follows:",0,,False
L,0,,False
(b;,0,,False
),0,,False
",",0,,False
1 |b |,0,,False
|b |,0,,False
max,0,,False
"i ,1",0,,False
"0, -sign(s {q,d1 }i -s {q,d2 }i )",0,,False
(2),0,,False
"(S({q,d1}i ; ) -S({q,d2}i ; )) ,",0,,False
"where  is the parameter determining the margin of hinge loss. We found that as we compress the outputs to the range of [-1,1],  , 1 works well as the margin for the hinge loss function.",0,,False
RankProb model : e third architecture is based on a pair-wise,0,,False
scenario during both training and inference (Figure 1c). is model,0,,False
"learns a ranking function R (q,d1,d2; ) which predicts the probability of document d1 to be ranked higher than d2 given q. Similar to the rank model, each training instance has ve elements:",0,,False
" ,"" (q,d1,d2,sq,d1 ,sq,d1 ). For a given batch of training instances, we""",0,,False
67,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
de ne our loss function based on cross-entropy as follows:,0,,False
L,0,,False
(b,0,,False
;,0,,False
),0,,False
",",0,,False
-,0,,False
1 |b |,0,,False
|b |,0,,False
P,0,,False
"i ,1",0,,False
"{q,d1,",0,,False
d2,0,,False
}i,0,,False
log(R,0,,False
(,0,,False
"{q,d",0,,False
"1,d2",0,,False
}i,0,,False
;,0,,False
),0,,False
),0,,False
(3),0,,False
"+ (1-P {q,d1,d2 }i )log(1-R ({q,d1,d2}i ; ))",0,,False
"where P {q,d1,d2 }i is the probability of document d1 being ranked",0,,False
"higher thand2, based on the scores obtained from training instancei :",0,,False
"P {q,d1,d2",0,,False
}i,0,,False
",",0,,False
"s {q,d1 }i s {q,d1 }i +s {q,d2",0,,False
}i,0,,False
(4),0,,False
"It is notable that at inference time, we need a scalar score for",0,,False
"each document. erefore, we need to turn the model's pair-wise",0,,False
"predictions into a score per document. To do so, for each document,",0,,False
"we calculate the average of predictions against all other candidate documents, which has O (n2) time complexity and is not practical",0,,False
in real-world applications. ere are some approximations could be,0,,False
applicable to decrease the time complexity at inference time [40].,0,,False
4.2 Neural Network Architecture,0,,False
"As shown in Figure 1, all the described ranking architectures share a",0,,False
neural network module. We opted for a simple feed-forward neural,0,,False
"network which is composed of: input layer z0, l -1 hidden layers, and the output layer zl . e input layer z0 provides a mapping  to encode the input query and document(s) into a xed-length",0,,False
vector. e exact speci cation of the input representation feature,0,,False
function  is given in the next subsection. Each hidden layer zi is a fully-connected layer that computes the following transformation:,0,,False
"zi ,"" (Wi .zi-1 +bi ); 1 <i <l -1,""",0,,False
(5),0,,False
"where Wi and bi respectively denote the weight matrix and the bias term corresponding to the ith hidden layer, and  (.) is the activation function. We use the recti er linear unit ReLU(x ) ,"" max(0,x ) as the activation function, which is a common choice in the deep learning literature [24]. e output layer zl is a fully-connected layer with a single continuous output. e activation function for the output layer depends on the ranking architecture that we use. For the score model architecture, we empirically found that a linear activation function works best, while tanh and the sigmoid functions are used for the rank model and rankprob model respectively.""",0,,False
"Furthermore, to prevent feature co-adaptation, we use dropout [36] as the regularization technique in all the models. Dropout sets a portion of hidden units to zero during the forward phase when computing the activations which prevents over ing.",1,ad,True
4.3 Input Representations,0,,False
"We explore three de nitions of the input layer representation z0 captured by a feature function  that maps the input into a xedsize vector which is further fed into the fully connected layers: (i) a conventional dense feature vector representation that contains various statistics describing the input query-document pair, (ii) a sparse vector containing bag-of-words representation, and (iii) bagof-embeddings averaged with learned weights. ese input representations de ne how much capacity is given to the network to extract discriminative signal from the training data and thus result in di erent generalization behavior of the networks. It is noteworthy that input representation of the networks in the score model and rank model is de ned for a pair of the query and the document, while",0,,False
"the network in the rankprob model needs to be fed by a triple of the query, the rst document, and the second document.",0,,False
"Dense vector representation (Dense) : In this se ing, we build a dense feature vector composed of features used by traditional IR methods, e.g., BM25. e goal here is to let the network t the function described by the BM25 formula when it receives exactly the same inputs. In more detail, our input vector is a concatenation (||) of the following inputs: total number of documents in the collection (i.e., N ), average length of documents in the collection (i.e., a (ld )D ), document length (i.e., ld ), frequency of each query term ti in the document (i.e., t f (ti ,d )), and document frequency of each query term (i.e., d f (ti )). erefore, for the point-wise se ing, we have the following input vector:",1,ad,True
" (q,d ) ,"" [N ||a (ld )D ||ld ||{d f (ti )||t f (ti ,d )}1i k ], (6)""",0,,False
"where k is set to a xed value (5 in our experiments). We truncate longer queries and do zero padding for shorter queries. For the networks in the rankprob model, we consider a similar function with additional elements: the length of the second document and the frequency of query terms in the second document.",1,ad,True
"Sparse vector representation (Sparse) : Next, we move away",0,,False
from a fully featurized representation that contains only aggregated,0,,False
statistics and let the network performs feature extraction for us. In,0,,False
"particular, we build a bag-of-words representation by extracting",0,,False
"term frequency vectors of query (t f q ), document (t f d ), and the collection (t f c ) and feed the network with concatenation of these",0,,False
"three vectors. For the point-wise se ing, we have the following",0,,False
input vector:,0,,False
" (q,d ) , [t f c ||t f q ||t f d ]",0,,False
(7),0,,False
"For the network in rankprob model, we have a similar input vector",0,,False
"with both t f d1 and t f d2 . Hence, the size of the input layer is 3 × ocab size in the point-wise se ing, and 4 × ocab size in the",0,,False
pair-wise se ing.,0,,False
"Embedding vector representation (Embed) : e major weakness of the previous input representation is that words are treated as discrete units, hence prohibiting the network from performing so matching between semantically similar words in queries and documents. In this input representation paradigm, we rely on word embeddings to obtain more powerful representation of queries and documents that could bridge the lexical chasm. e representation function  consists of three components: an embedding function E : V  Rm (where V denotes the vocabulary set and m is the embedding dimension), a weighting function W : V  R, and a compositionality function : (Rm,R)n  Rm . More formally, the function  for the point-wise se ing is de ned as:",1,ad,True
" (q,d ) , [",0,,False
"|q | i ,1",0,,False
(E,0,,False
(tiq,0,,False
"),W",0,,False
(tiq,0,,False
),0,,False
),0,,False
|,0,,False
|,0,,False
"|d | i ,1",0,,False
(E,0,,False
(tid,0,,False
"),W",0,,False
(tid,0,,False
),0,,False
")],",0,,False
(8),0,,False
"where tiq and tid denote the ith term in query q and document d, respectively. For the network of the rankprob model, another similar",0,,False
term is concatenated with the above vector for the second docu-,0,,False
ment. e embedding function E transforms each term to a dense,0,,False
"m-dimensional oat vector as its representation, which is learned",0,,False
during the training phase. e weighting function W assigns a,0,,False
"weight to each term in the vocabulary set, which is supposed to learn",0,,False
term global importance for the retrieval task. e compositionality,0,,False
function projects a set of n embedding and weighting pairs to an,0,,False
68,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"m-dimensional representation, independent from the value of n. e compositionality function is given by:",0,,False
n,0,,False
"n i ,1",0,,False
(E,0,,False
(ti,0,,False
"),W",0,,False
(ti,0,,False
),0,,False
),0,,False
",",0,,False
"W (ti ) ·E (ti ),",0,,False
(9),0,,False
"i ,1",0,,False
which is the weighted element-wise sum of the terms' embedding,0,,False
"vectors. W is the normalized weight that is learned for each term, given as follows:",0,,False
"W (ti ) ,",0,,False
"exp(W (ti )) nj,1exp(W (tj ))",0,,False
(10),0,,False
All combinations of di erent ranking architectures and di erent input representations presented in this section can be considered for developing ranking models.,0,,False
5 EXPERIMENTAL DESIGN,0,,False
"In this section, we describe the train and evaluation data, metrics we report, and detailed experimental setup. en we discuss the results.",0,,False
5.1 Data,0,,False
"Collections. In our experiments, we used two standard TREC collections: e rst collection (called Robust04) consists of over 500k news articles from di erent news agencies, that is available in TREC Disks 4 and 5 (excluding Congressional Records). is collection, which was used in TREC Robust Track 2004, is considered as a homogeneous collection, because of the nature and the quality of documents.",1,TREC,True
"e second collection (called ClueWeb) that we used is ClueWeb09 Category B, a large-scale web collection with over 50 million English documents, which is considered as a heterogeneous collection. is collection has been used in TREC Web Track, for several years. In our experiments with this collection, we ltered out the spam documents using the Waterloo spam scorer3 [7] with the default threshold 70%.",1,ClueWeb,True
e statistics of these collections are reported in Table 1.,0,,False
"Training query set. To train our neural ranking models, we used the unique queries (only the query string) appearing in the AOL query logs [30]. is query set contains web queries initiated by real users in the AOL search engine that were sampled from a threemonth period from March 1, 2006 to May 31, 2006. We ltered out a large volume of navigational queries containing URL substrings (""h p"", ""www."", "".com"", "".net"", "".org"", "".edu""). We also removed all non-alphanumeric characters from the queries. We made sure that no queries from the training set appear in our evaluation sets. For each dataset, we took queries that have at least ten hits in the target corpus using the pseudo-labeler method. Applying all these processes, we ended up with 6.15 million queries for the Robust04 dataset and 6.87 million queries for the ClueWeb dataset. In our experiments, we randomly selected 80% of the training queries as training set and the remaining 20% of the queries were chosen as validation set for hyper-parameter tuning. As the ""pseudo-labeler"" in our training data, we have used BM25 to score/rank documents in the collections given the queries in the training query set.",1,ad,True
Evaluation query sets. We use the following query sets for evaluation that contain human-labeled judgements: a set of 250 queries (TREC topics 301­450 and 601­700) for the Robust04 collection that were previously used in TREC Robust Track 2004. A set of 200 queries,1,TREC,True
3h p://plg.uwaterloo.ca/gvcormac/clueweb09spam/,0,,False
Table 1: Collections statistics.,0,,False
Collection,0,,False
Robust04 ClueWeb,1,Robust,True
Genre,0,,False
news webpages,0,,False
eries,0,,False
"301-450,601-700 1-200",0,,False
# docs,0,,False
528k 50m,0,,False
length,0,,False
"254 1,506",0,,False
(topics 1-200) were used for the experiments on the ClueWeb collection. ese queries were used in TREC Web Track 2009­2012. We only used the title of topics as queries.,1,ClueWeb,True
5.2 Evaluation Metrics.,0,,False
"To evaluate retrieval e ectiveness, we report three standard evaluation metrics: mean average precision (MAP) of the top-ranked 1000 documents, precision of the top 20 retrieved documents (P@20), and normalized discounted cumulative gain (nDCG) [19] calculated for the top 20 retrieved documents (nDCG@20). Statistically signi cant di erences of MAP, P@20, and nDCG@20 values are determined using the two-tailed paired t-test with p alue < 0.05, with Bonferroni correction.",1,MAP,True
5.3 Experimental Setup,0,,False
"All models described in Section 4 are implemented using TensorFlow [12, 37]. In all experiments, the parameters of the network are optimized employing the Adam optimizer [21] and using the computed gradient of the loss to perform the back-propagation algorithm. All model hyper-parameters were tuned on the respective validation set (see Section 5.1 for more detail) using batched GP bandits with an expected improvement acquisition function [8]. For each model, the size of hidden layers and the number of hidden layers were selected from [16,32,64,128,256,512,1024] and [1,2,3,4], respectively. e initial learning rate and the dropout parameter were selected from [1E -3,5E -4,1E -4,5E -5,1E -5] and [0.0,0.1,0.2,0.5], respectively. For models with embedding vector representation, we considered embedding sizes of [100,300,500,1000]. As the training data, we take the top 1000 retrieved documents for each query from training query set Q, to prepare the training data. In total, we have |Q |×1000 ( 6E10 examples in our data) point-wise example and  |Q |×10002 ( 6E13 examples in our data) pair-wise examples. e batch size in our experiments was selected from [128,256,512]. At inference time, for each query, we take the top 2000 retrieved documents using BM25 as candidate documents and re-rank them by the trained models. In our experiments, we use the Indri4 implementation of BM25 with the default parameters (i.e., k1 ,"" 1.2, b "","" 0.75, and k3 "", 1000).",1,ad,True
6 RESULTS AND DISCUSSION,0,,False
"In the following, we evaluate our neural rankers trained with di erent learning approaches (Section 4) and di erent input representations (Section 4.3). We a empt to break down our research questions to several subquestions, and provide empirical answers along with the intuition and analysis behind each question:",0,,False
"How do the neural models with di erent training objectives and input representations compare? Table 2 presents the performance of all model combinations. Interestingly, combinations of the rank model and the rankprob model with embedding vector representation outperform BM25 by signi cant margins in both collections. For instance, the rankprob model with embedding vector representation that shows the best performance among the other methods,",0,,False
4h ps://www.lemurproject.org/indri.php,0,,False
69,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 2: Performance of the di erent models on di erent datasets. or indicates that the improvements or degradations with respect to BM25 are statistically signi cant, at the 0.05 level using the paired two-tailed t-test.",1,ad,True
Method,0,,False
BM25,0,,False
Score + Dense Score + Sparse Score + Embed,0,,False
Rank + Dense Rank + Sparse Rank + Embed,0,,False
RankProb + Dense RankProb + Sparse RankProb + Embed,0,,False
MAP,1,MAP,True
0.2503,0,,False
0.1961 0.2141 0.2423,0,,False
0.1940 0.2213 0.2811,0,,False
0.2192 0.2246 0.2837,0,,False
Robust04,1,Robust,True
P@20 nDCG@20,0,,False
0.3569 0.4102,0,,False
0.2787 0.3180,0,,False
0.3501,0,,False
0.3260 0.3604,0,,False
0.3999,0,,False
0.2830 0.3216 0.3773,0,,False
0.3317 0.3628 0.4302,0,,False
0.2966 0.3250 0.3802,0,,False
0.3278 0.3763 0.4389,0,,False
MAP,1,MAP,True
0.1021,0,,False
0.0689 0.0701 0.1002,0,,False
0.0622 0.0776 0.1306,0,,False
0.0702 0.0894 0.1387,0,,False
ClueWeb,1,ClueWeb,True
P@20 nDCG@20,0,,False
0.2418 0.2070,0,,False
0.1518 0.1889,0,,False
0.2513,0,,False
0.1430 0.1495,0,,False
0.2130,0,,False
0.1516 0.1989 0.2839,0,,False
0.1383 0.1816 0.2216,0,,False
0.1711 0.2109 0.2967,0,,False
0.1506,0,,False
0.1916 0.2330,0,,False
"surprisingly, improves BM25 by over 13% and 35% in Robust04 and ClueWeb collections respectively, in terms of MAP. Similar improvements can be observed for the other evaluation metrics.",1,Robust,True
"Regarding the modeling architecture, in the rank model and the rankprob model, compared to the score model, we de ne objective functions that target to learn ranking instead of scoring. is is particularly important in weak supervision, as the scores are imperfect values--using the ranking objective alleviates this issue by forcing the model to learn a preference function rather than reproduce absolute scores. In other words, using the ranking objective instead of learning to predict calibrated scores allows the rank model and the rankprob model to learn to distinguish between examples whose scores are close. is way, some small amount of noise, which is a common problem in weak supervision, would not perturb the ranking as easily.",1,ad,True
"Regarding the input representations, embedding vector representation leads to be er performance compared to the other ones in all models. Using embedding vector representation not only provides the network with more information, but also lets the network to learn proper representation capturing the needed elements for the next layers with be er understanding of the interactions between query and documents. Providing the network with already engineered features would block it from going beyond the weak supervision signal and limit the ability of the models to learn latent features that are una ainable through feature engineering.",1,ad,True
"Note that although the rankprob model is more precise in terms of MAP, the rank model is much faster in the inference time (O (n) compared toO (n2)), which is a desirable property in real-life applications.",1,MAP,True
"Why do dense vector representation and sparse vector representation fail to replicate the performance of BM25? Although neural networks are capable of approximating arbitrarily complex non-linear functions, we observe that the models with dense vector representation fail to replicate the BM25 performance, while they are given the same feature inputs as the BM25 components (e.g., TF, IDF, average document length, etc). To ensure that the training converges and there is no over ing, we have looked into the training and validation loss values of di erent models during the training time. Figure 2 illustrates the loss curves for the training and validation sets (see Section 5.1) per training step for di erent models. As shown, in models with dense vector representation, the training",0,,False
"losses drop quickly to values close to zero while this is not the case for the validation losses, which is an indicator of over- ing on the training data. Although we have tried di erent regularization techniques, like l2-regularization and dropout with various parameters, there is less chance for generalization when the networks are fed with the fully featurized input. Note that over- ing would lead to poor performance, especially in weak supervision scenarios as the network learns to model imperfections from weak annotations. is phenomenon is also the case for models with the sparse vector representation, but with less impact. However, in the models with the embedding vector representation, the networks do not over t, which helps it to go beyond the weak supervision signals in the training data.",1,ad,True
"How are the models related? To be er understand the relationship of di erent neural models described above, we compare their performance across the query dimension following the approach in [28]. We assume that similar models should perform similarly for the same queries. Hence, we represent each model by a vector, called the performance vector, whose elements correspond to per query performance of the model, in terms of nDCG@20. e closer the performance vectors are, the more similar the models are in terms of query by query performance. For the sake of visualization, we reduce the vectors dimension by projecting them to a two-dimensional space, using t-Distributed Stochastic Neighbor Embedding (t-SNE)5.",0,,False
"Figure 3 illustrates the proximity of di erent models in the Robust04 collection. Based on this plot, models with similar input representations (same color) have quite close performance vectors, which means that they perform similarly for same queries. is is not necessarily the case for models with similar architecture (same shape). is suggests that the amount and the way that we provide information to the networks are the key factors in the ranking performance.",1,Robust,True
We also observe that the score model with dense vector representation is the closest to BM25 which is expected. It is also interesting that models with embedding vector representation are placed far away from other models which shows they perform di erently compared to the other input representations.,0,,False
"How meaningful are the compositionality weights learned in the embedding vector representation? In this experiment, we",0,,False
5h ps://lvdmaaten.github.io/tsne/,0,,False
70,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
(a) Score-Dense,0,,False
(b) Score-Sparse,0,,False
(c) Score-Embed,0,,False
(d) Rank-Dense,0,,False
(e) Rank-Sparse,0,,False
(f) Rank-Embed,0,,False
(g) RankProb-Dense,0,,False
(h) RankProb-Sparse,0,,False
(i) RankProb-Embed,0,,False
Figure 2: Training and validation loss curves for all combinations of di erent ranking architectures and feeding paradigms.,1,ad,True
Score + Dense BM25,0,,False
Rank + Dense RankProb + Dense Rank + Sparse,0,,False
RankProb + Sparse Score + Embed RankProb + Embed,0,,False
Score + Sparse,0,,False
Rank + Embed,0,,False
Figure 3: Proximity of di erent models in terms of query-,0,,False
by-query performance.,0,,False
"focus on the best performing combination, i.e., the rankprob model with embedding vector representation. To analyze what the network learns, we look into the weights W (see Section 4.3) learned by the network. Note that the weighting function W learns a global weight for each vocabulary term. We notice that in both collections there is a strong linear correlation between the learned weights and the inverse document frequency of terms. Figure 4 illustrates the sca er plots of the learned weight for each vocabulary term and its IDF, in both collections. is is an interesting observation as we do not provide any global corpus information to the network in training",0,,False
(a) Robust04,1,Robust,True
(b) ClueWeb,1,ClueWeb,True
(Pearson Correlation: 0.8243),0,,False
(Pearson Correlation: 0.7014),0,,False
Figure 4: Strong linear correlation between weight learned by the compositionality function in the embedding vector representation and inverse document frequency.,0,,False
and the network is able to infer such a global information by only observing individual training instances.,0,,False
"How well do other alternatives for the embedding and weighting functions in the embedding vector representation perform? Considering embedding vector representation as the input representation, we have examined di erent alternatives for the embedding",0,,False
71,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 3: Performance of the rankprob model with variants of the embedding vector representation on di erent datasets. indicates that the improvements over all other models are statistically signi cant, at the 0.05 level using the paired two-tailed",0,,False
"t-test, with Bonferroni correction.",0,,False
Embedding type,0,,False
Pretrained (external) + Uniform weighting Pretrained (external) + IDF weighting Pretrained (external) + Weight learning Pretrained (target) + Uniform weighting Pretrained (target) + IDF weighting Pretrained (target) + Weight learning Learned + Uniform weighting Learned + IDF weighting Learned + Weight learning,0,,False
MAP,1,MAP,True
0.1656 0.1711 0.1880 0.1217 0.1402 0.1477 0.2612 0.2676 0.2837,0,,False
Robust04,1,Robust,True
P@20 nDCG@20,0,,False
0.2543 0.2755 0.2890 0.2009 0.2230 0.2266 0.3602 0.3619 0.3802,0,,False
0.3017 0.3104 0.3413 0.2791 0.2876 0.2804 0.4180 0.4200 0.4389,0,,False
MAP,1,MAP,True
0.0612 0.0712 0.0756 0.0679 0.0779 0.0816 0.0912 0.1032 0.1387,0,,False
ClueWeb,1,ClueWeb,True
P@20 nDCG@20,0,,False
0.1300 0.1346 0.1344 0.1331 0.1674 0.1729 0.2216 0.2419 0.2967,0,,False
0.1401 0.1469 0.1583 0.1587 0.1540 0.1608 0.1841 0.1922 0.2330,0,,False
"function E: (1) employing pre-trained word embeddings learned from an external corpus (we used Google News), (2) employing pretrained word embeddings learned from the target corpus (using the skip-gram model [27]), and (3) learning embeddings during the network training as it is explained in Section 4.3. Furthermore, for the compositionality function , we tried di erent alternatives: (1) uniform weighting (simple averaging which is a common approach in compositionality function), (2) using IDF as xed weights instead of learning the weighting function W, and (3) learning weights during the training as described in Section 4.3.",1,ad,True
Table 3 presents the performance of all these combinations on both collections. We note that learning both embedding and weighting functions leads to the highest performance in both collections.,1,ad,True
"ese improvements are statistically signi cant. According to the results, regardless of the weighting approach, learning embeddings during training outperforms the models with xed pre-trained embeddings. is supports the hypothesis that with the embedding vector representation the neural networks learn an embedding that is based on the interactions of query and documents that tends to be tuned be er to the corresponding ranking task. Also, regardless of the embedding method, learning weights helps models to get be er performance compared to the xed weightings, with either IDF or uniform weights. Although weight learning can signi cantly a ect the performance, it has less impact than learning embeddings.",0,,False
"Note that in the models with pre-trained word embeddings, employing word embeddings trained on the target collection outperforms those trained on the external corpus in the ClueWeb collection; while this is not the case for the Robust04 collection. e reason could be related to the collection size, since the ClueWeb is approximately 100 times larger than the Robust04.",1,ClueWeb,True
"In addition to the aforementioned experiments, we have also tried initializing the embedding matrix with a pre-trained word embedding trained on the Google News corpus, instead of random initialization. Figure 5 presents the learning curve of the models. According to this gure, the model initialized by a pre-trained embedding performs be er than random initialization when a limited amount of training data is available. When enough training data is fed to the network, initializing with pre-trained embedding and random values converge to the same performance. An interesting observation here is that in both collections, these two initializations converge when the models exceed the performance of the weak",1,ad,True
(a) Robust04,1,Robust,True
(b) ClueWeb,1,ClueWeb,True
Figure 5: Performance of the rankprob model with learned,0,,False
"embedding, pre-trained embedding, and learned embedding",0,,False
"with pre-trained embedding as initialization, with respect to",0,,False
di erent amount of training data.,0,,False
"supervision source, which is BM25 in our experiments. is suggests that the convergence occurs when accurate representations are learned by the networks, regardless of the initialization.",0,,False
"Are deep neural networks a good choice for learning to rank with weak supervision? To see if there is a real bene t from using a non-linear neural network in di erent se ings, we examined RankSVM [20] as a strong-performing pair-wise learning to rank method with linear kernel that is fed with di erent inputs: dense vector representation, sparse vector representation, and embedding vector representation. Considering that o -the-shelf RankSVM is not able to learn embedding representations during training, for embedding vector representation, instead of learning embeddings we use a pre-trained embedding matrix trained on Google News and",1,ad,True
"xed IDF weights. e results are reported in Table 4. As BM25 is not a linear function,",0,,False
"RankSVM with linear kernel is not able to completely approximate it. However, surprisingly, for both dense vector representation and sparse vector representation, RankSVM works as well as neural networks (see Table 2). Also, compared to the corresponding experiment in Table 3, the performance of the neural network with an external pre-trained embedding and IDF weighting is not considerably be er than RankSVM. is shows that having non-linearity in neural networks does not help that much when we do not have representation learning as part of the model. Note that all of these results are still lower than BM25, which shows that they are not good at learning from weak supervision signals for ranking.",0,,False
72,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 4: Performance of the linear RankSVM with di erent features.,0,,False
Method,0,,False
RankSVM + Dense RankSVM + Sparse RankSVM + (Pretrained (external) + IDF weighting) Score (one layer with no nonlinearity) + Embed,0,,False
MAP,1,MAP,True
0.1983 0.2307 0.1539,0,,False
0.2103,0,,False
Robust04,1,Robust,True
P@20 nDCG@20 MAP,1,MAP,True
0.2841 0.3260 0.2121,0,,False
0.3375 0.3794 0.1852,0,,False
0.0761 0.0862 0.0633,0,,False
0.3986 0.3160 0.0645,0,,False
ClueWeb,1,ClueWeb,True
P@20 nDCG@20,0,,False
0.1840 0.2170 0.1572,0,,False
0.1637 0.1939 0.1494,0,,False
0.1421 0.1322,0,,False
"Table 5: Performance of the rankprob model with embedding vector representation in fully supervised setting, weak supervised setting, and weak supervised plus supervision as ne tuning. indicates that the improvements over all other models are",0,,False
"statistically signi cant, at the 0.05 level using the paired two-tailed t-test, with Bonferroni correction.",0,,False
Method,0,,False
MAP,1,MAP,True
Robust04 P@20 nDCG@20,1,Robust,True
MAP,1,MAP,True
ClueWeb P@20 nDCG@20,1,ClueWeb,True
Weakly supervised,0,,False
0.2837 0.3802,0,,False
Fully supervised,0,,False
0.1790 0.2863,0,,False
Weakly supervised + Fully supervised 0.2912 0.4126,0,,False
0.4389,0,,False
0.3402 0.4509,0,,False
0.1387 0.2967,0,,False
0.0680 0.1425 0.1520 0.3077,0,,False
0.2330,0,,False
0.1652 0.2461,0,,False
"We have also examined the score model with a network with a single linear hidden layer, with the embedding vector representation, which is equivalent to a linear regression model with the ability of representation learning. Comparing the results of this experiment with Score-Embed in Table 2, we can see that with a single-linear network we are not able to achieve a performance that is as good as a deep neural network with non-linearity. is shows that the most important superiority of deep neural networks over other machine learning methods is their ability to learn an e ective representation and take all the interactions between query and document(s) into consideration for approximating an e ective ranking/scoring function. is can be achieved when we have a deep enough network with non-linear activations.",0,,False
"How useful is learning with weak supervision for supervised ranking? In this set of experiments, we investigate whether employing weak supervision as a pre-training step helps to improve the performance of supervised ranking, when a small amount of training data is available. Table 5 shows the performance of the rankprob model with the embedding vector representation in three situations: (1) when it is only trained on weakly supervised data (similar to the previous experiments), (2) when it is only trained on supervised data, i.e., relevance judgments, and (3) when the parameters of the network is pre-trained using the weakly supervised data and then ne tuned using relevance judgments. In all the supervised scenarios, we performed 5-fold cross-validation over the queries of each collection and in each step, we used the TREC relevance judgements of the training set as supervised signal. For each query with m relevant documents, we also randomly sampled m non-relevant documents as negative instances. Binary labels are used in the experiments: 1 for relevant documents and 0 for non-relevant ones.",1,TREC,True
"e results in Table 5 suggest that pre-training the network with a weak supervision signal, signi cantly improves the performance of supervised ranking. e reason for the poor performance of the supervised model compared to the conventional learning to rank models is that the number of parameters are much larger, hence it needs much more data for training.",0,,False
"In situations when li le supervised data is available, it is especially helpful to use unsupervised pre-training which acts as a network",0,,False
pre-conditioning that puts the parameter values in the appropriate range that renders the optimization process more e ective for further supervised training [11].,0,,False
"With this experiment, we indicate that the idea of learning from weak supervision signals for neural ranking models, which is presented in this paper, not only enables us to learn neural ranking models when no supervised signal is available, but also has substantial positive e ects on the supervised ranking models with limited amount of training data.",0,,False
7 CONCLUSIONS,0,,False
"In this paper, we proposed to use traditional IR models such as BM25 as a weak supervision signal in order to generate large amounts of training data to train e ective neural ranking models. We examine various neural ranking models with di erent ranking architectures and objectives, and di erent input representations.",1,ad,True
"We used over six million queries to train our models and evaluated them on Robust04 and ClueWeb 09-Category B collections, in an ad-hoc retrieval se ing. e experiments showed that our best performing model signi cantly outperforms the BM25 model (our weak supervision signal) by over 13% and 35% MAP improvements in the Robust04 and ClueWeb collections, respectively. We also demonstrated that in the case of having a small amount of training data, we can improve the performance of supervised learning by pre-training the network on weakly supervised data.",1,Robust,True
"Based on our results, there are three key ingredients in neural ranking models that lead to good performance with weak supervision: e rst is the proper input representation. Providing the network with raw data and le ing the network to learn the features that ma er, gives the network a chance of learning how to ignore imperfection in the training data. e second ingredient is to target the right goal and de ne a proper objective function. In the case of having weakly annotated training data, by targeting some explicit labels from the data, we may end up with a model that learned to express the data very well, but is incapable of going beyond it. is is especially the case with deep neural networks where there are many parameters and it is easy to learn a model that over ts the data. e third ingredient is providing the network with a considerable amount of training examples. As an example, during the experiments we noticed that",1,ad,True
73,0,,False
Session 1B: Retrieval Models and Ranking 1,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"using the embedding vector representation, the network needs a lot of examples to learn embeddings that are more e ective for retrieval compared to pre-trained embeddings. anks to weak supervision, we can generate as much training data as we need with almost no cost.",0,,False
"Several future directions can be pursued. An immediate task would be to study the performance of more expressive neural network architectures e.g., CNNs and LSTMs, with weak supervision setup. Other experiment is to leverage multiple weak supervision signals from di erent sources. For example, we have other unsupervised ranking signals such as query likelihood and PageRank and taking them into consideration might bene t the learning process. Other future work would be to investigate the boosting mechanism for the method we have in this paper. In other words, it would be interesting to study if it is possible to use the trained model on weakly supervised data to annotate data with more quality from original source of annotation and leverage the new data to train a be er model. Finally, we can apply this idea to other information retrieval tasks, such as query/document classi cation and clustering.",0,,False
ACKNOWLEDGMENTS,0,,False
"is research was supported in part by Netherlands Organization for Scienti c Research through the Exploratory Political Search project (ExPoSe, NWO CI # 314.99.108), by the Digging into Data Challenge through the Digging Into Linked Parliamentary Data project (DiLiPaD, NWO Digging into Data # 600.006.014), and by the Center for Intelligent Information Retrieval. Any opinions, ndings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect those of the sponsors.",0,,False
REFERENCES,0,,False
"[1] Nima Asadi, Donald Metzler, Tamer Elsayed, and Jimmy Lin. 2011. Pseudo test collections for learning web search ranking functions. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. ACM, 1073­1082.",1,ad,True
"[2] Pierre Baldi. 2012. Autoencoders, unsupervised learning, and deep architectures. ICML unsupervised and transfer learning 27 (2012), 37­50.",0,,False
"[3] Lidong Bing, Sneha Chaudhari, Richard C Wang, and William W Cohen. 2015. Improving Distant Supervision for Information Extraction Using Label Propagation rough Lists. In EMNLP '15. 524­529.",0,,False
"[4] Alexey Borisov, Ilya Markov, Maarten de Rijke, and Pavel Serdyukov. 2016. A Neural Click Model for Web Search. In WWW '16. 531­541.",0,,False
"[5] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Sa¨ckinger, and Roopak Shah. 1993. Signature Veri cation Using a ""Siamese"" Time Delay Neural Network. In NIPS '93. 737­744.",0,,False
[6] Daniel Cohen and W. Bruce Cro . 2016. End to End Long Short Term Memory Networks for Non-Factoid estion Answering. In ICTIR '16. 143­146.,0,,False
"[7] Gordon V. Cormack, Mark D. Smucker, and Charles L. Clarke. 2011. E cient and E ective Spam Filtering and Re-ranking for Large Web Datasets. Inf. Retr. 14, 5 (2011), 441­465.",0,,False
"[8] omas Desautels, Andreas Krause, and Joel W Burdick. 2014. Parallelizing exploration-exploitation tradeo s in Gaussian process bandit optimization. Journal of Machine Learning Research 15, 1 (2014), 3873­3923.",1,ad,True
"[9] Fernando Diaz. 2016. Learning to Rank with Labeled Features. In ICTIR '16. 41­44. [10] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. ery Expansion with",0,,False
"Locally-Trained Word Embeddings. In ACL '16. [11] Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol,",0,,False
"Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research 11 (2010), 625­660. [12] Mart´in Abadi et al. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. (2015). h p://tensor ow.org/ So ware available from tensor ow.org. [13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In NIPS. 2672­2680. [14] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W. Bruce Cro . 2016. A Deep Relevance Matching Model for Ad-hoc Retrieval. In CIKM '16. 55­64. [15] Xianpei Han and Le Sun. 2016. Global Distant Supervision for Relation Extraction. In AAAI'16. 2950­2956.",1,ad,True
"[16] Ralf Herbrich, ore Graepel, and Klaus Obermayer. 1999. Support Vector Learning for Ordinal Regression. In ICANN '99. 97­102.",0,,False
"[17] Raphael Ho mann, Congle Zhang, Xiao Ling, Luke Ze lemoyer, and Daniel S. Weld. 2011. Knowledge-based Weak Supervision for Information Extraction of Overlapping Relations. In ACL '11. 541­550.",0,,False
"[18] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data. In CIKM '13. 2333­2338.",0,,False
"[19] Kalervo Ja¨rvelin and Jaana Keka¨la¨inen. 2002. Cumulated Gain-based Evaluation of IR Techniques. ACM Trans. Inf. Syst. 20, 4 (Oct. 2002), 422­446.",0,,False
[20] orsten Joachims. 2002. Optimizing Search Engines Using Clickthrough Data. In KDD '02. 133­142.,0,,False
[21] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014).,0,,False
"[22] Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In NIPS '15. 3294­3302.",0,,False
"[23] oc V Le and Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In ICML '14, Vol. 14. 1188­1196.",0,,False
"[24] Yann LeCun, Yoshua Bengio, and Geo rey Hinton. 2015. Deep Learning. Nature 521, 7553 (2015), 436­444.",0,,False
"[25] Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li Deng, Kevin Duh, and Ye-yi Wang. 2015. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classi cation and Information Retrieval. In NAACL '15. 912­921.",0,,False
[26] Zhengdong Lu and Hang Li. 2013. A Deep Architecture for Matching Short Texts. In NIPS '13. 1367­1375.,0,,False
"[27] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS '13. 3111­3119.",1,ad,True
"[28] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to Match Using Local and Distributed Representations of Text for Web Search. In WWW '17. 1291­1299.",0,,False
"[29] Kezban Dilek Onal, Ismail Sengor Altingovde, Pinar Karagoz, and Maarten de Rijke. 2016. Ge ing Started with Neural Models for Semantic Matching in Web Search. arXiv preprint arXiv:1611.03305 (2016).",0,,False
"[30] Greg Pass, Abdur Chowdhury, and Cayley Torgeson. 2006. A Picture of Search. In InfoScale '06.",0,,False
"[31] Je rey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global Vectors for Word Representation. In EMNLP '14. 1532­1543.",0,,False
"[32] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Hamed Zamani. 2017. Word Embedding Causes Topic Shi ing; Exploit Global Context!. In SIGIR'17.",0,,False
"[33] Navid Rekabsaz, Mihai Lupu, Allan Hanbury, and Guido Zuccon. 2016. Generalizing translation models in the probabilistic relevance framework. In Proceedings",0,,False
"of the 25th ACM International on Conference on Information and Knowledge Management. ACM, 711­720. [34] Aliaksei Severyn and Alessandro Moschi i. 2015. Twi er Sentiment Analysis with Deep Convolutional Neural Networks. In SIGIR '15. 959­962. [35] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gre´goire Mesnil. 2014. Learning Semantic Representations Using Convolutional Neural Networks for Web Search. In WWW '14. 373­374. [36] Nitish Srivastava, Geo rey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Over ing. J. Mach. Learn. Res. 15, 1 (2014), 1929­1958. [37] Yuan Tang. 2016. TF.Learn: TensorFlow's High-level Module for Distributed Machine Learning. arXiv preprint arXiv:1612.04251 (2016). [38] Christophe Van Gysel, Maarten de Rijke, and Evangelos Kanoulas. 2016. Learning latent vector spaces for product search. In CIKM'16. 165­174. [39] Christophe Van Gysel, Maarten de Rijke, and Marcel Worring. 2016. Unsupervised, e cient and semantic expertise retrieval. In WWW'16. 1069­1079. [40] Fabian L. Wauthier, Michael I. Jordan, and Nebojsa Jojic. 2013. E cient Ranking from Pairwise Comparisons. In ICML'13. 109­117. [41] Liu Yang, Qingyao Ai, Jiafeng Guo, and W. Bruce Cro . 2016. aNMM: Ranking Short Answer Texts with A ention-Based Neural Matching Model. In CIKM '16. 287­296. [42] Hamed Zamani, Michael Bendersky, Xuanhui Wang, and Mingyang Zhang. 2017. Situational Context for Ranking in Personal Search. In WWW '17. 1531­1540. [43] Hamed Zamani and W. Bruce Cro . 2016. Embedding-based ery Language Models. In ICTIR '16. 147­156. [44] Hamed Zamani and W. Bruce Cro . 2016. Estimating Embedding Vectors for",0,,False
eries. In ICTIR '16. 123­132. [45] Hamed Zamani and W. Bruce Cro . 2017. Relevance-based Word Embedding.,0,,False
"In SIGIR '17. [46] Ye Zhang, Md Musta zur Rahman, Alex Braylan, Brandon Dang, Heng-Lu Chang,",0,,False
"Henna Kim, inten McNamara, Aaron Angert, Edward Banner, Vivek Khetan, and others. 2016. Neural Information Retrieval: A Literature Review. arXiv preprint arXiv:1611.06792 (2016). [47] Guoqing Zheng and Jamie Callan. 2015. Learning to Reweight Terms with Distributed Representations. In SIGIR '15. 575­584.",0,,False
74,0,,False
,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Learning to Rank estion Answer Pairs with Holographic Dual LSTM Architecture,0,,False
Yi Tay,0,,False
Nanyang Technological University ytay017@e.ntu.edu.sg,0,,False
Minh C. Phan,0,,False
Nanyang Technological University phan0005@e.ntu.edu.sg,0,,False
Luu Anh Tuan,0,,False
Institute for Infocomm Research at.luu@i2r.a-star.edu.sg,0,,False
Siu Cheung Hui,0,,False
Nanyang Technological University asschui@ntu.edu.sg,0,,False
ABSTRACT,0,,False
"We describe a new deep learning architecture for learning to rank question answer pairs. Our approach extends the long short-term memory (LSTM) network with holographic composition to model the relationship between question and answer representations. As opposed to the neural tensor layer that has been adopted recently, the holographic composition provides the bene ts of scalable and rich representational learning approach without incurring huge parameter costs. Overall, we present Holographic Dual LSTM (HDLSTM), a uni ed architecture for both deep sentence modeling and semantic matching. Essentially, our model is trained end-to-end whereby the parameters of the LSTM are optimized in a way that best explains the correlation between question and answer representations. In addition, our proposed deep learning architecture requires no extensive feature engineering. Via extensive experiments, we show that HD-LSTM outperforms many other neural architectures on two popular benchmark QA datasets. Empirical studies con rm the e ectiveness of holographic composition over the neural tensor layer.",1,ad,True
KEYWORDS,0,,False
"Deep Learning, Long Short-Term Memory, Learning to Rank, estion Answering",0,,False
1 INTRODUCTION,1,DUC,True
"Learning to rank techniques are central to many web-based question answering (QA) systems such as factoid-based QA or communitybased QA (CQA). In these applications, questions are matched against an extensive database to nd the most relevant answer. Essentially, this is highly related to many search and information retrieval tasks such as traditional document retrieval and text matching. However, a key di erence is that questions and answers are o en much shorter compared to full- edged documents whereby the problem of lexical chasm [1, 8, 32] becomes more prevalent. As such, this makes the already di cult task of designing features for questions and answers even harder. Furthermore, traditional",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080790",1,ad,True
"approaches o en involve extensive handcra ed features and domain expertise which can be laborious and expensive. In addition, constructing features from textual clues [24, 27, 33, 36] such as lexical and syntactic features is di cult and provide limited bene ts. Overall, the challenges of learn-to-rank QA systems are two-fold. First, feature representations of questions and answers have to be learned or designed. Second, a similarity function has to be de ned to match questions to answers.",1,ad,True
"Recently, deep learning architectures have been an extremely popular choice for learning distributed representations of words, sentences or documents [11, 14]. Generally, this is known as representational learning whereby low dimensional vectors are learned for words or documents via neural networks such as the convolutional neural networks (CNN), recurrent neural network (RNN) or standard feed-forward multi-layer perceptron (MLP). is has widespread applications in the eld of NLP and IR such as semantic text matching [20, 25], relation detection [10], language modeling [14] and question answering [18, 20]. Essentially, the a ractiveness of these models stem from the fact that features are learned in deep learning architectures in an end-to-end fashion and o en require li le or no human involvement. Furthermore, the performance of these models are o en spectacular.",1,ad,True
"Additionally and recently, it has also been fashionable to model the relationship between vectors via tensor layers [18, 25, 30]. A recent work, the convolutional neural tensor network (CNTN) [18] demonstrates impressive results on community-based question answering. In their work, a convolutional neural network is used to learn representations for questions and answers while a tensor layer is used to model the relationship between the representations using an additional tensor parameter. is is powerful because the tensor layer models multiple views of dyadic interactions between question and answer pairs which enables rich representational learning. Overall, the CNTN is a uni ed architecture that learns representations and performs matching in an end-to-end fashion.",1,ad,True
"However, the use of a tensor layer may not be implication free. Firstly, adding a tensor layer severely increases the number of parameters which naturally and inevitably increases the risk of over ing. Secondly, this signi cantly increases computational and memory cost of the overall network. irdly, the inclusion of a tensor layer also indirectly restricts the expressiveness of the QA representations since increasing the parameters of the QA representations would easily incur memory and computational costs of quadratic scale at the tensor layer.",1,ad,True
695,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"In lieu of the above mentioned weaknesses, we propose an alternative to the tensor layer. For the rst time, we adopt holographic composition to model the relationship between question and answer embeddings. Our approach is largely based on holographic models of associative memory and employs circular correlation to learn the relationship between QA pairs. e prime contributions of our paper can be summarized as follows:",1,ad,True
"· For the rst time, we adopt holographic composition for modeling the interaction between representations of QA pairs. Unlike the tensor layer, our compositional approach is essentially parameterless, memory e cient and scalable. Furthermore, our approach also enables rich representational learning by employing circular correlation.",1,ad,True
"· As a whole, we present a novel deep learning architecture, HD-LSTM (Holographic Dual LSTM) for learning to rank QA pairs. Our model is a uni ed architecture similar to [18]. However, instead of the CNN, we use multi-layered long short-term memory neural networks to learn representations for questions and answers. Similar to other deep learning models, our approach does not require extensive feature engineering or domain knowledge.",1,ad,True
· We provide extensive experimental evidence of the e ectiveness of our model on both factoid question answering and community-based question answering. Our proposed approach outperforms many other neural architectures on TREC QA task and on the Yahoo CQA dataset.,1,TREC,True
2 RELATED WORK,0,,False
"Our work is concerned with ranking question and answer pairs to select the most suitable answer for each question. Across the rich history of IR research, techniques for doing so have been primarily focused on lexical and syntactic feature-dependent approaches.",0,,False
"ese techniques include the Tree Edit Distance (TED) model [5], Support Vector Machines (SVMs) with tree kernels [21] and linear chain Conditional Random Fields (CRFs) [33] with features from the TED model. However, apart from relying heavily on handcra ed features such as cumbersome parse trees, these approaches have limited performance and have been shown to be outclassed by modern deep learning approaches such as convolutional neural networks [20, 35].",0,,False
"e key intuition behind deep learning architectures is to learn low-dimensional representations of words, documents or sentences which can be used as input features. For example, Yu et al. [35] employed a convolutional neural network for feature learning of QA pairs and subsequently applied logistic regression for prediction. Despite its simplicity, the performance of Yu et al. has already surpassed all traditional approaches [5, 21, 27­29]. Another a ractive quality of deep learning architectures is that features can be learned in an end-to-end fashion. Severyn et al. [20] demonstrated a uni ed architecture that trains a convolutional neural network together with a multi-layer perceptron. In short, features are learned while the parameters of the network are optimized for the task at hand.",1,ad,True
"In the architectures of Severyn et al. [20] and Yu et al. [35], representations of questions and answers are learned separately and concatenated for prediction at the end. Qiu et al. [18] introduced a tensor layer to model the relationship between question and answer",0,,False
"representations. e tensor layer can be seen as a compositional technique to learn the relationship between two vectors and was adapted from the neural tensor network (NTN) by Socher et al. [22, 23]. e NTN was originally incepted in the eld of NLP for semantic parsing and used as a compositional operator in recursive neural tensor networks (RNTN) [23] and also relational learning on knowledge bases [22]. It has also recently seen adoption for modeling document novelty in [30]. e tensor layer models multiple dyadic interactions between two vectors via an additional tensor parameter. is suggests rich representational learning that is useful for matching text pairs.",1,ad,True
"Additionally, recurrent neural networks such as the long shortterm memory (LSTM) networks are also widely popular for learning sentence representations and has seen wide adoption in a variety of NLP tasks. Without an exception, LSTM networks are also widely adopted in QA [12, 25, 26]. e usage of grid-wise similarity matrices within neural architectures are also recently very fashionable and have seen wide adoption1 in QA tasks [4, 15, 25] to model the interactions between QA pairs. For example, in the MV-LSTM [25], all positional hidden states from both LSTMs are being matched grid-wise using a variety of similarity scoring functions followed by a max-pooling layer. On the other hand, the works of [31] are concerned with learning grid-wise a entions. On a side note, it is good to note that, grid-wise matching, though highly competitive, naturally incurs a prohibitive computational cost of quadratic scale.",1,ad,True
"As seen in many recent works, the tensor layer is highly popular to model relationship between two vectors [18, 25]. However, a tensor layer adds a signi cant amount of parameters to the network causing implications in terms of runtime, memory, risk of over tting as well as an inevitable restriction of exibility in designing representations for questions and answers. Speci cally, increasing the dimensionality of the LSTM or CNN output by x would incur a parameter cost of x2 in the tensor layer which can be non-ideal especially in terms of scalability. As an alternative to the tensor layer, our novel deep learning architecture adopts the circular correlation of vectors to model the interactions between question and answer representations. e circular correlation of vectors, along with circular convolution, are typically used in Holography to store and retrieve information [3, 17] and are also known as correlationconvolution (holographic-like) memories. Due to its connections with holographic models of associative memories [17], we refer to our model as Holographic Dual LSTM. It is good to note that a similar but fundamentally di erent work [16] also used holography inspired operations within recurrent neural networks. However, our work is the rst work to incorporate holographic representational learning for QA embeddings.",1,ad,True
"In addition, holographic composition [17] can also be interpreted as compressed tensor product which also enables rich representational learning without severely increasing the number of parameters of the network. In this case, the parameters of the network are learned in a way that best explains the correlation between questions and answers. In the same domain where the neural tensor network was incepted, holographic embeddings of knowledge graphs [13], demonstrates the e ectiveness of holographic composition in the task of relational learning on knowledge bases. As",1,ad,True
"1Notably, our proposed holographic composition can also be used for grid-wise matching.",0,,False
696,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"a whole, we propose a novel deep learning architecture based on long short-term memory neural networks while using holographic composition to model the interactions between QA embeddings, this enables rich representational learning with improved exibility and scalability. e outcome is a highly performant end-to-end deep learning architecture for learning to rank for QA applications.",0,,False
3 PRELIMINARIES,0,,False
"In this section, we introduce the background for the remainder of the paper. Namely, we formally give the problem de nition and introduce fundamental deep learning models required to understand the remainder of the paper.",0,,False
3.1 Problem Statement and Approach,0,,False
"e task of supervised learning to rank can be typically regarded as a binary classi cation problem. Given a set of questions qi  Q, the task is to rank a list of candidate answers ai  A. Speci cally, we try to learn a function f (q, a) that outputs a relevancy score f (q, a)  [0, 1] for each question answer pair. is score is then used to rank a list of possible candidates. Typically, there are three di erent ways for supervised text ranking, namely, pairwise, pointwise and listwise. Pairwise considers maximizing the margin between positive and negative question-answer pairs with an objective function such as the hinge loss. Pointwise considers each pair, positive or negative, individually. On the other hand, listwise considers a question and all candidate answers as a training instance. Naturally, pairwise and listwise are much harder to train, implement and take a longer time due to having to process more instances. erefore, in this work, we mainly consider a pointwise approach when designing our deep learning model.",0,,False
3.2 Long short-term Memory (LSTM),0,,False
"First, we introduce the Long Short-Term Memory (LSTM) [6]. LSTMs are a type of recurrent neural network that are capable of learning long term dependencies across sequences. Given an input sentence S ,"" (xo, x1..., xn ), the LSTM returns a sentence embedding ht for position t with the following equations:""",0,,False
"it ,  (Wi xt + Uiht -1 + bi ) ft ,  (Wf xt + Uf ht -1 + bf ) ct , ft ct -1 + it tanh(Wc xt + Ucht -1 + bc ) ot ,  (Woxt + Uoht -1 + bo ) ht , ot tanh(ct )",0,,False
"where xt and ht are the input vectors at time t. W, b, U are the parameters of the LSTM network and  ,"" {o, i, f , u, c}.  is the sigmoid function and ct is the cell state. For the sake of brevity, we omit the technical details of LSTM which can be found in many related works. e output of this layer is a sequence of hidden vectors H  RL×d where L is the maximum sequence length and d is the dimensional size of LSTM. It is also possible to stack layers of LSTMs on top of one another which form multi-layered LSTMs which we will adopt in our approach.""",1,ad,True
3.3 Neural Tensor Network,0,,False
"e Neural Tensor Network [22, 23] is a parameterized composition technique that learns the relationships between two vectors. e scoring function between two vectors are de ned as follows:",0,,False
"st (q, a) ,"" uT f (q T M[1:r ]a + V [q, a] + b)""",0,,False
(1),0,,False
"where f is a non-linear function such as tanh applied element wise. M[1:r ]  Rn×n×r is a tensor (3d matrix). For each slice of the tensor M, each bilinear tensor product q T Mr a returns a scalar value to form a r dimensional vector. e other parameters are the standard",0,,False
form of a neural network. We can clearly see that the NTN enables,0,,False
rich representational learning of embedding pairs by using a large,0,,False
number of parameters.,0,,False
4 OUR DEEP LEARNING MODEL,0,,False
"In this section, we introduce Holographic Dual LSTM for representational learning and ranking of short text pairs. In our model, we use a pair of multi-layered LSTMs denoted Q-LSTM and A-LSTM. First, the LSTMs learn sentence representations of question and answer pairs and subsequently holographic composition is employed to model the similarity between the outputs of Q-LSTM and A-LSTM. Finally, we pass the network through a fully connected hidden layer and perform binary classi cation. is is all done in an end-to-end fashion. Figure 1 shows the overall architecture.",0,,False
4.1 Learning QA Representations,0,,False
"Our model accepts two sequences of indices (one for question and the other for answer) and a one-hot encoded ground truth for training. ese sequence of indices are rst passed through a look-up layer. At this layer, each index is converted into a low-dimensional vector representation. e parameters of this layer are W  R|V |×n where V is the size of the vocabulary and n is the dimensionality of the word embeddings. Even though these word embeddings can be learned from the training process of our model, i.e., end-to-end, we do not do so since learning word embeddings o en require an extremely large corpus like Wikipedia. erefore, we initialize W with pretrained SkipGram [11] embeddings which is aligned with the works of [20, 35]. Next, these embeddings from question and answer sequences are fed into Q-LSTM and A-LSTM respectively. Subsequently, the last hidden output from Q-LSTM and A-LSTM are taken to be the nal representation for question and answer respectively.",1,Wiki,True
4.2 Holographic Matching of QA pairs,0,,False
"e QA embeddings learned from LSTM are then passed into what we call the holographic layer. In this section, we introduce our novel compositional deep learning model for modeling the relationship between q and a. We denote qa as a compositional operator applied to vectors q and a. We employ the circular correlation of vectors to learn relationships between question and answer embeddings.",0,,False
"qa ,q a",0,,False
(2),0,,False
697,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 1: Holographic Dual LSTM Deep Learning Model for Ranking of QA Pairs,0,,False
where : Rd × Rd  Rd denotes the circular correlation operator2.,0,,False
d -1,0,,False
"[q a]k , qi a(k+i ) mod d",0,,False
(3),0,,False
"i ,0",0,,False
Circular correlation can be computed as follows:,0,,False
"q a , F -1 (F (q) F (a))",0,,False
(4),0,,False
where F (.) and F -1 (.) are the fast Fourier transform (FFT) and,0,,False
inverse fast Fourier transform. F (q) denotes the complex conjugate,0,,False
"of F (q). is element-wise (or Hadamard) product. Additionally,",1,ad,True
circular correlation can be viewed as a compressed tensor product,0,,False
"[13]. In the tensor product [q  a]ij , qiaj a separate element is used to store each pairwise multiplication or interaction between q",0,,False
"and a. In circular correlation, each element of the composed vector",0,,False
is a sum of multiplicative interactions over a xed summation,0,,False
pa ern between q and a. Figure 2 describes this process where the,0,,False
circular arrows depict the summation process in which vector c is,0,,False
the result of composing q and a with circular correlation.,0,,False
One key advantage of this composition method is that there,1,ad,True
are no increase in parameters. e fact that the composed vector,0,,False
remains at the same length of its constituent vectors is an extremely,0,,False
a ractive quality of our proposed model. In the case where question,0,,False
"and answer representations are of di erent dimensions, we can",0,,False
simply zero-pad the vectors to make them the same length. As,1,ad,True
"circular correlation uses summation pa erns, it is still possible to",0,,False
"compose them without much implications. However, for this paper",0,,False
we consider that q and a to have the same dimensions.,0,,False
4.3 Holographic Hidden Layer,0,,False
"Subsequently, a fully connected hidden layer follows our compositional operator which forms the holographic layer.",0,,False
"hout ,  (Wh . [q a] + bh )",0,,False
(5),0,,False
"2For Holographic Composition, we use zero-indexed vectors for notational convenience.",0,,False
"Figure 2: Circular Correlation as Compressed Tensor Product, Circular Arrows denote Summation Process, Adapted from Plate (1995) [17].",0,,False
"where wh and bh are parameters of the hidden layer and  is a non-linear activation function like tanh. Traditionally, most models",1,ad,True
"[7, 20] use the composition operator of concatenation, denoted  to combine the vectors of questions and answers.  : Rd1 × Rd2  Rd1+d2 simply appends one vector a er another to form a vector of",0,,False
"their lengths combined. Obviously, concatenation does not consider",0,,False
"the relationship between latent features of QA embeddings. us,",0,,False
the relationship has to be learned from the parameters of the deep,0,,False
"learning model, i.e., the subsequent hidden layer. In summary, the",0,,False
fully connected dense layer that maps [q a] to hout forms the holographic hidden layer of our network.,0,,False
"Incorporating Additional Features Following the works of [2, 20], it is also possible (though optional) to incorporate additional features. First, we include an additional similarity measure in our model between QA embeddings. Namely, this similarity is known as the bilinear similarity which can be de ned as:",1,corpora,True
"sim(q, a) , q T Ma",0,,False
(6),0,,False
where M  Rn×n is a similarity matrix between vectors q  Rn and a  Rn . e bilinear similarity is a parameterized approach,0,,False
698,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"where M is an additional parameter of the network. e output of sim(q, a) is a scalar value that is concatenated with [q a]. We experimented with concatenation at hout but empirically found it to perform worse. It is also possible to include other manual features. For example, in [20], word overlap features Xf eat were included before the hidden layers at the join layer. e rationale for doing so is as follows: First, it is di cult to encode features like word overlap into deep learning architectures [20, 35]. Secondly, word overlap features are relatively easy and trivial to implement and incorporate. As such, we are able to do the same with our model.",1,ad,True
"us, when using external features, the inputs to the holographic hidden layer becomes a vector [[q a], sim(q, a), Xf eat ].",0,,False
4.4 So max Layer,0,,False
e output from the holographic hidden layer is then passed into a fully connected so max layer which introduces another two parameters Wf and bf .,0,,False
"p , so f tmax (Wf . hout + bf )",0,,False
(7),0,,False
where k is the weight vector of the kth class and x is the nal vector representation of question and answer a er passing through,0,,False
all the layers in the network.,0,,False
4.5 Training and Optimization,0,,False
"Finally, we describe the optimization and training procedure of our network. Our network minimizes the cross-entropy loss function as follows:",0,,False
N,0,,False
"L,-",0,,False
[ i log ai + (1 -,0,,False
i ) log(1 - ai )] +  ,0,,False
2 2,0,,False
(8),0,,False
"i ,1",0,,False
where a is the output of the so max layer.  contains all the pa-,0,,False
rameters of the network and ,0,,False
2 2,0,,False
is,0,,False
the,0,,False
L2,0,,False
regularization.,0,,False
e,0,,False
parameters of the network can be updated by Stochastic Gradient,1,ad,True
"Descent (SGD). In our network, we mainly employ the Adam [9]",0,,False
optimizer.,0,,False
5 DISCUSSION,0,,False
"In this section, we discuss and highlight some of the interesting and advantageous properties of our proposed approach.",1,ad,True
5.1 Complexity Analysis,0,,False
"To be er understand the merits of our proposed approach, we study the computational and memory complexity of our model with respect to the alternatives like the tensor layer.",0,,False
Operator Tensor Product  Concatenation  Circular Correlation,0,,False
#Parameters d2,0,,False
2d,0,,False
d,0,,False
Complexity O (d 2 ),0,,False
O(d ),0,,False
O(d log d ),0,,False
Table 1: Complexity Comparison between Compositional,0,,False
Operators,0,,False
"First, Table 1 shows the parameter cost and complexity for the",0,,False
"three di erent compositional operators. is assumes a simple example where q, a  Rd and a vector of the same dimensionality as",0,,False
Network #Parameters,0,,False
d/h/k On TREC,1,TREC,True
NTN d2k + 2dk + 2k 640 / 0 / 5 2.1M,0,,False
Ours,0,,False
2dh + 4h 640 / 64 / 0 41.2K,0,,False
Table 2: Memory Complexity Comparison between Tensor,0,,False
Layer and Holographic Layer,0,,False
Network,0,,False
Complexity,0,,False
NTN,0,,False
O(d2k + 2dk + 2k ),0,,False
Ours O(2dh + 4h + d log d ),0,,False
Table 3: Complexity Comparison between Tensor Layer and,0,,False
Holographic Layer,0,,False
"q a is used to map the composed vector into a scalar value. Clearly, the cost of a simple tensor layer is of quadratic scale which makes it di cult to scale with large d. Concatenation, on the other hand, doubles the length of composed vectors and increases the memory cost by a factor of two. Finally, we see that the parameter cost of circular correlation that we employ is only d. As such, there can be signi cantly less parameters in the subsequent layers. Finally, the computational complexity of circular correlation is also relatively low at d log d. Next, we compare the complexity of our network and the NTN. To enable direct comparison, we exclude any additional features at the holographic hidden layer of our network and include the subsequent so max layer. Finally, the overall network and similarity between q and a can be modeled as follows:",1,ad,True
"sh (q, a) , so f tmax (WfT f (WhT [q a] + bh ) + bf )",0,,False
(9),0,,False
"where Wh  Rd×r is parameters at the holographic hidden layer following the composition operation, bh is the scalar bias at the hidden layer, Wf  Rr ×2 converts the output at the hidden layer to a 2-class classi cation problem and q, a  Rd where d is the",0,,False
dimension size of the LSTM. f (.) is a non-linear activation function.,0,,False
"Note that since we consider use a so max layer at the output, our",0,,False
"nal output s (q, a) is a vector of 2 dimensions. Similarly, in the",0,,False
"traditional tensor layer described in Equation (1), we are able to simply adapt the vector u to become a weight matrix of u  Rk×2",1,ad,True
where k is the number of slices of tensors.,0,,False
"In Table 2 and Table 3, we compare the di erences between the",0,,False
tensor layer and our holographic layer with respect to the number,0,,False
of parameters and computational complexity respectively. Note,0,,False
"that d is the dimensionality of QA embeddings, h is the size of",0,,False
the hidden layer and k is the number of tensor slices. To facilitate,0,,False
"easier comparison, we do not include complexity from learning QA",0,,False
"representations, computing bias and activation functions but only",0,,False
"matrix and vector operations. From Table 2, it is clear and evident",0,,False
that our approach does not require as much parameters as the,0,,False
NTN. We also report the optimal dimensions of the QA embeddings,0,,False
and hidden layer size on TREC QA. We see that our model only requires 41.2k parameters3 as opposed to 2.1M parameters with,1,TREC,True
"the NTN. As such, we see that when optimal parameters required",0,,False
"for sentence modeling is high, the cost on the subsequent matching",0,,False
3We exclude word embedding and LSTM parameters in this comparison,0,,False
699,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"layer becomes impractical. e problem of quadratic scale is also re ected in computational complexity. us, from a theoretical point of view, the holographic composition can be seen as a memory e cient and faster alternative to the neural tensor network layer.",1,ad,True
5.2 Associative Holographic Memories,0,,False
"Holographic models of associative memories employ a series of convolutions and correlations to store and retrieve item pairs. is is sometimes referred to as convolution-correlation (holographiclike) memories [17]. At this point, it is apt to introduce circular convolution:  : Rd × Rd  Rd which is closely related to circular correlation.",0,,False
d -1,0,,False
"[q  a]k , qi a(k-i ) mod d",0,,False
(10),0,,False
"i ,0",0,,False
"In holographic associative memory models, association of vector",0,,False
pairs can be encoded via correlation and then decoded with circular convolution. e relationship between correlation and convolution is as follows:,0,,False
"q a , q~  a",0,,False
(11),0,,False
"where q~ is the approximate inverse of q such that qi ,"" q(-i mod d ). Typically, the encoding-decoding4 process is done via Hebbian learning in associative memory models. However, in our case, our model is holographic in the sense that correlation-convolution memories are learned implicitly via back-propagation. For example, let h be the input to the hidden layer hout , the gradients at a can be represented as:""",1,ad,True
E ai,0,,False
",",0,,False
k,0,,False
E hj,0,,False
q (k -j,0,,False
mod d ),0,,False
(12),0,,False
e gradient at ai according to Equation (12) [16] is equivalent to correlating h with the approximate inverse of q. Recall that,1,ad,True
correlating with the inverse is equivalent to circular convolution.,0,,False
"As such, this establishes the relation of our model to holographic",0,,False
"memories. Since our main point is to illustrate these connections,",0,,False
we omit the entire back-propagation derivation due to the lack of,0,,False
"space. Finally, we describe and summarize the overall advantages of",1,ad,True
employing a holographic layer in our deep architecture for learning,0,,False
to rank question answer pairs.,0,,False
"· Unlike circular convolution, circular correlation is noncommutative, i.e., q a a q. is is useful as most applications of text matching are non-symmetric. For example, questions to answers or queries to documents are not symmetric in nature. As such, we utilize correlation as the encoding operation and allow the network to decode via circular convolution while learning parameters.",0,,False
· e rst index of the circular correlation composed vector [q a]0 is the dot product of q and a. is is extremely helpful since question answer matching requires a measure of similarity.,0,,False
· e computational complexity of FFT is O(d lo d ) which makes it an e cient composition.,0,,False
"4Ideally, that the euclidean norm of q and a should be  1. However, our preliminary experiments showed that adding a normalization layer did not improve the performance.",1,ad,True
"· Our composition does not increase the dimensionality of its constituent vectors, i.e, the composition of q a preserves its dimensionality. On the other hand, concatenation doubles the parameter cost at the subsequent hidden layers. Furthermore, the relationship between question and answer embeddings have to be learned by the hidden layer.",0,,False
"· e association of two vectors, namely question and answer vectors are modeled end-to-end in the network. Via back-propagation, the network learns parameters that best explains this correlation via gradient descent.",1,ad,True
"Here it is good to note that the original holographic reduced representations [17] used convolution to encode and correlation to decode. However, this can be done vice versa as well [13].",0,,False
6 EMPIRICAL EVALUATION ON TREC QA,1,TREC,True
We evaluate our model on the TREC QA task of answer sentence selection on factoid based QA.,1,TREC,True
6.1 Experiment Setup,0,,False
"In this section, we introduce the dataset, evaluation procedure, metrics and compared baselines used in this experiment.",0,,False
"6.1.1 Dataset. In this task, we use the benchmark dataset provided by Wang et al. [29]. is dataset was collected from TREC QA tracks 8-13. In this task of factoid QA, questions are generally factual based questions such as ""What is the monetary value of the Nobel Peace Prize in 1989?"""". In this dataset, we are provided with two training sets, namely, TRAIN and TRAIN-ALL. TRAIN consists of QA pairs that have been manually judged and annotated. TRAINALL is a automatically judged dataset of QA pairs and contains a larger number of QA pairs. TRAIN-ALL, being a larger dataset, also contains more noise. Nevertheless, both datasets enable the comparison of all models with respect to availability and volume of training samples. Additionally, we are also provided with a development set for parameter tuning. e results of both training sets, development set and testing set are reported in Table 4. Finally, it is good to note that the maximum number of tokens for questions and answers are 11 and 38 respectively and the length of the vocabulary |V | , 16468.",1,TREC,True
Data TRAIN-ALL,0,,False
TRAIN DEV TEST,0,,False
# estions 1229 94 82 100,0,,False
# QA pairs 53417 4718 1148 1517,0,,False
% Correct 12 7.4 9.3 18.7,0,,False
Table 4: Dataset Statistics for TREC QA Dataset,1,TREC,True
"6.1.2 Evaluation Procedure and Metrics. Following the experimental procedure in [20], we report the results of all models in two se ings. In the rst se ing, we measure the representational learning ability of all deep learning models without the aid of external features. Conversely, in the second se ing, we include an additional feature vector Xf eat  R4 containing the count of word overlaps (ordinary and idf weighted) between question-answer pairs by considering inclusion and dis-inclusion of stop-words. Finally, the o cial evaluation metrics of MAP (Mean Average Precision) and MRR (Mean Reciprocal Rank) are used as our evaluation metrics.",1,ad,True
700,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
MRR is de,0,,False
ned as,0,,False
1 |q |,0,,False
"|Q | q,1",0,,False
1 r ank (q),0,,False
where rank(q),0,,False
is,0,,False
the rank of,0,,False
the,0,,False
rst correct answer. MAP is de,1,MAP,True
ned,0,,False
as,0,,False
1 Q,0,,False
"Q q,1",0,,False
A,0,,False
P (q).,0,,False
e MAP,1,MAP,True
"is the average precision across all queries qi  Q. For evaluation,",0,,False
we use the o cial trec eval script.,1,trec,True
"6.1.3 Algorithms Compared. Aside from comparison with all published state-of-the-art methods, we also evaluate our model against other deep learning architectures. Since the deep learning models compared in [20] are based on Convolutional Neural Networks, we additionally compare our model with MV-LSTM [25] and a simple LSTM baseline in our experimental evaluation. e following lists the major and popular deep learning based models for direct comparison with our model. Model names with  indicate that we implemented the model ourselves.",1,ad,True
"· CNN + Logistic Regression (Yu et al.) is is the model introduced in [35]. Representations of questions and answers are learned by a convolutional neural network. Subsequently, logistic regression over the learned features is used to score QA pairs. We report two se ings, namely Unigram and Bigram which are also reported in their work.",0,,False
"· CNN We implemented a CNN model following the architecture and hyperparameters of [20]. is model includes a bilinear similarity feature while concatenating two CNN encoded sentence representations. Unlike Yu et al.'s model, this work ranks QA pairs using an end-to-end architecture.",0,,False
"· CNTN We implemented a neural tensor network layer to performing matching of question and answer representations encoded by convolutional neural networks. is is similar to [18] but adopts the CNN architecture and hyperparameters of Severyn et al. [20]. For the tensor layer, we use k , 5 where k is the number of slices of the tensor.",1,ad,True
"· LSTM We consider both single layer and multi-layered LSTMs as our baselines. ese baseline models do not specially model the relationships between questions and answers. Instead, a concatenation operation is used to combine the QA embeddings in which the relationships between the two vectors are modeled by the hidden layer.",1,ad,True
"· MV-LSTM (Wan et al.) is model, introduced in [25], considers matching of multiple positional embeddings and subsequently applying max-pooling of top-k interactions. For scalability reasons, we only consider the bilinear similarity se ing for this model. We consider this su cient for three reasons. First, it is reported in [25] that the performance bene ts of tensor over bilinear is minimal. Second, it is extremely expensive computationally even when considering a bilinear similarity let alone the tensor similarity. ird, the comparison with this model mainly aims to investigate the e ectiveness of multiple positional embeddings.",0,,False
"· NTN-LSTM We consider a Neural Tensor Network + LSTM architecture instead of the CNTN to enable fairer comparison with our LSTM based model. In this model, we replace the holographic layer in HD-LSTM with a NTN layer which forms the major comparison in this paper. For the tensor layer, similar to the CNTN, we use k , 5 where k is the number of slices of the tensor.",1,ad,True
"· HD-LSTM (Ours) Holographic Dual LSTMS is the model architecture introduced in this paper. In our HD-LSTM model, the QA representations are merged with Holographic Composition.",0,,False
"6.1.4 Implementation Details and Hyperparameters. We implemented all deep learning architectures ourselves with the exception of Yu et al. [35] which we directly report the results. To facilitate fair comparison, we implement the exact architecture of the CNN model from Severyn et al. [20] ourselves using the same evaluation procedure and optimizer. All hyperparameters were tuned on the development set using extensive grid search. We trained all models using the Adam [9] optimizer with an initial learning rate of 10-5 for LSTM models and 10-2 for CNN models5 and minimized the same cross entropy loss in a point-wise fashion. We applied gradient clipping of 1.0 of the norm for all LSTM models.",1,ad,True
"With the exception of the single-layered LSTM and MV-LSTM, all LSTM-based models use a single-direction and multi-layered se ing.",0,,False
"e input sequences are all padded with zero vectors to the max length for questions and answers separately. e dimensionality of the LSTM models are tuned in multiples of 128 in the range of [128, 640] for TRAIN and amongst {512, 1024} for TRAIN-ALL in lieu of the larger dataset. Here it is good to note that the nal feature vector of the model in Severyn et al is  1000. e number of LSTM layers are tuned from a range of 2 - 4 and batch size is",1,ad,True
"xed to 256 for all LSTM based models. e hidden layer size for all LSTM models are amongst {32, 64, 128, 256, 512}. For regularization and preventing over- ing, we apply a dropout of d , 0.5 and set the regularization hyperparameter  ,"" 0.00001. For MV-LSTM, we followed the con guration se ing as stated in [25]. We used the pretrained word embeddings [20] of 50 dimensions trained on Wikipedia and AQUAINT corpus. e word embedding layer is set to non-trainable in lieu of the small dataset. We trained all models for a maximum of 30 epochs with early stopping, i.e., if the MAP score does not increase a er 5 epochs. We take MAP scores on the development set at every epoch and save the parameters of the network for the top three models on the development set. We report the best test score from the saved models. All experiments were conducted on a Linux machine with a single Nvidia GTX1070 GPU (8GB RAM).""",1,Wiki,True
6.2 Experimental Results,0,,False
"is section shows the experimental results on the TREC QA answer sentence selection task. Table 5 shows the result of all deep learning architectures in four di erent con gurations, i.e., di erent training sets (TRAIN vs TRAIN-ALL) and di erent feature se ings (with and without additional features). Overall, we see that HD-LSTM outperforms all other deep learning models. e relative ranking of each deep learning architecture is in concert with our intuitions. Using a tensor layer for matching improves performance over their base models which is aligned with the results of [18]. However, we see that HD-LSTM outperforms NTN-LSTM by a signi cant margin across all datasets and se ings despite being more e cient. is shows the e ectiveness of holographic composition for rich representational learning of QA pairs despite having less parameters. Additionally, the average increase over the baseline multi-layered",1,TREC,True
5 is learning rate works best for CNN models with Adam,0,,False
701,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Se ing 1 (raw),0,,False
TRAIN,0,,False
TRAIN-ALL,0,,False
Se ing 2 (with extra features),0,,False
TRAIN,0,,False
TRAIN-ALL,0,,False
All Average,0,,False
Model,0,,False
MAP MRR MAP MRR MAP MRR MAP MRR MAP MRR,1,MAP,True
CNN + LR (unigram) 0.5387 0.6284 0.5470 0.6329 0.6889 0.7727 0.6934 0.7677 0.6170 0.6982,0,,False
CNN + LR (bigram) 0.5476 0.6437 0.5693 0.6613 0.7058 0.7846 0.7113 0.7846 0.6335 0.7186,0,,False
LSTM (1 layer) LSTM,0,,False
0.5731 0.6056 0.6204 0.6685 0.6406 0.7494 0.6782 0.7604 0.6280 0.6960 0.6093 0.6821 0.5975 0.6533 0.7007 0.7777 0.7350 0.8064 0.6606 0.7299,0,,False
CNN CNTN,0,,False
0.5994 0.6584 0.6691 0.6880 0.7000 0.7469 0.7216 0.7899 0.6725 0.7208 0.6154 0.6701 0.6580 0.6978 0.7045 0.7562 0.7278 0.7831 0.6764 0.7268,0,,False
MV-LSTM NTN-LSTM,0,,False
0.6307 0.6675 0.6488 0.6824 0.7327 0.7940 0.7077 0.7821 0.6800 0.7315 0.6274 0.6831 0.6340 0.6772 0.7225 0.7904 0.7364 0.8009 0.6800 0.7379,0,,False
HD-LSTM,0,,False
0.6404 0.7123 0.6744 0.7511 0.7520 0.8146 0.7499 0.8153 0.7042 0.7733,0,,False
Table 5: Experimental Results of all Deep Learning Architectures on TREC QA Dataset. Best result is in boldface.,1,TREC,True
"LSTM are 4% and 5% in terms of MAP and MRR respectively which can be considered signi cant. We also note that there is quite significant improvement with using multi-layered LSTMs over a single layered LSTM. e performance of MV-LSTM is competitively similar to NTN-LSTM in this task. However, it is good to note that MV-LSTM takes  30s per epoch at the bilinear se ing as opposed to our model's  0.1s epoch with the same LSTM con gurations and se ings and on the same machine and GPU. On the other hand, we see that the baseline LSTM models perform worse than CNN based models whereby a single layer LSTM performs poorly and does almost as poor as CNN with logistic regression from Yu et al. [35]. However, the NTN-LSTM and MV-LSTM perform be er than the CNTN. It is good to note that our CNN model implementation achieves slightly worst results as compared to [20] because model parameters are saved at the batch level in their work while we evaluate at an epoch level instead. Nevertheless, the performance is quite similar.",1,MAP,True
"Finally, Table 6 shows the results of all published models including non deep learning systems. Evidently, deep learning has signi cantly outperformed traditional methods in this task. It is also good to note that HD-LSTM outperforms all models (both deep learning and non-deep learning) even with the smaller TRAIN dataset. We nd this result remarkable.",1,ad,True
Model,0,,False
MAP MRR,1,MAP,True
Wang et al. (2007) [29] Heilman and Smith (2010) [5] Wang and Manning (2010) [28] Yao (2013) [33] Severyn & Moschi i (2013) [21] Yih et al. (2013) [34] Yu et al. (2014) [35] Severyn et al. (2015) [20] HD-LSTM TRAIN HD-LSTM TRAIN-ALL,0,,False
0.6029 0.6091 0.5951 0.6307 0.6781 0.7092 0.7113 0.7459,0,,False
0.7520 0.7499,0,,False
0.6852 0.6917 0.6951 0.7477 0.7358 0.7700 0.7846 0.8078 0.8146,0,,False
0.8153,0,,False
Table 6: Performance Comparison of all Published Models on TREC QA Dataset.,1,TREC,True
7 EMPIRICAL EVALUATION ON,0,,False
COMMUNITY-BASED QA,0,,False
"In this experiment, we consider the task of community-based question answering (CQA). We use the Yahoo QA Dataset6 for this purpose. e objectives of this experiment are two-fold. First, we provide more experimental evidence of the QA ranking capabilities of our model. Second, we test all models on the Yahoo QA dataset which can be considered as a large web-scale dataset with a diverse range of topics which additionally includes informal social language.",1,Yahoo,True
7.1 Experimental Setup,0,,False
"We describe the dataset used, evaluation metrics and implementation details",0,,False
# QA Pairs # Correct,0,,False
TRAIN 253440,0,,False
50688,0,,False
DEV,0,,False
31680,0,,False
6336,0,,False
TEST,0,,False
31680,0,,False
6336,0,,False
Table 7: Dataset Statistics of Yahoo QA Dataset.,1,Yahoo,True
"7.1.1 Dataset. e dataset we use is the Yahoo QA Dataset containing 142,627 questions and answers. We select QA pairs containing questions and best answers of length 5-50 tokens a er",1,Yahoo,True
"ltering away non-alphanumeric characters. As such, we obtain 63, 360 QA pairs in the end. e total vocabulary size |V | of this dataset is 116,900. We construct negative samples for each question by sampling 4 samples from the top 1000 hits obtained via Lucene7 search. e overall statistics of the constructed dataset is shown in Table 7. In general, we can consider Yahoo to be a much larger dataset over TREC QA. Furthermore, in CQA, the questions and answers are generally of longer length.",1,Yahoo,True
"7.1.2 Baselines and Implementation Details. For this experiment, our comparison against competitors are similar to the rst experiment. Speci cally, we compare our model with LSTM (baseline), MV-LSTM (Bilinear) and NTN-LSTM for LSTM-based deep learning models along with CNN and CNTN. In addition, we include the popular Okapi BM25 benchmark [19] as an indicator of the",1,ad,True
"6h p://webscope.sandbox.yahoo.com/catalog.php?datatype,l&did,10 7h p://lucene.apache.org/core/",0,,False
702,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"di culty of the test set. Note that our experimental results would be naturally di erent from [25] due to di erent train/test/dev splits and variations in the negative sampling process. e implementation details for LSTM based deep learning models are the same as Section 6.1.4. However, due to scalability reasons and the requirement of processing signi cantly much more QA pairs, we limit the dimensions of the LSTM and hidden layer to be 50. e number of layers of the LSTM is also set to 1. For all models, we only consider a single direction LSTM. e other hyperparameters, including the choice of pretrained word embeddings, dropout and regularization are the same unless stated otherwise.",0,,False
7.1.3 Evaluation Metrics. For this experiment we use the same,0,,False
"metrics as [25], namely the Precision@1 and MRR. P@1 can be",0,,False
danednAe+diassthN1e,0,,False
"N i ,1",0,,False
ground,0,,False
"(r (A+) , 1) where  truth. For the sake of",0,,False
"is the indicator function brevity, we do not restate",0,,False
MRR as it is already de ned in Section 6.1.2. Note that we only,1,ad,True
consider the ranking of the ground truth amongst all the negative,0,,False
samples for a given question.,0,,False
7.2 Experimental Results,0,,False
"Table 8 shows the results of the experiments on Yahoo QA Dataset. We show that HD-LSTM achieves state-of-the-art performance on the Yahoo QA Dataset. First, we notice that the performance of Okapi BM25 model is only marginal compared to random guessing.",1,Yahoo,True
is signi es that the testing set is indeed a di cult one.,0,,False
Model,0,,False
P@1 MRR,0,,False
Random Guess Okapi BM-25 CNN CNTN LSTM NTN-LSTM HD-LSTM,0,,False
0.2000 0.2250 0.4125 0.4654 0.4875 0.5448,0,,False
0.5569,0,,False
0.4570 0.4927 0.6323 0.6687 0.6829 0.7309,0,,False
0.7347,0,,False
Table 8: Experimental Results on Yahoo QA Dataset.,1,Yahoo,True
"Unfortunately, we were not able to obtain any results with MVLSTM due to computational restrictions. Speci cally, each training instance involves 5000 matching computations to be made. Hence, each epoch takes easily  3 hours even with GPUs. Hence, from the perspective of practical applications, we can safely eliminate the MV-LSTM as an alternative. Once again, we see the trend that tensor layer improves results over their base models similar to the earlier evaluation on the TREC QA task. However, unlike the TREC datasets, the NTN-LSTM performs signi cantly be er than the baseline LSTM probably due to the larger dataset. On the other hand, we also observe that the LSTM performs be er compared to CNN on this dataset similar to the results reported in [25]. Finally, our HD-LSTM performs the best and outperforms the NTN-LSTM despite having less parameters and being more e cient as discussed in our complexity analysis section earlier.",1,ad,True
8 ANALYSIS OF HYPERPARAMETERS,0,,False
"In this section, we discuss some important observations in our experiments. such as hidden layer size on our model. In particular, we investigate the HD-LSTM, NTN-LSTM and the baseline LSTM.",0,,False
"Due to the lack of space, we only report the hyperparameter tuning process of the TREC QA task speci cally with respect to the MAP metric.",1,TREC,True
8.1 E ect of Embedding Dimension,0,,False
Figure 3: E ect of QA Embedding Size (LSTM Dimensions).,0,,False
"Figure 3 shows the in uence of the size of the QA embedding on MAP performance on TREC TRAIN dataset. We investigate the NTN-LSTM at three di erent k levels (the number of tensor slices). We see that NTN-LSTM outperforms LSTM and HD-LSTM when the dimensionality of QA embeddings are small. is is because the introduction of extra parameters of quadratic scale at the tensor layer helps the NTN-LSTM t the data. However, when increasing the dimensions of the sentence embedding, HD-LSTM starts to steadily outperform the NTN-LSTM. Furthermore, we note that at higher LSTM dimensions, i.e., 640, there is a steep decline in the performance of the NTN-LSTM probably due to over ing. Overall, the performance of the baseline LSTM cannot be compared to both the HD-LSTM and NTN-LSTM. Evidently, the method used to model the relationship between the embedding of text pairs is crucial and has implications on the entire network. We see that the holographic composition allows more representational freedom in the LSTM by allowing it to have larger dimensions of text representations without possible implications.",1,MAP,True
8.2 E ect of Hidden Layer Size,0,,False
Figure 4: E ect of the Size of Hidden Layer on MAP.,1,MAP,True
"e number of nodes at the hidden layer is an important hyperparameter in our experiments due to its close proximity and direct interaction with the composition between question and answer embeddings. Note that the hidden layer size is directly related to the number of parameters of the model. In this section, we aim",0,,False
703,0,,False
Session 6B: Conversations and Question Answering,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"to study the in uence of the size of the hidden layer with respect to the LSTM and HD-LSTM. Note that we are unable to directly compare with the NTN-LSTM as the tensor M in the NTN layer acts like a hidden layer. Figure 4 shows the e ect of the number of hidden nodes. Evidently, we see that a smaller hidden layer size bene ts the HD-LSTM. On the other hand, the performance of LSTM is only decent above a certain threshold of hidden layer size.",0,,False
"is is in concert with our understandings of the interactions of the parameters with the composition layer. Our model requires less parameters to model the relationship between text pairs because the correlation between question and answer embeddings is modeled via holographic composition. We see that our model achieves good results even with a smaller hidden layer, i.e., 64. Contrarily, LSTM requires more parameters to model the relationship between the text embeddings. We see that HD-LSTM with a small hidden layer produces the best results.",0,,False
9 CONCLUSION,0,,False
"We proposed a novel deep learning architecture based on holographic associative memories for learning to rank QA pairs. e circular correlation of vectors has a ractive qualities such as memory e ciency and rich representational learning. Additionally, we overcome the problem of scaling QA representations while keeping the compositional parameters low which is prevalent in models that adopt a tensor layer. We also outperform many variants of deep learning architectures including the NTN-LSTM and CNTN in the task of learning to rank for question answering applications.",1,ad,True
REFERENCES,0,,False
"[1] Adam L. Berger, Rich Caruana, David Cohn, Dayne Freitag, and Vibhu O. Mi al. 2000. Bridging the lexical chasm: statistical approaches to answer- nding. In SIGIR. 192­199. DOI: h p://dx.doi.org/10.1145/345508.345576",0,,False
"[2] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014. Open estion Answering with Weakly Supervised Embedding Models. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD 2014, Nancy, France, September 15-19, 2014. Proceedings, Part I. 165­180. DOI:h p://dx.doi.org/10.1007/978-3-662-44848-9 11",0,,False
"[3] D. Gabor. 1969. Associative Holographic Memories. IBM J. Res. Dev. 13, 2 (March 1969), 156­ 159. DOI:h p://dx.doi.org/10.1147/rd.132.0156",0,,False
"[4] Hua He, Kevin Gimpel, and Jimmy J Lin. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks.",0,,False
"[5] Michael Heilman and Noah A. Smith. 2010. Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to estions. In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 2-4, 2010, Los Angeles, California, USA. 1011­1019.",0,,False
"[6] Sepp Hochreiter and Ju¨rgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735­1780.",0,,False
"[7] Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional Neural Network Architectures for Matching Natural Language Sentences. In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, ebec, Canada. 2042­2050.",1,ad,True
"[8] Jiwoon Jeon, W. Bruce Cro , and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 2005 ACM CIKM International Conference on Information and Knowledge Management, Bremen, Germany, October 31 - November 5, 2005. 84­90. DOI:h p://dx.doi.org/10.1145/1099554.1099572",0,,False
[9] Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. CoRR abs/1412.6980 (2014).,0,,False
"[10] Anh Tuan Luu, Yi Tay, Siu Cheung Hui, and See-Kiong Ng. 2016. Learning Term Embeddings for Taxonomic Relation Identi cation Using Dynamic Weighting Neural Network. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016. 403­413.",0,,False
"[11] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Je rey Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. 3111­3119.",1,ad,True
"[12] Jonas Mueller and Aditya yagarajan. 2016. Siamese Recurrent Architectures for Learning Sentence Similarity. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 2786­2792.",0,,False
"[13] Maximilian Nickel, Lorenzo Rosasco, and Tomaso A. Poggio. 2016. Holographic Embeddings of Knowledge Graphs. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 1955­1961.",0,,False
"[14] Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He, Jianshu Chen, Xinying Song, and Rabab K. Ward. 2016. Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis and Application to Information Retrieval. IEEE/ACM Trans. Audio, Speech & Language Processing 24, 4 (2016), 694­707. DOI:h p://dx.doi.org/10.1109/TASLP. 2016.2520371",0,,False
"[15] Ankur P. Parikh, Oscar Ta¨ckstro¨m, Dipanjan Das, and Jakob Uszkoreit. 2016. A Decomposable A ention Model for Natural Language Inference. In Proceedings of the 2016 Conference on",0,,False
"Empirical Methods in Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4, 2016. 2249­2255. [16] Tony Plate. 1992. Holographic Recurrent Networks. In Advances in Neural Information Processing Systems 5, [NIPS Conference, Denver, Colorado, USA, November 30 - December 3, 1992]. 34­41. [17] Tony A. Plate. 1995. Holographic reduced representations. IEEE Trans. Neural Networks 6, 3 (1995), 623­641. DOI:h p://dx.doi.org/10.1109/72.377968 [18] Xipeng Qiu and Xuanjing Huang. 2015. Convolutional Neural Tensor Network Architecture for Community-Based estion Answering. In Proceedings of the Twenty-Fourth International Joint Conference on Arti cial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-31, 2015. 1305­1311. [19] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3. In Proceedings of e ird Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994. 109­126. [20] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, Santiago, Chile, August 9-13, 2015. 373­382. DOI:h p://dx.doi.org/10.1145/2766462.2767738 [21] Aliaksei Severyn, Alessandro Moschi i, Manos Tsagkias, Richard Berendsen, and Maarten de Rijke. 2014. A syntax-aware re-ranker for microblog retrieval. In e 37th International",1,ad,True
"ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '14, Gold Coast , QLD, Australia - July 06 - 11, 2014. 1067­1070. DOI:h p://dx.doi.org/10.1145/2600428. 2609511 [22] Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Y. Ng. 2013. Reasoning With Neural Tensor Networks for Knowledge Base Completion. In Advances in Neural",0,,False
Information Processing Systems 26: 27th Annual Conference on Neural Information Processing,0,,False
"Systems 2013. Proceedings of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States. 926­934. [23] Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Po s. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. Citeseer. [24] Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to Rank Answers to Non-Factoid estions from Web Collections. Computational Linguistics 37, 2 (2011), 351­383. DOI:h p://dx.doi.org/10.1162/COLI a 00051 [25] Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng. 2016. A Deep Architecture for Semantic Matching with Multiple Positional Sentence Representations. In Proceedings of the irtieth AAAI Conference on Arti cial Intelligence, February 12-17, 2016, Phoenix, Arizona, USA. 2835­2841. [26] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering. In Proceedings of the 53rd Annual Meeting of the Association",1,ad,True
for Computational Linguistics and the 7th International Joint Conference on Natural Language,0,,False
"Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 2: Short Papers. 707­712. [27] Kai Wang, Zhaoyan Ming, and Tat-Seng Chua. 2009. A syntactic tree matching approach to",0,,False
nding similar questions in community-based qa services. In Proceedings of the 32nd Annual,0,,False
"International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2009, Boston, MA, USA, July 19-23, 2009. 187­194. DOI:h p://dx.doi.org/10.1145/1571941. 1571975 [28] Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and estion Answering. In COLING 2010, 23rd",0,,False
"International Conference on Computational Linguistics, Proceedings of the Conference, 23-27 August 2010, Beijing, China. 1164­1172. [29] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint",0,,False
"Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, June 28-30, 2007, Prague, Czech Republic. 22­32. [30] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2016. Modeling Document Novelty with Neural Tensor Network for Search Result Diversi cation. In Proceedings of the",1,Novelty,True
"39th International ACM SIGIR conference on Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy, July 17-21, 2016. 395­404. DOI:h p://dx.doi.org/10.1145/2911451. 2911498 [31] Caiming Xiong, Victor Zhong, and Richard Socher. 2016. Dynamic Coa ention Networks For",0,,False
"estion Answering. CoRR abs/1611.01604 (2016). [32] Xiaobing Xue, Jiwoon Jeon, and W. Bruce Cro . 2008. Retrieval models for question and",0,,False
"answer archives. In Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2008, Singapore, July 20-24, 2008. 475­482. DOI:h p://dx.doi.org/10.1145/1390334.1390416 [33] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In Human Language Technologies:",0,,False
"Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA. 858­867. [34] Wen-tau Yih, Ming-Wei Chang, Christopher Meek, and Andrzej Pastusiak. 2013. estion Answering Using Enhanced Lexical Semantic Models. In Proceedings of the 51st Annual Meet-",0,,False
"ing of the Association for Computational Linguistics, ACL 2013, 4-9 August 2013, So a, Bulgaria, Volume 1: Long Papers. 1744­1753. [35] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. CoRR abs/1412.1632 (2014). [36] Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011. Phrase-Based Translation Model for",0,,False
estion Retrieval in Community estion Answer Archives. In e 49th Annual Meeting of,0,,False
"the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA. 653­662.",0,,False
704,0,,False
,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Word-Entity Duet Representations for Document Ranking,0,,False
Chenyan Xiong,0,,False
"Language Technologies Institute Carnegie Mellon University Pi sburgh, PA 15213, USA cx@cs.cmu.edu",0,,False
Jamie Callan,0,,False
"Language Technologies Institute Carnegie Mellon University Pi sburgh, PA 15213, USA callan@cs.cmu.edu",0,,False
Tie-Yan Liu,0,,False
"Microso Research Beijing 100080, P.R. China tie-yan.liu@microso .com",0,,False
ABSTRACT,0,,False
"is paper presents a word-entity duet framework for utilizing knowledge bases in ad-hoc retrieval. In this work, the query and documents are modeled by word-based representations and entitybased representations. Ranking features are generated by the interactions between the two representations, incorporating information from the word space, the entity space, and the cross-space connections through the knowledge graph. To handle the uncertainties from the automatically constructed entity representations, an attention-based ranking model AttR-Duet is developed. With back-propagation from ranking labels, the model learns simultaneously how to demote noisy entities and how to rank documents with the word-entity duet. Evaluation results on TREC Web Track ad-hoc task demonstrate that all of the four-way interactions in the duet are useful, the a ention mechanism successfully steers the model away from noisy entities, and together they signi cantly outperform both word-based and entity-based learning to rank systems.",1,ad-hoc,True
KEYWORDS,0,,False
"Word-Entity Duet, Entity-based Search, Explicit Semantics, Text Representation, Document Ranking.",0,,False
1 INTRODUCTION,1,DUC,True
"Utilizing knowledge bases in text-centric search is a recent breakthrough in information retrieval [5]. e rapid growth of information extraction techniques and community e orts have generated large scale general domain knowledge bases, such as DBpedia and Freebase. ese knowledge bases store rich semantics in semistructured formats and have great potential in improving text understanding and search accuracy.",0,,False
"ere are many possible ways to utilize knowledge bases' semantics in di erent components of a search system. ery representation can be improved by introducing related entities and their texts to expand the query [4, 20]. Document representation can be enriched by adding the annotated entities into the document's vector space model [17, 21, 23]. e ranking model can also be improved by utilizing the entities and their a ributes to build additional connections between query and documents [14, 19]. e",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080768",1,ad,True
rich choices of available information and techniques raise a new challenge of how to use all of them together and fully explore the potential of knowledge graphs in search engines.,0,,False
"is work proposes a new framework for utilizing knowledge bases in information retrieval. Instead of centering around words and using the knowledge graph as an additional resource, this work treats entities equally with words, and represents the query and documents using both word-based and entity-based representations.",1,ad,True
"us the interaction of query and document is no longer a `solo' of their words, but a `duet' of their words and entities. Working together, the word-based and entity-based representations form a four-way interaction: query words to document words (Qw-Dw), query entities to document words (Qe-Dw), query words to document entities (Qw-De), and query entities to document entities (Qe-De). is leads to a general methodology for incorporating knowledge graphs into text-centric search systems.",1,ad,True
"e rich and novel ranking evidence from the word-entity duet does come with a cost. Because it is created automatically, the entity-based representation also introduces uncertainties. For example, an entity can be mistakenly annotated to a query, and may mislead the search system. is paper develops an a ention-based ranking model, AttR-Duet, that employs a simple a ention mechanism to handle the noise in the entity representation. e matching component of AttR-Duet focuses on ranking with the word-entity duet, while its a ention component focuses on steering the model away from noisy entities. Trained directly from relevance judgments, AttR-Duet learns how to demote noisy entities and how to rank documents with the word-entity duet simultaneously.",1,ad,True
"e e ectiveness of AttR-Duet is demonstrated on ClueWeb Category B corpora and TREC Web Track queries. On both ClueWeb09B and ClueWeb12-B13 , AttR-Duet outperforms previous wordbased and entity-based ranking systems by at least 14%. We demonstrate that the entities bring additional exact match and so match ranking signals from the knowledge graph; all entity-based rankings perform similar or be er compared to solely word-based rankings. We also nd that, when the automatically-constructed entity representations are not as clean, the a ention mechanism is necessary for the ranking model to utilize the ranking signals from the knowledge graph. Jointly learned, the a ention mechanism is able to demote noisy entities and distill the ranking signals, while without such puri cation, ranking models become vulnerable to noisy entity representations, and the mixed evidence from the knowledge graph may be more of a distraction than an asset.",1,ClueWeb,True
"In the rest of this paper, Section 2 discusses related work; Section 3 presents the word-entity duet framework for utilizing knowledge graphs in ranking; Section 4 is about the a ention-based ranking model; Experimental se ings and evaluations are described in Section 5 and 6; e conclusions and future work are in Section 7.",0,,False
763,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
2 RELATED WORK,0,,False
"ere is a long history of research on utilizing semantic resources to improve information retrieval. Controlled vocabulary based information retrieval uses a set of expert-created index terms (mostly organized in ontologies) to represent query and documents. e retrieval is then performed by query and document's overlaps in the controlled vocabulary space, and the human knowledge in the controlled vocabulary is included. Controlled vocabulary is almost a necessity in some special domains. For example, in medical search where queries are o en about diseases, treatments, and genes, the search intent may not be covered by the query words, so external information about medical terms is needed. esauruses and lexical resources such as WordNet have also been used to address this issue. Synonyms and related concepts stored in these resources can be added to queries and documents to reduce language varieties and may improve the recall of search results [12].",1,ad,True
"Recently, large general domain knowledge bases, such as DBpedia [11] and Freebase [1], have emerged. Knowledge bases contain human knowledge about real-world entities, such as descriptions, a ributes, types, and relationships, usually in the form of knowledge graphs. ey share the same spirit with controlled vocabularies but are usually created by community e orts or information extraction systems, thus are o en at a larger scale. ese knowledge bases provide a new opportunity for search engines to be er `understand' queries and documents. Many new techniques have been developed to explore their potential in various components of ad-hoc retrieval.",1,ad-hoc,True
"An intuitive way is to use the texts associated with related entities to form be er query representations. Wikipedia contains well-wri en entity descriptions and can be used as an external and cleaner pseudo relevance feedback corpus to obtain be er expansion terms [24]. e descriptions of related Freebase entities have been utilized to provide be er expansion terms; the related entities are retrieved by entity search [3], or selected from top retrieved documents' annotations [8]. e text elds of related entities can also be used to provide expanded learning to rank features: Entity",1,Wiki,True
"ery Feature Expansion (EQFE) expands the query using the texts from related entities' a ributes, and these expanded texts generate rich ranking features [4].",0,,False
"Knowledge bases also provide additional connections between query and documents through related entities. Latent Entity Space (LES) builds an unsupervised retrieval model that ranks documents based on their textual similarities to latent entities' descriptions [14]. EsdRank models the connections between query to entities, and entities to documents using various information retrieval features.",1,ad,True
"ese connections are utilized by a latent space learning to rank model, which signi cantly improved state-of-the-art learning to rank methods [19].",0,,False
"A recent progress is to build entity-based representations for texts. Bag-of-entities representations built from entity annotations have been used in unsupervised retrieval models, including vector space models [21] and language models [17]. e entity-based so match was studied by Semantics-Enabled Language Model (SELM); it connects query and documents in the entity space using their entities' relatedness calculated from an entity linking system [6].",1,LM,True
ese unsupervised entity-based retrieval models perform be er,0,,False
"or can be e ectively combined with word-based retrieval models. Recently, Explicit Semantic Ranking (ESR) performs learning to rank with query and documents' entity representations in scholar search [23]. ESR rst trains entity embeddings using a knowledge graph, and then converts the distances in the embedding space to exact match and so match ranking features, which signi cantly improved the ranking accuracy of semanticscholar.org.",0,,False
3 WORD-ENTITY DUET FRAMEWORK,0,,False
"is section presents our word-entity duet framework for utilizing knowledge bases in search. Given a query q and a set of candidate documents D ,"" {d1, ..., d |D | }, our framework aims to provide a systematic approach to be er rank D for q, with the help of a knowledge graph (knowledge base) G. In the framework, query and documents are represented by two representations, one wordbased and one entity-based (Section 3.1). e two representations' interactions create the word-entity duet and provide four matching components (Section 3.2).""",0,,False
3.1 Word and Entity Based Representations,0,,False
"Word-based representations of query and document are standard bag-of-words: Qw(w) ,"" tf(w, q), and Dw(w) "","" tf(w, d). Each dimension in the bag-of-words Qw and Dw corresponds to a word w. Its weight is the word's frequency (tf) in the query or document.""",0,,False
"A standard approach is to use multiple elds of a document, for example, title and body. Each document eld is usually represented by a separate bag-of-words, for example, Dwtitle and Dwbody, and the ranking scores from di erent elds are combined by ranking models. In this work, we assume that a document may have multiple",0,,False
"elds. However, to make notation simpler, the eld notation is omi ed in the rest of this paper unless necessary.",0,,False
"Entity-based representations are bag-of-entities constructed from entity annotations [21]: Qe(e) ,"" tf(e, q) and De(e) "","" tf(e, d), where e is an entity linked to the query or the document. We use automatic entity annotations from an entity linking system to construct the bag-of-entities [21].""",0,,False
"An entity linking system nds the entity mentions (surface forms) in a text, and links each surface form to a corresponding entity. For example, the entity `Barack Obama' can be linked to the query `Obama Family Tree'. `Obama' is the surface form.",0,,False
Entity linking systems usually contain two main steps [7]:,0,,False
"(1) Spo ing: To nd surface forms in the text, for example, to identify the phrase `Obama'.",0,,False
"(2) Disambiguation: To link the most probable entity from the candidates of each surface form, for example, choosing `Barack Obama' from all possible Obama-related entities.",0,,False
"A commonly used information in spo ing is the linked probability (lp), the probability of a surface form being annotated in a training corpus, such as Wikipedia. A higher lp means the surface form is more likely to be linked. For example, `Obama' should have a higher lp than `table'. e disambiguation usually considers two factors.",1,Wiki,True
"e rst is commonness (CMNS), the universal probability of the surface form being linked to the entity. e second is the context in the text, which provides additional evidence for disambiguation. A con dence score is usually assigned to each annotated entity by",1,ad,True
764,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Ranking features from query words to document words (title and body) (Qw-Dw).,0,,False
Feature Description BM25 TF-IDF Boolean OR Boolean And Coordinate Match Language Model (Lm) Lm with JM smoothing Lm with Dirichlet smoothing Lm with two-way smoothing Total,0,,False
Dimension 2 2 2 2 2 2 2 2 2 18,0,,False
Table 2: Ranking features from query entities (name and description) to document words (title and body) (Qe-Dw).,0,,False
Feature Description BM25 TF-IDF Boolean Or Boolean And Coordinate Match Lm with Dirichlet Smoothing Total,0,,False
Dimension 4 4 4 4 4 4 24,0,,False
"the entity linking system, based on spo ing and disambiguation scores.",0,,False
"e bag-of-entities is not the set of surface forms that appear in the text (otherwise it is not much di erent from phrase-based representation). Instead, the entities are associated with rich semantics from the knowledge graph. For example, in Freebase, the information associated with each entity includes (but is not limited to) its name, alias, type, description, and relations with other entities. e entity-based representation makes these semantics available when matching query and documents.",1,ad,True
3.2 Matching with the Word-Entity Duet,0,,False
"By adding the entity based representation into the search system, the ranking is no longer a solo match between words, but a wordentity duet that includes four di erent ways a query can interact with a document: query words to document words (Qw-Dw); query entities to document words (Qe-Dw); query words to document entities (Qw-De); and query entities to document entities (Qe-De). Each of them is a matching component and generates unique ranking features to be used in our ranking model.",1,ad,True
"ery Words to Document Words (Qw-Dw): is interaction has been widely studied in information retrieval. e matches of Qw and Dw generate term-level statistics such as term frequency and inverse document frequency. ese statistics are combined in various ways by standard retrieval models, for example, BM25, language model (Lm), and vector space model. is work applies these standard retrieval models on document title and body elds to extract the ranking features Qw-Dw in Table 1.",0,,False
Table 3: Ranking features from query words to document entities (name and description) (Qw-De).,0,,False
Feature Description Top 3 Coordinate Match on Title Entities Top 5 Coordinate Match on Body Entities Top 3 TF-IDF on Title Entities Top 5 TF-IDF on Body Entities Top 3 Lm-Dirichlet on Title Entities Top 5 Lm-Dirichlet on Body Entities Total,0,,False
Dimension 6 10 6 10 6 10 48,0,,False
Table 4: Ranking features from query entities to document's title and body entities (Qe-De).,0,,False
Feature Description,0,,False
Dimension,0,,False
"Binned translation scores, 1 exact match bin, 5 so match Bins in the range [0, 1).",0,,False
12,0,,False
"ery Entities to Document Words (Qe-Dw): Knowledge bases contain textual a ributes about entities, such as names and descriptions. ese textual a ributes make it possible to build cross-space interactions between query entities and document words. Specifically, given a query entity e, we use its name and description as pseudo queries, and calculate their retrieval scores on a document's title and body bag-of-words, using standard retrieval models. e retrieval scores from query entities (name or description) to document's words (title or body) are used as ranking features Qe-Dw.",0,,False
"e detailed feature list is in Table 2. ery Words to Document Entities (Qw-De): Intuitively, the",0,,False
"texts from document entities should help the understanding of the document. For example, when reading a Wikipedia article, the description of a linked entity in the article is helpful for a reader who does not have the background knowledge about the entity.",1,ad,True
"e retrieval scores from the query words to document entities' name and descriptions are used to model this interaction. Di erent from Qe-Dw, in Qw-De, not all document entities are related to the query. To exclude unnecessary information, only the highest retrieval scores from each retrieval model are included as features:",0,,False
"Qw-De  max-k({score(q, e)|e  De}).",0,,False
"score(q, e) is the score of q and document entity e from a retrieval model. max-k() takes the k biggest scores from the set. Applying retrieval models on title and body entities' names and descriptions, the ranking features Qw-De in Table 3 are extracted. We choose a smaller k for title entities as titles are short and rarely have more than three entities.",0,,False
"ery Entities to Document Entities (Qe-De): ere are two ways the interactions in the entity space can be useful. e exact match signal addresses the vocabulary mismatch of surface forms [17, 21]. For example, two di erent surface forms, `Obama' and `US President', are linked to the same entity `Barack Obama' and thus are matched. e so match in the entity space is also useful. For example, a document that frequently mentions `the white house' and `executive order' may be relevant to the query `US President'.",1,ad,True
765,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"We choose a recent technique, Explicit Semantic Ranking (ESR), to model the exact match and so match in the entity space [23]. ESR rst calculates an entity translation matrix of the query and document using entity embeddings. en it gathers ranking features from the matrix by histogram pooling. ESR was originally applied in scholar search and its entity embeddings were trained using domain speci c information like author and venue.",0,,False
"In the general domain, there is much research that aims to learn entity embeddings from the knowledge graph [2, 13]. We choose the popular TransE model which is e ective and e cient to be applied on large knowledge graphs [2].",0,,False
"Given the triples (edges) from the knowledge graph (eh, p, et ), including eh and et the head entity and the tail entity, and p the edge type (predicate), TransE learns entity and relationship embeddings (eì and pì) by optimizing the following pairwise loss:",1,ad,True
"[1 + ||eìh + pì - eìt ||1 - ||eìh + pì - eìt ||1]+,",0,,False
"(eh,p,et )G (eh,p,et )G",0,,False
"where [·]+ is the hinge loss, G is the set of existing edges in the knowledge graph, and G is the randomly sampled negative instances. e loss function ensures that entities in similar graph structures are mapped closely in the embedding space, using the compositional assumption along the edge: eìh + pì , eìt .",0,,False
"e distance between two entity embeddings describes their similarity in the knowledge graph [2]. Using L1 similarity, a translation matrix can be calculated:",0,,False
"T (ei , ej ) , 1 - ||eìi - eìj ||1.",0,,False
(1),0,,False
T is the |Qe| × |De| translation matrix. ei and ej are the entities in the query and the document respectively.,0,,False
"en the histogram pooling technique is used to gather querydocument matching signals from T [9, 23]:",0,,False
"Sì(De) ,"" max T (e, De)""",0,,False
e Qe,0,,False
"Bk (Sì(De)) , log I (stk  Sìj (De) < edk ).",0,,False
j,0,,False
"Sì(d) is the max-pooled |De| dimensional vector, whose jth dimension is the maximum similarity of the jth document entity to any query entities. Bk () is the kth bin that counts the number of translation scores in its range [stk , edk ).",0,,False
"We use the same six bins as in the ESR paper: [1, 1], [0.8, 1),",0,,False
"[0.6, 0.8), [0.4, 0.6), [0, 2, 0.4), [0, 0, 2). e rst bin is the exact",0,,False
match bin and is equivalent to the entity frequency model [21].,0,,False
e other bin scores capture the so match signal between query,0,,False
and documents at di erent levels. ese bin scores generated the,0,,False
ranking features Qe-De in Table 4.,0,,False
3.3 Summary,0,,False
"e word-entity duet incorporates various semantics from the knowledge graph: e textual a ributes of entities are used to model the cross-space interactions (Qe-Dw and Qw-De); the relations in the knowledge graphs are used to model the interactions in the entity space (Qe-De), through the knowledge graph embedding.",1,corpora,True
e word-based retrieval models are also included (Qw-Dw).,0,,False
Table 5: Attention features for query entities.,0,,False
Feature Description Entropy of the Surface Form Is the Most Popular Candidate Entity Margin to the Next Candidate Entity Embedding Similarity with ery Total,0,,False
Dimension 1 1 1 1 4,0,,False
"Many prior methods are included in the duet framework. For example, the query expansion methods using Wikipedia or Freebase represent the query using related entities, and then use these entities' texts to build additional connections with the document's text [4, 20, 24]; the latent entity space techniques rst nd a set of highly related query entities, and then rank documents using their connections with these entities [14, 19]; the entity based ranking methods model the interactions between query and documents in the entity space using exact match [17, 21] and so match [23].",1,Wiki,True
"Each of the four interactions generates a set of ranking signals. A straightforward way is to use them as features in learning to rank models. However, the entity representations may include noise and generate misleading ranking signals, which motivates our AttR-Duet ranking model in the next section.",1,ad,True
4 ATTENTION BASED RANKING MODEL,0,,False
"Unlike bag-of-words, entity-based representations are constructed using automatic entity linking systems. It is inevitable that some entities are mistakenly annotated, especially in short queries where there is less context for disambiguation. If an unrelated entity is annotated to the query, it will introduce misleading ranking features; documents that match the unrelated entity might be promoted. Without additional information, ranking models have li le leverage to distinguish the useful signals brought in by correct entities from those by the noisy ones, and their accuracies might be limited.",1,ad,True
"We address this problem with an a ention based ranking model AttR-Duet. It rst extracts a ention features to describe the quality of query entities. en AttR-Duet builds a simple a ention mechanism using these features to demote noisy entities. e a ention and the matching of query-documents are trained together using back-propagation, enabling the model to learn simultaneously how to weight entities of varying quality and how to rank with the wordentity duet. e a ention features are described in Section 4.1. e details of the ranking model are discussed in Section 4.2.",1,ad,True
4.1 Attention Features,0,,False
Two groups of a ention features are extracted for each query entity to model its annotation ambiguity and its closeness to the query.,0,,False
"Annotation Ambiguity features describe the risk of an entity annotation. ere is a risk that the linker may fail to disambiguate the surface form to the correct entity, especially when the surface form is too ambiguous. For example, `Apple' in a short query can be the fruit or the brand. It is risky to put high a ention on it. ere are three ambiguity features used in AttR-Duet.",0,,False
"e rst feature is the entropy of the surface form. Given a training corpus, for example, Wikipedia, we gather the probability of a surface form being linked to di erent candidate entities, and",1,Wiki,True
766,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Qw-Dw Qw-De 1D-CNN,0,,False
"AttR-Duet: ,",0,,False
Obama Family,0,,False
Tree Qe-Dw Qe-De 1D-CNN,0,,False
Concatenation,0,,False
`Barack Obama',0,,False
`Family Tree',0,,False
Matching Feature Matching Model,0,,False
Dot Product,0,,False
1D-CNN,0,,False
Concatenation 1D-CNN,0,,False
Obama Family Tree,0,,False
`Barack Obama' `Family Tree',0,,False
Attention Model Attention Feature,0,,False
Figure 1: e Architecture of the Attention based Ranking Model for Word-Entity Duet (AttR-Duet). e le side models the query-document matching in the word-entity duet. e right side models the importances of query entities using attention features. ey together produce the nal ranking score.,0,,False
"calculate the entropy of this probability. e higher the entropy is, the more ambiguous the surface form is, and the less a ention the model should put on the corresponding query entity. e second feature is whether the annotated entity is the most popular candidate of the surface form, i.e. has the highest commonness score (CMNS). e third feature is the di erence between the linked entity's CMNS to the next candidate entity's.",0,,False
"A closeness a ention feature is extracted using the distance between the query entity and the query words in an embedding space. An entity and word joint embedding model are trained on a corpus including the original documents and the documents with surface forms replaced by linked entities. e cosine similarity between the entity embedding to the query embedding (the average of its words' embeddings) is used as the feature. Intuitively, a higher similarity score should lead to more a ention.",1,ad,True
"e full list of entity a ention features, A (e), is listed in Table 5.",0,,False
4.2 Model,0,,False
"e architecture of AttR-Duet is illustrated in Figure 1. It produces a ranking function f (q, d) that re-ranks candidate documents D for the query q, with the ranking features in Table 1-4 and a ention features in Table 5. f (q, d) is expected to weight query elements more properly and rank document more accurately.",0,,False
"Model inputs: Suppose the query contains words {w1, ..., wn } and entities {e1, ... , em }, there are four input feature matrices: Rw , Re , Aw , and Ae . Rw and Re are the ranking feature matrices for query words and entities in the document. Aw and Ae are the a ention feature matrices for words and entities. ese matrices' rows are feature vectors previously described:",0,,False
"Rw (wi , ·) , Qw-Dw(wi ) Qw-De(wi )",0,,False
(2),0,,False
"Re (ej , ·) , Qe-Dw(ej ) Qe-De(ej )",0,,False
(3),0,,False
"Aw (wi , ·) , 1",0,,False
(4),0,,False
"Ae (ej , ·) , A (ej ).",0,,False
(5),0,,False
"Qw-Dw, Qw-De, Qe-Dw, and Qe-De are the ranking features from the word-entity duet, as described in Section 3. concatenates the two",0,,False
"feature vectors of a query element. A (ej ) is the a ention features for entity ej (Table 5). In this work, we use uniform word a ention (Aw ,"" 1), because the main goal of the a ention mechanism is to handle the uncertainty in the entity representations.""",0,,False
"e matching part contains two Convolutional Neural Networks (CNN's). One matches query words to d (Rw ); the other one matches query entities to d (Re ). e convolution is applied on the query element (word/entity) dimension, assuming that the ranking evidence from di erent query words or entities should be treated the same. e simplest setup with one 1d CNN layer, 1 lter, and linear activation function can be considered as a linear model applied `convolutionally' on each word or entity:",0,,False
"Fw (wi ) ,"" Wwm · Rw (wi , ·) + bwm""",0,,False
(6),0,,False
"Fe (ej ) ,"" Wem · Re (ej , ·) + bem .""",0,,False
(7),0,,False
"Fw (wi ) and Fe (ej ) are the matching scores from query word wi and query entity ej , respectively. e matching scores from all query words form an n dimensional vector Fw , and those from entities form an m dimensional vector Fe . Wwm,Wem, bwm , and bem are the matching parameters to learn.",0,,False
e attention part also contains two CNN's. One weights query,0,,False
words with Aw and the other one weights query entities with Ae . e same convolution idea is applied as the a ention features on,0,,False
each query word/entity should be treated the same.,0,,False
e simplest set-ups with one CNN layer are:,0,,False
"w (wi ) ,"" ReLU(Wwa · Aw (wi , :) + bwa )""",0,,False
(8),0,,False
"e (ej ) ,"" ReLU(Wea · Ae (ej , :) + bea ).""",0,,False
(9),0,,False
"w (wi ) and e (ej ) are the a ention weights on word wi and entity ej . {Wwa ,Wea, bwa , bea } are the a ention parameters to learn. ReLU activation is used to ensure non-negative a ention weights.",0,,False
e matching scores can be negative because only the di erences between documents' matching scores ma er.,0,,False
e nal ranking score combines the matching scores using the a ention scores:,0,,False
"f (q, d) , Fw · w + Fe · e .",0,,False
(10),0,,False
e training is done by optimizing the pairwise hinge loss:,0,,False
"l(q, D) ,",0,,False
"[1 - f (q, d) + f (q, d )]+.",0,,False
(11),0,,False
d D+ d D-,0,,False
D+ and D- are the set of relevant documents and the set of irrel-,0,,False
"evant documents. [·]+ is the hinge loss. e loss function can be optimized using back-propagation in the neural network, and the",0,,False
matching part and the a ention part are learned simultaneously.,0,,False
767,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
5 EXPERIMENTAL METHODOLOGY,0,,False
"is section describes the experiment methodology, including dataset, baselines, and the implementation details of our methods.",0,,False
"Dataset: Ranking performances were evaluated on the TREC Web Track ad-hoc task, the standard benchmark for web search. TREC 2009-2012 provided 200 queries for ClueWeb09, and TREC 2013-2014 provided 100 queries for ClueWeb12. e Category B of both corpora (ClueWeb09-B and ClueWeb12-B13) and corresponding TREC relevance judgments were used.",1,TREC,True
"On ClueWeb09-B, the SDM runs provided by EQFE [4] are used as the base retrieval. It is a well-tuned Galago-based implementation and performs be er than Indri's SDM. All their se ings are inherited, including spam ltering using waterloo spam score (with a threshold of 60), INQUERY plus web-speci c stopwords removal, and KStemming. On ClueWeb12-B13, not all queries' rankings are available from prior work, and Indri's SDM performs similarly to language model. For simplicity, the base retrieval on ClueWeb12B13 used is Indri's default language model with KStemming, INQUERY stopword removal, and no spam ltering. All our methods and learning to rank baselines re-ranked the rst 100 documents from the base retrieval.",1,ClueWeb,True
"e ClueWeb web pages were parsed using Boilerpipe1. e `KeepEverythingExtractor' was used to keep as much text from the web page as possible, to minimize the parser's in uence. e documents were parsed to two elds: title and body. All the baselines and methods implemented by ourselves were built upon the same parsed results for fair comparisons.",1,ClueWeb,True
e Knowledge Graph used in this work is Freebase [1]. e query and document entities were both annotated by TagMe [7]. No,0,,False
"lter was applied on TagMe's results; all annotation were kept. is is the most widely used se ing of entity-based ranking methods on ClueWeb [17, 19, 21].",1,ClueWeb,True
"Baselines included standard word-based baselines: Indri's language model (Lm), sequential dependency model (SDM), and two state-of-the-art learning to rank methods: RankSVM2 [10] and coordinate ascent (Coor-Ascent3) [15]. RankSVM was trained and evaluated using a 10-fold cross-validation on each corpus. Each fold was split to train (80%), develop (10%), and test (10%). e develop part was used to tune the hyper-parameter c of the linear SVM from the set {1e - 05, 0.0001, 0.001, 0.01, 0.03, 0.05, 0.07, 0.1, 0.5, 1}. Coor-Ascent was trained using RankLib's recommended se ings, which worked well in our experiments. ey used the same word based ranking features as in Table 1.",0,,False
"Entity-based ranking baselines were also compared. EQFE [4], EsdRank [19], and BOE-TagMe [21] runs are provided on their authors' websites. e comparisons with EQFE and EsdRank were mainly done on ClueWeb09 as the full ranking results on ClueWeb12 are not publicly available. BOE-TagMe is the best TagMe based runs, which is TagMe-EF on ClueWeb09-B and TagMe-COOR on ClueWeb12-B13 [21]. Explicit Semantic Ranking (ESR) was implemented by ourselves as originally it was only applied on scholar search [23].",1,ClueWeb,True
1h ps://github.com/kohlschu er/boilerpipe 2h ps://www.cs.cornell.edu/people/tj/svm light/svm rank.html 3h ps://sourceforge.net/p/lemur/wiki/RankLib/,1,wiki,True
"ere are also other unsupervised entity-based systems [14, 17, 20]; it is unfair to compare them with supervised methods.",0,,False
"Evaluation was done by NDCG@20 and ERR@20, the o cial TREC Web Track ad-hoc task evaluation metrics. Statistical signi cances were tested by permutation test with p< 0.05.",1,TREC,True
"Feature Details: All parameters in the unsupervised retrieval model features were kept default. All texts were reduced to lower case, punctuation was discarded, and standard INQUERY stopwords were removed. Document elds included title and body, both parsed by Boilerpipe. Entity textual elds included name and description. When extracting Qw-De features, if a document did not have enough entities (3 in title or 5 in body), the feature values were set to -20.",0,,False
"e TransE embeddings were trained using Fast-TransX library4. e embedding dimension used is 50. When extracting the a ention features in Table 5, the word and entity joint embeddings were obtained by training a skip-gram model on the candidate documents using Google's word2vec [16] with 300 dimensions; the surface form's statistics were calculated from Google's FACC1 annotation [8]. Model Details: AttR-Duet was evaluated using 10-fold cross validation with the same partitions as RankSVM. Deeper neural networks were explored but did not provide much improvement so the simplest CNN se ing was used: 1 layer, 1 lter, linear activation for the ranking part, and ReLU activation for the a ention part. All CNN's weights were L2-regularized. Regularization weights were selected from the set {0, 0.001, 0.01, 0.1} using the develop fold in the cross validation. Training loss was optimized using the Nadam algorithm [18]. Our implementation was based on Keras. To facilitate implementation, input feature matrices of query elements were padded to the maximum length with zeros. Batch training was used, given the small size of training data. Using a common CPU, the training took 4-8 hours to converge on ClueWeb09-B and 2-4 hours on ClueWeb12-B13. e testing is e cient as the neural network is shallow. e document annotations, TransE embeddings, and surface form information can be obtained o line. ery entity linking is e cient given the short query length. If the embedding results and entities' texts are maintained in memory, the feature extraction is of the same complexity as typical learning to rank features.",1,ad,True
"e rankings, evaluation results, and the data used in our experiments are available online at h p://boston.lti.cs.cmu.edu/appendices/ SIGIR2017 word entity duet/.",0,,False
6 EVALUATION RESULTS,0,,False
is section rst evaluates the overall ranking performances of the word-entity duet with a ention based learning to rank. en it analyzes the two parts of AttR-Duet: Matching with the wordentity duet and the a ention mechanism.,0,,False
6.1 Overall Performance,0,,False
"e overall accuracies of AttR-Duet and baselines are shown in Table 6. Relative performances over RankSVM are shown in percentages. Win/Tie/Loss are the number of queries a method improves, does not change, and hurts, compared with RankSVM on NDCG@20.",0,,False
4h ps://github.com/thunlp/Fast-TransX,0,,False
768,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 6: Overall accuracies of AttR-Duet and baselines. (U) and (S) indicate unsupervised or supervised method. (E) indicates that information from entities is used. Relative performances compared with RankSVM are shown in percentages. Win/Tie/Loss are the number of queries a method improves, does not change, or hurts, compared with RankSVM on NDCG@20. Best results in each metric are marked bold. § marks statistically signi cant improvements (p< 0.05) over all baselines.",0,,False
Method,0,,False
Lm,0,,False
(U),0,,False
SDM,0,,False
(U),0,,False
RankSVM,0,,False
(S),0,,False
Coor-Ascent (S),0,,False
BOE-TagMe (UE),0,,False
ESR,0,,False
(SE),0,,False
EQFE5,0,,False
(SE),0,,False
EsdRank5,0,,False
(SE),0,,False
AttR-Duet (SE),0,,False
ClueWeb09-B,1,ClueWeb,True
NDCG@20,0,,False
ERR@20,0,,False
0.1757 -33.33% 0.1195 -22.63%,0,,False
0.2496 -5.26% 0.1387 -10.20%,0,,False
0.2635,0,,False
­ 0.1544,0,,False
­,0,,False
0.2681 +1.75% 0.1617 +4.72%,0,,False
0.2294 -12.94% 0.1488 -3.63%,0,,False
0.2695 +2.30% 0.1607 +4.06%,0,,False
0.2448 -7.10% 0.1419 -8.10%,0,,False
0.2644 +0.33% 0.1756 +13.69% 0.3197§ +21.32% 0.2026§ +31.21%,0,,False
W/T/L 47/28/125 62/38/100,0,,False
­/­/­ 71/47/82 74/25/101 80/39/81 77/33/90 88/28/84,0,,False
101/37/62,0,,False
ClueWeb12-B13,1,ClueWeb,True
NDCG@20,0,,False
ERR@20,0,,False
0.1060 -12.02% 0.0863 -6.67%,0,,False
0.1083 -10.14% 0.0905 -2.08%,0,,False
0.1205,0,,False
­ 0.0924,0,,False
­,0,,False
0.1206 +0.08% 0.0947 +2.42%,0,,False
0.1173 -2.64% 0.0950 +2.83%,0,,False
0.1166 -3.22% 0.0898 -2.81%,0,,False
,0,,False
­ n/a,0,,False
­,0,,False
,0,,False
­ n/a,0,,False
­,0,,False
0.1376§ +14.22% 0.1154§ +24.92%,0,,False
W/T/L 35/22/43 27/25/48,0,,False
­/­/­ 36/32/32 44/19/37 30/23/47,0,,False
­/­/­ ­/­/­,0,,False
45/24/31,0,,False
"Table 7: Ranking accuracy with each group of matching feature from the word-entity duet. Base Retrieval is SDM on ClueWeb09 and Lm on ClueWeb12. LeToR-Qw-Dw uses the query and document's BOW (Table 1). LeToR-Qe-Dw uses the query's BOE and document's BOW (Table 2), LeToR-Qw-De is the query BOW + document BOE (Table 3), and LeToR-Qe-De uses the query and document's BOE (Table 4). LeToR-All uses all groups. Relative performances in percentages, Win/Tie/Loss on NDCG@20, and statistically signi cant improvements () are all compared with Base Retrieval.",1,ClueWeb,True
Method Base Retrieval LeToR-Qw-Dw LeToR-Qe-Dw LeToR-Qw-De LeToR-Qe-De LeToR-All,0,,False
ClueWeb09-B,1,ClueWeb,True
NDCG@20,0,,False
ERR@20,0,,False
0.2496 0.2635,0,,False
0.2729 0.2867 0.2695 0.3099,0,,False
--,0,,False
+5.55% +9.33% +14.83% +7.97% +24.13%,0,,False
0.1387 0.1544 0.1824 0.1651 0.1607 0.1955,0,,False
--,0,,False
+11.36% +31.51% +19.07% +15.88% +40.97%,0,,False
W/T/L ­/­/­ 100/38/62 82/34/84 91/39/70 99/40/61 103/38/59,0,,False
ClueWeb12-B13,1,ClueWeb,True
NDCG@20,0,,False
ERR@20,0,,False
0.1060,0,,False
-- 0.0863,0,,False
--,0,,False
0.1205 +13.67% 0.0924 +7.14%,0,,False
0.1110 +4.66% 0.0928 +7.63%,0,,False
0.1146 +8.09% 0.0880 +1.96%,0,,False
0.1166 +10.01% 0.0898 +4.13%,0,,False
0.1205 +13.69% 0.1000 +15.93%,0,,False
W/T/L ­/­/­ 43/22/35 40/20/40 42/20/38 38/20/42 47/19/34,0,,False
Best results in each metric are marked Bold. § indicates statistically signi cant improvements over all available baselines.,0,,False
"AttR-Duet outperformed all baselines signi cantly by large margins. On ClueWeb09-B, a widely studied benchmark for web search, AttR-Duet improved RankSVM, a strong learning to rank baseline, by more than 20% at NDCG@20, and more than 30% at ERR@20, showing the advantage of the word-entity duet over bag-of-words. ESR, EQFE and EsdRank, previous state-of-the-art entity-based ranking methods, were outperformed by at least 15%. It is not surprising because the word-entity duet framework was designed to include all of their e ects, as discussed at Section 3.3. ClueWeb12-B13 has been considered a hard dataset due to its noisy corpus and harder queries. e size of its training data is also smaller, which limits the strength of our neural network. However, AttR-Duet still signi cantly outperformed all available baselines by at least 14%. e information from entities is e ective and also di erent with those from words: AttR-Duet in uences more than three-quarters of the queries, and improves the majority of them.",1,ClueWeb,True
"5Ranking results are obtained from the authors' websites. ClueWeb09-B scores are higher than in original papers [4, 19] as we evaluate them using TREC's Category B qrels. e original papers used Category A's qrels although they ranked Category B documents. EQFE and EsdRank's ClueWeb12 results are not available as they were only evaluated on the rst 50 queries of the 100.",1,ClueWeb,True
6.2 Matching with Word-Entity Duet,0,,False
"In a sophisticated system like AttR-Duet, it is hard to tell the contributions of di erent components. is experiment studies how each of the four-way interactions in the word-entity duet contributes to the ranking performance individually. For each group of the matching features in Table 1- 4, we train a RankSVM individually, which resulted in four ranking models: LeToR-Qw-Dw, LeToR-Qe-Dw, LeToR-Qw-De, and LeToR-Qe-De. LeToR-All which uses all ranking features is also evaluated. In LeToR-Qw-De and LeToR-Qe-De, the score of the base retrieval model is included as a feature, so that there is a feature to indicate the strength of the word-based match for the whole document. All these methods were trained and tested in the same se ing as RankSVM. As a result, LeToR-Qw-Dw is equivalent to the RankSVM baseline, and LeToR-Qe-De is equivalent to the ESR baseline.",0,,False
"eir evaluation results are listed in Table 7. Relative performances (percentages), Win/Tie/Loss, and statistically signi cant improvements () are all compared with Base Retrieval (SDM on ClueWeb09 and Lm on ClueWeb12). All four groups of matching features were able to improve the ranking accuracy of Base Retrieval when used individually as ranking features, demonstrating the usefulness of all matching components in the duet. On ClueWeb09-B, all three entity related components, LeToR-Qe-Dw,",1,ClueWeb,True
769,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 8: Examples of entities used in Qw-De and Qe-De. e rst half are examples of matched entities in relevant and irrelevant documents, which are used to extract Qw-De features. e second half are examples of entities falls into the exact match bin and the closest so match bins, used to extract Qe-De features.",0,,False
ery Uss Yorktown Charleston SC Flushing,0,,False
Examples of Most Similar Entities to the ery,0,,False
Top Entities in Relevant Documents Top Entities in Irrelevant Documents,0,,False
`USS Yorktown (CV-10)',0,,False
"`Charles Cornwallis', `USS Yorktown (CV-5)'",0,,False
"`Roosevelt Avenue', `Flushing, eens'",0,,False
"`Flushing (physiology)', `Flush (cards)'",0,,False
Examples of Neighbors in Knowledge Graph Embedding,0,,False
ery,0,,False
Entities in Exact Match Bin,0,,False
Entities in So Match Bins,0,,False
"Uss Yorktown Charleston SC `USS Yorktown (CV-10)', `Charleston, SC'",0,,False
"`Empire of Japan', `World War II'",0,,False
Flushing,0,,False
"`Flushing, eens'",0,,False
"`Brooklyn', `Manha an', `New York'",0,,False
Relative NDCG@20 Relative NDCG@20,0,,False
20%,0,,False
12%,0,,False
15%,0,,False
10%,0,,False
5%,0,,False
0%,0,,False
-5%,0,,False
ClueWeb09,1,ClueWeb,True
Top 1 Scores,0,,False
Top 2 Scores,0,,False
Top 4 Scores,0,,False
Top 5 Scores,0,,False
ClueWeb12,1,ClueWeb,True
Top 3 Scores,0,,False
9%,0,,False
6%,0,,False
3%,0,,False
0% ClueWeb09,1,ClueWeb,True
Exact Bins,0,,False
First 2 Bins,0,,False
First 4 Bins,0,,False
First 5 Bins,0,,False
ClueWeb12,1,ClueWeb,True
First 3 Bins All Bins,0,,False
(a) Features from ery Words to Document Entities (Qw-De) (b) Features from ery Entities to Document Entities (Qe-De),0,,False
"Figure 2: Incremental ranking feature analysis. e y-axis is the relative NDCG@20 improvement over the base retrieval. e x-axis refers to the features from only top k (1-5) entity match scores (2a), and the features from only rst k (1-6) bins in the ESR model (2b), both ordered incrementally from le to right.",0,,False
"LeToR-Qw-De, and LeToR-Qe-De, provided similar or be er performances than the word-based RankSVM. When all features were used together, LeToR-All signi cantly improved RankSVM by 17% and 26% on NDCG@20 and ERR@20, showing that the ranking evidence from di erent parts of the duet can reinforce each other.",0,,False
"On ClueWeb12-B13, entity-based matching was less e ective. LeToR-All's NDCG@20 was the same as RankSVM's, despite additional matching features. e di erence is that the annotation quality of TagMe on ClueWeb12 queries is lower (Table 9) [21]. e noisy entity representation may mislead the ranking model, and prevent the e ective usage of entities. To deal with this uncertainty is the motivation of the a ention based ranking model, which is studied in Section 6.4.",1,ClueWeb,True
6.3 Matching Feature Analysis,0,,False
"e features from the word space (Qw-Dw) are well understood, and the feature from the query entities to document words (Qe-Dw) have been studied in prior research [4, 14, 19]. is experiment analyzes the features from the two new components (Qw-De and Qe-De).",0,,False
"Qw-De features match the query words with the document entities. For each document, the query words are matched with the textual elds of document entities using retrieval models, and the highest scores are Qw-De features.",0,,False
"We performed an incremental feature analysis of LeToR-Qw-De. Starting with the highest scored entities from each group in Table 3, we incrementally added the next highest ones to the model and evaluated the ranking performance. e results are shown in Figure 2a. e y-axis is the relative NDCG@20 improvements over the base retrieval model. e x-axis is the used features. For example, `Top 3 Scores' uses the top 3 entities' retrieval scores in each row of Table 3.",1,ad,True
"e highest scores were very useful. Simply combining them with the base retrieval provided nearly 10% gain on ClueWeb09-B and about 7% on ClueWeb12-B13. Adding the following scores was not that stable, perhaps because the corresponding entities were rather noisy, given the simple retrieval models used to match query words with them. Nevertheless, the top 5 scores together further improve the ranking accuracy.",1,ClueWeb,True
"e rst half of Table 8 shows examples of entities with highest matching scores. We found that such `top' entities from relevant documents are frequently related to the query, for example, `Roosevelt Avenue' is an avenue across Flushing, NY. In comparison, entities from irrelevant documents are much noisier. Qw-De features make use of this information and generate useful ranking evidence.",0,,False
"Qe-De features are extracted using the Explicit Semantic Ranking (ESR) method [23]. ESR is built upon the translation model. It operates in the entity space, and extracts the ranking features using histogram pooling. ESR was originally applied on scholar search.",0,,False
770,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 9: ery annotation accuracy and the gain of attention mechanism. TagMe Accuracy includes the precision and recall of TagMe on ClueWeb queries, evaluated in prior research [21]. Attention Gains are the relative improvements of AttR-Duet compared with LeToR-All. Statistical signi cant gains are marked by .",1,ClueWeb,True
ClueWeb09 ClueWeb12,1,ClueWeb,True
TagMe Accuracy Precision Recall,0,,False
0.581 0.597 0.460 0.555,0,,False
Attention Gain,0,,False
NDCG@20 ERR@20,0,,False
+3.16%,0,,False
+3.65%,0,,False
+14.20% +15.45%,0,,False
Number of Queries Attention Relative Gain,0,,False
Number of Queries Attention Relative Gain,0,,False
NDCG ERR,0,,False
150,0,,False
8%,0,,False
6% 100,0,,False
4% 50,0,,False
2%,0,,False
0,0,,False
0%,0,,False
1,0,,False
2,0,,False
3,0,,False
(a) ClueWeb09-B,1,ClueWeb,True
NDCG ERR 50,0,,False
40,0,,False
40%,0,,False
30,0,,False
20,0,,False
20%,0,,False
10,0,,False
0,0,,False
0%,0,,False
1,0,,False
2,0,,False
3,0,,False
(b) ClueWeb12-B13,1,ClueWeb,True
"Figure 3: Attention mechanism's gain on queries that contain di erent number of entities. e x-axis is the number of entities in the queries. e y-axis is the number of queries in each group (histogram), and the gain from attention (plots).",0,,False
is work introduces ESR into web search and uses TransE model to train general domain embeddings from the knowledge graph.,0,,False
"To study the e ectiveness of ESR in our se ing, we also performed an incremental feature analysis of LeToR-Qe-De. Starting from the rst bin (exact match), the following bins (so matches) were incrementally added into RankSVM, and their rankings were evaluated. e results are shown in Figure 2b. e y-axis is the relative NDCG@20 over the base retrieval model they re-rank. e x-axis is the features used. For example, First 3 Bins refers to using the rst three bins: [1, 1], [0.8, 1), and [0.6, 0.8).",1,ad,True
"e observation on scholar search [23] holds on ClueWeb: Both exact match and so match with entities are useful. e exact match bin provided a 7% improvement on ClueWeb09-B, while only 2% on ClueWeb12-B13. Similar exact match results were also observed in a prior study [21]. It is another re ection of the entity annotation quality di erences on the two datasets. Adding the later bins almost always improves the ranking accuracy, especially on ClueWeb12.",1,ClueWeb,True
"e second half of Table 8 shows some examples of entities in the exact match bin and the nearest so match bins. e exact match bin includes the query entities and is expected to help. e rst so match bin usually contains related entities. For example, the neighbors of `USS Yorktown (CV-10)' include `World War II' which is when the ship was built. e further bins are mostly background noise because they are too far away. e improvements are mostly from the rst 3 bins.",0,,False
Table 10: Examples of learned attention. e entities in bold blue draw more attention; those in gray draw less attention.,0,,False
ery Balding Cure,0,,False
Nicolas Cage Movies Hawaiian Volcano Observatories Magnesium Rich Foods Kids Earth Day Activities,0,,False
Entity Attention,0,,False
`Cure' `Clare Balding',0,,False
`Nicolas Cage' `Pokemon (Anime)' `Volcano'; `Observatory' `Hawaiian Narrative'; `Magnesium'; `Food' `First World',0,,False
`Earth Day' `Youth Organizations in the USA',0,,False
6.4 Attention Mechanism Analysis,0,,False
"e last experiment studies the e ect of the a ention mechanism by comparing AttR-Duet with LeToR-All. If enforcing at a ention weights on all query words and entities, AttR-Duet is equivalent to LeToR-All: e matching features, model function, and loss function are all the same. e a ention part is their only di erence, whose e ect is re ected in this comparison.",0,,False
"e gains from the a ention mechanism are shown in Table 9. To be er understand the a ention mechanism's e ectiveness in demoting noisy query entities, the query annotation's quality evaluated in a prior work [21] is also listed. e percentages in the A ention Gain columns are relative improvements of AttR-Duet compared with LeToR-All.  marks statistical signi cance. Figure 3 breaks down the relative gains to queries with di erent numbers of query entities. e x-axis is the number of query entities. e histograms are the number of queries in each group, marked by the le y-axis.",0,,False
"e plots are the relative gains, marked by the right y-axis. e a ention mechanism is essential to ClueWeb12-B13. With-",1,ClueWeb,True
"out the a ention model, LeToR-All was confused by the noisy query entities and could not provide signi cant improvements over word-based models, as discussed in the last experiment. With the a ention mechanism, AttR-Duet improved LeToR-All by about 15%, outperforming all baselines. On ClueWeb09 where TagMe's accuracy is be er [21], the ranking evidence from the word-entity duet was clean enough for LeToR-All to improve ranking, so the a ention mechanism's e ect was smaller. Also, in general, the a ention mechanism is more e ective when there are more query entities, while if there is only one entity there is not much to tweak.",1,ClueWeb,True
"e motivation for using a ention is to handle the uncertainties in the query entities, a crucial challenge in utilizing knowledge bases in search. ese results demonstrated its ability to do so. We also found many intuitive examples in the learned a ention weights, some listed in Table 10. e bold blue entities on the rst line of each block gain more a ention (> 0.6 a ention score). ose in gray on the second line draw less a ention (< 0.4 score). e a ention mechanism steers the model away from those mistakenly linked query entities, which makes it possible to utilize the correct entities' ranking evidence from a noisy representation.",0,,False
771,0,,False
Session 7B: Entities,1,Session,True
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
7 CONCLUSIONS AND FUTURE WORK,0,,False
"is work presents a word-entity duet framework for utilizing knowledge bases in document ranking. In this paper, the query and documents are represented by both word-based and entitybased representations. e four-way interactions between the two representation spaces form a word-entity duet that can systematically incorporate various semantics from the knowledge graph. From query words to document words (Qw-Dw), word-based ranking features are included. From query entities to document entities (Qe-De), entity-based exact match and so match evidence from the knowledge graph structure are included. e entities' textual elds are used in the cross-space interactions Qe-Dw, which expands the query, and Qw-De, which enriches the document.",1,corpora,True
"To handle the uncertainty introduced from the automatic-thusnoisy entity representations, a new ranking model AttR-Duet is developed. It employs a simple a ention mechanism to demote the ambiguous or o -topic query entities, and learns simultaneously how to weight entities of varying quality and how to rank documents with the word-entity duet.",0,,False
"Experimental results on the TREC Web Track ad-hoc task demonstrate the e ectiveness of proposed methods. AttR-Duet signi cantly outperformed all word-based and entity-based ranking baselines on both ClueWeb corpora and all evaluation metrics. Further experiments reveal that the strength of the method comes from both the advanced matching evidence from the word-entity duet, and the a ention mechanism that successfully `puri es' them. On ClueWeb09 where the query entities are cleaner, all the entity related matching components from the duet provide similar or be er improvements compared with word-based features. On ClueWeb12 where the query entities are noisier, the a ention mechanism steers the ranking model away from noisy entities and is necessary for stable improvements.",1,TREC,True
"Our method provides a uni ed representation framework to utilize knowledge graphs in information retrieval. As the rst step, this work kept its components as simple as possible. It is easy to imagine further developments in various places. For example, the recent approaches in neural ranking with word embeddings can be incorporated [9]; be er knowledge graph embeddings can be used [13]; be er entity search methods can be applied when extracting word to entity features [3]; the a ention mechanism can be extended to document's entity-based representations. More sophisticated neural ranking models [22] can also be applied with the word-entity duet, especially when more training data are available.",1,corpora,True
8 ACKNOWLEDGMENTS,0,,False
"is research was supported by National Science Foundation (NSF) grant IIS-1422676, a Google Faculty Research Award, and a fellowship from the Allen Institute for Arti cial Intelligence. We thank Xu Han for helping us train the TransE embedding. Any opinions,",0,,False
"ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.",0,,False
REFERENCES,0,,False
"[1] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data (SIGMOD 2008). ACM, 1247­1250.",0,,False
"[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems (NIPS 2013). 2787­ 2795.",0,,False
"[3] Jing Chen, Chenyan Xiong, and Jamie Callan. 2016. An empirical study of learning to rank for entity search. In Proceedings of the 39th Annual International ACM",0,,False
"SIGIR Conference on Research and Development in Information Retrieval,(SIGIR 2016). ACM, 737­740. [4] Je rey Dalton, Laura Dietz, and James Allan. 2014. Entity query feature expansion using knowledge base links. In Proceedings of the 37th Annual International ACM",0,,False
"SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2014). ACM, 365­374. [5] Laura Dietz, Alexander Kotov, and Edgar Meij. 2017. Utilizing knowledge graphs in text-centric information retrieval. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 815­816. [6] Faezeh Ensan and Ebrahim Bagheri. 2017. Document retrieval model through semantic linking. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM 2017). ACM, 181­190. [7] Paolo Ferragina and Ugo Scaiella. 2010. Fast and accurate annotation of short texts with Wikipedia pages. arXiv preprint arXiv:1006.3498 (2010). [8] Evgeniy Gabrilovich, Michael Ringgaard, and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 201306-26, Format version 1, Correction level 0). (June 2013). [9] Jiafeng Guo, Yixing Fan, Qingyao Ai, and W.Bruce Cro . 2016. A deep relevance matching model for ad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information and Knowledge Management (CIKM 2016). ACM, 55­64. [10] orsten Joachims. 2002. Optimizing search engines using clickthrough data. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2002). ACM, 133­142. [11] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, So¨ren Auer, and Christian Bizer. 2014. DBpedia - A large-scale, multilingual knowledge base extracted from Wikipedia. Semantic Web Journal (2014). [12] Hang Li and Jun Xu. 2014. Semantic matching in search. Foundations and Trends in Information Retrieval 8 (2014), 89. [13] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the Twenty-Ninth AAAI Conference on Arti cial Intelligence (AAAI 2015). 2181­ 2187. [14] Xitong Liu and Hui Fang. 2015. Latent entity space: A novel retrieval approach for entity-bearing queries. Information Retrieval Journal 18, 6 (2015), 473­503. [15] Donald Metzler and W Bruce Cro . 2007. Linear feature-based models for information retrieval. Information Retrieval 10, 3 (2007), 257­274. [16] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Je Dean. 2013. Distributed representations of words and phrases and their compositionality. In",1,Wiki,True
"Proceedings of the 2 h Advances in Neural Information Processing Systems 2013 (NIPS 2013). 3111­3119. [17] Hadas Raviv, Oren Kurland, and David Carmel. 2016. Document retrieval using entity-based language models. In Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016). ACM, 65­74. [18] Ilya Sutskever, James Martens, George E Dahl, and Geo rey E Hinton. 2013. On the importance of initialization and momentum in deep learning.. In Proceedings of the 29th International Conference on Machine Learning (ICML 2013). 1139­1147. [19] Chenyan Xiong and Jamie Callan. 2015. EsdRank: Connecting query and documents through external semi-structured data. In Proceedings of the 24th ACM International Conference on Information and Knowledge Management (CIKM 2015). ACM, 951­960. [20] Chenyan Xiong and Jamie Callan. 2015. ery expansion with Freebase. In",1,ad,True
"Proceedings of the h ACM International Conference on the eory of Information Retrieval (ICTIR 2015). ACM, 111­120. [21] Chenyan Xiong, Jamie Callan, and Tie-Yan Liu. 2016. Bag-of-Entities representation for ranking. In Proceedings of the sixth ACM International Conference on the",0,,False
"eory of Information Retrieval (ICTIR 2016). ACM, 181­184. [22] Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power.",0,,False
2017. End-to-end neural ad-hoc ranking with kernel pooling. In Proceedings of,1,ad-hoc,True
"the 40th annual international ACM SIGIR conference on Research and Development in Information Retrieval (SIGIR 2017). ACM, To Appear. [23] Chenyan Xiong, Russell Power, and Jamie Callan. 2017. Explicit semantic ranking for academic search via knowledge graph embedding. In Proceedings of the 25th International Conference on World Wide Web (WWW 2017). ACM, 1271­1279. [24] Yang Xu, Gareth JF Jones, and Bin Wang. 2009. ery dependent pseudorelevance feedback based on Wikipedia. In Proceedings of the 32nd Annual In-",1,ad,True
"ternational ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2009). ACM, 59­66.",0,,False
772,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"On the Reusability of ""Living Labs"" Test Collections: A Case Study of Real-Time Summarization",0,,False
"Luchen Tan, Gaurav Baruah, and Jimmy Lin",0,,False
"David R. Cheriton School of Computer Science University of Waterloo, Ontario, Canada",1,ad,True
"{luchen.tan,gbaruah,jimmylin}@uwaterloo.ca",0,,False
ABSTRACT,0,,False
"Information retrieval test collections are typically built using data from large-scale evaluations in international forums such as TREC, CLEF, and NTCIR. Previous validation studies on pool-based test collections for ad hoc retrieval have examined their reusability to accurately assess the e ectiveness of systems that did not participate in the original evaluation. To our knowledge, the reusability of test collections derived from ""living labs"" evaluations, based on logs of user activity, has not been explored. In this paper, we performed a ""leave-one-out"" analysis of human judgment data derived from the TREC 2016 Real-Time Summarization Track and show that those judgments do not appear to be reusable. While this nding is limited to one speci c evaluation, it does call into question the reusability of test collections built from living labs in general, and at the very least suggests the need for additional work in validating such experimental instruments.",1,TREC,True
1 INTRODUCTION,1,DUC,True
"Test collections are indispensable experimental tools for information retrieval evaluation and play an important role in advancing the state of the art. Beyond the ability to accurately measure the e ectiveness of retrieval techniques, the reusability of test collections is an important and desirable characteristic. Test collections are o en constructed via international evaluation forums such as TREC, CLEF, and NTCIR: a reusable test collection can be used to assess systems that did not participate in the original evaluation. TREC test collections created in the 1990s are still useful today precisely because they are reusable.",1,ad,True
"Living labs [1, 9, 11, 12] refers to an emerging approach to evaluating information retrieval systems in live se ings with real users in natural task environments. Although such evaluation platforms are widespread in industry (e.g., frameworks for running large-scale A/B tests [8]), most academic researchers do not have access to them, and the living labs concept was designed to address this gap. us, the product of a living labs evaluation is a record of user actions that are then aggregated to assess the e ectiveness of the participating systems.",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080644",1,ad,True
"Following standard practices with ad hoc test collections constructed via pooling [13], it would be natural to treat these user judgments as part of a reusable test collection--that is, to evaluate systems that did not participate in the original evaluation. However, to our knowledge, the reusability of such test collections has not been explored, and it is unclear if post hoc measurements of new techniques are appropriate or accurate.",1,ad,True
"is paper explores the reusability of test collections built from living labs evaluations, using the TREC 2016 Real-Time Summarization (RTS) Track as a case study. We show--using a standard ""leave-one-out"" analysis--that user judgments cannot be used to reliably assess systems that did not participate in the original evaluation. us, we strongly caution researchers against using the RTS human judgments as a reusable test collection. To our knowledge, we are the rst to have examined this issue, and while our nding is limited to a single evaluation, this result calls into question the reusability of living labs data in general. At the very least, more research is needed to validate the appropriateness of reusing data from living labs as evaluation instruments.",1,TREC,True
2 BACKGROUND AND RELATED WORK,0,,False
"e validation of information retrieval evaluation resources (i.e., meta-evaluations) has a long history that dates back to at least the 1990s. For standard ad hoc retrieval test collections built by pooling the results of evaluation participants, researchers have examined the quality of the judgments from a variety of perspectives [13].",1,ad,True
"ere has been a long thread of research focused on reusability [2­ 4, 14]. e ndings are nuanced and reusability is a characteristic of individual test collections, but researchers are generally aware of the pitfalls of reusing relevance judgments from pooled evaluations. One useful technique to study reusability that we adopt in this paper is known as the ""leave-one-out"" analysis. We evaluate the output of a particular participant by removing its contributions to the pooled judgments--this simulates it never having participated in the original evaluation. We can then assess the impact on the participant's score. is procedure can be repeated for every participant, allowing us to broadly characterize reusability.",1,ad,True
"Although traditional pool-based ad hoc retrieval test collections are generally well-understood evaluation instruments for assessing the quality of retrieval algorithms, researchers o en nd divergences between system-centered metrics and user-focused metrics. In short, be er retrieval algorithms o en don't lead to be er task outcomes. ere is a long line of work exploring this disconnect (a full survey is beyond the scope of this short paper, but see Hersh et al. [7] for a starting point). is realization, in turn, drove researchers to consider more user-focused evaluations. e living labs idea is the latest evolution in this thread of work. In an a empt to",1,ad,True
793,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"leverage real users in realistic task environments, living labs share similar goals with Evaluation-as-a-Service (EaaS) [6] approaches that a empt to be er align evaluation methodologies with user task models and real-world constraints to increase the delity of research experiments.",0,,False
"Living labs evaluations are modeled a er online experiments in industry such as A/B tests [8] and interleaved evaluations for web search [5]. Of course, academic researchers have di culty gaining access to such experimental platforms: in this sense, living labs represent an a empt by academic researchers to replicate an evaluation framework that researchers in industry take for granted. One early a empt is described by Said et al. [11], where a company called Plista opened up their news recommender system to academic researchers, who were able to deploy their algorithms in a production se ing and receive user feedback. More recent a empts include a living labs deployment at CLEF 2015 [12] and the TREC 2016 Open Search Track [1], both of which explored vertical search using interleaved evaluations, where researchers were invited to submit their ranking algorithms. In this paper, we focus on the TREC 2016 Real-Time Summarization Track [9], a recent living labs evaluation. Unlike pool-based construction of test collections for ad hoc retrieval, to our knowledge there has not been reusability studies of data from such evaluations. As far as we are aware, researchers in industry are not concerned with this issue because they have access to large amounts of human editorial judgments (in the case of web search) and the ability to run online experiments on demand, and hence they would not have the need to reuse the results of, for example, previous interleaved ranking experiments.",1,ad,True
3 REAL-TIME SUMMARIZATION,0,,False
"e TREC 2016 Real-Time Summarization (RTS) Track tackled prospective information needs against real-time, continuous document streams, exempli ed by social media services such as Twi er. Systems are given a number of ""interest pro les"" representing users' needs (analogous to topics in ad hoc retrieval), and their task is to automatically monitor the document stream (Twi er in this case) to keep the user up to date with respect to the interest pro les.",1,TREC,True
"e evaluation speci cally considers the case where tweets are immediately pushed to users' mobile devices as noti cations. At a high level, these push noti cations must be relevant, novel, and timely. Here, we provide relevant background, but refer the reader to the track overview [9] for more details.",1,ad,True
"In order to evaluate push noti cation systems in a realistic setting, the track de ned an o cial evaluation period (from August 2, 2016 00:00:00 UTC to August 11, 2016 23:59:59 UTC) during which all participants ""listened"" to the so-called Twi er ""spritzer"" sample stream. is is putatively a 1% sample of all Twi er public posts, and is available to anyone with a Twi er account. e overall evaluation framework is shown in Figure 1. Before the evaluation period, participants ""registered"" their systems with the evaluation broker to request unique tokens (via a REST API), which are used in future requests to associate submi ed tweets with speci c systems. During the evaluation period, whenever a system identi ed a relevant tweet with respect to an interest pro le, the system submi ed the tweet id to the evaluation broker (also via a REST API), which recorded the submission time. Each system was allowed to push at",1,AP,True
Stream of Tweets,1,Tweet,True
Participating TREC RTS Systems evaluation broker,1,TREC,True
Twitter API,1,Twitter,True
Assessors,0,,False
"Figure 1: e evaluation setup for push noti cations: systems ""listen"" to the live Twitter sample stream and send results to the evaluation broker, which then delivers push noti cations to users.",1,Twitter,True
most ten tweets per interest pro le per day; this limit represents an a empt to model user fatigue.,0,,False
"e track organizers recruited a number of users who installed a custom app on their mobile devices. Prior to the beginning of the evaluation period, these users subscribed to interest pro les (i.e., topics) they wished to monitor. During the assessment period, whenever the evaluation broker received a system's submission, the tweet was immediately delivered to the mobile devices of users who had subscribed to the particular interest pro le and rendered as push noti cations--we implemented the temporal interleaving evaluation methodology for prospective information needs proposed by Qian et al. [10]. e user may choose to assess the tweet immediately, or if it arrived at an inopportune time, to ignore it. Either way, the tweet is added to a queue in the app on the user's mobile device, which she can access at any time to examine the queue of accumulated tweets. For each tweet, the user can make one of three judgments with respect to the associated interest pro le: relevant, if the tweet contains relevant and novel information; redundant, if the tweet contains relevant information, but is substantively similar to another tweet that the user had already seen; not relevant, if the tweet does not contain relevant information. As the user provides judgments, results are relayed back to the evaluation broker and recorded. ese judgments are then aggregated to assess the e ectiveness of participating systems.",1,ad,True
"Our setup has two distinct characteristics: First, judgments happen online as systems generate output, as opposed to traditional batch post-hoc evaluation methodologies, which consider the documents some time (typically, weeks) a er they have been generated by the systems. Second, our judgments are in situ, in the sense that the users are going about their daily activities (and are thus interrupted by the noti cations). is aspect of the design accurately mirrors the intended use of push noti cation systems. For these two reasons, the RTS track exempli es a living labs evaluation.",1,ad,True
4 EXPERIMENTS,0,,False
"In this paper, we analyzed data from the TREC 2016 RTS Track. In total, 18 groups from around the world participated, submi ing a total of 41 systems (runs). Over the evaluation period, these systems pushed a total of 161,726 tweets, or 95,113 unique tweets a er de-duplicating within pro les. e organizers recruited 13 users for the study, who collectively provided 12,115 judgments over the assessment period, with a minimum of 28 and a maximum of 3,791 by an individual user. Overall, 122 interest pro les received",1,TREC,True
794,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Figure 2: Results of the leave-one-out analysis. Each pair of bars represents precision before and a er the leave-one-out procedure. Error bars denote binomial con dence intervals. Darker pairs of bars represent signi cant di erences. Bars are sorted by ""a er"" precision.",0,,False
at least one judgment; 93 received at least 10 judgments; 67 received at least 50 judgments; 44 received at least 100 judgments.,0,,False
"Following the setup of the track, we evaluate systems in terms of ""strict"" precision, which is simply the fraction of user judgments that were relevant (more precisely, a micro-average across pro les).",0,,False
"e metric is ""strict"" in the sense that redundant judgments are treated as not relevant.",0,,False
4.1 Leave-One-Out Analysis,0,,False
"We began with a standard leave-one-out analysis at the group level. Since runs originating from each group are likely to be similar (e.g., same underlying algorithm but di erent parameter se ings), a group-level analysis be er matches real-world conditions.1 Speci cally, we removed the unique judgments associated with runs from a particular group to simulate what would have happened if the group had not participated in the living labs evaluation, and then evaluated runs from that group with the reduced set of judgments.",1,ad,True
"is procedure was then repeated for all the groups. In each case, we computed the precision of the runs with the reduced set of judgments (which we call the ""a er"" condition, i.e., a er the leave-one-out procedure). is precision can then be compared with the o cial (i.e., ""before"") precision using all judgments. If the judgments are reusable, we should not notice signi cant di erences in the score. at is, we would obtain an accurate measurement of precision whether or not the group had participated in the original living labs evaluation. Results of our leave-one-out analysis are shown in Figure 2. Each pair of bars represents precision before and a er the leave-one-out procedure. Error bars denote binomial con dence intervals: systems vary in the volume of tweets they push, and so the con dence intervals tend to be larger for systems that push fewer tweets. All pairs of bars are sorted by the ""a er"" precision, and the darker pairs represent signi cant di erences. From this analysis, we see that for 14 of 41 runs (approximately one third of the runs), the before and a er precision values are signi cantly di erent. ese di erences seem to be in the false positive direction, in the following sense: in trying to use RTS judgments as a reusable test collection to score a run, a researcher might obtain a score (i.e., an ""a er"" score) that is signi cantly higher than its ""true"" score (i.e., the ""before"" score). us, there is a danger of over-in ated e ectiveness. Interestingly, there doesn't appear to",1,ad,True
"1As a detail, the University of Waterloo submi ed runs that were quite similar, but using two di erent group ids. ese were collapsed into the same group.",0,,False
"Figure 3: Alternate presentation of the leave-one-out analysis results. e setup is exactly the same as in Figure 2, except that the bars are sorted by increasing push volume.",0,,False
"be a case where the ""a er"" condition under-reports the true (i.e., ""before"") precision.",0,,False
"One reasonable hypothesis might be that push volume (i.e., number of tweets that a system pushes) provides an important feature in determining whether post-hoc assessments are accurate. In Figure 3, the pairs of bars in Figure 2 are resorted by increasing push volume. e results appear to disprove this hypothesis: while high volume systems do show signi cant ""before"" vs. ""a er"" di erences, systems across the board (in terms of push volume) exhibit signi cant di erences. Note that low volume systems have large con dence intervals, and thus are less likely to exhibit signi cant di erences to begin with.",1,hoc,True
"Taken together, these results suggest that user judgments from the TREC 2016 RTS Track cannot be reused to reliably evaluate systems that did not participate in the original evaluation.",1,TREC,True
4.2 Temporal Analysis,0,,False
"Since systems pushed tweets at various times during the day, we wondered if there were temporal e ects impacting reusability. To explore this question, we performed a series of analyses focused on dropping judgments in di erent temporal segments. Speci cally, we divided each day into four-hour segments and separately dropped all judgments within that particular time window (across all days in the evaluation period). is yielded six di erent conditions, i.e., dropping judgments from 00:00 to 03:59, from 04:00 to 07:59, etc. All times are in UTC. For each of the segments, we can repeat our before/a er analysis, i.e., computing precision based on the full and reduced sets of judgments.",0,,False
"ese results are shown in Figure 4, where the pairs of bars are sorted by run id (in other words, arbitrarily), but the position of each pair of bars is consistent from plot to plot. Interestingly, we see signi cant di erences in the rst temporal segment (00:00 to 03:59) and the last temporal segment (20:00 to 23:59), but no where else. For the rst temporal segment, 12 out of 41 runs exhibit signi cant di erences, and for the nal segment, 3 out of 41 runs.",0,,False
"As a sanity check, we repeated the before/a er experiments randomly throwing away the same amount of data discarded in each of the temporal segments. For instance, 40% of the judgments are found in the rst temporal segment (00:00 to 03:59 UTC), and so as a comparison condition, we randomly discarded 40% of all judgments and repeated our before/a er analyses. e results averaged over ten distinct trials are shown in Figure 5. Over ten trials, we did not observe any signi cant di erences in any of the pairs. us, we can conclude that the signi cant di erences observed in Figure 4 are not due to simply have fewer judgments.",0,,False
795,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 4: Results of the before/a er analysis removing all judgments within a particular four-hour window. Each plot shows the before and a er precision of removing each fourhour block (in UTC). Runs are sorted by run id.,0,,False
"As yet, we have no adequate explanation for these ndings. UTC 00:00 corresponds to 20:00 local time for the users who participated in the study. While it is true that the RTS broker generally received more judgments during the evening hours throughout the evaluation period (corresponding to the rst and last temporal segments), we have con rmed above that the volume of judgments alone does not explain the signi cant di erences we observed. Judgments and system behavior (or both) around this time are somehow ""special"", in a way that we currently do not understand.",1,ad,True
5 CONCLUSIONS,0,,False
e main contribution of our work is the nding that user judgments from the TREC 2016 RTS Track do not appear to be reusable.,1,TREC,True
"at is, they cannot be used to reliably assess the e ectiveness of systems that did not participate in the original evaluation. Although our ndings are limited to a particular instance of a living",0,,False
Figure 5: Results of a before/a er analysis randomly drop-,0,,False
"ping 40% of the judgments, equal to the amount removed in",0,,False
the rst temporal segment. Runs are sorted by run id.,0,,False
"labs evaluation, it raises questions about the reusability of such test collections in general. Absent future work to the contrary, experimental results that derive from the reuse of human judgments as part of a living labs must be viewed with suspicion.",0,,False
"Taken together, these ndings unfortunately put information retrieval researchers in somewhat of a quandary: in the quest for more user-centered evaluation techniques that be er capture task-level e ectiveness, we might have compromised desirable properties of evaluation instruments previously taken for granted. We have identi ed the problem, which is an important rst step, but leave for future work how to actually "" x it"".",0,,False
REFERENCES,0,,False
"[1] Krisztian Balog, Anne Schuth, Peter Dekker, Narges Tavakolpoursaleh, Philipp Schaer, and Po-Yu Chuang. 2016. Overview of the TREC 2016 Open Search Track. In TREC.",1,TREC,True
"[2] Chris Buckley, Darrin Dimmick, Ian Soboro , and Ellen Voorhees. 2007. Bias and the Limits of Pooling for Large Collections. Information Retrieval 10, 6 (2007), 491­508.",0,,False
"[3] Stefan Bu¨ cher, Charles L. A. Clarke, Peter C. K. Yeung, and Ian Soboro . 2007. Reliable Information Retrieval Evaluation with Incomplete and Biased Judgements. In SIGIR. 63­70.",0,,False
"[4] Ben Cartere e, Evgeniy Gabrilovich, Vanja Josifovski, and Donald Metzler. 2010. Measuring the Reusability of Test Collections. In WSDM. 231­239.",0,,False
"[5] Olivier Chapelle, orsten Joachims, Filip Radlinski, and Yisong Yue. 2012. LargeScale Validation and Analysis of Interleaved Search Evaluation. ACM TOIS 30, 1 (2012), Article 6.",1,ad,True
"[6] Allan Hanbury, Henning Mu¨ller, Krisztian Balog, Torben Brodt, Gordon V. Cormack, Ivan Eggel, Tim Gollub, Frank Hopfgartner, Jayashree Kalpathy-Cramer, Noriko Kando, Anastasia Krithara, Jimmy Lin, Simon Mercer, and Martin Potthast. 2015. Evaluation-as-a-Service: Overview and Outlook. arXiv:1512.07454.",0,,False
"[7] William Hersh, Andrew Turpin, Susan Price, Benjamin Chan, Dale Kramer, Lyne a Sacherek, and Daniel Olson. 2000. Do Batch and User Evaluations Give the Same Results? In SIGIR. 17­24.",0,,False
"[8] Ron Kohavi, Randal M. Henne, and Dan Sommer eld. 2007. Practical Guide to Controlled Experiments on the Web: Listen to Your Customers not to the HiPPO. In KDD. 959­967.",0,,False
"[9] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees, and Fernando Diaz. 2016. Overview of the TREC 2016 Real-Time Summarization Track. In TREC.",1,ad,True
"[10] Xin Qian, Jimmy Lin, and Adam Roegiest. 2016. Interleaved Evaluation for Retrospective Summarization and Prospective Noti cation on Document Streams. In SIGIR. 175­184.",0,,False
"[11] Alan Said, Jimmy Lin, Alejandro Bellog´in, and Arjen P. de Vries. 2013. A Month in the Life of a Production News Recommender System. In CIKM Workshop on Living Labs for Information Retrieval Evaluation. 7­10.",0,,False
"[12] Anne Schuth, Krisztian Balog, and Liadh Kelly. 2015. Overview of the Living Labs for Information Retrieval Evaluation (LL4IR) CLEF Lab 2015. In CLEF.",1,ad,True
[13] Ellen M. Voorhees. 2002. e Philosophy of Information Retrieval Evaluation. In CLEF. 355­370.,1,CLEF,True
[14] Justin Zobel. 1998. How Reliable Are the Results of Large-Scale Information Retrieval Experiments? In SIGIR. 307­314.,0,,False
"Acknowledgments. is research was supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada, with additional contributions from the U.S. National Science Foundation under IIS-1218043 and CNS-1405688.",1,ad,True
796,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Automatically Extracting High- ality Negative Examples for Answer Selection in estion Answering,0,,False
"Haotian Zhang1, Jinfeng Rao2, Jimmy Lin1, and Mark D. Smucker3",0,,False
"1 David R. Cheriton School of Computer Science, University of Waterloo 2 Department of Computer Science, University of Maryland",0,,False
"3 Department of Management Sciences, University of Waterloo",0,,False
"{haotian.zhang,jimmylin,mark.smucker}@uwaterloo.ca,jinfeng@cs.umd.edu",0,,False
ABSTRACT,0,,False
"We propose a heuristic called ""one answer per document"" for automatically extracting high-quality negative examples for answer selection in question answering. Starting with a collection of question­answer pairs from the popular TrecQA dataset, we identify the original documents from which the answers were drawn. Sentences from these source documents that contain query terms (aside from the answers) are selected as negative examples. Training on the original data plus these negative examples yields improvements in e ectiveness by a margin that is comparable to successive recent publications on this dataset. Our technique is completely unsupervised, which means that the gains come essentially for free. We con rm that the improvements can be directly a ributed to our heuristic, as other approaches to extracting comparable amounts of training data are not e ective. Beyond the empirical validation of this heuristic, we also share our improved TrecQA dataset with the community to support further work in answer selection.",0,,False
1 INTRODUCTION,1,DUC,True
"ere are three key components to solving problems with machine learning: the training data, the model, and the optimization technique. To improve e ectiveness, data is o en the easiest path since in some applications it is easy to collect a large amount of data, such as user behavior logs in the web context. In contrast, improving models and optimization techniques o en require inspiration.",0,,False
"In this paper, we focus on the data dimension of improving answer selection for question answering. We propose a heuristic that we call ""one answer per document"", which yields a simple technique for extracting high-quality negative examples. Starting with question­answer pairs from the popular TrecQA dataset, one of the most widely-used collections for evaluating answer selection in question answering, we identify the original documents from which the answers are drawn. e best-matching sentences from these source documents that contain query terms (other than the answer sentences) are selected as negative examples. Training on the original data plus these negative examples yields improved",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080645",1,ad,True
"e ectiveness. Our intuition is that the answer to a question is only likely to occur once in a document, and thus other sentences in the document with query terms can serve as high-quality negative examples. We argue that these examples are particularly valuable because they lie near the decision boundary (by virtue of containing query terms). is intuition is con rmed by contrastive experiments that show alternative techniques for acquiring comparable amounts of training data are not e ective.",0,,False
"e contributions of this work are two-fold: First, we propose and empirically validate the e ectiveness of the ""one answer per document"" heuristic. Our approach is completely unsupervised, which means that gains in e ectiveness come with minimal e ort. Examining the history of improvements on this task, the gain we achieve is around the same level of e ectiveness as reported in successive recent publications on this dataset, nearly all of which come from improved modeling using neural networks. Second, our technique yields an improved and augmented version of the widely-used TrecQA dataset that we share with the community to foster further work on answer selection.",0,,False
2 BACKGROUND AND RELATED WORK,0,,False
"Answer selection is an important component of an overall question answering system: given a question q and a candidate set of sentences {s1, s2, . . . sn }, the task is to identify sentences that contain the answer. In a standard pipeline architecture [12], answer selection is applied to the output of a module that performs passage retrieval, typically using lightweight term-based matching. Selected sentences can then be directly presented to users or serve as input to subsequent stages that identify exact answers [13].",0,,False
"In recent years, researchers have had substantial success in tackling the answer selection problem with neural networks, e.g., [6, 7, 10, 11, 17]. e continuous representations that deep-learning approaches provide are e ective in combating data sparsity, a perpetual challenge in natural language processing tasks. Solutions based on neural networks represent an advance over previous approaches driven by feature engineering. Although our work is primarily about techniques for acquiring training data, we assume a deep-learning framework for evaluation purposes.",1,ad,True
"It is a well-known fact that the amount of training data drives e ectiveness in a broad range of tasks. Colloquially referred to as the ""unreasonable e ectiveness of data"" [5], researchers have been empirically examining the impact of training data for machine learning since at least the early 2000s. e seminal work of Banko and Brill [2] in examining the e ects of training data size on natural language disambiguation tasks contained a slightly subversive message, that the e ort of researchers might be be er spent gathering",1,ad,True
797,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Dataset TREC8,1,TREC,True
TREC9 TREC10,1,TREC,True
TREC11 TREC12 TREC13,1,TREC,True
Document Collections TREC disks 4&5 minus Congressional Record,1,TREC,True
AP newswire (Disks 1-3) Wall Street Journal (Disks 1-2) San Jose Mercury News (Disk 3),1,AP,True
Financial Times (Disk 4) Los Angeles Times (Disk 5) Foreign Broadcast Information Service (FBIS) (Disk 5),1,ad,True
AQUAINT disks,1,AQUAINT,True
Table 1: Source document collections for TrecQA.,0,,False
"training data as opposed to building more sophisticated models. Speci cally in the realm of question answering, researchers have long known that, all things being equal, larger collections yield higher e ectiveness due to data redundancy [3, 4].",0,,False
"In this context, our work focuses on gathering training data for answer selection in order to improve machine-learned models. Speci cally, our ""one answer per document"" heuristic is a nod to the ""one sense per discourse"" heuristic Yarowsky [16] applied to word-sense disambiguation, which dates back to the 1990s. is work is an early example of a clever technique for acquiring (noisy) labeled data for free, much like our work. e intuition behind the heuristic is that polysemous words occurring close together are unlikely to have di erent senses. For example, if the word ""bank"" occurs in nearby sentences, it is unlikely that one refers to a nancial institution and the other to the side of a river. is is an artifact of how authors naturally communicate when writing. From this heuristic Yarowsky described an approach to bootstrap a word-sense disambiguation algorithm. In the same way, our ""one answer per document"" heuristic re ects how authors write.",0,,False
"ere can, of course, be violations of this heuristic, but the point is that su cient signal can be extracted with this heuristic to aid in training machine-learned models. Our technique is closely related to what researchers today would call distant supervision, but our focus is speci cally on data acquisition.",0,,False
3 METHODS,0,,False
"In order to operationalize our ""one answer per document"" heuristic, we build on the TrecQA dataset that is broadly used as a benchmark for answer selection. Note that although this paper focuses on a speci c dataset--since one of our contributions is a resource we share with the community--the assumptions we make about the general technique are fairly minimal: simply that answer sentences are drawn from documents within some collection.",1,ad,True
"e TrecQA dataset was rst introduced by Wang et al. [14] and further elaborated by Yao et al. [15]. e dataset contains a set of factoid questions, each of which is associated with a number of candidate sentences that either contain or do not contain the answer (i.e., positive and negative examples). e questions are from the estion Answering Tracks from TREC 8­13, and the candidate answers are derived from the output of track participants, ultimately drawn from the collections listed in Table 1.",1,Track,True
"e TrecQA dataset comes pre-split into train, development, and test sets, with statistics shown in Table 2. estions from TREC",1,TREC,True
Set #,0,,False
Train Dev Test,0,,False
All,0,,False
estion,0,,False
"1,229 84 100",0,,False
"1,411",0,,False
# Pos Answers,0,,False
"6,403 222 284",0,,False
"6,909",0,,False
# Neg Answers,0,,False
"47,014 926",0,,False
"1,233",0,,False
"49,173",0,,False
Table 2: Statistics for various splits of TrecQA.,0,,False
"8­12 are used for training (1229 questions), while questions from TREC 13 are used for development (84 questions) and testing (100 questions). To generate the candidate answers for the development and test splits, sentences were selected from each question's evaluation pool that contained one or more non-stopwords from the question [14]. For generating the training candidates, in addition to the sentences that contain non-stopwords from the question, sentences that match the correct answer pa erns (from an automatic evaluation script) were also added. Data from all of TREC 13 (development and test splits) and the rst 100 questions from TREC 8­12 (training split) were manually assessed. e motivation behind the manual annotation e ort is that answer pa erns in the automatic evaluation script may yield false positives--i.e., sentences that match the pa ern may not actually contain correct answers.",1,TREC,True
"Although the TrecQA dataset was ultimately constructed from TREC evaluations, the provenance information connecting answer candidates to their source documents does not exist. erefore, to operationalize our ""one answer per document"" heuristic, we needed to ""backproject"" each answer candidate to recover its source document. Note that due to tokenization, case folding, and other sentence processing di erences, nding the answer sentence is more complex than just an exact string match.",1,TREC,True
"Answer backprojection was accomplished by rst indexing all the collections in Table 1 with Anserini,1 our information retrieval toolkit built on Lucene. We then issued each question as a query and retrieved the top 1000 hits using BM25. For each answer a, we used the shingle matching method [8, 9] to select the most likely candidate document d that contains the answer a. For an answer a, let s be the minimum span of words in a candidate document d that contains the most words from a in any order. A span s matches a well if s contains many words from a within a small window. We used the algorithm presented by Krenzel [8] to nd the shortest span s of shingle words within a document in linear time:",0,,False
Scores d,0,,False
",",0,,False
max,0,,False
|s  a|2 |s | · |a|,0,,False
(1),0,,False
"A er we nd the best matching document d for an answer a, we split the sentences in d using the NLTK Punkt sentence tokenizer.2 Equation (1) is used again to score all sentences in d; we take as the matching answer the highest scoring sentence above a threshold of 0.1. If no sentence scores above this threshold, we drop the answer from consideration. Based on spot-checking, this se ing is able to nd the source sentence with nearly perfect precision. Once we have found the source sentence, all other non-zero scoring sentences in the document provide negative examples for the answer selection task, based on our ""one answer per document"" heuristic.",0,,False
1h p://anserini.io/ 2h p://www.nltk.org/api/nltk.tokenize.html,0,,False
798,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
estion Answer,0,,False
Ranking Score Matching 0.681,0,,False
1,0,,False
0.244,0,,False
2,0,,False
0.208,0,,False
3,0,,False
0.203,0,,False
4,0,,False
0.195,0,,False
5,0,,False
0.133,0,,False
6,0,,False
0.130,0,,False
7,0,,False
0.125,0,,False
"Who is the author of the book , "" e Iron Lady : A Biography of Margaret atcher "" ? the iron lady ; a biography of margaret thatcher by hugo young -lrb- farrar , straus & giroux -rrb-",1,ad,True
Sentence,0,,False
"THE IRON LADY: A BIOGRAPHY OF MARGARET THATCHER BY HUGO YOUNG (FARRAR, STRAUS &amp; GIROUX: $25; 570 PP. In "" e Iron Lady,"" Young traces the winding staircase of fortune that transformed the younger daughter of a provincial English grocer into the greatest woman political leader since Catherine the Great. It is without question the best of a bevy of new",1,AP,True
"atcher biographies that set out the o en surprising, always dramatic story of the British political revolution of the 1980s. In this same revisionist mold, Hugo Young, the distinguished British journalist, has performed a brilliant dissection of the notion of atcher as a conservative icon.",0,,False
"e implied paradox has been nicely captured by a recent British assessment of the last six years titled "" e Free Economy and the Strong State: e Politics of atcherism"" by Andrew Gamble. It sees atcher as the new Me ernich (the 19thCentury master of the diplomatic nesse), as a power-driven politician and as a militant Puritan. Young observes that "" ere was a genuine clash of cultures, between an almost Cromwellian impatience with the status quo (on the part of the",1,ad,True
"atcherites) and the mandarin world of Whitehall, in which skepticism and rumination were more highly rated habits of mind than zeal or blind conviction.""",0,,False
"e only company nominated by atcher's team for denationalization was the National Freight Corp., and this from the people who much later made ""privatization"" one of the household words of the age.",1,ad,True
Table 3: Example backprojection of an answer to recover the source document (LA111289-0002) and the source sentence. Non-matching sentences serve as negative examples.,0,,False
"A complete example of this backprojection process is shown in Table 3. At the top we show the question and the answer we are trying to nd. First, we identi ed document LA111289-0002 as the source document. In this document, in addition to the topscoring sentence (a correct match), we also show the non-matching sentences in decreasing score order. is example illustrates why",1,ad,True
"nding the source answer requires more than an exact string match. e non-matching sentences show the intuition behind our ""one",0,,False
"answer per document"" heuristic--indeed, none of the sentences answer the question. Note that although sentence 3 contains the author ""Hugo Young"", it doesn't provide any contextual justi cation, i.e., there is no way for the reader to infer the answer in isolation. Accordingly, it should be considered a negative example.",1,ad,True
4 EVALUATION AND RESULTS,0,,False
We applied the procedure described above to backproject answer sentences from the TrecQA dataset to reconstruct their sources.,0,,False
"Operationalizing the ""one answer per document"" heuristic, nonmatching sentences from the source document containing the answer serve as negative examples we can use to augment the training data. We considered cases where m  {1, 3, 5, 7} of these top non-matching sentences are added to the training set as negative examples (see Table 3). We tokenized these sentences using the standard Penn Treebank format3 to match the original dataset.",1,ad,True
"How e ective is the ""one answer per document"" heuristic? To nd out, we trained an answer selection model using our augmented training data and compared the results with training on the original data. For this task, we used the convolutional neural network model of Severyn and Moschi i [11] (SM for short). eir model achieves competitive accuracy and the authors provide an open-source implementation.4 Following previous work, we evaluated the task in terms of Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). In order to train the model more e ciently, for negative sentences selected by our technique, we truncated their lengths to 60 tokens. All parameters and the training procedure remained the same as in the original model. We emphasize that in all our experiments, the only di erence is an augmented training set: the development set and test set remain exactly the same, thus supporting a fair comparison of results. Any di erence in e ectiveness can be directly a ributed to the training data.",1,MAP,True
"Results of this experiment are shown in Table 4. e row labeled ""Baseline SM Model"" is the result of our replication using the implementation provided by Severyn and Moschi i and in fact we obtain slightly higher e ectiveness than what they reported. e next four rows in the table show the e ects of adding di erent numbers of negative examples per each backprojected answer. For example, with m ,"" 5, we would add up to 6,403 × 5 "","" 32,015 negative examples. We see that the m "","" 1 condition reduces e ectiveness slightly, likely due to the noise introduced. Adding more sentences helps, peaking at m "","" 5, and then e ectiveness drops again.""",1,ad,True
"e intuition behind our ""one answer per document"" heuristic is that our data acquisition algorithm yields high-quality negative examples that are valuable because they lie near the decision boundary. Experimental results support this claim, but to further validate our heuristic, there are two alternative explanations to rule out: First, that these sentences might be even more useful as positive examples, and second, that the gains aren't derived from simply having more training data.",0,,False
"To explore the rst alternative explanation, we repeated the same experiment as above, augmenting the training set with m  {1, 3, 5, 7} of the top non-matching sentences, but as positive examples. Results are also shown in Table 4. We clearly see that such a treatment hurts e ectiveness for all examined values of m. is",0,,False
"nding is consistent with the assumption that the answer will only appear once in each document, thus supporting our heuristic.",0,,False
"To explore the second alternative explanation, we experimented with two di erent approaches to augmenting the training set: in the rst case, we selected ve random sentences from the answer document to serve as negative examples (and thus, they may or may not contain terms from the question), and in the second case, we randomly selected ve sentences from all documents to serve as",0,,False
3h p://www.nltk.org/ modules/nltk/tokenize/treebank.html 4h ps://github.com/aseveryn/deep-qa,0,,False
799,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Strategy,0,,False
Baseline SM Model,0,,False
Add top 1 sent as neg Add top 3 sent as neg Add top 5 sent as neg Add top 7 sent as neg,0,,False
Add top 1 sent as pos Add top 3 sent as pos Add top 5 sent as pos Add top 7 sent as pos,0,,False
Add random 5 neg sent from correct documents Add random 5 neg sent,0,,False
from all documents,0,,False
MAP,1,MAP,True
0.7538,0,,False
0.7468 0.7588 0.7612 0.7493,0,,False
0.7409 0.7193 0.7117 0.7016,0,,False
"0.7548 [0.7499, 0.7597]",0,,False
"0.7526 [0.7496, 0.7556]",0,,False
MRR,0,,False
0.8078,0,,False
0.7953 0.8012 0.8088 0.7993,0,,False
0.7974 0.7670 0.7456 0.7639,0,,False
"0.8075 [0.8038, 0.8112]",0,,False
"0.7969 [0.7883, 0.8054]",0,,False
Multi-Perspective CNN Add top 3 sent as neg Add top 5 sent as neg,0,,False
0.762 0.7864 0.7788,0,,False
0.830 0.8325 0.8316,0,,False
Table 4: Results of comparing di erent strategies.,0,,False
"negative examples. We conducted ve trials of each experimental condition so that we can compute the mean and 95% con dence intervals for both MAP and MRR. ese results are also shown in Table 4. We see that both sampling approaches have minimal e ect on e ectiveness (to be expected). Note that in these cases the neural network is trained with the same amount of data as in the negative sampling case. Combined with the above experiments, these results con rm that e ectiveness gains do not come from simply having more data, but having high-quality negative examples, thus supporting our ""one answer per document"" heuristic.",1,MAP,True
"Let us tackle the next possible criticism: that we are improving on a low baseline. As a point of reference, we can consult an ACL wiki page that nicely summarizes the state of the art in this answer selection task [1]. We clearly see that while the SM model isn't the top-performing model on this task, its e ectiveness remains competitive. To show the robustness of the e ectiveness gains that we observe, we also experimented with the multi-perspective convolutional neural network (MPCNN) architecture of He et al. [6], which also has open-source code available.5 Since this model is more complex than the SM model and hence takes longer to train, we only repeated the condition of adding m  {3, 5} negative examples. Results show gains in both conditions, with m , 3 appearing to be the be er se ing.",1,wiki,True
"Finally, let us try to contextualize the magnitude of gains that derive from our technique. e ACL wiki page [1] provides the history of e ectiveness improvements over time. Of course, in the beginning right a er this dataset was published, researchers made great strides in improving e ectiveness. However, the magnitude of advances has dramatically shrunk: in recent years, publications are reporting small gains in the second decimal point. All of these improvements are from increasingly-sophisticated neural network models. e magnitude of our observed improvements is comparable to di erences in successive recent publications on this particular dataset: for example, the improvement from He et al. [6] (published",1,wiki,True
5h ps://github.com/castorini/MP-CNN-Torch,0,,False
"in 2015) to Rao et al. [10] (published in 2016) is less than 0.02 in terms of absolute MAP. e magnitude of our gains is at least as large, and in fact, our best condition appears to be the highest reported result on TrecQA (as of this writing). Since our technique is completely unsupervised, these gains basically come for free.",1,MAP,True
"As a resource for the community, we release all data from this paper, including the source document mappings and the negative examples to augment the original TrecQA dataset.6",0,,False
5 CONCLUSIONS,0,,False
"Data, model, and optimization represent three di erent approaches to increasing the e ectiveness of machine learning solutions. is paper adopts the data approach to tackling answer selection: We begin with an intuition, the ""one answer per document"" heuristic, that we then operationalize into a data acquisition algorithm. Augmented training data improves the e ectiveness of existing models, and contrastive experiments rule out alternative explanations for our ndings, thus validating our approach. As applied to a speci c dataset, the widely-used TrecQA benchmark, our work yields an improved data resource that we share with the community. However, we believe that this heuristic is equally applicable to other tasks and datasets, a future direction that we are currently pursuing.",1,ad,True
REFERENCES,0,,False
"[1] ACL. 2017. estion Answering (State of the art). h p://www.aclweb.org/ aclwiki/index.php?title, estion Answering (State of the art). (2017). Accessed: 2017-05-01.",1,wiki,True
[2] Michele Banko and Eric Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation. In ACL. 26­33.,0,,False
"[3] Charles L. A. Clarke, Gordon Cormack, and omas Lynam. 2001. Exploiting Redundancy in estion Answering. In SIGIR. 375­383.",0,,False
"[4] Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin, and Andrew Ng. 2002. Web estion Answering: Is More Always Be er? In SIGIR. 291­298.",0,,False
"[5] Alon Halevy, Peter Norvig, and Fernando Pereira. 2009. e Unreasonable E ectiveness of Data. IEEE Intelligent Systems 24, 2 (2009), 8­12.",0,,False
"[6] Hua He, Kevin Gimpel, and Jimmy Lin. 2015. Multi-Perspective Sentence Similarity Modeling with Convolutional Neural Networks. In EMNLP. 1576­1586.",0,,False
[7] Hua He and Jimmy Lin. 2016. Pairwise Word Interaction Modeling with Neural Networks for Semantic Similarity Measurement. In NAACL-HLT. 937­948.,0,,False
[8] Steve Krenzel. 2010. Finding blurbs. h p://www.stevekrenzel.com/articles/ blurbs.,0,,False
"[9] Virgil Pavlu, Shahzad Rajput, Peter B. Golbus, and Javed A. Aslam. 2012. IR System Evaluation using Nugget-based Test Collections. In WSDM. 393­402.",1,ad,True
"[10] Jinfeng Rao, Hua He, and Jimmy Lin. 2016. Noise-Contrastive Estimation for Answer Selection with Deep Neural Networks. In CIKM. 1913­1916.",0,,False
[11] Aliaksei Severyn and Alessandro Moschi i. 2015. Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks. In SIGIR. 373­382.,0,,False
"[12] Stefanie Tellex, Boris Katz, Jimmy Lin, Gregory Marton, and Aaron Fernandes. 2003. antitative Evaluation of Passage Retrieval Algorithms for estion Answering. In SIGIR. 41­47.",0,,False
[13] Ellen M. Voorhees. 2002. Overview of the TREC 2002 estion Answering Track. In TREC.,1,TREC,True
"[14] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A asi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32.",0,,False
"[15] Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark. 2013. Answer Extraction as Sequence Tagging with Tree Edit Distance. In HLT-NAACL. 858­867.",0,,False
[16] David Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In ACL. 189­196.,0,,False
"[17] Lei Yu, Karl Moritz Hermann, Phil Blunsom, and Stephen Pulman. 2014. Deep Learning for Answer Sentence Selection. In NIPS Deep Learning Workshop.",0,,False
"Acknowledgments. is research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada and by a Google Founders Grant, with additional contributions from the U.S. National Science Foundation under CNS-1405688.",1,ad,True
6h ps://github.com/castorini/TrecQA-NegEx,0,,False
800,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Learning To Rank Resources,0,,False
Zhuyun Dai,0,,False
Carnegie Mellon University zhuyund@cs.cmu.edu,0,,False
Yubin Kim,0,,False
Carnegie Mellon University yubink@cs.cmu.edu,0,,False
Jamie Callan,0,,False
Carnegie Mellon University callan@cs.cmu.edu,0,,False
ABSTRACT,0,,False
"We present a learning-to-rank approach for resource selection. We develop features for resource ranking and present a training approach that does not require human judgments. Our method is well-suited to environments with a large number of resources such as selective search, is an improvement over the state-of-the-art in resource selection for selective search, and is statistically equivalent to exhaustive search even for recall-oriented metrics such as MAP@1000, an area in which selective search was lacking.",1,MAP,True
KEYWORDS,0,,False
"selective search, resource selection, federated search",0,,False
"ACM Reference format: Zhuyun Dai, Yubin Kim, and Jamie Callan. 2017. Learning To Rank Resources. In Proceedings of SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan, 4 pages. DOI: 10.1145/3077136.3080657",0,,False
2 RELATED WORK,0,,False
"ere are three main classes of resource selection algorithms: termbased, sample-based, and supervised approaches. Term-based algorithms models the language distribution of each shard. At query time, they determine the relevance of a shard by comparing the query to the stored language model [1, 12]. Sample-based algorithms estimate the relevance of a shard by querying a small sample index of the collection, known as the centralized sample index (CSI) [9, 11, 13, 14]. Supervised methods use training data to learn models to evaluate shard relevance, with most methods training a classi er per shard [2, 4]. However, training a classi er for every shard is expensive in selective search, where shards number in hundreds.",0,,False
"us, supervised methods have not been used for selective search. Techniques that train a single classi er would be more suitable for selective search. Balog [3] trained a learning-to-rank algorithm for a TREC task and Hong et al. [6] learned a joint probabilistic classi er. e la er is used as a baseline in this work.",1,TREC,True
1 INTRODUCTION,1,DUC,True
"Selective search is a federated search architecture where a collection is clustered into topical shards. At query time, a resource selection algorithm is used to select a small subset of shards to search.",0,,False
"Recent work showed that while selective search is equivalent to exhaustive search for shallow metrics (e.g. P@10), it performs worse for recall-oriented metrics (e.g. MAP) [5]. is is a problem because modern retrieval systems apply re-ranking operations to a base retrieval, which can require deep result lists [10].",1,MAP,True
"In this paper, we present learning to rank resources, a resource selection method based on learning-to-rank. While learning-to-rank has been widely studied for ranking documents, its application to ranking resources has not been studied in depth. We take advantage of characteristics of the resource ranking problem that are distinct from document ranking; we present new features; and we propose a training approach that uses exhaustive search results as the gold standard and show that human judgments are not necessary.",1,ad,True
"Our approach is suitable for e ciently ranking the hundreds of shards produced by selective search and is an improvement over the state-of-the-art in resource selection for selective search. In addition, our approach is statistically equivalent to exhaustive search in MAP@1000, a deep recall-oriented metric.",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080657",1,ad,True
3 MODEL,0,,False
"Let q denote a query and (q, si ) denote features extracted from the ith shard for the query. e goal of learning-to-rank is to",0,,False
"nd a shard scoring function f ((q, s)) that can minimize the loss",0,,False
function de,0,,False
ned as:,0,,False
L(f ),0,,False
",",0,,False
q,0,,False
Q,0,,False
l,0,,False
"(q,",0,,False
f,0,,False
)dP,0,,False
(q).,0,,False
"We use l(q, f )",0,,False
",",0,,False
"si >qsj 1{ f ((q, si )) < f ((q, sj ))} , where si >q sj denotes shard",0,,False
pairs for which si is ranked higher than sj in the gold standard,0,,False
shard ranking w.r.t query q.,0,,False
"We used SV Mr ank [7], which optimizes pair-wise loss. List-wise",0,,False
"algorithms such as ListMLE [16] produced similar results, thus we",0,,False
only report results with SV Mr ank .,0,,False
e training process requires a gold standard shard ranking for,0,,False
"each training query. We propose two de nitions of the ground truth,",0,,False
"relevance-based and overlap-based. In the relevance-based approach,",0,,False
the optimal shard ranking is determined by the number of relevant,0,,False
"documents a shard contains. us, the training data require queries",0,,False
"with relevance judgments, which can be expensive to obtain. e",0,,False
overlap-based approach assumes that the goal of selective search,0,,False
is to reproduce the document ranking of exhaustive search. e,0,,False
optimal shard ranking is determined by the number of documents,0,,False
in a shard that were ranked highly by exhaustive search. is does,0,,False
not require manual relevance judgments.,0,,False
4 FEATURES,0,,False
4.1 ery-Independent Information,0,,False
Shard Popularity: Indicates how o en the shard had relevant (relevance-based) or top-ranked (overlap-based) documents for training queries. It is query-independent and acts as a shard prior.,1,ad,True
837,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
4.2 Term-Based Statistics,0,,False
"Term-based features can be easily precomputed, thus are e cient. Taily Features: One feature is the Taily [1] score calculated for query q and shard s. However, Taily scores can vary greatly across shards and queries. For robustness, we add two additional features. If shard s is ranked rs for query q, the inverse rank is 1/rs , which directly describes the importance of s relative to other shards. e binned rank is ceilin (rs /b), where b is a bin-size. We use b ,"" 10, meaning that every 10 consecutive shards are considered equally relevant. is feature helps the model to ignore small di erences between shards with similar rankings. Champion List Features: For each query term, the top-k best documents were found. e number of documents each shard contributes to the top-k was stored for each shard-term pair. For multi-term queries, the feature values of each query term were summed. We use two values of k "","" {10, 100}, generating 2 features.""",1,ad,True
"ery Likelihood Features: e log-likelihood of a query with respect to the unigram language model of each shard is: L(q|s) ,",0,,False
"t q log p(t |s), where p(t |s) is the shard language model, the average of all document language models p(t |d) in the shard. Document language model p(t |d) is estimated using MLE with Jelinek-Mercer smoothing. ery likelihood, inverse query likelihood, and binned query likelihood features are created for body, title, and inlink representations, yielding a total of 9 features.",0,,False
"ery Term Statistics: e maximum and minimum shard term frequency across query terms, e.g. st fmax (q, s) ,"" maxt q st f (t, s), where st f (t, s) is the frequency of term t in shard s. We include the maximum and minimum of st f · id f where id f is the inverse document frequency over the collection. ese 4 features are created for body, title, and inlink representations, yielding 12 features. Bigram Log Frequency: e frequency of each bigram of the query in a shard is b fq (s) "","" b q log b fb (s), where b fb (s) is the frequency of bigram b in shard s. is feature can estimate term correlation. To save storage, we only store bigrams that appear more than 50 times in the collection.""",0,,False
4.3 Sample-Document (CSI-Based) Features,0,,False
"ese features are based on retrieval from the centralized sample index (CSI), which may provide term co-occurrence information. CSI retrieval is expensive, and thus is slower to calculate. Rank-S and ReDDE Features: Similar to Taily features, the shard scores given by Rank-S [9] and ReDDE [13], as well as the inverse rank and binned rank features for a total of 6 features. Average Distance to Shard Centroid: e distance between the top-k documents retrieved from the CSI to their respective shards' centroids. Intuitively, if the retrieved documents are close to the centroid, the shard is more likely to contain other similar, highlyscoring documents. For multiple documents from the same shard, the distances are averaged. We use two distance metrics: KL divergence and cosine similarity Note that because KL divergence measures distance rather than similarity, we use the inverse of the averaged KL divergence as the metric. We generated features for k ,"" {10, 100} and also a feature measuring the distance between the shard's centroid to its single highest scoring document in the top 100 of the CSI results, for a total of 6 features.""",0,,False
5 EXPERIMENTAL METHODOLOGY,0,,False
"Datasets: Experiments were conducted with ClueWeb09-B and Gov2. ClueWeb09-B (CW09-B) consists of 50 million pages from the ClueWeb09 dataset. Gov2 is 25 million web pages from the US government web domains. For relevance-based models, 200 queries from the TREC 09-12 Web Track topics were used for CW09-B, and 150 queries from the TREC 04-06 Terabyte Track topics were used for Gov2. Models were trained by 10-fold cross-validation. For overlap-based models, training queries were sampled from the AOL and Million ery Track query logs. Models were tested with the TREC queries. Optimal shard ranking for the overlap method was de ned by the number of documents each shard contains that were within the top N ,"" 2K retrieved from exhaustive search. We found N  [1K, 3K] produced stable results. Proposed methods and baselines: We used three sources of training data: relevance-based training data (L2R-TREC), and overlapbased training data (L2R-AOL and L2R-MQT). We used linear SV Mr ank , where C was chosen by cross validation. Our method was compared against state-of-the-art unsupervised models (Taily [1], ReDDE [13], and Rank-S [9]); and a supervised model Jnt [6]. Jnt was trained and tested using TREC queries with 10-fold cross-validation. Evaluation Metrics: Search accuracy was measured by P@10, NDCG@30 and MAP@1000. To test the proposed methods' superiority to baselines, a query-level permutation test with p < 0.05 was used. To test the equivalence to exhaustive search, a non-inferiority test [15] was used to assert that results of the more e cient selective search were at least as accurate as exhaustive search. e equivalence is established by rejecting the null hypothesis that selective search is at least 5% worse than exhaustive search with a 95% con dence interval. Selective Search Setup: We used 123 shards for CW09-B and 199 shards for Gov2 [5]. A 1% central sample index (CSI) was created for ReDDE and Rank-S baselines and CSI based features. Jnt followed the original implementation and used a 3% CSI. Search Engine Setup: Retrieval was performed with Indri, using default parameters. eries were issued using the sequential dependency model (SDM) with parameters (0.8, 0.1, 0.1). For CW09-B, documents with a Waterloo spam score below 50 were removed 1.""",1,ClueWeb,True
6 EXPERIMENTS,0,,False
6.1 Overall Comparison,0,,False
Our method was compared to four baselines and exhaustive search. We tested shard rank cuto s from 1­8% of total shards; 10 for CW09-B and 16 for Gov2. e automatic cuto s of Rank-S and Taily performed similarly to xed cuto s and are not shown. Shard rankings by L2R enabled more accurate search than all baselines in both datasets (Figure 1). e search accuracy of L2R models is higher than the baselines at nearly every shard rank cuto .,1,CW,True
"Table 1 compares L2R models and the baselines at two shard rank cuto s. e rst cuto is the point where the shallow metrics (P@10 and NDCG@30) stablize: 4 for CW09-B and 6 for Gov2. e second cuto is where MAP@1000 become stable: 8 for CW09-B and 12 for Gov2. L2R models improve over the baselines at both shard cuto s. For shallow metrics, L2R reaches exhaustive search",1,CW,True
1h ps://plg.uwaterloo.ca/ gvcormac/,0,,False
838,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"at the rst cuto . Furthermore, searching the rst 8 out of 12 shards ranked by L2R is statistically non-inferior to searching all shards exhaustively, even for the recall-oriented MAP@1000. All the baselines have a 10% gap from exhaustive search in MAP@1000.",1,MAP,True
6.2 E ect of Training Data,0,,False
"One might expect the relevance-based model (L2R-TREC) to be better than overlap-based models (L2R-AOL and AOL-MQT), because it uses manual relevance judgments. A model trained with overlap data might favor shards that contain false-positive documents. However, there is li le di erence between the two training methods. L2R-TREC was statistically be er than TREC or AOL for MAP@1000 in Gov2, but the relative gain is only 2%; in all other cases, there is no statistically signi cant di erences among the three models. Furthermore, models trained with relevance and overlap data agreed on which features are important (not shown due to space constraints).",1,TREC,True
"is analysis indicates that unlike learning to rank document models, we can train a learning to rank resource selector on a new dataset before we have relevance judgments.",0,,False
6.3 ery Length,0,,False
"We compare L2R-MQT to the baselines using MAP@1000 for queries with di erent lengths on CW09-B, shown in Figure 1. Gov2 and other training data produced similar results and are not shown. For single-term queries, existing methods are already equivalent to or be er than exhaustive search, and L2R-MQT retains this good performance. e advantage of L2R-MQT comes from multi-term queries, where the best baseline Jnt still has a 10% gap from exhaustive search. For these queries, the improvement of L2R-MQT over the Taily is expected, because Taily does not model term co-occurrence. However, L2R-MQT also out-performs ReDDE and Rank-S, which account for term co-occurrence by retrieving documents from the CSI, but are limited by only having a sample view of the collection. L2R draws evidence from both the sample and the whole collection. Jnt also fuses sample- and term-based features, but most of its features are derived from ReDDE or Taily-like methods and do not carry new information. L2R improved over Jnt by using novel features that encode new evidence.",1,MQ,True
"Figure 1: MAP@1000 for queries on CW09-B, grouped by query length. Parentheses on the X axis present the number of queries in each group. T is the shard rank cuto .",1,MAP,True
6.4 Feature Analysis,0,,False
"e L2R approach uses three classes of features: query-independent, term-based, and sample-document (CSI). ese three feature classes have substantially di erent computational costs and contributions.",0,,False
"Fast vs. Slow features: Sample-document (CSI-based) features have a high computational cost, because they search a sample (typically 1-2%) of the entire corpus. Term-based features have a low computational cost, because they lookup just a few statistics per query term per shard. Costs for query-independent features are lower still. e third experiment compares a slow model that uses all features (ALL) to a fast version that does not use sample-document features (FAST).",0,,False
"We estimate the resource selection cost by the amount of data retrieved from storage. For CSI-based features, the cost is the size of postings of every query term in the CSI. For term-based features, the cost is the amount of su cient statistics required to derive all term-based features. e query-independent feature only looks up the shard popularity, so the cost is one statistic per shard.",0,,False
"Table 2 compares FAST with ALL and baselines by their accuracy and average resource selection cost per query. ReDDE results were similar to Rank-S and are not shown. Taily has been the state-ofthe-art term-based (`faster') resource selection algorithm. However, FAST is substantially more accurate. FAST also outperformed Jnt with over 100× speed up. Compared to ALL, FAST is 67 times faster on CW09-B and 34 times faster on Gov2. Although FAST has slightly lower search accuracy than ALL, the gap is not large and is not statistically signi cant, indicating that the information from the CSI features can be covered by the more e cient features.",1,CW,True
"We conclude that a resource ranker composed of only queryindependent and term-based features is as accurate as exhaustive search and a ranker that includes CSI features. CSI features improve accuracy slightly, but at a signi cant additional computational cost.",1,ad,True
"Importance of Feature Types: We investigate the contribution of other types of features: query-independent features and term-based features, where the term-based features were sub-divided into unigram and bigram features. Table 3 presents the results for the leave-one-out analysis conducted on FAST. On CW09-B, removing any feature set from FAST led to lower performance. is indicates that each set of features covers di erent types of information, and all are necessary for accurate shard ranking. Among these features, unigram features were most important because CW09-B has many single-term queries. On Gov2, the only substantial di erence is observed when bigram features are excluded.",1,CW,True
7 CONCLUSION,0,,False
"is paper investigates a learning-to-rank approach to resource selection for selective search. Much a ention has been devoted to learning-to-rank documents, but there has been li le study of learning-to-rank resources such as index shards. Our research shows that training data for this task can be generated automatically using a slower system that searches all index shards for each query. is approach assumes that the goal of selective search is to mimic the accuracy of an exhaustive search system, but with lower computational cost. is assumption is not entirely true--we would like selective search to also be more accurate--but it is convenient and e ective.",0,,False
839,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"Table 1: Search accuracy comparison between 3 L2R models and baselines at two rank cuto s for two datasets. : statistically signi cant improvement compared to Jnt, the best resource selection baseline. : non-inferiority to exhaustive search .",0,,False
Method,0,,False
Redde Rank-S Taily Jnt L2R-TREC L2R-AOL L2R-MQT Exh,1,TREC,True
P @10,0,,False
0.355 0.350 0.346 0.370 0.374 0.374 0.382,0,,False
0.372,0,,False
"T,4 NDCG @30",0,,False
0.262 0.259 0.260,0,,False
0.269 0.281 0.281  0.285 ,0,,False
0.288,0,,False
CW09-B,1,CW,True
MAP @1000 0.176 0.175 0.172 0.178 0.192 0.191 0.193 0.208,1,MAP,True
P@10,0,,False
0.363 0.360 0.346 0.367 0.377 0.375 0.375 0.372,0,,False
"T,8",0,,False
NDCG,0,,False
@30 0.275,0,,False
0.268,0,,False
0.260 0.277 0.286  0.287  0.286 ,0,,False
0.288,0,,False
MAP @1000,1,MAP,True
0.187 0.183 0.175,0,,False
0.192 0.202  0.202  0.202 ,0,,False
0.208,0,,False
P,0,,False
@10 0.580,0,,False
0.570,0,,False
0.518 0.582 0.593 0.593 0.586,0,,False
0.585,0,,False
"T,6 NDCG @30",0,,False
0.445 0.440 0.403,0,,False
0.459 0.469 0.470  0.465,0,,False
0.479,0,,False
Gov2,1,Gov,True
MAP @1000 0.267 0.263 0.235 0.278 0.299 0.291 0.292 0.315,1,MAP,True
P,0,,False
@10 0.587 0.585,0,,False
0.530 0.588 0.591 0.587 0.593,0,,False
0.585,0,,False
"T,12",0,,False
NDCG,0,,False
@30 0.4600 0.461,0,,False
0.418 0.465 0.475  0.470 0.474 ,0,,False
0.479,0,,False
MAP @1000,1,MAP,True
0.289 0.286 0.256,0,,False
0.292 0.313  0.307  0.309 ,0,,False
0.315,0,,False
Table 2: E ectiveness and e ciency of FAST features. ALL uses all features. FAST does not use sample-document features. T: shard rank cuto . : non-inferiority to exhaustive.,0,,False
Cw09 -B,0,,False
"(T,8) Gov2",1,Gov,True
"(T,12)",0,,False
Method,0,,False
Redde Taily Jnt ALL FAST Redde Taily Jnt ALL FAST,0,,False
P,0,,False
@10 0.363,0,,False
0.346 0.367 0.375 0.373 0.579,0,,False
0.518 0.588 0.593 0.587,0,,False
NDCG,0,,False
@30 0.275,0,,False
0.260 0.277 0.286 0.285 0.445,0,,False
0.403 0.465 0.474 0.471,0,,False
MAP @1000 0.187 0.175 0.192 0.202 0.201 0.289 0.256 0.292 0.309 0.310,1,MAP,True
Average Cost,0,,False
"156,180 470",0,,False
"468,710 158,529",0,,False
"2,349 105,080",0,,False
"758 315,875 108,306",0,,False
"3,226",0,,False
Table 3: Performance of L2R-MQT using feature sets constructed with leave-one-out. `- X' means the feature was excluded from FAST. Text in bold indicates the lowest value in the column.,1,MQ,True
CW09 -B,1,CW,True
"(T,8)",0,,False
Gov2,1,Gov,True
"(T,12)",0,,False
Feature Set,0,,False
FAST - Unigram - Bigram - Independent,0,,False
FAST - Unigram - Bigram - Independent,0,,False
P@10 0.373,0,,False
0.303 0.364 0.368 0.592 0.592,0,,False
0.582 0.591,0,,False
NDCG@30 0.285,0,,False
0.226 0.275 0.282 0.471 0.468,0,,False
0.462 0.471,0,,False
MAP@1000 0.201,1,MAP,True
0.138 0.187 0.199 0.310 0.301,0,,False
0.296 0.303,0,,False
"We show that the learned resource selection algorithm produces search accuracy comparable to exhaustive search down to rank 1,000. is paper is the rst that we know of to demonstrate results that are statistically signi cantly equivalent to exhaustive search for MAP@1000 on an index that does not have badly skewed shard sizes. Accuracy this deep in the rankings opens up the possibility of using a learned reranker on results returned by a selective search system, which was not practical in the past.",1,MAP,True
"Most prior research found that sample-document algorithms such as ReDDE and Rank-S are a li le more accurate than term-based algorithms such as Taily for selective search resource selection; however, sample-document resource selection algorithms have far",0,,False
higher computational costs that increases query latency in some con gurations [8]. is work suggests that sample-document features provide only a small gain when combined with other types of features. It may no longer be necessary to choose between accuracy and query latency when using a learned resource ranker.,0,,False
8 ACKNOWLEDGMENTS,0,,False
"is research was supported by National Science Foundation (NSF) grant IIS-1302206. Yubin Kim is the recipient of the Natural Sciences and Engineering Research Council of Canada PGS-D3 (438411). Any opinions, ndings, and conclusions in this paper are the authors' and do not necessarily re ect those of the sponsors.",1,ad,True
REFERENCES,0,,False
"[1] R. Aly, D. Hiemstra, and T. Demeester. Taily: shard selection using the tail of score distributions. pages 673­682, 2013.",0,,False
"[2] J. Arguello, F. Diaz, J. Callan, and J. Crespo. Sources of evidence for vertical selection. pages 315­322, 2009.",0,,False
"[3] K. Balog. Learning to combine collection-centric and document-centric models for resource selection. In TREC, 2014.",1,TREC,True
"[4] S. Cetintas, L. Si, and H. Yuan. Learning from past queries for resource selection. pages 1867­1870, 2009.",0,,False
"[5] Z. Dai, X. Chenyan, and J. Callan. ery-biased partitioning for selective search. pages 1119­1128, 2016.",0,,False
"[6] D. Hong, L. Si, P. Bracke, M. Wi , and T. Juchcinski. A joint probabilistic classi cation model for resource selection. pages 98­105. ACM, 2010.",0,,False
"[7] T. Joachims. Training linear SVMs in linear time. In Proc. SIGKDD, pages 217­226, 2006.",0,,False
"[8] Y. Kim, J. Callan, J. S. Culpepper, and A. Mo at. Load-balancing in distributed selective search. pages 905­908, 2016.",1,ad,True
"[9] A. Kulkarni, A. S. Tigelaar, D. Hiemstra, and J. Callan. Shard ranking and cuto estimation for topically partitioned collections. pages 555­564, 2012.",0,,False
"[10] C. Macdonald, R. L. T. Santos, and I. Ounis. e whens and hows of learning to rank for web search. Inf. Retr., 16(5):584­628, 2013.",0,,False
"[11] I. Markov and F. Crestani. eoretical, qualitative, and quantitative analyses of small-document approaches to resource selection. 32(2):9, 2014.",0,,False
"[12] H. No elmann and N. Fuhr. Evaluating di erent methods of estimating retrieval quality for resource selection. pages 290­297, 2003.",0,,False
"[13] L. Si and J. P. Callan. Relevant document distribution estimation method for resource selection. pages 298­305, 2003.",0,,False
"[14] P. omas and M. Shokouhi. SUSHI: scoring scaled samples for server selection. pages 419­426, 2009.",0,,False
"[15] E. Walker and A. S. Nowacki. Understanding equivalence and noninferiority testing. Journal of General Internal Medicine, 26(2):192­196, 2011.",0,,False
"[16] F. Xia, T.-Y. Liu, J. Wang, W. Zhang, and H. Li. Listwise approach to learning to rank: theory and algorithm. In Proc. ICML, pages 1192­1199, 2008.",0,,False
840,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Generating Clinical eries from Patient Narratives,0,,False
A Comparison between Machines and Humans,0,,False
Bevan Koopman,0,,False
"CSIRO Brisbane, Australia bevan.koopman@csiro.au",1,CSIRO,True
Liam Cripwell,0,,False
"CSIRO Brisbane, Australia ljcripwell@gmail.com",1,CSIRO,True
Guido Zuccon,0,,False
"eensland University of Technology Brisbane, Australia g.zuccon@qut.edu.au",0,,False
ABSTRACT,0,,False
"is paper investigates how automated query generation methods can be used to derive e ective ad-hoc queries from verbose patient narratives. In a clinical se ing, automatic query generation provides a means of retrieving information relevant to a clinician, based on a patient record, but without the need for the clinician to manually author a query. Given verbose patient narratives, we evaluated a number of query reduction methods, both generic and domain speci c. Comparison was made against human generated queries, both in terms of retrieval e ectiveness and characteristics of human queries. ery reduction was an e ective means of generating ad-hoc queries from narratives. However, human generated queries were still signi cantly more e ective than automatically generated queries. Further improvements were possible if parameters of the query reduction methods were set on a per-query basis and a means of predicting this was developed. Under ideal conditions, automated methods can exceed humans. E ective human queries were found to contain many novel keywords not found in the narrative. Automated reduction methods may be handicapped in that they only use terms from narrative. Future work, therefore, may be directed toward be er understanding e ective human queries and automated query rewriting methods that a empt to model the inference of novel terms by exploiting semantic inference processes.",1,ad-hoc,True
CCS CONCEPTS,0,,False
·Information systems  Expert search;,0,,False
1 INTRODUCTION,1,DUC,True
"An electronic patient record is an invaluable source of information in clinical scenarios. Beyond its immediate use of describing a patient, it provides a reference for retrieving auxiliary information related to that patient such as relevant medical literature or clinical trials for which that patient may be eligible [4]. It is desirable to automatically initiate a search to retrieve such information from a patient record; however, patient records are verbose and using the entire record as an ad-hoc query is not e ective [6]. us, in this",1,ad-hoc,True
Work completed as part of an internship at CSIRO while a student at QUT.,1,CSIRO,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080661",1,ad,True
"paper, we investigate methods for automatically generating ad-hoc clinical queries from verbose patient reports.",1,ad-hoc,True
Our main research questions are: 1) Can a verbose patient narrative be reduced to a more e ective ad-hoc query? and 2) How does the e ectiveness of this query compare with human generated ad-hoc queries?,1,ad-hoc,True
2 RELATED WORK,0,,False
"e clinical task: Generating clinical queries from verbose patient narratives has previously been a empted as part of the TREC Clinical Decision Support (CDS) track [11]: given a verbose patient narrative, retrieve PubMed articles in a clinical decision support se ing. E ective teams typically applied a form of implicit query reduction by weighting terms from the patient narrative [1].",1,TREC,True
"A similar task to TREC CDS involved using the same verbose patient narratives but retrieving clinical trials for which that patient may be eligible [6]. is test collection is of particular relevance as a number of query variations are provided for each topic: the verbose patient narrative, a human provided summary and a number of ad-hoc queries provided by clinicians. ese ad-hoc queries therefore provide us with the human benchmark against which any automatically generated query can be evaluated.",1,TREC,True
"Dealing with verbose queries: Previous work has speci cally tackled the problem of dealing with verbose queries. Kumaran and Carvalho [6] approached the problem by generating shorter subqueries from the initial query and training a classi er to predict the quality of a given subquery based on various predictive measures [9]. Bendersky and Cro [2] developed a technique for automatically extracting key concepts from verbose queries that had the most impact on retrieval e ectiveness. Both these two techniques relied on generating permutations of sub-queries. A key di erence between these previous methods and this study was the length of the original verbose queries: Kumaran and Carvalho used topics 3­30 terms in length, Bendersky and Cro used topics 12­49 terms in length, while our patient reports were 39­204 terms in length. For such long queries, some predictive measures were infeasible; for example, generating all possible sub-queries for a 200 term query is intractable (200! combinations). Finally, it is unclear how these general methods translate to the nuances of medical IR [4].",1,ad,True
"Generating clinical queries: Speci c to the medical domain, Soldaini et al. experimented with query reduction techniques for searching medical literature [12], including some of those from Kumaran & Carvalho [9].",0,,False
Koopman et al. [5] experimented with a concept-based information retrieval approach to medical record search using the UMLS medical thesaurus. e experiment showed that queries and documents that are reduced to contain only their medical concepts,0,,False
853,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
proved e ective. is method relies on e ectively identifying medical concepts from free-text; a task that is possible using specialist medical information extraction systems [13].,0,,False
"Understanding e ective clinical queries: In order to implement an automated query generation method, it is important to identify the hallmarks of an e ective query. is was investigated in [7] where the query generation process of humans was examined in detail. e results of that study showed that the most e ective queriers were those that inferred novel keywords not present in the original patient narrative. ese ndings suggest that automated methods which merely reduce the query to contain a subset of its original terms (like those of Kumaran&Carvalho [6] and Bendersky&Cro [2]) may be limited in terms of potential e ectiveness. Our empirical investigation answers whether this is the case.",0,,False
3 METHODS,0,,False
e rst sub-section outlines speci c methods for generating shorter ad-hoc queries from a verbose patient narrative. ese basic methods are extended to a per-query adaptive approach in the next sub-section. We also outline speci c methods for analysing queries to understand more about how automatically generated queries compare with human generated queries.,1,ad-hoc,True
3.1 Automatic ery Reduction Methods,0,,False
"Proportional Inverse Document Frequency (IDF-r): Terms in the original patient narrative were ranked according to inverse document frequency; a proportion of the top ranked IDF terms were retained. is proportion, denoted r , was varied from 1/|D | to 1 where |D| was the total number of terms in the patient narratives.1",0,,False
e model was run and evaluated with r at all values between 0.01 and 1.0 with increments of 0.01. We denote this IDF-r. Reduce to only UMLS Medical Concepts (UMLS & TaskedUMLS): A model was developed that identi ed and retained only medical related terms from the original patient narrative. Medical terms were identi ed as those belonging to the UMLS medical thesaurus.2 Medical terms were identi ed using ickUMLS [13] -- an information extraction systems that maps free-text to UMLS concepts. We denote this model UMLS.,0,,False
"A variant of the UMLS model was also implemented to perform a further reduction to contain only Diagnosis, Treatment or Test related terms. is choice is based on prior studies that show medical professionals typically pose clinical questions around these three types [3] and these form the basis of the queries in TREC CDS [11]. We denote this model Tasked-UMLS. Combined model UMLS+IDF-r: Here the original patient narrative was rst reduced to contain only medical terms using the UMLS model and then a proportion of terms retained using the IDF-r model. We denote this model UMLS+IDF-r.",1,TREC,True
"1A top-k variant to the IDF-r model was also implemented to reduce the topic to include a xed k terms with highest IDF values; however, the results for this were less reliable than IDF-r due to the fact that the lengths of the patient narratives di ered considerably. 2h ps://www.nlm.nih.gov/research/umls/",0,,False
3.2 Per-query Reduction via ery,0,,False
Performance Predictors,0,,False
An important consideration for the aforementioned query reduc-,0,,False
tion methods was how much to reduce the query by (as indicated,0,,False
"by query reduction proportion parameter, r ). We hypothesised that",0,,False
"because topics di ered considerably in both length and content,",0,,False
a global se ing of r would have been sub-optimal (this was em-,0,,False
"pirically validated in our experiments). us, it was desirable to",0,,False
determine the query reduction proportion on a per-query basis.,0,,False
To do this we utilised ery Performance Predictors (QPPs) [9].,0,,False
"Speci cally, queries were generated for r , 0.01..1.0 in step of 0.01.",0,,False
For each generated query a number of QPPs were calculated. e,0,,False
speci c QPPs used were:,0,,False
Inverse Document Frequency (IDF): is was calculated and,0,,False
averaged across all query terms.,0,,False
I DFw,0,,False
",",0,,False
log,0,,False
1+N n(w ),0,,False
where N,0,,False
is,0,,False
the total number of documents in the collection and n(w ) is the,0,,False
collection frequency of term w.,0,,False
SCQ: A measure of how similar a query was to the collection as a,0,,False
whole;,0,,False
averaged,0,,False
across,0,,False
all query,0,,False
terms.,0,,False
SCQw,0,,False
",",0,,False
(1 + ln,0,,False
n,0,,False
(w N,0,,False
),0,,False
),0,,False
×,0,,False
ln,0,,False
(1,0,,False
+,0,,False
N Nw,0,,False
),0,,False
where,0,,False
Nw,0,,False
is,0,,False
the,0,,False
document,0,,False
frequency,0,,False
of w.,0,,False
Inverse Collection Term Frequency (ICTF): is was calculated,0,,False
and,0,,False
averaged,0,,False
across,0,,False
all,0,,False
terms.,0,,False
ICT Fw,0,,False
",",0,,False
log2,0,,False
n(w ) T,0,,False
where T,0,,False
is,0,,False
the,0,,False
total number of terms in the collection.,0,,False
ery Scope (QS): A measure of the size of the retrieved document,0,,False
set,0,,False
relative,0,,False
to,0,,False
the,0,,False
collection,0,,False
size:,0,,False
QS,0,,False
",",0,,False
-,0,,False
log,0,,False
nQ N,0,,False
",",0,,False
nQ,0,,False
is,0,,False
the,0,,False
number,0,,False
of documents that contain at least one of the query terms.,0,,False
e correlation between these predictors and the retrieval ef-,0,,False
"fectiveness of the queries was examined. In addition, the QPPs",1,ad,True
"were used as features in a model to prediction the value of r ; i.e.,",0,,False
"given a particular topic (patient narrative), determine what query",0,,False
reduction proportion should be applied to it in order to maximise,0,,False
"retrieval e ectiveness. Training data was obtained by selecting,",0,,False
"for each query topic, the best se ing of r according to precision",0,,False
"@ 5 (P5). is resulted in a total of 1289 topic, query pairs. (Note",0,,False
that for many queries there were many values of r with the same,0,,False
P5; hence the large number of training examples.) e training,0,,False
data was strati ed into four folds according to topic id (60 topics,0,,False
divided into folds of 15 topics). A Generalized Linear Model was,0,,False
then trained to predict r based on the QPPs; this was done via,0,,False
"4-fold cross validation. Finally, the predicted values of r were used",0,,False
"in IDF-r and UMLS+IDF-r, and P5, mean reciprocal rank (MRR) and",0,,False
INST calculated.,0,,False
3.3 ery Understanding Methods,0,,False
Analysis of how clinician formulate ad-hoc queries from patient,1,ad-hoc,True
narratives has shown that they sometimes selected keywords from,0,,False
the narrative and sometimes inferred novel terms not found in the,0,,False
narrative [7] . Here we consider the overlap of keywords in the,0,,False
clinician's ad-hoc query and corresponding narrative in order to,1,ad-hoc,True
be er understand how clinicians formulated their queries.,0,,False
e overlap of an ad-hoc query Q is de ned as the propor-,1,ad-hoc,True
"tion of keywords in Q that were contained in its narrative text,",0,,False
T: o,0,,False
"erlap(T ,Q ) ,",0,,False
|T Q |Q |,0,,False
|,0,,False
.,0,,False
e automated query reduction meth-,0,,False
ods outlined in this study clearing were limited to selecting only,0,,False
854,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
0.15,0,,False
INST,0,,False
0.5,0,,False
MRR 0.6,0,,False
P5,0,,False
0.4,0,,False
0.10,0,,False
0.4,0,,False
0.3,0,,False
0.05,0,,False
0.2,0,,False
0.2,0,,False
0.1 ,0,,False
0.00,0,,False
0.0,0,,False
NHaurSrmautamivnemAadry-hToaIDcsFkUe-UdMrM-LULSSM+LISDF-r,1,ad,True
NHaurSrmautamivnemAadry-hToaIDcsFkUe-UdMrM-LULSSM+LISDF-r,1,ad,True
NHaurSrmautamivnemAadry-hToaIDcsFkUe-UdMrM-LULSSM+LISDF-r,1,ad,True
Figure 1: Results for baselines and reduction methods.,0,,False
"keywords from the narrative (overlap ,"" 1.0). We, therefore, investigate how much of a handicap this was in comparison to human generated queries containing novel keywords.""",0,,False
3.4 Experimental Setup,0,,False
"Empirical evaluation was performed using a clinical trials test collection [6]. e collection contained 204,855 publicly available clinical trial documents, which we indexed using ElasticSearch with stemming and punctuation removal.3",0,,False
"e collection also contained 60 query topics. Each topic contained three di erent query variations: i) verbose patient case narratives (78 terms per topic); ii) shorter patient case summaries of the patient case narrative (22 terms per topic); and iii) short ad-hoc queries (4.2 terms per topic) provided by clinicians [6]. e narratives represented the original patient narrative (to which query reduction was applied). e shorter summary represented a human benchmark for summarising the narrative. e ad-hoc queries (n,489) represented a human benchmark against which automated methods could be compared.",1,ad-hoc,True
"A er query reduction was applied to the narrative, the reduced queries were issued to ElasticSearch and their e ectiveness evaluated using P5, MRR and INST (the evaluation measures for this test collection [6]). In addition, the full narrative, summary and ad-hoc queries were also evaluated as comparison baselines/benchmarks. Statistically signi cant di erences in retrieval e ectiveness was determined using a paired t-test.",1,ad,True
4 RESULTS & DISCUSSION,0,,False
"e retrieval results for the di erent query reduction methods and comparative baselines and benchmarks are shown in Figure 1. We observe that issuing the entire patient narrative exhibited the poorest retrieval e ectiveness. is motivates the develop of speci c query reduction methods. e shorter human-generated summaries were more e ective than the narrative (p ,"" 0.030). is nding highlights that a general reduction of query terms had a positive e ect on retrieval. However, the human ad-hoc queries proved far more e ective (statistically signi cant over all other methods). Humans were able to derive speci c query keywords that led to more relevant results being retrieved (more on this later). is""",1,ad,True
3ElasticSearch version -- 5.2.0 h ps://www.elastic.co/downloads/elasticsearch.,1,ad,True
"showed that although a summarisation method had the potential to improve e ectiveness, short, ad-hoc keyword queries were still the most e ective.",1,ad,True
"ery reduction via IDF-r proved to be e ective for speci c se ings of r . IDF-r showed a signi cant increase in e ectiveness in comparison to the narratives (p ,"" 0.040) when an appropriate query reduction proportion (r ) was chosen. Note that the boxplot shows the e ectiveness for all se ings of r , many of which would obviously be sub-optimal (e.g., r "", 0.01 where only 1% of terms were retained). e results for IDF-r showed that the removal of less informative terms was a simple but e ective means of improving retrieval e ectiveness.",0,,False
"Reducing the narrative to contain only medical terms via the UMLS method proved e ective over searching using the narrative (p , 0.031) but not over using the summary (p ,"" 0.395). e UMLS results showed that simply removing non-medical terms from the narrative was a very good reduction method. UMLS seemed to produce be er results than the IDF-r, although these were not signi cant (p "","" 0.088). Based on the positive results of the IDF-r and UMLS, a combined UMLS+IDF-r method was evaluated. However, UMLS+IDF-r was not statistically signi cantly di erent from UMLS (p "", 0.568) or IDF-r (p ,"" 0.072). However, the UMLS+IDF-r had the advantage of having similar e ectiveness but with far fewer query terms.""",1,ad,True
4.1 Understanding human queries,0,,False
"e results from Figure 1 also show that human ad-hoc queries were volatile: they had the greatest variation in e ectiveness. While all the best performing queries were ad-hoc, there were also ad-hoc queries that were among the worst performing. Additional analysis is required to determine the characteristics of good vs. bad ad-hoc queries. ese ndings may help in the development of e ective automatic query generation methods. is is le to future work.",1,ad-hoc,True
"Comparing the keywords from ad-hoc queries with those of the patient narrative, it was found that 49% of all queries had an overlap of 0.00; i.e., the ad-hoc query contained no common terms with the narrative. e mean overlap was only 0.26. is indicated that clinicians chose to formulate their own query terms rather than select those from the patient narrative. e inferring of novel query terms, particularly those related to medical treatments, has been found to correlate with higher retrieval e ectiveness [7]. ery reduction methods may be handicapped, therefore, by the fact that they source keywords from the narrative alone. We empirically evaluate this in the coming sections and consider further the issue of inferring novel query keywords.",1,ad-hoc,True
4.2 ery reduction proportion sensitivity,0,,False
"Figure 2 shows the sensitivity to retrieval e ectiveness of the query reduction proportion parameter, r (r ,"" 1.0 represents the original patient narrative). In general, reduction via IDF proved e ective (over the narrative baseline) when the narrative was reduced to approximately 25% of its original size.""",0,,False
"e results for both IDF-r and UMLS+IDF-r were all reported with a global reduction proportion, r , across all topics (i.e., across all patient narratives). Since narratives varied greatly in both length and in content, a global reduction proportion may have been quite",0,,False
855,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Query effectiveness,1,Query,True
INST,0,,False
MRR,0,,False
P5,0,,False
0.024,0,,False
0.20,0,,False
0.020,0,,False
0.15,0,,False
0.06,0,,False
0.016,0,,False
0.04,0,,False
0.10,0,,False
0.012 0.02,0,,False
0.05 0.008,0,,False
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00,0,,False
"Query reduction proporation, r",1,Query,True
"Figure 2: ery e ectiveness for di erent query reduction proportions, r using the IDF-r.",0,,False
Table 1: Retrieval results when predicting query reduction proportion. Percentages show improvements and  show signi cance when compared to best global r .,0,,False
Human adhoc Narrative Summary Average global r Best global r Per-query r -- QPP Per-query r -- Oracle,1,adhoc,True
P5,0,,False
0.1521 0.0508 0.0949 0.0551 0.0881 0.0861 (-2%) 0.1457 (+65%),0,,False
MRR,0,,False
0.2878 0.1312 0.1790 0.1513 0.2220 0.2432 (+10%) 0.3679 (+66%),0,,False
INST,0,,False
0.0476 0.0149 0.0306 0.0184 0.0247 0.0268 (+9%) 0.0368 (+49%),0,,False
"sub-optimal. us, we investigated the e ect of predicting r on a per-query basis.",0,,False
4.3 Predicting query reduction proportion,0,,False
"A Generalised Linear Regression Model was used to predict an appropriate value of r for a given query using the QPP measures of Section 3.2 as features. e results are shown in Table 1. We also report the ""oracle"" results indicating the retrieval e ectiveness if the best value of r was chosen on a per-query basis. Signi cant improvements in MRR were found when predicting r on a per query basis; no signi cant di erences were found for P5 and INST. is is in contrast to the oracle results that showed considerable improvement if the correct reduction proportion was chosen. Clearly, there is considerable room for improvement. e chief area of focus in this regard is the establishment of a richer set of features for the prediction of per-query r values. Particular points of inquiry would be in the evaluation of medical speci c features, such as mentions of particular diseases a ecting the patient, permanent demographic information (age, gender) and negated content (e.g., ""no fever""). A be er understanding of what constituted an e ective human query would help to inform such features.",0,,False
"e oracle results showed signi cant improvement over those produced by maintaining a static, global r value. is highlights that query reduction should ideally be done on a per-query basis. In addition to this, the oracle results were much higher than those of the human generated summaries, showing that automated query generation can improve upon human summarisation. Finally, the oracle results show comparable performance with the human adhoc queries. In the case of MRR, the automated query generation methods was statistically signi cantly be er than the human ad-hoc queries (p ,"" 0.041). Even though the query reduction method only used terms from the original narrative, the oracle results showed that the right query reduction method was in line with human""",1,ad,True
"generated queries that include novel terms. Given that human queries containing novel terms showed greater e ectiveness [7], it follows that automated methods for inferring such terms should be investigated. Common query expansion methods are relevant here. However, there are also a number of retrieval techniques, some speci c to the medical domain, that a empt to model the inference of novel terms by exploiting semantic inference processes [8, 10, 14].",0,,False
5 CONCLUSION,0,,False
"ery reduction was an e ective means of generating ad-hoc queries from verbose patient narratives. E ective query reduction methods included those that retained only medical terms and a proportion of high ranking IDF terms. ery reduction could be even more e ective if the query reduction proportion was determined on a per-query basis. Using standard query performance predictors as features resulted in only minor improvements. However, if an e ective query reduction proportion can be found then signi cant improvements are possible, approaching or exceeding human generated queries.",1,ad-hoc,True
"Human generated queries varied widely in e ectiveness. An analysis of human queries showed that many contained novel terms not found in the patient narrative. eries with novel terms have previously shown to be more e ective. ery reduction method may, therefore, be handicapped in that they only source keywords from the patient narrative. Future work, therefore, may be directed toward be er understanding e ective human queries and in automated retrieval methods that a empt to model the inference of novel terms by exploiting semantic inference processes.",0,,False
REFERENCES,0,,False
"[1] Saeid Balaneshinkordan, Alexander Kotov, and Railan Xisto. 2015. WSU-IR at TREC 2015 Clinical Decision Support Track: Joint Weighting of Explicit and Latent Medical ery Concepts from Diverse Sources. In TREC.",1,TREC,True
"[2] Michael Bendersky and W. Bruce Cro . 2008. Discovering Key Concepts in Verbose eries. In SIGIR. Singapore, 491­498.",0,,False
"[3] J.W. Ely, J.A. Oshero , P.N. Gorman, M.H. Ebell, M.L. Chambliss, E.A. Pifer, and P.Z. Stavri. 2000. A taxonomy of generic clinical questions: classi cation study. BMJ 321, 7258 (2000), 429­432.",0,,False
[4] William Hersh. 2008. Information retrieval: a health and biomedical perspective. Springer Science & Business Media.,0,,False
"[5] Bevan Koopman, Peter Bruza, Laurianne Sitbon, and Michael Lawley. 2011. AEHRC & QUT at TREC 2011 Medical Track : a concept-based information retrieval approach. In TREC. NIST, Gaithersburg, USA, 1­7.",1,TREC,True
[6] Bevan Koopman and Guido Zuccon. 2016. A Test Collection for Mathcing Patient Trials. In SIGIR. Pisa.,0,,False
"[7] Bevan Koopman, Guido Zuccon, and Peter Bruza. 2017. What makes an E ective Clinical ery and erier? JASIST To appear (2017).",0,,False
"[8] Bevan Koopman, Guido Zuccon, Peter Bruza, Laurianne Sitbon, and Michael Lawley. 2015. Information Retrieval as Semantic Inference: A Graph Inference Model applied to Medical Search. Information Retrieval 19, 1 (2015), 6­37.",0,,False
"[9] Giridhar Kumaran and Vitor R. Carvalho. 2009. Reducing Long eries Using ery ality Predictors. In SIGIR. Boston, USA, 564­571.",0,,False
"[10] Nut Limsopatham, Craig Macdonald, and Iadh Ounis. 2013. A Task-Speci c ery and Document Representation for Medical Records Search. In ECIR. 747­",1,ad,True
"751. [11] Kirk Roberts, Ma hew S Simpson, Ellen Voorhees, and William R Hersh. 2015.",0,,False
"Overview of the TREC 2015 Clinical Decision Support Track. In TREC. [12] Luca Soldaini, Arman Cohan, Andrew Yates, Nazli Goharian, and Ophir Frieder.",1,TREC,True
"2015. Retrieving Medical Literature for Clinical Decision Support. In ECIR. [13] Luca Soldaini and Nazli Goharian. 2016. ickumls: a Fast, Unsupervised",0,,False
"Approach for Medical Concept Extraction. In MedIR Workshop, SIGIR. [14] Wei Zhou, Clement Yu, Neil Smalheiser, Vetle Torvik, and Jie Hong. 2007.",0,,False
Knowledge-intensive conceptual retrieval and passage extraction of biomedical literature. In SIGIR. 655­662.,0,,False
856,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
An Extended Relevance Model for Session Search,1,Session,True
Nir Levine,0,,False
Technion - Israel Institute of Technology,0,,False
"Haifa, Israel 32000 levin.nir1@gmail.com",0,,False
Haggai Roitman,0,,False
"IBM Research - Haifa Haifa, Israel 31905 haggai@il.ibm.com",0,,False
Doron Cohen,0,,False
"IBM Research - Haifa Haifa, Israel 31905 doronc@il.ibm.com",0,,False
ABSTRACT,0,,False
"e session search task aims at best serving the user's information need given her previous search behavior during the session. We propose an extended relevance model that captures the user's dynamic information need in the session. Our relevance modelling approach is directly driven by the user's query reformulation (change) decisions and the estimate of how much the user's search behavior a ects such decisions. Overall, we demonstrate that, the proposed approach signi cantly boosts session search performance.",0,,False
1 INTRODUCTION,1,DUC,True
"We propose an extended relevance model for session search. Relevance models aim at identifying terms (words, concepts, etc) that are relevant to a given (user's) information need [5]. Within a session, user's information need, expressed as a sequence of one or more queries [1], may evolve over time. User's search behavior during the session may be utilized as an additional relevance feedback source by the underlying search system [1]. Given user's session history (i.e., previous queries, result impressions and clicks), the goal of the session search task is to best serve the user's newly submi ed query in the session [1].",1,ad,True
"We derive a relevance model that aims at ""tracking"" the user's dynamic information need by observing the user's search behavior so far during the session. To this end, the proposed relevance model is driven by the user's query reformulation decisions. Our relevance modelling approach relies on previous studies that suggest that user query change decisions may (at least partially) be explained by the previous user search behavior in the session [4, 9, 12]. We utilize the derived relevance model for re-ranking the search results that are retrieved for the current user information need in the session. Overall, we demonstrate that, our relevance modeling approach can signi cantly boost session search performance compared to many other alternatives that also utilize session data.",0,,False
2 RELATED WORK,0,,False
"Few previous works have also utilized the session context (i.e., previous queries, retrieved results and clicks) as an implicit feedback",0,,False
Work was done during a summer internship in IBM Research - Haifa.,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080664",1,ad,True
"source for re ning the user's query [3, 8, 10, 11]. To this end, the query language model was either combined with the language models of previous queries [11] or retrieved (clicked) results [8, 10]. In addition, di erent query score aggregation strategies for session search were explored [3]. Yet, none of these previous works have actually considered the user's query change process itself as a possible implicit feedback source.",1,ad,True
"Several recent works have studied various query reformulation (change) behaviors during search sessions [4, 9, 12]. Among the various features that were studied, word-level features were found to best explain the changes in user queries during search sessions [4, 9]. A notable feature was found to be the occurrence of query (changed) words in the contents of results that the user previously viewed or clicked [4, 9, 12].",0,,False
"Few previous works have also utilized query change for the session search task (e.g., [6, 12]). Common to such works is the modeling of user queries and their change as states and actions within various Reinforcement Learning inspired query weighting and aggregation schemes [7]; In this work we take a rather more ""traditional"" approach, inspired by the relevance model framework [5].",1,ad,True
3 APPROACH,1,AP,True
3.1 Session model,1,Session,True
"Session search is a multi-step process, where at each step t, the user may submit a new query qt . e search system then retrieves the top-k documents Dq[kt] from a given corpus D that best match the user's query1. e user may then examine the results list; each result usually includes a link to the actual content and is accompanied with a summary snippet. e user may also decide to click on one or more of the results in the list in order to examine their actual content. Let Cqt denote the corresponding set of clicked results in Dq[kt]. In case the user decides to continue and submits a subsequent query, step t ends and a new step t + 1 begins. Let Sn-1 represent the session history (i.e., user queries, retrieved result documents, and clicked results) that was ""recorded"" prior to the current (latest) submi ed user query qn . On each step 1  t  n - 1, the session history is represented by a tuple St ,"" Qt , Dt , Ct . Qt "","" (q1, q2, . . . , qt ) is the sequence of queries submi ed by the user up to step t . Dt "","" (Dq[k1], Dq[k2], . . . , Dq[kt]) is the corresponding sequence of (top-k) retrieved result lists. Ct "","" (Cq1 , Cq2 , . . . , Cqt ) further represents the corresponding sequence of user clicks.""",1,Session,True
"1With k , 10 in the TREC session track benchmarks [1] that we use later on in our evaluation.",1,TREC,True
865,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
3.2 Information need dynamics,0,,False
e session search task is to best answer the current user's query,0,,False
qn while considering Sn-1 [1]. Let I denote the user's (hidden) information need in the session. e goal of our relevance mod-,0,,False
"elling approach is, therefore, to be er capture the user's informa-",0,,False
tion need I which may evolve during the session. In order to cap-,0,,False
"ture such dynamics, let It further represent the user's information",0,,False
"need at step t. We now assume that, It depends both on the previ-",0,,False
ous (dynamic) information need It-1 prior to query qt submission,0,,False
and the possible change in such need It,0,,False
def,0,,False
",",0,,False
It -1  It ; It,0,,False
is,0,,False
assumed be to implied by the change the user has made to her pre-,1,ad,True
vious query qt -1 to obtain query qt .,0,,False
3.3 ery change as relevance feedback,0,,False
"We utilize the user's query reformulation (change) process during the session as an implicit relevance feedback for estimating the change in the user's information need It . As been suggested by previous works [4, 9, 12], user's query changed terms may actually occur in the contents of previously viewed (clicked) search results in St-1. is, therefore, may (partially) explain how the user decided to reformulate her query from qt-1 to qt [4, 9, 12]. Our proposed relevance model aims at exploiting such query changed term occurrences within the contents of previously viewed (clicked) results so as to discover those terms w (over some vocabulary V ) that are the most relevant to the current user's information need In . As a consequence, such terms may be used for query expansion aiming to be er serve the current user's information need In .",0,,False
"Given query qt , compared to the previous query qt-1, there can be three main query change types, namely term retention, addition and removal [4, 9, 12]. User term retention, given by the set of terms that appear in both query qt and qt-1 and denoted qt, usually represent the (general) thematic aspects of the user's information need [4, 9, 12]. Added terms (denoted qt+) are those terms that the user added to query qt-1 to obtain query qt . A user may add new related terms that were encountered in previous results so as to improve the chance of nding relevant content [4]. On the other hand, a user may remove terms from a previous query qt-1 (further denoted qt-) in order to terminate a subtask or trying to improve bad performing queries [4].",1,ad,True
3.4 Relevance model derivation,0,,False
"Similar to previous works on relevance models [5], our goal is to discover those terms w ( V ) that are the most relevant to the user's information need In ; To this end, given the user's current query qn and session history Sn-1, let Sn denote our estimate of the relevance (language) model. On each step 1  t  n, such estimation is given by the following rst-order autoregressive model:",0,,False
p(w,0,,False
|,0,,False
St,0,,False
),0,,False
def,0,,False
",",0,,False
"t p(w | St-1 ) + (1 - t )p(w | Ft ),",0,,False
(1),0,,False
"where  Ft now denotes the feedback model which depends on the user's (reformulated) query qt . While St-1 estimates the dynamic information need prior to step t (i.e., It-1),  Ft captures the relative change in such need at step t (i.e., It ).",0,,False
t further controls the relative importance we assign to model,0,,False
"exploitation (i.e., St-1 ) versus model exploration (i.e.,  Ft ). t parameter is dynamically determined based on the relevance model's",0,,False
self-clarity at step t [2]. Self-clarity estimates how much the prior,0,,False
"model St-1 already ""covers"" the feedback model  Ft ; formally:",1,ad,True
t,0,,False
def,0,,False
",",0,,False
· exp-DK L (Ft,0,,False
"St -1 ),",0,,False
(2),0,,False
"where   [0, 1] and DK L( Ft St-1 ) is the Kullback-Leibler divergence between the two (un-smoothed) language models [13].",0,,False
"Finally, given qn , the current user's query in the session, we de-",0,,False
rive the relevance model Sn by inductively applying Eq. 1 (with,0,,False
 S0,0,,False
def,0,,False
",",0,,False
0). We next derive the feedback model  Ft .,0,,False
3.5 Feedback model derivation,0,,False
"Our estimate of  Ft aims at discovering those terms (in qt , qt -1 or others in V ) that are most relevant to the change in user's dynamic",0,,False
"information need from It-1 to It (i.e., It ). Given queries qt and qt-1, we rst classify their occurring terms w  according to their role in the query change. Let qt further denote the set of terms w  that are classi ed to the same type of query change (i.e., qt  {qt , qt +, qt -}).",0,,False
Our relevance model now relies on the fact that query changed,0,,False
terms may also occur within the contents of results that were pre-,0,,False
"viously viewed (or clicked) by the user [4, 9, 12]. erefore, on",0,,False
"each step t, let Ft denote the set of results that are used for (implicit) relevance feedback. We determine the set of results to be",0,,False
"included in Ft as follows. If up to step t < n there is at least one clicked result, then we assign Ft ,"" 1j t Cqj . Otherwise, we rst de ne a pseudo information need Qt. Qt represents a (crude) estimate of the user's (dynamic) information need up to step t and""",0,,False
"is obtained by concatenating the text of all observed queries in Qt (with each query having the same importance, following [11]). We",0,,False
then de ne Ft as the set of top-m results in 1j t Dqj with the,0,,False
highest query-likelihood given Qt (representing pseudo-clicks). Let,0,,False
p[µ,0,,False
](w,0,,False
|x,0,,False
),0,,False
def,0,,False
",",0,,False
t,0,,False
f,0,,False
"(w,",0,,False
x,0,,False
)+µ,0,,False
t,0,,False
f,0,,False
"(w , D) |D|",0,,False
|x |+µ,0,,False
now denote the Dirichlet smoothed,0,,False
language model of text x with parameter µ [13]. Inspired by the,0,,False
"RM1 relevance model [5], we estimate  Ft as follows:",0,,False
def,0,,False
"p(w | Ft ) ,",0,,False
p[0](w |d ) ·,0,,False
"p(d |qt )p(qt ) , (3)",0,,False
d Ft,0,,False
qt,0,,False
"where p(qt ) denotes the (prior) likelihood that, while reformulating query qt-1 into qt , the user will choose to either add (i.e., qt +), remove (i.e., qt -) or retain (i.e., qt ) terms. Such likelihood can be pre-estimated [9] (i.e., parameterized); e.g., similarly",1,ad,True
"to the QCM approach [12]. Yet, for simplicity, in this work we as-",0,,False
"sume that every user query change action has the same odds (i.e.,",0,,False
p(qt ),0,,False
",",0,,False
1 3,0,,False
).,0,,False
"Please note that, the main di",0,,False
erence between our,0,,False
estimate of  Ft and the RM1 model is in the way the later scores documents in Ft . Such score in RM1 is based on a given query,0,,False
"qt [5], with no further distinction between the role that each query",0,,False
term plays or the fact that some of the terms are actually removed,0,,False
"terms that appeared in the previous query qt-1. Similar to RM1,",0,,False
we,0,,False
further,0,,False
estimate p(d |qt ),0,,False
d,0,,False
p(qt |d ) p(qt |d,0,,False
Ft,0,,False
),0,,False
.,0,,False
866,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
User added or retained terms are those terms that are preferred,1,ad,True
"to be included in the feedback documents Ft . On the other hand, removed terms are those terms that should not appear in Ft [4]. In accordance, we de ne:",0,,False
p(qt,0,,False
|d,0,,False
),0,,False
def,0,,False
",",0,,False
"p[µ](w |d ),",0,,False
w qt,0,,False
"1 - w qt- p[0](w |d ),",0,,False
"qt  {qt, qt +} qt , qt - (4)",0,,False
"In order to avoid query dri , on each step t, we further anchor",0,,False
the feedback model  Ft to the query model qt [5] as follows:,0,,False
p(w,0,,False
|,0,,False
 Ft,0,,False
),0,,False
def,0,,False
",",0,,False
"(1 - t )p[0](w |qt ) + t p(w | Ft ),",0,,False
(5),0,,False
where t,0,,False
def,0,,False
",",0,,False
" · sim(qt , qn ) is a dynamic query anchoring",0,,False
"parameter,   [0, 1] and sim(qt , qn ) is calculated using the (idf-",0,,False
boosted) Generalized-Jaccard similarity measure; i.e.:,0,,False
"min (t f (w, qt ), t f (w, qn )) · id f (w)",0,,False
s i m (qt,0,,False
",",0,,False
qn,0,,False
),0,,False
def,0,,False
",",0,,False
w qt qn,0,,False
"max (t f (w, qt ), t f (w, qn )) · id f (w)",0,,False
(6),0,,False
w qt qn,0,,False
"According to t de nition, the similar query qt is to the current query qn , the more relevant is the query change in user's information need It (modelled by  Ft ) is assumed to be to the current user's information need In; erefore, less query anchoring e ect is assumed to be needed using query qt .",0,,False
4 EVALUATION 4.1 Datasets,0,,False
2011,0,,False
2012,0,,False
2013,0,,False
(train),0,,False
(test),0,,False
(test),0,,False
Sessions,1,Session,True
Sessions,1,Session,True
76,0,,False
98,0,,False
87,0,,False
eries/session,0,,False
3.7±1.8,0,,False
3.0±1.6,0,,False
5.1±3.6,0,,False
Topics,0,,False
Sessions/topic,1,Session,True
1.2±0.5,0,,False
2.0±1.0,0,,False
2.2±1.0,0,,False
Judged docs/topic 313±115,0,,False
372±163,0,,False
268±117,0,,False
Collection,0,,False
Name,0,,False
ClueWeb09B ClueWeb09B ClueWeb12B,1,ClueWeb,True
#documents,0,,False
"28,810,564 28,810,564 15,700,650",0,,False
Table 1: TREC session track benchmarks,1,TREC,True
"Our evaluation is based on the TREC 2011-2013 session tracks [1] (see benchmarks details in Table 1). e Category B subsets of the ClueWeb09 (2011-2012 tracks) and ClueWeb12 (2013 track) collections were used. Each collection has nearly 50M documents. Documents with spam score below 70 were ltered out. Documents were indexed and searched using the Apache Solr2 search engine. Documents and queries were processed using Solr's English text analysis (i.e., tokenization, Poter stemming, stopwords, etc).",1,TREC,True
2h p://lucene.apache.org/solr/,0,,False
4.2 Baselines,0,,False
We compared our proposed relevance modelling approach (hereina er denoted SRM3) with several di erent types of baselines.,0,,False
"is includes state-of-the-art language modeling methods that utilize session context data (i.e., previous queries, viewed or clicked results); namely FixedInt [8] (with  ,"" 0.1,  "", 1.0 following [8]) and its Bayesian extension BayesInt [8] (with µ ,"" 0.2,  "","" 5.0, following [8]) ­ both methods combine the query qn model with the history queries Qn-1 and clicks Cn-1 centroid models; BatchUp [8] (with µ "","" 2.0,  "","" 15.0, following [8]) which iteratively interpolates the language model of clicks that occur up to each step t using a batched approach; and the Expectation Maximization (EM) based approach [10] (hereina er denoted LongTEM with q "","" 0, C "", 20 and N C ,"" 1, following [10]), which rst interpolates each query qt model with its corresponding session history model (based on both clicked (C) and non-clicked (NC) results in Dq[kt]); the (locally) interpolated query models are then combined based on their relevant session history using the EM-algorithm [10].""",0,,False
"Next, we implemented two versions of the Relevance Model [5]. e rst is the basic RM3 model, denoted RM3(qn ), learned using the last query qn and the top-m retrieved documents as pseudo relevance feedback. e second, denoted RM3(Qn ), uses the pseudo information need Qn (see Section 3.5) instead of qn . We also implemented two query aggregation methods, namely: QA(uniform) which is equivalent to submi ing Qn as the query [11]; the second, denoted QA(decay), further applies an exponential decay approach to prefer recent queries to earlier ones (with decay parameter  ,"" 0.92, following [3, 12]). We further implemented three versions of the ery Change Model (QCM) ­ an MDP-inspired query weighting and aggregation approach [12]. Following [12] recommendation, QCM's parameters were set as follows  "","" 2.2,  "","" 1.8,  "","" 0.07,  "", 0.4 and  ,"" 0.92. e three QCM versions are the basic QCM approach [12]; QCM(SAT) which utilizes only """"satis ed"""" clicks (i.e., clicks whose dwell-time is at least 30 seconds [12]); and QCM(DUP) which ignores duplicate session queries [12]. Finally, in order to evaluate the relative e ect of the query-change driven feedback model (i.e.,  Ft ), we implemented a variant of SRM by replacing the query-change driven score of Eq. 3 with the RM1 document score (i.e., p(d |qn )). Let SRM(QC) and SRM(RM1) further denote the query-change and """"RM1- avoured"""" variants of SRM, respectively. It is important to note that, SRM(RM1) still relies on the dynamic relevance model updating formula (see Eq. 1) and the dynamic coe cients t and t ­ both further depend on the session dynamics (captured by St-1 and  Ft ).""",1,ad,True
4.3 Setup,0,,False
"Our evaluation is equivalent to the TREC 2011-2012 RL4 and TREC 2013 RL2 sub-tasks [1]. To this end, given each session's (last) query qn, we rst retrieved the top-2000 documents with the highest query likelihood (QL) score4 to qn . Documents were then reranked using the various baselines by multiplying their (initial)",1,TREC,True
"3Stands for ""Session-Relevance Model"". 4For this we used Solr's LMSimilarity with Dirchlet smoothing parameter µ , 2500 which is similar to Indri's default parameter.",1,Session,True
867,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
TREC 2012,1,TREC,True
TREC 2013,1,TREC,True
Method Initial retrieval FixInt [8] BayesInt [8] BatchUp [8] LongTEM [10] RM3(qn ) [5] RM3(Qn ) QA(uniform) [11] QA(decay) [3] QCM [12] QCM(SAT) [12] QCM(DUP) [12] SRM(RM1) SRM(QC),0,,False
nDCG@10 0.249r q 0.333r q 0.334r q 0.320r q 0.332r q 0.311r q 0.305r q 0.301r q 0.303r q 0.329r q 0.298r q 0.299r q 0.348q 0.356r,0,,False
nDCG 0.256r q 0.296r q 0.297r q 0.288r q 0.295r q 0.284r q 0.284r q 0.282r q 0.284r q 0.262r q 0.281r q 0.281r q,0,,False
0.300,0,,False
0.304,0,,False
nERR@10 0.302r q 0.380r q 0.382r q 0.368r q 0.389r q 0.369r q 0.354r q 0.352r q 0.353r q 0.306r q 0.347r q 0.350r q 0.395q 0.405r,0,,False
MRR 0.594r q 0.679r q 0.674r q 0.664r q 0.667r q 0.654r q 0.647r q 0.646r q 0.645r q 0.574r q 0.635r q 0.631r q 0.699q 0.716r,0,,False
nDCG@10 0.113r q 0.165r q 0.171r q 0.181r q 0.167r q 0.134r q 0.153r q 0.160r q 0.163r q 0.158r q 0.158r q 0.160r q 0.188q 0.193r,0,,False
nDCG 0.105r q 0.132r q 0.131r q,0,,False
0.134 0.131r q 0.122r q 0.129r q 0.130r q 0.131r q 0.129r q 0.129r q 0.130r q,0,,False
0.137,0,,False
0.138,0,,False
nERR@10 0.140r q 0.209r q 0.208r q 0.233r q 0.205r q 0.161r q 0.203r q 0.204r q 0.207r q 0.201r q 0.202r q 0.208r q 0.240q 0.248r,0,,False
MRR 0.390r q 0.544r q 0.527r q 0.581r q 0.530r q 0.422r q 0.553r q 0.546r q 0.550r q 0.535r q 0.545r q 0.559r q 0.601q 0.612r,0,,False
"Table 2: Evaluation results. e r and q superscripts denote signi cant di erence with SRM(RM1) and SRM(QC), respectively (p < 0.05).",0,,False
"QL score with the score determined by each method. e document scores of the various language model baselines (i.e., FixInt, BayesInt, BatchUp, LongTEM and the variants of RM3 and SRM ) were further determined using the KL-divergence score [13]; where each baseline's learned model was clipped using a xed cuto of 100 terms [13]. e TREC session track trec eval tool5 was used for measuring retrieval performance. Using this tool, we measured the nDCG@10, nDCG (@2000), nERR@10 and MRR of each baseline. Finally, we tuned the RM3 and SRM's free parameters6 using the TREC 2011 track as a train set. e parameters were optimized so as to maximize MAP. e TREC 2012-2013 tracks were used as the test sets.",1,TREC,True
4.4 Results,0,,False
"e evaluation results are summarized in Table 2. e rst row reports the quality of the initial retrieval. Overall, compared to the various alternative baselines, the two SRM variants provided signi cantly be er performance; with at least +6.6%, +2.4%, +4.1% and +5.3% be er performance in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. e results clearly demonstrate the dominance of the session-context sensitive language modeling approaches (and the two SRM variants among them) over the other alternatives we evaluated. Furthermore, SRM's consideration of the user's query-change process as an additional relevance feedback source results in a more accurate estimate of the user's information need.",1,ad,True
"Next, compared to the RM3 variants, it is clear from the results that a dynamic relevance modeling approach that is driven by query-change (such as SRM) is a be er choice for the session search task. Moving from an ad-hoc relevance modelling approach (i.e., one that only focuses on the last query in the session) to a session-context sensitive approach provides signi cant boost in performance; with at least +14%, +7.0%, +9.8% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks.",1,ad-hoc,True
"5h p://trec.nist.gov/data/session/12/session eval main.py 6  {0.1, 0.2, . . . , 0.9},   {0.1, 0.2, . . . , 0.9}, m  {5, 10, . . . , 100}",1,trec,True
"We further observe that, compared to the baseline methods that",0,,False
"implement various query aggregation and scoring schemes (i.e.,",0,,False
"QA and QCM variants), a query-expansion strategy based on the",0,,False
user's dynamic information need (such as the one implemented by,0,,False
SRM variants) provides a much be er alternative; with at least,0,,False
"+18.5%, +6.1%, +15.1% and +9.5% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks.",0,,False
"Finally, comparing the two SRM variants side-by-side, it be-",0,,False
"comes even more clear that, using the query-change as an addi-",1,ad,True
tional relevance feedback source is the be er choice; with at least,0,,False
"+2.3%, +1.0%, +2.5% and +1.8% improvement in nDCG@10, nDCG, nERR@10 and MRR, respectively, for both test benchmarks. Please",0,,False
"recall that, SRM(QC) was trained with a xed and equal-valued",0,,False
"priors p(qt ). Hence, a further improvement may be obtained by be er tuning of these priors.",0,,False
REFERENCES,0,,False
"[1] Ben Cartere e, Paul Clough, Mark Hall, Evangelos Kanoulas, and Mark Sanderson. Evaluating retrieval over sessions: e trec session track 2011-2014. In Proceedings of SIGIR'2016.",1,trec,True
[2] Steve Cronen-Townsend and W. Bruce Cro . antifying query ambiguity. In Proceedings of HLT'2002.,0,,False
[3] Dongyi Guan and Hui Yang. ery aggregation in session search. In Proceedings of DUBMOD'2014.,0,,False
[4] Jiepu Jiang and Chaoqun Ni. What a ects word changes in query reformulation during a task-based search session? In Proceedings of CHIIR'2016.,0,,False
[5] Victor Lavrenko and W. Bruce Cro . Relevance based language models. In Proceedings of SIGIR'2001.,0,,False
"[6] Jiyun Luo, Xuchu Dong, and Hui Yang. Session search by direct policy learning. In Proceedings of ICTIR'2015.",1,Session,True
"[7] Jiyun Luo, Sicong Zhang, Xuchu Dong, and Hui Yang. Designing states, actions, and rewards for using pomdp in session search. In Proceedings of ECIR'2005, pages 526­537. Springer, 2015.",0,,False
"[8] Xuehua Shen, Bin Tan, and ChengXiang Zhai. Context-sensitive information retrieval using implicit feedback. In Proceedings of SIGIR'2005.",0,,False
"[9] Marc Sloan, Hui Yang, and Jun Wang. A term-based methodology for query reformulation understanding. Inf. Retr., 18(2):145­165, April 2015.",0,,False
"[10] Bin Tan, Xuehua Shen, and ChengXiang Zhai. Mining long-term search history to improve search accuracy. In Proceedings of KDD'2006.",0,,False
"[11] Christophe Van Gysel, Evangelos Kanoulas, and Maarten de Rijke. Lexical query modeling in session search. In Proceedings of ICTIR'2016.",0,,False
"[12] Hui Yang, Dongyi Guan, and Sicong Zhang. e query change model: Modeling session search as a markov decision process. ACM Trans. Inf. Syst., 33(4):20:1­ 20:33, May 2015.",0,,False
"[13] Chengxiang Zhai and John La erty. A study of smoothing methods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2), April 2004.",0,,False
868,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
An Enhanced Approach to ery Performance Prediction Using Reference Lists,0,,False
Haggai Roitman,0,,False
"IBM Research - Haifa Haifa, Israel 31905 haggai@il.ibm.com",0,,False
ABSTRACT,0,,False
"We address the problem of query performance prediction (QPP) using reference lists. To date, no previous QPP method has been fully successful in generating and utilizing several pseudo-e ective and pseudo-ine ective reference lists. In this work, we try to ll the gaps. We rst propose a novel unsupervised approach for generating and selecting both types of reference lists using query perturbation and statistical inference. We then propose an enhanced QPP approach that utilizes both types of selected reference lists.",1,ad,True
1 BACKGROUND,0,,False
"We address the problem of query performance prediction (QPP) using reference lists. We focus on post-retrieval QPP [2]. Given a query, a corpus and a retrieval method that evaluates the query, a post-retrieval QPP method predicts the e ectiveness of the query's retrieved result list [2].",1,ad,True
"While existing post-retrieval QPP methods may seem di erent at rst glance, as Kurland et al. [4] have pointed out, many of them are actually built on the same grounds. Common to such methods is the usage of a single list that acts as a pseudo-e ective (PE for short) or pseudo-ine ective (PIE for short) reference list (RL for short) for predicting the performance of a given target list [4]. Example methods that utilize a single PE-RL are the ery Feedback [12] (QF) and the Utility Estimation Framework [7] (UEF) methods. Example methods that utilize a single PIE-RL are the Weighted Information Gain [12] (WIG) and the Normalized ery Commitment [9] (NQC) methods. Given a target list for prediction and a RL, the former's performance is typically predicted according to the similarity between the two lists [4].",1,WIG,True
"Few previous works have further tried to utilize several RLs for QPP [4, 5, 7, 8, 11]. Yet, these works have either manually selected1 RLs [4, 7] or generated RLs with no type distinction (i.e., PE or PIE) [8, 11]. In addition, works that did consider the RL types have either used only PE-RLs [5, 7] or linearly combined two RLs, one of each type [4]. Lately, Shtok et al. [8] have suggested to combine an arbitrary number of PE-RLs and PIE-RLs based on the",1,ad,True
"1For example, Kurland et al. [4] have manually selected relevant documents for ""generating"" a PE-RL.",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080665",1,ad,True
"log-odds of the relevance of a given target list for prediction. Yet, as the authors have pointed out, they were not able to nd an educated way for generating PIE-RLs and, therefore, they could not fully validate their proposal [8].",0,,False
2 TOWARDS AN ENHANCED APPROACH,1,AP,True
In this work we address two main problems:,1,ad,True
(1) How to automatically generate PE-RLs and PIE-RLs given a target list for performance prediction?,0,,False
(2) How to utilize both types of RLs for QPP?,0,,False
"To address the rst problem, we propose a novel unsupervised approach for generating and selecting both types of RLs based on query perturbation and statistical inference. To this end, we utilize two state-of-the-art QPP methods as sample moments' estimators.",1,ad,True
"To address the second problem, we suggest an enhanced QPP approach that combines the selected PE-RLs and PIE-RLs together based on the weighted mean of their predicted qualities. For that, we utilize only the most signi cant PE-RLs and PEI-RLs and weigh them using a similarity measure that was never applied before for QPP. Our evaluation demonstrates that, overall, using our proposed RL-based QPP approach signi cantly improves prediction.",1,ad,True
3 FRAMEWORK,0,,False
3.1 Preliminaries,0,,False
"Let D denote a top-k (ranked) list of documents, retrieved from corpus C by some (retrieval) method M in response to query q. Let sM (d |q) denote the score assigned to a document d  C by method M given query q. Let sM (C |q) further denote the score assigned to the whole collection. e post-retrieval QPP task is to predict the retrieval quality (performance) of the target list D given query q [2]. We hereina er denote such prediction Q^(D|q).",0,,False
3.2 Candidate RLs Generation,0,,False
"As the rst step, we generate a pool of candidate RLs based on query-perturbation. ery perturbation is implemented so as to generate queries that are more or less relevant to the same information need of query q. To this end, let w denote a single term in the vocabulary V . Given query q, we rst induce a relevance model p(w |R) using the target list D's top-m ranked documents (denoted as D[m]) as pseudo-relevance feedback [6]. We next consider only the top-n terms w ( V ) with the highest likelihood p(w |R). For each considered term w, let qw denote the corresponding perturbed version of query q, obtained by expanding q with a single additional",1,ad,True
869,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
disjunctive term2 w. Using query qw and method M we then retrieve the corresponding (top-k) RL Dw and add it as a candidate to the pool.,1,ad,True
3.3 RLs Selection,0,,False
"As in any query expansion se ing, we anticipate that some expan-",0,,False
sion terms may improve query q; while some others may dri the,0,,False
"query and, therefore, have a negative e ect on the retrieval's qual-",0,,False
"ity [6]. Using the target list D as the baseline for comparison, we",0,,False
"further wish to select only those RLs Dw whose performance is (presumably) signi cantly be er (i.e., PE) or worse (i.e., PIE) than D's. Let Dr+ef and Dr-ef denote the set of PE-RLs and PIE-RLs that are selected, respectively.",0,,False
"Given query q, we now assume that method M's scores for doc-",0,,False
"uments in the corpus sM (d |q) are drawn from some (unknown) probability distribution. Hence, the target list D document scores",0,,False
"are actually samples from this distribution. Using this fact in mind,",0,,False
"we next describe two unsupervised schemes for selecting RLs Dw from the candidate pool to be assigned to either Dr+ef or Dr-ef . Both schemes are based on an initial step of score transformation,",0,,False
followed by the selection step that is based on statistical inference.,0,,False
"3.3.1 WIG-based selection. Given any query q (i.e., either q or",1,WIG,True
"qw ), we rst transform the scores of the documents in the corresponding (result) list D (i.e., either D or Dw ) that was retrieved by",0,,False
method M as follows:,0,,False
s~M,0,,False
(d,0,,False
|q ),0,,False
def,0,,False
",",0,,False
1 |q  |,0,,False
sM (d |q) - sM (C |q),0,,False
",",0,,False
(1),0,,False
where,0,,False
|q  |,0,,False
denotes,0,,False
query q,0,,False
length,0,,False
(note,0,,False
"that,",0,,False
|qw |,0,,False
def,0,,False
",",0,,False
|q| +,0,,False
"1). We now make the observation that, the WIG predictor [12],",1,WIG,True
de,0,,False
ned by Q^W IG (D|q),0,,False
def,0,,False
",",0,,False
1 k,0,,False
s~M (d |q),0,,False
d D,0,,False
(where,0,,False
k,0,,False
"k ),",0,,False
is actually the sample estimator of the mean of Eq. 1 transformed,0,,False
"scores' distribution. Using this observation, we now use statistical",0,,False
"inference for identifying (and selecting) both types of RLs. For a large enough sample size (e.g., k   30), under the as-",0,,False
"sumption that method M document scores are i.i.d, according to the Central Limit eorem, Q^W IG (D|q) (approximately) follows",0,,False
"a normal distribution. erefore, the decision whether a given RL",0,,False
Dw performs signi cantly be er or worse than the target list D,0,,False
may be validated by a statistical hypothesis test for the equality,0,,False
of (normal) means. As the null hypothesis H0 we shall assume,0,,False
"that, the two s~M (·|q) and s~M (·|qw ) score distributions have an equal mean. Whenever we can accept H0, then we reject Dw . On the other hand, assuming that H0 is rejected and Q^W IG (D|q) < Q^W IG (Dw |qw ), then we assume that Dw is a PE-RL of D and add it to Dr+ef . Similarly, assuming that H0 is rejected and Q^W IG (D|q) > Q^W IG (Dw |qw ), then we assume that Dw is a PIE-RL of D and add it to Dr-ef . To validate this hypothesis, we use the Welch's t-test [10] for equality of means with 95% of con dence.",1,ad,True
3.3.2 NQC-based selection. We now consider an alternative score,0,,False
transformation as follows:,0,,False
s~M,0,,False
(d,0,,False
|q),0,,False
def,0,,False
",",0,,False
sM (d |q) sM (C |q),0,,False
(2),0,,False
2 is is simply done by concatenating w to q's text.,0,,False
Using the fact that for any given variable x and real constant,0,,False
number c,0,,False
"0,",0,,False
V,0,,False
ar,0,,False
(,0,,False
1 c,0,,False
x,0,,False
),0,,False
def,0,,False
",",0,,False
1 c2,0,,False
V,0,,False
"ar (x),",0,,False
we,0,,False
now,0,,False
make,0,,False
another,0,,False
ob-,0,,False
"servation that, the NQC predictor [12], de",0,,False
ned,0,,False
by,0,,False
Q^,0,,False
N,0,,False
QC,0,,False
(D,0,,False
|q,0,,False
),0,,False
def,0,,False
",",0,,False
1 k d D,0,,False
s~M (d |q) - s~M (D|q),0,,False
2 (where k ,0,,False
 k and s~M (D|q),0,,False
further denotes the sample mean of D's documents transformed,0,,False
scores) is actually the sample estimator of the standard deviation of,0,,False
"Eq. 2 transformed scores' distribution. Similar to the WIG case, we",1,WIG,True
utilize a statistical test for selecting RLs from the candidate pool.,0,,False
"To this end, as the null hypothesis H0 we shall assume that, the two",0,,False
s~M (·|q) and s~M (·|qw ) score distributions have an equal variance (which is estimated by taking Q^N QC (D|q) to the power of two).,0,,False
"e rest of the decision, whether to reject a RL Dw or assign it to either Dr+ef or Dr-ef , is done in a similar way as was described for the WIG case. To validate this hypothesis, we use the Brown-",1,WIG,True
Forsythe test [1] for equality of variances with 95% of con dence.,0,,False
3.4 An Enhanced QPP Approach,0,,False
Here we propose an enhanced QPP approach based on both types,0,,False
of selected RLs (hereina er termed the Reference-Lists Selection-,0,,False
"based QPP method or RLS for short). For a given selected RL Dw ( Dr+ef  Dr-ef ), let p (Dw ) denote the p-value of the statistical",0,,False
"test [1, 10] used to validate its selection. us, the lower p (Dw ) is,",0,,False
the more con dence we have in Dw 's selection. Let Dr+e[lf] denote,0,,False
"the l PE-RLs Dw  Dr+ef with the lowest p (Dw ). In a similar way,",0,,False
we de ne Dr-e[lf]. We now propose to predict the performance of,0,,False
a given target list D based on the weighted mean of the predicted,0,,False
"qualities of its RLs3 Dw  Dr+e[lf]  Dr-e[lf], as follows:",0,,False
w · Q^ [base](Dw |qw ),0,,False
Q^ R[bLaSs,0,,False
e,0,,False
](D,0,,False
|q,0,,False
),0,,False
def,0,,False
",",0,,False
Dw,0,,False
w,0,,False
",",0,,False
(3),0,,False
Dw,0,,False
where Q^[base](Dw |qw ) is either the WIG or NQC base QPP method;,1,WIG,True
depending on which RLs selection approach we use. w further,0,,False
"denotes the weight (""importance"") of RL Dw and is calculated as",0,,False
follows:,0,,False
w,0,,False
def,0,,False
",",0,,False
"sim(D, Dw ), 1 - sim(D, Dw ),",0,,False
Dw  Dr+e[lf] Dw  Dr-e[lf],0,,False
(4),0,,False
"sim(D, Dw ) denotes the similarity between the target list D and a given RL Dw [5, 8]. erefore, according to Eq. 3, the similar the target list D is to the PE-RLs in Dr+e[lf] and the dissimilar it is from the PIE-RLs in Dr-e[lf], the be er its performance is predicted to be.",0,,False
"Finally, for measuring list-wise similarity, we adopt the Consistency-",1,ad,True
"Index measure, which was previously proposed in the context of",0,,False
"sequential forward (feature) selection [3]. As was noted in [3], the",0,,False
intersection size between two random subsets of a given set fol-,0,,False
"lows a hypergeometric distribution. e Consistency-Index, there-",0,,False
"fore, measures the level of agreement between the expected and",0,,False
the observed intersection sizes between two given subsets [3]. e,0,,False
"normalized-[0, 1] Consistency-Index based similarity is calculated",0,,False
as follows:,0,,False
"3Whenever Dr+e[lf]  Dr-e[lf] ,"" , we simply use Q^ [base](D |q).""",0,,False
870,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"sim(D,",0,,False
Dw,0,,False
),0,,False
def,0,,False
",",0,,False
1 2,0,,False
+,0,,False
|D,0,,False
 Dw | · 2k (nC,0,,False
nC -k,0,,False
- ),0,,False
k,0,,False
2,0,,False
",",0,,False
(5),0,,False
"where nC denotes the number of documents in corpus C. Note that, whenever |D  Dw | ,"" k, then sim(D, Dw ) "", 1; while when-",0,,False
ever,0,,False
|D,0,,False
Dw,0,,False
|,0,,False
",",0,,False
"0,",0,,False
then,0,,False
lim,0,,False
k,0,,False
nC 2,0,,False
"sim(D, Dw )",0,,False
",",0,,False
0.,0,,False
4 EVALUATION,0,,False
4.1 Datasets and Setup,0,,False
Corpus #documents,0,,False
eries,0,,False
Disks,0,,False
SJMN,1,SJMN,True
"90,257",0,,False
51-150,0,,False
3,0,,False
WSJ,0,,False
"173,252",0,,False
151-200,0,,False
1-2,0,,False
AP,1,AP,True
"242,918",0,,False
51-150,0,,False
1-3,0,,False
"ROBUST 528,155 301-450, 601-700 4&5-{CR}",0,,False
"WT10g 1,692,096",1,WT,True
451-550,0,,False
WT10g,1,WT,True
"GOV2 25,205,179",0,,False
701-850,0,,False
GOV2,0,,False
Table 1: TREC data used for experiments.,1,TREC,True
"e TREC corpora and queries used for the evaluation are speci ed in Table 1. ese benchmarks were used by previous QPP works [2], especially those that utilized reference lists (e.g., [4, 5, 7, 8]). Titles of TREC topics were used as queries. e Apache Lucene4 open source search library was used for indexing and searching documents. Documents and queries were processed using Lucene's English text analysis (i.e., tokenization, Poter stemming, stopwords, etc.). As the underlying retrieval method M we used Lucene's query-likelihood implementation with the Dirichlet smoothing parameter xed to µ ,"" 1000 (following [4, 5, 7, 8]).""",1,TREC,True
"For each query, we predicted the performance of the target list D based on its top-1000 retrieved documents (i.e., k ,"" 1000). Following the common practice [2], we measured the prediction over queries quality by the Pearson correlation between the predictor's values and the actual average precision (AP@1000) values calculated using TREC's relevance judgments.""",1,AP,True
"To generate candidate RLs, we used the Relevance Model 3 (RM3) [6], where we chose the top-100 (i.e., n , 100) terms w  V with the highest pRM3(w |R). e RM3 parameters were further xed as follows: m ,"" 10 (i.e., the number of pseudo-relevance feedback documents) and  "","" 0.9 (i.e., query anchoring) [6]. is has le us with only two parameters to tune: k   {30, 50, 100, 150, 200 , 500, 1000} ­ the sample size used for deriving the WIG and NQC RLs based selection methods; and l  {1, 2, . . . , 100} ­ the number of the lowest p-valued RLs in Dr+ef and Dr-ef to be used for the actual prediction. To this end, we used the SJMN corpus for training; with (k  "","" 100, l "", 5) and (k  ,"" 150, l "","" 3) tuned for the WIG and NQC based selection methods, respectively. We used the rest of the corpora for testing.""",1,WIG,True
4.2 Baselines,0,,False
"We compared our proposed QPP approach with the following baselines. First, we evaluated both basic QPP methods (i.e., WIG and NQC) as ""standalone"" methods. Following previous recommendations, we set k  (the number of high ranked documents used for",1,WIG,True
4h p://lucene.apache.org,0,,False
prediction) to 5 for WIG [12] and 150 for NQC [9]. We further,1,WIG,True
evaluated the QF method as an alternative QPP method. To this,0,,False
"end, we rst run an expanded query based on a selection of the",0,,False
top-100 terms w ( V ) with the highest contribution to the KL di-,0,,False
vergence between pRM1(w |R) and p(w |C) [12]; Let DR denote the corresponding retrieved RL [12]. e QF prediction is then sim-,0,,False
ply given by the number of documents that are shared among the,0,,False
"top-50 documents in D and RL DR [12]. We further evaluated state-of-the-art QPP alternatives [4, 5, 7, 8]",0,,False
that have also utilized one or more PE-RLs or PIE-RLs. e rst al-,0,,False
"ternative is the UEF method [7]. For a given target list D, UEF",0,,False
utilizes a single PE-RL obtained by reranking D according to a,0,,False
relevance model induced from the pseudo-relevance feedback set,0,,False
D[m].,0,,False
UEF prediction is calculated as follows:,0,,False
Q^U[bEaFse](D |q),0,,False
def,0,,False
",",0,,False
"sim(D, R (D))Q^[base](D[m]|q), where R (D) further denotes the",0,,False
the reranking (permutation) of D according to the (RM1) relevance,0,,False
"model pRM1(w |R) (i.e.,  ,"" 0, following [7]). Following [7], sim(D, R (D)) was measured by Pearson's correlation (on document scores) and""",0,,False
"se ing m , 5 and m , 150 for the WIG and NQC base predic-",1,WIG,True
"tors [7]. Next, we evaluated the RefList [5] method, an extended",0,,False
"approach of UEF [5], ""designed"" to utilize several RLs. To this",0,,False
"end, we follow [5] and generate 10 RLs Dµ by varying the smoothing parameter µ used for QL scoring of documents5 in the cor-",0,,False
pus given query q. RefList prediction is calculated as follows [5]:,0,,False
Q^,0,,False
[b as e ] Ref Lis,0,,False
t,0,,False
(D,0,,False
|q),0,,False
def,0,,False
",",0,,False
"sim(D, Dµ )Q^[base](Dµ |qµ ). sim(D, Dµ ) is",0,,False
µ,0,,False
again measured by Pearson's correlation [5]. We also implemented,0,,False
Kurland et al.'s [4] approach (denoted hereina er as PE-PIE) which,0,,False
predicts the quality based on a single PE-RL (D+) and a single,0,,False
PIE-RL (D-) as follows: Q^P E-P I E (D |q),0,,False
def,0,,False
",",0,,False
"sim(D, D+) - (1 -",0,,False
" )sim(D, D-); with   [0, 1] [4]. We further chose D+ and D-",0,,False
"to be the most signi cant PE-RL and PIE-RL (i.e., according to",0,,False
"p (Dw )) in Dr+ef and Dr-ef , respectively. e  smooth param-",0,,False
"eter was tuned using the SJMN dataset; yielding  ,"" 0.5. Finally,""",1,SJMN,True
we further implemented the LogOdds approach that was recently,0,,False
proposed by Shtok et al. [8]. LogOdds extends RefList with a,0,,False
"PE-PIE inspired PE-RLs and PIE-RLs utilization, calculated as:",0,,False
"sim(D, Dw )Q^[base](Dw |qw )",0,,False
Q^L[boasOed] d,0,,False
s,0,,False
(D,0,,False
|q),0,,False
def,0,,False
",",0,,False
log Dw  Dr+ef,0,,False
Dw  Dr-e f,0,,False
"sim(D, Dw )Q^[base](Dw |qw )",0,,False
"Following [8], sim(D, Dw ) was measured using the rank-biased overlap measure (RBO), with its free parameter set to 0.95 [8]. It is",0,,False
"worth noting again that, Shtok et al. [8] could not nd an educated",0,,False
"way to obtain PIE-RLs, and therefore, their approach has not been",0,,False
"fully validated until now [8]. Finally, we used base  {W IG, N QC} for instantiating Q^[base](·|·) in all the evaluated methods. Statistical di erence in correlations was further measured.",0,,False
4.3 Results,0,,False
"e results of our evaluation are summarized in Table 2. First, comparing RLS side-by-side with the WIG and NQC base methods, we observe that, RLS boosts the laters' performance (signi cantly in",1,WIG,True
"5 µ  {100, 200, 500, 800, 1500, 2000, 3000, 4000, 5000, 10000} [5].",0,,False
871,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
WSJ AP ROBUST WT10g GOV2,1,AP,True
WIG,1,WIG,True
.677 .617r,0,,False
.528r,0,,False
.407r .487r,0,,False
QF PE-PIE[WIG] UEF[WIG] RefList[WIG] LogOdds[WIG] RLS[WIG],1,WIG,True
.502br ..069077bbrr .668 -.065br .702,0,,False
.575br ...166155551bbrrr -.121br .678b,0,,False
.435br -....255267205632bbbrrrbr .591b,0,,False
.451br ..148285brr .398r -.130br .472b,0,,False
.368br ..346725brr -..511265brbr .533b,0,,False
(a) Methods comparison based on WIG as the base method,1,WIG,True
WSJ AP ROBUST WT10g GOV2,1,AP,True
NQC,0,,False
.727 .602r,0,,False
.557r,0,,False
.496r .348r,0,,False
QF PE-PIE[NQC] UEF[NQC] RefList[NQC] LogOdds[NQC] RLS[NQC],0,,False
...517051232bbrrr,0,,False
.725 -.012br .748,0,,False
..517457brr .625r .630r -.188br .653b,0,,False
...435308511bbrrr .552r -.223r,0,,False
.619b,0,,False
....425455391731bbbrrrr -.058br .553b,0,,False
..336083brr .372br .430b -.053br .424b,0,,False
(b) Methods comparison based on NQC as the base method,0,,False
"Table 2: Pearson's correlation to AP per corpus and evaluated QPP method. Numbers denoted with b and r further represent a statistical signi cant di erence with the base QPP method (i.e., either WIG or NQC) and RLS, respectively.",1,AP,True
"most cases); with an average of +10.2 ± 2% and +11.2 ± 3% improvement over WIG and NQC, respectively. Next, comparing RLS side-by-side with the other alternatives, we further observe that, RLS in all cases but one, provides be er prediction quality (again, signi cantly in most cases); with an average improvement of +5.2±1.6% and +3±1.4% over the best alternative, when WIG and NQC are used as the underlying base method, respectively.",1,WIG,True
"We next make the observation that, while RLS provided a consistent improvement over WIG and NQC, when the later were used as the underlying base methods, the other alternatives do not share the same behavior. Closer examination of the results of these alternatives across the corpora shades some light. Focusing on the UEF method, we can observe that, only in 7 out of the 10 cases it managed to improve the base method. is may be a ributed to the RL R (D) utilized by UEF for prediction, where for some corpora such RLs may not comply with UEF's PE-RL assumption (e.g., due to possible query-dri [6]). Hence, measuring the similarity with such RLs actually result in performance degradation. is argument is further supported by examining the performance of the RefList method (an extension of UEF [5]), where in only 4 out of the 10 cases it managed to improve over the base method. Such additional performance degradation may be a ributed to the fact that, RefList aggregates over several RLs with no distinction of their type, and, therefore, it may accumulate even more error.",1,WIG,True
"Closer examination of the two alternative methods that do distinguish between both types of RLs, PE-PIE and LogLoss, further reveals an interesting trend. First, we observe that, PE-PIE in 9 out of 10 cases has resulted in much worse performance than that of the base method used to obtain the two (PE and PIE) RLs. erefore, it seems that, a simple linear interpolation of a PE-RL with a PIE-RL as proposed by the PE-PIE method, where the dissimilarity from the PIE-RL is calculated by substraction, does not actually work well. Similar to the UEF vs. RefList case, a further comparison of the PE-PIE with LogLoss supports this argument. e summation over several PIE-RLs using a similar (log) substraction approach only results in further performance degradation due to more accumulated error. It is worth noting again that, both PE-PIE and LogLoss methods were never evaluated with automatically selected RLs. To remind, in [4], the PE-PIE method was tested with a manual selection of the PE-RL and the PIE-RL; whereas, the LogLoss method was not fully validated [8].",1,ad,True
"To conclude, among the various alternatives we have examined, none has exhibited a robust performance prediction similar to that",0,,False
"of the RLS approach. We believe that, our results shade new light on the problem of QPP using RLs and the challenges that the designers of such methods may face.",1,ad,True
ACKNOWLEDGEMENT,0,,False
I would like to thank the reviewers for their fruitful comments. Special thanks to my wife Janna and daughters Inbar and Einav for their endless love and support.,0,,False
REFERENCES,0,,False
"[1] Morton B Brown and Alan B Forsythe. Robust tests for the equality of variances. Journal of the American Statistical Association, 69(346):364­367, 1974.",1,Robust,True
"[2] David Carmel and Oren Kurland. ery performance prediction for ir. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '12, pages 1196­1197, New York, NY, USA, 2012. ACM.",0,,False
"[3] Ludmila I. Kuncheva. A stability index for feature selection. In Proceedings of the 25th Conference on Proceedings of the 25th IASTED International MultiConference: Arti cial Intelligence and Applications, AIAP'07, pages 390­395, Anaheim, CA, USA, 2007. ACTA Press.",1,AP,True
"[4] Oren Kurland, Anna Shtok, David Carmel, and Shay Hummel. A uni ed framework for post-retrieval query-performance prediction. In Proceedings of the ird International Conference on Advances in Information Retrieval eory, ICTIR'11, pages 15­26, Berlin, Heidelberg, 2011. Springer-Verlag.",0,,False
"[5] Oren Kurland, Anna Shtok, Shay Hummel, Fiana Raiber, David Carmel, and Ofri Rom. Back to the roots: A probabilistic framework for query-performance prediction. In Proceedings of the 21st ACM International Conference on Information and Knowledge Management, CIKM '12, pages 823­832, New York, NY, USA, 2012. ACM.",0,,False
"[6] Victor Lavrenko and W. Bruce Cro . Relevance based language models. In Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '01, pages 120­127, New York, NY, USA, 2001. ACM.",0,,False
"[7] Anna Shtok, Oren Kurland, and David Carmel. Using statistical decision theory and relevance models for query-performance prediction. In Proceedings of the 33rd International ACM SIGIR Conference on Research and Development in Information Retrieval.",0,,False
"[8] Anna Shtok, Oren Kurland, and David Carmel. ery performance prediction using reference lists. ACM Trans. Inf. Syst., 34(4):19:1­19:34, June 2016.",0,,False
"[9] Anna Shtok, Oren Kurland, David Carmel, Fiana Raiber, and Gad Markovits. Predicting query performance by query-dri estimation. ACM Trans. Inf. Syst., 30(2):11:1­11:35, May 2012.",1,ad,True
"[10] Bernard L Welch. e generalization of student's' problem when several di erent population variances are involved. Biometrika, 34(1/2):28­35, 1947.",0,,False
"[11] Elad Yom-Tov, Shai Fine, David Carmel, and Adam Darlow. Learning to estimate query di culty: Including applications to missing content detection and distributed information retrieval. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.",1,ad,True
"[12] Yun Zhou and W. Bruce Cro . ery performance prediction in web search environments. In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '07, pages 543­550, New York, NY, USA, 2007. ACM.",0,,False
872,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Sub-corpora Impact on System E ectiveness,1,corpora,True
Nicola Ferro,0,,False
"Department of Information Engineering, University of Padua, Italy ferro@dei.unipd.it",1,ad,True
ABSTRACT,0,,False
"Understanding the factors comprising IR system e ectiveness is of primary importance to compare di erent IR systems. E ectiveness is traditionally broken down, using ANOVA, into a topic and a system e ect but this leaves out a key component of our evaluation paradigm: the collections of documents. We break down e ectiveness into topic, system and sub-corpus e ects and compare it to the traditional break down, considering what happens when di erent evaluation measures come into play. We found that sub-corpora are a signi cant e ect. e consideration of which allows us to be more accurate in estimating what systems are signi cantly di erent. We also found that the sub-corpora a ect di erent evaluation measures in di erent ways and this may impact on what systems are considered signi cantly di erent.",1,ad,True
CCS CONCEPTS,0,,False
·Information systems Evaluation of retrieval results; Test collections;,0,,False
KEYWORDS,0,,False
experimental evaluation; retrieval e ectiveness; sub-corpus e ect; e ectiveness model; GLMM; ANOVA,1,LM,True
1 INTRODUCTION,1,DUC,True
"Studying the e ectiveness of Information Retrieval (IR) systems is a core area of investigation, the main goal of which is to compare di erent IR systems in a robust and repeatable way. Commonly, IR system e ectiveness is broken down as",0,,False
"e ectiveness score , topic e ect + system e ect",0,,False
"e topic e ect was shown to be greater than the system e ect using a two-way ANOVA to decompose e ectiveness as above [1, 14].",0,,False
"e decomposition allowed simultaneous multiple comparisons of IR systems on TREC data, determining which were signi cantly be er than others.",1,TREC,True
"To improve the estimation of the system e ect, you need to add components to the above model. For example, [10] showed that a topic*system interaction improved the estimation but the reported experiments relied on simulated data. Using a Grid of Points (GoP) approach (i.e. IR systems originated by a factorial combination of",1,ad,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080674",1,ad,True
Mark Sanderson,0,,False
"Computer Science, School of Science, RMIT University, Melbourne, Australia",0,,False
mark.sanderson@rmit.edu.au,0,,False
their components) the system e ect can be sub-divided into system components in order to be er understand system behavior [3].,0,,False
"However, at least one ""ingredient"" is missing from consideration: the collections of documents that are an integral part of the evaluation paradigm. Past work studied how sub-corpora impact IR e ectiveness [13] and how collection size and the choice of documents in uenced the way that a test collection ranked one retrieval system relative to another [7]. Both these studies highlighted the importance of sub-corpora to system performance but they did not incorporate the sub-corpus e ect into a wider model:",1,ad,True
"e ectiveness score , topic e ect + system e ect + sub-corpus e ect",0,,False
"By integrating topic, system, and sub-corpus e ects into the one model, comparisons can be made between the magnitude of the e ects and, potentially, signi cant di erences between systems can be more accurately calculated.",1,ad,True
is paper addresses two research questions:,1,ad,True
RQ1 what is the impact of considering sub-corpora in an e ectiveness model?,1,corpora,True
RQ2 how do di erent evaluation measures behave with respect to e ectiveness models including sub-corpus e ects?,0,,False
"e methodology is described next (Sec. 2) followed by experiments and ndings (Sec. 3), before nally concluding (Sec. 4).",0,,False
2 METHODOLOGY,0,,False
"A General Linear Mixed Model (GLMM) [11] explains the variation of a dependent variable (""Data"") in terms of a controlled variation",1,LM,True
"of independent variables (""Model"") in addition to a residual uncon-",1,ad,True
"trolled variation (""Error""): Data ,"" Model + Error. In GLMM terms, ANalysis Of VAriance (ANOVA) a empts to explain data (dependent variable scores) in terms of the experimental conditions (the model)""",1,LM,True
"and an error component. Typically, ANOVA is used to determine",0,,False
under which condition dependent variables di er and what propor-,0,,False
tion of variation can be a ributed to di erences between speci c,0,,False
"conditions, as de ned by the independent variable(s).",0,,False
"e experimental design determines how to compute the model and estimate its parameters. It is possible to have an independent measures design where di erent subjects participate in di erent experimental conditions (factors) or a repeated measures design, where each subject participates in all experimental conditions (factors). A",0,,False
"nal distinction is between crossed/factorial designs ­ where every level of one factor is measured in combination with every level of the other factors ­ and nested designs, where levels of a factor are grouped within each level of another nesting factor.",0,,False
e traditional crossed repeated measures two-way ANOVA,1,ad,True
"design, used in past work [1, 14], breaks down e ectiveness into a",0,,False
topic and a system e ect:,0,,False
"Yi j , µ ·· + i + j + i j",0,,False
(1),0,,False
901,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Factor j - System,0,,False
Factor k - Sub-corpus,0,,False
1,0,,False
···,0,,False
q,0,,False
1 ···,0,,False
r,0,,False
1 ···,0,,False
r,0,,False
effectiv.,0,,False
effectiv.,0,,False
effectiv.,0,,False
effectiv.,0,,False
1 score,0,,False
···,0,,False
score,0,,False
···,0,,False
score,0,,False
···,0,,False
score,0,,False
Y111,0,,False
Y11r,0,,False
Y1q1,0,,False
Y1qr,0,,False
XXX,0,,False
XXX,0,,False
XXX,0,,False
XXX,0,,False
XXX,0,,False
XXX,0,,False
XXX,0,,False
XXX,0,,False
Subject i - Topic,0,,False
effectiv.,0,,False
effectiv.,0,,False
effectiv.,0,,False
effectiv.,0,,False
p score,0,,False
···,0,,False
score,0,,False
···,0,,False
score,0,,False
···,0,,False
score,0,,False
Yp11,0,,False
Yp1r,0,,False
Ypq1,0,,False
Ypqr,0,,False
"Figure 1: Model for topic, system, and sub-corpus e ects.",0,,False
"where Yij is the e ectiveness score (from an evaluation measure) of the i-th subject in the j-th factor; µ ·· is the grand mean; i ,"" µi · -µ ·· is the e ect of the i-th subject, i.e. a topic, where µi · is the mean of the i-th subject; j "","" µ ·j - µ ·· is the e ect of the j-th factor, i.e. a system, where µ ·j is the mean of the j-th factor; nally, ij is the error commi ed by the model in predicting the e ectiveness score of the i-th subject in the factor j. Examining eq (1) on both a whole""",0,,False
and split collection (i.e. sub-corpora) we can understand changes,1,corpora,True
to e ectiveness between these two collection conditions.,0,,False
We also explore a crossed repeated measures three-way ANOVA,0,,False
"design, which breaks down e ectiveness into a topic, system, and",0,,False
"sub-corpus e ect, as shown in Figure 1:",0,,False
"Yi jk , µ ··· + i + j + k + ( )jk + i jk",0,,False
(2),0,,False
"where: Yijk is the e ectiveness score of the i-th subject in the j-th and k-th factors; µ ··· is the grand mean; i ,"" µi ·· - µ ··· is the e ect of the i-th subject, i.e. a topic, where µi ·· is the mean of the i-th subject; j "","" µ ·j · - µ ··· is the e ect of the j-th factor, i.e. a system, where µ ·j · is the mean of the j-th factor; k "","" µ ··k - µ ··· is the e ect of the k-th factor, i.e. a sub-corpus, where µ ··k is the mean of the k-th factor; ( )jk is the interaction between systems and sub-corpora; nally, ijk is the error commi ed by the model in predicting the e ectiveness score of the i-th subject in the two""",1,corpora,True
factors j and k.,0,,False
"We compare the GLMM models in eqs (1) and (2). Note, when we",1,LM,True
"apply eq (1) to sub-corpora, we use the design shown in Figure 1 but",1,corpora,True
"omit the k sub-corpus e ect. us, we obtain a two-way ANOVA where we have more replicates for each (topic, system) pair, one",0,,False
"for each sub-corpus. An ANOVA test outcome indicates, for each factor, the Sum of",0,,False
"Squares (SS), the Degrees of Freedom (DF), the Mean Squares (MS), the",0,,False
"F statistics, and the p-value of that factor, to determine signi cance.",0,,False
"We are also interested in determining the proportion of variance that is due to a particular factor: i.e. we estimate its e ect-size measure or Strength of Association (SOA), which is a ""standardized index and estimates a parameter that is independent of sample size and quanti es the magnitude of the di erence between populations or the relationship between explanatory and response variables"" [9].",0,,False
^ 2f act,0,,False
",",0,,False
d ff act (Ff act d ff act (Ff act -,0,,False
- 1) 1) + N,0,,False
is an unbiased estimator of the variance components associated,0,,False
"with the sources of variation in the design, where Ff act is the F-",0,,False
statistic and d ff act are the degrees of freedom for the factor while,0,,False
N is the total number of samples. e common rule of thumb [11],0,,False
"when classifying ^ 2f act e ect size is: > 0.14 is a large size e ect,",0,,False
"0.06­0.14 is a medium size e ect, and 0.01­0.06 is a small size e ect.",0,,False
Negative,0,,False
^,0,,False
2 f,0,,False
ac,0,,False
t,0,,False
values are considered as zero.,0,,False
"In experimentation, a Type I error occurs if a true null hypothesis",0,,False
is rejected. e probability of such an error is . e chances of,0,,False
making a Type I error for a series of comparisons is greater than the,0,,False
"error rate for a single comparison. If we consider C comparisons, the probability of at least one Type I error is 1-(1-)C , which increases",0,,False
with the number of comparisons. Type I errors are controlled by,0,,False
applying the Tukey Honestly Signi cant Di erence (HSD) test [5],0,,False
"with a signi cance level  , 0.05. Tukey's method is used in",0,,False
ANOVA to create con dence intervals for all pairwise di erences,0,,False
"between factor levels, while controlling the family error rate. Two",0,,False
levels u and of a factor are considered signi cantly di erent when,0,,False
"|t | ,",0,,False
|µ^u - µ^ |,0,,False
MSe r r or,0,,False
1 nu,0,,False
+,0,,False
1 n,0,,False
>,0,,False
1 2,0,,False
"q,k,",0,,False
N,0,,False
-k,0,,False
"where nu and n are the sizes of levels u and ; q,k, N -k is the upper 100(1-)th percentile of the studentized range distribution with parameter k and N - k degrees of freedom; k is the number of levels in the factor and N is the total number of observations.",0,,False
3 EXPERIMENTS,0,,False
"We used the TREC Adhoc T07 and T08 collections: 528,155 documents made up of four TIPSTER sub-corpora: Foreign Broadcast Information Service (TIPFBIS, 130,471 documents); Federal Register (TIPFR, 55,630 documents); Financial Times (TIPFT, 210,158 documents); and Los Angeles Times (TIPLA, 131,896 documents). T07 and T08 provide 50 topics: 351-400 and 401-450, as well as",1,TREC,True
"binary relevance judgments drawn from a pool depth of 100; 103 and 129 runs were submi ed to T07 and T08, respectively.",0,,False
We split the T07 and T08 runs on the four sub-corpora by keeping,1,corpora,True
the retrieved documents that belong to each sub-corpus. We applied,0,,False
the same split procedure to relevance judgments. is caused some,0,,False
"topics to have no relevant documents on some sub-corpora, which",1,corpora,True
suggests some kind of bias during topic creation and/or relevance,0,,False
"assessment. Consequently, we kept only the topics that have at",0,,False
"least one relevant document on each sub-corpus. is le us with 22 topics for T07 and 15 topics for T08. We used eight evaluation measures: Average Precision (AP), P@10; Rprec, Rank-Biased Precision (RBP) [8], Normalized Discounted Cumulated Gain (nDCG) [6], nDCG@20, Expected Reciprocal Rank (ERR) [2], and Twist [4].",1,AP,True
Code to run the experiments is available at: h ps://bitbucket.,0,,False
org/frrncl/sigir2017-fs/.,0,,False
3.1 RQ1 ­ Sub-corpora & e ectiveness models,1,corpora,True
Figure 2 shows a worked example of the outcome of the application,0,,False
of the models on T08 and AP. Figure 2(a) shows the ANOVA table,1,AP,True
for eq (1) on the whole collection. Both the topic and the system,0,,False
e,0,,False
ects are signi,0,,False
cant and large: the system e,0,,False
ect,0,,False
is,0,,False
about,0,,False
3 5,0,,False
the,0,,False
902,0,,False
Put Title Here,0,,False
"SIGIR'17, August 7­11 2017, Tokyo, Japan",0,,False
Put Title Here Short Research Paper,0,,False
"SIGIR'17,",0,,False
August,0,,False
7­11,0,,False
"Source 201T7o, Tpoickyo,",0,,False
SS DF,0,,False
"J3a1p.aS0n0I5G6 IR'1174 ,",0,,False
"A2.uM21gS4u7 st2729-.F12813,3201p7-v, aSluhei0nju^k02hf.u6a2,c2tT9i okyo,",0,,False
Japan,0,,False
Put Title Here,0,,False
Source SS DF MS,0,,False
F,0,,False
Topic 31.0056 14 2.2147 229.2833,0,,False
System 14.5575 128 0.1137,0,,False
p-value 0,0,,False
"S^ 0I2hGf.6aI2Rc2t'9i17,",0,,False
August,0,,False
7­11,0,,False
"Error 201T7o, Ttaolkyo,",0,,False
17.3092 Ja6p2.a8n722,0,,False
1792 1934,0,,False
0.0097,0,,False
11.7744 5.774e-160,0,,False
0.4161,0,,False
System 14.5575 128 0.1137 11.7744,0,,False
ESroruorrce 17.S3S092 1D79F2 0.M00S97,0,,False
F,0,,False
Toptailc 3612.08075262 19134 2.2147 229.2833,0,,False
5.774e-160 p-value,0,,False
0,0,,False
0.4161,0,,False
^,0,,False
2 hf,0,,False
ac,0,,False
t,0,,False
i,0,,False
0.6229,0,,False
Source SS,0,,False
DF MS,0,,False
F,0,,False
p-value,0,,False
^,0,,False
2 hf,0,,False
ac,0,,False
t,0,,False
i,0,,False
Topic 181.1610 14 12.9401 326.3519,0,,False
0 0.3705,0,,False
System 14.5575 128 0.1137 11.7744 5.774e-160 0.4161,0,,False
SEorurrocre 17S.3S092 1D79F2 0.0M09S7,0,,False
F,0,,False
p-value,0,,False
^,0,,False
2 hf,0,,False
ac,0,,False
t,0,,False
i,0,,False
TTooptiacl 16821..81762120 193144 12.9401 326.3519,0,,False
0 0.3705,0,,False
System 62.2931 128 Error 301.2262 7597 Total 544.6802 7739,0,,False
0.4867 12.2738 1.352e-220 0.0397,0,,False
0.1571,0,,False
(a),0,,False
ANSOysVteAm,0,,False
ESroruorrce,0,,False
tab62le.29f3o1 r,0,,False
301S.2S262,0,,False
m12o8del0.o48f6e7 q,0,,False
7D59F7 0M.03S97,0,,False
(112).2o73n8,0,,False
F,0,,False
th1.e352we-h22o0le,0,,False
p-value,0,,False
c^ 20hof.1al5cle7t 1iction.,0,,False
(b),0,,False
ANOVA,0,,False
Source,0,,False
table,0,,False
for,0,,False
SS,0,,False
model,0,,False
DF,0,,False
of,0,,False
eq,0,,False
MS,0,,False
(1),0,,False
on,0,,False
F,0,,False
the,0,,False
spu-bva-lcueorpo^2hrfaa.ct i,0,,False
Toptailc 158414.16681002 771349 12.9401 326.3519,0,,False
0 0.3705,0,,False
Topic,0,,False
181.1610 14 12.9401 349.9769,0,,False
0 0.3870,0,,False
System 62.2931 128,0,,False
ESroruorrce 301.2262 S7S597,0,,False
Topic Total 544.6801281.17671309,0,,False
System,0,,False
62.2931,0,,False
Sub-CoSropuursce,0,,False
21S.0S526,0,,False
STuobp-iCc orpus*System 18113.15691005,0,,False
ESyrrsoterm,0,,False
2626.259831,0,,False
TSuobta-Cl orpus,0,,False
52414.06582062,0,,False
0.4867 12.2738 1.352e-220 0.1571,0,,False
System,0,,False
62.2931 128 0.4867 13.1623 5.812e-238 0.1675,0,,False
D0.F0397 MS,0,,False
F,0,,False
p-value,0,,False
^,0,,False
2 hf,0,,False
ac,0,,False
t,0,,False
i,0,,False
Sub-Corpus,0,,False
21.0526 3 7.0175 189.7959 1.829e-118 0.0682,0,,False
14 12.9401 349.9769,0,,False
0 0.3870 Sub-Corpus*System 13.5905 384 0.0354 0.9572,0,,False
0.7137,0,,False
­,0,,False
128 0.4867 13.1623 5.812e-238 0.1675 Error,0,,False
266.5831 7210 0.0370,0,,False
DF3 3184,0,,False
7M.01S75 120.90430514,0,,False
189.F7959 1p.8-2v9ael-u1e18 ^ 20hf.0a6c8t 2i 3490.9756792Table 10:.7A1N307OVA0t.3a8b7l­e0,0,,False
Total for track,0,,False
T08,0,,False
and,0,,False
544.6802 measure AP: (a),1,AP,True
7739 is the,0,,False
model,0,,False
of,0,,False
eq.,0,,False
(1),0,,False
on,0,,False
the,0,,False
whole,0,,False
TIPSTER,0,,False
collection;,0,,False
(b),0,,False
is,0,,False
the,0,,False
model,0,,False
"7122180 0.40836770 13.1623of 5e.q8.12(1e)-2o3n8 the 0T.I16P7F5BIS, TIPFR, TIPFT, and TIPLA splits; and, (c) is the model of eq. (2) on the TIPFBIS, TIPFR, TIPFT, and TIPLA",0,,False
77339 7.0175 189.7959spl1i.t8s2.9e-118 0.0682,0,,False
Table 1: ANOVA table foSrutbra-CckorTp0u8sa*nSydsmteemasure13A.5P9:0(5a) is3t8h4e mo0d.0e3l5o4f eq. 0(1.9)5o7n2 the wh0o.7l1e3T7IPSTER co­llection; (b) is the model,0,,False
"of eq. (1) on the TIPFBISE, rTrIoPrFR, TIPFT, and TIP2L6A6s.5p8l3it1s; a7n2d10, (c) i0s.0t3h7e0model of eq. (2) on the TIPFBIS, TIPFR, TIPFT, and TIPLA",0,,False
splits. Table 1:,0,,False
ANOVA,0,,False
table,0,,False
To(tca)l ANOVA table54f4o.6r80m2 o7d7e39l,0,,False
for track T08 and measure AP: (a) is the,1,AP,True
of eq (2) on,0,,False
model of eq. (1),0,,False
"othneastibhmoseiuluwat rb35ho-tnhlcdeeoisnTirIgzpPseSbooTyfrEt[ahR1e,.c1too7lp]l.eicFcitenioaelnlcy;t,.(abc)cieossr(etddhine)ngdmMtinooagtdshienealrTeeuckffoeheyecHretSnsDt wpteitslhot, t",0,,False
"foratnhdesysstyemsteemectse, tffoeacllotw. us to distinguish between more",0,,False
"of signicantly dierent systems but being more ""picky"" on",0,,False
pairs what,0,,False
"of eq. (1) on the TIPFBIS, TIPFR, TIPFT, and TIPLA splits; and, (c) is the model of e2q,8.6(52)oount otfh8e,2T5I6PF(3B4I.S7,0%TI)PpFoRs,siTbIlPeFsTy,satenmd pTaIiPrLsAare signicantly",0,,False
systems are in the top group.,0,,False
"aspboliutts.35 the size of the topic eect. ese ndings are coherent with similar ndings by [1, 17]. Finally, according to the Tukey HSD test,",0,,False
"and system eedctis,etroenaltlowwituhs1t2o0doisuttinogfu1is2h9 b(9et3w.0e2e%n) msyosrteempasirbseing in the topof signicantly gdriouerpe.nt systems but being more ""picky"" on what",0,,False
4 CONCLUSION AND FUTURE WORK,0,,False
"(e) Main effects plot for the sub-corpus effect. (f) Interaction effects plot for the sub-corpus*system effect. 2,865 out of 8,256 (34.70%) possible system pairs are signicantly",0,,False
"dgasibrimooueiulrpater.35nttnhwdeiisntihgzse1bo2yf0t[ho1eu, 1tto7op]f.icF1ie2n9ael(lc9yt3,. .a0c2ce%osr)edsiynnsgdtetinomgtssheabreTeiunckgoehyienHretSnhDtewtteoitspht-, 2,8T65aboluet1o(bf )8s,2h5o6w(3s4t.h7e0%m)opdoeslsiobfleeqsuyastteiomn p(1a)irbsuatreapspiglineidctoantthlye fdoiurerdeinterwenitthsp1l2it0s o­uTtIoPfF1B2I9S,(T9I3P.0F2R%, )TIsyPsFtTe,mans dbeTiInPgLAin­tfhoer tTo0p8agnroduAp.P. We can note that both the topic and the system eects are sigTniabclaen1t(ban) dshlaorwges sthizee meoedcetls.oMf eoqreuoavtieor,nth(1e)tboupticaeppelicetdistomtohree pforuormdinieenrtenthtasnpltihtse­syTsItPemFBIeS,eTctIPwFhRo,sTeIsPiFzeT,isanadboTuItPL25Ath­efosirzTe0o8f tahnedtAopPi.cWeeeccatn. Fniontaelltyh, aatccboortdhinthgetototphiec TanudketyheHsSyDstetemste, 3,e3c0t4s oauret osifg8n,i25c6an(4t0a.n02d%la)rpgoesssiizbeleesyecsttse.mMpoarierosvaerre, tshiegntoipcicanetleyctdiis emreonret wiptmhrieotphmtaoc1iptn0io9ecfnoeutustteihnocagtfn.1sFt2uhi9bne-a(c8slol4yyl.s,l5eta0ecc%mtci)ooesnrydssiehntceagtmswtbsohebteohensienetTgosuidiznkeeectiyrhseeHaatsbSoeoDptu-htgteer25sorteut,lhp3ae.t,3iSvs0ioe4z,esotihzuoeetf ooff t8h,2e5t6op(4ic0.a0n2d%)sypsotsesmibeleesyctsst,etmo aplaloirws aursetsoigdnisitincagnutilsyh dbietwereeennt wmiotrhe1p0a9irosuotfosfig1n29i(c8a4n.5t0ly%d) isyesrteenmtssbyesitnemg isnbtuhtebteoipn-ggmroourpe. ""Spoi,ctkhye"" oimnpwachtaotfsuyssitnegmssuabr-ecoinlletchteiotnosphgarsobuepe.n to decrease the relative size of tFhineatlolyp,ictaabnled1s(ycs)tsehmowes etchtes,mtoodaellloowf euqsutaotidoinst(i2n)gaupisphliebdettwoetehne fmoourredpiaierrseonft ssipglnitis­caTnItlPyFdBiISe,rTeInPt FsyRs, tTeImPsFTb,uatnbdeiTnIgPmLAor­e f""opricTk0y8"" aonndwAhPa.tWsyestceamn snaortee itnhatthbeottohptghreotuopp.ic and the system eects are",1,ad,True
"Figure 2: Application of eq (1) and (2) to T08 sigFniinaclalyn,ttbabultea1ls(co) tshheowsusbt-hceormpuodsealnodf seuqbu-actoiornpu(2s*)saypsptelimedetoetchtes and AP both on the whole collection and the sub-corpora. aforuersidginierecnant ts.plitse­toTpIicPsFBaInSd,sTyIsPteFmR, TeIPeFctTs, aarnedlaTrIgPeLAsiz­efeor eTc0t8s",1,AP,True
wanhdilAe Pth. eWseubca-cnonrpoutes tahnadt bsuobth-ctohreputosp*sicysatnedmthee escytssteamreesmeacltlssaizree esigencitsc.a nt ebustizaelsooftthheessuybs-tceomrpeuseacntds sisuba-bcoourtpu25s*tshyestseimze eofetchtes,1,ad,True
"size of the topic e ect. ese ndings are consistent with past re- but the SS of the error is reduced by the amount corresponding to taorpeiscigeniecctawnth. ile ethtoepsiucsb-acnodrpsuystaenmdesuebc-tcsoarrpeulsa*rsgyestseimze eeccttss sults [1, 14]. e Tukey HSD test detects 1,825 out of 8,256 (22.11%) the SS of the sub-corpus and sub-corpus*system e ects. is makes warhei,lreetshpeecsutibv-ecloyr, paubsouant d110suabn-dco15rptuhse*syizseteomf teheescytsstaerme semaelclts.iFzeipossible system pairs as signi cantly di erent with 107 out of 129 the estimation of the e ect size of the topic and system e ects npetooaplseliscycit,bseal.ecc esocyrtsedtwiesnmihzgielpteooatiftrhhsteheaesTrueusbyks-isecgtyonermiHpSuceDasnaettneclystdstd,sii3us,b1ea0-rbc0eoonourtuptwtu25soit*fhtsh8y3e,s25t5seoi6mzue(t3eo7o.ff5e1t5ch2%te9s) (82.95%) systems being in the top-group, i.e. systems not signi - slightly more precise. e sub-corpus e ect is a signi cant medium (san2ura7ebl.l,-1yrc3,eo%aslcpl)ecescocytrtisdiotviennemsglyhst,oabasbethbioneeugetTniu1n10tkoteahydneedHctoSr15epDat-hgsteeersotstuh,izpe3e.,r1oSe0lfo0a,ttohtihvueetesosyimifszt8epe,2amo5cf6tetoh(3fee7uct.t5os.5ipnF%igic)cantly di erent from the top performing one. Figure 2(b) shows size e ect, about of the system and of the topic e ect, while possible system pairs are signicantly dierent with 35 out of 129",0,,False
systems are in the Ttoapblger1o(ubp).shows the model of equation (1) but applied to the,0,,False
"aos4Rynf EsdstieFgsCmynEsOisRteaNcmEraenCNeitnlLCyetUhcdfaspEoitnierSgsSuod,nItremotOiArodpiePincNnaga.elternWlnoorAstteyuweantNsphncttu.eaadDssmnnpltalsntoFirhtobgsdUetuei­esstTsyttTibihsznUIetaePgeitnRmeFubgBiEoseIemhtcShWto,bestrTce.hetItOMewPw""poRtFeohriRecpeoKn,koisTcyevmI""aesPornio,FzrdnteeTht,wiphesaaehntaioardsbpsytoTisucIttePemL25 Aetech­teeifscostirmzseToa0orr8eef",1,ad,True
"[1] D. Banks, P. Otvheer, taonpdiNc.-eF. Zechta.nFg.in19a9l9ly. ,BalicncdoMrdeinnagndtoEtlehpehaTnutsk: eSyixHASp-D test, 3,304 out",0,,False
"4R[2E] FCpCErEx.opOBaReucrNhicmEkeslCeeNntyotLCaaTnUnRwEoddEfSECiSEt.8IvhdMa,Oa2l1tu.5aN0aV.69toIinoo(oAnf4rouh0rieNtnm.e0osaI2Dn.tfi%f2o1o0nr)2F0mR59pUa.eo(ttrR8isoiTee4snvt.irUa5bRile0elv1eRt%ra(i)lsMeEvySsaayysylW st,st1eteD9emm.9Om9KE)p.sR,v7Haab­iKlaeur3rias4mnt.iaagIosnrnsie.unaIensntd1ihgT-E2eRn..EitMCo..cpa-gnrtolyupd.iSoe,rethnet",0,,False
[[13]] [2] [[34]],0,,False
[[45]],0,,False
[[56]],0,,False
[[67]] [8] [[79]] [8],0,,False
"VYWVDpCESI3iNORaCNSCRNMA6DPYTYSI3iNORaCNSCRNMA6DPYK2nnnn02ot0.n20otr.nxou.eoua..a.omh.omh............ufufeoet0ti00rPcrpndHPondHIbBonIbBnoCoFFuBCFFlhBJuFhKFKadakdnkan­­2eheeeearkle¨rklfrfureuueu,e,¨eosu¨aoshKyirchKy,irf),.yfhe.r6he6rAmnrrAmsrsruhXr,iouhXriohncficcUferaUerav44eHnrcheHnrchirSmornSonr4&rerrehrrakhgagepkqo.cp8oa.ce8aeoeoSomeSo,yme,eyrne2rnnntatasoblhoHbh.,usHels.n,eesnwrAwarAiaies2siScucruGrG,,ao,naelGoafeelGsafesetntntny(e(um­elumloolS,olaFS,oanF,rne,nPrrEenttrEsetreree.d.d,4,,onig.2,rogil.2roUmloa.Umada,.d,addaddaooaSTRSRaS4ac5Sc5nsdnICdnINCDnNDIngOIsnSngsaSnainniinniA6nee,R­eeG­RGReelC.leCd.eew(dAen..iomofasaetanpsAn..R..)R.dbtv.t.)btUvtdvd3dER3dtCRtm2.o.roe..reoiuModK2MdnLaKrer2yELeaeMyfnE.eeoMa4.J4iESiCregMreMhJMESJ7SRmoRuss0mel.ess0peel.r..avn.abvn.td.ltAlAsp.leAtfeefteiFe,tavivan1v1er.oKeriolnr,nwaorIlAJdaiaoIlAJiaihpapsta-1Mtiacnlasyiv1a.aalnav1..TaliT,..Te,crlzdrlzeidteaAeetreeorciornl.unlln..eui3enad.LteCiaHL,CicHrlvhlvckobtvcuov.eInoIgEnoeoPEePslpaailgaCdaigiChnaei%ahntPiaaa.tala.¨,lanV.,cnnrl.aesnrtIevIr(vt.n.erno(teollfhanlf,lenllnT,JnlTJEtKocioE.naeKinNaeaaopalmaotam)aao¨.sal..oI.io(Y((Yc(iifceeafslasdectylnldnaoterolW nfEirerpeSAnErSA2or.en2soeunesus.ssen.memm-rmksAnsckcsAfosdEr,IdEIdr0sdur.,c0y.,insa:FZntcysa:oZyG.skG.eetckhememt)cC)1sICleCtr1shIalCihta.ttrtDitDsunT.tushaT.,enuohani,anseo.sIiba.Ii6nmd6etaeM)taZM)ioaaRoAoiaaRmAt(m.sa(eansaaetaiePt.bciwn.fP..cbtvcunnCPnCPmiontemtonttvehksanvsk2enInsCACtAn.ati.a,n2i-a,2rernnslteeSn.ISAessgtImoAtesi0aoimigwgmowhbIol0ebKcIl0CMoKCn..nsCMiC.ni.IdlfzIl2gnop.l0non.,oc,ScoG0rSoGg0roano1Iiy1sgiMIig0R.MM ,Re.GM.2,G.inagis1ptc7anlhptc7rnsl9ndl9IpRnusdPIR0PthrA1,.ihA1uiKnmuiKI)en.mR)UmsRe(Ue8e8tgRoeynag58,repPa8,rePoo2abtPi2cbatPd1Idn..oIn.ns.7Cne7tteeehsa.tteeWfhsnerCW0nesrC0srtru9)iPhrrtf-Phbre.osecee.YostcetYontPvnP0vuecr0heeci(t­eio9(ccrosRetriisrerrihctr.sceesbiM.eeoMet9oMI.sMet9oIser.soimtr,nma9,nh'a.oa'on.nveneeuvKsnuhaKk)snsGakm)hinseGmiteNilmu.NfTl3fav3,tAua,t,nntAua,nalenunpn,onahen,ogrereor9eEre9LElaerLpNlDesnIpNltlDelRaraBTarllgdtar)rilgdt)reititiItatIlaawiew,ialnesiewu,lnnnahnnntEnilaheceEPnei,e.dlnie,e.onecnu,tpnTst,vUpy11TiUhuoaitdnosaiwdsfvwt1fvsWenbtWneaFjeaAatscrjlAaecy0YaloepeYottkspretr,Sab,Sedas(beeainieJanndiecJ-nicdltrooBntrMeoaneaiSaA.Aoa.oYsltA.Aoa.Yluterhrhmodc--mCdd-guCdovaurnnoSGryIninnZnhcInhtZnIontMLoLnnyc,.nkan,.bonMknadanMtoyauo.onsnaeud.aaKrtodaKrIoa.IS.5eePoy5do,Pohtt,ltmstt.mek2-salrs.k2alteebmtpRi,boeRC3iiilBdH.oC3iilBU..U.,itcnni0rl,oxoCp0,l1,oxSoCmeShmroec­teJepo­DeJpdsitdihh0pgSUnTo-IA0e9gSUnalIAoea¨aolsooengat¨smoSnua7rhna7acyeanBe15s9e.yAd9e9e.ryAs(deon.rebs(nISurboScn8rec8nprrtDEtauaE.vsseReau9a.vseRKVf(VieopAfci,eo.Ac,(.oP(EMdadsMdsdreteTts)Mdnsdrene-EtMneraEeroJr6ob.rJr6,rcorucoea,htrlnvs,eltgunsFseeOtgoJiAxgtosuJ3iAixlehEtu3oaie7hnHroon.droe6e.ou6aaenunaenlem)s,lnec)pl,­ryRnipp­nIi­dnlSdn2.sSor2.slteredpyat.Sedaae*.PcIsoPecI.tE7udei.,o7gIte3Ai,s1,gI.tevA1v.tg).2nernogwpssd2neSor.stiSc-r0.c-0alC4v2a­utlCa2a­hmfaTeYCoe0ncdYCoeds0nT,yhitdsT2.to.oet.n0asJn06lJme6l.cz1tme.ac1.psiia.IenM)0aein3M)asAo0lDmAo03uIueoDoad3(e6sLd(6LleSuluSn,s&en.6SuqP7&tde,6Sue07dIn0.In.p.eArspb.oe1AaonnRPe4tI7nn.RPdIr7ub.e.sru.aucvF.uovSAG sepAG ntpannCfd0,t-nrCf,mtCerrCeretrie:-etI(oebsoTraesnlborfgecaIe3o.gRcaIe3.MnOl0oRceMlsc.rola.RcrolsRSoessoedtto,slCi1,an,oulCi,amIemItse(noa.ps(cpa.priihtnpeTn-yWao2oyWTdh,22olauxSTl,2lroSlcttticrE2emariomrJaatiJafR0pNe0fo0pPIrNw0hvaPIhoupwnaihoo.eottnGa..prftbGA.r11o1Esre1bteobirwtecbieehckeehketuidWohoMsis66WohotnII66euottnIeCalospwalsswenRR25con(ecnReon)y)ceeeli))ettrsllottiill-s2y.,,,s...i,,,.anT.tnt*smift*s)htsrIgszthgshgt8eayaPeupyee3eem,mprsL2iasm5orestptA5scsoeefheeolit6islrimez­zmtaumeebohee(tdet3efif""eaeeeecovt7ooptluctwertl.offist5osceeessT.e1i5ttkapccciinh0ehF2%zzrytittng89eieeeecsss"")-",1,HP,True
"[10] WT. iJloenye&s, ASo. Tnus,rUpiSnA, S. . Mizzaro, F. Scholer, and M. Sanderson. 2014. Size and Source [9] TKMPer.aocJcha¨.enr2vri3:qeruUldiennIsnd.atenerArdsnCtJaaM.ntiKdoTeinnrkaaga¨lnlICa¨snaiocncnoetfinneors.nei2ssnt0coe0enn2oc.InineCfsIonuirfnmmorTuameltaisaottteniCodSnoGylalseantciedntmi-KoBsnna(-oTsBweOadlIseSeEd)dgv2eaE0lMvu, aa4altuni(oaaOntgcieotomonfb.eIeInRnrt",1,WT,True
2 5,0,,False
REFERENCES,0,,False
"[1] D. Banks, P. Over, and N.-F. Zhang. 1999. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval 1 (May 1999), 7­34. Issue 1-2.",1,TREC,True
"[2] C. Buckley and E. M. Voorhees. 2005. Retrieval System Evaluation. In TREC. Experiment and Evaluation in Information Retrieval, D. K. Harman and E. M. Voorhees (Eds.). MIT Press, Cambridge (MA), USA, 53­78.",1,TREC,True
"[3] S. Bu¨cher, C. L. A. Clarke, P. C. K. Yeung, and I. Soboro. 2007. Reliable Information Retrieval Evaluation with Incomplete and Biased Judgements. In Proc. 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007), W. Kraaij, A. P. de Vries, C. L. A. Clarke, N. Fuhr, and N. Kando (Eds.). ACM Press, New York, USA, 63­70.",0,,False
"[4] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In Proc. 18th International Conference on Information and Knowledge Management (CIKM 2009), D. W.-L. Cheung, I.-Y. Song, W. W. Chu, X. Hu, and J. J. Lin (Eds.). ACM Press, New York, USA, 621­630.",1,ad,True
"[5] N. Ferro and G. Silvello. 2016. A General Linear Mixed Models Approach to Study System Component Eects. In Proc. 39th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2016), R. Perego, F. Sebastiani, J. Aslam, I. Ruthven, and J. Zobel (Eds.). ACM Press, New York, USA, 25­34.",0,,False
"[6] N. Ferro, G. Silvello, H. Keskustalo, A. Pirkola, and K. Ja¨rvelin. 2016. e Twist Measure for IR Evaluation: Taking User's Eort Into Account. Journal of the American Society for Information Science and Technology (JASIST) 67, 3 (2016), 620­648.",0,,False
"[7] D. K. Harman. 2011. Information Retrieval Evaluation. Morgan & Claypool Publishers, USA.",0,,False
"[8] Y. Hochberg and A. C. Tamhane. 1987. Multiple Comparison Procedures. John Wiley & Sons, USA.",0,,False
"[9] K. Ja¨rvelin and J. Keka¨la¨inen. 2002. Cumulated Gain-Based Evaluation of IR Techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (October 2002), 422­446.",0,,False
"[10] T. Jones, A. Turpin, S. Mizzaro, F. Scholer, and M. Sanderson. 2014. Size and Source Maer: Understanding Inconsistencies in Test Collection-Based Evaluation. In Proc. 23rd International Conference on Information and Knowledge Management",0,,False
1 5,0,,False
"the model applied to the four sub-corpora. Both the topic and the the interaction between sub-corpora and systems is not signi cant. (27.13%) systems being in the top-group. So, the impact of using",1,corpora,True
"2002), 422­446.",0,,False
sub-collections,0,,False
hsasybseteenmto,0,,False
edecerecatsse,0,,False
"tahreereslaigtivneisizceaonf tthaentodpilcarge,",0,,False
"the system e ect is about [10] T. Jones, A. Turpin, S. Mizzaro, F. Scholer, and M. Maer: Understanding Inconsistencies in Test",0,,False
e Tukey HSD Sanderson. 2014. Size and Source,0,,False
Collection-Based Evaluation. In,0,,False
test,0,,False
reports,0,,False
that,0,,False
"1,993",0,,False
out,0,,False
of,0,,False
"8,256",0,,False
(24.14%),0,,False
possible,0,,False
2 5,0,,False
the,0,,False
size,0,,False
of,0,,False
the,0,,False
topic,0,,False
e,0,,False
ect.,0,,False
e Tukey HPrSocD. 23rtdeInstetrniantiodnailcCaontfeeresncte honaIntformation ansdyKnsotwelemdge pMaaniargesmeanrt e signi cantly di erent with 71 out of 129 (55.04%),1,HP,True
"1,872 out of 8,256 (22.67%) possible system pairs are signi cantly",0,,False
systems being in the top-group; this is coherent with the reduction,0,,False
di erent with 64 out of 129 (49.61%) systems being in the top-group. Measuring on sub-corpora tends to decrease the size of the sys-,1,corpora,True
"of the MSerror term which, being the other factors constant, makes the |t | statistics in the Tukey HSD test bigger, thus detecting more",0,,False
tem e ect relative to the topic e ect. More pairs of signi cantly,0,,False
signi cant di erences.,0,,False
di erent systems were found with fewer in the top group.,0,,False
Figure 2(e) shows the main e ects plot for the sub-corpus e ect:,0,,False
Figure 2(d) plots the AP marginal mean of systems on the whole,1,AP,True
sub-corpora a ect system e ectiveness. Figure 2(f) plots the in-,1,corpora,True
TIPSTER collection (black dashed line) and on the sub-corpora (red,1,corpora,True
teraction e ects for the sub-corpus*system e ect where each line,0,,False
solid line) together with their con dence intervals (shaded). e AP,1,ad,True
"represents a di erent system. Even if, in the case of AP, the ef-",1,AP,True
"values of systems change, but system ranking is not too dissimilar,",0,,False
"fect is not signi cant, we can note how sub-corpora a ect systems",1,corpora,True
"as suggested by the Kendall's correlation  , 0.8238. We can see",0,,False
"di erently. For example, the general trend is that systems have",0,,False
"how the use of sub-corpora makes the con dence intervals smaller,",1,corpora,True
"lower e ectiveness on the TIPFR sub-corpus, even if a few systems",0,,False
"suggesting more accuracy, as supported also by the outcomes of",0,,False
"behave the opposite way; similarly, TIPFT is the sub-corpus that",0,,False
the Tukey HSD test.,0,,False
results in highest e ectiveness but with some exceptions.,0,,False
Figure 2(c) shows eq (2) applied to the four sub-corpora. e SS of,1,corpora,True
the topic and system e ects is the same as in the case of Figure 2(b),0,,False
903,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Track T07,1,Track,True
^,0,,False
2 Topic,0,,False
^,0,,False
2 System,0,,False
^,0,,False
2 Sub-Corpus,0,,False
^,0,,False
2 Sub-Corpus*System,0,,False
AP 0.4065 (0.00) 0.1639 (0.00) 0.0075 (0.00),1,AP,True
­ (0.43) 0.9041,0,,False
P@10 0.2692 (0.00) 0.1050 (0.00) 0.0838 (0.00),0,,False
­ (1.00) 0.7746,0,,False
R-prec 0.3327 (0.00) 0.1319 (0.00) 0.0181 (0.00),0,,False
­ (0.53) 0.8591,0,,False
RBP 0.2836 (0.00) 0.1151 (0.00) 0.0878 (0.00),0,,False
­ (1.00) 0.8062,0,,False
nDCG 0.4013 (0.00) 0.2625 (0.00) 0.0048 (0.00) 0.0230 (0.00),0,,False
0.8991,0,,False
nDCG@20 0.3353 (0.00) 0.1624 (0.00) 0.0087 (0.00) 0.0112 (0.00),0,,False
0.7164,0,,False
ERR 0.2549 (0.00) 0.1155 (0.00) 0.0844 (0.00),0,,False
­ (0.87) 0.7518,0,,False
Twist 0.3192 (0.00) 0.1500 (0.00) 0.0207 (0.00),0,,False
­ (0.42) 0.8386,0,,False
Track T08,1,Track,True
^,0,,False
2 Topic,0,,False
^,0,,False
2 System,0,,False
^,0,,False
2 Sub-Corpus,0,,False
^,0,,False
2 Sub-Corpus*System,0,,False
AP 0.3870 (0.00) 0.1675 (0.00) 0.0682 (0.00),1,AP,True
­ (0.71) 0.8238,0,,False
P@10 0.2220 (0.00) 0.1162 (0.00) 0.1310 (0.00),0,,False
­ (0.74) 0.7229,0,,False
R-prec 0.2410 (0.00) 0.1232 (0.00) 0.0650 (0.00),0,,False
­ (0.75) 0.7604,0,,False
RBP 0.2316 (0.00) 0.1335 (0.00) 0.1631 (0.00),0,,False
­ (0.18) 0.7682,0,,False
nDCG 0.4429 (0.00) 0.3207 (0.00) 0.0491 (0.00) 0.0141 (0.00),0,,False
0.8162,0,,False
nDCG@20 0.4324 (0.00) 0.2135 (0.00) 0.0498 (0.00),0,,False
­ (0.22) 0.6696,0,,False
ERR 0.2044 (0.00) 0.1417 (0.00) 0.1710 (0.00) 0.0065 (0.04),0,,False
0.6887,0,,False
Twist 0.2045 (0.00) 0.1515 (0.00) 0.0964 (0.00),0,,False
­ (0.21) 0.7772,0,,False
"Table 1: E ect size (^ 2 SoA) and p-value for eq (2). Insigni cant e ects are in gray; small e ects, light blue; medium, blue; and large, dark blue. e  reports system ranking correlation when using the whole collection and sub-corpora.",1,corpora,True
3.2 RQ2 ­ Sub-corpora & evaluation measures,1,corpora,True
Table 1 shows eq (2) applied to the four sub-corpora for T07 and T08 for all evaluation measures. e topic e ect is signi cant and large in all cases while the system e ect is a signi cant medium size e ect in about half of the cases and large in the other half.,1,corpora,True
"e sub-corpora are always a signi cant e ect with small or medium size, except for RBP and ERR on T08 for which it is a large size. On T07, the sub-corpus e ect is always smaller than the system e ect, on T08 the sub-corpus e ect is bigger than the system e ect for P@10, RBP, and ERR. e sub-corpus*system interaction e ect is generally not signi cant, with the exception of nDCG and nDCG@20 on T07 and nDCG and ERR on T08 for which it is signi cant though small.",1,corpora,True
"Table 1 shows the Kendall's  correlations between the rankings of systems using eq (1) on the whole TIPSTER collection and eq (2) on the four sub-corpora. e rankings are generally correlated, indicating a good agreement between the two approaches, even if there are some cases where correlation drops, namely P@10, nDCG@20, and ERR, on T08 and nDCG@20 on T07.",1,corpora,True
4 CONCLUSION AND FUTURE WORK,0,,False
"We nd that sub-corpora are a signi cant e ect on system e ectiveness. While past work has indicated such an e ect exists, to the best of our knowledge, this is the rst time such an e ect has been integrated into a e ectiveness model and e ect sizes compared to other known factors. We nd that di erent evaluation measures are affected in di erent ways by sub-corpora, which may impact on what systems are considered signi cantly di erent to each other. We found that ranking systems using sub-corpora reasonably agrees with ranking systems with respect to a whole collection but using the information about sub-corpora allows a more accurate estimation of which systems are signi cantly di erent.",1,corpora,True
is is initial work. We recognize that the number of topics in our collections is small. We next plan to understand the impact of di erent kinds of sub-corpora. We also plan to extend the present methodology to study the impact of di erent collections on system performance rather than sub-corpora within one collection.,1,corpora,True
5 ACKNOWLEDGMENTS,0,,False
is work is supported in part by the Australian Research Council's,0,,False
Discovery Projects scheme (DP130104007) and a Google Faculty,0,,False
Award.,0,,False
REFERENCES,0,,False
"[1] D. Banks, P. Over, and N.-F. Zhang. 1999. Blind Men and Elephants: Six Approaches to TREC data. Information Retrieval 1, 1-2, 7­34.",1,TREC,True
"[2] O. Chapelle, D. Metzler, Y. Zhang, and P. Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In CIKM 2009. 621­630.",1,ad,True
[3] N. Ferro and G. Silvello. 2016. A General Linear Mixed Models Approach to Study System Component E ects. In SIGIR 2016. 25­34.,0,,False
"[4] N. Ferro, G. Silvello, H. Keskustalo, A. Pirkola, and K. Ja¨rvelin. 2016. e Twist Measure for IR Evaluation: Taking User's E ort Into Account. JASIST 67, 3, 620­648.",0,,False
"[5] Y. Hochberg and A. C. Tamhane. 1987. Multiple Comparison Procedures. John Wiley & Sons, USA.",0,,False
"[6] K. Ja¨rvelin and J. Keka¨la¨inen. 2002. Cumulated Gain-Based Evaluation of IR Techniques. ACM TOIS 20, 4, 422­446.",0,,False
"[7] T. Jones, A. Turpin, S. Mizzaro, F. Scholer, and M. Sanderson. 2014. Size and Source Ma er: Understanding Inconsistencies in Test Collection-Based Evaluation. In CIKM 2014. 1843­1846.",0,,False
"[8] A. Mo at and J. Zobel. 2008. Rank-biased Precision for Measurement of Retrieval E ectiveness. ACM TOIS 27, 1, 2:1­2:27.",0,,False
"[9] S. Olejnik and J. Algina. 2003. Generalized Eta and Omega Squared Statistics: Measures of E ect Size for Some Common Research Designs. Psychological Methods 8, 4, 434­447.",0,,False
[10] S. E. Robertson and E. Kanoulas. 2012. On Per-topic Variance in IR Evaluation. In SIGIR 2012. 891­900.,0,,False
"[11] A. Rutherford. 2011. ANOVA and ANCOVA. A GLM Approach (2nd ed.). John Wiley & Sons, New York, USA.",1,LM,True
"[12] T. Sakai. 2014. Statistical Reform in Information Retrieval? SIGIR Forum 48, 1, 3­12.",0,,False
"[13] M. Sanderson, A. Turpin, Y. Zhang, and F. Scholer. 2012. Di erences in E ectiveness Across Sub-collections. In CIKM 2012. 1965­1969.",0,,False
[14] J. M. Tague-Sutcli e and J. Blustein. 1994. A Statistical Analysis of the TREC-3 Data. In TREC-3. 385­398.,1,TREC,True
904,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Automatic and Semi-Automatic Document Selection for Technology-Assisted Review,0,,False
Maura R. Grossman,0,,False
University of Waterloo,0,,False
Gordon V. Cormack,0,,False
University of Waterloo,0,,False
Adam Roegiest,0,,False
Kira Systems Inc.,0,,False
ABSTRACT,0,,False
"In the TREC Total Recall Track (2015-2016), participating teams could employ either fully automatic or human-assisted (""semi-automatic"") methods to select documents for relevance assessment by a simulated human reviewer. According to the TREC 2016 evaluation, the fully automatic baseline method achieved a recall-precision breakeven (""R-precision"") score of 0.71, while the two semi-automatic efforts achieved scores of 0.67 and 0.51. In this work, we investigate the extent to which the observed effectiveness of the different methods may be confounded by chance, by inconsistent adherence to the Track guidelines, by selection bias in the evaluation method, or by discordant relevance assessments. We find no evidence that any of these factors could yield relative effectiveness scores inconsistent with the official TREC 2016 ranking.",1,TREC,True
"ACM Reference format: Maura R. Grossman, Gordon V. Cormack, and Adam Roegiest. 2017. Automatic and Semi-Automatic Document Selection for Technology-Assisted Review. In Proceedings of SIGIR '17, Shinjuku, Tokyo, Japan, August 07-11, 2017, 4 pages. http://dx.doi.org/10.1145/3077136.3080675",0,,False
1 INTRODUCTION,1,DUC,True
"The purpose of the TREC Total Recall Track [1, 5] was to evaluate, through controlled simulation, technology-assisted review (""TAR"") methods to achieve very high recall with a human-in-the-loop. Towards this end, the Track provided a web-based server that simulated a human-in-the-loop, by providing (pre-coded) relevance assessments, on a documentby-document basis, in response to requests from participating teams during their completion of the task. The objective of each team was to request assessments for as many relevant documents as possible, while requesting assessments for as few non-relevant documents as possible.",1,TREC,True
"Participants' methods were evaluated using the traditional set-based measures of recall and precision, as well as gain curves and a novel family of rank-based measures denoted by the Track coordinators as ""recall@aR+b."" For brevity, we limit our consideration to the special case of ""recall@R,"" which is equivalent to the recall-precision breakeven point, or R-precision. The recall-precision breakeven point is the",1,ad,True
"Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-5022-8/17/08. http://dx.doi.org/10.1145/3077136.3080675",1,ad,True
"proportion of documents for which relevant assessments are returned among the first R requests, where R is the number of relevant documents in the collection.",0,,False
"At the outset, participants retrieved the document collection from the assessment server, as well as a short topic description. Each run was declared by the participant to be either ""Automatic"" or ""Manual."" Automatic runs were permitted no manual intervention whatsoever; the only information available to an Automatic run was the collection, the topic statement, and the results of any assessments requested from the server. Manual runs were permitted unrestricted manual intervention, including, but not limited to, independent research, search of the collection, and human review of documents in the collection. As stated in the Track guidelines, ""[i]f documents are manually reviewed, the same documents must also be submitted to the assessment server, at the time they are reviewed.""1 One of the two Manual runs submitted to the TREC 2016 Total Recall Track conformed to this requirement; the other did not, in effect availing itself of pre-training not available to the other runs.",1,Track,True
"The pre-coded assessments that were used to simulate human feedback, as well as to evaluate the participating runs, were derived using a process similar to a Manual run, but with real human assessors-in-the-loop. Interactive search and judging [6], as well as two machine-learning methods, were used to identify potentially relevant documents, which were labeled as ""relevant"" or ""non-relevant,"" by a team of six assessors supervised by the National Institute of Standards and Technology (""NIST""). In total, the assessors reviewed 61,985 documents for relevance to 34 different topics, labeling 36,021 as ""relevant"" and 25,964 as ""non-relevant."" For the purposes of feedback and evaluation, the 36,021 documents were deemed ""relevant""; all others, whether reviewed or not, were deemed ""non-relevant.""",0,,False
"To provide an evaluation standard independent of the primary assessments, a non-uniform statistical sample [2] of 50 documents was drawn from the entire collection for each of the 34 topics; each sample was reviewed by three separate assessors from the same NIST team. The Track coordinators reported separate gain curves using each of these three alternate assessments, as well the majority vote among the three assessments, as a gold standard [1].",1,Track,True
"Figures 1 and 2, reproduce the gain curves from the TREC 2016 proceedings [1] for the three runs of interest, evaluated using the primary assessments and the majority vote of the three alternate assessments, respectively. The runs of interest for this study are BMI-Desc (the Automatic baseline that achieved recall@R of 0.71), eDiscoveryTeam (the Manual",1,TREC,True
1 http://cormack.uwaterloo.ca/total-recall/2016/guidelines.html.,0,,False
905,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
1.0,0,,False
1.0,0,,False
0.8,0,,False
0.8,0,,False
0.6,0,,False
0.6,0,,False
Recall,0,,False
Recall,0,,False
0.4,0,,False
0.4,0,,False
0.2 0.0,0,,False
0,0,,False
5000,0,,False
10000,0,,False
15000 Effort,0,,False
BMI-Desc catres-manual1 eDiscoveryTeam-Run1,0,,False
20000,0,,False
25000,0,,False
30000,0,,False
"Figure 1: Gain Curves Showing Recall (Averaged Over 34 Topics) as a Function of the Number of Documents Submitted, for the Athome4 (Jeb Bush) Test Collection.",1,Athome,True
1.0,0,,False
0.2 0.0,0,,False
0,0,,False
5000,0,,False
10000,0,,False
15000 Effort,0,,False
BMI-Desc catres-manual1 eDiscoveryTeam-Run1,0,,False
20000,0,,False
25000,0,,False
30000,0,,False
"Figure 3: Gain Curves Showing Recall of Documents That Were Unjudged in the Primary Assessment, According to the Majority Vote of the Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Documents Submitted, for the Athome4 (Jeb Bush) Test Collection.",1,Athome,True
0.8,0,,False
0.6,0,,False
Recall,0,,False
0.4,0,,False
0.2 0.0,0,,False
0,0,,False
5000,0,,False
10000,0,,False
15000 Effort,0,,False
BMI-Desc catres-manual1 eDiscoveryTeam-Run1,0,,False
20000,0,,False
25000,0,,False
30000,0,,False
"Figure 2: Gain Curves Showing Recall According to the Majority Vote of the Three Secondary Assessors (Averaged Over 34 Topics) as a Function of the Number of Documents Submitted, for the Athome4 (Jeb Bush) Test Collection.",1,Athome,True
"run that achieved recall@R of 0.67), and catres (the Manual run that achieved recall@R of 0.51). A paired t-test indicates that the 95% confidence interval of the difference between the BMI-Desc score and the e-DiscoveryTeam score is between -0.012 and +0.095; in other words, the difference is neither large nor significant. In contrast, the differences between these two scores and the catres score are both significant (  0.0001).",0,,False
"Taken at face value, these results might suggest that there is little to choose between the the Automatic baseline method and one Manual method, and that both are superior to the second Manual method. Such a conclusion, we argue, would be premature without first considering the confounding factors that are investigated in this work. One such factor has already been noted ­ the eDiscoveryTeam run was primed with the result of assessing more than one hundred documents from",1,ad,True
"the test collection, on average, per topic . For many topics, their ""recall@R"" results were derived from assessments of many more than R documents [3]. Another factor, suggested by the catres team [4], was that the catres run returned disproportionately fewer documents that had been reviewed by the NIST assessors, raising the question of whether or not these documents might have been coded as ""relevant"" had they been reviewed, thereby raising catres' score. Finally, discordant relevance assessments may have enured to the benefit of one run, at the expense of others.",1,ad,True
2 EQUALIZING PRIOR REVIEW,0,,False
We know of no way to exclude or to account for the effect,0,,False
"of the prior review for the eDiscoveryTeam runs. However,",0,,False
we were able to devise a way to afford the other runs an,0,,False
advantage equal in magnitude to the prior review conducted,1,ad,True
"by eDiscoveryTeam. Consider, for example Topic 434. eDis-",0,,False
coveryTeam reported having reviewed 83 documents for this,0,,False
"topic, and requested labels for only the 38 documents they",0,,False
"believed to be relevant, of which 33 were deemed relevant",0,,False
"by the simulated human assessor. Coincidentally, the offi-",0,,False
"cial value of  was also 38. Accordingly, this run scored",0,,False
recall@R,0,,False
",",0,,False
33 38,0,,False
",",0,,False
0.87.,0,,False
According,0,,False
to,0,,False
the,0,,False
Track,1,Track,True
"guidelines,",0,,False
we,0,,False
posit that the score should more properly be interpreted as,0,,False
"recall@83,""0.87. In contrast, if we consider the first 83 docu-""",0,,False
ments,0,,False
in,0,,False
the,0,,False
catres,0,,False
"run,",0,,False
we,0,,False
find,0,,False
that,0,,False
"recall@83,",0,,False
37 38,0,,False
",",0,,False
0.97.,0,,False
"The Automatic baseline method (""BMI-Desc"") achieved an",0,,False
"identical recall@83,0.97.",0,,False
"More generally, let us denote by  the number of docu-",0,,False
ments reviewed prior to a Manual run. Some smaller number,0,,False
"   of these documents will be deemed ""relevant"" and, we",0,,False
"assume, submitted to the assessment server. In general, it will",0,,False
"be the case that   , and the next  -  documents will",0,,False
"be considered in computing recall@R, when in fact, R+H-h",0,,False
906,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Run BMI-Desc eDiscoveryTeam,0,,False
catres,0,,False
Recall 0.79 0.67 0.61,0,,False
Std. Dev. 0.12 0.21 0.22,0,,False
p (vs. next) 0.0004 0.05,0,,False
"Table 1: Prior-Review Adjusted Recall@R+H-h, Averaged Over 34 Topics.",0,,False
Run eDiscoveryTeam,0,,False
BMI-Desc catres,0,,False
Recall 0.67 0.63 0.51,0,,False
Std. Dev. 0.34 0.34 0.34,0,,False
p (vs. Next) 0.4 0.01,0,,False
"Table 3: Recall@R as Evaluated by the Majority Vote of the Three Alternate Assessments, Averaged Over 34 Topics.",0,,False
Run BMI-Desc eDiscoveryTeam,0,,False
catres,0,,False
Judged 0.88 0.88 0.59,0,,False
Precision 0.80 0.83 0.80,0,,False
Std. Dev. 0.17 0.17 0.13,0,,False
Table 2: Precision of the Top R Results That Were Judged by NIST Assessors. None of the differences are significant ( > 0.05).,0,,False
Precision Std. Dev. p (vs. Next),0,,False
Judged 0.79,0,,False
0.22,0,,False
0.0001,0,,False
Unjudged 0.10,0,,False
0.18,0,,False
"Table 4: catres' Precision Among Judged and Unjudged Documents, According to a Sample Reviewed by the Second Author.",0,,False
"documents were assessed. We should compare any runs that received a ""head start"" using the measure recall@ +  - , where  is the number of relevant documents among the first  submitted. In both cases, we are replacing the threshold value of  by a somewhat greater value, affording a controlled comparison, at the expense of altering somewhat the objective function.",1,ad,True
"Table 1 shows the average recall@R+H-h over 34 topics, as well as the standard deviation, and p-values versus the next-ranked run. Recall of both BMI-Desc and catres increase substantially, from 0.51 and 0.71, to 0.61 to 0.79, respectively, when given the benefit of  documents of prior review. With this adjustment, we see that BMI-Desc achieves substantially and significantly higher recall than eDiscoveryTeam, for the same number of documents reviewed, according to the NIST primary assessments. eDiscoveryTeam also achieves significantly higher recall than catres, but the margin is substantially reduced.",1,ad,True
3 SELECTION BIAS,0,,False
"In the first R documents, BMI-Desc and eDiscoveryTeam submitted a substantially higher proportion of documents that had been judged by the NIST assessors, as compared to catres. Table 2 shows the proportion of judged documents for each run, and the precision ­ the proportion of relevant documents ­ among that set. It is clear from this table that the catres run achieves similar precision on the judged documents that it returns, but returns fewer judged documents overall, and hence achieves a lower recall@R score. If a substantial number of the unjudged documents were in fact relevant, the catres result would be under-reported.",1,ad,True
"Our first avenue of investigation was to consider the secondary assessments reported by the Track coordinators. If there were a substantial number of relevant unassessed documents returned by some runs and not others, this effect should manifest itself in the results reported with respect to these alternate assessments. Figure 2 shows the gain curve",1,Track,True
"achieved by the three runs, when evaluated by the majority vote of the three alternate assessors. We see very little difference compared to Figure 1, which is calculated with respect to the primary NIST assessments. It may be the case that eDiscoveryTeam's curve is closer to BMI-Desc, but the overall ordering remains intact.",0,,False
"To isolate the effect of unjudged documents, we obtained the raw scoring data from NIST, and calculated the recall of each run, as assessed by the majority vote of the three alternate assessors, considering only documents that were unjudged in the primary assessment. The result is shown as a gain curve in Figure 3. We had no prior hypothesis regarding what this gain curve would show. Somewhat to our surprise, we observed that the relative heights of the curves for the three runs were the same ­ there is no indication that the catres system was better at finding relevant, unjudged, documents than the others.",1,ad,True
"We also computed recall@R based on the majority-vote alternate assessments, the results of which are shown in Table 3. The results are generally consistent with those for the primary assessments, with larger variances, as expected. In this evaluation, eDiscoveryTeam achieves a higher score, but, as for the primary assessments, the difference is not significant. catres achieves lower recall, by a significant margin.",0,,False
"While the sampling and alternative assessments were independent of the primary method, the small sample size could miss small but important populations of unjudged relevant documents. For example, for Topic 434, an additional 38 relevant documents, over and above the 38 that were found by NIST, could escape sampling if they were dissimilar to the judged-relevant documents. We would expect, however, over 34 topics, that if this were a systematic issue, at least some such relevant documents would have come to light.",1,ad,True
"To avoid reliance on small samples, we sought further, direct confirmation or refutation of the hypothesis that the catres run included a substantial number of relevant unjudged documents. Towards this end, we reviewed a stratified random sample of ten documents for each topic: five judged",0,,False
907,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Method BMI-Desc BMI-eDT eDiscoveryTeam,0,,False
catres,0,,False
Recall 0.74 0.79 0.73 0.62,0,,False
Std. Dev. 0.17 0.14 0.22 0.22,0,,False
p (vs. eDT) 0.8 0.08 -,0,,False
0.002,0,,False
"Table 5: Recall@R+H-h, Averaged Over 34 Topics, Evaluated According to the ""Corrected"" Gold Standard. BMI-Desc is the official TREC baseline run, trained on feedback from the NIST gold standard; BMI-eDT is the same method, trained on feedback from the ""corrected"" gold standard.",1,TREC,True
"documents that were among the first R documents returned by catres, and five unjudged documents that were among the first R documents returned by catres. The review was conducted blind by the second author, who was familiar with the subject matter.2 The results, shown in Table 4 are consistent with the results in Table 2 with regard to precision among judged documents (0.80 vs. 0.79), and show eight times lower precision (0.10) among unjudged documents, leading us to conclude that unjudged relevant documents were not a major factor in the TREC 2016 Total Recall evaluation results.",1,ad,True
4 ASSESSOR DISCORD,0,,False
"Since the earliest days of IR evaluation, disputes over rele-",0,,False
vance have raised concerns [7]. The eDiscoveryTeam report,0,,False
"[3] suggested that the primary NIST assessments were flawed,",0,,False
"and that, instead, their results should be evaluated according",1,ad,True
"to their own ""corrected"" gold standard.",0,,False
Confusion matrices reported by eDiscoveryTeam suggest,0,,False
that the magnitude of discord between its assessments and,0,,False
the primary NIST assessments is well within the bounds of,0,,False
what would be expected for independent assessments [8]. Con-,0,,False
"sidering our running example of Topic 434, eDiscoveryTeam",0,,False
"reports that, according to their ""corrected"" gold standard,",0,,False
the NIST assessments contain five false positives and five,0,,False
"false negatives. In other words, the overlap (i.e., Jaccard co-",0,,False
efficient),0,,False
between,0,,False
the,0,,False
two,0,,False
gold,0,,False
standards,0,,False
is,0,,False
33 43,0,,False
",",0,,False
0.77,0,,False
­,0,,False
much,0,,False
higher than the overlap values reported in the literature for,0,,False
"informed expert assessors, which have not proven to affect",0,,False
the reliability of ad hoc system evaluation [8].,1,ad,True
"eDiscoveryTeam provided us with a copy of its ""corrected"" gold standard.3 The average per-topic overlap between the",0,,False
eDiscoveryTeam and primary NIST assessments was calcu-,0,,False
lated to be 0.75. Table 5 shows recall@R+H-r for the three,0,,False
"methods, evaluated by the eDiscoveryTeam's ""corrected"" gold",0,,False
standard. Even when BMI-Desc is trained using the pri-,0,,False
"mary NIST assessments, it achieves recall comparable to",0,,False
"2The 340 documents and their corresponding assessments are available from the Authors. 3Whether or not eDiscoveryTeam's relevance assessments are more ""correct"" than the primary NIST assessments is a subjective question that is beyond the scope of this work, and has no bearing on our results.",0,,False
"the eDiscoveryTeam run. When trained and evaluated using eDiscoveryTeam's ""corrected"" assessments, BMI-Desc achieves exactly the same recall ­ 0.79 ­ as when trained and evaluated using the primary NIST assessments. When trained on the primary NIST assessments and evaluated with respect to the eDiscoveryTeam assessments, BMI-Desc yields slightly, but not significantly, higher recall than the eDiscoveryTeam submission: 0.74 vs. 0.73. catres yields insubstantially different results, whether evaluated with respect to the eDiscoveryTeam ""corrected"" assessments or the primary NIST assessments: 0.62 vs. 0.61.",0,,False
"The impact of assessor discord on the reliability of Total Recall system evaluation has not previously been studied; the results above, and the agreement between Figures 1 and 2, suggest that, as with ad hoc retrieval, reliable evaluation of the relative effectiveness of Total Recall systems does not hinge on precise relevance assessments, either for feedback or for evaluation.",1,ad,True
5 CONCLUSIONS,0,,False
"Prior to the TREC Total Recall Track, we assumed that the best Manual runs would substantially outperform the best Automatic runs, as they did in previous TREC ad hoc tasks. We were surprised that they did not. For some topics, Manual runs achieved higher recall scores with less effort (discounting prior review), but no Manual method consistently improved on the fully Automatic TREC baseline method. In this study, we sought to determine whether the apparent superiority of the Automatic baseline method at TREC 2016 was real, or attributable to chance, failure to follow the Track guidelines, selection bias, or assessor discord. We found no evidence to suggest that the Manual runs were superior to the Automatic baseline, and we found evidence to suggest that, when review effort was properly controlled, the Automatic baseline method found more relevant documents with less effort than any Manual run in the TREC 2016 Total Recall Track.",1,TREC,True
"Based on all currently available evidence, the TREC Automatic baseline method remains the method to beat.",1,TREC,True
REFERENCES,0,,False
"[1] M. R. Grossman, G. V. Cormack, and A. Roegiest. TREC 2016 Total Recall Track Overview. In TREC 2016.",1,TREC,True
"[2] D. G. Horvitz and D. J. Thompson. A generalization of sampling without replacement from a finite universe. Journal of the American Statistical Association, 47(260):663­685, 1952.",0,,False
"[3] R. C. Losey, J. Sullivan, T. Reichenberger, L. Kuehn, and J. Grant. e-Discovery Team at TREC 2016 Total Recall Track. In TREC 2016.",1,TREC,True
"[4] J. Pickens, T. Gricks, B. Hardi, M. Noel, and J. Tredennick. An exploration of Total Recall with multiple manual seedings. In TREC 2016.",1,TREC,True
"[5] A. Roegiest, G. V. Cormack, M. R. Grossman, and C. L. A. Clarke. TREC 2015 Total Recall Track Overview. In TREC 2015.",1,TREC,True
[6] M. Sanderson and H. Joho. Forming test collections with no system pooling. In SIGIR 2004.,0,,False
[7] T. Saracevic. Why is relevance still the basic notion in information science? In ISI 2015.,0,,False
"[8] E. M. Voorhees. Variations in relevance judgments and the measurement of retrieval effectiveness. Informantion Processing & Management, 36(5), 2000.",0,,False
908,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Neural Network based Reinforcement Learning for Real-time Pushing on Text Stream,0,,False
Haihui Tan,0,,False
The Hong Kong Polytechnic University,0,,False
"Kowloon, Hong Kong tanhaihui92@gmail.com",0,,False
Ziyu Lu,0,,False
The Hong Kong Polytechnic University,0,,False
"Kowloon, Hong Kong luziyuhku@gmail.com",0,,False
Wenjie Li,0,,False
The Hong Kong Polytechnic University,0,,False
"Kowloon, Hong Kong cswjli@comp.polyu.edu.hk",0,,False
ABSTRACT,0,,False
"The massive amount of noisy and redundant information in text streams makes it a challenge for users to acquire timely and relevant information in social media. Real-time noti cation pushing on text stream is of practical importance. In this paper, we formulate the real-time pushing on text stream as a sequential decision making problem and propose a Neural Network based Reinforcement Learning (NNRL) algorithm for real-time decision making, e.g., push or skip the incoming text, with considering both history dependencies and future uncertainty. A novel Q-Network which contains a Long Short Term Memory (LSTM) layer and three fully connected neural network layers is designed to maximize the long-term rewards. Experiment results on the real data from TREC 2016 Real-time Summarization track show that our algorithm signi cantly outperforms state-of-the-art methods.",1,TREC,True
CCS CONCEPTS,0,,False
· Information systems  Document ltering;,0,,False
KEYWORDS,0,,False
"Text Stream, Real-Time Pushing, Deep Reinforcement Learning",0,,False
1 INTRODUCTION,1,DUC,True
"With the development of social media, text streams such as Twitter posts, news articles or user reviews are being generated in fastgrowing volumes. The explosive amount of noisy and redundant streaming texts are overwhelming and makes it di cult to nd useful information. Pushing or Summarizing the real-time streaming text for users is of practical importance. In recent decades, the real-time pushing on text streams has attracted increasing attention at Text Retrieve Conferences (TREC), e.g., the Microblog track in 2015 [4] and the Real-time Summarization track in 2016 [6]. For example, [9] which has achieved the best performance for the real-time ltering task at TREC 2015 Microblog track, explored dynamic emission strategies to maintain appropriate thresholds for pushing relevant tweets. Fan et al. [2] proposed an adaptive evolutionary ltering framework to lter interesting tweets from",1,Twitter,True
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. ISBN 978-1-4503-5022-8/17/08. . . $15.00. DOI: http://dx.doi.org/10.1145/3077136.3080677",1,ad,True
"the Twitter stream with respect to user interest pro les. However, pushing real-time text streams is not a short-term process. It is a dynamic forward decision process in which the current action will a ect further decisions (dependency) and further streaming texts generate uncertainty on the current decision. [5] treated real-time stream summarization as a sequential decision making problem and adopted a locally optimal learning to search algorithm to learn a policy for selecting or skipping a sentence in the text stream, by imitating a heuristic reference policy. However, the heuristic reference policy is di cult to construct for real-time streaming text and it needs massive human interventions. The real-time decision process needs to maximize the long-term rewards by taking both history dependencies and future uncertainty into consideration.",1,Twitter,True
"In this paper, we de ne the real-time pushing on text stream as a long-term optimization problem and propose a neural network based reinforcement learning algorithm as the solution. A novel Q-Network is de ned to learn a long-term policy for sequential decision making. When receiving text streams, the Q-network predicts the values for taking each action under the current state and chooses the action with the higher value. A long short-term memory (LSTM) layer is integrated into the Q-Network to represent the high-level abstraction of streaming text by considering both semantic and temporal dependencies of previously pushed texts. The Q-Network is continuously updated according to the observation of interrelationship between actions and rewards, and a long-term policy is explored and exploited to make real-time decisions on text stream. The main contributions of our paper is as follows:",0,,False
"· We formulate real-time pushing on text streams as a longterm optimization problem, by considering both history dependencies and future uncertainty of text stream.",0,,False
· A Neural Network based Reinforcement learning (NNRL) algorithm is proposed to maximize long-term rewards and obtain the long-term policy for real-time decision making.,0,,False
· Experimental evaluations on the real tweet stream show that our method has superior performance over the stateof-the-art methods.,0,,False
2 METHOD,0,,False
"Problem De nition: Given a text stream S ,"" {t1, t2, · · · , tn, · · · }, Our model makes real-time decisions from an action set A "","" {ap , as } for each incoming text t. ap is to push t while as is to skip it. As our target is to maximize the long-term rewards, we adopt a reinforcement learning framework to de ne and solve this problem.""",1,ad,True
"In reinforcement learning methods, the agent model selects an action from the action set A and passes it to the environment  which changes its inner state s and the reward score r . Given the",0,,False
913,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"state st ,"" [x1, a1, x2, a2, · · · , at -1, xt ] which is a sequence of observations and actions a, the model arrives at an optimal action-value function Q(s, a) which is the maximum expected return with a""",0,,False
policy  .,0,,False
2.1 Neural Network-based Reinforcement Learning (NNRL),0,,False
"In our problem, x is the feature representation of a text t, the action set A ,"" {ap , as }. Similar to Deep Q-Network [8], we propose a""",0,,False
neural network-based reinforcement learning (NNRL) algorithm,0,,False
"where a neural network approximation function as Q-Network is used to estimate the action-value function Q(s, a,  ) ,"" Q(s, a).  is""",0,,False
the weight parameters in Q-Network. The Q-Network is trained by minimizing the loss function L( ) as Equation 1.,0,,False
"L( ) ,"" Es,a(s,a)[ - Q(s, a;  )]2""",0,,False
(1),0,,False
"in which ,"" Es [r + maxa Q(s , a ;  )] is the target and (s, a) is the behavior distribution over s and a.""",0,,False
Algorithm 1 outlines our proposed neural network-based rein-,0,,False
forcement learning (NNRL) algorithm. N is the number of iterations,0,,False
Algorithm 1 N,0,,False
N,0,,False
R,0,,False
L,0,,False
1: Initialize  in the action-value function Q,0,,False
"2: for n ,"" 1, 2, · · · , N do""",0,,False
"3: for i ,"" 1, 2, · · · , M do""",0,,False
4:,0,,False
"Si ,"" [t1, t2, · · · , tn, · · · ]""",0,,False
5:,0,,False
"initialize s1 ,"" {X^1 }, X^1 is appended representation for x1""",0,,False
6:,0,,False
"for j ,"" 1, 2, · · · , |S | do""",0,,False
7:,0,,False
"Compute: Q (sj, ap ;  ) and Q (sj, as ;  )",0,,False
8:,0,,False
Action: Randomly select an action for aj with  probability;,0,,False
"Otherwise aj ,"" maxa A(Q (sj, a ;  )). Execute aj .""",0,,False
9:,0,,False
Observe: reward rj and xj+1; generate the appended,0,,False
representation X^ j+1,0,,False
",",0,,False
[X^,0,,False
(k j,0,,False
"),",0,,False
x,0,,False
j,0,,False
+1,0,,False
];,0,,False
set,0,,False
state,0,,False
s j +1,0,,False
",",0,,False
"[sj, aj, X^ j+1].",0,,False
10:,0,,False
Update Q-Network by performing a gradient descent step,1,ad,True
on the loss function in Equation 1.,0,,False
11:,0,,False
end for,0,,False
12: end for,0,,False
13: end for,0,,False
and M is the number of text streams (text episodes). Each text stream,0,,False
"for one topic, e.g. S ,"" {t1, t2, · · · , tn, · · · }, has one episode training. |S | is the number of texts in each text episode S. Firstly, it initializes the start state s1 and the (high-level) representation X^1. For each text t in the episode, it executes four steps. In the rst step, compute Q function values for two actions in A (push or skip) (Line 7). They are Q(sj , ap ;  ) and Q(sj , as ;  ) respectively. At the action step (line 8): with  probability, a random action is chosen for aj ; otherwise, aj "","" maxa A(Q(sj , a ;  )). Then execute aj . In the third step (line 9), observe the reward rj 1and xj+1 ; generate the representation X^j+1 "","" [X^j(k), xj+1] by appending the text feature vector xj+1 and the last k text feature vector from previously selected texts in X^j ; set the next state sj+1 "","" [sj , aj , X^j+1]. Finally, update the Q-Network by performing a gradient descent step [1]""",1,ad,True
"1we use expected gain (EG) as the reward. Only when the current text is the terminal of this episode, r equal to the EG value. Otherwise r is zero.",0,,False
on the loss function de ned in Equation 1 (line 10). The target j is set as Equation 2:,0,,False
"j,",0,,False
"rj rj +  maxa Q(sj+1, aj ; i )",0,,False
terminal non - terminal,0,,False
(2),0,,False
"If it is the terminal of the episode, j equals to the reward r . Oth-",0,,False
"erwise, j equals to rj +  maxa Q(sj+1, aj ; i ).  is the discounted",0,,False
"factor. After training, the Q-network is a neural network approxi-",0,,False
mation function for real-time decision making.,0,,False
2.2 Q-Network,0,,False
Q value for push Q value for skip fully connected fully connected fully connected,0,,False
A sequence of vectors,0,,False
LSTM Q-Network,0,,False
Tweet Skip,1,Tweet,True
Tweet Push,1,Tweet,True
Tweet ... ... Skip Timeline,1,Tweet,True
Tweet Push,1,Tweet,True
Tweet Push,1,Tweet,True
Tweet,1,Tweet,True
?,0,,False
Now,0,,False
... ...,0,,False
Figure 1: Q-Network,0,,False
"In this section, we demonstrate the design of our proposed Q-",0,,False
Network. Figure 1 shows the architecture of our Q-Network which,0,,False
consists of four layers. The rst layer is a single Long Short Term,0,,False
Memory (LSTM) [3] layer which generates the high-level abstrac-,0,,False
tion of the input sequence. Both semantic and temporal dependen-,0,,False
cies of the text stream are considered into LSTM by taking the last k,0,,False
previously selected text feature representation. Therefore the input,0,,False
"for LSTM is the representation X^ ,"" [x1, x2, · · · , xT ]. T is the length of X^ . LSTM computes the hidden vector H "","" [h1, h2, · · · , hT ] by iterating the following equations:""",0,,False
"it ,  (Wxi xt + Whiht -1 + Wcict -1 + bi ) ft ,  (Wx f xt + Whf ht -1 + Wcf ct -1 + bf ) ct , ft ct -1 + it tanh(Wxc xt + Whcht -1 + bc ) ot ,  (Wxoxt + Whoht -1 + Wco + bo ) ht , ot tanh(ct )",0,,False
"in which  is the logistic sigmoid function, and i, f , o and c are",0,,False
"respectively the input gate, forget gate, output gate and cell activa-",0,,False
"tion vectors, all of which are the same size as the hidden vector h. W terms denote weight matrices, e.g. Wxi is the input-input gate weight matrix and Whi is the hidden-input gate matrix. The b terms denote bias vectors, e.g. bh is the hidden bias vector.",0,,False
LSTM is connected with three fully connected neural networks.,0,,False
The output of LSTM hT (the last item in the hidden vector H ) acts,0,,False
as the input for the later fully connected neural network layers.,0,,False
Each fully connected neural network can be formated as follows:,0,,False
"Zi , ^iWi ;",0,,False
"^i+1 , F (Zi )",0,,False
914,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"where each output of previous (the i layer) layer ^ will be the input of the later layer (the i + 1 layer). Here, i indexes from 1 to 3. ^1 is the output of LSTM hT . W are the weight matrix of each fully connected neural network. F is the activation function and for the previous two fully connected neural networks, the activation function is ReLU and we use a linear activation function for the last fully connected neural network.",0,,False
3 EXPERIMENTS 3.1 Dataset,0,,False
"We use TREC 2016 Microblog Track 2 as data set to evaluate our proposed approach. The dataset consists of the Twitter's sample tweet streams ( approximately 1% of public tweets) during the o cial evaluation period from August 2, 2016 to August 11, 2016. 56 judged interest pro les (topics) of tweets have been given. Each interest pro le (topic) consists of title, description and narrative description. Each sample tweet has a given label for its corresponding topic: highly relevant, relevant and non-relevant. We pre-process the dataset by removing the non-English tweets and tweets which have fewer than 5 words. Also we lter out very irrelevant tweets, e.g. tweets which have no interest pro le keywords. After preprocessing, there is a total of 57,419 tweets for 56 topics. Each topic has a tweet stream. The average number of tweets per topic is about 96.40. We randomly choose 85% of topics (48 topics) of tweets as training set and the remaining as test set.",1,TREC,True
Table 1: Features used for basic representation,0,,False
Features Name,0,,False
Description,0,,False
Statistic Temporal Semantic,0,,False
N_{title} N_{Narr} N_{exp} N_{word} N_{hashtag} Time,0,,False
Time_interval,0,,False
Is_redundant is_link Relevance score Cosine Score Text vector,0,,False
"Number of terms in ""Title"" Number of terms in ""Narrative"" Number of terms in ""Description"" Number of words in tweet text Number of hashtag in tweet text the current time in one day (ms) time interval between the incoming tweet and the last selected tweet Flag about whether it is redundant Flag about whether a URL exists. SVM regression score Cosine similarity score Word embedding representation",0,,False
3.2 Features,0,,False
"Before we input the tweet stream into the Q-Network, we rstly extract some features as the basic presentation x for each tweet. The extracted features are listed in Table 1. There are three groups: respectively statistic features, temporal features and semantic features. For each tweet, we compute some statistics as features, including number of hashtag, number of terms and number of terms appearing in the title, narrative description and description. Also, two temporal features are extracted. They are the time in one day (milliseconds since 00:00 of one day) and the time interval between this tweet and the last selected tweet. As semantic features, we",0,,False
2 http://trecrts.github.io/,1,trec,True
"generate a text vector (dimension,""300) for a tweet by averaging the word embedding vectors in a tweet text. The word embedding representation is obtained from a pre-trained Google News corpus word vector model [7]. And we compute the relevance score from a SVM regression for each topic and the cosine similarity score. Also we use two boolean ags (is_link and is_redudant) about whether URL exists or it is redundant. We normalize all features, and concatenate them into one single vector as the basic presentation x for each tweet. The dimension of the basic presentation is 311.""",0,,False
3.3 Evaluation Metric,0,,False
We use two TREC 2016 Real-time Summarization Track o cial evaluation metrics. They are expected gain (EG) and Normalized Cumulative Gain (nCG). The expected gain (EG) is de ned:,1,TREC,True
EG,0,,False
",",0,,False
1 N,0,,False
G (t ),0,,False
where N is the number of tweets returned and G(t) is the gain of,0,,False
each tweet: Not relevant tweets receive a gain of 0; relevant tweets,0,,False
receive a gain of 0.5; Highly-relevant tweets receive a gain of 1.0.,0,,False
Normalized Cumulative Gain (nCG) is de ned:,0,,False
nCG,0,,False
",",0,,False
1 Z,0,,False
G (t ),0,,False
"where Z is the maximum possible gain (given the ten tweet per day limit). In order to di erentiate the performances in salient days when there are no relevant tweets, there are two variants for each metric, respectively EG-0, EG-1, nCG-0 and nCG-1. For salient days, the EG-1 and nCG-1 gives a score of one if not pushing any tweets, or zero otherwise. In the EG-0 and nCG-0 variants, for a silent day, the gain is zero [9].",0,,False
3.4 Compared Methods,0,,False
"We compared the following methods: · Query similarity (QS): it pushes the tweet whose relevance score is higher than a xed threshold . Cosine similarity is used to measure the relevance score between the topic title and the tweet text. · YoGosling [9]: implements simple dynamic emission strategies to maintain appropriate dynamic thresholds for pushing updates. It achieved the best performance at the TREC 2015 Microblog Track Real-time Filtering task [4]. · Features+StaticThreshold (FST) [6]:It develops a relevance estimation model based on both lexical and non-lexical features, and set a static threshold to push tweets with their manual observation. It achieved the best performance at the TREC 2016 Real-time Summarization track [6]. · NNRL: This is our proposed algorithm in Section 2.",1,Query,True
3.5 Implementation Setting,0,,False
"In our algorithm, the learning rate for gradient descent is 0.001 and the discount factor is 1. The  is annealed linearly from 1 to 0, then x it at 0 until converging to a suboptimal policy. Then  is annealed linearly from 0.1 to 0; x at 0 until converging to another suboptimal. Repeat the  exploration step thereafter. In our Q-network, the hidden state size of LSTM is 512. The size of appended sequence from the previously selected text sequence k is",1,ad,True
915,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
up to 10 due to LSTM training e ciency. Both the rst and second fully-connected neural network layer have 256 hidden units. The output layer is a fully-connected linear layer with a single output for each valid action.,0,,False
EG-0 EG-1,0,,False
0.05 EG-0 performance over different thresholds,0,,False
0.04 0.03,0,,False
Max: 0.0362,0,,False
0.02,0,,False
0.01,0,,False
0.000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Threshold (®),0,,False
(a) EG-0,0,,False
0.30 EG-1 performance over different thresholds,0,,False
0.25,0,,False
Max: 0.2483,0,,False
0.20,0,,False
0.15,0,,False
0.10,0,,False
0.05,0,,False
0.000.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Threshold (®),0,,False
(b) EG-1,0,,False
Figure 2: EG results with varying QS threshold ,0,,False
3.6 Experimental Results,0,,False
Table 2: Evaluation results for all compared methods,0,,False
"Method QS ( , 0.45) YoGosling FST NNRL",0,,False
EG-1 0.2483 0.2289 0.2698 0.2816,0,,False
EG-0 0.0322 0.0253 0.0483 0.0691,0,,False
nCG-1 0.2325 0.0253 0.2909 0.2971,0,,False
nCG-0 0.0164 0.0253 0.0695 0.0846,0,,False
"Table 2 shows the evaluation results. We can see that our proposed NNRL outperforms the other competitors for all evaluation metrics. Our superior performances lie on that we format this problem as a sequential decision making process and deal with both history dependencies and future uncertainty while the other three compared methods mainly adopt the static or dynamic threshold to lter out the incoming tweet. Figure 2 shows the EG evaluation results (similar results with nCG; we omit it due to space limitation) with varying the threshold  of the QS method. There are big di erences in EG performance when using di erent thresholds. Therefore, static methods like QS and FST are inappropriate for the real-time environment which requires a dynamic and adaptive mechanism to consider future uncertainty. Although YoGosling proposed some strategies for obtaining dynamic threshold, it ignores future uncertainty. We use an exemplary case for topic ""Hiroshima atomic bomb"" to demonstrate the adaptive ability of our method (NNLR) to address the potential future uncertainty. Table 3 shows a snippet of the tweet sequence of 7 tweets for topic ""Hiroshima atomic bomb"" with each tweet's time, raw text and the selected action in our method. At the start of the tweet sequence, our method pushes the second tweet rather than the rst one because the second one is more speci c and relevant to the topic. Our method decides to skip the rst tweet and waits for the potential better tweet (e.g. the second one) in the future. After pushing the second one, it skips the following relevant but redundant tweets. When it has pushed some highly-relevant tweets and time elapses, the pushing condition might change as very few relevant tweets come. Therefore, it",1,ad,True
"pushes the relevant one while the same text was skipped previously. For example, the rst tweet was skipped but the fourth tweet with the same content is pushed.",0,,False
"Table 3: An exemplary case for ""Hiroshima atomic bomb""",0,,False
Time Tweet Text,1,Tweet,True
Action,0,,False
08-02 04:21:18 08-09 17:10:15 08-09 17:15:11 08-09 21:59:15,0,,False
08-10 00:36:05,0,,False
08-10 02:14:43 08-11 03:41:12,0,,False
"Obama At Hiroshima: A World Without Nuclear Weapons ­ Ours - American Thinker RT @HistoryToLearn: Hiroshima, one year after the atomic bomb blast, 1946. RT @HistoryToLearn: Hiroshima, one year after the atomic bomb blast, 1946. Obama At #Hiroshima: A World Without Nuclear Weapons ­ Ours - American Thinker I dropped the bomb but now I have the proof so now I can drop the atomic bomb. I've never been so excited RT @AJENews: Nagasaki marks 71st anniversary of atomic bombing Nagasaki Marks 71st Anniversary of Atomic Bombing.",0,,False
Skip Push Skip Push Skip Skip Push,0,,False
4 CONCLUSIONS,0,,False
"In this paper, we propose a neural network based reinforcement",0,,False
learning algorithm to address real-time pushing on text stream. A,1,ad,True
novel Q-Network is designed to approximate the maximum long-,0,,False
term rewards. Experiment results on real data from TREC 2016,1,TREC,True
Real-time Summarization track demonstrate that our algorithm,0,,False
is superior to all compared methods for all the o cial evaluation,0,,False
metrics and has the ability to make real-time decisions. In future,0,,False
"work, we plan to study the case without speci c query topic.",0,,False
5 ACKNOWLEDGMENTS,0,,False
The work in this paper was supported by Research Grants Coun-,0,,False
"cil of Hong Kong (PolyU 152094/14E), National Natural Science",0,,False
Foundation of China (61272291) and The Hong Kong Polytechnic,0,,False
University (4-BCB5).,0,,False
REFERENCES,0,,False
[1] Leemon Baird and Andrew Moore. Gradient Descent for General Reinforcement Learning. In NIPS'98.,1,ad,True
"[2] Feifan Fan, Yansong Feng, Lili Yao, and Dongyan Zhao. Adaptive Evolutionary Filtering in Real-Time Twitter Stream. In CIKM '16.",1,Twitter,True
"[3] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long Short-Term Memory. Neural Comput. 9, 8 (1997), 1735­1780.",0,,False
"[4] Y. Wang G. Sherman J. Lin, M. Efron and E. Voorhees. Overview of the TREC-2015 Microblog Track. In TREC 2015.",1,TREC,True
"[5] Chris Kedzie, Fernando Diaz, and Kathleen McKeown. Real-Time Web Scale Event Summarization Using Sequential Decision Making. In IJCAI '16.",0,,False
"[6] Jimmy Lin, Adam Roegiest, Luchen Tan, Richard McCreadie, Ellen Voorhees, and Fernando Diaz. Overview of the TREC-2016 Microblog Track. In TREC 2016.",1,ad,True
"[7] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Je rey Dean. Distributed Representations of Words and Phrases and Their Compositionality. In NIPS'13.",1,ad,True
"[8] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. 2013. Playing Atari with Deep Reinforcement Learning. CoRR (2013).",0,,False
"[9] Luchen Tan, Adam Roegiest, Charles L.A. Clarke, and Jimmy Lin. Simple Dynamic Emission Strategies for Microblog Filtering. In SIGIR '16.",1,blog,True
916,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
The Impact of Linkage Methods in Hierarchical Clustering for Active Learning to Rank,0,,False
Ziming Li,0,,False
"University of Amsterdam Amsterdam, e Netherlands",0,,False
z.li@uva.nl,0,,False
ABSTRACT,0,,False
"Document ranking is a central problem in many areas, including information retrieval and recommendation. e goal of learning to rank is to automatically create ranking models from training data. e performance of ranking models is strongly a ected by the quality and quantity of training data. Collecting large scale training samples with relevance labels involves human labor which is timeconsuming and expensive. Selective sampling and active learning techniques have been developed and proven e ective in addressing this problem. However, most active methods do not scale well and need to rebuild the model a er selected samples are added to the previous training set. We propose a sampling method which selects a set of instances and labels the full set only once before training the ranking model. Our method is based on hierarchical agglomerative clustering (average linkage) and we also report the performance of other linkage criteria that measure the distance between two clusters of query-document pairs. Another di erence from previous hierarchical clustering is that we cluster the instances belonging to the same query, which usually outperforms the baselines.",1,ad,True
CCS CONCEPTS,0,,False
·Information systems Learning to rank;,0,,False
1 INTRODUCTION,1,DUC,True
"Document ranking is an essential feature in many information retrieval applications. How to sort the returned results according to their degree of relevance has given birth to the area of learning to rank [6], which aims to automatically create ranking models from a training dataset; the learned models are then used to rank the results of new queries. Many learning to rank algorithms have been proposed; they can be categorized into three types of approach: pointwise [2], pairwise [5] and listwise [1].",0,,False
"Like other supervised machine learning methods, the performance of a ranking model highly depends on the quantity and quality of training datasets. To create training datasets, experts are hired to manually provide relevance labels for training data, which is expensive and time-consuming. Human labeling, whether by",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR'17, August 7­11, 2017, Shinjuku, Tokyo, Japan. © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080684",1,ad,True
Maarten de Rijke,0,,False
"University of Amsterdam Amsterdam, e Netherlands",0,,False
derijke@uva.nl,0,,False
"experts or crowds, can be noisy and biased [11]. Active learning is a paradigm to reduce the labeling e ort [12]; it has mostly been studied in the context of classi cation tasks [9, 10, 14].",1,ad,True
"In this paper, we present an active learning method to select the most informative query-document pairs to be labeled for learning to rank. Our method relies on hierarchical clustering. Unlike traditional active learning methods, our method is unsupervised and the selected training sets can be used to train di erent learning to rank models. We build on the hypothesis that the information contained in an instance is highly correlated to the instance position in the feature space. Hierarchical clustering has the ability to group instances with similar information into the same cluster and each cluster can be represented by its centroid. While most active learning methods need to rebuild the training models each time new labeled documents are added to the training set, our method labels the instances only once before the training process. We rst evaluate our method on three datasets from Letor 3.0 and nd that the performance of our method is similar or superior to the baselines while we can achieve full training performance with fewer instances. We also analyze the limitations of our method and nd that the e ectiveness of our sampling method is closely related to the features and structures each dataset has.",1,ad,True
2 RELATED WORK,0,,False
"In order to address the lack of labeled data, active learning has proven to be a promising direction that aims to achieve high accuracy using as few labeled instances as possible [12]. A number of active learning methods have been proposed for classi cation. Most methods start with only a small set of labeled instances and sequentially select the most informative instances to be labeled by an oracle. e trained model will be updated when new labeled instances are added to the training set. Di erent strategies are proposed to choose the most informative instances that can maximize the information value to the current model [9, 10, 14].",1,ad,True
"It is not straightforward to extend these methods to ranking problems. On the one hand, these methods try to minimize the classi cation error and do not take into account the rank order while position-based measures are usually non-continuous and non-di erentiable. On the other, each instance in most supervised classi cation tasks can be treated as independent of each other while the instances in learning to rank are conditionally independent. Compared with traditional classi cation problems, there is limited work about active learning in ranking. Long et al. [8] adopt a two-stage active learning strategy schema to integrate query level and document level data selection. ey select samples minimizing the expected loss as the most informative ones and achieves good results in the case of web search ranking. But this method requires",1,ad,True
941,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"a relative big seed set and the ranking models are restricted to pointwise models. Donmez and Carbonell [3] select the documents that woluld impart the greatest change to the current model. While all these methods sequentially select instances to be labeled, Silva et al. [13] adopt hierarchical clustering to ""compress"" the original training set, which is the state-of-the-art selection method in overall performance and e ciency. e method we propose can be viewed as an extension of [13]. e di erence are that we cluster the query-document pairs belonging to the same query separately and average linkage is used, which achieves be er performance.",1,ad,True
3 METHOD,0,,False
3.1 Hierarchical agglomerative clustering,0,,False
"In hierarchical agglomerative clustering, each instance is regarded as a singleton cluster and then merged based on the distance or similarity between clusters until there is only one cluster that contains all instances. e structure of the nal cluster is a tree or dendrogram and each level of the resulting tree is a segmentation of the data. For two given clusters, C1 and C2, and a non-negative real value , if distance f (C1, C2) < , then C1 and C2 will be merged.",0,,False
"According to the merging rule, the number of clusters is associated with the value of , which is the indistinguishability threshold [13]. Di erent linkage criterions have been proposed to measure the distance or dissimilarity between two clusters. We use average as our linkage criterion and we also report the performance of minimum linkage, maximum linkage and ward linkage.",0,,False
"3.1.1 Minimum linkage clustering. In minimum linkage clustering (also called single linkage clustering), one of the simplest agglomerative hierarchical clustering methods, the value of the shortest link from any member of one cluster to the member of another cluster denotes the distance of these two clusters. For two clusters C1 and C2, the distance between C1 and C2 is:",0,,False
f,0,,False
"(C1, C2)",0,,False
",",0,,False
u,0,,False
min,0,,False
"C1, C2",0,,False
"d (u,",0,,False
"),",0,,False
"In this paper, d is the Euclidean distance. is method is also known as the nearest technique and used in [13]. In this case, minimum linkage clustering can group the instances in ""stringy"" clusters and be converted to nd the Minimum Spanning Tree (MST) of querydocument pairs [4]. By deleting all edges longer than a speci ed indistinguishability threshold in the MST, the remaining connected instances form a hierarchical cluster.",0,,False
3.1.2 Average linkage clustering. Average linkage clustering uses,0,,False
"the average of distances between all pairs of instances, where each",0,,False
"pair consists of two points from two di erent clusters, as the dis-",0,,False
tance between two clusters:,0,,False
f,0,,False
"(C1, C2)",0,,False
",",0,,False
N1,0,,False
1  N2,0,,False
"u C1,",0,,False
"d (u,",0,,False
C2,0,,False
"),",0,,False
"where N1 and N2 are the sizes of clusters C1 and C2, respectively. We use average linkage as our linkage criterion to measure the",0,,False
cluster distance and we perform clustering on each subset which,0,,False
contains all the query-document pairs belonging to the same query.,0,,False
"3.1.3 Maximum linkage clustering. Di erent from single linkage clustering, the distance of two clusters in maximum linkage clustering is the value of the largest link from one cluster to another",0,,False
"cluster. For two clusters C1 and C2, the distance is computed as follows:",0,,False
f,0,,False
"(C1, C2)",0,,False
",",0,,False
u,0,,False
max,0,,False
"C1, C2",0,,False
"d (u,",0,,False
"),",0,,False
where d is the Euclidean distance.,0,,False
"3.1.4 Ward linkage clustering. In Ward's linkage method, the",0,,False
distance between two clusters is the sum of the squares of the,0,,False
distances between all objects in the cluster and the centroid of the,0,,False
"cluster. For two clusters C1 and C2, the distance is computed as",0,,False
follows:,0,,False
"f (C1, C2) ,",0,,False
"d (x, µC1C2 )2,",0,,False
x C1C2,0,,False
where µ is the centroid of the new cluster merged from C1 and C2.,0,,False
3.2 Hierarchical clustering for learning to rank,0,,False
"In this paper, we apply di erent linkage criteria to hierarchical clustering. As shown in Algorithm 1, we rst cut all the querydocuments pairs into di erent subsets which have di erent query ids and then adopt hierarchical clustering on each subset. A er the last two steps, we can get the clusters on each subset. In our sampling strategy, we use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster. In fact, the clustering distribution shows that there is only one single point in most clusters. e nal dataset to be labeled is made up of all the selected instances.",1,ad,True
Algorithm 1 Hierarchical Clustering on Each ery (HCEQ),0,,False
"Require: Unlabeled dataset D with m queries, desired sampling",0,,False
"size n, linkage criterion linka e",0,,False
Ensure: e subset S to be labeled,0,,False
"1: D1, D2, . . . , Di , . . . , Dm  Di idin Dataset (D). 2: S  ",0,,False
"3: for all i  {1, 2, . . . , m} do",0,,False
4:,0,,False
ni,0,,False
n,0,,False
|Di | |D |,0,,False
"5: C1, C2, . . . , Cj , . . . , Cni  A Clusterin (Di , ni , linka e )",0,,False
"6: for all j  {1, 2, . . . , ni } do",0,,False
7:,0,,False
j  GeometricCentroid (Cj ),0,,False
8:,0,,False
"pj  N earest N ei hbour (Cj , j )",0,,False
9:,0,,False
S  S  {pj },0,,False
10: end for,0,,False
11: end for,0,,False
12: return S,0,,False
4 EXPERIMENTAL SETUP,0,,False
4.1 Data Sets and Evaluation Measure,0,,False
"To compare the performance of di erent linkage criteria, we apply hierarchical clustering to a well-known L2R benchmarking collection, Letor 3.0 [7]. In Letor 3.0, there are 7 datasets, based on two document collections: Gov and OHSUMED. We focus on topic distillation (TD2003, TD2004) from GOV and OHSUMED from OHSUMED. In the sets from GOV, each instance is represented by 64 features extracted from the corresponding query-document pairs and has a label indicating its relevance level. e datasets from GOV have binary relevance labels (relevant, not relevant) while",1,Gov,True
942,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"OHSUMED has 3 levels (de nitely relevant, possibly relevant, not relevant) and each instance is represented by a 45-dimensional feature vector. Di erent datasets have di erent numbers of instances; there are 50, 75 and 106 queries, each with 50K, 74K and 16K instances in TD2003, TD2004 and OHSUMED, respectively.",1,TD,True
e metric we use is Normalized Discounted Cumulative Gain (NDCG). We run 5-fold cross-validation on all datasets which are query-level normalized and report the average NDCG@10 on 5 folds as nal performance.,0,,False
4.2 Baselines,0,,False
"We compare the results obtained using our methods with the approach in [13], which is also based on hierarchical clustering; the results of random sampling and the full training set are also reported for reference:",0,,False
"Cover. e method proposed in [13] is an unsupervised and compression-based selection mechanism that tries to create a small and highly informative set to represent the original training dataset. Hierarchical clustering (single linkage) is employed to group querydocument pairs into di erent clusters with required number of clusters. e authors also use the instance closest to the geometric centroid of each cluster to represent all the query-document pairs in this cluster and form the nal training set to be labeled. Unlike our method, they perform clustering globally and instances belonging to di erent queries could be grouped into the same cluster.",0,,False
"Random. e instances to be labeled are selected randomly and no active learning methods are used here. For the same dataset, we run random sampling 10 times and report the average performance.",0,,False
Full Training. We use the labeled original training datasets to train the learning to rank models.,0,,False
"To be able to show the di erence between our methods and [13], we select SVMRank and Random Forests as our ranking models which are also used in [13]. e parameters of SVMRank and Random Forests are tuned using a small validation set.",0,,False
We run all sampling methods until the fraction of selected instances reaches 50% of the original set.,0,,False
5 RESULTS AND ANALYSIS,0,,False
5.1 Experimental Results,0,,False
"Fig. 1 shows the NDCG@10 of di erent linkage criteria and baselines (denoted by Average, Max, Min, Ward, Cover, Random and Full respectively) on the TD2003, TD2004 and OHSUMED datasets. As we use the instance closest to the centroid to represent the corresponding cluster, the instance selected before may not be selected in later sampling rounds; accuracy is not monotonically increasing.",1,TD,True
"On TD2003, in terms of SVMRank (Fig. 1(a)), the accuracy of Average rst exceeds the accuracy of Full at size 2%. Before 24% of the original training set have been selected, all the curves uctuate around 0.35 except Random and Ward. When more and more instances are added to the training set, the performance of Cover goes down and becomes worse than Full. For Random Forests (Fig. 1(d)), Average achieves the highest accuracy before 4% of the original training set has been selected and is the rst one to reach the same accuracy as Full. e accuracies of Ward, Max and Cover start from relatively low points. All methods achieve the performance of Full",1,TD,True
"at around 12% except Random. A er 18%, Cover stays below Full (close to 0.36) and uctuates around 0.35.",0,,False
"Fig. 1(b) and 1(e) describe the performance of di erent methods on the TD2004 dataset. In Fig. 1(b) we see that Average has the highest starting point when 2% of the original training set have been selected. However, a er one more percent has been added to the training set, all methods have a very similar performance at around 0.295. Average and Cover reach the performance of Full with about 5% and continue to rise before they reach their peaks with 13% and 8%, respectively. Except Random, all methods reach the performance of Full with about 11% selected; their accuracy stays higher than that of Full. In terms of Random Forests (Fig. 1(e)), Average still has the highest starting accuracy and a er the uctuations before 11% of the training set has been selected, Average always performs be er than Full and peaks around 19% of the training set, which is also the highest accuracy in all methods. Cover has a relatively high accuracy when the selected size is 7­ 11%.",1,TD,True
"e OHSUMED dataset is based on another document collection, di erent from TD2003 and TD2004. e performance of SVMRank and Random Forests on OHSUMED are shown in Fig. 1(c) and 1(f), respectively. In terms of SVMRank, an interesting thing is that Random has similar performance as hierarchical clustering, which means that hierarchical clustering plays a small role when selecting informative instances in this case. With respect to Random Forests, Average, Min, Ward and Max have very similar performance a er 16% of the training data has been selected and they reach the accuracy of Full at around 30%. Although Cover is the rst method to achieve full training set performance with around 7% and reaches the peak at the same time, its accuracy is not stable and dramatically decreases until 17% of the training set has been selected. A er 30% of the original training set has been selected, all methods achieve the same performance as Full.",1,TD,True
5.2 Analysis,0,,False
"As we can see from the results presented above, most of the time, Average outperforms other methods on the TD2003 and TD2004 datasets. Although Min and Cover both use the shortest link to measure the distance between two clusters, they have di erent performance and Min performs be er. One possible reason is that our selection mechanism can guarantee that the proportions of selected instances from each query are same and every query contributes to the nal performance. Another reason might be Cover clusters query-document pairs globally, which causes that query-document pairs from di erent queries will be represented by the instances from one speci c query. And this will limit the number of applicable instance pairs for pairwise ranking models.",1,TD,True
"On OHSUMED, Average, Min and Max have similar and stable performance while Cover uctuates dramatically when relatively few instances are selected, especially with the Random Forests learner. e di erence between the proposed methods and Random is not signi cant. How datasets are constructed and what structures datasets have will in uence the performance of clusteringbased active learning methods. For example, an instance from the OHSUMED dataset is represented by 45 features which is fewer than for TD2003 and 2004, and some speci c features could have greater impact on the clustering results. In addition, a query has",1,TD,True
943,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
NDCG@10,0,,False
0.37 0.35,0,,False
0.34 0.32,0,,False
0.45 0.43 0.41,0,,False
0.33 0.31,0,,False
0.30,0,,False
0.39 0.37 0.35,0,,False
NDCG@10,0,,False
NDCG@10,0,,False
0.29,0,,False
Average,0,,False
0.27,0,,False
Cover Max,0,,False
Min,0,,False
0.25,0,,False
Ward Random,0,,False
Full,0,,False
0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,0,,False
(a) SVMRank on TD2003,1,TD,True
% selected,0,,False
0.28,0,,False
Average,0,,False
Cover,0,,False
Max,0,,False
0.26,0,,False
Min,0,,False
Ward,0,,False
Random,0,,False
Full,0,,False
0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,0,,False
(b) SVMRank on TD2004,1,TD,True
% selected,0,,False
0.33,0,,False
0.31,0,,False
Average,0,,False
0.29,0,,False
Cover Max,0,,False
0.27,0,,False
Min Ward,0,,False
0.25,0,,False
Random Full,0,,False
0.23 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48,0,,False
(c) SVMRank on OHSUMED,0,,False
% selected,0,,False
NDCG@10,0,,False
0.38,0,,False
0.36,0,,False
0.34,0,,False
0.32,0,,False
0.30,0,,False
Average,0,,False
0.28,0,,False
Cover Max,0,,False
Min,0,,False
0.26,0,,False
Ward Random,0,,False
Full,0,,False
0.24 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,0,,False
%selected,0,,False
(d) Random Forests on TD2003,1,TD,True
NDCG@10,0,,False
0.35,0,,False
0.33,0,,False
0.31,0,,False
0.29,0,,False
Average Cover,0,,False
Max,0,,False
0.27,0,,False
Min Ward,0,,False
Random,0,,False
Full,0,,False
0.25 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,0,,False
%selected,0,,False
(e) Random Forests on TD2004,1,TD,True
NDCG@10,0,,False
0.45,0,,False
0.43,0,,False
0.41,0,,False
0.39,0,,False
0.37,0,,False
0.35,0,,False
0.33,0,,False
0.31,0,,False
0.29,0,,False
Average Cover,0,,False
0.27 0.25,0,,False
Max Min Ward,0,,False
0.23,0,,False
Random Full,0,,False
0.21 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 40 42 44 46 48 50,0,,False
%selected,0,,False
(f) Random Forests on OHSUMED,0,,False
"Figure 1: Performance on TD2003, TD2004, and OHSUMED. e x-axis displays the percentage of selected instances.",1,TD,True
"only about 152 associated documents in OHSUMED. When we cluster query-document pairs with respect to each query, the number of selected instances from each query is small and every individual query will have very few associated documents which can in uence the performance of ranking models.",0,,False
6 CONCLUSIONS AND FUTURE WORK,0,,False
"In this paper, we adopt hierarchical clustering to select the most informative instances for learning to rank and report the performance of di erent linkage criteria and baselines. On the Letor 3.0 dataset, the performance of average linkage is similar or superior to the baselines while fewer instances are needed. In the future, we will investigate how to make our method more stronger and robust on di erent datasets. One possible direction is to detect correlations between speci c features and clustering performance. How to choose an optimal fraction of instances for each query while the total number of selected instances is xed is another future direction worth exploring.",1,ad,True
"Acknowledgments. is research was supported by Ahold Delhaize, Amsterdam Data Science, the Bloomberg Research Grant program, the Dutch national program COMMIT, Elsevier, the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement nr 312827 (VOX-Pol), the Microso Research Ph.D. program, the Netherlands Institute for Sound and Vision, the Netherlands Organisation for Scienti c Research (NWO) under project nrs 612.001.116, HOR-11-10, CI-14-25, 652.002.001, 612.001.551, 652.001.003, and Yandex. All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.",0,,False
REFERENCES,0,,False
"[1] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML '07, pages 129­136, 2007.",0,,False
"[2] K. Crammer and Y. Singer. Pranking with ranking. In NIPS '01, pages 641­647, 2001.",0,,False
"[3] P. Donmez and J. G. Carbonell. Optimizing estimated loss reduction for active sampling in rank learning. In ICML '08, pages 248­255, 2008.",0,,False
"[4] J. C. Gower and G. Ross. Minimum spanning trees and single linkage cluster analysis. Applied statistics, pages 54­64, 1969.",0,,False
"[5] R. Herbrich, T. Graepel, and K. Obermayer. Support vector learning for ordinal regression. In ICANN '99, pages 97­102, 1999.",0,,False
"[6] T.-Y. Liu. Learning to rank for information retrieval. Foundations and Trends in Information Retrieval, 3(3):225­331, 2009.",0,,False
"[7] T.-Y. Liu, J. Xu, T. Qin, W. Xiong, and H. Li. Letor: Benchmark dataset for research on learning to rank for information retrieval. In SIGIR 2007 Workshop on Learning to Rank for Information Retrieval, pages 3­10, 2007.",0,,False
"[8] B. Long, J. Bian, O. Chapelle, Y. Zhang, Y. Inagaki, and Y. Chang. Active learning for ranking through expected loss optimization. IEEE Transactions on Knowledge and Data Engineering, 27(5):1180­1191, 2015.",0,,False
"[9] A. K. McCallumzy and K. Nigamy. Employing em and pool-based active learning for text classi cation. In ICML '98, pages 359­367, 1998.",0,,False
"[10] H. T. Nguyen and A. Smeulders. Active learning using pre-clustering. In ICML '04, page 79, 2004.",0,,False
"[11] F. Radlinski and T. Joachims. Active exploration for learning rankings from clickthrough data. In KDD '07, pages 570­579, 2007.",1,ad,True
"[12] B. Se les. Active learning literature survey. Technical report, University of Wisconsin­Madison, 2009.",1,ad,True
"[13] R. M. Silva, G. Gomes, M. S. Alvim, and M. A. Gon¸calves. Compression-based selective sampling for learning to rank. In CIKM '16, pages 247­256, 2016.",0,,False
"[14] S. Tong and D. Koller. Support vector machine active learning with applications to text classi cation. Journal of Machine Learning Research, 2(Nov):45­66, 2001.",0,,False
944,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Reinforcement Learning to Rank with Markov Decision Process,0,,False
"Zeng Wei, Jun Xu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng",0,,False
"CAS Key Lab of Network Data Science and Technology, Institute of Computing Technology, Chinese Academy of Sciences zengwei@so ware.ict.ac.cn,{junxu,lanyanyan,guojiafeng,cxq}@ict.ac.cn",1,ad,True
ABSTRACT,0,,False
"One of the central issues in learning to rank for information retrieval is to develop algorithms that construct ranking models by directly optimizing evaluation measures such as normalized discounted cumulative gain (NDCG). Existing methods usually focus on optimizing a speci c evaluation measure calculated at a xed position, e.g., NDCG calculated at a xed position K. In information retrieval the evaluation measures, including the widely used NDCG and P@K, are usually designed to evaluate the document ranking at all of the ranking positions, which provide much richer information than only measuring the document ranking at a single position. us, it is interesting to ask if we can devise an algorithm that has the ability of leveraging the measures calculated at all of the ranking postilions, for learning a be er ranking model. In this paper, we propose a novel learning to rank model on the basis of Markov decision process (MDP), referred to as MDPRank. In the learning phase of MDPRank, the construction of a document ranking is considered as a sequential decision making, each corresponds to an action of selecting a document for the corresponding position. e policy gradient algorithm of REINFORCE is adopted to train the model parameters. e evaluation measures calculated at every ranking positions are utilized as the immediate rewards to the corresponding actions, which guide the learning algorithm to adjust the model parameters so that the measure is optimized. Experimental results on LETOR benchmark datasets showed that MDPRank can outperform the state-of-the-art baselines.",1,ad,True
KEYWORDS,0,,False
learning to rank; Markov decision process,0,,False
"ACM Reference format: Zeng Wei, Jun Xu , Yanyan Lan, Jiafeng Guo, Xueqi Cheng. 2017. Reinforcement Learning to Rank with Markov Decision Process. In Proceedings of SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan, , 4 pages. DOI: h p://dx.doi.org/10.1145/3077136.3080685",0,,False
1 INTRODUCTION,1,DUC,True
"Learning to rank has been widely used in information retrieval and recommender systems. Among the learning to rank methods,",0,,False
* Corresponding author: Jun Xu,0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR17, August 7­11, 2017, Shinjuku, Tokyo, Japan © 2017 ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: h p://dx.doi.org/10.1145/3077136.3080685",1,ad,True
"directly optimizing the ranking evaluation measures is a representative approach and has been proved to be e ective. Following the idea of directly optimizing evaluation measures, a number of methods have been proposed, including SVMMAP [17] , Adarank [14] , and PermuRank [15] etc. In training, these methods rst construct a loss function on the basis of a prede ned ranking evaluation measure. en, approximations of the loss function or convex upper bounds upon the loss function are constructed and then optimized, leading to di erent directly optimizing learning to rank algorithms. In ranking, each document is assigned a relevance score based on the learned ranking model.",1,MAP,True
"Several learning to rank models that directly optimize evaluation measure have been proposed and applied to variant ranking tasks. Di erent loss functions and optimization techniques are adopted in these methods. For example, the loss function of SVMMAP is constructed on the basis of MAP. e upper bound of the hinge loss is de ned and is optimized with structure SVM. AdaRank, another directly optimizing method, construct its loss function on the basis of any evaluation measure whose values are between 0 and 1. Exponential upper bound is constructed and Boosting is adopted for conduct the optimization.",1,ad,True
"In general, all the methods that directly optimize evaluation measures perform well in many ranking tasks, especially in terms of the speci c evaluation measure used in the training phase. We also note that some widely used evaluation measures are designed to measure the goodness of a document ranking at all of the ranking positions. For example, the evaluation measure of NDCG can be calculated at all of the ranking positions, each re ects the goodness of the document ranking from the beginning to the corresponding position. Existing methods, however, can only directly optimize the evaluation measure calculated at a prede ned ranking position. For example, the AdaRank algorithm will focus on optimizing NDCG at rank K, if its loss function is con gured based on the evaluation measure NDCG@K. e information carried by the documents a er the rank K are ignored, because they have no contribution to NDCG@K. us, it is natural to ask is it possible to devise a learning to rank algorithm that directly optimizes evaluation measures and has the ability to leverage the measures at all of the ranks?",0,,False
"To answer the above question, we propose a new learning to rank model on the basis of Markov decision process (MDP) [10], called MDPRank. e training phase of MDPRank considers the construction of a document ranking as a process of sequential decision making and learns the the model parameters through maximizing the cumulated rewards to all of the decisions. Speci cally, the ranking of M documents is considered as a sequence of M discrete time steps where each time step corresponds to a ranking position.",0,,False
"e ranking of documents, thus, is formalized as a sequence of M decisions and each action corresponds to selecting one document. At each time step, the agent receives the environment's state and",0,,False
945,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"chooses an action on the basis of the state. One time step later, as a consequence of the action the system transit to a new state. At each time step, the chosen of the action depends on a policy, which is a function maps from the current state to a probability distribution of selecting each possible action.",0,,False
"Reinforcement learning is employed to train the model parameters. Given a set of labeled queries, at each time step, the agent can receive a numerical action-dependent reward which is de ned upon the evaluation measure calculated at that position. e policy gradient algorithm of REINFORCE [10] is adopted to adjust the model parameters so that expected long-term discounted rewards in terms of the evaluation measure is maximized. us, MDPRank can directly optimize the evaluation measure and leverages the performances at all of the ranks as rewards.",1,ad,True
"Compared with existing methods that directly optimize IR measure, MDPRank enjoys the following advantages: 1) the ability of utilizing the IR measures calculated at all of the ranking positions as supervision in training; 2) directly optimizes the IR measure on the training data without approximation or upper bounding.",1,ad,True
"To evaluate the e ectiveness of MDPRank, we conducted experiments on the basis of LETOR benchmark datasets. e experimental results showed that MDPRank can outperform the state-of-the-art learning to rank models including the methods that directly optimize evaluation measures such as SVM-MAP and AdaRank.",1,MAP,True
2 RELATED WORK,0,,False
"In learning to rank for information retrieval, one of the most straightforward way to learn the ranking model is directly optimizing the measure used for evaluating the ranking performance. A number of methods have been developed in recent years. Some methods try to construct a continuous and di erentiable approximation of the measure-based ranking error. For example, So Rank [11] makes use of the expectation of NDCG over all possible rankings as an approximation of the original evaluation measure NDCG. SmoothRank [2] smooth the evaluation measure by approximating the rank position. Some other methods try to optimize a continuous and di erentiable upper bound of the measure-based ranking error. For example, SVMMAP [17] , SVMNDCG [1] optimize the relaxation of IR evaluation measures of MAP and NDCG, respectively. Structure SVM is adopted for conducting the optimization in both of these two methods. AdaRank [14] directly optimizes the exponential upper bound of the ranking error with a Boosting procedure. [15] summarized the framework of directly optimizing evaluation measure and new algorithms can be derived and analyzed under the framework. Recently, the idea is applied to search result diversi cation. For example, in Xu et al. [16] and Xia et al. [13], structure Perceptron is utilized to directly optimizes the diversity evaluation measures.",1,MAP,True
"In this paper we propose to directly optimize ranking evaluation measure with MDP [10], which has been widely used in variant IR applications. For example, in [6], a win-win search framework based on partially observed Markov decision process (POMDP) is proposed to model session search as a dual-agent stochastic game. In the model, the state of the search users are encoded as a four hidden decision making states. In [18], the log-based document re-ranking is also modeled as a POMDP to improve the re-ranking performances. MDP is also used for building recommender systems.",0,,False
"For example, [9] designed an MDP-based recommendation model for taking both the long-term e ects of each recommendation and the expected value of each recommendation into account. e multibandit also widely used for ranking [4, 8] and recommendation [5].",0,,False
3 MDP FORMULATION OF LEARNING TO RANK,0,,False
"In supervised learning se ings, we are given N labeled training",0,,False
queries,0,,False
"(q(n), X (n), Y (n)) N , where X (n) ,",0,,False
"n,1",0,,False
"x(1n), · · · , x(Mnn)",0,,False
and,0,,False
"Y (n) ,",0,,False
"1(n), · · · ,",0,,False
(n) Mn,0,,False
are the query-document feature set and,0,,False
"the relevance label 1 set for documents retrieved by query q(n),",0,,False
"respectively; and Mn is the number of the candidate documents {d1, · · · , dMn } retrieved by query q(n).",0,,False
3.1 Ranking as MDP,0,,False
"e process of document ranking can be formalized as an MDP, in which the construction of a document ranking can be considered as a sequential decision making where each time step corresponds to a ranking position and each action selects a document for the corresponding position. e model, referred to as MDPRank, can be is represented by a tuple S, A, T , R,  composed by states, actions, transition, reward, and policy, which are respectively de ned as follows:",0,,False
"States S is a set of states which describe the environment. In ranking, the agent should know the ranking position as well as the the candidate document set that the agent can choose from. us, at time step t, the state st is de ned as a pair [t, Xt ] where Xt is the remaining documents for ranking.",0,,False
"Actions A is a discrete set of actions that an agent can take. e set of possible actions depends on the state st , denoted as A(st ). At the time step t, at  A(st ) selects a document xm(at )  Xt for the ranking position t + 1, where m(at ) is the index of the document selected by at .",0,,False
"Transition T (S, A) is a function T : S × A  S which maps a state st into a new state st+1 in response to the selected action at . Choosing an action at means removing the document xm(at ) out of the candidate set, as shown in the following equation:",0,,False
"st +1 ,"" T ([t, Xt ], at ) "","" [t + 1, Xt \ {xm(at )}],""",0,,False
"Reward R(S, A) is the immediate reward, also known as reinforcement. In ranking, the reward can be considered as an evaluation of the quality of the selected document. It is natural to de ne the reward function on the basis of the IR evaluation measures. In this paper, we de ne the reward received in responds to choosing action at as the promotion of the DCG [3]:",0,,False
"2 m(at ) - 1 t , 0",0,,False
"RDCG(st , at ) ,",0,,False
2 m(at ) -1 log2(t +1),0,,False
"t>0 ,",0,,False
(1),0,,False
where m(at ) is the relevance label of the selected document dm(at ). Note that the calculation of each reward corresponds the DCG at,0,,False
"each ranking position, which enables the learning of MDPRank to",0,,False
fully utilize the DCG values calculated at all ranking positions.,0,,False
"Policy  (a|s) : A × S  [0, 1] describes the behaviors of the",0,,False
"agent, which is a probabilistic distribution over the possible actions.",0,,False
"1 e relevance labels are a set of ranks {r1, · · · , r } and there exists a total order between the ranks: r r -1 · · · r1 where ` ' denotes a preference relationship.",0,,False
946,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Agent,0,,False
"state st ,"" [t, Xt]""",0,,False
"reward rt ,"" RDCG(st 1, at 1)""",0,,False
rt+1,0,,False
action at  (at|st; w),0,,False
Environment,0,,False
st+1,0,,False
Figure 1: e agent-environment interaction in MDP.,0,,False
"Speci cally, the policy of MDPRank calculates the probabilities of selecting each of the documents for the current ranking position:",0,,False
" (at |st ; w) ,",0,,False
exp wT xm(at ) a A(st ) exp wT xm(a),0,,False
",",0,,False
(2),0,,False
where w  RK is the model parameters whose dimension is same,0,,False
with the ranking feature.,0,,False
Construction of a document ranking given a training query can,0,,False
"be formalized as follows. Given a user query q, the set of M retrieved",0,,False
"documents X , and the corresponding human labels Y , the system",0,,False
"state is initialized as s0 ,"" [0, X ]. At each of the time steps t "",",0,,False
"0, · · · , M - 1, the agent receives the state st ,"" [t, Xt ], chooses an""",0,,False
"action at which selects the document xm(at ) from the document set and places it to the rank t. Moving to the next step t + 1, the",0,,False
"state becomes st+1 ,"" [t + 1, Xt+1]. In the same time, on the basis""",0,,False
"of the human labels m(at ) for the selected documents, the agent receives immediate reward rt+1 ,"" R(st , at ). e process is repeated""",0,,False
until all of the M documents are selected. Figure 1 illustrates the,0,,False
agent-environment the interaction in MDPRank.,0,,False
"In the online ranking/testing phase, there is no reward available",0,,False
because there exists no labels. e model fully trust the learned,0,,False
policy  and choose the action with maximal probability at each,0,,False
"time step. us, the online ranking is equivalent to assigning scores f (x; w) , wT x to all of the retrieved documents and sorting the",0,,False
documents in descending order.,0,,False
3.2 Learning with policy gradient,1,ad,True
"MDPRank has parameters w to determine. In this paper, we propose to learn the parameters with the REINFORCE [10, 12], a widely used policy gradient algorithm in reinforcement learning. e goal of the learning algorithm is to maximize the expected long term return from the beginning:",1,ad,True
"(w) , EZw [G(Z)]",0,,False
"where Z ,"" {xm(a0), xm(a1) · · · , xm(aM-1)} is the ranking list sampled from the MDPRank model and G(Z) is the long-term return""",0,,False
"of the sampled ranking list, which is de ned as the discounted sum",0,,False
of the rewards:,0,,False
M,0,,False
"G(Z) ,  k-1rk .",0,,False
"k ,1",0,,False
Note that the de nition of G is identical to the IR evaluation measure,0,,False
"DCG if  ,"" 1. us, the learning of MDPRank is actually directly""",0,,False
optimizing the evaluation measure.,0,,False
"According to REINFORCE algorithm, the gradient w (w) can be calculated as",1,ad,True
"w (w) ,""  t Gt w log w(at |st ; w),""",0,,False
Algorithm 1 MDPRank learning,0,,False
"Input: Labeled training set D ,"" {(q(n), X (n), Y (n))}nN"",""1, learning rate , discount factor  , and reward function R""",0,,False
Output: w,0,,False
1: Initialize w  random values,0,,False
2: repeat,0,,False
"3: w , 0",0,,False
"4: for all (q, X, Y )  D do",0,,False
5:,0,,False
"(s0, a0, r1, · · · , sM-1, aM-1, rM )  SampleAnEpisode(w, q, X, Y , R)",0,,False
"{Algorithm (2), and M , |X |}",0,,False
6:,0,,False
"for t , 0 to M - 1 do",0,,False
7:,0,,False
Gt ,0,,False
"M -t k ,1",0,,False
 k -1rt +k,0,,False
{Equation,0,,False
(3)},0,,False
8:,0,,False
w  w +  t Gt w log  (at |st ; w) {Equation (4)},0,,False
9:,0,,False
end for,0,,False
10: end for,0,,False
11: w  w + w,0,,False
12: until converge,0,,False
13: return w,0,,False
Algorithm 2 SampleAnEpisode,0,,False
"Input: Parameters w, q, X , Y , and R Output: An episode",0,,False
"1: Initialize s0  [0, X ], M  |X |, and episode E   2: for t ,"" 0 to M - 1 do 3: Sample an action at  A(st )   (at |st ; w) {Equation (2)} 4: rt +1  R(st , at ){Equation (1), calculation on the basis of Y } 5: Append (st , at , rt +1) at the end of E 6: State transition st +1  t + 1, X \ {xm(at ) } 7: end for 8: return E "","" (s0, a0, r1, · · · , sM-1, aM-1, rM )""",0,,False
"where is further be estimated with Monte-Carlo sampling and shown in Algorithm 1. Algorithm 2 shows the procedure of sampling an episode for Algorithm 1. Speci cally, Algorithm 1 updates the parameters via Monte-Carlo stochastic gradient ascent. At each iteration, an episode (consisting a sequence of M states, actions, and rewards) is sampled according to current policy. en, at each time step t of the sampled episode, the model parameters are adjusted according to the gradients of the parameters w log  (at |st ; w), scaled by the step size , the discount rate  t , and the long-term return of the sampled episode starting from t, denoted as Gt :",1,ad,True
M -t,0,,False
"Gt ,",0,,False
 k-1rt +k .,0,,False
(3),0,,False
"k ,1",0,,False
"e gradient of w at time step t is w log  (at |st ; w), which the direction that most increase the probability of repeating the action",1,ad,True
"at on future visits to state st , and is de ned as",0,,False
w log  (at |st ; w),0,,False
",",0,,False
w (at |st ; w)  (at |st ; w),0,,False
", xm(at ) -",0,,False
a At xm(a) exp wT xm(a) a At exp wT xm(a),0,,False
.,0,,False
(4),0,,False
"Intuitively, the se ing of Gt let the parameters move most in the directions that favor actions that yield the highest return. Note",0,,False
"that if  ,"" 1, G0 is exactly the evaluation measure calculated at the nal rank of the document list, i.e., DCG@M.""",0,,False
947,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
4 EXPERIMENTS,0,,False
4.1 Experimental settings,0,,False
We conducted experiments to test the performances of MDPRank using two LETOR benchmark datasets [7]: OHSUMED and Million,0,,False
"ery track of TREC2007 (MQ2007). Each dataset consists of queries, corresponding retrieved documents and human judged labels.",1,TREC,True
"e possible relevance labels are relevant, partially relevant, and not relevance. Following the LETOR con guration, we conducted 5-fold cross-validation experiments on these two datasets. e results reported were the average over the ve folds. In all of the experiments, we used LETOR standard features and set  ,"" 1 for making the algorithm to directly optimize DCG. NDCG at position of 1, 3, 5 and 10 were used for evaluation. We compared the proposed MDPRank with several state-of- the-art baselines in LETOR, including pairwise methods of RankSVM, listwise methods of ListNet, and methods that directly optimizing evaluation measures: AdaRank-MAP, AdaRank-NDCG [14], and SVMMAP [17] .""",1,MAP,True
4.2 Experimental results,0,,False
"Table 1 and Table 2 report the performances of MDPRank and all of the baseline methods on OSHUMED and MQ2007, respectively, in terms of NDCG at the positions of 1, 3, 5, and 10. Boldface indicates the highest score among all runs. From the results, we can see that MDPRank outperformed all of the baselines, except on MQ2007 in terms of NDCG@10. We conducted signi cant testing (t-test) on the improvements of MDPRank over the best baseline. e experimental results indicated that the improvements on OHSUMED are signi cant. e results show that MDPRank is e ective algorithm to directly optimize IR evaluation measures.",1,MQ,True
"One advantage of MDPRank is that it has the ability of utilizing evaluation measure calculated at all of the ranking positions (the rewards) as the supervision in training. To test the e ectiveness of this, we modi ed algorithm MDPRank so that the algorithm only utilizes the long term return of the whole episode for training, denoted as MDPRank(ReturnOnly). e modi cation actually controls the Algorithm 1 to execute line 7 and 8 only when t ,"" 0. From the results shown in Table 1 and Table 2, we can see that MDPRank(ReturnOnly) underperformed the original MDPRank, indicating that fully utilizing the evaluation measures calculated at all of the ranking positions is e ective for improving the ranking performances.""",1,ad,True
5 CONCLUSION,0,,False
"In this paper we have proposed to formalize learning to rank as an MDP and training with policy gradient, referred to as MDPRank. By de ning the MDP rewards on the basis of IR evaluation measure and adopting the REINFORCE algorithm for the optimization, MDPRank actually directly optimizes IR measures with Monte-Carlo stochastic gradient assent. Compared with existing learning to rank algorithms, MDPRank enjoys the advantage of fully utilizing the IR evaluation measures calculated at all of the ranking positions in the training phase. Experimental results based on LETOR benchmarks show that MDPRank cam outperform the state-of-the-art baselines. e experimental results also showed that utilizing the IR evaluation measures calculated at all of the ranking positions do help to improve the performances.",1,ad,True
Table 1: Ranking accuracies on OHSUMED dataset.,0,,False
Method,0,,False
RankSVM ListNet,0,,False
AdaRank-MAP AdaRank-NDCG,1,MAP,True
SVMMAP,1,MAP,True
MDPRank MDPRank(ReturnOnly),0,,False
NDCG@1 0.4958 0.5326 0.5388 0.5330 0.5229,0,,False
0.5925 0.5363,0,,False
NDCG@3 0.4207 0.4732 0.4682 0.4790 0.4663,0,,False
0.4992 0.4885,0,,False
NDCG@5 0.4164 0.4432 0.4613 0.4673 0.4516,0,,False
0.4909 0.46949,0,,False
NDCG@10,0,,False
0.4140 0.4410 0.4429 0.4496 0.4319,0,,False
0.4587 0.4591,0,,False
Table 2: Ranking accuracies on MQ2007 dataset.,1,MQ,True
Method,0,,False
RankSVM ListNet,0,,False
AdaRank-MAP AdaRank-NDCG,1,MAP,True
SVMMAP,1,MAP,True
MDPRank MDPRank(ReturnOnly),0,,False
NDCG@1 0.4045 0.4002 0.3821 0.3876 0.3853,0,,False
0.4061 0.4033,0,,False
NDCG@3 0.4019 0.4091 0.3984 0.4044 0.3899,0,,False
0.4101 0.4059,0,,False
NDCG@5 0.4072 0.4170 0.407 0.4102 0.3983,0,,False
0.4171 0.4113,0,,False
NDCG@10 0.4383,0,,False
0.4440 0.4335 0.4369 0.4187 0.4416 0.4350,0,,False
6 ACKNOWLEDGMENTS,0,,False
"e work was funded by the 973 Program of China under Grant No. 2014CB340401, National Key R&D Program of China under Grant No. 2016QY02D0405, the National Natural Science Foundation of China (NSFC) under Grants No. 61232010, 61472401, 61433014, 61425016, and 61203298, the Key Research Program of the CAS under Grant No. KGZD-EW-T03-2, and the Youth Innovation Promotion Association CAS under Grants No. 20144310 and 2016102.",0,,False
REFERENCES,0,,False
"[1] Soumen Chakrabarti, Rajiv Khanna, Uma Sawant, and Chiru Bha acharyya. 2008. Structured Learning for Non-smooth Ranking Losses. In Proceedings of SIGKDD. 88­96.",0,,False
"[2] Olivier Chapelle and Mingrui Wu. 2010. Gradient descent optimization of smoothed information retrieval metrics. Information Retrieval 13, 3 (2010), 216­235.",1,ad,True
"[3] Charles L.A. Clarke, Maheedhar Kolla, Gordon V. Cormack, Olga Vechtomova, Azin Ashkan, Stefan Bu¨ cher, and Ian MacKinnon. 2008. Novelty and Diversity in Information Retrieval Evaluation. In Proceedings of SIGIR. 659­666.",1,Novelty,True
"[4] Nathan Korda, Balazs Szorenyi, and Shuai Li. 2016. Distributed Clustering of Linear Bandits in Peer to Peer Networks. In Proceedings of ICML, 1301­1309.",0,,False
"[5] Shuai Li, Alexandros Karatzoglou, and Claudio Gentile. 2016. Collaborative Filtering Bandits. In Proceedings of SIGIR. 539­548.",0,,False
"[6] Jiyun Luo, Sicong Zhang, and Hui Yang. 2014. Win-win Search: Dual-agent Stochastic Game in Session Search. In Proceedings SIGIR. 587­596.",1,Session,True
"[7] Tao Qin, Tie-Yan Liu, Jun Xu, and Hang Li. 2009. LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval. Information Retrieval (2009).",0,,False
"[8] Filip Radlinski, Robert Kleinberg, and orsten Joachims. 2008. Learning Diverse Rankings with Multi-armed Bandits. In Proceedings of ICML. 784­791.",1,ad,True
"[9] Guy Shani, David Heckerman, and Ronen I. Brafman. 2005. An MDP-Based Recommender System. Machine Learning Research 6 (2005), 1265­1295.",0,,False
[10] Richard S. Su on and Andrew G. Barto. 2016. Reinforcement Learning: An Introduction (2nd ed.). MIT Press.,0,,False
"[11] Michael Taylor, John Guiver, Stephen Robertson, and Tom Minka. 2008. So Rank: Optimizing Non-smooth Rank Metrics. In Proceedings of WSDM. 77­86.",0,,False
"[12] Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8, 3 (1992), 229­256.",1,ad,True
"[13] Long Xia, Jun Xu, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2015. Learning Maximal Marginal Relevance Model via Directly Optimizing Diversity Evaluation Measures. In Proceedings of SIGIR. 113­122.",0,,False
[14] Jun Xu and Hang Li. 2007. AdaRank: A Boosting Algorithm for Information Retrieval. In Proceedings SIGIR. 391­398.,0,,False
"[15] Jun Xu, Tie-Yan Liu, Min Lu, Hang Li, and Wei-Ying Ma. 2008. Directly Optimizing Evaluation Measures in Learning to Rank. In Proceedings of SIGIR. 107­114.",0,,False
"[16] Jun Xu, Long Xia, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. 2017. Directly Optimize Diversity Evaluation Measures: A New Approach to Search Result Diversi cation. ACM TIST 8, 3 (2017), 41:1­41:26.",0,,False
"[17] Yisong Yue, omas Finley, Filip Radlinski, and orsten Joachims. 2007. A Support Vector Method for Optimizing Average Precision. In Proceedings of SIGIR. 271­278.",1,ad,True
"[18] Sicong Zhang, Jiyun Luo, and Hui Yang. 2014. A POMDP Model for Content-free Document Re-ranking. In Proceedings of SIGIR. 1139­1142.",0,,False
948,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Exploring the ery Halo E ect in Site Search,0,,False
Leading People to Longer eries,1,ad,True
Djoerd Hiemstra,0,,False
"University of Twente Enschede, e Netherlands",0,,False
hiemstra@cs.utwente.nl,0,,False
Claudia Hau,0,,False
"Del University of Technology Del , e Netherlands c.hau @tudel .nl",0,,False
Leif Azzopardi,0,,False
"University of Strathclyde Glasgow, Scotland",0,,False
leif.azzopardi@strath.ac.uk,0,,False
ABSTRACT,0,,False
"People tend to type short queries, however, the belief is that longer queries are more e ective. Consequently, a number of a empts have been made to encourage and motivate people to enter longer queries. While most have failed, a recent a empt -- conducted in a laboratory setup -- in which the query box has a halo or glow e ect, that changes as the query becomes longer, has been shown to increase query length by one term, on average. In this paper, we test whether a similar increase is observed when the same component is deployed in a production system for site search and used by real end users. To this end, we conducted two separate experiments, where the rate at which the color changes in the halo were varied. In both experiments users were assigned to one of two conditions: halo and no-halo. e experiments were ran over a y day period with 3,506 unique users submi ing over six thousand queries. In both experiments, however, we observed no signi cant di erence in query length. We also did not nd longer queries to result in greater retrieval performance. While, we did not reproduce the previous",1,ad,True
"ndings, our results indicate that the query halo e ect appears to be sensitive to performance and task, limiting its applicability to other contexts.",0,,False
KEYWORDS,0,,False
ery length; Halo; A/B testing; Research replication,0,,False
1 INTRODUCTION,1,DUC,True
"Providing a more detailed and richer description of an information need by issuing a longer query has been considered as a fundamental way for users to indicate to the system what is relevant [7]. However, most users tend to express very short ­ two to three ­ term queries [3, 6, 13]. As a result a number of e orts have been made which a empt to lead people to longer queries [2, 7, 8, 11]. It is commonly believed that longer queries (and thus a richer description of the user's underlying information need) will result in be er retrieval performance [7].",1,ad,True
"In this paper, we explore the query halo e ect, proposed in [2], where the search box glows and changes colour as the user types.",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro t or commercial advantage and that copies bear this notice and the full citation on the rst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permi ed. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci c permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 978-1-4503-5022-8/17/08. . . $15.00 DOI: 10.1145/3077136.3080696",1,ad,True
"In [2], Agapie et al. showed that the this halo e ect led people to enter longer queries in the context of complex Web search tasks. We now a empt to reproduce this nding in a di erent context: site search. Crucially, while [2] was performed in a laboratory se ing only, we a empt to reproduce the ndings in a live se ing with users issuing queries to ful ll their own information needs. To this end, we consider the following Research Questions:",0,,False
RQ1 Does the query halo e ect lead people to longer queries in a natural se ing? Or alternatively phrased: can we reproduce the ndings in [2] in a di erent context?,1,ad,True
RQ2 Does the assumption of longer queries being more effective hold in this se ing?,0,,False
"We empirically investigated these questions in a 50-day long A/B test setup implemented in a university site search engine. Our analyses are based on more than 6,000 submi ed queries in that time period. We nd that the query halo e ect did not entice people to submit longer queries: there was no signi cant di erence between the halo condition and no-halo condition across the two experiments performed. We also nd that longer queries, in this context, do not necessarily result in be er retrieval performance. We hypothesize that for the query halo e ect to take hold, the retrieval performance needs to be positively correlated with query length and the search task needs to be complex as in [2].",0,,False
2 BACKGROUND,0,,False
"Numerous studies have examined the length of queries submi ed by users to search systems in a number of di erent se ings. For Web-based queries the average number of terms is around 2.3 (2.35 Excite logs [13], 2.34 AOL logs [3], 2 (mode) MS logs [6]), while site search queries tend to be slightly shorter with a term length around 2.2 (2 (mode) UTennesse log [14] and 2.2 U.S. Government logs [9]). Since queries are typically short in the Web se ing, a driving hypothesis for leading people to longer queries is based on the strong assumption that: longer queries result in be er retrieval performance. However, in previous works [2, 7, 8, 11] this was not explicitly tested. In [4], Azzopardi studied the relationship between query length and retrieval performance (mean average precision) in the context of ad-hoc search on a number of TREC test collections using best match retrieval algorithms. In this simulated batch setup, performance does increase, but at a diminishing rate of return; queries of 2 to 3 terms in length result in the highest rate of return. With respect to the previous and present study on leading people to longer queries, the analysis con rmed that query length and performance are positively correlated, but only in a speci c context. Production search systems, however, o en use strategies beyond best match algorithms ­ typing more query terms o en reduces the number of results returned as terms are implicitly ANDed together.",1,Gov,True
981,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
So it is not clear whether the query-length assumption holds in such a se ing or for other tasks.,0,,False
"Karlgren and Franze´n [11] were one of the rst to try and lead people to longer queries in a laboratory setup by changing the query input design. ey modi ed the single-line query input box used by Web search engines and designed a query text box with multiple lines, such that the query terms would wrap as you typed ­ in the belief that this would illicit longer queries than the single-line query box. In the context of Web search, Karlgren and Franze´n found indeed that participants entered signi cantly longer queries using the multi-line input box. Belkin et al. [7] later hypothesized that this increase in query length was due to: (i) the larger perceived space and (ii) the visibility of the entire query (as in the single-line condition the query could be partially hidden if it is too long).",1,ad,True
"Belkin et al. [7] a empted to replicate Karlgren and Franze´n's nding in a slightly di erent context and also explored whether instructing users to enter questions as their query as opposed to key words, would lead to longer queries. In the context of a lab-based se ing (as part of the TREC interactive track), their results showed that when participants were in the ""instruction"" condition, they submi ed signi cantly longer queries. is is not too surprising, because turning a query for ""Gar eld"" into a question will require at least one more term, e.g. ""Who is Gar eld?"". Interestingly, they were unable to reproduce Karlgren and Franze´n's nding with respect to the di erent query box input modes. ey found no di erence between the multi-line and single line query box modes.",1,ad,True
"Belkin et al. [8] then followed up, by designing an experiment where the baseline interface was a multi-line query box and the experimental condition which used the same interface but included instructions to the participants: ""Information problem description (the more you say, the be er the results are likely to be)"". is instruction led the lab study participants in the experimental condition to enter on average two more query terms than the participants in the control condition.",0,,False
"Agapie et al. [2] hypothesized that a halo around the query box that re ects the length of the query being created could mitigate people's tendency to issue short queries. Speci cally, they further hypothesized that this halo e ect would ""nudge"" people to type in more query terms. In their experiments, they set up a 2x2 factorial design, where on one dimension they included a halo vs. no-halo, and on the other dimension they provided instructions vs. no instructions. In the instruction condition, the 61 lab study participants were told that, ""[the] system performs be er with longer queries."" Agapie et al. found the halo to lead the participants to construct signi cantly longer queries. However, drilling down, this was most pronounced when the halo was present, but no instructions (6.6 query terms on av.). When the halo and instructions were provided, they did not observe any signi cant increase in query length (4.5 query terms on av.). When only instructions were provided it resulted in longer queries (5.3) when compared to no halo and no instruction (4.2). In a subsequent experiment, they compared four conditions: no-halo, halo (pink to blue), inverted halo (blue to pink) and a static halo (blue). Again, they observed that the halo e ect (pink to blue) resulted in signi cantly longer queries than no halo or a static halo. However the inverted halo did not lead people to signi cantly longer queries.",1,ad,True
"In the aforementioned studies, performance, in terms of precisionrecall based measures, is not reported or considered, so it is actually unclear whether an increase in query length would translate into improved performance ­ and thus increased satisfaction for the user. In a recent theoretical cost-bene t analysis of query length and retrieval performance Azzopardi and Zuccon [5] show that in order for users to increase their query length either the retrieval performance needs to increase or the cost of entering terms needs to be reduced. In previous works on leading people to longer queries, neither the performance of the system is increased, nor is the cost of entering a query reduced. As such, their theory suggests that in practice larger query boxes or halo e ects, which do not modify either of these, are not going to lead to longer queries.",1,ad,True
3 REPRODUCING THE HALO EFFECT,1,DUC,True
"Reproducibility, the ability of an experimental study to be duplicated independently, recently gained renewed a ention in computer science and especially in information retrieval. e ACM's policy on Result and Artifact Review and Badging [10] distinguishes repeatability (same team, same experimental setup), replicability (di erent team, same experimental setup), and reproducibility (different team, di erent experimental setup). Ferro et al. [1] investigated the problems and approaches to reproducibility in information retrieval and various other sub- elds of computer science.",1,ad,True
"In line with those initiatives, our experiments aim to reproduce the work by Agapie et al. [2] in a live search engine. Experiments were run on the site search engine of the University of Twente1, a federated search engine [12] searching 35 resources including Google's site search, local courses, local news, the telephone directory, the university timetables, as well as results from the university's social media feeds, such as Facebook, Twi er and Flickr. Given a query2, the search engine returns ranked resource blocks with each block containing up to four (up to seven in the case of images) ranked items; each resource can only contribute a single block to a ranking. Figure 1 shows an example result page for the query ""library"" with three ranked blocks. Our query log records contain for each query the URLs of the search results that were clicked as well as the block rank of the clicked result. As the vast majority of queries yielded a single click, we computed the block rank MRR (the mean reciprocal rank over all submi ed queries in the respective condition) as our system's measure of retrieval performance.",0,,False
"e implementation uses the open source federated search engine Searsia3. Experiments were run as A/B tests where users were assigned randomly to either the control condition (the standard search box, labelled henceforth as no-halo condition) or the experimental condition (the search box with the query halo e ect, labelled as halo condition).",0,,False
"e query halo e ect was implemented as described by Agapie et al. [2]. e empty query input box has a pink (RGB #FF1493) halo that surrounds the text box. As the user types into the box, the halo begins to change color ­ becoming less and less pink, and then starts becoming more and more blue (RGB #3366CC). To adhere to",1,ad,True
"1h ps://utwente.nl/search 2In the 50-day period of our study, the ve most popular queries were minor, matlab, library, ces and eduroam. 3h p://searsia.org",0,,False
982,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Figure 3: Overview of the halo's full color spectrum. e extrema encode 0 terms (or 0 characters) on the le and 7 terms (or 22 characters) on the right.,0,,False
"Figure 1: Example block-based ranking. the university site's style, the halo surrounding the query box was slightly less wide4 than in [2]. Figure 2 shows an example of the halo e ect for queries of di erent lengths; Figure 3 shows the full color spectrum of the halo.",0,,False
"Figure 2: Example of the character transition halo e ect. In [2], the color was interpolated between pink and blue, with",0,,False
"queries of seven words or longer showing the bluest halo. While not explicitly noted in [2], the choice of colours seemed to be decided based on accessibility, as for example, red/pink to green may not have been distinguishable (due to red/green colour blindness).",0,,False
e transition from pink to blue was based on word boundaries.,0,,False
"4Speci cally, the query box's CSS property box-shadow contains a spread-radius of 3 pixels; the query box's border-color was set to the same color as the halo.",1,ad,True
"However to avoid it appearing mechanistic, a small (up to 100 millisecond delay) was randomly introduced.",0,,False
"In our work, we implemented two versions of the query halo: (i) a term-based transition, as done in [2] where a er seven terms the halo was bluest, and (ii) a character-based transition query halo box, where the transition was based on characters and moved more smoothly from pink to blue, where a er 22 characters the halo was the most bluish. is version shows a more immediate, but less obvious, change of colour as characters are typed.",0,,False
4 RESULTS,0,,False
4.1 Experiment 1: term halo vs. no-halo,0,,False
"e rst experiment ran between January 6, 2017 and January 31, 2017. In this period, in the no-halo condition, 884 di erent users submi ed 1, 623 queries (1, 301 unique). Assigned to the halo condition were 803 users who issued 1, 367 queries (1, 122 unique).",0,,False
"Table 1 (top) contains the results of this experiment. Users in the control condition submit on average 2.18 query terms, users in the halo condition match this almost perfectly with an average query length of 2.16. Spli ing the users according to their experience with the search system ­ users submi ed a single query across the three weeks (single query users) vs. users submi ing two or more queries (2+ query users) ­ does not yield a di erent picture. Figure 4 shows the query distribution across query term length, the halo condition does not lead users to change their querying behaviour with respect to query length. More than 70% of all issued queries contain either 1 or 2 terms.",1,ad,True
4.2 Experiment 2: character halo vs. no-halo,0,,False
"e second experiment ran between February 1, 2017 and February 25, 2017. In the no-halo condition, 857 di erent users submi ed 1, 395 queries (1, 157 unique). In the halo condition, 919 users issued 1, 641 queries of which 1, 314 were unique. In Table 1 the results of this experiment are summarized. e more dynamic change of the halo has no e ect: users in both the control and experimental conditions submit queries of similar length. Overall, we conclude that the query halo e ect does not lead to longer queries in this context and live se ing.",1,ad,True
4.3 Longer queries lead to more clicks?,1,ad,True
"In order to evaluate the suitability of the assumption that longer queries perform be er, we compute the block MRR across all the 6, 026 queries in our 50-day log (combined over conditions and experiments). Figure 5 shows that in our production system the common assumption that longer queries perform be er does not hold: longer queries do not lead to more clicks on be er ranked result blocks, and thus do not lead to higher MRR. eries with 2 to 4 terms perform similarly. In order to verify the quality of the",1,ad,True
983,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 1: Overview of the average query length (and standard deviation) in terms (Experiment 1) and characters (Experiment 2). None of the di erences are signi cant.,0,,False
Experiment 1 (terms),0,,False
no-halo,0,,False
halo,0,,False
All queries Single query users 2+ query users,0,,False
2.18 (1.36) 2.06 (1.11) 2.25 (1.48),0,,False
2.16 (1.32) 2.15 (1.40) 2.17 (1.27),0,,False
Experiment 2 (characters),0,,False
no-halo,0,,False
halo,0,,False
All queries,0,,False
16.52 (11.81) 16.39 (10.18),0,,False
Single query users 16.44 (12.03) 15.39 (10.14),0,,False
2+ query users,0,,False
16.58 (11.65) 17.00 (10.16),0,,False
Query Distribution,1,Query,True
0.4 no halo,0,,False
0.35,0,,False
halo,0,,False
0.3,0,,False
0.25,0,,False
0.2,0,,False
0.15,0,,False
0.1,0,,False
0.05,0,,False
0,0,,False
1,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
Query Length (in terms),1,Query,True
Figure 4: Number of queries issued for each condition in Experiment 1.,0,,False
0.65,0,,False
0.6,0,,False
Block MRR,0,,False
0.55,0,,False
0.5,0,,False
0.45,0,,False
0.4,0,,False
1,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
Query Length (in terms),1,Query,True
Figure 5: Mean reciprocal rank (block-based) computed over all queries in our log.,0,,False
"system, in Figure 6 we plot the distribution of search result across the ranked blocks: the vast majority (80%) of clicks appear within the rst block as one would expect for a functioning production system.",0,,False
Probability of Click,0,,False
1,0,,False
0.8,0,,False
0.6,0,,False
0.4,0,,False
0.2,0,,False
0,0,,False
1,0,,False
2,0,,False
3,0,,False
4,0,,False
5,0,,False
6,0,,False
7,0,,False
Rank,0,,False
Figure 6: Frequency of search result clicks on ranked blocks computed over all queries in our log.,0,,False
5 CONCLUSIONS,0,,False
"In this paper, we a empted to reproduce the ""query halo"" e ect observed (in a lab se ing over complex search tasks) by Agapie et al. [2] in the context of site search with users serving there own information needs. We were not able to reproduce the e ect, instead observing no di erences in the control vs. experimental conditions. Furthermore, in contrast to a common assumption, we found li le di erence in retrieval performance across query length (between 2-4 terms). ese results motivate further work in unpicking the complexities and relationships between query length, performance, task and motivators.",1,ad,True
ACKNOWLEDGMENTS,0,,False
We are grateful to Anne Heining of U. Twente Media & Communication for supporting the A/B tests.,0,,False
REFERENCES,0,,False
"[1] Nicola Ferro and Norbert Fuhr and Kalervo Ja¨rvelin and Noriko Kando and Ma hias Lippold and Justin Zobel. 2016. Increasing Reproducibility in Information Retrieval: Findings from the Dagstuhl Seminar on Reproducibility of Data-Oriented Experiments in e-Science. SIGIR Forum 50, 1, 68­82.",0,,False
"[2] Elena Agapie, Gene Golovchinsky, and Pernilla Qvarfordt. 2013. Leading people to longer queries. In Proceedings of SIGCHI'13. 3019­3022.",1,ad,True
[3] Avi Arampatzis and Jaap Kamps. 2008. A Study of ery Length. In Proceedings of SIGIR'08. 811­812.,0,,False
[4] Leif Azzopardi. 2009. ery Side Evaluation: An Empirical Analysis of E ectiveness and E ort. In Proceedings of SIGIR'09. 556­563.,0,,False
[5] Leif Azzopardi and Guido Zuccon. 2016. An Analysis of the Cost and Bene t of Search Interactions. In Proceedings of ICTIR'16. 59­68.,0,,False
"[6] Peter Bailey, Ryen W. White, Han Liu, and Giridhar Kumaran. 2010. Mining Historic ery Trails to Label Long and Rare Search Engine eries. ACM Trans. Web 4, 4, Article 15.",0,,False
"[7] Nicholas Belkin, Colleen Cool, Judy Jeng, A. Keller, Diane Kelly, Jay Kim, Hyuk Lee, M.-C. Tang, and X.-J. Yuan. 2001. Rutgers TREC 2001 Interactive Track Experience. In e 10th text retrieval conference (TREC).",1,TREC,True
"[8] Nicholas Belkin, Diane Kelly, G. Kim, Jay Kim, Hyuk Lee, G. Muresan, M.-C. Tang, X.-J. Yuan, and Colleen Cool. 2003. ery Length in Interactive Information Retrieval. In Proceedings of SIGIR'03. 205­212.",0,,False
"[9] Michael Chau, Xiao Fang, and Olivia R. Liu Sheng. 2005. Analysis of the ery Logs of a Web Site Search Engine. J. Am. Soc. Inf. Sci. Technol. 56, 13, 1363­1376.",0,,False
[10] Association for Computing Machinery. 2016. Result and Artifact Review and Badging. h ps://www.acm.org/publications/policies/artifact-review-badging.,1,ad,True
"[11] Kristofer Franze´n and Jussi Kalgren. 1997. Verbosity and interface design. In SICS Technical Report: T2000:04, Retrieved online at: h p://soda.swedish-ict.se/ 2623/2/irinterface.pdf on May 11, 2013.",0,,False
"[12] Milad Shokouhi and Luo Si. 2011. Federated search. Foundations and Trends in Information Retrieval 5, 1 (2011), 1­102.",1,ad,True
"[13] Craig Silverstein, Hannes Marais, Monika Henzinger, and Michael Moricz. 1999. Analysis of a very large Web search engine query log. SIGIR Forum 33, 1, 6­12.",0,,False
"[14] Peiling Wang, Michael W. Berry, and Yiheng Yang. 2003. Mining Longitudinal Web eries: Trends and Pa erns. J. Am. Soc. Inf. Sci. Technol. 54, 8, 743­758.",0,,False
984,0,,False
,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Enhancing Recurrent Neural Networks with Positional Attention for Question Answering,0,,False
"Qin Chen1, Qinmin Hu1, Jimmy Xiangji Huang2, Liang He1,3 and Weijie An1",0,,False
"1Department of Computer Science & Technology, East China Normal University, Shanghai, China 2Information Retrieval & Knowledge Management Research Lab, York University, Toronto, Canada",1,ad,True
"3Shanghai Engineering Research Center of Intelligent Service Robot, Shanghai, China",0,,False
"{qchen,wjan}@ica.stc.sh.cn,{qmhu,lhe}@cs.ecnu.edu.cn,jhuang@yorku.ca",0,,False
ABSTRACT,0,,False
"Attention based recurrent neural networks (RNN) have shown a great success for question answering (QA) in recent years. Although significant improvements have been achieved over the nonattentive models, the position information is not well studied within the attention-based framework. Motivated by the effectiveness of using the word positional context to enhance information retrieval, we assume that if a word in the question (i.e., question word) occurs in an answer sentence, the neighboring words should be given more attention since they intuitively contain more valuable information for question answering than those far away. Based on this assumption, we propose a positional attention based RNN model, which incorporates the positional context of the question words into the answers' attentive representations. Experiments on two benchmark datasets show the great advantages of our proposed model. Specifically, we achieve a maximum improvement of 8.83% over the classical attention based RNN model in terms of mean average precision. Furthermore, our model is comparable to if not better than the state-of-the-art approaches for question answering.",1,corpora,True
1 INTRODUCTION,1,DUC,True
"Recurrent neural networks (RNN) have been widely used for question answering (QA) due to its good performance [8­10]. In the RNN based QA models, each word in a question or an answer sentence is represented with a hidden vector first. Then, all the hidden vectors are aggregated for sentence representations. Afterwards, the best answer is selected from a candidate answer pool according to the sentence similarity.",0,,False
"One major challenge in RNN is how to aggregate the hidden vectors for sentence representations. Recently, the attention mechanism has shown its effectiveness for the attentive sentence representations in many NLP tasks including QA [1, 8, 9, 14]. In particular, a weight is automatically generated for each word via attention, and the sentence is represented as the weighted sum of the hidden vectors. Various attention mechanisms have been proposed in previous studies. In [8], the attentive weights of the words in answers relied on the hidden representation of questions. Santos et al. [1] proposed",0,,False
"Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '17, August 07-11, 2017, Shinjuku, Tokyo, Japan © 2017 Association for Computing Machinery. ACM ISBN 978-1-4503-5022-8/17/08. . . $15.00 https://doi.org/10.1145/3077136.3080699",1,ad,True
"a two-way attention mechanism, where the attentive weights for the question (answer) were influenced by the answer (question) representation according to the word-by-word interaction matrix. However, these attentions relied on the hidden vectors, which may excessively concern the words near the end of the sentence due to the abundant semantic accumulation over the word sequence in RNN. To alleviate the attention bias problem, Wang et al. [9] proposed three inner attention methods, which added the attention information before the hidden representations and achieved the state-of-the-art performance in QA.",1,ad,True
"To the best of our knowledge, all the previous attention mechanisms neglect the positional context, which has been extensively studied for performance boosting in information retrieval (IR). In [3] and [17], the occurrence positions of the query terms were modeled with various kernels and then integrated into traditional IR models to enhance the retrieval performance. Inspired by the effectiveness of the positional context in IR, we attempt to incorporate it into classical attentions to enhance the performance of RNN based QA. Specifically, it is assumed that if a word in the question (i.e., question word) occurs in an answer sentence, it will have an influence on the neighboring context. In other words, the neighboring words should be given more attention than those far away since they may contain more question relevant information. Based on this assumption, we propose a Positional Attention based RNN (RNN-POA) model, which models the position-aware influence of the question word for answers' attentive representations. To be specific, we first present a position-aware influence propagation strategy, in which the influence of the question word is propagated to other positions in the answer sentence by a distance-sensitive kernel. Then, a position-aware influence vector for each word is generated in the hidden space, according to the accumulated influence propagated by all the question words occurring in the answer. After that, the position-aware influence vector is integrated into the classical attention mechanism for answers' attentive representations. We perform experiments on two publicly available benchmark datasets, namely TREC-QA and WikiQA. The results show that our positional attention can significantly outperform the classical attention which does not involve any position information. Furthermore, the performance of our proposed RNN-POA model is comparable to the state-of-the-art approaches for question answering.",1,ad,True
"The main contributions of our work are as follows: (1) as far as we know, it is the first attempt to investigate the effectiveness of the positional context for answers' attentive representations; (2) we propose a positional attention based RNN model, which has been proved to be effective to boost the QA performance; (3) our positional attention approach can help alleviate the attention bias",0,,False
993,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"problem by utilizing the position information of the question words, instead of the semantic accumulated hidden representations.",1,ad,True
2 PROPOSED MODEL,0,,False
2.1 Framework of RNN-POA,0,,False
"Figure 1(b) shows the framework of our positional attention based RNN (RNN-POA) model for answer representations. To have a better comparison, the classical attention based question representation is also shown in Figure 1(a). We adopt the bidirectional long shortterm memory (BLSTM) [2] model for sentence modeling, which takes the pre-trained word embeddings as the input, and generates the hidden vectors by recurrent updates [2].",1,ad,True
"To obtain the composite representations of the questions, we adopt the classical attention used in [14], which solely relies on the hidden vectors for the attentive weight generation. Regarding to the answer, we propose a positional attention approach, and perform additional steps upon the classical attention as follows: (1) find the occurrence positions of the question words in an answer sentence; (2) propagate the influence of the question words to other positions with our position-aware influence propagation strategy; (3) generate the position-aware influence vector for each word in the answer sentence according to the propagated influence; (4) incorporate the position-aware influence vector into the classical attention mechanism.",1,ad,True
"With the attentive representations of both questions and answers, various similarity functions can be utilized to measure the relevance between them. We utilize the Manhattan distance similarity function with l1 norm (Formula (1)), which performs slightly better than the other alternatives such as cosine similarity as indicated in [5]:",0,,False
"sim(rq , ra ) , exp(-||rq - ra ||1)",0,,False
(1),0,,False
Hidden Vector,0,,False
ra,0,,False
Positional Attention,0,,False
rq Attention,0,,False
Hidden Vector,0,,False
Word Embedding,0,,False
Word Embedding Find occurrence positions,0,,False
Word Question,0,,False
Position-aware Influence Propagation,0,,False
Answer,0,,False
Word,0,,False
Question,0,,False
(a),0,,False
Position-aware Influence Vector (b),0,,False
Figure 1: (a) question representation with classical attention based RNN; (b) answer representation with positional attention based RNN.,0,,False
2.2 Position-aware Influence Propagation,0,,False
"Based on our previous assumption, a question word will have an influence on the neighboring context if it occurs in an answer sentence. Here we model the position-aware influence propagation with the Gaussian kernel, which has been proved to be effective for",0,,False
"position modeling in IR [3, 4, 17]:",0,,False
Kernel (u),0,,False
",",0,,False
exp(,0,,False
-u2 2 2,0,,False
),0,,False
(2),0,,False
where u is the distance between the question word and the current,0,,False
"word,  is a parameter which restricts the propagation scope, and",0,,False
Kernel (u) denotes the obtained influence corresponding to the,0,,False
distance of u based on the kernel.,0,,False
Note that the position-aware influence is diminishing when the,0,,False
"distance increases. In particular, when u ,"" 0 (i.e., the current word""",0,,False
"is exactly a question word), the maximum propagated influence is",0,,False
"obtained. As to the propagation scope  , the optimal value may vary",0,,False
"from words to words. In this paper, we apply a constant  value",0,,False
"for all question words, and focus on incorporating the positional",1,corpora,True
context into attentions.,0,,False
2.3 Position-aware Influence Vector,0,,False
"In this section, in order to model the influence in a high-dimensional space for attentions, we will demonstrate how to obtain the positionaware influence vector for each word in an answer sentence. First, we assume that the influence for a specific distance follows the Gaussian distributions over the hidden dimensions. Then, an influence base matrix K is defined based on the assumption, where each column denotes the influence base vector corresponding to a specific distance. More formally, each element of K is defined as:",0,,False
"K(i, u)  N (Kernel (u),  )",0,,False
(3),0,,False
"where K(i, u) denotes the influence corresponding to the distance",0,,False
"of u in the i's dimension, and N is the normal density with an",0,,False
expected value of Kernel (u) and standard deviation of  .,0,,False
"With the influence base matrix, the influence vector for a word",0,,False
at a specific position is obtained by accumulating the influence of,0,,False
all the question words occurring in the answer:,0,,False
"pj , Kcj",0,,False
(4),0,,False
"where pj denotes the accumulated influence vector for the word at position j, and cj is a distance count vector which measures the",0,,False
"count of question words with various distances. Specifically, for the",0,,False
"word at position j, the count of question words with a distance of",0,,False
"u, namely cj (u), is calculated as:",0,,False
"cj (u) , [(j - u)  pos (q)] + [(j + u)  pos (q)] (5)",0,,False
q Q,0,,False
"where Q represents a question containing multiple question words, q is a word in Q, pos (q) denotes the set of q's occurrence positions in an answer sentence, and [·] is an indicator function which equals",0,,False
to 1 if the condition satisfies and otherwise equals to 0.,0,,False
2.4 Positional Attention,0,,False
"In most previous attention mechanisms, the attentive weight of",0,,False
"a word relies on the hidden representations, while the position",0,,False
"information is not well investigated. In this section, we propose",0,,False
"a positional attention approach, which incorporates the position-",1,corpora,True
aware influence of the question words into answers' attentive rep-,0,,False
"resentations. Specifically, the attentive weight of a word at position j in the answer sentence is formulated as:",0,,False
"j ,",0,,False
"exp(e (hj , pj ))",0,,False
l k,0,,False
",1",0,,False
exp,0,,False
(e,0,,False
(hk,0,,False
",",0,,False
pk,0,,False
),0,,False
),0,,False
(6),0,,False
994,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
"where hj is the hidden vector at position j based on RNN, pj is the accumulated position-aware influence vector obtained by Formula (4), l denotes the sentence length, and e (·) is a score function which measures the word importance based on the hidden vector and the",0,,False
"position-aware influence vector. More formally, the score function",0,,False
is defined as:,0,,False
"e (hj , pj ) , vT tanh(WH hj + WP pj + b)",0,,False
(7),0,,False
"where WH and WP are matrices, b is a bias vector, tanh is the hyperbolic tangent function, v is a global vector and vT denotes its",0,,False
"transpose. WH , WP , b and v are the parameters. With the obtained attentive weights, an answer sentence is rep-",0,,False
resented by the weighted sum of all the hidden vectors:,0,,False
l,0,,False
"ra , j hj",0,,False
(8),0,,False
"j ,1",0,,False
3 EXPERIMENTS,0,,False
3.1 Experimental Setup,0,,False
"Datasets and Evaluation Metrics. We conduct experiments on two public question answering datasets: TREC-QA and WikiQA. TREC-QA was created by Wang et al. [11] based on the TREC QA track data. WikiQA [13] is an open domain question-answering dataset in which all answers are collected from the Wikipedia. Each dataset is split into 3 parts, i.e., train, dev and test, and the statistics are presented in Table 1. To evaluate the model performance, we adopt the mean average precision (MAP) and mean reciprocal rank (MRR), which are the primary metrics used in QA [1, 9].",1,TREC,True
"Table 1: Dataset Statistics. ""Avg QL"" and ""Avg AL"" denote the average length of questions and answers.",0,,False
Dataset,0,,False
# of quetions Avg QL (train/dev/test) (train/dev/test),0,,False
Avg AL (train/dev/test),0,,False
TREC-QA 1162/65/68 7.57/8.00/8.63 23.21/24.9/25.61 WikiQA 873/126/243 7.16/7.23/7.26 25.29/24.59/24.59,1,TREC,True
"Parameter Settings. For the word embeddings, we use the 100dimensional GloVe [6] word vectors1. The parameters in BLSTM are shared between questions and answers, which has been shown to be effective to improve the performance [9]. The dimension of the hidden vectors and the position-aware influence vectors is set to 50. Regarding to the propagation scope  (in Formula (2)), we investigate a list of values ranging from 5 to 55 with an interval of 10. The value of  (in Formula (3)) is empirically set to 0.1. We adopt the cross-entropy loss as the training objective, and utilize the Adadelta [16] algorithm for parameter update. The optimal parameters are obtained based on the best MAP performance on the development (dev) set.",1,ad,True
3.2 Effect of Positional Attention,0,,False
"To investigate the effect of our positional attention approach, two basic baselines which do not involve the position information, namely average pooling (""AVG"") [10] and the classical attention (""ATT"") used in [14], are integrated into the BLSTM based RNN model for",0,,False
1 http://nlp.stanford.edu/data/glove.6B.zip,0,,False
"comparisons. Table 2 shows the performance of various models. Statistical significant tests are performed based on the paired t-test at the 0.05 level. The symbols as  and represent significant improvements over ""AVG"" and ""ATT"" respectively. We observe that the classical attention mechanism slightly outperforms the average pooling method by capturing part of the informative words in answers. However, it does not pay specifical attention to the question words and their surrounding context, which loses some useful information for question answering. In our positional attention approach, the importance of the question word and the surrounding context is explicitly highlighted via the position-aware influence propagation of the question words. Therefore, we can achieve significant improvements over the two baselines on both QA datasets, and the maximum improvement is as high as 8.83% in terms of MAP.",1,MAP,True
Table 2: Performance of various models.,0,,False
Model,0,,False
TREC-QA MAP MRR,1,TREC,True
RNN-AVG 0.7064,0,,False
RNN-ATT 0.7180 RNN-POA 0.7814*,0,,False
0.8086,0,,False
0.8121 0.8513*,0,,False
WikiQA MAP MRR,1,Wiki,True
0.6889,0,,False
0.6961 0.7212*,0,,False
0.6999,0,,False
0.7085 0.7312*,0,,False
3.3 Performance Comparisons,0,,False
"To further evaluate the effectiveness of our proposed model, we compare it with the recent work in QA. Table 3 and Table 4 summarize the results on TREC-QA and WikiQA respectively. For TREC-QA, five strong baselines are used for comparisons: (1) a combination of the BLSTM model and BM25 model [10]; (2) inner attention based RNN models which added the attention information before the hidden representations [9]; (3) a convolutional neural network (CNN) based architecture using both the hidden features and the statistical features for ranking [7]; (4) a learning-to-rank method which leveraged the word alignment features and lexical features for ranking [12]; (5) an extended LSTM framework which incorporated CNN and built the attention matrix after sentence representations [8]. As to WikiQA, in addition to [8] and [9] mentioned above, we make a comparison with other two strong baselines: (1) a bigram CNN model with average pooling [13]; (2) a CNN model which used an interactive attention matrix for the attentive representations [15].",1,TREC,True
"It is observed that we achieve the new state-of-the-art performance on TREC-QA in terms of both MAP and MRR. Regarding to WikiQA, our RNN-POA model outperforms all the strong baselines except the inner attention based RNN models [9]. This validates our previous assumption that the words close to the question words should be given more attention than those far away. In addition, it has been proved to be effective for incorporating the positional context into answers' attentive representations.",1,TREC,True
3.4 Investigation of Propagation Scope ,0,,False
"The parameter  (in Formula (2)) controls the influence propagation scope of the question words. For a certain distance, the propagated position-aware influence increases when  grows. Figure 2 plots the MAP and MRR metrics over a set of  values ranging from 5 to 55 with a step of 10. We observe that the general tendency for each",1,MAP,True
995,0,,False
Short Research Paper,0,,False
"SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan",0,,False
Table 3: Performance comparisons on TREC-QA. The work marked with  used the cleaned dataset.,1,TREC,True
System,0,,False
MAP MRR,1,MAP,True
Wang and Nyberg (2015) [10] Wang et al. (2016) [9]  Severyn and Moschitti (2015) [7] Wang and Ittycheriah (2015) [12] Santos et al. (2016) [8],0,,False
0.7134 0.7369 0.7459 0.7460 0.7530,0,,False
0.7913 0.8208 0.8078 0.8200 0.8511,0,,False
RNN-POA,0,,False
0.7814 0.8513,0,,False
Table 4: Performance comparisons on WikiQA,1,Wiki,True
System,0,,False
MAP MRR,1,MAP,True
Yang et al. (2015) [13] Santos et al. (2016) [8] Yin et al. (2015) [15] Wang et al. (2016) [9],0,,False
0.6520 0.6886 0.6921 0.7341,0,,False
0.6652 0.6957 0.7108 0.7418,0,,False
RNN-POA,0,,False
0.7212 0.7312,0,,False
"evaluation metric is similar. Specifically, the performance increases when  grows at first. Then, it decreases and tends to be stable when  becomes larger. On the whole, a  value between 15 and 35 is recommended to be a reliable setting in our experiments.",0,,False
(a) MAP,1,MAP,True
(b) MRR,0,,False
Figure 2: Impact of the propagation scope ,0,,False
3.5 A Case Study,0,,False
"To have an intuitive understanding of our proposed RNN-POA model, we draw a word heatmap for a case based on the classical attention and our positional attention respectively in Figure 3. Obviously, to answer this question, we should focus on the words ""George Warrington"". However, the classical attention cares more about some irrelevant words such as ""market"". Although these words have some semantic relations with the words in the question (e.g., the word ""market"" co-occurs frequently with the words ""chief "" and ""executive""), the semantically related words are not necessarily useful for question answering. In contrast, our positional attention approach concerns more about the question words such as ""Amtrak"", ""president"", ""chief "" and ""executive"", as well as the surrounding context such as ""George Warrington"", which provides more valuable clues for question answering. That is why our proposed positional attention approach can achieve much better performance than the classical attention method.",0,,False
4 CONCLUSIONS,0,,False
"In this paper, we propose a positional attention based RNN (RNNPOA) model, which incorporates the positional context of the question words into answers' attentive representations. The experimental results on two benchmark datasets show the overwhelming",1,NP,True
Q: who is the president or chief executive of amtrak?,0,,False
"Long term success here has to do with doing it right ,
getting it right and increasing market share. said RNN-ATT George Warrington, Amtrak's president and chief",0,,False
executive.,0,,False
"Long term success here has to do with doing it right ,
getting it right and increasing market share. said RNN-POA George Warrington, Amtrak's president and chief",0,,False
executive.,0,,False
Figure 3: An example of RNN-ATT and RNN-POA.,0,,False
"superiority of our proposed model over the basic baselines which do not incorporate any position information. Furthermore, compared with the state-of-the-art approaches in question answering, our proposed model can achieve a considerable performance by merely incorporating the position information into the classical attention mechanism. In the future, we will investigate more strategies to model the position influence of the question words.",1,corpora,True
ACKNOWLEDGMENTS,0,,False
"This research is supported by the National Key Technology Support Program (No.2015BAH01F02), Science and Technology Commission of Shanghai Municipality (No.16511102702), and Shanghai Municipal Commission of Economy and Information Under Grant Project (No.201602024). This research is also supported by a Discovery grant from the Natural Sciences and Engineering Research Council (NSERC) of Canada and an NSERC CREATE award.",1,ad,True
REFERENCES,0,,False
"[1] Cicero Nogueira dos Santos, Ming Tan, Bing Xiang, and Bowen Zhou. 2016. Attentive pooling networks. arXiv preprint arXiv:1602.03609 (2016).",0,,False
"[2] Alex Graves and Jürgen Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks 18, 5 (2005), 602­610.",0,,False
"[3] Baiyan Liu, Xiangdong An, and Jimmy Xiangji Huang. 2015. Using term location information to enhance probabilistic information retrieval. In SIGIR. 883­886.",0,,False
"[4] Jun Miao, Jimmy Xiangji Huang, and Zheng Ye. 2012. Proximity-based rocchio's model for pseudo relevance. In SIGIR. 535­544.",0,,False
[5] Jonas Mueller and Aditya Thyagarajan. 2016. Siamese Recurrent Architectures for Learning Sentence Similarity. In AAAI. 2786­2792.,0,,False
"[6] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global Vectors for Word Representation. In EMNLP. 1532­1543.",0,,False
[7] Aliaksei Severyn and Alessandro Moschitti. 2015. Learning to rank short text pairs with convolutional deep neural networks. In SIGIR. 373­382.,0,,False
"[8] Ming Tan, Cicero dos Santos, Bing Xiang, and Bowen Zhou. 2015. LSTM-based deep learning models for non-factoid answer selection. arXiv:1511.04108 (2015).",0,,False
"[9] Bingning Wang, Kang Liu, and Jun Zhao. 2016. Inner attention based recurrent neural networks for answer selection. In ACL. 1288­1297.",0,,False
[10] Di Wang and Eric Nyberg. 2015. A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering.. In ACL. 707­712.,0,,False
"[11] Mengqiu Wang, Noah A Smith, and Teruko Mitamura. 2007. What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA. In EMNLP-CoNLL. 22­32.",0,,False
[12] Zhiguo Wang and Abraham Ittycheriah. 2015. FAQ-based Question Answering via Word Alignment. arXiv preprint arXiv:1507.02628 (2015).,0,,False
"[13] Yi Yang, Wen-tau Yih, and Christopher Meek. 2015. WikiQA: A Challenge Dataset for Open-Domain Question Answering.. In EMNLP. 2013­2018.",1,Wiki,True
"[14] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In NAACL-HLT. 1480­1489.",0,,False
"[15] Wenpeng Yin, Hinrich Schütze, Bing Xiang, and Bowen Zhou. 2015. ABCNN: Attention-based convolutional neural network for modeling sentence pairs. In arXiv preprint arXiv:1512.05193.",0,,False
[16] Matthew D Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv preprint arXiv:1212.5701 (2012).,1,ad,True
"[17] Jiashu Zhao, Jimmy Xiangji Huang, and Ben He. 2011. CRTER: using cross terms to enhance probabilistic information retrieval. In SIGIR. 155­164.",0,,False
996,0,,False
,0,,False

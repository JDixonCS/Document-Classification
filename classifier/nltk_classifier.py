# -*- coding: utf-8 -*-
"""NLTK-Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aLtoMC9Ae7zFcIGNnqVsLw71RpRRMx4Y
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import re
import csv
import time


from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import RandomizedSearchCV
from sklearn.tree import DecisionTreeClassifier

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier

from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import VarianceThreshold
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest
import os
# print(os.listdir("../input"))
plt.style.use('ggplot')

df = pd.DataFrame()

raw = open(r"C:\\Users\\z3696\\Documents\\Document-Classification\\classifier\\NIST_FULL\\2010-neg.txt", encoding="ISO-8859-1")
lines = raw.readlines()
raw.close()

# remove /n at the end of each line
for index, line in enumerate(lines):
    lines[index] = line.strip()
    print(lines[index])

neg_2010_df = pd.DataFrame(columns=['sentence'])
i = 0
first_col = ""
for line in lines:
    first_col = re.sub(r' \(.*', "", line)
    neg_2010_df.loc[i] = [first_col]
    i = i+1

neg_2010_df.head()
neg_2010_df['label'] = 0
#print(neg_2010_df)

raw1 = open(r"C:\\Users\\z3696\\Documents\\Document-Classification\\classifier\\NIST_FULL\\2010-pos.txt", encoding="ISO-8859-1")
lines1 = raw1.readlines()
raw1.close()

# remove /n at the end of each line
for index, line in enumerate(lines1):
    lines1[index] = line.strip()
    print(lines1[index])

pos_2010_df = pd.DataFrame(columns=['sentence'])
i = 0
first_col = ""
for line in lines1:
    first_col = re.sub(r' \(.*', "", line)
    pos_2010_df.loc[i] = [first_col]
    i = i+1

pos_2010_df.head()
pos_2010_df['label'] = 1
#print(pos_2010_df)

df = df.append(pos_2010_df)
df = df.append(neg_2010_df)
print(df)
df.shape



'''
#Text Preprocessing in NLP with Python codes
import pandas as pd
import numpy as np
import nltk
import re
import string
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support as score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import string
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
nltk.download('stopwords')
stopwords.words('english')

#library that contains punctuation
import string
string.punctuation

#defining the function to remove punctuation
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
#storing the puntuation free text
df['clean_msg']= df['sentence'].apply(lambda x:remove_punctuation(x))
df.tail()
df['msg_lower']= df['clean_msg'].apply(lambda x: x.lower())

#defining function for tokenization
import re
def tokenization(text):
    tokens = re.split('W+',text)
    return tokens
#applying function to the column
df['msg_tokenied']= df['msg_lower'].apply(lambda x: tokenization(x))

#importing nlp library
import nltk
#Stop words present in the library
stopwords = nltk.corpus.stopwords.words('english')
stopwords[0:10]
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're"]

#defining the function to remove stopwords from tokenized text
def remove_stopwords(text):
    output= [i for i in text if i not in stopwords]
    return output

#applying the function
df['no_stopwords']= df['msg_tokenied'].apply(lambda x:remove_stopwords(x)) 

#importing the Stemming function from nltk library
from nltk.stem.porter import PorterStemmer
#defining the object for stemming
porter_stemmer = PorterStemmer()

#defining a function for stemming
def stemming(text):
  stem_text = [porter_stemmer.stem(word) for word in text]
  return stem_text
  df['msg_stemmed']=df['no_sw_msg'].apply(lambda x: stemming(x))

from nltk.stem import WordNetLemmatizer
#defining the object for Lemmatization
wordnet_lemmatizer = WordNetLemmatizer()
#defining the function for lemmatization
def lemmatizer(text):
  lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
  return lemm_text
  df['msg_lemmatized']=df['no_stopwords'].apply(lambda x:lemmatizer(x))
  print(df.head())

'''

import pandas as pd
import numpy as np
import nltk
import re
import string
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
import seaborn as sns
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support as score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
import string
from nltk.corpus import stopwords
from nltk.corpus import wordnet
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
nltk.download('wordnet')
nltk.download('stopwords')
stopwords.words('english')

#Removing punctuations from entire dataset
punc_set = string.punctuation
punc_set

#Function for removing punctions
def remove_punc(text):
    clean = "".join([x.lower() for x in text if x not in punc_set])
    return clean

#Applying the 'remove_punc' function to entire dataset
df['no_punc'] = df['sentence'].apply(lambda z:remove_punc(z))

#Function for Tokenizing entire data for representing every word as datapoint
def tokenize(text):
    tokens = re.split("\W+",text)
    return tokens

#Applying the 'tokenize' function to entire dataset
df['tokenized_Data'] = df['no_punc'].apply(lambda z:tokenize(z))

#Importing stopwords from NLTK Library to remove stopwords now that we have tokenized it
stopwords = nltk.corpus.stopwords.words('english')

#Function for removing stopwords from single row
def remove_stopwords(tokenized_words):
    Ligit_text=[word for word in tokenized_words if word not in stopwords]
    return Ligit_text

#Applying the function 'remove_stopwords' from the entire dataset
df["no_stop"] = df["tokenized_Data"].apply(lambda z:remove_stopwords(z))

#Importing 'WordNetLemmatizer' as lemmatizing function to find lemma's of words
wnl = nltk.wordnet.WordNetLemmatizer()

#Function for lemmatizing the tokenzied text
def lemmatizing(tokenized_text):
    lemma = [wnl.lemmatize(word) for word in tokenized_text]
    return lemma

#Applying the 'lemmatizing' function to entire dataset     
df['lemmatized'] = df['no_stop'].apply(lambda z:lemmatizing(z))

# #Importing the 'SnowballStemmer' and declaring variable 'sno' to save the stemmer in.
# #This Stemmer gives slightly better results as compared to 'PorterStemmer'
# sno = nltk.SnowballStemmer('english')

# #Function for applying stemming to find stem roots of all words
# def stemming(tokenized_text):
#     text= [sno.stem(word) for word in tokenized_text]
#     return text

# #Applying the 'stemming' function to entire dataset
# data['ss_stemmed'] = data['lemmatized'].apply(lambda z:stemming(z))


# ps = nltk.PorterStemmer()

# def stemming(tokenized_text):
#     text= [ps.stem(word) for word in tokenized_text]
#     return text

# data['ps_stemmed'] = data['lemmatized'].apply(lambda z:stemming(z))

#This step is done here because, the 'lemmatized' column is a list of tokenized words and when we apply vectorization
#techniques such as count vectorizer or TFIDF, they require string input. Hence convert all tokenzied words to string
df['lemmatized'] = [" ".join(review) for review in df['lemmatized'].values]

df.head()

#Splitting data into smaller dataframes for the purpose of Training and Testing
x1 = df.iloc[0:39114,5]
x2 = df.iloc[39115:78229,5]
y1 = df.iloc[0:39114,1]
y2 = df.iloc[39115:78229,1]
print(x1.shape)
print(x2.shape)
print(y1.shape)
print(y2.shape)
'''
x = df['lemmatized'].values
y = df['label'].values
print(x.shape)
print(y.shape)
'''

count_vect = CountVectorizer()
c_train = count_vect.fit_transform(x1.values)
c_test = count_vect.transform(x2.values)
print(c_train.shape)
print(c_test.shape)

#Declaring and applying TFIDF functions to train and test data
tfidf_vect = TfidfVectorizer(ngram_range=(1,2))
tfidf_train = tfidf_vect.fit_transform(x1.values)
tfidf_test=tfidf_vect.transform(x2.values)
print(tfidf_train.shape)
print(tfidf_test.shape)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

tfidf_vect = TfidfVectorizer()
x_tfidf = tfidf_vect.fit_transform(df["lemmatized"])

x_train, x_test, y_train, y_test = train_test_split(x_tfidf,df["label"],test_size=0.5)

log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
log.fit(x_train,y_train)
ly_prediction = log.predict(x_test)
f1 = f1_score(ly_prediction,y_test)
print('LC score',f1*100)
print('LC Confusion Matrix', confusion_matrix(y_test,ly_prediction), "\n")
print('Logistic Classification', classification_report(y_test,ly_prediction), "\n")
print('LC Accuracy Score', accuracy_score(y_test, ly_prediction)*100)

# Support Vector Machine Classifier
from sklearn import svm
SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
SVM.fit(x_train,y_train)
# predict the labels on validation dataset
svy_pred = SVM.predict(x_test)
f111 = f1_score(svy_pred,y_test)
print('SVM score',f111*100)
# Use accuracy_score function to get the accuracy
print('SVM Confusion Matrix', confusion_matrix(y_test, svy_pred), "\n")
print('SVM Classification', classification_report(y_test, svy_pred), "\n")
print('SVM Accuracy Score', accuracy_score(y_test, svy_pred)*100)

from sklearn.naive_bayes import MultinomialNB
Naive = MultinomialNB()
Naive.fit(x_train,y_train)
# predict the labels on validation dataset
ny_pred = Naive.predict(x_test)
f1111 = f1_score(ny_pred,y_test)
# Use accuracy_score function to get the accuracy
print('Naive score',f1111*100)
print('Naive Confusion Matrix', confusion_matrix(y_test, ny_pred), "\n")
print('Naive Classification', classification_report(y_test, ny_pred), "\n")
print('Naive Accuracy Score', accuracy_score(y_test, ny_pred)*100)

# import linrary
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
xgb_model = XGBClassifier().fit(x_train, y_train)

# predict
xgb_y_predict = xgb_model.predict(x_test)

# accuracy score
#xgb_score = accuracy_score(xgb_y_predict, y_test)
print('XGB Confusion Matrix', confusion_matrix(xgb_y_predict, y_test), "\n")
print('XGB Classification', classification_report(xgb_y_predict, y_test), "\n")
print('XGB Accuracy Score', accuracy_score(xgb_y_predict, y_test)*100)

'''
# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=1000, random_state=0)
rfc.fit(x_train,y_train)
ry_pred = rfc.predict(x_test)
f11 = f1_score(ry_pred,y_test)
print('RFC F1 score',f11*100)
print('RFC Confusion Matrix', confusion_matrix(y_test,ry_pred), "\n")
print('Random Forest Classification', classification_report(y_test,ry_pred), "\n")
print('RFC Accuracy Score', accuracy_score(y_test, ry_pred)*100)
'''

df['label'].value_counts()

import seaborn as sns
g = sns.countplot(df['label'])
g.set_xticklabels(['Negative','Positive'])
plt.show()

# class count
label_count_neg, label_count_pos = df['label'].value_counts()

# Separate class
label_neg = df[df['label'] == 0]
label_pos = df[df['label'] == 1]# print the shape of the class
print('Label Negative:', label_neg.shape)
print('Label Positive:', label_pos.shape)

label_neg_under = label_neg.sample(label_count_pos)

test_under = pd.concat([label_neg_under, label_pos], axis=0)

print("total class of pos and neg :",test_under['label'].value_counts())# plot the count after under-sampeling
test_under['label'].value_counts().plot(kind='bar', title='label (target)')

label_pos_over = label_pos.sample(label_count_neg, replace=True)

test_over = pd.concat([label_pos_over, label_neg], axis=0)

print("total class of pos and neg:",test_under['label'].value_counts())# plot the count after under-sampeling
test_over['label'].value_counts().plot(kind='bar', title='label (target)')

import imblearn

# import library
from collections import Counter
from imblearn.under_sampling import RandomUnderSampler
x =x_tfidf 
y = df["label"]
rus = RandomUnderSampler(random_state=42, replacement=True)# fit predictor and target variable
x_rus, y_rus = rus.fit_resample(x, y)

print('Original dataset shape:', Counter(y))
print('Resample dataset shape', Counter(y_rus))

# import library
from collections import Counter
from imblearn.over_sampling import RandomOverSampler
x =x_tfidf 
y = df["label"]
ros = RandomOverSampler(random_state=42)

# fit predictor and target variable
x_ros, y_ros = ros.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

# import library
from imblearn.under_sampling import TomekLinks
from collections import Counter


tl = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl, y_tl = ros.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

# import library
from imblearn.under_sampling import TomekLinks
from collections import Counter

tl = RandomOverSampler(sampling_strategy='majority')

# fit predictor and target variable
x_tl, y_tl = ros.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

# import library
from imblearn.over_sampling import SMOTE

smote = SMOTE()

# fit predictor and target variable
x_smote, y_smote = smote.fit_resample(x, y)

print('Original dataset shape', Counter(y))
print('Resample dataset shape', Counter(y_ros))

from imblearn.under_sampling import NearMiss

nm = NearMiss()

x_nm, y_nm = nm.fit_resample(x, y)

print('Original dataset shape:', Counter(y))
print('Resample dataset shape:', Counter(y_nm))

# load library
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score

# we can add class_weight='balanced' to add panalize mistake
svc_model = SVC(class_weight='balanced', probability=True)

svc_model.fit(x_train, y_train)

svc_predict = svc_model.predict(x_test)# check performance
print('ROCAUC score:',roc_auc_score(y_test, svc_predict))
print('Accuracy score:',accuracy_score(y_test, svc_predict))
print('F1 score:',f1_score(y_test, svc_predict))

# load library
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()

# fit the predictor and target
rfc.fit(x_train, y_train)

# predict
rfc_predict = rfc.predict(x_test)# check performance
print('ROCAUC score:',roc_auc_score(y_test, rfc_predict))
print('Accuracy score:',accuracy_score(y_test, rfc_predict))
print('F1 score:',f1_score(y_test, rfc_predict))

z = SelectKBest(chi2, k=2).fit_transform(x_tfidf, df["label"])
#X2 = StandardScaler().fit_transform(x_tfidf)
X_Train, X_Test, Y_Train, Y_Test = train_test_split(z, df['label'], test_size = 0.30,
                                                    random_state = 101)
start = time.process_time()
trainedforest = RandomForestClassifier(n_estimators=700).fit(X_Train,Y_Train)
print(time.process_time() - start)
predictionforest = trainedforest.predict(X_Test)
print(confusion_matrix(Y_Test,predictionforest))
print(classification_report(Y_Test,predictionforest))

start = time.process_time()
trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train)
print(time.process_time() - start)
predictionstree = trainedtree.predict(X_Test)
print(confusion_matrix(Y_Test,predictionstree))
print(classification_report(Y_Test,predictionstree))
'''
z = SelectKBest(chi2, k=2).fit_transform(x_tfidf, df["label"])
print("After selecting best 3 features:", z.shape)
'''

'''
import statsmodels.api as sm
import pandas as pd
xtrain  = x_tfidf.toarray()
ytrain  = df['label']
log_reg = sm.Logit(ytrain, xtrain).fit()
print(log_reg.summary())
#rint(np.asarray(df))
'''
'''
df.lemmatized.unique()
print(df['lemmatized'].value_counts())
#print(df['lemmatized'].max())

ax = df['lemmatized'].value_counts().sort_index().plot(kind='bar', fontsize=14, figsize=(12,10))
ax.set_title('Phrase Count\n', fontsize=20)
ax.set_xlabel('Publication', fontsize=18)
ax.set_ylabel('Count', fontsize=18);



count_vect1 = CountVectorizer()
x_cvect1 = count_vect1.fit_transform(df["lemmatized"])

cx_train, cx_test, cy_train, cy_test = train_test_split(x_cvect1,df["label"],test_size=0.3)

log = LogisticRegression(penalty='l2',random_state=0, solver='lbfgs', multi_class='auto', max_iter=500)
log.fit(cx_train,cy_train)
lcy_prediction = log.predict(cx_test)
cf1 = f1_score(lcy_prediction,cy_test)
print('F1 score',cf1*100)
print('Confusion Matrix', confusion_matrix(cy_test,lcy_prediction), "\n")
print('Classification', classification_report(cy_test,lcy_prediction), "\n")
print('Accuracy Score', accuracy_score(cy_test, lcy_prediction)*100)

# Support Vector Machine Classifier
from sklearn import svm
SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')
SVM.fit(cx_train,cy_train)
# predict the labels on validation dataset
lsvy_pred = SVM.predict(x_test)
cf111 = f1_score(svy_pred,y_test)
print('SVM F1 score',cf111*100)
# Use accuracy_score function to get the accuracy
print('SVM Confusion Matrix', confusion_matrix(y_test, lsvy_pred), "\n")
print('SVM Classification', classification_report(y_test, lsvy_pred), "\n")
print('SVM Accuracy Score', accuracy_score(y_test, lsvy_pred)*100)

from sklearn.naive_bayes import MultinomialNB
Naive = MultinomialNB()
Naive.fit(x_train,y_train)
# predict the labels on validation dataset
lny_pred = Naive.predict(x_test)
f1111 = f1_score(lny_pred,y_test)
# Use accuracy_score function to get the accuracy
print('Naive F1 score',f1111*100)
print('Naive Confusion Matrix', confusion_matrix(y_test, lny_pred), "\n")
print('Naive Classification', classification_report(y_test, lny_pred), "\n")
print('Naive Accuracy Score', accuracy_score(y_test, lny_pred)*100)

# Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=1000, random_state=0)
rfc.fit(cx_train,cy_train)
lry_pred = rfc.predict(x_test)
cf11 = f1_score(lry_pred,y_test)
print('RFC F1 score', cf11*100)
print('RFC Confusion Matrix', confusion_matrix(y_test,lry_pred), "\n")
print('Random Forest Classification', classification_report(y_test,lry_pred), "\n")
print('RFC Accuracy Score', accuracy_score(y_test,lry_pred)*100)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
input_dim = c_train.shape[1]  # Number of features

model = Sequential()
model.add(layers.Dense(10, input_dim=input_dim, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
model.summary()

c_train = c_train.toarray()
c_test = c_test.toarray()
history = model.fit(c_train, cy_train,
                    epochs=100,
                    verbose=True,
                    validation_data=(c_test, cy_test),
                    batch_size=10)
'''
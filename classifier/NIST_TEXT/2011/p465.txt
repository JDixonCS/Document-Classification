On Theme Location Discovery for Travelogue Services


Mao Ye1 , Rong Xiao2, Wang-Chien Lee1, and Xing Xie2
Department of Computer Science & Engineering, The Pennsylvania State University, PA, USA. Microsoft Research Asia, Beijing, China.
1{mxy177,wlee}@cse.psu.edu 2{rong,xing.xie}@microsoft.com

ABSTRACT
In this paper, we aim to develop a travelogue service that discovers and conveys various travelogue digests, in form of theme locations, geographical scope, traveling trajectory and location snippet, to users. In this service, theme locations in a travelogue are the core information to discover. Thus we aim to address the problem of theme location discovery to enable the above travelogue services. Due to the inherent ambiguity of location relevance, we perform location relevance mining (LRM) in two complementary angles, relevance classification and relevance ranking, to provide comprehensive understanding of locations. Furthermore, we explore the textual (e.g., surrounding words) and geographical (e.g., geographical relationship among locations) features of locations to develop a co-training model for enhancement of classification performance. Built upon the mining result of LRM, we develop a series of techniques for provisioning of the aforementioned travelogue digests in our travelogue system. Finally, we conduct comprehensive experiments on collected travelogues to evaluate the performance of our location relevance mining techniques and demonstrate the effectiveness of the travelogue service.
Categories and Subject Descriptors
H.3.3 [Information Storage and Retrieval]: Information Filtering; I.2.7 [Artificial Intelligence]: Natural Language Processing
General Terms
Experimentation
Keywords
classification, ranking, co-training, travelogue services
1. INTRODUCTION
Continued flourish of World Wide Web (WWW) and advance of transportation systems have made travel, an integral part of our life, very easy. Particularly, with the advantages of Web 2.0 technology, many people are willing to
Part of this work was done when the author was visiting Microsoft Research Asia.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR '11, July 24-28, 2011, Beijing, China. Copyright 2011 ACM 978-1-4503-0757-4/11/07 ...$10.00.

share their travelogues, which record travel experiences, on weblogs, forums and social communities for travels, (e.g., TravelPod1, IgoUgo2 and TravelBlog3).
Travelogues, which usually contain rich travel information, such as the tours, lodging, meals, expenses, weather conditions, and so on, are highly valuable to a trip planner. Thus, many people turn to on-line travelogue archives or discussion forums to review travelogues well before taking their trips. While reading travelogues may be fun and enjoyable for some, digging into numerous travelogues to find useful trip information is a tedious and boring task for many. Thus, there is a need for developing automatic travelogue mining techniques to convey useful information in a travelogue to its readers in a more effective way.
Generally, there are two kinds of information for a travelogue to be mined; (1) "whereabout" of the travelogue, and (2) "how-about" of the mentioned attractions.
As a travelogue intends to record activities and experiences of its author at locations on a trip (i.e., the whereabout of the trip), a key problem related to (1) concerns extraction of theme locations, i.e., locations appeared in a travelogue which are closely relevant to the main themes of the travelogue. Location extraction techniques have been developed in the past [1, 14]. However, due to the nature of human language presentation, locations extracted are not necessarily theme locations of the travelogue. For example, in Figure 1, Siesta Key is very relevant to the theme of this paragraph but Boston is not.
Siesta Key - It is a paradise found in Sarasota, FL, located a little over an hour drive south to Tampa, FL, and is a favorite gateway spot for many Florida locals. I like to spend time there - mainly at beaches and bars ... We got back to Boston after one week vocation.
Figure 1: A paragraph in a travelogue for Siesta Key. Relevant locations are in bold, partial relevant locations are in teletype, and irrelevant locations are in italic.
Another important problem, related to (2) is about extracting specific how-about information regarding theme locations. Hao et. al. [7] pioneered a knowledge discovery technique that extracts general information (e.g., food, surfing, shopping) regarding travel destinations from a corpus of travelogues. However, this technique has mainly focussed on location information extraction without distinguishing
1http://www.travelpod.com 2http://igougo.com 3http://www.travelblog.com

465

whether locations in a travelogue is relevant to its themes or not. As a result, the result is inevitably biased by noisy locations.
To tackle the theme location discovery problem, we propose to extract textual features of locations (e.g., surrounding words and syntactic pattern) as well as geographical features of locations (e.g., individual location types, structural constraints derived from location ontology) for location relevance mining. With those features, we perform location relevance classification to differentiate relevant theme locations from others in a travelogue. Moreover, we propose a co-training model, by taking advantages of the independent textual and geographical views of the extracted features, to boost classification performance,
Unfortunately, human perception of locations relevant to the themes of a travelogue tends to be fuzzy! Different people may employ different personal criteria to assess relevant theme locations. What's more, the content of a travelogue may sometimes be insufficient for one to decide whether a location is a theme location or not. However, based on the context of a travelogue, we often can still tell whether a location lA is more relevant than another location lB to the travelogue, without necessarily deciding whether lA and lB are considered as theme locations or not. Based on this observation, we also develop algorithms for location relevance ranking.
Relevance classification and ranking provide two alternative and complementary aspects in our study of location relevance mining. To our best knowledge, there is no existing work addressing the issues of location relevance mining for a given travelogue, not to mention its support for provisioning of various information digests in a travelogue service. Built upon the mining result of location relevance mining, we develop a series of techniques, including theme locations detection, geographical scope calculation, travel trajectory extraction and location snippet extraction, to embody a travelogue service which not only enable trip planners. to find where to go but also what to do. Finally, we conduct a comprehensive set of experiments on collected travelogues to validate our ideas, evaluate the performance of our location relevance mining techniques, and demonstrate the effectiveness of the travelogue service.
The main contributions made in this paper are summarized as follows.
· We identify the problem of theme location discovery in travelogue services. To our best knowledge, this is the first work that explores mining techniques to convey location information in "one" travelogue to its readers in an effective way.
· We develop location relevance classification and ranking techniques, by exploring unique textual and geographical features inherent to travelogues, to discover theme locations effectively.
· Based on the theme locations discovered using our location relevance mining techniques, we develop a series of techniques for extracting various travelogue digests, including theme locations, geographical scope, traveling trajectory and location snippet, in support of the travelogue service we propose.
· Through the performance evaluation conducted on two travelogue corpus (in English and Chinese), we show that the location relevance mining techniques we developed can effectively discover theme locations and demonstrate that the mined theme locations are important for providing high-quality "whereabout" and

"how-about" information in a travelogue via the travelogue digests in our travelogue service.
The remainder of the paper is organized as follows. Section 2 introduces the most related literatures. Section 3 provides a overview about location relevance mining for theme location discovery. Section 4 describes the features we extract to perform location relevance study, and Section 5 discusses the detailed algorithms for location relevance mining. Section 6 introduces the techniques to realize travelogue services built upon LRM. Section 7 conducts performance evaluation to validate the proposed location relevance mining techniques and demonstrate the travelogue service. Finally, Section 8 concludes the paper and discusses the future work.
2. RELATED WORK
In this section, we first introduce some existing studies on spatial information retrieval, such as news and travelogues. Then we review techniques related to location relevance mining, followed by a brief summary of semi-supervised learning techniques.
Location-based search has received a lot of interests from both the academia and industry in the past few years. With advances in location extraction techniques [1, 14], spatial interests expressed in text can be learned to support web services for users looking for spatial information [6, 1, 17, 13]. Wang et. al. propose a notion of query dominant location (QDL) to detect the locations truly searched for by queries [17]. On the contrary, Ding et. al., propose to study the geographical extent of spatial web resources that their creators intend to reach [6]. Amitay et. al. propose to assign to each page a (or a few) "geographic focus" in [1]. They identify the focus of a given page by assigning different confidence score for different locations and use hierarchical structure of location terms to summarize locations with high confidence. Silva et. al. [16] propose to use a graph-based approach akin to PageRank to determine a single scopes for each document, based on a geographic knowledge repository. In [13], Aencar et. al. describe strategies for document classification into geography-related categories, using evidence extracted from Wikipedia. Recently, Hao et. al., aiming to provide travelogue services based on summarization of a travelogue corpus, propose to mine location-specific knowledge from a large collection of travelogues using a probabilistic generative model [7]. Different from most existing work, our study enables provisioning of travelogue digests in our travelogue system, and provide new views for users to understand a travelogue.
In this paper, we explore location relevance classification and ranking techniques to address the issues of location relevance mining. While there exist many textual classification techniques such as Support Vector Machine (SVM) [2] and logistic regression [2] and document ranking methods [15, 10, 9, 5] in the literature, the problem of location relevance mining discussed in this paper is very much new, which requires a unique treatment of certain geographical characteristics. For example, Siesta Key is a sibling city of Tampa, while Florida is the state where Siesta Key and Tampa reside. These unique geographical characteristics motivate us to explore interesting location features in devising techniques for location relevance classification and ranking.
In this paper, we follow the semi-supervised learning parad -igm to address the issues of location relevance mining. Semisupervised learning is desirable for our study because we have obtained a small set of labeled data and a large volume of unlabeled data. The key idea of semi-supervised learning is to enrich labeled training data by labeling unla-

466

beled data via certain learning techniques. Basically, there are three techniques to realize semi-supervise learning, including (1) using the EM (expectation-maximization) algorithm to estimate the parameters of a generative model and the labels of unlabel data [12], (2) defining a graph over the data instances on the basis of certain similarity metric and determining the labels of unlabeled data [3], and (3) using co-training algorithms to exploit the strengthes of multiple learners to label the unlabeled data [4, 8]. In this work, we realize the semi-supervised learning by exploring two independent views, i.e., textual and geographical views, in a co-training approach.
3. THEME LOCATION DISCOVERY - AN
OVERVIEW
Theme location discovery aims to discover and assess the theme locations in a given travelogue. The problem of theme location discovery raises two different yet complementary questions: (1) what locations in a travelogue are theme locations? (2) given two locations in a travelogue, which one is more likely to be a theme location?
These two questions are difficult to answer because we don't know exactly what is the main theme of a travelogue perceived by author and readers. However, based on the intention of travelogue writing, we argue that there indeed exist one or more themes in a travelogue, which match well with the purposes/activities/destinations of this trip. To identify the theme locations, one naive approach is to mandate the travelogue authors to supply them or have the readers to label them, but this manual approach is hard to enforce. Moreover, there are already a huge volume of travelogues made available on-line. Therefore, a mining technique that extracts the theme locations for a given travelogue automatically is highly desirable. Based on observations obtained in our preliminary study of labeled travelogues, the theme locations perceived by different people are similar to some extent, even though they may not be the same. Thus, by leveraging the human knowledge, we apply a supervised learning approach to automatically discover theme locations from travelogues using a labeled training dataset. Moreover, as mentioned earlier, we adopt a semi-supervised learning approach to enrich the limited labeled dataset in order to boost the learning performance. To answer the two questions raised above, we study the issues of location relevance for finding the theme locations in two complementary aspects: location relevance classification and location relevance ranking. Location relevance classification. The first question can be considered as a classical classification problem by extracting features for each candidate location in the travelogue. However, as we mentioned before, there is an inherent ambiguity among different people in terms of how relevant a candidate location is to the theme of the travelogue. What's more, the content of travelogue is sometimes insufficient to assess whether a location is a theme location or not. For example, in Figure 1, one may assess that Siesta Key is the only theme location; while another person may consider both Sarasota and Florida are also theme locations. Thus, as shown in Figure 2, the main challenge for location relevance classification is where to set a boundary separating theme location from irrelevant locations. Location relevance ranking. Instead of determining whether a location is a theme location, the second question we aim to answer is whether one location is more relevant than another location to the theme of this trip. As shown in Figure 2, assuming that Siesta Key is the sole theme, we may assess that Tampa is less relevant to the theme location

Figure 2: Dilemma in relevant location classification

than Sarasota. In this paper, we adopt two alternative approaches, namely location likelihood estimation and learning to rank, for location relevance ranking. As discussed in Section 2, while document classification and ranking techniques have been explored for document retrieval, there is an inherent difference between travelogues and documents because locations in a travelogue usually have some interesting relationship, e.g., Siesta Key is a sibling city of Tampa, while Florida is the state which covers Siesta Key and Tampa. Thus, conventional document mining techniques cannot be applied directly to our problem. These unique spatial characteristics motivates us to explore interesting features for location relevance mining.
4. FEATURE EXTRACTION
To perform location relevance mining, it is essential to identify useful features of a location that can help to assess whether the location is relevant to main themes of the travelogue or not. Since travelogues are textual documents recording the authors' experiences in touring places of interests, it naturally contains textual and geographical features. Thus, we identify features associate with locations in these two aspects. Table 1 summarizes the features extracted in our study.

Category

Features

Textual Geographical

F1 : is in title? F2 : number of appearance F3 : bag-of-words F4 : syntactic pattern
F5 : location type F6 : partonomy distance F7 : geographical proximity

Table 1: Features and their categories

On the one hand, for a travelogue, we usually have a title to summarize the theme of the travel. The most relevant locations are usually included there. What's more, interested locations might be mentioned several times in a travelogue, while irrelevant locations appear less frequently. According to this observation, we extract two kinds of features, namely, is in title? (denoted as F1) and number of appearance (denoted as F2), respectively. Besides, surrounding words of a location also provides useful hints. For example, in Figure 1, one may easily understand that "Siesta Key" is the most relevant location name in this paragraph, because important locations are usually heading the paragraphs. Thus for a location in a travelogue, we use its surrounding words to extract bag-of-word feature (denoted as F3) and syntactic pattern feature (denoted as F4) to describe the location. Those above four features are categorized as textual feature.
On the other hand, each location as a physical entity holds geographical properties in the real word. For example, location names referred in a travelogue usually have different location types in location partonomy. As shown in Figure 3, among the locations mentioned in the travelogue in Figure 1,

467

some of them are state name (e.g., Florida), while some are

county or township names (e.g., Sarasota and Siesta Key).

Location names with smaller scope usually hold higher rel-

evance, otherwise, travelers would not bother to mention

them in the travelogue. Thus, we consider the location type

to be an important feature, named as location type (denoted

as F5).

Nation

United States

State

Florida State

Massachusetts State

City/County Sarasota

Tampa

Boston

Township or even smaller

Siesta Key

(1) long distance away (2) belong to different states

Figure 3: Partonomy hierarchy of locations mentioned in the travelogue shown in Figure 1

Additionally, relevant locations are usually clustered in the partonomy hierarchy of a location ontology. Moreover, people would likely stay in the same state or the same city during the trip. In other words, partonomy distance among the locations provides important information to leverage relevant locations. More specifically, we extract the partonomy distance among locations as feature (denoted as F6) as follows. Let the partonomy distance (dpar) between two locations be the total number of hops in the partonomy hierarchy to their immediate common ancestor. Let l1 and l2 be two different locations mentioned in a given travelogue, and la be the immediate common ancestor of l1 and l2.

dpar(l1, l2) = hops(l1, la) + hops(l2, la).

(1)

where hops(lx, ly) denotes the number of hops between lx and ly along the path of partonomy hierarchy (it equals zero if lx = ly). For example, in Figure 3, we have the partonomy distance between "Siesta Key" and "Boston" computed as 5, where the immediate common ancestor is "United State".
Finally, if two locations are far away, e.g., Siesta Key and Boston in Figure 3, only one of them could likely be the relevant location for a travelogue. Therefore, we consider geographical proximity (dgeo) between two different locations (mentioned in the same travelogue) to be an important feature (denoted as F7). More specifically, let l1 and l2 be two different locations mentioned in a given travelogue, and their latitude and longitude are denoted as (x1, y1) and (x2, y2) respectively. For simplicity, we use Euclidian distance to present the geographical distance between l1 and l2 as follows.

dgeo(l1, l2) = (x1 - x2)2 + (y1 - y2)2

(2)

5. MINING ALGORITHMS
In this section, we introduce algorithms to realize the two mining tasks, namely, location relevance classification and location relevance ranking, for theme location discovery.
5.1 Location Relevance Classification
Location relevance classification aims to predict whether a location is relevant to the theme of the travelogue or not. As aforementioned, both textual and geographical features are able to assess whether a location is relevant to the theme of a travelogue or not, so we exploit both feature sets of a location to realize location relevance classification. Because of its popularity and good performance in text mining, we

textual Co-training

geographical

Figure 4: Co-training framework for location relevance classification
adopt SVM to perform supervised classification. Nevertheless, based on our preliminary study, directly applying SVM to perform supervised classification does not produce a good performance. On the one hand, location relevance classification is very challenging due to the inherent ambiguity of location relevance and the limited amount of labeled data available. On the other hand, we noticed that travelogues naturally contains two independent feature sets, i.e., textual and geographical features. Aiming to boost the performance of location relevance classification, we adopt the co-training framework (i.e., semi-supervised learning) as shown in Figure 4 to explore the two feature sets in order to enrich the training set with high-quality newly labeled data in order to obtain a better SVM classifier model. As one of the popular semi-supervised learning algorithms, co-training is a perfectly fit for the classification of location relevance here. It has been shown in [12] that co-training works well under two conditions: (1) each set of features is sufficient for classification, and (2) the two feature sets of each instance are conditionally independent. As both textual and geographical features provide strong hints about whether a location is relevant to the theme location of the travelogue. Based on our preliminary experimentation, we found that the SVM classifiers modeled based on either textual features or geographical features are capable of doing location relevance classification. Moreover, textual and geographical features are proposed to capture the characteristics of a location in totally independent angles, and thus complementary with each other. With both of the above conditions hold, we adopt co-training to boost the performance for location relevance classification.
Figure 4 illustrates the proposed framework for overall location relevance classification. As shown, the location extraction component in our system scans the travelogue corpus to extract the locations together with their features for each travelogue. The resulted datasets, including candidate set and training set corresponding to unlabeled and labeled travelogues, are fed to the co-training module to produce a enriched training set (including the original training set plus those high quality newly labeled data) for further classification in the final SVM module (i.e., the bottom box of the figure). Notice that, the final SVM module performs classification in the entire feature space including both textual and geographical features.
Co-training is an iterative process, which typically runs on a large corpus of text documents (i.e., the candidate set) together with a seed training data set (i.e., labeled travelogues here). The main idea of our co-training framework is to explore both textual features and geographical features associated with locations in travelogues to independently and iteratively mine high-quality labels for travelogues. Our co-

468

training algorithm starts with a small number of seed data in the training set. By treating the data instance space in two different views, namely, textual view and geographical view, we have X = Xt × Xg, where X denotes the entire feature space, while Xt and Xg present textual feature space and geographical feature space, respectively. Iterating between these two different views, we use two separate SVMs (one for each view) to decide the labels for those unlabeled travelogues in the candidate set. With newly labeled travelogues, we validate their labels by assigning confidence scores, which are derived based on the distance of a given travelogue to the classification hyperplane learned by SVM. More specifically, the confidence score s(x) for a newly labeled travelogue x  Xt (or Xg) is calculated as follows.

s(x) = |f (x)| = |wx + b|

(3)

where w is the optimized hyperplane, while b is the bias term. In order to enrich the training set with high-quality labeled travelogues, we derive a good confidence score threshold  based on a validation dataset and the current training model learned in each round. As such, only high-quality data (i.e., those newly labeled travelogues with high confidence scores (s(x)  ) are included the training set. As the co-training runs, the training set is enriched continuously until it converges eventually. Finally, a new training dataset larger than the original one is derived from the cotraining process. We then adopt SVM to learn upon this new training dataset in the complete feature space (including both textual and geographical features) to obtain the final classifier model.

5.2 Location Relevance Ranking
Location relevance ranking aims to sort locations in accordance with their relevances to the theme of a travelogue. It can be analogized to the problem of document relevance ranking. In location relevance ranking problem, we treat the themes of a travelogue as the query keywords, locations included in the travelogue as candidate documents. However, different from traditional document retrieval, the query keywords (i.e., themes) are implicitly contained in travelogues instead of explicitly specified by users. Here, we introduce two alternative approaches, namely, location likelihood estimation and learning to rank algorithms to address the ranking problem.

5.2.1 Location Likelihood Estimation
A travelogue is written to record the trip experiences of the author over visited theme locations. Thus, an idea to estimate and rank the relevance of a location in a travelogue to the main themes of the travelogue, motivated by language modeling in information retrieval, is to consider the generation probability of a location in the travelogue. Let l and t denote a location and a travelogue, we use p(l|t) to denote the probability of l appearing in the observed travelogue t. Intuitively, the larger the generation probability of l in t holds, the more relevant l is to the travelogue t. Thus, we define the relevance of location l to a travelogue t as below.

r(l, t) = p(l|t)

= p(l|Mt) + (1 - )p(l|Mc)

(4)

=  tfl,t + (1 - ) tfl

Nt

N

where  is a smoothing factor, and Mt and Mc are the location vocabularies built upon travelogue t and the whole corpus respectively. tfl,t, tfl are term frequency of location term l appearing in travelogue t and the whole travelogue

corpus respectively, and Nt and N are the number of location terms in travelogue t and the whole travelogue corpus, respectively.

5.2.2 Learning to Rank
Recall our example in Figure 1. There are five locations included in the travelogue, but each of them appears only once in the whole document. Based on location likelihood estimate, we may assign them the same rankings, given that smoothing is not applied. Since Siesta Key, Florida, Tampa, and Boston are all popular places, the smoothing technique is not expected to help here. The question is why the location likelihood estimation approach would suggest all those locations to have the same ranks, which is actually against our intuition. The reason is that the language model mainly considers the frequencies of locations, without taking account of other valuable features of given documents, e.g., the surrounding words of locations and the correlation among locations in a travelogue. Thus, we propose an alternative approach, learning to rank model, to realize location relevance ranking by exploiting the textual and geographical feature of the travelogue.
Due to its popularity and simplicity, we adopt Ranking SVM [9], which takes pair-wise relationship of locations in a travelogue to learn a ranking model. Given an input feature space X, where X includes both textual and geographical features, there exists an output space of ranks represented by labels Y = {r1, · · · , rq}, where q denotes the possible number of ranks. Further, assume that there exists a total order amongst the ranks rq rq-1 · · · r1, where denotes a preference relationship. Notice that, in this paper, we have three ranks of location relevance, namely "relevant", "partial relevant" and "irrelevant", and the preference relationship in location relevance ranking is "relevant partial relevant irrelevant".
Given a set of ranked instances Z ={(x1, y1), · · · , (xn, yn)}, where n is the number of training instances, (xj, yj)(1  j  n) is from the space of X × Y . If yi > yj, we have xi ranked ahead of xj, denoted as xi xj. Assume that F is the set of ranking functions, such that each function f  F can rank location instances as:

xi xj  f (xi) > f (xj )

(5)

In Ranking SVM, f is assumed to be a linear function [9],

f (x) = , x

(6)

where  is weight vector and ·, · denotes inner product. f (x) = 0 corresponds to a hyperplane in the feature space,. Thus we have

xi xj  , xi - xj > 0

(7)

Given an instance pair xi and xj, we create a new instance
xi - xj. If xi is ranked ahead of xj, we assign a label +1,
otherwise -1 to the newly generated instance. As such, we
produce a new data set upon which we can build a binary
classification model, i.e., the Ranking SVM model. With the Ranking SVM model, and the optimal setting of , for each location instance x we get its ranking score as f (x) = , x .

6. THE TRAVELOGUE SERVICE
In this section, we study how to realize the proposed travelogue service based on the mined location relevance. Particularly, we are interested in conveying theme locations, geographical scope, traveling trajectory and location snippets in a travelogue to the users. Figure 5 illustrates a mock

469

The Cloisters , a branch of the Metropolitan Museum of Art, is not just a museum filled with beautiful
medieval art, it 's a place to find serenity outside of New York...

The Cloisters

Central Park

Bronx Zoo

Figure 5: Travelogue service
up of our proposed travelogue service, which provides visualized information for a travelogue over a map. As shown, New York is the geographical scope of the trip. The traveler visited Central Park, Bronx Zoo and The Cloisters in sequence (i.e., three theme locations and a traveling trajectory). By moving the mouse cursor to one of the theme locations, associated location snippet pops out to provide some information. In the following, we present how we use the result of location relevance mining for provisioning of the digests in the travelogue service. Theme Locations. Locations is an essential entity of travelogues. Anticipating that users would be interested in quickly identifying where the trip activities have happened, we display the theme locations of a travelogue. As we mentioned before, the theme locations can be generated in two ways: i) automatically discovered from the travelogue; and ii) tagged by the travelogue author. Thus, our location relevance classification technique can be used to automatically identify the theme locations. For author-tagged theme locations, we develop a location tag recommendation service to assist the travelogue authors to tag theme locations when they upload their travelogues. Specifically, the location tag recommendation service, presented to the author as a list of ranked theme locations in accordance with their estimated relevances to the theme of the uploaded travelogue, is realized based on the location relevance ranking techniques presented earlier. Geographical Scope. As a travelogue may cover several theme locations, it is desirable to present the geographical scope of trip activities to its readers. Previous works define geographical scope as a single (or a few) location names to cover all locations mentioned in a web page [1, 17]. For our travelogue service, we argue that not all locations are useful for deducing the geographical scope for a travelogue, especially when the noisy locations may produce misleading results. Consider the travelogue in Figure 1, we may argue that its geographical scope should be narrowed down to Florida or even Siesta Key, since both Tampa and Boston are deviating from the theme of this travelogue. Therefore, we follow the ideas in [1] to compute the geographical scope but put our focus on how to derive the input location set, since noisy locations hamper the accuracy of expected geographical scope.
To compute the geographic scope of a travelogue t, we consider only the theme locations Lt obtained from location relevance classification which effectively purges noisy

locations from the entire location set Lt in t.4 Thus, the geographical scope computing algorithm takes Lt as input and operates upon the partonomy hierarchy of location ontology (see Figure 3 as an example). First, for each location l in Lt, we initiate an importance score vl of l as the number of its appearance in the travelogue. Next, we propagate the influence of l to its parent node l (and recursively to its ancestor nodes) by adding vl × d to vl where 0 < d < 1 is a decay factor. Consider the example in Figure 3, Siesta Key would propagate its influence of importance score to its parent node Sarasota and grand-parent node Florida because the traveler visits Siesta Key and she also stays at Sarasota and Florida. As such, each location in Lt obtains an importance score that takes influence from its descendants into account. We derive the geographical scope as a location set by adding the theme locations in order of their importance but skipping those with one or more ancestors or decedents already included in the set. Traveling Trajectory. Traveling trajectory (denoted as T ) is extracted based on the result of location relevance classification. Given a travelogue, we obtain a set of theme locations, denoted as L = {l1, l2, · · · }, from location relevance mining. Given two locations lx, ly  L, it is possible that lx is part of ly in the partonomy hierarchy. In this case, we exclude ly from the trajectory. For example, in Figure 3, if Siesta Key and Sarasota are both classified as theme locations, we consider Siesta Key be the reason for the traveler to visit Sarasota. Thus, for trajectory extraction, we highlight the specific point-of-interests and investigate their travel sequence to form a trajectory, which is a subset of L. In other words, locations included in a trajectory are not part of each other. The algorithm for forming POI set is shown in Algorithm 1.
Algorithm 1 POI Set Formation Algorithm
// Input: theme location set L for a given travelogue P OI  L for each li  T do
if exists lj  L such that lj is part of li then P OI  P OI - {li}
end if end for return P OI
Upon the POI set, we construct a trajectory T based on a simple assumption that the appearance sequence of those location in T is supposed to match well with the travel sequence of the traveler's real trajectory. Thus, we construct the trajectory by sorting locations in the POIs in accordance with the sequence of their appearances in the travelogue to form T . Of course, there still is a room for improvement in forming the trajectory. We plan to further investigate this issue in the future work. Location Snippet. Location snippet consists of a few (usually one or two) sentences extracted from a travelogue to describe the corresponding location. Given a location l in a travelogue, a desired location snippet is one that includes relevant information about l, e.g., interesting description about some features of l. Based on topic generative model, we may find a word w related to a location l. Specifically, we may derive a probability p(w|l) that models how likely a word w describes a location l for all words in the vocabulary V , which contains all possible words in travelogues [7]. Based on p(w|l), we construct a word set Wl (i.e., a subset of V )
4An alternative of Lt is to use the highest ranked locations obtained from location relevance ranking with some predetermined rank threshold.

470

which includes informative words regarding l, i.e., word w with high p(w|l) are included in Wl.
Next, for each theme location l, we determine the semantic relevance (SemRel) between l and a candidate snippet s by comparing the "word similarity" between Wl and the set of words in s, denoted as Ws. Intuitively, we can simply use cosine similarity to calculate the distance between Wl and Ws. However, cosine similarity does not capture the latent topic behind of words (e.g., "lunch" and "food" are treated as independent semantically). Thus, we propose to use latent topic distribution to measure the similarity between a location l and a candidate snippet s as follow.
SemRel(l, s) = w1Wl w2Ws TermSim(w1, w2) (8) log(1 + |Ws|)
where
TermSim(w1, w2) = exp{- DJS(w1 ||w2 )}, (9)
where w denotes the topic distribution of the word [7],  is a normalizing factor, and DJS(p||q) denotes the JensenShannon(JS) divergence.
Here please note that, while we only consider the semantic relevance in our selection of representative snippet for a theme location, the geographical relevance has been implicitly taken into consideration in the process of location relevance mining, which effectively purges noisy locations.

7. PERFORMANCE EVALUATION
In this section, we evaluate performance of the proposed location relevance mining techniques and demonstrate the effectiveness of the travelogue service.
7.1 Datasets
There are many sources of travelogues on the Web, ranging from Weblogs such as Windows Live Spaces to dedicated travel web-site such as TravelPod, IgoUgo and TravelBlog. We collected approximately 100,000 travelogues in English with location labels fallen in United States to form an English Corpus. Nevertheless, instead of relying on the location labels directly, we implemented a location extractor to extract locations mentioned in these travelogues, yielding 18,000 unique locations. Because some tasks require evaluation by human beings with travel-related background and knowledge, we also built a Chinese Corpus by collecting travelogues from Ctrip5, which consists of 94,000 Chinese travelogues related to around 32,000 locations in China.

7.2 Evaluation on LRM
Here, we first introduce the performance metrics and then present the results of our evaluation on location relevance mining.

7.2.1 Performance Metrics

In the experiments, Precision and Recall are used to evalu-

ate the performance of location relevance classification; Nor-

malized Discounted Cumulative Gain (NDCG) and Mean

Average Precision (MAP) are used to evaluate the perfor-

mance of location relevance ranking. Since Precision and

Recall are well known metrics, we only discuss NDCG and

MAP.

For location relevance ranking problem, given a travelogue

t, the NDCG score (NDCGt) is computed according to the

ranked location list Lt (i.e., the output of location relevance

ranking), as NDCGt = nt

, |Lt| 2rj -1
j=1 log(1+j)

where

rj

is

the

5http://www.ctrip.com

rank of the location at the jth position of the location list Lt,

and nt is a normalization constant, which is chosen so that a

perfect ranking's NDCG score is 1. The final NDCG score is

averaged over the scores of all the travelogues. In this paper,

the NDCG scores for ranked location lists which include top

1, 3, 5, 7, 9 and 11 locations are reported. MAP stands

for the mean of average precisions over all the travelogues.

Given a travelogue t and a ranked location list Lt, average

precision is defined as AveP ret =

|Lt | j=1

I

(j)(Rj R

/j)

,

where

R

and Rj denote the total number of relevant locations and

the number of relevant locations before the position (j + 1),

respectively, and I(j) is an indicator which takes value 1 if

the location at position j is relevant and value 0 otherwise.

7.2.2 Evaluation Results
An important factor crucial to performance of location relevance classification and ranking is the location features extracted for mining. Thus, to facilitate our studies on impact of extracted features, we form the following four feature groups: (1) baseline group - we consider "is in title ?" (F1) and "number of appearance" (F2) as two basic features, which are frequently used for text mining; (2) textual group - we consider all textual features; (3) geographical group - we consider all geographical features; (4) textual+geographical group - we consider both the textual and geographical features.
As there is no existing dataset for evaluation of location relevance mining, we have to built our own training dataset manually. Since we usually consider a destination in our trip as relevant locations, those pass-by/stop-by/nearby locations as partial relevant location, and others as irrelevant locations, we labeled 1,000 travelogues in the Chinese Corpus for our performance evaluation.

Precision Precision

1

1

baseline

SVM + Co-training

0.9

textual geo

0.9

SVM

0.8

textual + geo

0.8

0.7

0.6

0.7

0.5

0.2 0.4 0.6 0.8

1

Recall

(a) Feature groups

0.2 0.4 0.6 0.8

1

Recall

(b) Co-training

Figure 6: Location relevance classification

For location relevance classification, we consider relevant locations as positive cases and the other locations as negative cases. Among those 1,000 labeled travelogues, we randomly select 100 travelogues as the test set and the remaining travelogues for training. Figure 6(a) shows the precision-recall curves of location relevance classification with different feature groups. We found that the baseline group (i.e., whether the location appears in the title and the count of locations in a travelogue) already provides strong support for deciding relevant locations. However, classifier using baseline features is effective only to some extent. With more textual features, the location relevance classifier gain improvement. From the figure, we observe that geographical features perform classification very well, showing better performance than baseline features particularly when the precision is higher, as baseline features consider only very limited information. Also shown in Figure 6(a), combining textual and geographical features has the best performance because textual features and geographical features are independent. As a result, combining

471

those two provide more information to assess whether a location is relevance or not.
In this work, co-training approach has been adopted to improve the classification performance (i.e., to combine approximately 1,000 labeled travelogues with approximately 93,000 unlabeled travelogues). In our experiment, we set the labeled data set as seed training set. As discussed in Section 5, a confidence threshold  has been used in our co-training algorithm to derive high-quality labeled training set. Here we estimate  based on the precision threshold value of , which is set as  = 0.95 in the test. In the co-training process, both textual-feature classifier and geographical-feature classifier are trained as high-precision classifiers (i.e., by setting  = 0.95). In this experiment, after 4 round of iterations, co-training process terminates. The new training set is then used in classification. We compare the classification results by running the SVM classifier on the original training set (labeled as SVM) and new training set (labeled SVM+Co-Training). Figure 6(b) shows that cotraining leads to a better precision-recall performance. As a matter of fact, this improvement can be expected, because both textual features and geographical features can be used to perform the classification task. Moreover, those two are independent and even complementary, so the combination of them enhance the classification performance as shown in Figure 6(a).

Percentage

0.5

0.4

hyperplane

0.3

relevant paritial relevant irrelevant

0.2

0.1

-05

0

5

10

15

20

f score

Figure 7: Challenge in location relevance classification
To show the inherent challenges in location relevance classification, we collect a training set with relevant and irrelevant locations, with all partial relevant location filtered out. Let x denotes the feature vector of a location, we calculate f (x) = wx + b (denoted as f -score) in the trained SVM model (w is the optimized hyperplane, while b is the bias term) for each location in the travelogues of testing dataset. Here, f -score indicates the distance to the classifier hyperplan. Notice that in the test dataset, locations are categorized into three groups, i.e., relevant locations, partialrelevant locations and irrelevant locations. For each group, we plot its f -score distribution in Figure 7. We find that, f -score distribution of partial-relevant location overlaps to some extent with one of relevant locations. This is due to the ambiguity in location relevance perceived by different people. In contrast to the location relevance classification problem, it seems to be easier to rank the relevance of different locations for a given travelogue. In Figure 7, we find the "centers" of the distributions with respect to each location relevance class differ. As shown, the irrelevant location class is positioned in the leftmost part while the relevant location class stands at the rightmost part.
Next, we evaluate the performance of location relevance ranking techniques, including location likelihood estimation (LLE) and learning to rank (L2R), using the same dataset. There are three labeled ranks in our dataset, i.e., relevant, partial relevant and irrelevant. As discussed earlier, NDCG@N

and MAP are used as the performance metrics for the experiment.6

NDCG@N

1

0.95

LLE

L2R

0.9

0.9

MAP

0.8 LLE-0.2

LLE-0.6

0.7

LLE-1.0

L2R

0.6 1 3 5 7 9 11 N

(a) NDCG@N

0.85

0.8

0.75

0.2 0.4 0.6 0.8

1



(b) MAP

Figure 8: LLE with varied 

NDCG@N

1

0.9

0.8

LLE

L2R-baseline

L2R-textual

0.7

L2R-geo

L2R-textual+geo

0.6 1 3 5 7 9 11 N

(a) NDCG@N

MAP

1.1 LLE L2R-baseline
1 L2R-textual L2R-geo L2R-textual+geo
0.9
0.8
(b) MAP

Figure 9: Improvement with different feature groups
Since there is a smoothing factor  in the location likelihood estimation, we experimentally find the optimal setting of LLE by tuning  from 0.2 to 1.0 with a 0.2 step. Interestingly, although LLE is based on the idea of language model, it shows a behavior different from a conventional language model. Specifically, the smoothing parameter in a language model is used to improve the document ranking performance but it has no effect on location relevance ranking, i.e.,  = 1.0, as shown in our experiments. The is possibly because location relevance ranking is constrained within the given travelogue. As a result, count of a location appearance is much more important than its global popularity.
Next, we show impact of extracted feature groups on LLE and L2R via NDCG@N test (see Figure 9(a)) and MAP test (see Figure 9(b)), respectively. For simplicity, only the best performance (when  = 1.0) of LLE is plotted in these two figures. As shown L2R consistently outperforms LLE. Particularly, since LLE is mainly based on word frequency, which is considered as feature F2, the experiments demonstrate that both textual features and geographical features are indeed effective for improving the performance of location relevance ranking. Also note that the location relevance ranking problem is different from traditional document retrieval problem, where thousands and even millions of candidate documents are to be ranked for a given query. In location relevance ranking, the number of candidate locations in a given travelogue are usually no more than 15. Therefore, it is imperative to ensure all relevant locations ranked higher than the other locations. In the MAP test, we find that the learning to rank approach outperforms the location likelihood estimation approach significantly.
6Note that, we only consider relevant locations and other locations in measurement of MAP, since it considers binary cases in ranking performance evaluation.

472

7.3 Evaluation on the Travelogue Service

Finally, we demonstrate the effectiveness of our travelogue service by evaluating the proposed travelogue digests, including theme locations, geographical scope, traveling trajectory and location snippet.
7.3.1 Theme Locations

In our travelogue service, theme locations can be acquired via 1) location relevance classification; or 2) author tagging. The effectiveness of our classification technique has been studied in the last section. On the other hand, author tagging can be assisted by location tag recommendation backed by our location relevance ranking techniques. Here, we evaluate the precision@N and recall@N of tag recommendation (where N = 1, 3, 5, 7, 9, 11). Figure 10(a) and Figure 10(b) show that both L2R has a much better performance than LLE (especially when N = 3 and 5) in support of location tag recommendation. These results are consistent with the findings in Figure 9(b).

1

1

LLE

LLE L2R

0.8 L2R

0.8

0.6

Precision@N Recall@N

0.4 0.6
0.2

0.4 1 3 5 7 9 11 N
(a) Precision@N

0 1 3 5 7 9 11 N
(b) Recall@N

Figure 10: Theme location

3 Naive Classification
2

Average #

1

0 Missing scopes

Wrong scopes

Figure 11: Geographical scope

7.3.2 Geographical Scope
Here, we would like to evaluate whether location relevance mining can help improve the accuracy for geographical scope identification. Since there is no existing dataset with labeled geographical scope, we use the travelogues labeled with theme locations to derive the ground truth and use labeled theme locations as inputs for our geographical scope computation algorithm.
We introduce two performance metrics, namely, average number of missing scopes (Nm) (i.e., the correct scopes not found) and average number of wrong scopes (Nw) (i.e., the scopes found incorrectly) to evaluate the performance. Let S = {l1, l2, · · · } denote the ground truth scopes for a given travelogue and S be the result we compute. Given two locations l and l , let r(l, l ) denote that l is neither ancestor nor descendant of l in the ontology hierarchy, e.g., r(Boston, Florida) holds for the example in Figure 3 but r(Siesta key, Florida) does not. Thus, we have Nm = |{l|l  S, l  S, r(l, l )}| and Nw = |{l|l  S, l  S, r(l, l )}|.
We compare our approach which takes theme locations as input for geographical scope computation with a naive approach that considers all locations as input. As shown in Figure 11, our approach achieves a much better performance than the naive approach in terms of average number

of wrong scopes, while remaining to be competitive in terms of average number of missing scopes. Hence, location relevance mining which helps filter out irrelevant locations can improve the accuracy of geographical scope identification.
7.3.3 Traveling Trajectory
In this experiment, we prepared 100 travelogues with labeled traveling trajectories as the ground truth dataset. In order to evaluate the accuracy of our proposed trajectory extraction technique, we introduce a performance metrics, namely trajectory similarity (T S(t1, t2)), which is defined as the graph edit distance [11] between two traveling trajectories t1 and t2. Here, we compare our approach with a naive approach that forms a trajectory by considering all locations in a given travelogue as the POI set. As shown in Figure 12, our approach achieves a much better performance since both location relevance classification and POI set formation improve the trajectory extraction accuracy significantly.

Average TS Rating

5

12

naive

10

with R

4

with T

8

3

6

2

naive

4

topic modeling

1

the proposed approach

2

0

0

1

2

3

Aspect

Figure

12:

Test

on

TS

Figure 13: Evaluation location snippet

on

7.3.4 Location Snippets
Here we evaluate the effectiveness of the extracted location snippets. In [7], sentences/small paragraphs with multiple different locations are ignored due to the lack of location relevance mining techniques. However, those ignored snippets may contain interesting information to the users, because point of interests/locations are usually clustered in geographical proximity and mentioned close by in the text of travelogues. For example, Table 2 shows some interesting location snippets that we extract but missed in [7]. Take the location snippet for "Battery Park" as an example. There are quite a few "noise" locations in the snippet but they provide very informative knowledge about what to and see in Battery Park. In the snippet of "the Cloisters", we find that other locations also provide informative knowledge about where "the Cloisters" locates, i.e., "... around of New York. Located up at 190th Street ...". Finally, travelogue authors often describe locations by comparison, e.g., "Golden Gate Park" vs. "Central Park" in the snippet for "Golden Gate Park".
We conduct a user study to evaluate whether our location snippets are informative in terms of conveying information of the corresponding location. Based on the Chinese corpus, we prepared 20 travelogues, where each travelogue consists of more than 5 locations mentioned. Intuitively, if a small paragraph contains a location name l, then we consider this small paragraph should be describing this location in some way. Thus, a naive approach is to consider those small paragraphs as snippets for the corresponding location involved.
Twenty graduate students were asked to assess the snippets of all 20 travelogues in 1 to 5 ratings in three aspects: namely, (1) geographical-relevance (i.e., to what extent the snippets are describing the corresponding location), (2) comprehensiveness (i.e., to what extent the snippets provide rich information about the themes of the given travelogue), and

473

focused location Battery Park
The Cloisters
Golden Gate Park

noisy location(s) 94th Street, Manhattan, the World Trade Center, the Statue of Liberty Metropolitan Museum of Art, New York, 190th Street
NY, Central Park

snippet
Actually , I usually start at the Upper Westside around 94th Street and go all the way to Battery Park at the southern tip of Manhattan. Battery Park is a great place to end up. The scenery is great with the water, the marina, the World Trade Center and the Statue of Liberty... The Cloisters, a branch of the Metropolitan Museum of Art, is not just a museum filled with beautiful medieval art, it 's a place to find serenity around New York. Located up at 190th Street, the Cloisters is a bit of a trek but it's worth it... I call Golden Gate Park a smaller version of NY 's Central Park. It offers so many things to do that I never tire of jogging through it...

Table 2: Location snippet extraction for each individual travelogue.

(3) overall satisfaction. Accordingly, we want to demonstrate whether the proposed method can suggest snippets not only relevant to the corresponding location, but also comprehensive to provide interesting information. For each travelogue, we average all the users' evaluation in all three ratings of each snippet set in a travelogue. Three approaches (i.e., the naive approach, topic modeling approach [7] and the proposed approach) are compared as shown in Figure 13. Since topic modeling approach purges the snippets by deleting those containing multiple locations, it performs the best in terms of geographical relevance. However, our proposed approach also shows very competitive performance in terms of geographic relevance, because our location relevance mining helps to highlight theme locations. For the comprehensiveness, both naive approach and topic modeling approach show worse performance than our proposed approach does. The reason is that our proposed approach includes the location snippet as shown in Table 2, thus providing very comprehensive information against topic modeling approach. Notice that, the naive approach can not differentiate which location is relevant, thus showing the worst performance in the aspect of geographical relevance, and affecting the comprehensiveness consequently. Overall, our proposed approach shows the best performance and user satisfactory.
8. CONCLUSIONS AND FUTURE WORK
In this paper, we develop location relevance mining techniques to discover theme locations of travelogues in support of a proposed travelogue service. Due to inherent ambiguity of location relevance, we perform location relevance mining in two complementary angles, relevance classification and relevance ranking, to provide comprehensive understanding of locations in a travelogue. Furthermore, we explore the textual and geographical features of locations and adopt a cotraining model to improve the classification. Built upon the mining result of location relevance mining, we develop techniques for provisioning of various digests, including theme locations, geographical scope, traveling trajectory and location snippets, in our travelogue service. Finally, we conduct comprehensive experiments on collected travelogues to evaluate the performance of our location relevance mining techniques and demonstrate the effectiveness of the travelogue service.
In the future, we plan to improve the traveling trajectory extraction by considering context provided in travelogue, and utilize location relevance mining techniques to support other travelogue services, such as location-based search.

9. REFERENCES
[1] E. Amitay, N. Har'El, R. Sivan, and A. Soffer. Web-a-where: geotagging web content. In SIGIR, 2004.
[2] C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
[3] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML, 2001.
[4] A. Blum and T. M. Mitchell. Combining labeled and unlabeled data with co-training. In COLT, 1998.
[5] Z. Cao, T. Qin, T.-Y. Liu, M.-F. Tsai, and H. Li. Learning to rank: from pairwise approach to listwise approach. In ICML, 2007.
[6] J. Ding, L. Gravano, and N. Shivakumar. Computing geographical scopes of web resources. In VLDB, 2000.
[7] Q. Hao, R. Cai, C. Wang, R. Xiao, J.-M. Yang, Y. Pang, and L. Zhang. Equip tourists with knowledge mined from travelogues. In WWW, 2010.
[8] U. Irmak and R. Kraft. A scalable machine-learning approach for semi-structured named entity recognition. In WWW, 2010.
[9] T. Joachims. Optimizing search engines using clickthrough data. In KDD, 2002.
[10] J. D. Lafferty and C. Zhai. Document language models, query models, and risk minimization for information retrieval. In SIGIR, 2001.
[11] M. Neuhaus and H. Bunke. Bridging the Gap Between Graph Edit Distance and Kernel Machines. World Scientific Publishing Co., Inc., 2007.
[12] K. Nigam, A. McCallum, S. Thrun, and T. M. Mitchell. Text classification from labeled and unlabeled documents using em. Machine Learning, 2000.
[13] R. Odon de Alencar, C. A. Davis, Jr., and M. A. Gonc¸alves. Geographical classification of documents using evidence from wikipedia. In GIR, 2010.
[14] T. Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang. An Efficient Location Extraction Algorithm by Leveraging Web Contextual Information. In GIS, 2010.
[15] S. E. Robertson and D. A. Hull. The trec-9 filtering track final report. In TREC, 2000.
[16] M. J. Silva, B. Martins, M. S. Chaves, A. P. Afonso, and N. Cardoso. Adding geographic scopes to web resources. Computers, Environment and Urban Systems, 30(4):378­399, 2006.
[17] L. Wang, C. Wang, X. Xie, J. Forman, Y. Lu, W.-Y. Ma, and Y. Li. Detecting dominant locations from search queries. In SIGIR, 2005.

474


Ranking Tags in Resource Collections

Dimitrios Skoutas
L3S Research Center Hannover, Germany
skoutas@l3s.de

Mohammad Alrifai
L3S Research Center Hannover, Germany
alrifai@l3s.de

ABSTRACT
We examine different tag ranking strategies for constructing tag clouds to represent collections of tagged objects. The proposed methods are based on random walk on graphs, diversification, and rank aggregation, and they are empirically evaluated on a data set of tagged images from Flickr.
Categories and Subject Descriptors: H.3 Information Storage And Retrieval: Information Search and Retrieval
General Terms: Algorithms
Keywords: tag clouds, tag ranking
1. INTRODUCTION
Assigning tags to Web resources, such as documents, images, videos or bookmarks, has become popular as a simple, easy and effective way to facilitate their retrieval and management. To reduce the cost and improve the effectiveness of tagging, several approaches have focused on (re-)ranking or suggesting tags for an individual resource, based on its content or the tags of other similar resources (e.g. [3]).
In this work, we consider instead the problem of selecting and ranking tags to describe groups of tagged resources. Typically, tag clouds are employed for this purpose. The goal of a tag cloud is to visualize the most relevant and important tags for the items in a group. In practice, the most frequently occurring tags are selected. We propose and examine alternative methods to select and rank tags in groups of tagged objects, based on tag co-occurance, diversification of tags, and rank aggregation. To compare these methods, we have conducted an experimental evaluation on a large real-world dataset containing groups of tagged photos obtained from Flickr. The comparison considers a set of proposed metrics, namely coverage, overlap and selectivity, that characterize the usefulness of a tag cloud for search and navigation. The results show that even though ranking tags based on their frequency performs well in most cases, more effective rankings can be obtained using other methods.
2. TAG SELECTION STRATEGIES
2.1 Based on Frequency
Frequency scoring. Let f r(t, G) denote the frequency of tag t in the group G, i.e., the number of objects in the group containing t divided by the group size. A simple strategy,
Copyright is held by the author/owner(s). SIGIR'11, July 24­28, 2011, Beijing, China. ACM 978-1-4503-0757-4/11/07.

typically employed in practice, is to construct the tag cloud using the top-k most frequent tags. TF.IDF scoring. An extension is to rank tags based on the same idea as tf .idf scoring for document retrieval. This assigns a lower score to tags that also occur frequently in several other groups. Specifically, the score is modified as:

f (t) = f r(t, G) · idf (t)

(1)

where the idf of a tag is defined analogously to the inverse document frequency of a term in a document collection. Graph-based scoring. Another variation is to take into consideration co-occurrence of tags. For this purpose, we create a graph of the tags, where an edge between two tags ti and tj denotes that there is an object in G tagged with both ti and tj. The score of a tag is derived by performing a random walk on this graph. Initially, the score of each tag t is set to its frequency, i.e., f0(t) = f r(t, G). The transition probability from a tag ti to a tag tj is computed as

p(ti, tj ) = sim(ti, tj )

(2)

sim(ti, t)

tT (G)

where the similarity sim(ti, tj) between two tags can be computed based on the number of objects where these tags co-occur. After each iteration q, the score of each tag is updated according to:

fq(ti) = z ·

fq-1(tj ) · p(tj , ti) + (1 - z) · f0(ti) (3)

tj T (G)

with z being a weight parameter. The process is repeated until the score of each tag t converges to a value f (t).

2.2 Based on Diversity
This strategy increases the diversity [2] of the tags in the cloud to allow for more objects to be represented. We examine two variations. Diversity. The goal is to select tags that are not only as frequent as possible but also as dissimilar as possible from each other, in the sense that they appear in different sets of objects. Therefore, the score of a tag t is defined as

f (t) =  · f r(t, G) + (1 - ) · (1 - max sim(t, ti)) (4)
ti TG
where the parameter  is used to weight the importance of frequency with respect to the factor of diversity. A greedy approximation algorithm can be used to select the top-k tags, similarly to the method described in [2].

1207

(a) coverage

(b) overlap

(c) selectivity

Figure 1: Evaluation results for the metrics (a) coverage (b) overlap and (c) selectivity.

Novelty. Alternatively, we can define the score of tags using the notion of information nuggets. We consider each object to constitute one information nugget. Selecting a tag t, provides a set of information nuggets V (t) corresponding to the objects that this tag is assigned to. Let nv,TG be the number of times a nugget v appears in the tags currently contained in the cloud, and NV the total number of information nuggets in the group. Given a tag t and the current contents of the tag cloud TG, the score of t is defined as

(nv,TG )

vV (t)

f (t) =

(5)

NV

where () is a discount function that reduces the contribution of each information nugget based on the number of times it has already been seen. As above, a greedy approximation algorithm can be used to select the top-k tags.

2.3 Based on Rank Aggregation
A different direction is to rank the tags of the group by aggregating their rankings in individual objects. The Borda Count method can be used for this purpose [1]. According to it, the score of a tag is computed as

(ru(t))

uU (t)

f (t) =

(6)

|G|

where ru(t) is the position of the tag t in the list of tags assigned to object u and  a discount function as above.

3. EXPERIMENTAL EVALUATION

3.1 Dataset
We evaluated the different tag ranking methods on a data set comprising groups of tagged photos collected from Flickr. The data set contained in total 451 groups, 488,112 photos, and 112,514 tags, with each group having on average 1270 photos and 2707 distinct tags.

3.2 Metrics

We are interested in characteristics of a tag cloud that make it useful for search and navigation. For this purpose, we consider the three evaluation metrics described below. Coverage. We define the coverage of a tag cloud TG w.r.t. the group of objects G it represents as the portion of objects in G that have at least one tag appearing in TG:

coverage(TG) =

|{u  G

:

T (u)  TG |G|

= }|

(7)

Overlap. The overlap of two tags ti and tj is defined as the portion of objects tagged with both ti and tj, i.e., |U (ti)  U (tj)| / |U (tj)|. Accordingly, the overlap of the tag cloud is defined as the average overlap between each pair of tags:

|U (ti)  U (tj)|

overlap(TG)

=

ti,tj  TG

|U (tj)|

|TG| · (|TG| - 1)

(8)

Selectivity. Assume a user is interested in object u and selects all the tags of u appearing in the cloud. The result will contain u and all other objects also having (at least) those tags, i.e., every ui  G s.t. T (u)  TG  T (ui). The goal is to maximize the number of filtered out objects. We call this the selectivity of u w.r.t. TG. Thus, we measure the selectivity sel of the tag cloud by computing the average selectivity of the objects in the group:

|{ui  G : (T (u)  TG)  T (ui)}|

|G|

sel(TG) = uG

|G|

(9)

3.3 Results
We refer to the above methods using the abbreviations FRQ, TFIDF, RW, DIV, NOV, and RA, respectively. The presented results in Figure 1 are average values over all the 451 groups in the data set, for different tag cloud sizes.
FRQ performs reasonably well, mainly for coverage and selectivity, but less for overlap. RW and RA exhibit a similar performance to FRQ, with the latter outperforming the other two regarding overlap. In contrast, the performance of TFIDF is very low both for coverage and selectivity, but it outperforms all three previous approaches in overlap. Finally, DIV and NOV perform better than all other approaches. Especially for coverage, NOV achieves excellent results, ranging from 93% for top-k = 20 to 99% for top-k = 100. It also has good results for overlap for all values of k, although for k > 60 DIV becomes equally good or even better. As expected, increasing the size of the tag cloud improves the performance of all methods in all metrics.

4. REFERENCES
[1] J. A. Aslam and M. H. Montague. Models for metasearch. In SIGIR, pages 275­284, 2001.
[2] S. Gollapudi and A. Sharma. An axiomatic approach for result diversification. In WWW, pages 381­390, 2009.
[3] D. Liu, X.-S. Hua, L. Yang, M. Wang, and H.-J. Zhang. Tag ranking. In WWW, pages 351­360, 2009.

1208


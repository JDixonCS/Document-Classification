Enhancing Multi-Label Music Genre Classification Through Ensemble Techniques

Chris Sanden
Mathematics and Computer Science University of Lethbridge
Lethbridge, AB Canada T1K 3M4
sanden@cs.uleth.ca

John Z. Zhang
Mathematics and Computer Science University of Lethbridge
Lethbridge, AB Canada T1K 3M4
zhang@cs.uleth.ca

ABSTRACT
In the field of Music Information Retrieval (MIR), multilabel genre classification is the problem of assigning one or more genre labels to a music piece. In this work, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification. Our goal is to enhance classification performance by combining multiple classifiers. In addition, we also investigate some existing ensemble techniques from machine learning. The effectiveness of these techniques is demonstrated through a set of empirical experiments and various related issues are discussed. To the best of our knowledge, there has been limited work on applying ensemble techniques to multi-label genre classification in the literature and we consider the results in this work as our initial efforts toward this end. The significance of our work has two folds: (1) proposing a set of ensemble techniques specific to music genre classification and (2) shedding light on further research along this direction.
Categories and Subject Descriptors
H.5.5 [Sound and Music Computing]: Methodologies and techniques; H.3 [Information Storage and Retrieval]: Information search and retrieval
General Terms
Experimentation, Performance
Keywords
Music Information Retrieval (MIR), genre classification, multilabel classification, ensemble techniques
1. INTRODUCTION
For the past decade, digital music collections have been growing enormously in volume, due to advances in technologies, such as storage capacity, network transmission, data compression, information retrieval, etc. The rapid rise in
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGIR'11, July 24­28, 2011, Beijing, China. Copyright 2011 ACM 978-1-4503-0757-4/11/07 ...$10.00.

music downloading has created a major shift in the music industry away from physical media formats to electronic distributions. Large on-line music providers now offer catalogs that contain millions of songs. At present, these catalogs are commonly accessed through textual meta-data, such as genre, style, mood, artist, etc. While this meta-data is rich and descriptive, it is difficult to maintain and in many cases is not comprehensive, due to the ambiguity and subjectivity that is introduced in the annotation process [13]. Therefore, manual annotation is insufficient and ineffective when facing large volumes of music.
Music Information Retrieval (MIR) is an emerging area of interdisciplinary research that is engaged in the design and implementation of algorithmic approaches to manage digital music collections [6]. Various tasks, such as music classification, recommendation, fingerprinting, play-list generation, etc., are studied in MIR. In our work, we study the problem of automatic music genre classification. As a fundamental task of MIR, automatic genre classification has attracted considerable attention for many reasons. For instance, music genres have historically been used as categorical descriptions to organize music collections. While no universally accepted definition of genres exists, a genre can be described by the common characteristics shared by its members. These characteristics are related to the instrumentation, harmonic content, rhythmic structures, etc. [23] However, as McKay [11] indicates, there is some controversy regarding the current state of automatic genre classification with some works proposing that it be abandoned in favor of a more general similarity search problem. It has also been suggested that only limited agreement can be achieved among human annotators when classifying music by genres. This imposes an unavoidable ceiling on the performance of automatic genre classification. Moreover, the time and expertise needed to manually classify a corpus of music poses serious obstacles to generating quality ground-truth, against which classification performance can be evaluated. To further complicate matters, the understanding of existing genres can change with time, which can necessitate the re-annotation of ground truth [11].
Despite these controversies, genre categorization provides a common vocabulary which can be used to discuss musical categories. Additionally, users are already accustomed to browsing music collection by genres, and this approach is proved to be at least reasonably effective [11]. Although one may argue that generating ground truth is a serious issue that needs to be handled, manually labeling growing music collections is a much greater challenge. Therefore, contin-

705

uing efforts in automatic genre classification have much to offer. While previous approaches are concerned with learning from a set of music pieces that are each associated with a single label, it has been observed that a music piece could belong to multiple genres. Furthermore, different portions of a music piece might be classified as different genres which stand in contrast to each other [17]. Therefore, it is desirable that automatic genre classification be modeled as a multi-label process.
In our recent studies, we have identified a set of multi-label classification algorithms that perform well on a selection of music datasets. We believe that, due to the diversity of the datasets used and the comprehensibility of our experiments, our results are representative and can be further utilized for algorithmic improvements on genre classification. Toward this end, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification. In addition, we also investigate some existing ensemble techniques from machine learning. To the best of our knowledge, our work is the first to apply ensemble techniques to multi-label music genre classification. It is important to note that, it is commonly accepted that music genres may depend on cultural extrinsic habits. However, in our work, we assume that there are intrinsic properties of the music that can be used for automatic genre classification.
The paper is organized as follows. Section 2 introduces related works in multi-label classification of music while Section 3 discusses the multi-label classification algorithms and their evaluation metrics in our experiments. Section 4 presents our proposed ensemble techniques along with existing ones. Experiment setup is discussed in Section 5 and results are presented and analyzed in Section 6. Finally, Section 7 concludes our presentation with some future work.
2. PREVIOUS WORK
Multi-label classification has been used to categorize music into different emotions. Wieczorkowska et al. [25] address the problem of multi-label classification of emotions using a modified k-NN algorithm. A set of 875 audio samples, 30 seconds each, manually labeled into 13 classes of emotion are used for their experiment. The modified k-NN algorithm aims at taking multiple labels into account. Therefore, the algorithm returns a set of labels for each neighbor of a test instance. In addition, Trohidis et al. [19] evaluate and compare four multi-label classification algorithms for the purpose of detecting emotions in music. Experiments are conducted on a set of 593 samples, from which a 30-second excerpt is extracted and annotated with a set of emotions. It is claimed that the overall predictive performance is high and encourages further investigation of multi-label classification.
Multi-label classification has also been used for the purpose of automatic tag annotation of music. These tags can be any semantically meaningful words and can represent a variety of different concepts, including genre, instrumentation, emotions, geographic origins, social conditions, etc. Automatic tag annotation of music learns a relationship between acoustic features and words from a dataset of labeled audio tracks [12]. The resulting trained model can retrieve audio samples based on lists of tags and annotate unlabeled ones. For example, Wang et al. [24] study the problem of combining user-generated tags and music content for artistic style classification. They investigate the effectiveness of

using tags and audio content separately for clustering and propose a novel language model that makes use of them. Results show that tag features are more effective than music content for artistic style clustering, and the proposed model can marginally improve clustering performance by combining tags and music content.
Despite the volume of previous results on the application of multi-label techniques, to the best of our knowledge, there has been limited work on the application of them to music genre classification. Lukashevich et al. [10] present a two-dimensional approach for genre classification. The multi-label classification problem is decomposed into multiple single-class problems. The novelty of the approach lies in the combination of segment-wise and domain-specific genre classification. The music collection used for testing is comprised of 430 full-length music pieces from 16 world genres. However, as described by Zhang and Zhou [26], decomposing multi-label problems into multiple binary classifications does not consider the correlation between the labels of each instance. Therefore, the expressive power of such an approach can be limited.

3. MULTI-LABEL CLASSIFICATION ALGO-
RITHMS
Multi-label classification deals with problems where an object may belong to one or multiple classes simultaneously. These algorithms can be grouped into two categories as proposed in [20]: (1) problem transformation methods, and (2) algorithm adaptation methods. Problem transformation methods transform a multi-label classification problem into one or more single-label classification problems and offer the flexibility of using any single-label base classifier. Moreover, algorithm adaptation methods extend traditional single-label classifiers to handle multiple labels directly.

Multi-label Classifier
RAkEL CLR ML-kNN HOMER IBLR

Base Classifier
DT SVM N/A SVM N/A

Table 1: Multi-label classifiers along with the associated base classifier, where applicable.
Based on our recent studies with multi-label genre classification, in this work we select and combine the following algorithms using a set of ensemble techniques: Random kLabelsets (RAkEL), Calibrated Label Ranking (CLR), Multilabel k-Nearest Neighbor (ML-kNN), Hierarchy of Multi-label Classifiers (HOMER), and Instance Based Logistic Regression (IBLR). A Decision Tree (DT) is used as a base classifier for RAkEL while a Support Vector Machine (SVM) is used for CLR and HOMER. Table 1 presents the multi-label classification algorithms derived for our study. The details of them can be found in [21].
3.1 Evaluation Metrics
In order to evaluate the performance of multi-label classification algorithms, a selection of evaluation metrics are employed. These metrics can be categorized into three groups: example-based, label-based and rank-based. The first two

706

groups are based on a bipartition vectors while the third group derives ranking information from a score vector and conducts evaluations accordingly. Furthermore, different classification algorithms perform better under different evaluation metrics. Therefore, it is desirable that multiple and contrasting metrics are used in any multi-label classification experiment. We consider the following evaluation metrics for each group.
Example-based: Hamming Loss (HL), Accuracy (Accu.), Recall, F-Measure (F1) and Precision (Prec.);
Label-based: Micro Precision (MicroP), Micro F-Measure (MicroF1), Micro Recall (MicroR), Macro Precision (MacroP), Macro F-Measure (MacroF1) and Macro Recall (MacroR);
Rank-based: Average Precision (AP), Coverage (CO), Ranking Loss (RL), and One-Error (OE).
For the sake of space and due to the nature of our work, we will not digress into the details of these evaluation metrics. The interested reader is referred to [20, 21].
4. ENSEMBLE TECHNIQUES
In this section, we discuss and propose a set of ensemble techniques for improving multi-label music genre classification.
Despite extensive work in multi-label classification, there exist two major challenges. The first challenge is that in many situations, we can have highly imbalanced datasets, due to the availability of instances for some labels. The second is related to our limited knowledge regarding the correlation among class labels for a given dataset. Surprisingly, most multi-label classification algorithms are designed to focus mainly on the second problem and limited work has been devoted to handling imbalanced datasets [18].
One approach to these problems is an ensemble of multilabel classifiers, which consists of a set of individually trained classifiers whose predictions are combined when classifying new instances. This approach is generally more accurate and achieves greater predictive performance than any of the individual classifiers [4].
Ensembles can be homogeneous, where each individual classifier is constructed using the same algorithm, or heterogeneous, where each classifier is constructed from a different algorithm [4, 18]. Some multi-label classification algorithms directly use homogeneous or heterogeneous ensemble techniques internally to improve overall performance. For example, Instance Based Logistic Regression [5] uses a combination of Logistic Regression and Nearest Neighbor classifiers.
Tahir et al. [18] present a first study, as claimed, on combining the outputs of multi-label classification algorithms. They propose two heterogeneous ensemble techniques and apply them to publicly available datasets using several evaluation metrics. The results show that these approaches provide significant performance improvements when compared with individual multi-label classifiers.
The goal of our work is to use a heterogeneous ensemble of existing multi-label classification algorithms to improve music genre classification. In the following, we introduce a set of ensemble techniques that combine multiple classifiers for the purpose of music genre classification. We first introduce some notation. Let D denote a set of music pieces and let L = {l1, l2, ...lN } be the finite set of labels. Given a training

set Ds = {(x1, Y1), (x2, Y2), · · · , (xm, Ym)}, where xi  D is a single music piece and Yi  L is the label set associated with xi. We attempt to design a multi-label classifier that predicts a set of labels for an unseen music piece from a test set Dt = {(x1, Y1), (x2, Y2), · · · , (xn, Yn)}.
An ensemble of multi-label classifiers (EML) trains q multilabel classifiers H1, H2, ...Hq. In our work, all classifiers are likely to be unique and be able to generate different multi-label predictions. For an unseen instance xj, classifier Hk produces two N -dimensional vectors: a score vector Pkj = [pj1,k, pj2,k, · · · , pjN,k], where the value pjb,k is the confidence of the class label lb assigned by classifier Hk being correct, and a bipartition vector Vkj = [v1j,k, v2j,k, · · · , vNj ,k] where vbj,k is 1 if the class label lb is predicted by classifier Hk and 0 otherwise.
We denote by Heml the classifier obtained after applying an ensemble technique to the q classifiers. We use Pejml = [pj1,eml, pj2,eml, · · · , pjN,eml] to represent the resulting score vector for the unseen instance yj and use Vejml = [v1j,eml, v2j,eml, · · · , vNj ,eml] to represent the corresponding binary vector.
There are many techniques to combine the outputs of these q classifiers. In this work, we consider two sets of combiners based on the output of the classifiers. The first set is based on the bipartition vector while the second set is based on the score vector. These techniques are discussed below. It is important to note that, multi-label classification algorithms may only output a score vector. These classifiers can employ a thresholding method to produce bipartitions [7]. In our experiments, we assume that all of the classifiers output a score vector and bipartition vector.
4.1 Bipartition-based Ensemble
4.1.1 Intersection Rule
The Intersection rule (EM LI ) calculates the intersection of the bipartition vectors from the q classifiers using vij,eml = mins(vij,s), for i = 1, 2, · · · , N , i.e., the binary values for each label in all vectors are combined using the logical AND operator. The set of output labels common to all classifiers result in an ensemble decision. We propose this naive method to emphasize the common labels output by all individual classifiers.
4.1.2 Union Rule
The Union rule (EM LU ) calculates the union of the binary vectors from the q classifiers using vij,eml = maxs(vij,s), for i = 1, 2, · · · , N , i.e., the binary values for each label in all vectors are combined using the logical OR operator. An ensemble decision is constructed by computing, for each label, the union of outputs from the q classifiers. We propose this method to optimistically select a label for the ensemble if it is selected by any of the individual classifiers.
4.1.3 Majority Vote Rule
The Majority Vote rule (EM LMV ) counts how many times a label appears in the q classifiers. It is one of the most frequently used ways for combining label outputs [8].

vij,eml =

1 0

where i = 1, 2, · · · , N .

if

q s=1

vij,s/q



0.5

otherwise

707

4.2 Score-based Ensemble
4.2.1 Minimum Rule
The Minimum rule [8, 18](EM LMin) represents a pessimistic view of the scores from the q classifiers and only considers the smallest score for each class label. We calculate the minimum score as shown in the following equation,
pji,eml = mins(pji,s), i = 1, 2, · · · , N.
4.2.2 Maximum Rule
The Maximum rule [8, 18] (EM LMax) represents an optimistic view of the scores from the q classifiers and only considers the largest score for each class label. We calculate the maximum score as shown in the following equation,
pji,eml = maxs(pji,s), i = 1, 2, · · · , N.
4.2.3 Mean Rule
The Mean rule [8, 18] (EM LMean) considers the scores from the q classifiers for a class label. We calculate the mean score as shown in the following equation,
q
pji,eml = pji,s/q, i = 1, 2, · · · , N.
s=1
4.2.4 Top-k Rule
We propose a Top-k rule (denote as EM LT opk), which is a combination of the Maximum and Mean rules. It selects the top k largest scores and averages them. We use the following equation to calculate pji,eml. Constant k is a user-selected parameter. This method represents our desire to have a degree of certainty when calculating the score for Heml.
pji,eml = avg(topk(pji,1, pji,2, · · · , pji,q))
where i = 1, 2, · · · , N , function topk(·) picks the largest k elements from a set and function avg(·) averages them.
4.3 Score-based Label Selection
In addition, we propose a score-based label selection technique wherein we first consider the output score from each classifier and select the top n scores and the corresponding labels. Then, we merge the results from all the classifiers, from which we select the top k labels that appear the most, where n  k. If there is a tie, we arbitrarily select one. For each of these k labels, we set the corresponding entry to be "1" in the bipartition vector for the final ensemble Heml. For the other labels, we set the respective entries to be "0". In this proposed technique, since we first select the top n scores and then select the top k class labels accordingly, we refer to it as EM LCnLk in the following discussions. We note that, this technique produces a bipartition vector and will be compared to the bipartition-based ensemble techniques in our experiments.
4.4 Hierarchical Label Substitution
We propose a label substitution technique that utilizes taxonomy information of music genres to improve multilabel genre classification. We call this technique Hierarchical Label Substitution (HLS). The technique reduces the number of labels using a substitution method. A set of multi-label classifiers are trained and the resulting output is combined using one of the bipartition-based techniques

described above. A set of labels in the ensemble decision are then substituted based on a local genre hierarchy, represented as a taxonomy. For our experiments, we derive our local genre hierarchy based on the taxonomy of music genres developed by Allmusic1. Figure 1 depicts a small portion of the local genre hierarchy.
Figure 1: Example genre taxonomy used for Hierarchical Label Substitution.
Our hierarchy is constructed to only contain music genres in our datasets and consists of top level nodes referred to as "meta genres" [14], as shown in Figure 1. These should represent main musical genres, such as Classical, Rock, Jazz, etc. The hierarchy is further defined to consist of children nodes, i.e., sub-genres, which can be conceptualized as specific genres derived from a parent genre and become more concrete as the depth increases. For example, from Figure 1, Techno is considered as a sub-genre of Electronica. It is important to emphasize that our taxonomy is by no means authoritative but our substitution technique is applicable to any other genre taxonomy.
We define a depth d for label substitution, where all labels below d are substituted with their parent label at this level. For example, for sub-genre Dance, at d = 2, any occurrence of labels in the resulting classifier output below level d will be replaced with their parent label, e.g., Disco would be replaced with Dance and Techno with Electronica. It is easy to see that, as d decreases, the resulting label set shrinks.
Although our label substitution technique produces a smaller label set, the usefulness lies in its ability to simplify the label space by reducing the number of possible "overlapping" genres. Moreover it has been shown [1] that there is little consensus between the genre labels used by individuals. Simplification to a common set of high-level genres may provide a remedy to this.
5. EXPERIMENT SETUP
In this section we describe the preparation for our experiments. We apply the ensemble techniques discussed above to the multi-label classifiers in Table 1 and evaluate their performance. The evaluation is conducted using 10-fold cross validation. The Mulan [22] open source library for multilabel learning is used to train and evaluate each of the classifiers using default parameters, e.g., the number of neighbors is set to 10 for ML-kNN and IBLR, the SVM is trained with a linear kernel, etc.
5.1 Datasets
There are a variety of benchmark datasets available for multi-label classification in various domains. However, no dataset exists specifically for the task of multi-label music
1http://www.allmusic.com.

708

genre classification. Current datasets used in the evaluation of genre classification are comprised of music pieces annotated with a single genre. Therefore, for the purpose of our experiment, we derive three multi-genre datasets from the Magnatagatune collection [9].
Magnatagatune is a collection of approximately 21,000 music clips, each annotated with a combination of 188 different tags. The annotations are collected through an on-line game, referred to as "TagATune", developed to collect tags for music and sound clips. Each clip, 29 seconds in length and sampled at 16kHz, is an excerpt of a music piece published by Magnatune. All of the tags in the dataset have been verified, i.e., a tag is associated with a clip only if it is generated independently by more than two players. Moreover, only those tags that are associated with more than 50 clips are included in the collection.
For our experiment, we are only interested in those clips annotated with musical genres. A set of 22 genre tags, denoted as L, are identified and used to create a subset of the Magnatagatune collection, which is referred to as Ds in our following discussions. Ds is created by filtering the Magnatagatune collection to contain only clips annotated with the set of selected genre tags. The following three datasets are further derived from Ds and discussed further in Section 6.4.
The Random dataset, denoted DRa, consists of 1000 clips chosen at random from Ds. No other selection criteria are used in the creation of the dataset.
The UniqueArtist dataset, denoted DAr, consists of 198 clips derived from Ds. The dataset is created by randomly selecting a single clip from each artist in Ds. The use of an artist filter is recommended as it ensures that the artists in the test set are not present in the training set during classification [2].
The UniqueAlbum dataset, denoted as DAl, consists of 375 clips derived from Ds. The dataset is created by randomly selecting a single clip from each album in Ds. Similar to the artist filter, the use of an album filter ensures that clips from the same album are not present in the training and testing sets simultaneously.

Dataset (D) |D| |L| LC

DRa

1000 22 2.278

DAr

198 22 2.166

DAl

375 22 2.212

Table 2: Multi-genre music datasets and their statistics.
Table 2 displays the datasets and their associated statistics. The label cardinality (LC) of a dataset D is the average number of labels each instance has in D and is used to indicate the number of alternative labels that characterize the instances in a multi-label training dataset [20].
5.2 Audio Parameters
Prior to classification, the music pieces must be parameterized based on a set of features and their changes over time. However, there is no accepted criteria as which features are

best for music classification [3]. Therefore, we use the following set of features which are commonly used for genre classification: MFCC, ZCR, Spectral Centroid, Rolloff, Spectral Flux, and Chroma. The Marsyas2 audio processing framework is used for the computation of the features.
Following a general practice in MIR, we consider a bag-offrames approach based on the aforementioned features [15]. This approach consists of modeling the audio signal as the statistical distribution of audio features computed on individual, short segments. This process yields a large number of feature vectors. Therefore, the feature vectors are then aggregated together using statistical methods. Although more elaborate representations have been proposed in the literature, the simplicity of using a single vector for classification is appealing [12]. Frame-level features in our experiment are compressed into a single set of song-level features by computing the mean across the feature vectors [12].
Furthermore, we explore the effects of frame size on multilabel classification. For each <dataset, classifier> pair, we plot the classification performance as we adjust the frame size, fr, represented as the number of samples collected during a certain time period. Each pair is evaluated using fr  {256, 340, 512, 1024, 2048, 3200, 4096}. In the following, we only report results for fr = 340, corresponding to approximately 23 milliseconds. This frame size has been commonly used in the MIR literature for classification tasks. For instance, Tzanetakis and Cook [23] use a frame size of 23 milliseconds for automatic genre classification. We discuss the effects of frame size on classification performance in Section 6.4.
5.3 Ensemble Parameters
We use the following ensemble parameters in our experiments. For hierarchical label substitution (HLS), we set d = 1, i.e., sub-genres are replaced with their respective "meta genre". In addition, we set k = 3 for EM LT opk, that is, we select the top 3 largest scores and average them. For score-based label selection (EM LCnLk ), we set n = 3 and k = 2. More specifically, we first select the top 3 scores from each classifier and then select the top 2 class labels accordingly. We provide more discussions on these parameters in Section 6.4.
6. RESULTS AND DISCUSSIONS
In this section, we present the results from our experiments. Each ensemble technique is compared with the individual multi-label classifiers to determine its performance.
6.1 Random Dataset
Table 3 shows the comparison of the bipartition-based ensemble techniques with the individual multi-label classifiers for DRa using example-based evaluation metrics; the best result for each metric is shown in bold face. Note that in the Table 3, () indicates better performance when the value is smaller while () indicates better performance when the value is bigger. When the individual multi-label classifiers are compared with each other, we find that CLR and HOMER demonstrate the best performance for different example-based metrics. Furthermore, when we compare the classifiers using label-based metrics, presented in Table 4, we find that CLR performs well for MicroP, MacroP,
2http://marsyas.sness.net.

709

CLR
IBLR
MLKNN
RAkEL
HOMER
EMLU EMLI EMLM V HLS(EMLU ) HLS(EMLI ) HLS(EMLMV ) EMLCn Lk

HL  0.0567 0.0586 0.0576 0.0620 0.0635 0.0710 0.0607 0.0544 0.0402 0.0337 0.0304
0.0601

Accu.  0.5697 0.6045 0.6042 0.5918 0.6255 0.6239 0.4828 0.6233 0.7001 0.6198 0.7322
0.6277

Recall  0.5959 0.6481 0.6354 0.6648 0.6949 0.7893 0.4936 0.6559 0.8435
0.6198 0.7491 0.6731

F1  0.8098 0.8040 0.8120 0.7739 0.7867 0.7568 0.8008 0.8123 0.8177 0.9180
0.8978 0.7843

Prec.  0.8255 0.7715 0.7764 0.7357 0.7299 0.6861 0.8853 0.7958 0.7652 0.9453
0.8741 0.7390

Table 3: Comparison of bipartition-based ensembles for DRa using example-based metrics.

CLR
IBLR
MLKNN
RAkEL
HOMER
EMLU EMLI EMLM V HLS(EMLU ) HLS(EMLI ) HLS(EMLMV ) EMLCn Lk

MicroP  0.8273 0.7661 0.7841 0.7255 0.7039 0.6294 0.8904 0.8011 0.6623 0.9453
0.8566 0.7390

MicroF1  0.6759 0.6886 0.6874 0.6829 0.6877 0.6918 0.6171 0.7065 0.7081 0.6560 0.7262
0.6911

MicroR  0.5721 0.6259 0.6125 0.6453 0.6737 0.7688
0.4728 0.6322 0.7626 0.5031 0.6312 0.6494

MacroP  0.8141 0.6190 0.7910 0.6490 0.6347 0.4677 0.9196 0.8124 0.4731 0.9549
0.8363 0.7493

MacroF1  0.8088
0.6438 0.6622 0.6507 0.7110 0.6513 0.7637 0.6816 0.6206 0.7956 0.6735 0.6543

MacroR  0.2613 0.3277 0.2857 0.3317 0.3175 0.4221
0.2201 0.2905 0.3864 0.1788 0.2333 0.3258

Table 4: Comparison of bipartition-based ensembles for DRa using label-based metrics.

and MacroF1, while IBLR, HOMER, and RAkEL perform well for MicroF1, MicroR, and MacroR respectively. However, by using ensemble techniques, significant performance gains have been observed for a majority of example-based and label-based metrics. Specifically, HLS outperforms the individual multi-label classification algorithms and the EML techniques for example-based metrics and also offers an improvement in classification performance for various labelbased metrics. If we exclude HLS from our analysis and compare the remaining ensemble techniques, we find that a selection of them perform better for a majority of the evaluation metrics. However, we find that there is little consistency between the techniques making it difficult to select one that performs the best overall.

CLR IBLR MLKNN RAkEL HOMER EMLM ax EMLM in EMLM ean EMLT opk

AP  0.8080 0.8067 0.8094 0.7747 0.7240 0.7905 0.7584 0.8195
0.8195

CO 
3.2160 3.4510 3.4420 5.5360 7.2170
3.243 7.181 3.206 3.1690

RL 
0.0569 0.0649 0.0637 0.1129 0.1617 0.0592 0.1556 0.0556 0.0550

OE 
0.1940 0.2050 0.1980 0.2140 0.2440 0.2530 0.1860 0.1840 0.1820

Table 5: Comparison of score-based ensembles for DRa using rank-based metrics.
Table 5 presents the comparison of score-based ensemble techniques with the individual classifiers for DRa using rank-based evaluation metrics. First, when the individual classifiers are compared with each other, we find that CLR

delivers the best performance for all of the metrics. We find this interesting as CLR does not outperform the other multi-label classifiers for the example-based and label-based metrics. Further investigation is needed into this result. As before, the fusion of multi-label classifiers has improved the overall performance for rank-based metrics. We observe that EMLT opk makes an impact on the performance in comparison to the other EML techniques and multi-label classifiers. Moreover, it offers the best performance for all of the rankbased metrics for DRa.
We find that the largest performance improvements are offered by HLS for bipartition-based ensembles and EMLT opk for score-based ensembles, respectively. It should be noted that, the performance gain observed for HLS is predictable as the label space is simplified by reducing the number of possible "overlapping" genres. This is further discussed in Section 6.4.
6.2 UniqueArtist Dataset
Table 6 shows the comparison of the bipartition-based ensembles with the individual multi-label classifiers for DAr using example-based evaluation metrics. We find it interesting that for this data set, when the individual multilabel classifiers are compared with each other, we find a similar trend to the results reported in Table 3. That is, CLR and HOMER perform well for a selection of evaluation metrics. When we compare the individual classifiers using label-based metrics, presented in Table 7, we find that CLR performs well for MicroP, MicroF1, MacroP, and MacroF1, while HOMER and RAkEL perform well for MicroR, and MacroR, respectively. This is similar to the results reported in Table 4. As before, using the proposed ensemble of multi-

710

CLR
IBLR
MLKNN
RAkEL
HOMER
EMLU EMLI EMLM V HLS(EMLU ) HLS(EMLI ) HLS(EMLMV ) EMLCn Lk

HL  0.0618 0.0966 0.0750 0.0798 0.0818 0.1149 0.0749 0.0646 0.0763 0.0485 0.0432
0.0779

Accu.  0.4641 0.3811 0.3765 0.4500 0.4900 0.4647 0.2671 0.4719 0.5146 0.4134 0.6064
0.5053

Recall  0.4833 0.4684 0.3965 0.5234 0.5706 0.7210 0.2680 0.4934 0.7908
0.4134 0.6273 0.5735

F1  0.7390 0.6723 0.6801 0.6932 0.6984 0.6297 0.6865 0.7311 0.6753 0.8517
0.8465 0.7078

Prec.  0.8314 0.5691 0.7418 0.6498 0.6478 0.5139 0.9291 0.7843 0.5631 0.9401
0.8201 0.6133

Table 6: Comparison of bipartition-based ensembles for DAr using example-based metrics.

CLR
IBLR
MLKNN
RAkEL
HOMER
EMLU EMLI EMLM V HLS(EMLU ) HLS(EMLI ) HLS(EMLMV ) EMLCn Lk

MicroP  0.8164 0.5159 0.7229 0.6127 0.5951 0.4514 0.9227 0.7741 0.4786 0.9401
0.7979 0.6133

MicroF1  0.5995 0.4847 0.5025 0.5595 0.5759 0.5528 0.4034 0.5946 0.5744 0.4962 0.6338
0.5890

MicroR  0.4775 0.4603 0.3892 0.5182 0.5637 0.7194 0.2610 0.4847 0.7241
0.3409 0.5276 0.5671

MacroP  0.7903 0.3592 0.6971 0.5298 0.5184 0.3221 0.9137 0.7402 0.3342 0.9402
0.7781 0.5815

MacroF1  0.7590 0.6399 0.6920 0.6665 0.6760 0.6463 0.6463 0.7530 0.6415 0.6588 0.8005
0.7141

MacroR  0.2987 0.3440 0.2476 0.3448 0.3723 0.5386
0.1638 0.3058 0.5217 0.2014 0.3062 0.3658

Table 7: Comparison of bipartition-based ensembles for DAr using label-based metrics.

label classifiers has improved the overall performance for example-based and label-based metrics, i.e., HLS(EMLI ), HLS(EMLU ), and HLS(EMLMV ) deliver improvements for a selected set of example-based and label-based metrics. Furthermore, if we exclude HLS from our analysis and compare the remaining ensembles, we find that for a majority of the evaluation metrics, a selection of the ensembles demonstrate the best performance. However, analogous to DRa, we find it difficult to select one that performs the best overall.

CLR IBLR MLKNN RAkEL HOMER EMLM ax EMLM in EMLM ean EMLT opk

AP 
0.7386 0.6218 0.6907 0.6579 0.6591 0.6910 0.6574 0.7372 0.7430

CO 
4.1342
5.7395 4.8618 8.0492 7.4418 4.3029 8.7550 4.2997 4.3258

RL 
0.0895
0.1412 0.1100 0.1951 0.1762 0.0978 0.2165 0.0923 0.0929

OE 
0.2526 0.4453 0.2926 0.2995 0.3089 0.3695 0.2534 0.2482 0.2179

Table 8: Comparison of score-based ensembles for DAr using rank-based metrics.
Table 8 presents the comparison of score-based ensembles with the individual classifiers for DAr using rank-based evaluation metrics. When the individual multi-label classifiers are compared with each other, we find that CLR performs the best for all of the metrics. However, the combination of multi-label classifiers offers improvements for different evaluation metrics. For example, performance gains are observed for the evaluation metrics AP and OE with an ensemble using the Top-k rule (EMLT opk). We also observe that CLR

outperforms all of the proposed ensemble techniques for the evaluation metrics CO and RL. Note the difference in CO for CLR and EMLMean. This will be discussed further in Section 6.4.
We find it interesting that for this dataset the average classification performance of each multi-label classifier and ensemble is lower than the performance on DRa. These results are expected as classification accuracy can be lower if an artist filter is used since an artist is restricted from appearing in both the training and testing sets [16]. This will be discussed further in Section 6.4.
6.3 UniqueAlbum Dataset
Finally, Table 9 shows the comparison of the bipartitionbased ensembles with the individual multi-label classifiers for DAl using example-based evaluation metrics. It is easy to see that the results for the individual classifiers are similar to those observed for DRa and DAr. Specifically, we find that CLR performs well for HL, F1 and Prec. while HOMER delivers the best performance for Accu. and Recall. Moreover, when we compare the individual classifiers using label-based metrics, presented in Table 10, we find that CLR and HOMER perform well for various evaluation metrics. However, using the proposed ensemble techniques improves classification performance. Once again, we observe that HLS outperforms the other ensembles and classifiers for the majority of the evaluation metrics. If we exclude HLS from our analysis, we find that the performance of the remaining ensembles is similar to those reported on DRa and DAr, i.e., the ensembles demonstrate the best performance for a majority of the evaluation metrics. However, there is

711

CLR
IBLR
MLKNN
RAkEL
HOMER
EMLU EMLI EMLM V HLS(EMLU ) HLS(EMLI ) HLS(EMLMV ) EMLCn Lk

HL  0.0690 0.0843 0.0765 0.0844 0.0837 0.1071 0.0788 0.0687 0.0722 0.0505 0.0444
0.0774

Accu.  0.4240 0.4327 0.3893 0.4364 0.4964 0.4795 0.2661 0.4685 0.5309 0.4106 0.5891
0.5115

Recall  0.4438 0.5075 0.4105 0.5193 0.5896 0.7246 0.2705 0.4932 0.7950
0.4106 0.6084 0.5803

F1  0.7726 0.7008 0.7160 0.6830 0.7077 0.6465 0.7156 0.7551 0.6898 0.9001
0.8479 0.7078

Prec.  0.7915 0.6239 0.7189 0.6097 0.6208 0.5289 0.8560 0.7376 0.5778 0.8925
0.8034 0.6272

Table 9: Comparison of bipartition-based ensembles for DAl using example-based metrics.

CLR
IBLR
MLKNN
RAkEL
HOMER
EMLU EMLI EMLM V HLS(EMLU ) HLS(EMLI ) HLS(EMLMV ) EMLCn Lk

MicroP  0.7905 0.5977 0.7206 0.5942 0.5884 0.4788 0.8626 0.7468 0.4887 0.8925
0.7852 0.6272

MicroF1  0.5522 0.5425 0.5090 0.5479 0.5801 0.5739 0.3923 0.5822 0.5821 0.4613 0.6102
0.5956

MicroR  0.4276 0.4976 0.3960 0.5094 0.5753 0.7180 0.2558 0.4786 0.7227
0.3135 0.5004 0.5672

MacroP  0.7645 0.3931 0.6664 0.4929 0.5282 0.3176 0.8269 0.7075 0.2983 0.8993
0.7204 0.6308

MacroF1  0.6965 0.6262 0.6498 0.6156 0.6702 0.6098 0.5679 0.7246 0.5937 0.6222 0.7495
0.6484

MacroR  0.2408 0.3296 0.2220 0.3079 0.3394 0.4924
0.1400 0.2630 0.4760 0.1615 0.2458 0.3301

Table 10: Comparison of bipartition-based ensembles for DAl using label-based metrics.

little consistency between the techniques making it difficult to select one that performs the best overall.

CLR IBLR MLKNN RAkEL HOMER EMLM ax EMLM in EMLM ean EMLT opk

AP  0.7276 0.6719 0.7134 0.6572 0.6341 0.6892 0.6506 0.7388
0.7369

CO 
4.1210
4.9301 4.4022 7.2234 8.1062 4.2461 8.7422 4.1885 4.1936

RL  0.0872
0.1144 0.0957 0.1725 0.1988 0.0939 0.2130 0.0872
0.0882

OE 
0.3052 0.3977 0.2916 0.3275 0.3527 0.3834 0.2994 0.2658 0.2576

Table 11: Comparison of score-based ensembles for DAl using rank-based metrics.
Table 11 presents the comparison of score-based ensembles with the individual classifiers for DAl using rank-based evaluation metrics. With regard to the individual multi-label classifiers, we observe that CLR performs the best for all of the metrics. This is analogous to the results presented for DRa and DAr. As before, improvements are observed by using the proposed ensemble of multi-label classifiers. Specifically, EMLMean demonstrates the best performance of the proposed ensembles while EMLT opk follows shortly behind. Furthermore, we observe that CLR outperforms the other proposed ensemble techniques for Coverage. However, we note that the difference is marginal.
6.4 Discussions
The results in our experiments show the merits of combining multi-label classification algorithms for music genre clas-

sification. For all three datasets, we observe improvements to classification performance when using heterogeneous ensemble techniques. However, this performance gain is at the expense of inherent computational cost as individual classifiers need to be trained and combined. Additionally, some of the proposed ensemble techniques provide no improvement and it would be interesting to further explore them. It is important to note that all of the processing and experiments performed in this work are conducted off-line. Therefore, we are not primarily concerned with the inherent computational cost incurred. In the following, we discuss some other related issues.
Frame Size As discussed in Section 5, to capture the finetimescale structures of music, features are typically extracted from short audio frames. We present a brief systematic exploration of the effects of frame size on multi-label genre classification. We examine the classification performance of each classifier and EML as we adjust the frame size, fr, represented as the number of samples collected during a certain time period. Each classifier and ensemble is evaluated using fr  {256, 340, 512, 1024, 2048, 3200, 4096}, corresponding to approximately 16ms, 21ms, 32ms, 64ms, 128ms, 200ms, and 256ms, respectively.
For the sake of space, we only present the performance of the proposed ensembles along with two individual classification algorithms for comparison using DRa. Figure 2 shows the classification performance of the proposed bipartitionbased ensembles along with CLR and HOMER using the evaluation metric HL. Moreover, Figure 3 shows the classification performance of the proposed score-based ensembles using the evaluation metric AP. We observe that the pro-

712

Hamming Loss Average Precision

0.14 0.12
0.1 0.08

CLR
HOMER EMLU EMLI
EMLMV HLS(EMLU) HLS(EMLI) HLS(EMLMV)
EMLCnLk

0.06

0.04

0.02

256

340

512

1024

2048

3200

4096

Frame Size

Figure 2: Bipartition-based ensemble performance using DRa for various frame sizes.

0.85

0.8

0.75

0.7

0.65
0.6 256

CLR HOMER EMLMax EMLMin EMLMean EMLTopk
340

512

1024

2048

Frame Size

3200

4096

Figure 3: Score-based ensemble performance using DRa for various frame sizes.

posed ensembles tend to perform well when fr = {340, 512, 1024, 3200}. However, we note a decrease in performance when fr = {2048, 4096}. These results are consistently observed for the other evaluation metrics and datasets with the exception that classification performance increases for DAl when fr = 4096. These results lend support to the notion of using a frame size of 23ms, as proposed by various other works.
k AP  CO  RL  OE  2 0.8118 3.2420 0.0575 0.1950 3 0.8195 3.1690 0.0550 0.1820 4 0.8194 3.2000 0.0559 0.1760 5 0.8190 3.1690 0.0553 0.1900
Table 12: Classification performance of EM LT opk for different k.
Top-k rule: The Top-k rule (EM LT opk) selects the top k largest scores and averages them, where k is a user-selected parameter. To determine this value, we perform a series of experiments on DRa and examine the performance as we adjust k. Table 12 shows the classification performance of EM LT opk where k  {2, 3, 4, 5} using the evaluation metrics AP, CO, RL, and OE. We observe that EM LT opk demonstrates good achievement on DRa when k = 3 for the majority of evaluation metrics. We note that when k is set to the number of classifiers in the ensemble, the result will be the same as the one obtained using the Mean rule.
Score-based Label Selection: Similar to the Top-k rule, we perform a series of experiments on DRa to determine the values of n and k for EM LCnLk . Recall that in this ensemble, we first select the top n scores and then select the top k class labels accordingly. Table 13 shows the classification performance of EM LCnLk using the evaluation metrics HLoss, Accu., Recall, F1, and Prec. We observe that as both values of n and k increase, classification performance tends to decrease for the evaluation metrics. We can see that EM LCnLk performs best when n = 3 and k = 2. This could be plausibly explained by the label cardinality which is between 2 and 3 for each instance in our datasets, as shown in Table 2. It is important to note that we examine a wide range of values for n and k. However, due to space limitations, we only report a subset of the results.

Hierarchical Label Substitution: From the results presented above, we observe that Hierarchical Label Substitution (HLS) outperforms the bipartition-based ensemble techniques for a majority of the evaluation metrics. Moreover, significant performance gains are observed for all of the dataset when HLS is used with one of the proposed bipartition-based ensemble techniques. For example, there is a 14.51% increase in Prec. for DRa when compared to CLR. These results are attributed to the simplification of the label space by reducing the number of possible "overlapping" genres. We believe this ensemble produces a common set of high-level genres which is closer to the human experience.

n=3, k=2 n=4, k=2 n=5, k=2 n=5, k=3 n=5, k=5 n=10, k=3

HLoss  0.0603
0.0674 0.0681 0.0820 0.1483 0.1398

Accu.  0.6255
0.5799 0.5786 0.5339 0.3902 0.3006

Recall  0.6725 0.6366 0.6328 0.7825 0.8934
0.4876

F1  0.7799
0.7422 0.7484 0.7034 0.5611 0.5230

Prec.  0.7380
0.6990 0.6950 0.5790 0.4016 0.3670

Table 13: Classification performance of EM LCnLk for different n and k.

Data Selection We observe, on average, that DRa achieves the best dataset performance as shown in Tables 3, 4, and 5. Moreover, classification performance on DAr is lower than DRa and DAl. The artist filter limits performance by restricting artists from appearing in both the training and testing sets. In addition, DAl employs an album filter to exclude music pieces from the same album appearing in both training and testing sets. However, this does not exclude artists from appearing in them. For this reason, we observe that the classification performance on DAl is better than DAr but worse than DRa. We also notice a larger difference in the performance of the multi-label classifiers and ensemble techniques between DAr and DAl. For example, if we examine the evaluation metric CO for CLR and EMLMean we find that the difference between them in DAr (see Table 8) is larger than the difference between them in DAl (see Table 11). This is due to the data selection as there is some possible overlap in DAl, as discussed above.

713

7. CONCLUSION
In this work, we propose a set of ensemble techniques that not only improve upon individual multi-label classification algorithms, but also overcomes their limitations as aforementioned. For all of the datasets, we observe improvements to classification performance when using ensemble techniques. To the best of our knowledge, this is the first study that aims to combine heterogeneous multi-label classification algorithms for music genre classification.
To improve the impact of this work, we plan to use more multi-label classification algorithms and investigate other ensemble techniques. From our study, we observe that some of the proposed ensemble techniques provide no improvements and it would be interesting to further explore them. A wealth of work exists surrounding the area of ensemble methods and may offer some insight. Moreover, further investigation is needed into the selection of classifier parameters. Finally, while we focus on the computational classification of music genres, it would be interesting to investigate how to combine our conclusions here with human perception of music. We believe that this could further improve music genre prediction.
8. REFERENCES
[1] J. J. Aucouturier and F. Pachet. Representing musical genre: A state of the art. J. New Music Research, 32(1):83­93, 2003.
[2] J. J. Aucouturier and F. Pachet. Improving timbre similarity: How high is the sky? J, of Negative Results in Speech and Audio Sciences, 1(1), 2004.
[3] J. Bergstra, N. Casagrande, D. Erhan, D. Eck, and B. Kgl. Aggregate features and AdaBoost for music classification. Machine Learning: Special Issue on Machine Learning in Music, 65(2-3):2­3, 2006.
[4] N. V. Chawla and J. Sylvester. Exploiting diversity in ensembles: improving the performance on unbalanced datasets. In Proc. Int'l Conf. on Multiple Classifier Systems, pages 397­406, 2007.
[5] W. Cheng and E. Hullermeier. Combining instance-based learning and logistic regression for multilabel classification. Machine Learning, 76(2-3):211­225, 2009.
[6] J. Futrelle and J. S. Downie. Interdisciplinary communities and research issues in music information retrieval. In Proc. Int'l Conf. Music Information Retrieval, pages 121­131, 2002.
[7] M. Ioannou, G. Sakkas, G. Tsoumakas, and I. Vlahavas. Obtaining bipartitions from score vectors for Multi-Label classification. In Proc. Int'l Conf. Tools with Artificial Intelligence, pages 409­416, 2010.
[8] L. Kuncheva. Combining Pattern Classifiers: Methods and Algorithms. Wiley-Interscience, 2004.
[9] E. Law and L. von Ahn. Input-agreement: a new mechanism for collecting data using human computation games. In Proc. Int'l Conf. Human Factors in Computing Systems, pages 1197­1206, 2009.
[10] H. Lukashevich, J. Abeßer, C. Dittmar, and H. Grossmann. From multi-labeling to multi-domain-labeling: A novel two-dimensional approach to music genre classification. In Proc. Int'l Conf. Music Information Retrieval, pages 459­464, 2009.

[11] C. McKay and I. Fujinaga. Musical genre classification: Is it worth pursuing and how can it be improved? In Proc. Int'l Conf. Music Information Retrieval, pages 101­106, 2006.
[12] S. R. Ness, A. Theocharis, G. Tzanetakis, and L. G. Martins. Improving automatic music tag annotation using stacked generalization of probabilistic svm outputs. In Proc. ACM Int'l Conf. Multimedia, pages 705­708, 2009.
[13] F. Pachet. Content management for electronic music distribution: The real issues. Comm. of the ACM, 46(4):71­75, 2003.
[14] F. Pachet and D. Cazaly. A taxonomy of musical genres. In Proc. Content-Based Multimedia Information Access, 2000.
[15] F. Pachet and P. Roy. Improving multi-label analysis of music titles: A large-scale validation of the correction hypothesis. IEEE Trans. on Audio, Speech & Language Processing, 17(2):335­343, 2009.
[16] E. Pampalk, A. Flexer, and G. Widmer. Improvements of audio-based music similarity and genre classification. In Proc. Int'l Conf. Music Information Retrieval, pages 628­633, 2005.
[17] C. Sanden, C. R. Befus, and J. Z. Zhang. Perception based multi-label genre classification on music data. In Proc. Int'l Computer Music Conf., pages 9­15, 2010.
[18] M. Tahir, J. Kittler, K. Mikolajczyk, and F. Yan. Improving multilabel classification performance by using ensemble of multi-label classifiers. In Multiple Classifier Systems, volume 5997, chapter 2, pages 11­21. Springer Berlin Heidelberg, 2010.
[19] K. Trohidis, G. Tsoumakas, G. Kalliris, and I. Vlahavas. Multilabel classification of music into emotions. In Proc. Int'l Conf. Music Information Retrieval, pages 325­330, 2008.
[20] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. Int'l J. Data Warehousing and Mining, 3(3):1­13, 2007.
[21] G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data. Data Mining and Knowledge Discovery Handbook, 2010.
[22] G. Tsoumakas, J. Vilcek, E. Spyromitros, and I. Vlahavas. Mulan: A Java library for multi-label learning. J. Machine Learning Research, Accepted, 2010.
[23] G. Tzanetakis and P. Cook. Musical genre classification of audio signals. In IEEE Trans. on Speech and Audio Processing, pages 293­302, 2002.
[24] D. Wang, T. Li, and M. Ogihara. Are tags better than audio? The effect of joint use of tags and audio content features for artistic style clustering. In Proc. Int'l Conf. Music Information Retrieval, pages 57­62, 2010.
[25] A. Wieczorkowska, P. Synak, and R. Zbigniew. Multi-label classification of emotions in music. In Intelligent Information Processing and Web Mining, Advances in Soft Computing, pages 307­315. 2006.
[26] M. L. Zhang and Z. H. Zhou. ML-kNN: A lazy learning approach to multi-label learning. Pattern Recognition, 40(7):2038­2048, 2007.

714


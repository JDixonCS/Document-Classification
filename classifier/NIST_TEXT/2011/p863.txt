A Novel Corpus-Based Stemming Algorithm using Co-occurrence Statistics

Jiaul H. Paik
Indian Statistical Institute Kolkata, India
jia.paik@gmail.com

Dipasree Pal
Indian Statistical Institute Kolkata, India
dipasree.pal@gmail.com

Swapan K. Parui
Indian Statistical Institute Kolkata, India
swapan@isical.ac.in

ABSTRACT
We present a stemming algorithm for text retrieval. The algorithm uses the statistics collected on the basis of certain corpus analysis based on the co-occurrence between two word variants. We use a very simple co-occurrence measure that reflects how often a pair of word variants occurs in a document as well as in the whole corpus. A graph is formed where the word variants are the nodes and two word variants form an edge if they co-occur. On the basis of the co-occurrence measure, a certain edge strength is defined for each of the edges. Finally, on the basis of the edge strengths, we propose a partition algorithm that groups the word variants based on their strongest neighbours, that is, the neighbours with largest strengths.
Our stemming algorithm has two static parameters and does not use any other information except the co-occurrence statistics from the corpus. The experiments on TREC, CLEF and FIRE data consisting of four European and two Asian languages show a significant improvement over no-stem strategy on all the languages. Also, the proposed algorithm significantly outperforms a number of strong stemmers including the rule-based ones on a number of languages. For highly inflectional languages, a relative improvement of about 50% is obtained compared to un-normalized words and a relative improvement ranging from 5% to 16% is obtained compared to the rule based stemmer for the concerned language.
Categories and Subject Descriptors
H.3.1 [Information Systems]: Content Analysis and Indexing - linguistic processing, indexing; H.3.3 [Information Systems]: Information Search and Retrieval
General Terms
Algorithm, Experimentation
Keywords
Stemming, Corpus analysis
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for prof t or commercial advantage and that copies bear this notice and the full citation on the f rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specif c permission and/or a fee. SIGIR'11, July 24­28, 2011, Beijing, China. Copyright 2011 ACM 978-1-4503-0757-4/11/07 ...$10.00.

1. INTRODUCTION
Vocabulary mismatch between queries and documents is a well known problem in an IR system. There are several forms of vocabulary mismatch, and the variation of words because of the morphological process (both derivational and inflectional) is such an instance. Stemming is a form of natural language processing that is commonly used in an IR system to address the aforementioned problem.
Stemming is a well studied technology and several stemmers of varying spirit and flavor exist in the literature. The most commonly used stemmers encode a set of language specific rules to group the morphologically related words. The potential danger of such stemmers is that they often produce aggressive conflation (for example, Porter stemmer conflates both execute and executive to the same term [22]).
The corpus based stemmers on the other hand do not use language specific rules and try to find groups from the ambient corpus. Depending upon the nature of the algorithm, corpus based stemmers can be classified into two broad categories. In one category, simply the lexicon (acquired from the corpus) is analyzed to find groups, and in other an additional evidence such as co-occurrence of word variants or the context of the words in which they appear are used. The former category of corpus based stemmers is found to give comparable performances with the rule based stemmers, whereas corpus analysis based stemmers with a background stemmers are much better alternatives, particularly in precision oriented systems such as web search [18].
Therefore, a solution to this problem of aggressive stemming is the usage of co-occurrence data that carries an additional evidence of their relatedness. Like Xu et. al [22], we hypothesize that, if two words co-occur in a document quite often then the likelihood of their proximity is high. We further advance our argument that words in a set are related if they co-occur as well as they form a community among themselves. Therefore, words which co-occur within a community are much better members to be grouped in a class rather than words which do not. In doing so, we might miss out some words which are actually morphological variations. But we believe such cases is either infrequent or cause less damage.
Several corpus based approaches [12, 22] exist for stemming which perform efficiently. For example, YASS, a method based on a string distance measure and complete linkage clustering algorithm, performs fairly well on languages like English, French, Bengali etc., but it is relatively ineffective on highly inflectional languages like Marathi and Hungarian.
The method developed by Xu et. al. [22] carries certain

863

weaknesses. First, the association measure depends on some parameter and therefore proper tuning is necessary. Second, two graph partition algorithms (connected component and optimal partition) used have different behaviors. The connected component algorithm creates chains [7] and the optimal partition algorithm is infeasible for complex languages where the morphologically related classes have large sizes. Obviously, the optimal partition algorithm can be used by first creating the classes using some background stemmer (as in case of English they used Porter and Krovetz stemmer), but that requires an additional constraint and may be difficult to apply on a language which does not have a background stemmer.
One important criterion of clustering algorithms is the determination of parameters that is used to form the clusters. For example, in partitional clustering algorithms (like k-means), the number of clusters needs to be specified beforehand. Also, in hierarchical clustering algorithms (like single linkage), a similarity threshold value is to be specified to determine the actual number of clusters. Both methods are crucially dependent on the parameter values. Parameters also depend on several factors like, the nature of the language, or the nature of the corpus, or the kind of similarity measure used etc. So a desirable property of a clustering algorithm is its insensitivity to parameters. Therefore, we take this argument into account in designing our algorithm and finally end up developing a clustering method which does not depend on any parameter.
Therefore, in the light of the above the purpose of our algorithm is three fold. The first aim is to see whether solely based on co-occurrence data we can design an effective language independent stemming method for mono-lingual retrieval. The second aim is to see whether it can act as a better alternative to stemmers based on extensive language specific rules. Thirdly, we want to develop a method for which the necessity of parameter fine tuning is limited thereby improving the robustness in performance.
The overview of our method is the following. We first collect the co-occurrence of words from the given corpus. Then the words sharing a common prefix of a given length are mapped in a graph with their edge weights equal to the value given by the co-occurrence metric. These strengths are then used to redefine the co-occurrence strength between words using their common co-occurring neighbours (words). Finally, this strength works as the basis of our grouping algorithm which proceeds as follows. First, it identifies some specially designated edges called strong edges and deletes all other edges which are not strong. Once this has been done, the next step finds the clusters of words simply by applying the connected component algorithm.
The organization of our article is as follows. In the next section, we discuss about the background work on various stemming strategies. Section 3 presents the proposed approach. The description about the data and the retrieval system is given in section 4. Experimental results are presented in section 5. In section 6 we discuss about possible merits and demerits of the baseline stemmers and the proposed stemmer. We conclude in section 7.
2. RELATED WORK
Stemming is a long studied technology and several stemmers of various principles and flavors exist in the literature. Broadly, they can be categorized into two classes, one cat-

egory of stemmers relies on language specific rules and the other category is the corpus based approaches. The advantage of corpus based stemmers over the rule based methods is that they do not require knowledge of the specific languages. Additionally, corpus based stemmers reflect certain characteristics of the ambient corpus and therefore, are sometimes more effective compared to the rule based stemmers [22]. Corpus based stemmers can again be subdivided into mainly two types depending upon their principle. The one category simply analyzes the lexicon (words from a corpus) to understand the morphology of the concerned language. One category of corpus based methods, uses the context of the words to be stemmed or their co-occurrences in the particular corpus. For precision oriented systems, such analysis are reported to perform better than the blind rule based stemmers [22, 18]. Furthermore, corpus analysis can again be used to improve an already existing aggressive stemmer. Apart from the aforementioned methods, there are a number of works reporting the effectiveness of character n-gram tokenization for addressing the problem for alphabetic languages [14, 16]. In the rest of this section, we review some of these methods.
2.1 Language specif c methods
Perhaps the most widely used language specific stemmers used for English IR is the Porter [19] algorithm. A number of other language dependent methods exist such as Lovins [10] and Krovetz [8]. Lennon et. al. [9] compared the performance of Porter and Lovins stemmer on English queries and found little improvement. Harman [6] also experimented with English data using three stemmers without any positive outcomes. Nevertheless, subsequent studies [8, 22] demonstrated the usefulness of stemmers on simple language (like English) IR. The most notable advantage of these methods is their ease of use : rules once designed can be used on any corpus without further processing. Generally, these methods handle both derivational as well as inflectional morphology. But the obvious requirement to design such stemmers is the knowledge of the language.
A number of works have reported the effectiveness of light stemming approaches [20, 4] for a variety of languages both European and Asian. As opposed to the stemmers like Porter, light stemming methods focus on removing only inflectional variants from the end of a word.
2.2 Statistical methods
2.2.1 Lexicon analysis based approach
Majumder et al. [12] developed a language independent, clustering based unsupervised stemming algorithm (YASS) that is capable of handling a family of primarily suffixing languages. They defined a string distance measure between word pairs that reward long matching prefixes and penalize an early mismatch as follows. Let the length of two strings X and Y be n + 1 (if strings are of unequal length, null characters are added at the end of the shorter string to make them equal), and let m denote the position of the first mismatch between X and Y (i.e. x0 = y0, x1 = y1, . . . , xm-1 = ym-1, but xm = ym). Now define D as follows:

864

D(X, Y

)

=

n

-m m

+

1

×

n
X

1 2i-m

if m > 0,

i=m

 otherwise (1)

First of all, such a string distance does not have the knowl-

edge of whether one word is a morphological variation of the

other. The string distance returns a high similarity simply

because two words share a long prefix. Complete linkage

clustering was used to discover groups of (presumably mor-

phologically related) words. They concluded that in terms

of retrieval effectiveness, this approach is comparable to rule

based stemmers like Porter or Lovins for English. On Ben-

gali and French test collections, YASS provides substantial

performance improvement as compared to using no stem-

ming.

Goldsmith [5] presented an unsupervised morphological

analyzer based on minimum-description-length model. The

frequency of stems and suffixes that result from every pos-

sible breakpoint in each term of a collection is examined. A

breakpoint for each token is optimal if every instance of a

token must have the same breakpoint and which minimizes

the number of bits necessary to encode the collection. The

underlying intuition is that breakpoints should be chosen in

such a way that each token can be segmented into relatively

common stems and common suffixes.

Bacchin et al. [2] described a language independent prob-

abilistic model for stemmer generation which makes use of

the mutual reinforcement relationship between stems and

suffixes. From a finite collection of words they first generate

a set of substrings (prefixes and suffixes) by splitting each

word at all possible positions, except for those which gen-

erate empty substrings. Then they form a directed graph

where a node implies a substring and a directed edge be-

tween node x and y exists if there is a word z in the collection

such that z = xy. Now they estimate the prefix score and

suffix score using HITS algorithm with the assumption that

the good prefixes point to good suffixes, and good suffixes are

pointed to by good prefixes. Once the prefix and suffix scores

are estimated, the algorithm determines the most probable

split (into prefix and suffix pair) for each word in the collec-

tion which maximizes the probability of the prefix and suffix

pair.

Oard et al. [17] did suffix discovery statistically in a

text collection and eliminated the word endings. They first

counted the frequency of every one, two, three and four char-

acter suffix that would result in a stem of three or more

characters for the first 500,000 words of the collection. Then

they subtracted the frequency of the most common subsum-

ing suffix of the next longer length from each suffix (for ex-

ample, frequency of "-ing" from the the frequency of "-ng").

The adjusted frequencies were then used to sort all n-gram

suffixes in descending order. In case of English, the count

versus rank plot was found to be convex and so the rank

at which the second derivative was maximum was chosen as

the cutoff limit for the number of suffixes for each length.

2.2.2 Corpus analysis based approach
Xu and Croft [22] investigated the usefulness of corpus analysis in ad hoc retrieval. The basic assumption of their work is that word variants that should be conflated will occur in the same documents or, more specifically, in the same text window (they reported on 100 word text window).

Based on this assumption, they devise a co-occurrence metric that measures the association of two words. The measure is given below.

em(a,

b)

=

max

,,

nab - na

En(a, + nb

b)

,

« 0

where En(a, b) = knanb
where na, nb are the number of occurrences of words a and b in the corpus, and nab is the number of times both a and b fall in the chosen window. Here k is a parameter and can be estimated from the corpus. Two graph partitioning algorithms (connected component, optimal partitioning) are then used to refine the initial equivalence classes of words that shares a number of characters in their common prefix or the classes generated by different rule based stemmers like Porter or Krovetz. This technique helps to identify the corpus dependent conflations and also minimizes the bad conflations (universal and university to the same class) made by a linguistic rule based stemmers. The reported results show that corpus analysis indeed improved the performance significantly on both English and Spanish corpora. Our method is similar in spirit with this approach, but varies in many respects. First, we use much simpler co-occurrence measure. Second, we introduce a re-weighting scheme that uses the initially calculated co-occurrence to respect their community affinity. Finally, the method for forming clusters here is different from connected component algorithm and is based on the nearest neighbour. Therefore, the nature of our proposed clustering algorithm provides us the liberty to ignore the tuning of parameters.
Recently Peng et. al [18] proposed a context sensitive stemming approach to web search. On the query side, they propose a statistical language modeling based approach to predict which word variants are better forms than the original word for search purpose and expanding the query with only those forms. In order to decide which morphological variants are better candidates to add to the original query, they use the bigrams to the left and right of a word as its context features by mining a large web corpus. At the same time on the document side, they propose a conservative context sensitive matching for the transformed word variants, only matching document occurrences in the context of other terms in the query. The method is found to perform effectively on web data. However, their method is not completely independent unlike others, since they determine the possible morphological variants using an already existing stemmer (such as porter).
2.2.3 Character N-gram based approach
Very recently, character n-gram tokenization [16] has been used to address the morphological variations in alphabetic languages. In contrast with the other methods presented in this section, n-gram method not only handles derivational and inflectional morphology, but also copes with the spelling variations as well as the word forms due to compounding. In particular, 4 and 5 grams provide substantial performance improvement on a wide variety of languages of different origins. One major pitfall with n-grams is the increase in the size of the inverted index. A passage of k characters contains k - n + 1 n-grams of length n, but only approximately (k + 1)/(l + 1) words, where l is the average word length for the language [15]. As a consequence of index size expansion,

865

n-gram based indexing consumes 6 times disk space as opposed to plain word indexing and queries can take 8 times as long to execute.

3. PROPOSED WORK
The goal of the proposed method is to group the set of morphologically related words appearing in a corpus. The method takes a set of unique words and their co-occurrence information and returns a set of groups. We impose a necessary condition that the two words are morphologically related if they share a common prefix of given length. We follow here the same argument given by Xu et. al [22] (that if we choose too long common prefix the algorithm will fail to group words such as cat and cats) and consider only word pairs sharing first 3 characters (common prefix). This is the first static parameter (say, l1).
The algorithm comprises of three main modules as follows.

1. Determine the co-occurrence strength of word pairs.

2. Re-calculate the co-occurrence strength using neighbours.

3. Group the words based on their new co-occurrence strength.

The words in each group are likely to be morphologically related. Certainly, such an algorithm does not guarantee that all words in a group are related or all morphological variations will be grouped together. However, we expect that such an approach will reduce the erroneous conflations.
The following explanation will help us to understand more clearly the usefulness of co-occurrence information. Let a and b be two words in a corpus and let df (a) denotes the number of documents in that corpus that contain a. Now if we group a and b together, the effective df (denoted as edf ) of each of the words will be the following.

edf (a) = edf (b) = df (a) + df (b) - df (a  b)

Here, df (a  b) denotes the number of documents in which both a and b occur. Clearly, df (a  b)  min(df (a), df (b)), and the values of edf (a), edf (b) will go up if df (a  b) is low (means a and b co-occurs infrequently). So a ranking function that uses inverse document frequency as a measure of importance of a query term, may influence the ranking heavily because of such merging (putting a and b together). Furthermore, it is more dangerous to group an infrequent term with a frequently occurring term and if such a case happens co-occurrence information controls the effect to some extent.

3.1 Measuring co-occurrence strength
The basic assumption of our method is that a document conveying a topic very often uses a term which is related to the original terms of the topic [23]. For example, a document discussing about education most likely uses the term educate or its variants. Furthermore, in the case of a document discussing about a person very frequently uses its inflectional variants. Since morphological variations are semantically closely related with the meaning of the original terms, their document level co-occurrence provides a very useful information which can be used to group them together.
We define the co-occurrence strength of two words a and b in a corpus C as follows.

CO(a, b) = X min(tfa,d, tfb,d)

(2)

dC

where d and tfp,q represents the document and the term frequency of term p in the document q respectively. In other words, the quantity CO(a, b) measures how often two words a, b occur in the same document of a given corpus C. Note that unlike the co-occurrence measure given by Xu et.al. [22], CO(a, b) does not contain any parameter and therefore is much robust when the corpus or the language is changed.
3.2 Re-calculating co-occurrence strength
Once the co-occurrence strengths between words are computed, the words are then mapped in an undirected graph. A pair of words w1 and w2 is connected by an edge (w1, w2) if CO(w1, w2) > 0 and they have a common prefix of a given length (we chose 3) along with the suffixes (after truncating the longest common prefix) which are the residues (the ends after removal of longest common prefix) of more than one pair of co-occurring words with long common prefix (length larger than 5 which is the second static parameter, say, l2). The second condition is used as a heuristic to predict a morphological variation. The edge (w1, w2) is assigned a weight CO(w1, w2). Re-calculation of co-occurrence strength then operates on this graph and is motivated by the following principle. Two co-occurring words will receive higher weight if both of them co-occur with other words.

With the document level co-occurrence strength in hand, we can re-weight the strengths by the following formula. Here we consider the word pairs (a, b) for which CO(a, b) > 0.

X

RCO(a, b) = CO(a, b)+

min(CO(a, c), CO(c, b))×0.5 (3)

cNa,b

The notation Na,b denotes the set of common neighbours of a and b where a node w is a common neighbour of a and b if CO(a, w) > 0 and CO(b, w) > 0. Clearly, RCO(a, b)  CO(a, b) and they are equal only if a and b have no common co-occurring words. The value of RCO(a, b) depends on two factors given below.

1. The number of common co-occurring words of a and b. In other words, the value of RCO(a, b) is more or less proportional to the size of Na,b.

2. The strength in which they co-occur with the common co-occurring words, i.e, if two words co-occur frequently with a third word, the likelihood of their closeness would be high.

w1

w2

2 1

2

3

1

2

1

w3

w4

w5

w6

Figure 1: Before re-weighting
Figure 1 shows an motivating example underlying the role of co-occurring neighbours. The vertices represent the words and the edge weights represent their co-occurrence strength as computed by equation 2. A set of words is a better candidate to form a semantically related group if their cooccurring words form a community. It is, however, unlikely

866

w1

w2

2.5

1.5

2.5

3.5

1.5

2

2

w3

w4

w5

w6

Figure 2: After re-weighting
that all the words in the group will co-occur with each other, but a word related with the theme of a group, is likely to co-occur with many members of the group. So the information embedded in the neighbourhood of two words are used to better infer the proximity of the word pairs.
As an example, consider graph given in Figure 1. Clearly, solely based on the co-occurrence strength it is hard to determine whether w4 is to be grouped with w1 or w5, because both w1 and w5 have equal co-occurrence strength with w4. But, since w4 has more common neighbours with w1, the chance is high that w4 is semantically more related with w1 than with w5. The re-weighted graph in Figure 2 clears out the grouping dilemma using the measure given in equation 3.
3.3 Grouping the words
So far we have focused on measuring the co-occurrence strength between word pairs. Recall that our ultimate goal is to group morphologically related words. Here we present a simple algorithm based on the strongest neighbour of each word. The following principle is the key to our proposed algorithm. A word (say w1) will be grouped with another word (say w2) if either w1 is the strongest neighbour of w2 or w2 is the strongest neighbour of w1. The algorithm first identifies the strong edges from the given graph. A strong edge is an edge (u, v) if either of the following holds.
· argmax RCO(u, k) equals to v
1kn
· argmax RCO(v, k) equals to u
1kn
n represents the number of vertices of the graph.
Upon doing this, the edges which are not strong are deleted from the graph. Finally, the clusters are formed by finding connected components. Each such cluster represents a morphologically related group. The complete stemming algorithm is given in Figure 3.
The clustering algorithm based on the above principle does not involve any parameter. The parameter free nature of the algorithm makes it more robust unlike other clustering algorithm such as single linkage algorithm, where a threshold is to be set to form the groups. The threshold selection is not a trivial task particularly in a multilingual set up where the parameters are very often language dependent as well as corpus dependent [12, 11].
Note that the proposed algorithm differs from the single linkage clustering algorithm in two respects. First parameter selection is not required and second, the algorithm does not create too many elongated clusters as elongation of clusters primarily depends on the value of the similarity threshold.
To better understand the difference between the proposed strongest neighbour based algorithm and single linkage clus-

Input : A symmetric matrix A = {aij} of order n, where aij = CO(wi, wj ). Output : A set of clusters of words.

1: /* weights from neighbours */

2: for all i, j (1  i < j  n) do

3: if aij > 0 then

4:

let S = {m | aim > 0 and ajm > 0}

5:

bij  P min(aik, akj )

kS

6: end if

7: end for

8: /* adding the weights */

9: for all i, j (1  i < j  n) do

10: if aij > 0 then

11:

sij



aij

+

bij 2

12: end if

13: end for

14: let Es = {} /* the set of strong edges */

15: /* clustering the vertices */

16: for all i (1  i  n) do

17: let p  argmax sik
1kn

18: Es = Es  (i, p)

19: end for

20: Find the connected components considering only edges

in Es

21: Make each connected component a cluster

Figure 3: Stemming algorithm

2

w1

w2

2 w5

4

2

3

w3

1

w6

3

4

w4 2

w7

Figure 4: Input Graph
tering algorithm, consider the example graph of Figure 4. If single linkage clustering algorithm is used without any threshold the entire graph is returned as a cluster (Figure 5). On the contrary, the proposed strongest neighbour based algorithm automatically breaks the edge connecting w3 and w6 (Figure 6), since the nearest neighbours of w3 and w6 are w2 and w7 respectively (alternatively, the edge (w3, w6) is not a strong edge).
4. EVALUATION
In what follows, we describe an array of experiments for evaluating our proposed co-occurrence based stemmer on a number of languages. As part of our experiments we compare the performance of our method with a number of other stemmers (given in section 4.3).
In further explorations, we perform a deeper analysis to understand the impact of the proposed method on individual queries. In addition, we study the goodness of our method in improving precision at the top of the ranked list.

867

2

w1

w2

2 w5

4

2

3

w3

1

w6

3

4

w4 2

w7

Figure 5: Output of single linkage clustering

2

w1

w2

4

2

3 w3

2 w5

w6 3

4

w4 2

w7

Figure 6: Output of proposed algorithm

4.1 Data
The details of our experimental test sets are as follows. We conducted our experiments on six languages of various origin and morphological complexity to evaluate our algorithm. Among the languages studied, Marathi and Hungarian are highly inflectional and agglutinative in nature whereas English is the least inflectional. Table 1 summarizes the corpora and queries used for evaluation. For the English experiments we used the TREC 6-7-8 queries and the document collection comprises of FBIS, Financial Times and Los Angles Times. Both Marathi and Bengali documents and topics have been taken from FIRE1 08 and FIRE 10. Bulgarian and Hungarian documents and topics are taken from CLEF 06 and CLEF 07 and the Czech test sets (queries and documents) are from CLEF 07. 2 topics from Hungarian and 12 topics from Marathi have no relevant documents in the recall base. Therefore, during experiments we removed those queries from our test set. The queries in each language have been formed using title and description fields of the experimental topics. Standard stopword lists are used to remove insignificant words from the documents and queries and no other preprocessing steps were applied.

Table 1: Test Collection Language Documents Queries Source

Bulgarian Czech Hungarian Marathi Bengali English

87281 81735 49530 99362 123047 472525

100 CLEF

50 CLEF

98 CLEF

88

FIRE

100 FIRE

150 TREC

4.2 Evaluation Metrics and IR Model
We used MAP [13] as our primary evaluation metric and in addition we also report R-precision and precision at 10 in order to see the precision enhancing powers of the stemmers. In addition we also report the number of relevant document
1http://www.isical.ac.in/fire

retrieved in each language. Statistical significance tests were performed using paired t-test based on the query wise MAP values at a 95% confidence interval. All experiments have been conducted using a probabilistic divergence from randomness based model called IFB2 [1]. We produce all our experimental results using TERRIER2 retrieval system.
4.3 Baseline Stemmers
We compare the proposed method with three baselines stemmers and also against no stemming. Some recently developed rule based stemmers3 were chosen to compare the performance of our algorithm. The stemmer developed by Dolamic et. al. [4] is chosen as the rule based approach for Marathi and Bengali. The Hungarian stemmer is by Savoy [21] and the Czech stemmer is by Dolamic et. al [3]. Porter stemmer was used as English rule based stemmer. The stemmer proposed by Xu et. al. [22] was used as a baseline corpus analysis based stemmer. The YASS was chosen as the other lexicon analysis based statistical stemmer. The experimental results of YASS are produced using the code4 made available by the authors. We have implemented the stemmer proposed by Xu et. al. We did not implement the stemmer based on optimal partition technique, since that will be very inefficient for the kind of languages we are studying. Instead, we have implemented the algorithm based on connected component and trigram matching (the words sharing first three character).
Note that we have chosen the baseline stemmers which performs very well and also varies in their principle, since one of our goal is to see if the proposed method can be used as a better alternative to the existing stemmers. Some parameter needed to set for XU and YASS. XU depends on two parameters (namely k in em formula and the em score cut off value to determine the connected components) and YASS depends on similarity threshold value for cluster determination. We have chosen k = 2.74 × 10-6 and .01 as em cut off value for XU and for YASS we set the threshold to 1.5 as recommended by the respective authors.
5. RESULTS
In the tables we use the following abbreviations. NO means no stemming, RB for rule based stemmer, XU for the stemmer developed by [22] and PROP used for proposed method.

MAP R-Prec P@10 Rel.Ret

Table 2: Bulgarian NO RB XU YASS 0.2166 0.2794 0.2450 0.3071 0.2293 0.2930 0.2500 0.3161 0.2570 0.3270 0.2750 0.3480 1611 2003 1790 2085
No. of Rel. doc : 2261

PROP 0.3256 0.3289 0.3520 2101

The main experimental results are presented in Tables 2 to 7. Each table gives MAP, R-precision, precision at 10 and the number of relevant documents retrieved. From Table 2 it is clear that, on Bulgarian data all stemmers improved the MAP value. The MAP value achieved by the proposed
2http://terrier.org/ 3available at http://members.unine.ch/jacques.savoy/clef/index.html 4http://www.isical.ac.in/clia/resources.html

868

algorithm is much better than that achieved by either of rule based stemmer or XU. However, the difference is not very large compared to the performance achieved by YASS. R-precision improved consistently with the improvement of MAP.

MAP R-Prec p@10 Rel.Ret

Table 3: Czech NO RB XU YASS 0.2381 0.3409 0.2855 0.3415 0.2611 0.3456 0.2868 0.3295 0.2680 0.3480 0.3040 0.3400 551 667 632 672
No. of Rel. doc : 762

PROP 0.3624 0.3441 0.3700
681

The trend is similar for the languages like Czech (Table 3), Hungarian (Table 4) and (Marathi Table 5). Marathi and Hungarian are two highly inflectional languages and therefore, a significant benefit is achieved. Figure 7 clearly demonstrates the improvement graphically. The experimental results once again validate the conclusions made by Savoy et. al. [21] and Majumder et. al. [11].

MAP R-Prec P@10 Rel.Ret

Table 4: Hungarian NO RB XU YASS 0.2386 0.3132 0.2850 0.3060 0.2518 0.3117 0.3019 0.3152 0.3143 0.3990 0.3388 0.3837 1367 1723 1803 1730
No. of Rel. doc : 2219

PROP 0.3588 0.3585 0.4224 1964

The experimental results obtained by using the proposed approach is noticeably better than XU on Czech (26.9%), Bulgarian (32.9%) and Hungarian (25.9%). Also, the performance differences between the proposed method and YASS are very large on both Marathi (24.6%) and Hungarian (17.3), whereas performance differences are relatively small for languages such as Czech and Bulgarian. The comparison between PROP and no stem reveals that PROP achieves more than 50% MAP on four languages (namely, Czech, Marathi, Bulgarian and Hungarian). On Bengali data this improvement is almost 20%. Although, English results improved through the application of PROP but not as much as other languages, which certainly confirms the previous reported results [12]. The relative percentage (MAP) improvement between the PROP and other methods is given in Table 8.

MAP R-Prec P@10 Rel.Ret

Table 5: Marathi NO RB XU YASS 0.2918 0.3091 0.4097 0.3514 0.3010 0.3114 0.4066 0.3472 0.3273 0.3409 0.3864 0.3568 1463 1472 1621 1546
No. of Rel. doc : 1716

PROP 0.4380 0.4190 0.3977 1653

The stemmer based on language dependent rules has successfully handled morphological variations from retrieval perspective. Porter stemmer is the best among all the stemmers studied on TREC 6-7-8 queries. But when the proposed cooccurrence based stemmer is applied on more inflectional languages, all languages achieve consistently better average

precision than rule based stemmers. A particularly noticeable outcome is the performance of rule based stemmer on Marathi data. When all the languages have been benefited through the rule based stemmers significantly, Marathi results show very marginal improvement compared to no stemming. We suspect that there may be some error in the available code5.

MAP R-Prec P@10 Rel.Ret

Table 6: Bengali NO RB XU YASS 0.3611 0.4104 0.4073 0.4250 0.3582 0.3923 0.3983 0.4024 0.3960 0.4150 0.4060 0.4190 2173 2264 2279 2272
No. of Rel. doc : 2373

PROP 0.4310 0.4153 0.4220
2275

Figure 8 shows the precision at 10 achieved by the chosen approaches on all data sets. The observation that can be made from the figure is that once again the precision at top for more inflectional languages is noticeably high when the proposed method is applied. Comparison between PROP and XU shows that on Hungarian, Czech and Bulgarian PROP is noticeably better than XU. However, the differences are relatively small on Marathi, Bengali and English. On English, Porter stemmer achieves the maximum p@10 which is slightly better than the others. Rule based stemmer also does fairly better (compared to no stem) on Bengali, Czech, Hungarian and English, but poorly performs on Marathi and Bulgarian.
We further study the recall enhancing ability of the various methods studied. In Figure 9 we show the number of relevant documents retrieved at 1000. Clearly, except Bulgarian XU retrieved almost an equal number of documents with PROP and even better in English. PROP retrieved maximum number of documents in Hungarian and also in Marathi.
Table 9 provides the percentage of query in which the proposed method achieves better MAP than the other methods including the un-stemmed runs on all six languages. More than 80% of Marathi and Hungarian queries have been benefited if the words are normalized using the proposed co-occurrence based method compare to the un-normalized case. Substantial amount (close to 80%) of Bulgarian and Czech queries also achieves better average precision due to the use of the proposed stemmer. However, stemming hurts many Bengali and English queries. Particularly, almost onethird English queries have been hurt for using the proposed method. When, we compare the query-wise performance, we observed that in all other languages (except English), the proposed method outperformed the other stemmers. Only in English, Porter stemmer performed marginally better than the proposed method.
The outcomes of statistical significance test is summarized in Table 10. A star (or a cross) symbol in a cell (x, y) means the proposed method performs statistically significantly better (or statistically equally) than the method y on language x. Clearly, the choice of the proposed method over no stemming is always significantly beneficial to an IR system. YASS and the proposed method performed statistically equally on Bengali and English data. Furthermore, no significant difference is found between the proposed approach
5http://members.unine.ch/jacques.savoy/clef/index.html

869

MAP R-Prec P@10 Rel.Ret

Table 7: TREC 6-7-8

NO

RB

XU YASS

0.2290 0.2599 0.2504 0.2499

0.2733 0.3008 0.2934 0.2913

0.4327 0.4833 0.4600 0.4633

6812 7751 7751 7652

No. of Rel. doc : 12848

PROP 0.2582 0.3001 0.4727 7638

and both the Porter and XU stemmer on English data. But on all other languages the performance differences of the proposed method are statistically significant than the other methods (only exception is Czech where the performance difference between RB and PROP is not significant).

Table 8: Better MAP (%) by PROP

NO RB XU YASS

Bulgarian 50.3 16.5 32.9 6.0

Czech

52.2 6.3 26.9 6.1

Hungarian 50.4 14.6 25.9 17.3

Marathi 50.1 41.7 6.9 24.6

Bengali 19.4 5.0 5.8 1.4

English 12.8 -0.7 3.1 3.3

Table 9: Query (%) better by PROP

NO RB XU YASS

Bulgarian 79 61 78 58

Czech

78 64 80 56

Hungarian 85 58 69 64

Marathi 81 74 56 68

Bengali

72 57 61 53

English

63 47 55 56

6. DISCUSSION
In the previous section we have reported the experimental results only. In this section we shed some light on the possible merits and demerits of each of the methods.
We have already pointed out some cases where the rule based stemmer falters for less morphologically complex languages like English. But such errors may be more detrimental for complex languages. Rule based stemmers generally remove some hand picked suffixes from word endings and in doing so it may commit serious mistake by removing a valid word ending treating it as a suffix. Sometimes, it also fails to conflate related terms. Therefore, co-occurrence analysis can be a choice to reduce such errors.
Now, consider the case of XU. Almost on all languages it performed quite well in terms of retrieving number of relevant documents. We believe this is because of the kind of graph partitioning technique used to group morphologically related words. The connected component algorithm used by XU, creates long classes and therefore, although it enhances recall but hurts the precision, which ultimately leads to the overall degradation of MAP values.
Turning to YASS, we have noticed that it performs very well on Bengali, Czech and Bulgarian but performs less effectively on Hungarian and Marathi. It does not suffer from

0.45 0.4
0.35 0.3
0.25 0.2
0.15 0.1
0.05 0 Marathi Hungarian Czech Bulgarian Bengali TREC 6-8

NO RB XU YASS PROP

Figure 7: Mean average precision

0.5 0.45
0.4 0.35
0.3 0.25
0.2 0.15
0.1 0.05
0 Marathi Hungarian Czech Bulgarian Bengali TREC 6-8

NO RB XU YASS PROP

Figure 8: Precision at 10

8000 7000 6000 5000 4000 3000 2000 1000
0 Marathi Hungarian Czech Bulgarian Bengali TREC 6-8

NO RB XU YASS PROP

Figure 9: Relevant retrieved

870

Table 10: Statistical Significance

NO RB XU YASS

Bulgarian   



Czech

×



Hungarian   



Marathi





Bengali



×

English

 ×× ×

Table 11: Re-weighting vs. no re-weighting Language No Reweight Reweight

Bulgarian Czech Hungarian Marathi Bengali English

0.3087 0.3470 0.3220 0.4122 0.4131 0.2536

0.3256 0.3624 0.3588 0.4380 0.4310 0.2586

the problem of chaining in creating clusters, but has some other limitations. First, the string distance measure falters when suffix lengths are long, which is very common in agglutinative languages (private and privatization are kept in separate groups because of the long suffix ization at the end of privatization). Secondly, the string distance measure returns the same similarity value for the word pairs like {cat, cats} and {fat, fate}, which clearly is not right. In addition, complete linkage clustering produces different clusters of same objects if simply the objects are considered in different order, this erratically splits a valid cluster. Possibly, these are the main reasons why YASS is less effective on Marathi and Hungarian.
One important module of the proposed algorithm is the re-calculation of the initial co-occurrence strength. The recalculated strength more clearly discriminates a word association from the other co-occurring word. The nearest neighbour based algorithm, coupled with the re-calculated weight, more accurately forms the clusters. Table 11 demonstrates the usability of re-weighting approach. Clearly, on all languages the re-calculated weights improved the performance of the algorithm.
The proposed method has two parameters, namely, l1 and l2. We conducted our initial experiments with English and Bengali by setting l1 = 3 and l2 = 5 and then extended our experiments with other four languages under the same settings. A noticeable loss is observed for a larger value of l1, since a larger value of l1 leads to fragmentation of some valid groups. On the other hand, l2 = 6, 7, 8 led to marginal fluctuations in accuracy. For some languages, there was insignificant improvement while for the rest, there was insignificant loss. Finally, the choice of 0.5 in equation 3, was arbitrary and we did not experiment with any other value of this factor.
7. CONCLUSION
In this paper we have proposed a stemming algorithm based on co-occurrence of words in a corpus. One important characteristic of our algorithm is that its parameters are static. A set of experiments on several standard data sets shows that retrieval performance is significantly en-

hanced through the use of the method and in more than half of the languages the proposed approach outperformed the stemmers based on the language specific knowledge. Also, the method performed better than two other corpus based strong stemmers on all the languages. Further analysis reveals that the proposed stemmer outperforms the competing methods in large number of queries. Based on these results, we believe that the proposed method can be used as a better alternative to the rule based stemmers.
Acknowledgments
We are very grateful to Mandar Mitra for valuable discussions. We thank SIGIR and Singhal Family (Donald B. Crouch Travel Grant) for the student travel grant to the first author.
8. REFERENCES
[1] G. Amati and C. J. Van Rijsbergen. Probabilistic models of information retrieval based on measuring the divergence from randomness. ACM Trans. Inf. Syst., 20(4):357­389, 2002.
[2] M. Bacchin, N. Ferro, and M. Melucci. A probabilistic model for stemmer generation. Inf. Process. Manage., 41(1):121­137, 2005.
[3] L. Dolamic and J. Savoy. Indexing and stemming approaches for the czech language. Inf. Process. Manage., 45(6):714­720, 2009.
[4] L. Dolamic and J. Savoy. Comparative study of indexing and search strategies for the hindi, marathi, and bengali languages. ACM Trans. Asian Lang. Inf. Process., 9(3), 2010.
[5] J. Goldsmith. Unsupervised learning of the morphology of a natural language. Comput. Linguist., 27(2):153­198, 2001.
[6] D. Harman. How effective is suffixing. Journal of the American Society for Information Science, 42:7­15, 1991.
[7] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering: a review. ACM Comput. Surv., 31:264­323, September 1999.
[8] R. Krovetz. Viewing morphology as an inference process. In SIGIR '93: Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, pages 191­202, New York, NY, USA, 1993. ACM.
[9] M. Lennon, D. S. Pierce, B. D. Tarry, and P. Willett. An evaluation of some conflation algorithms for information retrieval, pages 99­105. Taylor Graham Publishing, London, UK, UK, 1988.
[10] J. Lovins. Development of a stemming algorithm. Mech. Tran. Comput. Linguistics, pages 22­31, 1968.
[11] P. Majumder, M. Mitra, and D. Pal. Bulgarian, hungarian and czech stemming using yass. In Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007, Budapest, Hungary, September 19-21, 2007, Revised Selected Papers, pages 49­56, Berlin, Heidelberg, 2008. Springer-Verlag.
[12] P. Majumder, M. Mitra, S. K. Parui, G. Kole, P. Mitra, and K. Datta. Yass: Yet another suffix stripper. ACM Trans. Inf. Syst., 25(4):18, 2007.

871

[13] C. D. Manning, P. Raghavan, and H. Schutze. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA, 2008.
[14] J. Mayfield and P. McNamee. Single n-gram stemming. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR '03, pages 415­416, New York, NY, USA, 2003. ACM.
[15] P. Mcnamee and J. Mayfield. Character n-gram tokenization for european language text retrieval. Inf. Retr., 7(1-2):73­97, 2004.
[16] P. McNamee, C. K. Nicholas, and J. Mayfield. Addressing morphological variation in alphabetic languages. In SIGIR, pages 75­82, 2009.
[17] D. W. Oard, G.-A. Levow, and C. I. Cabezas. Clef experiments at maryland: Statistical stemming and backoff translation. In CLEF, pages 176­187, 2000.
[18] F. Peng, N. Ahmed, X. Li, and Y. Lu. Context sensitive stemming for web search. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR '07, pages 639­646, New York, NY, USA, 2007. ACM.

[19] M. F. Porter. An algorithm for suffix stripping. pages 313­316, 1997.
[20] J. Savoy. Light stemming approaches for the french, portuguese, german and hungarian languages. In Proceedings of the 2006 ACM symposium on Applied computing, SAC '06, pages 1031­1035, New York, NY, USA, 2006. ACM.
[21] J. Savoy. Searching strategies for the hungarian language. Inf. Process. Manage., 44(1):310­324, 2008.
[22] J. Xu and W. B. Croft. Corpus-based stemming using cooccurrence of word variants. ACM Trans. Inf. Syst., 16(1):61­81, 1998.
[23] J. Xu and W. B. Croft. Improving the effectiveness of information retrieval with local context analysis. ACM Trans. Inf. Syst., 18:79­112, January 2000.

872

